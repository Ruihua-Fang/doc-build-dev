import{D as Sf,S as Zs,i as Gs,s as Qs,O as z,P as A,a as t,d as a,b as o,g as m,F as e,L as es,t as i,h,e as c,w as E,k as d,c as u,x as q,m as b,y as x,Q as Ti,q as v,o as $,B as y,n as Un,p as Kn,U as Lf,v as Df,V as Kp,W as Tf,X as If,H as bl,I as jl,J as gl,K as _l,M as Of,N as Ef}from"../chunks/vendor-5fc3b424.js";import{T as Mf}from"../chunks/Tip-12425c03.js";import{Y as Hf}from"../chunks/Youtube-22419068.js";import{I as Q,C as S}from"../chunks/CodeBlock-f0535003.js";import{D as Ff}from"../chunks/DocNotebookDropdown-d9b39aeb.js";var As=(g=>(g.OPEN="OPEN",g.CLOSED="CLOSED",g.HASHASHLINK="HASHASHLINK",g))(As||{});const Li={};function Jf(g){return Li[g]||(Li[g]=Sf("OPEN")),Li[g]}function Rf(g){let n,f,r,l,j,_;return{c(){n=z("svg"),f=z("defs"),r=z("clipPath"),l=z("rect"),j=z("g"),_=z("path"),this.h()},l(k){n=A(k,"svg",{class:!0,xmlns:!0,"xmlns:xlink":!0,"aria-hidden":!0,focusable:!0,role:!0,width:!0,height:!0,preserveAspectRatio:!0,viewBox:!0});var w=t(n);f=A(w,"defs",{});var C=t(f);r=A(C,"clipPath",{id:!0});var L=t(r);l=A(L,"rect",{x:!0,y:!0,width:!0,height:!0,fill:!0}),t(l).forEach(a),L.forEach(a),C.forEach(a),j=A(w,"g",{"clip-path":!0});var P=t(j);_=A(P,"path",{d:!0,fill:!0}),t(_).forEach(a),P.forEach(a),w.forEach(a),this.h()},h(){o(l,"x","3.05"),o(l,"y","0.5"),o(l,"width","25.73"),o(l,"height","31"),o(l,"fill","none"),o(r,"id","a"),o(_,"d","M24.94,9.51a12.81,12.81,0,0,1,0,18.16,12.68,12.68,0,0,1-18,0,12.81,12.81,0,0,1,0-18.16l9-9V5l-.84.83-6,6a9.58,9.58,0,1,0,13.55,0ZM20.44,9a1.68,1.68,0,1,1,1.67-1.67A1.68,1.68,0,0,1,20.44,9Z"),o(_,"fill","#ee4c2c"),o(j,"clip-path","url(#a)"),o(n,"class",g[0]),o(n,"xmlns","http://www.w3.org/2000/svg"),o(n,"xmlns:xlink","http://www.w3.org/1999/xlink"),o(n,"aria-hidden","true"),o(n,"focusable","false"),o(n,"role","img"),o(n,"width","1em"),o(n,"height","1em"),o(n,"preserveAspectRatio","xMidYMid meet"),o(n,"viewBox","0 0 32 32")},m(k,w){m(k,n,w),e(n,f),e(f,r),e(r,l),e(n,j),e(j,_)},p(k,[w]){w&1&&o(n,"class",k[0])},i:es,o:es,d(k){k&&a(n)}}}function Bf(g,n,f){let{classNames:r=""}=n;return g.$$set=l=>{"classNames"in l&&f(0,r=l.classNames)},[r]}class Vf extends Zs{constructor(n){super();Gs(this,n,Bf,Rf,Qs,{classNames:0})}}function Wf(g){let n,f,r,l;return{c(){n=z("svg"),f=z("path"),r=z("path"),l=z("path"),this.h()},l(j){n=A(j,"svg",{class:!0,xmlns:!0,"xmlns:xlink":!0,"aria-hidden":!0,focusable:!0,role:!0,width:!0,height:!0,preserveAspectRatio:!0,viewBox:!0});var _=t(n);f=A(_,"path",{d:!0,fill:!0}),t(f).forEach(a),r=A(_,"path",{d:!0,fill:!0}),t(r).forEach(a),l=A(_,"path",{d:!0,fill:!0}),t(l).forEach(a),_.forEach(a),this.h()},h(){o(f,"d","M145.726 42.065v42.07l72.861 42.07v-42.07l-72.86-42.07zM0 84.135v42.07l36.43 21.03V105.17L0 84.135zm109.291 21.035l-36.43 21.034v126.2l36.43 21.035v-84.135l36.435 21.035v-42.07l-36.435-21.034V105.17z"),o(f,"fill","#E55B2D"),o(r,"d","M145.726 42.065L36.43 105.17v42.065l72.861-42.065v42.065l36.435-21.03v-84.14zM255.022 63.1l-36.435 21.035v42.07l36.435-21.035V63.1zm-72.865 84.135l-36.43 21.035v42.07l36.43-21.036v-42.07zm-36.43 63.104l-36.436-21.035v84.135l36.435-21.035V210.34z"),o(r,"fill","#ED8E24"),o(l,"d","M145.726 0L0 84.135l36.43 21.035l109.296-63.105l72.861 42.07L255.022 63.1L145.726 0zm0 126.204l-36.435 21.03l36.435 21.036l36.43-21.035l-36.43-21.03z"),o(l,"fill","#F8BF3C"),o(n,"class",g[0]),o(n,"xmlns","http://www.w3.org/2000/svg"),o(n,"xmlns:xlink","http://www.w3.org/1999/xlink"),o(n,"aria-hidden","true"),o(n,"focusable","false"),o(n,"role","img"),o(n,"width","0.94em"),o(n,"height","1em"),o(n,"preserveAspectRatio","xMidYMid meet"),o(n,"viewBox","0 0 256 274")},m(j,_){m(j,n,_),e(n,f),e(n,r),e(n,l)},p(j,[_]){_&1&&o(n,"class",j[0])},i:es,o:es,d(j){j&&a(n)}}}function Uf(g,n,f){let{classNames:r=""}=n;return g.$$set=l=>{"classNames"in l&&f(0,r=l.classNames)},[r]}class Kf extends Zs{constructor(n){super();Gs(this,n,Uf,Wf,Qs,{classNames:0})}}function Yf(g){let n,f,r,l,j,_,k,w,C,L,P,I,T,V,W,F,D,O,J,U,N,H,R,M,ns,Z,X,us,ts,Ps,is,hs,G,ls,fs,ss,Cs,K,Ds,Ns,ds;return{c(){n=z("svg"),f=z("style"),r=i(`.J {
			stroke: #dce0df;
		}
		.K {
			stroke-linejoin: round;
		}
	`),l=z("g"),j=z("path"),_=z("path"),k=z("path"),w=z("path"),C=z("path"),L=z("path"),P=z("path"),I=z("path"),T=z("g"),V=z("path"),W=z("path"),F=z("path"),D=z("g"),O=z("path"),J=z("path"),U=z("path"),N=z("g"),H=z("path"),R=z("path"),M=z("g"),ns=z("path"),Z=z("path"),X=z("path"),us=z("path"),ts=z("path"),Ps=z("path"),is=z("path"),hs=z("path"),G=z("g"),ls=z("path"),fs=z("path"),ss=z("path"),Cs=z("path"),K=z("g"),Ds=z("path"),Ns=z("path"),ds=z("path"),this.h()},l(Y){n=A(Y,"svg",{class:!0,xmlns:!0,"xmlns:xlink":!0,"aria-hidden":!0,focusable:!0,role:!0,width:!0,height:!0,preserveAspectRatio:!0,viewBox:!0});var B=t(n);f=A(B,"style",{});var rs=t(f);r=h(rs,`.J {
			stroke: #dce0df;
		}
		.K {
			stroke-linejoin: round;
		}
	`),rs.forEach(a),l=A(B,"g",{fill:!0,class:!0});var ps=t(l);j=A(ps,"path",{d:!0}),t(j).forEach(a),_=A(ps,"path",{d:!0}),t(_).forEach(a),k=A(ps,"path",{d:!0}),t(k).forEach(a),w=A(ps,"path",{d:!0}),t(w).forEach(a),C=A(ps,"path",{d:!0}),t(C).forEach(a),L=A(ps,"path",{d:!0}),t(L).forEach(a),P=A(ps,"path",{d:!0}),t(P).forEach(a),I=A(ps,"path",{d:!0}),t(I).forEach(a),ps.forEach(a),T=A(B,"g",{fill:!0,class:!0});var bs=t(T);V=A(bs,"path",{d:!0}),t(V).forEach(a),W=A(bs,"path",{d:!0}),t(W).forEach(a),F=A(bs,"path",{d:!0}),t(F).forEach(a),bs.forEach(a),D=A(B,"g",{fill:!0,class:!0});var Ca=t(D);O=A(Ca,"path",{d:!0}),t(O).forEach(a),J=A(Ca,"path",{d:!0}),t(J).forEach(a),Ca.forEach(a),U=A(B,"path",{d:!0,fill:!0,class:!0}),t(U).forEach(a),N=A(B,"g",{fill:!0,class:!0});var Da=t(N);H=A(Da,"path",{d:!0}),t(H).forEach(a),R=A(Da,"path",{d:!0}),t(R).forEach(a),Da.forEach(a),M=A(B,"g",{fill:!0,class:!0});var as=t(M);ns=A(as,"path",{d:!0}),t(ns).forEach(a),Z=A(as,"path",{d:!0}),t(Z).forEach(a),X=A(as,"path",{d:!0}),t(X).forEach(a),us=A(as,"path",{d:!0}),t(us).forEach(a),ts=A(as,"path",{d:!0}),t(ts).forEach(a),Ps=A(as,"path",{d:!0}),t(Ps).forEach(a),is=A(as,"path",{d:!0}),t(is).forEach(a),as.forEach(a),hs=A(B,"path",{d:!0,fill:!0,class:!0}),t(hs).forEach(a),G=A(B,"g",{fill:!0,class:!0});var Ss=t(G);ls=A(Ss,"path",{d:!0}),t(ls).forEach(a),fs=A(Ss,"path",{d:!0}),t(fs).forEach(a),ss=A(Ss,"path",{d:!0}),t(ss).forEach(a),Cs=A(Ss,"path",{d:!0}),t(Cs).forEach(a),Ss.forEach(a),K=A(B,"g",{fill:!0,class:!0});var Xs=t(K);Ds=A(Xs,"path",{d:!0}),t(Ds).forEach(a),Ns=A(Xs,"path",{d:!0}),t(Ns).forEach(a),ds=A(Xs,"path",{d:!0}),t(ds).forEach(a),Xs.forEach(a),B.forEach(a),this.h()},h(){o(j,"d","M50.5 130.4l-25 43.31h50l25-43.31h-50z"),o(_,"d","M.5 217.01l25-43.3h50l-25 43.3H.5z"),o(k,"d","M125.5 173.71h-50l-25 43.3h50l25-43.3z"),o(w,"d","M175.5 173.71h-50l-25 43.3h50l25-43.3z"),o(C,"d","M150.5 130.4l-25 43.31h50l25-43.31h-50z"),o(L,"d","M175.5 87.1l-25 43.3h50l25-43.3h-50z"),o(P,"d","M200.5 43.8l-25 43.3h50l25-43.3h-50z"),o(I,"d","M225.5.5l-25 43.3h50l25-43.3h-50z"),o(l,"fill","#5e97f6"),o(l,"class","J K"),o(V,"d","M.5 217.01l25 43.3h50l-25-43.3H.5z"),o(W,"d","M125.5 260.31h-50l-25-43.3h50l25 43.3z"),o(F,"d","M175.5 260.31h-50l-25-43.3h50l25 43.3z"),o(T,"fill","#2a56c6"),o(T,"class","J K"),o(O,"d","M200.5 217.01l-25-43.3-25 43.3 25 43.3 25-43.3zm50-86.61l-25-43.3-25 43.3h50z"),o(J,"d","M250.5 43.8l-25 43.3 25 43.3 25-43.3-25-43.3z"),o(D,"fill","#00796b"),o(D,"class","J K"),o(U,"d","M125.5 173.71l-25-43.31-25 43.31h50z"),o(U,"fill","#3367d6"),o(U,"class","J K"),o(H,"d","M250.5 130.4h-50l-25 43.31h50l25-43.31z"),o(R,"d","M300.5 130.4h-50l-25 43.31h50l25-43.31z"),o(N,"fill","#26a69a"),o(N,"class","J K"),o(ns,"d","M350.5 43.8L325.5.5l-25 43.3 25 43.3 25-43.3z"),o(Z,"d","M375.5 87.1l-25-43.3-25 43.3 25 43.3 25-43.3z"),o(X,"d","M400.5 130.4l-25-43.3-25 43.3 25 43.31 25-43.31z"),o(us,"d","M425.5 173.71l-25-43.31-25 43.31 25 43.3 25-43.3z"),o(ts,"d","M450.5 217.01l-25-43.3-25 43.3 25 43.3 25-43.3zM425.5.5l-25 43.3 25 43.3 25-43.3-25-43.3z"),o(Ps,"d","M375.5 87.1l25-43.3 25 43.3-25 43.3-25-43.3zm-25 43.3l-25 43.31 25 43.3 25-43.3-25-43.31z"),o(is,"d","M325.5 260.31l-25-43.3 25-43.3 25 43.3-25 43.3z"),o(M,"fill","#9c27b0"),o(M,"class","J K"),o(hs,"d","M275.5 260.31l-25-43.3h50l25 43.3h-50z"),o(hs,"fill","#6a1b9a"),o(hs,"class","J K"),o(ls,"d","M225.5 173.71h-50l25 43.3h50l-25-43.3z"),o(fs,"d","M275.5 173.71h-50l25 43.3 25-43.3zm0-86.61l25 43.3h50l-25-43.3h-50z"),o(ss,"d","M300.5 43.8h-50l25 43.3h50l-25-43.3zm125 216.51l-25-43.3h-50l25 43.3h50z"),o(Cs,"d","M375.5 173.71l-25 43.3h50l-25-43.3z"),o(G,"fill","#00695c"),o(G,"class","J K"),o(Ds,"d","M325.5.5h-50l-25 43.3h50l25-43.3zm0 173.21h-50l-25 43.3h50l25-43.3z"),o(Ns,"d","M350.5 130.4h-50l-25 43.31h50l25-43.31zM425.5.5h-50l-25 43.3h50l25-43.3z"),o(ds,"d","M375.5 87.1l-25-43.3h50l-25 43.3z"),o(K,"fill","#ea80fc"),o(K,"class","J K"),o(n,"class",g[0]),o(n,"xmlns","http://www.w3.org/2000/svg"),o(n,"xmlns:xlink","http://www.w3.org/1999/xlink"),o(n,"aria-hidden","true"),o(n,"focusable","false"),o(n,"role","img"),o(n,"width","1.73em"),o(n,"height","1em"),o(n,"preserveAspectRatio","xMidYMid meet"),o(n,"viewBox","0 0 451 260.81")},m(Y,B){m(Y,n,B),e(n,f),e(f,r),e(n,l),e(l,j),e(l,_),e(l,k),e(l,w),e(l,C),e(l,L),e(l,P),e(l,I),e(n,T),e(T,V),e(T,W),e(T,F),e(n,D),e(D,O),e(D,J),e(n,U),e(n,N),e(N,H),e(N,R),e(n,M),e(M,ns),e(M,Z),e(M,X),e(M,us),e(M,ts),e(M,Ps),e(M,is),e(n,hs),e(n,G),e(G,ls),e(G,fs),e(G,ss),e(G,Cs),e(n,K),e(K,Ds),e(K,Ns),e(K,ds)},p(Y,[B]){B&1&&o(n,"class",Y[0])},i:es,o:es,d(Y){Y&&a(n)}}}function Zf(g,n,f){let{classNames:r=""}=n;return g.$$set=l=>{"classNames"in l&&f(0,r=l.classNames)},[r]}class Gf extends Zs{constructor(n){super();Gs(this,n,Zf,Yf,Qs,{classNames:0})}}function Qf(g){let n,f;return{c(){n=z("svg"),f=z("path"),this.h()},l(r){n=A(r,"svg",{class:!0,width:!0,height:!0,viewBox:!0,fill:!0,xmlns:!0});var l=t(n);f=A(l,"path",{d:!0,fill:!0}),t(f).forEach(a),l.forEach(a),this.h()},h(){o(f,"d","M0 4.50001C0.390979 2.37042 2.25728 0.756592 4.5 0.756592C6.74272 0.756592 8.60861 2.37042 9 4.50001C8.60902 6.62959 6.74272 8.24342 4.5 8.24342C2.25728 8.24342 0.391395 6.62959 0 4.50001ZM4.5 6.57968C5.05156 6.57968 5.58054 6.36057 5.97055 5.97056C6.36057 5.58054 6.57967 5.05157 6.57967 4.50001C6.57967 3.94844 6.36057 3.41947 5.97055 3.02945C5.58054 2.63944 5.05156 2.42033 4.5 2.42033C3.94844 2.42033 3.41946 2.63944 3.02945 3.02945C2.63943 3.41947 2.42033 3.94844 2.42033 4.50001C2.42033 5.05157 2.63943 5.58054 3.02945 5.97056C3.41946 6.36057 3.94844 6.57968 4.5 6.57968ZM4.5 5.74781C4.16906 5.74781 3.85168 5.61635 3.61767 5.38234C3.38366 5.14833 3.2522 4.83094 3.2522 4.50001C3.2522 4.16907 3.38366 3.85168 3.61767 3.61767C3.85168 3.38367 4.16906 3.2522 4.5 3.2522C4.83094 3.2522 5.14832 3.38367 5.38233 3.61767C5.61634 3.85168 5.7478 4.16907 5.7478 4.50001C5.7478 4.83094 5.61634 5.14833 5.38233 5.38234C5.14832 5.61635 4.83094 5.74781 4.5 5.74781Z"),o(f,"fill","currentColor"),o(n,"class",g[0]),o(n,"width",g[1]),o(n,"height",g[1]),o(n,"viewBox","0 0 9 9"),o(n,"fill","currentColor"),o(n,"xmlns","http://www.w3.org/2000/svg")},m(r,l){m(r,n,l),e(n,f)},p(r,[l]){l&1&&o(n,"class",r[0]),l&2&&o(n,"width",r[1]),l&2&&o(n,"height",r[1])},i:es,o:es,d(r){r&&a(n)}}}function Xf(g,n,f){let{classNames:r=""}=n,{size:l="1em"}=n;return g.$$set=j=>{"classNames"in j&&f(0,r=j.classNames),"size"in j&&f(1,l=j.size)},[r,l]}class sd extends Zs{constructor(n){super();Gs(this,n,Xf,Qf,Qs,{classNames:0,size:1})}}function ad(g){let n,f;return{c(){n=z("svg"),f=z("path"),this.h()},l(r){n=A(r,"svg",{class:!0,width:!0,height:!0,viewBox:!0,fill:!0,xmlns:!0});var l=t(n);f=A(l,"path",{d:!0,fill:!0}),t(f).forEach(a),l.forEach(a),this.h()},h(){o(f,"d","M1.39125 1.9725L0.0883333 0.669997L0.677917 0.0804138L8.9275 8.33041L8.33792 8.91958L6.95875 7.54041C6.22592 8.00523 5.37572 8.25138 4.50792 8.25C2.26125 8.25 0.392083 6.63333 0 4.5C0.179179 3.52946 0.667345 2.64287 1.39167 1.9725H1.39125ZM5.65667 6.23833L5.04667 5.62833C4.81335 5.73996 4.55116 5.77647 4.29622 5.73282C4.04129 5.68918 3.80617 5.56752 3.62328 5.38463C3.44039 5.20175 3.31874 4.96663 3.27509 4.71169C3.23144 4.45676 3.26795 4.19456 3.37958 3.96125L2.76958 3.35125C2.50447 3.75187 2.38595 4.2318 2.4341 4.70978C2.48225 5.18777 2.6941 5.63442 3.0338 5.97411C3.37349 6.31381 3.82015 6.52567 4.29813 6.57382C4.77611 6.62197 5.25605 6.50345 5.65667 6.23833ZM2.83042 1.06666C3.35 0.862497 3.91625 0.749997 4.50792 0.749997C6.75458 0.749997 8.62375 2.36666 9.01583 4.5C8.88816 5.19404 8.60119 5.84899 8.1775 6.41333L6.56917 4.805C6.61694 4.48317 6.58868 4.15463 6.48664 3.84569C6.3846 3.53675 6.21162 3.256 5.98156 3.02594C5.7515 2.79588 5.47075 2.6229 5.16181 2.52086C4.85287 2.41882 4.52433 2.39056 4.2025 2.43833L2.83042 1.06708V1.06666Z"),o(f,"fill","currentColor"),o(n,"class",g[0]),o(n,"width",g[1]),o(n,"height",g[1]),o(n,"viewBox","0 0 10 9"),o(n,"fill","currentColor"),o(n,"xmlns","http://www.w3.org/2000/svg")},m(r,l){m(r,n,l),e(n,f)},p(r,[l]){l&1&&o(n,"class",r[0]),l&2&&o(n,"width",r[1]),l&2&&o(n,"height",r[1])},i:es,o:es,d(r){r&&a(n)}}}function ed(g,n,f){let{classNames:r=""}=n,{size:l="1em"}=n;return g.$$set=j=>{"classNames"in j&&f(0,r=j.classNames),"size"in j&&f(1,l=j.size)},[r,l]}class nd extends Zs{constructor(n){super();Gs(this,n,ed,ad,Qs,{classNames:0,size:1})}}const{window:td}=Tf;function xf(g){let n,f,r,l,j,_,k,w,C,L;return f=new nd({props:{size:"0.9em"}}),{c(){n=c("div"),E(f.$$.fragment),r=d(),l=c("span"),j=i("Hide "),_=i(g[3]),k=i(" content"),this.h()},l(P){n=u(P,"DIV",{class:!0});var I=t(n);q(f.$$.fragment,I),r=b(I),l=u(I,"SPAN",{});var T=t(l);j=h(T,"Hide "),_=h(T,g[3]),k=h(T," content"),T.forEach(a),I.forEach(a),this.h()},h(){o(n,"class","cursor-pointer flex items-center justify-center space-x-1 text-sm px-2 bg-white dark:bg-gray-950 hover:underline leading-none")},m(P,I){m(P,n,I),x(f,n,null),e(n,r),e(n,l),e(l,j),e(l,_),e(l,k),w=!0,C||(L=Ti(n,"click",g[5]),C=!0)},p:es,i(P){w||(v(f.$$.fragment,P),w=!0)},o(P){$(f.$$.fragment,P),w=!1},d(P){P&&a(n),y(f),C=!1,L()}}}function ld(g){let n,f;const r=g[10].default,l=bl(r,g,g[9],null);return{c(){n=c("div"),l&&l.c(),this.h()},l(j){n=u(j,"DIV",{class:!0});var _=t(n);l&&l.l(_),_.forEach(a),this.h()},h(){o(n,"class","framework-content")},m(j,_){m(j,n,_),l&&l.m(n,null),f=!0},p(j,_){l&&l.p&&(!f||_&512)&&jl(l,r,j,j[9],f?_l(r,j[9],_,null):gl(j[9]),null)},i(j){f||(v(l,j),f=!0)},o(j){$(l,j),f=!1},d(j){j&&a(n),l&&l.d(j)}}}function rd(g){let n,f,r,l,j,_,k,w,C,L;return f=new sd({props:{size:"0.9em"}}),{c(){n=c("div"),E(f.$$.fragment),r=d(),l=c("span"),j=i("Show "),_=i(g[3]),k=i(" content"),this.h()},l(P){n=u(P,"DIV",{class:!0});var I=t(n);q(f.$$.fragment,I),r=b(I),l=u(I,"SPAN",{});var T=t(l);j=h(T,"Show "),_=h(T,g[3]),k=h(T," content"),T.forEach(a),I.forEach(a),this.h()},h(){o(n,"class","cursor-pointer mt-[-12.5px] flex items-center justify-center space-x-1 py-4 text-sm hover:underline leading-none")},m(P,I){m(P,n,I),x(f,n,null),e(n,r),e(n,l),e(l,j),e(l,_),e(l,k),w=!0,C||(L=Ti(n,"click",g[5]),C=!0)},p:es,i(P){w||(v(f.$$.fragment,P),w=!0)},o(P){$(f.$$.fragment,P),w=!1},d(P){P&&a(n),y(f),C=!1,L()}}}function pd(g){let n,f,r,l,j,_,k,w,C,L,P,I,T,V;var W=g[2];function F(N){return{}}W&&(l=new W(F()));let D=!g[1]&&xf(g);const O=[rd,ld],J=[];function U(N,H){return N[1]?0:1}return L=U(g),P=J[L]=O[L](g),{c(){n=c("div"),f=c("div"),r=c("div"),l&&E(l.$$.fragment),j=d(),_=c("span"),k=i(g[3]),w=d(),D&&D.c(),C=d(),P.c(),this.h()},l(N){n=u(N,"DIV",{class:!0});var H=t(n);f=u(H,"DIV",{class:!0});var R=t(f);r=u(R,"DIV",{class:!0});var M=t(r);l&&q(l.$$.fragment,M),j=b(M),_=u(M,"SPAN",{});var ns=t(_);k=h(ns,g[3]),ns.forEach(a),M.forEach(a),w=b(R),D&&D.l(R),R.forEach(a),C=b(H),P.l(H),H.forEach(a),this.h()},h(){o(r,"class","flex px-1 items-center space-x-1 bg-white dark:bg-gray-950"),o(f,"class","flex h-[22px] mt-[-12.5px] justify-between leading-none"),o(n,"class","border border-gray-200 rounded-xl px-4 relative")},m(N,H){m(N,n,H),e(n,f),e(f,r),l&&x(l,r,null),e(r,j),e(r,_),e(_,k),e(f,w),D&&D.m(f,null),e(n,C),J[L].m(n,null),g[11](n),I=!0,T||(V=Ti(td,"hashchange",g[6]),T=!0)},p(N,[H]){if(W!==(W=N[2])){if(l){Un();const M=l;$(M.$$.fragment,1,0,()=>{y(M,1)}),Kn()}W?(l=new W(F()),E(l.$$.fragment),v(l.$$.fragment,1),x(l,r,j)):l=null}N[1]?D&&(Un(),$(D,1,1,()=>{D=null}),Kn()):D?(D.p(N,H),H&2&&v(D,1)):(D=xf(N),D.c(),v(D,1),D.m(f,null));let R=L;L=U(N),L===R?J[L].p(N,H):(Un(),$(J[R],1,1,()=>{J[R]=null}),Kn(),P=J[L],P?P.p(N,H):(P=J[L]=O[L](N),P.c()),v(P,1),P.m(n,null))},i(N){I||(l&&v(l.$$.fragment,N),v(D),v(P),I=!0)},o(N){l&&$(l.$$.fragment,N),$(D),$(P),I=!1},d(N){N&&a(n),l&&y(l),D&&D.d(),J[L].d(),g[11](null),T=!1,V()}}}function od(g,n,f){let r,l,{$$slots:j={},$$scope:_}=n,{framework:k}=n,w,C=new Set;const L={pytorch:{Icon:Vf,label:"Pytorch"},tensorflow:{Icon:Kf,label:"TensorFlow"},jax:{Icon:Gf,label:"JAX"}},{Icon:P,label:I}=L[k],T=`hf_doc_framework_${k}_is_hidden`,V=Jf(k);Lf(g,V,O=>f(8,l=O));function W(){Kp(V,l=l!==As.CLOSED?As.CLOSED:As.OPEN,l),localStorage.setItem(T,l)}function F(){const O=window.location.hash.slice(1);C.has(O)&&(Kp(V,l=As.HASHASHLINK,l),localStorage.setItem(T,l))}Df(()=>{const O=window.location.hash.slice(1),J="header-link",U=w.querySelectorAll(`.${J}`);C=new Set([...U].map(H=>H.id));const N=localStorage.getItem(T);C.has(O)?Kp(V,l=As.HASHASHLINK,l):N===As.CLOSED&&l!==As.HASHASHLINK&&Kp(V,l=As.CLOSED,l)});function D(O){If[O?"unshift":"push"](()=>{w=O,f(0,w)})}return g.$$set=O=>{"framework"in O&&f(7,k=O.framework),"$$scope"in O&&f(9,_=O.$$scope)},g.$$.update=()=>{g.$$.dirty&256&&f(1,r=l===As.CLOSED)},[w,r,P,I,V,W,F,k,l,_,j,D]}class Ii extends Zs{constructor(n){super();Gs(this,n,od,pd,Qs,{framework:7})}}const cd=g=>({}),yf=g=>({}),ud=g=>({}),qf=g=>({}),id=g=>({}),zf=g=>({});function Af(g){let n,f;return n=new Ii({props:{framework:"pytorch",$$slots:{default:[hd]},$$scope:{ctx:g}}}),{c(){E(n.$$.fragment)},l(r){q(n.$$.fragment,r)},m(r,l){x(n,r,l),f=!0},p(r,l){const j={};l&16&&(j.$$scope={dirty:l,ctx:r}),n.$set(j)},i(r){f||(v(n.$$.fragment,r),f=!0)},o(r){$(n.$$.fragment,r),f=!1},d(r){y(n,r)}}}function hd(g){let n;const f=g[3].pytorch,r=bl(f,g,g[4],zf);return{c(){r&&r.c()},l(l){r&&r.l(l)},m(l,j){r&&r.m(l,j),n=!0},p(l,j){r&&r.p&&(!n||j&16)&&jl(r,f,l,l[4],n?_l(f,l[4],j,id):gl(l[4]),zf)},i(l){n||(v(r,l),n=!0)},o(l){$(r,l),n=!1},d(l){r&&r.d(l)}}}function Pf(g){let n,f;return n=new Ii({props:{framework:"tensorflow",$$slots:{default:[md]},$$scope:{ctx:g}}}),{c(){E(n.$$.fragment)},l(r){q(n.$$.fragment,r)},m(r,l){x(n,r,l),f=!0},p(r,l){const j={};l&16&&(j.$$scope={dirty:l,ctx:r}),n.$set(j)},i(r){f||(v(n.$$.fragment,r),f=!0)},o(r){$(n.$$.fragment,r),f=!1},d(r){y(n,r)}}}function md(g){let n;const f=g[3].tensorflow,r=bl(f,g,g[4],qf);return{c(){r&&r.c()},l(l){r&&r.l(l)},m(l,j){r&&r.m(l,j),n=!0},p(l,j){r&&r.p&&(!n||j&16)&&jl(r,f,l,l[4],n?_l(f,l[4],j,ud):gl(l[4]),qf)},i(l){n||(v(r,l),n=!0)},o(l){$(r,l),n=!1},d(l){r&&r.d(l)}}}function Cf(g){let n,f;return n=new Ii({props:{framework:"jax",$$slots:{default:[fd]},$$scope:{ctx:g}}}),{c(){E(n.$$.fragment)},l(r){q(n.$$.fragment,r)},m(r,l){x(n,r,l),f=!0},p(r,l){const j={};l&16&&(j.$$scope={dirty:l,ctx:r}),n.$set(j)},i(r){f||(v(n.$$.fragment,r),f=!0)},o(r){$(n.$$.fragment,r),f=!1},d(r){y(n,r)}}}function fd(g){let n;const f=g[3].jax,r=bl(f,g,g[4],yf);return{c(){r&&r.c()},l(l){r&&r.l(l)},m(l,j){r&&r.m(l,j),n=!0},p(l,j){r&&r.p&&(!n||j&16)&&jl(r,f,l,l[4],n?_l(f,l[4],j,cd):gl(l[4]),yf)},i(l){n||(v(r,l),n=!0)},o(l){$(r,l),n=!1},d(l){r&&r.d(l)}}}function dd(g){let n,f,r,l,j=g[0]&&Af(g),_=g[1]&&Pf(g),k=g[2]&&Cf(g);return{c(){n=c("div"),j&&j.c(),f=d(),_&&_.c(),r=d(),k&&k.c(),this.h()},l(w){n=u(w,"DIV",{class:!0});var C=t(n);j&&j.l(C),f=b(C),_&&_.l(C),r=b(C),k&&k.l(C),C.forEach(a),this.h()},h(){o(n,"class","space-y-10 py-6 2xl:py-8 2xl:-mx-4")},m(w,C){m(w,n,C),j&&j.m(n,null),e(n,f),_&&_.m(n,null),e(n,r),k&&k.m(n,null),l=!0},p(w,[C]){w[0]?j?(j.p(w,C),C&1&&v(j,1)):(j=Af(w),j.c(),v(j,1),j.m(n,f)):j&&(Un(),$(j,1,1,()=>{j=null}),Kn()),w[1]?_?(_.p(w,C),C&2&&v(_,1)):(_=Pf(w),_.c(),v(_,1),_.m(n,r)):_&&(Un(),$(_,1,1,()=>{_=null}),Kn()),w[2]?k?(k.p(w,C),C&4&&v(k,1)):(k=Cf(w),k.c(),v(k,1),k.m(n,null)):k&&(Un(),$(k,1,1,()=>{k=null}),Kn())},i(w){l||(v(j),v(_),v(k),l=!0)},o(w){$(j),$(_),$(k),l=!1},d(w){w&&a(n),j&&j.d(),_&&_.d(),k&&k.d()}}}function bd(g,n,f){let{$$slots:r={},$$scope:l}=n,{pytorch:j=!1}=n,{tensorflow:_=!1}=n,{jax:k=!1}=n;return g.$$set=w=>{"pytorch"in w&&f(0,j=w.pytorch),"tensorflow"in w&&f(1,_=w.tensorflow),"jax"in w&&f(2,k=w.jax),"$$scope"in w&&f(4,l=w.$$scope)},[j,_,k,r,l]}class jd extends Zs{constructor(n){super();Gs(this,n,bd,dd,Qs,{pytorch:0,tensorflow:1,jax:2})}}function gd(g){let n;const f=g[1].default,r=bl(f,g,g[0],null);return{c(){r&&r.c()},l(l){r&&r.l(l)},m(l,j){r&&r.m(l,j),n=!0},p(l,[j]){r&&r.p&&(!n||j&1)&&jl(r,f,l,l[0],n?_l(f,l[0],j,null):gl(l[0]),null)},i(l){n||(v(r,l),n=!0)},o(l){$(r,l),n=!1},d(l){r&&r.d(l)}}}function _d(g,n,f){let{$$slots:r={},$$scope:l}=n;return g.$$set=j=>{"$$scope"in j&&f(0,l=j.$$scope)},[l,r]}class Nf extends Zs{constructor(n){super();Gs(this,n,_d,gd,Qs,{})}}function vd(g){let n,f,r,l,j;return{c(){n=c("p"),f=i(`Se voc\xEA planeja em usar um modelo pr\xE9-treinado, \xE9 importante usar um tokenizer pr\xE9-treinado associado ao modelo.
Isso permite ter certeza de que o texto ser\xE1 dividido da mesma forma que o corpus do pr\xE9-treinamento, e
portanto tamb\xE9m manter a mesma correspond\xEAncia de tokens para \xEDndices (usualmente chamado de `),r=c("em"),l=i("vocab"),j=i(") durante o treinamento.")},l(_){n=u(_,"P",{});var k=t(n);f=h(k,`Se voc\xEA planeja em usar um modelo pr\xE9-treinado, \xE9 importante usar um tokenizer pr\xE9-treinado associado ao modelo.
Isso permite ter certeza de que o texto ser\xE1 dividido da mesma forma que o corpus do pr\xE9-treinamento, e
portanto tamb\xE9m manter a mesma correspond\xEAncia de tokens para \xEDndices (usualmente chamado de `),r=u(k,"EM",{});var w=t(r);l=h(w,"vocab"),w.forEach(a),j=h(k,") durante o treinamento."),k.forEach(a)},m(_,k){m(_,n,k),e(n,f),e(n,r),e(r,l),e(n,j)},d(_){_&&a(n)}}}function $d(g){let n,f;return n=new S({props:{code:`batch_sentences = [
    "But what about second breakfast?",
    "Don't think he knows about second breakfast, Pip.",
    "What about elevensies?",
]
encoded_input = tokenizer(batch, padding=True, truncation=True, return_tensors="pt")
print(encoded_input)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>batch_sentences = [
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;But what about second breakfast?&quot;</span>,
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;Don&#x27;t think he knows about second breakfast, Pip.&quot;</span>,
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;What about elevensies?&quot;</span>,
<span class="hljs-meta">... </span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>encoded_input = tokenizer(batch, padding=<span class="hljs-literal">True</span>, truncation=<span class="hljs-literal">True</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(encoded_input)
{<span class="hljs-string">&#x27;input_ids&#x27;</span>: tensor([[  <span class="hljs-number">101</span>,   <span class="hljs-number">153</span>,  <span class="hljs-number">7719</span>, <span class="hljs-number">21490</span>,  <span class="hljs-number">1122</span>,  <span class="hljs-number">1114</span>,  <span class="hljs-number">9582</span>,  <span class="hljs-number">1623</span>,   <span class="hljs-number">102</span>],
                      [  <span class="hljs-number">101</span>,  <span class="hljs-number">5226</span>,  <span class="hljs-number">1122</span>,  <span class="hljs-number">9649</span>,  <span class="hljs-number">1199</span>,  <span class="hljs-number">2610</span>,  <span class="hljs-number">1236</span>,   <span class="hljs-number">102</span>,     <span class="hljs-number">0</span>]]), 
 <span class="hljs-string">&#x27;token_type_ids&#x27;</span>: tensor([[<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
                           [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>]]), 
 <span class="hljs-string">&#x27;attention_mask&#x27;</span>: tensor([[<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>],
                           [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>]])}`}}),{c(){E(n.$$.fragment)},l(r){q(n.$$.fragment,r)},m(r,l){x(n,r,l),f=!0},p:es,i(r){f||(v(n.$$.fragment,r),f=!0)},o(r){$(n.$$.fragment,r),f=!1},d(r){y(n,r)}}}function wd(g){let n,f;return n=new Nf({props:{$$slots:{default:[$d]},$$scope:{ctx:g}}}),{c(){E(n.$$.fragment)},l(r){q(n.$$.fragment,r)},m(r,l){x(n,r,l),f=!0},p(r,l){const j={};l&2&&(j.$$scope={dirty:l,ctx:r}),n.$set(j)},i(r){f||(v(n.$$.fragment,r),f=!0)},o(r){$(n.$$.fragment,r),f=!1},d(r){y(n,r)}}}function kd(g){let n,f;return n=new S({props:{code:`batch_sentences = [
    "But what about second breakfast?",
    "Don't think he knows about second breakfast, Pip.",
    "What about elevensies?",
]
encoded_input = tokenizer(batch_sentences, padding=True, truncation=True, return_tensors="tf")
print(encoded_input)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>batch_sentences = [
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;But what about second breakfast?&quot;</span>,
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;Don&#x27;t think he knows about second breakfast, Pip.&quot;</span>,
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;What about elevensies?&quot;</span>,
<span class="hljs-meta">... </span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>encoded_input = tokenizer(batch_sentences, padding=<span class="hljs-literal">True</span>, truncation=<span class="hljs-literal">True</span>, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(encoded_input)
{<span class="hljs-string">&#x27;input_ids&#x27;</span>: &lt;tf.Tensor: shape=(<span class="hljs-number">2</span>, <span class="hljs-number">9</span>), dtype=int32, numpy=
array([[  <span class="hljs-number">101</span>,   <span class="hljs-number">153</span>,  <span class="hljs-number">7719</span>, <span class="hljs-number">21490</span>,  <span class="hljs-number">1122</span>,  <span class="hljs-number">1114</span>,  <span class="hljs-number">9582</span>,  <span class="hljs-number">1623</span>,   <span class="hljs-number">102</span>],
       [  <span class="hljs-number">101</span>,  <span class="hljs-number">5226</span>,  <span class="hljs-number">1122</span>,  <span class="hljs-number">9649</span>,  <span class="hljs-number">1199</span>,  <span class="hljs-number">2610</span>,  <span class="hljs-number">1236</span>,   <span class="hljs-number">102</span>,     <span class="hljs-number">0</span>]],
      dtype=int32)&gt;, 
 <span class="hljs-string">&#x27;token_type_ids&#x27;</span>: &lt;tf.Tensor: shape=(<span class="hljs-number">2</span>, <span class="hljs-number">9</span>), dtype=int32, numpy=
array([[<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
       [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>]], dtype=int32)&gt;, 
 <span class="hljs-string">&#x27;attention_mask&#x27;</span>: &lt;tf.Tensor: shape=(<span class="hljs-number">2</span>, <span class="hljs-number">9</span>), dtype=int32, numpy=
array([[<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>],
       [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>]], dtype=int32)&gt;}`}}),{c(){E(n.$$.fragment)},l(r){q(n.$$.fragment,r)},m(r,l){x(n,r,l),f=!0},p:es,i(r){f||(v(n.$$.fragment,r),f=!0)},o(r){$(n.$$.fragment,r),f=!1},d(r){y(n,r)}}}function Ed(g){let n,f;return n=new Nf({props:{$$slots:{default:[kd]},$$scope:{ctx:g}}}),{c(){E(n.$$.fragment)},l(r){q(n.$$.fragment,r)},m(r,l){x(n,r,l),f=!0},p(r,l){const j={};l&2&&(j.$$scope={dirty:l,ctx:r}),n.$set(j)},i(r){f||(v(n.$$.fragment,r),f=!0)},o(r){$(n.$$.fragment,r),f=!1},d(r){y(n,r)}}}function xd(g){let n,f,r,l,j,_,k,w,C,L,P,I,T,V,W,F,D,O,J,U,N,H,R,M,ns,Z,X,us,ts,Ps,is,hs,G,ls,fs,ss,Cs,K,Ds,Ns,ds,Y,B,rs,ps,bs,Ca,Da,as,Ss,Xs,vl,Ls,sa,Yn,Na,Yp,Zn,Zp,$l,aa,Gp,Gn,Qp,Xp,wl,Sa,kl,Ke,so,El,La,xl,Ye,ao,yl,js,Ze,Ge,eo,no,to,Qe,Xe,lo,ro,po,sn,an,oo,co,ql,ea,uo,Qn,io,ho,zl,Ta,Al,gs,mo,Xn,fo,bo,st,jo,go,Pl,en,_o,Cl,Ia,Dl,Ts,na,at,Oa,vo,et,$o,Nl,ta,wo,nt,ko,Eo,Sl,_s,xo,tt,yo,qo,lt,zo,Ao,Ll,Ma,Tl,nn,Po,Il,Is,la,rt,Ha,Co,pt,Do,Ol,tn,No,Ml,vs,So,ot,Lo,To,ct,Io,Oo,Hl,Fa,Fl,Os,ra,ut,Ja,Mo,it,Ho,Jl,ln,Fo,Rl,os,Jo,ht,Ro,Bo,mt,Vo,Wo,ft,Uo,Ko,Bl,pa,Vl,Ms,oa,dt,Ra,Yo,bt,Zo,Wl,ca,Go,rn,Qo,Xo,Ul,Ba,Kl,$s,sc,Va,ac,ec,Wa,nc,tc,Yl,Ua,Zl,ws,lc,jt,rc,pc,gt,oc,cc,Gl,Ka,Ql,pn,uc,Xl,ks,on,_t,ic,hc,mc,cn,vt,fc,dc,bc,un,$t,jc,gc,sr,Hs,ua,wt,Ya,_c,kt,vc,ar,ia,$c,Za,wc,kc,er,ha,Ec,Ga,xc,yc,nr,Qa,tr,hn,Xa,qc,se,Et,zc,Ac,lr,ae,rr,ee,xt,Pc,pr,ne,or,ma,Cc,yt,Dc,Nc,cr,Fs,fa,qt,te,Sc,zt,Lc,ur,cs,Tc,At,Ic,Oc,Pt,Mc,Hc,Ct,Fc,Jc,ir,da,Rc,Dt,Bc,Vc,hr,le,mr,Es,Wc,Nt,Uc,Kc,St,Yc,Zc,fr,re,dr,Js,ba,Lt,pe,Gc,Tt,Qc,br,mn,Xc,jr,oe,gr,fn,su,_r,ce,vr,dn,au,$r,ue,wr,bn,eu,kr,ie,Er,jn,nu,xr,Rs,ja,It,he,tu,Ot,lu,yr,gn,ru,qr,xs,pu,me,ou,cu,Mt,uu,iu,zr,fe,Ar,ga,hu,de,Ht,mu,fu,Pr,be,Cr,_n,vn,Oi,Dr,Bs,_a,Ft,je,du,Jt,bu,Nr,va,ju,Rt,gu,_u,Sr,ge,Lr,Vs,$a,Bt,_e,vu,Vt,$u,Tr,wa,wu,ve,Wt,ku,Eu,Ir,$n,ms,xu,$e,Ut,yu,qu,we,Kt,zu,Au,ke,Yt,Pu,Cu,Or,Ee,Mr,xe,Ws,Du,wn,Zt,Nu,Su,Gt,Lu,Tu,Hr,ye,Fr,qe,ze,Iu,Ae,Qt,Ou,Mu,Jr,Pe,Rr,Ce,De,Hu,Xt,Fu,Ju,Br,Ne,Vr,kn,Ru,Wr,Se,Ur,En,xn,Mi,Kr,Us,ka,sl,Le,Bu,al,Vu,Yr,yn,Wu,Zr,Ea,el,Uu,Ku,nl,Yu,Gr,xa,Zu,Te,Gu,Qu,Qr,Ie,Xr,ys,Xu,tl,si,ai,ll,ei,ni,sp,Oe,ap,qs,ti,rl,li,ri,pl,pi,oi,ep,Me,np,ya,ci,qn,ui,ii,tp,He,lp,Ks,qa,ol,Fe,hi,cl,mi,rp,zn,fi,pp,Je,op,An,Ys,di,ul,bi,ji,il,gi,_i,cp,Re,up,Be,Ve,vi,hl,$i,wi,ip,We,hp,zs,ki,ml,Ei,xi,fl,yi,qi,mp,Pn,zi,fp;return _=new Q({}),P=new Ff({props:{classNames:"absolute z-10 right-0 top-0",options:[{label:"Mixed",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/pt/preprocessing.ipynb"},{label:"PyTorch",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/pt/pytorch/preprocessing.ipynb"},{label:"TensorFlow",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/pt/tensorflow/preprocessing.ipynb"},{label:"Mixed",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/transformers_doc/pt/preprocessing.ipynb"},{label:"PyTorch",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/transformers_doc/pt/pytorch/preprocessing.ipynb"},{label:"TensorFlow",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/transformers_doc/pt/tensorflow/preprocessing.ipynb"}]}}),ts=new Q({}),ls=new Hf({props:{id:"Yffk5aydLzg"}}),Y=new Mf({props:{$$slots:{default:[vd]},$$scope:{ctx:g}}}),Na=new Q({}),Sa=new S({props:{code:`from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)`}}),La=new S({props:{code:`encoded_input = tokenizer("Do not meddle in the affairs of wizards, for they are subtle and quick to anger.")
print(encoded_input)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>encoded_input = tokenizer(<span class="hljs-string">&quot;Do not meddle in the affairs of wizards, for they are subtle and quick to anger.&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(encoded_input)
{<span class="hljs-string">&#x27;input_ids&#x27;</span>: [<span class="hljs-number">101</span>, <span class="hljs-number">2079</span>, <span class="hljs-number">2025</span>, <span class="hljs-number">19960</span>, <span class="hljs-number">10362</span>, <span class="hljs-number">1999</span>, <span class="hljs-number">1996</span>, <span class="hljs-number">3821</span>, <span class="hljs-number">1997</span>, <span class="hljs-number">16657</span>, <span class="hljs-number">1010</span>, <span class="hljs-number">2005</span>, <span class="hljs-number">2027</span>, <span class="hljs-number">2024</span>, <span class="hljs-number">11259</span>, <span class="hljs-number">1998</span>, <span class="hljs-number">4248</span>, <span class="hljs-number">2000</span>, <span class="hljs-number">4963</span>, <span class="hljs-number">1012</span>, <span class="hljs-number">102</span>], 
 <span class="hljs-string">&#x27;token_type_ids&#x27;</span>: [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>], 
 <span class="hljs-string">&#x27;attention_mask&#x27;</span>: [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]}`}}),Ta=new S({props:{code:'tokenizer.decode(encoded_input["input_ids"])',highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.decode(encoded_input[<span class="hljs-string">&quot;input_ids&quot;</span>])
<span class="hljs-string">&#x27;[CLS] Do not meddle in the affairs of wizards, for they are subtle and quick to anger. [SEP]&#x27;</span>`}}),Ia=new S({props:{code:`batch_sentences = [
    "But what about second breakfast?",
    "Don't think he knows about second breakfast, Pip.",
    "What about elevensies?",
]
encoded_inputs = tokenizer(batch_sentences)
print(encoded_inputs)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>batch_sentences = [
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;But what about second breakfast?&quot;</span>,
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;Don&#x27;t think he knows about second breakfast, Pip.&quot;</span>,
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;What about elevensies?&quot;</span>,
<span class="hljs-meta">... </span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>encoded_inputs = tokenizer(batch_sentences)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(encoded_inputs)
{<span class="hljs-string">&#x27;input_ids&#x27;</span>: [[<span class="hljs-number">101</span>, <span class="hljs-number">1252</span>, <span class="hljs-number">1184</span>, <span class="hljs-number">1164</span>, <span class="hljs-number">1248</span>, <span class="hljs-number">6462</span>, <span class="hljs-number">136</span>, <span class="hljs-number">102</span>], 
               [<span class="hljs-number">101</span>, <span class="hljs-number">1790</span>, <span class="hljs-number">112</span>, <span class="hljs-number">189</span>, <span class="hljs-number">1341</span>, <span class="hljs-number">1119</span>, <span class="hljs-number">3520</span>, <span class="hljs-number">1164</span>, <span class="hljs-number">1248</span>, <span class="hljs-number">6462</span>, <span class="hljs-number">117</span>, <span class="hljs-number">21902</span>, <span class="hljs-number">1643</span>, <span class="hljs-number">119</span>, <span class="hljs-number">102</span>], 
               [<span class="hljs-number">101</span>, <span class="hljs-number">1327</span>, <span class="hljs-number">1164</span>, <span class="hljs-number">5450</span>, <span class="hljs-number">23434</span>, <span class="hljs-number">136</span>, <span class="hljs-number">102</span>]], 
 <span class="hljs-string">&#x27;token_type_ids&#x27;</span>: [[<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>], 
                    [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>], 
                    [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>]], 
 <span class="hljs-string">&#x27;attention_mask&#x27;</span>: [[<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>], 
                    [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>], 
                    [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]]}`}}),Oa=new Q({}),Ma=new S({props:{code:`batch_sentences = [
    "But what about second breakfast?",
    "Don't think he knows about second breakfast, Pip.",
    "What about elevensies?",
]
encoded_input = tokenizer(batch_sentences, padding=True)
print(encoded_input)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>batch_sentences = [
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;But what about second breakfast?&quot;</span>,
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;Don&#x27;t think he knows about second breakfast, Pip.&quot;</span>,
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;What about elevensies?&quot;</span>,
<span class="hljs-meta">... </span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>encoded_input = tokenizer(batch_sentences, padding=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(encoded_input)
{<span class="hljs-string">&#x27;input_ids&#x27;</span>: [[<span class="hljs-number">101</span>, <span class="hljs-number">1252</span>, <span class="hljs-number">1184</span>, <span class="hljs-number">1164</span>, <span class="hljs-number">1248</span>, <span class="hljs-number">6462</span>, <span class="hljs-number">136</span>, <span class="hljs-number">102</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>], 
               [<span class="hljs-number">101</span>, <span class="hljs-number">1790</span>, <span class="hljs-number">112</span>, <span class="hljs-number">189</span>, <span class="hljs-number">1341</span>, <span class="hljs-number">1119</span>, <span class="hljs-number">3520</span>, <span class="hljs-number">1164</span>, <span class="hljs-number">1248</span>, <span class="hljs-number">6462</span>, <span class="hljs-number">117</span>, <span class="hljs-number">21902</span>, <span class="hljs-number">1643</span>, <span class="hljs-number">119</span>, <span class="hljs-number">102</span>], 
               [<span class="hljs-number">101</span>, <span class="hljs-number">1327</span>, <span class="hljs-number">1164</span>, <span class="hljs-number">5450</span>, <span class="hljs-number">23434</span>, <span class="hljs-number">136</span>, <span class="hljs-number">102</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>]], 
 <span class="hljs-string">&#x27;token_type_ids&#x27;</span>: [[<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>], 
                    [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>], 
                    [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>]], 
 <span class="hljs-string">&#x27;attention_mask&#x27;</span>: [[<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>], 
                    [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>], 
                    [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>]]}`}}),Ha=new Q({}),Fa=new S({props:{code:`batch_sentences = [
    "But what about second breakfast?",
    "Don't think he knows about second breakfast, Pip.",
    "What about elevensies?",
]
encoded_input = tokenizer(batch_sentences, padding=True, truncation=True)
print(encoded_input)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>batch_sentences = [
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;But what about second breakfast?&quot;</span>,
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;Don&#x27;t think he knows about second breakfast, Pip.&quot;</span>,
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;What about elevensies?&quot;</span>,
<span class="hljs-meta">... </span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>encoded_input = tokenizer(batch_sentences, padding=<span class="hljs-literal">True</span>, truncation=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(encoded_input)
{<span class="hljs-string">&#x27;input_ids&#x27;</span>: [[<span class="hljs-number">101</span>, <span class="hljs-number">1252</span>, <span class="hljs-number">1184</span>, <span class="hljs-number">1164</span>, <span class="hljs-number">1248</span>, <span class="hljs-number">6462</span>, <span class="hljs-number">136</span>, <span class="hljs-number">102</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>], 
               [<span class="hljs-number">101</span>, <span class="hljs-number">1790</span>, <span class="hljs-number">112</span>, <span class="hljs-number">189</span>, <span class="hljs-number">1341</span>, <span class="hljs-number">1119</span>, <span class="hljs-number">3520</span>, <span class="hljs-number">1164</span>, <span class="hljs-number">1248</span>, <span class="hljs-number">6462</span>, <span class="hljs-number">117</span>, <span class="hljs-number">21902</span>, <span class="hljs-number">1643</span>, <span class="hljs-number">119</span>, <span class="hljs-number">102</span>], 
               [<span class="hljs-number">101</span>, <span class="hljs-number">1327</span>, <span class="hljs-number">1164</span>, <span class="hljs-number">5450</span>, <span class="hljs-number">23434</span>, <span class="hljs-number">136</span>, <span class="hljs-number">102</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>]], 
 <span class="hljs-string">&#x27;token_type_ids&#x27;</span>: [[<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>], 
                    [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>], 
                    [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>]], 
 <span class="hljs-string">&#x27;attention_mask&#x27;</span>: [[<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>], 
                    [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>], 
                    [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>]]}`}}),Ja=new Q({}),pa=new jd({props:{pytorch:!0,tensorflow:!0,jax:!1,$$slots:{tensorflow:[Ed],pytorch:[wd]},$$scope:{ctx:g}}}),Ra=new Q({}),Ba=new S({props:{code:"pip install datasets",highlighted:"pip install datasets"}}),Ua=new S({props:{code:`from datasets import load_dataset, Audio

dataset = load_dataset("PolyAI/minds14", name="en-US", split="train")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset, Audio

<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;PolyAI/minds14&quot;</span>, name=<span class="hljs-string">&quot;en-US&quot;</span>, split=<span class="hljs-string">&quot;train&quot;</span>)`}}),Ka=new S({props:{code:'dataset[0]["audio"]',highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>dataset[<span class="hljs-number">0</span>][<span class="hljs-string">&quot;audio&quot;</span>]
{<span class="hljs-string">&#x27;array&#x27;</span>: array([ <span class="hljs-number">0.</span>        ,  <span class="hljs-number">0.00024414</span>, -<span class="hljs-number">0.00024414</span>, ..., -<span class="hljs-number">0.00024414</span>,
         <span class="hljs-number">0.</span>        ,  <span class="hljs-number">0.</span>        ], dtype=float32),
 <span class="hljs-string">&#x27;path&#x27;</span>: <span class="hljs-string">&#x27;/root/.cache/huggingface/datasets/downloads/extracted/f14948e0e84be638dd7943ac36518a4cf3324e8b7aa331c5ab11541518e9368c/en-US~JOINT_ACCOUNT/602ba55abb1e6d0fbce92065.wav&#x27;</span>,
 <span class="hljs-string">&#x27;sampling_rate&#x27;</span>: <span class="hljs-number">8000</span>}`}}),Ya=new Q({}),Qa=new S({props:{code:`dataset = load_dataset("PolyAI/minds14", name="en-US", split="train")
dataset[0]["audio"]`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;PolyAI/minds14&quot;</span>, name=<span class="hljs-string">&quot;en-US&quot;</span>, split=<span class="hljs-string">&quot;train&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset[<span class="hljs-number">0</span>][<span class="hljs-string">&quot;audio&quot;</span>]
{<span class="hljs-string">&#x27;array&#x27;</span>: array([ <span class="hljs-number">0.</span>        ,  <span class="hljs-number">0.00024414</span>, -<span class="hljs-number">0.00024414</span>, ..., -<span class="hljs-number">0.00024414</span>,
         <span class="hljs-number">0.</span>        ,  <span class="hljs-number">0.</span>        ], dtype=float32),
 <span class="hljs-string">&#x27;path&#x27;</span>: <span class="hljs-string">&#x27;/root/.cache/huggingface/datasets/downloads/extracted/f14948e0e84be638dd7943ac36518a4cf3324e8b7aa331c5ab11541518e9368c/en-US~JOINT_ACCOUNT/602ba55abb1e6d0fbce92065.wav&#x27;</span>,
 <span class="hljs-string">&#x27;sampling_rate&#x27;</span>: <span class="hljs-number">8000</span>}`}}),ae=new S({props:{code:'dataset = dataset.cast_column("audio", Audio(sampling_rate=16_000))',highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = dataset.cast_column(<span class="hljs-string">&quot;audio&quot;</span>, Audio(sampling_rate=<span class="hljs-number">16_000</span>))'}}),ne=new S({props:{code:'dataset[0]["audio"]',highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>dataset[<span class="hljs-number">0</span>][<span class="hljs-string">&quot;audio&quot;</span>]
{<span class="hljs-string">&#x27;array&#x27;</span>: array([ <span class="hljs-number">2.3443763e-05</span>,  <span class="hljs-number">2.1729663e-04</span>,  <span class="hljs-number">2.2145823e-04</span>, ...,
         <span class="hljs-number">3.8356509e-05</span>, -<span class="hljs-number">7.3497440e-06</span>, -<span class="hljs-number">2.1754686e-05</span>], dtype=float32),
 <span class="hljs-string">&#x27;path&#x27;</span>: <span class="hljs-string">&#x27;/root/.cache/huggingface/datasets/downloads/extracted/f14948e0e84be638dd7943ac36518a4cf3324e8b7aa331c5ab11541518e9368c/en-US~JOINT_ACCOUNT/602ba55abb1e6d0fbce92065.wav&#x27;</span>,
 <span class="hljs-string">&#x27;sampling_rate&#x27;</span>: <span class="hljs-number">16000</span>}`}}),te=new Q({}),le=new S({props:{code:`from transformers import AutoFeatureExtractor

feature_extractor = AutoFeatureExtractor.from_pretrained("facebook/wav2vec2-base")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoFeatureExtractor

<span class="hljs-meta">&gt;&gt;&gt; </span>feature_extractor = AutoFeatureExtractor.from_pretrained(<span class="hljs-string">&quot;facebook/wav2vec2-base&quot;</span>)`}}),re=new S({props:{code:`audio_input = [dataset[0]["audio"]["array"]]
feature_extractor(audio_input, sampling_rate=16000)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>audio_input = [dataset[<span class="hljs-number">0</span>][<span class="hljs-string">&quot;audio&quot;</span>][<span class="hljs-string">&quot;array&quot;</span>]]
<span class="hljs-meta">&gt;&gt;&gt; </span>feature_extractor(audio_input, sampling_rate=<span class="hljs-number">16000</span>)
{<span class="hljs-string">&#x27;input_values&#x27;</span>: [array([ <span class="hljs-number">3.8106556e-04</span>,  <span class="hljs-number">2.7506407e-03</span>,  <span class="hljs-number">2.8015103e-03</span>, ...,
        <span class="hljs-number">5.6335266e-04</span>,  <span class="hljs-number">4.6588284e-06</span>, -<span class="hljs-number">1.7142107e-04</span>], dtype=float32)]}`}}),pe=new Q({}),oe=new S({props:{code:`dataset[0]["audio"]["array"].shape

dataset[1]["audio"]["array"].shape`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>dataset[<span class="hljs-number">0</span>][<span class="hljs-string">&quot;audio&quot;</span>][<span class="hljs-string">&quot;array&quot;</span>].shape
(<span class="hljs-number">173398</span>,)

<span class="hljs-meta">&gt;&gt;&gt; </span>dataset[<span class="hljs-number">1</span>][<span class="hljs-string">&quot;audio&quot;</span>][<span class="hljs-string">&quot;array&quot;</span>].shape
(<span class="hljs-number">106496</span>,)`}}),ce=new S({props:{code:`def preprocess_function(examples):
    audio_arrays = [x["array"] for x in examples["audio"]]
    inputs = feature_extractor(
        audio_arrays,
        sampling_rate=16000,
        padding=True,
        max_length=100000,
        truncation=True,
    )
    return inputs`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">def</span> <span class="hljs-title function_">preprocess_function</span>(<span class="hljs-params">examples</span>):
<span class="hljs-meta">... </span>    audio_arrays = [x[<span class="hljs-string">&quot;array&quot;</span>] <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> examples[<span class="hljs-string">&quot;audio&quot;</span>]]
<span class="hljs-meta">... </span>    inputs = feature_extractor(
<span class="hljs-meta">... </span>        audio_arrays,
<span class="hljs-meta">... </span>        sampling_rate=<span class="hljs-number">16000</span>,
<span class="hljs-meta">... </span>        padding=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>        max_length=<span class="hljs-number">100000</span>,
<span class="hljs-meta">... </span>        truncation=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    )
<span class="hljs-meta">... </span>    <span class="hljs-keyword">return</span> inputs`}}),ue=new S({props:{code:"processed_dataset = preprocess_function(dataset[:5])",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>processed_dataset = preprocess_function(dataset[:<span class="hljs-number">5</span>])'}}),ie=new S({props:{code:`processed_dataset["input_values"][0].shape

processed_dataset["input_values"][1].shape`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>processed_dataset[<span class="hljs-string">&quot;input_values&quot;</span>][<span class="hljs-number">0</span>].shape
(<span class="hljs-number">100000</span>,)

<span class="hljs-meta">&gt;&gt;&gt; </span>processed_dataset[<span class="hljs-string">&quot;input_values&quot;</span>][<span class="hljs-number">1</span>].shape
(<span class="hljs-number">100000</span>,)`}}),he=new Q({}),fe=new S({props:{code:`from datasets import load_dataset

dataset = load_dataset("food101", split="train[:100]")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;food101&quot;</span>, split=<span class="hljs-string">&quot;train[:100]&quot;</span>)`}}),be=new S({props:{code:'dataset[0]["image"]',highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>dataset[<span class="hljs-number">0</span>][<span class="hljs-string">&quot;image&quot;</span>]'}}),je=new Q({}),ge=new S({props:{code:`from transformers import AutoFeatureExtractor

feature_extractor = AutoFeatureExtractor.from_pretrained("google/vit-base-patch16-224")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoFeatureExtractor

<span class="hljs-meta">&gt;&gt;&gt; </span>feature_extractor = AutoFeatureExtractor.from_pretrained(<span class="hljs-string">&quot;google/vit-base-patch16-224&quot;</span>)`}}),_e=new Q({}),Ee=new S({props:{code:`from torchvision.transforms import Compose, Normalize, RandomResizedCrop, ColorJitter, ToTensor

normalize = Normalize(mean=feature_extractor.image_mean, std=feature_extractor.image_std)
_transforms = Compose(
    [RandomResizedCrop(feature_extractor.size), ColorJitter(brightness=0.5, hue=0.5), ToTensor(), normalize]
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> torchvision.transforms <span class="hljs-keyword">import</span> Compose, Normalize, RandomResizedCrop, ColorJitter, ToTensor

<span class="hljs-meta">&gt;&gt;&gt; </span>normalize = Normalize(mean=feature_extractor.image_mean, std=feature_extractor.image_std)
<span class="hljs-meta">&gt;&gt;&gt; </span>_transforms = Compose(
<span class="hljs-meta">... </span>    [RandomResizedCrop(feature_extractor.size), ColorJitter(brightness=<span class="hljs-number">0.5</span>, hue=<span class="hljs-number">0.5</span>), ToTensor(), normalize]
<span class="hljs-meta">... </span>)`}}),ye=new S({props:{code:`def transforms(examples):
    examples["pixel_values"] = [_transforms(image.convert("RGB")) for image in examples["image"]]
    return examples`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">def</span> <span class="hljs-title function_">transforms</span>(<span class="hljs-params">examples</span>):
<span class="hljs-meta">... </span>    examples[<span class="hljs-string">&quot;pixel_values&quot;</span>] = [_transforms(image.convert(<span class="hljs-string">&quot;RGB&quot;</span>)) <span class="hljs-keyword">for</span> image <span class="hljs-keyword">in</span> examples[<span class="hljs-string">&quot;image&quot;</span>]]
<span class="hljs-meta">... </span>    <span class="hljs-keyword">return</span> examples`}}),Pe=new S({props:{code:"dataset.set_transform(transforms)",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>dataset.set_transform(transforms)'}}),Ne=new S({props:{code:'dataset[0]["image"]',highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>dataset[<span class="hljs-number">0</span>][<span class="hljs-string">&quot;image&quot;</span>]
{<span class="hljs-string">&#x27;image&#x27;</span>: &lt;PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=384x512 at <span class="hljs-number">0x7F1A7B0630D0</span>&gt;,
 <span class="hljs-string">&#x27;label&#x27;</span>: <span class="hljs-number">6</span>,
 <span class="hljs-string">&#x27;pixel_values&#x27;</span>: tensor([[[ <span class="hljs-number">0.0353</span>,  <span class="hljs-number">0.0745</span>,  <span class="hljs-number">0.1216</span>,  ..., -<span class="hljs-number">0.9922</span>, -<span class="hljs-number">0.9922</span>, -<span class="hljs-number">0.9922</span>],
          [-<span class="hljs-number">0.0196</span>,  <span class="hljs-number">0.0667</span>,  <span class="hljs-number">0.1294</span>,  ..., -<span class="hljs-number">0.9765</span>, -<span class="hljs-number">0.9843</span>, -<span class="hljs-number">0.9922</span>],
          [ <span class="hljs-number">0.0196</span>,  <span class="hljs-number">0.0824</span>,  <span class="hljs-number">0.1137</span>,  ..., -<span class="hljs-number">0.9765</span>, -<span class="hljs-number">0.9686</span>, -<span class="hljs-number">0.8667</span>],
          ...,
          [ <span class="hljs-number">0.0275</span>,  <span class="hljs-number">0.0745</span>,  <span class="hljs-number">0.0510</span>,  ..., -<span class="hljs-number">0.1137</span>, -<span class="hljs-number">0.1216</span>, -<span class="hljs-number">0.0824</span>],
          [ <span class="hljs-number">0.0667</span>,  <span class="hljs-number">0.0824</span>,  <span class="hljs-number">0.0667</span>,  ..., -<span class="hljs-number">0.0588</span>, -<span class="hljs-number">0.0745</span>, -<span class="hljs-number">0.0980</span>],
          [ <span class="hljs-number">0.0353</span>,  <span class="hljs-number">0.0353</span>,  <span class="hljs-number">0.0431</span>,  ..., -<span class="hljs-number">0.0039</span>, -<span class="hljs-number">0.0039</span>, -<span class="hljs-number">0.0588</span>]],
 
         [[ <span class="hljs-number">0.2078</span>,  <span class="hljs-number">0.2471</span>,  <span class="hljs-number">0.2863</span>,  ..., -<span class="hljs-number">0.9451</span>, -<span class="hljs-number">0.9373</span>, -<span class="hljs-number">0.9451</span>],
          [ <span class="hljs-number">0.1608</span>,  <span class="hljs-number">0.2471</span>,  <span class="hljs-number">0.3098</span>,  ..., -<span class="hljs-number">0.9373</span>, -<span class="hljs-number">0.9451</span>, -<span class="hljs-number">0.9373</span>],
          [ <span class="hljs-number">0.2078</span>,  <span class="hljs-number">0.2706</span>,  <span class="hljs-number">0.3020</span>,  ..., -<span class="hljs-number">0.9608</span>, -<span class="hljs-number">0.9373</span>, -<span class="hljs-number">0.8275</span>],
          ...,
          [-<span class="hljs-number">0.0353</span>,  <span class="hljs-number">0.0118</span>, -<span class="hljs-number">0.0039</span>,  ..., -<span class="hljs-number">0.2392</span>, -<span class="hljs-number">0.2471</span>, -<span class="hljs-number">0.2078</span>],
          [ <span class="hljs-number">0.0196</span>,  <span class="hljs-number">0.0353</span>,  <span class="hljs-number">0.0196</span>,  ..., -<span class="hljs-number">0.1843</span>, -<span class="hljs-number">0.2000</span>, -<span class="hljs-number">0.2235</span>],
          [-<span class="hljs-number">0.0118</span>, -<span class="hljs-number">0.0039</span>, -<span class="hljs-number">0.0039</span>,  ..., -<span class="hljs-number">0.0980</span>, -<span class="hljs-number">0.0980</span>, -<span class="hljs-number">0.1529</span>]],
 
         [[ <span class="hljs-number">0.3961</span>,  <span class="hljs-number">0.4431</span>,  <span class="hljs-number">0.4980</span>,  ..., -<span class="hljs-number">0.9216</span>, -<span class="hljs-number">0.9137</span>, -<span class="hljs-number">0.9216</span>],
          [ <span class="hljs-number">0.3569</span>,  <span class="hljs-number">0.4510</span>,  <span class="hljs-number">0.5216</span>,  ..., -<span class="hljs-number">0.9059</span>, -<span class="hljs-number">0.9137</span>, -<span class="hljs-number">0.9137</span>],
          [ <span class="hljs-number">0.4118</span>,  <span class="hljs-number">0.4745</span>,  <span class="hljs-number">0.5216</span>,  ..., -<span class="hljs-number">0.9137</span>, -<span class="hljs-number">0.8902</span>, -<span class="hljs-number">0.7804</span>],
          ...,
          [-<span class="hljs-number">0.2314</span>, -<span class="hljs-number">0.1922</span>, -<span class="hljs-number">0.2078</span>,  ..., -<span class="hljs-number">0.4196</span>, -<span class="hljs-number">0.4275</span>, -<span class="hljs-number">0.3882</span>],
          [-<span class="hljs-number">0.1843</span>, -<span class="hljs-number">0.1686</span>, -<span class="hljs-number">0.2000</span>,  ..., -<span class="hljs-number">0.3647</span>, -<span class="hljs-number">0.3804</span>, -<span class="hljs-number">0.4039</span>],
          [-<span class="hljs-number">0.1922</span>, -<span class="hljs-number">0.1922</span>, -<span class="hljs-number">0.1922</span>,  ..., -<span class="hljs-number">0.2941</span>, -<span class="hljs-number">0.2863</span>, -<span class="hljs-number">0.3412</span>]]])}`}}),Se=new S({props:{code:`import numpy as np
import matplotlib.pyplot as plt

img = dataset[0]["pixel_values"]
plt.imshow(img.permute(1, 2, 0))`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt

<span class="hljs-meta">&gt;&gt;&gt; </span>img = dataset[<span class="hljs-number">0</span>][<span class="hljs-string">&quot;pixel_values&quot;</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>plt.imshow(img.permute(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">0</span>))`}}),Le=new Q({}),Ie=new S({props:{code:`from datasets import load_dataset

lj_speech = load_dataset("lj_speech", split="train")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

<span class="hljs-meta">&gt;&gt;&gt; </span>lj_speech = load_dataset(<span class="hljs-string">&quot;lj_speech&quot;</span>, split=<span class="hljs-string">&quot;train&quot;</span>)`}}),Oe=new S({props:{code:'lj_speech = lj_speech.map(remove_columns=["file", "id", "normalized_text"])',highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>lj_speech = lj_speech.<span class="hljs-built_in">map</span>(remove_columns=[<span class="hljs-string">&quot;file&quot;</span>, <span class="hljs-string">&quot;id&quot;</span>, <span class="hljs-string">&quot;normalized_text&quot;</span>])'}}),Me=new S({props:{code:`lj_speech[0]["audio"]

lj_speech[0]["text"]`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>lj_speech[<span class="hljs-number">0</span>][<span class="hljs-string">&quot;audio&quot;</span>]
{<span class="hljs-string">&#x27;array&#x27;</span>: array([-<span class="hljs-number">7.3242188e-04</span>, -<span class="hljs-number">7.6293945e-04</span>, -<span class="hljs-number">6.4086914e-04</span>, ...,
         <span class="hljs-number">7.3242188e-04</span>,  <span class="hljs-number">2.1362305e-04</span>,  <span class="hljs-number">6.1035156e-05</span>], dtype=float32),
 <span class="hljs-string">&#x27;path&#x27;</span>: <span class="hljs-string">&#x27;/root/.cache/huggingface/datasets/downloads/extracted/917ece08c95cf0c4115e45294e3cd0dee724a1165b7fc11798369308a465bd26/LJSpeech-1.1/wavs/LJ001-0001.wav&#x27;</span>,
 <span class="hljs-string">&#x27;sampling_rate&#x27;</span>: <span class="hljs-number">22050</span>}

<span class="hljs-meta">&gt;&gt;&gt; </span>lj_speech[<span class="hljs-number">0</span>][<span class="hljs-string">&quot;text&quot;</span>]
<span class="hljs-string">&#x27;Printing, in the only sense with which we are at present concerned, differs from most if not from all the arts and crafts represented in the Exhibition&#x27;</span>`}}),He=new S({props:{code:'lj_speech = lj_speech.cast_column("audio", Audio(sampling_rate=16_000))',highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>lj_speech = lj_speech.cast_column(<span class="hljs-string">&quot;audio&quot;</span>, Audio(sampling_rate=<span class="hljs-number">16_000</span>))'}}),Fe=new Q({}),Je=new S({props:{code:`from transformers import AutoProcessor

processor = AutoProcessor.from_pretrained("facebook/wav2vec2-base-960h")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoProcessor

<span class="hljs-meta">&gt;&gt;&gt; </span>processor = AutoProcessor.from_pretrained(<span class="hljs-string">&quot;facebook/wav2vec2-base-960h&quot;</span>)`}}),Re=new S({props:{code:`def prepare_dataset(example):
    audio = example["audio"]

    example["input_values"] = processor(audio["array"], sampling_rate=16000)

    with processor.as_target_processor():
        example["labels"] = processor(example["text"]).input_ids
    return example`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">def</span> <span class="hljs-title function_">prepare_dataset</span>(<span class="hljs-params">example</span>):
<span class="hljs-meta">... </span>    audio = example[<span class="hljs-string">&quot;audio&quot;</span>]

<span class="hljs-meta">... </span>    example[<span class="hljs-string">&quot;input_values&quot;</span>] = processor(audio[<span class="hljs-string">&quot;array&quot;</span>], sampling_rate=<span class="hljs-number">16000</span>)

<span class="hljs-meta">... </span>    <span class="hljs-keyword">with</span> processor.as_target_processor():
<span class="hljs-meta">... </span>        example[<span class="hljs-string">&quot;labels&quot;</span>] = processor(example[<span class="hljs-string">&quot;text&quot;</span>]).input_ids
<span class="hljs-meta">... </span>    <span class="hljs-keyword">return</span> example`}}),We=new S({props:{code:"prepare_dataset(lj_speech[0])",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>prepare_dataset(lj_speech[<span class="hljs-number">0</span>])'}}),{c(){n=c("meta"),f=d(),r=c("h1"),l=c("a"),j=c("span"),E(_.$$.fragment),k=d(),w=c("span"),C=i("Preprocess"),L=d(),E(P.$$.fragment),I=d(),T=c("p"),V=i(`Antes de poder usar dados em seu modelo, os mesmos precisam ser processados em um formato aceito pelo modelo.
O modelo n\xE3o interpreta texto puro, imagens ou \xE1udio. Essas entradas precisam ser convertidas em n\xFAmeros e moldadas em tensores.
Nesse tutorial voc\xEA ir\xE1:`),W=d(),F=c("ul"),D=c("li"),O=i("Pr\xE9-processar dados em texto com um tokenizer."),J=d(),U=c("li"),N=i("Pr\xE9-processar dados de uma imagem ou \xE1udio com um extrator de caracter\xEDsticas (feature extractor)."),H=d(),R=c("li"),M=i("Pr\xE9-processar dados para uma tarefa multimodal com um processador."),ns=d(),Z=c("h2"),X=c("a"),us=c("span"),E(ts.$$.fragment),Ps=d(),is=c("span"),hs=i("NLP"),G=d(),E(ls.$$.fragment),fs=d(),ss=c("p"),Cs=i("A ferramenta principal para o processamento de dados textuais \xE9 um "),K=c("a"),Ds=i("tokenizer"),Ns=i(`. O Tokenizer
divide o texto em \u201Ctokens\u201D, seguindo um conjunto de regras pr\xE9-definidas.
Esses tokens s\xE3o convertidos em n\xFAmeros, usados em troca na constru\xE7\xE3o de tensores para a entrada de um modelo.
Quaisquer entradas adicionais solicitadas pelo modelo tamb\xE9m s\xE3o adicionadas por meio do tokenizer.`),ds=d(),E(Y.$$.fragment),B=d(),rs=c("p"),ps=i("Para come\xE7ar, carregue um modelo pr\xE9-treinado de tokenizer com a classe "),bs=c("code"),Ca=i("AutoTokenizer"),Da=i(`. Isso ir\xE1 baixar automaticamente
o `),as=c("em"),Ss=i("vocab"),Xs=i(" utilizado pelo modelo quando foi pr\xE9-treinado."),vl=d(),Ls=c("h3"),sa=c("a"),Yn=c("span"),E(Na.$$.fragment),Yp=d(),Zn=c("span"),Zp=i("Tokenizar"),$l=d(),aa=c("p"),Gp=i("Carregue um tokenizer pr\xE9-treinado com "),Gn=c("code"),Qp=i("AutoTokenizer.from_pretrained()"),Xp=i(":"),wl=d(),E(Sa.$$.fragment),kl=d(),Ke=c("p"),so=i("Em seguida, passe sua frase ao tokenizer:"),El=d(),E(La.$$.fragment),xl=d(),Ye=c("p"),ao=i("O tokenizer ir\xE1 retornar um dicion\xE1rio com tr\xEAs \xEDtens importantes:"),yl=d(),js=c("ul"),Ze=c("li"),Ge=c("a"),eo=i("input_ids"),no=i(" s\xE3o os \xEDndices correspondentes \xE0 cada token na frase."),to=d(),Qe=c("li"),Xe=c("a"),lo=i("attention_mask"),ro=i(" indica para cada token se deve receber aten\xE7\xE3o ou n\xE3o."),po=d(),sn=c("li"),an=c("a"),oo=i("token_type_ids"),co=i(" identifica \xE0 qual sequ\xEAncia cada token pertence quando h\xE1 mais de uma."),ql=d(),ea=c("p"),uo=i("Voc\xEA pode ent\xE3o decodificar os "),Qn=c("code"),io=i("input_ids"),ho=i(" para retornar a entrada original:"),zl=d(),E(Ta.$$.fragment),Al=d(),gs=c("p"),mo=i("Como pode ver, o tokenizer adicionou dois tokens especiais - "),Xn=c("code"),fo=i("CLS"),bo=i(" e "),st=c("code"),jo=i("SEP"),go=i(` (classificador e separador) - \xE0 frase.
Nem todos os modelos precisam de tokens especiais, mas caso precisem, o tokenizer ir\xE1 adicion\xE1-los automaticamente para voc\xEA`),Pl=d(),en=c("p"),_o=i("Se houver mais de uma frase que deseja processar, coloque-as numa lista para enviar ao tokenizer:"),Cl=d(),E(Ia.$$.fragment),Dl=d(),Ts=c("h3"),na=c("a"),at=c("span"),E(Oa.$$.fragment),vo=d(),et=c("span"),$o=i("Margens"),Nl=d(),ta=c("p"),wo=i(`Chegamos ent\xE3o num t\xF3pico importante. Quando um conjunto de frases s\xE3o processadas, nem todas mant\xE9m o mesmo tamanho.
Isso se torna um problema, pois os tensores (entradas do modelo), precisam de um tamanho uniforme.
Margens s\xE3o uma estrat\xE9gia para se ter certeza de que os tensores s\xE3o retangulares, ao adicionar um `),nt=c("em"),ko=i("padding token"),Eo=i(`
(token de margens) \xE0s frases com menos tokens.`),Sl=d(),_s=c("p"),xo=i("Defina o par\xE2mentro "),tt=c("code"),yo=i("padding"),qo=i(" para "),lt=c("code"),zo=i("True"),Ao=i(" para adicionar margens \xE0s sequ\xEAncias menores no conjunto, de modo a corresponder \xE0s sequ\xEAncias maiores."),Ll=d(),E(Ma.$$.fragment),Tl=d(),nn=c("p"),Po=i("Perceba que o tokenizer adicionou margens \xE0 primeira e \xE0 segunda frase iguais a 0, pois ambas frases eram menores que as restantes."),Il=d(),Is=c("h3"),la=c("a"),rt=c("span"),E(Ha.$$.fragment),Co=d(),pt=c("span"),Do=i("Truncar"),Ol=d(),tn=c("p"),No=i(`Do outro lado do espectro, as vezes uma sequ\xEAncia seja muito grande para um modelo. Nesse caso, \xE9 necess\xE1rio truncar
a sequ\xEAncia para que tenha um tamanho menor.`),Ml=d(),vs=c("p"),So=i("Defina o par\xE2mentro "),ot=c("code"),Lo=i("truncation"),To=i(" para "),ct=c("code"),Io=i("True"),Oo=i(" para truncar a sequ\xEAncia ao tamanho m\xE1ximo aceito pelo modelo:"),Hl=d(),E(Fa.$$.fragment),Fl=d(),Os=c("h3"),ra=c("a"),ut=c("span"),E(Ja.$$.fragment),Mo=d(),it=c("span"),Ho=i("Construindo tensores"),Jl=d(),ln=c("p"),Fo=i("Voc\xEA agora quer que o tokenizer retorne os tensores que dever\xE3o ser inseridos ao modelo."),Rl=d(),os=c("p"),Jo=i("Defina o par\xE2metro "),ht=c("code"),Ro=i("return_tensors"),Bo=i(" para "),mt=c("code"),Vo=i("pt"),Wo=i(" para o PyTorch, ou "),ft=c("code"),Uo=i("tf"),Ko=i(" para o TensorFlow:"),Bl=d(),E(pa.$$.fragment),Vl=d(),Ms=c("h2"),oa=c("a"),dt=c("span"),E(Ra.$$.fragment),Yo=d(),bt=c("span"),Zo=i("Audio"),Wl=d(),ca=c("p"),Go=i(`Entradas de \xC1udio s\xE3o processadas
Audio inputs are preprocessed differently than textual inputs, but the end goal remains the same: create numerical sequences the model can understand. A `),rn=c("a"),Qo=i("feature extractor"),Xo=i(" is designed for the express purpose of extracting features from raw image or audio data and converting them into tensors. Before you begin, install \u{1F917} Datasets to load an audio dataset to experiment with:"),Ul=d(),E(Ba.$$.fragment),Kl=d(),$s=c("p"),sc=i("Load the "),Va=c("a"),ac=i("MInDS-14"),ec=i(" dataset (see the \u{1F917} "),Wa=c("a"),nc=i("Datasets tutorial"),tc=i(" for more details on how to load a dataset):"),Yl=d(),E(Ua.$$.fragment),Zl=d(),ws=c("p"),lc=i("Access the first element of the "),jt=c("code"),rc=i("audio"),pc=i(" column to take a look at the input. Calling the "),gt=c("code"),oc=i("audio"),cc=i(" column will automatically load and resample the audio file:"),Gl=d(),E(Ka.$$.fragment),Ql=d(),pn=c("p"),uc=i("This returns three items:"),Xl=d(),ks=c("ul"),on=c("li"),_t=c("code"),ic=i("array"),hc=i(" is the speech signal loaded - and potentially resampled - as a 1D array."),mc=d(),cn=c("li"),vt=c("code"),fc=i("path"),dc=i(" points to the location of the audio file."),bc=d(),un=c("li"),$t=c("code"),jc=i("sampling_rate"),gc=i(" refers to how many data points in the speech signal are measured per second."),sr=d(),Hs=c("h3"),ua=c("a"),wt=c("span"),E(Ya.$$.fragment),_c=d(),kt=c("span"),vc=i("Resample"),ar=d(),ia=c("p"),$c=i("For this tutorial, you will use the "),Za=c("a"),wc=i("Wav2Vec2"),kc=i(" model. As you can see from the model card, the Wav2Vec2 model is pretrained on 16kHz sampled speech audio. It is important your audio data\u2019s sampling rate matches the sampling rate of the dataset used to pretrain the model. If your data\u2019s sampling rate isn\u2019t the same, then you need to resample your audio data."),er=d(),ha=c("p"),Ec=i("For example, the "),Ga=c("a"),xc=i("MInDS-14"),yc=i(" dataset has a sampling rate of 8000kHz. In order to use the Wav2Vec2 model with this dataset, upsample the sampling rate to 16kHz:"),nr=d(),E(Qa.$$.fragment),tr=d(),hn=c("ol"),Xa=c("li"),qc=i("Use \u{1F917} Datasets\u2019 "),se=c("a"),Et=c("code"),zc=i("cast_column"),Ac=i(" method to upsample the sampling rate to 16kHz:"),lr=d(),E(ae.$$.fragment),rr=d(),ee=c("ol"),xt=c("li"),Pc=i("Load the audio file:"),pr=d(),E(ne.$$.fragment),or=d(),ma=c("p"),Cc=i("As you can see, the "),yt=c("code"),Dc=i("sampling_rate"),Nc=i(" is now 16kHz!"),cr=d(),Fs=c("h3"),fa=c("a"),qt=c("span"),E(te.$$.fragment),Sc=d(),zt=c("span"),Lc=i("Feature extractor"),ur=d(),cs=c("p"),Tc=i("The next step is to load a feature extractor to normalize and pad the input. When padding textual data, a "),At=c("code"),Ic=i("0"),Oc=i(" is added for shorter sequences. The same idea applies to audio data, and the audio feature extractor will add a "),Pt=c("code"),Mc=i("0"),Hc=i(" - interpreted as silence - to "),Ct=c("code"),Fc=i("array"),Jc=i("."),ir=d(),da=c("p"),Rc=i("Load the feature extractor with "),Dt=c("code"),Bc=i("AutoFeatureExtractor.from_pretrained()"),Vc=i(":"),hr=d(),E(le.$$.fragment),mr=d(),Es=c("p"),Wc=i("Pass the audio "),Nt=c("code"),Uc=i("array"),Kc=i(" to the feature extractor. We also recommend adding the "),St=c("code"),Yc=i("sampling_rate"),Zc=i(" argument in the feature extractor in order to better debug any silent errors that may occur."),fr=d(),E(re.$$.fragment),dr=d(),Js=c("h3"),ba=c("a"),Lt=c("span"),E(pe.$$.fragment),Gc=d(),Tt=c("span"),Qc=i("Pad and truncate"),br=d(),mn=c("p"),Xc=i("Just like the tokenizer, you can apply padding or truncation to handle variable sequences in a batch. Take a look at the sequence length of these two audio samples:"),jr=d(),E(oe.$$.fragment),gr=d(),fn=c("p"),su=i("As you can see, the first sample has a longer sequence than the second sample. Let\u2019s create a function that will preprocess the dataset. Specify a maximum sample length, and the feature extractor will either pad or truncate the sequences to match it:"),_r=d(),E(ce.$$.fragment),vr=d(),dn=c("p"),au=i("Apply the function to the the first few examples in the dataset:"),$r=d(),E(ue.$$.fragment),wr=d(),bn=c("p"),eu=i("Now take another look at the processed sample lengths:"),kr=d(),E(ie.$$.fragment),Er=d(),jn=c("p"),nu=i("The lengths of the first two samples now match the maximum length you specified."),xr=d(),Rs=c("h2"),ja=c("a"),It=c("span"),E(he.$$.fragment),tu=d(),Ot=c("span"),lu=i("Vision"),yr=d(),gn=c("p"),ru=i("A feature extractor is also used to process images for vision tasks. Once again, the goal is to convert the raw image into a batch of tensors as input."),qr=d(),xs=c("p"),pu=i("Let\u2019s load the "),me=c("a"),ou=i("food101"),cu=i(" dataset for this tutorial. Use \u{1F917} Datasets "),Mt=c("code"),uu=i("split"),iu=i(" parameter to only load a small sample from the training split since the dataset is quite large:"),zr=d(),E(fe.$$.fragment),Ar=d(),ga=c("p"),hu=i("Next, take a look at the image with \u{1F917} Datasets "),de=c("a"),Ht=c("code"),mu=i("Image"),fu=i(" feature:"),Pr=d(),E(be.$$.fragment),Cr=d(),_n=c("p"),vn=c("img"),Dr=d(),Bs=c("h3"),_a=c("a"),Ft=c("span"),E(je.$$.fragment),du=d(),Jt=c("span"),bu=i("Feature extractor"),Nr=d(),va=c("p"),ju=i("Load the feature extractor with "),Rt=c("code"),gu=i("AutoFeatureExtractor.from_pretrained()"),_u=i(":"),Sr=d(),E(ge.$$.fragment),Lr=d(),Vs=c("h3"),$a=c("a"),Bt=c("span"),E(_e.$$.fragment),vu=d(),Vt=c("span"),$u=i("Data augmentation"),Tr=d(),wa=c("p"),wu=i("For vision tasks, it is common to add some type of data augmentation to the images as a part of preprocessing. You can add augmentations with any library you\u2019d like, but in this tutorial, you will use torchvision\u2019s "),ve=c("a"),Wt=c("code"),ku=i("transforms"),Eu=i(" module."),Ir=d(),$n=c("ol"),ms=c("li"),xu=i("Normalize the image and use "),$e=c("a"),Ut=c("code"),yu=i("Compose"),qu=i(" to chain some transforms - "),we=c("a"),Kt=c("code"),zu=i("RandomResizedCrop"),Au=i(" and "),ke=c("a"),Yt=c("code"),Pu=i("ColorJitter"),Cu=i(" - together:"),Or=d(),E(Ee.$$.fragment),Mr=d(),xe=c("ol"),Ws=c("li"),Du=i("The model accepts "),wn=c("a"),Zt=c("code"),Nu=i("pixel_values"),Su=i(" as it\u2019s input. This value is generated by the feature extractor. Create a function that generates "),Gt=c("code"),Lu=i("pixel_values"),Tu=i(" from the transforms:"),Hr=d(),E(ye.$$.fragment),Fr=d(),qe=c("ol"),ze=c("li"),Iu=i("Then use \u{1F917} Datasets "),Ae=c("a"),Qt=c("code"),Ou=i("set_transform"),Mu=i(" to apply the transforms on-the-fly:"),Jr=d(),E(Pe.$$.fragment),Rr=d(),Ce=c("ol"),De=c("li"),Hu=i("Now when you access the image, you will notice the feature extractor has added the model input "),Xt=c("code"),Fu=i("pixel_values"),Ju=i(":"),Br=d(),E(Ne.$$.fragment),Vr=d(),kn=c("p"),Ru=i("Here is what the image looks like after you preprocess it. Just as you\u2019d expect from the applied transforms, the image has been randomly cropped and it\u2019s color properties are different."),Wr=d(),E(Se.$$.fragment),Ur=d(),En=c("p"),xn=c("img"),Kr=d(),Us=c("h2"),ka=c("a"),sl=c("span"),E(Le.$$.fragment),Bu=d(),al=c("span"),Vu=i("Multimodal"),Yr=d(),yn=c("p"),Wu=i("For multimodal tasks. you will use a combination of everything you\u2019ve learned so far and apply your skills to a automatic speech recognition (ASR) task. This means you will need a:"),Zr=d(),Ea=c("ul"),el=c("li"),Uu=i("Feature extractor to preprocess the audio data."),Ku=d(),nl=c("li"),Yu=i("Tokenizer to process the text."),Gr=d(),xa=c("p"),Zu=i("Let\u2019s return to the "),Te=c("a"),Gu=i("LJ Speech"),Qu=i(" dataset:"),Qr=d(),E(Ie.$$.fragment),Xr=d(),ys=c("p"),Xu=i("Since you are mainly interested in the "),tl=c("code"),si=i("audio"),ai=i(" and "),ll=c("code"),ei=i("text"),ni=i(" column, remove the other columns:"),sp=d(),E(Oe.$$.fragment),ap=d(),qs=c("p"),ti=i("Now take a look at the "),rl=c("code"),li=i("audio"),ri=i(" and "),pl=c("code"),pi=i("text"),oi=i(" columns:"),ep=d(),E(Me.$$.fragment),np=d(),ya=c("p"),ci=i("Remember from the earlier section on processing audio data, you should always "),qn=c("a"),ui=i("resample"),ii=i(" your audio data\u2019s sampling rate to match the sampling rate of the dataset used to pretrain a model:"),tp=d(),E(He.$$.fragment),lp=d(),Ks=c("h3"),qa=c("a"),ol=c("span"),E(Fe.$$.fragment),hi=d(),cl=c("span"),mi=i("Processor"),rp=d(),zn=c("p"),fi=i("A processor combines a feature extractor and tokenizer. Load a processor with [`AutoProcessor.from_pretrained]:"),pp=d(),E(Je.$$.fragment),op=d(),An=c("ol"),Ys=c("li"),di=i("Create a function to process the audio data to "),ul=c("code"),bi=i("input_values"),ji=i(", and tokenizes the text to "),il=c("code"),gi=i("labels"),_i=i(". These are your inputs to the model:"),cp=d(),E(Re.$$.fragment),up=d(),Be=c("ol"),Ve=c("li"),vi=i("Apply the "),hl=c("code"),$i=i("prepare_dataset"),wi=i(" function to a sample:"),ip=d(),E(We.$$.fragment),hp=d(),zs=c("p"),ki=i("Notice the processor has added "),ml=c("code"),Ei=i("input_values"),xi=i(" and "),fl=c("code"),yi=i("labels"),qi=i(". The sampling rate has also been correctly downsampled to 16kHz."),mp=d(),Pn=c("p"),zi=i("Awesome, you should now be able to preprocess data for any modality and even combine different modalities! In the next tutorial, learn how to fine-tune a model on your newly preprocessed data."),this.h()},l(s){const p=Of('[data-svelte="svelte-1phssyn"]',document.head);n=u(p,"META",{name:!0,content:!0}),p.forEach(a),f=b(s),r=u(s,"H1",{class:!0});var Ue=t(r);l=u(Ue,"A",{id:!0,class:!0,href:!0});var dl=t(l);j=u(dl,"SPAN",{});var Hi=t(j);q(_.$$.fragment,Hi),Hi.forEach(a),dl.forEach(a),k=b(Ue),w=u(Ue,"SPAN",{});var Fi=t(w);C=h(Fi,"Preprocess"),Fi.forEach(a),Ue.forEach(a),L=b(s),q(P.$$.fragment,s),I=b(s),T=u(s,"P",{});var Ji=t(T);V=h(Ji,`Antes de poder usar dados em seu modelo, os mesmos precisam ser processados em um formato aceito pelo modelo.
O modelo n\xE3o interpreta texto puro, imagens ou \xE1udio. Essas entradas precisam ser convertidas em n\xFAmeros e moldadas em tensores.
Nesse tutorial voc\xEA ir\xE1:`),Ji.forEach(a),W=b(s),F=u(s,"UL",{});var Cn=t(F);D=u(Cn,"LI",{});var Ri=t(D);O=h(Ri,"Pr\xE9-processar dados em texto com um tokenizer."),Ri.forEach(a),J=b(Cn),U=u(Cn,"LI",{});var Bi=t(U);N=h(Bi,"Pr\xE9-processar dados de uma imagem ou \xE1udio com um extrator de caracter\xEDsticas (feature extractor)."),Bi.forEach(a),H=b(Cn),R=u(Cn,"LI",{});var Vi=t(R);M=h(Vi,"Pr\xE9-processar dados para uma tarefa multimodal com um processador."),Vi.forEach(a),Cn.forEach(a),ns=b(s),Z=u(s,"H2",{class:!0});var dp=t(Z);X=u(dp,"A",{id:!0,class:!0,href:!0});var Wi=t(X);us=u(Wi,"SPAN",{});var Ui=t(us);q(ts.$$.fragment,Ui),Ui.forEach(a),Wi.forEach(a),Ps=b(dp),is=u(dp,"SPAN",{});var Ki=t(is);hs=h(Ki,"NLP"),Ki.forEach(a),dp.forEach(a),G=b(s),q(ls.$$.fragment,s),fs=b(s),ss=u(s,"P",{});var bp=t(ss);Cs=h(bp,"A ferramenta principal para o processamento de dados textuais \xE9 um "),K=u(bp,"A",{href:!0});var Yi=t(K);Ds=h(Yi,"tokenizer"),Yi.forEach(a),Ns=h(bp,`. O Tokenizer
divide o texto em \u201Ctokens\u201D, seguindo um conjunto de regras pr\xE9-definidas.
Esses tokens s\xE3o convertidos em n\xFAmeros, usados em troca na constru\xE7\xE3o de tensores para a entrada de um modelo.
Quaisquer entradas adicionais solicitadas pelo modelo tamb\xE9m s\xE3o adicionadas por meio do tokenizer.`),bp.forEach(a),ds=b(s),q(Y.$$.fragment,s),B=b(s),rs=u(s,"P",{});var Dn=t(rs);ps=h(Dn,"Para come\xE7ar, carregue um modelo pr\xE9-treinado de tokenizer com a classe "),bs=u(Dn,"CODE",{});var Zi=t(bs);Ca=h(Zi,"AutoTokenizer"),Zi.forEach(a),Da=h(Dn,`. Isso ir\xE1 baixar automaticamente
o `),as=u(Dn,"EM",{});var Gi=t(as);Ss=h(Gi,"vocab"),Gi.forEach(a),Xs=h(Dn," utilizado pelo modelo quando foi pr\xE9-treinado."),Dn.forEach(a),vl=b(s),Ls=u(s,"H3",{class:!0});var jp=t(Ls);sa=u(jp,"A",{id:!0,class:!0,href:!0});var Qi=t(sa);Yn=u(Qi,"SPAN",{});var Xi=t(Yn);q(Na.$$.fragment,Xi),Xi.forEach(a),Qi.forEach(a),Yp=b(jp),Zn=u(jp,"SPAN",{});var sh=t(Zn);Zp=h(sh,"Tokenizar"),sh.forEach(a),jp.forEach(a),$l=b(s),aa=u(s,"P",{});var gp=t(aa);Gp=h(gp,"Carregue um tokenizer pr\xE9-treinado com "),Gn=u(gp,"CODE",{});var ah=t(Gn);Qp=h(ah,"AutoTokenizer.from_pretrained()"),ah.forEach(a),Xp=h(gp,":"),gp.forEach(a),wl=b(s),q(Sa.$$.fragment,s),kl=b(s),Ke=u(s,"P",{});var eh=t(Ke);so=h(eh,"Em seguida, passe sua frase ao tokenizer:"),eh.forEach(a),El=b(s),q(La.$$.fragment,s),xl=b(s),Ye=u(s,"P",{});var nh=t(Ye);ao=h(nh,"O tokenizer ir\xE1 retornar um dicion\xE1rio com tr\xEAs \xEDtens importantes:"),nh.forEach(a),yl=b(s),js=u(s,"UL",{});var Nn=t(js);Ze=u(Nn,"LI",{});var Ai=t(Ze);Ge=u(Ai,"A",{href:!0});var th=t(Ge);eo=h(th,"input_ids"),th.forEach(a),no=h(Ai," s\xE3o os \xEDndices correspondentes \xE0 cada token na frase."),Ai.forEach(a),to=b(Nn),Qe=u(Nn,"LI",{});var Pi=t(Qe);Xe=u(Pi,"A",{href:!0});var lh=t(Xe);lo=h(lh,"attention_mask"),lh.forEach(a),ro=h(Pi," indica para cada token se deve receber aten\xE7\xE3o ou n\xE3o."),Pi.forEach(a),po=b(Nn),sn=u(Nn,"LI",{});var Ci=t(sn);an=u(Ci,"A",{href:!0});var rh=t(an);oo=h(rh,"token_type_ids"),rh.forEach(a),co=h(Ci," identifica \xE0 qual sequ\xEAncia cada token pertence quando h\xE1 mais de uma."),Ci.forEach(a),Nn.forEach(a),ql=b(s),ea=u(s,"P",{});var _p=t(ea);uo=h(_p,"Voc\xEA pode ent\xE3o decodificar os "),Qn=u(_p,"CODE",{});var ph=t(Qn);io=h(ph,"input_ids"),ph.forEach(a),ho=h(_p," para retornar a entrada original:"),_p.forEach(a),zl=b(s),q(Ta.$$.fragment,s),Al=b(s),gs=u(s,"P",{});var Sn=t(gs);mo=h(Sn,"Como pode ver, o tokenizer adicionou dois tokens especiais - "),Xn=u(Sn,"CODE",{});var oh=t(Xn);fo=h(oh,"CLS"),oh.forEach(a),bo=h(Sn," e "),st=u(Sn,"CODE",{});var ch=t(st);jo=h(ch,"SEP"),ch.forEach(a),go=h(Sn,` (classificador e separador) - \xE0 frase.
Nem todos os modelos precisam de tokens especiais, mas caso precisem, o tokenizer ir\xE1 adicion\xE1-los automaticamente para voc\xEA`),Sn.forEach(a),Pl=b(s),en=u(s,"P",{});var uh=t(en);_o=h(uh,"Se houver mais de uma frase que deseja processar, coloque-as numa lista para enviar ao tokenizer:"),uh.forEach(a),Cl=b(s),q(Ia.$$.fragment,s),Dl=b(s),Ts=u(s,"H3",{class:!0});var vp=t(Ts);na=u(vp,"A",{id:!0,class:!0,href:!0});var ih=t(na);at=u(ih,"SPAN",{});var hh=t(at);q(Oa.$$.fragment,hh),hh.forEach(a),ih.forEach(a),vo=b(vp),et=u(vp,"SPAN",{});var mh=t(et);$o=h(mh,"Margens"),mh.forEach(a),vp.forEach(a),Nl=b(s),ta=u(s,"P",{});var $p=t(ta);wo=h($p,`Chegamos ent\xE3o num t\xF3pico importante. Quando um conjunto de frases s\xE3o processadas, nem todas mant\xE9m o mesmo tamanho.
Isso se torna um problema, pois os tensores (entradas do modelo), precisam de um tamanho uniforme.
Margens s\xE3o uma estrat\xE9gia para se ter certeza de que os tensores s\xE3o retangulares, ao adicionar um `),nt=u($p,"EM",{});var fh=t(nt);ko=h(fh,"padding token"),fh.forEach(a),Eo=h($p,`
(token de margens) \xE0s frases com menos tokens.`),$p.forEach(a),Sl=b(s),_s=u(s,"P",{});var Ln=t(_s);xo=h(Ln,"Defina o par\xE2mentro "),tt=u(Ln,"CODE",{});var dh=t(tt);yo=h(dh,"padding"),dh.forEach(a),qo=h(Ln," para "),lt=u(Ln,"CODE",{});var bh=t(lt);zo=h(bh,"True"),bh.forEach(a),Ao=h(Ln," para adicionar margens \xE0s sequ\xEAncias menores no conjunto, de modo a corresponder \xE0s sequ\xEAncias maiores."),Ln.forEach(a),Ll=b(s),q(Ma.$$.fragment,s),Tl=b(s),nn=u(s,"P",{});var jh=t(nn);Po=h(jh,"Perceba que o tokenizer adicionou margens \xE0 primeira e \xE0 segunda frase iguais a 0, pois ambas frases eram menores que as restantes."),jh.forEach(a),Il=b(s),Is=u(s,"H3",{class:!0});var wp=t(Is);la=u(wp,"A",{id:!0,class:!0,href:!0});var gh=t(la);rt=u(gh,"SPAN",{});var _h=t(rt);q(Ha.$$.fragment,_h),_h.forEach(a),gh.forEach(a),Co=b(wp),pt=u(wp,"SPAN",{});var vh=t(pt);Do=h(vh,"Truncar"),vh.forEach(a),wp.forEach(a),Ol=b(s),tn=u(s,"P",{});var $h=t(tn);No=h($h,`Do outro lado do espectro, as vezes uma sequ\xEAncia seja muito grande para um modelo. Nesse caso, \xE9 necess\xE1rio truncar
a sequ\xEAncia para que tenha um tamanho menor.`),$h.forEach(a),Ml=b(s),vs=u(s,"P",{});var Tn=t(vs);So=h(Tn,"Defina o par\xE2mentro "),ot=u(Tn,"CODE",{});var wh=t(ot);Lo=h(wh,"truncation"),wh.forEach(a),To=h(Tn," para "),ct=u(Tn,"CODE",{});var kh=t(ct);Io=h(kh,"True"),kh.forEach(a),Oo=h(Tn," para truncar a sequ\xEAncia ao tamanho m\xE1ximo aceito pelo modelo:"),Tn.forEach(a),Hl=b(s),q(Fa.$$.fragment,s),Fl=b(s),Os=u(s,"H3",{class:!0});var kp=t(Os);ra=u(kp,"A",{id:!0,class:!0,href:!0});var Eh=t(ra);ut=u(Eh,"SPAN",{});var xh=t(ut);q(Ja.$$.fragment,xh),xh.forEach(a),Eh.forEach(a),Mo=b(kp),it=u(kp,"SPAN",{});var yh=t(it);Ho=h(yh,"Construindo tensores"),yh.forEach(a),kp.forEach(a),Jl=b(s),ln=u(s,"P",{});var qh=t(ln);Fo=h(qh,"Voc\xEA agora quer que o tokenizer retorne os tensores que dever\xE3o ser inseridos ao modelo."),qh.forEach(a),Rl=b(s),os=u(s,"P",{});var za=t(os);Jo=h(za,"Defina o par\xE2metro "),ht=u(za,"CODE",{});var zh=t(ht);Ro=h(zh,"return_tensors"),zh.forEach(a),Bo=h(za," para "),mt=u(za,"CODE",{});var Ah=t(mt);Vo=h(Ah,"pt"),Ah.forEach(a),Wo=h(za," para o PyTorch, ou "),ft=u(za,"CODE",{});var Ph=t(ft);Uo=h(Ph,"tf"),Ph.forEach(a),Ko=h(za," para o TensorFlow:"),za.forEach(a),Bl=b(s),q(pa.$$.fragment,s),Vl=b(s),Ms=u(s,"H2",{class:!0});var Ep=t(Ms);oa=u(Ep,"A",{id:!0,class:!0,href:!0});var Ch=t(oa);dt=u(Ch,"SPAN",{});var Dh=t(dt);q(Ra.$$.fragment,Dh),Dh.forEach(a),Ch.forEach(a),Yo=b(Ep),bt=u(Ep,"SPAN",{});var Nh=t(bt);Zo=h(Nh,"Audio"),Nh.forEach(a),Ep.forEach(a),Wl=b(s),ca=u(s,"P",{});var xp=t(ca);Go=h(xp,`Entradas de \xC1udio s\xE3o processadas
Audio inputs are preprocessed differently than textual inputs, but the end goal remains the same: create numerical sequences the model can understand. A `),rn=u(xp,"A",{href:!0});var Sh=t(rn);Qo=h(Sh,"feature extractor"),Sh.forEach(a),Xo=h(xp," is designed for the express purpose of extracting features from raw image or audio data and converting them into tensors. Before you begin, install \u{1F917} Datasets to load an audio dataset to experiment with:"),xp.forEach(a),Ul=b(s),q(Ba.$$.fragment,s),Kl=b(s),$s=u(s,"P",{});var In=t($s);sc=h(In,"Load the "),Va=u(In,"A",{href:!0,rel:!0});var Lh=t(Va);ac=h(Lh,"MInDS-14"),Lh.forEach(a),ec=h(In," dataset (see the \u{1F917} "),Wa=u(In,"A",{href:!0,rel:!0});var Th=t(Wa);nc=h(Th,"Datasets tutorial"),Th.forEach(a),tc=h(In," for more details on how to load a dataset):"),In.forEach(a),Yl=b(s),q(Ua.$$.fragment,s),Zl=b(s),ws=u(s,"P",{});var On=t(ws);lc=h(On,"Access the first element of the "),jt=u(On,"CODE",{});var Ih=t(jt);rc=h(Ih,"audio"),Ih.forEach(a),pc=h(On," column to take a look at the input. Calling the "),gt=u(On,"CODE",{});var Oh=t(gt);oc=h(Oh,"audio"),Oh.forEach(a),cc=h(On," column will automatically load and resample the audio file:"),On.forEach(a),Gl=b(s),q(Ka.$$.fragment,s),Ql=b(s),pn=u(s,"P",{});var Mh=t(pn);uc=h(Mh,"This returns three items:"),Mh.forEach(a),Xl=b(s),ks=u(s,"UL",{});var Mn=t(ks);on=u(Mn,"LI",{});var Di=t(on);_t=u(Di,"CODE",{});var Hh=t(_t);ic=h(Hh,"array"),Hh.forEach(a),hc=h(Di," is the speech signal loaded - and potentially resampled - as a 1D array."),Di.forEach(a),mc=b(Mn),cn=u(Mn,"LI",{});var Ni=t(cn);vt=u(Ni,"CODE",{});var Fh=t(vt);fc=h(Fh,"path"),Fh.forEach(a),dc=h(Ni," points to the location of the audio file."),Ni.forEach(a),bc=b(Mn),un=u(Mn,"LI",{});var Si=t(un);$t=u(Si,"CODE",{});var Jh=t($t);jc=h(Jh,"sampling_rate"),Jh.forEach(a),gc=h(Si," refers to how many data points in the speech signal are measured per second."),Si.forEach(a),Mn.forEach(a),sr=b(s),Hs=u(s,"H3",{class:!0});var yp=t(Hs);ua=u(yp,"A",{id:!0,class:!0,href:!0});var Rh=t(ua);wt=u(Rh,"SPAN",{});var Bh=t(wt);q(Ya.$$.fragment,Bh),Bh.forEach(a),Rh.forEach(a),_c=b(yp),kt=u(yp,"SPAN",{});var Vh=t(kt);vc=h(Vh,"Resample"),Vh.forEach(a),yp.forEach(a),ar=b(s),ia=u(s,"P",{});var qp=t(ia);$c=h(qp,"For this tutorial, you will use the "),Za=u(qp,"A",{href:!0,rel:!0});var Wh=t(Za);wc=h(Wh,"Wav2Vec2"),Wh.forEach(a),kc=h(qp," model. As you can see from the model card, the Wav2Vec2 model is pretrained on 16kHz sampled speech audio. It is important your audio data\u2019s sampling rate matches the sampling rate of the dataset used to pretrain the model. If your data\u2019s sampling rate isn\u2019t the same, then you need to resample your audio data."),qp.forEach(a),er=b(s),ha=u(s,"P",{});var zp=t(ha);Ec=h(zp,"For example, the "),Ga=u(zp,"A",{href:!0,rel:!0});var Uh=t(Ga);xc=h(Uh,"MInDS-14"),Uh.forEach(a),yc=h(zp," dataset has a sampling rate of 8000kHz. In order to use the Wav2Vec2 model with this dataset, upsample the sampling rate to 16kHz:"),zp.forEach(a),nr=b(s),q(Qa.$$.fragment,s),tr=b(s),hn=u(s,"OL",{});var Kh=t(hn);Xa=u(Kh,"LI",{});var Ap=t(Xa);qc=h(Ap,"Use \u{1F917} Datasets\u2019 "),se=u(Ap,"A",{href:!0,rel:!0});var Yh=t(se);Et=u(Yh,"CODE",{});var Zh=t(Et);zc=h(Zh,"cast_column"),Zh.forEach(a),Yh.forEach(a),Ac=h(Ap," method to upsample the sampling rate to 16kHz:"),Ap.forEach(a),Kh.forEach(a),lr=b(s),q(ae.$$.fragment,s),rr=b(s),ee=u(s,"OL",{start:!0});var Gh=t(ee);xt=u(Gh,"LI",{});var Qh=t(xt);Pc=h(Qh,"Load the audio file:"),Qh.forEach(a),Gh.forEach(a),pr=b(s),q(ne.$$.fragment,s),or=b(s),ma=u(s,"P",{});var Pp=t(ma);Cc=h(Pp,"As you can see, the "),yt=u(Pp,"CODE",{});var Xh=t(yt);Dc=h(Xh,"sampling_rate"),Xh.forEach(a),Nc=h(Pp," is now 16kHz!"),Pp.forEach(a),cr=b(s),Fs=u(s,"H3",{class:!0});var Cp=t(Fs);fa=u(Cp,"A",{id:!0,class:!0,href:!0});var sm=t(fa);qt=u(sm,"SPAN",{});var am=t(qt);q(te.$$.fragment,am),am.forEach(a),sm.forEach(a),Sc=b(Cp),zt=u(Cp,"SPAN",{});var em=t(zt);Lc=h(em,"Feature extractor"),em.forEach(a),Cp.forEach(a),ur=b(s),cs=u(s,"P",{});var Aa=t(cs);Tc=h(Aa,"The next step is to load a feature extractor to normalize and pad the input. When padding textual data, a "),At=u(Aa,"CODE",{});var nm=t(At);Ic=h(nm,"0"),nm.forEach(a),Oc=h(Aa," is added for shorter sequences. The same idea applies to audio data, and the audio feature extractor will add a "),Pt=u(Aa,"CODE",{});var tm=t(Pt);Mc=h(tm,"0"),tm.forEach(a),Hc=h(Aa," - interpreted as silence - to "),Ct=u(Aa,"CODE",{});var lm=t(Ct);Fc=h(lm,"array"),lm.forEach(a),Jc=h(Aa,"."),Aa.forEach(a),ir=b(s),da=u(s,"P",{});var Dp=t(da);Rc=h(Dp,"Load the feature extractor with "),Dt=u(Dp,"CODE",{});var rm=t(Dt);Bc=h(rm,"AutoFeatureExtractor.from_pretrained()"),rm.forEach(a),Vc=h(Dp,":"),Dp.forEach(a),hr=b(s),q(le.$$.fragment,s),mr=b(s),Es=u(s,"P",{});var Hn=t(Es);Wc=h(Hn,"Pass the audio "),Nt=u(Hn,"CODE",{});var pm=t(Nt);Uc=h(pm,"array"),pm.forEach(a),Kc=h(Hn," to the feature extractor. We also recommend adding the "),St=u(Hn,"CODE",{});var om=t(St);Yc=h(om,"sampling_rate"),om.forEach(a),Zc=h(Hn," argument in the feature extractor in order to better debug any silent errors that may occur."),Hn.forEach(a),fr=b(s),q(re.$$.fragment,s),dr=b(s),Js=u(s,"H3",{class:!0});var Np=t(Js);ba=u(Np,"A",{id:!0,class:!0,href:!0});var cm=t(ba);Lt=u(cm,"SPAN",{});var um=t(Lt);q(pe.$$.fragment,um),um.forEach(a),cm.forEach(a),Gc=b(Np),Tt=u(Np,"SPAN",{});var im=t(Tt);Qc=h(im,"Pad and truncate"),im.forEach(a),Np.forEach(a),br=b(s),mn=u(s,"P",{});var hm=t(mn);Xc=h(hm,"Just like the tokenizer, you can apply padding or truncation to handle variable sequences in a batch. Take a look at the sequence length of these two audio samples:"),hm.forEach(a),jr=b(s),q(oe.$$.fragment,s),gr=b(s),fn=u(s,"P",{});var mm=t(fn);su=h(mm,"As you can see, the first sample has a longer sequence than the second sample. Let\u2019s create a function that will preprocess the dataset. Specify a maximum sample length, and the feature extractor will either pad or truncate the sequences to match it:"),mm.forEach(a),_r=b(s),q(ce.$$.fragment,s),vr=b(s),dn=u(s,"P",{});var fm=t(dn);au=h(fm,"Apply the function to the the first few examples in the dataset:"),fm.forEach(a),$r=b(s),q(ue.$$.fragment,s),wr=b(s),bn=u(s,"P",{});var dm=t(bn);eu=h(dm,"Now take another look at the processed sample lengths:"),dm.forEach(a),kr=b(s),q(ie.$$.fragment,s),Er=b(s),jn=u(s,"P",{});var bm=t(jn);nu=h(bm,"The lengths of the first two samples now match the maximum length you specified."),bm.forEach(a),xr=b(s),Rs=u(s,"H2",{class:!0});var Sp=t(Rs);ja=u(Sp,"A",{id:!0,class:!0,href:!0});var jm=t(ja);It=u(jm,"SPAN",{});var gm=t(It);q(he.$$.fragment,gm),gm.forEach(a),jm.forEach(a),tu=b(Sp),Ot=u(Sp,"SPAN",{});var _m=t(Ot);lu=h(_m,"Vision"),_m.forEach(a),Sp.forEach(a),yr=b(s),gn=u(s,"P",{});var vm=t(gn);ru=h(vm,"A feature extractor is also used to process images for vision tasks. Once again, the goal is to convert the raw image into a batch of tensors as input."),vm.forEach(a),qr=b(s),xs=u(s,"P",{});var Fn=t(xs);pu=h(Fn,"Let\u2019s load the "),me=u(Fn,"A",{href:!0,rel:!0});var $m=t(me);ou=h($m,"food101"),$m.forEach(a),cu=h(Fn," dataset for this tutorial. Use \u{1F917} Datasets "),Mt=u(Fn,"CODE",{});var wm=t(Mt);uu=h(wm,"split"),wm.forEach(a),iu=h(Fn," parameter to only load a small sample from the training split since the dataset is quite large:"),Fn.forEach(a),zr=b(s),q(fe.$$.fragment,s),Ar=b(s),ga=u(s,"P",{});var Lp=t(ga);hu=h(Lp,"Next, take a look at the image with \u{1F917} Datasets "),de=u(Lp,"A",{href:!0,rel:!0});var km=t(de);Ht=u(km,"CODE",{});var Em=t(Ht);mu=h(Em,"Image"),Em.forEach(a),km.forEach(a),fu=h(Lp," feature:"),Lp.forEach(a),Pr=b(s),q(be.$$.fragment,s),Cr=b(s),_n=u(s,"P",{});var xm=t(_n);vn=u(xm,"IMG",{src:!0,alt:!0}),xm.forEach(a),Dr=b(s),Bs=u(s,"H3",{class:!0});var Tp=t(Bs);_a=u(Tp,"A",{id:!0,class:!0,href:!0});var ym=t(_a);Ft=u(ym,"SPAN",{});var qm=t(Ft);q(je.$$.fragment,qm),qm.forEach(a),ym.forEach(a),du=b(Tp),Jt=u(Tp,"SPAN",{});var zm=t(Jt);bu=h(zm,"Feature extractor"),zm.forEach(a),Tp.forEach(a),Nr=b(s),va=u(s,"P",{});var Ip=t(va);ju=h(Ip,"Load the feature extractor with "),Rt=u(Ip,"CODE",{});var Am=t(Rt);gu=h(Am,"AutoFeatureExtractor.from_pretrained()"),Am.forEach(a),_u=h(Ip,":"),Ip.forEach(a),Sr=b(s),q(ge.$$.fragment,s),Lr=b(s),Vs=u(s,"H3",{class:!0});var Op=t(Vs);$a=u(Op,"A",{id:!0,class:!0,href:!0});var Pm=t($a);Bt=u(Pm,"SPAN",{});var Cm=t(Bt);q(_e.$$.fragment,Cm),Cm.forEach(a),Pm.forEach(a),vu=b(Op),Vt=u(Op,"SPAN",{});var Dm=t(Vt);$u=h(Dm,"Data augmentation"),Dm.forEach(a),Op.forEach(a),Tr=b(s),wa=u(s,"P",{});var Mp=t(wa);wu=h(Mp,"For vision tasks, it is common to add some type of data augmentation to the images as a part of preprocessing. You can add augmentations with any library you\u2019d like, but in this tutorial, you will use torchvision\u2019s "),ve=u(Mp,"A",{href:!0,rel:!0});var Nm=t(ve);Wt=u(Nm,"CODE",{});var Sm=t(Wt);ku=h(Sm,"transforms"),Sm.forEach(a),Nm.forEach(a),Eu=h(Mp," module."),Mp.forEach(a),Ir=b(s),$n=u(s,"OL",{});var Lm=t($n);ms=u(Lm,"LI",{});var Pa=t(ms);xu=h(Pa,"Normalize the image and use "),$e=u(Pa,"A",{href:!0,rel:!0});var Tm=t($e);Ut=u(Tm,"CODE",{});var Im=t(Ut);yu=h(Im,"Compose"),Im.forEach(a),Tm.forEach(a),qu=h(Pa," to chain some transforms - "),we=u(Pa,"A",{href:!0,rel:!0});var Om=t(we);Kt=u(Om,"CODE",{});var Mm=t(Kt);zu=h(Mm,"RandomResizedCrop"),Mm.forEach(a),Om.forEach(a),Au=h(Pa," and "),ke=u(Pa,"A",{href:!0,rel:!0});var Hm=t(ke);Yt=u(Hm,"CODE",{});var Fm=t(Yt);Pu=h(Fm,"ColorJitter"),Fm.forEach(a),Hm.forEach(a),Cu=h(Pa," - together:"),Pa.forEach(a),Lm.forEach(a),Or=b(s),q(Ee.$$.fragment,s),Mr=b(s),xe=u(s,"OL",{start:!0});var Jm=t(xe);Ws=u(Jm,"LI",{});var Jn=t(Ws);Du=h(Jn,"The model accepts "),wn=u(Jn,"A",{href:!0});var Rm=t(wn);Zt=u(Rm,"CODE",{});var Bm=t(Zt);Nu=h(Bm,"pixel_values"),Bm.forEach(a),Rm.forEach(a),Su=h(Jn," as it\u2019s input. This value is generated by the feature extractor. Create a function that generates "),Gt=u(Jn,"CODE",{});var Vm=t(Gt);Lu=h(Vm,"pixel_values"),Vm.forEach(a),Tu=h(Jn," from the transforms:"),Jn.forEach(a),Jm.forEach(a),Hr=b(s),q(ye.$$.fragment,s),Fr=b(s),qe=u(s,"OL",{start:!0});var Wm=t(qe);ze=u(Wm,"LI",{});var Hp=t(ze);Iu=h(Hp,"Then use \u{1F917} Datasets "),Ae=u(Hp,"A",{href:!0,rel:!0});var Um=t(Ae);Qt=u(Um,"CODE",{});var Km=t(Qt);Ou=h(Km,"set_transform"),Km.forEach(a),Um.forEach(a),Mu=h(Hp," to apply the transforms on-the-fly:"),Hp.forEach(a),Wm.forEach(a),Jr=b(s),q(Pe.$$.fragment,s),Rr=b(s),Ce=u(s,"OL",{start:!0});var Ym=t(Ce);De=u(Ym,"LI",{});var Fp=t(De);Hu=h(Fp,"Now when you access the image, you will notice the feature extractor has added the model input "),Xt=u(Fp,"CODE",{});var Zm=t(Xt);Fu=h(Zm,"pixel_values"),Zm.forEach(a),Ju=h(Fp,":"),Fp.forEach(a),Ym.forEach(a),Br=b(s),q(Ne.$$.fragment,s),Vr=b(s),kn=u(s,"P",{});var Gm=t(kn);Ru=h(Gm,"Here is what the image looks like after you preprocess it. Just as you\u2019d expect from the applied transforms, the image has been randomly cropped and it\u2019s color properties are different."),Gm.forEach(a),Wr=b(s),q(Se.$$.fragment,s),Ur=b(s),En=u(s,"P",{});var Qm=t(En);xn=u(Qm,"IMG",{src:!0,alt:!0}),Qm.forEach(a),Kr=b(s),Us=u(s,"H2",{class:!0});var Jp=t(Us);ka=u(Jp,"A",{id:!0,class:!0,href:!0});var Xm=t(ka);sl=u(Xm,"SPAN",{});var sf=t(sl);q(Le.$$.fragment,sf),sf.forEach(a),Xm.forEach(a),Bu=b(Jp),al=u(Jp,"SPAN",{});var af=t(al);Vu=h(af,"Multimodal"),af.forEach(a),Jp.forEach(a),Yr=b(s),yn=u(s,"P",{});var ef=t(yn);Wu=h(ef,"For multimodal tasks. you will use a combination of everything you\u2019ve learned so far and apply your skills to a automatic speech recognition (ASR) task. This means you will need a:"),ef.forEach(a),Zr=b(s),Ea=u(s,"UL",{});var Rp=t(Ea);el=u(Rp,"LI",{});var nf=t(el);Uu=h(nf,"Feature extractor to preprocess the audio data."),nf.forEach(a),Ku=b(Rp),nl=u(Rp,"LI",{});var tf=t(nl);Yu=h(tf,"Tokenizer to process the text."),tf.forEach(a),Rp.forEach(a),Gr=b(s),xa=u(s,"P",{});var Bp=t(xa);Zu=h(Bp,"Let\u2019s return to the "),Te=u(Bp,"A",{href:!0,rel:!0});var lf=t(Te);Gu=h(lf,"LJ Speech"),lf.forEach(a),Qu=h(Bp," dataset:"),Bp.forEach(a),Qr=b(s),q(Ie.$$.fragment,s),Xr=b(s),ys=u(s,"P",{});var Rn=t(ys);Xu=h(Rn,"Since you are mainly interested in the "),tl=u(Rn,"CODE",{});var rf=t(tl);si=h(rf,"audio"),rf.forEach(a),ai=h(Rn," and "),ll=u(Rn,"CODE",{});var pf=t(ll);ei=h(pf,"text"),pf.forEach(a),ni=h(Rn," column, remove the other columns:"),Rn.forEach(a),sp=b(s),q(Oe.$$.fragment,s),ap=b(s),qs=u(s,"P",{});var Bn=t(qs);ti=h(Bn,"Now take a look at the "),rl=u(Bn,"CODE",{});var of=t(rl);li=h(of,"audio"),of.forEach(a),ri=h(Bn," and "),pl=u(Bn,"CODE",{});var cf=t(pl);pi=h(cf,"text"),cf.forEach(a),oi=h(Bn," columns:"),Bn.forEach(a),ep=b(s),q(Me.$$.fragment,s),np=b(s),ya=u(s,"P",{});var Vp=t(ya);ci=h(Vp,"Remember from the earlier section on processing audio data, you should always "),qn=u(Vp,"A",{href:!0});var uf=t(qn);ui=h(uf,"resample"),uf.forEach(a),ii=h(Vp," your audio data\u2019s sampling rate to match the sampling rate of the dataset used to pretrain a model:"),Vp.forEach(a),tp=b(s),q(He.$$.fragment,s),lp=b(s),Ks=u(s,"H3",{class:!0});var Wp=t(Ks);qa=u(Wp,"A",{id:!0,class:!0,href:!0});var hf=t(qa);ol=u(hf,"SPAN",{});var mf=t(ol);q(Fe.$$.fragment,mf),mf.forEach(a),hf.forEach(a),hi=b(Wp),cl=u(Wp,"SPAN",{});var ff=t(cl);mi=h(ff,"Processor"),ff.forEach(a),Wp.forEach(a),rp=b(s),zn=u(s,"P",{});var df=t(zn);fi=h(df,"A processor combines a feature extractor and tokenizer. Load a processor with [`AutoProcessor.from_pretrained]:"),df.forEach(a),pp=b(s),q(Je.$$.fragment,s),op=b(s),An=u(s,"OL",{});var bf=t(An);Ys=u(bf,"LI",{});var Vn=t(Ys);di=h(Vn,"Create a function to process the audio data to "),ul=u(Vn,"CODE",{});var jf=t(ul);bi=h(jf,"input_values"),jf.forEach(a),ji=h(Vn,", and tokenizes the text to "),il=u(Vn,"CODE",{});var gf=t(il);gi=h(gf,"labels"),gf.forEach(a),_i=h(Vn,". These are your inputs to the model:"),Vn.forEach(a),bf.forEach(a),cp=b(s),q(Re.$$.fragment,s),up=b(s),Be=u(s,"OL",{start:!0});var _f=t(Be);Ve=u(_f,"LI",{});var Up=t(Ve);vi=h(Up,"Apply the "),hl=u(Up,"CODE",{});var vf=t(hl);$i=h(vf,"prepare_dataset"),vf.forEach(a),wi=h(Up," function to a sample:"),Up.forEach(a),_f.forEach(a),ip=b(s),q(We.$$.fragment,s),hp=b(s),zs=u(s,"P",{});var Wn=t(zs);ki=h(Wn,"Notice the processor has added "),ml=u(Wn,"CODE",{});var $f=t(ml);Ei=h($f,"input_values"),$f.forEach(a),xi=h(Wn," and "),fl=u(Wn,"CODE",{});var wf=t(fl);yi=h(wf,"labels"),wf.forEach(a),qi=h(Wn,". The sampling rate has also been correctly downsampled to 16kHz."),Wn.forEach(a),mp=b(s),Pn=u(s,"P",{});var kf=t(Pn);zi=h(kf,"Awesome, you should now be able to preprocess data for any modality and even combine different modalities! In the next tutorial, learn how to fine-tune a model on your newly preprocessed data."),kf.forEach(a),this.h()},h(){o(n,"name","hf:doc:metadata"),o(n,"content",JSON.stringify(yd)),o(l,"id","preprocess"),o(l,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),o(l,"href","#preprocess"),o(r,"class","relative group"),o(X,"id","nlp"),o(X,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),o(X,"href","#nlp"),o(Z,"class","relative group"),o(K,"href","main_classes/tokenizer"),o(sa,"id","tokenizar"),o(sa,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),o(sa,"href","#tokenizar"),o(Ls,"class","relative group"),o(Ge,"href","glossary#input-ids"),o(Xe,"href","glossary#attention-mask"),o(an,"href","glossary#token-type-ids"),o(na,"id","margens"),o(na,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),o(na,"href","#margens"),o(Ts,"class","relative group"),o(la,"id","truncar"),o(la,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),o(la,"href","#truncar"),o(Is,"class","relative group"),o(ra,"id","construindo-tensores"),o(ra,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),o(ra,"href","#construindo-tensores"),o(Os,"class","relative group"),o(oa,"id","audio"),o(oa,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),o(oa,"href","#audio"),o(Ms,"class","relative group"),o(rn,"href","main_classes/feature_extractor"),o(Va,"href","https://huggingface.co/datasets/PolyAI/minds14"),o(Va,"rel","nofollow"),o(Wa,"href","https://huggingface.co/docs/datasets/load_hub.html"),o(Wa,"rel","nofollow"),o(ua,"id","resample"),o(ua,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),o(ua,"href","#resample"),o(Hs,"class","relative group"),o(Za,"href","https://huggingface.co/facebook/wav2vec2-base"),o(Za,"rel","nofollow"),o(Ga,"href","https://huggingface.co/datasets/PolyAI/minds14"),o(Ga,"rel","nofollow"),o(se,"href","https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Dataset.cast_column"),o(se,"rel","nofollow"),o(ee,"start","2"),o(fa,"id","feature-extractor"),o(fa,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),o(fa,"href","#feature-extractor"),o(Fs,"class","relative group"),o(ba,"id","pad-and-truncate"),o(ba,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),o(ba,"href","#pad-and-truncate"),o(Js,"class","relative group"),o(ja,"id","vision"),o(ja,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),o(ja,"href","#vision"),o(Rs,"class","relative group"),o(me,"href","https://huggingface.co/datasets/food101"),o(me,"rel","nofollow"),o(de,"href","https://huggingface.co/docs/datasets/package_reference/main_classes.html?highlight=image#datasets.Image"),o(de,"rel","nofollow"),Ef(vn.src,Oi="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/vision-preprocess-tutorial.png")||o(vn,"src",Oi),o(vn,"alt","vision-preprocess-tutorial.png"),o(_a,"id","feature-extractor"),o(_a,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),o(_a,"href","#feature-extractor"),o(Bs,"class","relative group"),o($a,"id","data-augmentation"),o($a,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),o($a,"href","#data-augmentation"),o(Vs,"class","relative group"),o(ve,"href","https://pytorch.org/vision/stable/transforms.html"),o(ve,"rel","nofollow"),o($e,"href","https://pytorch.org/vision/master/generated/torchvision.transforms.Compose.html"),o($e,"rel","nofollow"),o(we,"href","https://pytorch.org/vision/main/generated/torchvision.transforms.RandomResizedCrop.html"),o(we,"rel","nofollow"),o(ke,"href","https://pytorch.org/vision/main/generated/torchvision.transforms.ColorJitter.html"),o(ke,"rel","nofollow"),o(wn,"href","model_doc/visionencoderdecoder#transformers.VisionEncoderDecoderModel.forward.pixel_values"),o(xe,"start","2"),o(Ae,"href","https://huggingface.co/docs/datasets/process.html#format-transform"),o(Ae,"rel","nofollow"),o(qe,"start","3"),o(Ce,"start","4"),Ef(xn.src,Mi="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/preprocessed_image.png")||o(xn,"src",Mi),o(xn,"alt","preprocessed_image"),o(ka,"id","multimodal"),o(ka,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),o(ka,"href","#multimodal"),o(Us,"class","relative group"),o(Te,"href","https://huggingface.co/datasets/lj_speech"),o(Te,"rel","nofollow"),o(qn,"href","preprocessing#audio"),o(qa,"id","processor"),o(qa,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),o(qa,"href","#processor"),o(Ks,"class","relative group"),o(Be,"start","2")},m(s,p){e(document.head,n),m(s,f,p),m(s,r,p),e(r,l),e(l,j),x(_,j,null),e(r,k),e(r,w),e(w,C),m(s,L,p),x(P,s,p),m(s,I,p),m(s,T,p),e(T,V),m(s,W,p),m(s,F,p),e(F,D),e(D,O),e(F,J),e(F,U),e(U,N),e(F,H),e(F,R),e(R,M),m(s,ns,p),m(s,Z,p),e(Z,X),e(X,us),x(ts,us,null),e(Z,Ps),e(Z,is),e(is,hs),m(s,G,p),x(ls,s,p),m(s,fs,p),m(s,ss,p),e(ss,Cs),e(ss,K),e(K,Ds),e(ss,Ns),m(s,ds,p),x(Y,s,p),m(s,B,p),m(s,rs,p),e(rs,ps),e(rs,bs),e(bs,Ca),e(rs,Da),e(rs,as),e(as,Ss),e(rs,Xs),m(s,vl,p),m(s,Ls,p),e(Ls,sa),e(sa,Yn),x(Na,Yn,null),e(Ls,Yp),e(Ls,Zn),e(Zn,Zp),m(s,$l,p),m(s,aa,p),e(aa,Gp),e(aa,Gn),e(Gn,Qp),e(aa,Xp),m(s,wl,p),x(Sa,s,p),m(s,kl,p),m(s,Ke,p),e(Ke,so),m(s,El,p),x(La,s,p),m(s,xl,p),m(s,Ye,p),e(Ye,ao),m(s,yl,p),m(s,js,p),e(js,Ze),e(Ze,Ge),e(Ge,eo),e(Ze,no),e(js,to),e(js,Qe),e(Qe,Xe),e(Xe,lo),e(Qe,ro),e(js,po),e(js,sn),e(sn,an),e(an,oo),e(sn,co),m(s,ql,p),m(s,ea,p),e(ea,uo),e(ea,Qn),e(Qn,io),e(ea,ho),m(s,zl,p),x(Ta,s,p),m(s,Al,p),m(s,gs,p),e(gs,mo),e(gs,Xn),e(Xn,fo),e(gs,bo),e(gs,st),e(st,jo),e(gs,go),m(s,Pl,p),m(s,en,p),e(en,_o),m(s,Cl,p),x(Ia,s,p),m(s,Dl,p),m(s,Ts,p),e(Ts,na),e(na,at),x(Oa,at,null),e(Ts,vo),e(Ts,et),e(et,$o),m(s,Nl,p),m(s,ta,p),e(ta,wo),e(ta,nt),e(nt,ko),e(ta,Eo),m(s,Sl,p),m(s,_s,p),e(_s,xo),e(_s,tt),e(tt,yo),e(_s,qo),e(_s,lt),e(lt,zo),e(_s,Ao),m(s,Ll,p),x(Ma,s,p),m(s,Tl,p),m(s,nn,p),e(nn,Po),m(s,Il,p),m(s,Is,p),e(Is,la),e(la,rt),x(Ha,rt,null),e(Is,Co),e(Is,pt),e(pt,Do),m(s,Ol,p),m(s,tn,p),e(tn,No),m(s,Ml,p),m(s,vs,p),e(vs,So),e(vs,ot),e(ot,Lo),e(vs,To),e(vs,ct),e(ct,Io),e(vs,Oo),m(s,Hl,p),x(Fa,s,p),m(s,Fl,p),m(s,Os,p),e(Os,ra),e(ra,ut),x(Ja,ut,null),e(Os,Mo),e(Os,it),e(it,Ho),m(s,Jl,p),m(s,ln,p),e(ln,Fo),m(s,Rl,p),m(s,os,p),e(os,Jo),e(os,ht),e(ht,Ro),e(os,Bo),e(os,mt),e(mt,Vo),e(os,Wo),e(os,ft),e(ft,Uo),e(os,Ko),m(s,Bl,p),x(pa,s,p),m(s,Vl,p),m(s,Ms,p),e(Ms,oa),e(oa,dt),x(Ra,dt,null),e(Ms,Yo),e(Ms,bt),e(bt,Zo),m(s,Wl,p),m(s,ca,p),e(ca,Go),e(ca,rn),e(rn,Qo),e(ca,Xo),m(s,Ul,p),x(Ba,s,p),m(s,Kl,p),m(s,$s,p),e($s,sc),e($s,Va),e(Va,ac),e($s,ec),e($s,Wa),e(Wa,nc),e($s,tc),m(s,Yl,p),x(Ua,s,p),m(s,Zl,p),m(s,ws,p),e(ws,lc),e(ws,jt),e(jt,rc),e(ws,pc),e(ws,gt),e(gt,oc),e(ws,cc),m(s,Gl,p),x(Ka,s,p),m(s,Ql,p),m(s,pn,p),e(pn,uc),m(s,Xl,p),m(s,ks,p),e(ks,on),e(on,_t),e(_t,ic),e(on,hc),e(ks,mc),e(ks,cn),e(cn,vt),e(vt,fc),e(cn,dc),e(ks,bc),e(ks,un),e(un,$t),e($t,jc),e(un,gc),m(s,sr,p),m(s,Hs,p),e(Hs,ua),e(ua,wt),x(Ya,wt,null),e(Hs,_c),e(Hs,kt),e(kt,vc),m(s,ar,p),m(s,ia,p),e(ia,$c),e(ia,Za),e(Za,wc),e(ia,kc),m(s,er,p),m(s,ha,p),e(ha,Ec),e(ha,Ga),e(Ga,xc),e(ha,yc),m(s,nr,p),x(Qa,s,p),m(s,tr,p),m(s,hn,p),e(hn,Xa),e(Xa,qc),e(Xa,se),e(se,Et),e(Et,zc),e(Xa,Ac),m(s,lr,p),x(ae,s,p),m(s,rr,p),m(s,ee,p),e(ee,xt),e(xt,Pc),m(s,pr,p),x(ne,s,p),m(s,or,p),m(s,ma,p),e(ma,Cc),e(ma,yt),e(yt,Dc),e(ma,Nc),m(s,cr,p),m(s,Fs,p),e(Fs,fa),e(fa,qt),x(te,qt,null),e(Fs,Sc),e(Fs,zt),e(zt,Lc),m(s,ur,p),m(s,cs,p),e(cs,Tc),e(cs,At),e(At,Ic),e(cs,Oc),e(cs,Pt),e(Pt,Mc),e(cs,Hc),e(cs,Ct),e(Ct,Fc),e(cs,Jc),m(s,ir,p),m(s,da,p),e(da,Rc),e(da,Dt),e(Dt,Bc),e(da,Vc),m(s,hr,p),x(le,s,p),m(s,mr,p),m(s,Es,p),e(Es,Wc),e(Es,Nt),e(Nt,Uc),e(Es,Kc),e(Es,St),e(St,Yc),e(Es,Zc),m(s,fr,p),x(re,s,p),m(s,dr,p),m(s,Js,p),e(Js,ba),e(ba,Lt),x(pe,Lt,null),e(Js,Gc),e(Js,Tt),e(Tt,Qc),m(s,br,p),m(s,mn,p),e(mn,Xc),m(s,jr,p),x(oe,s,p),m(s,gr,p),m(s,fn,p),e(fn,su),m(s,_r,p),x(ce,s,p),m(s,vr,p),m(s,dn,p),e(dn,au),m(s,$r,p),x(ue,s,p),m(s,wr,p),m(s,bn,p),e(bn,eu),m(s,kr,p),x(ie,s,p),m(s,Er,p),m(s,jn,p),e(jn,nu),m(s,xr,p),m(s,Rs,p),e(Rs,ja),e(ja,It),x(he,It,null),e(Rs,tu),e(Rs,Ot),e(Ot,lu),m(s,yr,p),m(s,gn,p),e(gn,ru),m(s,qr,p),m(s,xs,p),e(xs,pu),e(xs,me),e(me,ou),e(xs,cu),e(xs,Mt),e(Mt,uu),e(xs,iu),m(s,zr,p),x(fe,s,p),m(s,Ar,p),m(s,ga,p),e(ga,hu),e(ga,de),e(de,Ht),e(Ht,mu),e(ga,fu),m(s,Pr,p),x(be,s,p),m(s,Cr,p),m(s,_n,p),e(_n,vn),m(s,Dr,p),m(s,Bs,p),e(Bs,_a),e(_a,Ft),x(je,Ft,null),e(Bs,du),e(Bs,Jt),e(Jt,bu),m(s,Nr,p),m(s,va,p),e(va,ju),e(va,Rt),e(Rt,gu),e(va,_u),m(s,Sr,p),x(ge,s,p),m(s,Lr,p),m(s,Vs,p),e(Vs,$a),e($a,Bt),x(_e,Bt,null),e(Vs,vu),e(Vs,Vt),e(Vt,$u),m(s,Tr,p),m(s,wa,p),e(wa,wu),e(wa,ve),e(ve,Wt),e(Wt,ku),e(wa,Eu),m(s,Ir,p),m(s,$n,p),e($n,ms),e(ms,xu),e(ms,$e),e($e,Ut),e(Ut,yu),e(ms,qu),e(ms,we),e(we,Kt),e(Kt,zu),e(ms,Au),e(ms,ke),e(ke,Yt),e(Yt,Pu),e(ms,Cu),m(s,Or,p),x(Ee,s,p),m(s,Mr,p),m(s,xe,p),e(xe,Ws),e(Ws,Du),e(Ws,wn),e(wn,Zt),e(Zt,Nu),e(Ws,Su),e(Ws,Gt),e(Gt,Lu),e(Ws,Tu),m(s,Hr,p),x(ye,s,p),m(s,Fr,p),m(s,qe,p),e(qe,ze),e(ze,Iu),e(ze,Ae),e(Ae,Qt),e(Qt,Ou),e(ze,Mu),m(s,Jr,p),x(Pe,s,p),m(s,Rr,p),m(s,Ce,p),e(Ce,De),e(De,Hu),e(De,Xt),e(Xt,Fu),e(De,Ju),m(s,Br,p),x(Ne,s,p),m(s,Vr,p),m(s,kn,p),e(kn,Ru),m(s,Wr,p),x(Se,s,p),m(s,Ur,p),m(s,En,p),e(En,xn),m(s,Kr,p),m(s,Us,p),e(Us,ka),e(ka,sl),x(Le,sl,null),e(Us,Bu),e(Us,al),e(al,Vu),m(s,Yr,p),m(s,yn,p),e(yn,Wu),m(s,Zr,p),m(s,Ea,p),e(Ea,el),e(el,Uu),e(Ea,Ku),e(Ea,nl),e(nl,Yu),m(s,Gr,p),m(s,xa,p),e(xa,Zu),e(xa,Te),e(Te,Gu),e(xa,Qu),m(s,Qr,p),x(Ie,s,p),m(s,Xr,p),m(s,ys,p),e(ys,Xu),e(ys,tl),e(tl,si),e(ys,ai),e(ys,ll),e(ll,ei),e(ys,ni),m(s,sp,p),x(Oe,s,p),m(s,ap,p),m(s,qs,p),e(qs,ti),e(qs,rl),e(rl,li),e(qs,ri),e(qs,pl),e(pl,pi),e(qs,oi),m(s,ep,p),x(Me,s,p),m(s,np,p),m(s,ya,p),e(ya,ci),e(ya,qn),e(qn,ui),e(ya,ii),m(s,tp,p),x(He,s,p),m(s,lp,p),m(s,Ks,p),e(Ks,qa),e(qa,ol),x(Fe,ol,null),e(Ks,hi),e(Ks,cl),e(cl,mi),m(s,rp,p),m(s,zn,p),e(zn,fi),m(s,pp,p),x(Je,s,p),m(s,op,p),m(s,An,p),e(An,Ys),e(Ys,di),e(Ys,ul),e(ul,bi),e(Ys,ji),e(Ys,il),e(il,gi),e(Ys,_i),m(s,cp,p),x(Re,s,p),m(s,up,p),m(s,Be,p),e(Be,Ve),e(Ve,vi),e(Ve,hl),e(hl,$i),e(Ve,wi),m(s,ip,p),x(We,s,p),m(s,hp,p),m(s,zs,p),e(zs,ki),e(zs,ml),e(ml,Ei),e(zs,xi),e(zs,fl),e(fl,yi),e(zs,qi),m(s,mp,p),m(s,Pn,p),e(Pn,zi),fp=!0},p(s,[p]){const Ue={};p&2&&(Ue.$$scope={dirty:p,ctx:s}),Y.$set(Ue);const dl={};p&2&&(dl.$$scope={dirty:p,ctx:s}),pa.$set(dl)},i(s){fp||(v(_.$$.fragment,s),v(P.$$.fragment,s),v(ts.$$.fragment,s),v(ls.$$.fragment,s),v(Y.$$.fragment,s),v(Na.$$.fragment,s),v(Sa.$$.fragment,s),v(La.$$.fragment,s),v(Ta.$$.fragment,s),v(Ia.$$.fragment,s),v(Oa.$$.fragment,s),v(Ma.$$.fragment,s),v(Ha.$$.fragment,s),v(Fa.$$.fragment,s),v(Ja.$$.fragment,s),v(pa.$$.fragment,s),v(Ra.$$.fragment,s),v(Ba.$$.fragment,s),v(Ua.$$.fragment,s),v(Ka.$$.fragment,s),v(Ya.$$.fragment,s),v(Qa.$$.fragment,s),v(ae.$$.fragment,s),v(ne.$$.fragment,s),v(te.$$.fragment,s),v(le.$$.fragment,s),v(re.$$.fragment,s),v(pe.$$.fragment,s),v(oe.$$.fragment,s),v(ce.$$.fragment,s),v(ue.$$.fragment,s),v(ie.$$.fragment,s),v(he.$$.fragment,s),v(fe.$$.fragment,s),v(be.$$.fragment,s),v(je.$$.fragment,s),v(ge.$$.fragment,s),v(_e.$$.fragment,s),v(Ee.$$.fragment,s),v(ye.$$.fragment,s),v(Pe.$$.fragment,s),v(Ne.$$.fragment,s),v(Se.$$.fragment,s),v(Le.$$.fragment,s),v(Ie.$$.fragment,s),v(Oe.$$.fragment,s),v(Me.$$.fragment,s),v(He.$$.fragment,s),v(Fe.$$.fragment,s),v(Je.$$.fragment,s),v(Re.$$.fragment,s),v(We.$$.fragment,s),fp=!0)},o(s){$(_.$$.fragment,s),$(P.$$.fragment,s),$(ts.$$.fragment,s),$(ls.$$.fragment,s),$(Y.$$.fragment,s),$(Na.$$.fragment,s),$(Sa.$$.fragment,s),$(La.$$.fragment,s),$(Ta.$$.fragment,s),$(Ia.$$.fragment,s),$(Oa.$$.fragment,s),$(Ma.$$.fragment,s),$(Ha.$$.fragment,s),$(Fa.$$.fragment,s),$(Ja.$$.fragment,s),$(pa.$$.fragment,s),$(Ra.$$.fragment,s),$(Ba.$$.fragment,s),$(Ua.$$.fragment,s),$(Ka.$$.fragment,s),$(Ya.$$.fragment,s),$(Qa.$$.fragment,s),$(ae.$$.fragment,s),$(ne.$$.fragment,s),$(te.$$.fragment,s),$(le.$$.fragment,s),$(re.$$.fragment,s),$(pe.$$.fragment,s),$(oe.$$.fragment,s),$(ce.$$.fragment,s),$(ue.$$.fragment,s),$(ie.$$.fragment,s),$(he.$$.fragment,s),$(fe.$$.fragment,s),$(be.$$.fragment,s),$(je.$$.fragment,s),$(ge.$$.fragment,s),$(_e.$$.fragment,s),$(Ee.$$.fragment,s),$(ye.$$.fragment,s),$(Pe.$$.fragment,s),$(Ne.$$.fragment,s),$(Se.$$.fragment,s),$(Le.$$.fragment,s),$(Ie.$$.fragment,s),$(Oe.$$.fragment,s),$(Me.$$.fragment,s),$(He.$$.fragment,s),$(Fe.$$.fragment,s),$(Je.$$.fragment,s),$(Re.$$.fragment,s),$(We.$$.fragment,s),fp=!1},d(s){a(n),s&&a(f),s&&a(r),y(_),s&&a(L),y(P,s),s&&a(I),s&&a(T),s&&a(W),s&&a(F),s&&a(ns),s&&a(Z),y(ts),s&&a(G),y(ls,s),s&&a(fs),s&&a(ss),s&&a(ds),y(Y,s),s&&a(B),s&&a(rs),s&&a(vl),s&&a(Ls),y(Na),s&&a($l),s&&a(aa),s&&a(wl),y(Sa,s),s&&a(kl),s&&a(Ke),s&&a(El),y(La,s),s&&a(xl),s&&a(Ye),s&&a(yl),s&&a(js),s&&a(ql),s&&a(ea),s&&a(zl),y(Ta,s),s&&a(Al),s&&a(gs),s&&a(Pl),s&&a(en),s&&a(Cl),y(Ia,s),s&&a(Dl),s&&a(Ts),y(Oa),s&&a(Nl),s&&a(ta),s&&a(Sl),s&&a(_s),s&&a(Ll),y(Ma,s),s&&a(Tl),s&&a(nn),s&&a(Il),s&&a(Is),y(Ha),s&&a(Ol),s&&a(tn),s&&a(Ml),s&&a(vs),s&&a(Hl),y(Fa,s),s&&a(Fl),s&&a(Os),y(Ja),s&&a(Jl),s&&a(ln),s&&a(Rl),s&&a(os),s&&a(Bl),y(pa,s),s&&a(Vl),s&&a(Ms),y(Ra),s&&a(Wl),s&&a(ca),s&&a(Ul),y(Ba,s),s&&a(Kl),s&&a($s),s&&a(Yl),y(Ua,s),s&&a(Zl),s&&a(ws),s&&a(Gl),y(Ka,s),s&&a(Ql),s&&a(pn),s&&a(Xl),s&&a(ks),s&&a(sr),s&&a(Hs),y(Ya),s&&a(ar),s&&a(ia),s&&a(er),s&&a(ha),s&&a(nr),y(Qa,s),s&&a(tr),s&&a(hn),s&&a(lr),y(ae,s),s&&a(rr),s&&a(ee),s&&a(pr),y(ne,s),s&&a(or),s&&a(ma),s&&a(cr),s&&a(Fs),y(te),s&&a(ur),s&&a(cs),s&&a(ir),s&&a(da),s&&a(hr),y(le,s),s&&a(mr),s&&a(Es),s&&a(fr),y(re,s),s&&a(dr),s&&a(Js),y(pe),s&&a(br),s&&a(mn),s&&a(jr),y(oe,s),s&&a(gr),s&&a(fn),s&&a(_r),y(ce,s),s&&a(vr),s&&a(dn),s&&a($r),y(ue,s),s&&a(wr),s&&a(bn),s&&a(kr),y(ie,s),s&&a(Er),s&&a(jn),s&&a(xr),s&&a(Rs),y(he),s&&a(yr),s&&a(gn),s&&a(qr),s&&a(xs),s&&a(zr),y(fe,s),s&&a(Ar),s&&a(ga),s&&a(Pr),y(be,s),s&&a(Cr),s&&a(_n),s&&a(Dr),s&&a(Bs),y(je),s&&a(Nr),s&&a(va),s&&a(Sr),y(ge,s),s&&a(Lr),s&&a(Vs),y(_e),s&&a(Tr),s&&a(wa),s&&a(Ir),s&&a($n),s&&a(Or),y(Ee,s),s&&a(Mr),s&&a(xe),s&&a(Hr),y(ye,s),s&&a(Fr),s&&a(qe),s&&a(Jr),y(Pe,s),s&&a(Rr),s&&a(Ce),s&&a(Br),y(Ne,s),s&&a(Vr),s&&a(kn),s&&a(Wr),y(Se,s),s&&a(Ur),s&&a(En),s&&a(Kr),s&&a(Us),y(Le),s&&a(Yr),s&&a(yn),s&&a(Zr),s&&a(Ea),s&&a(Gr),s&&a(xa),s&&a(Qr),y(Ie,s),s&&a(Xr),s&&a(ys),s&&a(sp),y(Oe,s),s&&a(ap),s&&a(qs),s&&a(ep),y(Me,s),s&&a(np),s&&a(ya),s&&a(tp),y(He,s),s&&a(lp),s&&a(Ks),y(Fe),s&&a(rp),s&&a(zn),s&&a(pp),y(Je,s),s&&a(op),s&&a(An),s&&a(cp),y(Re,s),s&&a(up),s&&a(Be),s&&a(ip),y(We,s),s&&a(hp),s&&a(zs),s&&a(mp),s&&a(Pn)}}}const yd={local:"preprocess",sections:[{local:"nlp",sections:[{local:"tokenizar",title:"Tokenizar"},{local:"margens",title:"Margens"},{local:"truncar",title:"Truncar"},{local:"construindo-tensores",title:"Construindo tensores"}],title:"NLP"},{local:"audio",sections:[{local:"resample",title:"Resample"},{local:"feature-extractor",title:"Feature extractor"},{local:"pad-and-truncate",title:"Pad and truncate"}],title:"Audio"},{local:"vision",sections:[{local:"feature-extractor",title:"Feature extractor"},{local:"data-augmentation",title:"Data augmentation"}],title:"Vision"},{local:"multimodal",sections:[{local:"processor",title:"Processor"}],title:"Multimodal"}],title:"Preprocess"};function qd(g){return Df(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Nd extends Zs{constructor(n){super();Gs(this,n,qd,xd,Qs,{})}}export{Nd as default,yd as metadata};
