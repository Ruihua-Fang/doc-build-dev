import{S as Po,i as Io,s as qo,e as n,k as m,w,t as i,M as zo,c as s,d as a,m as u,a as r,x as $,h as l,b as c,N as Lo,F as t,g as p,y as V,q as y,o as x,B as A,v as Do,L as Ba}from"../../chunks/vendor-6b77c823.js";import{T as Fo}from"../../chunks/Tip-39098574.js";import{D as nt}from"../../chunks/Docstring-1088f2fb.js";import{C as Ja}from"../../chunks/CodeBlock-3a8b25a8.js";import{I as st}from"../../chunks/IconCopyLink-7a11ce68.js";import{E as Za}from"../../chunks/ExampleCodeBlock-5212b321.js";function So(T){let d,b,_,h,v;return h=new Ja({props:{code:`from transformers import VanModel, VanConfig

# Initializing a VAN van-base style configuration
configuration = VanConfig()
# Initializing a model from the van-base style configuration
model = VanModel(configuration)
# Accessing the model configuration
configuration = model.config`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> VanModel, VanConfig

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a VAN van-base style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = VanConfig()
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a model from the van-base style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = VanModel(configuration)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`}}),{c(){d=n("p"),b=i("Example:"),_=m(),w(h.$$.fragment)},l(o){d=s(o,"P",{});var g=r(d);b=l(g,"Example:"),g.forEach(a),_=u(o),$(h.$$.fragment,o)},m(o,g){p(o,d,g),t(d,b),p(o,_,g),V(h,o,g),v=!0},p:Ba,i(o){v||(y(h.$$.fragment,o),v=!0)},o(o){x(h.$$.fragment,o),v=!1},d(o){o&&a(d),o&&a(_),A(h,o)}}}function Oo(T){let d,b,_,h,v;return{c(){d=n("p"),b=i("Although the recipe for forward pass needs to be defined within this function, one should call the "),_=n("code"),h=i("Module"),v=i(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(o){d=s(o,"P",{});var g=r(d);b=l(g,"Although the recipe for forward pass needs to be defined within this function, one should call the "),_=s(g,"CODE",{});var M=r(_);h=l(M,"Module"),M.forEach(a),v=l(g,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),g.forEach(a)},m(o,g){p(o,d,g),t(d,b),t(d,_),t(_,h),t(d,v)},d(o){o&&a(d)}}}function Ho(T){let d,b,_,h,v;return h=new Ja({props:{code:`from transformers import AutoFeatureExtractor, VanModel
import torch
from datasets import load_dataset

dataset = load_dataset("huggingface/cats-image")
image = dataset["test"]["image"][0]

feature_extractor = AutoFeatureExtractor.from_pretrained("Visual-Attention-Network/van-base")
model = VanModel.from_pretrained("Visual-Attention-Network/van-base")

inputs = feature_extractor(image, return_tensors="pt")

with torch.no_grad():
    outputs = model(**inputs)

last_hidden_states = outputs.last_hidden_state
list(last_hidden_states.shape)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoFeatureExtractor, VanModel
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;huggingface/cats-image&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>image = dataset[<span class="hljs-string">&quot;test&quot;</span>][<span class="hljs-string">&quot;image&quot;</span>][<span class="hljs-number">0</span>]

<span class="hljs-meta">&gt;&gt;&gt; </span>feature_extractor = AutoFeatureExtractor.from_pretrained(<span class="hljs-string">&quot;Visual-Attention-Network/van-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = VanModel.from_pretrained(<span class="hljs-string">&quot;Visual-Attention-Network/van-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = feature_extractor(image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">list</span>(last_hidden_states.shape)
[<span class="hljs-number">1</span>, <span class="hljs-number">512</span>, <span class="hljs-number">7</span>, <span class="hljs-number">7</span>]`}}),{c(){d=n("p"),b=i("Example:"),_=m(),w(h.$$.fragment)},l(o){d=s(o,"P",{});var g=r(d);b=l(g,"Example:"),g.forEach(a),_=u(o),$(h.$$.fragment,o)},m(o,g){p(o,d,g),t(d,b),p(o,_,g),V(h,o,g),v=!0},p:Ba,i(o){v||(y(h.$$.fragment,o),v=!0)},o(o){x(h.$$.fragment,o),v=!1},d(o){o&&a(d),o&&a(_),A(h,o)}}}function Wo(T){let d,b,_,h,v;return{c(){d=n("p"),b=i("Although the recipe for forward pass needs to be defined within this function, one should call the "),_=n("code"),h=i("Module"),v=i(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(o){d=s(o,"P",{});var g=r(d);b=l(g,"Although the recipe for forward pass needs to be defined within this function, one should call the "),_=s(g,"CODE",{});var M=r(_);h=l(M,"Module"),M.forEach(a),v=l(g,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),g.forEach(a)},m(o,g){p(o,d,g),t(d,b),t(d,_),t(_,h),t(d,v)},d(o){o&&a(d)}}}function Uo(T){let d,b,_,h,v;return h=new Ja({props:{code:`from transformers import AutoFeatureExtractor, VanForImageClassification
import torch
from datasets import load_dataset

dataset = load_dataset("huggingface/cats-image")
image = dataset["test"]["image"][0]

feature_extractor = AutoFeatureExtractor.from_pretrained("Visual-Attention-Network/van-base")
model = VanForImageClassification.from_pretrained("Visual-Attention-Network/van-base")

inputs = feature_extractor(image, return_tensors="pt")

with torch.no_grad():
    logits = model(**inputs).logits

# model predicts one of the 1000 ImageNet classes
predicted_label = logits.argmax(-1).item()
print(model.config.id2label[predicted_label])`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoFeatureExtractor, VanForImageClassification
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;huggingface/cats-image&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>image = dataset[<span class="hljs-string">&quot;test&quot;</span>][<span class="hljs-string">&quot;image&quot;</span>][<span class="hljs-number">0</span>]

<span class="hljs-meta">&gt;&gt;&gt; </span>feature_extractor = AutoFeatureExtractor.from_pretrained(<span class="hljs-string">&quot;Visual-Attention-Network/van-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = VanForImageClassification.from_pretrained(<span class="hljs-string">&quot;Visual-Attention-Network/van-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = feature_extractor(image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    logits = model(**inputs).logits

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># model predicts one of the 1000 ImageNet classes</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>predicted_label = logits.argmax(-<span class="hljs-number">1</span>).item()
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(model.config.id2label[predicted_label])
tabby, tabby cat`}}),{c(){d=n("p"),b=i("Example:"),_=m(),w(h.$$.fragment)},l(o){d=s(o,"P",{});var g=r(d);b=l(g,"Example:"),g.forEach(a),_=u(o),$(h.$$.fragment,o)},m(o,g){p(o,d,g),t(d,b),p(o,_,g),V(h,o,g),v=!0},p:Ba,i(o){v||(y(h.$$.fragment,o),v=!0)},o(o){x(h.$$.fragment,o),v=!1},d(o){o&&a(d),o&&a(_),A(h,o)}}}function Ro(T){let d,b,_,h,v,o,g,M,Nt,rt,P,H,We,oe,Ft,Ue,Pt,it,W,It,ne,qt,zt,lt,Ce,Lt,ct,Te,Dt,dt,ke,se,St,re,Ot,Ht,ft,je,Wt,pt,Me,ie,Ut,Re,Rt,Gt,ht,U,Kt,le,Zt,Bt,mt,ce,Qa,ut,F,Jt,de,Qt,Xt,fe,Yt,ea,gt,I,R,Ge,pe,ta,Ke,aa,_t,E,he,oa,q,na,Ne,sa,ra,me,ia,la,ca,z,da,Fe,fa,pa,Pe,ha,ma,ua,G,vt,L,K,Ze,ue,ga,Be,_a,bt,N,ge,va,_e,ba,ve,wa,$a,Va,k,be,ya,D,xa,Ie,Aa,Ea,Je,Ca,Ta,ka,Z,ja,B,wt,S,J,Qe,we,Ma,Xe,Na,$t,C,$e,Fa,Ye,Pa,Ia,Ve,qa,ye,za,La,Da,j,xe,Sa,O,Oa,qe,Ha,Wa,et,Ua,Ra,Ga,Q,Ka,X,Vt;return o=new st({}),oe=new st({}),pe=new st({}),he=new nt({props:{name:"class transformers.VanConfig",anchor:"transformers.VanConfig",parameters:[{name:"image_size",val:" = 224"},{name:"num_channels",val:" = 3"},{name:"patch_sizes",val:" = [7, 3, 3, 3]"},{name:"strides",val:" = [4, 2, 2, 2]"},{name:"hidden_sizes",val:" = [64, 128, 320, 512]"},{name:"depths",val:" = [3, 3, 12, 3]"},{name:"mlp_ratios",val:" = [8, 8, 4, 4]"},{name:"hidden_act",val:" = 'gelu'"},{name:"initializer_range",val:" = 0.02"},{name:"layer_norm_eps",val:" = 1e-06"},{name:"layer_scale_init_value",val:" = 0.01"},{name:"drop_path_rate",val:" = 0.0"},{name:"dropout_rate",val:" = 0.0"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.VanConfig.image_size",description:`<strong>image_size</strong> (<code>int</code>, <em>optional</em>, defaults to 224) &#x2014;
The size (resolution) of each image.`,name:"image_size"},{anchor:"transformers.VanConfig.num_channels",description:`<strong>num_channels</strong> (<code>int</code>, <em>optional</em>, defaults to 3) &#x2014;
The number of input channels.`,name:"num_channels"},{anchor:"transformers.VanConfig.patch_sizes",description:`<strong>patch_sizes</strong> (<code>List[int]</code>, <em>optional</em>, defaults to <code>[7, 3, 3, 3]</code>) &#x2014;
Patch size to use in each stage&#x2019;s embedding layer.`,name:"patch_sizes"},{anchor:"transformers.VanConfig.strides",description:`<strong>strides</strong> (<code>List[int]</code>, <em>optional</em>, defaults to <code>[4, 2, 2, 2]</code>) &#x2014;
Stride size to use in each stage&#x2019;s embedding layer to downsample the input.`,name:"strides"},{anchor:"transformers.VanConfig.hidden_sizes",description:`<strong>hidden_sizes</strong> (<code>List[int]</code>, <em>optional</em>, defaults to <code>[64, 128, 320, 512]</code>) &#x2014;
Dimensionality (hidden size) at each stage.`,name:"hidden_sizes"},{anchor:"transformers.VanConfig.depths",description:`<strong>depths</strong> (<code>List[int]</code>, <em>optional</em>, defaults to <code>[3, 3, 12, 3]</code>) &#x2014;
Depth (number of layers) for each stage.`,name:"depths"},{anchor:"transformers.VanConfig.mlp_ratios",description:`<strong>mlp_ratios</strong> (<code>List[int]</code>, <em>optional</em>, defaults to <code>[8, 8, 4, 4]</code>) &#x2014;
The expansion ratio for mlp layer at each stage.`,name:"mlp_ratios"},{anchor:"transformers.VanConfig.hidden_act",description:`<strong>hidden_act</strong> (<code>str</code> or <code>function</code>, <em>optional</em>, defaults to <code>&quot;gelu&quot;</code>) &#x2014;
The non-linear activation function (function or string) in each layer. If string, <code>&quot;gelu&quot;</code>, <code>&quot;relu&quot;</code>,
<code>&quot;selu&quot;</code> and <code>&quot;gelu_new&quot;</code> are supported.`,name:"hidden_act"},{anchor:"transformers.VanConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.VanConfig.layer_norm_eps",description:`<strong>layer_norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-12) &#x2014;
The epsilon used by the layer normalization layers.`,name:"layer_norm_eps"},{anchor:"transformers.VanConfig.layer_scale_init_value",description:`<strong>layer_scale_init_value</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-2) &#x2014;
The initial value for layer scaling.`,name:"layer_scale_init_value"},{anchor:"transformers.VanConfig.drop_path_rate",description:`<strong>drop_path_rate</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout probability for stochastic depth.`,name:"drop_path_rate"},{anchor:"transformers.VanConfig.dropout_rate",description:`<strong>dropout_rate</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout probability for dropout.`,name:"dropout_rate"}],source:"https://github.com/huggingface/transformers/blob/pr_16925/src/transformers/models/van/configuration_van.py#L28"}}),G=new Za({props:{anchor:"transformers.VanConfig.example",$$slots:{default:[So]},$$scope:{ctx:T}}}),ue=new st({}),ge=new nt({props:{name:"class transformers.VanModel",anchor:"transformers.VanModel",parameters:[{name:"config",val:""}],parametersDescription:[{anchor:"transformers.VanModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_16925/en/model_doc/van#transformers.VanConfig">VanConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_16925/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/pr_16925/src/transformers/models/van/modeling_van.py#L455"}}),be=new nt({props:{name:"forward",anchor:"transformers.VanModel.forward",parameters:[{name:"pixel_values",val:""},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"}],parametersDescription:[{anchor:"transformers.VanModel.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values. Pixel values can be obtained using <a href="/docs/transformers/pr_16925/en/model_doc/auto#transformers.AutoFeatureExtractor">AutoFeatureExtractor</a>. See
<code>AutoFeatureExtractor.__call__()</code> for details.`,name:"pixel_values"},{anchor:"transformers.VanModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all stages. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.VanModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_16925/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/pr_16925/src/transformers/models/van/modeling_van.py#L465",returnDescription:`
<p>A <code>transformers.models.van.modeling_van.VanModelOutput</code> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/pr_16925/en/model_doc/van#transformers.VanConfig"
>VanConfig</a>) and inputs.</p>
<ul>
<li><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) \u2014 Last hidden states (final feature map) of the last stage of the model.</li>
<li><strong>pooler_output</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, config.hidden_sizes[-1])</code>) \u2014 Global average pooling of the last feature map followed by a layernorm.</li>
<li><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of each stage) of shape <code>(batch_size, num_channels, height, width)</code>. Hidden-states (also called feature maps) of the model at the output of each stage.</li>
</ul>
`,returnType:`
<p><code>transformers.models.van.modeling_van.VanModelOutput</code> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Z=new Fo({props:{$$slots:{default:[Oo]},$$scope:{ctx:T}}}),B=new Za({props:{anchor:"transformers.VanModel.forward.example",$$slots:{default:[Ho]},$$scope:{ctx:T}}}),we=new st({}),$e=new nt({props:{name:"class transformers.VanForImageClassification",anchor:"transformers.VanForImageClassification",parameters:[{name:"config",val:""}],parametersDescription:[{anchor:"transformers.VanForImageClassification.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_16925/en/model_doc/van#transformers.VanConfig">VanConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_16925/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/pr_16925/src/transformers/models/van/modeling_van.py#L506"}}),xe=new nt({props:{name:"forward",anchor:"transformers.VanForImageClassification.forward",parameters:[{name:"pixel_values",val:" = None"},{name:"labels",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"}],parametersDescription:[{anchor:"transformers.VanForImageClassification.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values. Pixel values can be obtained using <a href="/docs/transformers/pr_16925/en/model_doc/auto#transformers.AutoFeatureExtractor">AutoFeatureExtractor</a>. See
<code>AutoFeatureExtractor.__call__()</code> for details.`,name:"pixel_values"},{anchor:"transformers.VanForImageClassification.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all stages. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.VanForImageClassification.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_16925/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.VanForImageClassification.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for computing the image classification/regression loss. Indices should be in <code>[0, ..., config.num_labels - 1]</code>. If <code>config.num_labels == 1</code> a regression loss is computed (Mean-Square loss), If
<code>config.num_labels &gt; 1</code> a classification loss is computed (Cross-Entropy).`,name:"labels"}],source:"https://github.com/huggingface/transformers/blob/pr_16925/src/transformers/models/van/modeling_van.py#L518",returnDescription:`
<p>A <code>transformers.models.van.modeling_van.VanClassifierOutput</code> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/pr_16925/en/model_doc/van#transformers.VanConfig"
>VanConfig</a>) and inputs.</p>
<ul>
<li><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) \u2014 Classification (or regression if config.num_labels==1) loss.</li>
<li><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, config.num_labels)</code>) \u2014 Classification (or regression if config.num_labels==1) scores (before SoftMax).</li>
<li><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of each stage) of shape <code>(batch_size, num_channels, height, width)</code>. Hidden-states (also called feature maps) of the model at the output of each stage.</li>
</ul>
`,returnType:`
<p><code>transformers.models.van.modeling_van.VanClassifierOutput</code> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Q=new Fo({props:{$$slots:{default:[Wo]},$$scope:{ctx:T}}}),X=new Za({props:{anchor:"transformers.VanForImageClassification.forward.example",$$slots:{default:[Uo]},$$scope:{ctx:T}}}),{c(){d=n("meta"),b=m(),_=n("h1"),h=n("a"),v=n("span"),w(o.$$.fragment),g=m(),M=n("span"),Nt=i("VAN"),rt=m(),P=n("h2"),H=n("a"),We=n("span"),w(oe.$$.fragment),Ft=m(),Ue=n("span"),Pt=i("Overview"),it=m(),W=n("p"),It=i("The VAN model was proposed in "),ne=n("a"),qt=i("Visual Attention Network"),zt=i(" by Meng-Hao Guo, Cheng-Ze Lu, Zheng-Ning Liu, Ming-Ming Cheng, Shi-Min Hu."),lt=m(),Ce=n("p"),Lt=i("This paper introduces a new attention layer based on convolution operations able to capture both local and distant relationships. This is done by combining normal and large kernel convolution layers. The latter uses a dilated convolution to capture distant correlations."),ct=m(),Te=n("p"),Dt=i("The abstract from the paper is the following:"),dt=m(),ke=n("p"),se=n("em"),St=i("While originally designed for natural language processing tasks, the self-attention mechanism has recently taken various computer vision areas by storm. However, the 2D nature of images brings three challenges for applying self-attention in computer vision. (1) Treating images as 1D sequences neglects their 2D structures. (2) The quadratic complexity is too expensive for high-resolution images. (3) It only captures spatial adaptability but ignores channel adaptability. In this paper, we propose a novel large kernel attention (LKA) module to enable self-adaptive and long-range correlations in self-attention while avoiding the above issues. We further introduce a novel neural network based on LKA, namely Visual Attention Network (VAN). While extremely simple, VAN outperforms the state-of-the-art vision transformers and convolutional neural networks with a large margin in extensive experiments, including image classification, object detection, semantic segmentation, instance segmentation, etc. Code is available at "),re=n("a"),Ot=i("this https URL"),Ht=i("."),ft=m(),je=n("p"),Wt=i("Tips:"),pt=m(),Me=n("ul"),ie=n("li"),Ut=i("VAN does not have an embedding layer, thus the "),Re=n("code"),Rt=i("hidden_states"),Gt=i(" will have a length equal to the number of stages."),ht=m(),U=n("p"),Kt=i("The figure below illustrates the architecture of a Visual Aattention Layer. Taken from the "),le=n("a"),Zt=i("original paper"),Bt=i("."),mt=m(),ce=n("img"),ut=m(),F=n("p"),Jt=i("This model was contributed by "),de=n("a"),Qt=i("Francesco"),Xt=i(". The original code can be found "),fe=n("a"),Yt=i("here"),ea=i("."),gt=m(),I=n("h2"),R=n("a"),Ge=n("span"),w(pe.$$.fragment),ta=m(),Ke=n("span"),aa=i("VanConfig"),_t=m(),E=n("div"),w(he.$$.fragment),oa=m(),q=n("p"),na=i("This is the configuration class to store the configuration of a "),Ne=n("a"),sa=i("VanModel"),ra=i(`. It is used to instantiate a VAN model
according to the specified arguments, defining the model architecture. Instantiating a configuration with the
defaults will yield a similar configuration to that of the VAN `),me=n("a"),ia=i("van-base"),la=i(`
architecture.`),ca=m(),z=n("p"),da=i("Configuration objects inherit from "),Fe=n("a"),fa=i("PretrainedConfig"),pa=i(` and can be used to control the model outputs. Read the
documentation from `),Pe=n("a"),ha=i("PretrainedConfig"),ma=i(" for more information."),ua=m(),w(G.$$.fragment),vt=m(),L=n("h2"),K=n("a"),Ze=n("span"),w(ue.$$.fragment),ga=m(),Be=n("span"),_a=i("VanModel"),bt=m(),N=n("div"),w(ge.$$.fragment),va=m(),_e=n("p"),ba=i(`The bare VAN model outputting raw features without any specific head on top. Note, VAN does not have an embedding layer.
This model is a PyTorch `),ve=n("a"),wa=i("torch.nn.Module"),$a=i(` subclass. Use it
as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),Va=m(),k=n("div"),w(be.$$.fragment),ya=m(),D=n("p"),xa=i("The "),Ie=n("a"),Aa=i("VanModel"),Ea=i(" forward method, overrides the "),Je=n("code"),Ca=i("__call__"),Ta=i(" special method."),ka=m(),w(Z.$$.fragment),ja=m(),w(B.$$.fragment),wt=m(),S=n("h2"),J=n("a"),Qe=n("span"),w(we.$$.fragment),Ma=m(),Xe=n("span"),Na=i("VanForImageClassification"),$t=m(),C=n("div"),w($e.$$.fragment),Fa=m(),Ye=n("p"),Pa=i(`VAN Model with an image classification head on top (a linear layer on top of the pooled features), e.g. for
ImageNet.`),Ia=m(),Ve=n("p"),qa=i("This model is a PyTorch "),ye=n("a"),za=i("torch.nn.Module"),La=i(` subclass. Use it
as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),Da=m(),j=n("div"),w(xe.$$.fragment),Sa=m(),O=n("p"),Oa=i("The "),qe=n("a"),Ha=i("VanForImageClassification"),Wa=i(" forward method, overrides the "),et=n("code"),Ua=i("__call__"),Ra=i(" special method."),Ga=m(),w(Q.$$.fragment),Ka=m(),w(X.$$.fragment),this.h()},l(e){const f=zo('[data-svelte="svelte-1phssyn"]',document.head);d=s(f,"META",{name:!0,content:!0}),f.forEach(a),b=u(e),_=s(e,"H1",{class:!0});var Ae=r(_);h=s(Ae,"A",{id:!0,class:!0,href:!0});var tt=r(h);v=s(tt,"SPAN",{});var at=r(v);$(o.$$.fragment,at),at.forEach(a),tt.forEach(a),g=u(Ae),M=s(Ae,"SPAN",{});var ot=r(M);Nt=l(ot,"VAN"),ot.forEach(a),Ae.forEach(a),rt=u(e),P=s(e,"H2",{class:!0});var Ee=r(P);H=s(Ee,"A",{id:!0,class:!0,href:!0});var Xa=r(H);We=s(Xa,"SPAN",{});var Ya=r(We);$(oe.$$.fragment,Ya),Ya.forEach(a),Xa.forEach(a),Ft=u(Ee),Ue=s(Ee,"SPAN",{});var eo=r(Ue);Pt=l(eo,"Overview"),eo.forEach(a),Ee.forEach(a),it=u(e),W=s(e,"P",{});var yt=r(W);It=l(yt,"The VAN model was proposed in "),ne=s(yt,"A",{href:!0,rel:!0});var to=r(ne);qt=l(to,"Visual Attention Network"),to.forEach(a),zt=l(yt," by Meng-Hao Guo, Cheng-Ze Lu, Zheng-Ning Liu, Ming-Ming Cheng, Shi-Min Hu."),yt.forEach(a),lt=u(e),Ce=s(e,"P",{});var ao=r(Ce);Lt=l(ao,"This paper introduces a new attention layer based on convolution operations able to capture both local and distant relationships. This is done by combining normal and large kernel convolution layers. The latter uses a dilated convolution to capture distant correlations."),ao.forEach(a),ct=u(e),Te=s(e,"P",{});var oo=r(Te);Dt=l(oo,"The abstract from the paper is the following:"),oo.forEach(a),dt=u(e),ke=s(e,"P",{});var no=r(ke);se=s(no,"EM",{});var xt=r(se);St=l(xt,"While originally designed for natural language processing tasks, the self-attention mechanism has recently taken various computer vision areas by storm. However, the 2D nature of images brings three challenges for applying self-attention in computer vision. (1) Treating images as 1D sequences neglects their 2D structures. (2) The quadratic complexity is too expensive for high-resolution images. (3) It only captures spatial adaptability but ignores channel adaptability. In this paper, we propose a novel large kernel attention (LKA) module to enable self-adaptive and long-range correlations in self-attention while avoiding the above issues. We further introduce a novel neural network based on LKA, namely Visual Attention Network (VAN). While extremely simple, VAN outperforms the state-of-the-art vision transformers and convolutional neural networks with a large margin in extensive experiments, including image classification, object detection, semantic segmentation, instance segmentation, etc. Code is available at "),re=s(xt,"A",{href:!0,rel:!0});var so=r(re);Ot=l(so,"this https URL"),so.forEach(a),Ht=l(xt,"."),xt.forEach(a),no.forEach(a),ft=u(e),je=s(e,"P",{});var ro=r(je);Wt=l(ro,"Tips:"),ro.forEach(a),pt=u(e),Me=s(e,"UL",{});var io=r(Me);ie=s(io,"LI",{});var At=r(ie);Ut=l(At,"VAN does not have an embedding layer, thus the "),Re=s(At,"CODE",{});var lo=r(Re);Rt=l(lo,"hidden_states"),lo.forEach(a),Gt=l(At," will have a length equal to the number of stages."),At.forEach(a),io.forEach(a),ht=u(e),U=s(e,"P",{});var Et=r(U);Kt=l(Et,"The figure below illustrates the architecture of a Visual Aattention Layer. Taken from the "),le=s(Et,"A",{href:!0,rel:!0});var co=r(le);Zt=l(co,"original paper"),co.forEach(a),Bt=l(Et,"."),Et.forEach(a),mt=u(e),ce=s(e,"IMG",{width:!0,src:!0}),ut=u(e),F=s(e,"P",{});var ze=r(F);Jt=l(ze,"This model was contributed by "),de=s(ze,"A",{href:!0,rel:!0});var fo=r(de);Qt=l(fo,"Francesco"),fo.forEach(a),Xt=l(ze,". The original code can be found "),fe=s(ze,"A",{href:!0,rel:!0});var po=r(fe);Yt=l(po,"here"),po.forEach(a),ea=l(ze,"."),ze.forEach(a),gt=u(e),I=s(e,"H2",{class:!0});var Ct=r(I);R=s(Ct,"A",{id:!0,class:!0,href:!0});var ho=r(R);Ge=s(ho,"SPAN",{});var mo=r(Ge);$(pe.$$.fragment,mo),mo.forEach(a),ho.forEach(a),ta=u(Ct),Ke=s(Ct,"SPAN",{});var uo=r(Ke);aa=l(uo,"VanConfig"),uo.forEach(a),Ct.forEach(a),_t=u(e),E=s(e,"DIV",{class:!0});var Y=r(E);$(he.$$.fragment,Y),oa=u(Y),q=s(Y,"P",{});var Le=r(q);na=l(Le,"This is the configuration class to store the configuration of a "),Ne=s(Le,"A",{href:!0});var go=r(Ne);sa=l(go,"VanModel"),go.forEach(a),ra=l(Le,`. It is used to instantiate a VAN model
according to the specified arguments, defining the model architecture. Instantiating a configuration with the
defaults will yield a similar configuration to that of the VAN `),me=s(Le,"A",{href:!0,rel:!0});var _o=r(me);ia=l(_o,"van-base"),_o.forEach(a),la=l(Le,`
architecture.`),Le.forEach(a),ca=u(Y),z=s(Y,"P",{});var De=r(z);da=l(De,"Configuration objects inherit from "),Fe=s(De,"A",{href:!0});var vo=r(Fe);fa=l(vo,"PretrainedConfig"),vo.forEach(a),pa=l(De,` and can be used to control the model outputs. Read the
documentation from `),Pe=s(De,"A",{href:!0});var bo=r(Pe);ha=l(bo,"PretrainedConfig"),bo.forEach(a),ma=l(De," for more information."),De.forEach(a),ua=u(Y),$(G.$$.fragment,Y),Y.forEach(a),vt=u(e),L=s(e,"H2",{class:!0});var Tt=r(L);K=s(Tt,"A",{id:!0,class:!0,href:!0});var wo=r(K);Ze=s(wo,"SPAN",{});var $o=r(Ze);$(ue.$$.fragment,$o),$o.forEach(a),wo.forEach(a),ga=u(Tt),Be=s(Tt,"SPAN",{});var Vo=r(Be);_a=l(Vo,"VanModel"),Vo.forEach(a),Tt.forEach(a),bt=u(e),N=s(e,"DIV",{class:!0});var Se=r(N);$(ge.$$.fragment,Se),va=u(Se),_e=s(Se,"P",{});var kt=r(_e);ba=l(kt,`The bare VAN model outputting raw features without any specific head on top. Note, VAN does not have an embedding layer.
This model is a PyTorch `),ve=s(kt,"A",{href:!0,rel:!0});var yo=r(ve);wa=l(yo,"torch.nn.Module"),yo.forEach(a),$a=l(kt,` subclass. Use it
as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),kt.forEach(a),Va=u(Se),k=s(Se,"DIV",{class:!0});var ee=r(k);$(be.$$.fragment,ee),ya=u(ee),D=s(ee,"P",{});var Oe=r(D);xa=l(Oe,"The "),Ie=s(Oe,"A",{href:!0});var xo=r(Ie);Aa=l(xo,"VanModel"),xo.forEach(a),Ea=l(Oe," forward method, overrides the "),Je=s(Oe,"CODE",{});var Ao=r(Je);Ca=l(Ao,"__call__"),Ao.forEach(a),Ta=l(Oe," special method."),Oe.forEach(a),ka=u(ee),$(Z.$$.fragment,ee),ja=u(ee),$(B.$$.fragment,ee),ee.forEach(a),Se.forEach(a),wt=u(e),S=s(e,"H2",{class:!0});var jt=r(S);J=s(jt,"A",{id:!0,class:!0,href:!0});var Eo=r(J);Qe=s(Eo,"SPAN",{});var Co=r(Qe);$(we.$$.fragment,Co),Co.forEach(a),Eo.forEach(a),Ma=u(jt),Xe=s(jt,"SPAN",{});var To=r(Xe);Na=l(To,"VanForImageClassification"),To.forEach(a),jt.forEach(a),$t=u(e),C=s(e,"DIV",{class:!0});var te=r(C);$($e.$$.fragment,te),Fa=u(te),Ye=s(te,"P",{});var ko=r(Ye);Pa=l(ko,`VAN Model with an image classification head on top (a linear layer on top of the pooled features), e.g. for
ImageNet.`),ko.forEach(a),Ia=u(te),Ve=s(te,"P",{});var Mt=r(Ve);qa=l(Mt,"This model is a PyTorch "),ye=s(Mt,"A",{href:!0,rel:!0});var jo=r(ye);za=l(jo,"torch.nn.Module"),jo.forEach(a),La=l(Mt,` subclass. Use it
as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),Mt.forEach(a),Da=u(te),j=s(te,"DIV",{class:!0});var ae=r(j);$(xe.$$.fragment,ae),Sa=u(ae),O=s(ae,"P",{});var He=r(O);Oa=l(He,"The "),qe=s(He,"A",{href:!0});var Mo=r(qe);Ha=l(Mo,"VanForImageClassification"),Mo.forEach(a),Wa=l(He," forward method, overrides the "),et=s(He,"CODE",{});var No=r(et);Ua=l(No,"__call__"),No.forEach(a),Ra=l(He," special method."),He.forEach(a),Ga=u(ae),$(Q.$$.fragment,ae),Ka=u(ae),$(X.$$.fragment,ae),ae.forEach(a),te.forEach(a),this.h()},h(){c(d,"name","hf:doc:metadata"),c(d,"content",JSON.stringify(Go)),c(h,"id","van"),c(h,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(h,"href","#van"),c(_,"class","relative group"),c(H,"id","overview"),c(H,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(H,"href","#overview"),c(P,"class","relative group"),c(ne,"href","https://arxiv.org/abs/2202.09741"),c(ne,"rel","nofollow"),c(re,"href","https://github.com/Visual-Attention-Network/VAN-Classification"),c(re,"rel","nofollow"),c(le,"href","https://arxiv.org/abs/2202.09741"),c(le,"rel","nofollow"),c(ce,"width","600"),Lo(ce.src,Qa="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/van_architecture.png")||c(ce,"src",Qa),c(de,"href","https://huggingface.co/Francesco"),c(de,"rel","nofollow"),c(fe,"href","https://github.com/Visual-Attention-Network/VAN-Classification"),c(fe,"rel","nofollow"),c(R,"id","transformers.VanConfig"),c(R,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(R,"href","#transformers.VanConfig"),c(I,"class","relative group"),c(Ne,"href","/docs/transformers/pr_16925/en/model_doc/van#transformers.VanModel"),c(me,"href","https://huggingface.co/van-base"),c(me,"rel","nofollow"),c(Fe,"href","/docs/transformers/pr_16925/en/main_classes/configuration#transformers.PretrainedConfig"),c(Pe,"href","/docs/transformers/pr_16925/en/main_classes/configuration#transformers.PretrainedConfig"),c(E,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(K,"id","transformers.VanModel"),c(K,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(K,"href","#transformers.VanModel"),c(L,"class","relative group"),c(ve,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(ve,"rel","nofollow"),c(Ie,"href","/docs/transformers/pr_16925/en/model_doc/van#transformers.VanModel"),c(k,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(N,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(J,"id","transformers.VanForImageClassification"),c(J,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(J,"href","#transformers.VanForImageClassification"),c(S,"class","relative group"),c(ye,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(ye,"rel","nofollow"),c(qe,"href","/docs/transformers/pr_16925/en/model_doc/van#transformers.VanForImageClassification"),c(j,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(C,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(e,f){t(document.head,d),p(e,b,f),p(e,_,f),t(_,h),t(h,v),V(o,v,null),t(_,g),t(_,M),t(M,Nt),p(e,rt,f),p(e,P,f),t(P,H),t(H,We),V(oe,We,null),t(P,Ft),t(P,Ue),t(Ue,Pt),p(e,it,f),p(e,W,f),t(W,It),t(W,ne),t(ne,qt),t(W,zt),p(e,lt,f),p(e,Ce,f),t(Ce,Lt),p(e,ct,f),p(e,Te,f),t(Te,Dt),p(e,dt,f),p(e,ke,f),t(ke,se),t(se,St),t(se,re),t(re,Ot),t(se,Ht),p(e,ft,f),p(e,je,f),t(je,Wt),p(e,pt,f),p(e,Me,f),t(Me,ie),t(ie,Ut),t(ie,Re),t(Re,Rt),t(ie,Gt),p(e,ht,f),p(e,U,f),t(U,Kt),t(U,le),t(le,Zt),t(U,Bt),p(e,mt,f),p(e,ce,f),p(e,ut,f),p(e,F,f),t(F,Jt),t(F,de),t(de,Qt),t(F,Xt),t(F,fe),t(fe,Yt),t(F,ea),p(e,gt,f),p(e,I,f),t(I,R),t(R,Ge),V(pe,Ge,null),t(I,ta),t(I,Ke),t(Ke,aa),p(e,_t,f),p(e,E,f),V(he,E,null),t(E,oa),t(E,q),t(q,na),t(q,Ne),t(Ne,sa),t(q,ra),t(q,me),t(me,ia),t(q,la),t(E,ca),t(E,z),t(z,da),t(z,Fe),t(Fe,fa),t(z,pa),t(z,Pe),t(Pe,ha),t(z,ma),t(E,ua),V(G,E,null),p(e,vt,f),p(e,L,f),t(L,K),t(K,Ze),V(ue,Ze,null),t(L,ga),t(L,Be),t(Be,_a),p(e,bt,f),p(e,N,f),V(ge,N,null),t(N,va),t(N,_e),t(_e,ba),t(_e,ve),t(ve,wa),t(_e,$a),t(N,Va),t(N,k),V(be,k,null),t(k,ya),t(k,D),t(D,xa),t(D,Ie),t(Ie,Aa),t(D,Ea),t(D,Je),t(Je,Ca),t(D,Ta),t(k,ka),V(Z,k,null),t(k,ja),V(B,k,null),p(e,wt,f),p(e,S,f),t(S,J),t(J,Qe),V(we,Qe,null),t(S,Ma),t(S,Xe),t(Xe,Na),p(e,$t,f),p(e,C,f),V($e,C,null),t(C,Fa),t(C,Ye),t(Ye,Pa),t(C,Ia),t(C,Ve),t(Ve,qa),t(Ve,ye),t(ye,za),t(Ve,La),t(C,Da),t(C,j),V(xe,j,null),t(j,Sa),t(j,O),t(O,Oa),t(O,qe),t(qe,Ha),t(O,Wa),t(O,et),t(et,Ua),t(O,Ra),t(j,Ga),V(Q,j,null),t(j,Ka),V(X,j,null),Vt=!0},p(e,[f]){const Ae={};f&2&&(Ae.$$scope={dirty:f,ctx:e}),G.$set(Ae);const tt={};f&2&&(tt.$$scope={dirty:f,ctx:e}),Z.$set(tt);const at={};f&2&&(at.$$scope={dirty:f,ctx:e}),B.$set(at);const ot={};f&2&&(ot.$$scope={dirty:f,ctx:e}),Q.$set(ot);const Ee={};f&2&&(Ee.$$scope={dirty:f,ctx:e}),X.$set(Ee)},i(e){Vt||(y(o.$$.fragment,e),y(oe.$$.fragment,e),y(pe.$$.fragment,e),y(he.$$.fragment,e),y(G.$$.fragment,e),y(ue.$$.fragment,e),y(ge.$$.fragment,e),y(be.$$.fragment,e),y(Z.$$.fragment,e),y(B.$$.fragment,e),y(we.$$.fragment,e),y($e.$$.fragment,e),y(xe.$$.fragment,e),y(Q.$$.fragment,e),y(X.$$.fragment,e),Vt=!0)},o(e){x(o.$$.fragment,e),x(oe.$$.fragment,e),x(pe.$$.fragment,e),x(he.$$.fragment,e),x(G.$$.fragment,e),x(ue.$$.fragment,e),x(ge.$$.fragment,e),x(be.$$.fragment,e),x(Z.$$.fragment,e),x(B.$$.fragment,e),x(we.$$.fragment,e),x($e.$$.fragment,e),x(xe.$$.fragment,e),x(Q.$$.fragment,e),x(X.$$.fragment,e),Vt=!1},d(e){a(d),e&&a(b),e&&a(_),A(o),e&&a(rt),e&&a(P),A(oe),e&&a(it),e&&a(W),e&&a(lt),e&&a(Ce),e&&a(ct),e&&a(Te),e&&a(dt),e&&a(ke),e&&a(ft),e&&a(je),e&&a(pt),e&&a(Me),e&&a(ht),e&&a(U),e&&a(mt),e&&a(ce),e&&a(ut),e&&a(F),e&&a(gt),e&&a(I),A(pe),e&&a(_t),e&&a(E),A(he),A(G),e&&a(vt),e&&a(L),A(ue),e&&a(bt),e&&a(N),A(ge),A(be),A(Z),A(B),e&&a(wt),e&&a(S),A(we),e&&a($t),e&&a(C),A($e),A(xe),A(Q),A(X)}}}const Go={local:"van",sections:[{local:"overview",title:"Overview"},{local:"transformers.VanConfig",title:"VanConfig"},{local:"transformers.VanModel",title:"VanModel"},{local:"transformers.VanForImageClassification",title:"VanForImageClassification"}],title:"VAN"};function Ko(T){return Do(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class en extends Po{constructor(d){super();Io(this,d,Ko,Ro,qo,{})}}export{en as default,Go as metadata};
