import{S as R_t,i as S_t,s as P_t,e as a,k as l,w as f,t as o,L as $_t,c as n,d as t,m as i,a as s,x as m,h as r,b as c,J as e,g as b,y as g,q as h,o as p,B as _}from"../../chunks/vendor-9e2b328e.js";import{T as Lyr}from"../../chunks/Tip-76f97a76.js";import{D as E}from"../../chunks/Docstring-50fd6873.js";import{C as w}from"../../chunks/CodeBlock-88e23343.js";import{I as z}from"../../chunks/IconCopyLink-fd0e58fd.js";import"../../chunks/CopyButton-4ae140ab.js";function I_t(wi){let J,Ae,ie,me,to,ce,ue,Do,Ai,yf,sa,Li,Bi,rM,wf,ye,io,ki,Pn,tM,$n,In,aM,xi,jn,nM,Ri,Af,$a;return{c(){J=a("p"),Ae=o("If your "),ie=a("code"),me=o("NewModelConfig"),to=o(" is a subclass of "),ce=a("code"),ue=o("PretrainedConfig"),Do=o(`, make sure its
`),Ai=a("code"),yf=o("model_type"),sa=o(" attribute is set to the same key you use when registering the config (here "),Li=a("code"),Bi=o('"new-model"'),rM=o(")."),wf=l(),ye=a("p"),io=o("Likewise, if your "),ki=a("code"),Pn=o("NewModel"),tM=o(" is a subclass of "),$n=a("a"),In=o("PreTrainedModel"),aM=o(`, make sure its
`),xi=a("code"),jn=o("config_class"),nM=o(` attribute is set to the same class you use when registering the model (here
`),Ri=a("code"),Af=o("NewModelConfig"),$a=o(")."),this.h()},l(co){J=n(co,"P",{});var ge=s(J);Ae=r(ge,"If your "),ie=n(ge,"CODE",{});var G0=s(ie);me=r(G0,"NewModelConfig"),G0.forEach(t),to=r(ge," is a subclass of "),ce=n(ge,"CODE",{});var Si=s(ce);ue=r(Si,"PretrainedConfig"),Si.forEach(t),Do=r(ge,`, make sure its
`),Ai=n(ge,"CODE",{});var O0=s(Ai);yf=r(O0,"model_type"),O0.forEach(t),sa=r(ge," attribute is set to the same key you use when registering the config (here "),Li=n(ge,"CODE",{});var X0=s(Li);Bi=r(X0,'"new-model"'),X0.forEach(t),rM=r(ge,")."),ge.forEach(t),wf=i(co),ye=n(co,"P",{});var qo=s(ye);io=r(qo,"Likewise, if your "),ki=n(qo,"CODE",{});var Ia=s(ki);Pn=r(Ia,"NewModel"),Ia.forEach(t),tM=r(qo," is a subclass of "),$n=n(qo,"A",{href:!0});var z0=s($n);In=r(z0,"PreTrainedModel"),z0.forEach(t),aM=r(qo,`, make sure its
`),xi=n(qo,"CODE",{});var Lf=s(xi);jn=r(Lf,"config_class"),Lf.forEach(t),nM=r(qo,` attribute is set to the same class you use when registering the model (here
`),Ri=n(qo,"CODE",{});var V0=s(Ri);Af=r(V0,"NewModelConfig"),V0.forEach(t),$a=r(qo,")."),qo.forEach(t),this.h()},h(){c($n,"href","/docs/transformers/pr_15297/en/main_classes/model#transformers.PreTrainedModel")},m(co,ge){b(co,J,ge),e(J,Ae),e(J,ie),e(ie,me),e(J,to),e(J,ce),e(ce,ue),e(J,Do),e(J,Ai),e(Ai,yf),e(J,sa),e(J,Li),e(Li,Bi),e(J,rM),b(co,wf,ge),b(co,ye,ge),e(ye,io),e(ye,ki),e(ki,Pn),e(ye,tM),e(ye,$n),e($n,In),e(ye,aM),e(ye,xi),e(xi,jn),e(ye,nM),e(ye,Ri),e(Ri,Af),e(ye,$a)},d(co){co&&t(J),co&&t(wf),co&&t(ye)}}}function j_t(wi){let J,Ae,ie,me,to;return{c(){J=a("p"),Ae=o("Passing "),ie=a("code"),me=o("use_auth_token=True"),to=o(" is required when you want to use a private model.")},l(ce){J=n(ce,"P",{});var ue=s(J);Ae=r(ue,"Passing "),ie=n(ue,"CODE",{});var Do=s(ie);me=r(Do,"use_auth_token=True"),Do.forEach(t),to=r(ue," is required when you want to use a private model."),ue.forEach(t)},m(ce,ue){b(ce,J,ue),e(J,Ae),e(J,ie),e(ie,me),e(J,to)},d(ce){ce&&t(J)}}}function N_t(wi){let J,Ae,ie,me,to;return{c(){J=a("p"),Ae=o("Passing "),ie=a("code"),me=o("use_auth_token=True"),to=o(" is required when you want to use a private model.")},l(ce){J=n(ce,"P",{});var ue=s(J);Ae=r(ue,"Passing "),ie=n(ue,"CODE",{});var Do=s(ie);me=r(Do,"use_auth_token=True"),Do.forEach(t),to=r(ue," is required when you want to use a private model."),ue.forEach(t)},m(ce,ue){b(ce,J,ue),e(J,Ae),e(J,ie),e(ie,me),e(J,to)},d(ce){ce&&t(J)}}}function D_t(wi){let J,Ae,ie,me,to,ce,ue,Do,Ai,yf,sa,Li,Bi,rM,wf,ye,io,ki,Pn,tM,$n,In,aM,xi,jn,nM,Ri,Af,$a,co,ge,G0,Si,O0,X0,qo,Ia,z0,Lf,V0,uxe,iLe,Pi,Bf,DV,sM,bxe,qV,vxe,dLe,Nn,Txe,GV,Fxe,Cxe,OV,Mxe,Exe,cLe,lM,fLe,W0,yxe,mLe,kf,gLe,$i,xf,XV,iM,wxe,zV,Axe,hLe,Go,dM,Lxe,cM,Bxe,Q0,kxe,xxe,Rxe,fM,Sxe,VV,Pxe,$xe,Ixe,fo,mM,jxe,WV,Nxe,Dxe,Ii,qxe,QV,Gxe,Oxe,HV,Xxe,zxe,Vxe,v,Rf,UV,Wxe,Qxe,H0,Hxe,Uxe,Jxe,Sf,JV,Yxe,Kxe,U0,Zxe,eRe,oRe,Pf,YV,rRe,tRe,J0,aRe,nRe,sRe,$f,KV,lRe,iRe,Y0,dRe,cRe,fRe,If,ZV,mRe,gRe,K0,hRe,pRe,_Re,jf,eW,uRe,bRe,Z0,vRe,TRe,FRe,Nf,oW,CRe,MRe,eL,ERe,yRe,wRe,Df,rW,ARe,LRe,oL,BRe,kRe,xRe,qf,tW,RRe,SRe,rL,PRe,$Re,IRe,Gf,aW,jRe,NRe,tL,DRe,qRe,GRe,Of,nW,ORe,XRe,aL,zRe,VRe,WRe,Xf,sW,QRe,HRe,nL,URe,JRe,YRe,zf,lW,KRe,ZRe,sL,eSe,oSe,rSe,Vf,iW,tSe,aSe,lL,nSe,sSe,lSe,Wf,dW,iSe,dSe,iL,cSe,fSe,mSe,Qf,cW,gSe,hSe,dL,pSe,_Se,uSe,Hf,fW,bSe,vSe,cL,TSe,FSe,CSe,Uf,mW,MSe,ESe,fL,ySe,wSe,ASe,Jf,gW,LSe,BSe,mL,kSe,xSe,RSe,Yf,hW,SSe,PSe,gL,$Se,ISe,jSe,Kf,pW,NSe,DSe,hL,qSe,GSe,OSe,Zf,_W,XSe,zSe,pL,VSe,WSe,QSe,em,uW,HSe,USe,_L,JSe,YSe,KSe,om,bW,ZSe,ePe,uL,oPe,rPe,tPe,rm,vW,aPe,nPe,bL,sPe,lPe,iPe,tm,TW,dPe,cPe,vL,fPe,mPe,gPe,am,FW,hPe,pPe,TL,_Pe,uPe,bPe,nm,CW,vPe,TPe,FL,FPe,CPe,MPe,sm,MW,EPe,yPe,CL,wPe,APe,LPe,lm,EW,BPe,kPe,ML,xPe,RPe,SPe,im,yW,PPe,$Pe,EL,IPe,jPe,NPe,dm,wW,DPe,qPe,yL,GPe,OPe,XPe,cm,AW,zPe,VPe,wL,WPe,QPe,HPe,fm,LW,UPe,JPe,AL,YPe,KPe,ZPe,mm,BW,e$e,o$e,LL,r$e,t$e,a$e,gm,kW,n$e,s$e,BL,l$e,i$e,d$e,hm,xW,c$e,f$e,kL,m$e,g$e,h$e,pm,RW,p$e,_$e,xL,u$e,b$e,v$e,_m,SW,T$e,F$e,RL,C$e,M$e,E$e,um,PW,y$e,w$e,SL,A$e,L$e,B$e,bm,$W,k$e,x$e,PL,R$e,S$e,P$e,vm,IW,$$e,I$e,$L,j$e,N$e,D$e,Tm,jW,q$e,G$e,IL,O$e,X$e,z$e,Fm,NW,V$e,W$e,jL,Q$e,H$e,U$e,Cm,DW,J$e,Y$e,NL,K$e,Z$e,eIe,Mm,qW,oIe,rIe,DL,tIe,aIe,nIe,Em,GW,sIe,lIe,qL,iIe,dIe,cIe,ym,OW,fIe,mIe,GL,gIe,hIe,pIe,wm,XW,_Ie,uIe,OL,bIe,vIe,TIe,Am,zW,FIe,CIe,XL,MIe,EIe,yIe,Lm,VW,wIe,AIe,zL,LIe,BIe,kIe,Bm,WW,xIe,RIe,VL,SIe,PIe,$Ie,km,QW,IIe,jIe,WL,NIe,DIe,qIe,xm,HW,GIe,OIe,QL,XIe,zIe,VIe,Rm,UW,WIe,QIe,HL,HIe,UIe,JIe,Sm,JW,YIe,KIe,UL,ZIe,eje,oje,Pm,YW,rje,tje,JL,aje,nje,sje,$m,KW,lje,ije,YL,dje,cje,fje,Im,ZW,mje,gje,KL,hje,pje,_je,jm,eQ,uje,bje,ZL,vje,Tje,Fje,Nm,oQ,Cje,Mje,e8,Eje,yje,wje,Dm,rQ,Aje,Lje,o8,Bje,kje,xje,qm,tQ,Rje,Sje,r8,Pje,$je,Ije,Gm,aQ,jje,Nje,t8,Dje,qje,Gje,Om,nQ,Oje,Xje,a8,zje,Vje,Wje,Xm,sQ,Qje,Hje,n8,Uje,Jje,Yje,zm,lQ,Kje,Zje,s8,eNe,oNe,rNe,Vm,iQ,tNe,aNe,l8,nNe,sNe,lNe,Wm,dQ,iNe,dNe,i8,cNe,fNe,mNe,Qm,cQ,gNe,hNe,d8,pNe,_Ne,uNe,Hm,fQ,bNe,vNe,c8,TNe,FNe,CNe,Um,mQ,MNe,ENe,f8,yNe,wNe,ANe,Jm,gQ,LNe,BNe,m8,kNe,xNe,RNe,Ym,hQ,SNe,PNe,g8,$Ne,INe,jNe,Km,pQ,NNe,DNe,h8,qNe,GNe,ONe,Zm,_Q,XNe,zNe,p8,VNe,WNe,QNe,eg,uQ,HNe,UNe,_8,JNe,YNe,KNe,og,bQ,ZNe,eDe,u8,oDe,rDe,tDe,rg,vQ,aDe,nDe,b8,sDe,lDe,iDe,tg,TQ,dDe,cDe,v8,fDe,mDe,gDe,ag,FQ,hDe,pDe,T8,_De,uDe,bDe,ng,CQ,vDe,TDe,F8,FDe,CDe,MDe,sg,MQ,EDe,yDe,C8,wDe,ADe,LDe,lg,EQ,BDe,kDe,M8,xDe,RDe,SDe,ig,yQ,PDe,$De,E8,IDe,jDe,NDe,dg,wQ,DDe,qDe,y8,GDe,ODe,XDe,cg,AQ,zDe,VDe,w8,WDe,QDe,HDe,fg,LQ,UDe,JDe,A8,YDe,KDe,ZDe,mg,BQ,eqe,oqe,L8,rqe,tqe,aqe,gg,kQ,nqe,sqe,B8,lqe,iqe,dqe,hg,xQ,cqe,fqe,k8,mqe,gqe,hqe,RQ,pqe,_qe,gM,uqe,pg,hM,bqe,SQ,vqe,pLe,ji,_g,PQ,pM,Tqe,$Q,Fqe,_Le,Oo,_M,Cqe,uM,Mqe,x8,Eqe,yqe,wqe,bM,Aqe,IQ,Lqe,Bqe,kqe,mo,vM,xqe,jQ,Rqe,Sqe,ja,Pqe,NQ,$qe,Iqe,DQ,jqe,Nqe,qQ,Dqe,qqe,Gqe,M,Dn,GQ,Oqe,Xqe,R8,zqe,Vqe,S8,Wqe,Qqe,Hqe,qn,OQ,Uqe,Jqe,P8,Yqe,Kqe,$8,Zqe,eGe,oGe,Gn,XQ,rGe,tGe,I8,aGe,nGe,j8,sGe,lGe,iGe,ug,zQ,dGe,cGe,N8,fGe,mGe,gGe,On,VQ,hGe,pGe,D8,_Ge,uGe,q8,bGe,vGe,TGe,bg,WQ,FGe,CGe,G8,MGe,EGe,yGe,vg,QQ,wGe,AGe,O8,LGe,BGe,kGe,Tg,HQ,xGe,RGe,X8,SGe,PGe,$Ge,Xn,UQ,IGe,jGe,z8,NGe,DGe,V8,qGe,GGe,OGe,zn,JQ,XGe,zGe,W8,VGe,WGe,Q8,QGe,HGe,UGe,Vn,YQ,JGe,YGe,H8,KGe,ZGe,U8,eOe,oOe,rOe,Fg,KQ,tOe,aOe,J8,nOe,sOe,lOe,Cg,ZQ,iOe,dOe,Y8,cOe,fOe,mOe,Wn,eH,gOe,hOe,K8,pOe,_Oe,Z8,uOe,bOe,vOe,Mg,oH,TOe,FOe,eB,COe,MOe,EOe,Qn,rH,yOe,wOe,oB,AOe,LOe,rB,BOe,kOe,xOe,Hn,tH,ROe,SOe,tB,POe,$Oe,aB,IOe,jOe,NOe,Un,aH,DOe,qOe,nB,GOe,OOe,nH,XOe,zOe,VOe,Eg,sH,WOe,QOe,sB,HOe,UOe,JOe,Jn,lH,YOe,KOe,lB,ZOe,eXe,iB,oXe,rXe,tXe,yg,iH,aXe,nXe,dB,sXe,lXe,iXe,Yn,dH,dXe,cXe,cB,fXe,mXe,fB,gXe,hXe,pXe,Kn,cH,_Xe,uXe,mB,bXe,vXe,gB,TXe,FXe,CXe,Zn,fH,MXe,EXe,hB,yXe,wXe,pB,AXe,LXe,BXe,wg,mH,kXe,xXe,_B,RXe,SXe,PXe,es,gH,$Xe,IXe,uB,jXe,NXe,bB,DXe,qXe,GXe,Ag,hH,OXe,XXe,vB,zXe,VXe,WXe,os,pH,QXe,HXe,TB,UXe,JXe,FB,YXe,KXe,ZXe,rs,_H,eze,oze,CB,rze,tze,MB,aze,nze,sze,ts,uH,lze,ize,EB,dze,cze,yB,fze,mze,gze,as,bH,hze,pze,wB,_ze,uze,AB,bze,vze,Tze,Lg,vH,Fze,Cze,LB,Mze,Eze,yze,ns,TH,wze,Aze,BB,Lze,Bze,kB,kze,xze,Rze,ss,FH,Sze,Pze,xB,$ze,Ize,RB,jze,Nze,Dze,ls,CH,qze,Gze,SB,Oze,Xze,PB,zze,Vze,Wze,is,MH,Qze,Hze,$B,Uze,Jze,IB,Yze,Kze,Zze,ds,EH,eVe,oVe,jB,rVe,tVe,NB,aVe,nVe,sVe,cs,yH,lVe,iVe,DB,dVe,cVe,qB,fVe,mVe,gVe,Bg,wH,hVe,pVe,GB,_Ve,uVe,bVe,fs,AH,vVe,TVe,OB,FVe,CVe,XB,MVe,EVe,yVe,kg,LH,wVe,AVe,zB,LVe,BVe,kVe,xg,BH,xVe,RVe,VB,SVe,PVe,$Ve,ms,kH,IVe,jVe,WB,NVe,DVe,QB,qVe,GVe,OVe,gs,xH,XVe,zVe,HB,VVe,WVe,UB,QVe,HVe,UVe,Rg,RH,JVe,YVe,JB,KVe,ZVe,eWe,hs,SH,oWe,rWe,YB,tWe,aWe,KB,nWe,sWe,lWe,ps,PH,iWe,dWe,ZB,cWe,fWe,ek,mWe,gWe,hWe,_s,$H,pWe,_We,ok,uWe,bWe,rk,vWe,TWe,FWe,us,IH,CWe,MWe,tk,EWe,yWe,ak,wWe,AWe,LWe,bs,jH,BWe,kWe,nk,xWe,RWe,sk,SWe,PWe,$We,Sg,NH,IWe,jWe,lk,NWe,DWe,qWe,Pg,DH,GWe,OWe,ik,XWe,zWe,VWe,$g,qH,WWe,QWe,dk,HWe,UWe,JWe,Ig,GH,YWe,KWe,ck,ZWe,eQe,oQe,vs,OH,rQe,tQe,fk,aQe,nQe,mk,sQe,lQe,iQe,jg,XH,dQe,cQe,gk,fQe,mQe,gQe,Ts,zH,hQe,pQe,hk,_Qe,uQe,pk,bQe,vQe,TQe,Fs,VH,FQe,CQe,_k,MQe,EQe,uk,yQe,wQe,AQe,Cs,WH,LQe,BQe,bk,kQe,xQe,vk,RQe,SQe,PQe,Ms,QH,$Qe,IQe,Tk,jQe,NQe,Fk,DQe,qQe,GQe,Es,HH,OQe,XQe,Ck,zQe,VQe,Mk,WQe,QQe,HQe,ys,UH,UQe,JQe,Ek,YQe,KQe,yk,ZQe,eHe,oHe,Ng,JH,rHe,tHe,wk,aHe,nHe,sHe,Dg,YH,lHe,iHe,Ak,dHe,cHe,fHe,ws,KH,mHe,gHe,Lk,hHe,pHe,Bk,_He,uHe,bHe,As,ZH,vHe,THe,kk,FHe,CHe,xk,MHe,EHe,yHe,Ls,eU,wHe,AHe,Rk,LHe,BHe,Sk,kHe,xHe,RHe,qg,oU,SHe,PHe,Pk,$He,IHe,jHe,Gg,rU,NHe,DHe,$k,qHe,GHe,OHe,Og,tU,XHe,zHe,Ik,VHe,WHe,QHe,Xg,aU,HHe,UHe,jk,JHe,YHe,KHe,Bs,nU,ZHe,eUe,Nk,oUe,rUe,Dk,tUe,aUe,nUe,zg,sU,sUe,lUe,qk,iUe,dUe,cUe,Vg,lU,fUe,mUe,Gk,gUe,hUe,pUe,ks,iU,_Ue,uUe,Ok,bUe,vUe,Xk,TUe,FUe,CUe,xs,dU,MUe,EUe,zk,yUe,wUe,Vk,AUe,LUe,BUe,cU,kUe,xUe,TM,RUe,Wg,FM,SUe,fU,PUe,uLe,Ni,Qg,mU,CM,$Ue,gU,IUe,bLe,Xo,MM,jUe,EM,NUe,Wk,DUe,qUe,GUe,yM,OUe,hU,XUe,zUe,VUe,Le,wM,WUe,pU,QUe,HUe,Na,UUe,_U,JUe,YUe,uU,KUe,ZUe,bU,eJe,oJe,rJe,se,Hg,vU,tJe,aJe,Qk,nJe,sJe,lJe,Ug,TU,iJe,dJe,Hk,cJe,fJe,mJe,Jg,FU,gJe,hJe,Uk,pJe,_Je,uJe,Yg,CU,bJe,vJe,Jk,TJe,FJe,CJe,Kg,MU,MJe,EJe,Yk,yJe,wJe,AJe,Zg,EU,LJe,BJe,Kk,kJe,xJe,RJe,eh,yU,SJe,PJe,Zk,$Je,IJe,jJe,oh,wU,NJe,DJe,ex,qJe,GJe,OJe,rh,AU,XJe,zJe,ox,VJe,WJe,QJe,th,LU,HJe,UJe,rx,JJe,YJe,KJe,ah,BU,ZJe,eYe,tx,oYe,rYe,tYe,nh,kU,aYe,nYe,ax,sYe,lYe,iYe,sh,xU,dYe,cYe,nx,fYe,mYe,gYe,lh,RU,hYe,pYe,sx,_Ye,uYe,bYe,ih,SU,vYe,TYe,lx,FYe,CYe,MYe,dh,EYe,PU,yYe,wYe,AM,AYe,ch,LM,LYe,$U,BYe,vLe,Di,fh,IU,BM,kYe,jU,xYe,TLe,zo,kM,RYe,xM,SYe,ix,PYe,$Ye,IYe,RM,jYe,NU,NYe,DYe,qYe,Be,SM,GYe,DU,OYe,XYe,qi,zYe,qU,VYe,WYe,GU,QYe,HYe,UYe,we,mh,OU,JYe,YYe,dx,KYe,ZYe,eKe,gh,XU,oKe,rKe,cx,tKe,aKe,nKe,hh,zU,sKe,lKe,fx,iKe,dKe,cKe,ph,VU,fKe,mKe,mx,gKe,hKe,pKe,_h,WU,_Ke,uKe,gx,bKe,vKe,TKe,uh,QU,FKe,CKe,hx,MKe,EKe,yKe,bh,HU,wKe,AKe,px,LKe,BKe,kKe,vh,UU,xKe,RKe,_x,SKe,PKe,$Ke,Th,IKe,JU,jKe,NKe,PM,DKe,Fh,$M,qKe,YU,GKe,FLe,Gi,Ch,KU,IM,OKe,ZU,XKe,CLe,Vo,jM,zKe,Oi,VKe,eJ,WKe,QKe,oJ,HKe,UKe,JKe,NM,YKe,rJ,KKe,ZKe,eZe,Nr,DM,oZe,tJ,rZe,tZe,Xi,aZe,aJ,nZe,sZe,nJ,lZe,iZe,dZe,sJ,cZe,fZe,qM,mZe,ke,GM,gZe,lJ,hZe,pZe,Da,_Ze,iJ,uZe,bZe,dJ,vZe,TZe,cJ,FZe,CZe,MZe,F,Mh,fJ,EZe,yZe,ux,wZe,AZe,LZe,Eh,mJ,BZe,kZe,bx,xZe,RZe,SZe,yh,gJ,PZe,$Ze,vx,IZe,jZe,NZe,wh,hJ,DZe,qZe,Tx,GZe,OZe,XZe,Ah,pJ,zZe,VZe,Fx,WZe,QZe,HZe,Lh,_J,UZe,JZe,Cx,YZe,KZe,ZZe,Bh,uJ,eeo,oeo,Mx,reo,teo,aeo,kh,bJ,neo,seo,Ex,leo,ieo,deo,xh,vJ,ceo,feo,yx,meo,geo,heo,Rh,TJ,peo,_eo,wx,ueo,beo,veo,Sh,FJ,Teo,Feo,Ax,Ceo,Meo,Eeo,Ph,CJ,yeo,weo,Lx,Aeo,Leo,Beo,$h,MJ,keo,xeo,Bx,Reo,Seo,Peo,Ih,EJ,$eo,Ieo,kx,jeo,Neo,Deo,jh,yJ,qeo,Geo,xx,Oeo,Xeo,zeo,Nh,wJ,Veo,Weo,Rx,Qeo,Heo,Ueo,Dh,AJ,Jeo,Yeo,Sx,Keo,Zeo,eoo,qh,LJ,ooo,roo,Px,too,aoo,noo,Gh,BJ,soo,loo,$x,ioo,doo,coo,Oh,kJ,foo,moo,Ix,goo,hoo,poo,Xh,xJ,_oo,uoo,jx,boo,voo,Too,zh,RJ,Foo,Coo,Nx,Moo,Eoo,yoo,Vh,SJ,woo,Aoo,Dx,Loo,Boo,koo,Wh,PJ,xoo,Roo,qx,Soo,Poo,$oo,Qh,$J,Ioo,joo,Gx,Noo,Doo,qoo,Rs,IJ,Goo,Ooo,Ox,Xoo,zoo,Xx,Voo,Woo,Qoo,Hh,jJ,Hoo,Uoo,zx,Joo,Yoo,Koo,Uh,NJ,Zoo,ero,Vx,oro,rro,tro,Jh,DJ,aro,nro,Wx,sro,lro,iro,Yh,qJ,dro,cro,Qx,fro,mro,gro,Kh,GJ,hro,pro,Hx,_ro,uro,bro,Zh,OJ,vro,Tro,Ux,Fro,Cro,Mro,ep,XJ,Ero,yro,Jx,wro,Aro,Lro,op,zJ,Bro,kro,Yx,xro,Rro,Sro,rp,VJ,Pro,$ro,Kx,Iro,jro,Nro,tp,WJ,Dro,qro,Zx,Gro,Oro,Xro,ap,QJ,zro,Vro,eR,Wro,Qro,Hro,np,HJ,Uro,Jro,oR,Yro,Kro,Zro,sp,UJ,eto,oto,rR,rto,tto,ato,lp,JJ,nto,sto,tR,lto,ito,dto,ip,YJ,cto,fto,aR,mto,gto,hto,dp,KJ,pto,_to,nR,uto,bto,vto,cp,ZJ,Tto,Fto,sR,Cto,Mto,Eto,fp,eY,yto,wto,lR,Ato,Lto,Bto,mp,oY,kto,xto,iR,Rto,Sto,Pto,gp,rY,$to,Ito,dR,jto,Nto,Dto,hp,tY,qto,Gto,cR,Oto,Xto,zto,pp,aY,Vto,Wto,fR,Qto,Hto,Uto,_p,nY,Jto,Yto,mR,Kto,Zto,eao,up,sY,oao,rao,gR,tao,aao,nao,bp,lY,sao,lao,hR,iao,dao,cao,vp,iY,fao,mao,pR,gao,hao,pao,Tp,dY,_ao,uao,_R,bao,vao,Tao,Fp,cY,Fao,Cao,uR,Mao,Eao,yao,Cp,fY,wao,Aao,bR,Lao,Bao,kao,Mp,mY,xao,Rao,vR,Sao,Pao,$ao,Ep,gY,Iao,jao,TR,Nao,Dao,qao,yp,hY,Gao,Oao,FR,Xao,zao,Vao,wp,pY,Wao,Qao,CR,Hao,Uao,Jao,Ap,_Y,Yao,Kao,MR,Zao,eno,ono,Lp,uY,rno,tno,ER,ano,nno,sno,Bp,bY,lno,ino,yR,dno,cno,fno,kp,vY,mno,gno,wR,hno,pno,_no,xp,TY,uno,bno,AR,vno,Tno,Fno,Rp,FY,Cno,Mno,LR,Eno,yno,wno,Sp,CY,Ano,Lno,BR,Bno,kno,xno,Pp,MY,Rno,Sno,kR,Pno,$no,Ino,$p,EY,jno,Nno,xR,Dno,qno,Gno,Ip,yY,Ono,Xno,RR,zno,Vno,Wno,jp,wY,Qno,Hno,SR,Uno,Jno,Yno,Np,AY,Kno,Zno,PR,eso,oso,rso,Dp,LY,tso,aso,$R,nso,sso,lso,qp,BY,iso,dso,IR,cso,fso,mso,Gp,kY,gso,hso,jR,pso,_so,uso,Op,xY,bso,vso,NR,Tso,Fso,Cso,Xp,RY,Mso,Eso,DR,yso,wso,Aso,zp,SY,Lso,Bso,qR,kso,xso,Rso,Vp,PY,Sso,Pso,GR,$so,Iso,jso,Wp,$Y,Nso,Dso,OR,qso,Gso,Oso,Qp,IY,Xso,zso,XR,Vso,Wso,Qso,Hp,jY,Hso,Uso,zR,Jso,Yso,Kso,Up,NY,Zso,elo,VR,olo,rlo,tlo,Jp,DY,alo,nlo,WR,slo,llo,ilo,Yp,qY,dlo,clo,QR,flo,mlo,glo,Kp,hlo,GY,plo,_lo,OY,ulo,blo,XY,vlo,Tlo,OM,MLe,zi,Zp,zY,XM,Flo,VY,Clo,ELe,Wo,zM,Mlo,Vi,Elo,WY,ylo,wlo,QY,Alo,Llo,Blo,VM,klo,HY,xlo,Rlo,Slo,Dr,WM,Plo,UY,$lo,Ilo,Wi,jlo,JY,Nlo,Dlo,YY,qlo,Glo,Olo,KY,Xlo,zlo,QM,Vlo,xe,HM,Wlo,ZY,Qlo,Hlo,qa,Ulo,eK,Jlo,Ylo,oK,Klo,Zlo,rK,eio,oio,rio,x,e_,tK,tio,aio,HR,nio,sio,lio,o_,aK,iio,dio,UR,cio,fio,mio,r_,nK,gio,hio,JR,pio,_io,uio,t_,sK,bio,vio,YR,Tio,Fio,Cio,a_,lK,Mio,Eio,KR,yio,wio,Aio,n_,iK,Lio,Bio,ZR,kio,xio,Rio,s_,dK,Sio,Pio,eS,$io,Iio,jio,l_,cK,Nio,Dio,oS,qio,Gio,Oio,i_,fK,Xio,zio,rS,Vio,Wio,Qio,d_,mK,Hio,Uio,tS,Jio,Yio,Kio,c_,gK,Zio,edo,aS,odo,rdo,tdo,f_,hK,ado,ndo,nS,sdo,ldo,ido,m_,pK,ddo,cdo,sS,fdo,mdo,gdo,g_,_K,hdo,pdo,lS,_do,udo,bdo,h_,uK,vdo,Tdo,iS,Fdo,Cdo,Mdo,p_,bK,Edo,ydo,dS,wdo,Ado,Ldo,__,vK,Bdo,kdo,cS,xdo,Rdo,Sdo,u_,TK,Pdo,$do,fS,Ido,jdo,Ndo,b_,FK,Ddo,qdo,mS,Gdo,Odo,Xdo,v_,CK,zdo,Vdo,gS,Wdo,Qdo,Hdo,T_,MK,Udo,Jdo,hS,Ydo,Kdo,Zdo,F_,EK,eco,oco,pS,rco,tco,aco,C_,yK,nco,sco,_S,lco,ico,dco,M_,wK,cco,fco,uS,mco,gco,hco,E_,AK,pco,_co,bS,uco,bco,vco,y_,LK,Tco,Fco,vS,Cco,Mco,Eco,w_,BK,yco,wco,TS,Aco,Lco,Bco,A_,kK,kco,xco,FS,Rco,Sco,Pco,L_,xK,$co,Ico,CS,jco,Nco,Dco,B_,RK,qco,Gco,MS,Oco,Xco,zco,k_,SK,Vco,Wco,ES,Qco,Hco,Uco,x_,PK,Jco,Yco,yS,Kco,Zco,efo,R_,$K,ofo,rfo,wS,tfo,afo,nfo,S_,IK,sfo,lfo,AS,ifo,dfo,cfo,P_,jK,ffo,mfo,LS,gfo,hfo,pfo,$_,NK,_fo,ufo,BS,bfo,vfo,Tfo,I_,DK,Ffo,Cfo,kS,Mfo,Efo,yfo,j_,qK,wfo,Afo,xS,Lfo,Bfo,kfo,N_,xfo,GK,Rfo,Sfo,OK,Pfo,$fo,XK,Ifo,jfo,UM,yLe,Qi,D_,zK,JM,Nfo,VK,Dfo,wLe,Qo,YM,qfo,Hi,Gfo,WK,Ofo,Xfo,QK,zfo,Vfo,Wfo,KM,Qfo,HK,Hfo,Ufo,Jfo,qr,ZM,Yfo,UK,Kfo,Zfo,Ui,emo,JK,omo,rmo,YK,tmo,amo,nmo,KK,smo,lmo,eE,imo,Re,oE,dmo,ZK,cmo,fmo,Ga,mmo,eZ,gmo,hmo,oZ,pmo,_mo,rZ,umo,bmo,vmo,$,q_,tZ,Tmo,Fmo,RS,Cmo,Mmo,Emo,G_,aZ,ymo,wmo,SS,Amo,Lmo,Bmo,O_,nZ,kmo,xmo,PS,Rmo,Smo,Pmo,X_,sZ,$mo,Imo,$S,jmo,Nmo,Dmo,z_,lZ,qmo,Gmo,IS,Omo,Xmo,zmo,V_,iZ,Vmo,Wmo,jS,Qmo,Hmo,Umo,W_,dZ,Jmo,Ymo,NS,Kmo,Zmo,ego,Q_,cZ,ogo,rgo,DS,tgo,ago,ngo,H_,fZ,sgo,lgo,qS,igo,dgo,cgo,U_,mZ,fgo,mgo,GS,ggo,hgo,pgo,J_,gZ,_go,ugo,OS,bgo,vgo,Tgo,Y_,hZ,Fgo,Cgo,XS,Mgo,Ego,ygo,K_,pZ,wgo,Ago,zS,Lgo,Bgo,kgo,Z_,_Z,xgo,Rgo,VS,Sgo,Pgo,$go,eu,uZ,Igo,jgo,WS,Ngo,Dgo,qgo,ou,bZ,Ggo,Ogo,QS,Xgo,zgo,Vgo,ru,vZ,Wgo,Qgo,HS,Hgo,Ugo,Jgo,tu,TZ,Ygo,Kgo,US,Zgo,eho,oho,au,FZ,rho,tho,JS,aho,nho,sho,nu,CZ,lho,iho,YS,dho,cho,fho,su,MZ,mho,gho,KS,hho,pho,_ho,lu,EZ,uho,bho,ZS,vho,Tho,Fho,iu,yZ,Cho,Mho,eP,Eho,yho,who,du,wZ,Aho,Lho,oP,Bho,kho,xho,cu,AZ,Rho,Sho,rP,Pho,$ho,Iho,fu,LZ,jho,Nho,tP,Dho,qho,Gho,mu,BZ,Oho,Xho,aP,zho,Vho,Who,gu,kZ,Qho,Hho,nP,Uho,Jho,Yho,hu,xZ,Kho,Zho,sP,epo,opo,rpo,pu,RZ,tpo,apo,lP,npo,spo,lpo,_u,SZ,ipo,dpo,iP,cpo,fpo,mpo,uu,PZ,gpo,hpo,dP,ppo,_po,upo,bu,$Z,bpo,vpo,cP,Tpo,Fpo,Cpo,vu,IZ,Mpo,Epo,fP,ypo,wpo,Apo,Tu,Lpo,jZ,Bpo,kpo,NZ,xpo,Rpo,DZ,Spo,Ppo,rE,ALe,Ji,Fu,qZ,tE,$po,GZ,Ipo,LLe,Ho,aE,jpo,Yi,Npo,OZ,Dpo,qpo,XZ,Gpo,Opo,Xpo,nE,zpo,zZ,Vpo,Wpo,Qpo,Gr,sE,Hpo,VZ,Upo,Jpo,Ki,Ypo,WZ,Kpo,Zpo,QZ,e_o,o_o,r_o,HZ,t_o,a_o,lE,n_o,Se,iE,s_o,UZ,l_o,i_o,Oa,d_o,JZ,c_o,f_o,YZ,m_o,g_o,KZ,h_o,p_o,__o,I,Cu,ZZ,u_o,b_o,mP,v_o,T_o,F_o,Mu,eee,C_o,M_o,gP,E_o,y_o,w_o,Eu,oee,A_o,L_o,hP,B_o,k_o,x_o,yu,ree,R_o,S_o,pP,P_o,$_o,I_o,wu,tee,j_o,N_o,_P,D_o,q_o,G_o,Au,aee,O_o,X_o,uP,z_o,V_o,W_o,Lu,nee,Q_o,H_o,bP,U_o,J_o,Y_o,Bu,see,K_o,Z_o,vP,euo,ouo,ruo,ku,lee,tuo,auo,TP,nuo,suo,luo,xu,iee,iuo,duo,FP,cuo,fuo,muo,Ru,dee,guo,huo,CP,puo,_uo,uuo,Su,cee,buo,vuo,MP,Tuo,Fuo,Cuo,Pu,fee,Muo,Euo,EP,yuo,wuo,Auo,$u,mee,Luo,Buo,yP,kuo,xuo,Ruo,Iu,gee,Suo,Puo,wP,$uo,Iuo,juo,ju,hee,Nuo,Duo,AP,quo,Guo,Ouo,Nu,pee,Xuo,zuo,LP,Vuo,Wuo,Quo,Du,_ee,Huo,Uuo,BP,Juo,Yuo,Kuo,qu,uee,Zuo,e2o,kP,o2o,r2o,t2o,Gu,bee,a2o,n2o,xP,s2o,l2o,i2o,Ou,vee,d2o,c2o,RP,f2o,m2o,g2o,Xu,Tee,h2o,p2o,SP,_2o,u2o,b2o,zu,Fee,v2o,T2o,PP,F2o,C2o,M2o,Vu,Cee,E2o,y2o,$P,w2o,A2o,L2o,Wu,Mee,B2o,k2o,IP,x2o,R2o,S2o,Qu,Eee,P2o,$2o,jP,I2o,j2o,N2o,Hu,yee,D2o,q2o,NP,G2o,O2o,X2o,Uu,wee,z2o,V2o,DP,W2o,Q2o,H2o,Ju,Aee,U2o,J2o,qP,Y2o,K2o,Z2o,Yu,Lee,e1o,o1o,Bee,r1o,t1o,a1o,Ku,kee,n1o,s1o,GP,l1o,i1o,d1o,Zu,xee,c1o,f1o,OP,m1o,g1o,h1o,e2,Ree,p1o,_1o,XP,u1o,b1o,v1o,o2,See,T1o,F1o,zP,C1o,M1o,E1o,r2,y1o,Pee,w1o,A1o,$ee,L1o,B1o,Iee,k1o,x1o,dE,BLe,Zi,t2,jee,cE,R1o,Nee,S1o,kLe,Uo,fE,P1o,ed,$1o,Dee,I1o,j1o,qee,N1o,D1o,q1o,mE,G1o,Gee,O1o,X1o,z1o,Or,gE,V1o,Oee,W1o,Q1o,od,H1o,Xee,U1o,J1o,zee,Y1o,K1o,Z1o,Vee,ebo,obo,hE,rbo,Pe,pE,tbo,Wee,abo,nbo,Xa,sbo,Qee,lbo,ibo,Hee,dbo,cbo,Uee,fbo,mbo,gbo,ae,a2,Jee,hbo,pbo,VP,_bo,ubo,bbo,n2,Yee,vbo,Tbo,WP,Fbo,Cbo,Mbo,s2,Kee,Ebo,ybo,QP,wbo,Abo,Lbo,l2,Zee,Bbo,kbo,HP,xbo,Rbo,Sbo,i2,eoe,Pbo,$bo,UP,Ibo,jbo,Nbo,d2,ooe,Dbo,qbo,JP,Gbo,Obo,Xbo,c2,roe,zbo,Vbo,YP,Wbo,Qbo,Hbo,f2,toe,Ubo,Jbo,KP,Ybo,Kbo,Zbo,m2,aoe,e5o,o5o,ZP,r5o,t5o,a5o,g2,noe,n5o,s5o,e$,l5o,i5o,d5o,h2,soe,c5o,f5o,o$,m5o,g5o,h5o,p2,loe,p5o,_5o,r$,u5o,b5o,v5o,_2,ioe,T5o,F5o,t$,C5o,M5o,E5o,u2,doe,y5o,w5o,a$,A5o,L5o,B5o,b2,coe,k5o,x5o,n$,R5o,S5o,P5o,v2,foe,$5o,I5o,s$,j5o,N5o,D5o,T2,q5o,moe,G5o,O5o,goe,X5o,z5o,hoe,V5o,W5o,_E,xLe,rd,F2,poe,uE,Q5o,_oe,H5o,RLe,Jo,bE,U5o,td,J5o,uoe,Y5o,K5o,boe,Z5o,evo,ovo,vE,rvo,voe,tvo,avo,nvo,Xr,TE,svo,Toe,lvo,ivo,ad,dvo,Foe,cvo,fvo,Coe,mvo,gvo,hvo,Moe,pvo,_vo,FE,uvo,$e,CE,bvo,Eoe,vvo,Tvo,za,Fvo,yoe,Cvo,Mvo,woe,Evo,yvo,Aoe,wvo,Avo,Lvo,A,C2,Loe,Bvo,kvo,l$,xvo,Rvo,Svo,M2,Boe,Pvo,$vo,i$,Ivo,jvo,Nvo,E2,koe,Dvo,qvo,d$,Gvo,Ovo,Xvo,y2,xoe,zvo,Vvo,c$,Wvo,Qvo,Hvo,w2,Roe,Uvo,Jvo,f$,Yvo,Kvo,Zvo,A2,Soe,eTo,oTo,m$,rTo,tTo,aTo,L2,Poe,nTo,sTo,g$,lTo,iTo,dTo,B2,$oe,cTo,fTo,h$,mTo,gTo,hTo,k2,Ioe,pTo,_To,p$,uTo,bTo,vTo,x2,joe,TTo,FTo,_$,CTo,MTo,ETo,R2,Noe,yTo,wTo,u$,ATo,LTo,BTo,S2,Doe,kTo,xTo,b$,RTo,STo,PTo,P2,qoe,$To,ITo,v$,jTo,NTo,DTo,$2,Goe,qTo,GTo,T$,OTo,XTo,zTo,I2,Ooe,VTo,WTo,F$,QTo,HTo,UTo,j2,Xoe,JTo,YTo,C$,KTo,ZTo,e7o,N2,zoe,o7o,r7o,M$,t7o,a7o,n7o,D2,Voe,s7o,l7o,E$,i7o,d7o,c7o,q2,Woe,f7o,m7o,y$,g7o,h7o,p7o,G2,Qoe,_7o,u7o,w$,b7o,v7o,T7o,O2,Hoe,F7o,C7o,A$,M7o,E7o,y7o,X2,Uoe,w7o,A7o,L$,L7o,B7o,k7o,z2,Joe,x7o,R7o,B$,S7o,P7o,$7o,V2,Yoe,I7o,j7o,k$,N7o,D7o,q7o,W2,Koe,G7o,O7o,x$,X7o,z7o,V7o,Q2,Zoe,W7o,Q7o,R$,H7o,U7o,J7o,H2,ere,Y7o,K7o,S$,Z7o,eFo,oFo,U2,ore,rFo,tFo,P$,aFo,nFo,sFo,J2,rre,lFo,iFo,$$,dFo,cFo,fFo,Y2,tre,mFo,gFo,I$,hFo,pFo,_Fo,K2,are,uFo,bFo,j$,vFo,TFo,FFo,Z2,nre,CFo,MFo,N$,EFo,yFo,wFo,e1,sre,AFo,LFo,D$,BFo,kFo,xFo,o1,lre,RFo,SFo,q$,PFo,$Fo,IFo,r1,ire,jFo,NFo,G$,DFo,qFo,GFo,t1,dre,OFo,XFo,O$,zFo,VFo,WFo,a1,cre,QFo,HFo,X$,UFo,JFo,YFo,n1,fre,KFo,ZFo,z$,e9o,o9o,r9o,s1,mre,t9o,a9o,V$,n9o,s9o,l9o,l1,gre,i9o,d9o,W$,c9o,f9o,m9o,i1,hre,g9o,h9o,Q$,p9o,_9o,u9o,d1,pre,b9o,v9o,H$,T9o,F9o,C9o,c1,_re,M9o,E9o,U$,y9o,w9o,A9o,f1,ure,L9o,B9o,J$,k9o,x9o,R9o,m1,bre,S9o,P9o,Y$,$9o,I9o,j9o,g1,N9o,vre,D9o,q9o,Tre,G9o,O9o,Fre,X9o,z9o,ME,SLe,nd,h1,Cre,EE,V9o,Mre,W9o,PLe,Yo,yE,Q9o,sd,H9o,Ere,U9o,J9o,yre,Y9o,K9o,Z9o,wE,eCo,wre,oCo,rCo,tCo,zr,AE,aCo,Are,nCo,sCo,ld,lCo,Lre,iCo,dCo,Bre,cCo,fCo,mCo,kre,gCo,hCo,LE,pCo,Ie,BE,_Co,xre,uCo,bCo,Va,vCo,Rre,TCo,FCo,Sre,CCo,MCo,Pre,ECo,yCo,wCo,G,p1,$re,ACo,LCo,K$,BCo,kCo,xCo,_1,Ire,RCo,SCo,Z$,PCo,$Co,ICo,u1,jre,jCo,NCo,eI,DCo,qCo,GCo,b1,Nre,OCo,XCo,oI,zCo,VCo,WCo,v1,Dre,QCo,HCo,rI,UCo,JCo,YCo,T1,qre,KCo,ZCo,tI,e4o,o4o,r4o,F1,Gre,t4o,a4o,aI,n4o,s4o,l4o,C1,Ore,i4o,d4o,nI,c4o,f4o,m4o,M1,Xre,g4o,h4o,sI,p4o,_4o,u4o,E1,zre,b4o,v4o,lI,T4o,F4o,C4o,y1,Vre,M4o,E4o,iI,y4o,w4o,A4o,w1,Wre,L4o,B4o,dI,k4o,x4o,R4o,A1,Qre,S4o,P4o,cI,$4o,I4o,j4o,L1,Hre,N4o,D4o,fI,q4o,G4o,O4o,B1,Ure,X4o,z4o,mI,V4o,W4o,Q4o,k1,Jre,H4o,U4o,gI,J4o,Y4o,K4o,x1,Yre,Z4o,eMo,hI,oMo,rMo,tMo,R1,Kre,aMo,nMo,pI,sMo,lMo,iMo,S1,Zre,dMo,cMo,_I,fMo,mMo,gMo,P1,ete,hMo,pMo,uI,_Mo,uMo,bMo,$1,ote,vMo,TMo,bI,FMo,CMo,MMo,I1,rte,EMo,yMo,vI,wMo,AMo,LMo,j1,tte,BMo,kMo,TI,xMo,RMo,SMo,N1,ate,PMo,$Mo,FI,IMo,jMo,NMo,D1,nte,DMo,qMo,CI,GMo,OMo,XMo,q1,ste,zMo,VMo,MI,WMo,QMo,HMo,G1,lte,UMo,JMo,EI,YMo,KMo,ZMo,O1,eEo,ite,oEo,rEo,dte,tEo,aEo,cte,nEo,sEo,kE,$Le,id,X1,fte,xE,lEo,mte,iEo,ILe,Ko,RE,dEo,dd,cEo,gte,fEo,mEo,hte,gEo,hEo,pEo,SE,_Eo,pte,uEo,bEo,vEo,Vr,PE,TEo,_te,FEo,CEo,cd,MEo,ute,EEo,yEo,bte,wEo,AEo,LEo,vte,BEo,kEo,$E,xEo,je,IE,REo,Tte,SEo,PEo,Wa,$Eo,Fte,IEo,jEo,Cte,NEo,DEo,Mte,qEo,GEo,OEo,na,z1,Ete,XEo,zEo,yI,VEo,WEo,QEo,V1,yte,HEo,UEo,wI,JEo,YEo,KEo,W1,wte,ZEo,e3o,AI,o3o,r3o,t3o,Q1,Ate,a3o,n3o,LI,s3o,l3o,i3o,H1,Lte,d3o,c3o,BI,f3o,m3o,g3o,U1,h3o,Bte,p3o,_3o,kte,u3o,b3o,xte,v3o,T3o,jE,jLe,fd,J1,Rte,NE,F3o,Ste,C3o,NLe,Zo,DE,M3o,md,E3o,Pte,y3o,w3o,$te,A3o,L3o,B3o,qE,k3o,Ite,x3o,R3o,S3o,Wr,GE,P3o,jte,$3o,I3o,gd,j3o,Nte,N3o,D3o,Dte,q3o,G3o,O3o,qte,X3o,z3o,OE,V3o,Ne,XE,W3o,Gte,Q3o,H3o,Qa,U3o,Ote,J3o,Y3o,Xte,K3o,Z3o,zte,eyo,oyo,ryo,D,Y1,Vte,tyo,ayo,kI,nyo,syo,lyo,K1,Wte,iyo,dyo,xI,cyo,fyo,myo,Z1,Qte,gyo,hyo,RI,pyo,_yo,uyo,eb,Hte,byo,vyo,SI,Tyo,Fyo,Cyo,ob,Ute,Myo,Eyo,PI,yyo,wyo,Ayo,rb,Jte,Lyo,Byo,$I,kyo,xyo,Ryo,tb,Yte,Syo,Pyo,II,$yo,Iyo,jyo,ab,Kte,Nyo,Dyo,jI,qyo,Gyo,Oyo,nb,Zte,Xyo,zyo,NI,Vyo,Wyo,Qyo,sb,eae,Hyo,Uyo,DI,Jyo,Yyo,Kyo,lb,oae,Zyo,ewo,qI,owo,rwo,two,ib,rae,awo,nwo,GI,swo,lwo,iwo,db,tae,dwo,cwo,OI,fwo,mwo,gwo,cb,aae,hwo,pwo,XI,_wo,uwo,bwo,fb,nae,vwo,Two,zI,Fwo,Cwo,Mwo,mb,sae,Ewo,ywo,VI,wwo,Awo,Lwo,gb,lae,Bwo,kwo,WI,xwo,Rwo,Swo,hb,iae,Pwo,$wo,QI,Iwo,jwo,Nwo,pb,dae,Dwo,qwo,HI,Gwo,Owo,Xwo,_b,cae,zwo,Vwo,UI,Wwo,Qwo,Hwo,ub,fae,Uwo,Jwo,JI,Ywo,Kwo,Zwo,bb,mae,eAo,oAo,YI,rAo,tAo,aAo,vb,gae,nAo,sAo,KI,lAo,iAo,dAo,Tb,hae,cAo,fAo,ZI,mAo,gAo,hAo,Fb,pae,pAo,_Ao,ej,uAo,bAo,vAo,Cb,_ae,TAo,FAo,oj,CAo,MAo,EAo,Mb,uae,yAo,wAo,rj,AAo,LAo,BAo,Eb,bae,kAo,xAo,tj,RAo,SAo,PAo,yb,vae,$Ao,IAo,aj,jAo,NAo,DAo,wb,Tae,qAo,GAo,nj,OAo,XAo,zAo,Ab,Fae,VAo,WAo,sj,QAo,HAo,UAo,Lb,Cae,JAo,YAo,lj,KAo,ZAo,e6o,Bb,o6o,Mae,r6o,t6o,Eae,a6o,n6o,yae,s6o,l6o,zE,DLe,hd,kb,wae,VE,i6o,Aae,d6o,qLe,er,WE,c6o,pd,f6o,Lae,m6o,g6o,Bae,h6o,p6o,_6o,QE,u6o,kae,b6o,v6o,T6o,Qr,HE,F6o,xae,C6o,M6o,_d,E6o,Rae,y6o,w6o,Sae,A6o,L6o,B6o,Pae,k6o,x6o,UE,R6o,De,JE,S6o,$ae,P6o,$6o,Ha,I6o,Iae,j6o,N6o,jae,D6o,q6o,Nae,G6o,O6o,X6o,R,xb,Dae,z6o,V6o,ij,W6o,Q6o,H6o,Rb,qae,U6o,J6o,dj,Y6o,K6o,Z6o,Sb,Gae,e0o,o0o,cj,r0o,t0o,a0o,Pb,Oae,n0o,s0o,fj,l0o,i0o,d0o,$b,Xae,c0o,f0o,mj,m0o,g0o,h0o,Ib,zae,p0o,_0o,gj,u0o,b0o,v0o,jb,Vae,T0o,F0o,hj,C0o,M0o,E0o,Nb,Wae,y0o,w0o,pj,A0o,L0o,B0o,Db,Qae,k0o,x0o,_j,R0o,S0o,P0o,qb,Hae,$0o,I0o,uj,j0o,N0o,D0o,Gb,Uae,q0o,G0o,bj,O0o,X0o,z0o,Ob,Jae,V0o,W0o,vj,Q0o,H0o,U0o,Xb,Yae,J0o,Y0o,Tj,K0o,Z0o,eLo,zb,Kae,oLo,rLo,Fj,tLo,aLo,nLo,Vb,Zae,sLo,lLo,Cj,iLo,dLo,cLo,Wb,ene,fLo,mLo,Mj,gLo,hLo,pLo,Qb,one,_Lo,uLo,Ej,bLo,vLo,TLo,Hb,rne,FLo,CLo,yj,MLo,ELo,yLo,Ub,tne,wLo,ALo,wj,LLo,BLo,kLo,Jb,ane,xLo,RLo,Aj,SLo,PLo,$Lo,Yb,nne,ILo,jLo,Lj,NLo,DLo,qLo,Kb,sne,GLo,OLo,Bj,XLo,zLo,VLo,Zb,lne,WLo,QLo,kj,HLo,ULo,JLo,e5,ine,YLo,KLo,xj,ZLo,e8o,o8o,o5,dne,r8o,t8o,Rj,a8o,n8o,s8o,r5,cne,l8o,i8o,Sj,d8o,c8o,f8o,t5,fne,m8o,g8o,Pj,h8o,p8o,_8o,a5,mne,u8o,b8o,$j,v8o,T8o,F8o,n5,gne,C8o,M8o,Ij,E8o,y8o,w8o,s5,hne,A8o,L8o,jj,B8o,k8o,x8o,l5,pne,R8o,S8o,Nj,P8o,$8o,I8o,i5,_ne,j8o,N8o,Dj,D8o,q8o,G8o,d5,une,O8o,X8o,qj,z8o,V8o,W8o,c5,bne,Q8o,H8o,Gj,U8o,J8o,Y8o,f5,vne,K8o,Z8o,Oj,eBo,oBo,rBo,m5,Tne,tBo,aBo,Xj,nBo,sBo,lBo,g5,Fne,iBo,dBo,zj,cBo,fBo,mBo,h5,Cne,gBo,hBo,Vj,pBo,_Bo,uBo,p5,bBo,Mne,vBo,TBo,Ene,FBo,CBo,yne,MBo,EBo,YE,GLe,ud,_5,wne,KE,yBo,Ane,wBo,OLe,or,ZE,ABo,bd,LBo,Lne,BBo,kBo,Bne,xBo,RBo,SBo,e3,PBo,kne,$Bo,IBo,jBo,Hr,o3,NBo,xne,DBo,qBo,vd,GBo,Rne,OBo,XBo,Sne,zBo,VBo,WBo,Pne,QBo,HBo,r3,UBo,qe,t3,JBo,$ne,YBo,KBo,Ua,ZBo,Ine,eko,oko,jne,rko,tko,Nne,ako,nko,sko,Dne,u5,qne,lko,iko,Wj,dko,cko,fko,b5,mko,Gne,gko,hko,One,pko,_ko,Xne,uko,bko,a3,XLe,Td,v5,zne,n3,vko,Vne,Tko,zLe,rr,s3,Fko,Fd,Cko,Wne,Mko,Eko,Qne,yko,wko,Ako,l3,Lko,Hne,Bko,kko,xko,Ur,i3,Rko,Une,Sko,Pko,Cd,$ko,Jne,Iko,jko,Yne,Nko,Dko,qko,Kne,Gko,Oko,d3,Xko,Ge,c3,zko,Zne,Vko,Wko,Ja,Qko,ese,Hko,Uko,ose,Jko,Yko,rse,Kko,Zko,exo,be,T5,tse,oxo,rxo,Qj,txo,axo,nxo,F5,ase,sxo,lxo,Hj,ixo,dxo,cxo,Ss,nse,fxo,mxo,Uj,gxo,hxo,Jj,pxo,_xo,uxo,C5,sse,bxo,vxo,Yj,Txo,Fxo,Cxo,la,lse,Mxo,Exo,Kj,yxo,wxo,Zj,Axo,Lxo,eN,Bxo,kxo,xxo,M5,ise,Rxo,Sxo,oN,Pxo,$xo,Ixo,E5,dse,jxo,Nxo,rN,Dxo,qxo,Gxo,y5,cse,Oxo,Xxo,tN,zxo,Vxo,Wxo,w5,fse,Qxo,Hxo,aN,Uxo,Jxo,Yxo,A5,Kxo,mse,Zxo,eRo,gse,oRo,rRo,hse,tRo,aRo,f3,VLe,Md,L5,pse,m3,nRo,_se,sRo,WLe,tr,g3,lRo,Ed,iRo,use,dRo,cRo,bse,fRo,mRo,gRo,h3,hRo,vse,pRo,_Ro,uRo,Jr,p3,bRo,Tse,vRo,TRo,yd,FRo,Fse,CRo,MRo,Cse,ERo,yRo,wRo,Mse,ARo,LRo,_3,BRo,Oe,u3,kRo,Ese,xRo,RRo,Ya,SRo,yse,PRo,$Ro,wse,IRo,jRo,Ase,NRo,DRo,qRo,Lse,B5,Bse,GRo,ORo,nN,XRo,zRo,VRo,k5,WRo,kse,QRo,HRo,xse,URo,JRo,Rse,YRo,KRo,b3,QLe,wd,x5,Sse,v3,ZRo,Pse,eSo,HLe,ar,T3,oSo,Ad,rSo,$se,tSo,aSo,Ise,nSo,sSo,lSo,F3,iSo,jse,dSo,cSo,fSo,Yr,C3,mSo,Nse,gSo,hSo,Ld,pSo,Dse,_So,uSo,qse,bSo,vSo,TSo,Gse,FSo,CSo,M3,MSo,Xe,E3,ESo,Ose,ySo,wSo,Ka,ASo,Xse,LSo,BSo,zse,kSo,xSo,Vse,RSo,SSo,PSo,ao,R5,Wse,$So,ISo,sN,jSo,NSo,DSo,S5,Qse,qSo,GSo,lN,OSo,XSo,zSo,P5,Hse,VSo,WSo,iN,QSo,HSo,USo,$5,Use,JSo,YSo,dN,KSo,ZSo,ePo,I5,Jse,oPo,rPo,cN,tPo,aPo,nPo,j5,Yse,sPo,lPo,fN,iPo,dPo,cPo,N5,Kse,fPo,mPo,mN,gPo,hPo,pPo,D5,_Po,Zse,uPo,bPo,ele,vPo,TPo,ole,FPo,CPo,y3,ULe,Bd,q5,rle,w3,MPo,tle,EPo,JLe,nr,A3,yPo,kd,wPo,ale,APo,LPo,nle,BPo,kPo,xPo,L3,RPo,sle,SPo,PPo,$Po,Kr,B3,IPo,lle,jPo,NPo,xd,DPo,ile,qPo,GPo,dle,OPo,XPo,zPo,cle,VPo,WPo,k3,QPo,ze,x3,HPo,fle,UPo,JPo,Za,YPo,mle,KPo,ZPo,gle,e$o,o$o,hle,r$o,t$o,a$o,Rd,G5,ple,n$o,s$o,gN,l$o,i$o,d$o,O5,_le,c$o,f$o,hN,m$o,g$o,h$o,X5,ule,p$o,_$o,pN,u$o,b$o,v$o,z5,T$o,ble,F$o,C$o,vle,M$o,E$o,Tle,y$o,w$o,R3,YLe,Sd,V5,Fle,S3,A$o,Cle,L$o,KLe,sr,P3,B$o,Pd,k$o,Mle,x$o,R$o,Ele,S$o,P$o,$$o,$3,I$o,yle,j$o,N$o,D$o,Zr,I3,q$o,wle,G$o,O$o,$d,X$o,Ale,z$o,V$o,Lle,W$o,Q$o,H$o,Ble,U$o,J$o,j3,Y$o,Ve,N3,K$o,kle,Z$o,eIo,en,oIo,xle,rIo,tIo,Rle,aIo,nIo,Sle,sIo,lIo,iIo,no,W5,Ple,dIo,cIo,_N,fIo,mIo,gIo,Q5,$le,hIo,pIo,uN,_Io,uIo,bIo,H5,Ile,vIo,TIo,bN,FIo,CIo,MIo,U5,jle,EIo,yIo,vN,wIo,AIo,LIo,J5,Nle,BIo,kIo,TN,xIo,RIo,SIo,Y5,Dle,PIo,$Io,FN,IIo,jIo,NIo,K5,qle,DIo,qIo,CN,GIo,OIo,XIo,Z5,zIo,Gle,VIo,WIo,Ole,QIo,HIo,Xle,UIo,JIo,D3,ZLe,Id,ev,zle,q3,YIo,Vle,KIo,e8e,lr,G3,ZIo,jd,ejo,Wle,ojo,rjo,Qle,tjo,ajo,njo,O3,sjo,Hle,ljo,ijo,djo,et,X3,cjo,Ule,fjo,mjo,Nd,gjo,Jle,hjo,pjo,Yle,_jo,ujo,bjo,Kle,vjo,Tjo,z3,Fjo,We,V3,Cjo,Zle,Mjo,Ejo,on,yjo,eie,wjo,Ajo,oie,Ljo,Bjo,rie,kjo,xjo,Rjo,W3,ov,tie,Sjo,Pjo,MN,$jo,Ijo,jjo,rv,aie,Njo,Djo,EN,qjo,Gjo,Ojo,tv,Xjo,nie,zjo,Vjo,sie,Wjo,Qjo,lie,Hjo,Ujo,Q3,o8e,Dd,av,iie,H3,Jjo,die,Yjo,r8e,ir,U3,Kjo,qd,Zjo,cie,eNo,oNo,fie,rNo,tNo,aNo,J3,nNo,mie,sNo,lNo,iNo,ot,Y3,dNo,gie,cNo,fNo,Gd,mNo,hie,gNo,hNo,pie,pNo,_No,uNo,_ie,bNo,vNo,K3,TNo,Qe,Z3,FNo,uie,CNo,MNo,rn,ENo,bie,yNo,wNo,vie,ANo,LNo,Tie,BNo,kNo,xNo,Od,nv,Fie,RNo,SNo,yN,PNo,$No,INo,sv,Cie,jNo,NNo,wN,DNo,qNo,GNo,lv,Mie,ONo,XNo,AN,zNo,VNo,WNo,iv,QNo,Eie,HNo,UNo,yie,JNo,YNo,wie,KNo,ZNo,ey,t8e,Xd,dv,Aie,oy,eDo,Lie,oDo,a8e,dr,ry,rDo,zd,tDo,Bie,aDo,nDo,kie,sDo,lDo,iDo,ty,dDo,xie,cDo,fDo,mDo,rt,ay,gDo,Rie,hDo,pDo,Vd,_Do,Sie,uDo,bDo,Pie,vDo,TDo,FDo,$ie,CDo,MDo,ny,EDo,He,sy,yDo,Iie,wDo,ADo,tn,LDo,jie,BDo,kDo,Nie,xDo,RDo,Die,SDo,PDo,$Do,Wd,cv,qie,IDo,jDo,LN,NDo,DDo,qDo,fv,Gie,GDo,ODo,BN,XDo,zDo,VDo,mv,Oie,WDo,QDo,kN,HDo,UDo,JDo,gv,YDo,Xie,KDo,ZDo,zie,eqo,oqo,Vie,rqo,tqo,ly,n8e,Qd,hv,Wie,iy,aqo,Qie,nqo,s8e,cr,dy,sqo,Hd,lqo,Hie,iqo,dqo,Uie,cqo,fqo,mqo,cy,gqo,Jie,hqo,pqo,_qo,tt,fy,uqo,Yie,bqo,vqo,Ud,Tqo,Kie,Fqo,Cqo,Zie,Mqo,Eqo,yqo,ede,wqo,Aqo,my,Lqo,Ue,gy,Bqo,ode,kqo,xqo,an,Rqo,rde,Sqo,Pqo,tde,$qo,Iqo,ade,jqo,Nqo,Dqo,nde,pv,sde,qqo,Gqo,xN,Oqo,Xqo,zqo,_v,Vqo,lde,Wqo,Qqo,ide,Hqo,Uqo,dde,Jqo,Yqo,hy,l8e,Jd,uv,cde,py,Kqo,fde,Zqo,i8e,fr,_y,eGo,Yd,oGo,mde,rGo,tGo,gde,aGo,nGo,sGo,uy,lGo,hde,iGo,dGo,cGo,at,by,fGo,pde,mGo,gGo,Kd,hGo,_de,pGo,_Go,ude,uGo,bGo,vGo,bde,TGo,FGo,vy,CGo,Je,Ty,MGo,vde,EGo,yGo,nn,wGo,Tde,AGo,LGo,Fde,BGo,kGo,Cde,xGo,RGo,SGo,Mde,bv,Ede,PGo,$Go,RN,IGo,jGo,NGo,vv,DGo,yde,qGo,GGo,wde,OGo,XGo,Ade,zGo,VGo,Fy,d8e,Zd,Tv,Lde,Cy,WGo,Bde,QGo,c8e,mr,My,HGo,ec,UGo,kde,JGo,YGo,xde,KGo,ZGo,eOo,Ey,oOo,Rde,rOo,tOo,aOo,nt,yy,nOo,Sde,sOo,lOo,oc,iOo,Pde,dOo,cOo,$de,fOo,mOo,gOo,Ide,hOo,pOo,wy,_Oo,Ye,Ay,uOo,jde,bOo,vOo,sn,TOo,Nde,FOo,COo,Dde,MOo,EOo,qde,yOo,wOo,AOo,Ly,Fv,Gde,LOo,BOo,SN,kOo,xOo,ROo,Cv,Ode,SOo,POo,PN,$Oo,IOo,jOo,Mv,NOo,Xde,DOo,qOo,zde,GOo,OOo,Vde,XOo,zOo,By,f8e,rc,Ev,Wde,ky,VOo,Qde,WOo,m8e,gr,xy,QOo,tc,HOo,Hde,UOo,JOo,Ude,YOo,KOo,ZOo,Ry,eXo,Jde,oXo,rXo,tXo,st,Sy,aXo,Yde,nXo,sXo,ac,lXo,Kde,iXo,dXo,Zde,cXo,fXo,mXo,ece,gXo,hXo,Py,pXo,go,$y,_Xo,oce,uXo,bXo,ln,vXo,rce,TXo,FXo,tce,CXo,MXo,ace,EXo,yXo,wXo,B,yv,nce,AXo,LXo,$N,BXo,kXo,xXo,wv,sce,RXo,SXo,IN,PXo,$Xo,IXo,Av,lce,jXo,NXo,jN,DXo,qXo,GXo,Lv,ice,OXo,XXo,NN,zXo,VXo,WXo,Bv,dce,QXo,HXo,DN,UXo,JXo,YXo,kv,cce,KXo,ZXo,qN,ezo,ozo,rzo,xv,fce,tzo,azo,GN,nzo,szo,lzo,Rv,mce,izo,dzo,ON,czo,fzo,mzo,Sv,gce,gzo,hzo,XN,pzo,_zo,uzo,Pv,hce,bzo,vzo,zN,Tzo,Fzo,Czo,$v,pce,Mzo,Ezo,VN,yzo,wzo,Azo,Iv,_ce,Lzo,Bzo,WN,kzo,xzo,Rzo,jv,uce,Szo,Pzo,QN,$zo,Izo,jzo,Nv,bce,Nzo,Dzo,HN,qzo,Gzo,Ozo,Dv,vce,Xzo,zzo,UN,Vzo,Wzo,Qzo,Ps,Tce,Hzo,Uzo,JN,Jzo,Yzo,YN,Kzo,Zzo,eVo,qv,Fce,oVo,rVo,KN,tVo,aVo,nVo,Gv,Cce,sVo,lVo,ZN,iVo,dVo,cVo,Ov,Mce,fVo,mVo,eD,gVo,hVo,pVo,Xv,Ece,_Vo,uVo,oD,bVo,vVo,TVo,zv,yce,FVo,CVo,rD,MVo,EVo,yVo,Vv,wce,wVo,AVo,tD,LVo,BVo,kVo,Wv,Ace,xVo,RVo,aD,SVo,PVo,$Vo,Qv,Lce,IVo,jVo,nD,NVo,DVo,qVo,Hv,Bce,GVo,OVo,sD,XVo,zVo,VVo,Uv,kce,WVo,QVo,lD,HVo,UVo,JVo,Jv,xce,YVo,KVo,iD,ZVo,eWo,oWo,Yv,Rce,rWo,tWo,dD,aWo,nWo,sWo,Kv,Sce,lWo,iWo,cD,dWo,cWo,fWo,Zv,Pce,mWo,gWo,fD,hWo,pWo,_Wo,eT,$ce,uWo,bWo,mD,vWo,TWo,FWo,oT,Ice,CWo,MWo,gD,EWo,yWo,wWo,rT,jce,AWo,LWo,hD,BWo,kWo,xWo,tT,Nce,RWo,SWo,pD,PWo,$Wo,IWo,aT,Dce,jWo,NWo,_D,DWo,qWo,GWo,nT,qce,OWo,XWo,uD,zWo,VWo,WWo,sT,Gce,QWo,HWo,bD,UWo,JWo,YWo,lT,Oce,KWo,ZWo,vD,eQo,oQo,rQo,iT,Xce,tQo,aQo,TD,nQo,sQo,lQo,dT,zce,iQo,dQo,FD,cQo,fQo,mQo,cT,Vce,gQo,hQo,CD,pQo,_Qo,uQo,Wce,bQo,vQo,Iy,g8e,nc,fT,Qce,jy,TQo,Hce,FQo,h8e,hr,Ny,CQo,sc,MQo,Uce,EQo,yQo,Jce,wQo,AQo,LQo,Dy,BQo,Yce,kQo,xQo,RQo,lt,qy,SQo,Kce,PQo,$Qo,lc,IQo,Zce,jQo,NQo,efe,DQo,qQo,GQo,ofe,OQo,XQo,Gy,zQo,ho,Oy,VQo,rfe,WQo,QQo,dn,HQo,tfe,UQo,JQo,afe,YQo,KQo,nfe,ZQo,eHo,oHo,H,mT,sfe,rHo,tHo,MD,aHo,nHo,sHo,gT,lfe,lHo,iHo,ED,dHo,cHo,fHo,hT,ife,mHo,gHo,yD,hHo,pHo,_Ho,pT,dfe,uHo,bHo,wD,vHo,THo,FHo,_T,cfe,CHo,MHo,AD,EHo,yHo,wHo,uT,ffe,AHo,LHo,LD,BHo,kHo,xHo,bT,mfe,RHo,SHo,BD,PHo,$Ho,IHo,vT,gfe,jHo,NHo,kD,DHo,qHo,GHo,TT,hfe,OHo,XHo,xD,zHo,VHo,WHo,FT,pfe,QHo,HHo,RD,UHo,JHo,YHo,CT,_fe,KHo,ZHo,SD,eUo,oUo,rUo,MT,ufe,tUo,aUo,PD,nUo,sUo,lUo,ET,bfe,iUo,dUo,$D,cUo,fUo,mUo,yT,vfe,gUo,hUo,ID,pUo,_Uo,uUo,wT,Tfe,bUo,vUo,jD,TUo,FUo,CUo,AT,Ffe,MUo,EUo,ND,yUo,wUo,AUo,LT,Cfe,LUo,BUo,DD,kUo,xUo,RUo,BT,Mfe,SUo,PUo,qD,$Uo,IUo,jUo,kT,Efe,NUo,DUo,GD,qUo,GUo,OUo,xT,yfe,XUo,zUo,OD,VUo,WUo,QUo,RT,wfe,HUo,UUo,XD,JUo,YUo,KUo,ST,Afe,ZUo,eJo,zD,oJo,rJo,tJo,Lfe,aJo,nJo,Xy,p8e,ic,PT,Bfe,zy,sJo,kfe,lJo,_8e,pr,Vy,iJo,dc,dJo,xfe,cJo,fJo,Rfe,mJo,gJo,hJo,Wy,pJo,Sfe,_Jo,uJo,bJo,it,Qy,vJo,Pfe,TJo,FJo,cc,CJo,$fe,MJo,EJo,Ife,yJo,wJo,AJo,jfe,LJo,BJo,Hy,kJo,po,Uy,xJo,Nfe,RJo,SJo,cn,PJo,Dfe,$Jo,IJo,qfe,jJo,NJo,Gfe,DJo,qJo,GJo,he,$T,Ofe,OJo,XJo,VD,zJo,VJo,WJo,IT,Xfe,QJo,HJo,WD,UJo,JJo,YJo,jT,zfe,KJo,ZJo,QD,eYo,oYo,rYo,NT,Vfe,tYo,aYo,HD,nYo,sYo,lYo,DT,Wfe,iYo,dYo,UD,cYo,fYo,mYo,qT,Qfe,gYo,hYo,JD,pYo,_Yo,uYo,GT,Hfe,bYo,vYo,YD,TYo,FYo,CYo,OT,Ufe,MYo,EYo,KD,yYo,wYo,AYo,XT,Jfe,LYo,BYo,ZD,kYo,xYo,RYo,zT,Yfe,SYo,PYo,eq,$Yo,IYo,jYo,Kfe,NYo,DYo,Jy,u8e,fc,VT,Zfe,Yy,qYo,eme,GYo,b8e,_r,Ky,OYo,mc,XYo,ome,zYo,VYo,rme,WYo,QYo,HYo,Zy,UYo,tme,JYo,YYo,KYo,dt,ew,ZYo,ame,eKo,oKo,gc,rKo,nme,tKo,aKo,sme,nKo,sKo,lKo,lme,iKo,dKo,ow,cKo,_o,rw,fKo,ime,mKo,gKo,fn,hKo,dme,pKo,_Ko,cme,uKo,bKo,fme,vKo,TKo,FKo,mme,WT,gme,CKo,MKo,oq,EKo,yKo,wKo,hme,AKo,LKo,tw,v8e,hc,QT,pme,aw,BKo,_me,kKo,T8e,ur,nw,xKo,pc,RKo,ume,SKo,PKo,bme,$Ko,IKo,jKo,sw,NKo,vme,DKo,qKo,GKo,ct,lw,OKo,Tme,XKo,zKo,_c,VKo,Fme,WKo,QKo,Cme,HKo,UKo,JKo,Mme,YKo,KKo,iw,ZKo,uo,dw,eZo,Eme,oZo,rZo,mn,tZo,yme,aZo,nZo,wme,sZo,lZo,Ame,iZo,dZo,cZo,Y,HT,Lme,fZo,mZo,rq,gZo,hZo,pZo,UT,Bme,_Zo,uZo,tq,bZo,vZo,TZo,JT,kme,FZo,CZo,aq,MZo,EZo,yZo,YT,xme,wZo,AZo,nq,LZo,BZo,kZo,KT,Rme,xZo,RZo,sq,SZo,PZo,$Zo,ZT,Sme,IZo,jZo,lq,NZo,DZo,qZo,e7,Pme,GZo,OZo,iq,XZo,zZo,VZo,o7,$me,WZo,QZo,dq,HZo,UZo,JZo,r7,Ime,YZo,KZo,cq,ZZo,eer,oer,t7,jme,rer,ter,fq,aer,ner,ser,a7,Nme,ler,ier,mq,der,cer,fer,n7,Dme,mer,ger,gq,her,per,_er,s7,qme,uer,ber,hq,ver,Ter,Fer,l7,Gme,Cer,Mer,pq,Eer,yer,wer,i7,Ome,Aer,Ler,_q,Ber,ker,xer,d7,Xme,Rer,Ser,uq,Per,$er,Ier,c7,zme,jer,Ner,bq,Der,qer,Ger,f7,Vme,Oer,Xer,vq,zer,Ver,Wer,m7,Wme,Qer,Her,Tq,Uer,Jer,Yer,g7,Qme,Ker,Zer,Fq,eor,oor,ror,Hme,tor,aor,cw,F8e,uc,h7,Ume,fw,nor,Jme,sor,C8e,br,mw,lor,bc,ior,Yme,dor,cor,Kme,mor,gor,hor,gw,por,Zme,_or,uor,bor,ft,hw,vor,ege,Tor,For,vc,Cor,oge,Mor,Eor,rge,yor,wor,Aor,tge,Lor,Bor,pw,kor,bo,_w,xor,age,Ror,Sor,gn,Por,nge,$or,Ior,sge,jor,Nor,lge,Dor,qor,Gor,pe,p7,ige,Oor,Xor,Cq,zor,Vor,Wor,_7,dge,Qor,Hor,Mq,Uor,Jor,Yor,u7,cge,Kor,Zor,Eq,err,orr,rrr,b7,fge,trr,arr,yq,nrr,srr,lrr,v7,mge,irr,drr,wq,crr,frr,mrr,T7,gge,grr,hrr,Aq,prr,_rr,urr,F7,hge,brr,vrr,Lq,Trr,Frr,Crr,C7,pge,Mrr,Err,Bq,yrr,wrr,Arr,M7,_ge,Lrr,Brr,kq,krr,xrr,Rrr,E7,uge,Srr,Prr,xq,$rr,Irr,jrr,bge,Nrr,Drr,uw,M8e,Tc,y7,vge,bw,qrr,Tge,Grr,E8e,vr,vw,Orr,Fc,Xrr,Fge,zrr,Vrr,Cge,Wrr,Qrr,Hrr,Tw,Urr,Mge,Jrr,Yrr,Krr,mt,Fw,Zrr,Ege,etr,otr,Cc,rtr,yge,ttr,atr,wge,ntr,str,ltr,Age,itr,dtr,Cw,ctr,vo,Mw,ftr,Lge,mtr,gtr,hn,htr,Bge,ptr,_tr,kge,utr,btr,xge,vtr,Ttr,Ftr,X,w7,Rge,Ctr,Mtr,Rq,Etr,ytr,wtr,A7,Sge,Atr,Ltr,Sq,Btr,ktr,xtr,L7,Pge,Rtr,Str,Pq,Ptr,$tr,Itr,B7,$ge,jtr,Ntr,$q,Dtr,qtr,Gtr,k7,Ige,Otr,Xtr,Iq,ztr,Vtr,Wtr,x7,jge,Qtr,Htr,jq,Utr,Jtr,Ytr,R7,Nge,Ktr,Ztr,Nq,ear,oar,rar,S7,Dge,tar,aar,Dq,nar,sar,lar,P7,qge,iar,dar,qq,car,far,mar,$7,Gge,gar,har,Gq,par,_ar,uar,I7,Oge,bar,Tar,Oq,Far,Car,Mar,j7,Xge,Ear,yar,Xq,war,Aar,Lar,N7,zge,Bar,kar,zq,xar,Rar,Sar,D7,Vge,Par,$ar,Vq,Iar,jar,Nar,q7,Wge,Dar,qar,Wq,Gar,Oar,Xar,G7,Qge,zar,Var,Qq,War,Qar,Har,O7,Hge,Uar,Jar,Hq,Yar,Kar,Zar,X7,Uge,enr,onr,Uq,rnr,tnr,anr,z7,Jge,nnr,snr,Jq,lnr,inr,dnr,V7,Yge,cnr,fnr,Yq,mnr,gnr,hnr,W7,Kge,pnr,_nr,Kq,unr,bnr,vnr,Q7,Zge,Tnr,Fnr,Zq,Cnr,Mnr,Enr,H7,ehe,ynr,wnr,eG,Anr,Lnr,Bnr,U7,ohe,knr,xnr,oG,Rnr,Snr,Pnr,J7,rhe,$nr,Inr,rG,jnr,Nnr,Dnr,the,qnr,Gnr,Ew,y8e,Mc,Y7,ahe,yw,Onr,nhe,Xnr,w8e,Tr,ww,znr,Ec,Vnr,she,Wnr,Qnr,lhe,Hnr,Unr,Jnr,Aw,Ynr,ihe,Knr,Znr,esr,gt,Lw,osr,dhe,rsr,tsr,yc,asr,che,nsr,ssr,fhe,lsr,isr,dsr,mhe,csr,fsr,Bw,msr,To,kw,gsr,ghe,hsr,psr,pn,_sr,hhe,usr,bsr,phe,vsr,Tsr,_he,Fsr,Csr,Msr,te,K7,uhe,Esr,ysr,tG,wsr,Asr,Lsr,Z7,bhe,Bsr,ksr,aG,xsr,Rsr,Ssr,eF,vhe,Psr,$sr,nG,Isr,jsr,Nsr,oF,The,Dsr,qsr,sG,Gsr,Osr,Xsr,rF,Fhe,zsr,Vsr,lG,Wsr,Qsr,Hsr,tF,Che,Usr,Jsr,iG,Ysr,Ksr,Zsr,aF,Mhe,elr,olr,dG,rlr,tlr,alr,nF,Ehe,nlr,slr,cG,llr,ilr,dlr,sF,yhe,clr,flr,fG,mlr,glr,hlr,lF,whe,plr,_lr,mG,ulr,blr,vlr,iF,Ahe,Tlr,Flr,gG,Clr,Mlr,Elr,dF,Lhe,ylr,wlr,hG,Alr,Llr,Blr,cF,Bhe,klr,xlr,pG,Rlr,Slr,Plr,fF,khe,$lr,Ilr,_G,jlr,Nlr,Dlr,mF,xhe,qlr,Glr,uG,Olr,Xlr,zlr,gF,Rhe,Vlr,Wlr,bG,Qlr,Hlr,Ulr,hF,She,Jlr,Ylr,vG,Klr,Zlr,eir,Phe,oir,rir,xw,A8e,wc,pF,$he,Rw,tir,Ihe,air,L8e,Fr,Sw,nir,Ac,sir,jhe,lir,iir,Nhe,dir,cir,fir,Pw,mir,Dhe,gir,hir,pir,ht,$w,_ir,qhe,uir,bir,Lc,vir,Ghe,Tir,Fir,Ohe,Cir,Mir,Eir,Xhe,yir,wir,Iw,Air,Fo,jw,Lir,zhe,Bir,kir,_n,xir,Vhe,Rir,Sir,Whe,Pir,$ir,Qhe,Iir,jir,Nir,Hhe,_F,Uhe,Dir,qir,TG,Gir,Oir,Xir,Jhe,zir,Vir,Nw,B8e,Bc,uF,Yhe,Dw,Wir,Khe,Qir,k8e,Cr,qw,Hir,kc,Uir,Zhe,Jir,Yir,epe,Kir,Zir,edr,Gw,odr,ope,rdr,tdr,adr,pt,Ow,ndr,rpe,sdr,ldr,xc,idr,tpe,ddr,cdr,ape,fdr,mdr,gdr,npe,hdr,pdr,Xw,_dr,Co,zw,udr,spe,bdr,vdr,un,Tdr,lpe,Fdr,Cdr,ipe,Mdr,Edr,dpe,ydr,wdr,Adr,K,bF,cpe,Ldr,Bdr,FG,kdr,xdr,Rdr,vF,fpe,Sdr,Pdr,CG,$dr,Idr,jdr,TF,mpe,Ndr,Ddr,MG,qdr,Gdr,Odr,FF,gpe,Xdr,zdr,EG,Vdr,Wdr,Qdr,CF,hpe,Hdr,Udr,yG,Jdr,Ydr,Kdr,MF,ppe,Zdr,ecr,wG,ocr,rcr,tcr,EF,_pe,acr,ncr,AG,scr,lcr,icr,yF,upe,dcr,ccr,LG,fcr,mcr,gcr,wF,bpe,hcr,pcr,BG,_cr,ucr,bcr,AF,vpe,vcr,Tcr,kG,Fcr,Ccr,Mcr,LF,Tpe,Ecr,ycr,xG,wcr,Acr,Lcr,BF,Fpe,Bcr,kcr,RG,xcr,Rcr,Scr,kF,Cpe,Pcr,$cr,SG,Icr,jcr,Ncr,xF,Mpe,Dcr,qcr,PG,Gcr,Ocr,Xcr,RF,Epe,zcr,Vcr,$G,Wcr,Qcr,Hcr,SF,ype,Ucr,Jcr,IG,Ycr,Kcr,Zcr,PF,wpe,efr,ofr,jG,rfr,tfr,afr,$F,Ape,nfr,sfr,NG,lfr,ifr,dfr,IF,Lpe,cfr,ffr,DG,mfr,gfr,hfr,jF,Bpe,pfr,_fr,qG,ufr,bfr,vfr,kpe,Tfr,Ffr,Vw,x8e,Rc,NF,xpe,Ww,Cfr,Rpe,Mfr,R8e,Mr,Qw,Efr,Sc,yfr,Spe,wfr,Afr,Ppe,Lfr,Bfr,kfr,Hw,xfr,$pe,Rfr,Sfr,Pfr,_t,Uw,$fr,Ipe,Ifr,jfr,Pc,Nfr,jpe,Dfr,qfr,Npe,Gfr,Ofr,Xfr,Dpe,zfr,Vfr,Jw,Wfr,Mo,Yw,Qfr,qpe,Hfr,Ufr,bn,Jfr,Gpe,Yfr,Kfr,Ope,Zfr,emr,Xpe,omr,rmr,tmr,Z,DF,zpe,amr,nmr,GG,smr,lmr,imr,qF,Vpe,dmr,cmr,OG,fmr,mmr,gmr,GF,Wpe,hmr,pmr,XG,_mr,umr,bmr,OF,Qpe,vmr,Tmr,zG,Fmr,Cmr,Mmr,XF,Hpe,Emr,ymr,VG,wmr,Amr,Lmr,zF,Upe,Bmr,kmr,WG,xmr,Rmr,Smr,VF,Jpe,Pmr,$mr,QG,Imr,jmr,Nmr,WF,Ype,Dmr,qmr,HG,Gmr,Omr,Xmr,QF,Kpe,zmr,Vmr,UG,Wmr,Qmr,Hmr,HF,Zpe,Umr,Jmr,JG,Ymr,Kmr,Zmr,UF,e_e,egr,ogr,YG,rgr,tgr,agr,JF,o_e,ngr,sgr,KG,lgr,igr,dgr,YF,r_e,cgr,fgr,ZG,mgr,ggr,hgr,KF,t_e,pgr,_gr,eO,ugr,bgr,vgr,ZF,a_e,Tgr,Fgr,oO,Cgr,Mgr,Egr,e9,n_e,ygr,wgr,rO,Agr,Lgr,Bgr,o9,s_e,kgr,xgr,tO,Rgr,Sgr,Pgr,r9,l_e,$gr,Igr,aO,jgr,Ngr,Dgr,t9,i_e,qgr,Ggr,nO,Ogr,Xgr,zgr,d_e,Vgr,Wgr,Kw,S8e,$c,a9,c_e,Zw,Qgr,f_e,Hgr,P8e,Er,eA,Ugr,Ic,Jgr,m_e,Ygr,Kgr,g_e,Zgr,ehr,ohr,oA,rhr,h_e,thr,ahr,nhr,ut,rA,shr,p_e,lhr,ihr,jc,dhr,__e,chr,fhr,u_e,mhr,ghr,hhr,b_e,phr,_hr,tA,uhr,Eo,aA,bhr,v_e,vhr,Thr,vn,Fhr,T_e,Chr,Mhr,F_e,Ehr,yhr,C_e,whr,Ahr,Lhr,M_e,n9,E_e,Bhr,khr,sO,xhr,Rhr,Shr,y_e,Phr,$hr,nA,$8e,Nc,s9,w_e,sA,Ihr,A_e,jhr,I8e,yr,lA,Nhr,Dc,Dhr,L_e,qhr,Ghr,B_e,Ohr,Xhr,zhr,iA,Vhr,k_e,Whr,Qhr,Hhr,bt,dA,Uhr,x_e,Jhr,Yhr,qc,Khr,R_e,Zhr,epr,S_e,opr,rpr,tpr,P_e,apr,npr,cA,spr,yo,fA,lpr,$_e,ipr,dpr,Tn,cpr,I_e,fpr,mpr,j_e,gpr,hpr,N_e,ppr,_pr,upr,D_e,l9,q_e,bpr,vpr,lO,Tpr,Fpr,Cpr,G_e,Mpr,Epr,mA,j8e,Gc,i9,O_e,gA,ypr,X_e,wpr,N8e,wr,hA,Apr,Oc,Lpr,z_e,Bpr,kpr,V_e,xpr,Rpr,Spr,pA,Ppr,W_e,$pr,Ipr,jpr,vt,_A,Npr,Q_e,Dpr,qpr,Xc,Gpr,H_e,Opr,Xpr,U_e,zpr,Vpr,Wpr,J_e,Qpr,Hpr,uA,Upr,wo,bA,Jpr,Y_e,Ypr,Kpr,Fn,Zpr,K_e,e_r,o_r,Z_e,r_r,t_r,eue,a_r,n_r,s_r,V,d9,oue,l_r,i_r,iO,d_r,c_r,f_r,c9,rue,m_r,g_r,dO,h_r,p_r,__r,f9,tue,u_r,b_r,cO,v_r,T_r,F_r,m9,aue,C_r,M_r,fO,E_r,y_r,w_r,g9,nue,A_r,L_r,mO,B_r,k_r,x_r,h9,sue,R_r,S_r,gO,P_r,$_r,I_r,p9,lue,j_r,N_r,hO,D_r,q_r,G_r,_9,iue,O_r,X_r,pO,z_r,V_r,W_r,u9,due,Q_r,H_r,_O,U_r,J_r,Y_r,b9,cue,K_r,Z_r,uO,eur,our,rur,v9,fue,tur,aur,bO,nur,sur,lur,T9,mue,iur,dur,vO,cur,fur,mur,F9,gue,gur,hur,TO,pur,_ur,uur,C9,hue,bur,vur,FO,Tur,Fur,Cur,M9,pue,Mur,Eur,CO,yur,wur,Aur,E9,_ue,Lur,Bur,MO,kur,xur,Rur,y9,uue,Sur,Pur,EO,$ur,Iur,jur,w9,bue,Nur,Dur,yO,qur,Gur,Our,A9,vue,Xur,zur,wO,Vur,Wur,Qur,L9,Tue,Hur,Uur,AO,Jur,Yur,Kur,B9,Fue,Zur,e2r,LO,o2r,r2r,t2r,k9,Cue,a2r,n2r,BO,s2r,l2r,i2r,x9,Mue,d2r,c2r,kO,f2r,m2r,g2r,R9,Eue,h2r,p2r,xO,_2r,u2r,b2r,yue,v2r,T2r,vA,D8e,zc,S9,wue,TA,F2r,Aue,C2r,q8e,Ar,FA,M2r,Vc,E2r,Lue,y2r,w2r,Bue,A2r,L2r,B2r,CA,k2r,kue,x2r,R2r,S2r,Tt,MA,P2r,xue,$2r,I2r,Wc,j2r,Rue,N2r,D2r,Sue,q2r,G2r,O2r,Pue,X2r,z2r,EA,V2r,Ao,yA,W2r,$ue,Q2r,H2r,Cn,U2r,Iue,J2r,Y2r,jue,K2r,Z2r,Nue,e1r,o1r,r1r,Mn,P9,Due,t1r,a1r,RO,n1r,s1r,l1r,$9,que,i1r,d1r,SO,c1r,f1r,m1r,I9,Gue,g1r,h1r,PO,p1r,_1r,u1r,j9,Oue,b1r,v1r,$O,T1r,F1r,C1r,Xue,M1r,E1r,wA,G8e,Qc,N9,zue,AA,y1r,Vue,w1r,O8e,Lr,LA,A1r,Hc,L1r,Wue,B1r,k1r,Que,x1r,R1r,S1r,BA,P1r,Hue,$1r,I1r,j1r,Ft,kA,N1r,Uue,D1r,q1r,Uc,G1r,Jue,O1r,X1r,Yue,z1r,V1r,W1r,Kue,Q1r,H1r,xA,U1r,Lo,RA,J1r,Zue,Y1r,K1r,En,Z1r,e2e,ebr,obr,o2e,rbr,tbr,r2e,abr,nbr,sbr,fe,D9,t2e,lbr,ibr,IO,dbr,cbr,fbr,q9,a2e,mbr,gbr,jO,hbr,pbr,_br,G9,n2e,ubr,bbr,NO,vbr,Tbr,Fbr,O9,s2e,Cbr,Mbr,DO,Ebr,ybr,wbr,X9,l2e,Abr,Lbr,qO,Bbr,kbr,xbr,z9,i2e,Rbr,Sbr,GO,Pbr,$br,Ibr,V9,d2e,jbr,Nbr,OO,Dbr,qbr,Gbr,W9,c2e,Obr,Xbr,XO,zbr,Vbr,Wbr,Q9,f2e,Qbr,Hbr,zO,Ubr,Jbr,Ybr,H9,m2e,Kbr,Zbr,VO,e5r,o5r,r5r,U9,g2e,t5r,a5r,WO,n5r,s5r,l5r,h2e,i5r,d5r,SA,X8e,Jc,J9,p2e,PA,c5r,_2e,f5r,z8e,Br,$A,m5r,Yc,g5r,u2e,h5r,p5r,b2e,_5r,u5r,b5r,IA,v5r,v2e,T5r,F5r,C5r,Ct,jA,M5r,T2e,E5r,y5r,Kc,w5r,F2e,A5r,L5r,C2e,B5r,k5r,x5r,M2e,R5r,S5r,NA,P5r,Bo,DA,$5r,E2e,I5r,j5r,yn,N5r,y2e,D5r,q5r,w2e,G5r,O5r,A2e,X5r,z5r,V5r,ve,Y9,L2e,W5r,Q5r,QO,H5r,U5r,J5r,K9,B2e,Y5r,K5r,HO,Z5r,evr,ovr,Z9,k2e,rvr,tvr,UO,avr,nvr,svr,eC,x2e,lvr,ivr,JO,dvr,cvr,fvr,oC,R2e,mvr,gvr,YO,hvr,pvr,_vr,rC,S2e,uvr,bvr,KO,vvr,Tvr,Fvr,tC,P2e,Cvr,Mvr,ZO,Evr,yvr,wvr,aC,$2e,Avr,Lvr,eX,Bvr,kvr,xvr,nC,I2e,Rvr,Svr,oX,Pvr,$vr,Ivr,j2e,jvr,Nvr,qA,V8e,Zc,sC,N2e,GA,Dvr,D2e,qvr,W8e,kr,OA,Gvr,ef,Ovr,q2e,Xvr,zvr,G2e,Vvr,Wvr,Qvr,XA,Hvr,O2e,Uvr,Jvr,Yvr,Mt,zA,Kvr,X2e,Zvr,eTr,of,oTr,z2e,rTr,tTr,V2e,aTr,nTr,sTr,W2e,lTr,iTr,VA,dTr,ko,WA,cTr,Q2e,fTr,mTr,wn,gTr,H2e,hTr,pTr,U2e,_Tr,uTr,J2e,bTr,vTr,TTr,Te,lC,Y2e,FTr,CTr,rX,MTr,ETr,yTr,iC,K2e,wTr,ATr,tX,LTr,BTr,kTr,dC,Z2e,xTr,RTr,aX,STr,PTr,$Tr,cC,e1e,ITr,jTr,nX,NTr,DTr,qTr,fC,o1e,GTr,OTr,sX,XTr,zTr,VTr,mC,r1e,WTr,QTr,lX,HTr,UTr,JTr,gC,t1e,YTr,KTr,iX,ZTr,e7r,o7r,hC,a1e,r7r,t7r,dX,a7r,n7r,s7r,pC,n1e,l7r,i7r,cX,d7r,c7r,f7r,s1e,m7r,g7r,QA,Q8e,rf,_C,l1e,HA,h7r,i1e,p7r,H8e,xr,UA,_7r,tf,u7r,d1e,b7r,v7r,c1e,T7r,F7r,C7r,JA,M7r,f1e,E7r,y7r,w7r,Et,YA,A7r,m1e,L7r,B7r,af,k7r,g1e,x7r,R7r,h1e,S7r,P7r,$7r,p1e,I7r,j7r,KA,N7r,xo,ZA,D7r,_1e,q7r,G7r,An,O7r,u1e,X7r,z7r,b1e,V7r,W7r,v1e,Q7r,H7r,U7r,Fe,uC,T1e,J7r,Y7r,fX,K7r,Z7r,eFr,bC,F1e,oFr,rFr,mX,tFr,aFr,nFr,vC,C1e,sFr,lFr,gX,iFr,dFr,cFr,TC,M1e,fFr,mFr,hX,gFr,hFr,pFr,FC,E1e,_Fr,uFr,pX,bFr,vFr,TFr,CC,y1e,FFr,CFr,_X,MFr,EFr,yFr,MC,w1e,wFr,AFr,uX,LFr,BFr,kFr,EC,A1e,xFr,RFr,bX,SFr,PFr,$Fr,yC,L1e,IFr,jFr,vX,NFr,DFr,qFr,B1e,GFr,OFr,e6,U8e,nf,wC,k1e,o6,XFr,x1e,zFr,J8e,Rr,r6,VFr,sf,WFr,R1e,QFr,HFr,S1e,UFr,JFr,YFr,t6,KFr,P1e,ZFr,e9r,o9r,yt,a6,r9r,$1e,t9r,a9r,lf,n9r,I1e,s9r,l9r,j1e,i9r,d9r,c9r,N1e,f9r,m9r,n6,g9r,Ro,s6,h9r,D1e,p9r,_9r,Ln,u9r,q1e,b9r,v9r,G1e,T9r,F9r,O1e,C9r,M9r,E9r,Ce,AC,X1e,y9r,w9r,TX,A9r,L9r,B9r,LC,z1e,k9r,x9r,FX,R9r,S9r,P9r,BC,V1e,$9r,I9r,CX,j9r,N9r,D9r,kC,W1e,q9r,G9r,MX,O9r,X9r,z9r,xC,Q1e,V9r,W9r,EX,Q9r,H9r,U9r,RC,H1e,J9r,Y9r,yX,K9r,Z9r,eCr,SC,U1e,oCr,rCr,wX,tCr,aCr,nCr,PC,J1e,sCr,lCr,AX,iCr,dCr,cCr,$C,Y1e,fCr,mCr,LX,gCr,hCr,pCr,K1e,_Cr,uCr,l6,Y8e,df,IC,Z1e,i6,bCr,ebe,vCr,K8e,Sr,d6,TCr,cf,FCr,obe,CCr,MCr,rbe,ECr,yCr,wCr,c6,ACr,tbe,LCr,BCr,kCr,wt,f6,xCr,abe,RCr,SCr,ff,PCr,nbe,$Cr,ICr,sbe,jCr,NCr,DCr,lbe,qCr,GCr,m6,OCr,So,g6,XCr,ibe,zCr,VCr,Bn,WCr,dbe,QCr,HCr,cbe,UCr,JCr,fbe,YCr,KCr,ZCr,so,jC,mbe,e4r,o4r,BX,r4r,t4r,a4r,NC,gbe,n4r,s4r,kX,l4r,i4r,d4r,DC,hbe,c4r,f4r,xX,m4r,g4r,h4r,qC,pbe,p4r,_4r,RX,u4r,b4r,v4r,GC,_be,T4r,F4r,SX,C4r,M4r,E4r,OC,ube,y4r,w4r,PX,A4r,L4r,B4r,XC,bbe,k4r,x4r,$X,R4r,S4r,P4r,vbe,$4r,I4r,h6,Z8e,mf,zC,Tbe,p6,j4r,Fbe,N4r,eBe,Pr,_6,D4r,gf,q4r,Cbe,G4r,O4r,Mbe,X4r,z4r,V4r,u6,W4r,Ebe,Q4r,H4r,U4r,At,b6,J4r,ybe,Y4r,K4r,hf,Z4r,wbe,eMr,oMr,Abe,rMr,tMr,aMr,Lbe,nMr,sMr,v6,lMr,Po,T6,iMr,Bbe,dMr,cMr,kn,fMr,kbe,mMr,gMr,xbe,hMr,pMr,Rbe,_Mr,uMr,bMr,lo,VC,Sbe,vMr,TMr,IX,FMr,CMr,MMr,WC,Pbe,EMr,yMr,jX,wMr,AMr,LMr,QC,$be,BMr,kMr,NX,xMr,RMr,SMr,HC,Ibe,PMr,$Mr,DX,IMr,jMr,NMr,UC,jbe,DMr,qMr,qX,GMr,OMr,XMr,JC,Nbe,zMr,VMr,GX,WMr,QMr,HMr,YC,Dbe,UMr,JMr,OX,YMr,KMr,ZMr,qbe,eEr,oEr,F6,oBe,pf,KC,Gbe,C6,rEr,Obe,tEr,rBe,$r,M6,aEr,_f,nEr,Xbe,sEr,lEr,zbe,iEr,dEr,cEr,E6,fEr,Vbe,mEr,gEr,hEr,Lt,y6,pEr,Wbe,_Er,uEr,uf,bEr,Qbe,vEr,TEr,Hbe,FEr,CEr,MEr,Ube,EEr,yEr,w6,wEr,$o,A6,AEr,Jbe,LEr,BEr,xn,kEr,Ybe,xEr,REr,Kbe,SEr,PEr,Zbe,$Er,IEr,jEr,e5e,ZC,o5e,NEr,DEr,XX,qEr,GEr,OEr,r5e,XEr,zEr,L6,tBe,bf,e4,t5e,B6,VEr,a5e,WEr,aBe,Ir,k6,QEr,vf,HEr,n5e,UEr,JEr,s5e,YEr,KEr,ZEr,x6,e3r,l5e,o3r,r3r,t3r,Bt,R6,a3r,i5e,n3r,s3r,Tf,l3r,d5e,i3r,d3r,c5e,c3r,f3r,m3r,f5e,g3r,h3r,S6,p3r,Io,P6,_3r,m5e,u3r,b3r,Rn,v3r,g5e,T3r,F3r,h5e,C3r,M3r,p5e,E3r,y3r,w3r,$6,o4,_5e,A3r,L3r,zX,B3r,k3r,x3r,r4,u5e,R3r,S3r,VX,P3r,$3r,I3r,b5e,j3r,N3r,I6,nBe,Ff,t4,v5e,j6,D3r,T5e,q3r,sBe,jr,N6,G3r,Cf,O3r,F5e,X3r,z3r,C5e,V3r,W3r,Q3r,D6,H3r,M5e,U3r,J3r,Y3r,kt,q6,K3r,E5e,Z3r,eyr,Mf,oyr,y5e,ryr,tyr,w5e,ayr,nyr,syr,A5e,lyr,iyr,G6,dyr,jo,O6,cyr,L5e,fyr,myr,Sn,gyr,B5e,hyr,pyr,k5e,_yr,uyr,x5e,byr,vyr,Tyr,R5e,a4,S5e,Fyr,Cyr,WX,Myr,Eyr,yyr,P5e,wyr,Ayr,X6,lBe;return ce=new z({}),$a=new w({props:{code:'model = AutoModel.from_pretrained("bert-base-cased"),',highlighted:'model = AutoModel.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)'}}),sM=new z({}),lM=new w({props:{code:`from transformers import AutoConfig, AutoModel

AutoConfig.register("new-model", NewModelConfig)
AutoModel.register(NewModelConfig, NewModel),`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModel

AutoConfig.register(<span class="hljs-string">&quot;new-model&quot;</span>, NewModelConfig)
AutoModel.register(NewModelConfig, NewModel)`}}),kf=new Lyr({props:{warning:"&lcub;true}",$$slots:{default:[I_t]},$$scope:{ctx:wi}}}),iM=new z({}),dM=new E({props:{name:"class transformers.AutoConfig",anchor:"transformers.AutoConfig",parameters:[],source:"https://github.com/huggingface/transformers/blob/pr_15297/src/transformers/models/auto/configuration_auto.py#L515"}}),mM=new E({props:{name:"from_pretrained",anchor:"transformers.AutoConfig.from_pretrained",parameters:[{name:"pretrained_model_name_or_path",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15297/src/transformers/models/auto/configuration_auto.py#L538",parametersDescription:[{anchor:"transformers.AutoConfig.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model configuration hosted inside a model repo on
huggingface.co. Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or
namespaced under a user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing a configuration file saved using the
<a href="/docs/transformers/pr_15297/en/main_classes/configuration#transformers.PretrainedConfig.save_pretrained">save_pretrained()</a> method, or the <a href="/docs/transformers/pr_15297/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> method,
e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a saved configuration JSON <em>file</em>, e.g.,
<code>./my_model_directory/configuration.json</code>.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.AutoConfig.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.AutoConfig.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download the model weights and configuration files and override the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.AutoConfig.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.AutoConfig.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.AutoConfig.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.AutoConfig.from_pretrained.return_unused_kwargs",description:`<strong>return_unused_kwargs</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
If <code>False</code>, then this function returns just the final configuration object.</p>
<p>If <code>True</code>, then this functions returns a <code>Tuple(config, unused_kwargs)</code> where <em>unused_kwargs</em> is a
dictionary consisting of the key/value pairs whose keys are not configuration attributes: i.e., the
part of <code>kwargs</code> which has not been used to update <code>config</code> and is otherwise ignored.`,name:"return_unused_kwargs"},{anchor:"transformers.AutoConfig.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.AutoConfig.from_pretrained.kwargs(additional",description:`<strong>kwargs(additional</strong> keyword arguments, <em>optional</em>) &#x2014;
The values in kwargs of any keys which are configuration attributes will be used to override the loaded
values. Behavior concerning key/value pairs whose keys are <em>not</em> configuration attributes is controlled
by the <code>return_unused_kwargs</code> keyword parameter.`,name:"kwargs(additional"}]}}),gM=new w({props:{code:`from transformers import AutoConfig

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-uncased")

# Download configuration from huggingface.co (user-uploaded) and cache.
config = AutoConfig.from_pretrained("dbmdz/bert-base-german-cased")

# If configuration file is in a directory (e.g., was saved using *save_pretrained('./test/saved_model/')*).
config = AutoConfig.from_pretrained("./test/bert_saved_model/")

# Load a specific configuration file.
config = AutoConfig.from_pretrained("./test/bert_saved_model/my_configuration.json")

# Change some config attributes when loading a pretrained config.
config = AutoConfig.from_pretrained("bert-base-uncased", output_attentions=True, foo=False)
config.output_attentions

config, unused_kwargs = AutoConfig.from_pretrained(
    "bert-base-uncased", output_attentions=True, foo=False, return_unused_kwargs=True
)
config.output_attentions

config.unused_kwargs,`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-uncased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co (user-uploaded) and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;dbmdz/bert-base-german-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># If configuration file is in a directory (e.g., was saved using *save_pretrained(&#x27;./test/saved_model/&#x27;)*).</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./test/bert_saved_model/&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Load a specific configuration file.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./test/bert_saved_model/my_configuration.json&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Change some config attributes when loading a pretrained config.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-uncased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>, foo=<span class="hljs-literal">False</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>config, unused_kwargs = AutoConfig.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;bert-base-uncased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>, foo=<span class="hljs-literal">False</span>, return_unused_kwargs=<span class="hljs-literal">True</span>
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>config.unused_kwargs
{<span class="hljs-string">&#x27;foo&#x27;</span>: <span class="hljs-literal">False</span>}`}}),hM=new E({props:{name:"register",anchor:"transformers.AutoConfig.register",parameters:[{name:"model_type",val:""},{name:"config",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15297/src/transformers/models/auto/configuration_auto.py#L660",parametersDescription:[{anchor:"transformers.AutoConfig.register.model_type",description:"<strong>model_type</strong> (<code>str</code>) &#x2014; The model type like &#x201C;bert&#x201D; or &#x201C;gpt&#x201D;.",name:"model_type"},{anchor:"transformers.AutoConfig.register.config",description:'<strong>config</strong> (<a href="/docs/transformers/pr_15297/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014; The config to register.',name:"config"}]}}),pM=new z({}),_M=new E({props:{name:"class transformers.AutoTokenizer",anchor:"transformers.AutoTokenizer",parameters:[],source:"https://github.com/huggingface/transformers/blob/pr_15297/src/transformers/models/auto/tokenization_auto.py#L352"}}),vM=new E({props:{name:"from_pretrained",anchor:"transformers.AutoTokenizer.from_pretrained",parameters:[{name:"pretrained_model_name_or_path",val:""},{name:"*inputs",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15297/src/transformers/models/auto/tokenization_auto.py#L366",parametersDescription:[{anchor:"transformers.AutoTokenizer.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a predefined tokenizer hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing vocabulary files required by the tokenizer, for instance saved
using the <a href="/docs/transformers/pr_15297/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.save_pretrained">save_pretrained()</a> method, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a single saved vocabulary file if and only if the tokenizer only requires a
single vocabulary file (like Bert or XLNet), e.g.: <code>./my_model_directory/vocab.txt</code>. (Not
applicable to all derived classes)</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.AutoTokenizer.from_pretrained.inputs",description:`<strong>inputs</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the Tokenizer <code>__init__()</code> method.`,name:"inputs"},{anchor:"transformers.AutoTokenizer.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15297/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
The configuration object used to dertermine the tokenizer class to instantiate.`,name:"config"},{anchor:"transformers.AutoTokenizer.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.AutoTokenizer.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download the model weights and configuration files and override the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.AutoTokenizer.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.AutoTokenizer.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.AutoTokenizer.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.AutoTokenizer.from_pretrained.subfolder",description:`<strong>subfolder</strong> (<code>str</code>, <em>optional</em>) &#x2014;
In case the relevant files are located inside a subfolder of the model repo on huggingface.co (e.g. for
facebook/rag-token-base), specify it here.`,name:"subfolder"},{anchor:"transformers.AutoTokenizer.from_pretrained.use_fast",description:`<strong>use_fast</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to try to load the fast version of the tokenizer.`,name:"use_fast"},{anchor:"transformers.AutoTokenizer.from_pretrained.tokenizer_type",description:`<strong>tokenizer_type</strong> (<code>str</code>, <em>optional</em>) &#x2014;
Tokenizer type to be loaded.`,name:"tokenizer_type"},{anchor:"transformers.AutoTokenizer.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.AutoTokenizer.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Will be passed to the Tokenizer <code>__init__()</code> method. Can be used to set special tokens like
<code>bos_token</code>, <code>eos_token</code>, <code>unk_token</code>, <code>sep_token</code>, <code>pad_token</code>, <code>cls_token</code>, <code>mask_token</code>,
<code>additional_special_tokens</code>. See parameters in the <code>__init__()</code> for more details.`,name:"kwargs"}]}}),TM=new w({props:{code:`from transformers import AutoTokenizer

# Download vocabulary from huggingface.co and cache.
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

# Download vocabulary from huggingface.co (user-uploaded) and cache.
tokenizer = AutoTokenizer.from_pretrained("dbmdz/bert-base-german-cased")

# If vocabulary files are in a directory (e.g. tokenizer was saved using *save_pretrained('./test/saved_model/')*)
tokenizer = AutoTokenizer.from_pretrained("./test/bert_saved_model/"),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download vocabulary from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;bert-base-uncased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download vocabulary from huggingface.co (user-uploaded) and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;dbmdz/bert-base-german-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># If vocabulary files are in a directory (e.g. tokenizer was saved using *save_pretrained(&#x27;./test/saved_model/&#x27;)*)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;./test/bert_saved_model/&quot;</span>)`}}),FM=new E({props:{name:"register",anchor:"transformers.AutoTokenizer.register",parameters:[{name:"config_class",val:""},{name:"slow_tokenizer_class",val:" = None"},{name:"fast_tokenizer_class",val:" = None"}],source:"https://github.com/huggingface/transformers/blob/pr_15297/src/transformers/models/auto/tokenization_auto.py#L562",parametersDescription:[{anchor:"transformers.AutoTokenizer.register.config_class",description:`<strong>config_class</strong> (<a href="/docs/transformers/pr_15297/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The configuration corresponding to the model to register.`,name:"config_class"},{anchor:"transformers.AutoTokenizer.register.slow_tokenizer_class",description:`<strong>slow_tokenizer_class</strong> (<code>PretrainedTokenizer</code>, <em>optional</em>) &#x2014;
The slow tokenizer to register.`,name:"slow_tokenizer_class"},{anchor:"transformers.AutoTokenizer.register.slow_tokenizer_class",description:`<strong>slow_tokenizer_class</strong> (<code>PretrainedTokenizerFast</code>, <em>optional</em>) &#x2014;
The fast tokenizer to register.`,name:"slow_tokenizer_class"}]}}),CM=new z({}),MM=new E({props:{name:"class transformers.AutoFeatureExtractor",anchor:"transformers.AutoFeatureExtractor",parameters:[],source:"https://github.com/huggingface/transformers/blob/pr_15297/src/transformers/models/auto/feature_extraction_auto.py#L169"}}),wM=new E({props:{name:"from_pretrained",anchor:"transformers.AutoFeatureExtractor.from_pretrained",parameters:[{name:"pretrained_model_name_or_path",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15297/src/transformers/models/auto/feature_extraction_auto.py#L183",parametersDescription:[{anchor:"transformers.AutoFeatureExtractor.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
This can be either:</p>
<ul>
<li>a string, the <em>model id</em> of a pretrained feature_extractor hosted inside a model repo on
huggingface.co. Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or
namespaced under a user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>a path to a <em>directory</em> containing a feature extractor file saved using the
<a href="/docs/transformers/pr_15297/en/main_classes/feature_extractor#transformers.FeatureExtractionMixin.save_pretrained">save_pretrained()</a> method, e.g.,
<code>./my_model_directory/</code>.</li>
<li>a path or url to a saved feature extractor JSON <em>file</em>, e.g.,
<code>./my_model_directory/preprocessor_config.json</code>.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.AutoFeatureExtractor.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model feature extractor should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.AutoFeatureExtractor.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force to (re-)download the feature extractor files and override the cached versions
if they exist.`,name:"force_download"},{anchor:"transformers.AutoFeatureExtractor.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received file. Attempts to resume the download if such a file
exists.`,name:"resume_download"},{anchor:"transformers.AutoFeatureExtractor.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}.</code> The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.AutoFeatureExtractor.from_pretrained.use_auth_token",description:`<strong>use_auth_token</strong> (<code>str</code> or <em>bool</em>, <em>optional</em>) &#x2014;
The token to use as HTTP bearer authorization for remote files. If <code>True</code>, will use the token generated
when running <code>transformers-cli login</code> (stored in <code>~/.huggingface</code>).`,name:"use_auth_token"},{anchor:"transformers.AutoFeatureExtractor.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.AutoFeatureExtractor.from_pretrained.return_unused_kwargs",description:`<strong>return_unused_kwargs</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
If <code>False</code>, then this function returns just the final feature extractor object. If <code>True</code>, then this
functions returns a <code>Tuple(feature_extractor, unused_kwargs)</code> where <em>unused_kwargs</em> is a dictionary
consisting of the key/value pairs whose keys are not feature extractor attributes: i.e., the part of
<code>kwargs</code> which has not been used to update <code>feature_extractor</code> and is otherwise ignored.`,name:"return_unused_kwargs"},{anchor:"transformers.AutoFeatureExtractor.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.AutoFeatureExtractor.from_pretrained.kwargs",description:`<strong>kwargs</strong> (<code>Dict[str, Any]</code>, <em>optional</em>) &#x2014;
The values in kwargs of any keys which are feature extractor attributes will be used to override the
loaded values. Behavior concerning key/value pairs whose keys are <em>not</em> feature extractor attributes is
controlled by the <code>return_unused_kwargs</code> keyword parameter.`,name:"kwargs"}]}}),dh=new Lyr({props:{$$slots:{default:[j_t]},$$scope:{ctx:wi}}}),AM=new w({props:{code:`from transformers import AutoFeatureExtractor

# Download feature extractor from huggingface.co and cache.
feature_extractor = AutoFeatureExtractor.from_pretrained("facebook/wav2vec2-base-960h")

# If feature extractor files are in a directory (e.g. feature extractor was saved using *save_pretrained('./test/saved_model/')*)
feature_extractor = AutoFeatureExtractor.from_pretrained("./test/saved_model/"),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoFeatureExtractor

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download feature extractor from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>feature_extractor = AutoFeatureExtractor.from_pretrained(<span class="hljs-string">&quot;facebook/wav2vec2-base-960h&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># If feature extractor files are in a directory (e.g. feature extractor was saved using *save_pretrained(&#x27;./test/saved_model/&#x27;)*)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>feature_extractor = AutoFeatureExtractor.from_pretrained(<span class="hljs-string">&quot;./test/saved_model/&quot;</span>)`}}),LM=new E({props:{name:"register",anchor:"transformers.AutoFeatureExtractor.register",parameters:[{name:"config_class",val:""},{name:"feature_extractor_class",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15297/src/transformers/models/auto/feature_extraction_auto.py#L310",parametersDescription:[{anchor:"transformers.AutoFeatureExtractor.register.config_class",description:`<strong>config_class</strong> (<a href="/docs/transformers/pr_15297/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The configuration corresponding to the model to register.`,name:"config_class"},{anchor:"transformers.AutoFeatureExtractor.register.feature_extractor_class",description:"<strong>feature_extractor_class</strong> (<code>FeatureExtractorMixin</code>) &#x2014; The feature extractor to register.",name:"feature_extractor_class"}]}}),BM=new z({}),kM=new E({props:{name:"class transformers.AutoProcessor",anchor:"transformers.AutoProcessor",parameters:[],source:"https://github.com/huggingface/transformers/blob/pr_15297/src/transformers/models/auto/processing_auto.py#L71"}}),SM=new E({props:{name:"from_pretrained",anchor:"transformers.AutoProcessor.from_pretrained",parameters:[{name:"pretrained_model_name_or_path",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15297/src/transformers/models/auto/processing_auto.py#L85",parametersDescription:[{anchor:"transformers.AutoProcessor.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
This can be either:</p>
<ul>
<li>a string, the <em>model id</em> of a pretrained feature_extractor hosted inside a model repo on
huggingface.co. Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or
namespaced under a user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>a path to a <em>directory</em> containing a processor files saved using the <code>save_pretrained()</code> method,
e.g., <code>./my_model_directory/</code>.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.AutoProcessor.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model feature extractor should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.AutoProcessor.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force to (re-)download the feature extractor files and override the cached versions
if they exist.`,name:"force_download"},{anchor:"transformers.AutoProcessor.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received file. Attempts to resume the download if such a file
exists.`,name:"resume_download"},{anchor:"transformers.AutoProcessor.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}.</code> The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.AutoProcessor.from_pretrained.use_auth_token",description:`<strong>use_auth_token</strong> (<code>str</code> or <em>bool</em>, <em>optional</em>) &#x2014;
The token to use as HTTP bearer authorization for remote files. If <code>True</code>, will use the token generated
when running <code>transformers-cli login</code> (stored in <code>~/.huggingface</code>).`,name:"use_auth_token"},{anchor:"transformers.AutoProcessor.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.AutoProcessor.from_pretrained.return_unused_kwargs",description:`<strong>return_unused_kwargs</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
If <code>False</code>, then this function returns just the final feature extractor object. If <code>True</code>, then this
functions returns a <code>Tuple(feature_extractor, unused_kwargs)</code> where <em>unused_kwargs</em> is a dictionary
consisting of the key/value pairs whose keys are not feature extractor attributes: i.e., the part of
<code>kwargs</code> which has not been used to update <code>feature_extractor</code> and is otherwise ignored.`,name:"return_unused_kwargs"},{anchor:"transformers.AutoProcessor.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.AutoProcessor.from_pretrained.kwargs",description:`<strong>kwargs</strong> (<code>Dict[str, Any]</code>, <em>optional</em>) &#x2014;
The values in kwargs of any keys which are feature extractor attributes will be used to override the
loaded values. Behavior concerning key/value pairs whose keys are <em>not</em> feature extractor attributes is
controlled by the <code>return_unused_kwargs</code> keyword parameter.`,name:"kwargs"}]}}),Th=new Lyr({props:{$$slots:{default:[N_t]},$$scope:{ctx:wi}}}),PM=new w({props:{code:`from transformers import AutoProcessor

# Download processor from huggingface.co and cache.
processor = AutoProcessor.from_pretrained("facebook/wav2vec2-base-960h")

# If processor files are in a directory (e.g. processor was saved using *save_pretrained('./test/saved_model/')*)
processor = AutoProcessor.from_pretrained("./test/saved_model/"),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoProcessor

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download processor from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>processor = AutoProcessor.from_pretrained(<span class="hljs-string">&quot;facebook/wav2vec2-base-960h&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># If processor files are in a directory (e.g. processor was saved using *save_pretrained(&#x27;./test/saved_model/&#x27;)*)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>processor = AutoProcessor.from_pretrained(<span class="hljs-string">&quot;./test/saved_model/&quot;</span>)`}}),$M=new E({props:{name:"register",anchor:"transformers.AutoProcessor.register",parameters:[{name:"config_class",val:""},{name:"processor_class",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15297/src/transformers/models/auto/processing_auto.py#L238",parametersDescription:[{anchor:"transformers.AutoProcessor.register.config_class",description:`<strong>config_class</strong> (<a href="/docs/transformers/pr_15297/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The configuration corresponding to the model to register.`,name:"config_class"},{anchor:"transformers.AutoProcessor.register.processor_class",description:"<strong>processor_class</strong> (<code>FeatureExtractorMixin</code>) &#x2014; The processor to register.",name:"processor_class"}]}}),IM=new z({}),jM=new E({props:{name:"class transformers.AutoModel",anchor:"transformers.AutoModel",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15297/src/transformers/models/auto/modeling_auto.py#L672"}}),DM=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15297/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15297/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15297/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/albert#transformers.AlbertModel">AlbertModel</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/bart#transformers.BartModel">BartModel</a> (BART model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/beit#transformers.BeitConfig">BeitConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/beit#transformers.BeitModel">BeitModel</a> (BEiT model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/bert#transformers.BertModel">BertModel</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/bert-generation#transformers.BertGenerationConfig">BertGenerationConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/bert-generation#transformers.BertGenerationEncoder">BertGenerationEncoder</a> (Bert Generation model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/big_bird#transformers.BigBirdModel">BigBirdModel</a> (BigBird model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusConfig">BigBirdPegasusConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusModel">BigBirdPegasusModel</a> (BigBirdPegasus model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/blenderbot#transformers.BlenderbotConfig">BlenderbotConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/blenderbot#transformers.BlenderbotModel">BlenderbotModel</a> (Blenderbot model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/blenderbot-small#transformers.BlenderbotSmallConfig">BlenderbotSmallConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/blenderbot-small#transformers.BlenderbotSmallModel">BlenderbotSmallModel</a> (BlenderbotSmall model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/clip#transformers.CLIPConfig">CLIPConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/clip#transformers.CLIPModel">CLIPModel</a> (CLIP model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/ctrl#transformers.CTRLConfig">CTRLConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/ctrl#transformers.CTRLModel">CTRLModel</a> (CTRL model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/camembert#transformers.CamembertModel">CamembertModel</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/canine#transformers.CanineConfig">CanineConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/canine#transformers.CanineModel">CanineModel</a> (Canine model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/convbert#transformers.ConvBertModel">ConvBertModel</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/convnext#transformers.ConvNextConfig">ConvNextConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/convnext#transformers.ConvNextModel">ConvNextModel</a> (ConvNext model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/dpr#transformers.DPRConfig">DPRConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/dpr#transformers.DPRQuestionEncoder">DPRQuestionEncoder</a> (DPR model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/deberta#transformers.DebertaModel">DebertaModel</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/deberta-v2#transformers.DebertaV2Model">DebertaV2Model</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/deit#transformers.DeiTConfig">DeiTConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/deit#transformers.DeiTModel">DeiTModel</a> (DeiT model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/detr#transformers.DetrConfig">DetrConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/detr#transformers.DetrModel">DetrModel</a> (DETR model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/distilbert#transformers.DistilBertModel">DistilBertModel</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/electra#transformers.ElectraModel">ElectraModel</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/fnet#transformers.FNetConfig">FNetConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/fnet#transformers.FNetModel">FNetModel</a> (FNet model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/fsmt#transformers.FSMTConfig">FSMTConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/fsmt#transformers.FSMTModel">FSMTModel</a> (FairSeq Machine-Translation model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/flaubert#transformers.FlaubertModel">FlaubertModel</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/funnel#transformers.FunnelModel">FunnelModel</a> or <a href="/docs/transformers/pr_15297/en/model_doc/funnel#transformers.FunnelBaseModel">FunnelBaseModel</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/gpt2#transformers.GPT2Model">GPT2Model</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/gptj#transformers.GPTJConfig">GPTJConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/gptj#transformers.GPTJModel">GPTJModel</a> (GPT-J model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/gpt_neo#transformers.GPTNeoConfig">GPTNeoConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/gpt_neo#transformers.GPTNeoModel">GPTNeoModel</a> (GPT Neo model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/hubert#transformers.HubertConfig">HubertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/hubert#transformers.HubertModel">HubertModel</a> (Hubert model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/ibert#transformers.IBertConfig">IBertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/ibert#transformers.IBertModel">IBertModel</a> (I-BERT model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/imagegpt#transformers.ImageGPTConfig">ImageGPTConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/imagegpt#transformers.ImageGPTModel">ImageGPTModel</a> (ImageGPT model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/led#transformers.LEDConfig">LEDConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/led#transformers.LEDModel">LEDModel</a> (LED model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/layoutlm#transformers.LayoutLMModel">LayoutLMModel</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/layoutlmv2#transformers.LayoutLMv2Config">LayoutLMv2Config</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/layoutlmv2#transformers.LayoutLMv2Model">LayoutLMv2Model</a> (LayoutLMv2 model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/longformer#transformers.LongformerModel">LongformerModel</a> (Longformer model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/luke#transformers.LukeConfig">LukeConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/luke#transformers.LukeModel">LukeModel</a> (LUKE model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/lxmert#transformers.LxmertConfig">LxmertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/lxmert#transformers.LxmertModel">LxmertModel</a> (LXMERT model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/m2m_100#transformers.M2M100Config">M2M100Config</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/m2m_100#transformers.M2M100Model">M2M100Model</a> (M2M100 model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/mbart#transformers.MBartModel">MBartModel</a> (mBART model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/mpnet#transformers.MPNetModel">MPNetModel</a> (MPNet model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/mt5#transformers.MT5Config">MT5Config</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/mt5#transformers.MT5Model">MT5Model</a> (mT5 model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/marian#transformers.MarianConfig">MarianConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/marian#transformers.MarianModel">MarianModel</a> (Marian model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/megatron-bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/megatron-bert#transformers.MegatronBertModel">MegatronBertModel</a> (MegatronBert model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/mobilebert#transformers.MobileBertModel">MobileBertModel</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/nystromformer#transformers.NystromformerConfig">NystromformerConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/nystromformer#transformers.NystromformerModel">NystromformerModel</a> (Nystromformer model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig">OpenAIGPTConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/openai-gpt#transformers.OpenAIGPTModel">OpenAIGPTModel</a> (OpenAI GPT model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/plbart#transformers.PLBartConfig">PLBartConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/plbart#transformers.PLBartModel">PLBartModel</a> (PLBart model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/pegasus#transformers.PegasusConfig">PegasusConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/pegasus#transformers.PegasusModel">PegasusModel</a> (Pegasus model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/perceiver#transformers.PerceiverConfig">PerceiverConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/perceiver#transformers.PerceiverModel">PerceiverModel</a> (Perceiver model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/poolformer#transformers.PoolFormerConfig">PoolFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/poolformer#transformers.PoolFormerModel">PoolFormerModel</a> (PoolFormer model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/prophetnet#transformers.ProphetNetConfig">ProphetNetConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/prophetnet#transformers.ProphetNetModel">ProphetNetModel</a> (ProphetNet model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/qdqbert#transformers.QDQBertConfig">QDQBertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/qdqbert#transformers.QDQBertModel">QDQBertModel</a> (QDQBert model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/reformer#transformers.ReformerConfig">ReformerConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/reformer#transformers.ReformerModel">ReformerModel</a> (Reformer model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/rembert#transformers.RemBertModel">RemBertModel</a> (RemBERT model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/retribert#transformers.RetriBertConfig">RetriBertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/retribert#transformers.RetriBertModel">RetriBertModel</a> (RetriBERT model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/roformer#transformers.RoFormerModel">RoFormerModel</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/roberta#transformers.RobertaModel">RobertaModel</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/sew#transformers.SEWConfig">SEWConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/sew#transformers.SEWModel">SEWModel</a> (SEW model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/sew-d#transformers.SEWDConfig">SEWDConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/sew-d#transformers.SEWDModel">SEWDModel</a> (SEW-D model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/segformer#transformers.SegformerConfig">SegformerConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/segformer#transformers.SegformerModel">SegformerModel</a> (SegFormer model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/speech_to_text#transformers.Speech2TextConfig">Speech2TextConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/speech_to_text#transformers.Speech2TextModel">Speech2TextModel</a> (Speech2Text model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/splinter#transformers.SplinterConfig">SplinterConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/splinter#transformers.SplinterModel">SplinterModel</a> (Splinter model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/squeezebert#transformers.SqueezeBertConfig">SqueezeBertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/squeezebert#transformers.SqueezeBertModel">SqueezeBertModel</a> (SqueezeBERT model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/swin#transformers.SwinConfig">SwinConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/swin#transformers.SwinModel">SwinModel</a> (Swin model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/t5#transformers.T5Model">T5Model</a> (T5 model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/tapas#transformers.TapasModel">TapasModel</a> (TAPAS model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/transfo-xl#transformers.TransfoXLConfig">TransfoXLConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/transfo-xl#transformers.TransfoXLModel">TransfoXLModel</a> (Transformer-XL model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/unispeech#transformers.UniSpeechConfig">UniSpeechConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/unispeech#transformers.UniSpeechModel">UniSpeechModel</a> (UniSpeech model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/unispeech-sat#transformers.UniSpeechSatConfig">UniSpeechSatConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/unispeech-sat#transformers.UniSpeechSatModel">UniSpeechSatModel</a> (UniSpeechSat model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/vit#transformers.ViTConfig">ViTConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/vit#transformers.ViTModel">ViTModel</a> (ViT model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/vit_mae#transformers.ViTMAEConfig">ViTMAEConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/vit_mae#transformers.ViTMAEModel">ViTMAEModel</a> (ViTMAE model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/vilt#transformers.ViltConfig">ViltConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/vilt#transformers.ViltModel">ViltModel</a> (ViLT model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/vision-text-dual-encoder#transformers.VisionTextDualEncoderConfig">VisionTextDualEncoderConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/vision-text-dual-encoder#transformers.VisionTextDualEncoderModel">VisionTextDualEncoderModel</a> (VisionTextDualEncoder model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/visual_bert#transformers.VisualBertConfig">VisualBertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/visual_bert#transformers.VisualBertModel">VisualBertModel</a> (VisualBert model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/wav2vec2#transformers.Wav2Vec2Model">Wav2Vec2Model</a> (Wav2Vec2 model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/wavlm#transformers.WavLMConfig">WavLMConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/wavlm#transformers.WavLMModel">WavLMModel</a> (WavLM model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/xglm#transformers.XGLMConfig">XGLMConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/xglm#transformers.XGLMModel">XGLMModel</a> (XGLM model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/xlm#transformers.XLMModel">XLMModel</a> (XLM model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetConfig">XLMProphetNetConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetModel">XLMProphetNetModel</a> (XLMProphetNet model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/xlm-roberta#transformers.XLMRobertaModel">XLMRobertaModel</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig">XLMRobertaXLConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLModel">XLMRobertaXLModel</a> (XLM-RoBERTa-XL model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/xlnet#transformers.XLNetModel">XLNetModel</a> (XLNet model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/yoso#transformers.YosoConfig">YosoConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/yoso#transformers.YosoModel">YosoModel</a> (YOSO model)</li>
</ul>`,name:"config"}]}}),qM=new w({props:{code:`from transformers import AutoConfig, AutoModel

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModel.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModel.from_config(config)`}}),GM=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15297/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15297/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15297/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15297/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15297/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15297/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15297/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),OM=new w({props:{code:`from transformers import AutoConfig, AutoModel

# Download model and configuration from huggingface.co and cache.
model = AutoModel.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModel.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModel.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModel.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModel.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModel.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),XM=new z({}),zM=new E({props:{name:"class transformers.AutoModelForPreTraining",anchor:"transformers.AutoModelForPreTraining",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15297/src/transformers/models/auto/modeling_auto.py#L679"}}),WM=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15297/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15297/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15297/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/albert#transformers.AlbertForPreTraining">AlbertForPreTraining</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/bart#transformers.BartForConditionalGeneration">BartForConditionalGeneration</a> (BART model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/bert#transformers.BertForPreTraining">BertForPreTraining</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/big_bird#transformers.BigBirdForPreTraining">BigBirdForPreTraining</a> (BigBird model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/ctrl#transformers.CTRLConfig">CTRLConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/ctrl#transformers.CTRLLMHeadModel">CTRLLMHeadModel</a> (CTRL model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/camembert#transformers.CamembertForMaskedLM">CamembertForMaskedLM</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/deberta#transformers.DebertaForMaskedLM">DebertaForMaskedLM</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/deberta-v2#transformers.DebertaV2ForMaskedLM">DebertaV2ForMaskedLM</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/distilbert#transformers.DistilBertForMaskedLM">DistilBertForMaskedLM</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/electra#transformers.ElectraForPreTraining">ElectraForPreTraining</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/fnet#transformers.FNetConfig">FNetConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/fnet#transformers.FNetForPreTraining">FNetForPreTraining</a> (FNet model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/fsmt#transformers.FSMTConfig">FSMTConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/fsmt#transformers.FSMTForConditionalGeneration">FSMTForConditionalGeneration</a> (FairSeq Machine-Translation model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/flaubert#transformers.FlaubertWithLMHeadModel">FlaubertWithLMHeadModel</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/funnel#transformers.FunnelForPreTraining">FunnelForPreTraining</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/gpt2#transformers.GPT2LMHeadModel">GPT2LMHeadModel</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/ibert#transformers.IBertConfig">IBertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/ibert#transformers.IBertForMaskedLM">IBertForMaskedLM</a> (I-BERT model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/layoutlm#transformers.LayoutLMForMaskedLM">LayoutLMForMaskedLM</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/longformer#transformers.LongformerForMaskedLM">LongformerForMaskedLM</a> (Longformer model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/lxmert#transformers.LxmertConfig">LxmertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/lxmert#transformers.LxmertForPreTraining">LxmertForPreTraining</a> (LXMERT model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/mpnet#transformers.MPNetForMaskedLM">MPNetForMaskedLM</a> (MPNet model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/megatron-bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/megatron-bert#transformers.MegatronBertForPreTraining">MegatronBertForPreTraining</a> (MegatronBert model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/mobilebert#transformers.MobileBertForPreTraining">MobileBertForPreTraining</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig">OpenAIGPTConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/openai-gpt#transformers.OpenAIGPTLMHeadModel">OpenAIGPTLMHeadModel</a> (OpenAI GPT model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/retribert#transformers.RetriBertConfig">RetriBertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/retribert#transformers.RetriBertModel">RetriBertModel</a> (RetriBERT model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/roberta#transformers.RobertaForMaskedLM">RobertaForMaskedLM</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/squeezebert#transformers.SqueezeBertConfig">SqueezeBertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/squeezebert#transformers.SqueezeBertForMaskedLM">SqueezeBertForMaskedLM</a> (SqueezeBERT model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/t5#transformers.T5ForConditionalGeneration">T5ForConditionalGeneration</a> (T5 model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/tapas#transformers.TapasForMaskedLM">TapasForMaskedLM</a> (TAPAS model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/transfo-xl#transformers.TransfoXLConfig">TransfoXLConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/transfo-xl#transformers.TransfoXLLMHeadModel">TransfoXLLMHeadModel</a> (Transformer-XL model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/unispeech#transformers.UniSpeechConfig">UniSpeechConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/unispeech#transformers.UniSpeechForPreTraining">UniSpeechForPreTraining</a> (UniSpeech model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/unispeech-sat#transformers.UniSpeechSatConfig">UniSpeechSatConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/unispeech-sat#transformers.UniSpeechSatForPreTraining">UniSpeechSatForPreTraining</a> (UniSpeechSat model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/vit_mae#transformers.ViTMAEConfig">ViTMAEConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/vit_mae#transformers.ViTMAEForPreTraining">ViTMAEForPreTraining</a> (ViTMAE model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/visual_bert#transformers.VisualBertConfig">VisualBertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/visual_bert#transformers.VisualBertForPreTraining">VisualBertForPreTraining</a> (VisualBert model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/wav2vec2#transformers.Wav2Vec2ForPreTraining">Wav2Vec2ForPreTraining</a> (Wav2Vec2 model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/xlm#transformers.XLMWithLMHeadModel">XLMWithLMHeadModel</a> (XLM model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/xlm-roberta#transformers.XLMRobertaForMaskedLM">XLMRobertaForMaskedLM</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig">XLMRobertaXLConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForMaskedLM">XLMRobertaXLForMaskedLM</a> (XLM-RoBERTa-XL model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/xlnet#transformers.XLNetLMHeadModel">XLNetLMHeadModel</a> (XLNet model)</li>
</ul>`,name:"config"}]}}),QM=new w({props:{code:`from transformers import AutoConfig, AutoModelForPreTraining

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForPreTraining.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForPreTraining

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForPreTraining.from_config(config)`}}),HM=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15297/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15297/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15297/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15297/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15297/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15297/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15297/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),UM=new w({props:{code:`from transformers import AutoConfig, AutoModelForPreTraining

# Download model and configuration from huggingface.co and cache.
model = AutoModelForPreTraining.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForPreTraining.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForPreTraining.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForPreTraining

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForPreTraining.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForPreTraining.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForPreTraining.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),JM=new z({}),YM=new E({props:{name:"class transformers.AutoModelForCausalLM",anchor:"transformers.AutoModelForCausalLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15297/src/transformers/models/auto/modeling_auto.py#L694"}}),ZM=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15297/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15297/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15297/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/bart#transformers.BartForCausalLM">BartForCausalLM</a> (BART model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/bert#transformers.BertLMHeadModel">BertLMHeadModel</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/bert-generation#transformers.BertGenerationConfig">BertGenerationConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/bert-generation#transformers.BertGenerationDecoder">BertGenerationDecoder</a> (Bert Generation model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/big_bird#transformers.BigBirdForCausalLM">BigBirdForCausalLM</a> (BigBird model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusConfig">BigBirdPegasusConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForCausalLM">BigBirdPegasusForCausalLM</a> (BigBirdPegasus model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/blenderbot#transformers.BlenderbotConfig">BlenderbotConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/blenderbot#transformers.BlenderbotForCausalLM">BlenderbotForCausalLM</a> (Blenderbot model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/blenderbot-small#transformers.BlenderbotSmallConfig">BlenderbotSmallConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/blenderbot-small#transformers.BlenderbotSmallForCausalLM">BlenderbotSmallForCausalLM</a> (BlenderbotSmall model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/ctrl#transformers.CTRLConfig">CTRLConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/ctrl#transformers.CTRLLMHeadModel">CTRLLMHeadModel</a> (CTRL model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/camembert#transformers.CamembertForCausalLM">CamembertForCausalLM</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/electra#transformers.ElectraForCausalLM">ElectraForCausalLM</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/gpt2#transformers.GPT2LMHeadModel">GPT2LMHeadModel</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/gptj#transformers.GPTJConfig">GPTJConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/gptj#transformers.GPTJForCausalLM">GPTJForCausalLM</a> (GPT-J model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/gpt_neo#transformers.GPTNeoConfig">GPTNeoConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/gpt_neo#transformers.GPTNeoForCausalLM">GPTNeoForCausalLM</a> (GPT Neo model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/mbart#transformers.MBartForCausalLM">MBartForCausalLM</a> (mBART model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/marian#transformers.MarianConfig">MarianConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/marian#transformers.MarianForCausalLM">MarianForCausalLM</a> (Marian model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/megatron-bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/megatron-bert#transformers.MegatronBertForCausalLM">MegatronBertForCausalLM</a> (MegatronBert model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig">OpenAIGPTConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/openai-gpt#transformers.OpenAIGPTLMHeadModel">OpenAIGPTLMHeadModel</a> (OpenAI GPT model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/plbart#transformers.PLBartConfig">PLBartConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/plbart#transformers.PLBartForCausalLM">PLBartForCausalLM</a> (PLBart model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/pegasus#transformers.PegasusConfig">PegasusConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/pegasus#transformers.PegasusForCausalLM">PegasusForCausalLM</a> (Pegasus model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/prophetnet#transformers.ProphetNetConfig">ProphetNetConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/prophetnet#transformers.ProphetNetForCausalLM">ProphetNetForCausalLM</a> (ProphetNet model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/qdqbert#transformers.QDQBertConfig">QDQBertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/qdqbert#transformers.QDQBertLMHeadModel">QDQBertLMHeadModel</a> (QDQBert model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/reformer#transformers.ReformerConfig">ReformerConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/reformer#transformers.ReformerModelWithLMHead">ReformerModelWithLMHead</a> (Reformer model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/rembert#transformers.RemBertForCausalLM">RemBertForCausalLM</a> (RemBERT model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/roformer#transformers.RoFormerForCausalLM">RoFormerForCausalLM</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/roberta#transformers.RobertaForCausalLM">RobertaForCausalLM</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/speech_to_text_2#transformers.Speech2Text2Config">Speech2Text2Config</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/speech_to_text_2#transformers.Speech2Text2ForCausalLM">Speech2Text2ForCausalLM</a> (Speech2Text2 model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/trocr#transformers.TrOCRConfig">TrOCRConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/trocr#transformers.TrOCRForCausalLM">TrOCRForCausalLM</a> (TrOCR model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/transfo-xl#transformers.TransfoXLConfig">TransfoXLConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/transfo-xl#transformers.TransfoXLLMHeadModel">TransfoXLLMHeadModel</a> (Transformer-XL model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/xglm#transformers.XGLMConfig">XGLMConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/xglm#transformers.XGLMForCausalLM">XGLMForCausalLM</a> (XGLM model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/xlm#transformers.XLMWithLMHeadModel">XLMWithLMHeadModel</a> (XLM model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetConfig">XLMProphetNetConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetForCausalLM">XLMProphetNetForCausalLM</a> (XLMProphetNet model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/xlm-roberta#transformers.XLMRobertaForCausalLM">XLMRobertaForCausalLM</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig">XLMRobertaXLConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForCausalLM">XLMRobertaXLForCausalLM</a> (XLM-RoBERTa-XL model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/xlnet#transformers.XLNetLMHeadModel">XLNetLMHeadModel</a> (XLNet model)</li>
</ul>`,name:"config"}]}}),eE=new w({props:{code:`from transformers import AutoConfig, AutoModelForCausalLM

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForCausalLM.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForCausalLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCausalLM.from_config(config)`}}),oE=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15297/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15297/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15297/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15297/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15297/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15297/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15297/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),rE=new w({props:{code:`from transformers import AutoConfig, AutoModelForCausalLM

# Download model and configuration from huggingface.co and cache.
model = AutoModelForCausalLM.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForCausalLM.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForCausalLM.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForCausalLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCausalLM.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),tE=new z({}),aE=new E({props:{name:"class transformers.AutoModelForMaskedLM",anchor:"transformers.AutoModelForMaskedLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15297/src/transformers/models/auto/modeling_auto.py#L701"}}),sE=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15297/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15297/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15297/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/albert#transformers.AlbertForMaskedLM">AlbertForMaskedLM</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/bart#transformers.BartForConditionalGeneration">BartForConditionalGeneration</a> (BART model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/bert#transformers.BertForMaskedLM">BertForMaskedLM</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/big_bird#transformers.BigBirdForMaskedLM">BigBirdForMaskedLM</a> (BigBird model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/camembert#transformers.CamembertForMaskedLM">CamembertForMaskedLM</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/convbert#transformers.ConvBertForMaskedLM">ConvBertForMaskedLM</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/deberta#transformers.DebertaForMaskedLM">DebertaForMaskedLM</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/deberta-v2#transformers.DebertaV2ForMaskedLM">DebertaV2ForMaskedLM</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/distilbert#transformers.DistilBertForMaskedLM">DistilBertForMaskedLM</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/electra#transformers.ElectraForMaskedLM">ElectraForMaskedLM</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/fnet#transformers.FNetConfig">FNetConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/fnet#transformers.FNetForMaskedLM">FNetForMaskedLM</a> (FNet model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/flaubert#transformers.FlaubertWithLMHeadModel">FlaubertWithLMHeadModel</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/funnel#transformers.FunnelForMaskedLM">FunnelForMaskedLM</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/ibert#transformers.IBertConfig">IBertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/ibert#transformers.IBertForMaskedLM">IBertForMaskedLM</a> (I-BERT model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/layoutlm#transformers.LayoutLMForMaskedLM">LayoutLMForMaskedLM</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/longformer#transformers.LongformerForMaskedLM">LongformerForMaskedLM</a> (Longformer model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/mbart#transformers.MBartForConditionalGeneration">MBartForConditionalGeneration</a> (mBART model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/mpnet#transformers.MPNetForMaskedLM">MPNetForMaskedLM</a> (MPNet model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/megatron-bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/megatron-bert#transformers.MegatronBertForMaskedLM">MegatronBertForMaskedLM</a> (MegatronBert model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/mobilebert#transformers.MobileBertForMaskedLM">MobileBertForMaskedLM</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/nystromformer#transformers.NystromformerConfig">NystromformerConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/nystromformer#transformers.NystromformerForMaskedLM">NystromformerForMaskedLM</a> (Nystromformer model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/perceiver#transformers.PerceiverConfig">PerceiverConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/perceiver#transformers.PerceiverForMaskedLM">PerceiverForMaskedLM</a> (Perceiver model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/qdqbert#transformers.QDQBertConfig">QDQBertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/qdqbert#transformers.QDQBertForMaskedLM">QDQBertForMaskedLM</a> (QDQBert model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/reformer#transformers.ReformerConfig">ReformerConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/reformer#transformers.ReformerForMaskedLM">ReformerForMaskedLM</a> (Reformer model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/rembert#transformers.RemBertForMaskedLM">RemBertForMaskedLM</a> (RemBERT model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/roformer#transformers.RoFormerForMaskedLM">RoFormerForMaskedLM</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/roberta#transformers.RobertaForMaskedLM">RobertaForMaskedLM</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/squeezebert#transformers.SqueezeBertConfig">SqueezeBertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/squeezebert#transformers.SqueezeBertForMaskedLM">SqueezeBertForMaskedLM</a> (SqueezeBERT model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/tapas#transformers.TapasForMaskedLM">TapasForMaskedLM</a> (TAPAS model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <code>Wav2Vec2ForMaskedLM</code>(Wav2Vec2 model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/xlm#transformers.XLMWithLMHeadModel">XLMWithLMHeadModel</a> (XLM model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/xlm-roberta#transformers.XLMRobertaForMaskedLM">XLMRobertaForMaskedLM</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig">XLMRobertaXLConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForMaskedLM">XLMRobertaXLForMaskedLM</a> (XLM-RoBERTa-XL model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/yoso#transformers.YosoConfig">YosoConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/yoso#transformers.YosoForMaskedLM">YosoForMaskedLM</a> (YOSO model)</li>
</ul>`,name:"config"}]}}),lE=new w({props:{code:`from transformers import AutoConfig, AutoModelForMaskedLM

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForMaskedLM.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForMaskedLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMaskedLM.from_config(config)`}}),iE=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15297/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15297/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15297/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15297/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15297/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15297/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15297/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),dE=new w({props:{code:`from transformers import AutoConfig, AutoModelForMaskedLM

# Download model and configuration from huggingface.co and cache.
model = AutoModelForMaskedLM.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForMaskedLM.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForMaskedLM.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForMaskedLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMaskedLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMaskedLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMaskedLM.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),cE=new z({}),fE=new E({props:{name:"class transformers.AutoModelForSeq2SeqLM",anchor:"transformers.AutoModelForSeq2SeqLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15297/src/transformers/models/auto/modeling_auto.py#L708"}}),gE=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15297/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15297/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15297/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/bart#transformers.BartForConditionalGeneration">BartForConditionalGeneration</a> (BART model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusConfig">BigBirdPegasusConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForConditionalGeneration">BigBirdPegasusForConditionalGeneration</a> (BigBirdPegasus model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/blenderbot#transformers.BlenderbotConfig">BlenderbotConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/blenderbot#transformers.BlenderbotForConditionalGeneration">BlenderbotForConditionalGeneration</a> (Blenderbot model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/blenderbot-small#transformers.BlenderbotSmallConfig">BlenderbotSmallConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/blenderbot-small#transformers.BlenderbotSmallForConditionalGeneration">BlenderbotSmallForConditionalGeneration</a> (BlenderbotSmall model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/encoder-decoder#transformers.EncoderDecoderConfig">EncoderDecoderConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/encoder-decoder#transformers.EncoderDecoderModel">EncoderDecoderModel</a> (Encoder decoder model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/fsmt#transformers.FSMTConfig">FSMTConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/fsmt#transformers.FSMTForConditionalGeneration">FSMTForConditionalGeneration</a> (FairSeq Machine-Translation model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/led#transformers.LEDConfig">LEDConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/led#transformers.LEDForConditionalGeneration">LEDForConditionalGeneration</a> (LED model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/m2m_100#transformers.M2M100Config">M2M100Config</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/m2m_100#transformers.M2M100ForConditionalGeneration">M2M100ForConditionalGeneration</a> (M2M100 model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/mbart#transformers.MBartForConditionalGeneration">MBartForConditionalGeneration</a> (mBART model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/mt5#transformers.MT5Config">MT5Config</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/mt5#transformers.MT5ForConditionalGeneration">MT5ForConditionalGeneration</a> (mT5 model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/marian#transformers.MarianConfig">MarianConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/marian#transformers.MarianMTModel">MarianMTModel</a> (Marian model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/plbart#transformers.PLBartConfig">PLBartConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/plbart#transformers.PLBartForConditionalGeneration">PLBartForConditionalGeneration</a> (PLBart model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/pegasus#transformers.PegasusConfig">PegasusConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/pegasus#transformers.PegasusForConditionalGeneration">PegasusForConditionalGeneration</a> (Pegasus model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/prophetnet#transformers.ProphetNetConfig">ProphetNetConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/prophetnet#transformers.ProphetNetForConditionalGeneration">ProphetNetForConditionalGeneration</a> (ProphetNet model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/t5#transformers.T5ForConditionalGeneration">T5ForConditionalGeneration</a> (T5 model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetConfig">XLMProphetNetConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetForConditionalGeneration">XLMProphetNetForConditionalGeneration</a> (XLMProphetNet model)</li>
</ul>`,name:"config"}]}}),hE=new w({props:{code:`from transformers import AutoConfig, AutoModelForSeq2SeqLM

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("t5-base")
model = AutoModelForSeq2SeqLM.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForSeq2SeqLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSeq2SeqLM.from_config(config)`}}),pE=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15297/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15297/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15297/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15297/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15297/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15297/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15297/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),_E=new w({props:{code:`from transformers import AutoConfig, AutoModelForSeq2SeqLM

# Download model and configuration from huggingface.co and cache.
model = AutoModelForSeq2SeqLM.from_pretrained("t5-base")

# Update configuration during loading
model = AutoModelForSeq2SeqLM.from_pretrained("t5-base", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/t5_tf_model_config.json")
model = AutoModelForSeq2SeqLM.from_pretrained(
    "./tf_model/t5_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForSeq2SeqLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/t5_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSeq2SeqLM.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/t5_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),uE=new z({}),bE=new E({props:{name:"class transformers.AutoModelForSequenceClassification",anchor:"transformers.AutoModelForSequenceClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15297/src/transformers/models/auto/modeling_auto.py#L717"}}),TE=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15297/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15297/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15297/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/albert#transformers.AlbertForSequenceClassification">AlbertForSequenceClassification</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/bart#transformers.BartForSequenceClassification">BartForSequenceClassification</a> (BART model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/bert#transformers.BertForSequenceClassification">BertForSequenceClassification</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/big_bird#transformers.BigBirdForSequenceClassification">BigBirdForSequenceClassification</a> (BigBird model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusConfig">BigBirdPegasusConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForSequenceClassification">BigBirdPegasusForSequenceClassification</a> (BigBirdPegasus model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/ctrl#transformers.CTRLConfig">CTRLConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/ctrl#transformers.CTRLForSequenceClassification">CTRLForSequenceClassification</a> (CTRL model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/camembert#transformers.CamembertForSequenceClassification">CamembertForSequenceClassification</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/canine#transformers.CanineConfig">CanineConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/canine#transformers.CanineForSequenceClassification">CanineForSequenceClassification</a> (Canine model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/convbert#transformers.ConvBertForSequenceClassification">ConvBertForSequenceClassification</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/deberta#transformers.DebertaForSequenceClassification">DebertaForSequenceClassification</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/deberta-v2#transformers.DebertaV2ForSequenceClassification">DebertaV2ForSequenceClassification</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/distilbert#transformers.DistilBertForSequenceClassification">DistilBertForSequenceClassification</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/electra#transformers.ElectraForSequenceClassification">ElectraForSequenceClassification</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/fnet#transformers.FNetConfig">FNetConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/fnet#transformers.FNetForSequenceClassification">FNetForSequenceClassification</a> (FNet model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/flaubert#transformers.FlaubertForSequenceClassification">FlaubertForSequenceClassification</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/funnel#transformers.FunnelForSequenceClassification">FunnelForSequenceClassification</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/gpt2#transformers.GPT2ForSequenceClassification">GPT2ForSequenceClassification</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/gptj#transformers.GPTJConfig">GPTJConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/gptj#transformers.GPTJForSequenceClassification">GPTJForSequenceClassification</a> (GPT-J model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/gpt_neo#transformers.GPTNeoConfig">GPTNeoConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/gpt_neo#transformers.GPTNeoForSequenceClassification">GPTNeoForSequenceClassification</a> (GPT Neo model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/ibert#transformers.IBertConfig">IBertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/ibert#transformers.IBertForSequenceClassification">IBertForSequenceClassification</a> (I-BERT model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/led#transformers.LEDConfig">LEDConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/led#transformers.LEDForSequenceClassification">LEDForSequenceClassification</a> (LED model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/layoutlm#transformers.LayoutLMForSequenceClassification">LayoutLMForSequenceClassification</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/layoutlmv2#transformers.LayoutLMv2Config">LayoutLMv2Config</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/layoutlmv2#transformers.LayoutLMv2ForSequenceClassification">LayoutLMv2ForSequenceClassification</a> (LayoutLMv2 model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/longformer#transformers.LongformerForSequenceClassification">LongformerForSequenceClassification</a> (Longformer model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/mbart#transformers.MBartForSequenceClassification">MBartForSequenceClassification</a> (mBART model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/mpnet#transformers.MPNetForSequenceClassification">MPNetForSequenceClassification</a> (MPNet model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/megatron-bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/megatron-bert#transformers.MegatronBertForSequenceClassification">MegatronBertForSequenceClassification</a> (MegatronBert model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/mobilebert#transformers.MobileBertForSequenceClassification">MobileBertForSequenceClassification</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/nystromformer#transformers.NystromformerConfig">NystromformerConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/nystromformer#transformers.NystromformerForSequenceClassification">NystromformerForSequenceClassification</a> (Nystromformer model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig">OpenAIGPTConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/openai-gpt#transformers.OpenAIGPTForSequenceClassification">OpenAIGPTForSequenceClassification</a> (OpenAI GPT model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/plbart#transformers.PLBartConfig">PLBartConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/plbart#transformers.PLBartForSequenceClassification">PLBartForSequenceClassification</a> (PLBart model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/perceiver#transformers.PerceiverConfig">PerceiverConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/perceiver#transformers.PerceiverForSequenceClassification">PerceiverForSequenceClassification</a> (Perceiver model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/qdqbert#transformers.QDQBertConfig">QDQBertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/qdqbert#transformers.QDQBertForSequenceClassification">QDQBertForSequenceClassification</a> (QDQBert model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/reformer#transformers.ReformerConfig">ReformerConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/reformer#transformers.ReformerForSequenceClassification">ReformerForSequenceClassification</a> (Reformer model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/rembert#transformers.RemBertForSequenceClassification">RemBertForSequenceClassification</a> (RemBERT model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/roformer#transformers.RoFormerForSequenceClassification">RoFormerForSequenceClassification</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/roberta#transformers.RobertaForSequenceClassification">RobertaForSequenceClassification</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/squeezebert#transformers.SqueezeBertConfig">SqueezeBertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/squeezebert#transformers.SqueezeBertForSequenceClassification">SqueezeBertForSequenceClassification</a> (SqueezeBERT model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/tapas#transformers.TapasForSequenceClassification">TapasForSequenceClassification</a> (TAPAS model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/transfo-xl#transformers.TransfoXLConfig">TransfoXLConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/transfo-xl#transformers.TransfoXLForSequenceClassification">TransfoXLForSequenceClassification</a> (Transformer-XL model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/xlm#transformers.XLMForSequenceClassification">XLMForSequenceClassification</a> (XLM model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/xlm-roberta#transformers.XLMRobertaForSequenceClassification">XLMRobertaForSequenceClassification</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig">XLMRobertaXLConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForSequenceClassification">XLMRobertaXLForSequenceClassification</a> (XLM-RoBERTa-XL model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/xlnet#transformers.XLNetForSequenceClassification">XLNetForSequenceClassification</a> (XLNet model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/yoso#transformers.YosoConfig">YosoConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/yoso#transformers.YosoForSequenceClassification">YosoForSequenceClassification</a> (YOSO model)</li>
</ul>`,name:"config"}]}}),FE=new w({props:{code:`from transformers import AutoConfig, AutoModelForSequenceClassification

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForSequenceClassification.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSequenceClassification.from_config(config)`}}),CE=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15297/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15297/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15297/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15297/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15297/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15297/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15297/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),ME=new w({props:{code:`from transformers import AutoConfig, AutoModelForSequenceClassification

# Download model and configuration from huggingface.co and cache.
model = AutoModelForSequenceClassification.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForSequenceClassification.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForSequenceClassification.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSequenceClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),EE=new z({}),yE=new E({props:{name:"class transformers.AutoModelForMultipleChoice",anchor:"transformers.AutoModelForMultipleChoice",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15297/src/transformers/models/auto/modeling_auto.py#L751"}}),AE=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15297/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15297/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15297/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/albert#transformers.AlbertForMultipleChoice">AlbertForMultipleChoice</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/bert#transformers.BertForMultipleChoice">BertForMultipleChoice</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/big_bird#transformers.BigBirdForMultipleChoice">BigBirdForMultipleChoice</a> (BigBird model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/camembert#transformers.CamembertForMultipleChoice">CamembertForMultipleChoice</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/canine#transformers.CanineConfig">CanineConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/canine#transformers.CanineForMultipleChoice">CanineForMultipleChoice</a> (Canine model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/convbert#transformers.ConvBertForMultipleChoice">ConvBertForMultipleChoice</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/distilbert#transformers.DistilBertForMultipleChoice">DistilBertForMultipleChoice</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/electra#transformers.ElectraForMultipleChoice">ElectraForMultipleChoice</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/fnet#transformers.FNetConfig">FNetConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/fnet#transformers.FNetForMultipleChoice">FNetForMultipleChoice</a> (FNet model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/flaubert#transformers.FlaubertForMultipleChoice">FlaubertForMultipleChoice</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/funnel#transformers.FunnelForMultipleChoice">FunnelForMultipleChoice</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/ibert#transformers.IBertConfig">IBertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/ibert#transformers.IBertForMultipleChoice">IBertForMultipleChoice</a> (I-BERT model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/longformer#transformers.LongformerForMultipleChoice">LongformerForMultipleChoice</a> (Longformer model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/mpnet#transformers.MPNetForMultipleChoice">MPNetForMultipleChoice</a> (MPNet model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/megatron-bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/megatron-bert#transformers.MegatronBertForMultipleChoice">MegatronBertForMultipleChoice</a> (MegatronBert model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/mobilebert#transformers.MobileBertForMultipleChoice">MobileBertForMultipleChoice</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/nystromformer#transformers.NystromformerConfig">NystromformerConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/nystromformer#transformers.NystromformerForMultipleChoice">NystromformerForMultipleChoice</a> (Nystromformer model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/qdqbert#transformers.QDQBertConfig">QDQBertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/qdqbert#transformers.QDQBertForMultipleChoice">QDQBertForMultipleChoice</a> (QDQBert model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/rembert#transformers.RemBertForMultipleChoice">RemBertForMultipleChoice</a> (RemBERT model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/roformer#transformers.RoFormerForMultipleChoice">RoFormerForMultipleChoice</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/roberta#transformers.RobertaForMultipleChoice">RobertaForMultipleChoice</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/squeezebert#transformers.SqueezeBertConfig">SqueezeBertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/squeezebert#transformers.SqueezeBertForMultipleChoice">SqueezeBertForMultipleChoice</a> (SqueezeBERT model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/xlm#transformers.XLMForMultipleChoice">XLMForMultipleChoice</a> (XLM model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/xlm-roberta#transformers.XLMRobertaForMultipleChoice">XLMRobertaForMultipleChoice</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig">XLMRobertaXLConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForMultipleChoice">XLMRobertaXLForMultipleChoice</a> (XLM-RoBERTa-XL model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/xlnet#transformers.XLNetForMultipleChoice">XLNetForMultipleChoice</a> (XLNet model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/yoso#transformers.YosoConfig">YosoConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/yoso#transformers.YosoForMultipleChoice">YosoForMultipleChoice</a> (YOSO model)</li>
</ul>`,name:"config"}]}}),LE=new w({props:{code:`from transformers import AutoConfig, AutoModelForMultipleChoice

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForMultipleChoice.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForMultipleChoice

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMultipleChoice.from_config(config)`}}),BE=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15297/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15297/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15297/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15297/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15297/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15297/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15297/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),kE=new w({props:{code:`from transformers import AutoConfig, AutoModelForMultipleChoice

# Download model and configuration from huggingface.co and cache.
model = AutoModelForMultipleChoice.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForMultipleChoice.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForMultipleChoice.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForMultipleChoice

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMultipleChoice.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMultipleChoice.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMultipleChoice.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),xE=new z({}),RE=new E({props:{name:"class transformers.AutoModelForNextSentencePrediction",anchor:"transformers.AutoModelForNextSentencePrediction",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15297/src/transformers/models/auto/modeling_auto.py#L758"}}),PE=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15297/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15297/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15297/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/bert#transformers.BertForNextSentencePrediction">BertForNextSentencePrediction</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/fnet#transformers.FNetConfig">FNetConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/fnet#transformers.FNetForNextSentencePrediction">FNetForNextSentencePrediction</a> (FNet model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/megatron-bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/megatron-bert#transformers.MegatronBertForNextSentencePrediction">MegatronBertForNextSentencePrediction</a> (MegatronBert model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/mobilebert#transformers.MobileBertForNextSentencePrediction">MobileBertForNextSentencePrediction</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/qdqbert#transformers.QDQBertConfig">QDQBertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/qdqbert#transformers.QDQBertForNextSentencePrediction">QDQBertForNextSentencePrediction</a> (QDQBert model)</li>
</ul>`,name:"config"}]}}),$E=new w({props:{code:`from transformers import AutoConfig, AutoModelForNextSentencePrediction

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForNextSentencePrediction.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForNextSentencePrediction

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForNextSentencePrediction.from_config(config)`}}),IE=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15297/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15297/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15297/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15297/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15297/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15297/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15297/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),jE=new w({props:{code:`from transformers import AutoConfig, AutoModelForNextSentencePrediction

# Download model and configuration from huggingface.co and cache.
model = AutoModelForNextSentencePrediction.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForNextSentencePrediction.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForNextSentencePrediction.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForNextSentencePrediction

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForNextSentencePrediction.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForNextSentencePrediction.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForNextSentencePrediction.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),NE=new z({}),DE=new E({props:{name:"class transformers.AutoModelForTokenClassification",anchor:"transformers.AutoModelForTokenClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15297/src/transformers/models/auto/modeling_auto.py#L744"}}),GE=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15297/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15297/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15297/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/albert#transformers.AlbertForTokenClassification">AlbertForTokenClassification</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/bert#transformers.BertForTokenClassification">BertForTokenClassification</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/big_bird#transformers.BigBirdForTokenClassification">BigBirdForTokenClassification</a> (BigBird model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/camembert#transformers.CamembertForTokenClassification">CamembertForTokenClassification</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/canine#transformers.CanineConfig">CanineConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/canine#transformers.CanineForTokenClassification">CanineForTokenClassification</a> (Canine model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/convbert#transformers.ConvBertForTokenClassification">ConvBertForTokenClassification</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/deberta#transformers.DebertaForTokenClassification">DebertaForTokenClassification</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/deberta-v2#transformers.DebertaV2ForTokenClassification">DebertaV2ForTokenClassification</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/distilbert#transformers.DistilBertForTokenClassification">DistilBertForTokenClassification</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/electra#transformers.ElectraForTokenClassification">ElectraForTokenClassification</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/fnet#transformers.FNetConfig">FNetConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/fnet#transformers.FNetForTokenClassification">FNetForTokenClassification</a> (FNet model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/flaubert#transformers.FlaubertForTokenClassification">FlaubertForTokenClassification</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/funnel#transformers.FunnelForTokenClassification">FunnelForTokenClassification</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/gpt2#transformers.GPT2ForTokenClassification">GPT2ForTokenClassification</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/ibert#transformers.IBertConfig">IBertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/ibert#transformers.IBertForTokenClassification">IBertForTokenClassification</a> (I-BERT model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/layoutlm#transformers.LayoutLMForTokenClassification">LayoutLMForTokenClassification</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/layoutlmv2#transformers.LayoutLMv2Config">LayoutLMv2Config</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/layoutlmv2#transformers.LayoutLMv2ForTokenClassification">LayoutLMv2ForTokenClassification</a> (LayoutLMv2 model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/longformer#transformers.LongformerForTokenClassification">LongformerForTokenClassification</a> (Longformer model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/mpnet#transformers.MPNetForTokenClassification">MPNetForTokenClassification</a> (MPNet model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/megatron-bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/megatron-bert#transformers.MegatronBertForTokenClassification">MegatronBertForTokenClassification</a> (MegatronBert model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/mobilebert#transformers.MobileBertForTokenClassification">MobileBertForTokenClassification</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/nystromformer#transformers.NystromformerConfig">NystromformerConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/nystromformer#transformers.NystromformerForTokenClassification">NystromformerForTokenClassification</a> (Nystromformer model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/qdqbert#transformers.QDQBertConfig">QDQBertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/qdqbert#transformers.QDQBertForTokenClassification">QDQBertForTokenClassification</a> (QDQBert model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/rembert#transformers.RemBertForTokenClassification">RemBertForTokenClassification</a> (RemBERT model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/roformer#transformers.RoFormerForTokenClassification">RoFormerForTokenClassification</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/roberta#transformers.RobertaForTokenClassification">RobertaForTokenClassification</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/squeezebert#transformers.SqueezeBertConfig">SqueezeBertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/squeezebert#transformers.SqueezeBertForTokenClassification">SqueezeBertForTokenClassification</a> (SqueezeBERT model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/xlm#transformers.XLMForTokenClassification">XLMForTokenClassification</a> (XLM model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/xlm-roberta#transformers.XLMRobertaForTokenClassification">XLMRobertaForTokenClassification</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig">XLMRobertaXLConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForTokenClassification">XLMRobertaXLForTokenClassification</a> (XLM-RoBERTa-XL model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/xlnet#transformers.XLNetForTokenClassification">XLNetForTokenClassification</a> (XLNet model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/yoso#transformers.YosoConfig">YosoConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/yoso#transformers.YosoForTokenClassification">YosoForTokenClassification</a> (YOSO model)</li>
</ul>`,name:"config"}]}}),OE=new w({props:{code:`from transformers import AutoConfig, AutoModelForTokenClassification

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForTokenClassification.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForTokenClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForTokenClassification.from_config(config)`}}),XE=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15297/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15297/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15297/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15297/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15297/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15297/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15297/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),zE=new w({props:{code:`from transformers import AutoConfig, AutoModelForTokenClassification

# Download model and configuration from huggingface.co and cache.
model = AutoModelForTokenClassification.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForTokenClassification.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForTokenClassification.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForTokenClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForTokenClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForTokenClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForTokenClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),VE=new z({}),WE=new E({props:{name:"class transformers.AutoModelForQuestionAnswering",anchor:"transformers.AutoModelForQuestionAnswering",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15297/src/transformers/models/auto/modeling_auto.py#L726"}}),HE=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15297/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15297/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15297/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/albert#transformers.AlbertForQuestionAnswering">AlbertForQuestionAnswering</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/bart#transformers.BartForQuestionAnswering">BartForQuestionAnswering</a> (BART model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/bert#transformers.BertForQuestionAnswering">BertForQuestionAnswering</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/big_bird#transformers.BigBirdForQuestionAnswering">BigBirdForQuestionAnswering</a> (BigBird model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusConfig">BigBirdPegasusConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForQuestionAnswering">BigBirdPegasusForQuestionAnswering</a> (BigBirdPegasus model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/camembert#transformers.CamembertForQuestionAnswering">CamembertForQuestionAnswering</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/canine#transformers.CanineConfig">CanineConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/canine#transformers.CanineForQuestionAnswering">CanineForQuestionAnswering</a> (Canine model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/convbert#transformers.ConvBertForQuestionAnswering">ConvBertForQuestionAnswering</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/deberta#transformers.DebertaForQuestionAnswering">DebertaForQuestionAnswering</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/deberta-v2#transformers.DebertaV2ForQuestionAnswering">DebertaV2ForQuestionAnswering</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/distilbert#transformers.DistilBertForQuestionAnswering">DistilBertForQuestionAnswering</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/electra#transformers.ElectraForQuestionAnswering">ElectraForQuestionAnswering</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/fnet#transformers.FNetConfig">FNetConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/fnet#transformers.FNetForQuestionAnswering">FNetForQuestionAnswering</a> (FNet model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/flaubert#transformers.FlaubertForQuestionAnsweringSimple">FlaubertForQuestionAnsweringSimple</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/funnel#transformers.FunnelForQuestionAnswering">FunnelForQuestionAnswering</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/gptj#transformers.GPTJConfig">GPTJConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/gptj#transformers.GPTJForQuestionAnswering">GPTJForQuestionAnswering</a> (GPT-J model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/ibert#transformers.IBertConfig">IBertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/ibert#transformers.IBertForQuestionAnswering">IBertForQuestionAnswering</a> (I-BERT model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/led#transformers.LEDConfig">LEDConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/led#transformers.LEDForQuestionAnswering">LEDForQuestionAnswering</a> (LED model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/layoutlmv2#transformers.LayoutLMv2Config">LayoutLMv2Config</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/layoutlmv2#transformers.LayoutLMv2ForQuestionAnswering">LayoutLMv2ForQuestionAnswering</a> (LayoutLMv2 model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/longformer#transformers.LongformerForQuestionAnswering">LongformerForQuestionAnswering</a> (Longformer model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/lxmert#transformers.LxmertConfig">LxmertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/lxmert#transformers.LxmertForQuestionAnswering">LxmertForQuestionAnswering</a> (LXMERT model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/mbart#transformers.MBartForQuestionAnswering">MBartForQuestionAnswering</a> (mBART model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/mpnet#transformers.MPNetForQuestionAnswering">MPNetForQuestionAnswering</a> (MPNet model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/megatron-bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/megatron-bert#transformers.MegatronBertForQuestionAnswering">MegatronBertForQuestionAnswering</a> (MegatronBert model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/mobilebert#transformers.MobileBertForQuestionAnswering">MobileBertForQuestionAnswering</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/nystromformer#transformers.NystromformerConfig">NystromformerConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/nystromformer#transformers.NystromformerForQuestionAnswering">NystromformerForQuestionAnswering</a> (Nystromformer model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/qdqbert#transformers.QDQBertConfig">QDQBertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/qdqbert#transformers.QDQBertForQuestionAnswering">QDQBertForQuestionAnswering</a> (QDQBert model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/reformer#transformers.ReformerConfig">ReformerConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/reformer#transformers.ReformerForQuestionAnswering">ReformerForQuestionAnswering</a> (Reformer model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/rembert#transformers.RemBertForQuestionAnswering">RemBertForQuestionAnswering</a> (RemBERT model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/roformer#transformers.RoFormerForQuestionAnswering">RoFormerForQuestionAnswering</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/roberta#transformers.RobertaForQuestionAnswering">RobertaForQuestionAnswering</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/splinter#transformers.SplinterConfig">SplinterConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/splinter#transformers.SplinterForQuestionAnswering">SplinterForQuestionAnswering</a> (Splinter model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/squeezebert#transformers.SqueezeBertConfig">SqueezeBertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/squeezebert#transformers.SqueezeBertForQuestionAnswering">SqueezeBertForQuestionAnswering</a> (SqueezeBERT model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/xlm#transformers.XLMForQuestionAnsweringSimple">XLMForQuestionAnsweringSimple</a> (XLM model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/xlm-roberta#transformers.XLMRobertaForQuestionAnswering">XLMRobertaForQuestionAnswering</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig">XLMRobertaXLConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForQuestionAnswering">XLMRobertaXLForQuestionAnswering</a> (XLM-RoBERTa-XL model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/xlnet#transformers.XLNetForQuestionAnsweringSimple">XLNetForQuestionAnsweringSimple</a> (XLNet model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/yoso#transformers.YosoConfig">YosoConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/yoso#transformers.YosoForQuestionAnswering">YosoForQuestionAnswering</a> (YOSO model)</li>
</ul>`,name:"config"}]}}),UE=new w({props:{code:`from transformers import AutoConfig, AutoModelForQuestionAnswering

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForQuestionAnswering.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForQuestionAnswering.from_config(config)`}}),JE=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15297/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15297/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15297/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15297/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15297/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15297/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15297/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),YE=new w({props:{code:`from transformers import AutoConfig, AutoModelForQuestionAnswering

# Download model and configuration from huggingface.co and cache.
model = AutoModelForQuestionAnswering.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForQuestionAnswering.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForQuestionAnswering.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForQuestionAnswering.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),KE=new z({}),ZE=new E({props:{name:"class transformers.AutoModelForTableQuestionAnswering",anchor:"transformers.AutoModelForTableQuestionAnswering",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15297/src/transformers/models/auto/modeling_auto.py#L733"}}),o3=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15297/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15297/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15297/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/tapas#transformers.TapasForQuestionAnswering">TapasForQuestionAnswering</a> (TAPAS model)</li>
</ul>`,name:"config"}]}}),r3=new w({props:{code:`from transformers import AutoConfig, AutoModelForTableQuestionAnswering

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("google/tapas-base-finetuned-wtq")
model = AutoModelForTableQuestionAnswering.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForTableQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google/tapas-base-finetuned-wtq&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForTableQuestionAnswering.from_config(config)`}}),t3=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15297/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15297/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15297/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15297/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15297/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15297/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15297/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),a3=new w({props:{code:`from transformers import AutoConfig, AutoModelForTableQuestionAnswering

# Download model and configuration from huggingface.co and cache.
model = AutoModelForTableQuestionAnswering.from_pretrained("google/tapas-base-finetuned-wtq")

# Update configuration during loading
model = AutoModelForTableQuestionAnswering.from_pretrained("google/tapas-base-finetuned-wtq", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/tapas_tf_model_config.json")
model = AutoModelForTableQuestionAnswering.from_pretrained(
    "./tf_model/tapas_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForTableQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForTableQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;google/tapas-base-finetuned-wtq&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForTableQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;google/tapas-base-finetuned-wtq&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/tapas_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForTableQuestionAnswering.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/tapas_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),n3=new z({}),s3=new E({props:{name:"class transformers.AutoModelForImageClassification",anchor:"transformers.AutoModelForImageClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15297/src/transformers/models/auto/modeling_auto.py#L767"}}),i3=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15297/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15297/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15297/en/model_doc/beit#transformers.BeitConfig">BeitConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/beit#transformers.BeitForImageClassification">BeitForImageClassification</a> (BEiT model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/convnext#transformers.ConvNextConfig">ConvNextConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/convnext#transformers.ConvNextForImageClassification">ConvNextForImageClassification</a> (ConvNext model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/deit#transformers.DeiTConfig">DeiTConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/deit#transformers.DeiTForImageClassification">DeiTForImageClassification</a> or <a href="/docs/transformers/pr_15297/en/model_doc/deit#transformers.DeiTForImageClassificationWithTeacher">DeiTForImageClassificationWithTeacher</a> (DeiT model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/imagegpt#transformers.ImageGPTConfig">ImageGPTConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/imagegpt#transformers.ImageGPTForImageClassification">ImageGPTForImageClassification</a> (ImageGPT model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/perceiver#transformers.PerceiverConfig">PerceiverConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/perceiver#transformers.PerceiverForImageClassificationLearned">PerceiverForImageClassificationLearned</a> or <a href="/docs/transformers/pr_15297/en/model_doc/perceiver#transformers.PerceiverForImageClassificationFourier">PerceiverForImageClassificationFourier</a> or <a href="/docs/transformers/pr_15297/en/model_doc/perceiver#transformers.PerceiverForImageClassificationConvProcessing">PerceiverForImageClassificationConvProcessing</a> (Perceiver model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/poolformer#transformers.PoolFormerConfig">PoolFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/poolformer#transformers.PoolFormerForImageClassification">PoolFormerForImageClassification</a> (PoolFormer model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/segformer#transformers.SegformerConfig">SegformerConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/segformer#transformers.SegformerForImageClassification">SegformerForImageClassification</a> (SegFormer model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/swin#transformers.SwinConfig">SwinConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/swin#transformers.SwinForImageClassification">SwinForImageClassification</a> (Swin model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/vit#transformers.ViTConfig">ViTConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/vit#transformers.ViTForImageClassification">ViTForImageClassification</a> (ViT model)</li>
</ul>`,name:"config"}]}}),d3=new w({props:{code:`from transformers import AutoConfig, AutoModelForImageClassification

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForImageClassification.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForImageClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageClassification.from_config(config)`}}),c3=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15297/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15297/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15297/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15297/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15297/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15297/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15297/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),f3=new w({props:{code:`from transformers import AutoConfig, AutoModelForImageClassification

# Download model and configuration from huggingface.co and cache.
model = AutoModelForImageClassification.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForImageClassification.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForImageClassification.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForImageClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),m3=new z({}),g3=new E({props:{name:"class transformers.AutoModelForVision2Seq",anchor:"transformers.AutoModelForVision2Seq",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15297/src/transformers/models/auto/modeling_auto.py#L797"}}),p3=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15297/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15297/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15297/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderConfig">VisionEncoderDecoderConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderModel">VisionEncoderDecoderModel</a> (Vision Encoder decoder model)</li>
</ul>`,name:"config"}]}}),_3=new w({props:{code:`from transformers import AutoConfig, AutoModelForVision2Seq

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForVision2Seq.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForVision2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForVision2Seq.from_config(config)`}}),u3=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15297/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15297/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15297/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15297/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15297/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15297/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15297/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),b3=new w({props:{code:`from transformers import AutoConfig, AutoModelForVision2Seq

# Download model and configuration from huggingface.co and cache.
model = AutoModelForVision2Seq.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForVision2Seq.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForVision2Seq.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForVision2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForVision2Seq.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForVision2Seq.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForVision2Seq.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),v3=new z({}),T3=new E({props:{name:"class transformers.AutoModelForAudioClassification",anchor:"transformers.AutoModelForAudioClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15297/src/transformers/models/auto/modeling_auto.py#L804"}}),C3=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15297/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15297/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15297/en/model_doc/hubert#transformers.HubertConfig">HubertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/hubert#transformers.HubertForSequenceClassification">HubertForSequenceClassification</a> (Hubert model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/sew#transformers.SEWConfig">SEWConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/sew#transformers.SEWForSequenceClassification">SEWForSequenceClassification</a> (SEW model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/sew-d#transformers.SEWDConfig">SEWDConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/sew-d#transformers.SEWDForSequenceClassification">SEWDForSequenceClassification</a> (SEW-D model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/unispeech#transformers.UniSpeechConfig">UniSpeechConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/unispeech#transformers.UniSpeechForSequenceClassification">UniSpeechForSequenceClassification</a> (UniSpeech model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/unispeech-sat#transformers.UniSpeechSatConfig">UniSpeechSatConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/unispeech-sat#transformers.UniSpeechSatForSequenceClassification">UniSpeechSatForSequenceClassification</a> (UniSpeechSat model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/wav2vec2#transformers.Wav2Vec2ForSequenceClassification">Wav2Vec2ForSequenceClassification</a> (Wav2Vec2 model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/wavlm#transformers.WavLMConfig">WavLMConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/wavlm#transformers.WavLMForSequenceClassification">WavLMForSequenceClassification</a> (WavLM model)</li>
</ul>`,name:"config"}]}}),M3=new w({props:{code:`from transformers import AutoConfig, AutoModelForAudioClassification

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForAudioClassification.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForAudioClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioClassification.from_config(config)`}}),E3=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15297/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15297/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15297/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15297/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15297/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15297/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15297/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),y3=new w({props:{code:`from transformers import AutoConfig, AutoModelForAudioClassification

# Download model and configuration from huggingface.co and cache.
model = AutoModelForAudioClassification.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForAudioClassification.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForAudioClassification.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForAudioClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),w3=new z({}),A3=new E({props:{name:"class transformers.AutoModelForAudioFrameClassification",anchor:"transformers.AutoModelForAudioFrameClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15297/src/transformers/models/auto/modeling_auto.py#L827"}}),B3=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15297/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15297/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15297/en/model_doc/unispeech-sat#transformers.UniSpeechSatConfig">UniSpeechSatConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/unispeech-sat#transformers.UniSpeechSatForAudioFrameClassification">UniSpeechSatForAudioFrameClassification</a> (UniSpeechSat model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/wav2vec2#transformers.Wav2Vec2ForAudioFrameClassification">Wav2Vec2ForAudioFrameClassification</a> (Wav2Vec2 model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/wavlm#transformers.WavLMConfig">WavLMConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/wavlm#transformers.WavLMForAudioFrameClassification">WavLMForAudioFrameClassification</a> (WavLM model)</li>
</ul>`,name:"config"}]}}),k3=new w({props:{code:`from transformers import AutoConfig, AutoModelForAudioFrameClassification

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForAudioFrameClassification.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForAudioFrameClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioFrameClassification.from_config(config)`}}),x3=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15297/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15297/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15297/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15297/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15297/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15297/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15297/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),R3=new w({props:{code:`from transformers import AutoConfig, AutoModelForAudioFrameClassification

# Download model and configuration from huggingface.co and cache.
model = AutoModelForAudioFrameClassification.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForAudioFrameClassification.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForAudioFrameClassification.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForAudioFrameClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioFrameClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioFrameClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioFrameClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),S3=new z({}),P3=new E({props:{name:"class transformers.AutoModelForCTC",anchor:"transformers.AutoModelForCTC",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15297/src/transformers/models/auto/modeling_auto.py#L811"}}),I3=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15297/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15297/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15297/en/model_doc/hubert#transformers.HubertConfig">HubertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/hubert#transformers.HubertForCTC">HubertForCTC</a> (Hubert model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/sew#transformers.SEWConfig">SEWConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/sew#transformers.SEWForCTC">SEWForCTC</a> (SEW model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/sew-d#transformers.SEWDConfig">SEWDConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/sew-d#transformers.SEWDForCTC">SEWDForCTC</a> (SEW-D model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/unispeech#transformers.UniSpeechConfig">UniSpeechConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/unispeech#transformers.UniSpeechForCTC">UniSpeechForCTC</a> (UniSpeech model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/unispeech-sat#transformers.UniSpeechSatConfig">UniSpeechSatConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/unispeech-sat#transformers.UniSpeechSatForCTC">UniSpeechSatForCTC</a> (UniSpeechSat model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/wav2vec2#transformers.Wav2Vec2ForCTC">Wav2Vec2ForCTC</a> (Wav2Vec2 model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/wavlm#transformers.WavLMConfig">WavLMConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/wavlm#transformers.WavLMForCTC">WavLMForCTC</a> (WavLM model)</li>
</ul>`,name:"config"}]}}),j3=new w({props:{code:`from transformers import AutoConfig, AutoModelForCTC

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForCTC.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForCTC

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCTC.from_config(config)`}}),N3=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15297/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15297/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15297/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15297/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15297/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15297/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15297/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),D3=new w({props:{code:`from transformers import AutoConfig, AutoModelForCTC

# Download model and configuration from huggingface.co and cache.
model = AutoModelForCTC.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForCTC.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForCTC.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForCTC

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCTC.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCTC.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCTC.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),q3=new z({}),G3=new E({props:{name:"class transformers.AutoModelForSpeechSeq2Seq",anchor:"transformers.AutoModelForSpeechSeq2Seq",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15297/src/transformers/models/auto/modeling_auto.py#L818"}}),X3=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15297/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15297/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15297/en/model_doc/speech_to_text#transformers.Speech2TextConfig">Speech2TextConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/speech_to_text#transformers.Speech2TextForConditionalGeneration">Speech2TextForConditionalGeneration</a> (Speech2Text model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/speech-encoder-decoder#transformers.SpeechEncoderDecoderConfig">SpeechEncoderDecoderConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/speech-encoder-decoder#transformers.SpeechEncoderDecoderModel">SpeechEncoderDecoderModel</a> (Speech Encoder decoder model)</li>
</ul>`,name:"config"}]}}),z3=new w({props:{code:`from transformers import AutoConfig, AutoModelForSpeechSeq2Seq

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForSpeechSeq2Seq.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForSpeechSeq2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSpeechSeq2Seq.from_config(config)`}}),V3=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15297/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15297/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15297/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15297/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15297/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15297/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15297/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),Q3=new w({props:{code:`from transformers import AutoConfig, AutoModelForSpeechSeq2Seq

# Download model and configuration from huggingface.co and cache.
model = AutoModelForSpeechSeq2Seq.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForSpeechSeq2Seq.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForSpeechSeq2Seq.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForSpeechSeq2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSpeechSeq2Seq.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSpeechSeq2Seq.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSpeechSeq2Seq.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),H3=new z({}),U3=new E({props:{name:"class transformers.AutoModelForAudioXVector",anchor:"transformers.AutoModelForAudioXVector",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15297/src/transformers/models/auto/modeling_auto.py#L836"}}),Y3=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15297/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15297/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15297/en/model_doc/unispeech-sat#transformers.UniSpeechSatConfig">UniSpeechSatConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/unispeech-sat#transformers.UniSpeechSatForXVector">UniSpeechSatForXVector</a> (UniSpeechSat model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/wav2vec2#transformers.Wav2Vec2ForXVector">Wav2Vec2ForXVector</a> (Wav2Vec2 model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/wavlm#transformers.WavLMConfig">WavLMConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/wavlm#transformers.WavLMForXVector">WavLMForXVector</a> (WavLM model)</li>
</ul>`,name:"config"}]}}),K3=new w({props:{code:`from transformers import AutoConfig, AutoModelForAudioXVector

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForAudioXVector.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForAudioXVector

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioXVector.from_config(config)`}}),Z3=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15297/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15297/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15297/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15297/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15297/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15297/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15297/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),ey=new w({props:{code:`from transformers import AutoConfig, AutoModelForAudioXVector

# Download model and configuration from huggingface.co and cache.
model = AutoModelForAudioXVector.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForAudioXVector.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForAudioXVector.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForAudioXVector

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioXVector.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioXVector.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioXVector.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),oy=new z({}),ry=new E({props:{name:"class transformers.AutoModelForMaskedImageModeling",anchor:"transformers.AutoModelForMaskedImageModeling",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15297/src/transformers/models/auto/modeling_auto.py#L843"}}),ay=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15297/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15297/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15297/en/model_doc/deit#transformers.DeiTConfig">DeiTConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/deit#transformers.DeiTForMaskedImageModeling">DeiTForMaskedImageModeling</a> (DeiT model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/swin#transformers.SwinConfig">SwinConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/swin#transformers.SwinForMaskedImageModeling">SwinForMaskedImageModeling</a> (Swin model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/vit#transformers.ViTConfig">ViTConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/vit#transformers.ViTForMaskedImageModeling">ViTForMaskedImageModeling</a> (ViT model)</li>
</ul>`,name:"config"}]}}),ny=new w({props:{code:`from transformers import AutoConfig, AutoModelForMaskedImageModeling

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForMaskedImageModeling.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForMaskedImageModeling

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMaskedImageModeling.from_config(config)`}}),sy=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15297/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15297/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15297/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15297/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15297/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15297/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15297/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),ly=new w({props:{code:`from transformers import AutoConfig, AutoModelForMaskedImageModeling

# Download model and configuration from huggingface.co and cache.
model = AutoModelForMaskedImageModeling.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForMaskedImageModeling.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForMaskedImageModeling.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForMaskedImageModeling

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMaskedImageModeling.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMaskedImageModeling.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMaskedImageModeling.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),iy=new z({}),dy=new E({props:{name:"class transformers.AutoModelForObjectDetection",anchor:"transformers.AutoModelForObjectDetection",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15297/src/transformers/models/auto/modeling_auto.py#L790"}}),fy=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15297/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15297/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15297/en/model_doc/detr#transformers.DetrConfig">DetrConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/detr#transformers.DetrForObjectDetection">DetrForObjectDetection</a> (DETR model)</li>
</ul>`,name:"config"}]}}),my=new w({props:{code:`from transformers import AutoConfig, AutoModelForObjectDetection

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForObjectDetection.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForObjectDetection

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForObjectDetection.from_config(config)`}}),gy=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15297/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15297/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15297/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15297/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15297/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15297/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15297/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),hy=new w({props:{code:`from transformers import AutoConfig, AutoModelForObjectDetection

# Download model and configuration from huggingface.co and cache.
model = AutoModelForObjectDetection.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForObjectDetection.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForObjectDetection.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForObjectDetection

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForObjectDetection.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForObjectDetection.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForObjectDetection.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),py=new z({}),_y=new E({props:{name:"class transformers.AutoModelForImageSegmentation",anchor:"transformers.AutoModelForImageSegmentation",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15297/src/transformers/models/auto/modeling_auto.py#L774"}}),by=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15297/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15297/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15297/en/model_doc/detr#transformers.DetrConfig">DetrConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/detr#transformers.DetrForSegmentation">DetrForSegmentation</a> (DETR model)</li>
</ul>`,name:"config"}]}}),vy=new w({props:{code:`from transformers import AutoConfig, AutoModelForImageSegmentation

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForImageSegmentation.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForImageSegmentation

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageSegmentation.from_config(config)`}}),Ty=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15297/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15297/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15297/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15297/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15297/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15297/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15297/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),Fy=new w({props:{code:`from transformers import AutoConfig, AutoModelForImageSegmentation

# Download model and configuration from huggingface.co and cache.
model = AutoModelForImageSegmentation.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForImageSegmentation.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForImageSegmentation.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForImageSegmentation

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageSegmentation.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageSegmentation.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageSegmentation.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),Cy=new z({}),My=new E({props:{name:"class transformers.AutoModelForSemanticSegmentation",anchor:"transformers.AutoModelForSemanticSegmentation",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15297/src/transformers/models/auto/modeling_auto.py#L781"}}),yy=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15297/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15297/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15297/en/model_doc/beit#transformers.BeitConfig">BeitConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/beit#transformers.BeitForSemanticSegmentation">BeitForSemanticSegmentation</a> (BEiT model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/segformer#transformers.SegformerConfig">SegformerConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/segformer#transformers.SegformerForSemanticSegmentation">SegformerForSemanticSegmentation</a> (SegFormer model)</li>
</ul>`,name:"config"}]}}),wy=new w({props:{code:`from transformers import AutoConfig, AutoModelForSemanticSegmentation

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForSemanticSegmentation.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForSemanticSegmentation

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSemanticSegmentation.from_config(config)`}}),Ay=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15297/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15297/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15297/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15297/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15297/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15297/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15297/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),By=new w({props:{code:`from transformers import AutoConfig, AutoModelForSemanticSegmentation

# Download model and configuration from huggingface.co and cache.
model = AutoModelForSemanticSegmentation.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForSemanticSegmentation.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForSemanticSegmentation.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForSemanticSegmentation

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSemanticSegmentation.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSemanticSegmentation.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSemanticSegmentation.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),ky=new z({}),xy=new E({props:{name:"class transformers.TFAutoModel",anchor:"transformers.TFAutoModel",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15297/src/transformers/models/auto/modeling_tf_auto.py#L371"}}),Sy=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15297/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15297/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15297/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/albert#transformers.TFAlbertModel">TFAlbertModel</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/bart#transformers.TFBartModel">TFBartModel</a> (BART model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/bert#transformers.TFBertModel">TFBertModel</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/blenderbot#transformers.BlenderbotConfig">BlenderbotConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/blenderbot#transformers.TFBlenderbotModel">TFBlenderbotModel</a> (Blenderbot model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/blenderbot-small#transformers.BlenderbotSmallConfig">BlenderbotSmallConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/blenderbot-small#transformers.TFBlenderbotSmallModel">TFBlenderbotSmallModel</a> (BlenderbotSmall model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/clip#transformers.CLIPConfig">CLIPConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/clip#transformers.TFCLIPModel">TFCLIPModel</a> (CLIP model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/ctrl#transformers.CTRLConfig">CTRLConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/ctrl#transformers.TFCTRLModel">TFCTRLModel</a> (CTRL model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/camembert#transformers.TFCamembertModel">TFCamembertModel</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/convbert#transformers.TFConvBertModel">TFConvBertModel</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/dpr#transformers.DPRConfig">DPRConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/dpr#transformers.TFDPRQuestionEncoder">TFDPRQuestionEncoder</a> (DPR model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/deberta#transformers.TFDebertaModel">TFDebertaModel</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/deberta-v2#transformers.TFDebertaV2Model">TFDebertaV2Model</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/distilbert#transformers.TFDistilBertModel">TFDistilBertModel</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/electra#transformers.TFElectraModel">TFElectraModel</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/flaubert#transformers.TFFlaubertModel">TFFlaubertModel</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/funnel#transformers.TFFunnelModel">TFFunnelModel</a> or <a href="/docs/transformers/pr_15297/en/model_doc/funnel#transformers.TFFunnelBaseModel">TFFunnelBaseModel</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/gpt2#transformers.TFGPT2Model">TFGPT2Model</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/hubert#transformers.HubertConfig">HubertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/hubert#transformers.TFHubertModel">TFHubertModel</a> (Hubert model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/led#transformers.LEDConfig">LEDConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/led#transformers.TFLEDModel">TFLEDModel</a> (LED model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/layoutlm#transformers.TFLayoutLMModel">TFLayoutLMModel</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/longformer#transformers.TFLongformerModel">TFLongformerModel</a> (Longformer model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/lxmert#transformers.LxmertConfig">LxmertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/lxmert#transformers.TFLxmertModel">TFLxmertModel</a> (LXMERT model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/mbart#transformers.TFMBartModel">TFMBartModel</a> (mBART model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/mpnet#transformers.TFMPNetModel">TFMPNetModel</a> (MPNet model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/mt5#transformers.MT5Config">MT5Config</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/mt5#transformers.TFMT5Model">TFMT5Model</a> (mT5 model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/marian#transformers.MarianConfig">MarianConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/marian#transformers.TFMarianModel">TFMarianModel</a> (Marian model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/mobilebert#transformers.TFMobileBertModel">TFMobileBertModel</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig">OpenAIGPTConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/openai-gpt#transformers.TFOpenAIGPTModel">TFOpenAIGPTModel</a> (OpenAI GPT model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/pegasus#transformers.PegasusConfig">PegasusConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/pegasus#transformers.TFPegasusModel">TFPegasusModel</a> (Pegasus model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/rembert#transformers.TFRemBertModel">TFRemBertModel</a> (RemBERT model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/roformer#transformers.TFRoFormerModel">TFRoFormerModel</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/roberta#transformers.TFRobertaModel">TFRobertaModel</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/speech_to_text#transformers.Speech2TextConfig">Speech2TextConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/speech_to_text#transformers.TFSpeech2TextModel">TFSpeech2TextModel</a> (Speech2Text model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/t5#transformers.TFT5Model">TFT5Model</a> (T5 model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/tapas#transformers.TFTapasModel">TFTapasModel</a> (TAPAS model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/transfo-xl#transformers.TransfoXLConfig">TransfoXLConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/transfo-xl#transformers.TFTransfoXLModel">TFTransfoXLModel</a> (Transformer-XL model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/vit#transformers.ViTConfig">ViTConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/vit#transformers.TFViTModel">TFViTModel</a> (ViT model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/wav2vec2#transformers.TFWav2Vec2Model">TFWav2Vec2Model</a> (Wav2Vec2 model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/xlm#transformers.TFXLMModel">TFXLMModel</a> (XLM model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/xlm-roberta#transformers.TFXLMRobertaModel">TFXLMRobertaModel</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/xlnet#transformers.TFXLNetModel">TFXLNetModel</a> (XLNet model)</li>
</ul>`,name:"config"}]}}),Py=new w({props:{code:`from transformers import AutoConfig, TFAutoModel

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = TFAutoModel.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModel.from_config(config)`}}),$y=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15297/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15297/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15297/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15297/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15297/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),Iy=new w({props:{code:`from transformers import AutoConfig, TFAutoModel

# Download model and configuration from huggingface.co and cache.
model = TFAutoModel.from_pretrained("bert-base-cased")

# Update configuration during loading
model = TFAutoModel.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = TFAutoModel.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModel.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModel.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModel.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),jy=new z({}),Ny=new E({props:{name:"class transformers.TFAutoModelForPreTraining",anchor:"transformers.TFAutoModelForPreTraining",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15297/src/transformers/models/auto/modeling_tf_auto.py#L378"}}),qy=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15297/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15297/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15297/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/albert#transformers.TFAlbertForPreTraining">TFAlbertForPreTraining</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/bart#transformers.TFBartForConditionalGeneration">TFBartForConditionalGeneration</a> (BART model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/bert#transformers.TFBertForPreTraining">TFBertForPreTraining</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/ctrl#transformers.CTRLConfig">CTRLConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/ctrl#transformers.TFCTRLLMHeadModel">TFCTRLLMHeadModel</a> (CTRL model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/camembert#transformers.TFCamembertForMaskedLM">TFCamembertForMaskedLM</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/distilbert#transformers.TFDistilBertForMaskedLM">TFDistilBertForMaskedLM</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/electra#transformers.TFElectraForPreTraining">TFElectraForPreTraining</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/flaubert#transformers.TFFlaubertWithLMHeadModel">TFFlaubertWithLMHeadModel</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/funnel#transformers.TFFunnelForPreTraining">TFFunnelForPreTraining</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/gpt2#transformers.TFGPT2LMHeadModel">TFGPT2LMHeadModel</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/layoutlm#transformers.TFLayoutLMForMaskedLM">TFLayoutLMForMaskedLM</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/lxmert#transformers.LxmertConfig">LxmertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/lxmert#transformers.TFLxmertForPreTraining">TFLxmertForPreTraining</a> (LXMERT model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/mpnet#transformers.TFMPNetForMaskedLM">TFMPNetForMaskedLM</a> (MPNet model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/mobilebert#transformers.TFMobileBertForPreTraining">TFMobileBertForPreTraining</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig">OpenAIGPTConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/openai-gpt#transformers.TFOpenAIGPTLMHeadModel">TFOpenAIGPTLMHeadModel</a> (OpenAI GPT model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/roberta#transformers.TFRobertaForMaskedLM">TFRobertaForMaskedLM</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/t5#transformers.TFT5ForConditionalGeneration">TFT5ForConditionalGeneration</a> (T5 model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/tapas#transformers.TFTapasForMaskedLM">TFTapasForMaskedLM</a> (TAPAS model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/transfo-xl#transformers.TransfoXLConfig">TransfoXLConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/transfo-xl#transformers.TFTransfoXLLMHeadModel">TFTransfoXLLMHeadModel</a> (Transformer-XL model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/xlm#transformers.TFXLMWithLMHeadModel">TFXLMWithLMHeadModel</a> (XLM model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForMaskedLM">TFXLMRobertaForMaskedLM</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/xlnet#transformers.TFXLNetLMHeadModel">TFXLNetLMHeadModel</a> (XLNet model)</li>
</ul>`,name:"config"}]}}),Gy=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForPreTraining

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = TFAutoModelForPreTraining.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForPreTraining

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForPreTraining.from_config(config)`}}),Oy=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15297/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15297/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15297/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15297/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15297/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),Xy=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForPreTraining

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForPreTraining.from_pretrained("bert-base-cased")

# Update configuration during loading
model = TFAutoModelForPreTraining.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = TFAutoModelForPreTraining.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForPreTraining

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForPreTraining.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForPreTraining.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForPreTraining.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),zy=new z({}),Vy=new E({props:{name:"class transformers.TFAutoModelForCausalLM",anchor:"transformers.TFAutoModelForCausalLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15297/src/transformers/models/auto/modeling_tf_auto.py#L393"}}),Qy=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15297/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15297/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15297/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/bert#transformers.TFBertLMHeadModel">TFBertLMHeadModel</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/ctrl#transformers.CTRLConfig">CTRLConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/ctrl#transformers.TFCTRLLMHeadModel">TFCTRLLMHeadModel</a> (CTRL model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/gpt2#transformers.TFGPT2LMHeadModel">TFGPT2LMHeadModel</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig">OpenAIGPTConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/openai-gpt#transformers.TFOpenAIGPTLMHeadModel">TFOpenAIGPTLMHeadModel</a> (OpenAI GPT model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/rembert#transformers.TFRemBertForCausalLM">TFRemBertForCausalLM</a> (RemBERT model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/roformer#transformers.TFRoFormerForCausalLM">TFRoFormerForCausalLM</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/roberta#transformers.TFRobertaForCausalLM">TFRobertaForCausalLM</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/transfo-xl#transformers.TransfoXLConfig">TransfoXLConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/transfo-xl#transformers.TFTransfoXLLMHeadModel">TFTransfoXLLMHeadModel</a> (Transformer-XL model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/xlm#transformers.TFXLMWithLMHeadModel">TFXLMWithLMHeadModel</a> (XLM model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/xlnet#transformers.TFXLNetLMHeadModel">TFXLNetLMHeadModel</a> (XLNet model)</li>
</ul>`,name:"config"}]}}),Hy=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForCausalLM

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = TFAutoModelForCausalLM.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForCausalLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForCausalLM.from_config(config)`}}),Uy=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15297/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15297/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15297/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15297/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15297/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),Jy=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForCausalLM

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForCausalLM.from_pretrained("bert-base-cased")

# Update configuration during loading
model = TFAutoModelForCausalLM.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = TFAutoModelForCausalLM.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForCausalLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForCausalLM.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),Yy=new z({}),Ky=new E({props:{name:"class transformers.TFAutoModelForImageClassification",anchor:"transformers.TFAutoModelForImageClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15297/src/transformers/models/auto/modeling_tf_auto.py#L400"}}),ew=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15297/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15297/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15297/en/model_doc/vit#transformers.ViTConfig">ViTConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/vit#transformers.TFViTForImageClassification">TFViTForImageClassification</a> (ViT model)</li>
</ul>`,name:"config"}]}}),ow=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForImageClassification

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = TFAutoModelForImageClassification.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForImageClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForImageClassification.from_config(config)`}}),rw=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15297/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15297/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15297/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15297/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15297/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),tw=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForImageClassification

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForImageClassification.from_pretrained("bert-base-cased")

# Update configuration during loading
model = TFAutoModelForImageClassification.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = TFAutoModelForImageClassification.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForImageClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForImageClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForImageClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForImageClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),aw=new z({}),nw=new E({props:{name:"class transformers.TFAutoModelForMaskedLM",anchor:"transformers.TFAutoModelForMaskedLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15297/src/transformers/models/auto/modeling_tf_auto.py#L414"}}),lw=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15297/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15297/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15297/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/albert#transformers.TFAlbertForMaskedLM">TFAlbertForMaskedLM</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/bert#transformers.TFBertForMaskedLM">TFBertForMaskedLM</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/camembert#transformers.TFCamembertForMaskedLM">TFCamembertForMaskedLM</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/convbert#transformers.TFConvBertForMaskedLM">TFConvBertForMaskedLM</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/deberta#transformers.TFDebertaForMaskedLM">TFDebertaForMaskedLM</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/deberta-v2#transformers.TFDebertaV2ForMaskedLM">TFDebertaV2ForMaskedLM</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/distilbert#transformers.TFDistilBertForMaskedLM">TFDistilBertForMaskedLM</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/electra#transformers.TFElectraForMaskedLM">TFElectraForMaskedLM</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/flaubert#transformers.TFFlaubertWithLMHeadModel">TFFlaubertWithLMHeadModel</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/funnel#transformers.TFFunnelForMaskedLM">TFFunnelForMaskedLM</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/layoutlm#transformers.TFLayoutLMForMaskedLM">TFLayoutLMForMaskedLM</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/longformer#transformers.TFLongformerForMaskedLM">TFLongformerForMaskedLM</a> (Longformer model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/mpnet#transformers.TFMPNetForMaskedLM">TFMPNetForMaskedLM</a> (MPNet model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/mobilebert#transformers.TFMobileBertForMaskedLM">TFMobileBertForMaskedLM</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/rembert#transformers.TFRemBertForMaskedLM">TFRemBertForMaskedLM</a> (RemBERT model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/roformer#transformers.TFRoFormerForMaskedLM">TFRoFormerForMaskedLM</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/roberta#transformers.TFRobertaForMaskedLM">TFRobertaForMaskedLM</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/tapas#transformers.TFTapasForMaskedLM">TFTapasForMaskedLM</a> (TAPAS model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/xlm#transformers.TFXLMWithLMHeadModel">TFXLMWithLMHeadModel</a> (XLM model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForMaskedLM">TFXLMRobertaForMaskedLM</a> (XLM-RoBERTa model)</li>
</ul>`,name:"config"}]}}),iw=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForMaskedLM

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = TFAutoModelForMaskedLM.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForMaskedLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForMaskedLM.from_config(config)`}}),dw=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15297/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15297/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15297/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15297/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15297/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),cw=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForMaskedLM

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForMaskedLM.from_pretrained("bert-base-cased")

# Update configuration during loading
model = TFAutoModelForMaskedLM.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = TFAutoModelForMaskedLM.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForMaskedLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForMaskedLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForMaskedLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForMaskedLM.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),fw=new z({}),mw=new E({props:{name:"class transformers.TFAutoModelForSeq2SeqLM",anchor:"transformers.TFAutoModelForSeq2SeqLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15297/src/transformers/models/auto/modeling_tf_auto.py#L421"}}),hw=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15297/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15297/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15297/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/bart#transformers.TFBartForConditionalGeneration">TFBartForConditionalGeneration</a> (BART model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/blenderbot#transformers.BlenderbotConfig">BlenderbotConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/blenderbot#transformers.TFBlenderbotForConditionalGeneration">TFBlenderbotForConditionalGeneration</a> (Blenderbot model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/blenderbot-small#transformers.BlenderbotSmallConfig">BlenderbotSmallConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/blenderbot-small#transformers.TFBlenderbotSmallForConditionalGeneration">TFBlenderbotSmallForConditionalGeneration</a> (BlenderbotSmall model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/encoder-decoder#transformers.EncoderDecoderConfig">EncoderDecoderConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/encoder-decoder#transformers.TFEncoderDecoderModel">TFEncoderDecoderModel</a> (Encoder decoder model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/led#transformers.LEDConfig">LEDConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/led#transformers.TFLEDForConditionalGeneration">TFLEDForConditionalGeneration</a> (LED model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/mbart#transformers.TFMBartForConditionalGeneration">TFMBartForConditionalGeneration</a> (mBART model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/mt5#transformers.MT5Config">MT5Config</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/mt5#transformers.TFMT5ForConditionalGeneration">TFMT5ForConditionalGeneration</a> (mT5 model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/marian#transformers.MarianConfig">MarianConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/marian#transformers.TFMarianMTModel">TFMarianMTModel</a> (Marian model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/pegasus#transformers.PegasusConfig">PegasusConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/pegasus#transformers.TFPegasusForConditionalGeneration">TFPegasusForConditionalGeneration</a> (Pegasus model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/t5#transformers.TFT5ForConditionalGeneration">TFT5ForConditionalGeneration</a> (T5 model)</li>
</ul>`,name:"config"}]}}),pw=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForSeq2SeqLM

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("t5-base")
model = TFAutoModelForSeq2SeqLM.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForSeq2SeqLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSeq2SeqLM.from_config(config)`}}),_w=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15297/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15297/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15297/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15297/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15297/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),uw=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForSeq2SeqLM

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForSeq2SeqLM.from_pretrained("t5-base")

# Update configuration during loading
model = TFAutoModelForSeq2SeqLM.from_pretrained("t5-base", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/t5_pt_model_config.json")
model = TFAutoModelForSeq2SeqLM.from_pretrained(
    "./pt_model/t5_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForSeq2SeqLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/t5_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSeq2SeqLM.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/t5_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),bw=new z({}),vw=new E({props:{name:"class transformers.TFAutoModelForSequenceClassification",anchor:"transformers.TFAutoModelForSequenceClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15297/src/transformers/models/auto/modeling_tf_auto.py#L430"}}),Fw=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15297/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15297/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15297/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/albert#transformers.TFAlbertForSequenceClassification">TFAlbertForSequenceClassification</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/bert#transformers.TFBertForSequenceClassification">TFBertForSequenceClassification</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/ctrl#transformers.CTRLConfig">CTRLConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/ctrl#transformers.TFCTRLForSequenceClassification">TFCTRLForSequenceClassification</a> (CTRL model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/camembert#transformers.TFCamembertForSequenceClassification">TFCamembertForSequenceClassification</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/convbert#transformers.TFConvBertForSequenceClassification">TFConvBertForSequenceClassification</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/deberta#transformers.TFDebertaForSequenceClassification">TFDebertaForSequenceClassification</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/deberta-v2#transformers.TFDebertaV2ForSequenceClassification">TFDebertaV2ForSequenceClassification</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/distilbert#transformers.TFDistilBertForSequenceClassification">TFDistilBertForSequenceClassification</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/electra#transformers.TFElectraForSequenceClassification">TFElectraForSequenceClassification</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/flaubert#transformers.TFFlaubertForSequenceClassification">TFFlaubertForSequenceClassification</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/funnel#transformers.TFFunnelForSequenceClassification">TFFunnelForSequenceClassification</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/gpt2#transformers.TFGPT2ForSequenceClassification">TFGPT2ForSequenceClassification</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/layoutlm#transformers.TFLayoutLMForSequenceClassification">TFLayoutLMForSequenceClassification</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/longformer#transformers.TFLongformerForSequenceClassification">TFLongformerForSequenceClassification</a> (Longformer model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/mpnet#transformers.TFMPNetForSequenceClassification">TFMPNetForSequenceClassification</a> (MPNet model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/mobilebert#transformers.TFMobileBertForSequenceClassification">TFMobileBertForSequenceClassification</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig">OpenAIGPTConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/openai-gpt#transformers.TFOpenAIGPTForSequenceClassification">TFOpenAIGPTForSequenceClassification</a> (OpenAI GPT model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/rembert#transformers.TFRemBertForSequenceClassification">TFRemBertForSequenceClassification</a> (RemBERT model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/roformer#transformers.TFRoFormerForSequenceClassification">TFRoFormerForSequenceClassification</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/roberta#transformers.TFRobertaForSequenceClassification">TFRobertaForSequenceClassification</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/tapas#transformers.TFTapasForSequenceClassification">TFTapasForSequenceClassification</a> (TAPAS model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/transfo-xl#transformers.TransfoXLConfig">TransfoXLConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/transfo-xl#transformers.TFTransfoXLForSequenceClassification">TFTransfoXLForSequenceClassification</a> (Transformer-XL model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/xlm#transformers.TFXLMForSequenceClassification">TFXLMForSequenceClassification</a> (XLM model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForSequenceClassification">TFXLMRobertaForSequenceClassification</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/xlnet#transformers.TFXLNetForSequenceClassification">TFXLNetForSequenceClassification</a> (XLNet model)</li>
</ul>`,name:"config"}]}}),Cw=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForSequenceClassification

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = TFAutoModelForSequenceClassification.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSequenceClassification.from_config(config)`}}),Mw=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15297/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15297/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15297/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15297/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15297/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),Ew=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForSequenceClassification

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForSequenceClassification.from_pretrained("bert-base-cased")

# Update configuration during loading
model = TFAutoModelForSequenceClassification.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = TFAutoModelForSequenceClassification.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSequenceClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),yw=new z({}),ww=new E({props:{name:"class transformers.TFAutoModelForMultipleChoice",anchor:"transformers.TFAutoModelForMultipleChoice",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15297/src/transformers/models/auto/modeling_tf_auto.py#L466"}}),Lw=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15297/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15297/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15297/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/albert#transformers.TFAlbertForMultipleChoice">TFAlbertForMultipleChoice</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/bert#transformers.TFBertForMultipleChoice">TFBertForMultipleChoice</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/camembert#transformers.TFCamembertForMultipleChoice">TFCamembertForMultipleChoice</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/convbert#transformers.TFConvBertForMultipleChoice">TFConvBertForMultipleChoice</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/distilbert#transformers.TFDistilBertForMultipleChoice">TFDistilBertForMultipleChoice</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/electra#transformers.TFElectraForMultipleChoice">TFElectraForMultipleChoice</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/flaubert#transformers.TFFlaubertForMultipleChoice">TFFlaubertForMultipleChoice</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/funnel#transformers.TFFunnelForMultipleChoice">TFFunnelForMultipleChoice</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/longformer#transformers.TFLongformerForMultipleChoice">TFLongformerForMultipleChoice</a> (Longformer model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/mpnet#transformers.TFMPNetForMultipleChoice">TFMPNetForMultipleChoice</a> (MPNet model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/mobilebert#transformers.TFMobileBertForMultipleChoice">TFMobileBertForMultipleChoice</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/rembert#transformers.TFRemBertForMultipleChoice">TFRemBertForMultipleChoice</a> (RemBERT model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/roformer#transformers.TFRoFormerForMultipleChoice">TFRoFormerForMultipleChoice</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/roberta#transformers.TFRobertaForMultipleChoice">TFRobertaForMultipleChoice</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/xlm#transformers.TFXLMForMultipleChoice">TFXLMForMultipleChoice</a> (XLM model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForMultipleChoice">TFXLMRobertaForMultipleChoice</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/xlnet#transformers.TFXLNetForMultipleChoice">TFXLNetForMultipleChoice</a> (XLNet model)</li>
</ul>`,name:"config"}]}}),Bw=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForMultipleChoice

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = TFAutoModelForMultipleChoice.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForMultipleChoice

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForMultipleChoice.from_config(config)`}}),kw=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15297/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15297/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15297/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15297/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15297/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),xw=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForMultipleChoice

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForMultipleChoice.from_pretrained("bert-base-cased")

# Update configuration during loading
model = TFAutoModelForMultipleChoice.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = TFAutoModelForMultipleChoice.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForMultipleChoice

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForMultipleChoice.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForMultipleChoice.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForMultipleChoice.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),Rw=new z({}),Sw=new E({props:{name:"class transformers.TFAutoModelForTableQuestionAnswering",anchor:"transformers.TFAutoModelForTableQuestionAnswering",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15297/src/transformers/models/auto/modeling_tf_auto.py#L446"}}),$w=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15297/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15297/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15297/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/tapas#transformers.TFTapasForQuestionAnswering">TFTapasForQuestionAnswering</a> (TAPAS model)</li>
</ul>`,name:"config"}]}}),Iw=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForTableQuestionAnswering

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("google/tapas-base-finetuned-wtq")
model = TFAutoModelForTableQuestionAnswering.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForTableQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google/tapas-base-finetuned-wtq&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForTableQuestionAnswering.from_config(config)`}}),jw=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15297/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15297/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15297/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15297/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15297/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),Nw=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForTableQuestionAnswering

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForTableQuestionAnswering.from_pretrained("google/tapas-base-finetuned-wtq")

# Update configuration during loading
model = TFAutoModelForTableQuestionAnswering.from_pretrained("google/tapas-base-finetuned-wtq", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/tapas_pt_model_config.json")
model = TFAutoModelForTableQuestionAnswering.from_pretrained(
    "./pt_model/tapas_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForTableQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForTableQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;google/tapas-base-finetuned-wtq&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForTableQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;google/tapas-base-finetuned-wtq&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/tapas_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForTableQuestionAnswering.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/tapas_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),Dw=new z({}),qw=new E({props:{name:"class transformers.TFAutoModelForTokenClassification",anchor:"transformers.TFAutoModelForTokenClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15297/src/transformers/models/auto/modeling_tf_auto.py#L457"}}),Ow=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15297/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15297/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15297/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/albert#transformers.TFAlbertForTokenClassification">TFAlbertForTokenClassification</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/bert#transformers.TFBertForTokenClassification">TFBertForTokenClassification</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/camembert#transformers.TFCamembertForTokenClassification">TFCamembertForTokenClassification</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/convbert#transformers.TFConvBertForTokenClassification">TFConvBertForTokenClassification</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/deberta#transformers.TFDebertaForTokenClassification">TFDebertaForTokenClassification</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/deberta-v2#transformers.TFDebertaV2ForTokenClassification">TFDebertaV2ForTokenClassification</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/distilbert#transformers.TFDistilBertForTokenClassification">TFDistilBertForTokenClassification</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/electra#transformers.TFElectraForTokenClassification">TFElectraForTokenClassification</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/flaubert#transformers.TFFlaubertForTokenClassification">TFFlaubertForTokenClassification</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/funnel#transformers.TFFunnelForTokenClassification">TFFunnelForTokenClassification</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/layoutlm#transformers.TFLayoutLMForTokenClassification">TFLayoutLMForTokenClassification</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/longformer#transformers.TFLongformerForTokenClassification">TFLongformerForTokenClassification</a> (Longformer model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/mpnet#transformers.TFMPNetForTokenClassification">TFMPNetForTokenClassification</a> (MPNet model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/mobilebert#transformers.TFMobileBertForTokenClassification">TFMobileBertForTokenClassification</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/rembert#transformers.TFRemBertForTokenClassification">TFRemBertForTokenClassification</a> (RemBERT model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/roformer#transformers.TFRoFormerForTokenClassification">TFRoFormerForTokenClassification</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/roberta#transformers.TFRobertaForTokenClassification">TFRobertaForTokenClassification</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/xlm#transformers.TFXLMForTokenClassification">TFXLMForTokenClassification</a> (XLM model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForTokenClassification">TFXLMRobertaForTokenClassification</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/xlnet#transformers.TFXLNetForTokenClassification">TFXLNetForTokenClassification</a> (XLNet model)</li>
</ul>`,name:"config"}]}}),Xw=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForTokenClassification

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = TFAutoModelForTokenClassification.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForTokenClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForTokenClassification.from_config(config)`}}),zw=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15297/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15297/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15297/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15297/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15297/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),Vw=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForTokenClassification

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForTokenClassification.from_pretrained("bert-base-cased")

# Update configuration during loading
model = TFAutoModelForTokenClassification.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = TFAutoModelForTokenClassification.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForTokenClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForTokenClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForTokenClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForTokenClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),Ww=new z({}),Qw=new E({props:{name:"class transformers.TFAutoModelForQuestionAnswering",anchor:"transformers.TFAutoModelForQuestionAnswering",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15297/src/transformers/models/auto/modeling_tf_auto.py#L439"}}),Uw=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15297/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15297/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15297/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/albert#transformers.TFAlbertForQuestionAnswering">TFAlbertForQuestionAnswering</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/bert#transformers.TFBertForQuestionAnswering">TFBertForQuestionAnswering</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/camembert#transformers.TFCamembertForQuestionAnswering">TFCamembertForQuestionAnswering</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/convbert#transformers.TFConvBertForQuestionAnswering">TFConvBertForQuestionAnswering</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/deberta#transformers.TFDebertaForQuestionAnswering">TFDebertaForQuestionAnswering</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/deberta-v2#transformers.TFDebertaV2ForQuestionAnswering">TFDebertaV2ForQuestionAnswering</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/distilbert#transformers.TFDistilBertForQuestionAnswering">TFDistilBertForQuestionAnswering</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/electra#transformers.TFElectraForQuestionAnswering">TFElectraForQuestionAnswering</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/flaubert#transformers.TFFlaubertForQuestionAnsweringSimple">TFFlaubertForQuestionAnsweringSimple</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/funnel#transformers.TFFunnelForQuestionAnswering">TFFunnelForQuestionAnswering</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/longformer#transformers.TFLongformerForQuestionAnswering">TFLongformerForQuestionAnswering</a> (Longformer model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/mpnet#transformers.TFMPNetForQuestionAnswering">TFMPNetForQuestionAnswering</a> (MPNet model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/mobilebert#transformers.TFMobileBertForQuestionAnswering">TFMobileBertForQuestionAnswering</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/rembert#transformers.TFRemBertForQuestionAnswering">TFRemBertForQuestionAnswering</a> (RemBERT model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/roformer#transformers.TFRoFormerForQuestionAnswering">TFRoFormerForQuestionAnswering</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/roberta#transformers.TFRobertaForQuestionAnswering">TFRobertaForQuestionAnswering</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/xlm#transformers.TFXLMForQuestionAnsweringSimple">TFXLMForQuestionAnsweringSimple</a> (XLM model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForQuestionAnswering">TFXLMRobertaForQuestionAnswering</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/xlnet#transformers.TFXLNetForQuestionAnsweringSimple">TFXLNetForQuestionAnsweringSimple</a> (XLNet model)</li>
</ul>`,name:"config"}]}}),Jw=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForQuestionAnswering

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = TFAutoModelForQuestionAnswering.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForQuestionAnswering.from_config(config)`}}),Yw=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15297/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15297/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15297/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15297/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15297/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),Kw=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForQuestionAnswering

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForQuestionAnswering.from_pretrained("bert-base-cased")

# Update configuration during loading
model = TFAutoModelForQuestionAnswering.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = TFAutoModelForQuestionAnswering.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForQuestionAnswering.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),Zw=new z({}),eA=new E({props:{name:"class transformers.TFAutoModelForVision2Seq",anchor:"transformers.TFAutoModelForVision2Seq",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15297/src/transformers/models/auto/modeling_tf_auto.py#L407"}}),rA=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15297/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15297/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15297/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderConfig">VisionEncoderDecoderConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/vision-encoder-decoder#transformers.TFVisionEncoderDecoderModel">TFVisionEncoderDecoderModel</a> (Vision Encoder decoder model)</li>
</ul>`,name:"config"}]}}),tA=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForVision2Seq

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = TFAutoModelForVision2Seq.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForVision2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForVision2Seq.from_config(config)`}}),aA=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15297/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15297/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15297/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15297/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15297/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),nA=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForVision2Seq

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForVision2Seq.from_pretrained("bert-base-cased")

# Update configuration during loading
model = TFAutoModelForVision2Seq.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = TFAutoModelForVision2Seq.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForVision2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForVision2Seq.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForVision2Seq.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForVision2Seq.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),sA=new z({}),lA=new E({props:{name:"class transformers.TFAutoModelForSpeechSeq2Seq",anchor:"transformers.TFAutoModelForSpeechSeq2Seq",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15297/src/transformers/models/auto/modeling_tf_auto.py#L482"}}),dA=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15297/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15297/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15297/en/model_doc/speech_to_text#transformers.Speech2TextConfig">Speech2TextConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/speech_to_text#transformers.TFSpeech2TextForConditionalGeneration">TFSpeech2TextForConditionalGeneration</a> (Speech2Text model)</li>
</ul>`,name:"config"}]}}),cA=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForSpeechSeq2Seq

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = TFAutoModelForSpeechSeq2Seq.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForSpeechSeq2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSpeechSeq2Seq.from_config(config)`}}),fA=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15297/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15297/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15297/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15297/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15297/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),mA=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForSpeechSeq2Seq

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForSpeechSeq2Seq.from_pretrained("bert-base-cased")

# Update configuration during loading
model = TFAutoModelForSpeechSeq2Seq.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = TFAutoModelForSpeechSeq2Seq.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForSpeechSeq2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSpeechSeq2Seq.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSpeechSeq2Seq.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSpeechSeq2Seq.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),gA=new z({}),hA=new E({props:{name:"class transformers.FlaxAutoModel",anchor:"transformers.FlaxAutoModel",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15297/src/transformers/models/auto/modeling_flax_auto.py#L220"}}),_A=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15297/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15297/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15297/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/albert#transformers.FlaxAlbertModel">FlaxAlbertModel</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/bart#transformers.FlaxBartModel">FlaxBartModel</a> (BART model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/beit#transformers.BeitConfig">BeitConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/beit#transformers.FlaxBeitModel">FlaxBeitModel</a> (BEiT model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/bert#transformers.FlaxBertModel">FlaxBertModel</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/big_bird#transformers.FlaxBigBirdModel">FlaxBigBirdModel</a> (BigBird model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/blenderbot#transformers.BlenderbotConfig">BlenderbotConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/blenderbot#transformers.FlaxBlenderbotModel">FlaxBlenderbotModel</a> (Blenderbot model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/blenderbot-small#transformers.BlenderbotSmallConfig">BlenderbotSmallConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/blenderbot-small#transformers.FlaxBlenderbotSmallModel">FlaxBlenderbotSmallModel</a> (BlenderbotSmall model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/clip#transformers.CLIPConfig">CLIPConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/clip#transformers.FlaxCLIPModel">FlaxCLIPModel</a> (CLIP model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/distilbert#transformers.FlaxDistilBertModel">FlaxDistilBertModel</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/electra#transformers.FlaxElectraModel">FlaxElectraModel</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/gpt2#transformers.FlaxGPT2Model">FlaxGPT2Model</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/gptj#transformers.GPTJConfig">GPTJConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/gptj#transformers.FlaxGPTJModel">FlaxGPTJModel</a> (GPT-J model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/gpt_neo#transformers.GPTNeoConfig">GPTNeoConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/gpt_neo#transformers.FlaxGPTNeoModel">FlaxGPTNeoModel</a> (GPT Neo model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/mbart#transformers.FlaxMBartModel">FlaxMBartModel</a> (mBART model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/mt5#transformers.MT5Config">MT5Config</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/mt5#transformers.FlaxMT5Model">FlaxMT5Model</a> (mT5 model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/marian#transformers.MarianConfig">MarianConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/marian#transformers.FlaxMarianModel">FlaxMarianModel</a> (Marian model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/pegasus#transformers.PegasusConfig">PegasusConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/pegasus#transformers.FlaxPegasusModel">FlaxPegasusModel</a> (Pegasus model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/roformer#transformers.FlaxRoFormerModel">FlaxRoFormerModel</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/roberta#transformers.FlaxRobertaModel">FlaxRobertaModel</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/t5#transformers.FlaxT5Model">FlaxT5Model</a> (T5 model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/vit#transformers.ViTConfig">ViTConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/vit#transformers.FlaxViTModel">FlaxViTModel</a> (ViT model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/vision-text-dual-encoder#transformers.VisionTextDualEncoderConfig">VisionTextDualEncoderConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/vision-text-dual-encoder#transformers.FlaxVisionTextDualEncoderModel">FlaxVisionTextDualEncoderModel</a> (VisionTextDualEncoder model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/wav2vec2#transformers.FlaxWav2Vec2Model">FlaxWav2Vec2Model</a> (Wav2Vec2 model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/xglm#transformers.XGLMConfig">XGLMConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/xglm#transformers.FlaxXGLMModel">FlaxXGLMModel</a> (XGLM model)</li>
</ul>`,name:"config"}]}}),uA=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModel

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = FlaxAutoModel.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModel.from_config(config)`}}),bA=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15297/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15297/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15297/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15297/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15297/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),vA=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModel

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModel.from_pretrained("bert-base-cased")

# Update configuration during loading
model = FlaxAutoModel.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = FlaxAutoModel.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModel.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModel.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModel.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),TA=new z({}),FA=new E({props:{name:"class transformers.FlaxAutoModelForCausalLM",anchor:"transformers.FlaxAutoModelForCausalLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15297/src/transformers/models/auto/modeling_flax_auto.py#L234"}}),MA=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15297/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15297/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15297/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/gpt2#transformers.FlaxGPT2LMHeadModel">FlaxGPT2LMHeadModel</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/gptj#transformers.GPTJConfig">GPTJConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/gptj#transformers.FlaxGPTJForCausalLM">FlaxGPTJForCausalLM</a> (GPT-J model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/gpt_neo#transformers.GPTNeoConfig">GPTNeoConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/gpt_neo#transformers.FlaxGPTNeoForCausalLM">FlaxGPTNeoForCausalLM</a> (GPT Neo model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/xglm#transformers.XGLMConfig">XGLMConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/xglm#transformers.FlaxXGLMForCausalLM">FlaxXGLMForCausalLM</a> (XGLM model)</li>
</ul>`,name:"config"}]}}),EA=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForCausalLM

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = FlaxAutoModelForCausalLM.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForCausalLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForCausalLM.from_config(config)`}}),yA=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15297/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15297/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15297/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15297/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15297/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),wA=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForCausalLM

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForCausalLM.from_pretrained("bert-base-cased")

# Update configuration during loading
model = FlaxAutoModelForCausalLM.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = FlaxAutoModelForCausalLM.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForCausalLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForCausalLM.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),AA=new z({}),LA=new E({props:{name:"class transformers.FlaxAutoModelForPreTraining",anchor:"transformers.FlaxAutoModelForPreTraining",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15297/src/transformers/models/auto/modeling_flax_auto.py#L227"}}),kA=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15297/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15297/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15297/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/albert#transformers.FlaxAlbertForPreTraining">FlaxAlbertForPreTraining</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/bart#transformers.FlaxBartForConditionalGeneration">FlaxBartForConditionalGeneration</a> (BART model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/bert#transformers.FlaxBertForPreTraining">FlaxBertForPreTraining</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/big_bird#transformers.FlaxBigBirdForPreTraining">FlaxBigBirdForPreTraining</a> (BigBird model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/electra#transformers.FlaxElectraForPreTraining">FlaxElectraForPreTraining</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/mbart#transformers.FlaxMBartForConditionalGeneration">FlaxMBartForConditionalGeneration</a> (mBART model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/mt5#transformers.MT5Config">MT5Config</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/mt5#transformers.FlaxMT5ForConditionalGeneration">FlaxMT5ForConditionalGeneration</a> (mT5 model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/roformer#transformers.FlaxRoFormerForMaskedLM">FlaxRoFormerForMaskedLM</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/roberta#transformers.FlaxRobertaForMaskedLM">FlaxRobertaForMaskedLM</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/t5#transformers.FlaxT5ForConditionalGeneration">FlaxT5ForConditionalGeneration</a> (T5 model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/wav2vec2#transformers.FlaxWav2Vec2ForPreTraining">FlaxWav2Vec2ForPreTraining</a> (Wav2Vec2 model)</li>
</ul>`,name:"config"}]}}),xA=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForPreTraining

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = FlaxAutoModelForPreTraining.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForPreTraining

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForPreTraining.from_config(config)`}}),RA=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15297/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15297/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15297/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15297/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15297/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),SA=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForPreTraining

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForPreTraining.from_pretrained("bert-base-cased")

# Update configuration during loading
model = FlaxAutoModelForPreTraining.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = FlaxAutoModelForPreTraining.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForPreTraining

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForPreTraining.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForPreTraining.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForPreTraining.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),PA=new z({}),$A=new E({props:{name:"class transformers.FlaxAutoModelForMaskedLM",anchor:"transformers.FlaxAutoModelForMaskedLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15297/src/transformers/models/auto/modeling_flax_auto.py#L241"}}),jA=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15297/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15297/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15297/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/albert#transformers.FlaxAlbertForMaskedLM">FlaxAlbertForMaskedLM</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/bart#transformers.FlaxBartForConditionalGeneration">FlaxBartForConditionalGeneration</a> (BART model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/bert#transformers.FlaxBertForMaskedLM">FlaxBertForMaskedLM</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/big_bird#transformers.FlaxBigBirdForMaskedLM">FlaxBigBirdForMaskedLM</a> (BigBird model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/distilbert#transformers.FlaxDistilBertForMaskedLM">FlaxDistilBertForMaskedLM</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/electra#transformers.FlaxElectraForMaskedLM">FlaxElectraForMaskedLM</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/mbart#transformers.FlaxMBartForConditionalGeneration">FlaxMBartForConditionalGeneration</a> (mBART model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/roformer#transformers.FlaxRoFormerForMaskedLM">FlaxRoFormerForMaskedLM</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/roberta#transformers.FlaxRobertaForMaskedLM">FlaxRobertaForMaskedLM</a> (RoBERTa model)</li>
</ul>`,name:"config"}]}}),NA=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForMaskedLM

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = FlaxAutoModelForMaskedLM.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForMaskedLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForMaskedLM.from_config(config)`}}),DA=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15297/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15297/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15297/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15297/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15297/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),qA=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForMaskedLM

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForMaskedLM.from_pretrained("bert-base-cased")

# Update configuration during loading
model = FlaxAutoModelForMaskedLM.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = FlaxAutoModelForMaskedLM.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForMaskedLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForMaskedLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForMaskedLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForMaskedLM.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),GA=new z({}),OA=new E({props:{name:"class transformers.FlaxAutoModelForSeq2SeqLM",anchor:"transformers.FlaxAutoModelForSeq2SeqLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15297/src/transformers/models/auto/modeling_flax_auto.py#L248"}}),zA=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15297/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15297/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15297/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/bart#transformers.FlaxBartForConditionalGeneration">FlaxBartForConditionalGeneration</a> (BART model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/blenderbot#transformers.BlenderbotConfig">BlenderbotConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/blenderbot#transformers.FlaxBlenderbotForConditionalGeneration">FlaxBlenderbotForConditionalGeneration</a> (Blenderbot model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/blenderbot-small#transformers.BlenderbotSmallConfig">BlenderbotSmallConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/blenderbot-small#transformers.FlaxBlenderbotSmallForConditionalGeneration">FlaxBlenderbotSmallForConditionalGeneration</a> (BlenderbotSmall model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/encoder-decoder#transformers.EncoderDecoderConfig">EncoderDecoderConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/encoder-decoder#transformers.FlaxEncoderDecoderModel">FlaxEncoderDecoderModel</a> (Encoder decoder model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/mbart#transformers.FlaxMBartForConditionalGeneration">FlaxMBartForConditionalGeneration</a> (mBART model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/mt5#transformers.MT5Config">MT5Config</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/mt5#transformers.FlaxMT5ForConditionalGeneration">FlaxMT5ForConditionalGeneration</a> (mT5 model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/marian#transformers.MarianConfig">MarianConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/marian#transformers.FlaxMarianMTModel">FlaxMarianMTModel</a> (Marian model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/pegasus#transformers.PegasusConfig">PegasusConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/pegasus#transformers.FlaxPegasusForConditionalGeneration">FlaxPegasusForConditionalGeneration</a> (Pegasus model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/t5#transformers.FlaxT5ForConditionalGeneration">FlaxT5ForConditionalGeneration</a> (T5 model)</li>
</ul>`,name:"config"}]}}),VA=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForSeq2SeqLM

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("t5-base")
model = FlaxAutoModelForSeq2SeqLM.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForSeq2SeqLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForSeq2SeqLM.from_config(config)`}}),WA=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15297/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15297/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15297/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15297/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15297/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),QA=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForSeq2SeqLM

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForSeq2SeqLM.from_pretrained("t5-base")

# Update configuration during loading
model = FlaxAutoModelForSeq2SeqLM.from_pretrained("t5-base", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/t5_pt_model_config.json")
model = FlaxAutoModelForSeq2SeqLM.from_pretrained(
    "./pt_model/t5_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForSeq2SeqLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/t5_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForSeq2SeqLM.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/t5_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),HA=new z({}),UA=new E({props:{name:"class transformers.FlaxAutoModelForSequenceClassification",anchor:"transformers.FlaxAutoModelForSequenceClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15297/src/transformers/models/auto/modeling_flax_auto.py#L257"}}),YA=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15297/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15297/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15297/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/albert#transformers.FlaxAlbertForSequenceClassification">FlaxAlbertForSequenceClassification</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/bart#transformers.FlaxBartForSequenceClassification">FlaxBartForSequenceClassification</a> (BART model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/bert#transformers.FlaxBertForSequenceClassification">FlaxBertForSequenceClassification</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/big_bird#transformers.FlaxBigBirdForSequenceClassification">FlaxBigBirdForSequenceClassification</a> (BigBird model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/distilbert#transformers.FlaxDistilBertForSequenceClassification">FlaxDistilBertForSequenceClassification</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/electra#transformers.FlaxElectraForSequenceClassification">FlaxElectraForSequenceClassification</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/mbart#transformers.FlaxMBartForSequenceClassification">FlaxMBartForSequenceClassification</a> (mBART model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/roformer#transformers.FlaxRoFormerForSequenceClassification">FlaxRoFormerForSequenceClassification</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/roberta#transformers.FlaxRobertaForSequenceClassification">FlaxRobertaForSequenceClassification</a> (RoBERTa model)</li>
</ul>`,name:"config"}]}}),KA=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForSequenceClassification

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = FlaxAutoModelForSequenceClassification.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForSequenceClassification.from_config(config)`}}),ZA=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15297/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15297/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15297/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15297/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15297/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),e6=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForSequenceClassification

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForSequenceClassification.from_pretrained("bert-base-cased")

# Update configuration during loading
model = FlaxAutoModelForSequenceClassification.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = FlaxAutoModelForSequenceClassification.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForSequenceClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),o6=new z({}),r6=new E({props:{name:"class transformers.FlaxAutoModelForQuestionAnswering",anchor:"transformers.FlaxAutoModelForQuestionAnswering",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15297/src/transformers/models/auto/modeling_flax_auto.py#L266"}}),a6=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15297/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15297/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15297/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/albert#transformers.FlaxAlbertForQuestionAnswering">FlaxAlbertForQuestionAnswering</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/bart#transformers.FlaxBartForQuestionAnswering">FlaxBartForQuestionAnswering</a> (BART model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/bert#transformers.FlaxBertForQuestionAnswering">FlaxBertForQuestionAnswering</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/big_bird#transformers.FlaxBigBirdForQuestionAnswering">FlaxBigBirdForQuestionAnswering</a> (BigBird model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/distilbert#transformers.FlaxDistilBertForQuestionAnswering">FlaxDistilBertForQuestionAnswering</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/electra#transformers.FlaxElectraForQuestionAnswering">FlaxElectraForQuestionAnswering</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/mbart#transformers.FlaxMBartForQuestionAnswering">FlaxMBartForQuestionAnswering</a> (mBART model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/roformer#transformers.FlaxRoFormerForQuestionAnswering">FlaxRoFormerForQuestionAnswering</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/roberta#transformers.FlaxRobertaForQuestionAnswering">FlaxRobertaForQuestionAnswering</a> (RoBERTa model)</li>
</ul>`,name:"config"}]}}),n6=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForQuestionAnswering

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = FlaxAutoModelForQuestionAnswering.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForQuestionAnswering.from_config(config)`}}),s6=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15297/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15297/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15297/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15297/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15297/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),l6=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForQuestionAnswering

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForQuestionAnswering.from_pretrained("bert-base-cased")

# Update configuration during loading
model = FlaxAutoModelForQuestionAnswering.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = FlaxAutoModelForQuestionAnswering.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForQuestionAnswering.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),i6=new z({}),d6=new E({props:{name:"class transformers.FlaxAutoModelForTokenClassification",anchor:"transformers.FlaxAutoModelForTokenClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15297/src/transformers/models/auto/modeling_flax_auto.py#L273"}}),f6=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15297/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15297/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15297/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/albert#transformers.FlaxAlbertForTokenClassification">FlaxAlbertForTokenClassification</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/bert#transformers.FlaxBertForTokenClassification">FlaxBertForTokenClassification</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/big_bird#transformers.FlaxBigBirdForTokenClassification">FlaxBigBirdForTokenClassification</a> (BigBird model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/distilbert#transformers.FlaxDistilBertForTokenClassification">FlaxDistilBertForTokenClassification</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/electra#transformers.FlaxElectraForTokenClassification">FlaxElectraForTokenClassification</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/roformer#transformers.FlaxRoFormerForTokenClassification">FlaxRoFormerForTokenClassification</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/roberta#transformers.FlaxRobertaForTokenClassification">FlaxRobertaForTokenClassification</a> (RoBERTa model)</li>
</ul>`,name:"config"}]}}),m6=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForTokenClassification

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = FlaxAutoModelForTokenClassification.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForTokenClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForTokenClassification.from_config(config)`}}),g6=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15297/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15297/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15297/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15297/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15297/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),h6=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForTokenClassification

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForTokenClassification.from_pretrained("bert-base-cased")

# Update configuration during loading
model = FlaxAutoModelForTokenClassification.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = FlaxAutoModelForTokenClassification.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForTokenClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForTokenClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForTokenClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForTokenClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),p6=new z({}),_6=new E({props:{name:"class transformers.FlaxAutoModelForMultipleChoice",anchor:"transformers.FlaxAutoModelForMultipleChoice",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15297/src/transformers/models/auto/modeling_flax_auto.py#L282"}}),b6=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15297/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15297/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15297/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/albert#transformers.FlaxAlbertForMultipleChoice">FlaxAlbertForMultipleChoice</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/bert#transformers.FlaxBertForMultipleChoice">FlaxBertForMultipleChoice</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/big_bird#transformers.FlaxBigBirdForMultipleChoice">FlaxBigBirdForMultipleChoice</a> (BigBird model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/distilbert#transformers.FlaxDistilBertForMultipleChoice">FlaxDistilBertForMultipleChoice</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/electra#transformers.FlaxElectraForMultipleChoice">FlaxElectraForMultipleChoice</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/roformer#transformers.FlaxRoFormerForMultipleChoice">FlaxRoFormerForMultipleChoice</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/roberta#transformers.FlaxRobertaForMultipleChoice">FlaxRobertaForMultipleChoice</a> (RoBERTa model)</li>
</ul>`,name:"config"}]}}),v6=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForMultipleChoice

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = FlaxAutoModelForMultipleChoice.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForMultipleChoice

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForMultipleChoice.from_config(config)`}}),T6=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15297/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15297/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15297/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15297/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15297/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),F6=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForMultipleChoice

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForMultipleChoice.from_pretrained("bert-base-cased")

# Update configuration during loading
model = FlaxAutoModelForMultipleChoice.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = FlaxAutoModelForMultipleChoice.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForMultipleChoice

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForMultipleChoice.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForMultipleChoice.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForMultipleChoice.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),C6=new z({}),M6=new E({props:{name:"class transformers.FlaxAutoModelForNextSentencePrediction",anchor:"transformers.FlaxAutoModelForNextSentencePrediction",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15297/src/transformers/models/auto/modeling_flax_auto.py#L289"}}),y6=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15297/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15297/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15297/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/bert#transformers.FlaxBertForNextSentencePrediction">FlaxBertForNextSentencePrediction</a> (BERT model)</li>
</ul>`,name:"config"}]}}),w6=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForNextSentencePrediction

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = FlaxAutoModelForNextSentencePrediction.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForNextSentencePrediction

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForNextSentencePrediction.from_config(config)`}}),A6=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15297/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15297/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15297/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15297/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15297/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),L6=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForNextSentencePrediction

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForNextSentencePrediction.from_pretrained("bert-base-cased")

# Update configuration during loading
model = FlaxAutoModelForNextSentencePrediction.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = FlaxAutoModelForNextSentencePrediction.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForNextSentencePrediction

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForNextSentencePrediction.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForNextSentencePrediction.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForNextSentencePrediction.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),B6=new z({}),k6=new E({props:{name:"class transformers.FlaxAutoModelForImageClassification",anchor:"transformers.FlaxAutoModelForImageClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15297/src/transformers/models/auto/modeling_flax_auto.py#L298"}}),R6=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15297/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15297/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15297/en/model_doc/beit#transformers.BeitConfig">BeitConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/beit#transformers.FlaxBeitForImageClassification">FlaxBeitForImageClassification</a> (BEiT model)</li>
<li><a href="/docs/transformers/pr_15297/en/model_doc/vit#transformers.ViTConfig">ViTConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/vit#transformers.FlaxViTForImageClassification">FlaxViTForImageClassification</a> (ViT model)</li>
</ul>`,name:"config"}]}}),S6=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForImageClassification

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = FlaxAutoModelForImageClassification.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForImageClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForImageClassification.from_config(config)`}}),P6=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15297/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15297/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15297/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15297/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15297/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),I6=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForImageClassification

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForImageClassification.from_pretrained("bert-base-cased")

# Update configuration during loading
model = FlaxAutoModelForImageClassification.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = FlaxAutoModelForImageClassification.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForImageClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForImageClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForImageClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForImageClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),j6=new z({}),N6=new E({props:{name:"class transformers.FlaxAutoModelForVision2Seq",anchor:"transformers.FlaxAutoModelForVision2Seq",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15297/src/transformers/models/auto/modeling_flax_auto.py#L307"}}),q6=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15297/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15297/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15297/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderConfig">VisionEncoderDecoderConfig</a> configuration class: <a href="/docs/transformers/pr_15297/en/model_doc/vision-encoder-decoder#transformers.FlaxVisionEncoderDecoderModel">FlaxVisionEncoderDecoderModel</a> (Vision Encoder decoder model)</li>
</ul>`,name:"config"}]}}),G6=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForVision2Seq

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = FlaxAutoModelForVision2Seq.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForVision2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForVision2Seq.from_config(config)`}}),O6=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15297/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15297/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15297/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15297/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15297/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),X6=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForVision2Seq

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForVision2Seq.from_pretrained("bert-base-cased")

# Update configuration during loading
model = FlaxAutoModelForVision2Seq.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = FlaxAutoModelForVision2Seq.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForVision2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForVision2Seq.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForVision2Seq.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForVision2Seq.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),{c(){J=a("meta"),Ae=l(),ie=a("h1"),me=a("a"),to=a("span"),f(ce.$$.fragment),ue=l(),Do=a("span"),Ai=o("Auto Classes"),yf=l(),sa=a("p"),Li=o(`In many cases, the architecture you want to use can be guessed from the name or the path of the pretrained model you
are supplying to the `),Bi=a("code"),rM=o("from_pretrained()"),wf=o(` method. AutoClasses are here to do this job for you so that you
automatically retrieve the relevant model given the name/path to the pretrained weights/config/vocabulary.`),ye=l(),io=a("p"),ki=o("Instantiating one of "),Pn=a("a"),tM=o("AutoConfig"),$n=o(", "),In=a("a"),aM=o("AutoModel"),xi=o(`, and
`),jn=a("a"),nM=o("AutoTokenizer"),Ri=o(" will directly create a class of the relevant architecture. For instance"),Af=l(),f($a.$$.fragment),co=l(),ge=a("p"),G0=o("will create a model that is an instance of "),Si=a("a"),O0=o("BertModel"),X0=o("."),qo=l(),Ia=a("p"),z0=o("There is one class of "),Lf=a("code"),V0=o("AutoModel"),uxe=o(" for each task, and for each backend (PyTorch, TensorFlow, or Flax)."),iLe=l(),Pi=a("h2"),Bf=a("a"),DV=a("span"),f(sM.$$.fragment),bxe=l(),qV=a("span"),vxe=o("Extending the Auto Classes"),dLe=l(),Nn=a("p"),Txe=o(`Each of the auto classes has a method to be extended with your custom classes. For instance, if you have defined a
custom class of model `),GV=a("code"),Fxe=o("NewModel"),Cxe=o(", make sure you have a "),OV=a("code"),Mxe=o("NewModelConfig"),Exe=o(` then you can add those to the auto
classes like this:`),cLe=l(),f(lM.$$.fragment),fLe=l(),W0=a("p"),yxe=o("You will then be able to use the auto classes like you would usually do!"),mLe=l(),f(kf.$$.fragment),gLe=l(),$i=a("h2"),xf=a("a"),XV=a("span"),f(iM.$$.fragment),wxe=l(),zV=a("span"),Axe=o("AutoConfig"),hLe=l(),Go=a("div"),f(dM.$$.fragment),Lxe=l(),cM=a("p"),Bxe=o(`This is a generic configuration class that will be instantiated as one of the configuration classes of the library
when created with the `),Q0=a("a"),kxe=o("from_pretrained()"),xxe=o(" class method."),Rxe=l(),fM=a("p"),Sxe=o("This class cannot be instantiated directly using "),VV=a("code"),Pxe=o("__init__()"),$xe=o(" (throws an error)."),Ixe=l(),fo=a("div"),f(mM.$$.fragment),jxe=l(),WV=a("p"),Nxe=o("Instantiate one of the configuration classes of the library from a pretrained model configuration."),Dxe=l(),Ii=a("p"),qxe=o("The configuration class to instantiate is selected based on the "),QV=a("code"),Gxe=o("model_type"),Oxe=o(` property of the config object that
is loaded, or when it\u2019s missing, by falling back to using pattern matching on `),HV=a("code"),Xxe=o("pretrained_model_name_or_path"),zxe=o(":"),Vxe=l(),v=a("ul"),Rf=a("li"),UV=a("strong"),Wxe=o("albert"),Qxe=o(" \u2014 "),H0=a("a"),Hxe=o("AlbertConfig"),Uxe=o(" (ALBERT model)"),Jxe=l(),Sf=a("li"),JV=a("strong"),Yxe=o("bart"),Kxe=o(" \u2014 "),U0=a("a"),Zxe=o("BartConfig"),eRe=o(" (BART model)"),oRe=l(),Pf=a("li"),YV=a("strong"),rRe=o("beit"),tRe=o(" \u2014 "),J0=a("a"),aRe=o("BeitConfig"),nRe=o(" (BEiT model)"),sRe=l(),$f=a("li"),KV=a("strong"),lRe=o("bert"),iRe=o(" \u2014 "),Y0=a("a"),dRe=o("BertConfig"),cRe=o(" (BERT model)"),fRe=l(),If=a("li"),ZV=a("strong"),mRe=o("bert-generation"),gRe=o(" \u2014 "),K0=a("a"),hRe=o("BertGenerationConfig"),pRe=o(" (Bert Generation model)"),_Re=l(),jf=a("li"),eW=a("strong"),uRe=o("big_bird"),bRe=o(" \u2014 "),Z0=a("a"),vRe=o("BigBirdConfig"),TRe=o(" (BigBird model)"),FRe=l(),Nf=a("li"),oW=a("strong"),CRe=o("bigbird_pegasus"),MRe=o(" \u2014 "),eL=a("a"),ERe=o("BigBirdPegasusConfig"),yRe=o(" (BigBirdPegasus model)"),wRe=l(),Df=a("li"),rW=a("strong"),ARe=o("blenderbot"),LRe=o(" \u2014 "),oL=a("a"),BRe=o("BlenderbotConfig"),kRe=o(" (Blenderbot model)"),xRe=l(),qf=a("li"),tW=a("strong"),RRe=o("blenderbot-small"),SRe=o(" \u2014 "),rL=a("a"),PRe=o("BlenderbotSmallConfig"),$Re=o(" (BlenderbotSmall model)"),IRe=l(),Gf=a("li"),aW=a("strong"),jRe=o("camembert"),NRe=o(" \u2014 "),tL=a("a"),DRe=o("CamembertConfig"),qRe=o(" (CamemBERT model)"),GRe=l(),Of=a("li"),nW=a("strong"),ORe=o("canine"),XRe=o(" \u2014 "),aL=a("a"),zRe=o("CanineConfig"),VRe=o(" (Canine model)"),WRe=l(),Xf=a("li"),sW=a("strong"),QRe=o("clip"),HRe=o(" \u2014 "),nL=a("a"),URe=o("CLIPConfig"),JRe=o(" (CLIP model)"),YRe=l(),zf=a("li"),lW=a("strong"),KRe=o("convbert"),ZRe=o(" \u2014 "),sL=a("a"),eSe=o("ConvBertConfig"),oSe=o(" (ConvBERT model)"),rSe=l(),Vf=a("li"),iW=a("strong"),tSe=o("convnext"),aSe=o(" \u2014 "),lL=a("a"),nSe=o("ConvNextConfig"),sSe=o(" (ConvNext model)"),lSe=l(),Wf=a("li"),dW=a("strong"),iSe=o("ctrl"),dSe=o(" \u2014 "),iL=a("a"),cSe=o("CTRLConfig"),fSe=o(" (CTRL model)"),mSe=l(),Qf=a("li"),cW=a("strong"),gSe=o("deberta"),hSe=o(" \u2014 "),dL=a("a"),pSe=o("DebertaConfig"),_Se=o(" (DeBERTa model)"),uSe=l(),Hf=a("li"),fW=a("strong"),bSe=o("deberta-v2"),vSe=o(" \u2014 "),cL=a("a"),TSe=o("DebertaV2Config"),FSe=o(" (DeBERTa-v2 model)"),CSe=l(),Uf=a("li"),mW=a("strong"),MSe=o("deit"),ESe=o(" \u2014 "),fL=a("a"),ySe=o("DeiTConfig"),wSe=o(" (DeiT model)"),ASe=l(),Jf=a("li"),gW=a("strong"),LSe=o("detr"),BSe=o(" \u2014 "),mL=a("a"),kSe=o("DetrConfig"),xSe=o(" (DETR model)"),RSe=l(),Yf=a("li"),hW=a("strong"),SSe=o("distilbert"),PSe=o(" \u2014 "),gL=a("a"),$Se=o("DistilBertConfig"),ISe=o(" (DistilBERT model)"),jSe=l(),Kf=a("li"),pW=a("strong"),NSe=o("dpr"),DSe=o(" \u2014 "),hL=a("a"),qSe=o("DPRConfig"),GSe=o(" (DPR model)"),OSe=l(),Zf=a("li"),_W=a("strong"),XSe=o("electra"),zSe=o(" \u2014 "),pL=a("a"),VSe=o("ElectraConfig"),WSe=o(" (ELECTRA model)"),QSe=l(),em=a("li"),uW=a("strong"),HSe=o("encoder-decoder"),USe=o(" \u2014 "),_L=a("a"),JSe=o("EncoderDecoderConfig"),YSe=o(" (Encoder decoder model)"),KSe=l(),om=a("li"),bW=a("strong"),ZSe=o("flaubert"),ePe=o(" \u2014 "),uL=a("a"),oPe=o("FlaubertConfig"),rPe=o(" (FlauBERT model)"),tPe=l(),rm=a("li"),vW=a("strong"),aPe=o("fnet"),nPe=o(" \u2014 "),bL=a("a"),sPe=o("FNetConfig"),lPe=o(" (FNet model)"),iPe=l(),tm=a("li"),TW=a("strong"),dPe=o("fsmt"),cPe=o(" \u2014 "),vL=a("a"),fPe=o("FSMTConfig"),mPe=o(" (FairSeq Machine-Translation model)"),gPe=l(),am=a("li"),FW=a("strong"),hPe=o("funnel"),pPe=o(" \u2014 "),TL=a("a"),_Pe=o("FunnelConfig"),uPe=o(" (Funnel Transformer model)"),bPe=l(),nm=a("li"),CW=a("strong"),vPe=o("gpt2"),TPe=o(" \u2014 "),FL=a("a"),FPe=o("GPT2Config"),CPe=o(" (OpenAI GPT-2 model)"),MPe=l(),sm=a("li"),MW=a("strong"),EPe=o("gpt_neo"),yPe=o(" \u2014 "),CL=a("a"),wPe=o("GPTNeoConfig"),APe=o(" (GPT Neo model)"),LPe=l(),lm=a("li"),EW=a("strong"),BPe=o("gptj"),kPe=o(" \u2014 "),ML=a("a"),xPe=o("GPTJConfig"),RPe=o(" (GPT-J model)"),SPe=l(),im=a("li"),yW=a("strong"),PPe=o("hubert"),$Pe=o(" \u2014 "),EL=a("a"),IPe=o("HubertConfig"),jPe=o(" (Hubert model)"),NPe=l(),dm=a("li"),wW=a("strong"),DPe=o("ibert"),qPe=o(" \u2014 "),yL=a("a"),GPe=o("IBertConfig"),OPe=o(" (I-BERT model)"),XPe=l(),cm=a("li"),AW=a("strong"),zPe=o("imagegpt"),VPe=o(" \u2014 "),wL=a("a"),WPe=o("ImageGPTConfig"),QPe=o(" (ImageGPT model)"),HPe=l(),fm=a("li"),LW=a("strong"),UPe=o("layoutlm"),JPe=o(" \u2014 "),AL=a("a"),YPe=o("LayoutLMConfig"),KPe=o(" (LayoutLM model)"),ZPe=l(),mm=a("li"),BW=a("strong"),e$e=o("layoutlmv2"),o$e=o(" \u2014 "),LL=a("a"),r$e=o("LayoutLMv2Config"),t$e=o(" (LayoutLMv2 model)"),a$e=l(),gm=a("li"),kW=a("strong"),n$e=o("led"),s$e=o(" \u2014 "),BL=a("a"),l$e=o("LEDConfig"),i$e=o(" (LED model)"),d$e=l(),hm=a("li"),xW=a("strong"),c$e=o("longformer"),f$e=o(" \u2014 "),kL=a("a"),m$e=o("LongformerConfig"),g$e=o(" (Longformer model)"),h$e=l(),pm=a("li"),RW=a("strong"),p$e=o("luke"),_$e=o(" \u2014 "),xL=a("a"),u$e=o("LukeConfig"),b$e=o(" (LUKE model)"),v$e=l(),_m=a("li"),SW=a("strong"),T$e=o("lxmert"),F$e=o(" \u2014 "),RL=a("a"),C$e=o("LxmertConfig"),M$e=o(" (LXMERT model)"),E$e=l(),um=a("li"),PW=a("strong"),y$e=o("m2m_100"),w$e=o(" \u2014 "),SL=a("a"),A$e=o("M2M100Config"),L$e=o(" (M2M100 model)"),B$e=l(),bm=a("li"),$W=a("strong"),k$e=o("marian"),x$e=o(" \u2014 "),PL=a("a"),R$e=o("MarianConfig"),S$e=o(" (Marian model)"),P$e=l(),vm=a("li"),IW=a("strong"),$$e=o("mbart"),I$e=o(" \u2014 "),$L=a("a"),j$e=o("MBartConfig"),N$e=o(" (mBART model)"),D$e=l(),Tm=a("li"),jW=a("strong"),q$e=o("megatron-bert"),G$e=o(" \u2014 "),IL=a("a"),O$e=o("MegatronBertConfig"),X$e=o(" (MegatronBert model)"),z$e=l(),Fm=a("li"),NW=a("strong"),V$e=o("mobilebert"),W$e=o(" \u2014 "),jL=a("a"),Q$e=o("MobileBertConfig"),H$e=o(" (MobileBERT model)"),U$e=l(),Cm=a("li"),DW=a("strong"),J$e=o("mpnet"),Y$e=o(" \u2014 "),NL=a("a"),K$e=o("MPNetConfig"),Z$e=o(" (MPNet model)"),eIe=l(),Mm=a("li"),qW=a("strong"),oIe=o("mt5"),rIe=o(" \u2014 "),DL=a("a"),tIe=o("MT5Config"),aIe=o(" (mT5 model)"),nIe=l(),Em=a("li"),GW=a("strong"),sIe=o("nystromformer"),lIe=o(" \u2014 "),qL=a("a"),iIe=o("NystromformerConfig"),dIe=o(" (Nystromformer model)"),cIe=l(),ym=a("li"),OW=a("strong"),fIe=o("openai-gpt"),mIe=o(" \u2014 "),GL=a("a"),gIe=o("OpenAIGPTConfig"),hIe=o(" (OpenAI GPT model)"),pIe=l(),wm=a("li"),XW=a("strong"),_Ie=o("pegasus"),uIe=o(" \u2014 "),OL=a("a"),bIe=o("PegasusConfig"),vIe=o(" (Pegasus model)"),TIe=l(),Am=a("li"),zW=a("strong"),FIe=o("perceiver"),CIe=o(" \u2014 "),XL=a("a"),MIe=o("PerceiverConfig"),EIe=o(" (Perceiver model)"),yIe=l(),Lm=a("li"),VW=a("strong"),wIe=o("plbart"),AIe=o(" \u2014 "),zL=a("a"),LIe=o("PLBartConfig"),BIe=o(" (PLBart model)"),kIe=l(),Bm=a("li"),WW=a("strong"),xIe=o("poolformer"),RIe=o(" \u2014 "),VL=a("a"),SIe=o("PoolFormerConfig"),PIe=o(" (PoolFormer model)"),$Ie=l(),km=a("li"),QW=a("strong"),IIe=o("prophetnet"),jIe=o(" \u2014 "),WL=a("a"),NIe=o("ProphetNetConfig"),DIe=o(" (ProphetNet model)"),qIe=l(),xm=a("li"),HW=a("strong"),GIe=o("qdqbert"),OIe=o(" \u2014 "),QL=a("a"),XIe=o("QDQBertConfig"),zIe=o(" (QDQBert model)"),VIe=l(),Rm=a("li"),UW=a("strong"),WIe=o("rag"),QIe=o(" \u2014 "),HL=a("a"),HIe=o("RagConfig"),UIe=o(" (RAG model)"),JIe=l(),Sm=a("li"),JW=a("strong"),YIe=o("realm"),KIe=o(" \u2014 "),UL=a("a"),ZIe=o("RealmConfig"),eje=o(" (Realm model)"),oje=l(),Pm=a("li"),YW=a("strong"),rje=o("reformer"),tje=o(" \u2014 "),JL=a("a"),aje=o("ReformerConfig"),nje=o(" (Reformer model)"),sje=l(),$m=a("li"),KW=a("strong"),lje=o("rembert"),ije=o(" \u2014 "),YL=a("a"),dje=o("RemBertConfig"),cje=o(" (RemBERT model)"),fje=l(),Im=a("li"),ZW=a("strong"),mje=o("retribert"),gje=o(" \u2014 "),KL=a("a"),hje=o("RetriBertConfig"),pje=o(" (RetriBERT model)"),_je=l(),jm=a("li"),eQ=a("strong"),uje=o("roberta"),bje=o(" \u2014 "),ZL=a("a"),vje=o("RobertaConfig"),Tje=o(" (RoBERTa model)"),Fje=l(),Nm=a("li"),oQ=a("strong"),Cje=o("roformer"),Mje=o(" \u2014 "),e8=a("a"),Eje=o("RoFormerConfig"),yje=o(" (RoFormer model)"),wje=l(),Dm=a("li"),rQ=a("strong"),Aje=o("segformer"),Lje=o(" \u2014 "),o8=a("a"),Bje=o("SegformerConfig"),kje=o(" (SegFormer model)"),xje=l(),qm=a("li"),tQ=a("strong"),Rje=o("sew"),Sje=o(" \u2014 "),r8=a("a"),Pje=o("SEWConfig"),$je=o(" (SEW model)"),Ije=l(),Gm=a("li"),aQ=a("strong"),jje=o("sew-d"),Nje=o(" \u2014 "),t8=a("a"),Dje=o("SEWDConfig"),qje=o(" (SEW-D model)"),Gje=l(),Om=a("li"),nQ=a("strong"),Oje=o("speech-encoder-decoder"),Xje=o(" \u2014 "),a8=a("a"),zje=o("SpeechEncoderDecoderConfig"),Vje=o(" (Speech Encoder decoder model)"),Wje=l(),Xm=a("li"),sQ=a("strong"),Qje=o("speech_to_text"),Hje=o(" \u2014 "),n8=a("a"),Uje=o("Speech2TextConfig"),Jje=o(" (Speech2Text model)"),Yje=l(),zm=a("li"),lQ=a("strong"),Kje=o("speech_to_text_2"),Zje=o(" \u2014 "),s8=a("a"),eNe=o("Speech2Text2Config"),oNe=o(" (Speech2Text2 model)"),rNe=l(),Vm=a("li"),iQ=a("strong"),tNe=o("splinter"),aNe=o(" \u2014 "),l8=a("a"),nNe=o("SplinterConfig"),sNe=o(" (Splinter model)"),lNe=l(),Wm=a("li"),dQ=a("strong"),iNe=o("squeezebert"),dNe=o(" \u2014 "),i8=a("a"),cNe=o("SqueezeBertConfig"),fNe=o(" (SqueezeBERT model)"),mNe=l(),Qm=a("li"),cQ=a("strong"),gNe=o("swin"),hNe=o(" \u2014 "),d8=a("a"),pNe=o("SwinConfig"),_Ne=o(" (Swin model)"),uNe=l(),Hm=a("li"),fQ=a("strong"),bNe=o("t5"),vNe=o(" \u2014 "),c8=a("a"),TNe=o("T5Config"),FNe=o(" (T5 model)"),CNe=l(),Um=a("li"),mQ=a("strong"),MNe=o("tapas"),ENe=o(" \u2014 "),f8=a("a"),yNe=o("TapasConfig"),wNe=o(" (TAPAS model)"),ANe=l(),Jm=a("li"),gQ=a("strong"),LNe=o("transfo-xl"),BNe=o(" \u2014 "),m8=a("a"),kNe=o("TransfoXLConfig"),xNe=o(" (Transformer-XL model)"),RNe=l(),Ym=a("li"),hQ=a("strong"),SNe=o("trocr"),PNe=o(" \u2014 "),g8=a("a"),$Ne=o("TrOCRConfig"),INe=o(" (TrOCR model)"),jNe=l(),Km=a("li"),pQ=a("strong"),NNe=o("unispeech"),DNe=o(" \u2014 "),h8=a("a"),qNe=o("UniSpeechConfig"),GNe=o(" (UniSpeech model)"),ONe=l(),Zm=a("li"),_Q=a("strong"),XNe=o("unispeech-sat"),zNe=o(" \u2014 "),p8=a("a"),VNe=o("UniSpeechSatConfig"),WNe=o(" (UniSpeechSat model)"),QNe=l(),eg=a("li"),uQ=a("strong"),HNe=o("vilt"),UNe=o(" \u2014 "),_8=a("a"),JNe=o("ViltConfig"),YNe=o(" (ViLT model)"),KNe=l(),og=a("li"),bQ=a("strong"),ZNe=o("vision-encoder-decoder"),eDe=o(" \u2014 "),u8=a("a"),oDe=o("VisionEncoderDecoderConfig"),rDe=o(" (Vision Encoder decoder model)"),tDe=l(),rg=a("li"),vQ=a("strong"),aDe=o("vision-text-dual-encoder"),nDe=o(" \u2014 "),b8=a("a"),sDe=o("VisionTextDualEncoderConfig"),lDe=o(" (VisionTextDualEncoder model)"),iDe=l(),tg=a("li"),TQ=a("strong"),dDe=o("visual_bert"),cDe=o(" \u2014 "),v8=a("a"),fDe=o("VisualBertConfig"),mDe=o(" (VisualBert model)"),gDe=l(),ag=a("li"),FQ=a("strong"),hDe=o("vit"),pDe=o(" \u2014 "),T8=a("a"),_De=o("ViTConfig"),uDe=o(" (ViT model)"),bDe=l(),ng=a("li"),CQ=a("strong"),vDe=o("vit_mae"),TDe=o(" \u2014 "),F8=a("a"),FDe=o("ViTMAEConfig"),CDe=o(" (ViTMAE model)"),MDe=l(),sg=a("li"),MQ=a("strong"),EDe=o("wav2vec2"),yDe=o(" \u2014 "),C8=a("a"),wDe=o("Wav2Vec2Config"),ADe=o(" (Wav2Vec2 model)"),LDe=l(),lg=a("li"),EQ=a("strong"),BDe=o("wavlm"),kDe=o(" \u2014 "),M8=a("a"),xDe=o("WavLMConfig"),RDe=o(" (WavLM model)"),SDe=l(),ig=a("li"),yQ=a("strong"),PDe=o("xglm"),$De=o(" \u2014 "),E8=a("a"),IDe=o("XGLMConfig"),jDe=o(" (XGLM model)"),NDe=l(),dg=a("li"),wQ=a("strong"),DDe=o("xlm"),qDe=o(" \u2014 "),y8=a("a"),GDe=o("XLMConfig"),ODe=o(" (XLM model)"),XDe=l(),cg=a("li"),AQ=a("strong"),zDe=o("xlm-prophetnet"),VDe=o(" \u2014 "),w8=a("a"),WDe=o("XLMProphetNetConfig"),QDe=o(" (XLMProphetNet model)"),HDe=l(),fg=a("li"),LQ=a("strong"),UDe=o("xlm-roberta"),JDe=o(" \u2014 "),A8=a("a"),YDe=o("XLMRobertaConfig"),KDe=o(" (XLM-RoBERTa model)"),ZDe=l(),mg=a("li"),BQ=a("strong"),eqe=o("xlm-roberta-xl"),oqe=o(" \u2014 "),L8=a("a"),rqe=o("XLMRobertaXLConfig"),tqe=o(" (XLM-RoBERTa-XL model)"),aqe=l(),gg=a("li"),kQ=a("strong"),nqe=o("xlnet"),sqe=o(" \u2014 "),B8=a("a"),lqe=o("XLNetConfig"),iqe=o(" (XLNet model)"),dqe=l(),hg=a("li"),xQ=a("strong"),cqe=o("yoso"),fqe=o(" \u2014 "),k8=a("a"),mqe=o("YosoConfig"),gqe=o(" (YOSO model)"),hqe=l(),RQ=a("p"),pqe=o("Examples:"),_qe=l(),f(gM.$$.fragment),uqe=l(),pg=a("div"),f(hM.$$.fragment),bqe=l(),SQ=a("p"),vqe=o("Register a new configuration for this class."),pLe=l(),ji=a("h2"),_g=a("a"),PQ=a("span"),f(pM.$$.fragment),Tqe=l(),$Q=a("span"),Fqe=o("AutoTokenizer"),_Le=l(),Oo=a("div"),f(_M.$$.fragment),Cqe=l(),uM=a("p"),Mqe=o(`This is a generic tokenizer class that will be instantiated as one of the tokenizer classes of the library when
created with the `),x8=a("a"),Eqe=o("AutoTokenizer.from_pretrained()"),yqe=o(" class method."),wqe=l(),bM=a("p"),Aqe=o("This class cannot be instantiated directly using "),IQ=a("code"),Lqe=o("__init__()"),Bqe=o(" (throws an error)."),kqe=l(),mo=a("div"),f(vM.$$.fragment),xqe=l(),jQ=a("p"),Rqe=o("Instantiate one of the tokenizer classes of the library from a pretrained model vocabulary."),Sqe=l(),ja=a("p"),Pqe=o("The tokenizer class to instantiate is selected based on the "),NQ=a("code"),$qe=o("model_type"),Iqe=o(` property of the config object (either
passed as an argument or loaded from `),DQ=a("code"),jqe=o("pretrained_model_name_or_path"),Nqe=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),qQ=a("code"),Dqe=o("pretrained_model_name_or_path"),qqe=o(":"),Gqe=l(),M=a("ul"),Dn=a("li"),GQ=a("strong"),Oqe=o("albert"),Xqe=o(" \u2014 "),R8=a("a"),zqe=o("AlbertTokenizer"),Vqe=o(" or "),S8=a("a"),Wqe=o("AlbertTokenizerFast"),Qqe=o(" (ALBERT model)"),Hqe=l(),qn=a("li"),OQ=a("strong"),Uqe=o("bart"),Jqe=o(" \u2014 "),P8=a("a"),Yqe=o("BartTokenizer"),Kqe=o(" or "),$8=a("a"),Zqe=o("BartTokenizerFast"),eGe=o(" (BART model)"),oGe=l(),Gn=a("li"),XQ=a("strong"),rGe=o("barthez"),tGe=o(" \u2014 "),I8=a("a"),aGe=o("BarthezTokenizer"),nGe=o(" or "),j8=a("a"),sGe=o("BarthezTokenizerFast"),lGe=o(" (BARThez model)"),iGe=l(),ug=a("li"),zQ=a("strong"),dGe=o("bartpho"),cGe=o(" \u2014 "),N8=a("a"),fGe=o("BartphoTokenizer"),mGe=o(" (BARTpho model)"),gGe=l(),On=a("li"),VQ=a("strong"),hGe=o("bert"),pGe=o(" \u2014 "),D8=a("a"),_Ge=o("BertTokenizer"),uGe=o(" or "),q8=a("a"),bGe=o("BertTokenizerFast"),vGe=o(" (BERT model)"),TGe=l(),bg=a("li"),WQ=a("strong"),FGe=o("bert-generation"),CGe=o(" \u2014 "),G8=a("a"),MGe=o("BertGenerationTokenizer"),EGe=o(" (Bert Generation model)"),yGe=l(),vg=a("li"),QQ=a("strong"),wGe=o("bert-japanese"),AGe=o(" \u2014 "),O8=a("a"),LGe=o("BertJapaneseTokenizer"),BGe=o(" (BertJapanese model)"),kGe=l(),Tg=a("li"),HQ=a("strong"),xGe=o("bertweet"),RGe=o(" \u2014 "),X8=a("a"),SGe=o("BertweetTokenizer"),PGe=o(" (Bertweet model)"),$Ge=l(),Xn=a("li"),UQ=a("strong"),IGe=o("big_bird"),jGe=o(" \u2014 "),z8=a("a"),NGe=o("BigBirdTokenizer"),DGe=o(" or "),V8=a("a"),qGe=o("BigBirdTokenizerFast"),GGe=o(" (BigBird model)"),OGe=l(),zn=a("li"),JQ=a("strong"),XGe=o("bigbird_pegasus"),zGe=o(" \u2014 "),W8=a("a"),VGe=o("PegasusTokenizer"),WGe=o(" or "),Q8=a("a"),QGe=o("PegasusTokenizerFast"),HGe=o(" (BigBirdPegasus model)"),UGe=l(),Vn=a("li"),YQ=a("strong"),JGe=o("blenderbot"),YGe=o(" \u2014 "),H8=a("a"),KGe=o("BlenderbotTokenizer"),ZGe=o(" or "),U8=a("a"),eOe=o("BlenderbotTokenizerFast"),oOe=o(" (Blenderbot model)"),rOe=l(),Fg=a("li"),KQ=a("strong"),tOe=o("blenderbot-small"),aOe=o(" \u2014 "),J8=a("a"),nOe=o("BlenderbotSmallTokenizer"),sOe=o(" (BlenderbotSmall model)"),lOe=l(),Cg=a("li"),ZQ=a("strong"),iOe=o("byt5"),dOe=o(" \u2014 "),Y8=a("a"),cOe=o("ByT5Tokenizer"),fOe=o(" (ByT5 model)"),mOe=l(),Wn=a("li"),eH=a("strong"),gOe=o("camembert"),hOe=o(" \u2014 "),K8=a("a"),pOe=o("CamembertTokenizer"),_Oe=o(" or "),Z8=a("a"),uOe=o("CamembertTokenizerFast"),bOe=o(" (CamemBERT model)"),vOe=l(),Mg=a("li"),oH=a("strong"),TOe=o("canine"),FOe=o(" \u2014 "),eB=a("a"),COe=o("CanineTokenizer"),MOe=o(" (Canine model)"),EOe=l(),Qn=a("li"),rH=a("strong"),yOe=o("clip"),wOe=o(" \u2014 "),oB=a("a"),AOe=o("CLIPTokenizer"),LOe=o(" or "),rB=a("a"),BOe=o("CLIPTokenizerFast"),kOe=o(" (CLIP model)"),xOe=l(),Hn=a("li"),tH=a("strong"),ROe=o("convbert"),SOe=o(" \u2014 "),tB=a("a"),POe=o("ConvBertTokenizer"),$Oe=o(" or "),aB=a("a"),IOe=o("ConvBertTokenizerFast"),jOe=o(" (ConvBERT model)"),NOe=l(),Un=a("li"),aH=a("strong"),DOe=o("cpm"),qOe=o(" \u2014 "),nB=a("a"),GOe=o("CpmTokenizer"),OOe=o(" or "),nH=a("code"),XOe=o("CpmTokenizerFast"),zOe=o(" (CPM model)"),VOe=l(),Eg=a("li"),sH=a("strong"),WOe=o("ctrl"),QOe=o(" \u2014 "),sB=a("a"),HOe=o("CTRLTokenizer"),UOe=o(" (CTRL model)"),JOe=l(),Jn=a("li"),lH=a("strong"),YOe=o("deberta"),KOe=o(" \u2014 "),lB=a("a"),ZOe=o("DebertaTokenizer"),eXe=o(" or "),iB=a("a"),oXe=o("DebertaTokenizerFast"),rXe=o(" (DeBERTa model)"),tXe=l(),yg=a("li"),iH=a("strong"),aXe=o("deberta-v2"),nXe=o(" \u2014 "),dB=a("a"),sXe=o("DebertaV2Tokenizer"),lXe=o(" (DeBERTa-v2 model)"),iXe=l(),Yn=a("li"),dH=a("strong"),dXe=o("distilbert"),cXe=o(" \u2014 "),cB=a("a"),fXe=o("DistilBertTokenizer"),mXe=o(" or "),fB=a("a"),gXe=o("DistilBertTokenizerFast"),hXe=o(" (DistilBERT model)"),pXe=l(),Kn=a("li"),cH=a("strong"),_Xe=o("dpr"),uXe=o(" \u2014 "),mB=a("a"),bXe=o("DPRQuestionEncoderTokenizer"),vXe=o(" or "),gB=a("a"),TXe=o("DPRQuestionEncoderTokenizerFast"),FXe=o(" (DPR model)"),CXe=l(),Zn=a("li"),fH=a("strong"),MXe=o("electra"),EXe=o(" \u2014 "),hB=a("a"),yXe=o("ElectraTokenizer"),wXe=o(" or "),pB=a("a"),AXe=o("ElectraTokenizerFast"),LXe=o(" (ELECTRA model)"),BXe=l(),wg=a("li"),mH=a("strong"),kXe=o("flaubert"),xXe=o(" \u2014 "),_B=a("a"),RXe=o("FlaubertTokenizer"),SXe=o(" (FlauBERT model)"),PXe=l(),es=a("li"),gH=a("strong"),$Xe=o("fnet"),IXe=o(" \u2014 "),uB=a("a"),jXe=o("FNetTokenizer"),NXe=o(" or "),bB=a("a"),DXe=o("FNetTokenizerFast"),qXe=o(" (FNet model)"),GXe=l(),Ag=a("li"),hH=a("strong"),OXe=o("fsmt"),XXe=o(" \u2014 "),vB=a("a"),zXe=o("FSMTTokenizer"),VXe=o(" (FairSeq Machine-Translation model)"),WXe=l(),os=a("li"),pH=a("strong"),QXe=o("funnel"),HXe=o(" \u2014 "),TB=a("a"),UXe=o("FunnelTokenizer"),JXe=o(" or "),FB=a("a"),YXe=o("FunnelTokenizerFast"),KXe=o(" (Funnel Transformer model)"),ZXe=l(),rs=a("li"),_H=a("strong"),eze=o("gpt2"),oze=o(" \u2014 "),CB=a("a"),rze=o("GPT2Tokenizer"),tze=o(" or "),MB=a("a"),aze=o("GPT2TokenizerFast"),nze=o(" (OpenAI GPT-2 model)"),sze=l(),ts=a("li"),uH=a("strong"),lze=o("gpt_neo"),ize=o(" \u2014 "),EB=a("a"),dze=o("GPT2Tokenizer"),cze=o(" or "),yB=a("a"),fze=o("GPT2TokenizerFast"),mze=o(" (GPT Neo model)"),gze=l(),as=a("li"),bH=a("strong"),hze=o("herbert"),pze=o(" \u2014 "),wB=a("a"),_ze=o("HerbertTokenizer"),uze=o(" or "),AB=a("a"),bze=o("HerbertTokenizerFast"),vze=o(" (HerBERT model)"),Tze=l(),Lg=a("li"),vH=a("strong"),Fze=o("hubert"),Cze=o(" \u2014 "),LB=a("a"),Mze=o("Wav2Vec2CTCTokenizer"),Eze=o(" (Hubert model)"),yze=l(),ns=a("li"),TH=a("strong"),wze=o("ibert"),Aze=o(" \u2014 "),BB=a("a"),Lze=o("RobertaTokenizer"),Bze=o(" or "),kB=a("a"),kze=o("RobertaTokenizerFast"),xze=o(" (I-BERT model)"),Rze=l(),ss=a("li"),FH=a("strong"),Sze=o("layoutlm"),Pze=o(" \u2014 "),xB=a("a"),$ze=o("LayoutLMTokenizer"),Ize=o(" or "),RB=a("a"),jze=o("LayoutLMTokenizerFast"),Nze=o(" (LayoutLM model)"),Dze=l(),ls=a("li"),CH=a("strong"),qze=o("layoutlmv2"),Gze=o(" \u2014 "),SB=a("a"),Oze=o("LayoutLMv2Tokenizer"),Xze=o(" or "),PB=a("a"),zze=o("LayoutLMv2TokenizerFast"),Vze=o(" (LayoutLMv2 model)"),Wze=l(),is=a("li"),MH=a("strong"),Qze=o("layoutxlm"),Hze=o(" \u2014 "),$B=a("a"),Uze=o("LayoutXLMTokenizer"),Jze=o(" or "),IB=a("a"),Yze=o("LayoutXLMTokenizerFast"),Kze=o(" (LayoutXLM model)"),Zze=l(),ds=a("li"),EH=a("strong"),eVe=o("led"),oVe=o(" \u2014 "),jB=a("a"),rVe=o("LEDTokenizer"),tVe=o(" or "),NB=a("a"),aVe=o("LEDTokenizerFast"),nVe=o(" (LED model)"),sVe=l(),cs=a("li"),yH=a("strong"),lVe=o("longformer"),iVe=o(" \u2014 "),DB=a("a"),dVe=o("LongformerTokenizer"),cVe=o(" or "),qB=a("a"),fVe=o("LongformerTokenizerFast"),mVe=o(" (Longformer model)"),gVe=l(),Bg=a("li"),wH=a("strong"),hVe=o("luke"),pVe=o(" \u2014 "),GB=a("a"),_Ve=o("LukeTokenizer"),uVe=o(" (LUKE model)"),bVe=l(),fs=a("li"),AH=a("strong"),vVe=o("lxmert"),TVe=o(" \u2014 "),OB=a("a"),FVe=o("LxmertTokenizer"),CVe=o(" or "),XB=a("a"),MVe=o("LxmertTokenizerFast"),EVe=o(" (LXMERT model)"),yVe=l(),kg=a("li"),LH=a("strong"),wVe=o("m2m_100"),AVe=o(" \u2014 "),zB=a("a"),LVe=o("M2M100Tokenizer"),BVe=o(" (M2M100 model)"),kVe=l(),xg=a("li"),BH=a("strong"),xVe=o("marian"),RVe=o(" \u2014 "),VB=a("a"),SVe=o("MarianTokenizer"),PVe=o(" (Marian model)"),$Ve=l(),ms=a("li"),kH=a("strong"),IVe=o("mbart"),jVe=o(" \u2014 "),WB=a("a"),NVe=o("MBartTokenizer"),DVe=o(" or "),QB=a("a"),qVe=o("MBartTokenizerFast"),GVe=o(" (mBART model)"),OVe=l(),gs=a("li"),xH=a("strong"),XVe=o("mbart50"),zVe=o(" \u2014 "),HB=a("a"),VVe=o("MBart50Tokenizer"),WVe=o(" or "),UB=a("a"),QVe=o("MBart50TokenizerFast"),HVe=o(" (mBART-50 model)"),UVe=l(),Rg=a("li"),RH=a("strong"),JVe=o("mluke"),YVe=o(" \u2014 "),JB=a("a"),KVe=o("MLukeTokenizer"),ZVe=o(" (mLUKE model)"),eWe=l(),hs=a("li"),SH=a("strong"),oWe=o("mobilebert"),rWe=o(" \u2014 "),YB=a("a"),tWe=o("MobileBertTokenizer"),aWe=o(" or "),KB=a("a"),nWe=o("MobileBertTokenizerFast"),sWe=o(" (MobileBERT model)"),lWe=l(),ps=a("li"),PH=a("strong"),iWe=o("mpnet"),dWe=o(" \u2014 "),ZB=a("a"),cWe=o("MPNetTokenizer"),fWe=o(" or "),ek=a("a"),mWe=o("MPNetTokenizerFast"),gWe=o(" (MPNet model)"),hWe=l(),_s=a("li"),$H=a("strong"),pWe=o("mt5"),_We=o(" \u2014 "),ok=a("a"),uWe=o("MT5Tokenizer"),bWe=o(" or "),rk=a("a"),vWe=o("MT5TokenizerFast"),TWe=o(" (mT5 model)"),FWe=l(),us=a("li"),IH=a("strong"),CWe=o("openai-gpt"),MWe=o(" \u2014 "),tk=a("a"),EWe=o("OpenAIGPTTokenizer"),yWe=o(" or "),ak=a("a"),wWe=o("OpenAIGPTTokenizerFast"),AWe=o(" (OpenAI GPT model)"),LWe=l(),bs=a("li"),jH=a("strong"),BWe=o("pegasus"),kWe=o(" \u2014 "),nk=a("a"),xWe=o("PegasusTokenizer"),RWe=o(" or "),sk=a("a"),SWe=o("PegasusTokenizerFast"),PWe=o(" (Pegasus model)"),$We=l(),Sg=a("li"),NH=a("strong"),IWe=o("perceiver"),jWe=o(" \u2014 "),lk=a("a"),NWe=o("PerceiverTokenizer"),DWe=o(" (Perceiver model)"),qWe=l(),Pg=a("li"),DH=a("strong"),GWe=o("phobert"),OWe=o(" \u2014 "),ik=a("a"),XWe=o("PhobertTokenizer"),zWe=o(" (PhoBERT model)"),VWe=l(),$g=a("li"),qH=a("strong"),WWe=o("plbart"),QWe=o(" \u2014 "),dk=a("a"),HWe=o("PLBartTokenizer"),UWe=o(" (PLBart model)"),JWe=l(),Ig=a("li"),GH=a("strong"),YWe=o("prophetnet"),KWe=o(" \u2014 "),ck=a("a"),ZWe=o("ProphetNetTokenizer"),eQe=o(" (ProphetNet model)"),oQe=l(),vs=a("li"),OH=a("strong"),rQe=o("qdqbert"),tQe=o(" \u2014 "),fk=a("a"),aQe=o("BertTokenizer"),nQe=o(" or "),mk=a("a"),sQe=o("BertTokenizerFast"),lQe=o(" (QDQBert model)"),iQe=l(),jg=a("li"),XH=a("strong"),dQe=o("rag"),cQe=o(" \u2014 "),gk=a("a"),fQe=o("RagTokenizer"),mQe=o(" (RAG model)"),gQe=l(),Ts=a("li"),zH=a("strong"),hQe=o("realm"),pQe=o(" \u2014 "),hk=a("a"),_Qe=o("RealmTokenizer"),uQe=o(" or "),pk=a("a"),bQe=o("RealmTokenizerFast"),vQe=o(" (Realm model)"),TQe=l(),Fs=a("li"),VH=a("strong"),FQe=o("reformer"),CQe=o(" \u2014 "),_k=a("a"),MQe=o("ReformerTokenizer"),EQe=o(" or "),uk=a("a"),yQe=o("ReformerTokenizerFast"),wQe=o(" (Reformer model)"),AQe=l(),Cs=a("li"),WH=a("strong"),LQe=o("rembert"),BQe=o(" \u2014 "),bk=a("a"),kQe=o("RemBertTokenizer"),xQe=o(" or "),vk=a("a"),RQe=o("RemBertTokenizerFast"),SQe=o(" (RemBERT model)"),PQe=l(),Ms=a("li"),QH=a("strong"),$Qe=o("retribert"),IQe=o(" \u2014 "),Tk=a("a"),jQe=o("RetriBertTokenizer"),NQe=o(" or "),Fk=a("a"),DQe=o("RetriBertTokenizerFast"),qQe=o(" (RetriBERT model)"),GQe=l(),Es=a("li"),HH=a("strong"),OQe=o("roberta"),XQe=o(" \u2014 "),Ck=a("a"),zQe=o("RobertaTokenizer"),VQe=o(" or "),Mk=a("a"),WQe=o("RobertaTokenizerFast"),QQe=o(" (RoBERTa model)"),HQe=l(),ys=a("li"),UH=a("strong"),UQe=o("roformer"),JQe=o(" \u2014 "),Ek=a("a"),YQe=o("RoFormerTokenizer"),KQe=o(" or "),yk=a("a"),ZQe=o("RoFormerTokenizerFast"),eHe=o(" (RoFormer model)"),oHe=l(),Ng=a("li"),JH=a("strong"),rHe=o("speech_to_text"),tHe=o(" \u2014 "),wk=a("a"),aHe=o("Speech2TextTokenizer"),nHe=o(" (Speech2Text model)"),sHe=l(),Dg=a("li"),YH=a("strong"),lHe=o("speech_to_text_2"),iHe=o(" \u2014 "),Ak=a("a"),dHe=o("Speech2Text2Tokenizer"),cHe=o(" (Speech2Text2 model)"),fHe=l(),ws=a("li"),KH=a("strong"),mHe=o("splinter"),gHe=o(" \u2014 "),Lk=a("a"),hHe=o("SplinterTokenizer"),pHe=o(" or "),Bk=a("a"),_He=o("SplinterTokenizerFast"),uHe=o(" (Splinter model)"),bHe=l(),As=a("li"),ZH=a("strong"),vHe=o("squeezebert"),THe=o(" \u2014 "),kk=a("a"),FHe=o("SqueezeBertTokenizer"),CHe=o(" or "),xk=a("a"),MHe=o("SqueezeBertTokenizerFast"),EHe=o(" (SqueezeBERT model)"),yHe=l(),Ls=a("li"),eU=a("strong"),wHe=o("t5"),AHe=o(" \u2014 "),Rk=a("a"),LHe=o("T5Tokenizer"),BHe=o(" or "),Sk=a("a"),kHe=o("T5TokenizerFast"),xHe=o(" (T5 model)"),RHe=l(),qg=a("li"),oU=a("strong"),SHe=o("tapas"),PHe=o(" \u2014 "),Pk=a("a"),$He=o("TapasTokenizer"),IHe=o(" (TAPAS model)"),jHe=l(),Gg=a("li"),rU=a("strong"),NHe=o("transfo-xl"),DHe=o(" \u2014 "),$k=a("a"),qHe=o("TransfoXLTokenizer"),GHe=o(" (Transformer-XL model)"),OHe=l(),Og=a("li"),tU=a("strong"),XHe=o("wav2vec2"),zHe=o(" \u2014 "),Ik=a("a"),VHe=o("Wav2Vec2CTCTokenizer"),WHe=o(" (Wav2Vec2 model)"),QHe=l(),Xg=a("li"),aU=a("strong"),HHe=o("wav2vec2_phoneme"),UHe=o(" \u2014 "),jk=a("a"),JHe=o("Wav2Vec2PhonemeCTCTokenizer"),YHe=o(" (Wav2Vec2Phoneme model)"),KHe=l(),Bs=a("li"),nU=a("strong"),ZHe=o("xglm"),eUe=o(" \u2014 "),Nk=a("a"),oUe=o("XGLMTokenizer"),rUe=o(" or "),Dk=a("a"),tUe=o("XGLMTokenizerFast"),aUe=o(" (XGLM model)"),nUe=l(),zg=a("li"),sU=a("strong"),sUe=o("xlm"),lUe=o(" \u2014 "),qk=a("a"),iUe=o("XLMTokenizer"),dUe=o(" (XLM model)"),cUe=l(),Vg=a("li"),lU=a("strong"),fUe=o("xlm-prophetnet"),mUe=o(" \u2014 "),Gk=a("a"),gUe=o("XLMProphetNetTokenizer"),hUe=o(" (XLMProphetNet model)"),pUe=l(),ks=a("li"),iU=a("strong"),_Ue=o("xlm-roberta"),uUe=o(" \u2014 "),Ok=a("a"),bUe=o("XLMRobertaTokenizer"),vUe=o(" or "),Xk=a("a"),TUe=o("XLMRobertaTokenizerFast"),FUe=o(" (XLM-RoBERTa model)"),CUe=l(),xs=a("li"),dU=a("strong"),MUe=o("xlnet"),EUe=o(" \u2014 "),zk=a("a"),yUe=o("XLNetTokenizer"),wUe=o(" or "),Vk=a("a"),AUe=o("XLNetTokenizerFast"),LUe=o(" (XLNet model)"),BUe=l(),cU=a("p"),kUe=o("Examples:"),xUe=l(),f(TM.$$.fragment),RUe=l(),Wg=a("div"),f(FM.$$.fragment),SUe=l(),fU=a("p"),PUe=o("Register a new tokenizer in this mapping."),uLe=l(),Ni=a("h2"),Qg=a("a"),mU=a("span"),f(CM.$$.fragment),$Ue=l(),gU=a("span"),IUe=o("AutoFeatureExtractor"),bLe=l(),Xo=a("div"),f(MM.$$.fragment),jUe=l(),EM=a("p"),NUe=o(`This is a generic feature extractor class that will be instantiated as one of the feature extractor classes of the
library when created with the `),Wk=a("a"),DUe=o("AutoFeatureExtractor.from_pretrained()"),qUe=o(" class method."),GUe=l(),yM=a("p"),OUe=o("This class cannot be instantiated directly using "),hU=a("code"),XUe=o("__init__()"),zUe=o(" (throws an error)."),VUe=l(),Le=a("div"),f(wM.$$.fragment),WUe=l(),pU=a("p"),QUe=o("Instantiate one of the feature extractor classes of the library from a pretrained model vocabulary."),HUe=l(),Na=a("p"),UUe=o("The feature extractor class to instantiate is selected based on the "),_U=a("code"),JUe=o("model_type"),YUe=o(` property of the config object
(either passed as an argument or loaded from `),uU=a("code"),KUe=o("pretrained_model_name_or_path"),ZUe=o(` if possible), or when it\u2019s
missing, by falling back to using pattern matching on `),bU=a("code"),eJe=o("pretrained_model_name_or_path"),oJe=o(":"),rJe=l(),se=a("ul"),Hg=a("li"),vU=a("strong"),tJe=o("beit"),aJe=o(" \u2014 "),Qk=a("a"),nJe=o("BeitFeatureExtractor"),sJe=o(" (BEiT model)"),lJe=l(),Ug=a("li"),TU=a("strong"),iJe=o("clip"),dJe=o(" \u2014 "),Hk=a("a"),cJe=o("CLIPFeatureExtractor"),fJe=o(" (CLIP model)"),mJe=l(),Jg=a("li"),FU=a("strong"),gJe=o("convnext"),hJe=o(" \u2014 "),Uk=a("a"),pJe=o("ConvNextFeatureExtractor"),_Je=o(" (ConvNext model)"),uJe=l(),Yg=a("li"),CU=a("strong"),bJe=o("deit"),vJe=o(" \u2014 "),Jk=a("a"),TJe=o("DeiTFeatureExtractor"),FJe=o(" (DeiT model)"),CJe=l(),Kg=a("li"),MU=a("strong"),MJe=o("detr"),EJe=o(" \u2014 "),Yk=a("a"),yJe=o("DetrFeatureExtractor"),wJe=o(" (DETR model)"),AJe=l(),Zg=a("li"),EU=a("strong"),LJe=o("hubert"),BJe=o(" \u2014 "),Kk=a("a"),kJe=o("Wav2Vec2FeatureExtractor"),xJe=o(" (Hubert model)"),RJe=l(),eh=a("li"),yU=a("strong"),SJe=o("layoutlmv2"),PJe=o(" \u2014 "),Zk=a("a"),$Je=o("LayoutLMv2FeatureExtractor"),IJe=o(" (LayoutLMv2 model)"),jJe=l(),oh=a("li"),wU=a("strong"),NJe=o("perceiver"),DJe=o(" \u2014 "),ex=a("a"),qJe=o("PerceiverFeatureExtractor"),GJe=o(" (Perceiver model)"),OJe=l(),rh=a("li"),AU=a("strong"),XJe=o("poolformer"),zJe=o(" \u2014 "),ox=a("a"),VJe=o("PoolFormerFeatureExtractor"),WJe=o(" (PoolFormer model)"),QJe=l(),th=a("li"),LU=a("strong"),HJe=o("segformer"),UJe=o(" \u2014 "),rx=a("a"),JJe=o("SegformerFeatureExtractor"),YJe=o(" (SegFormer model)"),KJe=l(),ah=a("li"),BU=a("strong"),ZJe=o("speech_to_text"),eYe=o(" \u2014 "),tx=a("a"),oYe=o("Speech2TextFeatureExtractor"),rYe=o(" (Speech2Text model)"),tYe=l(),nh=a("li"),kU=a("strong"),aYe=o("swin"),nYe=o(" \u2014 "),ax=a("a"),sYe=o("ViTFeatureExtractor"),lYe=o(" (Swin model)"),iYe=l(),sh=a("li"),xU=a("strong"),dYe=o("vit"),cYe=o(" \u2014 "),nx=a("a"),fYe=o("ViTFeatureExtractor"),mYe=o(" (ViT model)"),gYe=l(),lh=a("li"),RU=a("strong"),hYe=o("vit_mae"),pYe=o(" \u2014 "),sx=a("a"),_Ye=o("ViTFeatureExtractor"),uYe=o(" (ViTMAE model)"),bYe=l(),ih=a("li"),SU=a("strong"),vYe=o("wav2vec2"),TYe=o(" \u2014 "),lx=a("a"),FYe=o("Wav2Vec2FeatureExtractor"),CYe=o(" (Wav2Vec2 model)"),MYe=l(),f(dh.$$.fragment),EYe=l(),PU=a("p"),yYe=o("Examples:"),wYe=l(),f(AM.$$.fragment),AYe=l(),ch=a("div"),f(LM.$$.fragment),LYe=l(),$U=a("p"),BYe=o("Register a new feature extractor for this class."),vLe=l(),Di=a("h2"),fh=a("a"),IU=a("span"),f(BM.$$.fragment),kYe=l(),jU=a("span"),xYe=o("AutoProcessor"),TLe=l(),zo=a("div"),f(kM.$$.fragment),RYe=l(),xM=a("p"),SYe=o(`This is a generic processor class that will be instantiated as one of the processor classes of the library when
created with the `),ix=a("a"),PYe=o("AutoProcessor.from_pretrained()"),$Ye=o(" class method."),IYe=l(),RM=a("p"),jYe=o("This class cannot be instantiated directly using "),NU=a("code"),NYe=o("__init__()"),DYe=o(" (throws an error)."),qYe=l(),Be=a("div"),f(SM.$$.fragment),GYe=l(),DU=a("p"),OYe=o("Instantiate one of the processor classes of the library from a pretrained model vocabulary."),XYe=l(),qi=a("p"),zYe=o("The processor class to instantiate is selected based on the "),qU=a("code"),VYe=o("model_type"),WYe=o(` property of the config object (either
passed as an argument or loaded from `),GU=a("code"),QYe=o("pretrained_model_name_or_path"),HYe=o(" if possible):"),UYe=l(),we=a("ul"),mh=a("li"),OU=a("strong"),JYe=o("clip"),YYe=o(" \u2014 "),dx=a("a"),KYe=o("CLIPProcessor"),ZYe=o(" (CLIP model)"),eKe=l(),gh=a("li"),XU=a("strong"),oKe=o("layoutlmv2"),rKe=o(" \u2014 "),cx=a("a"),tKe=o("LayoutLMv2Processor"),aKe=o(" (LayoutLMv2 model)"),nKe=l(),hh=a("li"),zU=a("strong"),sKe=o("layoutxlm"),lKe=o(" \u2014 "),fx=a("a"),iKe=o("LayoutXLMProcessor"),dKe=o(" (LayoutXLM model)"),cKe=l(),ph=a("li"),VU=a("strong"),fKe=o("speech_to_text"),mKe=o(" \u2014 "),mx=a("a"),gKe=o("Speech2TextProcessor"),hKe=o(" (Speech2Text model)"),pKe=l(),_h=a("li"),WU=a("strong"),_Ke=o("speech_to_text_2"),uKe=o(" \u2014 "),gx=a("a"),bKe=o("Speech2Text2Processor"),vKe=o(" (Speech2Text2 model)"),TKe=l(),uh=a("li"),QU=a("strong"),FKe=o("trocr"),CKe=o(" \u2014 "),hx=a("a"),MKe=o("TrOCRProcessor"),EKe=o(" (TrOCR model)"),yKe=l(),bh=a("li"),HU=a("strong"),wKe=o("vision-text-dual-encoder"),AKe=o(" \u2014 "),px=a("a"),LKe=o("VisionTextDualEncoderProcessor"),BKe=o(" (VisionTextDualEncoder model)"),kKe=l(),vh=a("li"),UU=a("strong"),xKe=o("wav2vec2"),RKe=o(" \u2014 "),_x=a("a"),SKe=o("Wav2Vec2Processor"),PKe=o(" (Wav2Vec2 model)"),$Ke=l(),f(Th.$$.fragment),IKe=l(),JU=a("p"),jKe=o("Examples:"),NKe=l(),f(PM.$$.fragment),DKe=l(),Fh=a("div"),f($M.$$.fragment),qKe=l(),YU=a("p"),GKe=o("Register a new processor for this class."),FLe=l(),Gi=a("h2"),Ch=a("a"),KU=a("span"),f(IM.$$.fragment),OKe=l(),ZU=a("span"),XKe=o("AutoModel"),CLe=l(),Vo=a("div"),f(jM.$$.fragment),zKe=l(),Oi=a("p"),VKe=o(`This is a generic model class that will be instantiated as one of the base model classes of the library when created
with the `),eJ=a("code"),WKe=o("from_pretrained()"),QKe=o("class method or the "),oJ=a("code"),HKe=o("from_config()"),UKe=o(`class
method.`),JKe=l(),NM=a("p"),YKe=o("This class cannot be instantiated directly using "),rJ=a("code"),KKe=o("__init__()"),ZKe=o(" (throws an error)."),eZe=l(),Nr=a("div"),f(DM.$$.fragment),oZe=l(),tJ=a("p"),rZe=o("Instantiates one of the base model classes of the library from a configuration."),tZe=l(),Xi=a("p"),aZe=o(`Note:
Loading a model from its configuration file does `),aJ=a("strong"),nZe=o("not"),sZe=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),nJ=a("code"),lZe=o("from_pretrained()"),iZe=o("to load the model weights."),dZe=l(),sJ=a("p"),cZe=o("Examples:"),fZe=l(),f(qM.$$.fragment),mZe=l(),ke=a("div"),f(GM.$$.fragment),gZe=l(),lJ=a("p"),hZe=o("Instantiate one of the base model classes of the library from a pretrained model."),pZe=l(),Da=a("p"),_Ze=o("The model class to instantiate is selected based on the "),iJ=a("code"),uZe=o("model_type"),bZe=o(` property of the config object (either
passed as an argument or loaded from `),dJ=a("code"),vZe=o("pretrained_model_name_or_path"),TZe=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),cJ=a("code"),FZe=o("pretrained_model_name_or_path"),CZe=o(":"),MZe=l(),F=a("ul"),Mh=a("li"),fJ=a("strong"),EZe=o("albert"),yZe=o(" \u2014 "),ux=a("a"),wZe=o("AlbertModel"),AZe=o(" (ALBERT model)"),LZe=l(),Eh=a("li"),mJ=a("strong"),BZe=o("bart"),kZe=o(" \u2014 "),bx=a("a"),xZe=o("BartModel"),RZe=o(" (BART model)"),SZe=l(),yh=a("li"),gJ=a("strong"),PZe=o("beit"),$Ze=o(" \u2014 "),vx=a("a"),IZe=o("BeitModel"),jZe=o(" (BEiT model)"),NZe=l(),wh=a("li"),hJ=a("strong"),DZe=o("bert"),qZe=o(" \u2014 "),Tx=a("a"),GZe=o("BertModel"),OZe=o(" (BERT model)"),XZe=l(),Ah=a("li"),pJ=a("strong"),zZe=o("bert-generation"),VZe=o(" \u2014 "),Fx=a("a"),WZe=o("BertGenerationEncoder"),QZe=o(" (Bert Generation model)"),HZe=l(),Lh=a("li"),_J=a("strong"),UZe=o("big_bird"),JZe=o(" \u2014 "),Cx=a("a"),YZe=o("BigBirdModel"),KZe=o(" (BigBird model)"),ZZe=l(),Bh=a("li"),uJ=a("strong"),eeo=o("bigbird_pegasus"),oeo=o(" \u2014 "),Mx=a("a"),reo=o("BigBirdPegasusModel"),teo=o(" (BigBirdPegasus model)"),aeo=l(),kh=a("li"),bJ=a("strong"),neo=o("blenderbot"),seo=o(" \u2014 "),Ex=a("a"),leo=o("BlenderbotModel"),ieo=o(" (Blenderbot model)"),deo=l(),xh=a("li"),vJ=a("strong"),ceo=o("blenderbot-small"),feo=o(" \u2014 "),yx=a("a"),meo=o("BlenderbotSmallModel"),geo=o(" (BlenderbotSmall model)"),heo=l(),Rh=a("li"),TJ=a("strong"),peo=o("camembert"),_eo=o(" \u2014 "),wx=a("a"),ueo=o("CamembertModel"),beo=o(" (CamemBERT model)"),veo=l(),Sh=a("li"),FJ=a("strong"),Teo=o("canine"),Feo=o(" \u2014 "),Ax=a("a"),Ceo=o("CanineModel"),Meo=o(" (Canine model)"),Eeo=l(),Ph=a("li"),CJ=a("strong"),yeo=o("clip"),weo=o(" \u2014 "),Lx=a("a"),Aeo=o("CLIPModel"),Leo=o(" (CLIP model)"),Beo=l(),$h=a("li"),MJ=a("strong"),keo=o("convbert"),xeo=o(" \u2014 "),Bx=a("a"),Reo=o("ConvBertModel"),Seo=o(" (ConvBERT model)"),Peo=l(),Ih=a("li"),EJ=a("strong"),$eo=o("convnext"),Ieo=o(" \u2014 "),kx=a("a"),jeo=o("ConvNextModel"),Neo=o(" (ConvNext model)"),Deo=l(),jh=a("li"),yJ=a("strong"),qeo=o("ctrl"),Geo=o(" \u2014 "),xx=a("a"),Oeo=o("CTRLModel"),Xeo=o(" (CTRL model)"),zeo=l(),Nh=a("li"),wJ=a("strong"),Veo=o("deberta"),Weo=o(" \u2014 "),Rx=a("a"),Qeo=o("DebertaModel"),Heo=o(" (DeBERTa model)"),Ueo=l(),Dh=a("li"),AJ=a("strong"),Jeo=o("deberta-v2"),Yeo=o(" \u2014 "),Sx=a("a"),Keo=o("DebertaV2Model"),Zeo=o(" (DeBERTa-v2 model)"),eoo=l(),qh=a("li"),LJ=a("strong"),ooo=o("deit"),roo=o(" \u2014 "),Px=a("a"),too=o("DeiTModel"),aoo=o(" (DeiT model)"),noo=l(),Gh=a("li"),BJ=a("strong"),soo=o("detr"),loo=o(" \u2014 "),$x=a("a"),ioo=o("DetrModel"),doo=o(" (DETR model)"),coo=l(),Oh=a("li"),kJ=a("strong"),foo=o("distilbert"),moo=o(" \u2014 "),Ix=a("a"),goo=o("DistilBertModel"),hoo=o(" (DistilBERT model)"),poo=l(),Xh=a("li"),xJ=a("strong"),_oo=o("dpr"),uoo=o(" \u2014 "),jx=a("a"),boo=o("DPRQuestionEncoder"),voo=o(" (DPR model)"),Too=l(),zh=a("li"),RJ=a("strong"),Foo=o("electra"),Coo=o(" \u2014 "),Nx=a("a"),Moo=o("ElectraModel"),Eoo=o(" (ELECTRA model)"),yoo=l(),Vh=a("li"),SJ=a("strong"),woo=o("flaubert"),Aoo=o(" \u2014 "),Dx=a("a"),Loo=o("FlaubertModel"),Boo=o(" (FlauBERT model)"),koo=l(),Wh=a("li"),PJ=a("strong"),xoo=o("fnet"),Roo=o(" \u2014 "),qx=a("a"),Soo=o("FNetModel"),Poo=o(" (FNet model)"),$oo=l(),Qh=a("li"),$J=a("strong"),Ioo=o("fsmt"),joo=o(" \u2014 "),Gx=a("a"),Noo=o("FSMTModel"),Doo=o(" (FairSeq Machine-Translation model)"),qoo=l(),Rs=a("li"),IJ=a("strong"),Goo=o("funnel"),Ooo=o(" \u2014 "),Ox=a("a"),Xoo=o("FunnelModel"),zoo=o(" or "),Xx=a("a"),Voo=o("FunnelBaseModel"),Woo=o(" (Funnel Transformer model)"),Qoo=l(),Hh=a("li"),jJ=a("strong"),Hoo=o("gpt2"),Uoo=o(" \u2014 "),zx=a("a"),Joo=o("GPT2Model"),Yoo=o(" (OpenAI GPT-2 model)"),Koo=l(),Uh=a("li"),NJ=a("strong"),Zoo=o("gpt_neo"),ero=o(" \u2014 "),Vx=a("a"),oro=o("GPTNeoModel"),rro=o(" (GPT Neo model)"),tro=l(),Jh=a("li"),DJ=a("strong"),aro=o("gptj"),nro=o(" \u2014 "),Wx=a("a"),sro=o("GPTJModel"),lro=o(" (GPT-J model)"),iro=l(),Yh=a("li"),qJ=a("strong"),dro=o("hubert"),cro=o(" \u2014 "),Qx=a("a"),fro=o("HubertModel"),mro=o(" (Hubert model)"),gro=l(),Kh=a("li"),GJ=a("strong"),hro=o("ibert"),pro=o(" \u2014 "),Hx=a("a"),_ro=o("IBertModel"),uro=o(" (I-BERT model)"),bro=l(),Zh=a("li"),OJ=a("strong"),vro=o("imagegpt"),Tro=o(" \u2014 "),Ux=a("a"),Fro=o("ImageGPTModel"),Cro=o(" (ImageGPT model)"),Mro=l(),ep=a("li"),XJ=a("strong"),Ero=o("layoutlm"),yro=o(" \u2014 "),Jx=a("a"),wro=o("LayoutLMModel"),Aro=o(" (LayoutLM model)"),Lro=l(),op=a("li"),zJ=a("strong"),Bro=o("layoutlmv2"),kro=o(" \u2014 "),Yx=a("a"),xro=o("LayoutLMv2Model"),Rro=o(" (LayoutLMv2 model)"),Sro=l(),rp=a("li"),VJ=a("strong"),Pro=o("led"),$ro=o(" \u2014 "),Kx=a("a"),Iro=o("LEDModel"),jro=o(" (LED model)"),Nro=l(),tp=a("li"),WJ=a("strong"),Dro=o("longformer"),qro=o(" \u2014 "),Zx=a("a"),Gro=o("LongformerModel"),Oro=o(" (Longformer model)"),Xro=l(),ap=a("li"),QJ=a("strong"),zro=o("luke"),Vro=o(" \u2014 "),eR=a("a"),Wro=o("LukeModel"),Qro=o(" (LUKE model)"),Hro=l(),np=a("li"),HJ=a("strong"),Uro=o("lxmert"),Jro=o(" \u2014 "),oR=a("a"),Yro=o("LxmertModel"),Kro=o(" (LXMERT model)"),Zro=l(),sp=a("li"),UJ=a("strong"),eto=o("m2m_100"),oto=o(" \u2014 "),rR=a("a"),rto=o("M2M100Model"),tto=o(" (M2M100 model)"),ato=l(),lp=a("li"),JJ=a("strong"),nto=o("marian"),sto=o(" \u2014 "),tR=a("a"),lto=o("MarianModel"),ito=o(" (Marian model)"),dto=l(),ip=a("li"),YJ=a("strong"),cto=o("mbart"),fto=o(" \u2014 "),aR=a("a"),mto=o("MBartModel"),gto=o(" (mBART model)"),hto=l(),dp=a("li"),KJ=a("strong"),pto=o("megatron-bert"),_to=o(" \u2014 "),nR=a("a"),uto=o("MegatronBertModel"),bto=o(" (MegatronBert model)"),vto=l(),cp=a("li"),ZJ=a("strong"),Tto=o("mobilebert"),Fto=o(" \u2014 "),sR=a("a"),Cto=o("MobileBertModel"),Mto=o(" (MobileBERT model)"),Eto=l(),fp=a("li"),eY=a("strong"),yto=o("mpnet"),wto=o(" \u2014 "),lR=a("a"),Ato=o("MPNetModel"),Lto=o(" (MPNet model)"),Bto=l(),mp=a("li"),oY=a("strong"),kto=o("mt5"),xto=o(" \u2014 "),iR=a("a"),Rto=o("MT5Model"),Sto=o(" (mT5 model)"),Pto=l(),gp=a("li"),rY=a("strong"),$to=o("nystromformer"),Ito=o(" \u2014 "),dR=a("a"),jto=o("NystromformerModel"),Nto=o(" (Nystromformer model)"),Dto=l(),hp=a("li"),tY=a("strong"),qto=o("openai-gpt"),Gto=o(" \u2014 "),cR=a("a"),Oto=o("OpenAIGPTModel"),Xto=o(" (OpenAI GPT model)"),zto=l(),pp=a("li"),aY=a("strong"),Vto=o("pegasus"),Wto=o(" \u2014 "),fR=a("a"),Qto=o("PegasusModel"),Hto=o(" (Pegasus model)"),Uto=l(),_p=a("li"),nY=a("strong"),Jto=o("perceiver"),Yto=o(" \u2014 "),mR=a("a"),Kto=o("PerceiverModel"),Zto=o(" (Perceiver model)"),eao=l(),up=a("li"),sY=a("strong"),oao=o("plbart"),rao=o(" \u2014 "),gR=a("a"),tao=o("PLBartModel"),aao=o(" (PLBart model)"),nao=l(),bp=a("li"),lY=a("strong"),sao=o("poolformer"),lao=o(" \u2014 "),hR=a("a"),iao=o("PoolFormerModel"),dao=o(" (PoolFormer model)"),cao=l(),vp=a("li"),iY=a("strong"),fao=o("prophetnet"),mao=o(" \u2014 "),pR=a("a"),gao=o("ProphetNetModel"),hao=o(" (ProphetNet model)"),pao=l(),Tp=a("li"),dY=a("strong"),_ao=o("qdqbert"),uao=o(" \u2014 "),_R=a("a"),bao=o("QDQBertModel"),vao=o(" (QDQBert model)"),Tao=l(),Fp=a("li"),cY=a("strong"),Fao=o("reformer"),Cao=o(" \u2014 "),uR=a("a"),Mao=o("ReformerModel"),Eao=o(" (Reformer model)"),yao=l(),Cp=a("li"),fY=a("strong"),wao=o("rembert"),Aao=o(" \u2014 "),bR=a("a"),Lao=o("RemBertModel"),Bao=o(" (RemBERT model)"),kao=l(),Mp=a("li"),mY=a("strong"),xao=o("retribert"),Rao=o(" \u2014 "),vR=a("a"),Sao=o("RetriBertModel"),Pao=o(" (RetriBERT model)"),$ao=l(),Ep=a("li"),gY=a("strong"),Iao=o("roberta"),jao=o(" \u2014 "),TR=a("a"),Nao=o("RobertaModel"),Dao=o(" (RoBERTa model)"),qao=l(),yp=a("li"),hY=a("strong"),Gao=o("roformer"),Oao=o(" \u2014 "),FR=a("a"),Xao=o("RoFormerModel"),zao=o(" (RoFormer model)"),Vao=l(),wp=a("li"),pY=a("strong"),Wao=o("segformer"),Qao=o(" \u2014 "),CR=a("a"),Hao=o("SegformerModel"),Uao=o(" (SegFormer model)"),Jao=l(),Ap=a("li"),_Y=a("strong"),Yao=o("sew"),Kao=o(" \u2014 "),MR=a("a"),Zao=o("SEWModel"),eno=o(" (SEW model)"),ono=l(),Lp=a("li"),uY=a("strong"),rno=o("sew-d"),tno=o(" \u2014 "),ER=a("a"),ano=o("SEWDModel"),nno=o(" (SEW-D model)"),sno=l(),Bp=a("li"),bY=a("strong"),lno=o("speech_to_text"),ino=o(" \u2014 "),yR=a("a"),dno=o("Speech2TextModel"),cno=o(" (Speech2Text model)"),fno=l(),kp=a("li"),vY=a("strong"),mno=o("splinter"),gno=o(" \u2014 "),wR=a("a"),hno=o("SplinterModel"),pno=o(" (Splinter model)"),_no=l(),xp=a("li"),TY=a("strong"),uno=o("squeezebert"),bno=o(" \u2014 "),AR=a("a"),vno=o("SqueezeBertModel"),Tno=o(" (SqueezeBERT model)"),Fno=l(),Rp=a("li"),FY=a("strong"),Cno=o("swin"),Mno=o(" \u2014 "),LR=a("a"),Eno=o("SwinModel"),yno=o(" (Swin model)"),wno=l(),Sp=a("li"),CY=a("strong"),Ano=o("t5"),Lno=o(" \u2014 "),BR=a("a"),Bno=o("T5Model"),kno=o(" (T5 model)"),xno=l(),Pp=a("li"),MY=a("strong"),Rno=o("tapas"),Sno=o(" \u2014 "),kR=a("a"),Pno=o("TapasModel"),$no=o(" (TAPAS model)"),Ino=l(),$p=a("li"),EY=a("strong"),jno=o("transfo-xl"),Nno=o(" \u2014 "),xR=a("a"),Dno=o("TransfoXLModel"),qno=o(" (Transformer-XL model)"),Gno=l(),Ip=a("li"),yY=a("strong"),Ono=o("unispeech"),Xno=o(" \u2014 "),RR=a("a"),zno=o("UniSpeechModel"),Vno=o(" (UniSpeech model)"),Wno=l(),jp=a("li"),wY=a("strong"),Qno=o("unispeech-sat"),Hno=o(" \u2014 "),SR=a("a"),Uno=o("UniSpeechSatModel"),Jno=o(" (UniSpeechSat model)"),Yno=l(),Np=a("li"),AY=a("strong"),Kno=o("vilt"),Zno=o(" \u2014 "),PR=a("a"),eso=o("ViltModel"),oso=o(" (ViLT model)"),rso=l(),Dp=a("li"),LY=a("strong"),tso=o("vision-text-dual-encoder"),aso=o(" \u2014 "),$R=a("a"),nso=o("VisionTextDualEncoderModel"),sso=o(" (VisionTextDualEncoder model)"),lso=l(),qp=a("li"),BY=a("strong"),iso=o("visual_bert"),dso=o(" \u2014 "),IR=a("a"),cso=o("VisualBertModel"),fso=o(" (VisualBert model)"),mso=l(),Gp=a("li"),kY=a("strong"),gso=o("vit"),hso=o(" \u2014 "),jR=a("a"),pso=o("ViTModel"),_so=o(" (ViT model)"),uso=l(),Op=a("li"),xY=a("strong"),bso=o("vit_mae"),vso=o(" \u2014 "),NR=a("a"),Tso=o("ViTMAEModel"),Fso=o(" (ViTMAE model)"),Cso=l(),Xp=a("li"),RY=a("strong"),Mso=o("wav2vec2"),Eso=o(" \u2014 "),DR=a("a"),yso=o("Wav2Vec2Model"),wso=o(" (Wav2Vec2 model)"),Aso=l(),zp=a("li"),SY=a("strong"),Lso=o("wavlm"),Bso=o(" \u2014 "),qR=a("a"),kso=o("WavLMModel"),xso=o(" (WavLM model)"),Rso=l(),Vp=a("li"),PY=a("strong"),Sso=o("xglm"),Pso=o(" \u2014 "),GR=a("a"),$so=o("XGLMModel"),Iso=o(" (XGLM model)"),jso=l(),Wp=a("li"),$Y=a("strong"),Nso=o("xlm"),Dso=o(" \u2014 "),OR=a("a"),qso=o("XLMModel"),Gso=o(" (XLM model)"),Oso=l(),Qp=a("li"),IY=a("strong"),Xso=o("xlm-prophetnet"),zso=o(" \u2014 "),XR=a("a"),Vso=o("XLMProphetNetModel"),Wso=o(" (XLMProphetNet model)"),Qso=l(),Hp=a("li"),jY=a("strong"),Hso=o("xlm-roberta"),Uso=o(" \u2014 "),zR=a("a"),Jso=o("XLMRobertaModel"),Yso=o(" (XLM-RoBERTa model)"),Kso=l(),Up=a("li"),NY=a("strong"),Zso=o("xlm-roberta-xl"),elo=o(" \u2014 "),VR=a("a"),olo=o("XLMRobertaXLModel"),rlo=o(" (XLM-RoBERTa-XL model)"),tlo=l(),Jp=a("li"),DY=a("strong"),alo=o("xlnet"),nlo=o(" \u2014 "),WR=a("a"),slo=o("XLNetModel"),llo=o(" (XLNet model)"),ilo=l(),Yp=a("li"),qY=a("strong"),dlo=o("yoso"),clo=o(" \u2014 "),QR=a("a"),flo=o("YosoModel"),mlo=o(" (YOSO model)"),glo=l(),Kp=a("p"),hlo=o("The model is set in evaluation mode by default using "),GY=a("code"),plo=o("model.eval()"),_lo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),OY=a("code"),ulo=o("model.train()"),blo=l(),XY=a("p"),vlo=o("Examples:"),Tlo=l(),f(OM.$$.fragment),MLe=l(),zi=a("h2"),Zp=a("a"),zY=a("span"),f(XM.$$.fragment),Flo=l(),VY=a("span"),Clo=o("AutoModelForPreTraining"),ELe=l(),Wo=a("div"),f(zM.$$.fragment),Mlo=l(),Vi=a("p"),Elo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a pretraining head) when created
with the `),WY=a("code"),ylo=o("from_pretrained()"),wlo=o("class method or the "),QY=a("code"),Alo=o("from_config()"),Llo=o(`class
method.`),Blo=l(),VM=a("p"),klo=o("This class cannot be instantiated directly using "),HY=a("code"),xlo=o("__init__()"),Rlo=o(" (throws an error)."),Slo=l(),Dr=a("div"),f(WM.$$.fragment),Plo=l(),UY=a("p"),$lo=o("Instantiates one of the model classes of the library (with a pretraining head) from a configuration."),Ilo=l(),Wi=a("p"),jlo=o(`Note:
Loading a model from its configuration file does `),JY=a("strong"),Nlo=o("not"),Dlo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),YY=a("code"),qlo=o("from_pretrained()"),Glo=o("to load the model weights."),Olo=l(),KY=a("p"),Xlo=o("Examples:"),zlo=l(),f(QM.$$.fragment),Vlo=l(),xe=a("div"),f(HM.$$.fragment),Wlo=l(),ZY=a("p"),Qlo=o("Instantiate one of the model classes of the library (with a pretraining head) from a pretrained model."),Hlo=l(),qa=a("p"),Ulo=o("The model class to instantiate is selected based on the "),eK=a("code"),Jlo=o("model_type"),Ylo=o(` property of the config object (either
passed as an argument or loaded from `),oK=a("code"),Klo=o("pretrained_model_name_or_path"),Zlo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),rK=a("code"),eio=o("pretrained_model_name_or_path"),oio=o(":"),rio=l(),x=a("ul"),e_=a("li"),tK=a("strong"),tio=o("albert"),aio=o(" \u2014 "),HR=a("a"),nio=o("AlbertForPreTraining"),sio=o(" (ALBERT model)"),lio=l(),o_=a("li"),aK=a("strong"),iio=o("bart"),dio=o(" \u2014 "),UR=a("a"),cio=o("BartForConditionalGeneration"),fio=o(" (BART model)"),mio=l(),r_=a("li"),nK=a("strong"),gio=o("bert"),hio=o(" \u2014 "),JR=a("a"),pio=o("BertForPreTraining"),_io=o(" (BERT model)"),uio=l(),t_=a("li"),sK=a("strong"),bio=o("big_bird"),vio=o(" \u2014 "),YR=a("a"),Tio=o("BigBirdForPreTraining"),Fio=o(" (BigBird model)"),Cio=l(),a_=a("li"),lK=a("strong"),Mio=o("camembert"),Eio=o(" \u2014 "),KR=a("a"),yio=o("CamembertForMaskedLM"),wio=o(" (CamemBERT model)"),Aio=l(),n_=a("li"),iK=a("strong"),Lio=o("ctrl"),Bio=o(" \u2014 "),ZR=a("a"),kio=o("CTRLLMHeadModel"),xio=o(" (CTRL model)"),Rio=l(),s_=a("li"),dK=a("strong"),Sio=o("deberta"),Pio=o(" \u2014 "),eS=a("a"),$io=o("DebertaForMaskedLM"),Iio=o(" (DeBERTa model)"),jio=l(),l_=a("li"),cK=a("strong"),Nio=o("deberta-v2"),Dio=o(" \u2014 "),oS=a("a"),qio=o("DebertaV2ForMaskedLM"),Gio=o(" (DeBERTa-v2 model)"),Oio=l(),i_=a("li"),fK=a("strong"),Xio=o("distilbert"),zio=o(" \u2014 "),rS=a("a"),Vio=o("DistilBertForMaskedLM"),Wio=o(" (DistilBERT model)"),Qio=l(),d_=a("li"),mK=a("strong"),Hio=o("electra"),Uio=o(" \u2014 "),tS=a("a"),Jio=o("ElectraForPreTraining"),Yio=o(" (ELECTRA model)"),Kio=l(),c_=a("li"),gK=a("strong"),Zio=o("flaubert"),edo=o(" \u2014 "),aS=a("a"),odo=o("FlaubertWithLMHeadModel"),rdo=o(" (FlauBERT model)"),tdo=l(),f_=a("li"),hK=a("strong"),ado=o("fnet"),ndo=o(" \u2014 "),nS=a("a"),sdo=o("FNetForPreTraining"),ldo=o(" (FNet model)"),ido=l(),m_=a("li"),pK=a("strong"),ddo=o("fsmt"),cdo=o(" \u2014 "),sS=a("a"),fdo=o("FSMTForConditionalGeneration"),mdo=o(" (FairSeq Machine-Translation model)"),gdo=l(),g_=a("li"),_K=a("strong"),hdo=o("funnel"),pdo=o(" \u2014 "),lS=a("a"),_do=o("FunnelForPreTraining"),udo=o(" (Funnel Transformer model)"),bdo=l(),h_=a("li"),uK=a("strong"),vdo=o("gpt2"),Tdo=o(" \u2014 "),iS=a("a"),Fdo=o("GPT2LMHeadModel"),Cdo=o(" (OpenAI GPT-2 model)"),Mdo=l(),p_=a("li"),bK=a("strong"),Edo=o("ibert"),ydo=o(" \u2014 "),dS=a("a"),wdo=o("IBertForMaskedLM"),Ado=o(" (I-BERT model)"),Ldo=l(),__=a("li"),vK=a("strong"),Bdo=o("layoutlm"),kdo=o(" \u2014 "),cS=a("a"),xdo=o("LayoutLMForMaskedLM"),Rdo=o(" (LayoutLM model)"),Sdo=l(),u_=a("li"),TK=a("strong"),Pdo=o("longformer"),$do=o(" \u2014 "),fS=a("a"),Ido=o("LongformerForMaskedLM"),jdo=o(" (Longformer model)"),Ndo=l(),b_=a("li"),FK=a("strong"),Ddo=o("lxmert"),qdo=o(" \u2014 "),mS=a("a"),Gdo=o("LxmertForPreTraining"),Odo=o(" (LXMERT model)"),Xdo=l(),v_=a("li"),CK=a("strong"),zdo=o("megatron-bert"),Vdo=o(" \u2014 "),gS=a("a"),Wdo=o("MegatronBertForPreTraining"),Qdo=o(" (MegatronBert model)"),Hdo=l(),T_=a("li"),MK=a("strong"),Udo=o("mobilebert"),Jdo=o(" \u2014 "),hS=a("a"),Ydo=o("MobileBertForPreTraining"),Kdo=o(" (MobileBERT model)"),Zdo=l(),F_=a("li"),EK=a("strong"),eco=o("mpnet"),oco=o(" \u2014 "),pS=a("a"),rco=o("MPNetForMaskedLM"),tco=o(" (MPNet model)"),aco=l(),C_=a("li"),yK=a("strong"),nco=o("openai-gpt"),sco=o(" \u2014 "),_S=a("a"),lco=o("OpenAIGPTLMHeadModel"),ico=o(" (OpenAI GPT model)"),dco=l(),M_=a("li"),wK=a("strong"),cco=o("retribert"),fco=o(" \u2014 "),uS=a("a"),mco=o("RetriBertModel"),gco=o(" (RetriBERT model)"),hco=l(),E_=a("li"),AK=a("strong"),pco=o("roberta"),_co=o(" \u2014 "),bS=a("a"),uco=o("RobertaForMaskedLM"),bco=o(" (RoBERTa model)"),vco=l(),y_=a("li"),LK=a("strong"),Tco=o("squeezebert"),Fco=o(" \u2014 "),vS=a("a"),Cco=o("SqueezeBertForMaskedLM"),Mco=o(" (SqueezeBERT model)"),Eco=l(),w_=a("li"),BK=a("strong"),yco=o("t5"),wco=o(" \u2014 "),TS=a("a"),Aco=o("T5ForConditionalGeneration"),Lco=o(" (T5 model)"),Bco=l(),A_=a("li"),kK=a("strong"),kco=o("tapas"),xco=o(" \u2014 "),FS=a("a"),Rco=o("TapasForMaskedLM"),Sco=o(" (TAPAS model)"),Pco=l(),L_=a("li"),xK=a("strong"),$co=o("transfo-xl"),Ico=o(" \u2014 "),CS=a("a"),jco=o("TransfoXLLMHeadModel"),Nco=o(" (Transformer-XL model)"),Dco=l(),B_=a("li"),RK=a("strong"),qco=o("unispeech"),Gco=o(" \u2014 "),MS=a("a"),Oco=o("UniSpeechForPreTraining"),Xco=o(" (UniSpeech model)"),zco=l(),k_=a("li"),SK=a("strong"),Vco=o("unispeech-sat"),Wco=o(" \u2014 "),ES=a("a"),Qco=o("UniSpeechSatForPreTraining"),Hco=o(" (UniSpeechSat model)"),Uco=l(),x_=a("li"),PK=a("strong"),Jco=o("visual_bert"),Yco=o(" \u2014 "),yS=a("a"),Kco=o("VisualBertForPreTraining"),Zco=o(" (VisualBert model)"),efo=l(),R_=a("li"),$K=a("strong"),ofo=o("vit_mae"),rfo=o(" \u2014 "),wS=a("a"),tfo=o("ViTMAEForPreTraining"),afo=o(" (ViTMAE model)"),nfo=l(),S_=a("li"),IK=a("strong"),sfo=o("wav2vec2"),lfo=o(" \u2014 "),AS=a("a"),ifo=o("Wav2Vec2ForPreTraining"),dfo=o(" (Wav2Vec2 model)"),cfo=l(),P_=a("li"),jK=a("strong"),ffo=o("xlm"),mfo=o(" \u2014 "),LS=a("a"),gfo=o("XLMWithLMHeadModel"),hfo=o(" (XLM model)"),pfo=l(),$_=a("li"),NK=a("strong"),_fo=o("xlm-roberta"),ufo=o(" \u2014 "),BS=a("a"),bfo=o("XLMRobertaForMaskedLM"),vfo=o(" (XLM-RoBERTa model)"),Tfo=l(),I_=a("li"),DK=a("strong"),Ffo=o("xlm-roberta-xl"),Cfo=o(" \u2014 "),kS=a("a"),Mfo=o("XLMRobertaXLForMaskedLM"),Efo=o(" (XLM-RoBERTa-XL model)"),yfo=l(),j_=a("li"),qK=a("strong"),wfo=o("xlnet"),Afo=o(" \u2014 "),xS=a("a"),Lfo=o("XLNetLMHeadModel"),Bfo=o(" (XLNet model)"),kfo=l(),N_=a("p"),xfo=o("The model is set in evaluation mode by default using "),GK=a("code"),Rfo=o("model.eval()"),Sfo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),OK=a("code"),Pfo=o("model.train()"),$fo=l(),XK=a("p"),Ifo=o("Examples:"),jfo=l(),f(UM.$$.fragment),yLe=l(),Qi=a("h2"),D_=a("a"),zK=a("span"),f(JM.$$.fragment),Nfo=l(),VK=a("span"),Dfo=o("AutoModelForCausalLM"),wLe=l(),Qo=a("div"),f(YM.$$.fragment),qfo=l(),Hi=a("p"),Gfo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a causal language modeling head) when created
with the `),WK=a("code"),Ofo=o("from_pretrained()"),Xfo=o("class method or the "),QK=a("code"),zfo=o("from_config()"),Vfo=o(`class
method.`),Wfo=l(),KM=a("p"),Qfo=o("This class cannot be instantiated directly using "),HK=a("code"),Hfo=o("__init__()"),Ufo=o(" (throws an error)."),Jfo=l(),qr=a("div"),f(ZM.$$.fragment),Yfo=l(),UK=a("p"),Kfo=o("Instantiates one of the model classes of the library (with a causal language modeling head) from a configuration."),Zfo=l(),Ui=a("p"),emo=o(`Note:
Loading a model from its configuration file does `),JK=a("strong"),omo=o("not"),rmo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),YK=a("code"),tmo=o("from_pretrained()"),amo=o("to load the model weights."),nmo=l(),KK=a("p"),smo=o("Examples:"),lmo=l(),f(eE.$$.fragment),imo=l(),Re=a("div"),f(oE.$$.fragment),dmo=l(),ZK=a("p"),cmo=o("Instantiate one of the model classes of the library (with a causal language modeling head) from a pretrained model."),fmo=l(),Ga=a("p"),mmo=o("The model class to instantiate is selected based on the "),eZ=a("code"),gmo=o("model_type"),hmo=o(` property of the config object (either
passed as an argument or loaded from `),oZ=a("code"),pmo=o("pretrained_model_name_or_path"),_mo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),rZ=a("code"),umo=o("pretrained_model_name_or_path"),bmo=o(":"),vmo=l(),$=a("ul"),q_=a("li"),tZ=a("strong"),Tmo=o("bart"),Fmo=o(" \u2014 "),RS=a("a"),Cmo=o("BartForCausalLM"),Mmo=o(" (BART model)"),Emo=l(),G_=a("li"),aZ=a("strong"),ymo=o("bert"),wmo=o(" \u2014 "),SS=a("a"),Amo=o("BertLMHeadModel"),Lmo=o(" (BERT model)"),Bmo=l(),O_=a("li"),nZ=a("strong"),kmo=o("bert-generation"),xmo=o(" \u2014 "),PS=a("a"),Rmo=o("BertGenerationDecoder"),Smo=o(" (Bert Generation model)"),Pmo=l(),X_=a("li"),sZ=a("strong"),$mo=o("big_bird"),Imo=o(" \u2014 "),$S=a("a"),jmo=o("BigBirdForCausalLM"),Nmo=o(" (BigBird model)"),Dmo=l(),z_=a("li"),lZ=a("strong"),qmo=o("bigbird_pegasus"),Gmo=o(" \u2014 "),IS=a("a"),Omo=o("BigBirdPegasusForCausalLM"),Xmo=o(" (BigBirdPegasus model)"),zmo=l(),V_=a("li"),iZ=a("strong"),Vmo=o("blenderbot"),Wmo=o(" \u2014 "),jS=a("a"),Qmo=o("BlenderbotForCausalLM"),Hmo=o(" (Blenderbot model)"),Umo=l(),W_=a("li"),dZ=a("strong"),Jmo=o("blenderbot-small"),Ymo=o(" \u2014 "),NS=a("a"),Kmo=o("BlenderbotSmallForCausalLM"),Zmo=o(" (BlenderbotSmall model)"),ego=l(),Q_=a("li"),cZ=a("strong"),ogo=o("camembert"),rgo=o(" \u2014 "),DS=a("a"),tgo=o("CamembertForCausalLM"),ago=o(" (CamemBERT model)"),ngo=l(),H_=a("li"),fZ=a("strong"),sgo=o("ctrl"),lgo=o(" \u2014 "),qS=a("a"),igo=o("CTRLLMHeadModel"),dgo=o(" (CTRL model)"),cgo=l(),U_=a("li"),mZ=a("strong"),fgo=o("electra"),mgo=o(" \u2014 "),GS=a("a"),ggo=o("ElectraForCausalLM"),hgo=o(" (ELECTRA model)"),pgo=l(),J_=a("li"),gZ=a("strong"),_go=o("gpt2"),ugo=o(" \u2014 "),OS=a("a"),bgo=o("GPT2LMHeadModel"),vgo=o(" (OpenAI GPT-2 model)"),Tgo=l(),Y_=a("li"),hZ=a("strong"),Fgo=o("gpt_neo"),Cgo=o(" \u2014 "),XS=a("a"),Mgo=o("GPTNeoForCausalLM"),Ego=o(" (GPT Neo model)"),ygo=l(),K_=a("li"),pZ=a("strong"),wgo=o("gptj"),Ago=o(" \u2014 "),zS=a("a"),Lgo=o("GPTJForCausalLM"),Bgo=o(" (GPT-J model)"),kgo=l(),Z_=a("li"),_Z=a("strong"),xgo=o("marian"),Rgo=o(" \u2014 "),VS=a("a"),Sgo=o("MarianForCausalLM"),Pgo=o(" (Marian model)"),$go=l(),eu=a("li"),uZ=a("strong"),Igo=o("mbart"),jgo=o(" \u2014 "),WS=a("a"),Ngo=o("MBartForCausalLM"),Dgo=o(" (mBART model)"),qgo=l(),ou=a("li"),bZ=a("strong"),Ggo=o("megatron-bert"),Ogo=o(" \u2014 "),QS=a("a"),Xgo=o("MegatronBertForCausalLM"),zgo=o(" (MegatronBert model)"),Vgo=l(),ru=a("li"),vZ=a("strong"),Wgo=o("openai-gpt"),Qgo=o(" \u2014 "),HS=a("a"),Hgo=o("OpenAIGPTLMHeadModel"),Ugo=o(" (OpenAI GPT model)"),Jgo=l(),tu=a("li"),TZ=a("strong"),Ygo=o("pegasus"),Kgo=o(" \u2014 "),US=a("a"),Zgo=o("PegasusForCausalLM"),eho=o(" (Pegasus model)"),oho=l(),au=a("li"),FZ=a("strong"),rho=o("plbart"),tho=o(" \u2014 "),JS=a("a"),aho=o("PLBartForCausalLM"),nho=o(" (PLBart model)"),sho=l(),nu=a("li"),CZ=a("strong"),lho=o("prophetnet"),iho=o(" \u2014 "),YS=a("a"),dho=o("ProphetNetForCausalLM"),cho=o(" (ProphetNet model)"),fho=l(),su=a("li"),MZ=a("strong"),mho=o("qdqbert"),gho=o(" \u2014 "),KS=a("a"),hho=o("QDQBertLMHeadModel"),pho=o(" (QDQBert model)"),_ho=l(),lu=a("li"),EZ=a("strong"),uho=o("reformer"),bho=o(" \u2014 "),ZS=a("a"),vho=o("ReformerModelWithLMHead"),Tho=o(" (Reformer model)"),Fho=l(),iu=a("li"),yZ=a("strong"),Cho=o("rembert"),Mho=o(" \u2014 "),eP=a("a"),Eho=o("RemBertForCausalLM"),yho=o(" (RemBERT model)"),who=l(),du=a("li"),wZ=a("strong"),Aho=o("roberta"),Lho=o(" \u2014 "),oP=a("a"),Bho=o("RobertaForCausalLM"),kho=o(" (RoBERTa model)"),xho=l(),cu=a("li"),AZ=a("strong"),Rho=o("roformer"),Sho=o(" \u2014 "),rP=a("a"),Pho=o("RoFormerForCausalLM"),$ho=o(" (RoFormer model)"),Iho=l(),fu=a("li"),LZ=a("strong"),jho=o("speech_to_text_2"),Nho=o(" \u2014 "),tP=a("a"),Dho=o("Speech2Text2ForCausalLM"),qho=o(" (Speech2Text2 model)"),Gho=l(),mu=a("li"),BZ=a("strong"),Oho=o("transfo-xl"),Xho=o(" \u2014 "),aP=a("a"),zho=o("TransfoXLLMHeadModel"),Vho=o(" (Transformer-XL model)"),Who=l(),gu=a("li"),kZ=a("strong"),Qho=o("trocr"),Hho=o(" \u2014 "),nP=a("a"),Uho=o("TrOCRForCausalLM"),Jho=o(" (TrOCR model)"),Yho=l(),hu=a("li"),xZ=a("strong"),Kho=o("xglm"),Zho=o(" \u2014 "),sP=a("a"),epo=o("XGLMForCausalLM"),opo=o(" (XGLM model)"),rpo=l(),pu=a("li"),RZ=a("strong"),tpo=o("xlm"),apo=o(" \u2014 "),lP=a("a"),npo=o("XLMWithLMHeadModel"),spo=o(" (XLM model)"),lpo=l(),_u=a("li"),SZ=a("strong"),ipo=o("xlm-prophetnet"),dpo=o(" \u2014 "),iP=a("a"),cpo=o("XLMProphetNetForCausalLM"),fpo=o(" (XLMProphetNet model)"),mpo=l(),uu=a("li"),PZ=a("strong"),gpo=o("xlm-roberta"),hpo=o(" \u2014 "),dP=a("a"),ppo=o("XLMRobertaForCausalLM"),_po=o(" (XLM-RoBERTa model)"),upo=l(),bu=a("li"),$Z=a("strong"),bpo=o("xlm-roberta-xl"),vpo=o(" \u2014 "),cP=a("a"),Tpo=o("XLMRobertaXLForCausalLM"),Fpo=o(" (XLM-RoBERTa-XL model)"),Cpo=l(),vu=a("li"),IZ=a("strong"),Mpo=o("xlnet"),Epo=o(" \u2014 "),fP=a("a"),ypo=o("XLNetLMHeadModel"),wpo=o(" (XLNet model)"),Apo=l(),Tu=a("p"),Lpo=o("The model is set in evaluation mode by default using "),jZ=a("code"),Bpo=o("model.eval()"),kpo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),NZ=a("code"),xpo=o("model.train()"),Rpo=l(),DZ=a("p"),Spo=o("Examples:"),Ppo=l(),f(rE.$$.fragment),ALe=l(),Ji=a("h2"),Fu=a("a"),qZ=a("span"),f(tE.$$.fragment),$po=l(),GZ=a("span"),Ipo=o("AutoModelForMaskedLM"),LLe=l(),Ho=a("div"),f(aE.$$.fragment),jpo=l(),Yi=a("p"),Npo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a masked language modeling head) when created
with the `),OZ=a("code"),Dpo=o("from_pretrained()"),qpo=o("class method or the "),XZ=a("code"),Gpo=o("from_config()"),Opo=o(`class
method.`),Xpo=l(),nE=a("p"),zpo=o("This class cannot be instantiated directly using "),zZ=a("code"),Vpo=o("__init__()"),Wpo=o(" (throws an error)."),Qpo=l(),Gr=a("div"),f(sE.$$.fragment),Hpo=l(),VZ=a("p"),Upo=o("Instantiates one of the model classes of the library (with a masked language modeling head) from a configuration."),Jpo=l(),Ki=a("p"),Ypo=o(`Note:
Loading a model from its configuration file does `),WZ=a("strong"),Kpo=o("not"),Zpo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),QZ=a("code"),e_o=o("from_pretrained()"),o_o=o("to load the model weights."),r_o=l(),HZ=a("p"),t_o=o("Examples:"),a_o=l(),f(lE.$$.fragment),n_o=l(),Se=a("div"),f(iE.$$.fragment),s_o=l(),UZ=a("p"),l_o=o("Instantiate one of the model classes of the library (with a masked language modeling head) from a pretrained model."),i_o=l(),Oa=a("p"),d_o=o("The model class to instantiate is selected based on the "),JZ=a("code"),c_o=o("model_type"),f_o=o(` property of the config object (either
passed as an argument or loaded from `),YZ=a("code"),m_o=o("pretrained_model_name_or_path"),g_o=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),KZ=a("code"),h_o=o("pretrained_model_name_or_path"),p_o=o(":"),__o=l(),I=a("ul"),Cu=a("li"),ZZ=a("strong"),u_o=o("albert"),b_o=o(" \u2014 "),mP=a("a"),v_o=o("AlbertForMaskedLM"),T_o=o(" (ALBERT model)"),F_o=l(),Mu=a("li"),eee=a("strong"),C_o=o("bart"),M_o=o(" \u2014 "),gP=a("a"),E_o=o("BartForConditionalGeneration"),y_o=o(" (BART model)"),w_o=l(),Eu=a("li"),oee=a("strong"),A_o=o("bert"),L_o=o(" \u2014 "),hP=a("a"),B_o=o("BertForMaskedLM"),k_o=o(" (BERT model)"),x_o=l(),yu=a("li"),ree=a("strong"),R_o=o("big_bird"),S_o=o(" \u2014 "),pP=a("a"),P_o=o("BigBirdForMaskedLM"),$_o=o(" (BigBird model)"),I_o=l(),wu=a("li"),tee=a("strong"),j_o=o("camembert"),N_o=o(" \u2014 "),_P=a("a"),D_o=o("CamembertForMaskedLM"),q_o=o(" (CamemBERT model)"),G_o=l(),Au=a("li"),aee=a("strong"),O_o=o("convbert"),X_o=o(" \u2014 "),uP=a("a"),z_o=o("ConvBertForMaskedLM"),V_o=o(" (ConvBERT model)"),W_o=l(),Lu=a("li"),nee=a("strong"),Q_o=o("deberta"),H_o=o(" \u2014 "),bP=a("a"),U_o=o("DebertaForMaskedLM"),J_o=o(" (DeBERTa model)"),Y_o=l(),Bu=a("li"),see=a("strong"),K_o=o("deberta-v2"),Z_o=o(" \u2014 "),vP=a("a"),euo=o("DebertaV2ForMaskedLM"),ouo=o(" (DeBERTa-v2 model)"),ruo=l(),ku=a("li"),lee=a("strong"),tuo=o("distilbert"),auo=o(" \u2014 "),TP=a("a"),nuo=o("DistilBertForMaskedLM"),suo=o(" (DistilBERT model)"),luo=l(),xu=a("li"),iee=a("strong"),iuo=o("electra"),duo=o(" \u2014 "),FP=a("a"),cuo=o("ElectraForMaskedLM"),fuo=o(" (ELECTRA model)"),muo=l(),Ru=a("li"),dee=a("strong"),guo=o("flaubert"),huo=o(" \u2014 "),CP=a("a"),puo=o("FlaubertWithLMHeadModel"),_uo=o(" (FlauBERT model)"),uuo=l(),Su=a("li"),cee=a("strong"),buo=o("fnet"),vuo=o(" \u2014 "),MP=a("a"),Tuo=o("FNetForMaskedLM"),Fuo=o(" (FNet model)"),Cuo=l(),Pu=a("li"),fee=a("strong"),Muo=o("funnel"),Euo=o(" \u2014 "),EP=a("a"),yuo=o("FunnelForMaskedLM"),wuo=o(" (Funnel Transformer model)"),Auo=l(),$u=a("li"),mee=a("strong"),Luo=o("ibert"),Buo=o(" \u2014 "),yP=a("a"),kuo=o("IBertForMaskedLM"),xuo=o(" (I-BERT model)"),Ruo=l(),Iu=a("li"),gee=a("strong"),Suo=o("layoutlm"),Puo=o(" \u2014 "),wP=a("a"),$uo=o("LayoutLMForMaskedLM"),Iuo=o(" (LayoutLM model)"),juo=l(),ju=a("li"),hee=a("strong"),Nuo=o("longformer"),Duo=o(" \u2014 "),AP=a("a"),quo=o("LongformerForMaskedLM"),Guo=o(" (Longformer model)"),Ouo=l(),Nu=a("li"),pee=a("strong"),Xuo=o("mbart"),zuo=o(" \u2014 "),LP=a("a"),Vuo=o("MBartForConditionalGeneration"),Wuo=o(" (mBART model)"),Quo=l(),Du=a("li"),_ee=a("strong"),Huo=o("megatron-bert"),Uuo=o(" \u2014 "),BP=a("a"),Juo=o("MegatronBertForMaskedLM"),Yuo=o(" (MegatronBert model)"),Kuo=l(),qu=a("li"),uee=a("strong"),Zuo=o("mobilebert"),e2o=o(" \u2014 "),kP=a("a"),o2o=o("MobileBertForMaskedLM"),r2o=o(" (MobileBERT model)"),t2o=l(),Gu=a("li"),bee=a("strong"),a2o=o("mpnet"),n2o=o(" \u2014 "),xP=a("a"),s2o=o("MPNetForMaskedLM"),l2o=o(" (MPNet model)"),i2o=l(),Ou=a("li"),vee=a("strong"),d2o=o("nystromformer"),c2o=o(" \u2014 "),RP=a("a"),f2o=o("NystromformerForMaskedLM"),m2o=o(" (Nystromformer model)"),g2o=l(),Xu=a("li"),Tee=a("strong"),h2o=o("perceiver"),p2o=o(" \u2014 "),SP=a("a"),_2o=o("PerceiverForMaskedLM"),u2o=o(" (Perceiver model)"),b2o=l(),zu=a("li"),Fee=a("strong"),v2o=o("qdqbert"),T2o=o(" \u2014 "),PP=a("a"),F2o=o("QDQBertForMaskedLM"),C2o=o(" (QDQBert model)"),M2o=l(),Vu=a("li"),Cee=a("strong"),E2o=o("reformer"),y2o=o(" \u2014 "),$P=a("a"),w2o=o("ReformerForMaskedLM"),A2o=o(" (Reformer model)"),L2o=l(),Wu=a("li"),Mee=a("strong"),B2o=o("rembert"),k2o=o(" \u2014 "),IP=a("a"),x2o=o("RemBertForMaskedLM"),R2o=o(" (RemBERT model)"),S2o=l(),Qu=a("li"),Eee=a("strong"),P2o=o("roberta"),$2o=o(" \u2014 "),jP=a("a"),I2o=o("RobertaForMaskedLM"),j2o=o(" (RoBERTa model)"),N2o=l(),Hu=a("li"),yee=a("strong"),D2o=o("roformer"),q2o=o(" \u2014 "),NP=a("a"),G2o=o("RoFormerForMaskedLM"),O2o=o(" (RoFormer model)"),X2o=l(),Uu=a("li"),wee=a("strong"),z2o=o("squeezebert"),V2o=o(" \u2014 "),DP=a("a"),W2o=o("SqueezeBertForMaskedLM"),Q2o=o(" (SqueezeBERT model)"),H2o=l(),Ju=a("li"),Aee=a("strong"),U2o=o("tapas"),J2o=o(" \u2014 "),qP=a("a"),Y2o=o("TapasForMaskedLM"),K2o=o(" (TAPAS model)"),Z2o=l(),Yu=a("li"),Lee=a("strong"),e1o=o("wav2vec2"),o1o=o(" \u2014 "),Bee=a("code"),r1o=o("Wav2Vec2ForMaskedLM"),t1o=o("(Wav2Vec2 model)"),a1o=l(),Ku=a("li"),kee=a("strong"),n1o=o("xlm"),s1o=o(" \u2014 "),GP=a("a"),l1o=o("XLMWithLMHeadModel"),i1o=o(" (XLM model)"),d1o=l(),Zu=a("li"),xee=a("strong"),c1o=o("xlm-roberta"),f1o=o(" \u2014 "),OP=a("a"),m1o=o("XLMRobertaForMaskedLM"),g1o=o(" (XLM-RoBERTa model)"),h1o=l(),e2=a("li"),Ree=a("strong"),p1o=o("xlm-roberta-xl"),_1o=o(" \u2014 "),XP=a("a"),u1o=o("XLMRobertaXLForMaskedLM"),b1o=o(" (XLM-RoBERTa-XL model)"),v1o=l(),o2=a("li"),See=a("strong"),T1o=o("yoso"),F1o=o(" \u2014 "),zP=a("a"),C1o=o("YosoForMaskedLM"),M1o=o(" (YOSO model)"),E1o=l(),r2=a("p"),y1o=o("The model is set in evaluation mode by default using "),Pee=a("code"),w1o=o("model.eval()"),A1o=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),$ee=a("code"),L1o=o("model.train()"),B1o=l(),Iee=a("p"),k1o=o("Examples:"),x1o=l(),f(dE.$$.fragment),BLe=l(),Zi=a("h2"),t2=a("a"),jee=a("span"),f(cE.$$.fragment),R1o=l(),Nee=a("span"),S1o=o("AutoModelForSeq2SeqLM"),kLe=l(),Uo=a("div"),f(fE.$$.fragment),P1o=l(),ed=a("p"),$1o=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence language modeling head) when created
with the `),Dee=a("code"),I1o=o("from_pretrained()"),j1o=o("class method or the "),qee=a("code"),N1o=o("from_config()"),D1o=o(`class
method.`),q1o=l(),mE=a("p"),G1o=o("This class cannot be instantiated directly using "),Gee=a("code"),O1o=o("__init__()"),X1o=o(" (throws an error)."),z1o=l(),Or=a("div"),f(gE.$$.fragment),V1o=l(),Oee=a("p"),W1o=o("Instantiates one of the model classes of the library (with a sequence-to-sequence language modeling head) from a configuration."),Q1o=l(),od=a("p"),H1o=o(`Note:
Loading a model from its configuration file does `),Xee=a("strong"),U1o=o("not"),J1o=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),zee=a("code"),Y1o=o("from_pretrained()"),K1o=o("to load the model weights."),Z1o=l(),Vee=a("p"),ebo=o("Examples:"),obo=l(),f(hE.$$.fragment),rbo=l(),Pe=a("div"),f(pE.$$.fragment),tbo=l(),Wee=a("p"),abo=o("Instantiate one of the model classes of the library (with a sequence-to-sequence language modeling head) from a pretrained model."),nbo=l(),Xa=a("p"),sbo=o("The model class to instantiate is selected based on the "),Qee=a("code"),lbo=o("model_type"),ibo=o(` property of the config object (either
passed as an argument or loaded from `),Hee=a("code"),dbo=o("pretrained_model_name_or_path"),cbo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Uee=a("code"),fbo=o("pretrained_model_name_or_path"),mbo=o(":"),gbo=l(),ae=a("ul"),a2=a("li"),Jee=a("strong"),hbo=o("bart"),pbo=o(" \u2014 "),VP=a("a"),_bo=o("BartForConditionalGeneration"),ubo=o(" (BART model)"),bbo=l(),n2=a("li"),Yee=a("strong"),vbo=o("bigbird_pegasus"),Tbo=o(" \u2014 "),WP=a("a"),Fbo=o("BigBirdPegasusForConditionalGeneration"),Cbo=o(" (BigBirdPegasus model)"),Mbo=l(),s2=a("li"),Kee=a("strong"),Ebo=o("blenderbot"),ybo=o(" \u2014 "),QP=a("a"),wbo=o("BlenderbotForConditionalGeneration"),Abo=o(" (Blenderbot model)"),Lbo=l(),l2=a("li"),Zee=a("strong"),Bbo=o("blenderbot-small"),kbo=o(" \u2014 "),HP=a("a"),xbo=o("BlenderbotSmallForConditionalGeneration"),Rbo=o(" (BlenderbotSmall model)"),Sbo=l(),i2=a("li"),eoe=a("strong"),Pbo=o("encoder-decoder"),$bo=o(" \u2014 "),UP=a("a"),Ibo=o("EncoderDecoderModel"),jbo=o(" (Encoder decoder model)"),Nbo=l(),d2=a("li"),ooe=a("strong"),Dbo=o("fsmt"),qbo=o(" \u2014 "),JP=a("a"),Gbo=o("FSMTForConditionalGeneration"),Obo=o(" (FairSeq Machine-Translation model)"),Xbo=l(),c2=a("li"),roe=a("strong"),zbo=o("led"),Vbo=o(" \u2014 "),YP=a("a"),Wbo=o("LEDForConditionalGeneration"),Qbo=o(" (LED model)"),Hbo=l(),f2=a("li"),toe=a("strong"),Ubo=o("m2m_100"),Jbo=o(" \u2014 "),KP=a("a"),Ybo=o("M2M100ForConditionalGeneration"),Kbo=o(" (M2M100 model)"),Zbo=l(),m2=a("li"),aoe=a("strong"),e5o=o("marian"),o5o=o(" \u2014 "),ZP=a("a"),r5o=o("MarianMTModel"),t5o=o(" (Marian model)"),a5o=l(),g2=a("li"),noe=a("strong"),n5o=o("mbart"),s5o=o(" \u2014 "),e$=a("a"),l5o=o("MBartForConditionalGeneration"),i5o=o(" (mBART model)"),d5o=l(),h2=a("li"),soe=a("strong"),c5o=o("mt5"),f5o=o(" \u2014 "),o$=a("a"),m5o=o("MT5ForConditionalGeneration"),g5o=o(" (mT5 model)"),h5o=l(),p2=a("li"),loe=a("strong"),p5o=o("pegasus"),_5o=o(" \u2014 "),r$=a("a"),u5o=o("PegasusForConditionalGeneration"),b5o=o(" (Pegasus model)"),v5o=l(),_2=a("li"),ioe=a("strong"),T5o=o("plbart"),F5o=o(" \u2014 "),t$=a("a"),C5o=o("PLBartForConditionalGeneration"),M5o=o(" (PLBart model)"),E5o=l(),u2=a("li"),doe=a("strong"),y5o=o("prophetnet"),w5o=o(" \u2014 "),a$=a("a"),A5o=o("ProphetNetForConditionalGeneration"),L5o=o(" (ProphetNet model)"),B5o=l(),b2=a("li"),coe=a("strong"),k5o=o("t5"),x5o=o(" \u2014 "),n$=a("a"),R5o=o("T5ForConditionalGeneration"),S5o=o(" (T5 model)"),P5o=l(),v2=a("li"),foe=a("strong"),$5o=o("xlm-prophetnet"),I5o=o(" \u2014 "),s$=a("a"),j5o=o("XLMProphetNetForConditionalGeneration"),N5o=o(" (XLMProphetNet model)"),D5o=l(),T2=a("p"),q5o=o("The model is set in evaluation mode by default using "),moe=a("code"),G5o=o("model.eval()"),O5o=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),goe=a("code"),X5o=o("model.train()"),z5o=l(),hoe=a("p"),V5o=o("Examples:"),W5o=l(),f(_E.$$.fragment),xLe=l(),rd=a("h2"),F2=a("a"),poe=a("span"),f(uE.$$.fragment),Q5o=l(),_oe=a("span"),H5o=o("AutoModelForSequenceClassification"),RLe=l(),Jo=a("div"),f(bE.$$.fragment),U5o=l(),td=a("p"),J5o=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence classification head) when created
with the `),uoe=a("code"),Y5o=o("from_pretrained()"),K5o=o("class method or the "),boe=a("code"),Z5o=o("from_config()"),evo=o(`class
method.`),ovo=l(),vE=a("p"),rvo=o("This class cannot be instantiated directly using "),voe=a("code"),tvo=o("__init__()"),avo=o(" (throws an error)."),nvo=l(),Xr=a("div"),f(TE.$$.fragment),svo=l(),Toe=a("p"),lvo=o("Instantiates one of the model classes of the library (with a sequence classification head) from a configuration."),ivo=l(),ad=a("p"),dvo=o(`Note:
Loading a model from its configuration file does `),Foe=a("strong"),cvo=o("not"),fvo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Coe=a("code"),mvo=o("from_pretrained()"),gvo=o("to load the model weights."),hvo=l(),Moe=a("p"),pvo=o("Examples:"),_vo=l(),f(FE.$$.fragment),uvo=l(),$e=a("div"),f(CE.$$.fragment),bvo=l(),Eoe=a("p"),vvo=o("Instantiate one of the model classes of the library (with a sequence classification head) from a pretrained model."),Tvo=l(),za=a("p"),Fvo=o("The model class to instantiate is selected based on the "),yoe=a("code"),Cvo=o("model_type"),Mvo=o(` property of the config object (either
passed as an argument or loaded from `),woe=a("code"),Evo=o("pretrained_model_name_or_path"),yvo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Aoe=a("code"),wvo=o("pretrained_model_name_or_path"),Avo=o(":"),Lvo=l(),A=a("ul"),C2=a("li"),Loe=a("strong"),Bvo=o("albert"),kvo=o(" \u2014 "),l$=a("a"),xvo=o("AlbertForSequenceClassification"),Rvo=o(" (ALBERT model)"),Svo=l(),M2=a("li"),Boe=a("strong"),Pvo=o("bart"),$vo=o(" \u2014 "),i$=a("a"),Ivo=o("BartForSequenceClassification"),jvo=o(" (BART model)"),Nvo=l(),E2=a("li"),koe=a("strong"),Dvo=o("bert"),qvo=o(" \u2014 "),d$=a("a"),Gvo=o("BertForSequenceClassification"),Ovo=o(" (BERT model)"),Xvo=l(),y2=a("li"),xoe=a("strong"),zvo=o("big_bird"),Vvo=o(" \u2014 "),c$=a("a"),Wvo=o("BigBirdForSequenceClassification"),Qvo=o(" (BigBird model)"),Hvo=l(),w2=a("li"),Roe=a("strong"),Uvo=o("bigbird_pegasus"),Jvo=o(" \u2014 "),f$=a("a"),Yvo=o("BigBirdPegasusForSequenceClassification"),Kvo=o(" (BigBirdPegasus model)"),Zvo=l(),A2=a("li"),Soe=a("strong"),eTo=o("camembert"),oTo=o(" \u2014 "),m$=a("a"),rTo=o("CamembertForSequenceClassification"),tTo=o(" (CamemBERT model)"),aTo=l(),L2=a("li"),Poe=a("strong"),nTo=o("canine"),sTo=o(" \u2014 "),g$=a("a"),lTo=o("CanineForSequenceClassification"),iTo=o(" (Canine model)"),dTo=l(),B2=a("li"),$oe=a("strong"),cTo=o("convbert"),fTo=o(" \u2014 "),h$=a("a"),mTo=o("ConvBertForSequenceClassification"),gTo=o(" (ConvBERT model)"),hTo=l(),k2=a("li"),Ioe=a("strong"),pTo=o("ctrl"),_To=o(" \u2014 "),p$=a("a"),uTo=o("CTRLForSequenceClassification"),bTo=o(" (CTRL model)"),vTo=l(),x2=a("li"),joe=a("strong"),TTo=o("deberta"),FTo=o(" \u2014 "),_$=a("a"),CTo=o("DebertaForSequenceClassification"),MTo=o(" (DeBERTa model)"),ETo=l(),R2=a("li"),Noe=a("strong"),yTo=o("deberta-v2"),wTo=o(" \u2014 "),u$=a("a"),ATo=o("DebertaV2ForSequenceClassification"),LTo=o(" (DeBERTa-v2 model)"),BTo=l(),S2=a("li"),Doe=a("strong"),kTo=o("distilbert"),xTo=o(" \u2014 "),b$=a("a"),RTo=o("DistilBertForSequenceClassification"),STo=o(" (DistilBERT model)"),PTo=l(),P2=a("li"),qoe=a("strong"),$To=o("electra"),ITo=o(" \u2014 "),v$=a("a"),jTo=o("ElectraForSequenceClassification"),NTo=o(" (ELECTRA model)"),DTo=l(),$2=a("li"),Goe=a("strong"),qTo=o("flaubert"),GTo=o(" \u2014 "),T$=a("a"),OTo=o("FlaubertForSequenceClassification"),XTo=o(" (FlauBERT model)"),zTo=l(),I2=a("li"),Ooe=a("strong"),VTo=o("fnet"),WTo=o(" \u2014 "),F$=a("a"),QTo=o("FNetForSequenceClassification"),HTo=o(" (FNet model)"),UTo=l(),j2=a("li"),Xoe=a("strong"),JTo=o("funnel"),YTo=o(" \u2014 "),C$=a("a"),KTo=o("FunnelForSequenceClassification"),ZTo=o(" (Funnel Transformer model)"),e7o=l(),N2=a("li"),zoe=a("strong"),o7o=o("gpt2"),r7o=o(" \u2014 "),M$=a("a"),t7o=o("GPT2ForSequenceClassification"),a7o=o(" (OpenAI GPT-2 model)"),n7o=l(),D2=a("li"),Voe=a("strong"),s7o=o("gpt_neo"),l7o=o(" \u2014 "),E$=a("a"),i7o=o("GPTNeoForSequenceClassification"),d7o=o(" (GPT Neo model)"),c7o=l(),q2=a("li"),Woe=a("strong"),f7o=o("gptj"),m7o=o(" \u2014 "),y$=a("a"),g7o=o("GPTJForSequenceClassification"),h7o=o(" (GPT-J model)"),p7o=l(),G2=a("li"),Qoe=a("strong"),_7o=o("ibert"),u7o=o(" \u2014 "),w$=a("a"),b7o=o("IBertForSequenceClassification"),v7o=o(" (I-BERT model)"),T7o=l(),O2=a("li"),Hoe=a("strong"),F7o=o("layoutlm"),C7o=o(" \u2014 "),A$=a("a"),M7o=o("LayoutLMForSequenceClassification"),E7o=o(" (LayoutLM model)"),y7o=l(),X2=a("li"),Uoe=a("strong"),w7o=o("layoutlmv2"),A7o=o(" \u2014 "),L$=a("a"),L7o=o("LayoutLMv2ForSequenceClassification"),B7o=o(" (LayoutLMv2 model)"),k7o=l(),z2=a("li"),Joe=a("strong"),x7o=o("led"),R7o=o(" \u2014 "),B$=a("a"),S7o=o("LEDForSequenceClassification"),P7o=o(" (LED model)"),$7o=l(),V2=a("li"),Yoe=a("strong"),I7o=o("longformer"),j7o=o(" \u2014 "),k$=a("a"),N7o=o("LongformerForSequenceClassification"),D7o=o(" (Longformer model)"),q7o=l(),W2=a("li"),Koe=a("strong"),G7o=o("mbart"),O7o=o(" \u2014 "),x$=a("a"),X7o=o("MBartForSequenceClassification"),z7o=o(" (mBART model)"),V7o=l(),Q2=a("li"),Zoe=a("strong"),W7o=o("megatron-bert"),Q7o=o(" \u2014 "),R$=a("a"),H7o=o("MegatronBertForSequenceClassification"),U7o=o(" (MegatronBert model)"),J7o=l(),H2=a("li"),ere=a("strong"),Y7o=o("mobilebert"),K7o=o(" \u2014 "),S$=a("a"),Z7o=o("MobileBertForSequenceClassification"),eFo=o(" (MobileBERT model)"),oFo=l(),U2=a("li"),ore=a("strong"),rFo=o("mpnet"),tFo=o(" \u2014 "),P$=a("a"),aFo=o("MPNetForSequenceClassification"),nFo=o(" (MPNet model)"),sFo=l(),J2=a("li"),rre=a("strong"),lFo=o("nystromformer"),iFo=o(" \u2014 "),$$=a("a"),dFo=o("NystromformerForSequenceClassification"),cFo=o(" (Nystromformer model)"),fFo=l(),Y2=a("li"),tre=a("strong"),mFo=o("openai-gpt"),gFo=o(" \u2014 "),I$=a("a"),hFo=o("OpenAIGPTForSequenceClassification"),pFo=o(" (OpenAI GPT model)"),_Fo=l(),K2=a("li"),are=a("strong"),uFo=o("perceiver"),bFo=o(" \u2014 "),j$=a("a"),vFo=o("PerceiverForSequenceClassification"),TFo=o(" (Perceiver model)"),FFo=l(),Z2=a("li"),nre=a("strong"),CFo=o("plbart"),MFo=o(" \u2014 "),N$=a("a"),EFo=o("PLBartForSequenceClassification"),yFo=o(" (PLBart model)"),wFo=l(),e1=a("li"),sre=a("strong"),AFo=o("qdqbert"),LFo=o(" \u2014 "),D$=a("a"),BFo=o("QDQBertForSequenceClassification"),kFo=o(" (QDQBert model)"),xFo=l(),o1=a("li"),lre=a("strong"),RFo=o("reformer"),SFo=o(" \u2014 "),q$=a("a"),PFo=o("ReformerForSequenceClassification"),$Fo=o(" (Reformer model)"),IFo=l(),r1=a("li"),ire=a("strong"),jFo=o("rembert"),NFo=o(" \u2014 "),G$=a("a"),DFo=o("RemBertForSequenceClassification"),qFo=o(" (RemBERT model)"),GFo=l(),t1=a("li"),dre=a("strong"),OFo=o("roberta"),XFo=o(" \u2014 "),O$=a("a"),zFo=o("RobertaForSequenceClassification"),VFo=o(" (RoBERTa model)"),WFo=l(),a1=a("li"),cre=a("strong"),QFo=o("roformer"),HFo=o(" \u2014 "),X$=a("a"),UFo=o("RoFormerForSequenceClassification"),JFo=o(" (RoFormer model)"),YFo=l(),n1=a("li"),fre=a("strong"),KFo=o("squeezebert"),ZFo=o(" \u2014 "),z$=a("a"),e9o=o("SqueezeBertForSequenceClassification"),o9o=o(" (SqueezeBERT model)"),r9o=l(),s1=a("li"),mre=a("strong"),t9o=o("tapas"),a9o=o(" \u2014 "),V$=a("a"),n9o=o("TapasForSequenceClassification"),s9o=o(" (TAPAS model)"),l9o=l(),l1=a("li"),gre=a("strong"),i9o=o("transfo-xl"),d9o=o(" \u2014 "),W$=a("a"),c9o=o("TransfoXLForSequenceClassification"),f9o=o(" (Transformer-XL model)"),m9o=l(),i1=a("li"),hre=a("strong"),g9o=o("xlm"),h9o=o(" \u2014 "),Q$=a("a"),p9o=o("XLMForSequenceClassification"),_9o=o(" (XLM model)"),u9o=l(),d1=a("li"),pre=a("strong"),b9o=o("xlm-roberta"),v9o=o(" \u2014 "),H$=a("a"),T9o=o("XLMRobertaForSequenceClassification"),F9o=o(" (XLM-RoBERTa model)"),C9o=l(),c1=a("li"),_re=a("strong"),M9o=o("xlm-roberta-xl"),E9o=o(" \u2014 "),U$=a("a"),y9o=o("XLMRobertaXLForSequenceClassification"),w9o=o(" (XLM-RoBERTa-XL model)"),A9o=l(),f1=a("li"),ure=a("strong"),L9o=o("xlnet"),B9o=o(" \u2014 "),J$=a("a"),k9o=o("XLNetForSequenceClassification"),x9o=o(" (XLNet model)"),R9o=l(),m1=a("li"),bre=a("strong"),S9o=o("yoso"),P9o=o(" \u2014 "),Y$=a("a"),$9o=o("YosoForSequenceClassification"),I9o=o(" (YOSO model)"),j9o=l(),g1=a("p"),N9o=o("The model is set in evaluation mode by default using "),vre=a("code"),D9o=o("model.eval()"),q9o=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Tre=a("code"),G9o=o("model.train()"),O9o=l(),Fre=a("p"),X9o=o("Examples:"),z9o=l(),f(ME.$$.fragment),SLe=l(),nd=a("h2"),h1=a("a"),Cre=a("span"),f(EE.$$.fragment),V9o=l(),Mre=a("span"),W9o=o("AutoModelForMultipleChoice"),PLe=l(),Yo=a("div"),f(yE.$$.fragment),Q9o=l(),sd=a("p"),H9o=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a multiple choice head) when created
with the `),Ere=a("code"),U9o=o("from_pretrained()"),J9o=o("class method or the "),yre=a("code"),Y9o=o("from_config()"),K9o=o(`class
method.`),Z9o=l(),wE=a("p"),eCo=o("This class cannot be instantiated directly using "),wre=a("code"),oCo=o("__init__()"),rCo=o(" (throws an error)."),tCo=l(),zr=a("div"),f(AE.$$.fragment),aCo=l(),Are=a("p"),nCo=o("Instantiates one of the model classes of the library (with a multiple choice head) from a configuration."),sCo=l(),ld=a("p"),lCo=o(`Note:
Loading a model from its configuration file does `),Lre=a("strong"),iCo=o("not"),dCo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Bre=a("code"),cCo=o("from_pretrained()"),fCo=o("to load the model weights."),mCo=l(),kre=a("p"),gCo=o("Examples:"),hCo=l(),f(LE.$$.fragment),pCo=l(),Ie=a("div"),f(BE.$$.fragment),_Co=l(),xre=a("p"),uCo=o("Instantiate one of the model classes of the library (with a multiple choice head) from a pretrained model."),bCo=l(),Va=a("p"),vCo=o("The model class to instantiate is selected based on the "),Rre=a("code"),TCo=o("model_type"),FCo=o(` property of the config object (either
passed as an argument or loaded from `),Sre=a("code"),CCo=o("pretrained_model_name_or_path"),MCo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Pre=a("code"),ECo=o("pretrained_model_name_or_path"),yCo=o(":"),wCo=l(),G=a("ul"),p1=a("li"),$re=a("strong"),ACo=o("albert"),LCo=o(" \u2014 "),K$=a("a"),BCo=o("AlbertForMultipleChoice"),kCo=o(" (ALBERT model)"),xCo=l(),_1=a("li"),Ire=a("strong"),RCo=o("bert"),SCo=o(" \u2014 "),Z$=a("a"),PCo=o("BertForMultipleChoice"),$Co=o(" (BERT model)"),ICo=l(),u1=a("li"),jre=a("strong"),jCo=o("big_bird"),NCo=o(" \u2014 "),eI=a("a"),DCo=o("BigBirdForMultipleChoice"),qCo=o(" (BigBird model)"),GCo=l(),b1=a("li"),Nre=a("strong"),OCo=o("camembert"),XCo=o(" \u2014 "),oI=a("a"),zCo=o("CamembertForMultipleChoice"),VCo=o(" (CamemBERT model)"),WCo=l(),v1=a("li"),Dre=a("strong"),QCo=o("canine"),HCo=o(" \u2014 "),rI=a("a"),UCo=o("CanineForMultipleChoice"),JCo=o(" (Canine model)"),YCo=l(),T1=a("li"),qre=a("strong"),KCo=o("convbert"),ZCo=o(" \u2014 "),tI=a("a"),e4o=o("ConvBertForMultipleChoice"),o4o=o(" (ConvBERT model)"),r4o=l(),F1=a("li"),Gre=a("strong"),t4o=o("distilbert"),a4o=o(" \u2014 "),aI=a("a"),n4o=o("DistilBertForMultipleChoice"),s4o=o(" (DistilBERT model)"),l4o=l(),C1=a("li"),Ore=a("strong"),i4o=o("electra"),d4o=o(" \u2014 "),nI=a("a"),c4o=o("ElectraForMultipleChoice"),f4o=o(" (ELECTRA model)"),m4o=l(),M1=a("li"),Xre=a("strong"),g4o=o("flaubert"),h4o=o(" \u2014 "),sI=a("a"),p4o=o("FlaubertForMultipleChoice"),_4o=o(" (FlauBERT model)"),u4o=l(),E1=a("li"),zre=a("strong"),b4o=o("fnet"),v4o=o(" \u2014 "),lI=a("a"),T4o=o("FNetForMultipleChoice"),F4o=o(" (FNet model)"),C4o=l(),y1=a("li"),Vre=a("strong"),M4o=o("funnel"),E4o=o(" \u2014 "),iI=a("a"),y4o=o("FunnelForMultipleChoice"),w4o=o(" (Funnel Transformer model)"),A4o=l(),w1=a("li"),Wre=a("strong"),L4o=o("ibert"),B4o=o(" \u2014 "),dI=a("a"),k4o=o("IBertForMultipleChoice"),x4o=o(" (I-BERT model)"),R4o=l(),A1=a("li"),Qre=a("strong"),S4o=o("longformer"),P4o=o(" \u2014 "),cI=a("a"),$4o=o("LongformerForMultipleChoice"),I4o=o(" (Longformer model)"),j4o=l(),L1=a("li"),Hre=a("strong"),N4o=o("megatron-bert"),D4o=o(" \u2014 "),fI=a("a"),q4o=o("MegatronBertForMultipleChoice"),G4o=o(" (MegatronBert model)"),O4o=l(),B1=a("li"),Ure=a("strong"),X4o=o("mobilebert"),z4o=o(" \u2014 "),mI=a("a"),V4o=o("MobileBertForMultipleChoice"),W4o=o(" (MobileBERT model)"),Q4o=l(),k1=a("li"),Jre=a("strong"),H4o=o("mpnet"),U4o=o(" \u2014 "),gI=a("a"),J4o=o("MPNetForMultipleChoice"),Y4o=o(" (MPNet model)"),K4o=l(),x1=a("li"),Yre=a("strong"),Z4o=o("nystromformer"),eMo=o(" \u2014 "),hI=a("a"),oMo=o("NystromformerForMultipleChoice"),rMo=o(" (Nystromformer model)"),tMo=l(),R1=a("li"),Kre=a("strong"),aMo=o("qdqbert"),nMo=o(" \u2014 "),pI=a("a"),sMo=o("QDQBertForMultipleChoice"),lMo=o(" (QDQBert model)"),iMo=l(),S1=a("li"),Zre=a("strong"),dMo=o("rembert"),cMo=o(" \u2014 "),_I=a("a"),fMo=o("RemBertForMultipleChoice"),mMo=o(" (RemBERT model)"),gMo=l(),P1=a("li"),ete=a("strong"),hMo=o("roberta"),pMo=o(" \u2014 "),uI=a("a"),_Mo=o("RobertaForMultipleChoice"),uMo=o(" (RoBERTa model)"),bMo=l(),$1=a("li"),ote=a("strong"),vMo=o("roformer"),TMo=o(" \u2014 "),bI=a("a"),FMo=o("RoFormerForMultipleChoice"),CMo=o(" (RoFormer model)"),MMo=l(),I1=a("li"),rte=a("strong"),EMo=o("squeezebert"),yMo=o(" \u2014 "),vI=a("a"),wMo=o("SqueezeBertForMultipleChoice"),AMo=o(" (SqueezeBERT model)"),LMo=l(),j1=a("li"),tte=a("strong"),BMo=o("xlm"),kMo=o(" \u2014 "),TI=a("a"),xMo=o("XLMForMultipleChoice"),RMo=o(" (XLM model)"),SMo=l(),N1=a("li"),ate=a("strong"),PMo=o("xlm-roberta"),$Mo=o(" \u2014 "),FI=a("a"),IMo=o("XLMRobertaForMultipleChoice"),jMo=o(" (XLM-RoBERTa model)"),NMo=l(),D1=a("li"),nte=a("strong"),DMo=o("xlm-roberta-xl"),qMo=o(" \u2014 "),CI=a("a"),GMo=o("XLMRobertaXLForMultipleChoice"),OMo=o(" (XLM-RoBERTa-XL model)"),XMo=l(),q1=a("li"),ste=a("strong"),zMo=o("xlnet"),VMo=o(" \u2014 "),MI=a("a"),WMo=o("XLNetForMultipleChoice"),QMo=o(" (XLNet model)"),HMo=l(),G1=a("li"),lte=a("strong"),UMo=o("yoso"),JMo=o(" \u2014 "),EI=a("a"),YMo=o("YosoForMultipleChoice"),KMo=o(" (YOSO model)"),ZMo=l(),O1=a("p"),eEo=o("The model is set in evaluation mode by default using "),ite=a("code"),oEo=o("model.eval()"),rEo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),dte=a("code"),tEo=o("model.train()"),aEo=l(),cte=a("p"),nEo=o("Examples:"),sEo=l(),f(kE.$$.fragment),$Le=l(),id=a("h2"),X1=a("a"),fte=a("span"),f(xE.$$.fragment),lEo=l(),mte=a("span"),iEo=o("AutoModelForNextSentencePrediction"),ILe=l(),Ko=a("div"),f(RE.$$.fragment),dEo=l(),dd=a("p"),cEo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a next sentence prediction head) when created
with the `),gte=a("code"),fEo=o("from_pretrained()"),mEo=o("class method or the "),hte=a("code"),gEo=o("from_config()"),hEo=o(`class
method.`),pEo=l(),SE=a("p"),_Eo=o("This class cannot be instantiated directly using "),pte=a("code"),uEo=o("__init__()"),bEo=o(" (throws an error)."),vEo=l(),Vr=a("div"),f(PE.$$.fragment),TEo=l(),_te=a("p"),FEo=o("Instantiates one of the model classes of the library (with a next sentence prediction head) from a configuration."),CEo=l(),cd=a("p"),MEo=o(`Note:
Loading a model from its configuration file does `),ute=a("strong"),EEo=o("not"),yEo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),bte=a("code"),wEo=o("from_pretrained()"),AEo=o("to load the model weights."),LEo=l(),vte=a("p"),BEo=o("Examples:"),kEo=l(),f($E.$$.fragment),xEo=l(),je=a("div"),f(IE.$$.fragment),REo=l(),Tte=a("p"),SEo=o("Instantiate one of the model classes of the library (with a next sentence prediction head) from a pretrained model."),PEo=l(),Wa=a("p"),$Eo=o("The model class to instantiate is selected based on the "),Fte=a("code"),IEo=o("model_type"),jEo=o(` property of the config object (either
passed as an argument or loaded from `),Cte=a("code"),NEo=o("pretrained_model_name_or_path"),DEo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Mte=a("code"),qEo=o("pretrained_model_name_or_path"),GEo=o(":"),OEo=l(),na=a("ul"),z1=a("li"),Ete=a("strong"),XEo=o("bert"),zEo=o(" \u2014 "),yI=a("a"),VEo=o("BertForNextSentencePrediction"),WEo=o(" (BERT model)"),QEo=l(),V1=a("li"),yte=a("strong"),HEo=o("fnet"),UEo=o(" \u2014 "),wI=a("a"),JEo=o("FNetForNextSentencePrediction"),YEo=o(" (FNet model)"),KEo=l(),W1=a("li"),wte=a("strong"),ZEo=o("megatron-bert"),e3o=o(" \u2014 "),AI=a("a"),o3o=o("MegatronBertForNextSentencePrediction"),r3o=o(" (MegatronBert model)"),t3o=l(),Q1=a("li"),Ate=a("strong"),a3o=o("mobilebert"),n3o=o(" \u2014 "),LI=a("a"),s3o=o("MobileBertForNextSentencePrediction"),l3o=o(" (MobileBERT model)"),i3o=l(),H1=a("li"),Lte=a("strong"),d3o=o("qdqbert"),c3o=o(" \u2014 "),BI=a("a"),f3o=o("QDQBertForNextSentencePrediction"),m3o=o(" (QDQBert model)"),g3o=l(),U1=a("p"),h3o=o("The model is set in evaluation mode by default using "),Bte=a("code"),p3o=o("model.eval()"),_3o=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),kte=a("code"),u3o=o("model.train()"),b3o=l(),xte=a("p"),v3o=o("Examples:"),T3o=l(),f(jE.$$.fragment),jLe=l(),fd=a("h2"),J1=a("a"),Rte=a("span"),f(NE.$$.fragment),F3o=l(),Ste=a("span"),C3o=o("AutoModelForTokenClassification"),NLe=l(),Zo=a("div"),f(DE.$$.fragment),M3o=l(),md=a("p"),E3o=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a token classification head) when created
with the `),Pte=a("code"),y3o=o("from_pretrained()"),w3o=o("class method or the "),$te=a("code"),A3o=o("from_config()"),L3o=o(`class
method.`),B3o=l(),qE=a("p"),k3o=o("This class cannot be instantiated directly using "),Ite=a("code"),x3o=o("__init__()"),R3o=o(" (throws an error)."),S3o=l(),Wr=a("div"),f(GE.$$.fragment),P3o=l(),jte=a("p"),$3o=o("Instantiates one of the model classes of the library (with a token classification head) from a configuration."),I3o=l(),gd=a("p"),j3o=o(`Note:
Loading a model from its configuration file does `),Nte=a("strong"),N3o=o("not"),D3o=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Dte=a("code"),q3o=o("from_pretrained()"),G3o=o("to load the model weights."),O3o=l(),qte=a("p"),X3o=o("Examples:"),z3o=l(),f(OE.$$.fragment),V3o=l(),Ne=a("div"),f(XE.$$.fragment),W3o=l(),Gte=a("p"),Q3o=o("Instantiate one of the model classes of the library (with a token classification head) from a pretrained model."),H3o=l(),Qa=a("p"),U3o=o("The model class to instantiate is selected based on the "),Ote=a("code"),J3o=o("model_type"),Y3o=o(` property of the config object (either
passed as an argument or loaded from `),Xte=a("code"),K3o=o("pretrained_model_name_or_path"),Z3o=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),zte=a("code"),eyo=o("pretrained_model_name_or_path"),oyo=o(":"),ryo=l(),D=a("ul"),Y1=a("li"),Vte=a("strong"),tyo=o("albert"),ayo=o(" \u2014 "),kI=a("a"),nyo=o("AlbertForTokenClassification"),syo=o(" (ALBERT model)"),lyo=l(),K1=a("li"),Wte=a("strong"),iyo=o("bert"),dyo=o(" \u2014 "),xI=a("a"),cyo=o("BertForTokenClassification"),fyo=o(" (BERT model)"),myo=l(),Z1=a("li"),Qte=a("strong"),gyo=o("big_bird"),hyo=o(" \u2014 "),RI=a("a"),pyo=o("BigBirdForTokenClassification"),_yo=o(" (BigBird model)"),uyo=l(),eb=a("li"),Hte=a("strong"),byo=o("camembert"),vyo=o(" \u2014 "),SI=a("a"),Tyo=o("CamembertForTokenClassification"),Fyo=o(" (CamemBERT model)"),Cyo=l(),ob=a("li"),Ute=a("strong"),Myo=o("canine"),Eyo=o(" \u2014 "),PI=a("a"),yyo=o("CanineForTokenClassification"),wyo=o(" (Canine model)"),Ayo=l(),rb=a("li"),Jte=a("strong"),Lyo=o("convbert"),Byo=o(" \u2014 "),$I=a("a"),kyo=o("ConvBertForTokenClassification"),xyo=o(" (ConvBERT model)"),Ryo=l(),tb=a("li"),Yte=a("strong"),Syo=o("deberta"),Pyo=o(" \u2014 "),II=a("a"),$yo=o("DebertaForTokenClassification"),Iyo=o(" (DeBERTa model)"),jyo=l(),ab=a("li"),Kte=a("strong"),Nyo=o("deberta-v2"),Dyo=o(" \u2014 "),jI=a("a"),qyo=o("DebertaV2ForTokenClassification"),Gyo=o(" (DeBERTa-v2 model)"),Oyo=l(),nb=a("li"),Zte=a("strong"),Xyo=o("distilbert"),zyo=o(" \u2014 "),NI=a("a"),Vyo=o("DistilBertForTokenClassification"),Wyo=o(" (DistilBERT model)"),Qyo=l(),sb=a("li"),eae=a("strong"),Hyo=o("electra"),Uyo=o(" \u2014 "),DI=a("a"),Jyo=o("ElectraForTokenClassification"),Yyo=o(" (ELECTRA model)"),Kyo=l(),lb=a("li"),oae=a("strong"),Zyo=o("flaubert"),ewo=o(" \u2014 "),qI=a("a"),owo=o("FlaubertForTokenClassification"),rwo=o(" (FlauBERT model)"),two=l(),ib=a("li"),rae=a("strong"),awo=o("fnet"),nwo=o(" \u2014 "),GI=a("a"),swo=o("FNetForTokenClassification"),lwo=o(" (FNet model)"),iwo=l(),db=a("li"),tae=a("strong"),dwo=o("funnel"),cwo=o(" \u2014 "),OI=a("a"),fwo=o("FunnelForTokenClassification"),mwo=o(" (Funnel Transformer model)"),gwo=l(),cb=a("li"),aae=a("strong"),hwo=o("gpt2"),pwo=o(" \u2014 "),XI=a("a"),_wo=o("GPT2ForTokenClassification"),uwo=o(" (OpenAI GPT-2 model)"),bwo=l(),fb=a("li"),nae=a("strong"),vwo=o("ibert"),Two=o(" \u2014 "),zI=a("a"),Fwo=o("IBertForTokenClassification"),Cwo=o(" (I-BERT model)"),Mwo=l(),mb=a("li"),sae=a("strong"),Ewo=o("layoutlm"),ywo=o(" \u2014 "),VI=a("a"),wwo=o("LayoutLMForTokenClassification"),Awo=o(" (LayoutLM model)"),Lwo=l(),gb=a("li"),lae=a("strong"),Bwo=o("layoutlmv2"),kwo=o(" \u2014 "),WI=a("a"),xwo=o("LayoutLMv2ForTokenClassification"),Rwo=o(" (LayoutLMv2 model)"),Swo=l(),hb=a("li"),iae=a("strong"),Pwo=o("longformer"),$wo=o(" \u2014 "),QI=a("a"),Iwo=o("LongformerForTokenClassification"),jwo=o(" (Longformer model)"),Nwo=l(),pb=a("li"),dae=a("strong"),Dwo=o("megatron-bert"),qwo=o(" \u2014 "),HI=a("a"),Gwo=o("MegatronBertForTokenClassification"),Owo=o(" (MegatronBert model)"),Xwo=l(),_b=a("li"),cae=a("strong"),zwo=o("mobilebert"),Vwo=o(" \u2014 "),UI=a("a"),Wwo=o("MobileBertForTokenClassification"),Qwo=o(" (MobileBERT model)"),Hwo=l(),ub=a("li"),fae=a("strong"),Uwo=o("mpnet"),Jwo=o(" \u2014 "),JI=a("a"),Ywo=o("MPNetForTokenClassification"),Kwo=o(" (MPNet model)"),Zwo=l(),bb=a("li"),mae=a("strong"),eAo=o("nystromformer"),oAo=o(" \u2014 "),YI=a("a"),rAo=o("NystromformerForTokenClassification"),tAo=o(" (Nystromformer model)"),aAo=l(),vb=a("li"),gae=a("strong"),nAo=o("qdqbert"),sAo=o(" \u2014 "),KI=a("a"),lAo=o("QDQBertForTokenClassification"),iAo=o(" (QDQBert model)"),dAo=l(),Tb=a("li"),hae=a("strong"),cAo=o("rembert"),fAo=o(" \u2014 "),ZI=a("a"),mAo=o("RemBertForTokenClassification"),gAo=o(" (RemBERT model)"),hAo=l(),Fb=a("li"),pae=a("strong"),pAo=o("roberta"),_Ao=o(" \u2014 "),ej=a("a"),uAo=o("RobertaForTokenClassification"),bAo=o(" (RoBERTa model)"),vAo=l(),Cb=a("li"),_ae=a("strong"),TAo=o("roformer"),FAo=o(" \u2014 "),oj=a("a"),CAo=o("RoFormerForTokenClassification"),MAo=o(" (RoFormer model)"),EAo=l(),Mb=a("li"),uae=a("strong"),yAo=o("squeezebert"),wAo=o(" \u2014 "),rj=a("a"),AAo=o("SqueezeBertForTokenClassification"),LAo=o(" (SqueezeBERT model)"),BAo=l(),Eb=a("li"),bae=a("strong"),kAo=o("xlm"),xAo=o(" \u2014 "),tj=a("a"),RAo=o("XLMForTokenClassification"),SAo=o(" (XLM model)"),PAo=l(),yb=a("li"),vae=a("strong"),$Ao=o("xlm-roberta"),IAo=o(" \u2014 "),aj=a("a"),jAo=o("XLMRobertaForTokenClassification"),NAo=o(" (XLM-RoBERTa model)"),DAo=l(),wb=a("li"),Tae=a("strong"),qAo=o("xlm-roberta-xl"),GAo=o(" \u2014 "),nj=a("a"),OAo=o("XLMRobertaXLForTokenClassification"),XAo=o(" (XLM-RoBERTa-XL model)"),zAo=l(),Ab=a("li"),Fae=a("strong"),VAo=o("xlnet"),WAo=o(" \u2014 "),sj=a("a"),QAo=o("XLNetForTokenClassification"),HAo=o(" (XLNet model)"),UAo=l(),Lb=a("li"),Cae=a("strong"),JAo=o("yoso"),YAo=o(" \u2014 "),lj=a("a"),KAo=o("YosoForTokenClassification"),ZAo=o(" (YOSO model)"),e6o=l(),Bb=a("p"),o6o=o("The model is set in evaluation mode by default using "),Mae=a("code"),r6o=o("model.eval()"),t6o=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Eae=a("code"),a6o=o("model.train()"),n6o=l(),yae=a("p"),s6o=o("Examples:"),l6o=l(),f(zE.$$.fragment),DLe=l(),hd=a("h2"),kb=a("a"),wae=a("span"),f(VE.$$.fragment),i6o=l(),Aae=a("span"),d6o=o("AutoModelForQuestionAnswering"),qLe=l(),er=a("div"),f(WE.$$.fragment),c6o=l(),pd=a("p"),f6o=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a question answering head) when created
with the `),Lae=a("code"),m6o=o("from_pretrained()"),g6o=o("class method or the "),Bae=a("code"),h6o=o("from_config()"),p6o=o(`class
method.`),_6o=l(),QE=a("p"),u6o=o("This class cannot be instantiated directly using "),kae=a("code"),b6o=o("__init__()"),v6o=o(" (throws an error)."),T6o=l(),Qr=a("div"),f(HE.$$.fragment),F6o=l(),xae=a("p"),C6o=o("Instantiates one of the model classes of the library (with a question answering head) from a configuration."),M6o=l(),_d=a("p"),E6o=o(`Note:
Loading a model from its configuration file does `),Rae=a("strong"),y6o=o("not"),w6o=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Sae=a("code"),A6o=o("from_pretrained()"),L6o=o("to load the model weights."),B6o=l(),Pae=a("p"),k6o=o("Examples:"),x6o=l(),f(UE.$$.fragment),R6o=l(),De=a("div"),f(JE.$$.fragment),S6o=l(),$ae=a("p"),P6o=o("Instantiate one of the model classes of the library (with a question answering head) from a pretrained model."),$6o=l(),Ha=a("p"),I6o=o("The model class to instantiate is selected based on the "),Iae=a("code"),j6o=o("model_type"),N6o=o(` property of the config object (either
passed as an argument or loaded from `),jae=a("code"),D6o=o("pretrained_model_name_or_path"),q6o=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Nae=a("code"),G6o=o("pretrained_model_name_or_path"),O6o=o(":"),X6o=l(),R=a("ul"),xb=a("li"),Dae=a("strong"),z6o=o("albert"),V6o=o(" \u2014 "),ij=a("a"),W6o=o("AlbertForQuestionAnswering"),Q6o=o(" (ALBERT model)"),H6o=l(),Rb=a("li"),qae=a("strong"),U6o=o("bart"),J6o=o(" \u2014 "),dj=a("a"),Y6o=o("BartForQuestionAnswering"),K6o=o(" (BART model)"),Z6o=l(),Sb=a("li"),Gae=a("strong"),e0o=o("bert"),o0o=o(" \u2014 "),cj=a("a"),r0o=o("BertForQuestionAnswering"),t0o=o(" (BERT model)"),a0o=l(),Pb=a("li"),Oae=a("strong"),n0o=o("big_bird"),s0o=o(" \u2014 "),fj=a("a"),l0o=o("BigBirdForQuestionAnswering"),i0o=o(" (BigBird model)"),d0o=l(),$b=a("li"),Xae=a("strong"),c0o=o("bigbird_pegasus"),f0o=o(" \u2014 "),mj=a("a"),m0o=o("BigBirdPegasusForQuestionAnswering"),g0o=o(" (BigBirdPegasus model)"),h0o=l(),Ib=a("li"),zae=a("strong"),p0o=o("camembert"),_0o=o(" \u2014 "),gj=a("a"),u0o=o("CamembertForQuestionAnswering"),b0o=o(" (CamemBERT model)"),v0o=l(),jb=a("li"),Vae=a("strong"),T0o=o("canine"),F0o=o(" \u2014 "),hj=a("a"),C0o=o("CanineForQuestionAnswering"),M0o=o(" (Canine model)"),E0o=l(),Nb=a("li"),Wae=a("strong"),y0o=o("convbert"),w0o=o(" \u2014 "),pj=a("a"),A0o=o("ConvBertForQuestionAnswering"),L0o=o(" (ConvBERT model)"),B0o=l(),Db=a("li"),Qae=a("strong"),k0o=o("deberta"),x0o=o(" \u2014 "),_j=a("a"),R0o=o("DebertaForQuestionAnswering"),S0o=o(" (DeBERTa model)"),P0o=l(),qb=a("li"),Hae=a("strong"),$0o=o("deberta-v2"),I0o=o(" \u2014 "),uj=a("a"),j0o=o("DebertaV2ForQuestionAnswering"),N0o=o(" (DeBERTa-v2 model)"),D0o=l(),Gb=a("li"),Uae=a("strong"),q0o=o("distilbert"),G0o=o(" \u2014 "),bj=a("a"),O0o=o("DistilBertForQuestionAnswering"),X0o=o(" (DistilBERT model)"),z0o=l(),Ob=a("li"),Jae=a("strong"),V0o=o("electra"),W0o=o(" \u2014 "),vj=a("a"),Q0o=o("ElectraForQuestionAnswering"),H0o=o(" (ELECTRA model)"),U0o=l(),Xb=a("li"),Yae=a("strong"),J0o=o("flaubert"),Y0o=o(" \u2014 "),Tj=a("a"),K0o=o("FlaubertForQuestionAnsweringSimple"),Z0o=o(" (FlauBERT model)"),eLo=l(),zb=a("li"),Kae=a("strong"),oLo=o("fnet"),rLo=o(" \u2014 "),Fj=a("a"),tLo=o("FNetForQuestionAnswering"),aLo=o(" (FNet model)"),nLo=l(),Vb=a("li"),Zae=a("strong"),sLo=o("funnel"),lLo=o(" \u2014 "),Cj=a("a"),iLo=o("FunnelForQuestionAnswering"),dLo=o(" (Funnel Transformer model)"),cLo=l(),Wb=a("li"),ene=a("strong"),fLo=o("gptj"),mLo=o(" \u2014 "),Mj=a("a"),gLo=o("GPTJForQuestionAnswering"),hLo=o(" (GPT-J model)"),pLo=l(),Qb=a("li"),one=a("strong"),_Lo=o("ibert"),uLo=o(" \u2014 "),Ej=a("a"),bLo=o("IBertForQuestionAnswering"),vLo=o(" (I-BERT model)"),TLo=l(),Hb=a("li"),rne=a("strong"),FLo=o("layoutlmv2"),CLo=o(" \u2014 "),yj=a("a"),MLo=o("LayoutLMv2ForQuestionAnswering"),ELo=o(" (LayoutLMv2 model)"),yLo=l(),Ub=a("li"),tne=a("strong"),wLo=o("led"),ALo=o(" \u2014 "),wj=a("a"),LLo=o("LEDForQuestionAnswering"),BLo=o(" (LED model)"),kLo=l(),Jb=a("li"),ane=a("strong"),xLo=o("longformer"),RLo=o(" \u2014 "),Aj=a("a"),SLo=o("LongformerForQuestionAnswering"),PLo=o(" (Longformer model)"),$Lo=l(),Yb=a("li"),nne=a("strong"),ILo=o("lxmert"),jLo=o(" \u2014 "),Lj=a("a"),NLo=o("LxmertForQuestionAnswering"),DLo=o(" (LXMERT model)"),qLo=l(),Kb=a("li"),sne=a("strong"),GLo=o("mbart"),OLo=o(" \u2014 "),Bj=a("a"),XLo=o("MBartForQuestionAnswering"),zLo=o(" (mBART model)"),VLo=l(),Zb=a("li"),lne=a("strong"),WLo=o("megatron-bert"),QLo=o(" \u2014 "),kj=a("a"),HLo=o("MegatronBertForQuestionAnswering"),ULo=o(" (MegatronBert model)"),JLo=l(),e5=a("li"),ine=a("strong"),YLo=o("mobilebert"),KLo=o(" \u2014 "),xj=a("a"),ZLo=o("MobileBertForQuestionAnswering"),e8o=o(" (MobileBERT model)"),o8o=l(),o5=a("li"),dne=a("strong"),r8o=o("mpnet"),t8o=o(" \u2014 "),Rj=a("a"),a8o=o("MPNetForQuestionAnswering"),n8o=o(" (MPNet model)"),s8o=l(),r5=a("li"),cne=a("strong"),l8o=o("nystromformer"),i8o=o(" \u2014 "),Sj=a("a"),d8o=o("NystromformerForQuestionAnswering"),c8o=o(" (Nystromformer model)"),f8o=l(),t5=a("li"),fne=a("strong"),m8o=o("qdqbert"),g8o=o(" \u2014 "),Pj=a("a"),h8o=o("QDQBertForQuestionAnswering"),p8o=o(" (QDQBert model)"),_8o=l(),a5=a("li"),mne=a("strong"),u8o=o("reformer"),b8o=o(" \u2014 "),$j=a("a"),v8o=o("ReformerForQuestionAnswering"),T8o=o(" (Reformer model)"),F8o=l(),n5=a("li"),gne=a("strong"),C8o=o("rembert"),M8o=o(" \u2014 "),Ij=a("a"),E8o=o("RemBertForQuestionAnswering"),y8o=o(" (RemBERT model)"),w8o=l(),s5=a("li"),hne=a("strong"),A8o=o("roberta"),L8o=o(" \u2014 "),jj=a("a"),B8o=o("RobertaForQuestionAnswering"),k8o=o(" (RoBERTa model)"),x8o=l(),l5=a("li"),pne=a("strong"),R8o=o("roformer"),S8o=o(" \u2014 "),Nj=a("a"),P8o=o("RoFormerForQuestionAnswering"),$8o=o(" (RoFormer model)"),I8o=l(),i5=a("li"),_ne=a("strong"),j8o=o("splinter"),N8o=o(" \u2014 "),Dj=a("a"),D8o=o("SplinterForQuestionAnswering"),q8o=o(" (Splinter model)"),G8o=l(),d5=a("li"),une=a("strong"),O8o=o("squeezebert"),X8o=o(" \u2014 "),qj=a("a"),z8o=o("SqueezeBertForQuestionAnswering"),V8o=o(" (SqueezeBERT model)"),W8o=l(),c5=a("li"),bne=a("strong"),Q8o=o("xlm"),H8o=o(" \u2014 "),Gj=a("a"),U8o=o("XLMForQuestionAnsweringSimple"),J8o=o(" (XLM model)"),Y8o=l(),f5=a("li"),vne=a("strong"),K8o=o("xlm-roberta"),Z8o=o(" \u2014 "),Oj=a("a"),eBo=o("XLMRobertaForQuestionAnswering"),oBo=o(" (XLM-RoBERTa model)"),rBo=l(),m5=a("li"),Tne=a("strong"),tBo=o("xlm-roberta-xl"),aBo=o(" \u2014 "),Xj=a("a"),nBo=o("XLMRobertaXLForQuestionAnswering"),sBo=o(" (XLM-RoBERTa-XL model)"),lBo=l(),g5=a("li"),Fne=a("strong"),iBo=o("xlnet"),dBo=o(" \u2014 "),zj=a("a"),cBo=o("XLNetForQuestionAnsweringSimple"),fBo=o(" (XLNet model)"),mBo=l(),h5=a("li"),Cne=a("strong"),gBo=o("yoso"),hBo=o(" \u2014 "),Vj=a("a"),pBo=o("YosoForQuestionAnswering"),_Bo=o(" (YOSO model)"),uBo=l(),p5=a("p"),bBo=o("The model is set in evaluation mode by default using "),Mne=a("code"),vBo=o("model.eval()"),TBo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Ene=a("code"),FBo=o("model.train()"),CBo=l(),yne=a("p"),MBo=o("Examples:"),EBo=l(),f(YE.$$.fragment),GLe=l(),ud=a("h2"),_5=a("a"),wne=a("span"),f(KE.$$.fragment),yBo=l(),Ane=a("span"),wBo=o("AutoModelForTableQuestionAnswering"),OLe=l(),or=a("div"),f(ZE.$$.fragment),ABo=l(),bd=a("p"),LBo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a table question answering head) when created
with the `),Lne=a("code"),BBo=o("from_pretrained()"),kBo=o("class method or the "),Bne=a("code"),xBo=o("from_config()"),RBo=o(`class
method.`),SBo=l(),e3=a("p"),PBo=o("This class cannot be instantiated directly using "),kne=a("code"),$Bo=o("__init__()"),IBo=o(" (throws an error)."),jBo=l(),Hr=a("div"),f(o3.$$.fragment),NBo=l(),xne=a("p"),DBo=o("Instantiates one of the model classes of the library (with a table question answering head) from a configuration."),qBo=l(),vd=a("p"),GBo=o(`Note:
Loading a model from its configuration file does `),Rne=a("strong"),OBo=o("not"),XBo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Sne=a("code"),zBo=o("from_pretrained()"),VBo=o("to load the model weights."),WBo=l(),Pne=a("p"),QBo=o("Examples:"),HBo=l(),f(r3.$$.fragment),UBo=l(),qe=a("div"),f(t3.$$.fragment),JBo=l(),$ne=a("p"),YBo=o("Instantiate one of the model classes of the library (with a table question answering head) from a pretrained model."),KBo=l(),Ua=a("p"),ZBo=o("The model class to instantiate is selected based on the "),Ine=a("code"),eko=o("model_type"),oko=o(` property of the config object (either
passed as an argument or loaded from `),jne=a("code"),rko=o("pretrained_model_name_or_path"),tko=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Nne=a("code"),ako=o("pretrained_model_name_or_path"),nko=o(":"),sko=l(),Dne=a("ul"),u5=a("li"),qne=a("strong"),lko=o("tapas"),iko=o(" \u2014 "),Wj=a("a"),dko=o("TapasForQuestionAnswering"),cko=o(" (TAPAS model)"),fko=l(),b5=a("p"),mko=o("The model is set in evaluation mode by default using "),Gne=a("code"),gko=o("model.eval()"),hko=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),One=a("code"),pko=o("model.train()"),_ko=l(),Xne=a("p"),uko=o("Examples:"),bko=l(),f(a3.$$.fragment),XLe=l(),Td=a("h2"),v5=a("a"),zne=a("span"),f(n3.$$.fragment),vko=l(),Vne=a("span"),Tko=o("AutoModelForImageClassification"),zLe=l(),rr=a("div"),f(s3.$$.fragment),Fko=l(),Fd=a("p"),Cko=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a image classification head) when created
with the `),Wne=a("code"),Mko=o("from_pretrained()"),Eko=o("class method or the "),Qne=a("code"),yko=o("from_config()"),wko=o(`class
method.`),Ako=l(),l3=a("p"),Lko=o("This class cannot be instantiated directly using "),Hne=a("code"),Bko=o("__init__()"),kko=o(" (throws an error)."),xko=l(),Ur=a("div"),f(i3.$$.fragment),Rko=l(),Une=a("p"),Sko=o("Instantiates one of the model classes of the library (with a image classification head) from a configuration."),Pko=l(),Cd=a("p"),$ko=o(`Note:
Loading a model from its configuration file does `),Jne=a("strong"),Iko=o("not"),jko=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Yne=a("code"),Nko=o("from_pretrained()"),Dko=o("to load the model weights."),qko=l(),Kne=a("p"),Gko=o("Examples:"),Oko=l(),f(d3.$$.fragment),Xko=l(),Ge=a("div"),f(c3.$$.fragment),zko=l(),Zne=a("p"),Vko=o("Instantiate one of the model classes of the library (with a image classification head) from a pretrained model."),Wko=l(),Ja=a("p"),Qko=o("The model class to instantiate is selected based on the "),ese=a("code"),Hko=o("model_type"),Uko=o(` property of the config object (either
passed as an argument or loaded from `),ose=a("code"),Jko=o("pretrained_model_name_or_path"),Yko=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),rse=a("code"),Kko=o("pretrained_model_name_or_path"),Zko=o(":"),exo=l(),be=a("ul"),T5=a("li"),tse=a("strong"),oxo=o("beit"),rxo=o(" \u2014 "),Qj=a("a"),txo=o("BeitForImageClassification"),axo=o(" (BEiT model)"),nxo=l(),F5=a("li"),ase=a("strong"),sxo=o("convnext"),lxo=o(" \u2014 "),Hj=a("a"),ixo=o("ConvNextForImageClassification"),dxo=o(" (ConvNext model)"),cxo=l(),Ss=a("li"),nse=a("strong"),fxo=o("deit"),mxo=o(" \u2014 "),Uj=a("a"),gxo=o("DeiTForImageClassification"),hxo=o(" or "),Jj=a("a"),pxo=o("DeiTForImageClassificationWithTeacher"),_xo=o(" (DeiT model)"),uxo=l(),C5=a("li"),sse=a("strong"),bxo=o("imagegpt"),vxo=o(" \u2014 "),Yj=a("a"),Txo=o("ImageGPTForImageClassification"),Fxo=o(" (ImageGPT model)"),Cxo=l(),la=a("li"),lse=a("strong"),Mxo=o("perceiver"),Exo=o(" \u2014 "),Kj=a("a"),yxo=o("PerceiverForImageClassificationLearned"),wxo=o(" or "),Zj=a("a"),Axo=o("PerceiverForImageClassificationFourier"),Lxo=o(" or "),eN=a("a"),Bxo=o("PerceiverForImageClassificationConvProcessing"),kxo=o(" (Perceiver model)"),xxo=l(),M5=a("li"),ise=a("strong"),Rxo=o("poolformer"),Sxo=o(" \u2014 "),oN=a("a"),Pxo=o("PoolFormerForImageClassification"),$xo=o(" (PoolFormer model)"),Ixo=l(),E5=a("li"),dse=a("strong"),jxo=o("segformer"),Nxo=o(" \u2014 "),rN=a("a"),Dxo=o("SegformerForImageClassification"),qxo=o(" (SegFormer model)"),Gxo=l(),y5=a("li"),cse=a("strong"),Oxo=o("swin"),Xxo=o(" \u2014 "),tN=a("a"),zxo=o("SwinForImageClassification"),Vxo=o(" (Swin model)"),Wxo=l(),w5=a("li"),fse=a("strong"),Qxo=o("vit"),Hxo=o(" \u2014 "),aN=a("a"),Uxo=o("ViTForImageClassification"),Jxo=o(" (ViT model)"),Yxo=l(),A5=a("p"),Kxo=o("The model is set in evaluation mode by default using "),mse=a("code"),Zxo=o("model.eval()"),eRo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),gse=a("code"),oRo=o("model.train()"),rRo=l(),hse=a("p"),tRo=o("Examples:"),aRo=l(),f(f3.$$.fragment),VLe=l(),Md=a("h2"),L5=a("a"),pse=a("span"),f(m3.$$.fragment),nRo=l(),_se=a("span"),sRo=o("AutoModelForVision2Seq"),WLe=l(),tr=a("div"),f(g3.$$.fragment),lRo=l(),Ed=a("p"),iRo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a vision-to-text modeling head) when created
with the `),use=a("code"),dRo=o("from_pretrained()"),cRo=o("class method or the "),bse=a("code"),fRo=o("from_config()"),mRo=o(`class
method.`),gRo=l(),h3=a("p"),hRo=o("This class cannot be instantiated directly using "),vse=a("code"),pRo=o("__init__()"),_Ro=o(" (throws an error)."),uRo=l(),Jr=a("div"),f(p3.$$.fragment),bRo=l(),Tse=a("p"),vRo=o("Instantiates one of the model classes of the library (with a vision-to-text modeling head) from a configuration."),TRo=l(),yd=a("p"),FRo=o(`Note:
Loading a model from its configuration file does `),Fse=a("strong"),CRo=o("not"),MRo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Cse=a("code"),ERo=o("from_pretrained()"),yRo=o("to load the model weights."),wRo=l(),Mse=a("p"),ARo=o("Examples:"),LRo=l(),f(_3.$$.fragment),BRo=l(),Oe=a("div"),f(u3.$$.fragment),kRo=l(),Ese=a("p"),xRo=o("Instantiate one of the model classes of the library (with a vision-to-text modeling head) from a pretrained model."),RRo=l(),Ya=a("p"),SRo=o("The model class to instantiate is selected based on the "),yse=a("code"),PRo=o("model_type"),$Ro=o(` property of the config object (either
passed as an argument or loaded from `),wse=a("code"),IRo=o("pretrained_model_name_or_path"),jRo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Ase=a("code"),NRo=o("pretrained_model_name_or_path"),DRo=o(":"),qRo=l(),Lse=a("ul"),B5=a("li"),Bse=a("strong"),GRo=o("vision-encoder-decoder"),ORo=o(" \u2014 "),nN=a("a"),XRo=o("VisionEncoderDecoderModel"),zRo=o(" (Vision Encoder decoder model)"),VRo=l(),k5=a("p"),WRo=o("The model is set in evaluation mode by default using "),kse=a("code"),QRo=o("model.eval()"),HRo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),xse=a("code"),URo=o("model.train()"),JRo=l(),Rse=a("p"),YRo=o("Examples:"),KRo=l(),f(b3.$$.fragment),QLe=l(),wd=a("h2"),x5=a("a"),Sse=a("span"),f(v3.$$.fragment),ZRo=l(),Pse=a("span"),eSo=o("AutoModelForAudioClassification"),HLe=l(),ar=a("div"),f(T3.$$.fragment),oSo=l(),Ad=a("p"),rSo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a audio classification head) when created
with the `),$se=a("code"),tSo=o("from_pretrained()"),aSo=o("class method or the "),Ise=a("code"),nSo=o("from_config()"),sSo=o(`class
method.`),lSo=l(),F3=a("p"),iSo=o("This class cannot be instantiated directly using "),jse=a("code"),dSo=o("__init__()"),cSo=o(" (throws an error)."),fSo=l(),Yr=a("div"),f(C3.$$.fragment),mSo=l(),Nse=a("p"),gSo=o("Instantiates one of the model classes of the library (with a audio classification head) from a configuration."),hSo=l(),Ld=a("p"),pSo=o(`Note:
Loading a model from its configuration file does `),Dse=a("strong"),_So=o("not"),uSo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),qse=a("code"),bSo=o("from_pretrained()"),vSo=o("to load the model weights."),TSo=l(),Gse=a("p"),FSo=o("Examples:"),CSo=l(),f(M3.$$.fragment),MSo=l(),Xe=a("div"),f(E3.$$.fragment),ESo=l(),Ose=a("p"),ySo=o("Instantiate one of the model classes of the library (with a audio classification head) from a pretrained model."),wSo=l(),Ka=a("p"),ASo=o("The model class to instantiate is selected based on the "),Xse=a("code"),LSo=o("model_type"),BSo=o(` property of the config object (either
passed as an argument or loaded from `),zse=a("code"),kSo=o("pretrained_model_name_or_path"),xSo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Vse=a("code"),RSo=o("pretrained_model_name_or_path"),SSo=o(":"),PSo=l(),ao=a("ul"),R5=a("li"),Wse=a("strong"),$So=o("hubert"),ISo=o(" \u2014 "),sN=a("a"),jSo=o("HubertForSequenceClassification"),NSo=o(" (Hubert model)"),DSo=l(),S5=a("li"),Qse=a("strong"),qSo=o("sew"),GSo=o(" \u2014 "),lN=a("a"),OSo=o("SEWForSequenceClassification"),XSo=o(" (SEW model)"),zSo=l(),P5=a("li"),Hse=a("strong"),VSo=o("sew-d"),WSo=o(" \u2014 "),iN=a("a"),QSo=o("SEWDForSequenceClassification"),HSo=o(" (SEW-D model)"),USo=l(),$5=a("li"),Use=a("strong"),JSo=o("unispeech"),YSo=o(" \u2014 "),dN=a("a"),KSo=o("UniSpeechForSequenceClassification"),ZSo=o(" (UniSpeech model)"),ePo=l(),I5=a("li"),Jse=a("strong"),oPo=o("unispeech-sat"),rPo=o(" \u2014 "),cN=a("a"),tPo=o("UniSpeechSatForSequenceClassification"),aPo=o(" (UniSpeechSat model)"),nPo=l(),j5=a("li"),Yse=a("strong"),sPo=o("wav2vec2"),lPo=o(" \u2014 "),fN=a("a"),iPo=o("Wav2Vec2ForSequenceClassification"),dPo=o(" (Wav2Vec2 model)"),cPo=l(),N5=a("li"),Kse=a("strong"),fPo=o("wavlm"),mPo=o(" \u2014 "),mN=a("a"),gPo=o("WavLMForSequenceClassification"),hPo=o(" (WavLM model)"),pPo=l(),D5=a("p"),_Po=o("The model is set in evaluation mode by default using "),Zse=a("code"),uPo=o("model.eval()"),bPo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),ele=a("code"),vPo=o("model.train()"),TPo=l(),ole=a("p"),FPo=o("Examples:"),CPo=l(),f(y3.$$.fragment),ULe=l(),Bd=a("h2"),q5=a("a"),rle=a("span"),f(w3.$$.fragment),MPo=l(),tle=a("span"),EPo=o("AutoModelForAudioFrameClassification"),JLe=l(),nr=a("div"),f(A3.$$.fragment),yPo=l(),kd=a("p"),wPo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a audio frame (token) classification head) when created
with the `),ale=a("code"),APo=o("from_pretrained()"),LPo=o("class method or the "),nle=a("code"),BPo=o("from_config()"),kPo=o(`class
method.`),xPo=l(),L3=a("p"),RPo=o("This class cannot be instantiated directly using "),sle=a("code"),SPo=o("__init__()"),PPo=o(" (throws an error)."),$Po=l(),Kr=a("div"),f(B3.$$.fragment),IPo=l(),lle=a("p"),jPo=o("Instantiates one of the model classes of the library (with a audio frame (token) classification head) from a configuration."),NPo=l(),xd=a("p"),DPo=o(`Note:
Loading a model from its configuration file does `),ile=a("strong"),qPo=o("not"),GPo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),dle=a("code"),OPo=o("from_pretrained()"),XPo=o("to load the model weights."),zPo=l(),cle=a("p"),VPo=o("Examples:"),WPo=l(),f(k3.$$.fragment),QPo=l(),ze=a("div"),f(x3.$$.fragment),HPo=l(),fle=a("p"),UPo=o("Instantiate one of the model classes of the library (with a audio frame (token) classification head) from a pretrained model."),JPo=l(),Za=a("p"),YPo=o("The model class to instantiate is selected based on the "),mle=a("code"),KPo=o("model_type"),ZPo=o(` property of the config object (either
passed as an argument or loaded from `),gle=a("code"),e$o=o("pretrained_model_name_or_path"),o$o=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),hle=a("code"),r$o=o("pretrained_model_name_or_path"),t$o=o(":"),a$o=l(),Rd=a("ul"),G5=a("li"),ple=a("strong"),n$o=o("unispeech-sat"),s$o=o(" \u2014 "),gN=a("a"),l$o=o("UniSpeechSatForAudioFrameClassification"),i$o=o(" (UniSpeechSat model)"),d$o=l(),O5=a("li"),_le=a("strong"),c$o=o("wav2vec2"),f$o=o(" \u2014 "),hN=a("a"),m$o=o("Wav2Vec2ForAudioFrameClassification"),g$o=o(" (Wav2Vec2 model)"),h$o=l(),X5=a("li"),ule=a("strong"),p$o=o("wavlm"),_$o=o(" \u2014 "),pN=a("a"),u$o=o("WavLMForAudioFrameClassification"),b$o=o(" (WavLM model)"),v$o=l(),z5=a("p"),T$o=o("The model is set in evaluation mode by default using "),ble=a("code"),F$o=o("model.eval()"),C$o=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),vle=a("code"),M$o=o("model.train()"),E$o=l(),Tle=a("p"),y$o=o("Examples:"),w$o=l(),f(R3.$$.fragment),YLe=l(),Sd=a("h2"),V5=a("a"),Fle=a("span"),f(S3.$$.fragment),A$o=l(),Cle=a("span"),L$o=o("AutoModelForCTC"),KLe=l(),sr=a("div"),f(P3.$$.fragment),B$o=l(),Pd=a("p"),k$o=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a connectionist temporal classification head) when created
with the `),Mle=a("code"),x$o=o("from_pretrained()"),R$o=o("class method or the "),Ele=a("code"),S$o=o("from_config()"),P$o=o(`class
method.`),$$o=l(),$3=a("p"),I$o=o("This class cannot be instantiated directly using "),yle=a("code"),j$o=o("__init__()"),N$o=o(" (throws an error)."),D$o=l(),Zr=a("div"),f(I3.$$.fragment),q$o=l(),wle=a("p"),G$o=o("Instantiates one of the model classes of the library (with a connectionist temporal classification head) from a configuration."),O$o=l(),$d=a("p"),X$o=o(`Note:
Loading a model from its configuration file does `),Ale=a("strong"),z$o=o("not"),V$o=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Lle=a("code"),W$o=o("from_pretrained()"),Q$o=o("to load the model weights."),H$o=l(),Ble=a("p"),U$o=o("Examples:"),J$o=l(),f(j3.$$.fragment),Y$o=l(),Ve=a("div"),f(N3.$$.fragment),K$o=l(),kle=a("p"),Z$o=o("Instantiate one of the model classes of the library (with a connectionist temporal classification head) from a pretrained model."),eIo=l(),en=a("p"),oIo=o("The model class to instantiate is selected based on the "),xle=a("code"),rIo=o("model_type"),tIo=o(` property of the config object (either
passed as an argument or loaded from `),Rle=a("code"),aIo=o("pretrained_model_name_or_path"),nIo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Sle=a("code"),sIo=o("pretrained_model_name_or_path"),lIo=o(":"),iIo=l(),no=a("ul"),W5=a("li"),Ple=a("strong"),dIo=o("hubert"),cIo=o(" \u2014 "),_N=a("a"),fIo=o("HubertForCTC"),mIo=o(" (Hubert model)"),gIo=l(),Q5=a("li"),$le=a("strong"),hIo=o("sew"),pIo=o(" \u2014 "),uN=a("a"),_Io=o("SEWForCTC"),uIo=o(" (SEW model)"),bIo=l(),H5=a("li"),Ile=a("strong"),vIo=o("sew-d"),TIo=o(" \u2014 "),bN=a("a"),FIo=o("SEWDForCTC"),CIo=o(" (SEW-D model)"),MIo=l(),U5=a("li"),jle=a("strong"),EIo=o("unispeech"),yIo=o(" \u2014 "),vN=a("a"),wIo=o("UniSpeechForCTC"),AIo=o(" (UniSpeech model)"),LIo=l(),J5=a("li"),Nle=a("strong"),BIo=o("unispeech-sat"),kIo=o(" \u2014 "),TN=a("a"),xIo=o("UniSpeechSatForCTC"),RIo=o(" (UniSpeechSat model)"),SIo=l(),Y5=a("li"),Dle=a("strong"),PIo=o("wav2vec2"),$Io=o(" \u2014 "),FN=a("a"),IIo=o("Wav2Vec2ForCTC"),jIo=o(" (Wav2Vec2 model)"),NIo=l(),K5=a("li"),qle=a("strong"),DIo=o("wavlm"),qIo=o(" \u2014 "),CN=a("a"),GIo=o("WavLMForCTC"),OIo=o(" (WavLM model)"),XIo=l(),Z5=a("p"),zIo=o("The model is set in evaluation mode by default using "),Gle=a("code"),VIo=o("model.eval()"),WIo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Ole=a("code"),QIo=o("model.train()"),HIo=l(),Xle=a("p"),UIo=o("Examples:"),JIo=l(),f(D3.$$.fragment),ZLe=l(),Id=a("h2"),ev=a("a"),zle=a("span"),f(q3.$$.fragment),YIo=l(),Vle=a("span"),KIo=o("AutoModelForSpeechSeq2Seq"),e8e=l(),lr=a("div"),f(G3.$$.fragment),ZIo=l(),jd=a("p"),ejo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) when created
with the `),Wle=a("code"),ojo=o("from_pretrained()"),rjo=o("class method or the "),Qle=a("code"),tjo=o("from_config()"),ajo=o(`class
method.`),njo=l(),O3=a("p"),sjo=o("This class cannot be instantiated directly using "),Hle=a("code"),ljo=o("__init__()"),ijo=o(" (throws an error)."),djo=l(),et=a("div"),f(X3.$$.fragment),cjo=l(),Ule=a("p"),fjo=o("Instantiates one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) from a configuration."),mjo=l(),Nd=a("p"),gjo=o(`Note:
Loading a model from its configuration file does `),Jle=a("strong"),hjo=o("not"),pjo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Yle=a("code"),_jo=o("from_pretrained()"),ujo=o("to load the model weights."),bjo=l(),Kle=a("p"),vjo=o("Examples:"),Tjo=l(),f(z3.$$.fragment),Fjo=l(),We=a("div"),f(V3.$$.fragment),Cjo=l(),Zle=a("p"),Mjo=o("Instantiate one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) from a pretrained model."),Ejo=l(),on=a("p"),yjo=o("The model class to instantiate is selected based on the "),eie=a("code"),wjo=o("model_type"),Ajo=o(` property of the config object (either
passed as an argument or loaded from `),oie=a("code"),Ljo=o("pretrained_model_name_or_path"),Bjo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),rie=a("code"),kjo=o("pretrained_model_name_or_path"),xjo=o(":"),Rjo=l(),W3=a("ul"),ov=a("li"),tie=a("strong"),Sjo=o("speech-encoder-decoder"),Pjo=o(" \u2014 "),MN=a("a"),$jo=o("SpeechEncoderDecoderModel"),Ijo=o(" (Speech Encoder decoder model)"),jjo=l(),rv=a("li"),aie=a("strong"),Njo=o("speech_to_text"),Djo=o(" \u2014 "),EN=a("a"),qjo=o("Speech2TextForConditionalGeneration"),Gjo=o(" (Speech2Text model)"),Ojo=l(),tv=a("p"),Xjo=o("The model is set in evaluation mode by default using "),nie=a("code"),zjo=o("model.eval()"),Vjo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),sie=a("code"),Wjo=o("model.train()"),Qjo=l(),lie=a("p"),Hjo=o("Examples:"),Ujo=l(),f(Q3.$$.fragment),o8e=l(),Dd=a("h2"),av=a("a"),iie=a("span"),f(H3.$$.fragment),Jjo=l(),die=a("span"),Yjo=o("AutoModelForAudioXVector"),r8e=l(),ir=a("div"),f(U3.$$.fragment),Kjo=l(),qd=a("p"),Zjo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a audio retrieval via x-vector head) when created
with the `),cie=a("code"),eNo=o("from_pretrained()"),oNo=o("class method or the "),fie=a("code"),rNo=o("from_config()"),tNo=o(`class
method.`),aNo=l(),J3=a("p"),nNo=o("This class cannot be instantiated directly using "),mie=a("code"),sNo=o("__init__()"),lNo=o(" (throws an error)."),iNo=l(),ot=a("div"),f(Y3.$$.fragment),dNo=l(),gie=a("p"),cNo=o("Instantiates one of the model classes of the library (with a audio retrieval via x-vector head) from a configuration."),fNo=l(),Gd=a("p"),mNo=o(`Note:
Loading a model from its configuration file does `),hie=a("strong"),gNo=o("not"),hNo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),pie=a("code"),pNo=o("from_pretrained()"),_No=o("to load the model weights."),uNo=l(),_ie=a("p"),bNo=o("Examples:"),vNo=l(),f(K3.$$.fragment),TNo=l(),Qe=a("div"),f(Z3.$$.fragment),FNo=l(),uie=a("p"),CNo=o("Instantiate one of the model classes of the library (with a audio retrieval via x-vector head) from a pretrained model."),MNo=l(),rn=a("p"),ENo=o("The model class to instantiate is selected based on the "),bie=a("code"),yNo=o("model_type"),wNo=o(` property of the config object (either
passed as an argument or loaded from `),vie=a("code"),ANo=o("pretrained_model_name_or_path"),LNo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Tie=a("code"),BNo=o("pretrained_model_name_or_path"),kNo=o(":"),xNo=l(),Od=a("ul"),nv=a("li"),Fie=a("strong"),RNo=o("unispeech-sat"),SNo=o(" \u2014 "),yN=a("a"),PNo=o("UniSpeechSatForXVector"),$No=o(" (UniSpeechSat model)"),INo=l(),sv=a("li"),Cie=a("strong"),jNo=o("wav2vec2"),NNo=o(" \u2014 "),wN=a("a"),DNo=o("Wav2Vec2ForXVector"),qNo=o(" (Wav2Vec2 model)"),GNo=l(),lv=a("li"),Mie=a("strong"),ONo=o("wavlm"),XNo=o(" \u2014 "),AN=a("a"),zNo=o("WavLMForXVector"),VNo=o(" (WavLM model)"),WNo=l(),iv=a("p"),QNo=o("The model is set in evaluation mode by default using "),Eie=a("code"),HNo=o("model.eval()"),UNo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),yie=a("code"),JNo=o("model.train()"),YNo=l(),wie=a("p"),KNo=o("Examples:"),ZNo=l(),f(ey.$$.fragment),t8e=l(),Xd=a("h2"),dv=a("a"),Aie=a("span"),f(oy.$$.fragment),eDo=l(),Lie=a("span"),oDo=o("AutoModelForMaskedImageModeling"),a8e=l(),dr=a("div"),f(ry.$$.fragment),rDo=l(),zd=a("p"),tDo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a masked image modeling head) when created
with the `),Bie=a("code"),aDo=o("from_pretrained()"),nDo=o("class method or the "),kie=a("code"),sDo=o("from_config()"),lDo=o(`class
method.`),iDo=l(),ty=a("p"),dDo=o("This class cannot be instantiated directly using "),xie=a("code"),cDo=o("__init__()"),fDo=o(" (throws an error)."),mDo=l(),rt=a("div"),f(ay.$$.fragment),gDo=l(),Rie=a("p"),hDo=o("Instantiates one of the model classes of the library (with a masked image modeling head) from a configuration."),pDo=l(),Vd=a("p"),_Do=o(`Note:
Loading a model from its configuration file does `),Sie=a("strong"),uDo=o("not"),bDo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Pie=a("code"),vDo=o("from_pretrained()"),TDo=o("to load the model weights."),FDo=l(),$ie=a("p"),CDo=o("Examples:"),MDo=l(),f(ny.$$.fragment),EDo=l(),He=a("div"),f(sy.$$.fragment),yDo=l(),Iie=a("p"),wDo=o("Instantiate one of the model classes of the library (with a masked image modeling head) from a pretrained model."),ADo=l(),tn=a("p"),LDo=o("The model class to instantiate is selected based on the "),jie=a("code"),BDo=o("model_type"),kDo=o(` property of the config object (either
passed as an argument or loaded from `),Nie=a("code"),xDo=o("pretrained_model_name_or_path"),RDo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Die=a("code"),SDo=o("pretrained_model_name_or_path"),PDo=o(":"),$Do=l(),Wd=a("ul"),cv=a("li"),qie=a("strong"),IDo=o("deit"),jDo=o(" \u2014 "),LN=a("a"),NDo=o("DeiTForMaskedImageModeling"),DDo=o(" (DeiT model)"),qDo=l(),fv=a("li"),Gie=a("strong"),GDo=o("swin"),ODo=o(" \u2014 "),BN=a("a"),XDo=o("SwinForMaskedImageModeling"),zDo=o(" (Swin model)"),VDo=l(),mv=a("li"),Oie=a("strong"),WDo=o("vit"),QDo=o(" \u2014 "),kN=a("a"),HDo=o("ViTForMaskedImageModeling"),UDo=o(" (ViT model)"),JDo=l(),gv=a("p"),YDo=o("The model is set in evaluation mode by default using "),Xie=a("code"),KDo=o("model.eval()"),ZDo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),zie=a("code"),eqo=o("model.train()"),oqo=l(),Vie=a("p"),rqo=o("Examples:"),tqo=l(),f(ly.$$.fragment),n8e=l(),Qd=a("h2"),hv=a("a"),Wie=a("span"),f(iy.$$.fragment),aqo=l(),Qie=a("span"),nqo=o("AutoModelForObjectDetection"),s8e=l(),cr=a("div"),f(dy.$$.fragment),sqo=l(),Hd=a("p"),lqo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a object detection head) when created
with the `),Hie=a("code"),iqo=o("from_pretrained()"),dqo=o("class method or the "),Uie=a("code"),cqo=o("from_config()"),fqo=o(`class
method.`),mqo=l(),cy=a("p"),gqo=o("This class cannot be instantiated directly using "),Jie=a("code"),hqo=o("__init__()"),pqo=o(" (throws an error)."),_qo=l(),tt=a("div"),f(fy.$$.fragment),uqo=l(),Yie=a("p"),bqo=o("Instantiates one of the model classes of the library (with a object detection head) from a configuration."),vqo=l(),Ud=a("p"),Tqo=o(`Note:
Loading a model from its configuration file does `),Kie=a("strong"),Fqo=o("not"),Cqo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Zie=a("code"),Mqo=o("from_pretrained()"),Eqo=o("to load the model weights."),yqo=l(),ede=a("p"),wqo=o("Examples:"),Aqo=l(),f(my.$$.fragment),Lqo=l(),Ue=a("div"),f(gy.$$.fragment),Bqo=l(),ode=a("p"),kqo=o("Instantiate one of the model classes of the library (with a object detection head) from a pretrained model."),xqo=l(),an=a("p"),Rqo=o("The model class to instantiate is selected based on the "),rde=a("code"),Sqo=o("model_type"),Pqo=o(` property of the config object (either
passed as an argument or loaded from `),tde=a("code"),$qo=o("pretrained_model_name_or_path"),Iqo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),ade=a("code"),jqo=o("pretrained_model_name_or_path"),Nqo=o(":"),Dqo=l(),nde=a("ul"),pv=a("li"),sde=a("strong"),qqo=o("detr"),Gqo=o(" \u2014 "),xN=a("a"),Oqo=o("DetrForObjectDetection"),Xqo=o(" (DETR model)"),zqo=l(),_v=a("p"),Vqo=o("The model is set in evaluation mode by default using "),lde=a("code"),Wqo=o("model.eval()"),Qqo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),ide=a("code"),Hqo=o("model.train()"),Uqo=l(),dde=a("p"),Jqo=o("Examples:"),Yqo=l(),f(hy.$$.fragment),l8e=l(),Jd=a("h2"),uv=a("a"),cde=a("span"),f(py.$$.fragment),Kqo=l(),fde=a("span"),Zqo=o("AutoModelForImageSegmentation"),i8e=l(),fr=a("div"),f(_y.$$.fragment),eGo=l(),Yd=a("p"),oGo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a image segmentation head) when created
with the `),mde=a("code"),rGo=o("from_pretrained()"),tGo=o("class method or the "),gde=a("code"),aGo=o("from_config()"),nGo=o(`class
method.`),sGo=l(),uy=a("p"),lGo=o("This class cannot be instantiated directly using "),hde=a("code"),iGo=o("__init__()"),dGo=o(" (throws an error)."),cGo=l(),at=a("div"),f(by.$$.fragment),fGo=l(),pde=a("p"),mGo=o("Instantiates one of the model classes of the library (with a image segmentation head) from a configuration."),gGo=l(),Kd=a("p"),hGo=o(`Note:
Loading a model from its configuration file does `),_de=a("strong"),pGo=o("not"),_Go=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),ude=a("code"),uGo=o("from_pretrained()"),bGo=o("to load the model weights."),vGo=l(),bde=a("p"),TGo=o("Examples:"),FGo=l(),f(vy.$$.fragment),CGo=l(),Je=a("div"),f(Ty.$$.fragment),MGo=l(),vde=a("p"),EGo=o("Instantiate one of the model classes of the library (with a image segmentation head) from a pretrained model."),yGo=l(),nn=a("p"),wGo=o("The model class to instantiate is selected based on the "),Tde=a("code"),AGo=o("model_type"),LGo=o(` property of the config object (either
passed as an argument or loaded from `),Fde=a("code"),BGo=o("pretrained_model_name_or_path"),kGo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Cde=a("code"),xGo=o("pretrained_model_name_or_path"),RGo=o(":"),SGo=l(),Mde=a("ul"),bv=a("li"),Ede=a("strong"),PGo=o("detr"),$Go=o(" \u2014 "),RN=a("a"),IGo=o("DetrForSegmentation"),jGo=o(" (DETR model)"),NGo=l(),vv=a("p"),DGo=o("The model is set in evaluation mode by default using "),yde=a("code"),qGo=o("model.eval()"),GGo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),wde=a("code"),OGo=o("model.train()"),XGo=l(),Ade=a("p"),zGo=o("Examples:"),VGo=l(),f(Fy.$$.fragment),d8e=l(),Zd=a("h2"),Tv=a("a"),Lde=a("span"),f(Cy.$$.fragment),WGo=l(),Bde=a("span"),QGo=o("AutoModelForSemanticSegmentation"),c8e=l(),mr=a("div"),f(My.$$.fragment),HGo=l(),ec=a("p"),UGo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a semantic segmentation head) when created
with the `),kde=a("code"),JGo=o("from_pretrained()"),YGo=o("class method or the "),xde=a("code"),KGo=o("from_config()"),ZGo=o(`class
method.`),eOo=l(),Ey=a("p"),oOo=o("This class cannot be instantiated directly using "),Rde=a("code"),rOo=o("__init__()"),tOo=o(" (throws an error)."),aOo=l(),nt=a("div"),f(yy.$$.fragment),nOo=l(),Sde=a("p"),sOo=o("Instantiates one of the model classes of the library (with a semantic segmentation head) from a configuration."),lOo=l(),oc=a("p"),iOo=o(`Note:
Loading a model from its configuration file does `),Pde=a("strong"),dOo=o("not"),cOo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),$de=a("code"),fOo=o("from_pretrained()"),mOo=o("to load the model weights."),gOo=l(),Ide=a("p"),hOo=o("Examples:"),pOo=l(),f(wy.$$.fragment),_Oo=l(),Ye=a("div"),f(Ay.$$.fragment),uOo=l(),jde=a("p"),bOo=o("Instantiate one of the model classes of the library (with a semantic segmentation head) from a pretrained model."),vOo=l(),sn=a("p"),TOo=o("The model class to instantiate is selected based on the "),Nde=a("code"),FOo=o("model_type"),COo=o(` property of the config object (either
passed as an argument or loaded from `),Dde=a("code"),MOo=o("pretrained_model_name_or_path"),EOo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),qde=a("code"),yOo=o("pretrained_model_name_or_path"),wOo=o(":"),AOo=l(),Ly=a("ul"),Fv=a("li"),Gde=a("strong"),LOo=o("beit"),BOo=o(" \u2014 "),SN=a("a"),kOo=o("BeitForSemanticSegmentation"),xOo=o(" (BEiT model)"),ROo=l(),Cv=a("li"),Ode=a("strong"),SOo=o("segformer"),POo=o(" \u2014 "),PN=a("a"),$Oo=o("SegformerForSemanticSegmentation"),IOo=o(" (SegFormer model)"),jOo=l(),Mv=a("p"),NOo=o("The model is set in evaluation mode by default using "),Xde=a("code"),DOo=o("model.eval()"),qOo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),zde=a("code"),GOo=o("model.train()"),OOo=l(),Vde=a("p"),XOo=o("Examples:"),zOo=l(),f(By.$$.fragment),f8e=l(),rc=a("h2"),Ev=a("a"),Wde=a("span"),f(ky.$$.fragment),VOo=l(),Qde=a("span"),WOo=o("TFAutoModel"),m8e=l(),gr=a("div"),f(xy.$$.fragment),QOo=l(),tc=a("p"),HOo=o(`This is a generic model class that will be instantiated as one of the base model classes of the library when created
with the `),Hde=a("code"),UOo=o("from_pretrained()"),JOo=o("class method or the "),Ude=a("code"),YOo=o("from_config()"),KOo=o(`class
method.`),ZOo=l(),Ry=a("p"),eXo=o("This class cannot be instantiated directly using "),Jde=a("code"),oXo=o("__init__()"),rXo=o(" (throws an error)."),tXo=l(),st=a("div"),f(Sy.$$.fragment),aXo=l(),Yde=a("p"),nXo=o("Instantiates one of the base model classes of the library from a configuration."),sXo=l(),ac=a("p"),lXo=o(`Note:
Loading a model from its configuration file does `),Kde=a("strong"),iXo=o("not"),dXo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Zde=a("code"),cXo=o("from_pretrained()"),fXo=o("to load the model weights."),mXo=l(),ece=a("p"),gXo=o("Examples:"),hXo=l(),f(Py.$$.fragment),pXo=l(),go=a("div"),f($y.$$.fragment),_Xo=l(),oce=a("p"),uXo=o("Instantiate one of the base model classes of the library from a pretrained model."),bXo=l(),ln=a("p"),vXo=o("The model class to instantiate is selected based on the "),rce=a("code"),TXo=o("model_type"),FXo=o(` property of the config object (either
passed as an argument or loaded from `),tce=a("code"),CXo=o("pretrained_model_name_or_path"),MXo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),ace=a("code"),EXo=o("pretrained_model_name_or_path"),yXo=o(":"),wXo=l(),B=a("ul"),yv=a("li"),nce=a("strong"),AXo=o("albert"),LXo=o(" \u2014 "),$N=a("a"),BXo=o("TFAlbertModel"),kXo=o(" (ALBERT model)"),xXo=l(),wv=a("li"),sce=a("strong"),RXo=o("bart"),SXo=o(" \u2014 "),IN=a("a"),PXo=o("TFBartModel"),$Xo=o(" (BART model)"),IXo=l(),Av=a("li"),lce=a("strong"),jXo=o("bert"),NXo=o(" \u2014 "),jN=a("a"),DXo=o("TFBertModel"),qXo=o(" (BERT model)"),GXo=l(),Lv=a("li"),ice=a("strong"),OXo=o("blenderbot"),XXo=o(" \u2014 "),NN=a("a"),zXo=o("TFBlenderbotModel"),VXo=o(" (Blenderbot model)"),WXo=l(),Bv=a("li"),dce=a("strong"),QXo=o("blenderbot-small"),HXo=o(" \u2014 "),DN=a("a"),UXo=o("TFBlenderbotSmallModel"),JXo=o(" (BlenderbotSmall model)"),YXo=l(),kv=a("li"),cce=a("strong"),KXo=o("camembert"),ZXo=o(" \u2014 "),qN=a("a"),ezo=o("TFCamembertModel"),ozo=o(" (CamemBERT model)"),rzo=l(),xv=a("li"),fce=a("strong"),tzo=o("clip"),azo=o(" \u2014 "),GN=a("a"),nzo=o("TFCLIPModel"),szo=o(" (CLIP model)"),lzo=l(),Rv=a("li"),mce=a("strong"),izo=o("convbert"),dzo=o(" \u2014 "),ON=a("a"),czo=o("TFConvBertModel"),fzo=o(" (ConvBERT model)"),mzo=l(),Sv=a("li"),gce=a("strong"),gzo=o("ctrl"),hzo=o(" \u2014 "),XN=a("a"),pzo=o("TFCTRLModel"),_zo=o(" (CTRL model)"),uzo=l(),Pv=a("li"),hce=a("strong"),bzo=o("deberta"),vzo=o(" \u2014 "),zN=a("a"),Tzo=o("TFDebertaModel"),Fzo=o(" (DeBERTa model)"),Czo=l(),$v=a("li"),pce=a("strong"),Mzo=o("deberta-v2"),Ezo=o(" \u2014 "),VN=a("a"),yzo=o("TFDebertaV2Model"),wzo=o(" (DeBERTa-v2 model)"),Azo=l(),Iv=a("li"),_ce=a("strong"),Lzo=o("distilbert"),Bzo=o(" \u2014 "),WN=a("a"),kzo=o("TFDistilBertModel"),xzo=o(" (DistilBERT model)"),Rzo=l(),jv=a("li"),uce=a("strong"),Szo=o("dpr"),Pzo=o(" \u2014 "),QN=a("a"),$zo=o("TFDPRQuestionEncoder"),Izo=o(" (DPR model)"),jzo=l(),Nv=a("li"),bce=a("strong"),Nzo=o("electra"),Dzo=o(" \u2014 "),HN=a("a"),qzo=o("TFElectraModel"),Gzo=o(" (ELECTRA model)"),Ozo=l(),Dv=a("li"),vce=a("strong"),Xzo=o("flaubert"),zzo=o(" \u2014 "),UN=a("a"),Vzo=o("TFFlaubertModel"),Wzo=o(" (FlauBERT model)"),Qzo=l(),Ps=a("li"),Tce=a("strong"),Hzo=o("funnel"),Uzo=o(" \u2014 "),JN=a("a"),Jzo=o("TFFunnelModel"),Yzo=o(" or "),YN=a("a"),Kzo=o("TFFunnelBaseModel"),Zzo=o(" (Funnel Transformer model)"),eVo=l(),qv=a("li"),Fce=a("strong"),oVo=o("gpt2"),rVo=o(" \u2014 "),KN=a("a"),tVo=o("TFGPT2Model"),aVo=o(" (OpenAI GPT-2 model)"),nVo=l(),Gv=a("li"),Cce=a("strong"),sVo=o("hubert"),lVo=o(" \u2014 "),ZN=a("a"),iVo=o("TFHubertModel"),dVo=o(" (Hubert model)"),cVo=l(),Ov=a("li"),Mce=a("strong"),fVo=o("layoutlm"),mVo=o(" \u2014 "),eD=a("a"),gVo=o("TFLayoutLMModel"),hVo=o(" (LayoutLM model)"),pVo=l(),Xv=a("li"),Ece=a("strong"),_Vo=o("led"),uVo=o(" \u2014 "),oD=a("a"),bVo=o("TFLEDModel"),vVo=o(" (LED model)"),TVo=l(),zv=a("li"),yce=a("strong"),FVo=o("longformer"),CVo=o(" \u2014 "),rD=a("a"),MVo=o("TFLongformerModel"),EVo=o(" (Longformer model)"),yVo=l(),Vv=a("li"),wce=a("strong"),wVo=o("lxmert"),AVo=o(" \u2014 "),tD=a("a"),LVo=o("TFLxmertModel"),BVo=o(" (LXMERT model)"),kVo=l(),Wv=a("li"),Ace=a("strong"),xVo=o("marian"),RVo=o(" \u2014 "),aD=a("a"),SVo=o("TFMarianModel"),PVo=o(" (Marian model)"),$Vo=l(),Qv=a("li"),Lce=a("strong"),IVo=o("mbart"),jVo=o(" \u2014 "),nD=a("a"),NVo=o("TFMBartModel"),DVo=o(" (mBART model)"),qVo=l(),Hv=a("li"),Bce=a("strong"),GVo=o("mobilebert"),OVo=o(" \u2014 "),sD=a("a"),XVo=o("TFMobileBertModel"),zVo=o(" (MobileBERT model)"),VVo=l(),Uv=a("li"),kce=a("strong"),WVo=o("mpnet"),QVo=o(" \u2014 "),lD=a("a"),HVo=o("TFMPNetModel"),UVo=o(" (MPNet model)"),JVo=l(),Jv=a("li"),xce=a("strong"),YVo=o("mt5"),KVo=o(" \u2014 "),iD=a("a"),ZVo=o("TFMT5Model"),eWo=o(" (mT5 model)"),oWo=l(),Yv=a("li"),Rce=a("strong"),rWo=o("openai-gpt"),tWo=o(" \u2014 "),dD=a("a"),aWo=o("TFOpenAIGPTModel"),nWo=o(" (OpenAI GPT model)"),sWo=l(),Kv=a("li"),Sce=a("strong"),lWo=o("pegasus"),iWo=o(" \u2014 "),cD=a("a"),dWo=o("TFPegasusModel"),cWo=o(" (Pegasus model)"),fWo=l(),Zv=a("li"),Pce=a("strong"),mWo=o("rembert"),gWo=o(" \u2014 "),fD=a("a"),hWo=o("TFRemBertModel"),pWo=o(" (RemBERT model)"),_Wo=l(),eT=a("li"),$ce=a("strong"),uWo=o("roberta"),bWo=o(" \u2014 "),mD=a("a"),vWo=o("TFRobertaModel"),TWo=o(" (RoBERTa model)"),FWo=l(),oT=a("li"),Ice=a("strong"),CWo=o("roformer"),MWo=o(" \u2014 "),gD=a("a"),EWo=o("TFRoFormerModel"),yWo=o(" (RoFormer model)"),wWo=l(),rT=a("li"),jce=a("strong"),AWo=o("speech_to_text"),LWo=o(" \u2014 "),hD=a("a"),BWo=o("TFSpeech2TextModel"),kWo=o(" (Speech2Text model)"),xWo=l(),tT=a("li"),Nce=a("strong"),RWo=o("t5"),SWo=o(" \u2014 "),pD=a("a"),PWo=o("TFT5Model"),$Wo=o(" (T5 model)"),IWo=l(),aT=a("li"),Dce=a("strong"),jWo=o("tapas"),NWo=o(" \u2014 "),_D=a("a"),DWo=o("TFTapasModel"),qWo=o(" (TAPAS model)"),GWo=l(),nT=a("li"),qce=a("strong"),OWo=o("transfo-xl"),XWo=o(" \u2014 "),uD=a("a"),zWo=o("TFTransfoXLModel"),VWo=o(" (Transformer-XL model)"),WWo=l(),sT=a("li"),Gce=a("strong"),QWo=o("vit"),HWo=o(" \u2014 "),bD=a("a"),UWo=o("TFViTModel"),JWo=o(" (ViT model)"),YWo=l(),lT=a("li"),Oce=a("strong"),KWo=o("wav2vec2"),ZWo=o(" \u2014 "),vD=a("a"),eQo=o("TFWav2Vec2Model"),oQo=o(" (Wav2Vec2 model)"),rQo=l(),iT=a("li"),Xce=a("strong"),tQo=o("xlm"),aQo=o(" \u2014 "),TD=a("a"),nQo=o("TFXLMModel"),sQo=o(" (XLM model)"),lQo=l(),dT=a("li"),zce=a("strong"),iQo=o("xlm-roberta"),dQo=o(" \u2014 "),FD=a("a"),cQo=o("TFXLMRobertaModel"),fQo=o(" (XLM-RoBERTa model)"),mQo=l(),cT=a("li"),Vce=a("strong"),gQo=o("xlnet"),hQo=o(" \u2014 "),CD=a("a"),pQo=o("TFXLNetModel"),_Qo=o(" (XLNet model)"),uQo=l(),Wce=a("p"),bQo=o("Examples:"),vQo=l(),f(Iy.$$.fragment),g8e=l(),nc=a("h2"),fT=a("a"),Qce=a("span"),f(jy.$$.fragment),TQo=l(),Hce=a("span"),FQo=o("TFAutoModelForPreTraining"),h8e=l(),hr=a("div"),f(Ny.$$.fragment),CQo=l(),sc=a("p"),MQo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a pretraining head) when created
with the `),Uce=a("code"),EQo=o("from_pretrained()"),yQo=o("class method or the "),Jce=a("code"),wQo=o("from_config()"),AQo=o(`class
method.`),LQo=l(),Dy=a("p"),BQo=o("This class cannot be instantiated directly using "),Yce=a("code"),kQo=o("__init__()"),xQo=o(" (throws an error)."),RQo=l(),lt=a("div"),f(qy.$$.fragment),SQo=l(),Kce=a("p"),PQo=o("Instantiates one of the model classes of the library (with a pretraining head) from a configuration."),$Qo=l(),lc=a("p"),IQo=o(`Note:
Loading a model from its configuration file does `),Zce=a("strong"),jQo=o("not"),NQo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),efe=a("code"),DQo=o("from_pretrained()"),qQo=o("to load the model weights."),GQo=l(),ofe=a("p"),OQo=o("Examples:"),XQo=l(),f(Gy.$$.fragment),zQo=l(),ho=a("div"),f(Oy.$$.fragment),VQo=l(),rfe=a("p"),WQo=o("Instantiate one of the model classes of the library (with a pretraining head) from a pretrained model."),QQo=l(),dn=a("p"),HQo=o("The model class to instantiate is selected based on the "),tfe=a("code"),UQo=o("model_type"),JQo=o(` property of the config object (either
passed as an argument or loaded from `),afe=a("code"),YQo=o("pretrained_model_name_or_path"),KQo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),nfe=a("code"),ZQo=o("pretrained_model_name_or_path"),eHo=o(":"),oHo=l(),H=a("ul"),mT=a("li"),sfe=a("strong"),rHo=o("albert"),tHo=o(" \u2014 "),MD=a("a"),aHo=o("TFAlbertForPreTraining"),nHo=o(" (ALBERT model)"),sHo=l(),gT=a("li"),lfe=a("strong"),lHo=o("bart"),iHo=o(" \u2014 "),ED=a("a"),dHo=o("TFBartForConditionalGeneration"),cHo=o(" (BART model)"),fHo=l(),hT=a("li"),ife=a("strong"),mHo=o("bert"),gHo=o(" \u2014 "),yD=a("a"),hHo=o("TFBertForPreTraining"),pHo=o(" (BERT model)"),_Ho=l(),pT=a("li"),dfe=a("strong"),uHo=o("camembert"),bHo=o(" \u2014 "),wD=a("a"),vHo=o("TFCamembertForMaskedLM"),THo=o(" (CamemBERT model)"),FHo=l(),_T=a("li"),cfe=a("strong"),CHo=o("ctrl"),MHo=o(" \u2014 "),AD=a("a"),EHo=o("TFCTRLLMHeadModel"),yHo=o(" (CTRL model)"),wHo=l(),uT=a("li"),ffe=a("strong"),AHo=o("distilbert"),LHo=o(" \u2014 "),LD=a("a"),BHo=o("TFDistilBertForMaskedLM"),kHo=o(" (DistilBERT model)"),xHo=l(),bT=a("li"),mfe=a("strong"),RHo=o("electra"),SHo=o(" \u2014 "),BD=a("a"),PHo=o("TFElectraForPreTraining"),$Ho=o(" (ELECTRA model)"),IHo=l(),vT=a("li"),gfe=a("strong"),jHo=o("flaubert"),NHo=o(" \u2014 "),kD=a("a"),DHo=o("TFFlaubertWithLMHeadModel"),qHo=o(" (FlauBERT model)"),GHo=l(),TT=a("li"),hfe=a("strong"),OHo=o("funnel"),XHo=o(" \u2014 "),xD=a("a"),zHo=o("TFFunnelForPreTraining"),VHo=o(" (Funnel Transformer model)"),WHo=l(),FT=a("li"),pfe=a("strong"),QHo=o("gpt2"),HHo=o(" \u2014 "),RD=a("a"),UHo=o("TFGPT2LMHeadModel"),JHo=o(" (OpenAI GPT-2 model)"),YHo=l(),CT=a("li"),_fe=a("strong"),KHo=o("layoutlm"),ZHo=o(" \u2014 "),SD=a("a"),eUo=o("TFLayoutLMForMaskedLM"),oUo=o(" (LayoutLM model)"),rUo=l(),MT=a("li"),ufe=a("strong"),tUo=o("lxmert"),aUo=o(" \u2014 "),PD=a("a"),nUo=o("TFLxmertForPreTraining"),sUo=o(" (LXMERT model)"),lUo=l(),ET=a("li"),bfe=a("strong"),iUo=o("mobilebert"),dUo=o(" \u2014 "),$D=a("a"),cUo=o("TFMobileBertForPreTraining"),fUo=o(" (MobileBERT model)"),mUo=l(),yT=a("li"),vfe=a("strong"),gUo=o("mpnet"),hUo=o(" \u2014 "),ID=a("a"),pUo=o("TFMPNetForMaskedLM"),_Uo=o(" (MPNet model)"),uUo=l(),wT=a("li"),Tfe=a("strong"),bUo=o("openai-gpt"),vUo=o(" \u2014 "),jD=a("a"),TUo=o("TFOpenAIGPTLMHeadModel"),FUo=o(" (OpenAI GPT model)"),CUo=l(),AT=a("li"),Ffe=a("strong"),MUo=o("roberta"),EUo=o(" \u2014 "),ND=a("a"),yUo=o("TFRobertaForMaskedLM"),wUo=o(" (RoBERTa model)"),AUo=l(),LT=a("li"),Cfe=a("strong"),LUo=o("t5"),BUo=o(" \u2014 "),DD=a("a"),kUo=o("TFT5ForConditionalGeneration"),xUo=o(" (T5 model)"),RUo=l(),BT=a("li"),Mfe=a("strong"),SUo=o("tapas"),PUo=o(" \u2014 "),qD=a("a"),$Uo=o("TFTapasForMaskedLM"),IUo=o(" (TAPAS model)"),jUo=l(),kT=a("li"),Efe=a("strong"),NUo=o("transfo-xl"),DUo=o(" \u2014 "),GD=a("a"),qUo=o("TFTransfoXLLMHeadModel"),GUo=o(" (Transformer-XL model)"),OUo=l(),xT=a("li"),yfe=a("strong"),XUo=o("xlm"),zUo=o(" \u2014 "),OD=a("a"),VUo=o("TFXLMWithLMHeadModel"),WUo=o(" (XLM model)"),QUo=l(),RT=a("li"),wfe=a("strong"),HUo=o("xlm-roberta"),UUo=o(" \u2014 "),XD=a("a"),JUo=o("TFXLMRobertaForMaskedLM"),YUo=o(" (XLM-RoBERTa model)"),KUo=l(),ST=a("li"),Afe=a("strong"),ZUo=o("xlnet"),eJo=o(" \u2014 "),zD=a("a"),oJo=o("TFXLNetLMHeadModel"),rJo=o(" (XLNet model)"),tJo=l(),Lfe=a("p"),aJo=o("Examples:"),nJo=l(),f(Xy.$$.fragment),p8e=l(),ic=a("h2"),PT=a("a"),Bfe=a("span"),f(zy.$$.fragment),sJo=l(),kfe=a("span"),lJo=o("TFAutoModelForCausalLM"),_8e=l(),pr=a("div"),f(Vy.$$.fragment),iJo=l(),dc=a("p"),dJo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a causal language modeling head) when created
with the `),xfe=a("code"),cJo=o("from_pretrained()"),fJo=o("class method or the "),Rfe=a("code"),mJo=o("from_config()"),gJo=o(`class
method.`),hJo=l(),Wy=a("p"),pJo=o("This class cannot be instantiated directly using "),Sfe=a("code"),_Jo=o("__init__()"),uJo=o(" (throws an error)."),bJo=l(),it=a("div"),f(Qy.$$.fragment),vJo=l(),Pfe=a("p"),TJo=o("Instantiates one of the model classes of the library (with a causal language modeling head) from a configuration."),FJo=l(),cc=a("p"),CJo=o(`Note:
Loading a model from its configuration file does `),$fe=a("strong"),MJo=o("not"),EJo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Ife=a("code"),yJo=o("from_pretrained()"),wJo=o("to load the model weights."),AJo=l(),jfe=a("p"),LJo=o("Examples:"),BJo=l(),f(Hy.$$.fragment),kJo=l(),po=a("div"),f(Uy.$$.fragment),xJo=l(),Nfe=a("p"),RJo=o("Instantiate one of the model classes of the library (with a causal language modeling head) from a pretrained model."),SJo=l(),cn=a("p"),PJo=o("The model class to instantiate is selected based on the "),Dfe=a("code"),$Jo=o("model_type"),IJo=o(` property of the config object (either
passed as an argument or loaded from `),qfe=a("code"),jJo=o("pretrained_model_name_or_path"),NJo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Gfe=a("code"),DJo=o("pretrained_model_name_or_path"),qJo=o(":"),GJo=l(),he=a("ul"),$T=a("li"),Ofe=a("strong"),OJo=o("bert"),XJo=o(" \u2014 "),VD=a("a"),zJo=o("TFBertLMHeadModel"),VJo=o(" (BERT model)"),WJo=l(),IT=a("li"),Xfe=a("strong"),QJo=o("ctrl"),HJo=o(" \u2014 "),WD=a("a"),UJo=o("TFCTRLLMHeadModel"),JJo=o(" (CTRL model)"),YJo=l(),jT=a("li"),zfe=a("strong"),KJo=o("gpt2"),ZJo=o(" \u2014 "),QD=a("a"),eYo=o("TFGPT2LMHeadModel"),oYo=o(" (OpenAI GPT-2 model)"),rYo=l(),NT=a("li"),Vfe=a("strong"),tYo=o("openai-gpt"),aYo=o(" \u2014 "),HD=a("a"),nYo=o("TFOpenAIGPTLMHeadModel"),sYo=o(" (OpenAI GPT model)"),lYo=l(),DT=a("li"),Wfe=a("strong"),iYo=o("rembert"),dYo=o(" \u2014 "),UD=a("a"),cYo=o("TFRemBertForCausalLM"),fYo=o(" (RemBERT model)"),mYo=l(),qT=a("li"),Qfe=a("strong"),gYo=o("roberta"),hYo=o(" \u2014 "),JD=a("a"),pYo=o("TFRobertaForCausalLM"),_Yo=o(" (RoBERTa model)"),uYo=l(),GT=a("li"),Hfe=a("strong"),bYo=o("roformer"),vYo=o(" \u2014 "),YD=a("a"),TYo=o("TFRoFormerForCausalLM"),FYo=o(" (RoFormer model)"),CYo=l(),OT=a("li"),Ufe=a("strong"),MYo=o("transfo-xl"),EYo=o(" \u2014 "),KD=a("a"),yYo=o("TFTransfoXLLMHeadModel"),wYo=o(" (Transformer-XL model)"),AYo=l(),XT=a("li"),Jfe=a("strong"),LYo=o("xlm"),BYo=o(" \u2014 "),ZD=a("a"),kYo=o("TFXLMWithLMHeadModel"),xYo=o(" (XLM model)"),RYo=l(),zT=a("li"),Yfe=a("strong"),SYo=o("xlnet"),PYo=o(" \u2014 "),eq=a("a"),$Yo=o("TFXLNetLMHeadModel"),IYo=o(" (XLNet model)"),jYo=l(),Kfe=a("p"),NYo=o("Examples:"),DYo=l(),f(Jy.$$.fragment),u8e=l(),fc=a("h2"),VT=a("a"),Zfe=a("span"),f(Yy.$$.fragment),qYo=l(),eme=a("span"),GYo=o("TFAutoModelForImageClassification"),b8e=l(),_r=a("div"),f(Ky.$$.fragment),OYo=l(),mc=a("p"),XYo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a image classification head) when created
with the `),ome=a("code"),zYo=o("from_pretrained()"),VYo=o("class method or the "),rme=a("code"),WYo=o("from_config()"),QYo=o(`class
method.`),HYo=l(),Zy=a("p"),UYo=o("This class cannot be instantiated directly using "),tme=a("code"),JYo=o("__init__()"),YYo=o(" (throws an error)."),KYo=l(),dt=a("div"),f(ew.$$.fragment),ZYo=l(),ame=a("p"),eKo=o("Instantiates one of the model classes of the library (with a image classification head) from a configuration."),oKo=l(),gc=a("p"),rKo=o(`Note:
Loading a model from its configuration file does `),nme=a("strong"),tKo=o("not"),aKo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),sme=a("code"),nKo=o("from_pretrained()"),sKo=o("to load the model weights."),lKo=l(),lme=a("p"),iKo=o("Examples:"),dKo=l(),f(ow.$$.fragment),cKo=l(),_o=a("div"),f(rw.$$.fragment),fKo=l(),ime=a("p"),mKo=o("Instantiate one of the model classes of the library (with a image classification head) from a pretrained model."),gKo=l(),fn=a("p"),hKo=o("The model class to instantiate is selected based on the "),dme=a("code"),pKo=o("model_type"),_Ko=o(` property of the config object (either
passed as an argument or loaded from `),cme=a("code"),uKo=o("pretrained_model_name_or_path"),bKo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),fme=a("code"),vKo=o("pretrained_model_name_or_path"),TKo=o(":"),FKo=l(),mme=a("ul"),WT=a("li"),gme=a("strong"),CKo=o("vit"),MKo=o(" \u2014 "),oq=a("a"),EKo=o("TFViTForImageClassification"),yKo=o(" (ViT model)"),wKo=l(),hme=a("p"),AKo=o("Examples:"),LKo=l(),f(tw.$$.fragment),v8e=l(),hc=a("h2"),QT=a("a"),pme=a("span"),f(aw.$$.fragment),BKo=l(),_me=a("span"),kKo=o("TFAutoModelForMaskedLM"),T8e=l(),ur=a("div"),f(nw.$$.fragment),xKo=l(),pc=a("p"),RKo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a masked language modeling head) when created
with the `),ume=a("code"),SKo=o("from_pretrained()"),PKo=o("class method or the "),bme=a("code"),$Ko=o("from_config()"),IKo=o(`class
method.`),jKo=l(),sw=a("p"),NKo=o("This class cannot be instantiated directly using "),vme=a("code"),DKo=o("__init__()"),qKo=o(" (throws an error)."),GKo=l(),ct=a("div"),f(lw.$$.fragment),OKo=l(),Tme=a("p"),XKo=o("Instantiates one of the model classes of the library (with a masked language modeling head) from a configuration."),zKo=l(),_c=a("p"),VKo=o(`Note:
Loading a model from its configuration file does `),Fme=a("strong"),WKo=o("not"),QKo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Cme=a("code"),HKo=o("from_pretrained()"),UKo=o("to load the model weights."),JKo=l(),Mme=a("p"),YKo=o("Examples:"),KKo=l(),f(iw.$$.fragment),ZKo=l(),uo=a("div"),f(dw.$$.fragment),eZo=l(),Eme=a("p"),oZo=o("Instantiate one of the model classes of the library (with a masked language modeling head) from a pretrained model."),rZo=l(),mn=a("p"),tZo=o("The model class to instantiate is selected based on the "),yme=a("code"),aZo=o("model_type"),nZo=o(` property of the config object (either
passed as an argument or loaded from `),wme=a("code"),sZo=o("pretrained_model_name_or_path"),lZo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Ame=a("code"),iZo=o("pretrained_model_name_or_path"),dZo=o(":"),cZo=l(),Y=a("ul"),HT=a("li"),Lme=a("strong"),fZo=o("albert"),mZo=o(" \u2014 "),rq=a("a"),gZo=o("TFAlbertForMaskedLM"),hZo=o(" (ALBERT model)"),pZo=l(),UT=a("li"),Bme=a("strong"),_Zo=o("bert"),uZo=o(" \u2014 "),tq=a("a"),bZo=o("TFBertForMaskedLM"),vZo=o(" (BERT model)"),TZo=l(),JT=a("li"),kme=a("strong"),FZo=o("camembert"),CZo=o(" \u2014 "),aq=a("a"),MZo=o("TFCamembertForMaskedLM"),EZo=o(" (CamemBERT model)"),yZo=l(),YT=a("li"),xme=a("strong"),wZo=o("convbert"),AZo=o(" \u2014 "),nq=a("a"),LZo=o("TFConvBertForMaskedLM"),BZo=o(" (ConvBERT model)"),kZo=l(),KT=a("li"),Rme=a("strong"),xZo=o("deberta"),RZo=o(" \u2014 "),sq=a("a"),SZo=o("TFDebertaForMaskedLM"),PZo=o(" (DeBERTa model)"),$Zo=l(),ZT=a("li"),Sme=a("strong"),IZo=o("deberta-v2"),jZo=o(" \u2014 "),lq=a("a"),NZo=o("TFDebertaV2ForMaskedLM"),DZo=o(" (DeBERTa-v2 model)"),qZo=l(),e7=a("li"),Pme=a("strong"),GZo=o("distilbert"),OZo=o(" \u2014 "),iq=a("a"),XZo=o("TFDistilBertForMaskedLM"),zZo=o(" (DistilBERT model)"),VZo=l(),o7=a("li"),$me=a("strong"),WZo=o("electra"),QZo=o(" \u2014 "),dq=a("a"),HZo=o("TFElectraForMaskedLM"),UZo=o(" (ELECTRA model)"),JZo=l(),r7=a("li"),Ime=a("strong"),YZo=o("flaubert"),KZo=o(" \u2014 "),cq=a("a"),ZZo=o("TFFlaubertWithLMHeadModel"),eer=o(" (FlauBERT model)"),oer=l(),t7=a("li"),jme=a("strong"),rer=o("funnel"),ter=o(" \u2014 "),fq=a("a"),aer=o("TFFunnelForMaskedLM"),ner=o(" (Funnel Transformer model)"),ser=l(),a7=a("li"),Nme=a("strong"),ler=o("layoutlm"),ier=o(" \u2014 "),mq=a("a"),der=o("TFLayoutLMForMaskedLM"),cer=o(" (LayoutLM model)"),fer=l(),n7=a("li"),Dme=a("strong"),mer=o("longformer"),ger=o(" \u2014 "),gq=a("a"),her=o("TFLongformerForMaskedLM"),per=o(" (Longformer model)"),_er=l(),s7=a("li"),qme=a("strong"),uer=o("mobilebert"),ber=o(" \u2014 "),hq=a("a"),ver=o("TFMobileBertForMaskedLM"),Ter=o(" (MobileBERT model)"),Fer=l(),l7=a("li"),Gme=a("strong"),Cer=o("mpnet"),Mer=o(" \u2014 "),pq=a("a"),Eer=o("TFMPNetForMaskedLM"),yer=o(" (MPNet model)"),wer=l(),i7=a("li"),Ome=a("strong"),Aer=o("rembert"),Ler=o(" \u2014 "),_q=a("a"),Ber=o("TFRemBertForMaskedLM"),ker=o(" (RemBERT model)"),xer=l(),d7=a("li"),Xme=a("strong"),Rer=o("roberta"),Ser=o(" \u2014 "),uq=a("a"),Per=o("TFRobertaForMaskedLM"),$er=o(" (RoBERTa model)"),Ier=l(),c7=a("li"),zme=a("strong"),jer=o("roformer"),Ner=o(" \u2014 "),bq=a("a"),Der=o("TFRoFormerForMaskedLM"),qer=o(" (RoFormer model)"),Ger=l(),f7=a("li"),Vme=a("strong"),Oer=o("tapas"),Xer=o(" \u2014 "),vq=a("a"),zer=o("TFTapasForMaskedLM"),Ver=o(" (TAPAS model)"),Wer=l(),m7=a("li"),Wme=a("strong"),Qer=o("xlm"),Her=o(" \u2014 "),Tq=a("a"),Uer=o("TFXLMWithLMHeadModel"),Jer=o(" (XLM model)"),Yer=l(),g7=a("li"),Qme=a("strong"),Ker=o("xlm-roberta"),Zer=o(" \u2014 "),Fq=a("a"),eor=o("TFXLMRobertaForMaskedLM"),oor=o(" (XLM-RoBERTa model)"),ror=l(),Hme=a("p"),tor=o("Examples:"),aor=l(),f(cw.$$.fragment),F8e=l(),uc=a("h2"),h7=a("a"),Ume=a("span"),f(fw.$$.fragment),nor=l(),Jme=a("span"),sor=o("TFAutoModelForSeq2SeqLM"),C8e=l(),br=a("div"),f(mw.$$.fragment),lor=l(),bc=a("p"),ior=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence language modeling head) when created
with the `),Yme=a("code"),dor=o("from_pretrained()"),cor=o("class method or the "),Kme=a("code"),mor=o("from_config()"),gor=o(`class
method.`),hor=l(),gw=a("p"),por=o("This class cannot be instantiated directly using "),Zme=a("code"),_or=o("__init__()"),uor=o(" (throws an error)."),bor=l(),ft=a("div"),f(hw.$$.fragment),vor=l(),ege=a("p"),Tor=o("Instantiates one of the model classes of the library (with a sequence-to-sequence language modeling head) from a configuration."),For=l(),vc=a("p"),Cor=o(`Note:
Loading a model from its configuration file does `),oge=a("strong"),Mor=o("not"),Eor=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),rge=a("code"),yor=o("from_pretrained()"),wor=o("to load the model weights."),Aor=l(),tge=a("p"),Lor=o("Examples:"),Bor=l(),f(pw.$$.fragment),kor=l(),bo=a("div"),f(_w.$$.fragment),xor=l(),age=a("p"),Ror=o("Instantiate one of the model classes of the library (with a sequence-to-sequence language modeling head) from a pretrained model."),Sor=l(),gn=a("p"),Por=o("The model class to instantiate is selected based on the "),nge=a("code"),$or=o("model_type"),Ior=o(` property of the config object (either
passed as an argument or loaded from `),sge=a("code"),jor=o("pretrained_model_name_or_path"),Nor=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),lge=a("code"),Dor=o("pretrained_model_name_or_path"),qor=o(":"),Gor=l(),pe=a("ul"),p7=a("li"),ige=a("strong"),Oor=o("bart"),Xor=o(" \u2014 "),Cq=a("a"),zor=o("TFBartForConditionalGeneration"),Vor=o(" (BART model)"),Wor=l(),_7=a("li"),dge=a("strong"),Qor=o("blenderbot"),Hor=o(" \u2014 "),Mq=a("a"),Uor=o("TFBlenderbotForConditionalGeneration"),Jor=o(" (Blenderbot model)"),Yor=l(),u7=a("li"),cge=a("strong"),Kor=o("blenderbot-small"),Zor=o(" \u2014 "),Eq=a("a"),err=o("TFBlenderbotSmallForConditionalGeneration"),orr=o(" (BlenderbotSmall model)"),rrr=l(),b7=a("li"),fge=a("strong"),trr=o("encoder-decoder"),arr=o(" \u2014 "),yq=a("a"),nrr=o("TFEncoderDecoderModel"),srr=o(" (Encoder decoder model)"),lrr=l(),v7=a("li"),mge=a("strong"),irr=o("led"),drr=o(" \u2014 "),wq=a("a"),crr=o("TFLEDForConditionalGeneration"),frr=o(" (LED model)"),mrr=l(),T7=a("li"),gge=a("strong"),grr=o("marian"),hrr=o(" \u2014 "),Aq=a("a"),prr=o("TFMarianMTModel"),_rr=o(" (Marian model)"),urr=l(),F7=a("li"),hge=a("strong"),brr=o("mbart"),vrr=o(" \u2014 "),Lq=a("a"),Trr=o("TFMBartForConditionalGeneration"),Frr=o(" (mBART model)"),Crr=l(),C7=a("li"),pge=a("strong"),Mrr=o("mt5"),Err=o(" \u2014 "),Bq=a("a"),yrr=o("TFMT5ForConditionalGeneration"),wrr=o(" (mT5 model)"),Arr=l(),M7=a("li"),_ge=a("strong"),Lrr=o("pegasus"),Brr=o(" \u2014 "),kq=a("a"),krr=o("TFPegasusForConditionalGeneration"),xrr=o(" (Pegasus model)"),Rrr=l(),E7=a("li"),uge=a("strong"),Srr=o("t5"),Prr=o(" \u2014 "),xq=a("a"),$rr=o("TFT5ForConditionalGeneration"),Irr=o(" (T5 model)"),jrr=l(),bge=a("p"),Nrr=o("Examples:"),Drr=l(),f(uw.$$.fragment),M8e=l(),Tc=a("h2"),y7=a("a"),vge=a("span"),f(bw.$$.fragment),qrr=l(),Tge=a("span"),Grr=o("TFAutoModelForSequenceClassification"),E8e=l(),vr=a("div"),f(vw.$$.fragment),Orr=l(),Fc=a("p"),Xrr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence classification head) when created
with the `),Fge=a("code"),zrr=o("from_pretrained()"),Vrr=o("class method or the "),Cge=a("code"),Wrr=o("from_config()"),Qrr=o(`class
method.`),Hrr=l(),Tw=a("p"),Urr=o("This class cannot be instantiated directly using "),Mge=a("code"),Jrr=o("__init__()"),Yrr=o(" (throws an error)."),Krr=l(),mt=a("div"),f(Fw.$$.fragment),Zrr=l(),Ege=a("p"),etr=o("Instantiates one of the model classes of the library (with a sequence classification head) from a configuration."),otr=l(),Cc=a("p"),rtr=o(`Note:
Loading a model from its configuration file does `),yge=a("strong"),ttr=o("not"),atr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),wge=a("code"),ntr=o("from_pretrained()"),str=o("to load the model weights."),ltr=l(),Age=a("p"),itr=o("Examples:"),dtr=l(),f(Cw.$$.fragment),ctr=l(),vo=a("div"),f(Mw.$$.fragment),ftr=l(),Lge=a("p"),mtr=o("Instantiate one of the model classes of the library (with a sequence classification head) from a pretrained model."),gtr=l(),hn=a("p"),htr=o("The model class to instantiate is selected based on the "),Bge=a("code"),ptr=o("model_type"),_tr=o(` property of the config object (either
passed as an argument or loaded from `),kge=a("code"),utr=o("pretrained_model_name_or_path"),btr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),xge=a("code"),vtr=o("pretrained_model_name_or_path"),Ttr=o(":"),Ftr=l(),X=a("ul"),w7=a("li"),Rge=a("strong"),Ctr=o("albert"),Mtr=o(" \u2014 "),Rq=a("a"),Etr=o("TFAlbertForSequenceClassification"),ytr=o(" (ALBERT model)"),wtr=l(),A7=a("li"),Sge=a("strong"),Atr=o("bert"),Ltr=o(" \u2014 "),Sq=a("a"),Btr=o("TFBertForSequenceClassification"),ktr=o(" (BERT model)"),xtr=l(),L7=a("li"),Pge=a("strong"),Rtr=o("camembert"),Str=o(" \u2014 "),Pq=a("a"),Ptr=o("TFCamembertForSequenceClassification"),$tr=o(" (CamemBERT model)"),Itr=l(),B7=a("li"),$ge=a("strong"),jtr=o("convbert"),Ntr=o(" \u2014 "),$q=a("a"),Dtr=o("TFConvBertForSequenceClassification"),qtr=o(" (ConvBERT model)"),Gtr=l(),k7=a("li"),Ige=a("strong"),Otr=o("ctrl"),Xtr=o(" \u2014 "),Iq=a("a"),ztr=o("TFCTRLForSequenceClassification"),Vtr=o(" (CTRL model)"),Wtr=l(),x7=a("li"),jge=a("strong"),Qtr=o("deberta"),Htr=o(" \u2014 "),jq=a("a"),Utr=o("TFDebertaForSequenceClassification"),Jtr=o(" (DeBERTa model)"),Ytr=l(),R7=a("li"),Nge=a("strong"),Ktr=o("deberta-v2"),Ztr=o(" \u2014 "),Nq=a("a"),ear=o("TFDebertaV2ForSequenceClassification"),oar=o(" (DeBERTa-v2 model)"),rar=l(),S7=a("li"),Dge=a("strong"),tar=o("distilbert"),aar=o(" \u2014 "),Dq=a("a"),nar=o("TFDistilBertForSequenceClassification"),sar=o(" (DistilBERT model)"),lar=l(),P7=a("li"),qge=a("strong"),iar=o("electra"),dar=o(" \u2014 "),qq=a("a"),car=o("TFElectraForSequenceClassification"),far=o(" (ELECTRA model)"),mar=l(),$7=a("li"),Gge=a("strong"),gar=o("flaubert"),har=o(" \u2014 "),Gq=a("a"),par=o("TFFlaubertForSequenceClassification"),_ar=o(" (FlauBERT model)"),uar=l(),I7=a("li"),Oge=a("strong"),bar=o("funnel"),Tar=o(" \u2014 "),Oq=a("a"),Far=o("TFFunnelForSequenceClassification"),Car=o(" (Funnel Transformer model)"),Mar=l(),j7=a("li"),Xge=a("strong"),Ear=o("gpt2"),yar=o(" \u2014 "),Xq=a("a"),war=o("TFGPT2ForSequenceClassification"),Aar=o(" (OpenAI GPT-2 model)"),Lar=l(),N7=a("li"),zge=a("strong"),Bar=o("layoutlm"),kar=o(" \u2014 "),zq=a("a"),xar=o("TFLayoutLMForSequenceClassification"),Rar=o(" (LayoutLM model)"),Sar=l(),D7=a("li"),Vge=a("strong"),Par=o("longformer"),$ar=o(" \u2014 "),Vq=a("a"),Iar=o("TFLongformerForSequenceClassification"),jar=o(" (Longformer model)"),Nar=l(),q7=a("li"),Wge=a("strong"),Dar=o("mobilebert"),qar=o(" \u2014 "),Wq=a("a"),Gar=o("TFMobileBertForSequenceClassification"),Oar=o(" (MobileBERT model)"),Xar=l(),G7=a("li"),Qge=a("strong"),zar=o("mpnet"),Var=o(" \u2014 "),Qq=a("a"),War=o("TFMPNetForSequenceClassification"),Qar=o(" (MPNet model)"),Har=l(),O7=a("li"),Hge=a("strong"),Uar=o("openai-gpt"),Jar=o(" \u2014 "),Hq=a("a"),Yar=o("TFOpenAIGPTForSequenceClassification"),Kar=o(" (OpenAI GPT model)"),Zar=l(),X7=a("li"),Uge=a("strong"),enr=o("rembert"),onr=o(" \u2014 "),Uq=a("a"),rnr=o("TFRemBertForSequenceClassification"),tnr=o(" (RemBERT model)"),anr=l(),z7=a("li"),Jge=a("strong"),nnr=o("roberta"),snr=o(" \u2014 "),Jq=a("a"),lnr=o("TFRobertaForSequenceClassification"),inr=o(" (RoBERTa model)"),dnr=l(),V7=a("li"),Yge=a("strong"),cnr=o("roformer"),fnr=o(" \u2014 "),Yq=a("a"),mnr=o("TFRoFormerForSequenceClassification"),gnr=o(" (RoFormer model)"),hnr=l(),W7=a("li"),Kge=a("strong"),pnr=o("tapas"),_nr=o(" \u2014 "),Kq=a("a"),unr=o("TFTapasForSequenceClassification"),bnr=o(" (TAPAS model)"),vnr=l(),Q7=a("li"),Zge=a("strong"),Tnr=o("transfo-xl"),Fnr=o(" \u2014 "),Zq=a("a"),Cnr=o("TFTransfoXLForSequenceClassification"),Mnr=o(" (Transformer-XL model)"),Enr=l(),H7=a("li"),ehe=a("strong"),ynr=o("xlm"),wnr=o(" \u2014 "),eG=a("a"),Anr=o("TFXLMForSequenceClassification"),Lnr=o(" (XLM model)"),Bnr=l(),U7=a("li"),ohe=a("strong"),knr=o("xlm-roberta"),xnr=o(" \u2014 "),oG=a("a"),Rnr=o("TFXLMRobertaForSequenceClassification"),Snr=o(" (XLM-RoBERTa model)"),Pnr=l(),J7=a("li"),rhe=a("strong"),$nr=o("xlnet"),Inr=o(" \u2014 "),rG=a("a"),jnr=o("TFXLNetForSequenceClassification"),Nnr=o(" (XLNet model)"),Dnr=l(),the=a("p"),qnr=o("Examples:"),Gnr=l(),f(Ew.$$.fragment),y8e=l(),Mc=a("h2"),Y7=a("a"),ahe=a("span"),f(yw.$$.fragment),Onr=l(),nhe=a("span"),Xnr=o("TFAutoModelForMultipleChoice"),w8e=l(),Tr=a("div"),f(ww.$$.fragment),znr=l(),Ec=a("p"),Vnr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a multiple choice head) when created
with the `),she=a("code"),Wnr=o("from_pretrained()"),Qnr=o("class method or the "),lhe=a("code"),Hnr=o("from_config()"),Unr=o(`class
method.`),Jnr=l(),Aw=a("p"),Ynr=o("This class cannot be instantiated directly using "),ihe=a("code"),Knr=o("__init__()"),Znr=o(" (throws an error)."),esr=l(),gt=a("div"),f(Lw.$$.fragment),osr=l(),dhe=a("p"),rsr=o("Instantiates one of the model classes of the library (with a multiple choice head) from a configuration."),tsr=l(),yc=a("p"),asr=o(`Note:
Loading a model from its configuration file does `),che=a("strong"),nsr=o("not"),ssr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),fhe=a("code"),lsr=o("from_pretrained()"),isr=o("to load the model weights."),dsr=l(),mhe=a("p"),csr=o("Examples:"),fsr=l(),f(Bw.$$.fragment),msr=l(),To=a("div"),f(kw.$$.fragment),gsr=l(),ghe=a("p"),hsr=o("Instantiate one of the model classes of the library (with a multiple choice head) from a pretrained model."),psr=l(),pn=a("p"),_sr=o("The model class to instantiate is selected based on the "),hhe=a("code"),usr=o("model_type"),bsr=o(` property of the config object (either
passed as an argument or loaded from `),phe=a("code"),vsr=o("pretrained_model_name_or_path"),Tsr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),_he=a("code"),Fsr=o("pretrained_model_name_or_path"),Csr=o(":"),Msr=l(),te=a("ul"),K7=a("li"),uhe=a("strong"),Esr=o("albert"),ysr=o(" \u2014 "),tG=a("a"),wsr=o("TFAlbertForMultipleChoice"),Asr=o(" (ALBERT model)"),Lsr=l(),Z7=a("li"),bhe=a("strong"),Bsr=o("bert"),ksr=o(" \u2014 "),aG=a("a"),xsr=o("TFBertForMultipleChoice"),Rsr=o(" (BERT model)"),Ssr=l(),eF=a("li"),vhe=a("strong"),Psr=o("camembert"),$sr=o(" \u2014 "),nG=a("a"),Isr=o("TFCamembertForMultipleChoice"),jsr=o(" (CamemBERT model)"),Nsr=l(),oF=a("li"),The=a("strong"),Dsr=o("convbert"),qsr=o(" \u2014 "),sG=a("a"),Gsr=o("TFConvBertForMultipleChoice"),Osr=o(" (ConvBERT model)"),Xsr=l(),rF=a("li"),Fhe=a("strong"),zsr=o("distilbert"),Vsr=o(" \u2014 "),lG=a("a"),Wsr=o("TFDistilBertForMultipleChoice"),Qsr=o(" (DistilBERT model)"),Hsr=l(),tF=a("li"),Che=a("strong"),Usr=o("electra"),Jsr=o(" \u2014 "),iG=a("a"),Ysr=o("TFElectraForMultipleChoice"),Ksr=o(" (ELECTRA model)"),Zsr=l(),aF=a("li"),Mhe=a("strong"),elr=o("flaubert"),olr=o(" \u2014 "),dG=a("a"),rlr=o("TFFlaubertForMultipleChoice"),tlr=o(" (FlauBERT model)"),alr=l(),nF=a("li"),Ehe=a("strong"),nlr=o("funnel"),slr=o(" \u2014 "),cG=a("a"),llr=o("TFFunnelForMultipleChoice"),ilr=o(" (Funnel Transformer model)"),dlr=l(),sF=a("li"),yhe=a("strong"),clr=o("longformer"),flr=o(" \u2014 "),fG=a("a"),mlr=o("TFLongformerForMultipleChoice"),glr=o(" (Longformer model)"),hlr=l(),lF=a("li"),whe=a("strong"),plr=o("mobilebert"),_lr=o(" \u2014 "),mG=a("a"),ulr=o("TFMobileBertForMultipleChoice"),blr=o(" (MobileBERT model)"),vlr=l(),iF=a("li"),Ahe=a("strong"),Tlr=o("mpnet"),Flr=o(" \u2014 "),gG=a("a"),Clr=o("TFMPNetForMultipleChoice"),Mlr=o(" (MPNet model)"),Elr=l(),dF=a("li"),Lhe=a("strong"),ylr=o("rembert"),wlr=o(" \u2014 "),hG=a("a"),Alr=o("TFRemBertForMultipleChoice"),Llr=o(" (RemBERT model)"),Blr=l(),cF=a("li"),Bhe=a("strong"),klr=o("roberta"),xlr=o(" \u2014 "),pG=a("a"),Rlr=o("TFRobertaForMultipleChoice"),Slr=o(" (RoBERTa model)"),Plr=l(),fF=a("li"),khe=a("strong"),$lr=o("roformer"),Ilr=o(" \u2014 "),_G=a("a"),jlr=o("TFRoFormerForMultipleChoice"),Nlr=o(" (RoFormer model)"),Dlr=l(),mF=a("li"),xhe=a("strong"),qlr=o("xlm"),Glr=o(" \u2014 "),uG=a("a"),Olr=o("TFXLMForMultipleChoice"),Xlr=o(" (XLM model)"),zlr=l(),gF=a("li"),Rhe=a("strong"),Vlr=o("xlm-roberta"),Wlr=o(" \u2014 "),bG=a("a"),Qlr=o("TFXLMRobertaForMultipleChoice"),Hlr=o(" (XLM-RoBERTa model)"),Ulr=l(),hF=a("li"),She=a("strong"),Jlr=o("xlnet"),Ylr=o(" \u2014 "),vG=a("a"),Klr=o("TFXLNetForMultipleChoice"),Zlr=o(" (XLNet model)"),eir=l(),Phe=a("p"),oir=o("Examples:"),rir=l(),f(xw.$$.fragment),A8e=l(),wc=a("h2"),pF=a("a"),$he=a("span"),f(Rw.$$.fragment),tir=l(),Ihe=a("span"),air=o("TFAutoModelForTableQuestionAnswering"),L8e=l(),Fr=a("div"),f(Sw.$$.fragment),nir=l(),Ac=a("p"),sir=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a table question answering head) when created
with the `),jhe=a("code"),lir=o("from_pretrained()"),iir=o("class method or the "),Nhe=a("code"),dir=o("from_config()"),cir=o(`class
method.`),fir=l(),Pw=a("p"),mir=o("This class cannot be instantiated directly using "),Dhe=a("code"),gir=o("__init__()"),hir=o(" (throws an error)."),pir=l(),ht=a("div"),f($w.$$.fragment),_ir=l(),qhe=a("p"),uir=o("Instantiates one of the model classes of the library (with a table question answering head) from a configuration."),bir=l(),Lc=a("p"),vir=o(`Note:
Loading a model from its configuration file does `),Ghe=a("strong"),Tir=o("not"),Fir=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Ohe=a("code"),Cir=o("from_pretrained()"),Mir=o("to load the model weights."),Eir=l(),Xhe=a("p"),yir=o("Examples:"),wir=l(),f(Iw.$$.fragment),Air=l(),Fo=a("div"),f(jw.$$.fragment),Lir=l(),zhe=a("p"),Bir=o("Instantiate one of the model classes of the library (with a table question answering head) from a pretrained model."),kir=l(),_n=a("p"),xir=o("The model class to instantiate is selected based on the "),Vhe=a("code"),Rir=o("model_type"),Sir=o(` property of the config object (either
passed as an argument or loaded from `),Whe=a("code"),Pir=o("pretrained_model_name_or_path"),$ir=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Qhe=a("code"),Iir=o("pretrained_model_name_or_path"),jir=o(":"),Nir=l(),Hhe=a("ul"),_F=a("li"),Uhe=a("strong"),Dir=o("tapas"),qir=o(" \u2014 "),TG=a("a"),Gir=o("TFTapasForQuestionAnswering"),Oir=o(" (TAPAS model)"),Xir=l(),Jhe=a("p"),zir=o("Examples:"),Vir=l(),f(Nw.$$.fragment),B8e=l(),Bc=a("h2"),uF=a("a"),Yhe=a("span"),f(Dw.$$.fragment),Wir=l(),Khe=a("span"),Qir=o("TFAutoModelForTokenClassification"),k8e=l(),Cr=a("div"),f(qw.$$.fragment),Hir=l(),kc=a("p"),Uir=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a token classification head) when created
with the `),Zhe=a("code"),Jir=o("from_pretrained()"),Yir=o("class method or the "),epe=a("code"),Kir=o("from_config()"),Zir=o(`class
method.`),edr=l(),Gw=a("p"),odr=o("This class cannot be instantiated directly using "),ope=a("code"),rdr=o("__init__()"),tdr=o(" (throws an error)."),adr=l(),pt=a("div"),f(Ow.$$.fragment),ndr=l(),rpe=a("p"),sdr=o("Instantiates one of the model classes of the library (with a token classification head) from a configuration."),ldr=l(),xc=a("p"),idr=o(`Note:
Loading a model from its configuration file does `),tpe=a("strong"),ddr=o("not"),cdr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),ape=a("code"),fdr=o("from_pretrained()"),mdr=o("to load the model weights."),gdr=l(),npe=a("p"),hdr=o("Examples:"),pdr=l(),f(Xw.$$.fragment),_dr=l(),Co=a("div"),f(zw.$$.fragment),udr=l(),spe=a("p"),bdr=o("Instantiate one of the model classes of the library (with a token classification head) from a pretrained model."),vdr=l(),un=a("p"),Tdr=o("The model class to instantiate is selected based on the "),lpe=a("code"),Fdr=o("model_type"),Cdr=o(` property of the config object (either
passed as an argument or loaded from `),ipe=a("code"),Mdr=o("pretrained_model_name_or_path"),Edr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),dpe=a("code"),ydr=o("pretrained_model_name_or_path"),wdr=o(":"),Adr=l(),K=a("ul"),bF=a("li"),cpe=a("strong"),Ldr=o("albert"),Bdr=o(" \u2014 "),FG=a("a"),kdr=o("TFAlbertForTokenClassification"),xdr=o(" (ALBERT model)"),Rdr=l(),vF=a("li"),fpe=a("strong"),Sdr=o("bert"),Pdr=o(" \u2014 "),CG=a("a"),$dr=o("TFBertForTokenClassification"),Idr=o(" (BERT model)"),jdr=l(),TF=a("li"),mpe=a("strong"),Ndr=o("camembert"),Ddr=o(" \u2014 "),MG=a("a"),qdr=o("TFCamembertForTokenClassification"),Gdr=o(" (CamemBERT model)"),Odr=l(),FF=a("li"),gpe=a("strong"),Xdr=o("convbert"),zdr=o(" \u2014 "),EG=a("a"),Vdr=o("TFConvBertForTokenClassification"),Wdr=o(" (ConvBERT model)"),Qdr=l(),CF=a("li"),hpe=a("strong"),Hdr=o("deberta"),Udr=o(" \u2014 "),yG=a("a"),Jdr=o("TFDebertaForTokenClassification"),Ydr=o(" (DeBERTa model)"),Kdr=l(),MF=a("li"),ppe=a("strong"),Zdr=o("deberta-v2"),ecr=o(" \u2014 "),wG=a("a"),ocr=o("TFDebertaV2ForTokenClassification"),rcr=o(" (DeBERTa-v2 model)"),tcr=l(),EF=a("li"),_pe=a("strong"),acr=o("distilbert"),ncr=o(" \u2014 "),AG=a("a"),scr=o("TFDistilBertForTokenClassification"),lcr=o(" (DistilBERT model)"),icr=l(),yF=a("li"),upe=a("strong"),dcr=o("electra"),ccr=o(" \u2014 "),LG=a("a"),fcr=o("TFElectraForTokenClassification"),mcr=o(" (ELECTRA model)"),gcr=l(),wF=a("li"),bpe=a("strong"),hcr=o("flaubert"),pcr=o(" \u2014 "),BG=a("a"),_cr=o("TFFlaubertForTokenClassification"),ucr=o(" (FlauBERT model)"),bcr=l(),AF=a("li"),vpe=a("strong"),vcr=o("funnel"),Tcr=o(" \u2014 "),kG=a("a"),Fcr=o("TFFunnelForTokenClassification"),Ccr=o(" (Funnel Transformer model)"),Mcr=l(),LF=a("li"),Tpe=a("strong"),Ecr=o("layoutlm"),ycr=o(" \u2014 "),xG=a("a"),wcr=o("TFLayoutLMForTokenClassification"),Acr=o(" (LayoutLM model)"),Lcr=l(),BF=a("li"),Fpe=a("strong"),Bcr=o("longformer"),kcr=o(" \u2014 "),RG=a("a"),xcr=o("TFLongformerForTokenClassification"),Rcr=o(" (Longformer model)"),Scr=l(),kF=a("li"),Cpe=a("strong"),Pcr=o("mobilebert"),$cr=o(" \u2014 "),SG=a("a"),Icr=o("TFMobileBertForTokenClassification"),jcr=o(" (MobileBERT model)"),Ncr=l(),xF=a("li"),Mpe=a("strong"),Dcr=o("mpnet"),qcr=o(" \u2014 "),PG=a("a"),Gcr=o("TFMPNetForTokenClassification"),Ocr=o(" (MPNet model)"),Xcr=l(),RF=a("li"),Epe=a("strong"),zcr=o("rembert"),Vcr=o(" \u2014 "),$G=a("a"),Wcr=o("TFRemBertForTokenClassification"),Qcr=o(" (RemBERT model)"),Hcr=l(),SF=a("li"),ype=a("strong"),Ucr=o("roberta"),Jcr=o(" \u2014 "),IG=a("a"),Ycr=o("TFRobertaForTokenClassification"),Kcr=o(" (RoBERTa model)"),Zcr=l(),PF=a("li"),wpe=a("strong"),efr=o("roformer"),ofr=o(" \u2014 "),jG=a("a"),rfr=o("TFRoFormerForTokenClassification"),tfr=o(" (RoFormer model)"),afr=l(),$F=a("li"),Ape=a("strong"),nfr=o("xlm"),sfr=o(" \u2014 "),NG=a("a"),lfr=o("TFXLMForTokenClassification"),ifr=o(" (XLM model)"),dfr=l(),IF=a("li"),Lpe=a("strong"),cfr=o("xlm-roberta"),ffr=o(" \u2014 "),DG=a("a"),mfr=o("TFXLMRobertaForTokenClassification"),gfr=o(" (XLM-RoBERTa model)"),hfr=l(),jF=a("li"),Bpe=a("strong"),pfr=o("xlnet"),_fr=o(" \u2014 "),qG=a("a"),ufr=o("TFXLNetForTokenClassification"),bfr=o(" (XLNet model)"),vfr=l(),kpe=a("p"),Tfr=o("Examples:"),Ffr=l(),f(Vw.$$.fragment),x8e=l(),Rc=a("h2"),NF=a("a"),xpe=a("span"),f(Ww.$$.fragment),Cfr=l(),Rpe=a("span"),Mfr=o("TFAutoModelForQuestionAnswering"),R8e=l(),Mr=a("div"),f(Qw.$$.fragment),Efr=l(),Sc=a("p"),yfr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a question answering head) when created
with the `),Spe=a("code"),wfr=o("from_pretrained()"),Afr=o("class method or the "),Ppe=a("code"),Lfr=o("from_config()"),Bfr=o(`class
method.`),kfr=l(),Hw=a("p"),xfr=o("This class cannot be instantiated directly using "),$pe=a("code"),Rfr=o("__init__()"),Sfr=o(" (throws an error)."),Pfr=l(),_t=a("div"),f(Uw.$$.fragment),$fr=l(),Ipe=a("p"),Ifr=o("Instantiates one of the model classes of the library (with a question answering head) from a configuration."),jfr=l(),Pc=a("p"),Nfr=o(`Note:
Loading a model from its configuration file does `),jpe=a("strong"),Dfr=o("not"),qfr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Npe=a("code"),Gfr=o("from_pretrained()"),Ofr=o("to load the model weights."),Xfr=l(),Dpe=a("p"),zfr=o("Examples:"),Vfr=l(),f(Jw.$$.fragment),Wfr=l(),Mo=a("div"),f(Yw.$$.fragment),Qfr=l(),qpe=a("p"),Hfr=o("Instantiate one of the model classes of the library (with a question answering head) from a pretrained model."),Ufr=l(),bn=a("p"),Jfr=o("The model class to instantiate is selected based on the "),Gpe=a("code"),Yfr=o("model_type"),Kfr=o(` property of the config object (either
passed as an argument or loaded from `),Ope=a("code"),Zfr=o("pretrained_model_name_or_path"),emr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Xpe=a("code"),omr=o("pretrained_model_name_or_path"),rmr=o(":"),tmr=l(),Z=a("ul"),DF=a("li"),zpe=a("strong"),amr=o("albert"),nmr=o(" \u2014 "),GG=a("a"),smr=o("TFAlbertForQuestionAnswering"),lmr=o(" (ALBERT model)"),imr=l(),qF=a("li"),Vpe=a("strong"),dmr=o("bert"),cmr=o(" \u2014 "),OG=a("a"),fmr=o("TFBertForQuestionAnswering"),mmr=o(" (BERT model)"),gmr=l(),GF=a("li"),Wpe=a("strong"),hmr=o("camembert"),pmr=o(" \u2014 "),XG=a("a"),_mr=o("TFCamembertForQuestionAnswering"),umr=o(" (CamemBERT model)"),bmr=l(),OF=a("li"),Qpe=a("strong"),vmr=o("convbert"),Tmr=o(" \u2014 "),zG=a("a"),Fmr=o("TFConvBertForQuestionAnswering"),Cmr=o(" (ConvBERT model)"),Mmr=l(),XF=a("li"),Hpe=a("strong"),Emr=o("deberta"),ymr=o(" \u2014 "),VG=a("a"),wmr=o("TFDebertaForQuestionAnswering"),Amr=o(" (DeBERTa model)"),Lmr=l(),zF=a("li"),Upe=a("strong"),Bmr=o("deberta-v2"),kmr=o(" \u2014 "),WG=a("a"),xmr=o("TFDebertaV2ForQuestionAnswering"),Rmr=o(" (DeBERTa-v2 model)"),Smr=l(),VF=a("li"),Jpe=a("strong"),Pmr=o("distilbert"),$mr=o(" \u2014 "),QG=a("a"),Imr=o("TFDistilBertForQuestionAnswering"),jmr=o(" (DistilBERT model)"),Nmr=l(),WF=a("li"),Ype=a("strong"),Dmr=o("electra"),qmr=o(" \u2014 "),HG=a("a"),Gmr=o("TFElectraForQuestionAnswering"),Omr=o(" (ELECTRA model)"),Xmr=l(),QF=a("li"),Kpe=a("strong"),zmr=o("flaubert"),Vmr=o(" \u2014 "),UG=a("a"),Wmr=o("TFFlaubertForQuestionAnsweringSimple"),Qmr=o(" (FlauBERT model)"),Hmr=l(),HF=a("li"),Zpe=a("strong"),Umr=o("funnel"),Jmr=o(" \u2014 "),JG=a("a"),Ymr=o("TFFunnelForQuestionAnswering"),Kmr=o(" (Funnel Transformer model)"),Zmr=l(),UF=a("li"),e_e=a("strong"),egr=o("longformer"),ogr=o(" \u2014 "),YG=a("a"),rgr=o("TFLongformerForQuestionAnswering"),tgr=o(" (Longformer model)"),agr=l(),JF=a("li"),o_e=a("strong"),ngr=o("mobilebert"),sgr=o(" \u2014 "),KG=a("a"),lgr=o("TFMobileBertForQuestionAnswering"),igr=o(" (MobileBERT model)"),dgr=l(),YF=a("li"),r_e=a("strong"),cgr=o("mpnet"),fgr=o(" \u2014 "),ZG=a("a"),mgr=o("TFMPNetForQuestionAnswering"),ggr=o(" (MPNet model)"),hgr=l(),KF=a("li"),t_e=a("strong"),pgr=o("rembert"),_gr=o(" \u2014 "),eO=a("a"),ugr=o("TFRemBertForQuestionAnswering"),bgr=o(" (RemBERT model)"),vgr=l(),ZF=a("li"),a_e=a("strong"),Tgr=o("roberta"),Fgr=o(" \u2014 "),oO=a("a"),Cgr=o("TFRobertaForQuestionAnswering"),Mgr=o(" (RoBERTa model)"),Egr=l(),e9=a("li"),n_e=a("strong"),ygr=o("roformer"),wgr=o(" \u2014 "),rO=a("a"),Agr=o("TFRoFormerForQuestionAnswering"),Lgr=o(" (RoFormer model)"),Bgr=l(),o9=a("li"),s_e=a("strong"),kgr=o("xlm"),xgr=o(" \u2014 "),tO=a("a"),Rgr=o("TFXLMForQuestionAnsweringSimple"),Sgr=o(" (XLM model)"),Pgr=l(),r9=a("li"),l_e=a("strong"),$gr=o("xlm-roberta"),Igr=o(" \u2014 "),aO=a("a"),jgr=o("TFXLMRobertaForQuestionAnswering"),Ngr=o(" (XLM-RoBERTa model)"),Dgr=l(),t9=a("li"),i_e=a("strong"),qgr=o("xlnet"),Ggr=o(" \u2014 "),nO=a("a"),Ogr=o("TFXLNetForQuestionAnsweringSimple"),Xgr=o(" (XLNet model)"),zgr=l(),d_e=a("p"),Vgr=o("Examples:"),Wgr=l(),f(Kw.$$.fragment),S8e=l(),$c=a("h2"),a9=a("a"),c_e=a("span"),f(Zw.$$.fragment),Qgr=l(),f_e=a("span"),Hgr=o("TFAutoModelForVision2Seq"),P8e=l(),Er=a("div"),f(eA.$$.fragment),Ugr=l(),Ic=a("p"),Jgr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a vision-to-text modeling head) when created
with the `),m_e=a("code"),Ygr=o("from_pretrained()"),Kgr=o("class method or the "),g_e=a("code"),Zgr=o("from_config()"),ehr=o(`class
method.`),ohr=l(),oA=a("p"),rhr=o("This class cannot be instantiated directly using "),h_e=a("code"),thr=o("__init__()"),ahr=o(" (throws an error)."),nhr=l(),ut=a("div"),f(rA.$$.fragment),shr=l(),p_e=a("p"),lhr=o("Instantiates one of the model classes of the library (with a vision-to-text modeling head) from a configuration."),ihr=l(),jc=a("p"),dhr=o(`Note:
Loading a model from its configuration file does `),__e=a("strong"),chr=o("not"),fhr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),u_e=a("code"),mhr=o("from_pretrained()"),ghr=o("to load the model weights."),hhr=l(),b_e=a("p"),phr=o("Examples:"),_hr=l(),f(tA.$$.fragment),uhr=l(),Eo=a("div"),f(aA.$$.fragment),bhr=l(),v_e=a("p"),vhr=o("Instantiate one of the model classes of the library (with a vision-to-text modeling head) from a pretrained model."),Thr=l(),vn=a("p"),Fhr=o("The model class to instantiate is selected based on the "),T_e=a("code"),Chr=o("model_type"),Mhr=o(` property of the config object (either
passed as an argument or loaded from `),F_e=a("code"),Ehr=o("pretrained_model_name_or_path"),yhr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),C_e=a("code"),whr=o("pretrained_model_name_or_path"),Ahr=o(":"),Lhr=l(),M_e=a("ul"),n9=a("li"),E_e=a("strong"),Bhr=o("vision-encoder-decoder"),khr=o(" \u2014 "),sO=a("a"),xhr=o("TFVisionEncoderDecoderModel"),Rhr=o(" (Vision Encoder decoder model)"),Shr=l(),y_e=a("p"),Phr=o("Examples:"),$hr=l(),f(nA.$$.fragment),$8e=l(),Nc=a("h2"),s9=a("a"),w_e=a("span"),f(sA.$$.fragment),Ihr=l(),A_e=a("span"),jhr=o("TFAutoModelForSpeechSeq2Seq"),I8e=l(),yr=a("div"),f(lA.$$.fragment),Nhr=l(),Dc=a("p"),Dhr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) when created
with the `),L_e=a("code"),qhr=o("from_pretrained()"),Ghr=o("class method or the "),B_e=a("code"),Ohr=o("from_config()"),Xhr=o(`class
method.`),zhr=l(),iA=a("p"),Vhr=o("This class cannot be instantiated directly using "),k_e=a("code"),Whr=o("__init__()"),Qhr=o(" (throws an error)."),Hhr=l(),bt=a("div"),f(dA.$$.fragment),Uhr=l(),x_e=a("p"),Jhr=o("Instantiates one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) from a configuration."),Yhr=l(),qc=a("p"),Khr=o(`Note:
Loading a model from its configuration file does `),R_e=a("strong"),Zhr=o("not"),epr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),S_e=a("code"),opr=o("from_pretrained()"),rpr=o("to load the model weights."),tpr=l(),P_e=a("p"),apr=o("Examples:"),npr=l(),f(cA.$$.fragment),spr=l(),yo=a("div"),f(fA.$$.fragment),lpr=l(),$_e=a("p"),ipr=o("Instantiate one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) from a pretrained model."),dpr=l(),Tn=a("p"),cpr=o("The model class to instantiate is selected based on the "),I_e=a("code"),fpr=o("model_type"),mpr=o(` property of the config object (either
passed as an argument or loaded from `),j_e=a("code"),gpr=o("pretrained_model_name_or_path"),hpr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),N_e=a("code"),ppr=o("pretrained_model_name_or_path"),_pr=o(":"),upr=l(),D_e=a("ul"),l9=a("li"),q_e=a("strong"),bpr=o("speech_to_text"),vpr=o(" \u2014 "),lO=a("a"),Tpr=o("TFSpeech2TextForConditionalGeneration"),Fpr=o(" (Speech2Text model)"),Cpr=l(),G_e=a("p"),Mpr=o("Examples:"),Epr=l(),f(mA.$$.fragment),j8e=l(),Gc=a("h2"),i9=a("a"),O_e=a("span"),f(gA.$$.fragment),ypr=l(),X_e=a("span"),wpr=o("FlaxAutoModel"),N8e=l(),wr=a("div"),f(hA.$$.fragment),Apr=l(),Oc=a("p"),Lpr=o(`This is a generic model class that will be instantiated as one of the base model classes of the library when created
with the `),z_e=a("code"),Bpr=o("from_pretrained()"),kpr=o("class method or the "),V_e=a("code"),xpr=o("from_config()"),Rpr=o(`class
method.`),Spr=l(),pA=a("p"),Ppr=o("This class cannot be instantiated directly using "),W_e=a("code"),$pr=o("__init__()"),Ipr=o(" (throws an error)."),jpr=l(),vt=a("div"),f(_A.$$.fragment),Npr=l(),Q_e=a("p"),Dpr=o("Instantiates one of the base model classes of the library from a configuration."),qpr=l(),Xc=a("p"),Gpr=o(`Note:
Loading a model from its configuration file does `),H_e=a("strong"),Opr=o("not"),Xpr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),U_e=a("code"),zpr=o("from_pretrained()"),Vpr=o("to load the model weights."),Wpr=l(),J_e=a("p"),Qpr=o("Examples:"),Hpr=l(),f(uA.$$.fragment),Upr=l(),wo=a("div"),f(bA.$$.fragment),Jpr=l(),Y_e=a("p"),Ypr=o("Instantiate one of the base model classes of the library from a pretrained model."),Kpr=l(),Fn=a("p"),Zpr=o("The model class to instantiate is selected based on the "),K_e=a("code"),e_r=o("model_type"),o_r=o(` property of the config object (either
passed as an argument or loaded from `),Z_e=a("code"),r_r=o("pretrained_model_name_or_path"),t_r=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),eue=a("code"),a_r=o("pretrained_model_name_or_path"),n_r=o(":"),s_r=l(),V=a("ul"),d9=a("li"),oue=a("strong"),l_r=o("albert"),i_r=o(" \u2014 "),iO=a("a"),d_r=o("FlaxAlbertModel"),c_r=o(" (ALBERT model)"),f_r=l(),c9=a("li"),rue=a("strong"),m_r=o("bart"),g_r=o(" \u2014 "),dO=a("a"),h_r=o("FlaxBartModel"),p_r=o(" (BART model)"),__r=l(),f9=a("li"),tue=a("strong"),u_r=o("beit"),b_r=o(" \u2014 "),cO=a("a"),v_r=o("FlaxBeitModel"),T_r=o(" (BEiT model)"),F_r=l(),m9=a("li"),aue=a("strong"),C_r=o("bert"),M_r=o(" \u2014 "),fO=a("a"),E_r=o("FlaxBertModel"),y_r=o(" (BERT model)"),w_r=l(),g9=a("li"),nue=a("strong"),A_r=o("big_bird"),L_r=o(" \u2014 "),mO=a("a"),B_r=o("FlaxBigBirdModel"),k_r=o(" (BigBird model)"),x_r=l(),h9=a("li"),sue=a("strong"),R_r=o("blenderbot"),S_r=o(" \u2014 "),gO=a("a"),P_r=o("FlaxBlenderbotModel"),$_r=o(" (Blenderbot model)"),I_r=l(),p9=a("li"),lue=a("strong"),j_r=o("blenderbot-small"),N_r=o(" \u2014 "),hO=a("a"),D_r=o("FlaxBlenderbotSmallModel"),q_r=o(" (BlenderbotSmall model)"),G_r=l(),_9=a("li"),iue=a("strong"),O_r=o("clip"),X_r=o(" \u2014 "),pO=a("a"),z_r=o("FlaxCLIPModel"),V_r=o(" (CLIP model)"),W_r=l(),u9=a("li"),due=a("strong"),Q_r=o("distilbert"),H_r=o(" \u2014 "),_O=a("a"),U_r=o("FlaxDistilBertModel"),J_r=o(" (DistilBERT model)"),Y_r=l(),b9=a("li"),cue=a("strong"),K_r=o("electra"),Z_r=o(" \u2014 "),uO=a("a"),eur=o("FlaxElectraModel"),our=o(" (ELECTRA model)"),rur=l(),v9=a("li"),fue=a("strong"),tur=o("gpt2"),aur=o(" \u2014 "),bO=a("a"),nur=o("FlaxGPT2Model"),sur=o(" (OpenAI GPT-2 model)"),lur=l(),T9=a("li"),mue=a("strong"),iur=o("gpt_neo"),dur=o(" \u2014 "),vO=a("a"),cur=o("FlaxGPTNeoModel"),fur=o(" (GPT Neo model)"),mur=l(),F9=a("li"),gue=a("strong"),gur=o("gptj"),hur=o(" \u2014 "),TO=a("a"),pur=o("FlaxGPTJModel"),_ur=o(" (GPT-J model)"),uur=l(),C9=a("li"),hue=a("strong"),bur=o("marian"),vur=o(" \u2014 "),FO=a("a"),Tur=o("FlaxMarianModel"),Fur=o(" (Marian model)"),Cur=l(),M9=a("li"),pue=a("strong"),Mur=o("mbart"),Eur=o(" \u2014 "),CO=a("a"),yur=o("FlaxMBartModel"),wur=o(" (mBART model)"),Aur=l(),E9=a("li"),_ue=a("strong"),Lur=o("mt5"),Bur=o(" \u2014 "),MO=a("a"),kur=o("FlaxMT5Model"),xur=o(" (mT5 model)"),Rur=l(),y9=a("li"),uue=a("strong"),Sur=o("pegasus"),Pur=o(" \u2014 "),EO=a("a"),$ur=o("FlaxPegasusModel"),Iur=o(" (Pegasus model)"),jur=l(),w9=a("li"),bue=a("strong"),Nur=o("roberta"),Dur=o(" \u2014 "),yO=a("a"),qur=o("FlaxRobertaModel"),Gur=o(" (RoBERTa model)"),Our=l(),A9=a("li"),vue=a("strong"),Xur=o("roformer"),zur=o(" \u2014 "),wO=a("a"),Vur=o("FlaxRoFormerModel"),Wur=o(" (RoFormer model)"),Qur=l(),L9=a("li"),Tue=a("strong"),Hur=o("t5"),Uur=o(" \u2014 "),AO=a("a"),Jur=o("FlaxT5Model"),Yur=o(" (T5 model)"),Kur=l(),B9=a("li"),Fue=a("strong"),Zur=o("vision-text-dual-encoder"),e2r=o(" \u2014 "),LO=a("a"),o2r=o("FlaxVisionTextDualEncoderModel"),r2r=o(" (VisionTextDualEncoder model)"),t2r=l(),k9=a("li"),Cue=a("strong"),a2r=o("vit"),n2r=o(" \u2014 "),BO=a("a"),s2r=o("FlaxViTModel"),l2r=o(" (ViT model)"),i2r=l(),x9=a("li"),Mue=a("strong"),d2r=o("wav2vec2"),c2r=o(" \u2014 "),kO=a("a"),f2r=o("FlaxWav2Vec2Model"),m2r=o(" (Wav2Vec2 model)"),g2r=l(),R9=a("li"),Eue=a("strong"),h2r=o("xglm"),p2r=o(" \u2014 "),xO=a("a"),_2r=o("FlaxXGLMModel"),u2r=o(" (XGLM model)"),b2r=l(),yue=a("p"),v2r=o("Examples:"),T2r=l(),f(vA.$$.fragment),D8e=l(),zc=a("h2"),S9=a("a"),wue=a("span"),f(TA.$$.fragment),F2r=l(),Aue=a("span"),C2r=o("FlaxAutoModelForCausalLM"),q8e=l(),Ar=a("div"),f(FA.$$.fragment),M2r=l(),Vc=a("p"),E2r=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a causal language modeling head) when created
with the `),Lue=a("code"),y2r=o("from_pretrained()"),w2r=o("class method or the "),Bue=a("code"),A2r=o("from_config()"),L2r=o(`class
method.`),B2r=l(),CA=a("p"),k2r=o("This class cannot be instantiated directly using "),kue=a("code"),x2r=o("__init__()"),R2r=o(" (throws an error)."),S2r=l(),Tt=a("div"),f(MA.$$.fragment),P2r=l(),xue=a("p"),$2r=o("Instantiates one of the model classes of the library (with a causal language modeling head) from a configuration."),I2r=l(),Wc=a("p"),j2r=o(`Note:
Loading a model from its configuration file does `),Rue=a("strong"),N2r=o("not"),D2r=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Sue=a("code"),q2r=o("from_pretrained()"),G2r=o("to load the model weights."),O2r=l(),Pue=a("p"),X2r=o("Examples:"),z2r=l(),f(EA.$$.fragment),V2r=l(),Ao=a("div"),f(yA.$$.fragment),W2r=l(),$ue=a("p"),Q2r=o("Instantiate one of the model classes of the library (with a causal language modeling head) from a pretrained model."),H2r=l(),Cn=a("p"),U2r=o("The model class to instantiate is selected based on the "),Iue=a("code"),J2r=o("model_type"),Y2r=o(` property of the config object (either
passed as an argument or loaded from `),jue=a("code"),K2r=o("pretrained_model_name_or_path"),Z2r=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Nue=a("code"),e1r=o("pretrained_model_name_or_path"),o1r=o(":"),r1r=l(),Mn=a("ul"),P9=a("li"),Due=a("strong"),t1r=o("gpt2"),a1r=o(" \u2014 "),RO=a("a"),n1r=o("FlaxGPT2LMHeadModel"),s1r=o(" (OpenAI GPT-2 model)"),l1r=l(),$9=a("li"),que=a("strong"),i1r=o("gpt_neo"),d1r=o(" \u2014 "),SO=a("a"),c1r=o("FlaxGPTNeoForCausalLM"),f1r=o(" (GPT Neo model)"),m1r=l(),I9=a("li"),Gue=a("strong"),g1r=o("gptj"),h1r=o(" \u2014 "),PO=a("a"),p1r=o("FlaxGPTJForCausalLM"),_1r=o(" (GPT-J model)"),u1r=l(),j9=a("li"),Oue=a("strong"),b1r=o("xglm"),v1r=o(" \u2014 "),$O=a("a"),T1r=o("FlaxXGLMForCausalLM"),F1r=o(" (XGLM model)"),C1r=l(),Xue=a("p"),M1r=o("Examples:"),E1r=l(),f(wA.$$.fragment),G8e=l(),Qc=a("h2"),N9=a("a"),zue=a("span"),f(AA.$$.fragment),y1r=l(),Vue=a("span"),w1r=o("FlaxAutoModelForPreTraining"),O8e=l(),Lr=a("div"),f(LA.$$.fragment),A1r=l(),Hc=a("p"),L1r=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a pretraining head) when created
with the `),Wue=a("code"),B1r=o("from_pretrained()"),k1r=o("class method or the "),Que=a("code"),x1r=o("from_config()"),R1r=o(`class
method.`),S1r=l(),BA=a("p"),P1r=o("This class cannot be instantiated directly using "),Hue=a("code"),$1r=o("__init__()"),I1r=o(" (throws an error)."),j1r=l(),Ft=a("div"),f(kA.$$.fragment),N1r=l(),Uue=a("p"),D1r=o("Instantiates one of the model classes of the library (with a pretraining head) from a configuration."),q1r=l(),Uc=a("p"),G1r=o(`Note:
Loading a model from its configuration file does `),Jue=a("strong"),O1r=o("not"),X1r=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Yue=a("code"),z1r=o("from_pretrained()"),V1r=o("to load the model weights."),W1r=l(),Kue=a("p"),Q1r=o("Examples:"),H1r=l(),f(xA.$$.fragment),U1r=l(),Lo=a("div"),f(RA.$$.fragment),J1r=l(),Zue=a("p"),Y1r=o("Instantiate one of the model classes of the library (with a pretraining head) from a pretrained model."),K1r=l(),En=a("p"),Z1r=o("The model class to instantiate is selected based on the "),e2e=a("code"),ebr=o("model_type"),obr=o(` property of the config object (either
passed as an argument or loaded from `),o2e=a("code"),rbr=o("pretrained_model_name_or_path"),tbr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),r2e=a("code"),abr=o("pretrained_model_name_or_path"),nbr=o(":"),sbr=l(),fe=a("ul"),D9=a("li"),t2e=a("strong"),lbr=o("albert"),ibr=o(" \u2014 "),IO=a("a"),dbr=o("FlaxAlbertForPreTraining"),cbr=o(" (ALBERT model)"),fbr=l(),q9=a("li"),a2e=a("strong"),mbr=o("bart"),gbr=o(" \u2014 "),jO=a("a"),hbr=o("FlaxBartForConditionalGeneration"),pbr=o(" (BART model)"),_br=l(),G9=a("li"),n2e=a("strong"),ubr=o("bert"),bbr=o(" \u2014 "),NO=a("a"),vbr=o("FlaxBertForPreTraining"),Tbr=o(" (BERT model)"),Fbr=l(),O9=a("li"),s2e=a("strong"),Cbr=o("big_bird"),Mbr=o(" \u2014 "),DO=a("a"),Ebr=o("FlaxBigBirdForPreTraining"),ybr=o(" (BigBird model)"),wbr=l(),X9=a("li"),l2e=a("strong"),Abr=o("electra"),Lbr=o(" \u2014 "),qO=a("a"),Bbr=o("FlaxElectraForPreTraining"),kbr=o(" (ELECTRA model)"),xbr=l(),z9=a("li"),i2e=a("strong"),Rbr=o("mbart"),Sbr=o(" \u2014 "),GO=a("a"),Pbr=o("FlaxMBartForConditionalGeneration"),$br=o(" (mBART model)"),Ibr=l(),V9=a("li"),d2e=a("strong"),jbr=o("mt5"),Nbr=o(" \u2014 "),OO=a("a"),Dbr=o("FlaxMT5ForConditionalGeneration"),qbr=o(" (mT5 model)"),Gbr=l(),W9=a("li"),c2e=a("strong"),Obr=o("roberta"),Xbr=o(" \u2014 "),XO=a("a"),zbr=o("FlaxRobertaForMaskedLM"),Vbr=o(" (RoBERTa model)"),Wbr=l(),Q9=a("li"),f2e=a("strong"),Qbr=o("roformer"),Hbr=o(" \u2014 "),zO=a("a"),Ubr=o("FlaxRoFormerForMaskedLM"),Jbr=o(" (RoFormer model)"),Ybr=l(),H9=a("li"),m2e=a("strong"),Kbr=o("t5"),Zbr=o(" \u2014 "),VO=a("a"),e5r=o("FlaxT5ForConditionalGeneration"),o5r=o(" (T5 model)"),r5r=l(),U9=a("li"),g2e=a("strong"),t5r=o("wav2vec2"),a5r=o(" \u2014 "),WO=a("a"),n5r=o("FlaxWav2Vec2ForPreTraining"),s5r=o(" (Wav2Vec2 model)"),l5r=l(),h2e=a("p"),i5r=o("Examples:"),d5r=l(),f(SA.$$.fragment),X8e=l(),Jc=a("h2"),J9=a("a"),p2e=a("span"),f(PA.$$.fragment),c5r=l(),_2e=a("span"),f5r=o("FlaxAutoModelForMaskedLM"),z8e=l(),Br=a("div"),f($A.$$.fragment),m5r=l(),Yc=a("p"),g5r=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a masked language modeling head) when created
with the `),u2e=a("code"),h5r=o("from_pretrained()"),p5r=o("class method or the "),b2e=a("code"),_5r=o("from_config()"),u5r=o(`class
method.`),b5r=l(),IA=a("p"),v5r=o("This class cannot be instantiated directly using "),v2e=a("code"),T5r=o("__init__()"),F5r=o(" (throws an error)."),C5r=l(),Ct=a("div"),f(jA.$$.fragment),M5r=l(),T2e=a("p"),E5r=o("Instantiates one of the model classes of the library (with a masked language modeling head) from a configuration."),y5r=l(),Kc=a("p"),w5r=o(`Note:
Loading a model from its configuration file does `),F2e=a("strong"),A5r=o("not"),L5r=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),C2e=a("code"),B5r=o("from_pretrained()"),k5r=o("to load the model weights."),x5r=l(),M2e=a("p"),R5r=o("Examples:"),S5r=l(),f(NA.$$.fragment),P5r=l(),Bo=a("div"),f(DA.$$.fragment),$5r=l(),E2e=a("p"),I5r=o("Instantiate one of the model classes of the library (with a masked language modeling head) from a pretrained model."),j5r=l(),yn=a("p"),N5r=o("The model class to instantiate is selected based on the "),y2e=a("code"),D5r=o("model_type"),q5r=o(` property of the config object (either
passed as an argument or loaded from `),w2e=a("code"),G5r=o("pretrained_model_name_or_path"),O5r=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),A2e=a("code"),X5r=o("pretrained_model_name_or_path"),z5r=o(":"),V5r=l(),ve=a("ul"),Y9=a("li"),L2e=a("strong"),W5r=o("albert"),Q5r=o(" \u2014 "),QO=a("a"),H5r=o("FlaxAlbertForMaskedLM"),U5r=o(" (ALBERT model)"),J5r=l(),K9=a("li"),B2e=a("strong"),Y5r=o("bart"),K5r=o(" \u2014 "),HO=a("a"),Z5r=o("FlaxBartForConditionalGeneration"),evr=o(" (BART model)"),ovr=l(),Z9=a("li"),k2e=a("strong"),rvr=o("bert"),tvr=o(" \u2014 "),UO=a("a"),avr=o("FlaxBertForMaskedLM"),nvr=o(" (BERT model)"),svr=l(),eC=a("li"),x2e=a("strong"),lvr=o("big_bird"),ivr=o(" \u2014 "),JO=a("a"),dvr=o("FlaxBigBirdForMaskedLM"),cvr=o(" (BigBird model)"),fvr=l(),oC=a("li"),R2e=a("strong"),mvr=o("distilbert"),gvr=o(" \u2014 "),YO=a("a"),hvr=o("FlaxDistilBertForMaskedLM"),pvr=o(" (DistilBERT model)"),_vr=l(),rC=a("li"),S2e=a("strong"),uvr=o("electra"),bvr=o(" \u2014 "),KO=a("a"),vvr=o("FlaxElectraForMaskedLM"),Tvr=o(" (ELECTRA model)"),Fvr=l(),tC=a("li"),P2e=a("strong"),Cvr=o("mbart"),Mvr=o(" \u2014 "),ZO=a("a"),Evr=o("FlaxMBartForConditionalGeneration"),yvr=o(" (mBART model)"),wvr=l(),aC=a("li"),$2e=a("strong"),Avr=o("roberta"),Lvr=o(" \u2014 "),eX=a("a"),Bvr=o("FlaxRobertaForMaskedLM"),kvr=o(" (RoBERTa model)"),xvr=l(),nC=a("li"),I2e=a("strong"),Rvr=o("roformer"),Svr=o(" \u2014 "),oX=a("a"),Pvr=o("FlaxRoFormerForMaskedLM"),$vr=o(" (RoFormer model)"),Ivr=l(),j2e=a("p"),jvr=o("Examples:"),Nvr=l(),f(qA.$$.fragment),V8e=l(),Zc=a("h2"),sC=a("a"),N2e=a("span"),f(GA.$$.fragment),Dvr=l(),D2e=a("span"),qvr=o("FlaxAutoModelForSeq2SeqLM"),W8e=l(),kr=a("div"),f(OA.$$.fragment),Gvr=l(),ef=a("p"),Ovr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence language modeling head) when created
with the `),q2e=a("code"),Xvr=o("from_pretrained()"),zvr=o("class method or the "),G2e=a("code"),Vvr=o("from_config()"),Wvr=o(`class
method.`),Qvr=l(),XA=a("p"),Hvr=o("This class cannot be instantiated directly using "),O2e=a("code"),Uvr=o("__init__()"),Jvr=o(" (throws an error)."),Yvr=l(),Mt=a("div"),f(zA.$$.fragment),Kvr=l(),X2e=a("p"),Zvr=o("Instantiates one of the model classes of the library (with a sequence-to-sequence language modeling head) from a configuration."),eTr=l(),of=a("p"),oTr=o(`Note:
Loading a model from its configuration file does `),z2e=a("strong"),rTr=o("not"),tTr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),V2e=a("code"),aTr=o("from_pretrained()"),nTr=o("to load the model weights."),sTr=l(),W2e=a("p"),lTr=o("Examples:"),iTr=l(),f(VA.$$.fragment),dTr=l(),ko=a("div"),f(WA.$$.fragment),cTr=l(),Q2e=a("p"),fTr=o("Instantiate one of the model classes of the library (with a sequence-to-sequence language modeling head) from a pretrained model."),mTr=l(),wn=a("p"),gTr=o("The model class to instantiate is selected based on the "),H2e=a("code"),hTr=o("model_type"),pTr=o(` property of the config object (either
passed as an argument or loaded from `),U2e=a("code"),_Tr=o("pretrained_model_name_or_path"),uTr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),J2e=a("code"),bTr=o("pretrained_model_name_or_path"),vTr=o(":"),TTr=l(),Te=a("ul"),lC=a("li"),Y2e=a("strong"),FTr=o("bart"),CTr=o(" \u2014 "),rX=a("a"),MTr=o("FlaxBartForConditionalGeneration"),ETr=o(" (BART model)"),yTr=l(),iC=a("li"),K2e=a("strong"),wTr=o("blenderbot"),ATr=o(" \u2014 "),tX=a("a"),LTr=o("FlaxBlenderbotForConditionalGeneration"),BTr=o(" (Blenderbot model)"),kTr=l(),dC=a("li"),Z2e=a("strong"),xTr=o("blenderbot-small"),RTr=o(" \u2014 "),aX=a("a"),STr=o("FlaxBlenderbotSmallForConditionalGeneration"),PTr=o(" (BlenderbotSmall model)"),$Tr=l(),cC=a("li"),e1e=a("strong"),ITr=o("encoder-decoder"),jTr=o(" \u2014 "),nX=a("a"),NTr=o("FlaxEncoderDecoderModel"),DTr=o(" (Encoder decoder model)"),qTr=l(),fC=a("li"),o1e=a("strong"),GTr=o("marian"),OTr=o(" \u2014 "),sX=a("a"),XTr=o("FlaxMarianMTModel"),zTr=o(" (Marian model)"),VTr=l(),mC=a("li"),r1e=a("strong"),WTr=o("mbart"),QTr=o(" \u2014 "),lX=a("a"),HTr=o("FlaxMBartForConditionalGeneration"),UTr=o(" (mBART model)"),JTr=l(),gC=a("li"),t1e=a("strong"),YTr=o("mt5"),KTr=o(" \u2014 "),iX=a("a"),ZTr=o("FlaxMT5ForConditionalGeneration"),e7r=o(" (mT5 model)"),o7r=l(),hC=a("li"),a1e=a("strong"),r7r=o("pegasus"),t7r=o(" \u2014 "),dX=a("a"),a7r=o("FlaxPegasusForConditionalGeneration"),n7r=o(" (Pegasus model)"),s7r=l(),pC=a("li"),n1e=a("strong"),l7r=o("t5"),i7r=o(" \u2014 "),cX=a("a"),d7r=o("FlaxT5ForConditionalGeneration"),c7r=o(" (T5 model)"),f7r=l(),s1e=a("p"),m7r=o("Examples:"),g7r=l(),f(QA.$$.fragment),Q8e=l(),rf=a("h2"),_C=a("a"),l1e=a("span"),f(HA.$$.fragment),h7r=l(),i1e=a("span"),p7r=o("FlaxAutoModelForSequenceClassification"),H8e=l(),xr=a("div"),f(UA.$$.fragment),_7r=l(),tf=a("p"),u7r=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence classification head) when created
with the `),d1e=a("code"),b7r=o("from_pretrained()"),v7r=o("class method or the "),c1e=a("code"),T7r=o("from_config()"),F7r=o(`class
method.`),C7r=l(),JA=a("p"),M7r=o("This class cannot be instantiated directly using "),f1e=a("code"),E7r=o("__init__()"),y7r=o(" (throws an error)."),w7r=l(),Et=a("div"),f(YA.$$.fragment),A7r=l(),m1e=a("p"),L7r=o("Instantiates one of the model classes of the library (with a sequence classification head) from a configuration."),B7r=l(),af=a("p"),k7r=o(`Note:
Loading a model from its configuration file does `),g1e=a("strong"),x7r=o("not"),R7r=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),h1e=a("code"),S7r=o("from_pretrained()"),P7r=o("to load the model weights."),$7r=l(),p1e=a("p"),I7r=o("Examples:"),j7r=l(),f(KA.$$.fragment),N7r=l(),xo=a("div"),f(ZA.$$.fragment),D7r=l(),_1e=a("p"),q7r=o("Instantiate one of the model classes of the library (with a sequence classification head) from a pretrained model."),G7r=l(),An=a("p"),O7r=o("The model class to instantiate is selected based on the "),u1e=a("code"),X7r=o("model_type"),z7r=o(` property of the config object (either
passed as an argument or loaded from `),b1e=a("code"),V7r=o("pretrained_model_name_or_path"),W7r=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),v1e=a("code"),Q7r=o("pretrained_model_name_or_path"),H7r=o(":"),U7r=l(),Fe=a("ul"),uC=a("li"),T1e=a("strong"),J7r=o("albert"),Y7r=o(" \u2014 "),fX=a("a"),K7r=o("FlaxAlbertForSequenceClassification"),Z7r=o(" (ALBERT model)"),eFr=l(),bC=a("li"),F1e=a("strong"),oFr=o("bart"),rFr=o(" \u2014 "),mX=a("a"),tFr=o("FlaxBartForSequenceClassification"),aFr=o(" (BART model)"),nFr=l(),vC=a("li"),C1e=a("strong"),sFr=o("bert"),lFr=o(" \u2014 "),gX=a("a"),iFr=o("FlaxBertForSequenceClassification"),dFr=o(" (BERT model)"),cFr=l(),TC=a("li"),M1e=a("strong"),fFr=o("big_bird"),mFr=o(" \u2014 "),hX=a("a"),gFr=o("FlaxBigBirdForSequenceClassification"),hFr=o(" (BigBird model)"),pFr=l(),FC=a("li"),E1e=a("strong"),_Fr=o("distilbert"),uFr=o(" \u2014 "),pX=a("a"),bFr=o("FlaxDistilBertForSequenceClassification"),vFr=o(" (DistilBERT model)"),TFr=l(),CC=a("li"),y1e=a("strong"),FFr=o("electra"),CFr=o(" \u2014 "),_X=a("a"),MFr=o("FlaxElectraForSequenceClassification"),EFr=o(" (ELECTRA model)"),yFr=l(),MC=a("li"),w1e=a("strong"),wFr=o("mbart"),AFr=o(" \u2014 "),uX=a("a"),LFr=o("FlaxMBartForSequenceClassification"),BFr=o(" (mBART model)"),kFr=l(),EC=a("li"),A1e=a("strong"),xFr=o("roberta"),RFr=o(" \u2014 "),bX=a("a"),SFr=o("FlaxRobertaForSequenceClassification"),PFr=o(" (RoBERTa model)"),$Fr=l(),yC=a("li"),L1e=a("strong"),IFr=o("roformer"),jFr=o(" \u2014 "),vX=a("a"),NFr=o("FlaxRoFormerForSequenceClassification"),DFr=o(" (RoFormer model)"),qFr=l(),B1e=a("p"),GFr=o("Examples:"),OFr=l(),f(e6.$$.fragment),U8e=l(),nf=a("h2"),wC=a("a"),k1e=a("span"),f(o6.$$.fragment),XFr=l(),x1e=a("span"),zFr=o("FlaxAutoModelForQuestionAnswering"),J8e=l(),Rr=a("div"),f(r6.$$.fragment),VFr=l(),sf=a("p"),WFr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a question answering head) when created
with the `),R1e=a("code"),QFr=o("from_pretrained()"),HFr=o("class method or the "),S1e=a("code"),UFr=o("from_config()"),JFr=o(`class
method.`),YFr=l(),t6=a("p"),KFr=o("This class cannot be instantiated directly using "),P1e=a("code"),ZFr=o("__init__()"),e9r=o(" (throws an error)."),o9r=l(),yt=a("div"),f(a6.$$.fragment),r9r=l(),$1e=a("p"),t9r=o("Instantiates one of the model classes of the library (with a question answering head) from a configuration."),a9r=l(),lf=a("p"),n9r=o(`Note:
Loading a model from its configuration file does `),I1e=a("strong"),s9r=o("not"),l9r=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),j1e=a("code"),i9r=o("from_pretrained()"),d9r=o("to load the model weights."),c9r=l(),N1e=a("p"),f9r=o("Examples:"),m9r=l(),f(n6.$$.fragment),g9r=l(),Ro=a("div"),f(s6.$$.fragment),h9r=l(),D1e=a("p"),p9r=o("Instantiate one of the model classes of the library (with a question answering head) from a pretrained model."),_9r=l(),Ln=a("p"),u9r=o("The model class to instantiate is selected based on the "),q1e=a("code"),b9r=o("model_type"),v9r=o(` property of the config object (either
passed as an argument or loaded from `),G1e=a("code"),T9r=o("pretrained_model_name_or_path"),F9r=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),O1e=a("code"),C9r=o("pretrained_model_name_or_path"),M9r=o(":"),E9r=l(),Ce=a("ul"),AC=a("li"),X1e=a("strong"),y9r=o("albert"),w9r=o(" \u2014 "),TX=a("a"),A9r=o("FlaxAlbertForQuestionAnswering"),L9r=o(" (ALBERT model)"),B9r=l(),LC=a("li"),z1e=a("strong"),k9r=o("bart"),x9r=o(" \u2014 "),FX=a("a"),R9r=o("FlaxBartForQuestionAnswering"),S9r=o(" (BART model)"),P9r=l(),BC=a("li"),V1e=a("strong"),$9r=o("bert"),I9r=o(" \u2014 "),CX=a("a"),j9r=o("FlaxBertForQuestionAnswering"),N9r=o(" (BERT model)"),D9r=l(),kC=a("li"),W1e=a("strong"),q9r=o("big_bird"),G9r=o(" \u2014 "),MX=a("a"),O9r=o("FlaxBigBirdForQuestionAnswering"),X9r=o(" (BigBird model)"),z9r=l(),xC=a("li"),Q1e=a("strong"),V9r=o("distilbert"),W9r=o(" \u2014 "),EX=a("a"),Q9r=o("FlaxDistilBertForQuestionAnswering"),H9r=o(" (DistilBERT model)"),U9r=l(),RC=a("li"),H1e=a("strong"),J9r=o("electra"),Y9r=o(" \u2014 "),yX=a("a"),K9r=o("FlaxElectraForQuestionAnswering"),Z9r=o(" (ELECTRA model)"),eCr=l(),SC=a("li"),U1e=a("strong"),oCr=o("mbart"),rCr=o(" \u2014 "),wX=a("a"),tCr=o("FlaxMBartForQuestionAnswering"),aCr=o(" (mBART model)"),nCr=l(),PC=a("li"),J1e=a("strong"),sCr=o("roberta"),lCr=o(" \u2014 "),AX=a("a"),iCr=o("FlaxRobertaForQuestionAnswering"),dCr=o(" (RoBERTa model)"),cCr=l(),$C=a("li"),Y1e=a("strong"),fCr=o("roformer"),mCr=o(" \u2014 "),LX=a("a"),gCr=o("FlaxRoFormerForQuestionAnswering"),hCr=o(" (RoFormer model)"),pCr=l(),K1e=a("p"),_Cr=o("Examples:"),uCr=l(),f(l6.$$.fragment),Y8e=l(),df=a("h2"),IC=a("a"),Z1e=a("span"),f(i6.$$.fragment),bCr=l(),ebe=a("span"),vCr=o("FlaxAutoModelForTokenClassification"),K8e=l(),Sr=a("div"),f(d6.$$.fragment),TCr=l(),cf=a("p"),FCr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a token classification head) when created
with the `),obe=a("code"),CCr=o("from_pretrained()"),MCr=o("class method or the "),rbe=a("code"),ECr=o("from_config()"),yCr=o(`class
method.`),wCr=l(),c6=a("p"),ACr=o("This class cannot be instantiated directly using "),tbe=a("code"),LCr=o("__init__()"),BCr=o(" (throws an error)."),kCr=l(),wt=a("div"),f(f6.$$.fragment),xCr=l(),abe=a("p"),RCr=o("Instantiates one of the model classes of the library (with a token classification head) from a configuration."),SCr=l(),ff=a("p"),PCr=o(`Note:
Loading a model from its configuration file does `),nbe=a("strong"),$Cr=o("not"),ICr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),sbe=a("code"),jCr=o("from_pretrained()"),NCr=o("to load the model weights."),DCr=l(),lbe=a("p"),qCr=o("Examples:"),GCr=l(),f(m6.$$.fragment),OCr=l(),So=a("div"),f(g6.$$.fragment),XCr=l(),ibe=a("p"),zCr=o("Instantiate one of the model classes of the library (with a token classification head) from a pretrained model."),VCr=l(),Bn=a("p"),WCr=o("The model class to instantiate is selected based on the "),dbe=a("code"),QCr=o("model_type"),HCr=o(` property of the config object (either
passed as an argument or loaded from `),cbe=a("code"),UCr=o("pretrained_model_name_or_path"),JCr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),fbe=a("code"),YCr=o("pretrained_model_name_or_path"),KCr=o(":"),ZCr=l(),so=a("ul"),jC=a("li"),mbe=a("strong"),e4r=o("albert"),o4r=o(" \u2014 "),BX=a("a"),r4r=o("FlaxAlbertForTokenClassification"),t4r=o(" (ALBERT model)"),a4r=l(),NC=a("li"),gbe=a("strong"),n4r=o("bert"),s4r=o(" \u2014 "),kX=a("a"),l4r=o("FlaxBertForTokenClassification"),i4r=o(" (BERT model)"),d4r=l(),DC=a("li"),hbe=a("strong"),c4r=o("big_bird"),f4r=o(" \u2014 "),xX=a("a"),m4r=o("FlaxBigBirdForTokenClassification"),g4r=o(" (BigBird model)"),h4r=l(),qC=a("li"),pbe=a("strong"),p4r=o("distilbert"),_4r=o(" \u2014 "),RX=a("a"),u4r=o("FlaxDistilBertForTokenClassification"),b4r=o(" (DistilBERT model)"),v4r=l(),GC=a("li"),_be=a("strong"),T4r=o("electra"),F4r=o(" \u2014 "),SX=a("a"),C4r=o("FlaxElectraForTokenClassification"),M4r=o(" (ELECTRA model)"),E4r=l(),OC=a("li"),ube=a("strong"),y4r=o("roberta"),w4r=o(" \u2014 "),PX=a("a"),A4r=o("FlaxRobertaForTokenClassification"),L4r=o(" (RoBERTa model)"),B4r=l(),XC=a("li"),bbe=a("strong"),k4r=o("roformer"),x4r=o(" \u2014 "),$X=a("a"),R4r=o("FlaxRoFormerForTokenClassification"),S4r=o(" (RoFormer model)"),P4r=l(),vbe=a("p"),$4r=o("Examples:"),I4r=l(),f(h6.$$.fragment),Z8e=l(),mf=a("h2"),zC=a("a"),Tbe=a("span"),f(p6.$$.fragment),j4r=l(),Fbe=a("span"),N4r=o("FlaxAutoModelForMultipleChoice"),eBe=l(),Pr=a("div"),f(_6.$$.fragment),D4r=l(),gf=a("p"),q4r=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a multiple choice head) when created
with the `),Cbe=a("code"),G4r=o("from_pretrained()"),O4r=o("class method or the "),Mbe=a("code"),X4r=o("from_config()"),z4r=o(`class
method.`),V4r=l(),u6=a("p"),W4r=o("This class cannot be instantiated directly using "),Ebe=a("code"),Q4r=o("__init__()"),H4r=o(" (throws an error)."),U4r=l(),At=a("div"),f(b6.$$.fragment),J4r=l(),ybe=a("p"),Y4r=o("Instantiates one of the model classes of the library (with a multiple choice head) from a configuration."),K4r=l(),hf=a("p"),Z4r=o(`Note:
Loading a model from its configuration file does `),wbe=a("strong"),eMr=o("not"),oMr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Abe=a("code"),rMr=o("from_pretrained()"),tMr=o("to load the model weights."),aMr=l(),Lbe=a("p"),nMr=o("Examples:"),sMr=l(),f(v6.$$.fragment),lMr=l(),Po=a("div"),f(T6.$$.fragment),iMr=l(),Bbe=a("p"),dMr=o("Instantiate one of the model classes of the library (with a multiple choice head) from a pretrained model."),cMr=l(),kn=a("p"),fMr=o("The model class to instantiate is selected based on the "),kbe=a("code"),mMr=o("model_type"),gMr=o(` property of the config object (either
passed as an argument or loaded from `),xbe=a("code"),hMr=o("pretrained_model_name_or_path"),pMr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Rbe=a("code"),_Mr=o("pretrained_model_name_or_path"),uMr=o(":"),bMr=l(),lo=a("ul"),VC=a("li"),Sbe=a("strong"),vMr=o("albert"),TMr=o(" \u2014 "),IX=a("a"),FMr=o("FlaxAlbertForMultipleChoice"),CMr=o(" (ALBERT model)"),MMr=l(),WC=a("li"),Pbe=a("strong"),EMr=o("bert"),yMr=o(" \u2014 "),jX=a("a"),wMr=o("FlaxBertForMultipleChoice"),AMr=o(" (BERT model)"),LMr=l(),QC=a("li"),$be=a("strong"),BMr=o("big_bird"),kMr=o(" \u2014 "),NX=a("a"),xMr=o("FlaxBigBirdForMultipleChoice"),RMr=o(" (BigBird model)"),SMr=l(),HC=a("li"),Ibe=a("strong"),PMr=o("distilbert"),$Mr=o(" \u2014 "),DX=a("a"),IMr=o("FlaxDistilBertForMultipleChoice"),jMr=o(" (DistilBERT model)"),NMr=l(),UC=a("li"),jbe=a("strong"),DMr=o("electra"),qMr=o(" \u2014 "),qX=a("a"),GMr=o("FlaxElectraForMultipleChoice"),OMr=o(" (ELECTRA model)"),XMr=l(),JC=a("li"),Nbe=a("strong"),zMr=o("roberta"),VMr=o(" \u2014 "),GX=a("a"),WMr=o("FlaxRobertaForMultipleChoice"),QMr=o(" (RoBERTa model)"),HMr=l(),YC=a("li"),Dbe=a("strong"),UMr=o("roformer"),JMr=o(" \u2014 "),OX=a("a"),YMr=o("FlaxRoFormerForMultipleChoice"),KMr=o(" (RoFormer model)"),ZMr=l(),qbe=a("p"),eEr=o("Examples:"),oEr=l(),f(F6.$$.fragment),oBe=l(),pf=a("h2"),KC=a("a"),Gbe=a("span"),f(C6.$$.fragment),rEr=l(),Obe=a("span"),tEr=o("FlaxAutoModelForNextSentencePrediction"),rBe=l(),$r=a("div"),f(M6.$$.fragment),aEr=l(),_f=a("p"),nEr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a next sentence prediction head) when created
with the `),Xbe=a("code"),sEr=o("from_pretrained()"),lEr=o("class method or the "),zbe=a("code"),iEr=o("from_config()"),dEr=o(`class
method.`),cEr=l(),E6=a("p"),fEr=o("This class cannot be instantiated directly using "),Vbe=a("code"),mEr=o("__init__()"),gEr=o(" (throws an error)."),hEr=l(),Lt=a("div"),f(y6.$$.fragment),pEr=l(),Wbe=a("p"),_Er=o("Instantiates one of the model classes of the library (with a next sentence prediction head) from a configuration."),uEr=l(),uf=a("p"),bEr=o(`Note:
Loading a model from its configuration file does `),Qbe=a("strong"),vEr=o("not"),TEr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Hbe=a("code"),FEr=o("from_pretrained()"),CEr=o("to load the model weights."),MEr=l(),Ube=a("p"),EEr=o("Examples:"),yEr=l(),f(w6.$$.fragment),wEr=l(),$o=a("div"),f(A6.$$.fragment),AEr=l(),Jbe=a("p"),LEr=o("Instantiate one of the model classes of the library (with a next sentence prediction head) from a pretrained model."),BEr=l(),xn=a("p"),kEr=o("The model class to instantiate is selected based on the "),Ybe=a("code"),xEr=o("model_type"),REr=o(` property of the config object (either
passed as an argument or loaded from `),Kbe=a("code"),SEr=o("pretrained_model_name_or_path"),PEr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Zbe=a("code"),$Er=o("pretrained_model_name_or_path"),IEr=o(":"),jEr=l(),e5e=a("ul"),ZC=a("li"),o5e=a("strong"),NEr=o("bert"),DEr=o(" \u2014 "),XX=a("a"),qEr=o("FlaxBertForNextSentencePrediction"),GEr=o(" (BERT model)"),OEr=l(),r5e=a("p"),XEr=o("Examples:"),zEr=l(),f(L6.$$.fragment),tBe=l(),bf=a("h2"),e4=a("a"),t5e=a("span"),f(B6.$$.fragment),VEr=l(),a5e=a("span"),WEr=o("FlaxAutoModelForImageClassification"),aBe=l(),Ir=a("div"),f(k6.$$.fragment),QEr=l(),vf=a("p"),HEr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a image classification head) when created
with the `),n5e=a("code"),UEr=o("from_pretrained()"),JEr=o("class method or the "),s5e=a("code"),YEr=o("from_config()"),KEr=o(`class
method.`),ZEr=l(),x6=a("p"),e3r=o("This class cannot be instantiated directly using "),l5e=a("code"),o3r=o("__init__()"),r3r=o(" (throws an error)."),t3r=l(),Bt=a("div"),f(R6.$$.fragment),a3r=l(),i5e=a("p"),n3r=o("Instantiates one of the model classes of the library (with a image classification head) from a configuration."),s3r=l(),Tf=a("p"),l3r=o(`Note:
Loading a model from its configuration file does `),d5e=a("strong"),i3r=o("not"),d3r=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),c5e=a("code"),c3r=o("from_pretrained()"),f3r=o("to load the model weights."),m3r=l(),f5e=a("p"),g3r=o("Examples:"),h3r=l(),f(S6.$$.fragment),p3r=l(),Io=a("div"),f(P6.$$.fragment),_3r=l(),m5e=a("p"),u3r=o("Instantiate one of the model classes of the library (with a image classification head) from a pretrained model."),b3r=l(),Rn=a("p"),v3r=o("The model class to instantiate is selected based on the "),g5e=a("code"),T3r=o("model_type"),F3r=o(` property of the config object (either
passed as an argument or loaded from `),h5e=a("code"),C3r=o("pretrained_model_name_or_path"),M3r=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),p5e=a("code"),E3r=o("pretrained_model_name_or_path"),y3r=o(":"),w3r=l(),$6=a("ul"),o4=a("li"),_5e=a("strong"),A3r=o("beit"),L3r=o(" \u2014 "),zX=a("a"),B3r=o("FlaxBeitForImageClassification"),k3r=o(" (BEiT model)"),x3r=l(),r4=a("li"),u5e=a("strong"),R3r=o("vit"),S3r=o(" \u2014 "),VX=a("a"),P3r=o("FlaxViTForImageClassification"),$3r=o(" (ViT model)"),I3r=l(),b5e=a("p"),j3r=o("Examples:"),N3r=l(),f(I6.$$.fragment),nBe=l(),Ff=a("h2"),t4=a("a"),v5e=a("span"),f(j6.$$.fragment),D3r=l(),T5e=a("span"),q3r=o("FlaxAutoModelForVision2Seq"),sBe=l(),jr=a("div"),f(N6.$$.fragment),G3r=l(),Cf=a("p"),O3r=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a vision-to-text modeling head) when created
with the `),F5e=a("code"),X3r=o("from_pretrained()"),z3r=o("class method or the "),C5e=a("code"),V3r=o("from_config()"),W3r=o(`class
method.`),Q3r=l(),D6=a("p"),H3r=o("This class cannot be instantiated directly using "),M5e=a("code"),U3r=o("__init__()"),J3r=o(" (throws an error)."),Y3r=l(),kt=a("div"),f(q6.$$.fragment),K3r=l(),E5e=a("p"),Z3r=o("Instantiates one of the model classes of the library (with a vision-to-text modeling head) from a configuration."),eyr=l(),Mf=a("p"),oyr=o(`Note:
Loading a model from its configuration file does `),y5e=a("strong"),ryr=o("not"),tyr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),w5e=a("code"),ayr=o("from_pretrained()"),nyr=o("to load the model weights."),syr=l(),A5e=a("p"),lyr=o("Examples:"),iyr=l(),f(G6.$$.fragment),dyr=l(),jo=a("div"),f(O6.$$.fragment),cyr=l(),L5e=a("p"),fyr=o("Instantiate one of the model classes of the library (with a vision-to-text modeling head) from a pretrained model."),myr=l(),Sn=a("p"),gyr=o("The model class to instantiate is selected based on the "),B5e=a("code"),hyr=o("model_type"),pyr=o(` property of the config object (either
passed as an argument or loaded from `),k5e=a("code"),_yr=o("pretrained_model_name_or_path"),uyr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),x5e=a("code"),byr=o("pretrained_model_name_or_path"),vyr=o(":"),Tyr=l(),R5e=a("ul"),a4=a("li"),S5e=a("strong"),Fyr=o("vision-encoder-decoder"),Cyr=o(" \u2014 "),WX=a("a"),Myr=o("FlaxVisionEncoderDecoderModel"),Eyr=o(" (Vision Encoder decoder model)"),yyr=l(),P5e=a("p"),wyr=o("Examples:"),Ayr=l(),f(X6.$$.fragment),this.h()},l(d){const u=$_t('[data-svelte="svelte-1phssyn"]',document.head);J=n(u,"META",{name:!0,content:!0}),u.forEach(t),Ae=i(d),ie=n(d,"H1",{class:!0});var z6=s(ie);me=n(z6,"A",{id:!0,class:!0,href:!0});var $5e=s(me);to=n($5e,"SPAN",{});var I5e=s(to);m(ce.$$.fragment,I5e),I5e.forEach(t),$5e.forEach(t),ue=i(z6),Do=n(z6,"SPAN",{});var Byr=s(Do);Ai=r(Byr,"Auto Classes"),Byr.forEach(t),z6.forEach(t),yf=i(d),sa=n(d,"P",{});var iBe=s(sa);Li=r(iBe,`In many cases, the architecture you want to use can be guessed from the name or the path of the pretrained model you
are supplying to the `),Bi=n(iBe,"CODE",{});var kyr=s(Bi);rM=r(kyr,"from_pretrained()"),kyr.forEach(t),wf=r(iBe,` method. AutoClasses are here to do this job for you so that you
automatically retrieve the relevant model given the name/path to the pretrained weights/config/vocabulary.`),iBe.forEach(t),ye=i(d),io=n(d,"P",{});var n4=s(io);ki=r(n4,"Instantiating one of "),Pn=n(n4,"A",{href:!0});var xyr=s(Pn);tM=r(xyr,"AutoConfig"),xyr.forEach(t),$n=r(n4,", "),In=n(n4,"A",{href:!0});var Ryr=s(In);aM=r(Ryr,"AutoModel"),Ryr.forEach(t),xi=r(n4,`, and
`),jn=n(n4,"A",{href:!0});var Syr=s(jn);nM=r(Syr,"AutoTokenizer"),Syr.forEach(t),Ri=r(n4," will directly create a class of the relevant architecture. For instance"),n4.forEach(t),Af=i(d),m($a.$$.fragment,d),co=i(d),ge=n(d,"P",{});var dBe=s(ge);G0=r(dBe,"will create a model that is an instance of "),Si=n(dBe,"A",{href:!0});var Pyr=s(Si);O0=r(Pyr,"BertModel"),Pyr.forEach(t),X0=r(dBe,"."),dBe.forEach(t),qo=i(d),Ia=n(d,"P",{});var cBe=s(Ia);z0=r(cBe,"There is one class of "),Lf=n(cBe,"CODE",{});var $yr=s(Lf);V0=r($yr,"AutoModel"),$yr.forEach(t),uxe=r(cBe," for each task, and for each backend (PyTorch, TensorFlow, or Flax)."),cBe.forEach(t),iLe=i(d),Pi=n(d,"H2",{class:!0});var fBe=s(Pi);Bf=n(fBe,"A",{id:!0,class:!0,href:!0});var Iyr=s(Bf);DV=n(Iyr,"SPAN",{});var jyr=s(DV);m(sM.$$.fragment,jyr),jyr.forEach(t),Iyr.forEach(t),bxe=i(fBe),qV=n(fBe,"SPAN",{});var Nyr=s(qV);vxe=r(Nyr,"Extending the Auto Classes"),Nyr.forEach(t),fBe.forEach(t),dLe=i(d),Nn=n(d,"P",{});var QX=s(Nn);Txe=r(QX,`Each of the auto classes has a method to be extended with your custom classes. For instance, if you have defined a
custom class of model `),GV=n(QX,"CODE",{});var Dyr=s(GV);Fxe=r(Dyr,"NewModel"),Dyr.forEach(t),Cxe=r(QX,", make sure you have a "),OV=n(QX,"CODE",{});var qyr=s(OV);Mxe=r(qyr,"NewModelConfig"),qyr.forEach(t),Exe=r(QX,` then you can add those to the auto
classes like this:`),QX.forEach(t),cLe=i(d),m(lM.$$.fragment,d),fLe=i(d),W0=n(d,"P",{});var Gyr=s(W0);yxe=r(Gyr,"You will then be able to use the auto classes like you would usually do!"),Gyr.forEach(t),mLe=i(d),m(kf.$$.fragment,d),gLe=i(d),$i=n(d,"H2",{class:!0});var mBe=s($i);xf=n(mBe,"A",{id:!0,class:!0,href:!0});var Oyr=s(xf);XV=n(Oyr,"SPAN",{});var Xyr=s(XV);m(iM.$$.fragment,Xyr),Xyr.forEach(t),Oyr.forEach(t),wxe=i(mBe),zV=n(mBe,"SPAN",{});var zyr=s(zV);Axe=r(zyr,"AutoConfig"),zyr.forEach(t),mBe.forEach(t),hLe=i(d),Go=n(d,"DIV",{class:!0});var $s=s(Go);m(dM.$$.fragment,$s),Lxe=i($s),cM=n($s,"P",{});var gBe=s(cM);Bxe=r(gBe,`This is a generic configuration class that will be instantiated as one of the configuration classes of the library
when created with the `),Q0=n(gBe,"A",{href:!0});var Vyr=s(Q0);kxe=r(Vyr,"from_pretrained()"),Vyr.forEach(t),xxe=r(gBe," class method."),gBe.forEach(t),Rxe=i($s),fM=n($s,"P",{});var hBe=s(fM);Sxe=r(hBe,"This class cannot be instantiated directly using "),VV=n(hBe,"CODE",{});var Wyr=s(VV);Pxe=r(Wyr,"__init__()"),Wyr.forEach(t),$xe=r(hBe," (throws an error)."),hBe.forEach(t),Ixe=i($s),fo=n($s,"DIV",{class:!0});var ia=s(fo);m(mM.$$.fragment,ia),jxe=i(ia),WV=n(ia,"P",{});var Qyr=s(WV);Nxe=r(Qyr,"Instantiate one of the configuration classes of the library from a pretrained model configuration."),Qyr.forEach(t),Dxe=i(ia),Ii=n(ia,"P",{});var HX=s(Ii);qxe=r(HX,"The configuration class to instantiate is selected based on the "),QV=n(HX,"CODE",{});var Hyr=s(QV);Gxe=r(Hyr,"model_type"),Hyr.forEach(t),Oxe=r(HX,` property of the config object that
is loaded, or when it\u2019s missing, by falling back to using pattern matching on `),HV=n(HX,"CODE",{});var Uyr=s(HV);Xxe=r(Uyr,"pretrained_model_name_or_path"),Uyr.forEach(t),zxe=r(HX,":"),HX.forEach(t),Vxe=i(ia),v=n(ia,"UL",{});var T=s(v);Rf=n(T,"LI",{});var j5e=s(Rf);UV=n(j5e,"STRONG",{});var Jyr=s(UV);Wxe=r(Jyr,"albert"),Jyr.forEach(t),Qxe=r(j5e," \u2014 "),H0=n(j5e,"A",{href:!0});var Yyr=s(H0);Hxe=r(Yyr,"AlbertConfig"),Yyr.forEach(t),Uxe=r(j5e," (ALBERT model)"),j5e.forEach(t),Jxe=i(T),Sf=n(T,"LI",{});var N5e=s(Sf);JV=n(N5e,"STRONG",{});var Kyr=s(JV);Yxe=r(Kyr,"bart"),Kyr.forEach(t),Kxe=r(N5e," \u2014 "),U0=n(N5e,"A",{href:!0});var Zyr=s(U0);Zxe=r(Zyr,"BartConfig"),Zyr.forEach(t),eRe=r(N5e," (BART model)"),N5e.forEach(t),oRe=i(T),Pf=n(T,"LI",{});var D5e=s(Pf);YV=n(D5e,"STRONG",{});var ewr=s(YV);rRe=r(ewr,"beit"),ewr.forEach(t),tRe=r(D5e," \u2014 "),J0=n(D5e,"A",{href:!0});var owr=s(J0);aRe=r(owr,"BeitConfig"),owr.forEach(t),nRe=r(D5e," (BEiT model)"),D5e.forEach(t),sRe=i(T),$f=n(T,"LI",{});var q5e=s($f);KV=n(q5e,"STRONG",{});var rwr=s(KV);lRe=r(rwr,"bert"),rwr.forEach(t),iRe=r(q5e," \u2014 "),Y0=n(q5e,"A",{href:!0});var twr=s(Y0);dRe=r(twr,"BertConfig"),twr.forEach(t),cRe=r(q5e," (BERT model)"),q5e.forEach(t),fRe=i(T),If=n(T,"LI",{});var G5e=s(If);ZV=n(G5e,"STRONG",{});var awr=s(ZV);mRe=r(awr,"bert-generation"),awr.forEach(t),gRe=r(G5e," \u2014 "),K0=n(G5e,"A",{href:!0});var nwr=s(K0);hRe=r(nwr,"BertGenerationConfig"),nwr.forEach(t),pRe=r(G5e," (Bert Generation model)"),G5e.forEach(t),_Re=i(T),jf=n(T,"LI",{});var O5e=s(jf);eW=n(O5e,"STRONG",{});var swr=s(eW);uRe=r(swr,"big_bird"),swr.forEach(t),bRe=r(O5e," \u2014 "),Z0=n(O5e,"A",{href:!0});var lwr=s(Z0);vRe=r(lwr,"BigBirdConfig"),lwr.forEach(t),TRe=r(O5e," (BigBird model)"),O5e.forEach(t),FRe=i(T),Nf=n(T,"LI",{});var X5e=s(Nf);oW=n(X5e,"STRONG",{});var iwr=s(oW);CRe=r(iwr,"bigbird_pegasus"),iwr.forEach(t),MRe=r(X5e," \u2014 "),eL=n(X5e,"A",{href:!0});var dwr=s(eL);ERe=r(dwr,"BigBirdPegasusConfig"),dwr.forEach(t),yRe=r(X5e," (BigBirdPegasus model)"),X5e.forEach(t),wRe=i(T),Df=n(T,"LI",{});var z5e=s(Df);rW=n(z5e,"STRONG",{});var cwr=s(rW);ARe=r(cwr,"blenderbot"),cwr.forEach(t),LRe=r(z5e," \u2014 "),oL=n(z5e,"A",{href:!0});var fwr=s(oL);BRe=r(fwr,"BlenderbotConfig"),fwr.forEach(t),kRe=r(z5e," (Blenderbot model)"),z5e.forEach(t),xRe=i(T),qf=n(T,"LI",{});var V5e=s(qf);tW=n(V5e,"STRONG",{});var mwr=s(tW);RRe=r(mwr,"blenderbot-small"),mwr.forEach(t),SRe=r(V5e," \u2014 "),rL=n(V5e,"A",{href:!0});var gwr=s(rL);PRe=r(gwr,"BlenderbotSmallConfig"),gwr.forEach(t),$Re=r(V5e," (BlenderbotSmall model)"),V5e.forEach(t),IRe=i(T),Gf=n(T,"LI",{});var W5e=s(Gf);aW=n(W5e,"STRONG",{});var hwr=s(aW);jRe=r(hwr,"camembert"),hwr.forEach(t),NRe=r(W5e," \u2014 "),tL=n(W5e,"A",{href:!0});var pwr=s(tL);DRe=r(pwr,"CamembertConfig"),pwr.forEach(t),qRe=r(W5e," (CamemBERT model)"),W5e.forEach(t),GRe=i(T),Of=n(T,"LI",{});var Q5e=s(Of);nW=n(Q5e,"STRONG",{});var _wr=s(nW);ORe=r(_wr,"canine"),_wr.forEach(t),XRe=r(Q5e," \u2014 "),aL=n(Q5e,"A",{href:!0});var uwr=s(aL);zRe=r(uwr,"CanineConfig"),uwr.forEach(t),VRe=r(Q5e," (Canine model)"),Q5e.forEach(t),WRe=i(T),Xf=n(T,"LI",{});var H5e=s(Xf);sW=n(H5e,"STRONG",{});var bwr=s(sW);QRe=r(bwr,"clip"),bwr.forEach(t),HRe=r(H5e," \u2014 "),nL=n(H5e,"A",{href:!0});var vwr=s(nL);URe=r(vwr,"CLIPConfig"),vwr.forEach(t),JRe=r(H5e," (CLIP model)"),H5e.forEach(t),YRe=i(T),zf=n(T,"LI",{});var U5e=s(zf);lW=n(U5e,"STRONG",{});var Twr=s(lW);KRe=r(Twr,"convbert"),Twr.forEach(t),ZRe=r(U5e," \u2014 "),sL=n(U5e,"A",{href:!0});var Fwr=s(sL);eSe=r(Fwr,"ConvBertConfig"),Fwr.forEach(t),oSe=r(U5e," (ConvBERT model)"),U5e.forEach(t),rSe=i(T),Vf=n(T,"LI",{});var J5e=s(Vf);iW=n(J5e,"STRONG",{});var Cwr=s(iW);tSe=r(Cwr,"convnext"),Cwr.forEach(t),aSe=r(J5e," \u2014 "),lL=n(J5e,"A",{href:!0});var Mwr=s(lL);nSe=r(Mwr,"ConvNextConfig"),Mwr.forEach(t),sSe=r(J5e," (ConvNext model)"),J5e.forEach(t),lSe=i(T),Wf=n(T,"LI",{});var Y5e=s(Wf);dW=n(Y5e,"STRONG",{});var Ewr=s(dW);iSe=r(Ewr,"ctrl"),Ewr.forEach(t),dSe=r(Y5e," \u2014 "),iL=n(Y5e,"A",{href:!0});var ywr=s(iL);cSe=r(ywr,"CTRLConfig"),ywr.forEach(t),fSe=r(Y5e," (CTRL model)"),Y5e.forEach(t),mSe=i(T),Qf=n(T,"LI",{});var K5e=s(Qf);cW=n(K5e,"STRONG",{});var wwr=s(cW);gSe=r(wwr,"deberta"),wwr.forEach(t),hSe=r(K5e," \u2014 "),dL=n(K5e,"A",{href:!0});var Awr=s(dL);pSe=r(Awr,"DebertaConfig"),Awr.forEach(t),_Se=r(K5e," (DeBERTa model)"),K5e.forEach(t),uSe=i(T),Hf=n(T,"LI",{});var Z5e=s(Hf);fW=n(Z5e,"STRONG",{});var Lwr=s(fW);bSe=r(Lwr,"deberta-v2"),Lwr.forEach(t),vSe=r(Z5e," \u2014 "),cL=n(Z5e,"A",{href:!0});var Bwr=s(cL);TSe=r(Bwr,"DebertaV2Config"),Bwr.forEach(t),FSe=r(Z5e," (DeBERTa-v2 model)"),Z5e.forEach(t),CSe=i(T),Uf=n(T,"LI",{});var eve=s(Uf);mW=n(eve,"STRONG",{});var kwr=s(mW);MSe=r(kwr,"deit"),kwr.forEach(t),ESe=r(eve," \u2014 "),fL=n(eve,"A",{href:!0});var xwr=s(fL);ySe=r(xwr,"DeiTConfig"),xwr.forEach(t),wSe=r(eve," (DeiT model)"),eve.forEach(t),ASe=i(T),Jf=n(T,"LI",{});var ove=s(Jf);gW=n(ove,"STRONG",{});var Rwr=s(gW);LSe=r(Rwr,"detr"),Rwr.forEach(t),BSe=r(ove," \u2014 "),mL=n(ove,"A",{href:!0});var Swr=s(mL);kSe=r(Swr,"DetrConfig"),Swr.forEach(t),xSe=r(ove," (DETR model)"),ove.forEach(t),RSe=i(T),Yf=n(T,"LI",{});var rve=s(Yf);hW=n(rve,"STRONG",{});var Pwr=s(hW);SSe=r(Pwr,"distilbert"),Pwr.forEach(t),PSe=r(rve," \u2014 "),gL=n(rve,"A",{href:!0});var $wr=s(gL);$Se=r($wr,"DistilBertConfig"),$wr.forEach(t),ISe=r(rve," (DistilBERT model)"),rve.forEach(t),jSe=i(T),Kf=n(T,"LI",{});var tve=s(Kf);pW=n(tve,"STRONG",{});var Iwr=s(pW);NSe=r(Iwr,"dpr"),Iwr.forEach(t),DSe=r(tve," \u2014 "),hL=n(tve,"A",{href:!0});var jwr=s(hL);qSe=r(jwr,"DPRConfig"),jwr.forEach(t),GSe=r(tve," (DPR model)"),tve.forEach(t),OSe=i(T),Zf=n(T,"LI",{});var ave=s(Zf);_W=n(ave,"STRONG",{});var Nwr=s(_W);XSe=r(Nwr,"electra"),Nwr.forEach(t),zSe=r(ave," \u2014 "),pL=n(ave,"A",{href:!0});var Dwr=s(pL);VSe=r(Dwr,"ElectraConfig"),Dwr.forEach(t),WSe=r(ave," (ELECTRA model)"),ave.forEach(t),QSe=i(T),em=n(T,"LI",{});var nve=s(em);uW=n(nve,"STRONG",{});var qwr=s(uW);HSe=r(qwr,"encoder-decoder"),qwr.forEach(t),USe=r(nve," \u2014 "),_L=n(nve,"A",{href:!0});var Gwr=s(_L);JSe=r(Gwr,"EncoderDecoderConfig"),Gwr.forEach(t),YSe=r(nve," (Encoder decoder model)"),nve.forEach(t),KSe=i(T),om=n(T,"LI",{});var sve=s(om);bW=n(sve,"STRONG",{});var Owr=s(bW);ZSe=r(Owr,"flaubert"),Owr.forEach(t),ePe=r(sve," \u2014 "),uL=n(sve,"A",{href:!0});var Xwr=s(uL);oPe=r(Xwr,"FlaubertConfig"),Xwr.forEach(t),rPe=r(sve," (FlauBERT model)"),sve.forEach(t),tPe=i(T),rm=n(T,"LI",{});var lve=s(rm);vW=n(lve,"STRONG",{});var zwr=s(vW);aPe=r(zwr,"fnet"),zwr.forEach(t),nPe=r(lve," \u2014 "),bL=n(lve,"A",{href:!0});var Vwr=s(bL);sPe=r(Vwr,"FNetConfig"),Vwr.forEach(t),lPe=r(lve," (FNet model)"),lve.forEach(t),iPe=i(T),tm=n(T,"LI",{});var ive=s(tm);TW=n(ive,"STRONG",{});var Wwr=s(TW);dPe=r(Wwr,"fsmt"),Wwr.forEach(t),cPe=r(ive," \u2014 "),vL=n(ive,"A",{href:!0});var Qwr=s(vL);fPe=r(Qwr,"FSMTConfig"),Qwr.forEach(t),mPe=r(ive," (FairSeq Machine-Translation model)"),ive.forEach(t),gPe=i(T),am=n(T,"LI",{});var dve=s(am);FW=n(dve,"STRONG",{});var Hwr=s(FW);hPe=r(Hwr,"funnel"),Hwr.forEach(t),pPe=r(dve," \u2014 "),TL=n(dve,"A",{href:!0});var Uwr=s(TL);_Pe=r(Uwr,"FunnelConfig"),Uwr.forEach(t),uPe=r(dve," (Funnel Transformer model)"),dve.forEach(t),bPe=i(T),nm=n(T,"LI",{});var cve=s(nm);CW=n(cve,"STRONG",{});var Jwr=s(CW);vPe=r(Jwr,"gpt2"),Jwr.forEach(t),TPe=r(cve," \u2014 "),FL=n(cve,"A",{href:!0});var Ywr=s(FL);FPe=r(Ywr,"GPT2Config"),Ywr.forEach(t),CPe=r(cve," (OpenAI GPT-2 model)"),cve.forEach(t),MPe=i(T),sm=n(T,"LI",{});var fve=s(sm);MW=n(fve,"STRONG",{});var Kwr=s(MW);EPe=r(Kwr,"gpt_neo"),Kwr.forEach(t),yPe=r(fve," \u2014 "),CL=n(fve,"A",{href:!0});var Zwr=s(CL);wPe=r(Zwr,"GPTNeoConfig"),Zwr.forEach(t),APe=r(fve," (GPT Neo model)"),fve.forEach(t),LPe=i(T),lm=n(T,"LI",{});var mve=s(lm);EW=n(mve,"STRONG",{});var eAr=s(EW);BPe=r(eAr,"gptj"),eAr.forEach(t),kPe=r(mve," \u2014 "),ML=n(mve,"A",{href:!0});var oAr=s(ML);xPe=r(oAr,"GPTJConfig"),oAr.forEach(t),RPe=r(mve," (GPT-J model)"),mve.forEach(t),SPe=i(T),im=n(T,"LI",{});var gve=s(im);yW=n(gve,"STRONG",{});var rAr=s(yW);PPe=r(rAr,"hubert"),rAr.forEach(t),$Pe=r(gve," \u2014 "),EL=n(gve,"A",{href:!0});var tAr=s(EL);IPe=r(tAr,"HubertConfig"),tAr.forEach(t),jPe=r(gve," (Hubert model)"),gve.forEach(t),NPe=i(T),dm=n(T,"LI",{});var hve=s(dm);wW=n(hve,"STRONG",{});var aAr=s(wW);DPe=r(aAr,"ibert"),aAr.forEach(t),qPe=r(hve," \u2014 "),yL=n(hve,"A",{href:!0});var nAr=s(yL);GPe=r(nAr,"IBertConfig"),nAr.forEach(t),OPe=r(hve," (I-BERT model)"),hve.forEach(t),XPe=i(T),cm=n(T,"LI",{});var pve=s(cm);AW=n(pve,"STRONG",{});var sAr=s(AW);zPe=r(sAr,"imagegpt"),sAr.forEach(t),VPe=r(pve," \u2014 "),wL=n(pve,"A",{href:!0});var lAr=s(wL);WPe=r(lAr,"ImageGPTConfig"),lAr.forEach(t),QPe=r(pve," (ImageGPT model)"),pve.forEach(t),HPe=i(T),fm=n(T,"LI",{});var _ve=s(fm);LW=n(_ve,"STRONG",{});var iAr=s(LW);UPe=r(iAr,"layoutlm"),iAr.forEach(t),JPe=r(_ve," \u2014 "),AL=n(_ve,"A",{href:!0});var dAr=s(AL);YPe=r(dAr,"LayoutLMConfig"),dAr.forEach(t),KPe=r(_ve," (LayoutLM model)"),_ve.forEach(t),ZPe=i(T),mm=n(T,"LI",{});var uve=s(mm);BW=n(uve,"STRONG",{});var cAr=s(BW);e$e=r(cAr,"layoutlmv2"),cAr.forEach(t),o$e=r(uve," \u2014 "),LL=n(uve,"A",{href:!0});var fAr=s(LL);r$e=r(fAr,"LayoutLMv2Config"),fAr.forEach(t),t$e=r(uve," (LayoutLMv2 model)"),uve.forEach(t),a$e=i(T),gm=n(T,"LI",{});var bve=s(gm);kW=n(bve,"STRONG",{});var mAr=s(kW);n$e=r(mAr,"led"),mAr.forEach(t),s$e=r(bve," \u2014 "),BL=n(bve,"A",{href:!0});var gAr=s(BL);l$e=r(gAr,"LEDConfig"),gAr.forEach(t),i$e=r(bve," (LED model)"),bve.forEach(t),d$e=i(T),hm=n(T,"LI",{});var vve=s(hm);xW=n(vve,"STRONG",{});var hAr=s(xW);c$e=r(hAr,"longformer"),hAr.forEach(t),f$e=r(vve," \u2014 "),kL=n(vve,"A",{href:!0});var pAr=s(kL);m$e=r(pAr,"LongformerConfig"),pAr.forEach(t),g$e=r(vve," (Longformer model)"),vve.forEach(t),h$e=i(T),pm=n(T,"LI",{});var Tve=s(pm);RW=n(Tve,"STRONG",{});var _Ar=s(RW);p$e=r(_Ar,"luke"),_Ar.forEach(t),_$e=r(Tve," \u2014 "),xL=n(Tve,"A",{href:!0});var uAr=s(xL);u$e=r(uAr,"LukeConfig"),uAr.forEach(t),b$e=r(Tve," (LUKE model)"),Tve.forEach(t),v$e=i(T),_m=n(T,"LI",{});var Fve=s(_m);SW=n(Fve,"STRONG",{});var bAr=s(SW);T$e=r(bAr,"lxmert"),bAr.forEach(t),F$e=r(Fve," \u2014 "),RL=n(Fve,"A",{href:!0});var vAr=s(RL);C$e=r(vAr,"LxmertConfig"),vAr.forEach(t),M$e=r(Fve," (LXMERT model)"),Fve.forEach(t),E$e=i(T),um=n(T,"LI",{});var Cve=s(um);PW=n(Cve,"STRONG",{});var TAr=s(PW);y$e=r(TAr,"m2m_100"),TAr.forEach(t),w$e=r(Cve," \u2014 "),SL=n(Cve,"A",{href:!0});var FAr=s(SL);A$e=r(FAr,"M2M100Config"),FAr.forEach(t),L$e=r(Cve," (M2M100 model)"),Cve.forEach(t),B$e=i(T),bm=n(T,"LI",{});var Mve=s(bm);$W=n(Mve,"STRONG",{});var CAr=s($W);k$e=r(CAr,"marian"),CAr.forEach(t),x$e=r(Mve," \u2014 "),PL=n(Mve,"A",{href:!0});var MAr=s(PL);R$e=r(MAr,"MarianConfig"),MAr.forEach(t),S$e=r(Mve," (Marian model)"),Mve.forEach(t),P$e=i(T),vm=n(T,"LI",{});var Eve=s(vm);IW=n(Eve,"STRONG",{});var EAr=s(IW);$$e=r(EAr,"mbart"),EAr.forEach(t),I$e=r(Eve," \u2014 "),$L=n(Eve,"A",{href:!0});var yAr=s($L);j$e=r(yAr,"MBartConfig"),yAr.forEach(t),N$e=r(Eve," (mBART model)"),Eve.forEach(t),D$e=i(T),Tm=n(T,"LI",{});var yve=s(Tm);jW=n(yve,"STRONG",{});var wAr=s(jW);q$e=r(wAr,"megatron-bert"),wAr.forEach(t),G$e=r(yve," \u2014 "),IL=n(yve,"A",{href:!0});var AAr=s(IL);O$e=r(AAr,"MegatronBertConfig"),AAr.forEach(t),X$e=r(yve," (MegatronBert model)"),yve.forEach(t),z$e=i(T),Fm=n(T,"LI",{});var wve=s(Fm);NW=n(wve,"STRONG",{});var LAr=s(NW);V$e=r(LAr,"mobilebert"),LAr.forEach(t),W$e=r(wve," \u2014 "),jL=n(wve,"A",{href:!0});var BAr=s(jL);Q$e=r(BAr,"MobileBertConfig"),BAr.forEach(t),H$e=r(wve," (MobileBERT model)"),wve.forEach(t),U$e=i(T),Cm=n(T,"LI",{});var Ave=s(Cm);DW=n(Ave,"STRONG",{});var kAr=s(DW);J$e=r(kAr,"mpnet"),kAr.forEach(t),Y$e=r(Ave," \u2014 "),NL=n(Ave,"A",{href:!0});var xAr=s(NL);K$e=r(xAr,"MPNetConfig"),xAr.forEach(t),Z$e=r(Ave," (MPNet model)"),Ave.forEach(t),eIe=i(T),Mm=n(T,"LI",{});var Lve=s(Mm);qW=n(Lve,"STRONG",{});var RAr=s(qW);oIe=r(RAr,"mt5"),RAr.forEach(t),rIe=r(Lve," \u2014 "),DL=n(Lve,"A",{href:!0});var SAr=s(DL);tIe=r(SAr,"MT5Config"),SAr.forEach(t),aIe=r(Lve," (mT5 model)"),Lve.forEach(t),nIe=i(T),Em=n(T,"LI",{});var Bve=s(Em);GW=n(Bve,"STRONG",{});var PAr=s(GW);sIe=r(PAr,"nystromformer"),PAr.forEach(t),lIe=r(Bve," \u2014 "),qL=n(Bve,"A",{href:!0});var $Ar=s(qL);iIe=r($Ar,"NystromformerConfig"),$Ar.forEach(t),dIe=r(Bve," (Nystromformer model)"),Bve.forEach(t),cIe=i(T),ym=n(T,"LI",{});var kve=s(ym);OW=n(kve,"STRONG",{});var IAr=s(OW);fIe=r(IAr,"openai-gpt"),IAr.forEach(t),mIe=r(kve," \u2014 "),GL=n(kve,"A",{href:!0});var jAr=s(GL);gIe=r(jAr,"OpenAIGPTConfig"),jAr.forEach(t),hIe=r(kve," (OpenAI GPT model)"),kve.forEach(t),pIe=i(T),wm=n(T,"LI",{});var xve=s(wm);XW=n(xve,"STRONG",{});var NAr=s(XW);_Ie=r(NAr,"pegasus"),NAr.forEach(t),uIe=r(xve," \u2014 "),OL=n(xve,"A",{href:!0});var DAr=s(OL);bIe=r(DAr,"PegasusConfig"),DAr.forEach(t),vIe=r(xve," (Pegasus model)"),xve.forEach(t),TIe=i(T),Am=n(T,"LI",{});var Rve=s(Am);zW=n(Rve,"STRONG",{});var qAr=s(zW);FIe=r(qAr,"perceiver"),qAr.forEach(t),CIe=r(Rve," \u2014 "),XL=n(Rve,"A",{href:!0});var GAr=s(XL);MIe=r(GAr,"PerceiverConfig"),GAr.forEach(t),EIe=r(Rve," (Perceiver model)"),Rve.forEach(t),yIe=i(T),Lm=n(T,"LI",{});var Sve=s(Lm);VW=n(Sve,"STRONG",{});var OAr=s(VW);wIe=r(OAr,"plbart"),OAr.forEach(t),AIe=r(Sve," \u2014 "),zL=n(Sve,"A",{href:!0});var XAr=s(zL);LIe=r(XAr,"PLBartConfig"),XAr.forEach(t),BIe=r(Sve," (PLBart model)"),Sve.forEach(t),kIe=i(T),Bm=n(T,"LI",{});var Pve=s(Bm);WW=n(Pve,"STRONG",{});var zAr=s(WW);xIe=r(zAr,"poolformer"),zAr.forEach(t),RIe=r(Pve," \u2014 "),VL=n(Pve,"A",{href:!0});var VAr=s(VL);SIe=r(VAr,"PoolFormerConfig"),VAr.forEach(t),PIe=r(Pve," (PoolFormer model)"),Pve.forEach(t),$Ie=i(T),km=n(T,"LI",{});var $ve=s(km);QW=n($ve,"STRONG",{});var WAr=s(QW);IIe=r(WAr,"prophetnet"),WAr.forEach(t),jIe=r($ve," \u2014 "),WL=n($ve,"A",{href:!0});var QAr=s(WL);NIe=r(QAr,"ProphetNetConfig"),QAr.forEach(t),DIe=r($ve," (ProphetNet model)"),$ve.forEach(t),qIe=i(T),xm=n(T,"LI",{});var Ive=s(xm);HW=n(Ive,"STRONG",{});var HAr=s(HW);GIe=r(HAr,"qdqbert"),HAr.forEach(t),OIe=r(Ive," \u2014 "),QL=n(Ive,"A",{href:!0});var UAr=s(QL);XIe=r(UAr,"QDQBertConfig"),UAr.forEach(t),zIe=r(Ive," (QDQBert model)"),Ive.forEach(t),VIe=i(T),Rm=n(T,"LI",{});var jve=s(Rm);UW=n(jve,"STRONG",{});var JAr=s(UW);WIe=r(JAr,"rag"),JAr.forEach(t),QIe=r(jve," \u2014 "),HL=n(jve,"A",{href:!0});var YAr=s(HL);HIe=r(YAr,"RagConfig"),YAr.forEach(t),UIe=r(jve," (RAG model)"),jve.forEach(t),JIe=i(T),Sm=n(T,"LI",{});var Nve=s(Sm);JW=n(Nve,"STRONG",{});var KAr=s(JW);YIe=r(KAr,"realm"),KAr.forEach(t),KIe=r(Nve," \u2014 "),UL=n(Nve,"A",{href:!0});var ZAr=s(UL);ZIe=r(ZAr,"RealmConfig"),ZAr.forEach(t),eje=r(Nve," (Realm model)"),Nve.forEach(t),oje=i(T),Pm=n(T,"LI",{});var Dve=s(Pm);YW=n(Dve,"STRONG",{});var e6r=s(YW);rje=r(e6r,"reformer"),e6r.forEach(t),tje=r(Dve," \u2014 "),JL=n(Dve,"A",{href:!0});var o6r=s(JL);aje=r(o6r,"ReformerConfig"),o6r.forEach(t),nje=r(Dve," (Reformer model)"),Dve.forEach(t),sje=i(T),$m=n(T,"LI",{});var qve=s($m);KW=n(qve,"STRONG",{});var r6r=s(KW);lje=r(r6r,"rembert"),r6r.forEach(t),ije=r(qve," \u2014 "),YL=n(qve,"A",{href:!0});var t6r=s(YL);dje=r(t6r,"RemBertConfig"),t6r.forEach(t),cje=r(qve," (RemBERT model)"),qve.forEach(t),fje=i(T),Im=n(T,"LI",{});var Gve=s(Im);ZW=n(Gve,"STRONG",{});var a6r=s(ZW);mje=r(a6r,"retribert"),a6r.forEach(t),gje=r(Gve," \u2014 "),KL=n(Gve,"A",{href:!0});var n6r=s(KL);hje=r(n6r,"RetriBertConfig"),n6r.forEach(t),pje=r(Gve," (RetriBERT model)"),Gve.forEach(t),_je=i(T),jm=n(T,"LI",{});var Ove=s(jm);eQ=n(Ove,"STRONG",{});var s6r=s(eQ);uje=r(s6r,"roberta"),s6r.forEach(t),bje=r(Ove," \u2014 "),ZL=n(Ove,"A",{href:!0});var l6r=s(ZL);vje=r(l6r,"RobertaConfig"),l6r.forEach(t),Tje=r(Ove," (RoBERTa model)"),Ove.forEach(t),Fje=i(T),Nm=n(T,"LI",{});var Xve=s(Nm);oQ=n(Xve,"STRONG",{});var i6r=s(oQ);Cje=r(i6r,"roformer"),i6r.forEach(t),Mje=r(Xve," \u2014 "),e8=n(Xve,"A",{href:!0});var d6r=s(e8);Eje=r(d6r,"RoFormerConfig"),d6r.forEach(t),yje=r(Xve," (RoFormer model)"),Xve.forEach(t),wje=i(T),Dm=n(T,"LI",{});var zve=s(Dm);rQ=n(zve,"STRONG",{});var c6r=s(rQ);Aje=r(c6r,"segformer"),c6r.forEach(t),Lje=r(zve," \u2014 "),o8=n(zve,"A",{href:!0});var f6r=s(o8);Bje=r(f6r,"SegformerConfig"),f6r.forEach(t),kje=r(zve," (SegFormer model)"),zve.forEach(t),xje=i(T),qm=n(T,"LI",{});var Vve=s(qm);tQ=n(Vve,"STRONG",{});var m6r=s(tQ);Rje=r(m6r,"sew"),m6r.forEach(t),Sje=r(Vve," \u2014 "),r8=n(Vve,"A",{href:!0});var g6r=s(r8);Pje=r(g6r,"SEWConfig"),g6r.forEach(t),$je=r(Vve," (SEW model)"),Vve.forEach(t),Ije=i(T),Gm=n(T,"LI",{});var Wve=s(Gm);aQ=n(Wve,"STRONG",{});var h6r=s(aQ);jje=r(h6r,"sew-d"),h6r.forEach(t),Nje=r(Wve," \u2014 "),t8=n(Wve,"A",{href:!0});var p6r=s(t8);Dje=r(p6r,"SEWDConfig"),p6r.forEach(t),qje=r(Wve," (SEW-D model)"),Wve.forEach(t),Gje=i(T),Om=n(T,"LI",{});var Qve=s(Om);nQ=n(Qve,"STRONG",{});var _6r=s(nQ);Oje=r(_6r,"speech-encoder-decoder"),_6r.forEach(t),Xje=r(Qve," \u2014 "),a8=n(Qve,"A",{href:!0});var u6r=s(a8);zje=r(u6r,"SpeechEncoderDecoderConfig"),u6r.forEach(t),Vje=r(Qve," (Speech Encoder decoder model)"),Qve.forEach(t),Wje=i(T),Xm=n(T,"LI",{});var Hve=s(Xm);sQ=n(Hve,"STRONG",{});var b6r=s(sQ);Qje=r(b6r,"speech_to_text"),b6r.forEach(t),Hje=r(Hve," \u2014 "),n8=n(Hve,"A",{href:!0});var v6r=s(n8);Uje=r(v6r,"Speech2TextConfig"),v6r.forEach(t),Jje=r(Hve," (Speech2Text model)"),Hve.forEach(t),Yje=i(T),zm=n(T,"LI",{});var Uve=s(zm);lQ=n(Uve,"STRONG",{});var T6r=s(lQ);Kje=r(T6r,"speech_to_text_2"),T6r.forEach(t),Zje=r(Uve," \u2014 "),s8=n(Uve,"A",{href:!0});var F6r=s(s8);eNe=r(F6r,"Speech2Text2Config"),F6r.forEach(t),oNe=r(Uve," (Speech2Text2 model)"),Uve.forEach(t),rNe=i(T),Vm=n(T,"LI",{});var Jve=s(Vm);iQ=n(Jve,"STRONG",{});var C6r=s(iQ);tNe=r(C6r,"splinter"),C6r.forEach(t),aNe=r(Jve," \u2014 "),l8=n(Jve,"A",{href:!0});var M6r=s(l8);nNe=r(M6r,"SplinterConfig"),M6r.forEach(t),sNe=r(Jve," (Splinter model)"),Jve.forEach(t),lNe=i(T),Wm=n(T,"LI",{});var Yve=s(Wm);dQ=n(Yve,"STRONG",{});var E6r=s(dQ);iNe=r(E6r,"squeezebert"),E6r.forEach(t),dNe=r(Yve," \u2014 "),i8=n(Yve,"A",{href:!0});var y6r=s(i8);cNe=r(y6r,"SqueezeBertConfig"),y6r.forEach(t),fNe=r(Yve," (SqueezeBERT model)"),Yve.forEach(t),mNe=i(T),Qm=n(T,"LI",{});var Kve=s(Qm);cQ=n(Kve,"STRONG",{});var w6r=s(cQ);gNe=r(w6r,"swin"),w6r.forEach(t),hNe=r(Kve," \u2014 "),d8=n(Kve,"A",{href:!0});var A6r=s(d8);pNe=r(A6r,"SwinConfig"),A6r.forEach(t),_Ne=r(Kve," (Swin model)"),Kve.forEach(t),uNe=i(T),Hm=n(T,"LI",{});var Zve=s(Hm);fQ=n(Zve,"STRONG",{});var L6r=s(fQ);bNe=r(L6r,"t5"),L6r.forEach(t),vNe=r(Zve," \u2014 "),c8=n(Zve,"A",{href:!0});var B6r=s(c8);TNe=r(B6r,"T5Config"),B6r.forEach(t),FNe=r(Zve," (T5 model)"),Zve.forEach(t),CNe=i(T),Um=n(T,"LI",{});var eTe=s(Um);mQ=n(eTe,"STRONG",{});var k6r=s(mQ);MNe=r(k6r,"tapas"),k6r.forEach(t),ENe=r(eTe," \u2014 "),f8=n(eTe,"A",{href:!0});var x6r=s(f8);yNe=r(x6r,"TapasConfig"),x6r.forEach(t),wNe=r(eTe," (TAPAS model)"),eTe.forEach(t),ANe=i(T),Jm=n(T,"LI",{});var oTe=s(Jm);gQ=n(oTe,"STRONG",{});var R6r=s(gQ);LNe=r(R6r,"transfo-xl"),R6r.forEach(t),BNe=r(oTe," \u2014 "),m8=n(oTe,"A",{href:!0});var S6r=s(m8);kNe=r(S6r,"TransfoXLConfig"),S6r.forEach(t),xNe=r(oTe," (Transformer-XL model)"),oTe.forEach(t),RNe=i(T),Ym=n(T,"LI",{});var rTe=s(Ym);hQ=n(rTe,"STRONG",{});var P6r=s(hQ);SNe=r(P6r,"trocr"),P6r.forEach(t),PNe=r(rTe," \u2014 "),g8=n(rTe,"A",{href:!0});var $6r=s(g8);$Ne=r($6r,"TrOCRConfig"),$6r.forEach(t),INe=r(rTe," (TrOCR model)"),rTe.forEach(t),jNe=i(T),Km=n(T,"LI",{});var tTe=s(Km);pQ=n(tTe,"STRONG",{});var I6r=s(pQ);NNe=r(I6r,"unispeech"),I6r.forEach(t),DNe=r(tTe," \u2014 "),h8=n(tTe,"A",{href:!0});var j6r=s(h8);qNe=r(j6r,"UniSpeechConfig"),j6r.forEach(t),GNe=r(tTe," (UniSpeech model)"),tTe.forEach(t),ONe=i(T),Zm=n(T,"LI",{});var aTe=s(Zm);_Q=n(aTe,"STRONG",{});var N6r=s(_Q);XNe=r(N6r,"unispeech-sat"),N6r.forEach(t),zNe=r(aTe," \u2014 "),p8=n(aTe,"A",{href:!0});var D6r=s(p8);VNe=r(D6r,"UniSpeechSatConfig"),D6r.forEach(t),WNe=r(aTe," (UniSpeechSat model)"),aTe.forEach(t),QNe=i(T),eg=n(T,"LI",{});var nTe=s(eg);uQ=n(nTe,"STRONG",{});var q6r=s(uQ);HNe=r(q6r,"vilt"),q6r.forEach(t),UNe=r(nTe," \u2014 "),_8=n(nTe,"A",{href:!0});var G6r=s(_8);JNe=r(G6r,"ViltConfig"),G6r.forEach(t),YNe=r(nTe," (ViLT model)"),nTe.forEach(t),KNe=i(T),og=n(T,"LI",{});var sTe=s(og);bQ=n(sTe,"STRONG",{});var O6r=s(bQ);ZNe=r(O6r,"vision-encoder-decoder"),O6r.forEach(t),eDe=r(sTe," \u2014 "),u8=n(sTe,"A",{href:!0});var X6r=s(u8);oDe=r(X6r,"VisionEncoderDecoderConfig"),X6r.forEach(t),rDe=r(sTe," (Vision Encoder decoder model)"),sTe.forEach(t),tDe=i(T),rg=n(T,"LI",{});var lTe=s(rg);vQ=n(lTe,"STRONG",{});var z6r=s(vQ);aDe=r(z6r,"vision-text-dual-encoder"),z6r.forEach(t),nDe=r(lTe," \u2014 "),b8=n(lTe,"A",{href:!0});var V6r=s(b8);sDe=r(V6r,"VisionTextDualEncoderConfig"),V6r.forEach(t),lDe=r(lTe," (VisionTextDualEncoder model)"),lTe.forEach(t),iDe=i(T),tg=n(T,"LI",{});var iTe=s(tg);TQ=n(iTe,"STRONG",{});var W6r=s(TQ);dDe=r(W6r,"visual_bert"),W6r.forEach(t),cDe=r(iTe," \u2014 "),v8=n(iTe,"A",{href:!0});var Q6r=s(v8);fDe=r(Q6r,"VisualBertConfig"),Q6r.forEach(t),mDe=r(iTe," (VisualBert model)"),iTe.forEach(t),gDe=i(T),ag=n(T,"LI",{});var dTe=s(ag);FQ=n(dTe,"STRONG",{});var H6r=s(FQ);hDe=r(H6r,"vit"),H6r.forEach(t),pDe=r(dTe," \u2014 "),T8=n(dTe,"A",{href:!0});var U6r=s(T8);_De=r(U6r,"ViTConfig"),U6r.forEach(t),uDe=r(dTe," (ViT model)"),dTe.forEach(t),bDe=i(T),ng=n(T,"LI",{});var cTe=s(ng);CQ=n(cTe,"STRONG",{});var J6r=s(CQ);vDe=r(J6r,"vit_mae"),J6r.forEach(t),TDe=r(cTe," \u2014 "),F8=n(cTe,"A",{href:!0});var Y6r=s(F8);FDe=r(Y6r,"ViTMAEConfig"),Y6r.forEach(t),CDe=r(cTe," (ViTMAE model)"),cTe.forEach(t),MDe=i(T),sg=n(T,"LI",{});var fTe=s(sg);MQ=n(fTe,"STRONG",{});var K6r=s(MQ);EDe=r(K6r,"wav2vec2"),K6r.forEach(t),yDe=r(fTe," \u2014 "),C8=n(fTe,"A",{href:!0});var Z6r=s(C8);wDe=r(Z6r,"Wav2Vec2Config"),Z6r.forEach(t),ADe=r(fTe," (Wav2Vec2 model)"),fTe.forEach(t),LDe=i(T),lg=n(T,"LI",{});var mTe=s(lg);EQ=n(mTe,"STRONG",{});var e0r=s(EQ);BDe=r(e0r,"wavlm"),e0r.forEach(t),kDe=r(mTe," \u2014 "),M8=n(mTe,"A",{href:!0});var o0r=s(M8);xDe=r(o0r,"WavLMConfig"),o0r.forEach(t),RDe=r(mTe," (WavLM model)"),mTe.forEach(t),SDe=i(T),ig=n(T,"LI",{});var gTe=s(ig);yQ=n(gTe,"STRONG",{});var r0r=s(yQ);PDe=r(r0r,"xglm"),r0r.forEach(t),$De=r(gTe," \u2014 "),E8=n(gTe,"A",{href:!0});var t0r=s(E8);IDe=r(t0r,"XGLMConfig"),t0r.forEach(t),jDe=r(gTe," (XGLM model)"),gTe.forEach(t),NDe=i(T),dg=n(T,"LI",{});var hTe=s(dg);wQ=n(hTe,"STRONG",{});var a0r=s(wQ);DDe=r(a0r,"xlm"),a0r.forEach(t),qDe=r(hTe," \u2014 "),y8=n(hTe,"A",{href:!0});var n0r=s(y8);GDe=r(n0r,"XLMConfig"),n0r.forEach(t),ODe=r(hTe," (XLM model)"),hTe.forEach(t),XDe=i(T),cg=n(T,"LI",{});var pTe=s(cg);AQ=n(pTe,"STRONG",{});var s0r=s(AQ);zDe=r(s0r,"xlm-prophetnet"),s0r.forEach(t),VDe=r(pTe," \u2014 "),w8=n(pTe,"A",{href:!0});var l0r=s(w8);WDe=r(l0r,"XLMProphetNetConfig"),l0r.forEach(t),QDe=r(pTe," (XLMProphetNet model)"),pTe.forEach(t),HDe=i(T),fg=n(T,"LI",{});var _Te=s(fg);LQ=n(_Te,"STRONG",{});var i0r=s(LQ);UDe=r(i0r,"xlm-roberta"),i0r.forEach(t),JDe=r(_Te," \u2014 "),A8=n(_Te,"A",{href:!0});var d0r=s(A8);YDe=r(d0r,"XLMRobertaConfig"),d0r.forEach(t),KDe=r(_Te," (XLM-RoBERTa model)"),_Te.forEach(t),ZDe=i(T),mg=n(T,"LI",{});var uTe=s(mg);BQ=n(uTe,"STRONG",{});var c0r=s(BQ);eqe=r(c0r,"xlm-roberta-xl"),c0r.forEach(t),oqe=r(uTe," \u2014 "),L8=n(uTe,"A",{href:!0});var f0r=s(L8);rqe=r(f0r,"XLMRobertaXLConfig"),f0r.forEach(t),tqe=r(uTe," (XLM-RoBERTa-XL model)"),uTe.forEach(t),aqe=i(T),gg=n(T,"LI",{});var bTe=s(gg);kQ=n(bTe,"STRONG",{});var m0r=s(kQ);nqe=r(m0r,"xlnet"),m0r.forEach(t),sqe=r(bTe," \u2014 "),B8=n(bTe,"A",{href:!0});var g0r=s(B8);lqe=r(g0r,"XLNetConfig"),g0r.forEach(t),iqe=r(bTe," (XLNet model)"),bTe.forEach(t),dqe=i(T),hg=n(T,"LI",{});var vTe=s(hg);xQ=n(vTe,"STRONG",{});var h0r=s(xQ);cqe=r(h0r,"yoso"),h0r.forEach(t),fqe=r(vTe," \u2014 "),k8=n(vTe,"A",{href:!0});var p0r=s(k8);mqe=r(p0r,"YosoConfig"),p0r.forEach(t),gqe=r(vTe," (YOSO model)"),vTe.forEach(t),T.forEach(t),hqe=i(ia),RQ=n(ia,"P",{});var _0r=s(RQ);pqe=r(_0r,"Examples:"),_0r.forEach(t),_qe=i(ia),m(gM.$$.fragment,ia),ia.forEach(t),uqe=i($s),pg=n($s,"DIV",{class:!0});var pBe=s(pg);m(hM.$$.fragment,pBe),bqe=i(pBe),SQ=n(pBe,"P",{});var u0r=s(SQ);vqe=r(u0r,"Register a new configuration for this class."),u0r.forEach(t),pBe.forEach(t),$s.forEach(t),pLe=i(d),ji=n(d,"H2",{class:!0});var _Be=s(ji);_g=n(_Be,"A",{id:!0,class:!0,href:!0});var b0r=s(_g);PQ=n(b0r,"SPAN",{});var v0r=s(PQ);m(pM.$$.fragment,v0r),v0r.forEach(t),b0r.forEach(t),Tqe=i(_Be),$Q=n(_Be,"SPAN",{});var T0r=s($Q);Fqe=r(T0r,"AutoTokenizer"),T0r.forEach(t),_Be.forEach(t),_Le=i(d),Oo=n(d,"DIV",{class:!0});var Is=s(Oo);m(_M.$$.fragment,Is),Cqe=i(Is),uM=n(Is,"P",{});var uBe=s(uM);Mqe=r(uBe,`This is a generic tokenizer class that will be instantiated as one of the tokenizer classes of the library when
created with the `),x8=n(uBe,"A",{href:!0});var F0r=s(x8);Eqe=r(F0r,"AutoTokenizer.from_pretrained()"),F0r.forEach(t),yqe=r(uBe," class method."),uBe.forEach(t),wqe=i(Is),bM=n(Is,"P",{});var bBe=s(bM);Aqe=r(bBe,"This class cannot be instantiated directly using "),IQ=n(bBe,"CODE",{});var C0r=s(IQ);Lqe=r(C0r,"__init__()"),C0r.forEach(t),Bqe=r(bBe," (throws an error)."),bBe.forEach(t),kqe=i(Is),mo=n(Is,"DIV",{class:!0});var da=s(mo);m(vM.$$.fragment,da),xqe=i(da),jQ=n(da,"P",{});var M0r=s(jQ);Rqe=r(M0r,"Instantiate one of the tokenizer classes of the library from a pretrained model vocabulary."),M0r.forEach(t),Sqe=i(da),ja=n(da,"P",{});var s4=s(ja);Pqe=r(s4,"The tokenizer class to instantiate is selected based on the "),NQ=n(s4,"CODE",{});var E0r=s(NQ);$qe=r(E0r,"model_type"),E0r.forEach(t),Iqe=r(s4,` property of the config object (either
passed as an argument or loaded from `),DQ=n(s4,"CODE",{});var y0r=s(DQ);jqe=r(y0r,"pretrained_model_name_or_path"),y0r.forEach(t),Nqe=r(s4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),qQ=n(s4,"CODE",{});var w0r=s(qQ);Dqe=r(w0r,"pretrained_model_name_or_path"),w0r.forEach(t),qqe=r(s4,":"),s4.forEach(t),Gqe=i(da),M=n(da,"UL",{});var y=s(M);Dn=n(y,"LI",{});var V6=s(Dn);GQ=n(V6,"STRONG",{});var A0r=s(GQ);Oqe=r(A0r,"albert"),A0r.forEach(t),Xqe=r(V6," \u2014 "),R8=n(V6,"A",{href:!0});var L0r=s(R8);zqe=r(L0r,"AlbertTokenizer"),L0r.forEach(t),Vqe=r(V6," or "),S8=n(V6,"A",{href:!0});var B0r=s(S8);Wqe=r(B0r,"AlbertTokenizerFast"),B0r.forEach(t),Qqe=r(V6," (ALBERT model)"),V6.forEach(t),Hqe=i(y),qn=n(y,"LI",{});var W6=s(qn);OQ=n(W6,"STRONG",{});var k0r=s(OQ);Uqe=r(k0r,"bart"),k0r.forEach(t),Jqe=r(W6," \u2014 "),P8=n(W6,"A",{href:!0});var x0r=s(P8);Yqe=r(x0r,"BartTokenizer"),x0r.forEach(t),Kqe=r(W6," or "),$8=n(W6,"A",{href:!0});var R0r=s($8);Zqe=r(R0r,"BartTokenizerFast"),R0r.forEach(t),eGe=r(W6," (BART model)"),W6.forEach(t),oGe=i(y),Gn=n(y,"LI",{});var Q6=s(Gn);XQ=n(Q6,"STRONG",{});var S0r=s(XQ);rGe=r(S0r,"barthez"),S0r.forEach(t),tGe=r(Q6," \u2014 "),I8=n(Q6,"A",{href:!0});var P0r=s(I8);aGe=r(P0r,"BarthezTokenizer"),P0r.forEach(t),nGe=r(Q6," or "),j8=n(Q6,"A",{href:!0});var $0r=s(j8);sGe=r($0r,"BarthezTokenizerFast"),$0r.forEach(t),lGe=r(Q6," (BARThez model)"),Q6.forEach(t),iGe=i(y),ug=n(y,"LI",{});var TTe=s(ug);zQ=n(TTe,"STRONG",{});var I0r=s(zQ);dGe=r(I0r,"bartpho"),I0r.forEach(t),cGe=r(TTe," \u2014 "),N8=n(TTe,"A",{href:!0});var j0r=s(N8);fGe=r(j0r,"BartphoTokenizer"),j0r.forEach(t),mGe=r(TTe," (BARTpho model)"),TTe.forEach(t),gGe=i(y),On=n(y,"LI",{});var H6=s(On);VQ=n(H6,"STRONG",{});var N0r=s(VQ);hGe=r(N0r,"bert"),N0r.forEach(t),pGe=r(H6," \u2014 "),D8=n(H6,"A",{href:!0});var D0r=s(D8);_Ge=r(D0r,"BertTokenizer"),D0r.forEach(t),uGe=r(H6," or "),q8=n(H6,"A",{href:!0});var q0r=s(q8);bGe=r(q0r,"BertTokenizerFast"),q0r.forEach(t),vGe=r(H6," (BERT model)"),H6.forEach(t),TGe=i(y),bg=n(y,"LI",{});var FTe=s(bg);WQ=n(FTe,"STRONG",{});var G0r=s(WQ);FGe=r(G0r,"bert-generation"),G0r.forEach(t),CGe=r(FTe," \u2014 "),G8=n(FTe,"A",{href:!0});var O0r=s(G8);MGe=r(O0r,"BertGenerationTokenizer"),O0r.forEach(t),EGe=r(FTe," (Bert Generation model)"),FTe.forEach(t),yGe=i(y),vg=n(y,"LI",{});var CTe=s(vg);QQ=n(CTe,"STRONG",{});var X0r=s(QQ);wGe=r(X0r,"bert-japanese"),X0r.forEach(t),AGe=r(CTe," \u2014 "),O8=n(CTe,"A",{href:!0});var z0r=s(O8);LGe=r(z0r,"BertJapaneseTokenizer"),z0r.forEach(t),BGe=r(CTe," (BertJapanese model)"),CTe.forEach(t),kGe=i(y),Tg=n(y,"LI",{});var MTe=s(Tg);HQ=n(MTe,"STRONG",{});var V0r=s(HQ);xGe=r(V0r,"bertweet"),V0r.forEach(t),RGe=r(MTe," \u2014 "),X8=n(MTe,"A",{href:!0});var W0r=s(X8);SGe=r(W0r,"BertweetTokenizer"),W0r.forEach(t),PGe=r(MTe," (Bertweet model)"),MTe.forEach(t),$Ge=i(y),Xn=n(y,"LI",{});var U6=s(Xn);UQ=n(U6,"STRONG",{});var Q0r=s(UQ);IGe=r(Q0r,"big_bird"),Q0r.forEach(t),jGe=r(U6," \u2014 "),z8=n(U6,"A",{href:!0});var H0r=s(z8);NGe=r(H0r,"BigBirdTokenizer"),H0r.forEach(t),DGe=r(U6," or "),V8=n(U6,"A",{href:!0});var U0r=s(V8);qGe=r(U0r,"BigBirdTokenizerFast"),U0r.forEach(t),GGe=r(U6," (BigBird model)"),U6.forEach(t),OGe=i(y),zn=n(y,"LI",{});var J6=s(zn);JQ=n(J6,"STRONG",{});var J0r=s(JQ);XGe=r(J0r,"bigbird_pegasus"),J0r.forEach(t),zGe=r(J6," \u2014 "),W8=n(J6,"A",{href:!0});var Y0r=s(W8);VGe=r(Y0r,"PegasusTokenizer"),Y0r.forEach(t),WGe=r(J6," or "),Q8=n(J6,"A",{href:!0});var K0r=s(Q8);QGe=r(K0r,"PegasusTokenizerFast"),K0r.forEach(t),HGe=r(J6," (BigBirdPegasus model)"),J6.forEach(t),UGe=i(y),Vn=n(y,"LI",{});var Y6=s(Vn);YQ=n(Y6,"STRONG",{});var Z0r=s(YQ);JGe=r(Z0r,"blenderbot"),Z0r.forEach(t),YGe=r(Y6," \u2014 "),H8=n(Y6,"A",{href:!0});var eLr=s(H8);KGe=r(eLr,"BlenderbotTokenizer"),eLr.forEach(t),ZGe=r(Y6," or "),U8=n(Y6,"A",{href:!0});var oLr=s(U8);eOe=r(oLr,"BlenderbotTokenizerFast"),oLr.forEach(t),oOe=r(Y6," (Blenderbot model)"),Y6.forEach(t),rOe=i(y),Fg=n(y,"LI",{});var ETe=s(Fg);KQ=n(ETe,"STRONG",{});var rLr=s(KQ);tOe=r(rLr,"blenderbot-small"),rLr.forEach(t),aOe=r(ETe," \u2014 "),J8=n(ETe,"A",{href:!0});var tLr=s(J8);nOe=r(tLr,"BlenderbotSmallTokenizer"),tLr.forEach(t),sOe=r(ETe," (BlenderbotSmall model)"),ETe.forEach(t),lOe=i(y),Cg=n(y,"LI",{});var yTe=s(Cg);ZQ=n(yTe,"STRONG",{});var aLr=s(ZQ);iOe=r(aLr,"byt5"),aLr.forEach(t),dOe=r(yTe," \u2014 "),Y8=n(yTe,"A",{href:!0});var nLr=s(Y8);cOe=r(nLr,"ByT5Tokenizer"),nLr.forEach(t),fOe=r(yTe," (ByT5 model)"),yTe.forEach(t),mOe=i(y),Wn=n(y,"LI",{});var K6=s(Wn);eH=n(K6,"STRONG",{});var sLr=s(eH);gOe=r(sLr,"camembert"),sLr.forEach(t),hOe=r(K6," \u2014 "),K8=n(K6,"A",{href:!0});var lLr=s(K8);pOe=r(lLr,"CamembertTokenizer"),lLr.forEach(t),_Oe=r(K6," or "),Z8=n(K6,"A",{href:!0});var iLr=s(Z8);uOe=r(iLr,"CamembertTokenizerFast"),iLr.forEach(t),bOe=r(K6," (CamemBERT model)"),K6.forEach(t),vOe=i(y),Mg=n(y,"LI",{});var wTe=s(Mg);oH=n(wTe,"STRONG",{});var dLr=s(oH);TOe=r(dLr,"canine"),dLr.forEach(t),FOe=r(wTe," \u2014 "),eB=n(wTe,"A",{href:!0});var cLr=s(eB);COe=r(cLr,"CanineTokenizer"),cLr.forEach(t),MOe=r(wTe," (Canine model)"),wTe.forEach(t),EOe=i(y),Qn=n(y,"LI",{});var Z6=s(Qn);rH=n(Z6,"STRONG",{});var fLr=s(rH);yOe=r(fLr,"clip"),fLr.forEach(t),wOe=r(Z6," \u2014 "),oB=n(Z6,"A",{href:!0});var mLr=s(oB);AOe=r(mLr,"CLIPTokenizer"),mLr.forEach(t),LOe=r(Z6," or "),rB=n(Z6,"A",{href:!0});var gLr=s(rB);BOe=r(gLr,"CLIPTokenizerFast"),gLr.forEach(t),kOe=r(Z6," (CLIP model)"),Z6.forEach(t),xOe=i(y),Hn=n(y,"LI",{});var e0=s(Hn);tH=n(e0,"STRONG",{});var hLr=s(tH);ROe=r(hLr,"convbert"),hLr.forEach(t),SOe=r(e0," \u2014 "),tB=n(e0,"A",{href:!0});var pLr=s(tB);POe=r(pLr,"ConvBertTokenizer"),pLr.forEach(t),$Oe=r(e0," or "),aB=n(e0,"A",{href:!0});var _Lr=s(aB);IOe=r(_Lr,"ConvBertTokenizerFast"),_Lr.forEach(t),jOe=r(e0," (ConvBERT model)"),e0.forEach(t),NOe=i(y),Un=n(y,"LI",{});var o0=s(Un);aH=n(o0,"STRONG",{});var uLr=s(aH);DOe=r(uLr,"cpm"),uLr.forEach(t),qOe=r(o0," \u2014 "),nB=n(o0,"A",{href:!0});var bLr=s(nB);GOe=r(bLr,"CpmTokenizer"),bLr.forEach(t),OOe=r(o0," or "),nH=n(o0,"CODE",{});var vLr=s(nH);XOe=r(vLr,"CpmTokenizerFast"),vLr.forEach(t),zOe=r(o0," (CPM model)"),o0.forEach(t),VOe=i(y),Eg=n(y,"LI",{});var ATe=s(Eg);sH=n(ATe,"STRONG",{});var TLr=s(sH);WOe=r(TLr,"ctrl"),TLr.forEach(t),QOe=r(ATe," \u2014 "),sB=n(ATe,"A",{href:!0});var FLr=s(sB);HOe=r(FLr,"CTRLTokenizer"),FLr.forEach(t),UOe=r(ATe," (CTRL model)"),ATe.forEach(t),JOe=i(y),Jn=n(y,"LI",{});var r0=s(Jn);lH=n(r0,"STRONG",{});var CLr=s(lH);YOe=r(CLr,"deberta"),CLr.forEach(t),KOe=r(r0," \u2014 "),lB=n(r0,"A",{href:!0});var MLr=s(lB);ZOe=r(MLr,"DebertaTokenizer"),MLr.forEach(t),eXe=r(r0," or "),iB=n(r0,"A",{href:!0});var ELr=s(iB);oXe=r(ELr,"DebertaTokenizerFast"),ELr.forEach(t),rXe=r(r0," (DeBERTa model)"),r0.forEach(t),tXe=i(y),yg=n(y,"LI",{});var LTe=s(yg);iH=n(LTe,"STRONG",{});var yLr=s(iH);aXe=r(yLr,"deberta-v2"),yLr.forEach(t),nXe=r(LTe," \u2014 "),dB=n(LTe,"A",{href:!0});var wLr=s(dB);sXe=r(wLr,"DebertaV2Tokenizer"),wLr.forEach(t),lXe=r(LTe," (DeBERTa-v2 model)"),LTe.forEach(t),iXe=i(y),Yn=n(y,"LI",{});var t0=s(Yn);dH=n(t0,"STRONG",{});var ALr=s(dH);dXe=r(ALr,"distilbert"),ALr.forEach(t),cXe=r(t0," \u2014 "),cB=n(t0,"A",{href:!0});var LLr=s(cB);fXe=r(LLr,"DistilBertTokenizer"),LLr.forEach(t),mXe=r(t0," or "),fB=n(t0,"A",{href:!0});var BLr=s(fB);gXe=r(BLr,"DistilBertTokenizerFast"),BLr.forEach(t),hXe=r(t0," (DistilBERT model)"),t0.forEach(t),pXe=i(y),Kn=n(y,"LI",{});var a0=s(Kn);cH=n(a0,"STRONG",{});var kLr=s(cH);_Xe=r(kLr,"dpr"),kLr.forEach(t),uXe=r(a0," \u2014 "),mB=n(a0,"A",{href:!0});var xLr=s(mB);bXe=r(xLr,"DPRQuestionEncoderTokenizer"),xLr.forEach(t),vXe=r(a0," or "),gB=n(a0,"A",{href:!0});var RLr=s(gB);TXe=r(RLr,"DPRQuestionEncoderTokenizerFast"),RLr.forEach(t),FXe=r(a0," (DPR model)"),a0.forEach(t),CXe=i(y),Zn=n(y,"LI",{});var n0=s(Zn);fH=n(n0,"STRONG",{});var SLr=s(fH);MXe=r(SLr,"electra"),SLr.forEach(t),EXe=r(n0," \u2014 "),hB=n(n0,"A",{href:!0});var PLr=s(hB);yXe=r(PLr,"ElectraTokenizer"),PLr.forEach(t),wXe=r(n0," or "),pB=n(n0,"A",{href:!0});var $Lr=s(pB);AXe=r($Lr,"ElectraTokenizerFast"),$Lr.forEach(t),LXe=r(n0," (ELECTRA model)"),n0.forEach(t),BXe=i(y),wg=n(y,"LI",{});var BTe=s(wg);mH=n(BTe,"STRONG",{});var ILr=s(mH);kXe=r(ILr,"flaubert"),ILr.forEach(t),xXe=r(BTe," \u2014 "),_B=n(BTe,"A",{href:!0});var jLr=s(_B);RXe=r(jLr,"FlaubertTokenizer"),jLr.forEach(t),SXe=r(BTe," (FlauBERT model)"),BTe.forEach(t),PXe=i(y),es=n(y,"LI",{});var s0=s(es);gH=n(s0,"STRONG",{});var NLr=s(gH);$Xe=r(NLr,"fnet"),NLr.forEach(t),IXe=r(s0," \u2014 "),uB=n(s0,"A",{href:!0});var DLr=s(uB);jXe=r(DLr,"FNetTokenizer"),DLr.forEach(t),NXe=r(s0," or "),bB=n(s0,"A",{href:!0});var qLr=s(bB);DXe=r(qLr,"FNetTokenizerFast"),qLr.forEach(t),qXe=r(s0," (FNet model)"),s0.forEach(t),GXe=i(y),Ag=n(y,"LI",{});var kTe=s(Ag);hH=n(kTe,"STRONG",{});var GLr=s(hH);OXe=r(GLr,"fsmt"),GLr.forEach(t),XXe=r(kTe," \u2014 "),vB=n(kTe,"A",{href:!0});var OLr=s(vB);zXe=r(OLr,"FSMTTokenizer"),OLr.forEach(t),VXe=r(kTe," (FairSeq Machine-Translation model)"),kTe.forEach(t),WXe=i(y),os=n(y,"LI",{});var l0=s(os);pH=n(l0,"STRONG",{});var XLr=s(pH);QXe=r(XLr,"funnel"),XLr.forEach(t),HXe=r(l0," \u2014 "),TB=n(l0,"A",{href:!0});var zLr=s(TB);UXe=r(zLr,"FunnelTokenizer"),zLr.forEach(t),JXe=r(l0," or "),FB=n(l0,"A",{href:!0});var VLr=s(FB);YXe=r(VLr,"FunnelTokenizerFast"),VLr.forEach(t),KXe=r(l0," (Funnel Transformer model)"),l0.forEach(t),ZXe=i(y),rs=n(y,"LI",{});var i0=s(rs);_H=n(i0,"STRONG",{});var WLr=s(_H);eze=r(WLr,"gpt2"),WLr.forEach(t),oze=r(i0," \u2014 "),CB=n(i0,"A",{href:!0});var QLr=s(CB);rze=r(QLr,"GPT2Tokenizer"),QLr.forEach(t),tze=r(i0," or "),MB=n(i0,"A",{href:!0});var HLr=s(MB);aze=r(HLr,"GPT2TokenizerFast"),HLr.forEach(t),nze=r(i0," (OpenAI GPT-2 model)"),i0.forEach(t),sze=i(y),ts=n(y,"LI",{});var d0=s(ts);uH=n(d0,"STRONG",{});var ULr=s(uH);lze=r(ULr,"gpt_neo"),ULr.forEach(t),ize=r(d0," \u2014 "),EB=n(d0,"A",{href:!0});var JLr=s(EB);dze=r(JLr,"GPT2Tokenizer"),JLr.forEach(t),cze=r(d0," or "),yB=n(d0,"A",{href:!0});var YLr=s(yB);fze=r(YLr,"GPT2TokenizerFast"),YLr.forEach(t),mze=r(d0," (GPT Neo model)"),d0.forEach(t),gze=i(y),as=n(y,"LI",{});var c0=s(as);bH=n(c0,"STRONG",{});var KLr=s(bH);hze=r(KLr,"herbert"),KLr.forEach(t),pze=r(c0," \u2014 "),wB=n(c0,"A",{href:!0});var ZLr=s(wB);_ze=r(ZLr,"HerbertTokenizer"),ZLr.forEach(t),uze=r(c0," or "),AB=n(c0,"A",{href:!0});var e8r=s(AB);bze=r(e8r,"HerbertTokenizerFast"),e8r.forEach(t),vze=r(c0," (HerBERT model)"),c0.forEach(t),Tze=i(y),Lg=n(y,"LI",{});var xTe=s(Lg);vH=n(xTe,"STRONG",{});var o8r=s(vH);Fze=r(o8r,"hubert"),o8r.forEach(t),Cze=r(xTe," \u2014 "),LB=n(xTe,"A",{href:!0});var r8r=s(LB);Mze=r(r8r,"Wav2Vec2CTCTokenizer"),r8r.forEach(t),Eze=r(xTe," (Hubert model)"),xTe.forEach(t),yze=i(y),ns=n(y,"LI",{});var f0=s(ns);TH=n(f0,"STRONG",{});var t8r=s(TH);wze=r(t8r,"ibert"),t8r.forEach(t),Aze=r(f0," \u2014 "),BB=n(f0,"A",{href:!0});var a8r=s(BB);Lze=r(a8r,"RobertaTokenizer"),a8r.forEach(t),Bze=r(f0," or "),kB=n(f0,"A",{href:!0});var n8r=s(kB);kze=r(n8r,"RobertaTokenizerFast"),n8r.forEach(t),xze=r(f0," (I-BERT model)"),f0.forEach(t),Rze=i(y),ss=n(y,"LI",{});var m0=s(ss);FH=n(m0,"STRONG",{});var s8r=s(FH);Sze=r(s8r,"layoutlm"),s8r.forEach(t),Pze=r(m0," \u2014 "),xB=n(m0,"A",{href:!0});var l8r=s(xB);$ze=r(l8r,"LayoutLMTokenizer"),l8r.forEach(t),Ize=r(m0," or "),RB=n(m0,"A",{href:!0});var i8r=s(RB);jze=r(i8r,"LayoutLMTokenizerFast"),i8r.forEach(t),Nze=r(m0," (LayoutLM model)"),m0.forEach(t),Dze=i(y),ls=n(y,"LI",{});var g0=s(ls);CH=n(g0,"STRONG",{});var d8r=s(CH);qze=r(d8r,"layoutlmv2"),d8r.forEach(t),Gze=r(g0," \u2014 "),SB=n(g0,"A",{href:!0});var c8r=s(SB);Oze=r(c8r,"LayoutLMv2Tokenizer"),c8r.forEach(t),Xze=r(g0," or "),PB=n(g0,"A",{href:!0});var f8r=s(PB);zze=r(f8r,"LayoutLMv2TokenizerFast"),f8r.forEach(t),Vze=r(g0," (LayoutLMv2 model)"),g0.forEach(t),Wze=i(y),is=n(y,"LI",{});var h0=s(is);MH=n(h0,"STRONG",{});var m8r=s(MH);Qze=r(m8r,"layoutxlm"),m8r.forEach(t),Hze=r(h0," \u2014 "),$B=n(h0,"A",{href:!0});var g8r=s($B);Uze=r(g8r,"LayoutXLMTokenizer"),g8r.forEach(t),Jze=r(h0," or "),IB=n(h0,"A",{href:!0});var h8r=s(IB);Yze=r(h8r,"LayoutXLMTokenizerFast"),h8r.forEach(t),Kze=r(h0," (LayoutXLM model)"),h0.forEach(t),Zze=i(y),ds=n(y,"LI",{});var p0=s(ds);EH=n(p0,"STRONG",{});var p8r=s(EH);eVe=r(p8r,"led"),p8r.forEach(t),oVe=r(p0," \u2014 "),jB=n(p0,"A",{href:!0});var _8r=s(jB);rVe=r(_8r,"LEDTokenizer"),_8r.forEach(t),tVe=r(p0," or "),NB=n(p0,"A",{href:!0});var u8r=s(NB);aVe=r(u8r,"LEDTokenizerFast"),u8r.forEach(t),nVe=r(p0," (LED model)"),p0.forEach(t),sVe=i(y),cs=n(y,"LI",{});var _0=s(cs);yH=n(_0,"STRONG",{});var b8r=s(yH);lVe=r(b8r,"longformer"),b8r.forEach(t),iVe=r(_0," \u2014 "),DB=n(_0,"A",{href:!0});var v8r=s(DB);dVe=r(v8r,"LongformerTokenizer"),v8r.forEach(t),cVe=r(_0," or "),qB=n(_0,"A",{href:!0});var T8r=s(qB);fVe=r(T8r,"LongformerTokenizerFast"),T8r.forEach(t),mVe=r(_0," (Longformer model)"),_0.forEach(t),gVe=i(y),Bg=n(y,"LI",{});var RTe=s(Bg);wH=n(RTe,"STRONG",{});var F8r=s(wH);hVe=r(F8r,"luke"),F8r.forEach(t),pVe=r(RTe," \u2014 "),GB=n(RTe,"A",{href:!0});var C8r=s(GB);_Ve=r(C8r,"LukeTokenizer"),C8r.forEach(t),uVe=r(RTe," (LUKE model)"),RTe.forEach(t),bVe=i(y),fs=n(y,"LI",{});var u0=s(fs);AH=n(u0,"STRONG",{});var M8r=s(AH);vVe=r(M8r,"lxmert"),M8r.forEach(t),TVe=r(u0," \u2014 "),OB=n(u0,"A",{href:!0});var E8r=s(OB);FVe=r(E8r,"LxmertTokenizer"),E8r.forEach(t),CVe=r(u0," or "),XB=n(u0,"A",{href:!0});var y8r=s(XB);MVe=r(y8r,"LxmertTokenizerFast"),y8r.forEach(t),EVe=r(u0," (LXMERT model)"),u0.forEach(t),yVe=i(y),kg=n(y,"LI",{});var STe=s(kg);LH=n(STe,"STRONG",{});var w8r=s(LH);wVe=r(w8r,"m2m_100"),w8r.forEach(t),AVe=r(STe," \u2014 "),zB=n(STe,"A",{href:!0});var A8r=s(zB);LVe=r(A8r,"M2M100Tokenizer"),A8r.forEach(t),BVe=r(STe," (M2M100 model)"),STe.forEach(t),kVe=i(y),xg=n(y,"LI",{});var PTe=s(xg);BH=n(PTe,"STRONG",{});var L8r=s(BH);xVe=r(L8r,"marian"),L8r.forEach(t),RVe=r(PTe," \u2014 "),VB=n(PTe,"A",{href:!0});var B8r=s(VB);SVe=r(B8r,"MarianTokenizer"),B8r.forEach(t),PVe=r(PTe," (Marian model)"),PTe.forEach(t),$Ve=i(y),ms=n(y,"LI",{});var b0=s(ms);kH=n(b0,"STRONG",{});var k8r=s(kH);IVe=r(k8r,"mbart"),k8r.forEach(t),jVe=r(b0," \u2014 "),WB=n(b0,"A",{href:!0});var x8r=s(WB);NVe=r(x8r,"MBartTokenizer"),x8r.forEach(t),DVe=r(b0," or "),QB=n(b0,"A",{href:!0});var R8r=s(QB);qVe=r(R8r,"MBartTokenizerFast"),R8r.forEach(t),GVe=r(b0," (mBART model)"),b0.forEach(t),OVe=i(y),gs=n(y,"LI",{});var v0=s(gs);xH=n(v0,"STRONG",{});var S8r=s(xH);XVe=r(S8r,"mbart50"),S8r.forEach(t),zVe=r(v0," \u2014 "),HB=n(v0,"A",{href:!0});var P8r=s(HB);VVe=r(P8r,"MBart50Tokenizer"),P8r.forEach(t),WVe=r(v0," or "),UB=n(v0,"A",{href:!0});var $8r=s(UB);QVe=r($8r,"MBart50TokenizerFast"),$8r.forEach(t),HVe=r(v0," (mBART-50 model)"),v0.forEach(t),UVe=i(y),Rg=n(y,"LI",{});var $Te=s(Rg);RH=n($Te,"STRONG",{});var I8r=s(RH);JVe=r(I8r,"mluke"),I8r.forEach(t),YVe=r($Te," \u2014 "),JB=n($Te,"A",{href:!0});var j8r=s(JB);KVe=r(j8r,"MLukeTokenizer"),j8r.forEach(t),ZVe=r($Te," (mLUKE model)"),$Te.forEach(t),eWe=i(y),hs=n(y,"LI",{});var T0=s(hs);SH=n(T0,"STRONG",{});var N8r=s(SH);oWe=r(N8r,"mobilebert"),N8r.forEach(t),rWe=r(T0," \u2014 "),YB=n(T0,"A",{href:!0});var D8r=s(YB);tWe=r(D8r,"MobileBertTokenizer"),D8r.forEach(t),aWe=r(T0," or "),KB=n(T0,"A",{href:!0});var q8r=s(KB);nWe=r(q8r,"MobileBertTokenizerFast"),q8r.forEach(t),sWe=r(T0," (MobileBERT model)"),T0.forEach(t),lWe=i(y),ps=n(y,"LI",{});var F0=s(ps);PH=n(F0,"STRONG",{});var G8r=s(PH);iWe=r(G8r,"mpnet"),G8r.forEach(t),dWe=r(F0," \u2014 "),ZB=n(F0,"A",{href:!0});var O8r=s(ZB);cWe=r(O8r,"MPNetTokenizer"),O8r.forEach(t),fWe=r(F0," or "),ek=n(F0,"A",{href:!0});var X8r=s(ek);mWe=r(X8r,"MPNetTokenizerFast"),X8r.forEach(t),gWe=r(F0," (MPNet model)"),F0.forEach(t),hWe=i(y),_s=n(y,"LI",{});var C0=s(_s);$H=n(C0,"STRONG",{});var z8r=s($H);pWe=r(z8r,"mt5"),z8r.forEach(t),_We=r(C0," \u2014 "),ok=n(C0,"A",{href:!0});var V8r=s(ok);uWe=r(V8r,"MT5Tokenizer"),V8r.forEach(t),bWe=r(C0," or "),rk=n(C0,"A",{href:!0});var W8r=s(rk);vWe=r(W8r,"MT5TokenizerFast"),W8r.forEach(t),TWe=r(C0," (mT5 model)"),C0.forEach(t),FWe=i(y),us=n(y,"LI",{});var M0=s(us);IH=n(M0,"STRONG",{});var Q8r=s(IH);CWe=r(Q8r,"openai-gpt"),Q8r.forEach(t),MWe=r(M0," \u2014 "),tk=n(M0,"A",{href:!0});var H8r=s(tk);EWe=r(H8r,"OpenAIGPTTokenizer"),H8r.forEach(t),yWe=r(M0," or "),ak=n(M0,"A",{href:!0});var U8r=s(ak);wWe=r(U8r,"OpenAIGPTTokenizerFast"),U8r.forEach(t),AWe=r(M0," (OpenAI GPT model)"),M0.forEach(t),LWe=i(y),bs=n(y,"LI",{});var E0=s(bs);jH=n(E0,"STRONG",{});var J8r=s(jH);BWe=r(J8r,"pegasus"),J8r.forEach(t),kWe=r(E0," \u2014 "),nk=n(E0,"A",{href:!0});var Y8r=s(nk);xWe=r(Y8r,"PegasusTokenizer"),Y8r.forEach(t),RWe=r(E0," or "),sk=n(E0,"A",{href:!0});var K8r=s(sk);SWe=r(K8r,"PegasusTokenizerFast"),K8r.forEach(t),PWe=r(E0," (Pegasus model)"),E0.forEach(t),$We=i(y),Sg=n(y,"LI",{});var ITe=s(Sg);NH=n(ITe,"STRONG",{});var Z8r=s(NH);IWe=r(Z8r,"perceiver"),Z8r.forEach(t),jWe=r(ITe," \u2014 "),lk=n(ITe,"A",{href:!0});var eBr=s(lk);NWe=r(eBr,"PerceiverTokenizer"),eBr.forEach(t),DWe=r(ITe," (Perceiver model)"),ITe.forEach(t),qWe=i(y),Pg=n(y,"LI",{});var jTe=s(Pg);DH=n(jTe,"STRONG",{});var oBr=s(DH);GWe=r(oBr,"phobert"),oBr.forEach(t),OWe=r(jTe," \u2014 "),ik=n(jTe,"A",{href:!0});var rBr=s(ik);XWe=r(rBr,"PhobertTokenizer"),rBr.forEach(t),zWe=r(jTe," (PhoBERT model)"),jTe.forEach(t),VWe=i(y),$g=n(y,"LI",{});var NTe=s($g);qH=n(NTe,"STRONG",{});var tBr=s(qH);WWe=r(tBr,"plbart"),tBr.forEach(t),QWe=r(NTe," \u2014 "),dk=n(NTe,"A",{href:!0});var aBr=s(dk);HWe=r(aBr,"PLBartTokenizer"),aBr.forEach(t),UWe=r(NTe," (PLBart model)"),NTe.forEach(t),JWe=i(y),Ig=n(y,"LI",{});var DTe=s(Ig);GH=n(DTe,"STRONG",{});var nBr=s(GH);YWe=r(nBr,"prophetnet"),nBr.forEach(t),KWe=r(DTe," \u2014 "),ck=n(DTe,"A",{href:!0});var sBr=s(ck);ZWe=r(sBr,"ProphetNetTokenizer"),sBr.forEach(t),eQe=r(DTe," (ProphetNet model)"),DTe.forEach(t),oQe=i(y),vs=n(y,"LI",{});var y0=s(vs);OH=n(y0,"STRONG",{});var lBr=s(OH);rQe=r(lBr,"qdqbert"),lBr.forEach(t),tQe=r(y0," \u2014 "),fk=n(y0,"A",{href:!0});var iBr=s(fk);aQe=r(iBr,"BertTokenizer"),iBr.forEach(t),nQe=r(y0," or "),mk=n(y0,"A",{href:!0});var dBr=s(mk);sQe=r(dBr,"BertTokenizerFast"),dBr.forEach(t),lQe=r(y0," (QDQBert model)"),y0.forEach(t),iQe=i(y),jg=n(y,"LI",{});var qTe=s(jg);XH=n(qTe,"STRONG",{});var cBr=s(XH);dQe=r(cBr,"rag"),cBr.forEach(t),cQe=r(qTe," \u2014 "),gk=n(qTe,"A",{href:!0});var fBr=s(gk);fQe=r(fBr,"RagTokenizer"),fBr.forEach(t),mQe=r(qTe," (RAG model)"),qTe.forEach(t),gQe=i(y),Ts=n(y,"LI",{});var w0=s(Ts);zH=n(w0,"STRONG",{});var mBr=s(zH);hQe=r(mBr,"realm"),mBr.forEach(t),pQe=r(w0," \u2014 "),hk=n(w0,"A",{href:!0});var gBr=s(hk);_Qe=r(gBr,"RealmTokenizer"),gBr.forEach(t),uQe=r(w0," or "),pk=n(w0,"A",{href:!0});var hBr=s(pk);bQe=r(hBr,"RealmTokenizerFast"),hBr.forEach(t),vQe=r(w0," (Realm model)"),w0.forEach(t),TQe=i(y),Fs=n(y,"LI",{});var A0=s(Fs);VH=n(A0,"STRONG",{});var pBr=s(VH);FQe=r(pBr,"reformer"),pBr.forEach(t),CQe=r(A0," \u2014 "),_k=n(A0,"A",{href:!0});var _Br=s(_k);MQe=r(_Br,"ReformerTokenizer"),_Br.forEach(t),EQe=r(A0," or "),uk=n(A0,"A",{href:!0});var uBr=s(uk);yQe=r(uBr,"ReformerTokenizerFast"),uBr.forEach(t),wQe=r(A0," (Reformer model)"),A0.forEach(t),AQe=i(y),Cs=n(y,"LI",{});var L0=s(Cs);WH=n(L0,"STRONG",{});var bBr=s(WH);LQe=r(bBr,"rembert"),bBr.forEach(t),BQe=r(L0," \u2014 "),bk=n(L0,"A",{href:!0});var vBr=s(bk);kQe=r(vBr,"RemBertTokenizer"),vBr.forEach(t),xQe=r(L0," or "),vk=n(L0,"A",{href:!0});var TBr=s(vk);RQe=r(TBr,"RemBertTokenizerFast"),TBr.forEach(t),SQe=r(L0," (RemBERT model)"),L0.forEach(t),PQe=i(y),Ms=n(y,"LI",{});var B0=s(Ms);QH=n(B0,"STRONG",{});var FBr=s(QH);$Qe=r(FBr,"retribert"),FBr.forEach(t),IQe=r(B0," \u2014 "),Tk=n(B0,"A",{href:!0});var CBr=s(Tk);jQe=r(CBr,"RetriBertTokenizer"),CBr.forEach(t),NQe=r(B0," or "),Fk=n(B0,"A",{href:!0});var MBr=s(Fk);DQe=r(MBr,"RetriBertTokenizerFast"),MBr.forEach(t),qQe=r(B0," (RetriBERT model)"),B0.forEach(t),GQe=i(y),Es=n(y,"LI",{});var k0=s(Es);HH=n(k0,"STRONG",{});var EBr=s(HH);OQe=r(EBr,"roberta"),EBr.forEach(t),XQe=r(k0," \u2014 "),Ck=n(k0,"A",{href:!0});var yBr=s(Ck);zQe=r(yBr,"RobertaTokenizer"),yBr.forEach(t),VQe=r(k0," or "),Mk=n(k0,"A",{href:!0});var wBr=s(Mk);WQe=r(wBr,"RobertaTokenizerFast"),wBr.forEach(t),QQe=r(k0," (RoBERTa model)"),k0.forEach(t),HQe=i(y),ys=n(y,"LI",{});var x0=s(ys);UH=n(x0,"STRONG",{});var ABr=s(UH);UQe=r(ABr,"roformer"),ABr.forEach(t),JQe=r(x0," \u2014 "),Ek=n(x0,"A",{href:!0});var LBr=s(Ek);YQe=r(LBr,"RoFormerTokenizer"),LBr.forEach(t),KQe=r(x0," or "),yk=n(x0,"A",{href:!0});var BBr=s(yk);ZQe=r(BBr,"RoFormerTokenizerFast"),BBr.forEach(t),eHe=r(x0," (RoFormer model)"),x0.forEach(t),oHe=i(y),Ng=n(y,"LI",{});var GTe=s(Ng);JH=n(GTe,"STRONG",{});var kBr=s(JH);rHe=r(kBr,"speech_to_text"),kBr.forEach(t),tHe=r(GTe," \u2014 "),wk=n(GTe,"A",{href:!0});var xBr=s(wk);aHe=r(xBr,"Speech2TextTokenizer"),xBr.forEach(t),nHe=r(GTe," (Speech2Text model)"),GTe.forEach(t),sHe=i(y),Dg=n(y,"LI",{});var OTe=s(Dg);YH=n(OTe,"STRONG",{});var RBr=s(YH);lHe=r(RBr,"speech_to_text_2"),RBr.forEach(t),iHe=r(OTe," \u2014 "),Ak=n(OTe,"A",{href:!0});var SBr=s(Ak);dHe=r(SBr,"Speech2Text2Tokenizer"),SBr.forEach(t),cHe=r(OTe," (Speech2Text2 model)"),OTe.forEach(t),fHe=i(y),ws=n(y,"LI",{});var R0=s(ws);KH=n(R0,"STRONG",{});var PBr=s(KH);mHe=r(PBr,"splinter"),PBr.forEach(t),gHe=r(R0," \u2014 "),Lk=n(R0,"A",{href:!0});var $Br=s(Lk);hHe=r($Br,"SplinterTokenizer"),$Br.forEach(t),pHe=r(R0," or "),Bk=n(R0,"A",{href:!0});var IBr=s(Bk);_He=r(IBr,"SplinterTokenizerFast"),IBr.forEach(t),uHe=r(R0," (Splinter model)"),R0.forEach(t),bHe=i(y),As=n(y,"LI",{});var S0=s(As);ZH=n(S0,"STRONG",{});var jBr=s(ZH);vHe=r(jBr,"squeezebert"),jBr.forEach(t),THe=r(S0," \u2014 "),kk=n(S0,"A",{href:!0});var NBr=s(kk);FHe=r(NBr,"SqueezeBertTokenizer"),NBr.forEach(t),CHe=r(S0," or "),xk=n(S0,"A",{href:!0});var DBr=s(xk);MHe=r(DBr,"SqueezeBertTokenizerFast"),DBr.forEach(t),EHe=r(S0," (SqueezeBERT model)"),S0.forEach(t),yHe=i(y),Ls=n(y,"LI",{});var P0=s(Ls);eU=n(P0,"STRONG",{});var qBr=s(eU);wHe=r(qBr,"t5"),qBr.forEach(t),AHe=r(P0," \u2014 "),Rk=n(P0,"A",{href:!0});var GBr=s(Rk);LHe=r(GBr,"T5Tokenizer"),GBr.forEach(t),BHe=r(P0," or "),Sk=n(P0,"A",{href:!0});var OBr=s(Sk);kHe=r(OBr,"T5TokenizerFast"),OBr.forEach(t),xHe=r(P0," (T5 model)"),P0.forEach(t),RHe=i(y),qg=n(y,"LI",{});var XTe=s(qg);oU=n(XTe,"STRONG",{});var XBr=s(oU);SHe=r(XBr,"tapas"),XBr.forEach(t),PHe=r(XTe," \u2014 "),Pk=n(XTe,"A",{href:!0});var zBr=s(Pk);$He=r(zBr,"TapasTokenizer"),zBr.forEach(t),IHe=r(XTe," (TAPAS model)"),XTe.forEach(t),jHe=i(y),Gg=n(y,"LI",{});var zTe=s(Gg);rU=n(zTe,"STRONG",{});var VBr=s(rU);NHe=r(VBr,"transfo-xl"),VBr.forEach(t),DHe=r(zTe," \u2014 "),$k=n(zTe,"A",{href:!0});var WBr=s($k);qHe=r(WBr,"TransfoXLTokenizer"),WBr.forEach(t),GHe=r(zTe," (Transformer-XL model)"),zTe.forEach(t),OHe=i(y),Og=n(y,"LI",{});var VTe=s(Og);tU=n(VTe,"STRONG",{});var QBr=s(tU);XHe=r(QBr,"wav2vec2"),QBr.forEach(t),zHe=r(VTe," \u2014 "),Ik=n(VTe,"A",{href:!0});var HBr=s(Ik);VHe=r(HBr,"Wav2Vec2CTCTokenizer"),HBr.forEach(t),WHe=r(VTe," (Wav2Vec2 model)"),VTe.forEach(t),QHe=i(y),Xg=n(y,"LI",{});var WTe=s(Xg);aU=n(WTe,"STRONG",{});var UBr=s(aU);HHe=r(UBr,"wav2vec2_phoneme"),UBr.forEach(t),UHe=r(WTe," \u2014 "),jk=n(WTe,"A",{href:!0});var JBr=s(jk);JHe=r(JBr,"Wav2Vec2PhonemeCTCTokenizer"),JBr.forEach(t),YHe=r(WTe," (Wav2Vec2Phoneme model)"),WTe.forEach(t),KHe=i(y),Bs=n(y,"LI",{});var $0=s(Bs);nU=n($0,"STRONG",{});var YBr=s(nU);ZHe=r(YBr,"xglm"),YBr.forEach(t),eUe=r($0," \u2014 "),Nk=n($0,"A",{href:!0});var KBr=s(Nk);oUe=r(KBr,"XGLMTokenizer"),KBr.forEach(t),rUe=r($0," or "),Dk=n($0,"A",{href:!0});var ZBr=s(Dk);tUe=r(ZBr,"XGLMTokenizerFast"),ZBr.forEach(t),aUe=r($0," (XGLM model)"),$0.forEach(t),nUe=i(y),zg=n(y,"LI",{});var QTe=s(zg);sU=n(QTe,"STRONG",{});var ekr=s(sU);sUe=r(ekr,"xlm"),ekr.forEach(t),lUe=r(QTe," \u2014 "),qk=n(QTe,"A",{href:!0});var okr=s(qk);iUe=r(okr,"XLMTokenizer"),okr.forEach(t),dUe=r(QTe," (XLM model)"),QTe.forEach(t),cUe=i(y),Vg=n(y,"LI",{});var HTe=s(Vg);lU=n(HTe,"STRONG",{});var rkr=s(lU);fUe=r(rkr,"xlm-prophetnet"),rkr.forEach(t),mUe=r(HTe," \u2014 "),Gk=n(HTe,"A",{href:!0});var tkr=s(Gk);gUe=r(tkr,"XLMProphetNetTokenizer"),tkr.forEach(t),hUe=r(HTe," (XLMProphetNet model)"),HTe.forEach(t),pUe=i(y),ks=n(y,"LI",{});var I0=s(ks);iU=n(I0,"STRONG",{});var akr=s(iU);_Ue=r(akr,"xlm-roberta"),akr.forEach(t),uUe=r(I0," \u2014 "),Ok=n(I0,"A",{href:!0});var nkr=s(Ok);bUe=r(nkr,"XLMRobertaTokenizer"),nkr.forEach(t),vUe=r(I0," or "),Xk=n(I0,"A",{href:!0});var skr=s(Xk);TUe=r(skr,"XLMRobertaTokenizerFast"),skr.forEach(t),FUe=r(I0," (XLM-RoBERTa model)"),I0.forEach(t),CUe=i(y),xs=n(y,"LI",{});var j0=s(xs);dU=n(j0,"STRONG",{});var lkr=s(dU);MUe=r(lkr,"xlnet"),lkr.forEach(t),EUe=r(j0," \u2014 "),zk=n(j0,"A",{href:!0});var ikr=s(zk);yUe=r(ikr,"XLNetTokenizer"),ikr.forEach(t),wUe=r(j0," or "),Vk=n(j0,"A",{href:!0});var dkr=s(Vk);AUe=r(dkr,"XLNetTokenizerFast"),dkr.forEach(t),LUe=r(j0," (XLNet model)"),j0.forEach(t),y.forEach(t),BUe=i(da),cU=n(da,"P",{});var ckr=s(cU);kUe=r(ckr,"Examples:"),ckr.forEach(t),xUe=i(da),m(TM.$$.fragment,da),da.forEach(t),RUe=i(Is),Wg=n(Is,"DIV",{class:!0});var vBe=s(Wg);m(FM.$$.fragment,vBe),SUe=i(vBe),fU=n(vBe,"P",{});var fkr=s(fU);PUe=r(fkr,"Register a new tokenizer in this mapping."),fkr.forEach(t),vBe.forEach(t),Is.forEach(t),uLe=i(d),Ni=n(d,"H2",{class:!0});var TBe=s(Ni);Qg=n(TBe,"A",{id:!0,class:!0,href:!0});var mkr=s(Qg);mU=n(mkr,"SPAN",{});var gkr=s(mU);m(CM.$$.fragment,gkr),gkr.forEach(t),mkr.forEach(t),$Ue=i(TBe),gU=n(TBe,"SPAN",{});var hkr=s(gU);IUe=r(hkr,"AutoFeatureExtractor"),hkr.forEach(t),TBe.forEach(t),bLe=i(d),Xo=n(d,"DIV",{class:!0});var js=s(Xo);m(MM.$$.fragment,js),jUe=i(js),EM=n(js,"P",{});var FBe=s(EM);NUe=r(FBe,`This is a generic feature extractor class that will be instantiated as one of the feature extractor classes of the
library when created with the `),Wk=n(FBe,"A",{href:!0});var pkr=s(Wk);DUe=r(pkr,"AutoFeatureExtractor.from_pretrained()"),pkr.forEach(t),qUe=r(FBe," class method."),FBe.forEach(t),GUe=i(js),yM=n(js,"P",{});var CBe=s(yM);OUe=r(CBe,"This class cannot be instantiated directly using "),hU=n(CBe,"CODE",{});var _kr=s(hU);XUe=r(_kr,"__init__()"),_kr.forEach(t),zUe=r(CBe," (throws an error)."),CBe.forEach(t),VUe=i(js),Le=n(js,"DIV",{class:!0});var xt=s(Le);m(wM.$$.fragment,xt),WUe=i(xt),pU=n(xt,"P",{});var ukr=s(pU);QUe=r(ukr,"Instantiate one of the feature extractor classes of the library from a pretrained model vocabulary."),ukr.forEach(t),HUe=i(xt),Na=n(xt,"P",{});var l4=s(Na);UUe=r(l4,"The feature extractor class to instantiate is selected based on the "),_U=n(l4,"CODE",{});var bkr=s(_U);JUe=r(bkr,"model_type"),bkr.forEach(t),YUe=r(l4,` property of the config object
(either passed as an argument or loaded from `),uU=n(l4,"CODE",{});var vkr=s(uU);KUe=r(vkr,"pretrained_model_name_or_path"),vkr.forEach(t),ZUe=r(l4,` if possible), or when it\u2019s
missing, by falling back to using pattern matching on `),bU=n(l4,"CODE",{});var Tkr=s(bU);eJe=r(Tkr,"pretrained_model_name_or_path"),Tkr.forEach(t),oJe=r(l4,":"),l4.forEach(t),rJe=i(xt),se=n(xt,"UL",{});var de=s(se);Hg=n(de,"LI",{});var UTe=s(Hg);vU=n(UTe,"STRONG",{});var Fkr=s(vU);tJe=r(Fkr,"beit"),Fkr.forEach(t),aJe=r(UTe," \u2014 "),Qk=n(UTe,"A",{href:!0});var Ckr=s(Qk);nJe=r(Ckr,"BeitFeatureExtractor"),Ckr.forEach(t),sJe=r(UTe," (BEiT model)"),UTe.forEach(t),lJe=i(de),Ug=n(de,"LI",{});var JTe=s(Ug);TU=n(JTe,"STRONG",{});var Mkr=s(TU);iJe=r(Mkr,"clip"),Mkr.forEach(t),dJe=r(JTe," \u2014 "),Hk=n(JTe,"A",{href:!0});var Ekr=s(Hk);cJe=r(Ekr,"CLIPFeatureExtractor"),Ekr.forEach(t),fJe=r(JTe," (CLIP model)"),JTe.forEach(t),mJe=i(de),Jg=n(de,"LI",{});var YTe=s(Jg);FU=n(YTe,"STRONG",{});var ykr=s(FU);gJe=r(ykr,"convnext"),ykr.forEach(t),hJe=r(YTe," \u2014 "),Uk=n(YTe,"A",{href:!0});var wkr=s(Uk);pJe=r(wkr,"ConvNextFeatureExtractor"),wkr.forEach(t),_Je=r(YTe," (ConvNext model)"),YTe.forEach(t),uJe=i(de),Yg=n(de,"LI",{});var KTe=s(Yg);CU=n(KTe,"STRONG",{});var Akr=s(CU);bJe=r(Akr,"deit"),Akr.forEach(t),vJe=r(KTe," \u2014 "),Jk=n(KTe,"A",{href:!0});var Lkr=s(Jk);TJe=r(Lkr,"DeiTFeatureExtractor"),Lkr.forEach(t),FJe=r(KTe," (DeiT model)"),KTe.forEach(t),CJe=i(de),Kg=n(de,"LI",{});var ZTe=s(Kg);MU=n(ZTe,"STRONG",{});var Bkr=s(MU);MJe=r(Bkr,"detr"),Bkr.forEach(t),EJe=r(ZTe," \u2014 "),Yk=n(ZTe,"A",{href:!0});var kkr=s(Yk);yJe=r(kkr,"DetrFeatureExtractor"),kkr.forEach(t),wJe=r(ZTe," (DETR model)"),ZTe.forEach(t),AJe=i(de),Zg=n(de,"LI",{});var e7e=s(Zg);EU=n(e7e,"STRONG",{});var xkr=s(EU);LJe=r(xkr,"hubert"),xkr.forEach(t),BJe=r(e7e," \u2014 "),Kk=n(e7e,"A",{href:!0});var Rkr=s(Kk);kJe=r(Rkr,"Wav2Vec2FeatureExtractor"),Rkr.forEach(t),xJe=r(e7e," (Hubert model)"),e7e.forEach(t),RJe=i(de),eh=n(de,"LI",{});var o7e=s(eh);yU=n(o7e,"STRONG",{});var Skr=s(yU);SJe=r(Skr,"layoutlmv2"),Skr.forEach(t),PJe=r(o7e," \u2014 "),Zk=n(o7e,"A",{href:!0});var Pkr=s(Zk);$Je=r(Pkr,"LayoutLMv2FeatureExtractor"),Pkr.forEach(t),IJe=r(o7e," (LayoutLMv2 model)"),o7e.forEach(t),jJe=i(de),oh=n(de,"LI",{});var r7e=s(oh);wU=n(r7e,"STRONG",{});var $kr=s(wU);NJe=r($kr,"perceiver"),$kr.forEach(t),DJe=r(r7e," \u2014 "),ex=n(r7e,"A",{href:!0});var Ikr=s(ex);qJe=r(Ikr,"PerceiverFeatureExtractor"),Ikr.forEach(t),GJe=r(r7e," (Perceiver model)"),r7e.forEach(t),OJe=i(de),rh=n(de,"LI",{});var t7e=s(rh);AU=n(t7e,"STRONG",{});var jkr=s(AU);XJe=r(jkr,"poolformer"),jkr.forEach(t),zJe=r(t7e," \u2014 "),ox=n(t7e,"A",{href:!0});var Nkr=s(ox);VJe=r(Nkr,"PoolFormerFeatureExtractor"),Nkr.forEach(t),WJe=r(t7e," (PoolFormer model)"),t7e.forEach(t),QJe=i(de),th=n(de,"LI",{});var a7e=s(th);LU=n(a7e,"STRONG",{});var Dkr=s(LU);HJe=r(Dkr,"segformer"),Dkr.forEach(t),UJe=r(a7e," \u2014 "),rx=n(a7e,"A",{href:!0});var qkr=s(rx);JJe=r(qkr,"SegformerFeatureExtractor"),qkr.forEach(t),YJe=r(a7e," (SegFormer model)"),a7e.forEach(t),KJe=i(de),ah=n(de,"LI",{});var n7e=s(ah);BU=n(n7e,"STRONG",{});var Gkr=s(BU);ZJe=r(Gkr,"speech_to_text"),Gkr.forEach(t),eYe=r(n7e," \u2014 "),tx=n(n7e,"A",{href:!0});var Okr=s(tx);oYe=r(Okr,"Speech2TextFeatureExtractor"),Okr.forEach(t),rYe=r(n7e," (Speech2Text model)"),n7e.forEach(t),tYe=i(de),nh=n(de,"LI",{});var s7e=s(nh);kU=n(s7e,"STRONG",{});var Xkr=s(kU);aYe=r(Xkr,"swin"),Xkr.forEach(t),nYe=r(s7e," \u2014 "),ax=n(s7e,"A",{href:!0});var zkr=s(ax);sYe=r(zkr,"ViTFeatureExtractor"),zkr.forEach(t),lYe=r(s7e," (Swin model)"),s7e.forEach(t),iYe=i(de),sh=n(de,"LI",{});var l7e=s(sh);xU=n(l7e,"STRONG",{});var Vkr=s(xU);dYe=r(Vkr,"vit"),Vkr.forEach(t),cYe=r(l7e," \u2014 "),nx=n(l7e,"A",{href:!0});var Wkr=s(nx);fYe=r(Wkr,"ViTFeatureExtractor"),Wkr.forEach(t),mYe=r(l7e," (ViT model)"),l7e.forEach(t),gYe=i(de),lh=n(de,"LI",{});var i7e=s(lh);RU=n(i7e,"STRONG",{});var Qkr=s(RU);hYe=r(Qkr,"vit_mae"),Qkr.forEach(t),pYe=r(i7e," \u2014 "),sx=n(i7e,"A",{href:!0});var Hkr=s(sx);_Ye=r(Hkr,"ViTFeatureExtractor"),Hkr.forEach(t),uYe=r(i7e," (ViTMAE model)"),i7e.forEach(t),bYe=i(de),ih=n(de,"LI",{});var d7e=s(ih);SU=n(d7e,"STRONG",{});var Ukr=s(SU);vYe=r(Ukr,"wav2vec2"),Ukr.forEach(t),TYe=r(d7e," \u2014 "),lx=n(d7e,"A",{href:!0});var Jkr=s(lx);FYe=r(Jkr,"Wav2Vec2FeatureExtractor"),Jkr.forEach(t),CYe=r(d7e," (Wav2Vec2 model)"),d7e.forEach(t),de.forEach(t),MYe=i(xt),m(dh.$$.fragment,xt),EYe=i(xt),PU=n(xt,"P",{});var Ykr=s(PU);yYe=r(Ykr,"Examples:"),Ykr.forEach(t),wYe=i(xt),m(AM.$$.fragment,xt),xt.forEach(t),AYe=i(js),ch=n(js,"DIV",{class:!0});var MBe=s(ch);m(LM.$$.fragment,MBe),LYe=i(MBe),$U=n(MBe,"P",{});var Kkr=s($U);BYe=r(Kkr,"Register a new feature extractor for this class."),Kkr.forEach(t),MBe.forEach(t),js.forEach(t),vLe=i(d),Di=n(d,"H2",{class:!0});var EBe=s(Di);fh=n(EBe,"A",{id:!0,class:!0,href:!0});var Zkr=s(fh);IU=n(Zkr,"SPAN",{});var exr=s(IU);m(BM.$$.fragment,exr),exr.forEach(t),Zkr.forEach(t),kYe=i(EBe),jU=n(EBe,"SPAN",{});var oxr=s(jU);xYe=r(oxr,"AutoProcessor"),oxr.forEach(t),EBe.forEach(t),TLe=i(d),zo=n(d,"DIV",{class:!0});var Ns=s(zo);m(kM.$$.fragment,Ns),RYe=i(Ns),xM=n(Ns,"P",{});var yBe=s(xM);SYe=r(yBe,`This is a generic processor class that will be instantiated as one of the processor classes of the library when
created with the `),ix=n(yBe,"A",{href:!0});var rxr=s(ix);PYe=r(rxr,"AutoProcessor.from_pretrained()"),rxr.forEach(t),$Ye=r(yBe," class method."),yBe.forEach(t),IYe=i(Ns),RM=n(Ns,"P",{});var wBe=s(RM);jYe=r(wBe,"This class cannot be instantiated directly using "),NU=n(wBe,"CODE",{});var txr=s(NU);NYe=r(txr,"__init__()"),txr.forEach(t),DYe=r(wBe," (throws an error)."),wBe.forEach(t),qYe=i(Ns),Be=n(Ns,"DIV",{class:!0});var Rt=s(Be);m(SM.$$.fragment,Rt),GYe=i(Rt),DU=n(Rt,"P",{});var axr=s(DU);OYe=r(axr,"Instantiate one of the processor classes of the library from a pretrained model vocabulary."),axr.forEach(t),XYe=i(Rt),qi=n(Rt,"P",{});var UX=s(qi);zYe=r(UX,"The processor class to instantiate is selected based on the "),qU=n(UX,"CODE",{});var nxr=s(qU);VYe=r(nxr,"model_type"),nxr.forEach(t),WYe=r(UX,` property of the config object (either
passed as an argument or loaded from `),GU=n(UX,"CODE",{});var sxr=s(GU);QYe=r(sxr,"pretrained_model_name_or_path"),sxr.forEach(t),HYe=r(UX," if possible):"),UX.forEach(t),UYe=i(Rt),we=n(Rt,"UL",{});var No=s(we);mh=n(No,"LI",{});var c7e=s(mh);OU=n(c7e,"STRONG",{});var lxr=s(OU);JYe=r(lxr,"clip"),lxr.forEach(t),YYe=r(c7e," \u2014 "),dx=n(c7e,"A",{href:!0});var ixr=s(dx);KYe=r(ixr,"CLIPProcessor"),ixr.forEach(t),ZYe=r(c7e," (CLIP model)"),c7e.forEach(t),eKe=i(No),gh=n(No,"LI",{});var f7e=s(gh);XU=n(f7e,"STRONG",{});var dxr=s(XU);oKe=r(dxr,"layoutlmv2"),dxr.forEach(t),rKe=r(f7e," \u2014 "),cx=n(f7e,"A",{href:!0});var cxr=s(cx);tKe=r(cxr,"LayoutLMv2Processor"),cxr.forEach(t),aKe=r(f7e," (LayoutLMv2 model)"),f7e.forEach(t),nKe=i(No),hh=n(No,"LI",{});var m7e=s(hh);zU=n(m7e,"STRONG",{});var fxr=s(zU);sKe=r(fxr,"layoutxlm"),fxr.forEach(t),lKe=r(m7e," \u2014 "),fx=n(m7e,"A",{href:!0});var mxr=s(fx);iKe=r(mxr,"LayoutXLMProcessor"),mxr.forEach(t),dKe=r(m7e," (LayoutXLM model)"),m7e.forEach(t),cKe=i(No),ph=n(No,"LI",{});var g7e=s(ph);VU=n(g7e,"STRONG",{});var gxr=s(VU);fKe=r(gxr,"speech_to_text"),gxr.forEach(t),mKe=r(g7e," \u2014 "),mx=n(g7e,"A",{href:!0});var hxr=s(mx);gKe=r(hxr,"Speech2TextProcessor"),hxr.forEach(t),hKe=r(g7e," (Speech2Text model)"),g7e.forEach(t),pKe=i(No),_h=n(No,"LI",{});var h7e=s(_h);WU=n(h7e,"STRONG",{});var pxr=s(WU);_Ke=r(pxr,"speech_to_text_2"),pxr.forEach(t),uKe=r(h7e," \u2014 "),gx=n(h7e,"A",{href:!0});var _xr=s(gx);bKe=r(_xr,"Speech2Text2Processor"),_xr.forEach(t),vKe=r(h7e," (Speech2Text2 model)"),h7e.forEach(t),TKe=i(No),uh=n(No,"LI",{});var p7e=s(uh);QU=n(p7e,"STRONG",{});var uxr=s(QU);FKe=r(uxr,"trocr"),uxr.forEach(t),CKe=r(p7e," \u2014 "),hx=n(p7e,"A",{href:!0});var bxr=s(hx);MKe=r(bxr,"TrOCRProcessor"),bxr.forEach(t),EKe=r(p7e," (TrOCR model)"),p7e.forEach(t),yKe=i(No),bh=n(No,"LI",{});var _7e=s(bh);HU=n(_7e,"STRONG",{});var vxr=s(HU);wKe=r(vxr,"vision-text-dual-encoder"),vxr.forEach(t),AKe=r(_7e," \u2014 "),px=n(_7e,"A",{href:!0});var Txr=s(px);LKe=r(Txr,"VisionTextDualEncoderProcessor"),Txr.forEach(t),BKe=r(_7e," (VisionTextDualEncoder model)"),_7e.forEach(t),kKe=i(No),vh=n(No,"LI",{});var u7e=s(vh);UU=n(u7e,"STRONG",{});var Fxr=s(UU);xKe=r(Fxr,"wav2vec2"),Fxr.forEach(t),RKe=r(u7e," \u2014 "),_x=n(u7e,"A",{href:!0});var Cxr=s(_x);SKe=r(Cxr,"Wav2Vec2Processor"),Cxr.forEach(t),PKe=r(u7e," (Wav2Vec2 model)"),u7e.forEach(t),No.forEach(t),$Ke=i(Rt),m(Th.$$.fragment,Rt),IKe=i(Rt),JU=n(Rt,"P",{});var Mxr=s(JU);jKe=r(Mxr,"Examples:"),Mxr.forEach(t),NKe=i(Rt),m(PM.$$.fragment,Rt),Rt.forEach(t),DKe=i(Ns),Fh=n(Ns,"DIV",{class:!0});var ABe=s(Fh);m($M.$$.fragment,ABe),qKe=i(ABe),YU=n(ABe,"P",{});var Exr=s(YU);GKe=r(Exr,"Register a new processor for this class."),Exr.forEach(t),ABe.forEach(t),Ns.forEach(t),FLe=i(d),Gi=n(d,"H2",{class:!0});var LBe=s(Gi);Ch=n(LBe,"A",{id:!0,class:!0,href:!0});var yxr=s(Ch);KU=n(yxr,"SPAN",{});var wxr=s(KU);m(IM.$$.fragment,wxr),wxr.forEach(t),yxr.forEach(t),OKe=i(LBe),ZU=n(LBe,"SPAN",{});var Axr=s(ZU);XKe=r(Axr,"AutoModel"),Axr.forEach(t),LBe.forEach(t),CLe=i(d),Vo=n(d,"DIV",{class:!0});var Ds=s(Vo);m(jM.$$.fragment,Ds),zKe=i(Ds),Oi=n(Ds,"P",{});var JX=s(Oi);VKe=r(JX,`This is a generic model class that will be instantiated as one of the base model classes of the library when created
with the `),eJ=n(JX,"CODE",{});var Lxr=s(eJ);WKe=r(Lxr,"from_pretrained()"),Lxr.forEach(t),QKe=r(JX,"class method or the "),oJ=n(JX,"CODE",{});var Bxr=s(oJ);HKe=r(Bxr,"from_config()"),Bxr.forEach(t),UKe=r(JX,`class
method.`),JX.forEach(t),JKe=i(Ds),NM=n(Ds,"P",{});var BBe=s(NM);YKe=r(BBe,"This class cannot be instantiated directly using "),rJ=n(BBe,"CODE",{});var kxr=s(rJ);KKe=r(kxr,"__init__()"),kxr.forEach(t),ZKe=r(BBe," (throws an error)."),BBe.forEach(t),eZe=i(Ds),Nr=n(Ds,"DIV",{class:!0});var qs=s(Nr);m(DM.$$.fragment,qs),oZe=i(qs),tJ=n(qs,"P",{});var xxr=s(tJ);rZe=r(xxr,"Instantiates one of the base model classes of the library from a configuration."),xxr.forEach(t),tZe=i(qs),Xi=n(qs,"P",{});var YX=s(Xi);aZe=r(YX,`Note:
Loading a model from its configuration file does `),aJ=n(YX,"STRONG",{});var Rxr=s(aJ);nZe=r(Rxr,"not"),Rxr.forEach(t),sZe=r(YX,` load the model weights. It only affects the
model\u2019s configuration. Use `),nJ=n(YX,"CODE",{});var Sxr=s(nJ);lZe=r(Sxr,"from_pretrained()"),Sxr.forEach(t),iZe=r(YX,"to load the model weights."),YX.forEach(t),dZe=i(qs),sJ=n(qs,"P",{});var Pxr=s(sJ);cZe=r(Pxr,"Examples:"),Pxr.forEach(t),fZe=i(qs),m(qM.$$.fragment,qs),qs.forEach(t),mZe=i(Ds),ke=n(Ds,"DIV",{class:!0});var St=s(ke);m(GM.$$.fragment,St),gZe=i(St),lJ=n(St,"P",{});var $xr=s(lJ);hZe=r($xr,"Instantiate one of the base model classes of the library from a pretrained model."),$xr.forEach(t),pZe=i(St),Da=n(St,"P",{});var i4=s(Da);_Ze=r(i4,"The model class to instantiate is selected based on the "),iJ=n(i4,"CODE",{});var Ixr=s(iJ);uZe=r(Ixr,"model_type"),Ixr.forEach(t),bZe=r(i4,` property of the config object (either
passed as an argument or loaded from `),dJ=n(i4,"CODE",{});var jxr=s(dJ);vZe=r(jxr,"pretrained_model_name_or_path"),jxr.forEach(t),TZe=r(i4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),cJ=n(i4,"CODE",{});var Nxr=s(cJ);FZe=r(Nxr,"pretrained_model_name_or_path"),Nxr.forEach(t),CZe=r(i4,":"),i4.forEach(t),MZe=i(St),F=n(St,"UL",{});var C=s(F);Mh=n(C,"LI",{});var b7e=s(Mh);fJ=n(b7e,"STRONG",{});var Dxr=s(fJ);EZe=r(Dxr,"albert"),Dxr.forEach(t),yZe=r(b7e," \u2014 "),ux=n(b7e,"A",{href:!0});var qxr=s(ux);wZe=r(qxr,"AlbertModel"),qxr.forEach(t),AZe=r(b7e," (ALBERT model)"),b7e.forEach(t),LZe=i(C),Eh=n(C,"LI",{});var v7e=s(Eh);mJ=n(v7e,"STRONG",{});var Gxr=s(mJ);BZe=r(Gxr,"bart"),Gxr.forEach(t),kZe=r(v7e," \u2014 "),bx=n(v7e,"A",{href:!0});var Oxr=s(bx);xZe=r(Oxr,"BartModel"),Oxr.forEach(t),RZe=r(v7e," (BART model)"),v7e.forEach(t),SZe=i(C),yh=n(C,"LI",{});var T7e=s(yh);gJ=n(T7e,"STRONG",{});var Xxr=s(gJ);PZe=r(Xxr,"beit"),Xxr.forEach(t),$Ze=r(T7e," \u2014 "),vx=n(T7e,"A",{href:!0});var zxr=s(vx);IZe=r(zxr,"BeitModel"),zxr.forEach(t),jZe=r(T7e," (BEiT model)"),T7e.forEach(t),NZe=i(C),wh=n(C,"LI",{});var F7e=s(wh);hJ=n(F7e,"STRONG",{});var Vxr=s(hJ);DZe=r(Vxr,"bert"),Vxr.forEach(t),qZe=r(F7e," \u2014 "),Tx=n(F7e,"A",{href:!0});var Wxr=s(Tx);GZe=r(Wxr,"BertModel"),Wxr.forEach(t),OZe=r(F7e," (BERT model)"),F7e.forEach(t),XZe=i(C),Ah=n(C,"LI",{});var C7e=s(Ah);pJ=n(C7e,"STRONG",{});var Qxr=s(pJ);zZe=r(Qxr,"bert-generation"),Qxr.forEach(t),VZe=r(C7e," \u2014 "),Fx=n(C7e,"A",{href:!0});var Hxr=s(Fx);WZe=r(Hxr,"BertGenerationEncoder"),Hxr.forEach(t),QZe=r(C7e," (Bert Generation model)"),C7e.forEach(t),HZe=i(C),Lh=n(C,"LI",{});var M7e=s(Lh);_J=n(M7e,"STRONG",{});var Uxr=s(_J);UZe=r(Uxr,"big_bird"),Uxr.forEach(t),JZe=r(M7e," \u2014 "),Cx=n(M7e,"A",{href:!0});var Jxr=s(Cx);YZe=r(Jxr,"BigBirdModel"),Jxr.forEach(t),KZe=r(M7e," (BigBird model)"),M7e.forEach(t),ZZe=i(C),Bh=n(C,"LI",{});var E7e=s(Bh);uJ=n(E7e,"STRONG",{});var Yxr=s(uJ);eeo=r(Yxr,"bigbird_pegasus"),Yxr.forEach(t),oeo=r(E7e," \u2014 "),Mx=n(E7e,"A",{href:!0});var Kxr=s(Mx);reo=r(Kxr,"BigBirdPegasusModel"),Kxr.forEach(t),teo=r(E7e," (BigBirdPegasus model)"),E7e.forEach(t),aeo=i(C),kh=n(C,"LI",{});var y7e=s(kh);bJ=n(y7e,"STRONG",{});var Zxr=s(bJ);neo=r(Zxr,"blenderbot"),Zxr.forEach(t),seo=r(y7e," \u2014 "),Ex=n(y7e,"A",{href:!0});var eRr=s(Ex);leo=r(eRr,"BlenderbotModel"),eRr.forEach(t),ieo=r(y7e," (Blenderbot model)"),y7e.forEach(t),deo=i(C),xh=n(C,"LI",{});var w7e=s(xh);vJ=n(w7e,"STRONG",{});var oRr=s(vJ);ceo=r(oRr,"blenderbot-small"),oRr.forEach(t),feo=r(w7e," \u2014 "),yx=n(w7e,"A",{href:!0});var rRr=s(yx);meo=r(rRr,"BlenderbotSmallModel"),rRr.forEach(t),geo=r(w7e," (BlenderbotSmall model)"),w7e.forEach(t),heo=i(C),Rh=n(C,"LI",{});var A7e=s(Rh);TJ=n(A7e,"STRONG",{});var tRr=s(TJ);peo=r(tRr,"camembert"),tRr.forEach(t),_eo=r(A7e," \u2014 "),wx=n(A7e,"A",{href:!0});var aRr=s(wx);ueo=r(aRr,"CamembertModel"),aRr.forEach(t),beo=r(A7e," (CamemBERT model)"),A7e.forEach(t),veo=i(C),Sh=n(C,"LI",{});var L7e=s(Sh);FJ=n(L7e,"STRONG",{});var nRr=s(FJ);Teo=r(nRr,"canine"),nRr.forEach(t),Feo=r(L7e," \u2014 "),Ax=n(L7e,"A",{href:!0});var sRr=s(Ax);Ceo=r(sRr,"CanineModel"),sRr.forEach(t),Meo=r(L7e," (Canine model)"),L7e.forEach(t),Eeo=i(C),Ph=n(C,"LI",{});var B7e=s(Ph);CJ=n(B7e,"STRONG",{});var lRr=s(CJ);yeo=r(lRr,"clip"),lRr.forEach(t),weo=r(B7e," \u2014 "),Lx=n(B7e,"A",{href:!0});var iRr=s(Lx);Aeo=r(iRr,"CLIPModel"),iRr.forEach(t),Leo=r(B7e," (CLIP model)"),B7e.forEach(t),Beo=i(C),$h=n(C,"LI",{});var k7e=s($h);MJ=n(k7e,"STRONG",{});var dRr=s(MJ);keo=r(dRr,"convbert"),dRr.forEach(t),xeo=r(k7e," \u2014 "),Bx=n(k7e,"A",{href:!0});var cRr=s(Bx);Reo=r(cRr,"ConvBertModel"),cRr.forEach(t),Seo=r(k7e," (ConvBERT model)"),k7e.forEach(t),Peo=i(C),Ih=n(C,"LI",{});var x7e=s(Ih);EJ=n(x7e,"STRONG",{});var fRr=s(EJ);$eo=r(fRr,"convnext"),fRr.forEach(t),Ieo=r(x7e," \u2014 "),kx=n(x7e,"A",{href:!0});var mRr=s(kx);jeo=r(mRr,"ConvNextModel"),mRr.forEach(t),Neo=r(x7e," (ConvNext model)"),x7e.forEach(t),Deo=i(C),jh=n(C,"LI",{});var R7e=s(jh);yJ=n(R7e,"STRONG",{});var gRr=s(yJ);qeo=r(gRr,"ctrl"),gRr.forEach(t),Geo=r(R7e," \u2014 "),xx=n(R7e,"A",{href:!0});var hRr=s(xx);Oeo=r(hRr,"CTRLModel"),hRr.forEach(t),Xeo=r(R7e," (CTRL model)"),R7e.forEach(t),zeo=i(C),Nh=n(C,"LI",{});var S7e=s(Nh);wJ=n(S7e,"STRONG",{});var pRr=s(wJ);Veo=r(pRr,"deberta"),pRr.forEach(t),Weo=r(S7e," \u2014 "),Rx=n(S7e,"A",{href:!0});var _Rr=s(Rx);Qeo=r(_Rr,"DebertaModel"),_Rr.forEach(t),Heo=r(S7e," (DeBERTa model)"),S7e.forEach(t),Ueo=i(C),Dh=n(C,"LI",{});var P7e=s(Dh);AJ=n(P7e,"STRONG",{});var uRr=s(AJ);Jeo=r(uRr,"deberta-v2"),uRr.forEach(t),Yeo=r(P7e," \u2014 "),Sx=n(P7e,"A",{href:!0});var bRr=s(Sx);Keo=r(bRr,"DebertaV2Model"),bRr.forEach(t),Zeo=r(P7e," (DeBERTa-v2 model)"),P7e.forEach(t),eoo=i(C),qh=n(C,"LI",{});var $7e=s(qh);LJ=n($7e,"STRONG",{});var vRr=s(LJ);ooo=r(vRr,"deit"),vRr.forEach(t),roo=r($7e," \u2014 "),Px=n($7e,"A",{href:!0});var TRr=s(Px);too=r(TRr,"DeiTModel"),TRr.forEach(t),aoo=r($7e," (DeiT model)"),$7e.forEach(t),noo=i(C),Gh=n(C,"LI",{});var I7e=s(Gh);BJ=n(I7e,"STRONG",{});var FRr=s(BJ);soo=r(FRr,"detr"),FRr.forEach(t),loo=r(I7e," \u2014 "),$x=n(I7e,"A",{href:!0});var CRr=s($x);ioo=r(CRr,"DetrModel"),CRr.forEach(t),doo=r(I7e," (DETR model)"),I7e.forEach(t),coo=i(C),Oh=n(C,"LI",{});var j7e=s(Oh);kJ=n(j7e,"STRONG",{});var MRr=s(kJ);foo=r(MRr,"distilbert"),MRr.forEach(t),moo=r(j7e," \u2014 "),Ix=n(j7e,"A",{href:!0});var ERr=s(Ix);goo=r(ERr,"DistilBertModel"),ERr.forEach(t),hoo=r(j7e," (DistilBERT model)"),j7e.forEach(t),poo=i(C),Xh=n(C,"LI",{});var N7e=s(Xh);xJ=n(N7e,"STRONG",{});var yRr=s(xJ);_oo=r(yRr,"dpr"),yRr.forEach(t),uoo=r(N7e," \u2014 "),jx=n(N7e,"A",{href:!0});var wRr=s(jx);boo=r(wRr,"DPRQuestionEncoder"),wRr.forEach(t),voo=r(N7e," (DPR model)"),N7e.forEach(t),Too=i(C),zh=n(C,"LI",{});var D7e=s(zh);RJ=n(D7e,"STRONG",{});var ARr=s(RJ);Foo=r(ARr,"electra"),ARr.forEach(t),Coo=r(D7e," \u2014 "),Nx=n(D7e,"A",{href:!0});var LRr=s(Nx);Moo=r(LRr,"ElectraModel"),LRr.forEach(t),Eoo=r(D7e," (ELECTRA model)"),D7e.forEach(t),yoo=i(C),Vh=n(C,"LI",{});var q7e=s(Vh);SJ=n(q7e,"STRONG",{});var BRr=s(SJ);woo=r(BRr,"flaubert"),BRr.forEach(t),Aoo=r(q7e," \u2014 "),Dx=n(q7e,"A",{href:!0});var kRr=s(Dx);Loo=r(kRr,"FlaubertModel"),kRr.forEach(t),Boo=r(q7e," (FlauBERT model)"),q7e.forEach(t),koo=i(C),Wh=n(C,"LI",{});var G7e=s(Wh);PJ=n(G7e,"STRONG",{});var xRr=s(PJ);xoo=r(xRr,"fnet"),xRr.forEach(t),Roo=r(G7e," \u2014 "),qx=n(G7e,"A",{href:!0});var RRr=s(qx);Soo=r(RRr,"FNetModel"),RRr.forEach(t),Poo=r(G7e," (FNet model)"),G7e.forEach(t),$oo=i(C),Qh=n(C,"LI",{});var O7e=s(Qh);$J=n(O7e,"STRONG",{});var SRr=s($J);Ioo=r(SRr,"fsmt"),SRr.forEach(t),joo=r(O7e," \u2014 "),Gx=n(O7e,"A",{href:!0});var PRr=s(Gx);Noo=r(PRr,"FSMTModel"),PRr.forEach(t),Doo=r(O7e," (FairSeq Machine-Translation model)"),O7e.forEach(t),qoo=i(C),Rs=n(C,"LI",{});var N0=s(Rs);IJ=n(N0,"STRONG",{});var $Rr=s(IJ);Goo=r($Rr,"funnel"),$Rr.forEach(t),Ooo=r(N0," \u2014 "),Ox=n(N0,"A",{href:!0});var IRr=s(Ox);Xoo=r(IRr,"FunnelModel"),IRr.forEach(t),zoo=r(N0," or "),Xx=n(N0,"A",{href:!0});var jRr=s(Xx);Voo=r(jRr,"FunnelBaseModel"),jRr.forEach(t),Woo=r(N0," (Funnel Transformer model)"),N0.forEach(t),Qoo=i(C),Hh=n(C,"LI",{});var X7e=s(Hh);jJ=n(X7e,"STRONG",{});var NRr=s(jJ);Hoo=r(NRr,"gpt2"),NRr.forEach(t),Uoo=r(X7e," \u2014 "),zx=n(X7e,"A",{href:!0});var DRr=s(zx);Joo=r(DRr,"GPT2Model"),DRr.forEach(t),Yoo=r(X7e," (OpenAI GPT-2 model)"),X7e.forEach(t),Koo=i(C),Uh=n(C,"LI",{});var z7e=s(Uh);NJ=n(z7e,"STRONG",{});var qRr=s(NJ);Zoo=r(qRr,"gpt_neo"),qRr.forEach(t),ero=r(z7e," \u2014 "),Vx=n(z7e,"A",{href:!0});var GRr=s(Vx);oro=r(GRr,"GPTNeoModel"),GRr.forEach(t),rro=r(z7e," (GPT Neo model)"),z7e.forEach(t),tro=i(C),Jh=n(C,"LI",{});var V7e=s(Jh);DJ=n(V7e,"STRONG",{});var ORr=s(DJ);aro=r(ORr,"gptj"),ORr.forEach(t),nro=r(V7e," \u2014 "),Wx=n(V7e,"A",{href:!0});var XRr=s(Wx);sro=r(XRr,"GPTJModel"),XRr.forEach(t),lro=r(V7e," (GPT-J model)"),V7e.forEach(t),iro=i(C),Yh=n(C,"LI",{});var W7e=s(Yh);qJ=n(W7e,"STRONG",{});var zRr=s(qJ);dro=r(zRr,"hubert"),zRr.forEach(t),cro=r(W7e," \u2014 "),Qx=n(W7e,"A",{href:!0});var VRr=s(Qx);fro=r(VRr,"HubertModel"),VRr.forEach(t),mro=r(W7e," (Hubert model)"),W7e.forEach(t),gro=i(C),Kh=n(C,"LI",{});var Q7e=s(Kh);GJ=n(Q7e,"STRONG",{});var WRr=s(GJ);hro=r(WRr,"ibert"),WRr.forEach(t),pro=r(Q7e," \u2014 "),Hx=n(Q7e,"A",{href:!0});var QRr=s(Hx);_ro=r(QRr,"IBertModel"),QRr.forEach(t),uro=r(Q7e," (I-BERT model)"),Q7e.forEach(t),bro=i(C),Zh=n(C,"LI",{});var H7e=s(Zh);OJ=n(H7e,"STRONG",{});var HRr=s(OJ);vro=r(HRr,"imagegpt"),HRr.forEach(t),Tro=r(H7e," \u2014 "),Ux=n(H7e,"A",{href:!0});var URr=s(Ux);Fro=r(URr,"ImageGPTModel"),URr.forEach(t),Cro=r(H7e," (ImageGPT model)"),H7e.forEach(t),Mro=i(C),ep=n(C,"LI",{});var U7e=s(ep);XJ=n(U7e,"STRONG",{});var JRr=s(XJ);Ero=r(JRr,"layoutlm"),JRr.forEach(t),yro=r(U7e," \u2014 "),Jx=n(U7e,"A",{href:!0});var YRr=s(Jx);wro=r(YRr,"LayoutLMModel"),YRr.forEach(t),Aro=r(U7e," (LayoutLM model)"),U7e.forEach(t),Lro=i(C),op=n(C,"LI",{});var J7e=s(op);zJ=n(J7e,"STRONG",{});var KRr=s(zJ);Bro=r(KRr,"layoutlmv2"),KRr.forEach(t),kro=r(J7e," \u2014 "),Yx=n(J7e,"A",{href:!0});var ZRr=s(Yx);xro=r(ZRr,"LayoutLMv2Model"),ZRr.forEach(t),Rro=r(J7e," (LayoutLMv2 model)"),J7e.forEach(t),Sro=i(C),rp=n(C,"LI",{});var Y7e=s(rp);VJ=n(Y7e,"STRONG",{});var eSr=s(VJ);Pro=r(eSr,"led"),eSr.forEach(t),$ro=r(Y7e," \u2014 "),Kx=n(Y7e,"A",{href:!0});var oSr=s(Kx);Iro=r(oSr,"LEDModel"),oSr.forEach(t),jro=r(Y7e," (LED model)"),Y7e.forEach(t),Nro=i(C),tp=n(C,"LI",{});var K7e=s(tp);WJ=n(K7e,"STRONG",{});var rSr=s(WJ);Dro=r(rSr,"longformer"),rSr.forEach(t),qro=r(K7e," \u2014 "),Zx=n(K7e,"A",{href:!0});var tSr=s(Zx);Gro=r(tSr,"LongformerModel"),tSr.forEach(t),Oro=r(K7e," (Longformer model)"),K7e.forEach(t),Xro=i(C),ap=n(C,"LI",{});var Z7e=s(ap);QJ=n(Z7e,"STRONG",{});var aSr=s(QJ);zro=r(aSr,"luke"),aSr.forEach(t),Vro=r(Z7e," \u2014 "),eR=n(Z7e,"A",{href:!0});var nSr=s(eR);Wro=r(nSr,"LukeModel"),nSr.forEach(t),Qro=r(Z7e," (LUKE model)"),Z7e.forEach(t),Hro=i(C),np=n(C,"LI",{});var eFe=s(np);HJ=n(eFe,"STRONG",{});var sSr=s(HJ);Uro=r(sSr,"lxmert"),sSr.forEach(t),Jro=r(eFe," \u2014 "),oR=n(eFe,"A",{href:!0});var lSr=s(oR);Yro=r(lSr,"LxmertModel"),lSr.forEach(t),Kro=r(eFe," (LXMERT model)"),eFe.forEach(t),Zro=i(C),sp=n(C,"LI",{});var oFe=s(sp);UJ=n(oFe,"STRONG",{});var iSr=s(UJ);eto=r(iSr,"m2m_100"),iSr.forEach(t),oto=r(oFe," \u2014 "),rR=n(oFe,"A",{href:!0});var dSr=s(rR);rto=r(dSr,"M2M100Model"),dSr.forEach(t),tto=r(oFe," (M2M100 model)"),oFe.forEach(t),ato=i(C),lp=n(C,"LI",{});var rFe=s(lp);JJ=n(rFe,"STRONG",{});var cSr=s(JJ);nto=r(cSr,"marian"),cSr.forEach(t),sto=r(rFe," \u2014 "),tR=n(rFe,"A",{href:!0});var fSr=s(tR);lto=r(fSr,"MarianModel"),fSr.forEach(t),ito=r(rFe," (Marian model)"),rFe.forEach(t),dto=i(C),ip=n(C,"LI",{});var tFe=s(ip);YJ=n(tFe,"STRONG",{});var mSr=s(YJ);cto=r(mSr,"mbart"),mSr.forEach(t),fto=r(tFe," \u2014 "),aR=n(tFe,"A",{href:!0});var gSr=s(aR);mto=r(gSr,"MBartModel"),gSr.forEach(t),gto=r(tFe," (mBART model)"),tFe.forEach(t),hto=i(C),dp=n(C,"LI",{});var aFe=s(dp);KJ=n(aFe,"STRONG",{});var hSr=s(KJ);pto=r(hSr,"megatron-bert"),hSr.forEach(t),_to=r(aFe," \u2014 "),nR=n(aFe,"A",{href:!0});var pSr=s(nR);uto=r(pSr,"MegatronBertModel"),pSr.forEach(t),bto=r(aFe," (MegatronBert model)"),aFe.forEach(t),vto=i(C),cp=n(C,"LI",{});var nFe=s(cp);ZJ=n(nFe,"STRONG",{});var _Sr=s(ZJ);Tto=r(_Sr,"mobilebert"),_Sr.forEach(t),Fto=r(nFe," \u2014 "),sR=n(nFe,"A",{href:!0});var uSr=s(sR);Cto=r(uSr,"MobileBertModel"),uSr.forEach(t),Mto=r(nFe," (MobileBERT model)"),nFe.forEach(t),Eto=i(C),fp=n(C,"LI",{});var sFe=s(fp);eY=n(sFe,"STRONG",{});var bSr=s(eY);yto=r(bSr,"mpnet"),bSr.forEach(t),wto=r(sFe," \u2014 "),lR=n(sFe,"A",{href:!0});var vSr=s(lR);Ato=r(vSr,"MPNetModel"),vSr.forEach(t),Lto=r(sFe," (MPNet model)"),sFe.forEach(t),Bto=i(C),mp=n(C,"LI",{});var lFe=s(mp);oY=n(lFe,"STRONG",{});var TSr=s(oY);kto=r(TSr,"mt5"),TSr.forEach(t),xto=r(lFe," \u2014 "),iR=n(lFe,"A",{href:!0});var FSr=s(iR);Rto=r(FSr,"MT5Model"),FSr.forEach(t),Sto=r(lFe," (mT5 model)"),lFe.forEach(t),Pto=i(C),gp=n(C,"LI",{});var iFe=s(gp);rY=n(iFe,"STRONG",{});var CSr=s(rY);$to=r(CSr,"nystromformer"),CSr.forEach(t),Ito=r(iFe," \u2014 "),dR=n(iFe,"A",{href:!0});var MSr=s(dR);jto=r(MSr,"NystromformerModel"),MSr.forEach(t),Nto=r(iFe," (Nystromformer model)"),iFe.forEach(t),Dto=i(C),hp=n(C,"LI",{});var dFe=s(hp);tY=n(dFe,"STRONG",{});var ESr=s(tY);qto=r(ESr,"openai-gpt"),ESr.forEach(t),Gto=r(dFe," \u2014 "),cR=n(dFe,"A",{href:!0});var ySr=s(cR);Oto=r(ySr,"OpenAIGPTModel"),ySr.forEach(t),Xto=r(dFe," (OpenAI GPT model)"),dFe.forEach(t),zto=i(C),pp=n(C,"LI",{});var cFe=s(pp);aY=n(cFe,"STRONG",{});var wSr=s(aY);Vto=r(wSr,"pegasus"),wSr.forEach(t),Wto=r(cFe," \u2014 "),fR=n(cFe,"A",{href:!0});var ASr=s(fR);Qto=r(ASr,"PegasusModel"),ASr.forEach(t),Hto=r(cFe," (Pegasus model)"),cFe.forEach(t),Uto=i(C),_p=n(C,"LI",{});var fFe=s(_p);nY=n(fFe,"STRONG",{});var LSr=s(nY);Jto=r(LSr,"perceiver"),LSr.forEach(t),Yto=r(fFe," \u2014 "),mR=n(fFe,"A",{href:!0});var BSr=s(mR);Kto=r(BSr,"PerceiverModel"),BSr.forEach(t),Zto=r(fFe," (Perceiver model)"),fFe.forEach(t),eao=i(C),up=n(C,"LI",{});var mFe=s(up);sY=n(mFe,"STRONG",{});var kSr=s(sY);oao=r(kSr,"plbart"),kSr.forEach(t),rao=r(mFe," \u2014 "),gR=n(mFe,"A",{href:!0});var xSr=s(gR);tao=r(xSr,"PLBartModel"),xSr.forEach(t),aao=r(mFe," (PLBart model)"),mFe.forEach(t),nao=i(C),bp=n(C,"LI",{});var gFe=s(bp);lY=n(gFe,"STRONG",{});var RSr=s(lY);sao=r(RSr,"poolformer"),RSr.forEach(t),lao=r(gFe," \u2014 "),hR=n(gFe,"A",{href:!0});var SSr=s(hR);iao=r(SSr,"PoolFormerModel"),SSr.forEach(t),dao=r(gFe," (PoolFormer model)"),gFe.forEach(t),cao=i(C),vp=n(C,"LI",{});var hFe=s(vp);iY=n(hFe,"STRONG",{});var PSr=s(iY);fao=r(PSr,"prophetnet"),PSr.forEach(t),mao=r(hFe," \u2014 "),pR=n(hFe,"A",{href:!0});var $Sr=s(pR);gao=r($Sr,"ProphetNetModel"),$Sr.forEach(t),hao=r(hFe," (ProphetNet model)"),hFe.forEach(t),pao=i(C),Tp=n(C,"LI",{});var pFe=s(Tp);dY=n(pFe,"STRONG",{});var ISr=s(dY);_ao=r(ISr,"qdqbert"),ISr.forEach(t),uao=r(pFe," \u2014 "),_R=n(pFe,"A",{href:!0});var jSr=s(_R);bao=r(jSr,"QDQBertModel"),jSr.forEach(t),vao=r(pFe," (QDQBert model)"),pFe.forEach(t),Tao=i(C),Fp=n(C,"LI",{});var _Fe=s(Fp);cY=n(_Fe,"STRONG",{});var NSr=s(cY);Fao=r(NSr,"reformer"),NSr.forEach(t),Cao=r(_Fe," \u2014 "),uR=n(_Fe,"A",{href:!0});var DSr=s(uR);Mao=r(DSr,"ReformerModel"),DSr.forEach(t),Eao=r(_Fe," (Reformer model)"),_Fe.forEach(t),yao=i(C),Cp=n(C,"LI",{});var uFe=s(Cp);fY=n(uFe,"STRONG",{});var qSr=s(fY);wao=r(qSr,"rembert"),qSr.forEach(t),Aao=r(uFe," \u2014 "),bR=n(uFe,"A",{href:!0});var GSr=s(bR);Lao=r(GSr,"RemBertModel"),GSr.forEach(t),Bao=r(uFe," (RemBERT model)"),uFe.forEach(t),kao=i(C),Mp=n(C,"LI",{});var bFe=s(Mp);mY=n(bFe,"STRONG",{});var OSr=s(mY);xao=r(OSr,"retribert"),OSr.forEach(t),Rao=r(bFe," \u2014 "),vR=n(bFe,"A",{href:!0});var XSr=s(vR);Sao=r(XSr,"RetriBertModel"),XSr.forEach(t),Pao=r(bFe," (RetriBERT model)"),bFe.forEach(t),$ao=i(C),Ep=n(C,"LI",{});var vFe=s(Ep);gY=n(vFe,"STRONG",{});var zSr=s(gY);Iao=r(zSr,"roberta"),zSr.forEach(t),jao=r(vFe," \u2014 "),TR=n(vFe,"A",{href:!0});var VSr=s(TR);Nao=r(VSr,"RobertaModel"),VSr.forEach(t),Dao=r(vFe," (RoBERTa model)"),vFe.forEach(t),qao=i(C),yp=n(C,"LI",{});var TFe=s(yp);hY=n(TFe,"STRONG",{});var WSr=s(hY);Gao=r(WSr,"roformer"),WSr.forEach(t),Oao=r(TFe," \u2014 "),FR=n(TFe,"A",{href:!0});var QSr=s(FR);Xao=r(QSr,"RoFormerModel"),QSr.forEach(t),zao=r(TFe," (RoFormer model)"),TFe.forEach(t),Vao=i(C),wp=n(C,"LI",{});var FFe=s(wp);pY=n(FFe,"STRONG",{});var HSr=s(pY);Wao=r(HSr,"segformer"),HSr.forEach(t),Qao=r(FFe," \u2014 "),CR=n(FFe,"A",{href:!0});var USr=s(CR);Hao=r(USr,"SegformerModel"),USr.forEach(t),Uao=r(FFe," (SegFormer model)"),FFe.forEach(t),Jao=i(C),Ap=n(C,"LI",{});var CFe=s(Ap);_Y=n(CFe,"STRONG",{});var JSr=s(_Y);Yao=r(JSr,"sew"),JSr.forEach(t),Kao=r(CFe," \u2014 "),MR=n(CFe,"A",{href:!0});var YSr=s(MR);Zao=r(YSr,"SEWModel"),YSr.forEach(t),eno=r(CFe," (SEW model)"),CFe.forEach(t),ono=i(C),Lp=n(C,"LI",{});var MFe=s(Lp);uY=n(MFe,"STRONG",{});var KSr=s(uY);rno=r(KSr,"sew-d"),KSr.forEach(t),tno=r(MFe," \u2014 "),ER=n(MFe,"A",{href:!0});var ZSr=s(ER);ano=r(ZSr,"SEWDModel"),ZSr.forEach(t),nno=r(MFe," (SEW-D model)"),MFe.forEach(t),sno=i(C),Bp=n(C,"LI",{});var EFe=s(Bp);bY=n(EFe,"STRONG",{});var ePr=s(bY);lno=r(ePr,"speech_to_text"),ePr.forEach(t),ino=r(EFe," \u2014 "),yR=n(EFe,"A",{href:!0});var oPr=s(yR);dno=r(oPr,"Speech2TextModel"),oPr.forEach(t),cno=r(EFe," (Speech2Text model)"),EFe.forEach(t),fno=i(C),kp=n(C,"LI",{});var yFe=s(kp);vY=n(yFe,"STRONG",{});var rPr=s(vY);mno=r(rPr,"splinter"),rPr.forEach(t),gno=r(yFe," \u2014 "),wR=n(yFe,"A",{href:!0});var tPr=s(wR);hno=r(tPr,"SplinterModel"),tPr.forEach(t),pno=r(yFe," (Splinter model)"),yFe.forEach(t),_no=i(C),xp=n(C,"LI",{});var wFe=s(xp);TY=n(wFe,"STRONG",{});var aPr=s(TY);uno=r(aPr,"squeezebert"),aPr.forEach(t),bno=r(wFe," \u2014 "),AR=n(wFe,"A",{href:!0});var nPr=s(AR);vno=r(nPr,"SqueezeBertModel"),nPr.forEach(t),Tno=r(wFe," (SqueezeBERT model)"),wFe.forEach(t),Fno=i(C),Rp=n(C,"LI",{});var AFe=s(Rp);FY=n(AFe,"STRONG",{});var sPr=s(FY);Cno=r(sPr,"swin"),sPr.forEach(t),Mno=r(AFe," \u2014 "),LR=n(AFe,"A",{href:!0});var lPr=s(LR);Eno=r(lPr,"SwinModel"),lPr.forEach(t),yno=r(AFe," (Swin model)"),AFe.forEach(t),wno=i(C),Sp=n(C,"LI",{});var LFe=s(Sp);CY=n(LFe,"STRONG",{});var iPr=s(CY);Ano=r(iPr,"t5"),iPr.forEach(t),Lno=r(LFe," \u2014 "),BR=n(LFe,"A",{href:!0});var dPr=s(BR);Bno=r(dPr,"T5Model"),dPr.forEach(t),kno=r(LFe," (T5 model)"),LFe.forEach(t),xno=i(C),Pp=n(C,"LI",{});var BFe=s(Pp);MY=n(BFe,"STRONG",{});var cPr=s(MY);Rno=r(cPr,"tapas"),cPr.forEach(t),Sno=r(BFe," \u2014 "),kR=n(BFe,"A",{href:!0});var fPr=s(kR);Pno=r(fPr,"TapasModel"),fPr.forEach(t),$no=r(BFe," (TAPAS model)"),BFe.forEach(t),Ino=i(C),$p=n(C,"LI",{});var kFe=s($p);EY=n(kFe,"STRONG",{});var mPr=s(EY);jno=r(mPr,"transfo-xl"),mPr.forEach(t),Nno=r(kFe," \u2014 "),xR=n(kFe,"A",{href:!0});var gPr=s(xR);Dno=r(gPr,"TransfoXLModel"),gPr.forEach(t),qno=r(kFe," (Transformer-XL model)"),kFe.forEach(t),Gno=i(C),Ip=n(C,"LI",{});var xFe=s(Ip);yY=n(xFe,"STRONG",{});var hPr=s(yY);Ono=r(hPr,"unispeech"),hPr.forEach(t),Xno=r(xFe," \u2014 "),RR=n(xFe,"A",{href:!0});var pPr=s(RR);zno=r(pPr,"UniSpeechModel"),pPr.forEach(t),Vno=r(xFe," (UniSpeech model)"),xFe.forEach(t),Wno=i(C),jp=n(C,"LI",{});var RFe=s(jp);wY=n(RFe,"STRONG",{});var _Pr=s(wY);Qno=r(_Pr,"unispeech-sat"),_Pr.forEach(t),Hno=r(RFe," \u2014 "),SR=n(RFe,"A",{href:!0});var uPr=s(SR);Uno=r(uPr,"UniSpeechSatModel"),uPr.forEach(t),Jno=r(RFe," (UniSpeechSat model)"),RFe.forEach(t),Yno=i(C),Np=n(C,"LI",{});var SFe=s(Np);AY=n(SFe,"STRONG",{});var bPr=s(AY);Kno=r(bPr,"vilt"),bPr.forEach(t),Zno=r(SFe," \u2014 "),PR=n(SFe,"A",{href:!0});var vPr=s(PR);eso=r(vPr,"ViltModel"),vPr.forEach(t),oso=r(SFe," (ViLT model)"),SFe.forEach(t),rso=i(C),Dp=n(C,"LI",{});var PFe=s(Dp);LY=n(PFe,"STRONG",{});var TPr=s(LY);tso=r(TPr,"vision-text-dual-encoder"),TPr.forEach(t),aso=r(PFe," \u2014 "),$R=n(PFe,"A",{href:!0});var FPr=s($R);nso=r(FPr,"VisionTextDualEncoderModel"),FPr.forEach(t),sso=r(PFe," (VisionTextDualEncoder model)"),PFe.forEach(t),lso=i(C),qp=n(C,"LI",{});var $Fe=s(qp);BY=n($Fe,"STRONG",{});var CPr=s(BY);iso=r(CPr,"visual_bert"),CPr.forEach(t),dso=r($Fe," \u2014 "),IR=n($Fe,"A",{href:!0});var MPr=s(IR);cso=r(MPr,"VisualBertModel"),MPr.forEach(t),fso=r($Fe," (VisualBert model)"),$Fe.forEach(t),mso=i(C),Gp=n(C,"LI",{});var IFe=s(Gp);kY=n(IFe,"STRONG",{});var EPr=s(kY);gso=r(EPr,"vit"),EPr.forEach(t),hso=r(IFe," \u2014 "),jR=n(IFe,"A",{href:!0});var yPr=s(jR);pso=r(yPr,"ViTModel"),yPr.forEach(t),_so=r(IFe," (ViT model)"),IFe.forEach(t),uso=i(C),Op=n(C,"LI",{});var jFe=s(Op);xY=n(jFe,"STRONG",{});var wPr=s(xY);bso=r(wPr,"vit_mae"),wPr.forEach(t),vso=r(jFe," \u2014 "),NR=n(jFe,"A",{href:!0});var APr=s(NR);Tso=r(APr,"ViTMAEModel"),APr.forEach(t),Fso=r(jFe," (ViTMAE model)"),jFe.forEach(t),Cso=i(C),Xp=n(C,"LI",{});var NFe=s(Xp);RY=n(NFe,"STRONG",{});var LPr=s(RY);Mso=r(LPr,"wav2vec2"),LPr.forEach(t),Eso=r(NFe," \u2014 "),DR=n(NFe,"A",{href:!0});var BPr=s(DR);yso=r(BPr,"Wav2Vec2Model"),BPr.forEach(t),wso=r(NFe," (Wav2Vec2 model)"),NFe.forEach(t),Aso=i(C),zp=n(C,"LI",{});var DFe=s(zp);SY=n(DFe,"STRONG",{});var kPr=s(SY);Lso=r(kPr,"wavlm"),kPr.forEach(t),Bso=r(DFe," \u2014 "),qR=n(DFe,"A",{href:!0});var xPr=s(qR);kso=r(xPr,"WavLMModel"),xPr.forEach(t),xso=r(DFe," (WavLM model)"),DFe.forEach(t),Rso=i(C),Vp=n(C,"LI",{});var qFe=s(Vp);PY=n(qFe,"STRONG",{});var RPr=s(PY);Sso=r(RPr,"xglm"),RPr.forEach(t),Pso=r(qFe," \u2014 "),GR=n(qFe,"A",{href:!0});var SPr=s(GR);$so=r(SPr,"XGLMModel"),SPr.forEach(t),Iso=r(qFe," (XGLM model)"),qFe.forEach(t),jso=i(C),Wp=n(C,"LI",{});var GFe=s(Wp);$Y=n(GFe,"STRONG",{});var PPr=s($Y);Nso=r(PPr,"xlm"),PPr.forEach(t),Dso=r(GFe," \u2014 "),OR=n(GFe,"A",{href:!0});var $Pr=s(OR);qso=r($Pr,"XLMModel"),$Pr.forEach(t),Gso=r(GFe," (XLM model)"),GFe.forEach(t),Oso=i(C),Qp=n(C,"LI",{});var OFe=s(Qp);IY=n(OFe,"STRONG",{});var IPr=s(IY);Xso=r(IPr,"xlm-prophetnet"),IPr.forEach(t),zso=r(OFe," \u2014 "),XR=n(OFe,"A",{href:!0});var jPr=s(XR);Vso=r(jPr,"XLMProphetNetModel"),jPr.forEach(t),Wso=r(OFe," (XLMProphetNet model)"),OFe.forEach(t),Qso=i(C),Hp=n(C,"LI",{});var XFe=s(Hp);jY=n(XFe,"STRONG",{});var NPr=s(jY);Hso=r(NPr,"xlm-roberta"),NPr.forEach(t),Uso=r(XFe," \u2014 "),zR=n(XFe,"A",{href:!0});var DPr=s(zR);Jso=r(DPr,"XLMRobertaModel"),DPr.forEach(t),Yso=r(XFe," (XLM-RoBERTa model)"),XFe.forEach(t),Kso=i(C),Up=n(C,"LI",{});var zFe=s(Up);NY=n(zFe,"STRONG",{});var qPr=s(NY);Zso=r(qPr,"xlm-roberta-xl"),qPr.forEach(t),elo=r(zFe," \u2014 "),VR=n(zFe,"A",{href:!0});var GPr=s(VR);olo=r(GPr,"XLMRobertaXLModel"),GPr.forEach(t),rlo=r(zFe," (XLM-RoBERTa-XL model)"),zFe.forEach(t),tlo=i(C),Jp=n(C,"LI",{});var VFe=s(Jp);DY=n(VFe,"STRONG",{});var OPr=s(DY);alo=r(OPr,"xlnet"),OPr.forEach(t),nlo=r(VFe," \u2014 "),WR=n(VFe,"A",{href:!0});var XPr=s(WR);slo=r(XPr,"XLNetModel"),XPr.forEach(t),llo=r(VFe," (XLNet model)"),VFe.forEach(t),ilo=i(C),Yp=n(C,"LI",{});var WFe=s(Yp);qY=n(WFe,"STRONG",{});var zPr=s(qY);dlo=r(zPr,"yoso"),zPr.forEach(t),clo=r(WFe," \u2014 "),QR=n(WFe,"A",{href:!0});var VPr=s(QR);flo=r(VPr,"YosoModel"),VPr.forEach(t),mlo=r(WFe," (YOSO model)"),WFe.forEach(t),C.forEach(t),glo=i(St),Kp=n(St,"P",{});var QFe=s(Kp);hlo=r(QFe,"The model is set in evaluation mode by default using "),GY=n(QFe,"CODE",{});var WPr=s(GY);plo=r(WPr,"model.eval()"),WPr.forEach(t),_lo=r(QFe,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),OY=n(QFe,"CODE",{});var QPr=s(OY);ulo=r(QPr,"model.train()"),QPr.forEach(t),QFe.forEach(t),blo=i(St),XY=n(St,"P",{});var HPr=s(XY);vlo=r(HPr,"Examples:"),HPr.forEach(t),Tlo=i(St),m(OM.$$.fragment,St),St.forEach(t),Ds.forEach(t),MLe=i(d),zi=n(d,"H2",{class:!0});var kBe=s(zi);Zp=n(kBe,"A",{id:!0,class:!0,href:!0});var UPr=s(Zp);zY=n(UPr,"SPAN",{});var JPr=s(zY);m(XM.$$.fragment,JPr),JPr.forEach(t),UPr.forEach(t),Flo=i(kBe),VY=n(kBe,"SPAN",{});var YPr=s(VY);Clo=r(YPr,"AutoModelForPreTraining"),YPr.forEach(t),kBe.forEach(t),ELe=i(d),Wo=n(d,"DIV",{class:!0});var Gs=s(Wo);m(zM.$$.fragment,Gs),Mlo=i(Gs),Vi=n(Gs,"P",{});var KX=s(Vi);Elo=r(KX,`This is a generic model class that will be instantiated as one of the model classes of the library (with a pretraining head) when created
with the `),WY=n(KX,"CODE",{});var KPr=s(WY);ylo=r(KPr,"from_pretrained()"),KPr.forEach(t),wlo=r(KX,"class method or the "),QY=n(KX,"CODE",{});var ZPr=s(QY);Alo=r(ZPr,"from_config()"),ZPr.forEach(t),Llo=r(KX,`class
method.`),KX.forEach(t),Blo=i(Gs),VM=n(Gs,"P",{});var xBe=s(VM);klo=r(xBe,"This class cannot be instantiated directly using "),HY=n(xBe,"CODE",{});var e$r=s(HY);xlo=r(e$r,"__init__()"),e$r.forEach(t),Rlo=r(xBe," (throws an error)."),xBe.forEach(t),Slo=i(Gs),Dr=n(Gs,"DIV",{class:!0});var Os=s(Dr);m(WM.$$.fragment,Os),Plo=i(Os),UY=n(Os,"P",{});var o$r=s(UY);$lo=r(o$r,"Instantiates one of the model classes of the library (with a pretraining head) from a configuration."),o$r.forEach(t),Ilo=i(Os),Wi=n(Os,"P",{});var ZX=s(Wi);jlo=r(ZX,`Note:
Loading a model from its configuration file does `),JY=n(ZX,"STRONG",{});var r$r=s(JY);Nlo=r(r$r,"not"),r$r.forEach(t),Dlo=r(ZX,` load the model weights. It only affects the
model\u2019s configuration. Use `),YY=n(ZX,"CODE",{});var t$r=s(YY);qlo=r(t$r,"from_pretrained()"),t$r.forEach(t),Glo=r(ZX,"to load the model weights."),ZX.forEach(t),Olo=i(Os),KY=n(Os,"P",{});var a$r=s(KY);Xlo=r(a$r,"Examples:"),a$r.forEach(t),zlo=i(Os),m(QM.$$.fragment,Os),Os.forEach(t),Vlo=i(Gs),xe=n(Gs,"DIV",{class:!0});var Pt=s(xe);m(HM.$$.fragment,Pt),Wlo=i(Pt),ZY=n(Pt,"P",{});var n$r=s(ZY);Qlo=r(n$r,"Instantiate one of the model classes of the library (with a pretraining head) from a pretrained model."),n$r.forEach(t),Hlo=i(Pt),qa=n(Pt,"P",{});var d4=s(qa);Ulo=r(d4,"The model class to instantiate is selected based on the "),eK=n(d4,"CODE",{});var s$r=s(eK);Jlo=r(s$r,"model_type"),s$r.forEach(t),Ylo=r(d4,` property of the config object (either
passed as an argument or loaded from `),oK=n(d4,"CODE",{});var l$r=s(oK);Klo=r(l$r,"pretrained_model_name_or_path"),l$r.forEach(t),Zlo=r(d4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),rK=n(d4,"CODE",{});var i$r=s(rK);eio=r(i$r,"pretrained_model_name_or_path"),i$r.forEach(t),oio=r(d4,":"),d4.forEach(t),rio=i(Pt),x=n(Pt,"UL",{});var S=s(x);e_=n(S,"LI",{});var HFe=s(e_);tK=n(HFe,"STRONG",{});var d$r=s(tK);tio=r(d$r,"albert"),d$r.forEach(t),aio=r(HFe," \u2014 "),HR=n(HFe,"A",{href:!0});var c$r=s(HR);nio=r(c$r,"AlbertForPreTraining"),c$r.forEach(t),sio=r(HFe," (ALBERT model)"),HFe.forEach(t),lio=i(S),o_=n(S,"LI",{});var UFe=s(o_);aK=n(UFe,"STRONG",{});var f$r=s(aK);iio=r(f$r,"bart"),f$r.forEach(t),dio=r(UFe," \u2014 "),UR=n(UFe,"A",{href:!0});var m$r=s(UR);cio=r(m$r,"BartForConditionalGeneration"),m$r.forEach(t),fio=r(UFe," (BART model)"),UFe.forEach(t),mio=i(S),r_=n(S,"LI",{});var JFe=s(r_);nK=n(JFe,"STRONG",{});var g$r=s(nK);gio=r(g$r,"bert"),g$r.forEach(t),hio=r(JFe," \u2014 "),JR=n(JFe,"A",{href:!0});var h$r=s(JR);pio=r(h$r,"BertForPreTraining"),h$r.forEach(t),_io=r(JFe," (BERT model)"),JFe.forEach(t),uio=i(S),t_=n(S,"LI",{});var YFe=s(t_);sK=n(YFe,"STRONG",{});var p$r=s(sK);bio=r(p$r,"big_bird"),p$r.forEach(t),vio=r(YFe," \u2014 "),YR=n(YFe,"A",{href:!0});var _$r=s(YR);Tio=r(_$r,"BigBirdForPreTraining"),_$r.forEach(t),Fio=r(YFe," (BigBird model)"),YFe.forEach(t),Cio=i(S),a_=n(S,"LI",{});var KFe=s(a_);lK=n(KFe,"STRONG",{});var u$r=s(lK);Mio=r(u$r,"camembert"),u$r.forEach(t),Eio=r(KFe," \u2014 "),KR=n(KFe,"A",{href:!0});var b$r=s(KR);yio=r(b$r,"CamembertForMaskedLM"),b$r.forEach(t),wio=r(KFe," (CamemBERT model)"),KFe.forEach(t),Aio=i(S),n_=n(S,"LI",{});var ZFe=s(n_);iK=n(ZFe,"STRONG",{});var v$r=s(iK);Lio=r(v$r,"ctrl"),v$r.forEach(t),Bio=r(ZFe," \u2014 "),ZR=n(ZFe,"A",{href:!0});var T$r=s(ZR);kio=r(T$r,"CTRLLMHeadModel"),T$r.forEach(t),xio=r(ZFe," (CTRL model)"),ZFe.forEach(t),Rio=i(S),s_=n(S,"LI",{});var e9e=s(s_);dK=n(e9e,"STRONG",{});var F$r=s(dK);Sio=r(F$r,"deberta"),F$r.forEach(t),Pio=r(e9e," \u2014 "),eS=n(e9e,"A",{href:!0});var C$r=s(eS);$io=r(C$r,"DebertaForMaskedLM"),C$r.forEach(t),Iio=r(e9e," (DeBERTa model)"),e9e.forEach(t),jio=i(S),l_=n(S,"LI",{});var o9e=s(l_);cK=n(o9e,"STRONG",{});var M$r=s(cK);Nio=r(M$r,"deberta-v2"),M$r.forEach(t),Dio=r(o9e," \u2014 "),oS=n(o9e,"A",{href:!0});var E$r=s(oS);qio=r(E$r,"DebertaV2ForMaskedLM"),E$r.forEach(t),Gio=r(o9e," (DeBERTa-v2 model)"),o9e.forEach(t),Oio=i(S),i_=n(S,"LI",{});var r9e=s(i_);fK=n(r9e,"STRONG",{});var y$r=s(fK);Xio=r(y$r,"distilbert"),y$r.forEach(t),zio=r(r9e," \u2014 "),rS=n(r9e,"A",{href:!0});var w$r=s(rS);Vio=r(w$r,"DistilBertForMaskedLM"),w$r.forEach(t),Wio=r(r9e," (DistilBERT model)"),r9e.forEach(t),Qio=i(S),d_=n(S,"LI",{});var t9e=s(d_);mK=n(t9e,"STRONG",{});var A$r=s(mK);Hio=r(A$r,"electra"),A$r.forEach(t),Uio=r(t9e," \u2014 "),tS=n(t9e,"A",{href:!0});var L$r=s(tS);Jio=r(L$r,"ElectraForPreTraining"),L$r.forEach(t),Yio=r(t9e," (ELECTRA model)"),t9e.forEach(t),Kio=i(S),c_=n(S,"LI",{});var a9e=s(c_);gK=n(a9e,"STRONG",{});var B$r=s(gK);Zio=r(B$r,"flaubert"),B$r.forEach(t),edo=r(a9e," \u2014 "),aS=n(a9e,"A",{href:!0});var k$r=s(aS);odo=r(k$r,"FlaubertWithLMHeadModel"),k$r.forEach(t),rdo=r(a9e," (FlauBERT model)"),a9e.forEach(t),tdo=i(S),f_=n(S,"LI",{});var n9e=s(f_);hK=n(n9e,"STRONG",{});var x$r=s(hK);ado=r(x$r,"fnet"),x$r.forEach(t),ndo=r(n9e," \u2014 "),nS=n(n9e,"A",{href:!0});var R$r=s(nS);sdo=r(R$r,"FNetForPreTraining"),R$r.forEach(t),ldo=r(n9e," (FNet model)"),n9e.forEach(t),ido=i(S),m_=n(S,"LI",{});var s9e=s(m_);pK=n(s9e,"STRONG",{});var S$r=s(pK);ddo=r(S$r,"fsmt"),S$r.forEach(t),cdo=r(s9e," \u2014 "),sS=n(s9e,"A",{href:!0});var P$r=s(sS);fdo=r(P$r,"FSMTForConditionalGeneration"),P$r.forEach(t),mdo=r(s9e," (FairSeq Machine-Translation model)"),s9e.forEach(t),gdo=i(S),g_=n(S,"LI",{});var l9e=s(g_);_K=n(l9e,"STRONG",{});var $$r=s(_K);hdo=r($$r,"funnel"),$$r.forEach(t),pdo=r(l9e," \u2014 "),lS=n(l9e,"A",{href:!0});var I$r=s(lS);_do=r(I$r,"FunnelForPreTraining"),I$r.forEach(t),udo=r(l9e," (Funnel Transformer model)"),l9e.forEach(t),bdo=i(S),h_=n(S,"LI",{});var i9e=s(h_);uK=n(i9e,"STRONG",{});var j$r=s(uK);vdo=r(j$r,"gpt2"),j$r.forEach(t),Tdo=r(i9e," \u2014 "),iS=n(i9e,"A",{href:!0});var N$r=s(iS);Fdo=r(N$r,"GPT2LMHeadModel"),N$r.forEach(t),Cdo=r(i9e," (OpenAI GPT-2 model)"),i9e.forEach(t),Mdo=i(S),p_=n(S,"LI",{});var d9e=s(p_);bK=n(d9e,"STRONG",{});var D$r=s(bK);Edo=r(D$r,"ibert"),D$r.forEach(t),ydo=r(d9e," \u2014 "),dS=n(d9e,"A",{href:!0});var q$r=s(dS);wdo=r(q$r,"IBertForMaskedLM"),q$r.forEach(t),Ado=r(d9e," (I-BERT model)"),d9e.forEach(t),Ldo=i(S),__=n(S,"LI",{});var c9e=s(__);vK=n(c9e,"STRONG",{});var G$r=s(vK);Bdo=r(G$r,"layoutlm"),G$r.forEach(t),kdo=r(c9e," \u2014 "),cS=n(c9e,"A",{href:!0});var O$r=s(cS);xdo=r(O$r,"LayoutLMForMaskedLM"),O$r.forEach(t),Rdo=r(c9e," (LayoutLM model)"),c9e.forEach(t),Sdo=i(S),u_=n(S,"LI",{});var f9e=s(u_);TK=n(f9e,"STRONG",{});var X$r=s(TK);Pdo=r(X$r,"longformer"),X$r.forEach(t),$do=r(f9e," \u2014 "),fS=n(f9e,"A",{href:!0});var z$r=s(fS);Ido=r(z$r,"LongformerForMaskedLM"),z$r.forEach(t),jdo=r(f9e," (Longformer model)"),f9e.forEach(t),Ndo=i(S),b_=n(S,"LI",{});var m9e=s(b_);FK=n(m9e,"STRONG",{});var V$r=s(FK);Ddo=r(V$r,"lxmert"),V$r.forEach(t),qdo=r(m9e," \u2014 "),mS=n(m9e,"A",{href:!0});var W$r=s(mS);Gdo=r(W$r,"LxmertForPreTraining"),W$r.forEach(t),Odo=r(m9e," (LXMERT model)"),m9e.forEach(t),Xdo=i(S),v_=n(S,"LI",{});var g9e=s(v_);CK=n(g9e,"STRONG",{});var Q$r=s(CK);zdo=r(Q$r,"megatron-bert"),Q$r.forEach(t),Vdo=r(g9e," \u2014 "),gS=n(g9e,"A",{href:!0});var H$r=s(gS);Wdo=r(H$r,"MegatronBertForPreTraining"),H$r.forEach(t),Qdo=r(g9e," (MegatronBert model)"),g9e.forEach(t),Hdo=i(S),T_=n(S,"LI",{});var h9e=s(T_);MK=n(h9e,"STRONG",{});var U$r=s(MK);Udo=r(U$r,"mobilebert"),U$r.forEach(t),Jdo=r(h9e," \u2014 "),hS=n(h9e,"A",{href:!0});var J$r=s(hS);Ydo=r(J$r,"MobileBertForPreTraining"),J$r.forEach(t),Kdo=r(h9e," (MobileBERT model)"),h9e.forEach(t),Zdo=i(S),F_=n(S,"LI",{});var p9e=s(F_);EK=n(p9e,"STRONG",{});var Y$r=s(EK);eco=r(Y$r,"mpnet"),Y$r.forEach(t),oco=r(p9e," \u2014 "),pS=n(p9e,"A",{href:!0});var K$r=s(pS);rco=r(K$r,"MPNetForMaskedLM"),K$r.forEach(t),tco=r(p9e," (MPNet model)"),p9e.forEach(t),aco=i(S),C_=n(S,"LI",{});var _9e=s(C_);yK=n(_9e,"STRONG",{});var Z$r=s(yK);nco=r(Z$r,"openai-gpt"),Z$r.forEach(t),sco=r(_9e," \u2014 "),_S=n(_9e,"A",{href:!0});var eIr=s(_S);lco=r(eIr,"OpenAIGPTLMHeadModel"),eIr.forEach(t),ico=r(_9e," (OpenAI GPT model)"),_9e.forEach(t),dco=i(S),M_=n(S,"LI",{});var u9e=s(M_);wK=n(u9e,"STRONG",{});var oIr=s(wK);cco=r(oIr,"retribert"),oIr.forEach(t),fco=r(u9e," \u2014 "),uS=n(u9e,"A",{href:!0});var rIr=s(uS);mco=r(rIr,"RetriBertModel"),rIr.forEach(t),gco=r(u9e," (RetriBERT model)"),u9e.forEach(t),hco=i(S),E_=n(S,"LI",{});var b9e=s(E_);AK=n(b9e,"STRONG",{});var tIr=s(AK);pco=r(tIr,"roberta"),tIr.forEach(t),_co=r(b9e," \u2014 "),bS=n(b9e,"A",{href:!0});var aIr=s(bS);uco=r(aIr,"RobertaForMaskedLM"),aIr.forEach(t),bco=r(b9e," (RoBERTa model)"),b9e.forEach(t),vco=i(S),y_=n(S,"LI",{});var v9e=s(y_);LK=n(v9e,"STRONG",{});var nIr=s(LK);Tco=r(nIr,"squeezebert"),nIr.forEach(t),Fco=r(v9e," \u2014 "),vS=n(v9e,"A",{href:!0});var sIr=s(vS);Cco=r(sIr,"SqueezeBertForMaskedLM"),sIr.forEach(t),Mco=r(v9e," (SqueezeBERT model)"),v9e.forEach(t),Eco=i(S),w_=n(S,"LI",{});var T9e=s(w_);BK=n(T9e,"STRONG",{});var lIr=s(BK);yco=r(lIr,"t5"),lIr.forEach(t),wco=r(T9e," \u2014 "),TS=n(T9e,"A",{href:!0});var iIr=s(TS);Aco=r(iIr,"T5ForConditionalGeneration"),iIr.forEach(t),Lco=r(T9e," (T5 model)"),T9e.forEach(t),Bco=i(S),A_=n(S,"LI",{});var F9e=s(A_);kK=n(F9e,"STRONG",{});var dIr=s(kK);kco=r(dIr,"tapas"),dIr.forEach(t),xco=r(F9e," \u2014 "),FS=n(F9e,"A",{href:!0});var cIr=s(FS);Rco=r(cIr,"TapasForMaskedLM"),cIr.forEach(t),Sco=r(F9e," (TAPAS model)"),F9e.forEach(t),Pco=i(S),L_=n(S,"LI",{});var C9e=s(L_);xK=n(C9e,"STRONG",{});var fIr=s(xK);$co=r(fIr,"transfo-xl"),fIr.forEach(t),Ico=r(C9e," \u2014 "),CS=n(C9e,"A",{href:!0});var mIr=s(CS);jco=r(mIr,"TransfoXLLMHeadModel"),mIr.forEach(t),Nco=r(C9e," (Transformer-XL model)"),C9e.forEach(t),Dco=i(S),B_=n(S,"LI",{});var M9e=s(B_);RK=n(M9e,"STRONG",{});var gIr=s(RK);qco=r(gIr,"unispeech"),gIr.forEach(t),Gco=r(M9e," \u2014 "),MS=n(M9e,"A",{href:!0});var hIr=s(MS);Oco=r(hIr,"UniSpeechForPreTraining"),hIr.forEach(t),Xco=r(M9e," (UniSpeech model)"),M9e.forEach(t),zco=i(S),k_=n(S,"LI",{});var E9e=s(k_);SK=n(E9e,"STRONG",{});var pIr=s(SK);Vco=r(pIr,"unispeech-sat"),pIr.forEach(t),Wco=r(E9e," \u2014 "),ES=n(E9e,"A",{href:!0});var _Ir=s(ES);Qco=r(_Ir,"UniSpeechSatForPreTraining"),_Ir.forEach(t),Hco=r(E9e," (UniSpeechSat model)"),E9e.forEach(t),Uco=i(S),x_=n(S,"LI",{});var y9e=s(x_);PK=n(y9e,"STRONG",{});var uIr=s(PK);Jco=r(uIr,"visual_bert"),uIr.forEach(t),Yco=r(y9e," \u2014 "),yS=n(y9e,"A",{href:!0});var bIr=s(yS);Kco=r(bIr,"VisualBertForPreTraining"),bIr.forEach(t),Zco=r(y9e," (VisualBert model)"),y9e.forEach(t),efo=i(S),R_=n(S,"LI",{});var w9e=s(R_);$K=n(w9e,"STRONG",{});var vIr=s($K);ofo=r(vIr,"vit_mae"),vIr.forEach(t),rfo=r(w9e," \u2014 "),wS=n(w9e,"A",{href:!0});var TIr=s(wS);tfo=r(TIr,"ViTMAEForPreTraining"),TIr.forEach(t),afo=r(w9e," (ViTMAE model)"),w9e.forEach(t),nfo=i(S),S_=n(S,"LI",{});var A9e=s(S_);IK=n(A9e,"STRONG",{});var FIr=s(IK);sfo=r(FIr,"wav2vec2"),FIr.forEach(t),lfo=r(A9e," \u2014 "),AS=n(A9e,"A",{href:!0});var CIr=s(AS);ifo=r(CIr,"Wav2Vec2ForPreTraining"),CIr.forEach(t),dfo=r(A9e," (Wav2Vec2 model)"),A9e.forEach(t),cfo=i(S),P_=n(S,"LI",{});var L9e=s(P_);jK=n(L9e,"STRONG",{});var MIr=s(jK);ffo=r(MIr,"xlm"),MIr.forEach(t),mfo=r(L9e," \u2014 "),LS=n(L9e,"A",{href:!0});var EIr=s(LS);gfo=r(EIr,"XLMWithLMHeadModel"),EIr.forEach(t),hfo=r(L9e," (XLM model)"),L9e.forEach(t),pfo=i(S),$_=n(S,"LI",{});var B9e=s($_);NK=n(B9e,"STRONG",{});var yIr=s(NK);_fo=r(yIr,"xlm-roberta"),yIr.forEach(t),ufo=r(B9e," \u2014 "),BS=n(B9e,"A",{href:!0});var wIr=s(BS);bfo=r(wIr,"XLMRobertaForMaskedLM"),wIr.forEach(t),vfo=r(B9e," (XLM-RoBERTa model)"),B9e.forEach(t),Tfo=i(S),I_=n(S,"LI",{});var k9e=s(I_);DK=n(k9e,"STRONG",{});var AIr=s(DK);Ffo=r(AIr,"xlm-roberta-xl"),AIr.forEach(t),Cfo=r(k9e," \u2014 "),kS=n(k9e,"A",{href:!0});var LIr=s(kS);Mfo=r(LIr,"XLMRobertaXLForMaskedLM"),LIr.forEach(t),Efo=r(k9e," (XLM-RoBERTa-XL model)"),k9e.forEach(t),yfo=i(S),j_=n(S,"LI",{});var x9e=s(j_);qK=n(x9e,"STRONG",{});var BIr=s(qK);wfo=r(BIr,"xlnet"),BIr.forEach(t),Afo=r(x9e," \u2014 "),xS=n(x9e,"A",{href:!0});var kIr=s(xS);Lfo=r(kIr,"XLNetLMHeadModel"),kIr.forEach(t),Bfo=r(x9e," (XLNet model)"),x9e.forEach(t),S.forEach(t),kfo=i(Pt),N_=n(Pt,"P",{});var R9e=s(N_);xfo=r(R9e,"The model is set in evaluation mode by default using "),GK=n(R9e,"CODE",{});var xIr=s(GK);Rfo=r(xIr,"model.eval()"),xIr.forEach(t),Sfo=r(R9e,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),OK=n(R9e,"CODE",{});var RIr=s(OK);Pfo=r(RIr,"model.train()"),RIr.forEach(t),R9e.forEach(t),$fo=i(Pt),XK=n(Pt,"P",{});var SIr=s(XK);Ifo=r(SIr,"Examples:"),SIr.forEach(t),jfo=i(Pt),m(UM.$$.fragment,Pt),Pt.forEach(t),Gs.forEach(t),yLe=i(d),Qi=n(d,"H2",{class:!0});var RBe=s(Qi);D_=n(RBe,"A",{id:!0,class:!0,href:!0});var PIr=s(D_);zK=n(PIr,"SPAN",{});var $Ir=s(zK);m(JM.$$.fragment,$Ir),$Ir.forEach(t),PIr.forEach(t),Nfo=i(RBe),VK=n(RBe,"SPAN",{});var IIr=s(VK);Dfo=r(IIr,"AutoModelForCausalLM"),IIr.forEach(t),RBe.forEach(t),wLe=i(d),Qo=n(d,"DIV",{class:!0});var Xs=s(Qo);m(YM.$$.fragment,Xs),qfo=i(Xs),Hi=n(Xs,"P",{});var ez=s(Hi);Gfo=r(ez,`This is a generic model class that will be instantiated as one of the model classes of the library (with a causal language modeling head) when created
with the `),WK=n(ez,"CODE",{});var jIr=s(WK);Ofo=r(jIr,"from_pretrained()"),jIr.forEach(t),Xfo=r(ez,"class method or the "),QK=n(ez,"CODE",{});var NIr=s(QK);zfo=r(NIr,"from_config()"),NIr.forEach(t),Vfo=r(ez,`class
method.`),ez.forEach(t),Wfo=i(Xs),KM=n(Xs,"P",{});var SBe=s(KM);Qfo=r(SBe,"This class cannot be instantiated directly using "),HK=n(SBe,"CODE",{});var DIr=s(HK);Hfo=r(DIr,"__init__()"),DIr.forEach(t),Ufo=r(SBe," (throws an error)."),SBe.forEach(t),Jfo=i(Xs),qr=n(Xs,"DIV",{class:!0});var zs=s(qr);m(ZM.$$.fragment,zs),Yfo=i(zs),UK=n(zs,"P",{});var qIr=s(UK);Kfo=r(qIr,"Instantiates one of the model classes of the library (with a causal language modeling head) from a configuration."),qIr.forEach(t),Zfo=i(zs),Ui=n(zs,"P",{});var oz=s(Ui);emo=r(oz,`Note:
Loading a model from its configuration file does `),JK=n(oz,"STRONG",{});var GIr=s(JK);omo=r(GIr,"not"),GIr.forEach(t),rmo=r(oz,` load the model weights. It only affects the
model\u2019s configuration. Use `),YK=n(oz,"CODE",{});var OIr=s(YK);tmo=r(OIr,"from_pretrained()"),OIr.forEach(t),amo=r(oz,"to load the model weights."),oz.forEach(t),nmo=i(zs),KK=n(zs,"P",{});var XIr=s(KK);smo=r(XIr,"Examples:"),XIr.forEach(t),lmo=i(zs),m(eE.$$.fragment,zs),zs.forEach(t),imo=i(Xs),Re=n(Xs,"DIV",{class:!0});var $t=s(Re);m(oE.$$.fragment,$t),dmo=i($t),ZK=n($t,"P",{});var zIr=s(ZK);cmo=r(zIr,"Instantiate one of the model classes of the library (with a causal language modeling head) from a pretrained model."),zIr.forEach(t),fmo=i($t),Ga=n($t,"P",{});var c4=s(Ga);mmo=r(c4,"The model class to instantiate is selected based on the "),eZ=n(c4,"CODE",{});var VIr=s(eZ);gmo=r(VIr,"model_type"),VIr.forEach(t),hmo=r(c4,` property of the config object (either
passed as an argument or loaded from `),oZ=n(c4,"CODE",{});var WIr=s(oZ);pmo=r(WIr,"pretrained_model_name_or_path"),WIr.forEach(t),_mo=r(c4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),rZ=n(c4,"CODE",{});var QIr=s(rZ);umo=r(QIr,"pretrained_model_name_or_path"),QIr.forEach(t),bmo=r(c4,":"),c4.forEach(t),vmo=i($t),$=n($t,"UL",{});var j=s($);q_=n(j,"LI",{});var S9e=s(q_);tZ=n(S9e,"STRONG",{});var HIr=s(tZ);Tmo=r(HIr,"bart"),HIr.forEach(t),Fmo=r(S9e," \u2014 "),RS=n(S9e,"A",{href:!0});var UIr=s(RS);Cmo=r(UIr,"BartForCausalLM"),UIr.forEach(t),Mmo=r(S9e," (BART model)"),S9e.forEach(t),Emo=i(j),G_=n(j,"LI",{});var P9e=s(G_);aZ=n(P9e,"STRONG",{});var JIr=s(aZ);ymo=r(JIr,"bert"),JIr.forEach(t),wmo=r(P9e," \u2014 "),SS=n(P9e,"A",{href:!0});var YIr=s(SS);Amo=r(YIr,"BertLMHeadModel"),YIr.forEach(t),Lmo=r(P9e," (BERT model)"),P9e.forEach(t),Bmo=i(j),O_=n(j,"LI",{});var $9e=s(O_);nZ=n($9e,"STRONG",{});var KIr=s(nZ);kmo=r(KIr,"bert-generation"),KIr.forEach(t),xmo=r($9e," \u2014 "),PS=n($9e,"A",{href:!0});var ZIr=s(PS);Rmo=r(ZIr,"BertGenerationDecoder"),ZIr.forEach(t),Smo=r($9e," (Bert Generation model)"),$9e.forEach(t),Pmo=i(j),X_=n(j,"LI",{});var I9e=s(X_);sZ=n(I9e,"STRONG",{});var ejr=s(sZ);$mo=r(ejr,"big_bird"),ejr.forEach(t),Imo=r(I9e," \u2014 "),$S=n(I9e,"A",{href:!0});var ojr=s($S);jmo=r(ojr,"BigBirdForCausalLM"),ojr.forEach(t),Nmo=r(I9e," (BigBird model)"),I9e.forEach(t),Dmo=i(j),z_=n(j,"LI",{});var j9e=s(z_);lZ=n(j9e,"STRONG",{});var rjr=s(lZ);qmo=r(rjr,"bigbird_pegasus"),rjr.forEach(t),Gmo=r(j9e," \u2014 "),IS=n(j9e,"A",{href:!0});var tjr=s(IS);Omo=r(tjr,"BigBirdPegasusForCausalLM"),tjr.forEach(t),Xmo=r(j9e," (BigBirdPegasus model)"),j9e.forEach(t),zmo=i(j),V_=n(j,"LI",{});var N9e=s(V_);iZ=n(N9e,"STRONG",{});var ajr=s(iZ);Vmo=r(ajr,"blenderbot"),ajr.forEach(t),Wmo=r(N9e," \u2014 "),jS=n(N9e,"A",{href:!0});var njr=s(jS);Qmo=r(njr,"BlenderbotForCausalLM"),njr.forEach(t),Hmo=r(N9e," (Blenderbot model)"),N9e.forEach(t),Umo=i(j),W_=n(j,"LI",{});var D9e=s(W_);dZ=n(D9e,"STRONG",{});var sjr=s(dZ);Jmo=r(sjr,"blenderbot-small"),sjr.forEach(t),Ymo=r(D9e," \u2014 "),NS=n(D9e,"A",{href:!0});var ljr=s(NS);Kmo=r(ljr,"BlenderbotSmallForCausalLM"),ljr.forEach(t),Zmo=r(D9e," (BlenderbotSmall model)"),D9e.forEach(t),ego=i(j),Q_=n(j,"LI",{});var q9e=s(Q_);cZ=n(q9e,"STRONG",{});var ijr=s(cZ);ogo=r(ijr,"camembert"),ijr.forEach(t),rgo=r(q9e," \u2014 "),DS=n(q9e,"A",{href:!0});var djr=s(DS);tgo=r(djr,"CamembertForCausalLM"),djr.forEach(t),ago=r(q9e," (CamemBERT model)"),q9e.forEach(t),ngo=i(j),H_=n(j,"LI",{});var G9e=s(H_);fZ=n(G9e,"STRONG",{});var cjr=s(fZ);sgo=r(cjr,"ctrl"),cjr.forEach(t),lgo=r(G9e," \u2014 "),qS=n(G9e,"A",{href:!0});var fjr=s(qS);igo=r(fjr,"CTRLLMHeadModel"),fjr.forEach(t),dgo=r(G9e," (CTRL model)"),G9e.forEach(t),cgo=i(j),U_=n(j,"LI",{});var O9e=s(U_);mZ=n(O9e,"STRONG",{});var mjr=s(mZ);fgo=r(mjr,"electra"),mjr.forEach(t),mgo=r(O9e," \u2014 "),GS=n(O9e,"A",{href:!0});var gjr=s(GS);ggo=r(gjr,"ElectraForCausalLM"),gjr.forEach(t),hgo=r(O9e," (ELECTRA model)"),O9e.forEach(t),pgo=i(j),J_=n(j,"LI",{});var X9e=s(J_);gZ=n(X9e,"STRONG",{});var hjr=s(gZ);_go=r(hjr,"gpt2"),hjr.forEach(t),ugo=r(X9e," \u2014 "),OS=n(X9e,"A",{href:!0});var pjr=s(OS);bgo=r(pjr,"GPT2LMHeadModel"),pjr.forEach(t),vgo=r(X9e," (OpenAI GPT-2 model)"),X9e.forEach(t),Tgo=i(j),Y_=n(j,"LI",{});var z9e=s(Y_);hZ=n(z9e,"STRONG",{});var _jr=s(hZ);Fgo=r(_jr,"gpt_neo"),_jr.forEach(t),Cgo=r(z9e," \u2014 "),XS=n(z9e,"A",{href:!0});var ujr=s(XS);Mgo=r(ujr,"GPTNeoForCausalLM"),ujr.forEach(t),Ego=r(z9e," (GPT Neo model)"),z9e.forEach(t),ygo=i(j),K_=n(j,"LI",{});var V9e=s(K_);pZ=n(V9e,"STRONG",{});var bjr=s(pZ);wgo=r(bjr,"gptj"),bjr.forEach(t),Ago=r(V9e," \u2014 "),zS=n(V9e,"A",{href:!0});var vjr=s(zS);Lgo=r(vjr,"GPTJForCausalLM"),vjr.forEach(t),Bgo=r(V9e," (GPT-J model)"),V9e.forEach(t),kgo=i(j),Z_=n(j,"LI",{});var W9e=s(Z_);_Z=n(W9e,"STRONG",{});var Tjr=s(_Z);xgo=r(Tjr,"marian"),Tjr.forEach(t),Rgo=r(W9e," \u2014 "),VS=n(W9e,"A",{href:!0});var Fjr=s(VS);Sgo=r(Fjr,"MarianForCausalLM"),Fjr.forEach(t),Pgo=r(W9e," (Marian model)"),W9e.forEach(t),$go=i(j),eu=n(j,"LI",{});var Q9e=s(eu);uZ=n(Q9e,"STRONG",{});var Cjr=s(uZ);Igo=r(Cjr,"mbart"),Cjr.forEach(t),jgo=r(Q9e," \u2014 "),WS=n(Q9e,"A",{href:!0});var Mjr=s(WS);Ngo=r(Mjr,"MBartForCausalLM"),Mjr.forEach(t),Dgo=r(Q9e," (mBART model)"),Q9e.forEach(t),qgo=i(j),ou=n(j,"LI",{});var H9e=s(ou);bZ=n(H9e,"STRONG",{});var Ejr=s(bZ);Ggo=r(Ejr,"megatron-bert"),Ejr.forEach(t),Ogo=r(H9e," \u2014 "),QS=n(H9e,"A",{href:!0});var yjr=s(QS);Xgo=r(yjr,"MegatronBertForCausalLM"),yjr.forEach(t),zgo=r(H9e," (MegatronBert model)"),H9e.forEach(t),Vgo=i(j),ru=n(j,"LI",{});var U9e=s(ru);vZ=n(U9e,"STRONG",{});var wjr=s(vZ);Wgo=r(wjr,"openai-gpt"),wjr.forEach(t),Qgo=r(U9e," \u2014 "),HS=n(U9e,"A",{href:!0});var Ajr=s(HS);Hgo=r(Ajr,"OpenAIGPTLMHeadModel"),Ajr.forEach(t),Ugo=r(U9e," (OpenAI GPT model)"),U9e.forEach(t),Jgo=i(j),tu=n(j,"LI",{});var J9e=s(tu);TZ=n(J9e,"STRONG",{});var Ljr=s(TZ);Ygo=r(Ljr,"pegasus"),Ljr.forEach(t),Kgo=r(J9e," \u2014 "),US=n(J9e,"A",{href:!0});var Bjr=s(US);Zgo=r(Bjr,"PegasusForCausalLM"),Bjr.forEach(t),eho=r(J9e," (Pegasus model)"),J9e.forEach(t),oho=i(j),au=n(j,"LI",{});var Y9e=s(au);FZ=n(Y9e,"STRONG",{});var kjr=s(FZ);rho=r(kjr,"plbart"),kjr.forEach(t),tho=r(Y9e," \u2014 "),JS=n(Y9e,"A",{href:!0});var xjr=s(JS);aho=r(xjr,"PLBartForCausalLM"),xjr.forEach(t),nho=r(Y9e," (PLBart model)"),Y9e.forEach(t),sho=i(j),nu=n(j,"LI",{});var K9e=s(nu);CZ=n(K9e,"STRONG",{});var Rjr=s(CZ);lho=r(Rjr,"prophetnet"),Rjr.forEach(t),iho=r(K9e," \u2014 "),YS=n(K9e,"A",{href:!0});var Sjr=s(YS);dho=r(Sjr,"ProphetNetForCausalLM"),Sjr.forEach(t),cho=r(K9e," (ProphetNet model)"),K9e.forEach(t),fho=i(j),su=n(j,"LI",{});var Z9e=s(su);MZ=n(Z9e,"STRONG",{});var Pjr=s(MZ);mho=r(Pjr,"qdqbert"),Pjr.forEach(t),gho=r(Z9e," \u2014 "),KS=n(Z9e,"A",{href:!0});var $jr=s(KS);hho=r($jr,"QDQBertLMHeadModel"),$jr.forEach(t),pho=r(Z9e," (QDQBert model)"),Z9e.forEach(t),_ho=i(j),lu=n(j,"LI",{});var eCe=s(lu);EZ=n(eCe,"STRONG",{});var Ijr=s(EZ);uho=r(Ijr,"reformer"),Ijr.forEach(t),bho=r(eCe," \u2014 "),ZS=n(eCe,"A",{href:!0});var jjr=s(ZS);vho=r(jjr,"ReformerModelWithLMHead"),jjr.forEach(t),Tho=r(eCe," (Reformer model)"),eCe.forEach(t),Fho=i(j),iu=n(j,"LI",{});var oCe=s(iu);yZ=n(oCe,"STRONG",{});var Njr=s(yZ);Cho=r(Njr,"rembert"),Njr.forEach(t),Mho=r(oCe," \u2014 "),eP=n(oCe,"A",{href:!0});var Djr=s(eP);Eho=r(Djr,"RemBertForCausalLM"),Djr.forEach(t),yho=r(oCe," (RemBERT model)"),oCe.forEach(t),who=i(j),du=n(j,"LI",{});var rCe=s(du);wZ=n(rCe,"STRONG",{});var qjr=s(wZ);Aho=r(qjr,"roberta"),qjr.forEach(t),Lho=r(rCe," \u2014 "),oP=n(rCe,"A",{href:!0});var Gjr=s(oP);Bho=r(Gjr,"RobertaForCausalLM"),Gjr.forEach(t),kho=r(rCe," (RoBERTa model)"),rCe.forEach(t),xho=i(j),cu=n(j,"LI",{});var tCe=s(cu);AZ=n(tCe,"STRONG",{});var Ojr=s(AZ);Rho=r(Ojr,"roformer"),Ojr.forEach(t),Sho=r(tCe," \u2014 "),rP=n(tCe,"A",{href:!0});var Xjr=s(rP);Pho=r(Xjr,"RoFormerForCausalLM"),Xjr.forEach(t),$ho=r(tCe," (RoFormer model)"),tCe.forEach(t),Iho=i(j),fu=n(j,"LI",{});var aCe=s(fu);LZ=n(aCe,"STRONG",{});var zjr=s(LZ);jho=r(zjr,"speech_to_text_2"),zjr.forEach(t),Nho=r(aCe," \u2014 "),tP=n(aCe,"A",{href:!0});var Vjr=s(tP);Dho=r(Vjr,"Speech2Text2ForCausalLM"),Vjr.forEach(t),qho=r(aCe," (Speech2Text2 model)"),aCe.forEach(t),Gho=i(j),mu=n(j,"LI",{});var nCe=s(mu);BZ=n(nCe,"STRONG",{});var Wjr=s(BZ);Oho=r(Wjr,"transfo-xl"),Wjr.forEach(t),Xho=r(nCe," \u2014 "),aP=n(nCe,"A",{href:!0});var Qjr=s(aP);zho=r(Qjr,"TransfoXLLMHeadModel"),Qjr.forEach(t),Vho=r(nCe," (Transformer-XL model)"),nCe.forEach(t),Who=i(j),gu=n(j,"LI",{});var sCe=s(gu);kZ=n(sCe,"STRONG",{});var Hjr=s(kZ);Qho=r(Hjr,"trocr"),Hjr.forEach(t),Hho=r(sCe," \u2014 "),nP=n(sCe,"A",{href:!0});var Ujr=s(nP);Uho=r(Ujr,"TrOCRForCausalLM"),Ujr.forEach(t),Jho=r(sCe," (TrOCR model)"),sCe.forEach(t),Yho=i(j),hu=n(j,"LI",{});var lCe=s(hu);xZ=n(lCe,"STRONG",{});var Jjr=s(xZ);Kho=r(Jjr,"xglm"),Jjr.forEach(t),Zho=r(lCe," \u2014 "),sP=n(lCe,"A",{href:!0});var Yjr=s(sP);epo=r(Yjr,"XGLMForCausalLM"),Yjr.forEach(t),opo=r(lCe," (XGLM model)"),lCe.forEach(t),rpo=i(j),pu=n(j,"LI",{});var iCe=s(pu);RZ=n(iCe,"STRONG",{});var Kjr=s(RZ);tpo=r(Kjr,"xlm"),Kjr.forEach(t),apo=r(iCe," \u2014 "),lP=n(iCe,"A",{href:!0});var Zjr=s(lP);npo=r(Zjr,"XLMWithLMHeadModel"),Zjr.forEach(t),spo=r(iCe," (XLM model)"),iCe.forEach(t),lpo=i(j),_u=n(j,"LI",{});var dCe=s(_u);SZ=n(dCe,"STRONG",{});var eNr=s(SZ);ipo=r(eNr,"xlm-prophetnet"),eNr.forEach(t),dpo=r(dCe," \u2014 "),iP=n(dCe,"A",{href:!0});var oNr=s(iP);cpo=r(oNr,"XLMProphetNetForCausalLM"),oNr.forEach(t),fpo=r(dCe," (XLMProphetNet model)"),dCe.forEach(t),mpo=i(j),uu=n(j,"LI",{});var cCe=s(uu);PZ=n(cCe,"STRONG",{});var rNr=s(PZ);gpo=r(rNr,"xlm-roberta"),rNr.forEach(t),hpo=r(cCe," \u2014 "),dP=n(cCe,"A",{href:!0});var tNr=s(dP);ppo=r(tNr,"XLMRobertaForCausalLM"),tNr.forEach(t),_po=r(cCe," (XLM-RoBERTa model)"),cCe.forEach(t),upo=i(j),bu=n(j,"LI",{});var fCe=s(bu);$Z=n(fCe,"STRONG",{});var aNr=s($Z);bpo=r(aNr,"xlm-roberta-xl"),aNr.forEach(t),vpo=r(fCe," \u2014 "),cP=n(fCe,"A",{href:!0});var nNr=s(cP);Tpo=r(nNr,"XLMRobertaXLForCausalLM"),nNr.forEach(t),Fpo=r(fCe," (XLM-RoBERTa-XL model)"),fCe.forEach(t),Cpo=i(j),vu=n(j,"LI",{});var mCe=s(vu);IZ=n(mCe,"STRONG",{});var sNr=s(IZ);Mpo=r(sNr,"xlnet"),sNr.forEach(t),Epo=r(mCe," \u2014 "),fP=n(mCe,"A",{href:!0});var lNr=s(fP);ypo=r(lNr,"XLNetLMHeadModel"),lNr.forEach(t),wpo=r(mCe," (XLNet model)"),mCe.forEach(t),j.forEach(t),Apo=i($t),Tu=n($t,"P",{});var gCe=s(Tu);Lpo=r(gCe,"The model is set in evaluation mode by default using "),jZ=n(gCe,"CODE",{});var iNr=s(jZ);Bpo=r(iNr,"model.eval()"),iNr.forEach(t),kpo=r(gCe,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),NZ=n(gCe,"CODE",{});var dNr=s(NZ);xpo=r(dNr,"model.train()"),dNr.forEach(t),gCe.forEach(t),Rpo=i($t),DZ=n($t,"P",{});var cNr=s(DZ);Spo=r(cNr,"Examples:"),cNr.forEach(t),Ppo=i($t),m(rE.$$.fragment,$t),$t.forEach(t),Xs.forEach(t),ALe=i(d),Ji=n(d,"H2",{class:!0});var PBe=s(Ji);Fu=n(PBe,"A",{id:!0,class:!0,href:!0});var fNr=s(Fu);qZ=n(fNr,"SPAN",{});var mNr=s(qZ);m(tE.$$.fragment,mNr),mNr.forEach(t),fNr.forEach(t),$po=i(PBe),GZ=n(PBe,"SPAN",{});var gNr=s(GZ);Ipo=r(gNr,"AutoModelForMaskedLM"),gNr.forEach(t),PBe.forEach(t),LLe=i(d),Ho=n(d,"DIV",{class:!0});var Vs=s(Ho);m(aE.$$.fragment,Vs),jpo=i(Vs),Yi=n(Vs,"P",{});var rz=s(Yi);Npo=r(rz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a masked language modeling head) when created
with the `),OZ=n(rz,"CODE",{});var hNr=s(OZ);Dpo=r(hNr,"from_pretrained()"),hNr.forEach(t),qpo=r(rz,"class method or the "),XZ=n(rz,"CODE",{});var pNr=s(XZ);Gpo=r(pNr,"from_config()"),pNr.forEach(t),Opo=r(rz,`class
method.`),rz.forEach(t),Xpo=i(Vs),nE=n(Vs,"P",{});var $Be=s(nE);zpo=r($Be,"This class cannot be instantiated directly using "),zZ=n($Be,"CODE",{});var _Nr=s(zZ);Vpo=r(_Nr,"__init__()"),_Nr.forEach(t),Wpo=r($Be," (throws an error)."),$Be.forEach(t),Qpo=i(Vs),Gr=n(Vs,"DIV",{class:!0});var Ws=s(Gr);m(sE.$$.fragment,Ws),Hpo=i(Ws),VZ=n(Ws,"P",{});var uNr=s(VZ);Upo=r(uNr,"Instantiates one of the model classes of the library (with a masked language modeling head) from a configuration."),uNr.forEach(t),Jpo=i(Ws),Ki=n(Ws,"P",{});var tz=s(Ki);Ypo=r(tz,`Note:
Loading a model from its configuration file does `),WZ=n(tz,"STRONG",{});var bNr=s(WZ);Kpo=r(bNr,"not"),bNr.forEach(t),Zpo=r(tz,` load the model weights. It only affects the
model\u2019s configuration. Use `),QZ=n(tz,"CODE",{});var vNr=s(QZ);e_o=r(vNr,"from_pretrained()"),vNr.forEach(t),o_o=r(tz,"to load the model weights."),tz.forEach(t),r_o=i(Ws),HZ=n(Ws,"P",{});var TNr=s(HZ);t_o=r(TNr,"Examples:"),TNr.forEach(t),a_o=i(Ws),m(lE.$$.fragment,Ws),Ws.forEach(t),n_o=i(Vs),Se=n(Vs,"DIV",{class:!0});var It=s(Se);m(iE.$$.fragment,It),s_o=i(It),UZ=n(It,"P",{});var FNr=s(UZ);l_o=r(FNr,"Instantiate one of the model classes of the library (with a masked language modeling head) from a pretrained model."),FNr.forEach(t),i_o=i(It),Oa=n(It,"P",{});var f4=s(Oa);d_o=r(f4,"The model class to instantiate is selected based on the "),JZ=n(f4,"CODE",{});var CNr=s(JZ);c_o=r(CNr,"model_type"),CNr.forEach(t),f_o=r(f4,` property of the config object (either
passed as an argument or loaded from `),YZ=n(f4,"CODE",{});var MNr=s(YZ);m_o=r(MNr,"pretrained_model_name_or_path"),MNr.forEach(t),g_o=r(f4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),KZ=n(f4,"CODE",{});var ENr=s(KZ);h_o=r(ENr,"pretrained_model_name_or_path"),ENr.forEach(t),p_o=r(f4,":"),f4.forEach(t),__o=i(It),I=n(It,"UL",{});var N=s(I);Cu=n(N,"LI",{});var hCe=s(Cu);ZZ=n(hCe,"STRONG",{});var yNr=s(ZZ);u_o=r(yNr,"albert"),yNr.forEach(t),b_o=r(hCe," \u2014 "),mP=n(hCe,"A",{href:!0});var wNr=s(mP);v_o=r(wNr,"AlbertForMaskedLM"),wNr.forEach(t),T_o=r(hCe," (ALBERT model)"),hCe.forEach(t),F_o=i(N),Mu=n(N,"LI",{});var pCe=s(Mu);eee=n(pCe,"STRONG",{});var ANr=s(eee);C_o=r(ANr,"bart"),ANr.forEach(t),M_o=r(pCe," \u2014 "),gP=n(pCe,"A",{href:!0});var LNr=s(gP);E_o=r(LNr,"BartForConditionalGeneration"),LNr.forEach(t),y_o=r(pCe," (BART model)"),pCe.forEach(t),w_o=i(N),Eu=n(N,"LI",{});var _Ce=s(Eu);oee=n(_Ce,"STRONG",{});var BNr=s(oee);A_o=r(BNr,"bert"),BNr.forEach(t),L_o=r(_Ce," \u2014 "),hP=n(_Ce,"A",{href:!0});var kNr=s(hP);B_o=r(kNr,"BertForMaskedLM"),kNr.forEach(t),k_o=r(_Ce," (BERT model)"),_Ce.forEach(t),x_o=i(N),yu=n(N,"LI",{});var uCe=s(yu);ree=n(uCe,"STRONG",{});var xNr=s(ree);R_o=r(xNr,"big_bird"),xNr.forEach(t),S_o=r(uCe," \u2014 "),pP=n(uCe,"A",{href:!0});var RNr=s(pP);P_o=r(RNr,"BigBirdForMaskedLM"),RNr.forEach(t),$_o=r(uCe," (BigBird model)"),uCe.forEach(t),I_o=i(N),wu=n(N,"LI",{});var bCe=s(wu);tee=n(bCe,"STRONG",{});var SNr=s(tee);j_o=r(SNr,"camembert"),SNr.forEach(t),N_o=r(bCe," \u2014 "),_P=n(bCe,"A",{href:!0});var PNr=s(_P);D_o=r(PNr,"CamembertForMaskedLM"),PNr.forEach(t),q_o=r(bCe," (CamemBERT model)"),bCe.forEach(t),G_o=i(N),Au=n(N,"LI",{});var vCe=s(Au);aee=n(vCe,"STRONG",{});var $Nr=s(aee);O_o=r($Nr,"convbert"),$Nr.forEach(t),X_o=r(vCe," \u2014 "),uP=n(vCe,"A",{href:!0});var INr=s(uP);z_o=r(INr,"ConvBertForMaskedLM"),INr.forEach(t),V_o=r(vCe," (ConvBERT model)"),vCe.forEach(t),W_o=i(N),Lu=n(N,"LI",{});var TCe=s(Lu);nee=n(TCe,"STRONG",{});var jNr=s(nee);Q_o=r(jNr,"deberta"),jNr.forEach(t),H_o=r(TCe," \u2014 "),bP=n(TCe,"A",{href:!0});var NNr=s(bP);U_o=r(NNr,"DebertaForMaskedLM"),NNr.forEach(t),J_o=r(TCe," (DeBERTa model)"),TCe.forEach(t),Y_o=i(N),Bu=n(N,"LI",{});var FCe=s(Bu);see=n(FCe,"STRONG",{});var DNr=s(see);K_o=r(DNr,"deberta-v2"),DNr.forEach(t),Z_o=r(FCe," \u2014 "),vP=n(FCe,"A",{href:!0});var qNr=s(vP);euo=r(qNr,"DebertaV2ForMaskedLM"),qNr.forEach(t),ouo=r(FCe," (DeBERTa-v2 model)"),FCe.forEach(t),ruo=i(N),ku=n(N,"LI",{});var CCe=s(ku);lee=n(CCe,"STRONG",{});var GNr=s(lee);tuo=r(GNr,"distilbert"),GNr.forEach(t),auo=r(CCe," \u2014 "),TP=n(CCe,"A",{href:!0});var ONr=s(TP);nuo=r(ONr,"DistilBertForMaskedLM"),ONr.forEach(t),suo=r(CCe," (DistilBERT model)"),CCe.forEach(t),luo=i(N),xu=n(N,"LI",{});var MCe=s(xu);iee=n(MCe,"STRONG",{});var XNr=s(iee);iuo=r(XNr,"electra"),XNr.forEach(t),duo=r(MCe," \u2014 "),FP=n(MCe,"A",{href:!0});var zNr=s(FP);cuo=r(zNr,"ElectraForMaskedLM"),zNr.forEach(t),fuo=r(MCe," (ELECTRA model)"),MCe.forEach(t),muo=i(N),Ru=n(N,"LI",{});var ECe=s(Ru);dee=n(ECe,"STRONG",{});var VNr=s(dee);guo=r(VNr,"flaubert"),VNr.forEach(t),huo=r(ECe," \u2014 "),CP=n(ECe,"A",{href:!0});var WNr=s(CP);puo=r(WNr,"FlaubertWithLMHeadModel"),WNr.forEach(t),_uo=r(ECe," (FlauBERT model)"),ECe.forEach(t),uuo=i(N),Su=n(N,"LI",{});var yCe=s(Su);cee=n(yCe,"STRONG",{});var QNr=s(cee);buo=r(QNr,"fnet"),QNr.forEach(t),vuo=r(yCe," \u2014 "),MP=n(yCe,"A",{href:!0});var HNr=s(MP);Tuo=r(HNr,"FNetForMaskedLM"),HNr.forEach(t),Fuo=r(yCe," (FNet model)"),yCe.forEach(t),Cuo=i(N),Pu=n(N,"LI",{});var wCe=s(Pu);fee=n(wCe,"STRONG",{});var UNr=s(fee);Muo=r(UNr,"funnel"),UNr.forEach(t),Euo=r(wCe," \u2014 "),EP=n(wCe,"A",{href:!0});var JNr=s(EP);yuo=r(JNr,"FunnelForMaskedLM"),JNr.forEach(t),wuo=r(wCe," (Funnel Transformer model)"),wCe.forEach(t),Auo=i(N),$u=n(N,"LI",{});var ACe=s($u);mee=n(ACe,"STRONG",{});var YNr=s(mee);Luo=r(YNr,"ibert"),YNr.forEach(t),Buo=r(ACe," \u2014 "),yP=n(ACe,"A",{href:!0});var KNr=s(yP);kuo=r(KNr,"IBertForMaskedLM"),KNr.forEach(t),xuo=r(ACe," (I-BERT model)"),ACe.forEach(t),Ruo=i(N),Iu=n(N,"LI",{});var LCe=s(Iu);gee=n(LCe,"STRONG",{});var ZNr=s(gee);Suo=r(ZNr,"layoutlm"),ZNr.forEach(t),Puo=r(LCe," \u2014 "),wP=n(LCe,"A",{href:!0});var eDr=s(wP);$uo=r(eDr,"LayoutLMForMaskedLM"),eDr.forEach(t),Iuo=r(LCe," (LayoutLM model)"),LCe.forEach(t),juo=i(N),ju=n(N,"LI",{});var BCe=s(ju);hee=n(BCe,"STRONG",{});var oDr=s(hee);Nuo=r(oDr,"longformer"),oDr.forEach(t),Duo=r(BCe," \u2014 "),AP=n(BCe,"A",{href:!0});var rDr=s(AP);quo=r(rDr,"LongformerForMaskedLM"),rDr.forEach(t),Guo=r(BCe," (Longformer model)"),BCe.forEach(t),Ouo=i(N),Nu=n(N,"LI",{});var kCe=s(Nu);pee=n(kCe,"STRONG",{});var tDr=s(pee);Xuo=r(tDr,"mbart"),tDr.forEach(t),zuo=r(kCe," \u2014 "),LP=n(kCe,"A",{href:!0});var aDr=s(LP);Vuo=r(aDr,"MBartForConditionalGeneration"),aDr.forEach(t),Wuo=r(kCe," (mBART model)"),kCe.forEach(t),Quo=i(N),Du=n(N,"LI",{});var xCe=s(Du);_ee=n(xCe,"STRONG",{});var nDr=s(_ee);Huo=r(nDr,"megatron-bert"),nDr.forEach(t),Uuo=r(xCe," \u2014 "),BP=n(xCe,"A",{href:!0});var sDr=s(BP);Juo=r(sDr,"MegatronBertForMaskedLM"),sDr.forEach(t),Yuo=r(xCe," (MegatronBert model)"),xCe.forEach(t),Kuo=i(N),qu=n(N,"LI",{});var RCe=s(qu);uee=n(RCe,"STRONG",{});var lDr=s(uee);Zuo=r(lDr,"mobilebert"),lDr.forEach(t),e2o=r(RCe," \u2014 "),kP=n(RCe,"A",{href:!0});var iDr=s(kP);o2o=r(iDr,"MobileBertForMaskedLM"),iDr.forEach(t),r2o=r(RCe," (MobileBERT model)"),RCe.forEach(t),t2o=i(N),Gu=n(N,"LI",{});var SCe=s(Gu);bee=n(SCe,"STRONG",{});var dDr=s(bee);a2o=r(dDr,"mpnet"),dDr.forEach(t),n2o=r(SCe," \u2014 "),xP=n(SCe,"A",{href:!0});var cDr=s(xP);s2o=r(cDr,"MPNetForMaskedLM"),cDr.forEach(t),l2o=r(SCe," (MPNet model)"),SCe.forEach(t),i2o=i(N),Ou=n(N,"LI",{});var PCe=s(Ou);vee=n(PCe,"STRONG",{});var fDr=s(vee);d2o=r(fDr,"nystromformer"),fDr.forEach(t),c2o=r(PCe," \u2014 "),RP=n(PCe,"A",{href:!0});var mDr=s(RP);f2o=r(mDr,"NystromformerForMaskedLM"),mDr.forEach(t),m2o=r(PCe," (Nystromformer model)"),PCe.forEach(t),g2o=i(N),Xu=n(N,"LI",{});var $Ce=s(Xu);Tee=n($Ce,"STRONG",{});var gDr=s(Tee);h2o=r(gDr,"perceiver"),gDr.forEach(t),p2o=r($Ce," \u2014 "),SP=n($Ce,"A",{href:!0});var hDr=s(SP);_2o=r(hDr,"PerceiverForMaskedLM"),hDr.forEach(t),u2o=r($Ce," (Perceiver model)"),$Ce.forEach(t),b2o=i(N),zu=n(N,"LI",{});var ICe=s(zu);Fee=n(ICe,"STRONG",{});var pDr=s(Fee);v2o=r(pDr,"qdqbert"),pDr.forEach(t),T2o=r(ICe," \u2014 "),PP=n(ICe,"A",{href:!0});var _Dr=s(PP);F2o=r(_Dr,"QDQBertForMaskedLM"),_Dr.forEach(t),C2o=r(ICe," (QDQBert model)"),ICe.forEach(t),M2o=i(N),Vu=n(N,"LI",{});var jCe=s(Vu);Cee=n(jCe,"STRONG",{});var uDr=s(Cee);E2o=r(uDr,"reformer"),uDr.forEach(t),y2o=r(jCe," \u2014 "),$P=n(jCe,"A",{href:!0});var bDr=s($P);w2o=r(bDr,"ReformerForMaskedLM"),bDr.forEach(t),A2o=r(jCe," (Reformer model)"),jCe.forEach(t),L2o=i(N),Wu=n(N,"LI",{});var NCe=s(Wu);Mee=n(NCe,"STRONG",{});var vDr=s(Mee);B2o=r(vDr,"rembert"),vDr.forEach(t),k2o=r(NCe," \u2014 "),IP=n(NCe,"A",{href:!0});var TDr=s(IP);x2o=r(TDr,"RemBertForMaskedLM"),TDr.forEach(t),R2o=r(NCe," (RemBERT model)"),NCe.forEach(t),S2o=i(N),Qu=n(N,"LI",{});var DCe=s(Qu);Eee=n(DCe,"STRONG",{});var FDr=s(Eee);P2o=r(FDr,"roberta"),FDr.forEach(t),$2o=r(DCe," \u2014 "),jP=n(DCe,"A",{href:!0});var CDr=s(jP);I2o=r(CDr,"RobertaForMaskedLM"),CDr.forEach(t),j2o=r(DCe," (RoBERTa model)"),DCe.forEach(t),N2o=i(N),Hu=n(N,"LI",{});var qCe=s(Hu);yee=n(qCe,"STRONG",{});var MDr=s(yee);D2o=r(MDr,"roformer"),MDr.forEach(t),q2o=r(qCe," \u2014 "),NP=n(qCe,"A",{href:!0});var EDr=s(NP);G2o=r(EDr,"RoFormerForMaskedLM"),EDr.forEach(t),O2o=r(qCe," (RoFormer model)"),qCe.forEach(t),X2o=i(N),Uu=n(N,"LI",{});var GCe=s(Uu);wee=n(GCe,"STRONG",{});var yDr=s(wee);z2o=r(yDr,"squeezebert"),yDr.forEach(t),V2o=r(GCe," \u2014 "),DP=n(GCe,"A",{href:!0});var wDr=s(DP);W2o=r(wDr,"SqueezeBertForMaskedLM"),wDr.forEach(t),Q2o=r(GCe," (SqueezeBERT model)"),GCe.forEach(t),H2o=i(N),Ju=n(N,"LI",{});var OCe=s(Ju);Aee=n(OCe,"STRONG",{});var ADr=s(Aee);U2o=r(ADr,"tapas"),ADr.forEach(t),J2o=r(OCe," \u2014 "),qP=n(OCe,"A",{href:!0});var LDr=s(qP);Y2o=r(LDr,"TapasForMaskedLM"),LDr.forEach(t),K2o=r(OCe," (TAPAS model)"),OCe.forEach(t),Z2o=i(N),Yu=n(N,"LI",{});var XCe=s(Yu);Lee=n(XCe,"STRONG",{});var BDr=s(Lee);e1o=r(BDr,"wav2vec2"),BDr.forEach(t),o1o=r(XCe," \u2014 "),Bee=n(XCe,"CODE",{});var kDr=s(Bee);r1o=r(kDr,"Wav2Vec2ForMaskedLM"),kDr.forEach(t),t1o=r(XCe,"(Wav2Vec2 model)"),XCe.forEach(t),a1o=i(N),Ku=n(N,"LI",{});var zCe=s(Ku);kee=n(zCe,"STRONG",{});var xDr=s(kee);n1o=r(xDr,"xlm"),xDr.forEach(t),s1o=r(zCe," \u2014 "),GP=n(zCe,"A",{href:!0});var RDr=s(GP);l1o=r(RDr,"XLMWithLMHeadModel"),RDr.forEach(t),i1o=r(zCe," (XLM model)"),zCe.forEach(t),d1o=i(N),Zu=n(N,"LI",{});var VCe=s(Zu);xee=n(VCe,"STRONG",{});var SDr=s(xee);c1o=r(SDr,"xlm-roberta"),SDr.forEach(t),f1o=r(VCe," \u2014 "),OP=n(VCe,"A",{href:!0});var PDr=s(OP);m1o=r(PDr,"XLMRobertaForMaskedLM"),PDr.forEach(t),g1o=r(VCe," (XLM-RoBERTa model)"),VCe.forEach(t),h1o=i(N),e2=n(N,"LI",{});var WCe=s(e2);Ree=n(WCe,"STRONG",{});var $Dr=s(Ree);p1o=r($Dr,"xlm-roberta-xl"),$Dr.forEach(t),_1o=r(WCe," \u2014 "),XP=n(WCe,"A",{href:!0});var IDr=s(XP);u1o=r(IDr,"XLMRobertaXLForMaskedLM"),IDr.forEach(t),b1o=r(WCe," (XLM-RoBERTa-XL model)"),WCe.forEach(t),v1o=i(N),o2=n(N,"LI",{});var QCe=s(o2);See=n(QCe,"STRONG",{});var jDr=s(See);T1o=r(jDr,"yoso"),jDr.forEach(t),F1o=r(QCe," \u2014 "),zP=n(QCe,"A",{href:!0});var NDr=s(zP);C1o=r(NDr,"YosoForMaskedLM"),NDr.forEach(t),M1o=r(QCe," (YOSO model)"),QCe.forEach(t),N.forEach(t),E1o=i(It),r2=n(It,"P",{});var HCe=s(r2);y1o=r(HCe,"The model is set in evaluation mode by default using "),Pee=n(HCe,"CODE",{});var DDr=s(Pee);w1o=r(DDr,"model.eval()"),DDr.forEach(t),A1o=r(HCe,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),$ee=n(HCe,"CODE",{});var qDr=s($ee);L1o=r(qDr,"model.train()"),qDr.forEach(t),HCe.forEach(t),B1o=i(It),Iee=n(It,"P",{});var GDr=s(Iee);k1o=r(GDr,"Examples:"),GDr.forEach(t),x1o=i(It),m(dE.$$.fragment,It),It.forEach(t),Vs.forEach(t),BLe=i(d),Zi=n(d,"H2",{class:!0});var IBe=s(Zi);t2=n(IBe,"A",{id:!0,class:!0,href:!0});var ODr=s(t2);jee=n(ODr,"SPAN",{});var XDr=s(jee);m(cE.$$.fragment,XDr),XDr.forEach(t),ODr.forEach(t),R1o=i(IBe),Nee=n(IBe,"SPAN",{});var zDr=s(Nee);S1o=r(zDr,"AutoModelForSeq2SeqLM"),zDr.forEach(t),IBe.forEach(t),kLe=i(d),Uo=n(d,"DIV",{class:!0});var Qs=s(Uo);m(fE.$$.fragment,Qs),P1o=i(Qs),ed=n(Qs,"P",{});var az=s(ed);$1o=r(az,`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence language modeling head) when created
with the `),Dee=n(az,"CODE",{});var VDr=s(Dee);I1o=r(VDr,"from_pretrained()"),VDr.forEach(t),j1o=r(az,"class method or the "),qee=n(az,"CODE",{});var WDr=s(qee);N1o=r(WDr,"from_config()"),WDr.forEach(t),D1o=r(az,`class
method.`),az.forEach(t),q1o=i(Qs),mE=n(Qs,"P",{});var jBe=s(mE);G1o=r(jBe,"This class cannot be instantiated directly using "),Gee=n(jBe,"CODE",{});var QDr=s(Gee);O1o=r(QDr,"__init__()"),QDr.forEach(t),X1o=r(jBe," (throws an error)."),jBe.forEach(t),z1o=i(Qs),Or=n(Qs,"DIV",{class:!0});var Hs=s(Or);m(gE.$$.fragment,Hs),V1o=i(Hs),Oee=n(Hs,"P",{});var HDr=s(Oee);W1o=r(HDr,"Instantiates one of the model classes of the library (with a sequence-to-sequence language modeling head) from a configuration."),HDr.forEach(t),Q1o=i(Hs),od=n(Hs,"P",{});var nz=s(od);H1o=r(nz,`Note:
Loading a model from its configuration file does `),Xee=n(nz,"STRONG",{});var UDr=s(Xee);U1o=r(UDr,"not"),UDr.forEach(t),J1o=r(nz,` load the model weights. It only affects the
model\u2019s configuration. Use `),zee=n(nz,"CODE",{});var JDr=s(zee);Y1o=r(JDr,"from_pretrained()"),JDr.forEach(t),K1o=r(nz,"to load the model weights."),nz.forEach(t),Z1o=i(Hs),Vee=n(Hs,"P",{});var YDr=s(Vee);ebo=r(YDr,"Examples:"),YDr.forEach(t),obo=i(Hs),m(hE.$$.fragment,Hs),Hs.forEach(t),rbo=i(Qs),Pe=n(Qs,"DIV",{class:!0});var jt=s(Pe);m(pE.$$.fragment,jt),tbo=i(jt),Wee=n(jt,"P",{});var KDr=s(Wee);abo=r(KDr,"Instantiate one of the model classes of the library (with a sequence-to-sequence language modeling head) from a pretrained model."),KDr.forEach(t),nbo=i(jt),Xa=n(jt,"P",{});var m4=s(Xa);sbo=r(m4,"The model class to instantiate is selected based on the "),Qee=n(m4,"CODE",{});var ZDr=s(Qee);lbo=r(ZDr,"model_type"),ZDr.forEach(t),ibo=r(m4,` property of the config object (either
passed as an argument or loaded from `),Hee=n(m4,"CODE",{});var eqr=s(Hee);dbo=r(eqr,"pretrained_model_name_or_path"),eqr.forEach(t),cbo=r(m4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Uee=n(m4,"CODE",{});var oqr=s(Uee);fbo=r(oqr,"pretrained_model_name_or_path"),oqr.forEach(t),mbo=r(m4,":"),m4.forEach(t),gbo=i(jt),ae=n(jt,"UL",{});var le=s(ae);a2=n(le,"LI",{});var UCe=s(a2);Jee=n(UCe,"STRONG",{});var rqr=s(Jee);hbo=r(rqr,"bart"),rqr.forEach(t),pbo=r(UCe," \u2014 "),VP=n(UCe,"A",{href:!0});var tqr=s(VP);_bo=r(tqr,"BartForConditionalGeneration"),tqr.forEach(t),ubo=r(UCe," (BART model)"),UCe.forEach(t),bbo=i(le),n2=n(le,"LI",{});var JCe=s(n2);Yee=n(JCe,"STRONG",{});var aqr=s(Yee);vbo=r(aqr,"bigbird_pegasus"),aqr.forEach(t),Tbo=r(JCe," \u2014 "),WP=n(JCe,"A",{href:!0});var nqr=s(WP);Fbo=r(nqr,"BigBirdPegasusForConditionalGeneration"),nqr.forEach(t),Cbo=r(JCe," (BigBirdPegasus model)"),JCe.forEach(t),Mbo=i(le),s2=n(le,"LI",{});var YCe=s(s2);Kee=n(YCe,"STRONG",{});var sqr=s(Kee);Ebo=r(sqr,"blenderbot"),sqr.forEach(t),ybo=r(YCe," \u2014 "),QP=n(YCe,"A",{href:!0});var lqr=s(QP);wbo=r(lqr,"BlenderbotForConditionalGeneration"),lqr.forEach(t),Abo=r(YCe," (Blenderbot model)"),YCe.forEach(t),Lbo=i(le),l2=n(le,"LI",{});var KCe=s(l2);Zee=n(KCe,"STRONG",{});var iqr=s(Zee);Bbo=r(iqr,"blenderbot-small"),iqr.forEach(t),kbo=r(KCe," \u2014 "),HP=n(KCe,"A",{href:!0});var dqr=s(HP);xbo=r(dqr,"BlenderbotSmallForConditionalGeneration"),dqr.forEach(t),Rbo=r(KCe," (BlenderbotSmall model)"),KCe.forEach(t),Sbo=i(le),i2=n(le,"LI",{});var ZCe=s(i2);eoe=n(ZCe,"STRONG",{});var cqr=s(eoe);Pbo=r(cqr,"encoder-decoder"),cqr.forEach(t),$bo=r(ZCe," \u2014 "),UP=n(ZCe,"A",{href:!0});var fqr=s(UP);Ibo=r(fqr,"EncoderDecoderModel"),fqr.forEach(t),jbo=r(ZCe," (Encoder decoder model)"),ZCe.forEach(t),Nbo=i(le),d2=n(le,"LI",{});var e4e=s(d2);ooe=n(e4e,"STRONG",{});var mqr=s(ooe);Dbo=r(mqr,"fsmt"),mqr.forEach(t),qbo=r(e4e," \u2014 "),JP=n(e4e,"A",{href:!0});var gqr=s(JP);Gbo=r(gqr,"FSMTForConditionalGeneration"),gqr.forEach(t),Obo=r(e4e," (FairSeq Machine-Translation model)"),e4e.forEach(t),Xbo=i(le),c2=n(le,"LI",{});var o4e=s(c2);roe=n(o4e,"STRONG",{});var hqr=s(roe);zbo=r(hqr,"led"),hqr.forEach(t),Vbo=r(o4e," \u2014 "),YP=n(o4e,"A",{href:!0});var pqr=s(YP);Wbo=r(pqr,"LEDForConditionalGeneration"),pqr.forEach(t),Qbo=r(o4e," (LED model)"),o4e.forEach(t),Hbo=i(le),f2=n(le,"LI",{});var r4e=s(f2);toe=n(r4e,"STRONG",{});var _qr=s(toe);Ubo=r(_qr,"m2m_100"),_qr.forEach(t),Jbo=r(r4e," \u2014 "),KP=n(r4e,"A",{href:!0});var uqr=s(KP);Ybo=r(uqr,"M2M100ForConditionalGeneration"),uqr.forEach(t),Kbo=r(r4e," (M2M100 model)"),r4e.forEach(t),Zbo=i(le),m2=n(le,"LI",{});var t4e=s(m2);aoe=n(t4e,"STRONG",{});var bqr=s(aoe);e5o=r(bqr,"marian"),bqr.forEach(t),o5o=r(t4e," \u2014 "),ZP=n(t4e,"A",{href:!0});var vqr=s(ZP);r5o=r(vqr,"MarianMTModel"),vqr.forEach(t),t5o=r(t4e," (Marian model)"),t4e.forEach(t),a5o=i(le),g2=n(le,"LI",{});var a4e=s(g2);noe=n(a4e,"STRONG",{});var Tqr=s(noe);n5o=r(Tqr,"mbart"),Tqr.forEach(t),s5o=r(a4e," \u2014 "),e$=n(a4e,"A",{href:!0});var Fqr=s(e$);l5o=r(Fqr,"MBartForConditionalGeneration"),Fqr.forEach(t),i5o=r(a4e," (mBART model)"),a4e.forEach(t),d5o=i(le),h2=n(le,"LI",{});var n4e=s(h2);soe=n(n4e,"STRONG",{});var Cqr=s(soe);c5o=r(Cqr,"mt5"),Cqr.forEach(t),f5o=r(n4e," \u2014 "),o$=n(n4e,"A",{href:!0});var Mqr=s(o$);m5o=r(Mqr,"MT5ForConditionalGeneration"),Mqr.forEach(t),g5o=r(n4e," (mT5 model)"),n4e.forEach(t),h5o=i(le),p2=n(le,"LI",{});var s4e=s(p2);loe=n(s4e,"STRONG",{});var Eqr=s(loe);p5o=r(Eqr,"pegasus"),Eqr.forEach(t),_5o=r(s4e," \u2014 "),r$=n(s4e,"A",{href:!0});var yqr=s(r$);u5o=r(yqr,"PegasusForConditionalGeneration"),yqr.forEach(t),b5o=r(s4e," (Pegasus model)"),s4e.forEach(t),v5o=i(le),_2=n(le,"LI",{});var l4e=s(_2);ioe=n(l4e,"STRONG",{});var wqr=s(ioe);T5o=r(wqr,"plbart"),wqr.forEach(t),F5o=r(l4e," \u2014 "),t$=n(l4e,"A",{href:!0});var Aqr=s(t$);C5o=r(Aqr,"PLBartForConditionalGeneration"),Aqr.forEach(t),M5o=r(l4e," (PLBart model)"),l4e.forEach(t),E5o=i(le),u2=n(le,"LI",{});var i4e=s(u2);doe=n(i4e,"STRONG",{});var Lqr=s(doe);y5o=r(Lqr,"prophetnet"),Lqr.forEach(t),w5o=r(i4e," \u2014 "),a$=n(i4e,"A",{href:!0});var Bqr=s(a$);A5o=r(Bqr,"ProphetNetForConditionalGeneration"),Bqr.forEach(t),L5o=r(i4e," (ProphetNet model)"),i4e.forEach(t),B5o=i(le),b2=n(le,"LI",{});var d4e=s(b2);coe=n(d4e,"STRONG",{});var kqr=s(coe);k5o=r(kqr,"t5"),kqr.forEach(t),x5o=r(d4e," \u2014 "),n$=n(d4e,"A",{href:!0});var xqr=s(n$);R5o=r(xqr,"T5ForConditionalGeneration"),xqr.forEach(t),S5o=r(d4e," (T5 model)"),d4e.forEach(t),P5o=i(le),v2=n(le,"LI",{});var c4e=s(v2);foe=n(c4e,"STRONG",{});var Rqr=s(foe);$5o=r(Rqr,"xlm-prophetnet"),Rqr.forEach(t),I5o=r(c4e," \u2014 "),s$=n(c4e,"A",{href:!0});var Sqr=s(s$);j5o=r(Sqr,"XLMProphetNetForConditionalGeneration"),Sqr.forEach(t),N5o=r(c4e," (XLMProphetNet model)"),c4e.forEach(t),le.forEach(t),D5o=i(jt),T2=n(jt,"P",{});var f4e=s(T2);q5o=r(f4e,"The model is set in evaluation mode by default using "),moe=n(f4e,"CODE",{});var Pqr=s(moe);G5o=r(Pqr,"model.eval()"),Pqr.forEach(t),O5o=r(f4e,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),goe=n(f4e,"CODE",{});var $qr=s(goe);X5o=r($qr,"model.train()"),$qr.forEach(t),f4e.forEach(t),z5o=i(jt),hoe=n(jt,"P",{});var Iqr=s(hoe);V5o=r(Iqr,"Examples:"),Iqr.forEach(t),W5o=i(jt),m(_E.$$.fragment,jt),jt.forEach(t),Qs.forEach(t),xLe=i(d),rd=n(d,"H2",{class:!0});var NBe=s(rd);F2=n(NBe,"A",{id:!0,class:!0,href:!0});var jqr=s(F2);poe=n(jqr,"SPAN",{});var Nqr=s(poe);m(uE.$$.fragment,Nqr),Nqr.forEach(t),jqr.forEach(t),Q5o=i(NBe),_oe=n(NBe,"SPAN",{});var Dqr=s(_oe);H5o=r(Dqr,"AutoModelForSequenceClassification"),Dqr.forEach(t),NBe.forEach(t),RLe=i(d),Jo=n(d,"DIV",{class:!0});var Us=s(Jo);m(bE.$$.fragment,Us),U5o=i(Us),td=n(Us,"P",{});var sz=s(td);J5o=r(sz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence classification head) when created
with the `),uoe=n(sz,"CODE",{});var qqr=s(uoe);Y5o=r(qqr,"from_pretrained()"),qqr.forEach(t),K5o=r(sz,"class method or the "),boe=n(sz,"CODE",{});var Gqr=s(boe);Z5o=r(Gqr,"from_config()"),Gqr.forEach(t),evo=r(sz,`class
method.`),sz.forEach(t),ovo=i(Us),vE=n(Us,"P",{});var DBe=s(vE);rvo=r(DBe,"This class cannot be instantiated directly using "),voe=n(DBe,"CODE",{});var Oqr=s(voe);tvo=r(Oqr,"__init__()"),Oqr.forEach(t),avo=r(DBe," (throws an error)."),DBe.forEach(t),nvo=i(Us),Xr=n(Us,"DIV",{class:!0});var Js=s(Xr);m(TE.$$.fragment,Js),svo=i(Js),Toe=n(Js,"P",{});var Xqr=s(Toe);lvo=r(Xqr,"Instantiates one of the model classes of the library (with a sequence classification head) from a configuration."),Xqr.forEach(t),ivo=i(Js),ad=n(Js,"P",{});var lz=s(ad);dvo=r(lz,`Note:
Loading a model from its configuration file does `),Foe=n(lz,"STRONG",{});var zqr=s(Foe);cvo=r(zqr,"not"),zqr.forEach(t),fvo=r(lz,` load the model weights. It only affects the
model\u2019s configuration. Use `),Coe=n(lz,"CODE",{});var Vqr=s(Coe);mvo=r(Vqr,"from_pretrained()"),Vqr.forEach(t),gvo=r(lz,"to load the model weights."),lz.forEach(t),hvo=i(Js),Moe=n(Js,"P",{});var Wqr=s(Moe);pvo=r(Wqr,"Examples:"),Wqr.forEach(t),_vo=i(Js),m(FE.$$.fragment,Js),Js.forEach(t),uvo=i(Us),$e=n(Us,"DIV",{class:!0});var Nt=s($e);m(CE.$$.fragment,Nt),bvo=i(Nt),Eoe=n(Nt,"P",{});var Qqr=s(Eoe);vvo=r(Qqr,"Instantiate one of the model classes of the library (with a sequence classification head) from a pretrained model."),Qqr.forEach(t),Tvo=i(Nt),za=n(Nt,"P",{});var g4=s(za);Fvo=r(g4,"The model class to instantiate is selected based on the "),yoe=n(g4,"CODE",{});var Hqr=s(yoe);Cvo=r(Hqr,"model_type"),Hqr.forEach(t),Mvo=r(g4,` property of the config object (either
passed as an argument or loaded from `),woe=n(g4,"CODE",{});var Uqr=s(woe);Evo=r(Uqr,"pretrained_model_name_or_path"),Uqr.forEach(t),yvo=r(g4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Aoe=n(g4,"CODE",{});var Jqr=s(Aoe);wvo=r(Jqr,"pretrained_model_name_or_path"),Jqr.forEach(t),Avo=r(g4,":"),g4.forEach(t),Lvo=i(Nt),A=n(Nt,"UL",{});var L=s(A);C2=n(L,"LI",{});var m4e=s(C2);Loe=n(m4e,"STRONG",{});var Yqr=s(Loe);Bvo=r(Yqr,"albert"),Yqr.forEach(t),kvo=r(m4e," \u2014 "),l$=n(m4e,"A",{href:!0});var Kqr=s(l$);xvo=r(Kqr,"AlbertForSequenceClassification"),Kqr.forEach(t),Rvo=r(m4e," (ALBERT model)"),m4e.forEach(t),Svo=i(L),M2=n(L,"LI",{});var g4e=s(M2);Boe=n(g4e,"STRONG",{});var Zqr=s(Boe);Pvo=r(Zqr,"bart"),Zqr.forEach(t),$vo=r(g4e," \u2014 "),i$=n(g4e,"A",{href:!0});var eGr=s(i$);Ivo=r(eGr,"BartForSequenceClassification"),eGr.forEach(t),jvo=r(g4e," (BART model)"),g4e.forEach(t),Nvo=i(L),E2=n(L,"LI",{});var h4e=s(E2);koe=n(h4e,"STRONG",{});var oGr=s(koe);Dvo=r(oGr,"bert"),oGr.forEach(t),qvo=r(h4e," \u2014 "),d$=n(h4e,"A",{href:!0});var rGr=s(d$);Gvo=r(rGr,"BertForSequenceClassification"),rGr.forEach(t),Ovo=r(h4e," (BERT model)"),h4e.forEach(t),Xvo=i(L),y2=n(L,"LI",{});var p4e=s(y2);xoe=n(p4e,"STRONG",{});var tGr=s(xoe);zvo=r(tGr,"big_bird"),tGr.forEach(t),Vvo=r(p4e," \u2014 "),c$=n(p4e,"A",{href:!0});var aGr=s(c$);Wvo=r(aGr,"BigBirdForSequenceClassification"),aGr.forEach(t),Qvo=r(p4e," (BigBird model)"),p4e.forEach(t),Hvo=i(L),w2=n(L,"LI",{});var _4e=s(w2);Roe=n(_4e,"STRONG",{});var nGr=s(Roe);Uvo=r(nGr,"bigbird_pegasus"),nGr.forEach(t),Jvo=r(_4e," \u2014 "),f$=n(_4e,"A",{href:!0});var sGr=s(f$);Yvo=r(sGr,"BigBirdPegasusForSequenceClassification"),sGr.forEach(t),Kvo=r(_4e," (BigBirdPegasus model)"),_4e.forEach(t),Zvo=i(L),A2=n(L,"LI",{});var u4e=s(A2);Soe=n(u4e,"STRONG",{});var lGr=s(Soe);eTo=r(lGr,"camembert"),lGr.forEach(t),oTo=r(u4e," \u2014 "),m$=n(u4e,"A",{href:!0});var iGr=s(m$);rTo=r(iGr,"CamembertForSequenceClassification"),iGr.forEach(t),tTo=r(u4e," (CamemBERT model)"),u4e.forEach(t),aTo=i(L),L2=n(L,"LI",{});var b4e=s(L2);Poe=n(b4e,"STRONG",{});var dGr=s(Poe);nTo=r(dGr,"canine"),dGr.forEach(t),sTo=r(b4e," \u2014 "),g$=n(b4e,"A",{href:!0});var cGr=s(g$);lTo=r(cGr,"CanineForSequenceClassification"),cGr.forEach(t),iTo=r(b4e," (Canine model)"),b4e.forEach(t),dTo=i(L),B2=n(L,"LI",{});var v4e=s(B2);$oe=n(v4e,"STRONG",{});var fGr=s($oe);cTo=r(fGr,"convbert"),fGr.forEach(t),fTo=r(v4e," \u2014 "),h$=n(v4e,"A",{href:!0});var mGr=s(h$);mTo=r(mGr,"ConvBertForSequenceClassification"),mGr.forEach(t),gTo=r(v4e," (ConvBERT model)"),v4e.forEach(t),hTo=i(L),k2=n(L,"LI",{});var T4e=s(k2);Ioe=n(T4e,"STRONG",{});var gGr=s(Ioe);pTo=r(gGr,"ctrl"),gGr.forEach(t),_To=r(T4e," \u2014 "),p$=n(T4e,"A",{href:!0});var hGr=s(p$);uTo=r(hGr,"CTRLForSequenceClassification"),hGr.forEach(t),bTo=r(T4e," (CTRL model)"),T4e.forEach(t),vTo=i(L),x2=n(L,"LI",{});var F4e=s(x2);joe=n(F4e,"STRONG",{});var pGr=s(joe);TTo=r(pGr,"deberta"),pGr.forEach(t),FTo=r(F4e," \u2014 "),_$=n(F4e,"A",{href:!0});var _Gr=s(_$);CTo=r(_Gr,"DebertaForSequenceClassification"),_Gr.forEach(t),MTo=r(F4e," (DeBERTa model)"),F4e.forEach(t),ETo=i(L),R2=n(L,"LI",{});var C4e=s(R2);Noe=n(C4e,"STRONG",{});var uGr=s(Noe);yTo=r(uGr,"deberta-v2"),uGr.forEach(t),wTo=r(C4e," \u2014 "),u$=n(C4e,"A",{href:!0});var bGr=s(u$);ATo=r(bGr,"DebertaV2ForSequenceClassification"),bGr.forEach(t),LTo=r(C4e," (DeBERTa-v2 model)"),C4e.forEach(t),BTo=i(L),S2=n(L,"LI",{});var M4e=s(S2);Doe=n(M4e,"STRONG",{});var vGr=s(Doe);kTo=r(vGr,"distilbert"),vGr.forEach(t),xTo=r(M4e," \u2014 "),b$=n(M4e,"A",{href:!0});var TGr=s(b$);RTo=r(TGr,"DistilBertForSequenceClassification"),TGr.forEach(t),STo=r(M4e," (DistilBERT model)"),M4e.forEach(t),PTo=i(L),P2=n(L,"LI",{});var E4e=s(P2);qoe=n(E4e,"STRONG",{});var FGr=s(qoe);$To=r(FGr,"electra"),FGr.forEach(t),ITo=r(E4e," \u2014 "),v$=n(E4e,"A",{href:!0});var CGr=s(v$);jTo=r(CGr,"ElectraForSequenceClassification"),CGr.forEach(t),NTo=r(E4e," (ELECTRA model)"),E4e.forEach(t),DTo=i(L),$2=n(L,"LI",{});var y4e=s($2);Goe=n(y4e,"STRONG",{});var MGr=s(Goe);qTo=r(MGr,"flaubert"),MGr.forEach(t),GTo=r(y4e," \u2014 "),T$=n(y4e,"A",{href:!0});var EGr=s(T$);OTo=r(EGr,"FlaubertForSequenceClassification"),EGr.forEach(t),XTo=r(y4e," (FlauBERT model)"),y4e.forEach(t),zTo=i(L),I2=n(L,"LI",{});var w4e=s(I2);Ooe=n(w4e,"STRONG",{});var yGr=s(Ooe);VTo=r(yGr,"fnet"),yGr.forEach(t),WTo=r(w4e," \u2014 "),F$=n(w4e,"A",{href:!0});var wGr=s(F$);QTo=r(wGr,"FNetForSequenceClassification"),wGr.forEach(t),HTo=r(w4e," (FNet model)"),w4e.forEach(t),UTo=i(L),j2=n(L,"LI",{});var A4e=s(j2);Xoe=n(A4e,"STRONG",{});var AGr=s(Xoe);JTo=r(AGr,"funnel"),AGr.forEach(t),YTo=r(A4e," \u2014 "),C$=n(A4e,"A",{href:!0});var LGr=s(C$);KTo=r(LGr,"FunnelForSequenceClassification"),LGr.forEach(t),ZTo=r(A4e," (Funnel Transformer model)"),A4e.forEach(t),e7o=i(L),N2=n(L,"LI",{});var L4e=s(N2);zoe=n(L4e,"STRONG",{});var BGr=s(zoe);o7o=r(BGr,"gpt2"),BGr.forEach(t),r7o=r(L4e," \u2014 "),M$=n(L4e,"A",{href:!0});var kGr=s(M$);t7o=r(kGr,"GPT2ForSequenceClassification"),kGr.forEach(t),a7o=r(L4e," (OpenAI GPT-2 model)"),L4e.forEach(t),n7o=i(L),D2=n(L,"LI",{});var B4e=s(D2);Voe=n(B4e,"STRONG",{});var xGr=s(Voe);s7o=r(xGr,"gpt_neo"),xGr.forEach(t),l7o=r(B4e," \u2014 "),E$=n(B4e,"A",{href:!0});var RGr=s(E$);i7o=r(RGr,"GPTNeoForSequenceClassification"),RGr.forEach(t),d7o=r(B4e," (GPT Neo model)"),B4e.forEach(t),c7o=i(L),q2=n(L,"LI",{});var k4e=s(q2);Woe=n(k4e,"STRONG",{});var SGr=s(Woe);f7o=r(SGr,"gptj"),SGr.forEach(t),m7o=r(k4e," \u2014 "),y$=n(k4e,"A",{href:!0});var PGr=s(y$);g7o=r(PGr,"GPTJForSequenceClassification"),PGr.forEach(t),h7o=r(k4e," (GPT-J model)"),k4e.forEach(t),p7o=i(L),G2=n(L,"LI",{});var x4e=s(G2);Qoe=n(x4e,"STRONG",{});var $Gr=s(Qoe);_7o=r($Gr,"ibert"),$Gr.forEach(t),u7o=r(x4e," \u2014 "),w$=n(x4e,"A",{href:!0});var IGr=s(w$);b7o=r(IGr,"IBertForSequenceClassification"),IGr.forEach(t),v7o=r(x4e," (I-BERT model)"),x4e.forEach(t),T7o=i(L),O2=n(L,"LI",{});var R4e=s(O2);Hoe=n(R4e,"STRONG",{});var jGr=s(Hoe);F7o=r(jGr,"layoutlm"),jGr.forEach(t),C7o=r(R4e," \u2014 "),A$=n(R4e,"A",{href:!0});var NGr=s(A$);M7o=r(NGr,"LayoutLMForSequenceClassification"),NGr.forEach(t),E7o=r(R4e," (LayoutLM model)"),R4e.forEach(t),y7o=i(L),X2=n(L,"LI",{});var S4e=s(X2);Uoe=n(S4e,"STRONG",{});var DGr=s(Uoe);w7o=r(DGr,"layoutlmv2"),DGr.forEach(t),A7o=r(S4e," \u2014 "),L$=n(S4e,"A",{href:!0});var qGr=s(L$);L7o=r(qGr,"LayoutLMv2ForSequenceClassification"),qGr.forEach(t),B7o=r(S4e," (LayoutLMv2 model)"),S4e.forEach(t),k7o=i(L),z2=n(L,"LI",{});var P4e=s(z2);Joe=n(P4e,"STRONG",{});var GGr=s(Joe);x7o=r(GGr,"led"),GGr.forEach(t),R7o=r(P4e," \u2014 "),B$=n(P4e,"A",{href:!0});var OGr=s(B$);S7o=r(OGr,"LEDForSequenceClassification"),OGr.forEach(t),P7o=r(P4e," (LED model)"),P4e.forEach(t),$7o=i(L),V2=n(L,"LI",{});var $4e=s(V2);Yoe=n($4e,"STRONG",{});var XGr=s(Yoe);I7o=r(XGr,"longformer"),XGr.forEach(t),j7o=r($4e," \u2014 "),k$=n($4e,"A",{href:!0});var zGr=s(k$);N7o=r(zGr,"LongformerForSequenceClassification"),zGr.forEach(t),D7o=r($4e," (Longformer model)"),$4e.forEach(t),q7o=i(L),W2=n(L,"LI",{});var I4e=s(W2);Koe=n(I4e,"STRONG",{});var VGr=s(Koe);G7o=r(VGr,"mbart"),VGr.forEach(t),O7o=r(I4e," \u2014 "),x$=n(I4e,"A",{href:!0});var WGr=s(x$);X7o=r(WGr,"MBartForSequenceClassification"),WGr.forEach(t),z7o=r(I4e," (mBART model)"),I4e.forEach(t),V7o=i(L),Q2=n(L,"LI",{});var j4e=s(Q2);Zoe=n(j4e,"STRONG",{});var QGr=s(Zoe);W7o=r(QGr,"megatron-bert"),QGr.forEach(t),Q7o=r(j4e," \u2014 "),R$=n(j4e,"A",{href:!0});var HGr=s(R$);H7o=r(HGr,"MegatronBertForSequenceClassification"),HGr.forEach(t),U7o=r(j4e," (MegatronBert model)"),j4e.forEach(t),J7o=i(L),H2=n(L,"LI",{});var N4e=s(H2);ere=n(N4e,"STRONG",{});var UGr=s(ere);Y7o=r(UGr,"mobilebert"),UGr.forEach(t),K7o=r(N4e," \u2014 "),S$=n(N4e,"A",{href:!0});var JGr=s(S$);Z7o=r(JGr,"MobileBertForSequenceClassification"),JGr.forEach(t),eFo=r(N4e," (MobileBERT model)"),N4e.forEach(t),oFo=i(L),U2=n(L,"LI",{});var D4e=s(U2);ore=n(D4e,"STRONG",{});var YGr=s(ore);rFo=r(YGr,"mpnet"),YGr.forEach(t),tFo=r(D4e," \u2014 "),P$=n(D4e,"A",{href:!0});var KGr=s(P$);aFo=r(KGr,"MPNetForSequenceClassification"),KGr.forEach(t),nFo=r(D4e," (MPNet model)"),D4e.forEach(t),sFo=i(L),J2=n(L,"LI",{});var q4e=s(J2);rre=n(q4e,"STRONG",{});var ZGr=s(rre);lFo=r(ZGr,"nystromformer"),ZGr.forEach(t),iFo=r(q4e," \u2014 "),$$=n(q4e,"A",{href:!0});var eOr=s($$);dFo=r(eOr,"NystromformerForSequenceClassification"),eOr.forEach(t),cFo=r(q4e," (Nystromformer model)"),q4e.forEach(t),fFo=i(L),Y2=n(L,"LI",{});var G4e=s(Y2);tre=n(G4e,"STRONG",{});var oOr=s(tre);mFo=r(oOr,"openai-gpt"),oOr.forEach(t),gFo=r(G4e," \u2014 "),I$=n(G4e,"A",{href:!0});var rOr=s(I$);hFo=r(rOr,"OpenAIGPTForSequenceClassification"),rOr.forEach(t),pFo=r(G4e," (OpenAI GPT model)"),G4e.forEach(t),_Fo=i(L),K2=n(L,"LI",{});var O4e=s(K2);are=n(O4e,"STRONG",{});var tOr=s(are);uFo=r(tOr,"perceiver"),tOr.forEach(t),bFo=r(O4e," \u2014 "),j$=n(O4e,"A",{href:!0});var aOr=s(j$);vFo=r(aOr,"PerceiverForSequenceClassification"),aOr.forEach(t),TFo=r(O4e," (Perceiver model)"),O4e.forEach(t),FFo=i(L),Z2=n(L,"LI",{});var X4e=s(Z2);nre=n(X4e,"STRONG",{});var nOr=s(nre);CFo=r(nOr,"plbart"),nOr.forEach(t),MFo=r(X4e," \u2014 "),N$=n(X4e,"A",{href:!0});var sOr=s(N$);EFo=r(sOr,"PLBartForSequenceClassification"),sOr.forEach(t),yFo=r(X4e," (PLBart model)"),X4e.forEach(t),wFo=i(L),e1=n(L,"LI",{});var z4e=s(e1);sre=n(z4e,"STRONG",{});var lOr=s(sre);AFo=r(lOr,"qdqbert"),lOr.forEach(t),LFo=r(z4e," \u2014 "),D$=n(z4e,"A",{href:!0});var iOr=s(D$);BFo=r(iOr,"QDQBertForSequenceClassification"),iOr.forEach(t),kFo=r(z4e," (QDQBert model)"),z4e.forEach(t),xFo=i(L),o1=n(L,"LI",{});var V4e=s(o1);lre=n(V4e,"STRONG",{});var dOr=s(lre);RFo=r(dOr,"reformer"),dOr.forEach(t),SFo=r(V4e," \u2014 "),q$=n(V4e,"A",{href:!0});var cOr=s(q$);PFo=r(cOr,"ReformerForSequenceClassification"),cOr.forEach(t),$Fo=r(V4e," (Reformer model)"),V4e.forEach(t),IFo=i(L),r1=n(L,"LI",{});var W4e=s(r1);ire=n(W4e,"STRONG",{});var fOr=s(ire);jFo=r(fOr,"rembert"),fOr.forEach(t),NFo=r(W4e," \u2014 "),G$=n(W4e,"A",{href:!0});var mOr=s(G$);DFo=r(mOr,"RemBertForSequenceClassification"),mOr.forEach(t),qFo=r(W4e," (RemBERT model)"),W4e.forEach(t),GFo=i(L),t1=n(L,"LI",{});var Q4e=s(t1);dre=n(Q4e,"STRONG",{});var gOr=s(dre);OFo=r(gOr,"roberta"),gOr.forEach(t),XFo=r(Q4e," \u2014 "),O$=n(Q4e,"A",{href:!0});var hOr=s(O$);zFo=r(hOr,"RobertaForSequenceClassification"),hOr.forEach(t),VFo=r(Q4e," (RoBERTa model)"),Q4e.forEach(t),WFo=i(L),a1=n(L,"LI",{});var H4e=s(a1);cre=n(H4e,"STRONG",{});var pOr=s(cre);QFo=r(pOr,"roformer"),pOr.forEach(t),HFo=r(H4e," \u2014 "),X$=n(H4e,"A",{href:!0});var _Or=s(X$);UFo=r(_Or,"RoFormerForSequenceClassification"),_Or.forEach(t),JFo=r(H4e," (RoFormer model)"),H4e.forEach(t),YFo=i(L),n1=n(L,"LI",{});var U4e=s(n1);fre=n(U4e,"STRONG",{});var uOr=s(fre);KFo=r(uOr,"squeezebert"),uOr.forEach(t),ZFo=r(U4e," \u2014 "),z$=n(U4e,"A",{href:!0});var bOr=s(z$);e9o=r(bOr,"SqueezeBertForSequenceClassification"),bOr.forEach(t),o9o=r(U4e," (SqueezeBERT model)"),U4e.forEach(t),r9o=i(L),s1=n(L,"LI",{});var J4e=s(s1);mre=n(J4e,"STRONG",{});var vOr=s(mre);t9o=r(vOr,"tapas"),vOr.forEach(t),a9o=r(J4e," \u2014 "),V$=n(J4e,"A",{href:!0});var TOr=s(V$);n9o=r(TOr,"TapasForSequenceClassification"),TOr.forEach(t),s9o=r(J4e," (TAPAS model)"),J4e.forEach(t),l9o=i(L),l1=n(L,"LI",{});var Y4e=s(l1);gre=n(Y4e,"STRONG",{});var FOr=s(gre);i9o=r(FOr,"transfo-xl"),FOr.forEach(t),d9o=r(Y4e," \u2014 "),W$=n(Y4e,"A",{href:!0});var COr=s(W$);c9o=r(COr,"TransfoXLForSequenceClassification"),COr.forEach(t),f9o=r(Y4e," (Transformer-XL model)"),Y4e.forEach(t),m9o=i(L),i1=n(L,"LI",{});var K4e=s(i1);hre=n(K4e,"STRONG",{});var MOr=s(hre);g9o=r(MOr,"xlm"),MOr.forEach(t),h9o=r(K4e," \u2014 "),Q$=n(K4e,"A",{href:!0});var EOr=s(Q$);p9o=r(EOr,"XLMForSequenceClassification"),EOr.forEach(t),_9o=r(K4e," (XLM model)"),K4e.forEach(t),u9o=i(L),d1=n(L,"LI",{});var Z4e=s(d1);pre=n(Z4e,"STRONG",{});var yOr=s(pre);b9o=r(yOr,"xlm-roberta"),yOr.forEach(t),v9o=r(Z4e," \u2014 "),H$=n(Z4e,"A",{href:!0});var wOr=s(H$);T9o=r(wOr,"XLMRobertaForSequenceClassification"),wOr.forEach(t),F9o=r(Z4e," (XLM-RoBERTa model)"),Z4e.forEach(t),C9o=i(L),c1=n(L,"LI",{});var eMe=s(c1);_re=n(eMe,"STRONG",{});var AOr=s(_re);M9o=r(AOr,"xlm-roberta-xl"),AOr.forEach(t),E9o=r(eMe," \u2014 "),U$=n(eMe,"A",{href:!0});var LOr=s(U$);y9o=r(LOr,"XLMRobertaXLForSequenceClassification"),LOr.forEach(t),w9o=r(eMe," (XLM-RoBERTa-XL model)"),eMe.forEach(t),A9o=i(L),f1=n(L,"LI",{});var oMe=s(f1);ure=n(oMe,"STRONG",{});var BOr=s(ure);L9o=r(BOr,"xlnet"),BOr.forEach(t),B9o=r(oMe," \u2014 "),J$=n(oMe,"A",{href:!0});var kOr=s(J$);k9o=r(kOr,"XLNetForSequenceClassification"),kOr.forEach(t),x9o=r(oMe," (XLNet model)"),oMe.forEach(t),R9o=i(L),m1=n(L,"LI",{});var rMe=s(m1);bre=n(rMe,"STRONG",{});var xOr=s(bre);S9o=r(xOr,"yoso"),xOr.forEach(t),P9o=r(rMe," \u2014 "),Y$=n(rMe,"A",{href:!0});var ROr=s(Y$);$9o=r(ROr,"YosoForSequenceClassification"),ROr.forEach(t),I9o=r(rMe," (YOSO model)"),rMe.forEach(t),L.forEach(t),j9o=i(Nt),g1=n(Nt,"P",{});var tMe=s(g1);N9o=r(tMe,"The model is set in evaluation mode by default using "),vre=n(tMe,"CODE",{});var SOr=s(vre);D9o=r(SOr,"model.eval()"),SOr.forEach(t),q9o=r(tMe,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Tre=n(tMe,"CODE",{});var POr=s(Tre);G9o=r(POr,"model.train()"),POr.forEach(t),tMe.forEach(t),O9o=i(Nt),Fre=n(Nt,"P",{});var $Or=s(Fre);X9o=r($Or,"Examples:"),$Or.forEach(t),z9o=i(Nt),m(ME.$$.fragment,Nt),Nt.forEach(t),Us.forEach(t),SLe=i(d),nd=n(d,"H2",{class:!0});var qBe=s(nd);h1=n(qBe,"A",{id:!0,class:!0,href:!0});var IOr=s(h1);Cre=n(IOr,"SPAN",{});var jOr=s(Cre);m(EE.$$.fragment,jOr),jOr.forEach(t),IOr.forEach(t),V9o=i(qBe),Mre=n(qBe,"SPAN",{});var NOr=s(Mre);W9o=r(NOr,"AutoModelForMultipleChoice"),NOr.forEach(t),qBe.forEach(t),PLe=i(d),Yo=n(d,"DIV",{class:!0});var Ys=s(Yo);m(yE.$$.fragment,Ys),Q9o=i(Ys),sd=n(Ys,"P",{});var iz=s(sd);H9o=r(iz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a multiple choice head) when created
with the `),Ere=n(iz,"CODE",{});var DOr=s(Ere);U9o=r(DOr,"from_pretrained()"),DOr.forEach(t),J9o=r(iz,"class method or the "),yre=n(iz,"CODE",{});var qOr=s(yre);Y9o=r(qOr,"from_config()"),qOr.forEach(t),K9o=r(iz,`class
method.`),iz.forEach(t),Z9o=i(Ys),wE=n(Ys,"P",{});var GBe=s(wE);eCo=r(GBe,"This class cannot be instantiated directly using "),wre=n(GBe,"CODE",{});var GOr=s(wre);oCo=r(GOr,"__init__()"),GOr.forEach(t),rCo=r(GBe," (throws an error)."),GBe.forEach(t),tCo=i(Ys),zr=n(Ys,"DIV",{class:!0});var Ks=s(zr);m(AE.$$.fragment,Ks),aCo=i(Ks),Are=n(Ks,"P",{});var OOr=s(Are);nCo=r(OOr,"Instantiates one of the model classes of the library (with a multiple choice head) from a configuration."),OOr.forEach(t),sCo=i(Ks),ld=n(Ks,"P",{});var dz=s(ld);lCo=r(dz,`Note:
Loading a model from its configuration file does `),Lre=n(dz,"STRONG",{});var XOr=s(Lre);iCo=r(XOr,"not"),XOr.forEach(t),dCo=r(dz,` load the model weights. It only affects the
model\u2019s configuration. Use `),Bre=n(dz,"CODE",{});var zOr=s(Bre);cCo=r(zOr,"from_pretrained()"),zOr.forEach(t),fCo=r(dz,"to load the model weights."),dz.forEach(t),mCo=i(Ks),kre=n(Ks,"P",{});var VOr=s(kre);gCo=r(VOr,"Examples:"),VOr.forEach(t),hCo=i(Ks),m(LE.$$.fragment,Ks),Ks.forEach(t),pCo=i(Ys),Ie=n(Ys,"DIV",{class:!0});var Dt=s(Ie);m(BE.$$.fragment,Dt),_Co=i(Dt),xre=n(Dt,"P",{});var WOr=s(xre);uCo=r(WOr,"Instantiate one of the model classes of the library (with a multiple choice head) from a pretrained model."),WOr.forEach(t),bCo=i(Dt),Va=n(Dt,"P",{});var h4=s(Va);vCo=r(h4,"The model class to instantiate is selected based on the "),Rre=n(h4,"CODE",{});var QOr=s(Rre);TCo=r(QOr,"model_type"),QOr.forEach(t),FCo=r(h4,` property of the config object (either
passed as an argument or loaded from `),Sre=n(h4,"CODE",{});var HOr=s(Sre);CCo=r(HOr,"pretrained_model_name_or_path"),HOr.forEach(t),MCo=r(h4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Pre=n(h4,"CODE",{});var UOr=s(Pre);ECo=r(UOr,"pretrained_model_name_or_path"),UOr.forEach(t),yCo=r(h4,":"),h4.forEach(t),wCo=i(Dt),G=n(Dt,"UL",{});var O=s(G);p1=n(O,"LI",{});var aMe=s(p1);$re=n(aMe,"STRONG",{});var JOr=s($re);ACo=r(JOr,"albert"),JOr.forEach(t),LCo=r(aMe," \u2014 "),K$=n(aMe,"A",{href:!0});var YOr=s(K$);BCo=r(YOr,"AlbertForMultipleChoice"),YOr.forEach(t),kCo=r(aMe," (ALBERT model)"),aMe.forEach(t),xCo=i(O),_1=n(O,"LI",{});var nMe=s(_1);Ire=n(nMe,"STRONG",{});var KOr=s(Ire);RCo=r(KOr,"bert"),KOr.forEach(t),SCo=r(nMe," \u2014 "),Z$=n(nMe,"A",{href:!0});var ZOr=s(Z$);PCo=r(ZOr,"BertForMultipleChoice"),ZOr.forEach(t),$Co=r(nMe," (BERT model)"),nMe.forEach(t),ICo=i(O),u1=n(O,"LI",{});var sMe=s(u1);jre=n(sMe,"STRONG",{});var eXr=s(jre);jCo=r(eXr,"big_bird"),eXr.forEach(t),NCo=r(sMe," \u2014 "),eI=n(sMe,"A",{href:!0});var oXr=s(eI);DCo=r(oXr,"BigBirdForMultipleChoice"),oXr.forEach(t),qCo=r(sMe," (BigBird model)"),sMe.forEach(t),GCo=i(O),b1=n(O,"LI",{});var lMe=s(b1);Nre=n(lMe,"STRONG",{});var rXr=s(Nre);OCo=r(rXr,"camembert"),rXr.forEach(t),XCo=r(lMe," \u2014 "),oI=n(lMe,"A",{href:!0});var tXr=s(oI);zCo=r(tXr,"CamembertForMultipleChoice"),tXr.forEach(t),VCo=r(lMe," (CamemBERT model)"),lMe.forEach(t),WCo=i(O),v1=n(O,"LI",{});var iMe=s(v1);Dre=n(iMe,"STRONG",{});var aXr=s(Dre);QCo=r(aXr,"canine"),aXr.forEach(t),HCo=r(iMe," \u2014 "),rI=n(iMe,"A",{href:!0});var nXr=s(rI);UCo=r(nXr,"CanineForMultipleChoice"),nXr.forEach(t),JCo=r(iMe," (Canine model)"),iMe.forEach(t),YCo=i(O),T1=n(O,"LI",{});var dMe=s(T1);qre=n(dMe,"STRONG",{});var sXr=s(qre);KCo=r(sXr,"convbert"),sXr.forEach(t),ZCo=r(dMe," \u2014 "),tI=n(dMe,"A",{href:!0});var lXr=s(tI);e4o=r(lXr,"ConvBertForMultipleChoice"),lXr.forEach(t),o4o=r(dMe," (ConvBERT model)"),dMe.forEach(t),r4o=i(O),F1=n(O,"LI",{});var cMe=s(F1);Gre=n(cMe,"STRONG",{});var iXr=s(Gre);t4o=r(iXr,"distilbert"),iXr.forEach(t),a4o=r(cMe," \u2014 "),aI=n(cMe,"A",{href:!0});var dXr=s(aI);n4o=r(dXr,"DistilBertForMultipleChoice"),dXr.forEach(t),s4o=r(cMe," (DistilBERT model)"),cMe.forEach(t),l4o=i(O),C1=n(O,"LI",{});var fMe=s(C1);Ore=n(fMe,"STRONG",{});var cXr=s(Ore);i4o=r(cXr,"electra"),cXr.forEach(t),d4o=r(fMe," \u2014 "),nI=n(fMe,"A",{href:!0});var fXr=s(nI);c4o=r(fXr,"ElectraForMultipleChoice"),fXr.forEach(t),f4o=r(fMe," (ELECTRA model)"),fMe.forEach(t),m4o=i(O),M1=n(O,"LI",{});var mMe=s(M1);Xre=n(mMe,"STRONG",{});var mXr=s(Xre);g4o=r(mXr,"flaubert"),mXr.forEach(t),h4o=r(mMe," \u2014 "),sI=n(mMe,"A",{href:!0});var gXr=s(sI);p4o=r(gXr,"FlaubertForMultipleChoice"),gXr.forEach(t),_4o=r(mMe," (FlauBERT model)"),mMe.forEach(t),u4o=i(O),E1=n(O,"LI",{});var gMe=s(E1);zre=n(gMe,"STRONG",{});var hXr=s(zre);b4o=r(hXr,"fnet"),hXr.forEach(t),v4o=r(gMe," \u2014 "),lI=n(gMe,"A",{href:!0});var pXr=s(lI);T4o=r(pXr,"FNetForMultipleChoice"),pXr.forEach(t),F4o=r(gMe," (FNet model)"),gMe.forEach(t),C4o=i(O),y1=n(O,"LI",{});var hMe=s(y1);Vre=n(hMe,"STRONG",{});var _Xr=s(Vre);M4o=r(_Xr,"funnel"),_Xr.forEach(t),E4o=r(hMe," \u2014 "),iI=n(hMe,"A",{href:!0});var uXr=s(iI);y4o=r(uXr,"FunnelForMultipleChoice"),uXr.forEach(t),w4o=r(hMe," (Funnel Transformer model)"),hMe.forEach(t),A4o=i(O),w1=n(O,"LI",{});var pMe=s(w1);Wre=n(pMe,"STRONG",{});var bXr=s(Wre);L4o=r(bXr,"ibert"),bXr.forEach(t),B4o=r(pMe," \u2014 "),dI=n(pMe,"A",{href:!0});var vXr=s(dI);k4o=r(vXr,"IBertForMultipleChoice"),vXr.forEach(t),x4o=r(pMe," (I-BERT model)"),pMe.forEach(t),R4o=i(O),A1=n(O,"LI",{});var _Me=s(A1);Qre=n(_Me,"STRONG",{});var TXr=s(Qre);S4o=r(TXr,"longformer"),TXr.forEach(t),P4o=r(_Me," \u2014 "),cI=n(_Me,"A",{href:!0});var FXr=s(cI);$4o=r(FXr,"LongformerForMultipleChoice"),FXr.forEach(t),I4o=r(_Me," (Longformer model)"),_Me.forEach(t),j4o=i(O),L1=n(O,"LI",{});var uMe=s(L1);Hre=n(uMe,"STRONG",{});var CXr=s(Hre);N4o=r(CXr,"megatron-bert"),CXr.forEach(t),D4o=r(uMe," \u2014 "),fI=n(uMe,"A",{href:!0});var MXr=s(fI);q4o=r(MXr,"MegatronBertForMultipleChoice"),MXr.forEach(t),G4o=r(uMe," (MegatronBert model)"),uMe.forEach(t),O4o=i(O),B1=n(O,"LI",{});var bMe=s(B1);Ure=n(bMe,"STRONG",{});var EXr=s(Ure);X4o=r(EXr,"mobilebert"),EXr.forEach(t),z4o=r(bMe," \u2014 "),mI=n(bMe,"A",{href:!0});var yXr=s(mI);V4o=r(yXr,"MobileBertForMultipleChoice"),yXr.forEach(t),W4o=r(bMe," (MobileBERT model)"),bMe.forEach(t),Q4o=i(O),k1=n(O,"LI",{});var vMe=s(k1);Jre=n(vMe,"STRONG",{});var wXr=s(Jre);H4o=r(wXr,"mpnet"),wXr.forEach(t),U4o=r(vMe," \u2014 "),gI=n(vMe,"A",{href:!0});var AXr=s(gI);J4o=r(AXr,"MPNetForMultipleChoice"),AXr.forEach(t),Y4o=r(vMe," (MPNet model)"),vMe.forEach(t),K4o=i(O),x1=n(O,"LI",{});var TMe=s(x1);Yre=n(TMe,"STRONG",{});var LXr=s(Yre);Z4o=r(LXr,"nystromformer"),LXr.forEach(t),eMo=r(TMe," \u2014 "),hI=n(TMe,"A",{href:!0});var BXr=s(hI);oMo=r(BXr,"NystromformerForMultipleChoice"),BXr.forEach(t),rMo=r(TMe," (Nystromformer model)"),TMe.forEach(t),tMo=i(O),R1=n(O,"LI",{});var FMe=s(R1);Kre=n(FMe,"STRONG",{});var kXr=s(Kre);aMo=r(kXr,"qdqbert"),kXr.forEach(t),nMo=r(FMe," \u2014 "),pI=n(FMe,"A",{href:!0});var xXr=s(pI);sMo=r(xXr,"QDQBertForMultipleChoice"),xXr.forEach(t),lMo=r(FMe," (QDQBert model)"),FMe.forEach(t),iMo=i(O),S1=n(O,"LI",{});var CMe=s(S1);Zre=n(CMe,"STRONG",{});var RXr=s(Zre);dMo=r(RXr,"rembert"),RXr.forEach(t),cMo=r(CMe," \u2014 "),_I=n(CMe,"A",{href:!0});var SXr=s(_I);fMo=r(SXr,"RemBertForMultipleChoice"),SXr.forEach(t),mMo=r(CMe," (RemBERT model)"),CMe.forEach(t),gMo=i(O),P1=n(O,"LI",{});var MMe=s(P1);ete=n(MMe,"STRONG",{});var PXr=s(ete);hMo=r(PXr,"roberta"),PXr.forEach(t),pMo=r(MMe," \u2014 "),uI=n(MMe,"A",{href:!0});var $Xr=s(uI);_Mo=r($Xr,"RobertaForMultipleChoice"),$Xr.forEach(t),uMo=r(MMe," (RoBERTa model)"),MMe.forEach(t),bMo=i(O),$1=n(O,"LI",{});var EMe=s($1);ote=n(EMe,"STRONG",{});var IXr=s(ote);vMo=r(IXr,"roformer"),IXr.forEach(t),TMo=r(EMe," \u2014 "),bI=n(EMe,"A",{href:!0});var jXr=s(bI);FMo=r(jXr,"RoFormerForMultipleChoice"),jXr.forEach(t),CMo=r(EMe," (RoFormer model)"),EMe.forEach(t),MMo=i(O),I1=n(O,"LI",{});var yMe=s(I1);rte=n(yMe,"STRONG",{});var NXr=s(rte);EMo=r(NXr,"squeezebert"),NXr.forEach(t),yMo=r(yMe," \u2014 "),vI=n(yMe,"A",{href:!0});var DXr=s(vI);wMo=r(DXr,"SqueezeBertForMultipleChoice"),DXr.forEach(t),AMo=r(yMe," (SqueezeBERT model)"),yMe.forEach(t),LMo=i(O),j1=n(O,"LI",{});var wMe=s(j1);tte=n(wMe,"STRONG",{});var qXr=s(tte);BMo=r(qXr,"xlm"),qXr.forEach(t),kMo=r(wMe," \u2014 "),TI=n(wMe,"A",{href:!0});var GXr=s(TI);xMo=r(GXr,"XLMForMultipleChoice"),GXr.forEach(t),RMo=r(wMe," (XLM model)"),wMe.forEach(t),SMo=i(O),N1=n(O,"LI",{});var AMe=s(N1);ate=n(AMe,"STRONG",{});var OXr=s(ate);PMo=r(OXr,"xlm-roberta"),OXr.forEach(t),$Mo=r(AMe," \u2014 "),FI=n(AMe,"A",{href:!0});var XXr=s(FI);IMo=r(XXr,"XLMRobertaForMultipleChoice"),XXr.forEach(t),jMo=r(AMe," (XLM-RoBERTa model)"),AMe.forEach(t),NMo=i(O),D1=n(O,"LI",{});var LMe=s(D1);nte=n(LMe,"STRONG",{});var zXr=s(nte);DMo=r(zXr,"xlm-roberta-xl"),zXr.forEach(t),qMo=r(LMe," \u2014 "),CI=n(LMe,"A",{href:!0});var VXr=s(CI);GMo=r(VXr,"XLMRobertaXLForMultipleChoice"),VXr.forEach(t),OMo=r(LMe," (XLM-RoBERTa-XL model)"),LMe.forEach(t),XMo=i(O),q1=n(O,"LI",{});var BMe=s(q1);ste=n(BMe,"STRONG",{});var WXr=s(ste);zMo=r(WXr,"xlnet"),WXr.forEach(t),VMo=r(BMe," \u2014 "),MI=n(BMe,"A",{href:!0});var QXr=s(MI);WMo=r(QXr,"XLNetForMultipleChoice"),QXr.forEach(t),QMo=r(BMe," (XLNet model)"),BMe.forEach(t),HMo=i(O),G1=n(O,"LI",{});var kMe=s(G1);lte=n(kMe,"STRONG",{});var HXr=s(lte);UMo=r(HXr,"yoso"),HXr.forEach(t),JMo=r(kMe," \u2014 "),EI=n(kMe,"A",{href:!0});var UXr=s(EI);YMo=r(UXr,"YosoForMultipleChoice"),UXr.forEach(t),KMo=r(kMe," (YOSO model)"),kMe.forEach(t),O.forEach(t),ZMo=i(Dt),O1=n(Dt,"P",{});var xMe=s(O1);eEo=r(xMe,"The model is set in evaluation mode by default using "),ite=n(xMe,"CODE",{});var JXr=s(ite);oEo=r(JXr,"model.eval()"),JXr.forEach(t),rEo=r(xMe,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),dte=n(xMe,"CODE",{});var YXr=s(dte);tEo=r(YXr,"model.train()"),YXr.forEach(t),xMe.forEach(t),aEo=i(Dt),cte=n(Dt,"P",{});var KXr=s(cte);nEo=r(KXr,"Examples:"),KXr.forEach(t),sEo=i(Dt),m(kE.$$.fragment,Dt),Dt.forEach(t),Ys.forEach(t),$Le=i(d),id=n(d,"H2",{class:!0});var OBe=s(id);X1=n(OBe,"A",{id:!0,class:!0,href:!0});var ZXr=s(X1);fte=n(ZXr,"SPAN",{});var ezr=s(fte);m(xE.$$.fragment,ezr),ezr.forEach(t),ZXr.forEach(t),lEo=i(OBe),mte=n(OBe,"SPAN",{});var ozr=s(mte);iEo=r(ozr,"AutoModelForNextSentencePrediction"),ozr.forEach(t),OBe.forEach(t),ILe=i(d),Ko=n(d,"DIV",{class:!0});var Zs=s(Ko);m(RE.$$.fragment,Zs),dEo=i(Zs),dd=n(Zs,"P",{});var cz=s(dd);cEo=r(cz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a next sentence prediction head) when created
with the `),gte=n(cz,"CODE",{});var rzr=s(gte);fEo=r(rzr,"from_pretrained()"),rzr.forEach(t),mEo=r(cz,"class method or the "),hte=n(cz,"CODE",{});var tzr=s(hte);gEo=r(tzr,"from_config()"),tzr.forEach(t),hEo=r(cz,`class
method.`),cz.forEach(t),pEo=i(Zs),SE=n(Zs,"P",{});var XBe=s(SE);_Eo=r(XBe,"This class cannot be instantiated directly using "),pte=n(XBe,"CODE",{});var azr=s(pte);uEo=r(azr,"__init__()"),azr.forEach(t),bEo=r(XBe," (throws an error)."),XBe.forEach(t),vEo=i(Zs),Vr=n(Zs,"DIV",{class:!0});var el=s(Vr);m(PE.$$.fragment,el),TEo=i(el),_te=n(el,"P",{});var nzr=s(_te);FEo=r(nzr,"Instantiates one of the model classes of the library (with a next sentence prediction head) from a configuration."),nzr.forEach(t),CEo=i(el),cd=n(el,"P",{});var fz=s(cd);MEo=r(fz,`Note:
Loading a model from its configuration file does `),ute=n(fz,"STRONG",{});var szr=s(ute);EEo=r(szr,"not"),szr.forEach(t),yEo=r(fz,` load the model weights. It only affects the
model\u2019s configuration. Use `),bte=n(fz,"CODE",{});var lzr=s(bte);wEo=r(lzr,"from_pretrained()"),lzr.forEach(t),AEo=r(fz,"to load the model weights."),fz.forEach(t),LEo=i(el),vte=n(el,"P",{});var izr=s(vte);BEo=r(izr,"Examples:"),izr.forEach(t),kEo=i(el),m($E.$$.fragment,el),el.forEach(t),xEo=i(Zs),je=n(Zs,"DIV",{class:!0});var qt=s(je);m(IE.$$.fragment,qt),REo=i(qt),Tte=n(qt,"P",{});var dzr=s(Tte);SEo=r(dzr,"Instantiate one of the model classes of the library (with a next sentence prediction head) from a pretrained model."),dzr.forEach(t),PEo=i(qt),Wa=n(qt,"P",{});var p4=s(Wa);$Eo=r(p4,"The model class to instantiate is selected based on the "),Fte=n(p4,"CODE",{});var czr=s(Fte);IEo=r(czr,"model_type"),czr.forEach(t),jEo=r(p4,` property of the config object (either
passed as an argument or loaded from `),Cte=n(p4,"CODE",{});var fzr=s(Cte);NEo=r(fzr,"pretrained_model_name_or_path"),fzr.forEach(t),DEo=r(p4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Mte=n(p4,"CODE",{});var mzr=s(Mte);qEo=r(mzr,"pretrained_model_name_or_path"),mzr.forEach(t),GEo=r(p4,":"),p4.forEach(t),OEo=i(qt),na=n(qt,"UL",{});var ol=s(na);z1=n(ol,"LI",{});var RMe=s(z1);Ete=n(RMe,"STRONG",{});var gzr=s(Ete);XEo=r(gzr,"bert"),gzr.forEach(t),zEo=r(RMe," \u2014 "),yI=n(RMe,"A",{href:!0});var hzr=s(yI);VEo=r(hzr,"BertForNextSentencePrediction"),hzr.forEach(t),WEo=r(RMe," (BERT model)"),RMe.forEach(t),QEo=i(ol),V1=n(ol,"LI",{});var SMe=s(V1);yte=n(SMe,"STRONG",{});var pzr=s(yte);HEo=r(pzr,"fnet"),pzr.forEach(t),UEo=r(SMe," \u2014 "),wI=n(SMe,"A",{href:!0});var _zr=s(wI);JEo=r(_zr,"FNetForNextSentencePrediction"),_zr.forEach(t),YEo=r(SMe," (FNet model)"),SMe.forEach(t),KEo=i(ol),W1=n(ol,"LI",{});var PMe=s(W1);wte=n(PMe,"STRONG",{});var uzr=s(wte);ZEo=r(uzr,"megatron-bert"),uzr.forEach(t),e3o=r(PMe," \u2014 "),AI=n(PMe,"A",{href:!0});var bzr=s(AI);o3o=r(bzr,"MegatronBertForNextSentencePrediction"),bzr.forEach(t),r3o=r(PMe," (MegatronBert model)"),PMe.forEach(t),t3o=i(ol),Q1=n(ol,"LI",{});var $Me=s(Q1);Ate=n($Me,"STRONG",{});var vzr=s(Ate);a3o=r(vzr,"mobilebert"),vzr.forEach(t),n3o=r($Me," \u2014 "),LI=n($Me,"A",{href:!0});var Tzr=s(LI);s3o=r(Tzr,"MobileBertForNextSentencePrediction"),Tzr.forEach(t),l3o=r($Me," (MobileBERT model)"),$Me.forEach(t),i3o=i(ol),H1=n(ol,"LI",{});var IMe=s(H1);Lte=n(IMe,"STRONG",{});var Fzr=s(Lte);d3o=r(Fzr,"qdqbert"),Fzr.forEach(t),c3o=r(IMe," \u2014 "),BI=n(IMe,"A",{href:!0});var Czr=s(BI);f3o=r(Czr,"QDQBertForNextSentencePrediction"),Czr.forEach(t),m3o=r(IMe," (QDQBert model)"),IMe.forEach(t),ol.forEach(t),g3o=i(qt),U1=n(qt,"P",{});var jMe=s(U1);h3o=r(jMe,"The model is set in evaluation mode by default using "),Bte=n(jMe,"CODE",{});var Mzr=s(Bte);p3o=r(Mzr,"model.eval()"),Mzr.forEach(t),_3o=r(jMe,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),kte=n(jMe,"CODE",{});var Ezr=s(kte);u3o=r(Ezr,"model.train()"),Ezr.forEach(t),jMe.forEach(t),b3o=i(qt),xte=n(qt,"P",{});var yzr=s(xte);v3o=r(yzr,"Examples:"),yzr.forEach(t),T3o=i(qt),m(jE.$$.fragment,qt),qt.forEach(t),Zs.forEach(t),jLe=i(d),fd=n(d,"H2",{class:!0});var zBe=s(fd);J1=n(zBe,"A",{id:!0,class:!0,href:!0});var wzr=s(J1);Rte=n(wzr,"SPAN",{});var Azr=s(Rte);m(NE.$$.fragment,Azr),Azr.forEach(t),wzr.forEach(t),F3o=i(zBe),Ste=n(zBe,"SPAN",{});var Lzr=s(Ste);C3o=r(Lzr,"AutoModelForTokenClassification"),Lzr.forEach(t),zBe.forEach(t),NLe=i(d),Zo=n(d,"DIV",{class:!0});var rl=s(Zo);m(DE.$$.fragment,rl),M3o=i(rl),md=n(rl,"P",{});var mz=s(md);E3o=r(mz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a token classification head) when created
with the `),Pte=n(mz,"CODE",{});var Bzr=s(Pte);y3o=r(Bzr,"from_pretrained()"),Bzr.forEach(t),w3o=r(mz,"class method or the "),$te=n(mz,"CODE",{});var kzr=s($te);A3o=r(kzr,"from_config()"),kzr.forEach(t),L3o=r(mz,`class
method.`),mz.forEach(t),B3o=i(rl),qE=n(rl,"P",{});var VBe=s(qE);k3o=r(VBe,"This class cannot be instantiated directly using "),Ite=n(VBe,"CODE",{});var xzr=s(Ite);x3o=r(xzr,"__init__()"),xzr.forEach(t),R3o=r(VBe," (throws an error)."),VBe.forEach(t),S3o=i(rl),Wr=n(rl,"DIV",{class:!0});var tl=s(Wr);m(GE.$$.fragment,tl),P3o=i(tl),jte=n(tl,"P",{});var Rzr=s(jte);$3o=r(Rzr,"Instantiates one of the model classes of the library (with a token classification head) from a configuration."),Rzr.forEach(t),I3o=i(tl),gd=n(tl,"P",{});var gz=s(gd);j3o=r(gz,`Note:
Loading a model from its configuration file does `),Nte=n(gz,"STRONG",{});var Szr=s(Nte);N3o=r(Szr,"not"),Szr.forEach(t),D3o=r(gz,` load the model weights. It only affects the
model\u2019s configuration. Use `),Dte=n(gz,"CODE",{});var Pzr=s(Dte);q3o=r(Pzr,"from_pretrained()"),Pzr.forEach(t),G3o=r(gz,"to load the model weights."),gz.forEach(t),O3o=i(tl),qte=n(tl,"P",{});var $zr=s(qte);X3o=r($zr,"Examples:"),$zr.forEach(t),z3o=i(tl),m(OE.$$.fragment,tl),tl.forEach(t),V3o=i(rl),Ne=n(rl,"DIV",{class:!0});var Gt=s(Ne);m(XE.$$.fragment,Gt),W3o=i(Gt),Gte=n(Gt,"P",{});var Izr=s(Gte);Q3o=r(Izr,"Instantiate one of the model classes of the library (with a token classification head) from a pretrained model."),Izr.forEach(t),H3o=i(Gt),Qa=n(Gt,"P",{});var _4=s(Qa);U3o=r(_4,"The model class to instantiate is selected based on the "),Ote=n(_4,"CODE",{});var jzr=s(Ote);J3o=r(jzr,"model_type"),jzr.forEach(t),Y3o=r(_4,` property of the config object (either
passed as an argument or loaded from `),Xte=n(_4,"CODE",{});var Nzr=s(Xte);K3o=r(Nzr,"pretrained_model_name_or_path"),Nzr.forEach(t),Z3o=r(_4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),zte=n(_4,"CODE",{});var Dzr=s(zte);eyo=r(Dzr,"pretrained_model_name_or_path"),Dzr.forEach(t),oyo=r(_4,":"),_4.forEach(t),ryo=i(Gt),D=n(Gt,"UL",{});var q=s(D);Y1=n(q,"LI",{});var NMe=s(Y1);Vte=n(NMe,"STRONG",{});var qzr=s(Vte);tyo=r(qzr,"albert"),qzr.forEach(t),ayo=r(NMe," \u2014 "),kI=n(NMe,"A",{href:!0});var Gzr=s(kI);nyo=r(Gzr,"AlbertForTokenClassification"),Gzr.forEach(t),syo=r(NMe," (ALBERT model)"),NMe.forEach(t),lyo=i(q),K1=n(q,"LI",{});var DMe=s(K1);Wte=n(DMe,"STRONG",{});var Ozr=s(Wte);iyo=r(Ozr,"bert"),Ozr.forEach(t),dyo=r(DMe," \u2014 "),xI=n(DMe,"A",{href:!0});var Xzr=s(xI);cyo=r(Xzr,"BertForTokenClassification"),Xzr.forEach(t),fyo=r(DMe," (BERT model)"),DMe.forEach(t),myo=i(q),Z1=n(q,"LI",{});var qMe=s(Z1);Qte=n(qMe,"STRONG",{});var zzr=s(Qte);gyo=r(zzr,"big_bird"),zzr.forEach(t),hyo=r(qMe," \u2014 "),RI=n(qMe,"A",{href:!0});var Vzr=s(RI);pyo=r(Vzr,"BigBirdForTokenClassification"),Vzr.forEach(t),_yo=r(qMe," (BigBird model)"),qMe.forEach(t),uyo=i(q),eb=n(q,"LI",{});var GMe=s(eb);Hte=n(GMe,"STRONG",{});var Wzr=s(Hte);byo=r(Wzr,"camembert"),Wzr.forEach(t),vyo=r(GMe," \u2014 "),SI=n(GMe,"A",{href:!0});var Qzr=s(SI);Tyo=r(Qzr,"CamembertForTokenClassification"),Qzr.forEach(t),Fyo=r(GMe," (CamemBERT model)"),GMe.forEach(t),Cyo=i(q),ob=n(q,"LI",{});var OMe=s(ob);Ute=n(OMe,"STRONG",{});var Hzr=s(Ute);Myo=r(Hzr,"canine"),Hzr.forEach(t),Eyo=r(OMe," \u2014 "),PI=n(OMe,"A",{href:!0});var Uzr=s(PI);yyo=r(Uzr,"CanineForTokenClassification"),Uzr.forEach(t),wyo=r(OMe," (Canine model)"),OMe.forEach(t),Ayo=i(q),rb=n(q,"LI",{});var XMe=s(rb);Jte=n(XMe,"STRONG",{});var Jzr=s(Jte);Lyo=r(Jzr,"convbert"),Jzr.forEach(t),Byo=r(XMe," \u2014 "),$I=n(XMe,"A",{href:!0});var Yzr=s($I);kyo=r(Yzr,"ConvBertForTokenClassification"),Yzr.forEach(t),xyo=r(XMe," (ConvBERT model)"),XMe.forEach(t),Ryo=i(q),tb=n(q,"LI",{});var zMe=s(tb);Yte=n(zMe,"STRONG",{});var Kzr=s(Yte);Syo=r(Kzr,"deberta"),Kzr.forEach(t),Pyo=r(zMe," \u2014 "),II=n(zMe,"A",{href:!0});var Zzr=s(II);$yo=r(Zzr,"DebertaForTokenClassification"),Zzr.forEach(t),Iyo=r(zMe," (DeBERTa model)"),zMe.forEach(t),jyo=i(q),ab=n(q,"LI",{});var VMe=s(ab);Kte=n(VMe,"STRONG",{});var eVr=s(Kte);Nyo=r(eVr,"deberta-v2"),eVr.forEach(t),Dyo=r(VMe," \u2014 "),jI=n(VMe,"A",{href:!0});var oVr=s(jI);qyo=r(oVr,"DebertaV2ForTokenClassification"),oVr.forEach(t),Gyo=r(VMe," (DeBERTa-v2 model)"),VMe.forEach(t),Oyo=i(q),nb=n(q,"LI",{});var WMe=s(nb);Zte=n(WMe,"STRONG",{});var rVr=s(Zte);Xyo=r(rVr,"distilbert"),rVr.forEach(t),zyo=r(WMe," \u2014 "),NI=n(WMe,"A",{href:!0});var tVr=s(NI);Vyo=r(tVr,"DistilBertForTokenClassification"),tVr.forEach(t),Wyo=r(WMe," (DistilBERT model)"),WMe.forEach(t),Qyo=i(q),sb=n(q,"LI",{});var QMe=s(sb);eae=n(QMe,"STRONG",{});var aVr=s(eae);Hyo=r(aVr,"electra"),aVr.forEach(t),Uyo=r(QMe," \u2014 "),DI=n(QMe,"A",{href:!0});var nVr=s(DI);Jyo=r(nVr,"ElectraForTokenClassification"),nVr.forEach(t),Yyo=r(QMe," (ELECTRA model)"),QMe.forEach(t),Kyo=i(q),lb=n(q,"LI",{});var HMe=s(lb);oae=n(HMe,"STRONG",{});var sVr=s(oae);Zyo=r(sVr,"flaubert"),sVr.forEach(t),ewo=r(HMe," \u2014 "),qI=n(HMe,"A",{href:!0});var lVr=s(qI);owo=r(lVr,"FlaubertForTokenClassification"),lVr.forEach(t),rwo=r(HMe," (FlauBERT model)"),HMe.forEach(t),two=i(q),ib=n(q,"LI",{});var UMe=s(ib);rae=n(UMe,"STRONG",{});var iVr=s(rae);awo=r(iVr,"fnet"),iVr.forEach(t),nwo=r(UMe," \u2014 "),GI=n(UMe,"A",{href:!0});var dVr=s(GI);swo=r(dVr,"FNetForTokenClassification"),dVr.forEach(t),lwo=r(UMe," (FNet model)"),UMe.forEach(t),iwo=i(q),db=n(q,"LI",{});var JMe=s(db);tae=n(JMe,"STRONG",{});var cVr=s(tae);dwo=r(cVr,"funnel"),cVr.forEach(t),cwo=r(JMe," \u2014 "),OI=n(JMe,"A",{href:!0});var fVr=s(OI);fwo=r(fVr,"FunnelForTokenClassification"),fVr.forEach(t),mwo=r(JMe," (Funnel Transformer model)"),JMe.forEach(t),gwo=i(q),cb=n(q,"LI",{});var YMe=s(cb);aae=n(YMe,"STRONG",{});var mVr=s(aae);hwo=r(mVr,"gpt2"),mVr.forEach(t),pwo=r(YMe," \u2014 "),XI=n(YMe,"A",{href:!0});var gVr=s(XI);_wo=r(gVr,"GPT2ForTokenClassification"),gVr.forEach(t),uwo=r(YMe," (OpenAI GPT-2 model)"),YMe.forEach(t),bwo=i(q),fb=n(q,"LI",{});var KMe=s(fb);nae=n(KMe,"STRONG",{});var hVr=s(nae);vwo=r(hVr,"ibert"),hVr.forEach(t),Two=r(KMe," \u2014 "),zI=n(KMe,"A",{href:!0});var pVr=s(zI);Fwo=r(pVr,"IBertForTokenClassification"),pVr.forEach(t),Cwo=r(KMe," (I-BERT model)"),KMe.forEach(t),Mwo=i(q),mb=n(q,"LI",{});var ZMe=s(mb);sae=n(ZMe,"STRONG",{});var _Vr=s(sae);Ewo=r(_Vr,"layoutlm"),_Vr.forEach(t),ywo=r(ZMe," \u2014 "),VI=n(ZMe,"A",{href:!0});var uVr=s(VI);wwo=r(uVr,"LayoutLMForTokenClassification"),uVr.forEach(t),Awo=r(ZMe," (LayoutLM model)"),ZMe.forEach(t),Lwo=i(q),gb=n(q,"LI",{});var eEe=s(gb);lae=n(eEe,"STRONG",{});var bVr=s(lae);Bwo=r(bVr,"layoutlmv2"),bVr.forEach(t),kwo=r(eEe," \u2014 "),WI=n(eEe,"A",{href:!0});var vVr=s(WI);xwo=r(vVr,"LayoutLMv2ForTokenClassification"),vVr.forEach(t),Rwo=r(eEe," (LayoutLMv2 model)"),eEe.forEach(t),Swo=i(q),hb=n(q,"LI",{});var oEe=s(hb);iae=n(oEe,"STRONG",{});var TVr=s(iae);Pwo=r(TVr,"longformer"),TVr.forEach(t),$wo=r(oEe," \u2014 "),QI=n(oEe,"A",{href:!0});var FVr=s(QI);Iwo=r(FVr,"LongformerForTokenClassification"),FVr.forEach(t),jwo=r(oEe," (Longformer model)"),oEe.forEach(t),Nwo=i(q),pb=n(q,"LI",{});var rEe=s(pb);dae=n(rEe,"STRONG",{});var CVr=s(dae);Dwo=r(CVr,"megatron-bert"),CVr.forEach(t),qwo=r(rEe," \u2014 "),HI=n(rEe,"A",{href:!0});var MVr=s(HI);Gwo=r(MVr,"MegatronBertForTokenClassification"),MVr.forEach(t),Owo=r(rEe," (MegatronBert model)"),rEe.forEach(t),Xwo=i(q),_b=n(q,"LI",{});var tEe=s(_b);cae=n(tEe,"STRONG",{});var EVr=s(cae);zwo=r(EVr,"mobilebert"),EVr.forEach(t),Vwo=r(tEe," \u2014 "),UI=n(tEe,"A",{href:!0});var yVr=s(UI);Wwo=r(yVr,"MobileBertForTokenClassification"),yVr.forEach(t),Qwo=r(tEe," (MobileBERT model)"),tEe.forEach(t),Hwo=i(q),ub=n(q,"LI",{});var aEe=s(ub);fae=n(aEe,"STRONG",{});var wVr=s(fae);Uwo=r(wVr,"mpnet"),wVr.forEach(t),Jwo=r(aEe," \u2014 "),JI=n(aEe,"A",{href:!0});var AVr=s(JI);Ywo=r(AVr,"MPNetForTokenClassification"),AVr.forEach(t),Kwo=r(aEe," (MPNet model)"),aEe.forEach(t),Zwo=i(q),bb=n(q,"LI",{});var nEe=s(bb);mae=n(nEe,"STRONG",{});var LVr=s(mae);eAo=r(LVr,"nystromformer"),LVr.forEach(t),oAo=r(nEe," \u2014 "),YI=n(nEe,"A",{href:!0});var BVr=s(YI);rAo=r(BVr,"NystromformerForTokenClassification"),BVr.forEach(t),tAo=r(nEe," (Nystromformer model)"),nEe.forEach(t),aAo=i(q),vb=n(q,"LI",{});var sEe=s(vb);gae=n(sEe,"STRONG",{});var kVr=s(gae);nAo=r(kVr,"qdqbert"),kVr.forEach(t),sAo=r(sEe," \u2014 "),KI=n(sEe,"A",{href:!0});var xVr=s(KI);lAo=r(xVr,"QDQBertForTokenClassification"),xVr.forEach(t),iAo=r(sEe," (QDQBert model)"),sEe.forEach(t),dAo=i(q),Tb=n(q,"LI",{});var lEe=s(Tb);hae=n(lEe,"STRONG",{});var RVr=s(hae);cAo=r(RVr,"rembert"),RVr.forEach(t),fAo=r(lEe," \u2014 "),ZI=n(lEe,"A",{href:!0});var SVr=s(ZI);mAo=r(SVr,"RemBertForTokenClassification"),SVr.forEach(t),gAo=r(lEe," (RemBERT model)"),lEe.forEach(t),hAo=i(q),Fb=n(q,"LI",{});var iEe=s(Fb);pae=n(iEe,"STRONG",{});var PVr=s(pae);pAo=r(PVr,"roberta"),PVr.forEach(t),_Ao=r(iEe," \u2014 "),ej=n(iEe,"A",{href:!0});var $Vr=s(ej);uAo=r($Vr,"RobertaForTokenClassification"),$Vr.forEach(t),bAo=r(iEe," (RoBERTa model)"),iEe.forEach(t),vAo=i(q),Cb=n(q,"LI",{});var dEe=s(Cb);_ae=n(dEe,"STRONG",{});var IVr=s(_ae);TAo=r(IVr,"roformer"),IVr.forEach(t),FAo=r(dEe," \u2014 "),oj=n(dEe,"A",{href:!0});var jVr=s(oj);CAo=r(jVr,"RoFormerForTokenClassification"),jVr.forEach(t),MAo=r(dEe," (RoFormer model)"),dEe.forEach(t),EAo=i(q),Mb=n(q,"LI",{});var cEe=s(Mb);uae=n(cEe,"STRONG",{});var NVr=s(uae);yAo=r(NVr,"squeezebert"),NVr.forEach(t),wAo=r(cEe," \u2014 "),rj=n(cEe,"A",{href:!0});var DVr=s(rj);AAo=r(DVr,"SqueezeBertForTokenClassification"),DVr.forEach(t),LAo=r(cEe," (SqueezeBERT model)"),cEe.forEach(t),BAo=i(q),Eb=n(q,"LI",{});var fEe=s(Eb);bae=n(fEe,"STRONG",{});var qVr=s(bae);kAo=r(qVr,"xlm"),qVr.forEach(t),xAo=r(fEe," \u2014 "),tj=n(fEe,"A",{href:!0});var GVr=s(tj);RAo=r(GVr,"XLMForTokenClassification"),GVr.forEach(t),SAo=r(fEe," (XLM model)"),fEe.forEach(t),PAo=i(q),yb=n(q,"LI",{});var mEe=s(yb);vae=n(mEe,"STRONG",{});var OVr=s(vae);$Ao=r(OVr,"xlm-roberta"),OVr.forEach(t),IAo=r(mEe," \u2014 "),aj=n(mEe,"A",{href:!0});var XVr=s(aj);jAo=r(XVr,"XLMRobertaForTokenClassification"),XVr.forEach(t),NAo=r(mEe," (XLM-RoBERTa model)"),mEe.forEach(t),DAo=i(q),wb=n(q,"LI",{});var gEe=s(wb);Tae=n(gEe,"STRONG",{});var zVr=s(Tae);qAo=r(zVr,"xlm-roberta-xl"),zVr.forEach(t),GAo=r(gEe," \u2014 "),nj=n(gEe,"A",{href:!0});var VVr=s(nj);OAo=r(VVr,"XLMRobertaXLForTokenClassification"),VVr.forEach(t),XAo=r(gEe," (XLM-RoBERTa-XL model)"),gEe.forEach(t),zAo=i(q),Ab=n(q,"LI",{});var hEe=s(Ab);Fae=n(hEe,"STRONG",{});var WVr=s(Fae);VAo=r(WVr,"xlnet"),WVr.forEach(t),WAo=r(hEe," \u2014 "),sj=n(hEe,"A",{href:!0});var QVr=s(sj);QAo=r(QVr,"XLNetForTokenClassification"),QVr.forEach(t),HAo=r(hEe," (XLNet model)"),hEe.forEach(t),UAo=i(q),Lb=n(q,"LI",{});var pEe=s(Lb);Cae=n(pEe,"STRONG",{});var HVr=s(Cae);JAo=r(HVr,"yoso"),HVr.forEach(t),YAo=r(pEe," \u2014 "),lj=n(pEe,"A",{href:!0});var UVr=s(lj);KAo=r(UVr,"YosoForTokenClassification"),UVr.forEach(t),ZAo=r(pEe," (YOSO model)"),pEe.forEach(t),q.forEach(t),e6o=i(Gt),Bb=n(Gt,"P",{});var _Ee=s(Bb);o6o=r(_Ee,"The model is set in evaluation mode by default using "),Mae=n(_Ee,"CODE",{});var JVr=s(Mae);r6o=r(JVr,"model.eval()"),JVr.forEach(t),t6o=r(_Ee,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Eae=n(_Ee,"CODE",{});var YVr=s(Eae);a6o=r(YVr,"model.train()"),YVr.forEach(t),_Ee.forEach(t),n6o=i(Gt),yae=n(Gt,"P",{});var KVr=s(yae);s6o=r(KVr,"Examples:"),KVr.forEach(t),l6o=i(Gt),m(zE.$$.fragment,Gt),Gt.forEach(t),rl.forEach(t),DLe=i(d),hd=n(d,"H2",{class:!0});var WBe=s(hd);kb=n(WBe,"A",{id:!0,class:!0,href:!0});var ZVr=s(kb);wae=n(ZVr,"SPAN",{});var eWr=s(wae);m(VE.$$.fragment,eWr),eWr.forEach(t),ZVr.forEach(t),i6o=i(WBe),Aae=n(WBe,"SPAN",{});var oWr=s(Aae);d6o=r(oWr,"AutoModelForQuestionAnswering"),oWr.forEach(t),WBe.forEach(t),qLe=i(d),er=n(d,"DIV",{class:!0});var al=s(er);m(WE.$$.fragment,al),c6o=i(al),pd=n(al,"P",{});var hz=s(pd);f6o=r(hz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a question answering head) when created
with the `),Lae=n(hz,"CODE",{});var rWr=s(Lae);m6o=r(rWr,"from_pretrained()"),rWr.forEach(t),g6o=r(hz,"class method or the "),Bae=n(hz,"CODE",{});var tWr=s(Bae);h6o=r(tWr,"from_config()"),tWr.forEach(t),p6o=r(hz,`class
method.`),hz.forEach(t),_6o=i(al),QE=n(al,"P",{});var QBe=s(QE);u6o=r(QBe,"This class cannot be instantiated directly using "),kae=n(QBe,"CODE",{});var aWr=s(kae);b6o=r(aWr,"__init__()"),aWr.forEach(t),v6o=r(QBe," (throws an error)."),QBe.forEach(t),T6o=i(al),Qr=n(al,"DIV",{class:!0});var nl=s(Qr);m(HE.$$.fragment,nl),F6o=i(nl),xae=n(nl,"P",{});var nWr=s(xae);C6o=r(nWr,"Instantiates one of the model classes of the library (with a question answering head) from a configuration."),nWr.forEach(t),M6o=i(nl),_d=n(nl,"P",{});var pz=s(_d);E6o=r(pz,`Note:
Loading a model from its configuration file does `),Rae=n(pz,"STRONG",{});var sWr=s(Rae);y6o=r(sWr,"not"),sWr.forEach(t),w6o=r(pz,` load the model weights. It only affects the
model\u2019s configuration. Use `),Sae=n(pz,"CODE",{});var lWr=s(Sae);A6o=r(lWr,"from_pretrained()"),lWr.forEach(t),L6o=r(pz,"to load the model weights."),pz.forEach(t),B6o=i(nl),Pae=n(nl,"P",{});var iWr=s(Pae);k6o=r(iWr,"Examples:"),iWr.forEach(t),x6o=i(nl),m(UE.$$.fragment,nl),nl.forEach(t),R6o=i(al),De=n(al,"DIV",{class:!0});var Ot=s(De);m(JE.$$.fragment,Ot),S6o=i(Ot),$ae=n(Ot,"P",{});var dWr=s($ae);P6o=r(dWr,"Instantiate one of the model classes of the library (with a question answering head) from a pretrained model."),dWr.forEach(t),$6o=i(Ot),Ha=n(Ot,"P",{});var u4=s(Ha);I6o=r(u4,"The model class to instantiate is selected based on the "),Iae=n(u4,"CODE",{});var cWr=s(Iae);j6o=r(cWr,"model_type"),cWr.forEach(t),N6o=r(u4,` property of the config object (either
passed as an argument or loaded from `),jae=n(u4,"CODE",{});var fWr=s(jae);D6o=r(fWr,"pretrained_model_name_or_path"),fWr.forEach(t),q6o=r(u4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Nae=n(u4,"CODE",{});var mWr=s(Nae);G6o=r(mWr,"pretrained_model_name_or_path"),mWr.forEach(t),O6o=r(u4,":"),u4.forEach(t),X6o=i(Ot),R=n(Ot,"UL",{});var P=s(R);xb=n(P,"LI",{});var uEe=s(xb);Dae=n(uEe,"STRONG",{});var gWr=s(Dae);z6o=r(gWr,"albert"),gWr.forEach(t),V6o=r(uEe," \u2014 "),ij=n(uEe,"A",{href:!0});var hWr=s(ij);W6o=r(hWr,"AlbertForQuestionAnswering"),hWr.forEach(t),Q6o=r(uEe," (ALBERT model)"),uEe.forEach(t),H6o=i(P),Rb=n(P,"LI",{});var bEe=s(Rb);qae=n(bEe,"STRONG",{});var pWr=s(qae);U6o=r(pWr,"bart"),pWr.forEach(t),J6o=r(bEe," \u2014 "),dj=n(bEe,"A",{href:!0});var _Wr=s(dj);Y6o=r(_Wr,"BartForQuestionAnswering"),_Wr.forEach(t),K6o=r(bEe," (BART model)"),bEe.forEach(t),Z6o=i(P),Sb=n(P,"LI",{});var vEe=s(Sb);Gae=n(vEe,"STRONG",{});var uWr=s(Gae);e0o=r(uWr,"bert"),uWr.forEach(t),o0o=r(vEe," \u2014 "),cj=n(vEe,"A",{href:!0});var bWr=s(cj);r0o=r(bWr,"BertForQuestionAnswering"),bWr.forEach(t),t0o=r(vEe," (BERT model)"),vEe.forEach(t),a0o=i(P),Pb=n(P,"LI",{});var TEe=s(Pb);Oae=n(TEe,"STRONG",{});var vWr=s(Oae);n0o=r(vWr,"big_bird"),vWr.forEach(t),s0o=r(TEe," \u2014 "),fj=n(TEe,"A",{href:!0});var TWr=s(fj);l0o=r(TWr,"BigBirdForQuestionAnswering"),TWr.forEach(t),i0o=r(TEe," (BigBird model)"),TEe.forEach(t),d0o=i(P),$b=n(P,"LI",{});var FEe=s($b);Xae=n(FEe,"STRONG",{});var FWr=s(Xae);c0o=r(FWr,"bigbird_pegasus"),FWr.forEach(t),f0o=r(FEe," \u2014 "),mj=n(FEe,"A",{href:!0});var CWr=s(mj);m0o=r(CWr,"BigBirdPegasusForQuestionAnswering"),CWr.forEach(t),g0o=r(FEe," (BigBirdPegasus model)"),FEe.forEach(t),h0o=i(P),Ib=n(P,"LI",{});var CEe=s(Ib);zae=n(CEe,"STRONG",{});var MWr=s(zae);p0o=r(MWr,"camembert"),MWr.forEach(t),_0o=r(CEe," \u2014 "),gj=n(CEe,"A",{href:!0});var EWr=s(gj);u0o=r(EWr,"CamembertForQuestionAnswering"),EWr.forEach(t),b0o=r(CEe," (CamemBERT model)"),CEe.forEach(t),v0o=i(P),jb=n(P,"LI",{});var MEe=s(jb);Vae=n(MEe,"STRONG",{});var yWr=s(Vae);T0o=r(yWr,"canine"),yWr.forEach(t),F0o=r(MEe," \u2014 "),hj=n(MEe,"A",{href:!0});var wWr=s(hj);C0o=r(wWr,"CanineForQuestionAnswering"),wWr.forEach(t),M0o=r(MEe," (Canine model)"),MEe.forEach(t),E0o=i(P),Nb=n(P,"LI",{});var EEe=s(Nb);Wae=n(EEe,"STRONG",{});var AWr=s(Wae);y0o=r(AWr,"convbert"),AWr.forEach(t),w0o=r(EEe," \u2014 "),pj=n(EEe,"A",{href:!0});var LWr=s(pj);A0o=r(LWr,"ConvBertForQuestionAnswering"),LWr.forEach(t),L0o=r(EEe," (ConvBERT model)"),EEe.forEach(t),B0o=i(P),Db=n(P,"LI",{});var yEe=s(Db);Qae=n(yEe,"STRONG",{});var BWr=s(Qae);k0o=r(BWr,"deberta"),BWr.forEach(t),x0o=r(yEe," \u2014 "),_j=n(yEe,"A",{href:!0});var kWr=s(_j);R0o=r(kWr,"DebertaForQuestionAnswering"),kWr.forEach(t),S0o=r(yEe," (DeBERTa model)"),yEe.forEach(t),P0o=i(P),qb=n(P,"LI",{});var wEe=s(qb);Hae=n(wEe,"STRONG",{});var xWr=s(Hae);$0o=r(xWr,"deberta-v2"),xWr.forEach(t),I0o=r(wEe," \u2014 "),uj=n(wEe,"A",{href:!0});var RWr=s(uj);j0o=r(RWr,"DebertaV2ForQuestionAnswering"),RWr.forEach(t),N0o=r(wEe," (DeBERTa-v2 model)"),wEe.forEach(t),D0o=i(P),Gb=n(P,"LI",{});var AEe=s(Gb);Uae=n(AEe,"STRONG",{});var SWr=s(Uae);q0o=r(SWr,"distilbert"),SWr.forEach(t),G0o=r(AEe," \u2014 "),bj=n(AEe,"A",{href:!0});var PWr=s(bj);O0o=r(PWr,"DistilBertForQuestionAnswering"),PWr.forEach(t),X0o=r(AEe," (DistilBERT model)"),AEe.forEach(t),z0o=i(P),Ob=n(P,"LI",{});var LEe=s(Ob);Jae=n(LEe,"STRONG",{});var $Wr=s(Jae);V0o=r($Wr,"electra"),$Wr.forEach(t),W0o=r(LEe," \u2014 "),vj=n(LEe,"A",{href:!0});var IWr=s(vj);Q0o=r(IWr,"ElectraForQuestionAnswering"),IWr.forEach(t),H0o=r(LEe," (ELECTRA model)"),LEe.forEach(t),U0o=i(P),Xb=n(P,"LI",{});var BEe=s(Xb);Yae=n(BEe,"STRONG",{});var jWr=s(Yae);J0o=r(jWr,"flaubert"),jWr.forEach(t),Y0o=r(BEe," \u2014 "),Tj=n(BEe,"A",{href:!0});var NWr=s(Tj);K0o=r(NWr,"FlaubertForQuestionAnsweringSimple"),NWr.forEach(t),Z0o=r(BEe," (FlauBERT model)"),BEe.forEach(t),eLo=i(P),zb=n(P,"LI",{});var kEe=s(zb);Kae=n(kEe,"STRONG",{});var DWr=s(Kae);oLo=r(DWr,"fnet"),DWr.forEach(t),rLo=r(kEe," \u2014 "),Fj=n(kEe,"A",{href:!0});var qWr=s(Fj);tLo=r(qWr,"FNetForQuestionAnswering"),qWr.forEach(t),aLo=r(kEe," (FNet model)"),kEe.forEach(t),nLo=i(P),Vb=n(P,"LI",{});var xEe=s(Vb);Zae=n(xEe,"STRONG",{});var GWr=s(Zae);sLo=r(GWr,"funnel"),GWr.forEach(t),lLo=r(xEe," \u2014 "),Cj=n(xEe,"A",{href:!0});var OWr=s(Cj);iLo=r(OWr,"FunnelForQuestionAnswering"),OWr.forEach(t),dLo=r(xEe," (Funnel Transformer model)"),xEe.forEach(t),cLo=i(P),Wb=n(P,"LI",{});var REe=s(Wb);ene=n(REe,"STRONG",{});var XWr=s(ene);fLo=r(XWr,"gptj"),XWr.forEach(t),mLo=r(REe," \u2014 "),Mj=n(REe,"A",{href:!0});var zWr=s(Mj);gLo=r(zWr,"GPTJForQuestionAnswering"),zWr.forEach(t),hLo=r(REe," (GPT-J model)"),REe.forEach(t),pLo=i(P),Qb=n(P,"LI",{});var SEe=s(Qb);one=n(SEe,"STRONG",{});var VWr=s(one);_Lo=r(VWr,"ibert"),VWr.forEach(t),uLo=r(SEe," \u2014 "),Ej=n(SEe,"A",{href:!0});var WWr=s(Ej);bLo=r(WWr,"IBertForQuestionAnswering"),WWr.forEach(t),vLo=r(SEe," (I-BERT model)"),SEe.forEach(t),TLo=i(P),Hb=n(P,"LI",{});var PEe=s(Hb);rne=n(PEe,"STRONG",{});var QWr=s(rne);FLo=r(QWr,"layoutlmv2"),QWr.forEach(t),CLo=r(PEe," \u2014 "),yj=n(PEe,"A",{href:!0});var HWr=s(yj);MLo=r(HWr,"LayoutLMv2ForQuestionAnswering"),HWr.forEach(t),ELo=r(PEe," (LayoutLMv2 model)"),PEe.forEach(t),yLo=i(P),Ub=n(P,"LI",{});var $Ee=s(Ub);tne=n($Ee,"STRONG",{});var UWr=s(tne);wLo=r(UWr,"led"),UWr.forEach(t),ALo=r($Ee," \u2014 "),wj=n($Ee,"A",{href:!0});var JWr=s(wj);LLo=r(JWr,"LEDForQuestionAnswering"),JWr.forEach(t),BLo=r($Ee," (LED model)"),$Ee.forEach(t),kLo=i(P),Jb=n(P,"LI",{});var IEe=s(Jb);ane=n(IEe,"STRONG",{});var YWr=s(ane);xLo=r(YWr,"longformer"),YWr.forEach(t),RLo=r(IEe," \u2014 "),Aj=n(IEe,"A",{href:!0});var KWr=s(Aj);SLo=r(KWr,"LongformerForQuestionAnswering"),KWr.forEach(t),PLo=r(IEe," (Longformer model)"),IEe.forEach(t),$Lo=i(P),Yb=n(P,"LI",{});var jEe=s(Yb);nne=n(jEe,"STRONG",{});var ZWr=s(nne);ILo=r(ZWr,"lxmert"),ZWr.forEach(t),jLo=r(jEe," \u2014 "),Lj=n(jEe,"A",{href:!0});var eQr=s(Lj);NLo=r(eQr,"LxmertForQuestionAnswering"),eQr.forEach(t),DLo=r(jEe," (LXMERT model)"),jEe.forEach(t),qLo=i(P),Kb=n(P,"LI",{});var NEe=s(Kb);sne=n(NEe,"STRONG",{});var oQr=s(sne);GLo=r(oQr,"mbart"),oQr.forEach(t),OLo=r(NEe," \u2014 "),Bj=n(NEe,"A",{href:!0});var rQr=s(Bj);XLo=r(rQr,"MBartForQuestionAnswering"),rQr.forEach(t),zLo=r(NEe," (mBART model)"),NEe.forEach(t),VLo=i(P),Zb=n(P,"LI",{});var DEe=s(Zb);lne=n(DEe,"STRONG",{});var tQr=s(lne);WLo=r(tQr,"megatron-bert"),tQr.forEach(t),QLo=r(DEe," \u2014 "),kj=n(DEe,"A",{href:!0});var aQr=s(kj);HLo=r(aQr,"MegatronBertForQuestionAnswering"),aQr.forEach(t),ULo=r(DEe," (MegatronBert model)"),DEe.forEach(t),JLo=i(P),e5=n(P,"LI",{});var qEe=s(e5);ine=n(qEe,"STRONG",{});var nQr=s(ine);YLo=r(nQr,"mobilebert"),nQr.forEach(t),KLo=r(qEe," \u2014 "),xj=n(qEe,"A",{href:!0});var sQr=s(xj);ZLo=r(sQr,"MobileBertForQuestionAnswering"),sQr.forEach(t),e8o=r(qEe," (MobileBERT model)"),qEe.forEach(t),o8o=i(P),o5=n(P,"LI",{});var GEe=s(o5);dne=n(GEe,"STRONG",{});var lQr=s(dne);r8o=r(lQr,"mpnet"),lQr.forEach(t),t8o=r(GEe," \u2014 "),Rj=n(GEe,"A",{href:!0});var iQr=s(Rj);a8o=r(iQr,"MPNetForQuestionAnswering"),iQr.forEach(t),n8o=r(GEe," (MPNet model)"),GEe.forEach(t),s8o=i(P),r5=n(P,"LI",{});var OEe=s(r5);cne=n(OEe,"STRONG",{});var dQr=s(cne);l8o=r(dQr,"nystromformer"),dQr.forEach(t),i8o=r(OEe," \u2014 "),Sj=n(OEe,"A",{href:!0});var cQr=s(Sj);d8o=r(cQr,"NystromformerForQuestionAnswering"),cQr.forEach(t),c8o=r(OEe," (Nystromformer model)"),OEe.forEach(t),f8o=i(P),t5=n(P,"LI",{});var XEe=s(t5);fne=n(XEe,"STRONG",{});var fQr=s(fne);m8o=r(fQr,"qdqbert"),fQr.forEach(t),g8o=r(XEe," \u2014 "),Pj=n(XEe,"A",{href:!0});var mQr=s(Pj);h8o=r(mQr,"QDQBertForQuestionAnswering"),mQr.forEach(t),p8o=r(XEe," (QDQBert model)"),XEe.forEach(t),_8o=i(P),a5=n(P,"LI",{});var zEe=s(a5);mne=n(zEe,"STRONG",{});var gQr=s(mne);u8o=r(gQr,"reformer"),gQr.forEach(t),b8o=r(zEe," \u2014 "),$j=n(zEe,"A",{href:!0});var hQr=s($j);v8o=r(hQr,"ReformerForQuestionAnswering"),hQr.forEach(t),T8o=r(zEe," (Reformer model)"),zEe.forEach(t),F8o=i(P),n5=n(P,"LI",{});var VEe=s(n5);gne=n(VEe,"STRONG",{});var pQr=s(gne);C8o=r(pQr,"rembert"),pQr.forEach(t),M8o=r(VEe," \u2014 "),Ij=n(VEe,"A",{href:!0});var _Qr=s(Ij);E8o=r(_Qr,"RemBertForQuestionAnswering"),_Qr.forEach(t),y8o=r(VEe," (RemBERT model)"),VEe.forEach(t),w8o=i(P),s5=n(P,"LI",{});var WEe=s(s5);hne=n(WEe,"STRONG",{});var uQr=s(hne);A8o=r(uQr,"roberta"),uQr.forEach(t),L8o=r(WEe," \u2014 "),jj=n(WEe,"A",{href:!0});var bQr=s(jj);B8o=r(bQr,"RobertaForQuestionAnswering"),bQr.forEach(t),k8o=r(WEe," (RoBERTa model)"),WEe.forEach(t),x8o=i(P),l5=n(P,"LI",{});var QEe=s(l5);pne=n(QEe,"STRONG",{});var vQr=s(pne);R8o=r(vQr,"roformer"),vQr.forEach(t),S8o=r(QEe," \u2014 "),Nj=n(QEe,"A",{href:!0});var TQr=s(Nj);P8o=r(TQr,"RoFormerForQuestionAnswering"),TQr.forEach(t),$8o=r(QEe," (RoFormer model)"),QEe.forEach(t),I8o=i(P),i5=n(P,"LI",{});var HEe=s(i5);_ne=n(HEe,"STRONG",{});var FQr=s(_ne);j8o=r(FQr,"splinter"),FQr.forEach(t),N8o=r(HEe," \u2014 "),Dj=n(HEe,"A",{href:!0});var CQr=s(Dj);D8o=r(CQr,"SplinterForQuestionAnswering"),CQr.forEach(t),q8o=r(HEe," (Splinter model)"),HEe.forEach(t),G8o=i(P),d5=n(P,"LI",{});var UEe=s(d5);une=n(UEe,"STRONG",{});var MQr=s(une);O8o=r(MQr,"squeezebert"),MQr.forEach(t),X8o=r(UEe," \u2014 "),qj=n(UEe,"A",{href:!0});var EQr=s(qj);z8o=r(EQr,"SqueezeBertForQuestionAnswering"),EQr.forEach(t),V8o=r(UEe," (SqueezeBERT model)"),UEe.forEach(t),W8o=i(P),c5=n(P,"LI",{});var JEe=s(c5);bne=n(JEe,"STRONG",{});var yQr=s(bne);Q8o=r(yQr,"xlm"),yQr.forEach(t),H8o=r(JEe," \u2014 "),Gj=n(JEe,"A",{href:!0});var wQr=s(Gj);U8o=r(wQr,"XLMForQuestionAnsweringSimple"),wQr.forEach(t),J8o=r(JEe," (XLM model)"),JEe.forEach(t),Y8o=i(P),f5=n(P,"LI",{});var YEe=s(f5);vne=n(YEe,"STRONG",{});var AQr=s(vne);K8o=r(AQr,"xlm-roberta"),AQr.forEach(t),Z8o=r(YEe," \u2014 "),Oj=n(YEe,"A",{href:!0});var LQr=s(Oj);eBo=r(LQr,"XLMRobertaForQuestionAnswering"),LQr.forEach(t),oBo=r(YEe," (XLM-RoBERTa model)"),YEe.forEach(t),rBo=i(P),m5=n(P,"LI",{});var KEe=s(m5);Tne=n(KEe,"STRONG",{});var BQr=s(Tne);tBo=r(BQr,"xlm-roberta-xl"),BQr.forEach(t),aBo=r(KEe," \u2014 "),Xj=n(KEe,"A",{href:!0});var kQr=s(Xj);nBo=r(kQr,"XLMRobertaXLForQuestionAnswering"),kQr.forEach(t),sBo=r(KEe," (XLM-RoBERTa-XL model)"),KEe.forEach(t),lBo=i(P),g5=n(P,"LI",{});var ZEe=s(g5);Fne=n(ZEe,"STRONG",{});var xQr=s(Fne);iBo=r(xQr,"xlnet"),xQr.forEach(t),dBo=r(ZEe," \u2014 "),zj=n(ZEe,"A",{href:!0});var RQr=s(zj);cBo=r(RQr,"XLNetForQuestionAnsweringSimple"),RQr.forEach(t),fBo=r(ZEe," (XLNet model)"),ZEe.forEach(t),mBo=i(P),h5=n(P,"LI",{});var e3e=s(h5);Cne=n(e3e,"STRONG",{});var SQr=s(Cne);gBo=r(SQr,"yoso"),SQr.forEach(t),hBo=r(e3e," \u2014 "),Vj=n(e3e,"A",{href:!0});var PQr=s(Vj);pBo=r(PQr,"YosoForQuestionAnswering"),PQr.forEach(t),_Bo=r(e3e," (YOSO model)"),e3e.forEach(t),P.forEach(t),uBo=i(Ot),p5=n(Ot,"P",{});var o3e=s(p5);bBo=r(o3e,"The model is set in evaluation mode by default using "),Mne=n(o3e,"CODE",{});var $Qr=s(Mne);vBo=r($Qr,"model.eval()"),$Qr.forEach(t),TBo=r(o3e,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Ene=n(o3e,"CODE",{});var IQr=s(Ene);FBo=r(IQr,"model.train()"),IQr.forEach(t),o3e.forEach(t),CBo=i(Ot),yne=n(Ot,"P",{});var jQr=s(yne);MBo=r(jQr,"Examples:"),jQr.forEach(t),EBo=i(Ot),m(YE.$$.fragment,Ot),Ot.forEach(t),al.forEach(t),GLe=i(d),ud=n(d,"H2",{class:!0});var HBe=s(ud);_5=n(HBe,"A",{id:!0,class:!0,href:!0});var NQr=s(_5);wne=n(NQr,"SPAN",{});var DQr=s(wne);m(KE.$$.fragment,DQr),DQr.forEach(t),NQr.forEach(t),yBo=i(HBe),Ane=n(HBe,"SPAN",{});var qQr=s(Ane);wBo=r(qQr,"AutoModelForTableQuestionAnswering"),qQr.forEach(t),HBe.forEach(t),OLe=i(d),or=n(d,"DIV",{class:!0});var sl=s(or);m(ZE.$$.fragment,sl),ABo=i(sl),bd=n(sl,"P",{});var _z=s(bd);LBo=r(_z,`This is a generic model class that will be instantiated as one of the model classes of the library (with a table question answering head) when created
with the `),Lne=n(_z,"CODE",{});var GQr=s(Lne);BBo=r(GQr,"from_pretrained()"),GQr.forEach(t),kBo=r(_z,"class method or the "),Bne=n(_z,"CODE",{});var OQr=s(Bne);xBo=r(OQr,"from_config()"),OQr.forEach(t),RBo=r(_z,`class
method.`),_z.forEach(t),SBo=i(sl),e3=n(sl,"P",{});var UBe=s(e3);PBo=r(UBe,"This class cannot be instantiated directly using "),kne=n(UBe,"CODE",{});var XQr=s(kne);$Bo=r(XQr,"__init__()"),XQr.forEach(t),IBo=r(UBe," (throws an error)."),UBe.forEach(t),jBo=i(sl),Hr=n(sl,"DIV",{class:!0});var ll=s(Hr);m(o3.$$.fragment,ll),NBo=i(ll),xne=n(ll,"P",{});var zQr=s(xne);DBo=r(zQr,"Instantiates one of the model classes of the library (with a table question answering head) from a configuration."),zQr.forEach(t),qBo=i(ll),vd=n(ll,"P",{});var uz=s(vd);GBo=r(uz,`Note:
Loading a model from its configuration file does `),Rne=n(uz,"STRONG",{});var VQr=s(Rne);OBo=r(VQr,"not"),VQr.forEach(t),XBo=r(uz,` load the model weights. It only affects the
model\u2019s configuration. Use `),Sne=n(uz,"CODE",{});var WQr=s(Sne);zBo=r(WQr,"from_pretrained()"),WQr.forEach(t),VBo=r(uz,"to load the model weights."),uz.forEach(t),WBo=i(ll),Pne=n(ll,"P",{});var QQr=s(Pne);QBo=r(QQr,"Examples:"),QQr.forEach(t),HBo=i(ll),m(r3.$$.fragment,ll),ll.forEach(t),UBo=i(sl),qe=n(sl,"DIV",{class:!0});var Xt=s(qe);m(t3.$$.fragment,Xt),JBo=i(Xt),$ne=n(Xt,"P",{});var HQr=s($ne);YBo=r(HQr,"Instantiate one of the model classes of the library (with a table question answering head) from a pretrained model."),HQr.forEach(t),KBo=i(Xt),Ua=n(Xt,"P",{});var b4=s(Ua);ZBo=r(b4,"The model class to instantiate is selected based on the "),Ine=n(b4,"CODE",{});var UQr=s(Ine);eko=r(UQr,"model_type"),UQr.forEach(t),oko=r(b4,` property of the config object (either
passed as an argument or loaded from `),jne=n(b4,"CODE",{});var JQr=s(jne);rko=r(JQr,"pretrained_model_name_or_path"),JQr.forEach(t),tko=r(b4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Nne=n(b4,"CODE",{});var YQr=s(Nne);ako=r(YQr,"pretrained_model_name_or_path"),YQr.forEach(t),nko=r(b4,":"),b4.forEach(t),sko=i(Xt),Dne=n(Xt,"UL",{});var KQr=s(Dne);u5=n(KQr,"LI",{});var r3e=s(u5);qne=n(r3e,"STRONG",{});var ZQr=s(qne);lko=r(ZQr,"tapas"),ZQr.forEach(t),iko=r(r3e," \u2014 "),Wj=n(r3e,"A",{href:!0});var eHr=s(Wj);dko=r(eHr,"TapasForQuestionAnswering"),eHr.forEach(t),cko=r(r3e," (TAPAS model)"),r3e.forEach(t),KQr.forEach(t),fko=i(Xt),b5=n(Xt,"P",{});var t3e=s(b5);mko=r(t3e,"The model is set in evaluation mode by default using "),Gne=n(t3e,"CODE",{});var oHr=s(Gne);gko=r(oHr,"model.eval()"),oHr.forEach(t),hko=r(t3e,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),One=n(t3e,"CODE",{});var rHr=s(One);pko=r(rHr,"model.train()"),rHr.forEach(t),t3e.forEach(t),_ko=i(Xt),Xne=n(Xt,"P",{});var tHr=s(Xne);uko=r(tHr,"Examples:"),tHr.forEach(t),bko=i(Xt),m(a3.$$.fragment,Xt),Xt.forEach(t),sl.forEach(t),XLe=i(d),Td=n(d,"H2",{class:!0});var JBe=s(Td);v5=n(JBe,"A",{id:!0,class:!0,href:!0});var aHr=s(v5);zne=n(aHr,"SPAN",{});var nHr=s(zne);m(n3.$$.fragment,nHr),nHr.forEach(t),aHr.forEach(t),vko=i(JBe),Vne=n(JBe,"SPAN",{});var sHr=s(Vne);Tko=r(sHr,"AutoModelForImageClassification"),sHr.forEach(t),JBe.forEach(t),zLe=i(d),rr=n(d,"DIV",{class:!0});var il=s(rr);m(s3.$$.fragment,il),Fko=i(il),Fd=n(il,"P",{});var bz=s(Fd);Cko=r(bz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a image classification head) when created
with the `),Wne=n(bz,"CODE",{});var lHr=s(Wne);Mko=r(lHr,"from_pretrained()"),lHr.forEach(t),Eko=r(bz,"class method or the "),Qne=n(bz,"CODE",{});var iHr=s(Qne);yko=r(iHr,"from_config()"),iHr.forEach(t),wko=r(bz,`class
method.`),bz.forEach(t),Ako=i(il),l3=n(il,"P",{});var YBe=s(l3);Lko=r(YBe,"This class cannot be instantiated directly using "),Hne=n(YBe,"CODE",{});var dHr=s(Hne);Bko=r(dHr,"__init__()"),dHr.forEach(t),kko=r(YBe," (throws an error)."),YBe.forEach(t),xko=i(il),Ur=n(il,"DIV",{class:!0});var dl=s(Ur);m(i3.$$.fragment,dl),Rko=i(dl),Une=n(dl,"P",{});var cHr=s(Une);Sko=r(cHr,"Instantiates one of the model classes of the library (with a image classification head) from a configuration."),cHr.forEach(t),Pko=i(dl),Cd=n(dl,"P",{});var vz=s(Cd);$ko=r(vz,`Note:
Loading a model from its configuration file does `),Jne=n(vz,"STRONG",{});var fHr=s(Jne);Iko=r(fHr,"not"),fHr.forEach(t),jko=r(vz,` load the model weights. It only affects the
model\u2019s configuration. Use `),Yne=n(vz,"CODE",{});var mHr=s(Yne);Nko=r(mHr,"from_pretrained()"),mHr.forEach(t),Dko=r(vz,"to load the model weights."),vz.forEach(t),qko=i(dl),Kne=n(dl,"P",{});var gHr=s(Kne);Gko=r(gHr,"Examples:"),gHr.forEach(t),Oko=i(dl),m(d3.$$.fragment,dl),dl.forEach(t),Xko=i(il),Ge=n(il,"DIV",{class:!0});var zt=s(Ge);m(c3.$$.fragment,zt),zko=i(zt),Zne=n(zt,"P",{});var hHr=s(Zne);Vko=r(hHr,"Instantiate one of the model classes of the library (with a image classification head) from a pretrained model."),hHr.forEach(t),Wko=i(zt),Ja=n(zt,"P",{});var v4=s(Ja);Qko=r(v4,"The model class to instantiate is selected based on the "),ese=n(v4,"CODE",{});var pHr=s(ese);Hko=r(pHr,"model_type"),pHr.forEach(t),Uko=r(v4,` property of the config object (either
passed as an argument or loaded from `),ose=n(v4,"CODE",{});var _Hr=s(ose);Jko=r(_Hr,"pretrained_model_name_or_path"),_Hr.forEach(t),Yko=r(v4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),rse=n(v4,"CODE",{});var uHr=s(rse);Kko=r(uHr,"pretrained_model_name_or_path"),uHr.forEach(t),Zko=r(v4,":"),v4.forEach(t),exo=i(zt),be=n(zt,"UL",{});var Ke=s(be);T5=n(Ke,"LI",{});var a3e=s(T5);tse=n(a3e,"STRONG",{});var bHr=s(tse);oxo=r(bHr,"beit"),bHr.forEach(t),rxo=r(a3e," \u2014 "),Qj=n(a3e,"A",{href:!0});var vHr=s(Qj);txo=r(vHr,"BeitForImageClassification"),vHr.forEach(t),axo=r(a3e," (BEiT model)"),a3e.forEach(t),nxo=i(Ke),F5=n(Ke,"LI",{});var n3e=s(F5);ase=n(n3e,"STRONG",{});var THr=s(ase);sxo=r(THr,"convnext"),THr.forEach(t),lxo=r(n3e," \u2014 "),Hj=n(n3e,"A",{href:!0});var FHr=s(Hj);ixo=r(FHr,"ConvNextForImageClassification"),FHr.forEach(t),dxo=r(n3e," (ConvNext model)"),n3e.forEach(t),cxo=i(Ke),Ss=n(Ke,"LI",{});var D0=s(Ss);nse=n(D0,"STRONG",{});var CHr=s(nse);fxo=r(CHr,"deit"),CHr.forEach(t),mxo=r(D0," \u2014 "),Uj=n(D0,"A",{href:!0});var MHr=s(Uj);gxo=r(MHr,"DeiTForImageClassification"),MHr.forEach(t),hxo=r(D0," or "),Jj=n(D0,"A",{href:!0});var EHr=s(Jj);pxo=r(EHr,"DeiTForImageClassificationWithTeacher"),EHr.forEach(t),_xo=r(D0," (DeiT model)"),D0.forEach(t),uxo=i(Ke),C5=n(Ke,"LI",{});var s3e=s(C5);sse=n(s3e,"STRONG",{});var yHr=s(sse);bxo=r(yHr,"imagegpt"),yHr.forEach(t),vxo=r(s3e," \u2014 "),Yj=n(s3e,"A",{href:!0});var wHr=s(Yj);Txo=r(wHr,"ImageGPTForImageClassification"),wHr.forEach(t),Fxo=r(s3e," (ImageGPT model)"),s3e.forEach(t),Cxo=i(Ke),la=n(Ke,"LI",{});var Ef=s(la);lse=n(Ef,"STRONG",{});var AHr=s(lse);Mxo=r(AHr,"perceiver"),AHr.forEach(t),Exo=r(Ef," \u2014 "),Kj=n(Ef,"A",{href:!0});var LHr=s(Kj);yxo=r(LHr,"PerceiverForImageClassificationLearned"),LHr.forEach(t),wxo=r(Ef," or "),Zj=n(Ef,"A",{href:!0});var BHr=s(Zj);Axo=r(BHr,"PerceiverForImageClassificationFourier"),BHr.forEach(t),Lxo=r(Ef," or "),eN=n(Ef,"A",{href:!0});var kHr=s(eN);Bxo=r(kHr,"PerceiverForImageClassificationConvProcessing"),kHr.forEach(t),kxo=r(Ef," (Perceiver model)"),Ef.forEach(t),xxo=i(Ke),M5=n(Ke,"LI",{});var l3e=s(M5);ise=n(l3e,"STRONG",{});var xHr=s(ise);Rxo=r(xHr,"poolformer"),xHr.forEach(t),Sxo=r(l3e," \u2014 "),oN=n(l3e,"A",{href:!0});var RHr=s(oN);Pxo=r(RHr,"PoolFormerForImageClassification"),RHr.forEach(t),$xo=r(l3e," (PoolFormer model)"),l3e.forEach(t),Ixo=i(Ke),E5=n(Ke,"LI",{});var i3e=s(E5);dse=n(i3e,"STRONG",{});var SHr=s(dse);jxo=r(SHr,"segformer"),SHr.forEach(t),Nxo=r(i3e," \u2014 "),rN=n(i3e,"A",{href:!0});var PHr=s(rN);Dxo=r(PHr,"SegformerForImageClassification"),PHr.forEach(t),qxo=r(i3e," (SegFormer model)"),i3e.forEach(t),Gxo=i(Ke),y5=n(Ke,"LI",{});var d3e=s(y5);cse=n(d3e,"STRONG",{});var $Hr=s(cse);Oxo=r($Hr,"swin"),$Hr.forEach(t),Xxo=r(d3e," \u2014 "),tN=n(d3e,"A",{href:!0});var IHr=s(tN);zxo=r(IHr,"SwinForImageClassification"),IHr.forEach(t),Vxo=r(d3e," (Swin model)"),d3e.forEach(t),Wxo=i(Ke),w5=n(Ke,"LI",{});var c3e=s(w5);fse=n(c3e,"STRONG",{});var jHr=s(fse);Qxo=r(jHr,"vit"),jHr.forEach(t),Hxo=r(c3e," \u2014 "),aN=n(c3e,"A",{href:!0});var NHr=s(aN);Uxo=r(NHr,"ViTForImageClassification"),NHr.forEach(t),Jxo=r(c3e," (ViT model)"),c3e.forEach(t),Ke.forEach(t),Yxo=i(zt),A5=n(zt,"P",{});var f3e=s(A5);Kxo=r(f3e,"The model is set in evaluation mode by default using "),mse=n(f3e,"CODE",{});var DHr=s(mse);Zxo=r(DHr,"model.eval()"),DHr.forEach(t),eRo=r(f3e,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),gse=n(f3e,"CODE",{});var qHr=s(gse);oRo=r(qHr,"model.train()"),qHr.forEach(t),f3e.forEach(t),rRo=i(zt),hse=n(zt,"P",{});var GHr=s(hse);tRo=r(GHr,"Examples:"),GHr.forEach(t),aRo=i(zt),m(f3.$$.fragment,zt),zt.forEach(t),il.forEach(t),VLe=i(d),Md=n(d,"H2",{class:!0});var KBe=s(Md);L5=n(KBe,"A",{id:!0,class:!0,href:!0});var OHr=s(L5);pse=n(OHr,"SPAN",{});var XHr=s(pse);m(m3.$$.fragment,XHr),XHr.forEach(t),OHr.forEach(t),nRo=i(KBe),_se=n(KBe,"SPAN",{});var zHr=s(_se);sRo=r(zHr,"AutoModelForVision2Seq"),zHr.forEach(t),KBe.forEach(t),WLe=i(d),tr=n(d,"DIV",{class:!0});var cl=s(tr);m(g3.$$.fragment,cl),lRo=i(cl),Ed=n(cl,"P",{});var Tz=s(Ed);iRo=r(Tz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a vision-to-text modeling head) when created
with the `),use=n(Tz,"CODE",{});var VHr=s(use);dRo=r(VHr,"from_pretrained()"),VHr.forEach(t),cRo=r(Tz,"class method or the "),bse=n(Tz,"CODE",{});var WHr=s(bse);fRo=r(WHr,"from_config()"),WHr.forEach(t),mRo=r(Tz,`class
method.`),Tz.forEach(t),gRo=i(cl),h3=n(cl,"P",{});var ZBe=s(h3);hRo=r(ZBe,"This class cannot be instantiated directly using "),vse=n(ZBe,"CODE",{});var QHr=s(vse);pRo=r(QHr,"__init__()"),QHr.forEach(t),_Ro=r(ZBe," (throws an error)."),ZBe.forEach(t),uRo=i(cl),Jr=n(cl,"DIV",{class:!0});var fl=s(Jr);m(p3.$$.fragment,fl),bRo=i(fl),Tse=n(fl,"P",{});var HHr=s(Tse);vRo=r(HHr,"Instantiates one of the model classes of the library (with a vision-to-text modeling head) from a configuration."),HHr.forEach(t),TRo=i(fl),yd=n(fl,"P",{});var Fz=s(yd);FRo=r(Fz,`Note:
Loading a model from its configuration file does `),Fse=n(Fz,"STRONG",{});var UHr=s(Fse);CRo=r(UHr,"not"),UHr.forEach(t),MRo=r(Fz,` load the model weights. It only affects the
model\u2019s configuration. Use `),Cse=n(Fz,"CODE",{});var JHr=s(Cse);ERo=r(JHr,"from_pretrained()"),JHr.forEach(t),yRo=r(Fz,"to load the model weights."),Fz.forEach(t),wRo=i(fl),Mse=n(fl,"P",{});var YHr=s(Mse);ARo=r(YHr,"Examples:"),YHr.forEach(t),LRo=i(fl),m(_3.$$.fragment,fl),fl.forEach(t),BRo=i(cl),Oe=n(cl,"DIV",{class:!0});var Vt=s(Oe);m(u3.$$.fragment,Vt),kRo=i(Vt),Ese=n(Vt,"P",{});var KHr=s(Ese);xRo=r(KHr,"Instantiate one of the model classes of the library (with a vision-to-text modeling head) from a pretrained model."),KHr.forEach(t),RRo=i(Vt),Ya=n(Vt,"P",{});var T4=s(Ya);SRo=r(T4,"The model class to instantiate is selected based on the "),yse=n(T4,"CODE",{});var ZHr=s(yse);PRo=r(ZHr,"model_type"),ZHr.forEach(t),$Ro=r(T4,` property of the config object (either
passed as an argument or loaded from `),wse=n(T4,"CODE",{});var eUr=s(wse);IRo=r(eUr,"pretrained_model_name_or_path"),eUr.forEach(t),jRo=r(T4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Ase=n(T4,"CODE",{});var oUr=s(Ase);NRo=r(oUr,"pretrained_model_name_or_path"),oUr.forEach(t),DRo=r(T4,":"),T4.forEach(t),qRo=i(Vt),Lse=n(Vt,"UL",{});var rUr=s(Lse);B5=n(rUr,"LI",{});var m3e=s(B5);Bse=n(m3e,"STRONG",{});var tUr=s(Bse);GRo=r(tUr,"vision-encoder-decoder"),tUr.forEach(t),ORo=r(m3e," \u2014 "),nN=n(m3e,"A",{href:!0});var aUr=s(nN);XRo=r(aUr,"VisionEncoderDecoderModel"),aUr.forEach(t),zRo=r(m3e," (Vision Encoder decoder model)"),m3e.forEach(t),rUr.forEach(t),VRo=i(Vt),k5=n(Vt,"P",{});var g3e=s(k5);WRo=r(g3e,"The model is set in evaluation mode by default using "),kse=n(g3e,"CODE",{});var nUr=s(kse);QRo=r(nUr,"model.eval()"),nUr.forEach(t),HRo=r(g3e,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),xse=n(g3e,"CODE",{});var sUr=s(xse);URo=r(sUr,"model.train()"),sUr.forEach(t),g3e.forEach(t),JRo=i(Vt),Rse=n(Vt,"P",{});var lUr=s(Rse);YRo=r(lUr,"Examples:"),lUr.forEach(t),KRo=i(Vt),m(b3.$$.fragment,Vt),Vt.forEach(t),cl.forEach(t),QLe=i(d),wd=n(d,"H2",{class:!0});var eke=s(wd);x5=n(eke,"A",{id:!0,class:!0,href:!0});var iUr=s(x5);Sse=n(iUr,"SPAN",{});var dUr=s(Sse);m(v3.$$.fragment,dUr),dUr.forEach(t),iUr.forEach(t),ZRo=i(eke),Pse=n(eke,"SPAN",{});var cUr=s(Pse);eSo=r(cUr,"AutoModelForAudioClassification"),cUr.forEach(t),eke.forEach(t),HLe=i(d),ar=n(d,"DIV",{class:!0});var ml=s(ar);m(T3.$$.fragment,ml),oSo=i(ml),Ad=n(ml,"P",{});var Cz=s(Ad);rSo=r(Cz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a audio classification head) when created
with the `),$se=n(Cz,"CODE",{});var fUr=s($se);tSo=r(fUr,"from_pretrained()"),fUr.forEach(t),aSo=r(Cz,"class method or the "),Ise=n(Cz,"CODE",{});var mUr=s(Ise);nSo=r(mUr,"from_config()"),mUr.forEach(t),sSo=r(Cz,`class
method.`),Cz.forEach(t),lSo=i(ml),F3=n(ml,"P",{});var oke=s(F3);iSo=r(oke,"This class cannot be instantiated directly using "),jse=n(oke,"CODE",{});var gUr=s(jse);dSo=r(gUr,"__init__()"),gUr.forEach(t),cSo=r(oke," (throws an error)."),oke.forEach(t),fSo=i(ml),Yr=n(ml,"DIV",{class:!0});var gl=s(Yr);m(C3.$$.fragment,gl),mSo=i(gl),Nse=n(gl,"P",{});var hUr=s(Nse);gSo=r(hUr,"Instantiates one of the model classes of the library (with a audio classification head) from a configuration."),hUr.forEach(t),hSo=i(gl),Ld=n(gl,"P",{});var Mz=s(Ld);pSo=r(Mz,`Note:
Loading a model from its configuration file does `),Dse=n(Mz,"STRONG",{});var pUr=s(Dse);_So=r(pUr,"not"),pUr.forEach(t),uSo=r(Mz,` load the model weights. It only affects the
model\u2019s configuration. Use `),qse=n(Mz,"CODE",{});var _Ur=s(qse);bSo=r(_Ur,"from_pretrained()"),_Ur.forEach(t),vSo=r(Mz,"to load the model weights."),Mz.forEach(t),TSo=i(gl),Gse=n(gl,"P",{});var uUr=s(Gse);FSo=r(uUr,"Examples:"),uUr.forEach(t),CSo=i(gl),m(M3.$$.fragment,gl),gl.forEach(t),MSo=i(ml),Xe=n(ml,"DIV",{class:!0});var Wt=s(Xe);m(E3.$$.fragment,Wt),ESo=i(Wt),Ose=n(Wt,"P",{});var bUr=s(Ose);ySo=r(bUr,"Instantiate one of the model classes of the library (with a audio classification head) from a pretrained model."),bUr.forEach(t),wSo=i(Wt),Ka=n(Wt,"P",{});var F4=s(Ka);ASo=r(F4,"The model class to instantiate is selected based on the "),Xse=n(F4,"CODE",{});var vUr=s(Xse);LSo=r(vUr,"model_type"),vUr.forEach(t),BSo=r(F4,` property of the config object (either
passed as an argument or loaded from `),zse=n(F4,"CODE",{});var TUr=s(zse);kSo=r(TUr,"pretrained_model_name_or_path"),TUr.forEach(t),xSo=r(F4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Vse=n(F4,"CODE",{});var FUr=s(Vse);RSo=r(FUr,"pretrained_model_name_or_path"),FUr.forEach(t),SSo=r(F4,":"),F4.forEach(t),PSo=i(Wt),ao=n(Wt,"UL",{});var Qt=s(ao);R5=n(Qt,"LI",{});var h3e=s(R5);Wse=n(h3e,"STRONG",{});var CUr=s(Wse);$So=r(CUr,"hubert"),CUr.forEach(t),ISo=r(h3e," \u2014 "),sN=n(h3e,"A",{href:!0});var MUr=s(sN);jSo=r(MUr,"HubertForSequenceClassification"),MUr.forEach(t),NSo=r(h3e," (Hubert model)"),h3e.forEach(t),DSo=i(Qt),S5=n(Qt,"LI",{});var p3e=s(S5);Qse=n(p3e,"STRONG",{});var EUr=s(Qse);qSo=r(EUr,"sew"),EUr.forEach(t),GSo=r(p3e," \u2014 "),lN=n(p3e,"A",{href:!0});var yUr=s(lN);OSo=r(yUr,"SEWForSequenceClassification"),yUr.forEach(t),XSo=r(p3e," (SEW model)"),p3e.forEach(t),zSo=i(Qt),P5=n(Qt,"LI",{});var _3e=s(P5);Hse=n(_3e,"STRONG",{});var wUr=s(Hse);VSo=r(wUr,"sew-d"),wUr.forEach(t),WSo=r(_3e," \u2014 "),iN=n(_3e,"A",{href:!0});var AUr=s(iN);QSo=r(AUr,"SEWDForSequenceClassification"),AUr.forEach(t),HSo=r(_3e," (SEW-D model)"),_3e.forEach(t),USo=i(Qt),$5=n(Qt,"LI",{});var u3e=s($5);Use=n(u3e,"STRONG",{});var LUr=s(Use);JSo=r(LUr,"unispeech"),LUr.forEach(t),YSo=r(u3e," \u2014 "),dN=n(u3e,"A",{href:!0});var BUr=s(dN);KSo=r(BUr,"UniSpeechForSequenceClassification"),BUr.forEach(t),ZSo=r(u3e," (UniSpeech model)"),u3e.forEach(t),ePo=i(Qt),I5=n(Qt,"LI",{});var b3e=s(I5);Jse=n(b3e,"STRONG",{});var kUr=s(Jse);oPo=r(kUr,"unispeech-sat"),kUr.forEach(t),rPo=r(b3e," \u2014 "),cN=n(b3e,"A",{href:!0});var xUr=s(cN);tPo=r(xUr,"UniSpeechSatForSequenceClassification"),xUr.forEach(t),aPo=r(b3e," (UniSpeechSat model)"),b3e.forEach(t),nPo=i(Qt),j5=n(Qt,"LI",{});var v3e=s(j5);Yse=n(v3e,"STRONG",{});var RUr=s(Yse);sPo=r(RUr,"wav2vec2"),RUr.forEach(t),lPo=r(v3e," \u2014 "),fN=n(v3e,"A",{href:!0});var SUr=s(fN);iPo=r(SUr,"Wav2Vec2ForSequenceClassification"),SUr.forEach(t),dPo=r(v3e," (Wav2Vec2 model)"),v3e.forEach(t),cPo=i(Qt),N5=n(Qt,"LI",{});var T3e=s(N5);Kse=n(T3e,"STRONG",{});var PUr=s(Kse);fPo=r(PUr,"wavlm"),PUr.forEach(t),mPo=r(T3e," \u2014 "),mN=n(T3e,"A",{href:!0});var $Ur=s(mN);gPo=r($Ur,"WavLMForSequenceClassification"),$Ur.forEach(t),hPo=r(T3e," (WavLM model)"),T3e.forEach(t),Qt.forEach(t),pPo=i(Wt),D5=n(Wt,"P",{});var F3e=s(D5);_Po=r(F3e,"The model is set in evaluation mode by default using "),Zse=n(F3e,"CODE",{});var IUr=s(Zse);uPo=r(IUr,"model.eval()"),IUr.forEach(t),bPo=r(F3e,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),ele=n(F3e,"CODE",{});var jUr=s(ele);vPo=r(jUr,"model.train()"),jUr.forEach(t),F3e.forEach(t),TPo=i(Wt),ole=n(Wt,"P",{});var NUr=s(ole);FPo=r(NUr,"Examples:"),NUr.forEach(t),CPo=i(Wt),m(y3.$$.fragment,Wt),Wt.forEach(t),ml.forEach(t),ULe=i(d),Bd=n(d,"H2",{class:!0});var rke=s(Bd);q5=n(rke,"A",{id:!0,class:!0,href:!0});var DUr=s(q5);rle=n(DUr,"SPAN",{});var qUr=s(rle);m(w3.$$.fragment,qUr),qUr.forEach(t),DUr.forEach(t),MPo=i(rke),tle=n(rke,"SPAN",{});var GUr=s(tle);EPo=r(GUr,"AutoModelForAudioFrameClassification"),GUr.forEach(t),rke.forEach(t),JLe=i(d),nr=n(d,"DIV",{class:!0});var hl=s(nr);m(A3.$$.fragment,hl),yPo=i(hl),kd=n(hl,"P",{});var Ez=s(kd);wPo=r(Ez,`This is a generic model class that will be instantiated as one of the model classes of the library (with a audio frame (token) classification head) when created
with the `),ale=n(Ez,"CODE",{});var OUr=s(ale);APo=r(OUr,"from_pretrained()"),OUr.forEach(t),LPo=r(Ez,"class method or the "),nle=n(Ez,"CODE",{});var XUr=s(nle);BPo=r(XUr,"from_config()"),XUr.forEach(t),kPo=r(Ez,`class
method.`),Ez.forEach(t),xPo=i(hl),L3=n(hl,"P",{});var tke=s(L3);RPo=r(tke,"This class cannot be instantiated directly using "),sle=n(tke,"CODE",{});var zUr=s(sle);SPo=r(zUr,"__init__()"),zUr.forEach(t),PPo=r(tke," (throws an error)."),tke.forEach(t),$Po=i(hl),Kr=n(hl,"DIV",{class:!0});var pl=s(Kr);m(B3.$$.fragment,pl),IPo=i(pl),lle=n(pl,"P",{});var VUr=s(lle);jPo=r(VUr,"Instantiates one of the model classes of the library (with a audio frame (token) classification head) from a configuration."),VUr.forEach(t),NPo=i(pl),xd=n(pl,"P",{});var yz=s(xd);DPo=r(yz,`Note:
Loading a model from its configuration file does `),ile=n(yz,"STRONG",{});var WUr=s(ile);qPo=r(WUr,"not"),WUr.forEach(t),GPo=r(yz,` load the model weights. It only affects the
model\u2019s configuration. Use `),dle=n(yz,"CODE",{});var QUr=s(dle);OPo=r(QUr,"from_pretrained()"),QUr.forEach(t),XPo=r(yz,"to load the model weights."),yz.forEach(t),zPo=i(pl),cle=n(pl,"P",{});var HUr=s(cle);VPo=r(HUr,"Examples:"),HUr.forEach(t),WPo=i(pl),m(k3.$$.fragment,pl),pl.forEach(t),QPo=i(hl),ze=n(hl,"DIV",{class:!0});var Ht=s(ze);m(x3.$$.fragment,Ht),HPo=i(Ht),fle=n(Ht,"P",{});var UUr=s(fle);UPo=r(UUr,"Instantiate one of the model classes of the library (with a audio frame (token) classification head) from a pretrained model."),UUr.forEach(t),JPo=i(Ht),Za=n(Ht,"P",{});var C4=s(Za);YPo=r(C4,"The model class to instantiate is selected based on the "),mle=n(C4,"CODE",{});var JUr=s(mle);KPo=r(JUr,"model_type"),JUr.forEach(t),ZPo=r(C4,` property of the config object (either
passed as an argument or loaded from `),gle=n(C4,"CODE",{});var YUr=s(gle);e$o=r(YUr,"pretrained_model_name_or_path"),YUr.forEach(t),o$o=r(C4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),hle=n(C4,"CODE",{});var KUr=s(hle);r$o=r(KUr,"pretrained_model_name_or_path"),KUr.forEach(t),t$o=r(C4,":"),C4.forEach(t),a$o=i(Ht),Rd=n(Ht,"UL",{});var wz=s(Rd);G5=n(wz,"LI",{});var C3e=s(G5);ple=n(C3e,"STRONG",{});var ZUr=s(ple);n$o=r(ZUr,"unispeech-sat"),ZUr.forEach(t),s$o=r(C3e," \u2014 "),gN=n(C3e,"A",{href:!0});var eJr=s(gN);l$o=r(eJr,"UniSpeechSatForAudioFrameClassification"),eJr.forEach(t),i$o=r(C3e," (UniSpeechSat model)"),C3e.forEach(t),d$o=i(wz),O5=n(wz,"LI",{});var M3e=s(O5);_le=n(M3e,"STRONG",{});var oJr=s(_le);c$o=r(oJr,"wav2vec2"),oJr.forEach(t),f$o=r(M3e," \u2014 "),hN=n(M3e,"A",{href:!0});var rJr=s(hN);m$o=r(rJr,"Wav2Vec2ForAudioFrameClassification"),rJr.forEach(t),g$o=r(M3e," (Wav2Vec2 model)"),M3e.forEach(t),h$o=i(wz),X5=n(wz,"LI",{});var E3e=s(X5);ule=n(E3e,"STRONG",{});var tJr=s(ule);p$o=r(tJr,"wavlm"),tJr.forEach(t),_$o=r(E3e," \u2014 "),pN=n(E3e,"A",{href:!0});var aJr=s(pN);u$o=r(aJr,"WavLMForAudioFrameClassification"),aJr.forEach(t),b$o=r(E3e," (WavLM model)"),E3e.forEach(t),wz.forEach(t),v$o=i(Ht),z5=n(Ht,"P",{});var y3e=s(z5);T$o=r(y3e,"The model is set in evaluation mode by default using "),ble=n(y3e,"CODE",{});var nJr=s(ble);F$o=r(nJr,"model.eval()"),nJr.forEach(t),C$o=r(y3e,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),vle=n(y3e,"CODE",{});var sJr=s(vle);M$o=r(sJr,"model.train()"),sJr.forEach(t),y3e.forEach(t),E$o=i(Ht),Tle=n(Ht,"P",{});var lJr=s(Tle);y$o=r(lJr,"Examples:"),lJr.forEach(t),w$o=i(Ht),m(R3.$$.fragment,Ht),Ht.forEach(t),hl.forEach(t),YLe=i(d),Sd=n(d,"H2",{class:!0});var ake=s(Sd);V5=n(ake,"A",{id:!0,class:!0,href:!0});var iJr=s(V5);Fle=n(iJr,"SPAN",{});var dJr=s(Fle);m(S3.$$.fragment,dJr),dJr.forEach(t),iJr.forEach(t),A$o=i(ake),Cle=n(ake,"SPAN",{});var cJr=s(Cle);L$o=r(cJr,"AutoModelForCTC"),cJr.forEach(t),ake.forEach(t),KLe=i(d),sr=n(d,"DIV",{class:!0});var _l=s(sr);m(P3.$$.fragment,_l),B$o=i(_l),Pd=n(_l,"P",{});var Az=s(Pd);k$o=r(Az,`This is a generic model class that will be instantiated as one of the model classes of the library (with a connectionist temporal classification head) when created
with the `),Mle=n(Az,"CODE",{});var fJr=s(Mle);x$o=r(fJr,"from_pretrained()"),fJr.forEach(t),R$o=r(Az,"class method or the "),Ele=n(Az,"CODE",{});var mJr=s(Ele);S$o=r(mJr,"from_config()"),mJr.forEach(t),P$o=r(Az,`class
method.`),Az.forEach(t),$$o=i(_l),$3=n(_l,"P",{});var nke=s($3);I$o=r(nke,"This class cannot be instantiated directly using "),yle=n(nke,"CODE",{});var gJr=s(yle);j$o=r(gJr,"__init__()"),gJr.forEach(t),N$o=r(nke," (throws an error)."),nke.forEach(t),D$o=i(_l),Zr=n(_l,"DIV",{class:!0});var ul=s(Zr);m(I3.$$.fragment,ul),q$o=i(ul),wle=n(ul,"P",{});var hJr=s(wle);G$o=r(hJr,"Instantiates one of the model classes of the library (with a connectionist temporal classification head) from a configuration."),hJr.forEach(t),O$o=i(ul),$d=n(ul,"P",{});var Lz=s($d);X$o=r(Lz,`Note:
Loading a model from its configuration file does `),Ale=n(Lz,"STRONG",{});var pJr=s(Ale);z$o=r(pJr,"not"),pJr.forEach(t),V$o=r(Lz,` load the model weights. It only affects the
model\u2019s configuration. Use `),Lle=n(Lz,"CODE",{});var _Jr=s(Lle);W$o=r(_Jr,"from_pretrained()"),_Jr.forEach(t),Q$o=r(Lz,"to load the model weights."),Lz.forEach(t),H$o=i(ul),Ble=n(ul,"P",{});var uJr=s(Ble);U$o=r(uJr,"Examples:"),uJr.forEach(t),J$o=i(ul),m(j3.$$.fragment,ul),ul.forEach(t),Y$o=i(_l),Ve=n(_l,"DIV",{class:!0});var Ut=s(Ve);m(N3.$$.fragment,Ut),K$o=i(Ut),kle=n(Ut,"P",{});var bJr=s(kle);Z$o=r(bJr,"Instantiate one of the model classes of the library (with a connectionist temporal classification head) from a pretrained model."),bJr.forEach(t),eIo=i(Ut),en=n(Ut,"P",{});var M4=s(en);oIo=r(M4,"The model class to instantiate is selected based on the "),xle=n(M4,"CODE",{});var vJr=s(xle);rIo=r(vJr,"model_type"),vJr.forEach(t),tIo=r(M4,` property of the config object (either
passed as an argument or loaded from `),Rle=n(M4,"CODE",{});var TJr=s(Rle);aIo=r(TJr,"pretrained_model_name_or_path"),TJr.forEach(t),nIo=r(M4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Sle=n(M4,"CODE",{});var FJr=s(Sle);sIo=r(FJr,"pretrained_model_name_or_path"),FJr.forEach(t),lIo=r(M4,":"),M4.forEach(t),iIo=i(Ut),no=n(Ut,"UL",{});var Jt=s(no);W5=n(Jt,"LI",{});var w3e=s(W5);Ple=n(w3e,"STRONG",{});var CJr=s(Ple);dIo=r(CJr,"hubert"),CJr.forEach(t),cIo=r(w3e," \u2014 "),_N=n(w3e,"A",{href:!0});var MJr=s(_N);fIo=r(MJr,"HubertForCTC"),MJr.forEach(t),mIo=r(w3e," (Hubert model)"),w3e.forEach(t),gIo=i(Jt),Q5=n(Jt,"LI",{});var A3e=s(Q5);$le=n(A3e,"STRONG",{});var EJr=s($le);hIo=r(EJr,"sew"),EJr.forEach(t),pIo=r(A3e," \u2014 "),uN=n(A3e,"A",{href:!0});var yJr=s(uN);_Io=r(yJr,"SEWForCTC"),yJr.forEach(t),uIo=r(A3e," (SEW model)"),A3e.forEach(t),bIo=i(Jt),H5=n(Jt,"LI",{});var L3e=s(H5);Ile=n(L3e,"STRONG",{});var wJr=s(Ile);vIo=r(wJr,"sew-d"),wJr.forEach(t),TIo=r(L3e," \u2014 "),bN=n(L3e,"A",{href:!0});var AJr=s(bN);FIo=r(AJr,"SEWDForCTC"),AJr.forEach(t),CIo=r(L3e," (SEW-D model)"),L3e.forEach(t),MIo=i(Jt),U5=n(Jt,"LI",{});var B3e=s(U5);jle=n(B3e,"STRONG",{});var LJr=s(jle);EIo=r(LJr,"unispeech"),LJr.forEach(t),yIo=r(B3e," \u2014 "),vN=n(B3e,"A",{href:!0});var BJr=s(vN);wIo=r(BJr,"UniSpeechForCTC"),BJr.forEach(t),AIo=r(B3e," (UniSpeech model)"),B3e.forEach(t),LIo=i(Jt),J5=n(Jt,"LI",{});var k3e=s(J5);Nle=n(k3e,"STRONG",{});var kJr=s(Nle);BIo=r(kJr,"unispeech-sat"),kJr.forEach(t),kIo=r(k3e," \u2014 "),TN=n(k3e,"A",{href:!0});var xJr=s(TN);xIo=r(xJr,"UniSpeechSatForCTC"),xJr.forEach(t),RIo=r(k3e," (UniSpeechSat model)"),k3e.forEach(t),SIo=i(Jt),Y5=n(Jt,"LI",{});var x3e=s(Y5);Dle=n(x3e,"STRONG",{});var RJr=s(Dle);PIo=r(RJr,"wav2vec2"),RJr.forEach(t),$Io=r(x3e," \u2014 "),FN=n(x3e,"A",{href:!0});var SJr=s(FN);IIo=r(SJr,"Wav2Vec2ForCTC"),SJr.forEach(t),jIo=r(x3e," (Wav2Vec2 model)"),x3e.forEach(t),NIo=i(Jt),K5=n(Jt,"LI",{});var R3e=s(K5);qle=n(R3e,"STRONG",{});var PJr=s(qle);DIo=r(PJr,"wavlm"),PJr.forEach(t),qIo=r(R3e," \u2014 "),CN=n(R3e,"A",{href:!0});var $Jr=s(CN);GIo=r($Jr,"WavLMForCTC"),$Jr.forEach(t),OIo=r(R3e," (WavLM model)"),R3e.forEach(t),Jt.forEach(t),XIo=i(Ut),Z5=n(Ut,"P",{});var S3e=s(Z5);zIo=r(S3e,"The model is set in evaluation mode by default using "),Gle=n(S3e,"CODE",{});var IJr=s(Gle);VIo=r(IJr,"model.eval()"),IJr.forEach(t),WIo=r(S3e,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Ole=n(S3e,"CODE",{});var jJr=s(Ole);QIo=r(jJr,"model.train()"),jJr.forEach(t),S3e.forEach(t),HIo=i(Ut),Xle=n(Ut,"P",{});var NJr=s(Xle);UIo=r(NJr,"Examples:"),NJr.forEach(t),JIo=i(Ut),m(D3.$$.fragment,Ut),Ut.forEach(t),_l.forEach(t),ZLe=i(d),Id=n(d,"H2",{class:!0});var ske=s(Id);ev=n(ske,"A",{id:!0,class:!0,href:!0});var DJr=s(ev);zle=n(DJr,"SPAN",{});var qJr=s(zle);m(q3.$$.fragment,qJr),qJr.forEach(t),DJr.forEach(t),YIo=i(ske),Vle=n(ske,"SPAN",{});var GJr=s(Vle);KIo=r(GJr,"AutoModelForSpeechSeq2Seq"),GJr.forEach(t),ske.forEach(t),e8e=i(d),lr=n(d,"DIV",{class:!0});var bl=s(lr);m(G3.$$.fragment,bl),ZIo=i(bl),jd=n(bl,"P",{});var Bz=s(jd);ejo=r(Bz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) when created
with the `),Wle=n(Bz,"CODE",{});var OJr=s(Wle);ojo=r(OJr,"from_pretrained()"),OJr.forEach(t),rjo=r(Bz,"class method or the "),Qle=n(Bz,"CODE",{});var XJr=s(Qle);tjo=r(XJr,"from_config()"),XJr.forEach(t),ajo=r(Bz,`class
method.`),Bz.forEach(t),njo=i(bl),O3=n(bl,"P",{});var lke=s(O3);sjo=r(lke,"This class cannot be instantiated directly using "),Hle=n(lke,"CODE",{});var zJr=s(Hle);ljo=r(zJr,"__init__()"),zJr.forEach(t),ijo=r(lke," (throws an error)."),lke.forEach(t),djo=i(bl),et=n(bl,"DIV",{class:!0});var vl=s(et);m(X3.$$.fragment,vl),cjo=i(vl),Ule=n(vl,"P",{});var VJr=s(Ule);fjo=r(VJr,"Instantiates one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) from a configuration."),VJr.forEach(t),mjo=i(vl),Nd=n(vl,"P",{});var kz=s(Nd);gjo=r(kz,`Note:
Loading a model from its configuration file does `),Jle=n(kz,"STRONG",{});var WJr=s(Jle);hjo=r(WJr,"not"),WJr.forEach(t),pjo=r(kz,` load the model weights. It only affects the
model\u2019s configuration. Use `),Yle=n(kz,"CODE",{});var QJr=s(Yle);_jo=r(QJr,"from_pretrained()"),QJr.forEach(t),ujo=r(kz,"to load the model weights."),kz.forEach(t),bjo=i(vl),Kle=n(vl,"P",{});var HJr=s(Kle);vjo=r(HJr,"Examples:"),HJr.forEach(t),Tjo=i(vl),m(z3.$$.fragment,vl),vl.forEach(t),Fjo=i(bl),We=n(bl,"DIV",{class:!0});var Yt=s(We);m(V3.$$.fragment,Yt),Cjo=i(Yt),Zle=n(Yt,"P",{});var UJr=s(Zle);Mjo=r(UJr,"Instantiate one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) from a pretrained model."),UJr.forEach(t),Ejo=i(Yt),on=n(Yt,"P",{});var E4=s(on);yjo=r(E4,"The model class to instantiate is selected based on the "),eie=n(E4,"CODE",{});var JJr=s(eie);wjo=r(JJr,"model_type"),JJr.forEach(t),Ajo=r(E4,` property of the config object (either
passed as an argument or loaded from `),oie=n(E4,"CODE",{});var YJr=s(oie);Ljo=r(YJr,"pretrained_model_name_or_path"),YJr.forEach(t),Bjo=r(E4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),rie=n(E4,"CODE",{});var KJr=s(rie);kjo=r(KJr,"pretrained_model_name_or_path"),KJr.forEach(t),xjo=r(E4,":"),E4.forEach(t),Rjo=i(Yt),W3=n(Yt,"UL",{});var ike=s(W3);ov=n(ike,"LI",{});var P3e=s(ov);tie=n(P3e,"STRONG",{});var ZJr=s(tie);Sjo=r(ZJr,"speech-encoder-decoder"),ZJr.forEach(t),Pjo=r(P3e," \u2014 "),MN=n(P3e,"A",{href:!0});var eYr=s(MN);$jo=r(eYr,"SpeechEncoderDecoderModel"),eYr.forEach(t),Ijo=r(P3e," (Speech Encoder decoder model)"),P3e.forEach(t),jjo=i(ike),rv=n(ike,"LI",{});var $3e=s(rv);aie=n($3e,"STRONG",{});var oYr=s(aie);Njo=r(oYr,"speech_to_text"),oYr.forEach(t),Djo=r($3e," \u2014 "),EN=n($3e,"A",{href:!0});var rYr=s(EN);qjo=r(rYr,"Speech2TextForConditionalGeneration"),rYr.forEach(t),Gjo=r($3e," (Speech2Text model)"),$3e.forEach(t),ike.forEach(t),Ojo=i(Yt),tv=n(Yt,"P",{});var I3e=s(tv);Xjo=r(I3e,"The model is set in evaluation mode by default using "),nie=n(I3e,"CODE",{});var tYr=s(nie);zjo=r(tYr,"model.eval()"),tYr.forEach(t),Vjo=r(I3e,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),sie=n(I3e,"CODE",{});var aYr=s(sie);Wjo=r(aYr,"model.train()"),aYr.forEach(t),I3e.forEach(t),Qjo=i(Yt),lie=n(Yt,"P",{});var nYr=s(lie);Hjo=r(nYr,"Examples:"),nYr.forEach(t),Ujo=i(Yt),m(Q3.$$.fragment,Yt),Yt.forEach(t),bl.forEach(t),o8e=i(d),Dd=n(d,"H2",{class:!0});var dke=s(Dd);av=n(dke,"A",{id:!0,class:!0,href:!0});var sYr=s(av);iie=n(sYr,"SPAN",{});var lYr=s(iie);m(H3.$$.fragment,lYr),lYr.forEach(t),sYr.forEach(t),Jjo=i(dke),die=n(dke,"SPAN",{});var iYr=s(die);Yjo=r(iYr,"AutoModelForAudioXVector"),iYr.forEach(t),dke.forEach(t),r8e=i(d),ir=n(d,"DIV",{class:!0});var Tl=s(ir);m(U3.$$.fragment,Tl),Kjo=i(Tl),qd=n(Tl,"P",{});var xz=s(qd);Zjo=r(xz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a audio retrieval via x-vector head) when created
with the `),cie=n(xz,"CODE",{});var dYr=s(cie);eNo=r(dYr,"from_pretrained()"),dYr.forEach(t),oNo=r(xz,"class method or the "),fie=n(xz,"CODE",{});var cYr=s(fie);rNo=r(cYr,"from_config()"),cYr.forEach(t),tNo=r(xz,`class
method.`),xz.forEach(t),aNo=i(Tl),J3=n(Tl,"P",{});var cke=s(J3);nNo=r(cke,"This class cannot be instantiated directly using "),mie=n(cke,"CODE",{});var fYr=s(mie);sNo=r(fYr,"__init__()"),fYr.forEach(t),lNo=r(cke," (throws an error)."),cke.forEach(t),iNo=i(Tl),ot=n(Tl,"DIV",{class:!0});var Fl=s(ot);m(Y3.$$.fragment,Fl),dNo=i(Fl),gie=n(Fl,"P",{});var mYr=s(gie);cNo=r(mYr,"Instantiates one of the model classes of the library (with a audio retrieval via x-vector head) from a configuration."),mYr.forEach(t),fNo=i(Fl),Gd=n(Fl,"P",{});var Rz=s(Gd);mNo=r(Rz,`Note:
Loading a model from its configuration file does `),hie=n(Rz,"STRONG",{});var gYr=s(hie);gNo=r(gYr,"not"),gYr.forEach(t),hNo=r(Rz,` load the model weights. It only affects the
model\u2019s configuration. Use `),pie=n(Rz,"CODE",{});var hYr=s(pie);pNo=r(hYr,"from_pretrained()"),hYr.forEach(t),_No=r(Rz,"to load the model weights."),Rz.forEach(t),uNo=i(Fl),_ie=n(Fl,"P",{});var pYr=s(_ie);bNo=r(pYr,"Examples:"),pYr.forEach(t),vNo=i(Fl),m(K3.$$.fragment,Fl),Fl.forEach(t),TNo=i(Tl),Qe=n(Tl,"DIV",{class:!0});var Kt=s(Qe);m(Z3.$$.fragment,Kt),FNo=i(Kt),uie=n(Kt,"P",{});var _Yr=s(uie);CNo=r(_Yr,"Instantiate one of the model classes of the library (with a audio retrieval via x-vector head) from a pretrained model."),_Yr.forEach(t),MNo=i(Kt),rn=n(Kt,"P",{});var y4=s(rn);ENo=r(y4,"The model class to instantiate is selected based on the "),bie=n(y4,"CODE",{});var uYr=s(bie);yNo=r(uYr,"model_type"),uYr.forEach(t),wNo=r(y4,` property of the config object (either
passed as an argument or loaded from `),vie=n(y4,"CODE",{});var bYr=s(vie);ANo=r(bYr,"pretrained_model_name_or_path"),bYr.forEach(t),LNo=r(y4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Tie=n(y4,"CODE",{});var vYr=s(Tie);BNo=r(vYr,"pretrained_model_name_or_path"),vYr.forEach(t),kNo=r(y4,":"),y4.forEach(t),xNo=i(Kt),Od=n(Kt,"UL",{});var Sz=s(Od);nv=n(Sz,"LI",{});var j3e=s(nv);Fie=n(j3e,"STRONG",{});var TYr=s(Fie);RNo=r(TYr,"unispeech-sat"),TYr.forEach(t),SNo=r(j3e," \u2014 "),yN=n(j3e,"A",{href:!0});var FYr=s(yN);PNo=r(FYr,"UniSpeechSatForXVector"),FYr.forEach(t),$No=r(j3e," (UniSpeechSat model)"),j3e.forEach(t),INo=i(Sz),sv=n(Sz,"LI",{});var N3e=s(sv);Cie=n(N3e,"STRONG",{});var CYr=s(Cie);jNo=r(CYr,"wav2vec2"),CYr.forEach(t),NNo=r(N3e," \u2014 "),wN=n(N3e,"A",{href:!0});var MYr=s(wN);DNo=r(MYr,"Wav2Vec2ForXVector"),MYr.forEach(t),qNo=r(N3e," (Wav2Vec2 model)"),N3e.forEach(t),GNo=i(Sz),lv=n(Sz,"LI",{});var D3e=s(lv);Mie=n(D3e,"STRONG",{});var EYr=s(Mie);ONo=r(EYr,"wavlm"),EYr.forEach(t),XNo=r(D3e," \u2014 "),AN=n(D3e,"A",{href:!0});var yYr=s(AN);zNo=r(yYr,"WavLMForXVector"),yYr.forEach(t),VNo=r(D3e," (WavLM model)"),D3e.forEach(t),Sz.forEach(t),WNo=i(Kt),iv=n(Kt,"P",{});var q3e=s(iv);QNo=r(q3e,"The model is set in evaluation mode by default using "),Eie=n(q3e,"CODE",{});var wYr=s(Eie);HNo=r(wYr,"model.eval()"),wYr.forEach(t),UNo=r(q3e,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),yie=n(q3e,"CODE",{});var AYr=s(yie);JNo=r(AYr,"model.train()"),AYr.forEach(t),q3e.forEach(t),YNo=i(Kt),wie=n(Kt,"P",{});var LYr=s(wie);KNo=r(LYr,"Examples:"),LYr.forEach(t),ZNo=i(Kt),m(ey.$$.fragment,Kt),Kt.forEach(t),Tl.forEach(t),t8e=i(d),Xd=n(d,"H2",{class:!0});var fke=s(Xd);dv=n(fke,"A",{id:!0,class:!0,href:!0});var BYr=s(dv);Aie=n(BYr,"SPAN",{});var kYr=s(Aie);m(oy.$$.fragment,kYr),kYr.forEach(t),BYr.forEach(t),eDo=i(fke),Lie=n(fke,"SPAN",{});var xYr=s(Lie);oDo=r(xYr,"AutoModelForMaskedImageModeling"),xYr.forEach(t),fke.forEach(t),a8e=i(d),dr=n(d,"DIV",{class:!0});var Cl=s(dr);m(ry.$$.fragment,Cl),rDo=i(Cl),zd=n(Cl,"P",{});var Pz=s(zd);tDo=r(Pz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a masked image modeling head) when created
with the `),Bie=n(Pz,"CODE",{});var RYr=s(Bie);aDo=r(RYr,"from_pretrained()"),RYr.forEach(t),nDo=r(Pz,"class method or the "),kie=n(Pz,"CODE",{});var SYr=s(kie);sDo=r(SYr,"from_config()"),SYr.forEach(t),lDo=r(Pz,`class
method.`),Pz.forEach(t),iDo=i(Cl),ty=n(Cl,"P",{});var mke=s(ty);dDo=r(mke,"This class cannot be instantiated directly using "),xie=n(mke,"CODE",{});var PYr=s(xie);cDo=r(PYr,"__init__()"),PYr.forEach(t),fDo=r(mke," (throws an error)."),mke.forEach(t),mDo=i(Cl),rt=n(Cl,"DIV",{class:!0});var Ml=s(rt);m(ay.$$.fragment,Ml),gDo=i(Ml),Rie=n(Ml,"P",{});var $Yr=s(Rie);hDo=r($Yr,"Instantiates one of the model classes of the library (with a masked image modeling head) from a configuration."),$Yr.forEach(t),pDo=i(Ml),Vd=n(Ml,"P",{});var $z=s(Vd);_Do=r($z,`Note:
Loading a model from its configuration file does `),Sie=n($z,"STRONG",{});var IYr=s(Sie);uDo=r(IYr,"not"),IYr.forEach(t),bDo=r($z,` load the model weights. It only affects the
model\u2019s configuration. Use `),Pie=n($z,"CODE",{});var jYr=s(Pie);vDo=r(jYr,"from_pretrained()"),jYr.forEach(t),TDo=r($z,"to load the model weights."),$z.forEach(t),FDo=i(Ml),$ie=n(Ml,"P",{});var NYr=s($ie);CDo=r(NYr,"Examples:"),NYr.forEach(t),MDo=i(Ml),m(ny.$$.fragment,Ml),Ml.forEach(t),EDo=i(Cl),He=n(Cl,"DIV",{class:!0});var Zt=s(He);m(sy.$$.fragment,Zt),yDo=i(Zt),Iie=n(Zt,"P",{});var DYr=s(Iie);wDo=r(DYr,"Instantiate one of the model classes of the library (with a masked image modeling head) from a pretrained model."),DYr.forEach(t),ADo=i(Zt),tn=n(Zt,"P",{});var w4=s(tn);LDo=r(w4,"The model class to instantiate is selected based on the "),jie=n(w4,"CODE",{});var qYr=s(jie);BDo=r(qYr,"model_type"),qYr.forEach(t),kDo=r(w4,` property of the config object (either
passed as an argument or loaded from `),Nie=n(w4,"CODE",{});var GYr=s(Nie);xDo=r(GYr,"pretrained_model_name_or_path"),GYr.forEach(t),RDo=r(w4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Die=n(w4,"CODE",{});var OYr=s(Die);SDo=r(OYr,"pretrained_model_name_or_path"),OYr.forEach(t),PDo=r(w4,":"),w4.forEach(t),$Do=i(Zt),Wd=n(Zt,"UL",{});var Iz=s(Wd);cv=n(Iz,"LI",{});var G3e=s(cv);qie=n(G3e,"STRONG",{});var XYr=s(qie);IDo=r(XYr,"deit"),XYr.forEach(t),jDo=r(G3e," \u2014 "),LN=n(G3e,"A",{href:!0});var zYr=s(LN);NDo=r(zYr,"DeiTForMaskedImageModeling"),zYr.forEach(t),DDo=r(G3e," (DeiT model)"),G3e.forEach(t),qDo=i(Iz),fv=n(Iz,"LI",{});var O3e=s(fv);Gie=n(O3e,"STRONG",{});var VYr=s(Gie);GDo=r(VYr,"swin"),VYr.forEach(t),ODo=r(O3e," \u2014 "),BN=n(O3e,"A",{href:!0});var WYr=s(BN);XDo=r(WYr,"SwinForMaskedImageModeling"),WYr.forEach(t),zDo=r(O3e," (Swin model)"),O3e.forEach(t),VDo=i(Iz),mv=n(Iz,"LI",{});var X3e=s(mv);Oie=n(X3e,"STRONG",{});var QYr=s(Oie);WDo=r(QYr,"vit"),QYr.forEach(t),QDo=r(X3e," \u2014 "),kN=n(X3e,"A",{href:!0});var HYr=s(kN);HDo=r(HYr,"ViTForMaskedImageModeling"),HYr.forEach(t),UDo=r(X3e," (ViT model)"),X3e.forEach(t),Iz.forEach(t),JDo=i(Zt),gv=n(Zt,"P",{});var z3e=s(gv);YDo=r(z3e,"The model is set in evaluation mode by default using "),Xie=n(z3e,"CODE",{});var UYr=s(Xie);KDo=r(UYr,"model.eval()"),UYr.forEach(t),ZDo=r(z3e,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),zie=n(z3e,"CODE",{});var JYr=s(zie);eqo=r(JYr,"model.train()"),JYr.forEach(t),z3e.forEach(t),oqo=i(Zt),Vie=n(Zt,"P",{});var YYr=s(Vie);rqo=r(YYr,"Examples:"),YYr.forEach(t),tqo=i(Zt),m(ly.$$.fragment,Zt),Zt.forEach(t),Cl.forEach(t),n8e=i(d),Qd=n(d,"H2",{class:!0});var gke=s(Qd);hv=n(gke,"A",{id:!0,class:!0,href:!0});var KYr=s(hv);Wie=n(KYr,"SPAN",{});var ZYr=s(Wie);m(iy.$$.fragment,ZYr),ZYr.forEach(t),KYr.forEach(t),aqo=i(gke),Qie=n(gke,"SPAN",{});var eKr=s(Qie);nqo=r(eKr,"AutoModelForObjectDetection"),eKr.forEach(t),gke.forEach(t),s8e=i(d),cr=n(d,"DIV",{class:!0});var El=s(cr);m(dy.$$.fragment,El),sqo=i(El),Hd=n(El,"P",{});var jz=s(Hd);lqo=r(jz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a object detection head) when created
with the `),Hie=n(jz,"CODE",{});var oKr=s(Hie);iqo=r(oKr,"from_pretrained()"),oKr.forEach(t),dqo=r(jz,"class method or the "),Uie=n(jz,"CODE",{});var rKr=s(Uie);cqo=r(rKr,"from_config()"),rKr.forEach(t),fqo=r(jz,`class
method.`),jz.forEach(t),mqo=i(El),cy=n(El,"P",{});var hke=s(cy);gqo=r(hke,"This class cannot be instantiated directly using "),Jie=n(hke,"CODE",{});var tKr=s(Jie);hqo=r(tKr,"__init__()"),tKr.forEach(t),pqo=r(hke," (throws an error)."),hke.forEach(t),_qo=i(El),tt=n(El,"DIV",{class:!0});var yl=s(tt);m(fy.$$.fragment,yl),uqo=i(yl),Yie=n(yl,"P",{});var aKr=s(Yie);bqo=r(aKr,"Instantiates one of the model classes of the library (with a object detection head) from a configuration."),aKr.forEach(t),vqo=i(yl),Ud=n(yl,"P",{});var Nz=s(Ud);Tqo=r(Nz,`Note:
Loading a model from its configuration file does `),Kie=n(Nz,"STRONG",{});var nKr=s(Kie);Fqo=r(nKr,"not"),nKr.forEach(t),Cqo=r(Nz,` load the model weights. It only affects the
model\u2019s configuration. Use `),Zie=n(Nz,"CODE",{});var sKr=s(Zie);Mqo=r(sKr,"from_pretrained()"),sKr.forEach(t),Eqo=r(Nz,"to load the model weights."),Nz.forEach(t),yqo=i(yl),ede=n(yl,"P",{});var lKr=s(ede);wqo=r(lKr,"Examples:"),lKr.forEach(t),Aqo=i(yl),m(my.$$.fragment,yl),yl.forEach(t),Lqo=i(El),Ue=n(El,"DIV",{class:!0});var ea=s(Ue);m(gy.$$.fragment,ea),Bqo=i(ea),ode=n(ea,"P",{});var iKr=s(ode);kqo=r(iKr,"Instantiate one of the model classes of the library (with a object detection head) from a pretrained model."),iKr.forEach(t),xqo=i(ea),an=n(ea,"P",{});var A4=s(an);Rqo=r(A4,"The model class to instantiate is selected based on the "),rde=n(A4,"CODE",{});var dKr=s(rde);Sqo=r(dKr,"model_type"),dKr.forEach(t),Pqo=r(A4,` property of the config object (either
passed as an argument or loaded from `),tde=n(A4,"CODE",{});var cKr=s(tde);$qo=r(cKr,"pretrained_model_name_or_path"),cKr.forEach(t),Iqo=r(A4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),ade=n(A4,"CODE",{});var fKr=s(ade);jqo=r(fKr,"pretrained_model_name_or_path"),fKr.forEach(t),Nqo=r(A4,":"),A4.forEach(t),Dqo=i(ea),nde=n(ea,"UL",{});var mKr=s(nde);pv=n(mKr,"LI",{});var V3e=s(pv);sde=n(V3e,"STRONG",{});var gKr=s(sde);qqo=r(gKr,"detr"),gKr.forEach(t),Gqo=r(V3e," \u2014 "),xN=n(V3e,"A",{href:!0});var hKr=s(xN);Oqo=r(hKr,"DetrForObjectDetection"),hKr.forEach(t),Xqo=r(V3e," (DETR model)"),V3e.forEach(t),mKr.forEach(t),zqo=i(ea),_v=n(ea,"P",{});var W3e=s(_v);Vqo=r(W3e,"The model is set in evaluation mode by default using "),lde=n(W3e,"CODE",{});var pKr=s(lde);Wqo=r(pKr,"model.eval()"),pKr.forEach(t),Qqo=r(W3e,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),ide=n(W3e,"CODE",{});var _Kr=s(ide);Hqo=r(_Kr,"model.train()"),_Kr.forEach(t),W3e.forEach(t),Uqo=i(ea),dde=n(ea,"P",{});var uKr=s(dde);Jqo=r(uKr,"Examples:"),uKr.forEach(t),Yqo=i(ea),m(hy.$$.fragment,ea),ea.forEach(t),El.forEach(t),l8e=i(d),Jd=n(d,"H2",{class:!0});var pke=s(Jd);uv=n(pke,"A",{id:!0,class:!0,href:!0});var bKr=s(uv);cde=n(bKr,"SPAN",{});var vKr=s(cde);m(py.$$.fragment,vKr),vKr.forEach(t),bKr.forEach(t),Kqo=i(pke),fde=n(pke,"SPAN",{});var TKr=s(fde);Zqo=r(TKr,"AutoModelForImageSegmentation"),TKr.forEach(t),pke.forEach(t),i8e=i(d),fr=n(d,"DIV",{class:!0});var wl=s(fr);m(_y.$$.fragment,wl),eGo=i(wl),Yd=n(wl,"P",{});var Dz=s(Yd);oGo=r(Dz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a image segmentation head) when created
with the `),mde=n(Dz,"CODE",{});var FKr=s(mde);rGo=r(FKr,"from_pretrained()"),FKr.forEach(t),tGo=r(Dz,"class method or the "),gde=n(Dz,"CODE",{});var CKr=s(gde);aGo=r(CKr,"from_config()"),CKr.forEach(t),nGo=r(Dz,`class
method.`),Dz.forEach(t),sGo=i(wl),uy=n(wl,"P",{});var _ke=s(uy);lGo=r(_ke,"This class cannot be instantiated directly using "),hde=n(_ke,"CODE",{});var MKr=s(hde);iGo=r(MKr,"__init__()"),MKr.forEach(t),dGo=r(_ke," (throws an error)."),_ke.forEach(t),cGo=i(wl),at=n(wl,"DIV",{class:!0});var Al=s(at);m(by.$$.fragment,Al),fGo=i(Al),pde=n(Al,"P",{});var EKr=s(pde);mGo=r(EKr,"Instantiates one of the model classes of the library (with a image segmentation head) from a configuration."),EKr.forEach(t),gGo=i(Al),Kd=n(Al,"P",{});var qz=s(Kd);hGo=r(qz,`Note:
Loading a model from its configuration file does `),_de=n(qz,"STRONG",{});var yKr=s(_de);pGo=r(yKr,"not"),yKr.forEach(t),_Go=r(qz,` load the model weights. It only affects the
model\u2019s configuration. Use `),ude=n(qz,"CODE",{});var wKr=s(ude);uGo=r(wKr,"from_pretrained()"),wKr.forEach(t),bGo=r(qz,"to load the model weights."),qz.forEach(t),vGo=i(Al),bde=n(Al,"P",{});var AKr=s(bde);TGo=r(AKr,"Examples:"),AKr.forEach(t),FGo=i(Al),m(vy.$$.fragment,Al),Al.forEach(t),CGo=i(wl),Je=n(wl,"DIV",{class:!0});var oa=s(Je);m(Ty.$$.fragment,oa),MGo=i(oa),vde=n(oa,"P",{});var LKr=s(vde);EGo=r(LKr,"Instantiate one of the model classes of the library (with a image segmentation head) from a pretrained model."),LKr.forEach(t),yGo=i(oa),nn=n(oa,"P",{});var L4=s(nn);wGo=r(L4,"The model class to instantiate is selected based on the "),Tde=n(L4,"CODE",{});var BKr=s(Tde);AGo=r(BKr,"model_type"),BKr.forEach(t),LGo=r(L4,` property of the config object (either
passed as an argument or loaded from `),Fde=n(L4,"CODE",{});var kKr=s(Fde);BGo=r(kKr,"pretrained_model_name_or_path"),kKr.forEach(t),kGo=r(L4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Cde=n(L4,"CODE",{});var xKr=s(Cde);xGo=r(xKr,"pretrained_model_name_or_path"),xKr.forEach(t),RGo=r(L4,":"),L4.forEach(t),SGo=i(oa),Mde=n(oa,"UL",{});var RKr=s(Mde);bv=n(RKr,"LI",{});var Q3e=s(bv);Ede=n(Q3e,"STRONG",{});var SKr=s(Ede);PGo=r(SKr,"detr"),SKr.forEach(t),$Go=r(Q3e," \u2014 "),RN=n(Q3e,"A",{href:!0});var PKr=s(RN);IGo=r(PKr,"DetrForSegmentation"),PKr.forEach(t),jGo=r(Q3e," (DETR model)"),Q3e.forEach(t),RKr.forEach(t),NGo=i(oa),vv=n(oa,"P",{});var H3e=s(vv);DGo=r(H3e,"The model is set in evaluation mode by default using "),yde=n(H3e,"CODE",{});var $Kr=s(yde);qGo=r($Kr,"model.eval()"),$Kr.forEach(t),GGo=r(H3e,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),wde=n(H3e,"CODE",{});var IKr=s(wde);OGo=r(IKr,"model.train()"),IKr.forEach(t),H3e.forEach(t),XGo=i(oa),Ade=n(oa,"P",{});var jKr=s(Ade);zGo=r(jKr,"Examples:"),jKr.forEach(t),VGo=i(oa),m(Fy.$$.fragment,oa),oa.forEach(t),wl.forEach(t),d8e=i(d),Zd=n(d,"H2",{class:!0});var uke=s(Zd);Tv=n(uke,"A",{id:!0,class:!0,href:!0});var NKr=s(Tv);Lde=n(NKr,"SPAN",{});var DKr=s(Lde);m(Cy.$$.fragment,DKr),DKr.forEach(t),NKr.forEach(t),WGo=i(uke),Bde=n(uke,"SPAN",{});var qKr=s(Bde);QGo=r(qKr,"AutoModelForSemanticSegmentation"),qKr.forEach(t),uke.forEach(t),c8e=i(d),mr=n(d,"DIV",{class:!0});var Ll=s(mr);m(My.$$.fragment,Ll),HGo=i(Ll),ec=n(Ll,"P",{});var Gz=s(ec);UGo=r(Gz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a semantic segmentation head) when created
with the `),kde=n(Gz,"CODE",{});var GKr=s(kde);JGo=r(GKr,"from_pretrained()"),GKr.forEach(t),YGo=r(Gz,"class method or the "),xde=n(Gz,"CODE",{});var OKr=s(xde);KGo=r(OKr,"from_config()"),OKr.forEach(t),ZGo=r(Gz,`class
method.`),Gz.forEach(t),eOo=i(Ll),Ey=n(Ll,"P",{});var bke=s(Ey);oOo=r(bke,"This class cannot be instantiated directly using "),Rde=n(bke,"CODE",{});var XKr=s(Rde);rOo=r(XKr,"__init__()"),XKr.forEach(t),tOo=r(bke," (throws an error)."),bke.forEach(t),aOo=i(Ll),nt=n(Ll,"DIV",{class:!0});var Bl=s(nt);m(yy.$$.fragment,Bl),nOo=i(Bl),Sde=n(Bl,"P",{});var zKr=s(Sde);sOo=r(zKr,"Instantiates one of the model classes of the library (with a semantic segmentation head) from a configuration."),zKr.forEach(t),lOo=i(Bl),oc=n(Bl,"P",{});var Oz=s(oc);iOo=r(Oz,`Note:
Loading a model from its configuration file does `),Pde=n(Oz,"STRONG",{});var VKr=s(Pde);dOo=r(VKr,"not"),VKr.forEach(t),cOo=r(Oz,` load the model weights. It only affects the
model\u2019s configuration. Use `),$de=n(Oz,"CODE",{});var WKr=s($de);fOo=r(WKr,"from_pretrained()"),WKr.forEach(t),mOo=r(Oz,"to load the model weights."),Oz.forEach(t),gOo=i(Bl),Ide=n(Bl,"P",{});var QKr=s(Ide);hOo=r(QKr,"Examples:"),QKr.forEach(t),pOo=i(Bl),m(wy.$$.fragment,Bl),Bl.forEach(t),_Oo=i(Ll),Ye=n(Ll,"DIV",{class:!0});var ra=s(Ye);m(Ay.$$.fragment,ra),uOo=i(ra),jde=n(ra,"P",{});var HKr=s(jde);bOo=r(HKr,"Instantiate one of the model classes of the library (with a semantic segmentation head) from a pretrained model."),HKr.forEach(t),vOo=i(ra),sn=n(ra,"P",{});var B4=s(sn);TOo=r(B4,"The model class to instantiate is selected based on the "),Nde=n(B4,"CODE",{});var UKr=s(Nde);FOo=r(UKr,"model_type"),UKr.forEach(t),COo=r(B4,` property of the config object (either
passed as an argument or loaded from `),Dde=n(B4,"CODE",{});var JKr=s(Dde);MOo=r(JKr,"pretrained_model_name_or_path"),JKr.forEach(t),EOo=r(B4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),qde=n(B4,"CODE",{});var YKr=s(qde);yOo=r(YKr,"pretrained_model_name_or_path"),YKr.forEach(t),wOo=r(B4,":"),B4.forEach(t),AOo=i(ra),Ly=n(ra,"UL",{});var vke=s(Ly);Fv=n(vke,"LI",{});var U3e=s(Fv);Gde=n(U3e,"STRONG",{});var KKr=s(Gde);LOo=r(KKr,"beit"),KKr.forEach(t),BOo=r(U3e," \u2014 "),SN=n(U3e,"A",{href:!0});var ZKr=s(SN);kOo=r(ZKr,"BeitForSemanticSegmentation"),ZKr.forEach(t),xOo=r(U3e," (BEiT model)"),U3e.forEach(t),ROo=i(vke),Cv=n(vke,"LI",{});var J3e=s(Cv);Ode=n(J3e,"STRONG",{});var eZr=s(Ode);SOo=r(eZr,"segformer"),eZr.forEach(t),POo=r(J3e," \u2014 "),PN=n(J3e,"A",{href:!0});var oZr=s(PN);$Oo=r(oZr,"SegformerForSemanticSegmentation"),oZr.forEach(t),IOo=r(J3e," (SegFormer model)"),J3e.forEach(t),vke.forEach(t),jOo=i(ra),Mv=n(ra,"P",{});var Y3e=s(Mv);NOo=r(Y3e,"The model is set in evaluation mode by default using "),Xde=n(Y3e,"CODE",{});var rZr=s(Xde);DOo=r(rZr,"model.eval()"),rZr.forEach(t),qOo=r(Y3e,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),zde=n(Y3e,"CODE",{});var tZr=s(zde);GOo=r(tZr,"model.train()"),tZr.forEach(t),Y3e.forEach(t),OOo=i(ra),Vde=n(ra,"P",{});var aZr=s(Vde);XOo=r(aZr,"Examples:"),aZr.forEach(t),zOo=i(ra),m(By.$$.fragment,ra),ra.forEach(t),Ll.forEach(t),f8e=i(d),rc=n(d,"H2",{class:!0});var Tke=s(rc);Ev=n(Tke,"A",{id:!0,class:!0,href:!0});var nZr=s(Ev);Wde=n(nZr,"SPAN",{});var sZr=s(Wde);m(ky.$$.fragment,sZr),sZr.forEach(t),nZr.forEach(t),VOo=i(Tke),Qde=n(Tke,"SPAN",{});var lZr=s(Qde);WOo=r(lZr,"TFAutoModel"),lZr.forEach(t),Tke.forEach(t),m8e=i(d),gr=n(d,"DIV",{class:!0});var kl=s(gr);m(xy.$$.fragment,kl),QOo=i(kl),tc=n(kl,"P",{});var Xz=s(tc);HOo=r(Xz,`This is a generic model class that will be instantiated as one of the base model classes of the library when created
with the `),Hde=n(Xz,"CODE",{});var iZr=s(Hde);UOo=r(iZr,"from_pretrained()"),iZr.forEach(t),JOo=r(Xz,"class method or the "),Ude=n(Xz,"CODE",{});var dZr=s(Ude);YOo=r(dZr,"from_config()"),dZr.forEach(t),KOo=r(Xz,`class
method.`),Xz.forEach(t),ZOo=i(kl),Ry=n(kl,"P",{});var Fke=s(Ry);eXo=r(Fke,"This class cannot be instantiated directly using "),Jde=n(Fke,"CODE",{});var cZr=s(Jde);oXo=r(cZr,"__init__()"),cZr.forEach(t),rXo=r(Fke," (throws an error)."),Fke.forEach(t),tXo=i(kl),st=n(kl,"DIV",{class:!0});var xl=s(st);m(Sy.$$.fragment,xl),aXo=i(xl),Yde=n(xl,"P",{});var fZr=s(Yde);nXo=r(fZr,"Instantiates one of the base model classes of the library from a configuration."),fZr.forEach(t),sXo=i(xl),ac=n(xl,"P",{});var zz=s(ac);lXo=r(zz,`Note:
Loading a model from its configuration file does `),Kde=n(zz,"STRONG",{});var mZr=s(Kde);iXo=r(mZr,"not"),mZr.forEach(t),dXo=r(zz,` load the model weights. It only affects the
model\u2019s configuration. Use `),Zde=n(zz,"CODE",{});var gZr=s(Zde);cXo=r(gZr,"from_pretrained()"),gZr.forEach(t),fXo=r(zz,"to load the model weights."),zz.forEach(t),mXo=i(xl),ece=n(xl,"P",{});var hZr=s(ece);gXo=r(hZr,"Examples:"),hZr.forEach(t),hXo=i(xl),m(Py.$$.fragment,xl),xl.forEach(t),pXo=i(kl),go=n(kl,"DIV",{class:!0});var ca=s(go);m($y.$$.fragment,ca),_Xo=i(ca),oce=n(ca,"P",{});var pZr=s(oce);uXo=r(pZr,"Instantiate one of the base model classes of the library from a pretrained model."),pZr.forEach(t),bXo=i(ca),ln=n(ca,"P",{});var k4=s(ln);vXo=r(k4,"The model class to instantiate is selected based on the "),rce=n(k4,"CODE",{});var _Zr=s(rce);TXo=r(_Zr,"model_type"),_Zr.forEach(t),FXo=r(k4,` property of the config object (either
passed as an argument or loaded from `),tce=n(k4,"CODE",{});var uZr=s(tce);CXo=r(uZr,"pretrained_model_name_or_path"),uZr.forEach(t),MXo=r(k4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),ace=n(k4,"CODE",{});var bZr=s(ace);EXo=r(bZr,"pretrained_model_name_or_path"),bZr.forEach(t),yXo=r(k4,":"),k4.forEach(t),wXo=i(ca),B=n(ca,"UL",{});var k=s(B);yv=n(k,"LI",{});var K3e=s(yv);nce=n(K3e,"STRONG",{});var vZr=s(nce);AXo=r(vZr,"albert"),vZr.forEach(t),LXo=r(K3e," \u2014 "),$N=n(K3e,"A",{href:!0});var TZr=s($N);BXo=r(TZr,"TFAlbertModel"),TZr.forEach(t),kXo=r(K3e," (ALBERT model)"),K3e.forEach(t),xXo=i(k),wv=n(k,"LI",{});var Z3e=s(wv);sce=n(Z3e,"STRONG",{});var FZr=s(sce);RXo=r(FZr,"bart"),FZr.forEach(t),SXo=r(Z3e," \u2014 "),IN=n(Z3e,"A",{href:!0});var CZr=s(IN);PXo=r(CZr,"TFBartModel"),CZr.forEach(t),$Xo=r(Z3e," (BART model)"),Z3e.forEach(t),IXo=i(k),Av=n(k,"LI",{});var eye=s(Av);lce=n(eye,"STRONG",{});var MZr=s(lce);jXo=r(MZr,"bert"),MZr.forEach(t),NXo=r(eye," \u2014 "),jN=n(eye,"A",{href:!0});var EZr=s(jN);DXo=r(EZr,"TFBertModel"),EZr.forEach(t),qXo=r(eye," (BERT model)"),eye.forEach(t),GXo=i(k),Lv=n(k,"LI",{});var oye=s(Lv);ice=n(oye,"STRONG",{});var yZr=s(ice);OXo=r(yZr,"blenderbot"),yZr.forEach(t),XXo=r(oye," \u2014 "),NN=n(oye,"A",{href:!0});var wZr=s(NN);zXo=r(wZr,"TFBlenderbotModel"),wZr.forEach(t),VXo=r(oye," (Blenderbot model)"),oye.forEach(t),WXo=i(k),Bv=n(k,"LI",{});var rye=s(Bv);dce=n(rye,"STRONG",{});var AZr=s(dce);QXo=r(AZr,"blenderbot-small"),AZr.forEach(t),HXo=r(rye," \u2014 "),DN=n(rye,"A",{href:!0});var LZr=s(DN);UXo=r(LZr,"TFBlenderbotSmallModel"),LZr.forEach(t),JXo=r(rye," (BlenderbotSmall model)"),rye.forEach(t),YXo=i(k),kv=n(k,"LI",{});var tye=s(kv);cce=n(tye,"STRONG",{});var BZr=s(cce);KXo=r(BZr,"camembert"),BZr.forEach(t),ZXo=r(tye," \u2014 "),qN=n(tye,"A",{href:!0});var kZr=s(qN);ezo=r(kZr,"TFCamembertModel"),kZr.forEach(t),ozo=r(tye," (CamemBERT model)"),tye.forEach(t),rzo=i(k),xv=n(k,"LI",{});var aye=s(xv);fce=n(aye,"STRONG",{});var xZr=s(fce);tzo=r(xZr,"clip"),xZr.forEach(t),azo=r(aye," \u2014 "),GN=n(aye,"A",{href:!0});var RZr=s(GN);nzo=r(RZr,"TFCLIPModel"),RZr.forEach(t),szo=r(aye," (CLIP model)"),aye.forEach(t),lzo=i(k),Rv=n(k,"LI",{});var nye=s(Rv);mce=n(nye,"STRONG",{});var SZr=s(mce);izo=r(SZr,"convbert"),SZr.forEach(t),dzo=r(nye," \u2014 "),ON=n(nye,"A",{href:!0});var PZr=s(ON);czo=r(PZr,"TFConvBertModel"),PZr.forEach(t),fzo=r(nye," (ConvBERT model)"),nye.forEach(t),mzo=i(k),Sv=n(k,"LI",{});var sye=s(Sv);gce=n(sye,"STRONG",{});var $Zr=s(gce);gzo=r($Zr,"ctrl"),$Zr.forEach(t),hzo=r(sye," \u2014 "),XN=n(sye,"A",{href:!0});var IZr=s(XN);pzo=r(IZr,"TFCTRLModel"),IZr.forEach(t),_zo=r(sye," (CTRL model)"),sye.forEach(t),uzo=i(k),Pv=n(k,"LI",{});var lye=s(Pv);hce=n(lye,"STRONG",{});var jZr=s(hce);bzo=r(jZr,"deberta"),jZr.forEach(t),vzo=r(lye," \u2014 "),zN=n(lye,"A",{href:!0});var NZr=s(zN);Tzo=r(NZr,"TFDebertaModel"),NZr.forEach(t),Fzo=r(lye," (DeBERTa model)"),lye.forEach(t),Czo=i(k),$v=n(k,"LI",{});var iye=s($v);pce=n(iye,"STRONG",{});var DZr=s(pce);Mzo=r(DZr,"deberta-v2"),DZr.forEach(t),Ezo=r(iye," \u2014 "),VN=n(iye,"A",{href:!0});var qZr=s(VN);yzo=r(qZr,"TFDebertaV2Model"),qZr.forEach(t),wzo=r(iye," (DeBERTa-v2 model)"),iye.forEach(t),Azo=i(k),Iv=n(k,"LI",{});var dye=s(Iv);_ce=n(dye,"STRONG",{});var GZr=s(_ce);Lzo=r(GZr,"distilbert"),GZr.forEach(t),Bzo=r(dye," \u2014 "),WN=n(dye,"A",{href:!0});var OZr=s(WN);kzo=r(OZr,"TFDistilBertModel"),OZr.forEach(t),xzo=r(dye," (DistilBERT model)"),dye.forEach(t),Rzo=i(k),jv=n(k,"LI",{});var cye=s(jv);uce=n(cye,"STRONG",{});var XZr=s(uce);Szo=r(XZr,"dpr"),XZr.forEach(t),Pzo=r(cye," \u2014 "),QN=n(cye,"A",{href:!0});var zZr=s(QN);$zo=r(zZr,"TFDPRQuestionEncoder"),zZr.forEach(t),Izo=r(cye," (DPR model)"),cye.forEach(t),jzo=i(k),Nv=n(k,"LI",{});var fye=s(Nv);bce=n(fye,"STRONG",{});var VZr=s(bce);Nzo=r(VZr,"electra"),VZr.forEach(t),Dzo=r(fye," \u2014 "),HN=n(fye,"A",{href:!0});var WZr=s(HN);qzo=r(WZr,"TFElectraModel"),WZr.forEach(t),Gzo=r(fye," (ELECTRA model)"),fye.forEach(t),Ozo=i(k),Dv=n(k,"LI",{});var mye=s(Dv);vce=n(mye,"STRONG",{});var QZr=s(vce);Xzo=r(QZr,"flaubert"),QZr.forEach(t),zzo=r(mye," \u2014 "),UN=n(mye,"A",{href:!0});var HZr=s(UN);Vzo=r(HZr,"TFFlaubertModel"),HZr.forEach(t),Wzo=r(mye," (FlauBERT model)"),mye.forEach(t),Qzo=i(k),Ps=n(k,"LI",{});var q0=s(Ps);Tce=n(q0,"STRONG",{});var UZr=s(Tce);Hzo=r(UZr,"funnel"),UZr.forEach(t),Uzo=r(q0," \u2014 "),JN=n(q0,"A",{href:!0});var JZr=s(JN);Jzo=r(JZr,"TFFunnelModel"),JZr.forEach(t),Yzo=r(q0," or "),YN=n(q0,"A",{href:!0});var YZr=s(YN);Kzo=r(YZr,"TFFunnelBaseModel"),YZr.forEach(t),Zzo=r(q0," (Funnel Transformer model)"),q0.forEach(t),eVo=i(k),qv=n(k,"LI",{});var gye=s(qv);Fce=n(gye,"STRONG",{});var KZr=s(Fce);oVo=r(KZr,"gpt2"),KZr.forEach(t),rVo=r(gye," \u2014 "),KN=n(gye,"A",{href:!0});var ZZr=s(KN);tVo=r(ZZr,"TFGPT2Model"),ZZr.forEach(t),aVo=r(gye," (OpenAI GPT-2 model)"),gye.forEach(t),nVo=i(k),Gv=n(k,"LI",{});var hye=s(Gv);Cce=n(hye,"STRONG",{});var eet=s(Cce);sVo=r(eet,"hubert"),eet.forEach(t),lVo=r(hye," \u2014 "),ZN=n(hye,"A",{href:!0});var oet=s(ZN);iVo=r(oet,"TFHubertModel"),oet.forEach(t),dVo=r(hye," (Hubert model)"),hye.forEach(t),cVo=i(k),Ov=n(k,"LI",{});var pye=s(Ov);Mce=n(pye,"STRONG",{});var ret=s(Mce);fVo=r(ret,"layoutlm"),ret.forEach(t),mVo=r(pye," \u2014 "),eD=n(pye,"A",{href:!0});var tet=s(eD);gVo=r(tet,"TFLayoutLMModel"),tet.forEach(t),hVo=r(pye," (LayoutLM model)"),pye.forEach(t),pVo=i(k),Xv=n(k,"LI",{});var _ye=s(Xv);Ece=n(_ye,"STRONG",{});var aet=s(Ece);_Vo=r(aet,"led"),aet.forEach(t),uVo=r(_ye," \u2014 "),oD=n(_ye,"A",{href:!0});var net=s(oD);bVo=r(net,"TFLEDModel"),net.forEach(t),vVo=r(_ye," (LED model)"),_ye.forEach(t),TVo=i(k),zv=n(k,"LI",{});var uye=s(zv);yce=n(uye,"STRONG",{});var set=s(yce);FVo=r(set,"longformer"),set.forEach(t),CVo=r(uye," \u2014 "),rD=n(uye,"A",{href:!0});var iet=s(rD);MVo=r(iet,"TFLongformerModel"),iet.forEach(t),EVo=r(uye," (Longformer model)"),uye.forEach(t),yVo=i(k),Vv=n(k,"LI",{});var bye=s(Vv);wce=n(bye,"STRONG",{});var det=s(wce);wVo=r(det,"lxmert"),det.forEach(t),AVo=r(bye," \u2014 "),tD=n(bye,"A",{href:!0});var cet=s(tD);LVo=r(cet,"TFLxmertModel"),cet.forEach(t),BVo=r(bye," (LXMERT model)"),bye.forEach(t),kVo=i(k),Wv=n(k,"LI",{});var vye=s(Wv);Ace=n(vye,"STRONG",{});var fet=s(Ace);xVo=r(fet,"marian"),fet.forEach(t),RVo=r(vye," \u2014 "),aD=n(vye,"A",{href:!0});var met=s(aD);SVo=r(met,"TFMarianModel"),met.forEach(t),PVo=r(vye," (Marian model)"),vye.forEach(t),$Vo=i(k),Qv=n(k,"LI",{});var Tye=s(Qv);Lce=n(Tye,"STRONG",{});var get=s(Lce);IVo=r(get,"mbart"),get.forEach(t),jVo=r(Tye," \u2014 "),nD=n(Tye,"A",{href:!0});var het=s(nD);NVo=r(het,"TFMBartModel"),het.forEach(t),DVo=r(Tye," (mBART model)"),Tye.forEach(t),qVo=i(k),Hv=n(k,"LI",{});var Fye=s(Hv);Bce=n(Fye,"STRONG",{});var pet=s(Bce);GVo=r(pet,"mobilebert"),pet.forEach(t),OVo=r(Fye," \u2014 "),sD=n(Fye,"A",{href:!0});var _et=s(sD);XVo=r(_et,"TFMobileBertModel"),_et.forEach(t),zVo=r(Fye," (MobileBERT model)"),Fye.forEach(t),VVo=i(k),Uv=n(k,"LI",{});var Cye=s(Uv);kce=n(Cye,"STRONG",{});var uet=s(kce);WVo=r(uet,"mpnet"),uet.forEach(t),QVo=r(Cye," \u2014 "),lD=n(Cye,"A",{href:!0});var bet=s(lD);HVo=r(bet,"TFMPNetModel"),bet.forEach(t),UVo=r(Cye," (MPNet model)"),Cye.forEach(t),JVo=i(k),Jv=n(k,"LI",{});var Mye=s(Jv);xce=n(Mye,"STRONG",{});var vet=s(xce);YVo=r(vet,"mt5"),vet.forEach(t),KVo=r(Mye," \u2014 "),iD=n(Mye,"A",{href:!0});var Tet=s(iD);ZVo=r(Tet,"TFMT5Model"),Tet.forEach(t),eWo=r(Mye," (mT5 model)"),Mye.forEach(t),oWo=i(k),Yv=n(k,"LI",{});var Eye=s(Yv);Rce=n(Eye,"STRONG",{});var Fet=s(Rce);rWo=r(Fet,"openai-gpt"),Fet.forEach(t),tWo=r(Eye," \u2014 "),dD=n(Eye,"A",{href:!0});var Cet=s(dD);aWo=r(Cet,"TFOpenAIGPTModel"),Cet.forEach(t),nWo=r(Eye," (OpenAI GPT model)"),Eye.forEach(t),sWo=i(k),Kv=n(k,"LI",{});var yye=s(Kv);Sce=n(yye,"STRONG",{});var Met=s(Sce);lWo=r(Met,"pegasus"),Met.forEach(t),iWo=r(yye," \u2014 "),cD=n(yye,"A",{href:!0});var Eet=s(cD);dWo=r(Eet,"TFPegasusModel"),Eet.forEach(t),cWo=r(yye," (Pegasus model)"),yye.forEach(t),fWo=i(k),Zv=n(k,"LI",{});var wye=s(Zv);Pce=n(wye,"STRONG",{});var yet=s(Pce);mWo=r(yet,"rembert"),yet.forEach(t),gWo=r(wye," \u2014 "),fD=n(wye,"A",{href:!0});var wet=s(fD);hWo=r(wet,"TFRemBertModel"),wet.forEach(t),pWo=r(wye," (RemBERT model)"),wye.forEach(t),_Wo=i(k),eT=n(k,"LI",{});var Aye=s(eT);$ce=n(Aye,"STRONG",{});var Aet=s($ce);uWo=r(Aet,"roberta"),Aet.forEach(t),bWo=r(Aye," \u2014 "),mD=n(Aye,"A",{href:!0});var Let=s(mD);vWo=r(Let,"TFRobertaModel"),Let.forEach(t),TWo=r(Aye," (RoBERTa model)"),Aye.forEach(t),FWo=i(k),oT=n(k,"LI",{});var Lye=s(oT);Ice=n(Lye,"STRONG",{});var Bet=s(Ice);CWo=r(Bet,"roformer"),Bet.forEach(t),MWo=r(Lye," \u2014 "),gD=n(Lye,"A",{href:!0});var ket=s(gD);EWo=r(ket,"TFRoFormerModel"),ket.forEach(t),yWo=r(Lye," (RoFormer model)"),Lye.forEach(t),wWo=i(k),rT=n(k,"LI",{});var Bye=s(rT);jce=n(Bye,"STRONG",{});var xet=s(jce);AWo=r(xet,"speech_to_text"),xet.forEach(t),LWo=r(Bye," \u2014 "),hD=n(Bye,"A",{href:!0});var Ret=s(hD);BWo=r(Ret,"TFSpeech2TextModel"),Ret.forEach(t),kWo=r(Bye," (Speech2Text model)"),Bye.forEach(t),xWo=i(k),tT=n(k,"LI",{});var kye=s(tT);Nce=n(kye,"STRONG",{});var Set=s(Nce);RWo=r(Set,"t5"),Set.forEach(t),SWo=r(kye," \u2014 "),pD=n(kye,"A",{href:!0});var Pet=s(pD);PWo=r(Pet,"TFT5Model"),Pet.forEach(t),$Wo=r(kye," (T5 model)"),kye.forEach(t),IWo=i(k),aT=n(k,"LI",{});var xye=s(aT);Dce=n(xye,"STRONG",{});var $et=s(Dce);jWo=r($et,"tapas"),$et.forEach(t),NWo=r(xye," \u2014 "),_D=n(xye,"A",{href:!0});var Iet=s(_D);DWo=r(Iet,"TFTapasModel"),Iet.forEach(t),qWo=r(xye," (TAPAS model)"),xye.forEach(t),GWo=i(k),nT=n(k,"LI",{});var Rye=s(nT);qce=n(Rye,"STRONG",{});var jet=s(qce);OWo=r(jet,"transfo-xl"),jet.forEach(t),XWo=r(Rye," \u2014 "),uD=n(Rye,"A",{href:!0});var Net=s(uD);zWo=r(Net,"TFTransfoXLModel"),Net.forEach(t),VWo=r(Rye," (Transformer-XL model)"),Rye.forEach(t),WWo=i(k),sT=n(k,"LI",{});var Sye=s(sT);Gce=n(Sye,"STRONG",{});var Det=s(Gce);QWo=r(Det,"vit"),Det.forEach(t),HWo=r(Sye," \u2014 "),bD=n(Sye,"A",{href:!0});var qet=s(bD);UWo=r(qet,"TFViTModel"),qet.forEach(t),JWo=r(Sye," (ViT model)"),Sye.forEach(t),YWo=i(k),lT=n(k,"LI",{});var Pye=s(lT);Oce=n(Pye,"STRONG",{});var Get=s(Oce);KWo=r(Get,"wav2vec2"),Get.forEach(t),ZWo=r(Pye," \u2014 "),vD=n(Pye,"A",{href:!0});var Oet=s(vD);eQo=r(Oet,"TFWav2Vec2Model"),Oet.forEach(t),oQo=r(Pye," (Wav2Vec2 model)"),Pye.forEach(t),rQo=i(k),iT=n(k,"LI",{});var $ye=s(iT);Xce=n($ye,"STRONG",{});var Xet=s(Xce);tQo=r(Xet,"xlm"),Xet.forEach(t),aQo=r($ye," \u2014 "),TD=n($ye,"A",{href:!0});var zet=s(TD);nQo=r(zet,"TFXLMModel"),zet.forEach(t),sQo=r($ye," (XLM model)"),$ye.forEach(t),lQo=i(k),dT=n(k,"LI",{});var Iye=s(dT);zce=n(Iye,"STRONG",{});var Vet=s(zce);iQo=r(Vet,"xlm-roberta"),Vet.forEach(t),dQo=r(Iye," \u2014 "),FD=n(Iye,"A",{href:!0});var Wet=s(FD);cQo=r(Wet,"TFXLMRobertaModel"),Wet.forEach(t),fQo=r(Iye," (XLM-RoBERTa model)"),Iye.forEach(t),mQo=i(k),cT=n(k,"LI",{});var jye=s(cT);Vce=n(jye,"STRONG",{});var Qet=s(Vce);gQo=r(Qet,"xlnet"),Qet.forEach(t),hQo=r(jye," \u2014 "),CD=n(jye,"A",{href:!0});var Het=s(CD);pQo=r(Het,"TFXLNetModel"),Het.forEach(t),_Qo=r(jye," (XLNet model)"),jye.forEach(t),k.forEach(t),uQo=i(ca),Wce=n(ca,"P",{});var Uet=s(Wce);bQo=r(Uet,"Examples:"),Uet.forEach(t),vQo=i(ca),m(Iy.$$.fragment,ca),ca.forEach(t),kl.forEach(t),g8e=i(d),nc=n(d,"H2",{class:!0});var Cke=s(nc);fT=n(Cke,"A",{id:!0,class:!0,href:!0});var Jet=s(fT);Qce=n(Jet,"SPAN",{});var Yet=s(Qce);m(jy.$$.fragment,Yet),Yet.forEach(t),Jet.forEach(t),TQo=i(Cke),Hce=n(Cke,"SPAN",{});var Ket=s(Hce);FQo=r(Ket,"TFAutoModelForPreTraining"),Ket.forEach(t),Cke.forEach(t),h8e=i(d),hr=n(d,"DIV",{class:!0});var Rl=s(hr);m(Ny.$$.fragment,Rl),CQo=i(Rl),sc=n(Rl,"P",{});var Vz=s(sc);MQo=r(Vz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a pretraining head) when created
with the `),Uce=n(Vz,"CODE",{});var Zet=s(Uce);EQo=r(Zet,"from_pretrained()"),Zet.forEach(t),yQo=r(Vz,"class method or the "),Jce=n(Vz,"CODE",{});var eot=s(Jce);wQo=r(eot,"from_config()"),eot.forEach(t),AQo=r(Vz,`class
method.`),Vz.forEach(t),LQo=i(Rl),Dy=n(Rl,"P",{});var Mke=s(Dy);BQo=r(Mke,"This class cannot be instantiated directly using "),Yce=n(Mke,"CODE",{});var oot=s(Yce);kQo=r(oot,"__init__()"),oot.forEach(t),xQo=r(Mke," (throws an error)."),Mke.forEach(t),RQo=i(Rl),lt=n(Rl,"DIV",{class:!0});var Sl=s(lt);m(qy.$$.fragment,Sl),SQo=i(Sl),Kce=n(Sl,"P",{});var rot=s(Kce);PQo=r(rot,"Instantiates one of the model classes of the library (with a pretraining head) from a configuration."),rot.forEach(t),$Qo=i(Sl),lc=n(Sl,"P",{});var Wz=s(lc);IQo=r(Wz,`Note:
Loading a model from its configuration file does `),Zce=n(Wz,"STRONG",{});var tot=s(Zce);jQo=r(tot,"not"),tot.forEach(t),NQo=r(Wz,` load the model weights. It only affects the
model\u2019s configuration. Use `),efe=n(Wz,"CODE",{});var aot=s(efe);DQo=r(aot,"from_pretrained()"),aot.forEach(t),qQo=r(Wz,"to load the model weights."),Wz.forEach(t),GQo=i(Sl),ofe=n(Sl,"P",{});var not=s(ofe);OQo=r(not,"Examples:"),not.forEach(t),XQo=i(Sl),m(Gy.$$.fragment,Sl),Sl.forEach(t),zQo=i(Rl),ho=n(Rl,"DIV",{class:!0});var fa=s(ho);m(Oy.$$.fragment,fa),VQo=i(fa),rfe=n(fa,"P",{});var sot=s(rfe);WQo=r(sot,"Instantiate one of the model classes of the library (with a pretraining head) from a pretrained model."),sot.forEach(t),QQo=i(fa),dn=n(fa,"P",{});var x4=s(dn);HQo=r(x4,"The model class to instantiate is selected based on the "),tfe=n(x4,"CODE",{});var lot=s(tfe);UQo=r(lot,"model_type"),lot.forEach(t),JQo=r(x4,` property of the config object (either
passed as an argument or loaded from `),afe=n(x4,"CODE",{});var iot=s(afe);YQo=r(iot,"pretrained_model_name_or_path"),iot.forEach(t),KQo=r(x4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),nfe=n(x4,"CODE",{});var dot=s(nfe);ZQo=r(dot,"pretrained_model_name_or_path"),dot.forEach(t),eHo=r(x4,":"),x4.forEach(t),oHo=i(fa),H=n(fa,"UL",{});var U=s(H);mT=n(U,"LI",{});var Nye=s(mT);sfe=n(Nye,"STRONG",{});var cot=s(sfe);rHo=r(cot,"albert"),cot.forEach(t),tHo=r(Nye," \u2014 "),MD=n(Nye,"A",{href:!0});var fot=s(MD);aHo=r(fot,"TFAlbertForPreTraining"),fot.forEach(t),nHo=r(Nye," (ALBERT model)"),Nye.forEach(t),sHo=i(U),gT=n(U,"LI",{});var Dye=s(gT);lfe=n(Dye,"STRONG",{});var mot=s(lfe);lHo=r(mot,"bart"),mot.forEach(t),iHo=r(Dye," \u2014 "),ED=n(Dye,"A",{href:!0});var got=s(ED);dHo=r(got,"TFBartForConditionalGeneration"),got.forEach(t),cHo=r(Dye," (BART model)"),Dye.forEach(t),fHo=i(U),hT=n(U,"LI",{});var qye=s(hT);ife=n(qye,"STRONG",{});var hot=s(ife);mHo=r(hot,"bert"),hot.forEach(t),gHo=r(qye," \u2014 "),yD=n(qye,"A",{href:!0});var pot=s(yD);hHo=r(pot,"TFBertForPreTraining"),pot.forEach(t),pHo=r(qye," (BERT model)"),qye.forEach(t),_Ho=i(U),pT=n(U,"LI",{});var Gye=s(pT);dfe=n(Gye,"STRONG",{});var _ot=s(dfe);uHo=r(_ot,"camembert"),_ot.forEach(t),bHo=r(Gye," \u2014 "),wD=n(Gye,"A",{href:!0});var uot=s(wD);vHo=r(uot,"TFCamembertForMaskedLM"),uot.forEach(t),THo=r(Gye," (CamemBERT model)"),Gye.forEach(t),FHo=i(U),_T=n(U,"LI",{});var Oye=s(_T);cfe=n(Oye,"STRONG",{});var bot=s(cfe);CHo=r(bot,"ctrl"),bot.forEach(t),MHo=r(Oye," \u2014 "),AD=n(Oye,"A",{href:!0});var vot=s(AD);EHo=r(vot,"TFCTRLLMHeadModel"),vot.forEach(t),yHo=r(Oye," (CTRL model)"),Oye.forEach(t),wHo=i(U),uT=n(U,"LI",{});var Xye=s(uT);ffe=n(Xye,"STRONG",{});var Tot=s(ffe);AHo=r(Tot,"distilbert"),Tot.forEach(t),LHo=r(Xye," \u2014 "),LD=n(Xye,"A",{href:!0});var Fot=s(LD);BHo=r(Fot,"TFDistilBertForMaskedLM"),Fot.forEach(t),kHo=r(Xye," (DistilBERT model)"),Xye.forEach(t),xHo=i(U),bT=n(U,"LI",{});var zye=s(bT);mfe=n(zye,"STRONG",{});var Cot=s(mfe);RHo=r(Cot,"electra"),Cot.forEach(t),SHo=r(zye," \u2014 "),BD=n(zye,"A",{href:!0});var Mot=s(BD);PHo=r(Mot,"TFElectraForPreTraining"),Mot.forEach(t),$Ho=r(zye," (ELECTRA model)"),zye.forEach(t),IHo=i(U),vT=n(U,"LI",{});var Vye=s(vT);gfe=n(Vye,"STRONG",{});var Eot=s(gfe);jHo=r(Eot,"flaubert"),Eot.forEach(t),NHo=r(Vye," \u2014 "),kD=n(Vye,"A",{href:!0});var yot=s(kD);DHo=r(yot,"TFFlaubertWithLMHeadModel"),yot.forEach(t),qHo=r(Vye," (FlauBERT model)"),Vye.forEach(t),GHo=i(U),TT=n(U,"LI",{});var Wye=s(TT);hfe=n(Wye,"STRONG",{});var wot=s(hfe);OHo=r(wot,"funnel"),wot.forEach(t),XHo=r(Wye," \u2014 "),xD=n(Wye,"A",{href:!0});var Aot=s(xD);zHo=r(Aot,"TFFunnelForPreTraining"),Aot.forEach(t),VHo=r(Wye," (Funnel Transformer model)"),Wye.forEach(t),WHo=i(U),FT=n(U,"LI",{});var Qye=s(FT);pfe=n(Qye,"STRONG",{});var Lot=s(pfe);QHo=r(Lot,"gpt2"),Lot.forEach(t),HHo=r(Qye," \u2014 "),RD=n(Qye,"A",{href:!0});var Bot=s(RD);UHo=r(Bot,"TFGPT2LMHeadModel"),Bot.forEach(t),JHo=r(Qye," (OpenAI GPT-2 model)"),Qye.forEach(t),YHo=i(U),CT=n(U,"LI",{});var Hye=s(CT);_fe=n(Hye,"STRONG",{});var kot=s(_fe);KHo=r(kot,"layoutlm"),kot.forEach(t),ZHo=r(Hye," \u2014 "),SD=n(Hye,"A",{href:!0});var xot=s(SD);eUo=r(xot,"TFLayoutLMForMaskedLM"),xot.forEach(t),oUo=r(Hye," (LayoutLM model)"),Hye.forEach(t),rUo=i(U),MT=n(U,"LI",{});var Uye=s(MT);ufe=n(Uye,"STRONG",{});var Rot=s(ufe);tUo=r(Rot,"lxmert"),Rot.forEach(t),aUo=r(Uye," \u2014 "),PD=n(Uye,"A",{href:!0});var Sot=s(PD);nUo=r(Sot,"TFLxmertForPreTraining"),Sot.forEach(t),sUo=r(Uye," (LXMERT model)"),Uye.forEach(t),lUo=i(U),ET=n(U,"LI",{});var Jye=s(ET);bfe=n(Jye,"STRONG",{});var Pot=s(bfe);iUo=r(Pot,"mobilebert"),Pot.forEach(t),dUo=r(Jye," \u2014 "),$D=n(Jye,"A",{href:!0});var $ot=s($D);cUo=r($ot,"TFMobileBertForPreTraining"),$ot.forEach(t),fUo=r(Jye," (MobileBERT model)"),Jye.forEach(t),mUo=i(U),yT=n(U,"LI",{});var Yye=s(yT);vfe=n(Yye,"STRONG",{});var Iot=s(vfe);gUo=r(Iot,"mpnet"),Iot.forEach(t),hUo=r(Yye," \u2014 "),ID=n(Yye,"A",{href:!0});var jot=s(ID);pUo=r(jot,"TFMPNetForMaskedLM"),jot.forEach(t),_Uo=r(Yye," (MPNet model)"),Yye.forEach(t),uUo=i(U),wT=n(U,"LI",{});var Kye=s(wT);Tfe=n(Kye,"STRONG",{});var Not=s(Tfe);bUo=r(Not,"openai-gpt"),Not.forEach(t),vUo=r(Kye," \u2014 "),jD=n(Kye,"A",{href:!0});var Dot=s(jD);TUo=r(Dot,"TFOpenAIGPTLMHeadModel"),Dot.forEach(t),FUo=r(Kye," (OpenAI GPT model)"),Kye.forEach(t),CUo=i(U),AT=n(U,"LI",{});var Zye=s(AT);Ffe=n(Zye,"STRONG",{});var qot=s(Ffe);MUo=r(qot,"roberta"),qot.forEach(t),EUo=r(Zye," \u2014 "),ND=n(Zye,"A",{href:!0});var Got=s(ND);yUo=r(Got,"TFRobertaForMaskedLM"),Got.forEach(t),wUo=r(Zye," (RoBERTa model)"),Zye.forEach(t),AUo=i(U),LT=n(U,"LI",{});var ewe=s(LT);Cfe=n(ewe,"STRONG",{});var Oot=s(Cfe);LUo=r(Oot,"t5"),Oot.forEach(t),BUo=r(ewe," \u2014 "),DD=n(ewe,"A",{href:!0});var Xot=s(DD);kUo=r(Xot,"TFT5ForConditionalGeneration"),Xot.forEach(t),xUo=r(ewe," (T5 model)"),ewe.forEach(t),RUo=i(U),BT=n(U,"LI",{});var owe=s(BT);Mfe=n(owe,"STRONG",{});var zot=s(Mfe);SUo=r(zot,"tapas"),zot.forEach(t),PUo=r(owe," \u2014 "),qD=n(owe,"A",{href:!0});var Vot=s(qD);$Uo=r(Vot,"TFTapasForMaskedLM"),Vot.forEach(t),IUo=r(owe," (TAPAS model)"),owe.forEach(t),jUo=i(U),kT=n(U,"LI",{});var rwe=s(kT);Efe=n(rwe,"STRONG",{});var Wot=s(Efe);NUo=r(Wot,"transfo-xl"),Wot.forEach(t),DUo=r(rwe," \u2014 "),GD=n(rwe,"A",{href:!0});var Qot=s(GD);qUo=r(Qot,"TFTransfoXLLMHeadModel"),Qot.forEach(t),GUo=r(rwe," (Transformer-XL model)"),rwe.forEach(t),OUo=i(U),xT=n(U,"LI",{});var twe=s(xT);yfe=n(twe,"STRONG",{});var Hot=s(yfe);XUo=r(Hot,"xlm"),Hot.forEach(t),zUo=r(twe," \u2014 "),OD=n(twe,"A",{href:!0});var Uot=s(OD);VUo=r(Uot,"TFXLMWithLMHeadModel"),Uot.forEach(t),WUo=r(twe," (XLM model)"),twe.forEach(t),QUo=i(U),RT=n(U,"LI",{});var awe=s(RT);wfe=n(awe,"STRONG",{});var Jot=s(wfe);HUo=r(Jot,"xlm-roberta"),Jot.forEach(t),UUo=r(awe," \u2014 "),XD=n(awe,"A",{href:!0});var Yot=s(XD);JUo=r(Yot,"TFXLMRobertaForMaskedLM"),Yot.forEach(t),YUo=r(awe," (XLM-RoBERTa model)"),awe.forEach(t),KUo=i(U),ST=n(U,"LI",{});var nwe=s(ST);Afe=n(nwe,"STRONG",{});var Kot=s(Afe);ZUo=r(Kot,"xlnet"),Kot.forEach(t),eJo=r(nwe," \u2014 "),zD=n(nwe,"A",{href:!0});var Zot=s(zD);oJo=r(Zot,"TFXLNetLMHeadModel"),Zot.forEach(t),rJo=r(nwe," (XLNet model)"),nwe.forEach(t),U.forEach(t),tJo=i(fa),Lfe=n(fa,"P",{});var ert=s(Lfe);aJo=r(ert,"Examples:"),ert.forEach(t),nJo=i(fa),m(Xy.$$.fragment,fa),fa.forEach(t),Rl.forEach(t),p8e=i(d),ic=n(d,"H2",{class:!0});var Eke=s(ic);PT=n(Eke,"A",{id:!0,class:!0,href:!0});var ort=s(PT);Bfe=n(ort,"SPAN",{});var rrt=s(Bfe);m(zy.$$.fragment,rrt),rrt.forEach(t),ort.forEach(t),sJo=i(Eke),kfe=n(Eke,"SPAN",{});var trt=s(kfe);lJo=r(trt,"TFAutoModelForCausalLM"),trt.forEach(t),Eke.forEach(t),_8e=i(d),pr=n(d,"DIV",{class:!0});var Pl=s(pr);m(Vy.$$.fragment,Pl),iJo=i(Pl),dc=n(Pl,"P",{});var Qz=s(dc);dJo=r(Qz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a causal language modeling head) when created
with the `),xfe=n(Qz,"CODE",{});var art=s(xfe);cJo=r(art,"from_pretrained()"),art.forEach(t),fJo=r(Qz,"class method or the "),Rfe=n(Qz,"CODE",{});var nrt=s(Rfe);mJo=r(nrt,"from_config()"),nrt.forEach(t),gJo=r(Qz,`class
method.`),Qz.forEach(t),hJo=i(Pl),Wy=n(Pl,"P",{});var yke=s(Wy);pJo=r(yke,"This class cannot be instantiated directly using "),Sfe=n(yke,"CODE",{});var srt=s(Sfe);_Jo=r(srt,"__init__()"),srt.forEach(t),uJo=r(yke," (throws an error)."),yke.forEach(t),bJo=i(Pl),it=n(Pl,"DIV",{class:!0});var $l=s(it);m(Qy.$$.fragment,$l),vJo=i($l),Pfe=n($l,"P",{});var lrt=s(Pfe);TJo=r(lrt,"Instantiates one of the model classes of the library (with a causal language modeling head) from a configuration."),lrt.forEach(t),FJo=i($l),cc=n($l,"P",{});var Hz=s(cc);CJo=r(Hz,`Note:
Loading a model from its configuration file does `),$fe=n(Hz,"STRONG",{});var irt=s($fe);MJo=r(irt,"not"),irt.forEach(t),EJo=r(Hz,` load the model weights. It only affects the
model\u2019s configuration. Use `),Ife=n(Hz,"CODE",{});var drt=s(Ife);yJo=r(drt,"from_pretrained()"),drt.forEach(t),wJo=r(Hz,"to load the model weights."),Hz.forEach(t),AJo=i($l),jfe=n($l,"P",{});var crt=s(jfe);LJo=r(crt,"Examples:"),crt.forEach(t),BJo=i($l),m(Hy.$$.fragment,$l),$l.forEach(t),kJo=i(Pl),po=n(Pl,"DIV",{class:!0});var ma=s(po);m(Uy.$$.fragment,ma),xJo=i(ma),Nfe=n(ma,"P",{});var frt=s(Nfe);RJo=r(frt,"Instantiate one of the model classes of the library (with a causal language modeling head) from a pretrained model."),frt.forEach(t),SJo=i(ma),cn=n(ma,"P",{});var R4=s(cn);PJo=r(R4,"The model class to instantiate is selected based on the "),Dfe=n(R4,"CODE",{});var mrt=s(Dfe);$Jo=r(mrt,"model_type"),mrt.forEach(t),IJo=r(R4,` property of the config object (either
passed as an argument or loaded from `),qfe=n(R4,"CODE",{});var grt=s(qfe);jJo=r(grt,"pretrained_model_name_or_path"),grt.forEach(t),NJo=r(R4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Gfe=n(R4,"CODE",{});var hrt=s(Gfe);DJo=r(hrt,"pretrained_model_name_or_path"),hrt.forEach(t),qJo=r(R4,":"),R4.forEach(t),GJo=i(ma),he=n(ma,"UL",{});var Me=s(he);$T=n(Me,"LI",{});var swe=s($T);Ofe=n(swe,"STRONG",{});var prt=s(Ofe);OJo=r(prt,"bert"),prt.forEach(t),XJo=r(swe," \u2014 "),VD=n(swe,"A",{href:!0});var _rt=s(VD);zJo=r(_rt,"TFBertLMHeadModel"),_rt.forEach(t),VJo=r(swe," (BERT model)"),swe.forEach(t),WJo=i(Me),IT=n(Me,"LI",{});var lwe=s(IT);Xfe=n(lwe,"STRONG",{});var urt=s(Xfe);QJo=r(urt,"ctrl"),urt.forEach(t),HJo=r(lwe," \u2014 "),WD=n(lwe,"A",{href:!0});var brt=s(WD);UJo=r(brt,"TFCTRLLMHeadModel"),brt.forEach(t),JJo=r(lwe," (CTRL model)"),lwe.forEach(t),YJo=i(Me),jT=n(Me,"LI",{});var iwe=s(jT);zfe=n(iwe,"STRONG",{});var vrt=s(zfe);KJo=r(vrt,"gpt2"),vrt.forEach(t),ZJo=r(iwe," \u2014 "),QD=n(iwe,"A",{href:!0});var Trt=s(QD);eYo=r(Trt,"TFGPT2LMHeadModel"),Trt.forEach(t),oYo=r(iwe," (OpenAI GPT-2 model)"),iwe.forEach(t),rYo=i(Me),NT=n(Me,"LI",{});var dwe=s(NT);Vfe=n(dwe,"STRONG",{});var Frt=s(Vfe);tYo=r(Frt,"openai-gpt"),Frt.forEach(t),aYo=r(dwe," \u2014 "),HD=n(dwe,"A",{href:!0});var Crt=s(HD);nYo=r(Crt,"TFOpenAIGPTLMHeadModel"),Crt.forEach(t),sYo=r(dwe," (OpenAI GPT model)"),dwe.forEach(t),lYo=i(Me),DT=n(Me,"LI",{});var cwe=s(DT);Wfe=n(cwe,"STRONG",{});var Mrt=s(Wfe);iYo=r(Mrt,"rembert"),Mrt.forEach(t),dYo=r(cwe," \u2014 "),UD=n(cwe,"A",{href:!0});var Ert=s(UD);cYo=r(Ert,"TFRemBertForCausalLM"),Ert.forEach(t),fYo=r(cwe," (RemBERT model)"),cwe.forEach(t),mYo=i(Me),qT=n(Me,"LI",{});var fwe=s(qT);Qfe=n(fwe,"STRONG",{});var yrt=s(Qfe);gYo=r(yrt,"roberta"),yrt.forEach(t),hYo=r(fwe," \u2014 "),JD=n(fwe,"A",{href:!0});var wrt=s(JD);pYo=r(wrt,"TFRobertaForCausalLM"),wrt.forEach(t),_Yo=r(fwe," (RoBERTa model)"),fwe.forEach(t),uYo=i(Me),GT=n(Me,"LI",{});var mwe=s(GT);Hfe=n(mwe,"STRONG",{});var Art=s(Hfe);bYo=r(Art,"roformer"),Art.forEach(t),vYo=r(mwe," \u2014 "),YD=n(mwe,"A",{href:!0});var Lrt=s(YD);TYo=r(Lrt,"TFRoFormerForCausalLM"),Lrt.forEach(t),FYo=r(mwe," (RoFormer model)"),mwe.forEach(t),CYo=i(Me),OT=n(Me,"LI",{});var gwe=s(OT);Ufe=n(gwe,"STRONG",{});var Brt=s(Ufe);MYo=r(Brt,"transfo-xl"),Brt.forEach(t),EYo=r(gwe," \u2014 "),KD=n(gwe,"A",{href:!0});var krt=s(KD);yYo=r(krt,"TFTransfoXLLMHeadModel"),krt.forEach(t),wYo=r(gwe," (Transformer-XL model)"),gwe.forEach(t),AYo=i(Me),XT=n(Me,"LI",{});var hwe=s(XT);Jfe=n(hwe,"STRONG",{});var xrt=s(Jfe);LYo=r(xrt,"xlm"),xrt.forEach(t),BYo=r(hwe," \u2014 "),ZD=n(hwe,"A",{href:!0});var Rrt=s(ZD);kYo=r(Rrt,"TFXLMWithLMHeadModel"),Rrt.forEach(t),xYo=r(hwe," (XLM model)"),hwe.forEach(t),RYo=i(Me),zT=n(Me,"LI",{});var pwe=s(zT);Yfe=n(pwe,"STRONG",{});var Srt=s(Yfe);SYo=r(Srt,"xlnet"),Srt.forEach(t),PYo=r(pwe," \u2014 "),eq=n(pwe,"A",{href:!0});var Prt=s(eq);$Yo=r(Prt,"TFXLNetLMHeadModel"),Prt.forEach(t),IYo=r(pwe," (XLNet model)"),pwe.forEach(t),Me.forEach(t),jYo=i(ma),Kfe=n(ma,"P",{});var $rt=s(Kfe);NYo=r($rt,"Examples:"),$rt.forEach(t),DYo=i(ma),m(Jy.$$.fragment,ma),ma.forEach(t),Pl.forEach(t),u8e=i(d),fc=n(d,"H2",{class:!0});var wke=s(fc);VT=n(wke,"A",{id:!0,class:!0,href:!0});var Irt=s(VT);Zfe=n(Irt,"SPAN",{});var jrt=s(Zfe);m(Yy.$$.fragment,jrt),jrt.forEach(t),Irt.forEach(t),qYo=i(wke),eme=n(wke,"SPAN",{});var Nrt=s(eme);GYo=r(Nrt,"TFAutoModelForImageClassification"),Nrt.forEach(t),wke.forEach(t),b8e=i(d),_r=n(d,"DIV",{class:!0});var Il=s(_r);m(Ky.$$.fragment,Il),OYo=i(Il),mc=n(Il,"P",{});var Uz=s(mc);XYo=r(Uz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a image classification head) when created
with the `),ome=n(Uz,"CODE",{});var Drt=s(ome);zYo=r(Drt,"from_pretrained()"),Drt.forEach(t),VYo=r(Uz,"class method or the "),rme=n(Uz,"CODE",{});var qrt=s(rme);WYo=r(qrt,"from_config()"),qrt.forEach(t),QYo=r(Uz,`class
method.`),Uz.forEach(t),HYo=i(Il),Zy=n(Il,"P",{});var Ake=s(Zy);UYo=r(Ake,"This class cannot be instantiated directly using "),tme=n(Ake,"CODE",{});var Grt=s(tme);JYo=r(Grt,"__init__()"),Grt.forEach(t),YYo=r(Ake," (throws an error)."),Ake.forEach(t),KYo=i(Il),dt=n(Il,"DIV",{class:!0});var jl=s(dt);m(ew.$$.fragment,jl),ZYo=i(jl),ame=n(jl,"P",{});var Ort=s(ame);eKo=r(Ort,"Instantiates one of the model classes of the library (with a image classification head) from a configuration."),Ort.forEach(t),oKo=i(jl),gc=n(jl,"P",{});var Jz=s(gc);rKo=r(Jz,`Note:
Loading a model from its configuration file does `),nme=n(Jz,"STRONG",{});var Xrt=s(nme);tKo=r(Xrt,"not"),Xrt.forEach(t),aKo=r(Jz,` load the model weights. It only affects the
model\u2019s configuration. Use `),sme=n(Jz,"CODE",{});var zrt=s(sme);nKo=r(zrt,"from_pretrained()"),zrt.forEach(t),sKo=r(Jz,"to load the model weights."),Jz.forEach(t),lKo=i(jl),lme=n(jl,"P",{});var Vrt=s(lme);iKo=r(Vrt,"Examples:"),Vrt.forEach(t),dKo=i(jl),m(ow.$$.fragment,jl),jl.forEach(t),cKo=i(Il),_o=n(Il,"DIV",{class:!0});var ga=s(_o);m(rw.$$.fragment,ga),fKo=i(ga),ime=n(ga,"P",{});var Wrt=s(ime);mKo=r(Wrt,"Instantiate one of the model classes of the library (with a image classification head) from a pretrained model."),Wrt.forEach(t),gKo=i(ga),fn=n(ga,"P",{});var S4=s(fn);hKo=r(S4,"The model class to instantiate is selected based on the "),dme=n(S4,"CODE",{});var Qrt=s(dme);pKo=r(Qrt,"model_type"),Qrt.forEach(t),_Ko=r(S4,` property of the config object (either
passed as an argument or loaded from `),cme=n(S4,"CODE",{});var Hrt=s(cme);uKo=r(Hrt,"pretrained_model_name_or_path"),Hrt.forEach(t),bKo=r(S4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),fme=n(S4,"CODE",{});var Urt=s(fme);vKo=r(Urt,"pretrained_model_name_or_path"),Urt.forEach(t),TKo=r(S4,":"),S4.forEach(t),FKo=i(ga),mme=n(ga,"UL",{});var Jrt=s(mme);WT=n(Jrt,"LI",{});var _we=s(WT);gme=n(_we,"STRONG",{});var Yrt=s(gme);CKo=r(Yrt,"vit"),Yrt.forEach(t),MKo=r(_we," \u2014 "),oq=n(_we,"A",{href:!0});var Krt=s(oq);EKo=r(Krt,"TFViTForImageClassification"),Krt.forEach(t),yKo=r(_we," (ViT model)"),_we.forEach(t),Jrt.forEach(t),wKo=i(ga),hme=n(ga,"P",{});var Zrt=s(hme);AKo=r(Zrt,"Examples:"),Zrt.forEach(t),LKo=i(ga),m(tw.$$.fragment,ga),ga.forEach(t),Il.forEach(t),v8e=i(d),hc=n(d,"H2",{class:!0});var Lke=s(hc);QT=n(Lke,"A",{id:!0,class:!0,href:!0});var ett=s(QT);pme=n(ett,"SPAN",{});var ott=s(pme);m(aw.$$.fragment,ott),ott.forEach(t),ett.forEach(t),BKo=i(Lke),_me=n(Lke,"SPAN",{});var rtt=s(_me);kKo=r(rtt,"TFAutoModelForMaskedLM"),rtt.forEach(t),Lke.forEach(t),T8e=i(d),ur=n(d,"DIV",{class:!0});var Nl=s(ur);m(nw.$$.fragment,Nl),xKo=i(Nl),pc=n(Nl,"P",{});var Yz=s(pc);RKo=r(Yz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a masked language modeling head) when created
with the `),ume=n(Yz,"CODE",{});var ttt=s(ume);SKo=r(ttt,"from_pretrained()"),ttt.forEach(t),PKo=r(Yz,"class method or the "),bme=n(Yz,"CODE",{});var att=s(bme);$Ko=r(att,"from_config()"),att.forEach(t),IKo=r(Yz,`class
method.`),Yz.forEach(t),jKo=i(Nl),sw=n(Nl,"P",{});var Bke=s(sw);NKo=r(Bke,"This class cannot be instantiated directly using "),vme=n(Bke,"CODE",{});var ntt=s(vme);DKo=r(ntt,"__init__()"),ntt.forEach(t),qKo=r(Bke," (throws an error)."),Bke.forEach(t),GKo=i(Nl),ct=n(Nl,"DIV",{class:!0});var Dl=s(ct);m(lw.$$.fragment,Dl),OKo=i(Dl),Tme=n(Dl,"P",{});var stt=s(Tme);XKo=r(stt,"Instantiates one of the model classes of the library (with a masked language modeling head) from a configuration."),stt.forEach(t),zKo=i(Dl),_c=n(Dl,"P",{});var Kz=s(_c);VKo=r(Kz,`Note:
Loading a model from its configuration file does `),Fme=n(Kz,"STRONG",{});var ltt=s(Fme);WKo=r(ltt,"not"),ltt.forEach(t),QKo=r(Kz,` load the model weights. It only affects the
model\u2019s configuration. Use `),Cme=n(Kz,"CODE",{});var itt=s(Cme);HKo=r(itt,"from_pretrained()"),itt.forEach(t),UKo=r(Kz,"to load the model weights."),Kz.forEach(t),JKo=i(Dl),Mme=n(Dl,"P",{});var dtt=s(Mme);YKo=r(dtt,"Examples:"),dtt.forEach(t),KKo=i(Dl),m(iw.$$.fragment,Dl),Dl.forEach(t),ZKo=i(Nl),uo=n(Nl,"DIV",{class:!0});var ha=s(uo);m(dw.$$.fragment,ha),eZo=i(ha),Eme=n(ha,"P",{});var ctt=s(Eme);oZo=r(ctt,"Instantiate one of the model classes of the library (with a masked language modeling head) from a pretrained model."),ctt.forEach(t),rZo=i(ha),mn=n(ha,"P",{});var P4=s(mn);tZo=r(P4,"The model class to instantiate is selected based on the "),yme=n(P4,"CODE",{});var ftt=s(yme);aZo=r(ftt,"model_type"),ftt.forEach(t),nZo=r(P4,` property of the config object (either
passed as an argument or loaded from `),wme=n(P4,"CODE",{});var mtt=s(wme);sZo=r(mtt,"pretrained_model_name_or_path"),mtt.forEach(t),lZo=r(P4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Ame=n(P4,"CODE",{});var gtt=s(Ame);iZo=r(gtt,"pretrained_model_name_or_path"),gtt.forEach(t),dZo=r(P4,":"),P4.forEach(t),cZo=i(ha),Y=n(ha,"UL",{});var ee=s(Y);HT=n(ee,"LI",{});var uwe=s(HT);Lme=n(uwe,"STRONG",{});var htt=s(Lme);fZo=r(htt,"albert"),htt.forEach(t),mZo=r(uwe," \u2014 "),rq=n(uwe,"A",{href:!0});var ptt=s(rq);gZo=r(ptt,"TFAlbertForMaskedLM"),ptt.forEach(t),hZo=r(uwe," (ALBERT model)"),uwe.forEach(t),pZo=i(ee),UT=n(ee,"LI",{});var bwe=s(UT);Bme=n(bwe,"STRONG",{});var _tt=s(Bme);_Zo=r(_tt,"bert"),_tt.forEach(t),uZo=r(bwe," \u2014 "),tq=n(bwe,"A",{href:!0});var utt=s(tq);bZo=r(utt,"TFBertForMaskedLM"),utt.forEach(t),vZo=r(bwe," (BERT model)"),bwe.forEach(t),TZo=i(ee),JT=n(ee,"LI",{});var vwe=s(JT);kme=n(vwe,"STRONG",{});var btt=s(kme);FZo=r(btt,"camembert"),btt.forEach(t),CZo=r(vwe," \u2014 "),aq=n(vwe,"A",{href:!0});var vtt=s(aq);MZo=r(vtt,"TFCamembertForMaskedLM"),vtt.forEach(t),EZo=r(vwe," (CamemBERT model)"),vwe.forEach(t),yZo=i(ee),YT=n(ee,"LI",{});var Twe=s(YT);xme=n(Twe,"STRONG",{});var Ttt=s(xme);wZo=r(Ttt,"convbert"),Ttt.forEach(t),AZo=r(Twe," \u2014 "),nq=n(Twe,"A",{href:!0});var Ftt=s(nq);LZo=r(Ftt,"TFConvBertForMaskedLM"),Ftt.forEach(t),BZo=r(Twe," (ConvBERT model)"),Twe.forEach(t),kZo=i(ee),KT=n(ee,"LI",{});var Fwe=s(KT);Rme=n(Fwe,"STRONG",{});var Ctt=s(Rme);xZo=r(Ctt,"deberta"),Ctt.forEach(t),RZo=r(Fwe," \u2014 "),sq=n(Fwe,"A",{href:!0});var Mtt=s(sq);SZo=r(Mtt,"TFDebertaForMaskedLM"),Mtt.forEach(t),PZo=r(Fwe," (DeBERTa model)"),Fwe.forEach(t),$Zo=i(ee),ZT=n(ee,"LI",{});var Cwe=s(ZT);Sme=n(Cwe,"STRONG",{});var Ett=s(Sme);IZo=r(Ett,"deberta-v2"),Ett.forEach(t),jZo=r(Cwe," \u2014 "),lq=n(Cwe,"A",{href:!0});var ytt=s(lq);NZo=r(ytt,"TFDebertaV2ForMaskedLM"),ytt.forEach(t),DZo=r(Cwe," (DeBERTa-v2 model)"),Cwe.forEach(t),qZo=i(ee),e7=n(ee,"LI",{});var Mwe=s(e7);Pme=n(Mwe,"STRONG",{});var wtt=s(Pme);GZo=r(wtt,"distilbert"),wtt.forEach(t),OZo=r(Mwe," \u2014 "),iq=n(Mwe,"A",{href:!0});var Att=s(iq);XZo=r(Att,"TFDistilBertForMaskedLM"),Att.forEach(t),zZo=r(Mwe," (DistilBERT model)"),Mwe.forEach(t),VZo=i(ee),o7=n(ee,"LI",{});var Ewe=s(o7);$me=n(Ewe,"STRONG",{});var Ltt=s($me);WZo=r(Ltt,"electra"),Ltt.forEach(t),QZo=r(Ewe," \u2014 "),dq=n(Ewe,"A",{href:!0});var Btt=s(dq);HZo=r(Btt,"TFElectraForMaskedLM"),Btt.forEach(t),UZo=r(Ewe," (ELECTRA model)"),Ewe.forEach(t),JZo=i(ee),r7=n(ee,"LI",{});var ywe=s(r7);Ime=n(ywe,"STRONG",{});var ktt=s(Ime);YZo=r(ktt,"flaubert"),ktt.forEach(t),KZo=r(ywe," \u2014 "),cq=n(ywe,"A",{href:!0});var xtt=s(cq);ZZo=r(xtt,"TFFlaubertWithLMHeadModel"),xtt.forEach(t),eer=r(ywe," (FlauBERT model)"),ywe.forEach(t),oer=i(ee),t7=n(ee,"LI",{});var wwe=s(t7);jme=n(wwe,"STRONG",{});var Rtt=s(jme);rer=r(Rtt,"funnel"),Rtt.forEach(t),ter=r(wwe," \u2014 "),fq=n(wwe,"A",{href:!0});var Stt=s(fq);aer=r(Stt,"TFFunnelForMaskedLM"),Stt.forEach(t),ner=r(wwe," (Funnel Transformer model)"),wwe.forEach(t),ser=i(ee),a7=n(ee,"LI",{});var Awe=s(a7);Nme=n(Awe,"STRONG",{});var Ptt=s(Nme);ler=r(Ptt,"layoutlm"),Ptt.forEach(t),ier=r(Awe," \u2014 "),mq=n(Awe,"A",{href:!0});var $tt=s(mq);der=r($tt,"TFLayoutLMForMaskedLM"),$tt.forEach(t),cer=r(Awe," (LayoutLM model)"),Awe.forEach(t),fer=i(ee),n7=n(ee,"LI",{});var Lwe=s(n7);Dme=n(Lwe,"STRONG",{});var Itt=s(Dme);mer=r(Itt,"longformer"),Itt.forEach(t),ger=r(Lwe," \u2014 "),gq=n(Lwe,"A",{href:!0});var jtt=s(gq);her=r(jtt,"TFLongformerForMaskedLM"),jtt.forEach(t),per=r(Lwe," (Longformer model)"),Lwe.forEach(t),_er=i(ee),s7=n(ee,"LI",{});var Bwe=s(s7);qme=n(Bwe,"STRONG",{});var Ntt=s(qme);uer=r(Ntt,"mobilebert"),Ntt.forEach(t),ber=r(Bwe," \u2014 "),hq=n(Bwe,"A",{href:!0});var Dtt=s(hq);ver=r(Dtt,"TFMobileBertForMaskedLM"),Dtt.forEach(t),Ter=r(Bwe," (MobileBERT model)"),Bwe.forEach(t),Fer=i(ee),l7=n(ee,"LI",{});var kwe=s(l7);Gme=n(kwe,"STRONG",{});var qtt=s(Gme);Cer=r(qtt,"mpnet"),qtt.forEach(t),Mer=r(kwe," \u2014 "),pq=n(kwe,"A",{href:!0});var Gtt=s(pq);Eer=r(Gtt,"TFMPNetForMaskedLM"),Gtt.forEach(t),yer=r(kwe," (MPNet model)"),kwe.forEach(t),wer=i(ee),i7=n(ee,"LI",{});var xwe=s(i7);Ome=n(xwe,"STRONG",{});var Ott=s(Ome);Aer=r(Ott,"rembert"),Ott.forEach(t),Ler=r(xwe," \u2014 "),_q=n(xwe,"A",{href:!0});var Xtt=s(_q);Ber=r(Xtt,"TFRemBertForMaskedLM"),Xtt.forEach(t),ker=r(xwe," (RemBERT model)"),xwe.forEach(t),xer=i(ee),d7=n(ee,"LI",{});var Rwe=s(d7);Xme=n(Rwe,"STRONG",{});var ztt=s(Xme);Rer=r(ztt,"roberta"),ztt.forEach(t),Ser=r(Rwe," \u2014 "),uq=n(Rwe,"A",{href:!0});var Vtt=s(uq);Per=r(Vtt,"TFRobertaForMaskedLM"),Vtt.forEach(t),$er=r(Rwe," (RoBERTa model)"),Rwe.forEach(t),Ier=i(ee),c7=n(ee,"LI",{});var Swe=s(c7);zme=n(Swe,"STRONG",{});var Wtt=s(zme);jer=r(Wtt,"roformer"),Wtt.forEach(t),Ner=r(Swe," \u2014 "),bq=n(Swe,"A",{href:!0});var Qtt=s(bq);Der=r(Qtt,"TFRoFormerForMaskedLM"),Qtt.forEach(t),qer=r(Swe," (RoFormer model)"),Swe.forEach(t),Ger=i(ee),f7=n(ee,"LI",{});var Pwe=s(f7);Vme=n(Pwe,"STRONG",{});var Htt=s(Vme);Oer=r(Htt,"tapas"),Htt.forEach(t),Xer=r(Pwe," \u2014 "),vq=n(Pwe,"A",{href:!0});var Utt=s(vq);zer=r(Utt,"TFTapasForMaskedLM"),Utt.forEach(t),Ver=r(Pwe," (TAPAS model)"),Pwe.forEach(t),Wer=i(ee),m7=n(ee,"LI",{});var $we=s(m7);Wme=n($we,"STRONG",{});var Jtt=s(Wme);Qer=r(Jtt,"xlm"),Jtt.forEach(t),Her=r($we," \u2014 "),Tq=n($we,"A",{href:!0});var Ytt=s(Tq);Uer=r(Ytt,"TFXLMWithLMHeadModel"),Ytt.forEach(t),Jer=r($we," (XLM model)"),$we.forEach(t),Yer=i(ee),g7=n(ee,"LI",{});var Iwe=s(g7);Qme=n(Iwe,"STRONG",{});var Ktt=s(Qme);Ker=r(Ktt,"xlm-roberta"),Ktt.forEach(t),Zer=r(Iwe," \u2014 "),Fq=n(Iwe,"A",{href:!0});var Ztt=s(Fq);eor=r(Ztt,"TFXLMRobertaForMaskedLM"),Ztt.forEach(t),oor=r(Iwe," (XLM-RoBERTa model)"),Iwe.forEach(t),ee.forEach(t),ror=i(ha),Hme=n(ha,"P",{});var eat=s(Hme);tor=r(eat,"Examples:"),eat.forEach(t),aor=i(ha),m(cw.$$.fragment,ha),ha.forEach(t),Nl.forEach(t),F8e=i(d),uc=n(d,"H2",{class:!0});var kke=s(uc);h7=n(kke,"A",{id:!0,class:!0,href:!0});var oat=s(h7);Ume=n(oat,"SPAN",{});var rat=s(Ume);m(fw.$$.fragment,rat),rat.forEach(t),oat.forEach(t),nor=i(kke),Jme=n(kke,"SPAN",{});var tat=s(Jme);sor=r(tat,"TFAutoModelForSeq2SeqLM"),tat.forEach(t),kke.forEach(t),C8e=i(d),br=n(d,"DIV",{class:!0});var ql=s(br);m(mw.$$.fragment,ql),lor=i(ql),bc=n(ql,"P",{});var Zz=s(bc);ior=r(Zz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence language modeling head) when created
with the `),Yme=n(Zz,"CODE",{});var aat=s(Yme);dor=r(aat,"from_pretrained()"),aat.forEach(t),cor=r(Zz,"class method or the "),Kme=n(Zz,"CODE",{});var nat=s(Kme);mor=r(nat,"from_config()"),nat.forEach(t),gor=r(Zz,`class
method.`),Zz.forEach(t),hor=i(ql),gw=n(ql,"P",{});var xke=s(gw);por=r(xke,"This class cannot be instantiated directly using "),Zme=n(xke,"CODE",{});var sat=s(Zme);_or=r(sat,"__init__()"),sat.forEach(t),uor=r(xke," (throws an error)."),xke.forEach(t),bor=i(ql),ft=n(ql,"DIV",{class:!0});var Gl=s(ft);m(hw.$$.fragment,Gl),vor=i(Gl),ege=n(Gl,"P",{});var lat=s(ege);Tor=r(lat,"Instantiates one of the model classes of the library (with a sequence-to-sequence language modeling head) from a configuration."),lat.forEach(t),For=i(Gl),vc=n(Gl,"P",{});var eV=s(vc);Cor=r(eV,`Note:
Loading a model from its configuration file does `),oge=n(eV,"STRONG",{});var iat=s(oge);Mor=r(iat,"not"),iat.forEach(t),Eor=r(eV,` load the model weights. It only affects the
model\u2019s configuration. Use `),rge=n(eV,"CODE",{});var dat=s(rge);yor=r(dat,"from_pretrained()"),dat.forEach(t),wor=r(eV,"to load the model weights."),eV.forEach(t),Aor=i(Gl),tge=n(Gl,"P",{});var cat=s(tge);Lor=r(cat,"Examples:"),cat.forEach(t),Bor=i(Gl),m(pw.$$.fragment,Gl),Gl.forEach(t),kor=i(ql),bo=n(ql,"DIV",{class:!0});var pa=s(bo);m(_w.$$.fragment,pa),xor=i(pa),age=n(pa,"P",{});var fat=s(age);Ror=r(fat,"Instantiate one of the model classes of the library (with a sequence-to-sequence language modeling head) from a pretrained model."),fat.forEach(t),Sor=i(pa),gn=n(pa,"P",{});var $4=s(gn);Por=r($4,"The model class to instantiate is selected based on the "),nge=n($4,"CODE",{});var mat=s(nge);$or=r(mat,"model_type"),mat.forEach(t),Ior=r($4,` property of the config object (either
passed as an argument or loaded from `),sge=n($4,"CODE",{});var gat=s(sge);jor=r(gat,"pretrained_model_name_or_path"),gat.forEach(t),Nor=r($4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),lge=n($4,"CODE",{});var hat=s(lge);Dor=r(hat,"pretrained_model_name_or_path"),hat.forEach(t),qor=r($4,":"),$4.forEach(t),Gor=i(pa),pe=n(pa,"UL",{});var Ee=s(pe);p7=n(Ee,"LI",{});var jwe=s(p7);ige=n(jwe,"STRONG",{});var pat=s(ige);Oor=r(pat,"bart"),pat.forEach(t),Xor=r(jwe," \u2014 "),Cq=n(jwe,"A",{href:!0});var _at=s(Cq);zor=r(_at,"TFBartForConditionalGeneration"),_at.forEach(t),Vor=r(jwe," (BART model)"),jwe.forEach(t),Wor=i(Ee),_7=n(Ee,"LI",{});var Nwe=s(_7);dge=n(Nwe,"STRONG",{});var uat=s(dge);Qor=r(uat,"blenderbot"),uat.forEach(t),Hor=r(Nwe," \u2014 "),Mq=n(Nwe,"A",{href:!0});var bat=s(Mq);Uor=r(bat,"TFBlenderbotForConditionalGeneration"),bat.forEach(t),Jor=r(Nwe," (Blenderbot model)"),Nwe.forEach(t),Yor=i(Ee),u7=n(Ee,"LI",{});var Dwe=s(u7);cge=n(Dwe,"STRONG",{});var vat=s(cge);Kor=r(vat,"blenderbot-small"),vat.forEach(t),Zor=r(Dwe," \u2014 "),Eq=n(Dwe,"A",{href:!0});var Tat=s(Eq);err=r(Tat,"TFBlenderbotSmallForConditionalGeneration"),Tat.forEach(t),orr=r(Dwe," (BlenderbotSmall model)"),Dwe.forEach(t),rrr=i(Ee),b7=n(Ee,"LI",{});var qwe=s(b7);fge=n(qwe,"STRONG",{});var Fat=s(fge);trr=r(Fat,"encoder-decoder"),Fat.forEach(t),arr=r(qwe," \u2014 "),yq=n(qwe,"A",{href:!0});var Cat=s(yq);nrr=r(Cat,"TFEncoderDecoderModel"),Cat.forEach(t),srr=r(qwe," (Encoder decoder model)"),qwe.forEach(t),lrr=i(Ee),v7=n(Ee,"LI",{});var Gwe=s(v7);mge=n(Gwe,"STRONG",{});var Mat=s(mge);irr=r(Mat,"led"),Mat.forEach(t),drr=r(Gwe," \u2014 "),wq=n(Gwe,"A",{href:!0});var Eat=s(wq);crr=r(Eat,"TFLEDForConditionalGeneration"),Eat.forEach(t),frr=r(Gwe," (LED model)"),Gwe.forEach(t),mrr=i(Ee),T7=n(Ee,"LI",{});var Owe=s(T7);gge=n(Owe,"STRONG",{});var yat=s(gge);grr=r(yat,"marian"),yat.forEach(t),hrr=r(Owe," \u2014 "),Aq=n(Owe,"A",{href:!0});var wat=s(Aq);prr=r(wat,"TFMarianMTModel"),wat.forEach(t),_rr=r(Owe," (Marian model)"),Owe.forEach(t),urr=i(Ee),F7=n(Ee,"LI",{});var Xwe=s(F7);hge=n(Xwe,"STRONG",{});var Aat=s(hge);brr=r(Aat,"mbart"),Aat.forEach(t),vrr=r(Xwe," \u2014 "),Lq=n(Xwe,"A",{href:!0});var Lat=s(Lq);Trr=r(Lat,"TFMBartForConditionalGeneration"),Lat.forEach(t),Frr=r(Xwe," (mBART model)"),Xwe.forEach(t),Crr=i(Ee),C7=n(Ee,"LI",{});var zwe=s(C7);pge=n(zwe,"STRONG",{});var Bat=s(pge);Mrr=r(Bat,"mt5"),Bat.forEach(t),Err=r(zwe," \u2014 "),Bq=n(zwe,"A",{href:!0});var kat=s(Bq);yrr=r(kat,"TFMT5ForConditionalGeneration"),kat.forEach(t),wrr=r(zwe," (mT5 model)"),zwe.forEach(t),Arr=i(Ee),M7=n(Ee,"LI",{});var Vwe=s(M7);_ge=n(Vwe,"STRONG",{});var xat=s(_ge);Lrr=r(xat,"pegasus"),xat.forEach(t),Brr=r(Vwe," \u2014 "),kq=n(Vwe,"A",{href:!0});var Rat=s(kq);krr=r(Rat,"TFPegasusForConditionalGeneration"),Rat.forEach(t),xrr=r(Vwe," (Pegasus model)"),Vwe.forEach(t),Rrr=i(Ee),E7=n(Ee,"LI",{});var Wwe=s(E7);uge=n(Wwe,"STRONG",{});var Sat=s(uge);Srr=r(Sat,"t5"),Sat.forEach(t),Prr=r(Wwe," \u2014 "),xq=n(Wwe,"A",{href:!0});var Pat=s(xq);$rr=r(Pat,"TFT5ForConditionalGeneration"),Pat.forEach(t),Irr=r(Wwe," (T5 model)"),Wwe.forEach(t),Ee.forEach(t),jrr=i(pa),bge=n(pa,"P",{});var $at=s(bge);Nrr=r($at,"Examples:"),$at.forEach(t),Drr=i(pa),m(uw.$$.fragment,pa),pa.forEach(t),ql.forEach(t),M8e=i(d),Tc=n(d,"H2",{class:!0});var Rke=s(Tc);y7=n(Rke,"A",{id:!0,class:!0,href:!0});var Iat=s(y7);vge=n(Iat,"SPAN",{});var jat=s(vge);m(bw.$$.fragment,jat),jat.forEach(t),Iat.forEach(t),qrr=i(Rke),Tge=n(Rke,"SPAN",{});var Nat=s(Tge);Grr=r(Nat,"TFAutoModelForSequenceClassification"),Nat.forEach(t),Rke.forEach(t),E8e=i(d),vr=n(d,"DIV",{class:!0});var Ol=s(vr);m(vw.$$.fragment,Ol),Orr=i(Ol),Fc=n(Ol,"P",{});var oV=s(Fc);Xrr=r(oV,`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence classification head) when created
with the `),Fge=n(oV,"CODE",{});var Dat=s(Fge);zrr=r(Dat,"from_pretrained()"),Dat.forEach(t),Vrr=r(oV,"class method or the "),Cge=n(oV,"CODE",{});var qat=s(Cge);Wrr=r(qat,"from_config()"),qat.forEach(t),Qrr=r(oV,`class
method.`),oV.forEach(t),Hrr=i(Ol),Tw=n(Ol,"P",{});var Ske=s(Tw);Urr=r(Ske,"This class cannot be instantiated directly using "),Mge=n(Ske,"CODE",{});var Gat=s(Mge);Jrr=r(Gat,"__init__()"),Gat.forEach(t),Yrr=r(Ske," (throws an error)."),Ske.forEach(t),Krr=i(Ol),mt=n(Ol,"DIV",{class:!0});var Xl=s(mt);m(Fw.$$.fragment,Xl),Zrr=i(Xl),Ege=n(Xl,"P",{});var Oat=s(Ege);etr=r(Oat,"Instantiates one of the model classes of the library (with a sequence classification head) from a configuration."),Oat.forEach(t),otr=i(Xl),Cc=n(Xl,"P",{});var rV=s(Cc);rtr=r(rV,`Note:
Loading a model from its configuration file does `),yge=n(rV,"STRONG",{});var Xat=s(yge);ttr=r(Xat,"not"),Xat.forEach(t),atr=r(rV,` load the model weights. It only affects the
model\u2019s configuration. Use `),wge=n(rV,"CODE",{});var zat=s(wge);ntr=r(zat,"from_pretrained()"),zat.forEach(t),str=r(rV,"to load the model weights."),rV.forEach(t),ltr=i(Xl),Age=n(Xl,"P",{});var Vat=s(Age);itr=r(Vat,"Examples:"),Vat.forEach(t),dtr=i(Xl),m(Cw.$$.fragment,Xl),Xl.forEach(t),ctr=i(Ol),vo=n(Ol,"DIV",{class:!0});var _a=s(vo);m(Mw.$$.fragment,_a),ftr=i(_a),Lge=n(_a,"P",{});var Wat=s(Lge);mtr=r(Wat,"Instantiate one of the model classes of the library (with a sequence classification head) from a pretrained model."),Wat.forEach(t),gtr=i(_a),hn=n(_a,"P",{});var I4=s(hn);htr=r(I4,"The model class to instantiate is selected based on the "),Bge=n(I4,"CODE",{});var Qat=s(Bge);ptr=r(Qat,"model_type"),Qat.forEach(t),_tr=r(I4,` property of the config object (either
passed as an argument or loaded from `),kge=n(I4,"CODE",{});var Hat=s(kge);utr=r(Hat,"pretrained_model_name_or_path"),Hat.forEach(t),btr=r(I4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),xge=n(I4,"CODE",{});var Uat=s(xge);vtr=r(Uat,"pretrained_model_name_or_path"),Uat.forEach(t),Ttr=r(I4,":"),I4.forEach(t),Ftr=i(_a),X=n(_a,"UL",{});var W=s(X);w7=n(W,"LI",{});var Qwe=s(w7);Rge=n(Qwe,"STRONG",{});var Jat=s(Rge);Ctr=r(Jat,"albert"),Jat.forEach(t),Mtr=r(Qwe," \u2014 "),Rq=n(Qwe,"A",{href:!0});var Yat=s(Rq);Etr=r(Yat,"TFAlbertForSequenceClassification"),Yat.forEach(t),ytr=r(Qwe," (ALBERT model)"),Qwe.forEach(t),wtr=i(W),A7=n(W,"LI",{});var Hwe=s(A7);Sge=n(Hwe,"STRONG",{});var Kat=s(Sge);Atr=r(Kat,"bert"),Kat.forEach(t),Ltr=r(Hwe," \u2014 "),Sq=n(Hwe,"A",{href:!0});var Zat=s(Sq);Btr=r(Zat,"TFBertForSequenceClassification"),Zat.forEach(t),ktr=r(Hwe," (BERT model)"),Hwe.forEach(t),xtr=i(W),L7=n(W,"LI",{});var Uwe=s(L7);Pge=n(Uwe,"STRONG",{});var ent=s(Pge);Rtr=r(ent,"camembert"),ent.forEach(t),Str=r(Uwe," \u2014 "),Pq=n(Uwe,"A",{href:!0});var ont=s(Pq);Ptr=r(ont,"TFCamembertForSequenceClassification"),ont.forEach(t),$tr=r(Uwe," (CamemBERT model)"),Uwe.forEach(t),Itr=i(W),B7=n(W,"LI",{});var Jwe=s(B7);$ge=n(Jwe,"STRONG",{});var rnt=s($ge);jtr=r(rnt,"convbert"),rnt.forEach(t),Ntr=r(Jwe," \u2014 "),$q=n(Jwe,"A",{href:!0});var tnt=s($q);Dtr=r(tnt,"TFConvBertForSequenceClassification"),tnt.forEach(t),qtr=r(Jwe," (ConvBERT model)"),Jwe.forEach(t),Gtr=i(W),k7=n(W,"LI",{});var Ywe=s(k7);Ige=n(Ywe,"STRONG",{});var ant=s(Ige);Otr=r(ant,"ctrl"),ant.forEach(t),Xtr=r(Ywe," \u2014 "),Iq=n(Ywe,"A",{href:!0});var nnt=s(Iq);ztr=r(nnt,"TFCTRLForSequenceClassification"),nnt.forEach(t),Vtr=r(Ywe," (CTRL model)"),Ywe.forEach(t),Wtr=i(W),x7=n(W,"LI",{});var Kwe=s(x7);jge=n(Kwe,"STRONG",{});var snt=s(jge);Qtr=r(snt,"deberta"),snt.forEach(t),Htr=r(Kwe," \u2014 "),jq=n(Kwe,"A",{href:!0});var lnt=s(jq);Utr=r(lnt,"TFDebertaForSequenceClassification"),lnt.forEach(t),Jtr=r(Kwe," (DeBERTa model)"),Kwe.forEach(t),Ytr=i(W),R7=n(W,"LI",{});var Zwe=s(R7);Nge=n(Zwe,"STRONG",{});var int=s(Nge);Ktr=r(int,"deberta-v2"),int.forEach(t),Ztr=r(Zwe," \u2014 "),Nq=n(Zwe,"A",{href:!0});var dnt=s(Nq);ear=r(dnt,"TFDebertaV2ForSequenceClassification"),dnt.forEach(t),oar=r(Zwe," (DeBERTa-v2 model)"),Zwe.forEach(t),rar=i(W),S7=n(W,"LI",{});var eAe=s(S7);Dge=n(eAe,"STRONG",{});var cnt=s(Dge);tar=r(cnt,"distilbert"),cnt.forEach(t),aar=r(eAe," \u2014 "),Dq=n(eAe,"A",{href:!0});var fnt=s(Dq);nar=r(fnt,"TFDistilBertForSequenceClassification"),fnt.forEach(t),sar=r(eAe," (DistilBERT model)"),eAe.forEach(t),lar=i(W),P7=n(W,"LI",{});var oAe=s(P7);qge=n(oAe,"STRONG",{});var mnt=s(qge);iar=r(mnt,"electra"),mnt.forEach(t),dar=r(oAe," \u2014 "),qq=n(oAe,"A",{href:!0});var gnt=s(qq);car=r(gnt,"TFElectraForSequenceClassification"),gnt.forEach(t),far=r(oAe," (ELECTRA model)"),oAe.forEach(t),mar=i(W),$7=n(W,"LI",{});var rAe=s($7);Gge=n(rAe,"STRONG",{});var hnt=s(Gge);gar=r(hnt,"flaubert"),hnt.forEach(t),har=r(rAe," \u2014 "),Gq=n(rAe,"A",{href:!0});var pnt=s(Gq);par=r(pnt,"TFFlaubertForSequenceClassification"),pnt.forEach(t),_ar=r(rAe," (FlauBERT model)"),rAe.forEach(t),uar=i(W),I7=n(W,"LI",{});var tAe=s(I7);Oge=n(tAe,"STRONG",{});var _nt=s(Oge);bar=r(_nt,"funnel"),_nt.forEach(t),Tar=r(tAe," \u2014 "),Oq=n(tAe,"A",{href:!0});var unt=s(Oq);Far=r(unt,"TFFunnelForSequenceClassification"),unt.forEach(t),Car=r(tAe," (Funnel Transformer model)"),tAe.forEach(t),Mar=i(W),j7=n(W,"LI",{});var aAe=s(j7);Xge=n(aAe,"STRONG",{});var bnt=s(Xge);Ear=r(bnt,"gpt2"),bnt.forEach(t),yar=r(aAe," \u2014 "),Xq=n(aAe,"A",{href:!0});var vnt=s(Xq);war=r(vnt,"TFGPT2ForSequenceClassification"),vnt.forEach(t),Aar=r(aAe," (OpenAI GPT-2 model)"),aAe.forEach(t),Lar=i(W),N7=n(W,"LI",{});var nAe=s(N7);zge=n(nAe,"STRONG",{});var Tnt=s(zge);Bar=r(Tnt,"layoutlm"),Tnt.forEach(t),kar=r(nAe," \u2014 "),zq=n(nAe,"A",{href:!0});var Fnt=s(zq);xar=r(Fnt,"TFLayoutLMForSequenceClassification"),Fnt.forEach(t),Rar=r(nAe," (LayoutLM model)"),nAe.forEach(t),Sar=i(W),D7=n(W,"LI",{});var sAe=s(D7);Vge=n(sAe,"STRONG",{});var Cnt=s(Vge);Par=r(Cnt,"longformer"),Cnt.forEach(t),$ar=r(sAe," \u2014 "),Vq=n(sAe,"A",{href:!0});var Mnt=s(Vq);Iar=r(Mnt,"TFLongformerForSequenceClassification"),Mnt.forEach(t),jar=r(sAe," (Longformer model)"),sAe.forEach(t),Nar=i(W),q7=n(W,"LI",{});var lAe=s(q7);Wge=n(lAe,"STRONG",{});var Ent=s(Wge);Dar=r(Ent,"mobilebert"),Ent.forEach(t),qar=r(lAe," \u2014 "),Wq=n(lAe,"A",{href:!0});var ynt=s(Wq);Gar=r(ynt,"TFMobileBertForSequenceClassification"),ynt.forEach(t),Oar=r(lAe," (MobileBERT model)"),lAe.forEach(t),Xar=i(W),G7=n(W,"LI",{});var iAe=s(G7);Qge=n(iAe,"STRONG",{});var wnt=s(Qge);zar=r(wnt,"mpnet"),wnt.forEach(t),Var=r(iAe," \u2014 "),Qq=n(iAe,"A",{href:!0});var Ant=s(Qq);War=r(Ant,"TFMPNetForSequenceClassification"),Ant.forEach(t),Qar=r(iAe," (MPNet model)"),iAe.forEach(t),Har=i(W),O7=n(W,"LI",{});var dAe=s(O7);Hge=n(dAe,"STRONG",{});var Lnt=s(Hge);Uar=r(Lnt,"openai-gpt"),Lnt.forEach(t),Jar=r(dAe," \u2014 "),Hq=n(dAe,"A",{href:!0});var Bnt=s(Hq);Yar=r(Bnt,"TFOpenAIGPTForSequenceClassification"),Bnt.forEach(t),Kar=r(dAe," (OpenAI GPT model)"),dAe.forEach(t),Zar=i(W),X7=n(W,"LI",{});var cAe=s(X7);Uge=n(cAe,"STRONG",{});var knt=s(Uge);enr=r(knt,"rembert"),knt.forEach(t),onr=r(cAe," \u2014 "),Uq=n(cAe,"A",{href:!0});var xnt=s(Uq);rnr=r(xnt,"TFRemBertForSequenceClassification"),xnt.forEach(t),tnr=r(cAe," (RemBERT model)"),cAe.forEach(t),anr=i(W),z7=n(W,"LI",{});var fAe=s(z7);Jge=n(fAe,"STRONG",{});var Rnt=s(Jge);nnr=r(Rnt,"roberta"),Rnt.forEach(t),snr=r(fAe," \u2014 "),Jq=n(fAe,"A",{href:!0});var Snt=s(Jq);lnr=r(Snt,"TFRobertaForSequenceClassification"),Snt.forEach(t),inr=r(fAe," (RoBERTa model)"),fAe.forEach(t),dnr=i(W),V7=n(W,"LI",{});var mAe=s(V7);Yge=n(mAe,"STRONG",{});var Pnt=s(Yge);cnr=r(Pnt,"roformer"),Pnt.forEach(t),fnr=r(mAe," \u2014 "),Yq=n(mAe,"A",{href:!0});var $nt=s(Yq);mnr=r($nt,"TFRoFormerForSequenceClassification"),$nt.forEach(t),gnr=r(mAe," (RoFormer model)"),mAe.forEach(t),hnr=i(W),W7=n(W,"LI",{});var gAe=s(W7);Kge=n(gAe,"STRONG",{});var Int=s(Kge);pnr=r(Int,"tapas"),Int.forEach(t),_nr=r(gAe," \u2014 "),Kq=n(gAe,"A",{href:!0});var jnt=s(Kq);unr=r(jnt,"TFTapasForSequenceClassification"),jnt.forEach(t),bnr=r(gAe," (TAPAS model)"),gAe.forEach(t),vnr=i(W),Q7=n(W,"LI",{});var hAe=s(Q7);Zge=n(hAe,"STRONG",{});var Nnt=s(Zge);Tnr=r(Nnt,"transfo-xl"),Nnt.forEach(t),Fnr=r(hAe," \u2014 "),Zq=n(hAe,"A",{href:!0});var Dnt=s(Zq);Cnr=r(Dnt,"TFTransfoXLForSequenceClassification"),Dnt.forEach(t),Mnr=r(hAe," (Transformer-XL model)"),hAe.forEach(t),Enr=i(W),H7=n(W,"LI",{});var pAe=s(H7);ehe=n(pAe,"STRONG",{});var qnt=s(ehe);ynr=r(qnt,"xlm"),qnt.forEach(t),wnr=r(pAe," \u2014 "),eG=n(pAe,"A",{href:!0});var Gnt=s(eG);Anr=r(Gnt,"TFXLMForSequenceClassification"),Gnt.forEach(t),Lnr=r(pAe," (XLM model)"),pAe.forEach(t),Bnr=i(W),U7=n(W,"LI",{});var _Ae=s(U7);ohe=n(_Ae,"STRONG",{});var Ont=s(ohe);knr=r(Ont,"xlm-roberta"),Ont.forEach(t),xnr=r(_Ae," \u2014 "),oG=n(_Ae,"A",{href:!0});var Xnt=s(oG);Rnr=r(Xnt,"TFXLMRobertaForSequenceClassification"),Xnt.forEach(t),Snr=r(_Ae," (XLM-RoBERTa model)"),_Ae.forEach(t),Pnr=i(W),J7=n(W,"LI",{});var uAe=s(J7);rhe=n(uAe,"STRONG",{});var znt=s(rhe);$nr=r(znt,"xlnet"),znt.forEach(t),Inr=r(uAe," \u2014 "),rG=n(uAe,"A",{href:!0});var Vnt=s(rG);jnr=r(Vnt,"TFXLNetForSequenceClassification"),Vnt.forEach(t),Nnr=r(uAe," (XLNet model)"),uAe.forEach(t),W.forEach(t),Dnr=i(_a),the=n(_a,"P",{});var Wnt=s(the);qnr=r(Wnt,"Examples:"),Wnt.forEach(t),Gnr=i(_a),m(Ew.$$.fragment,_a),_a.forEach(t),Ol.forEach(t),y8e=i(d),Mc=n(d,"H2",{class:!0});var Pke=s(Mc);Y7=n(Pke,"A",{id:!0,class:!0,href:!0});var Qnt=s(Y7);ahe=n(Qnt,"SPAN",{});var Hnt=s(ahe);m(yw.$$.fragment,Hnt),Hnt.forEach(t),Qnt.forEach(t),Onr=i(Pke),nhe=n(Pke,"SPAN",{});var Unt=s(nhe);Xnr=r(Unt,"TFAutoModelForMultipleChoice"),Unt.forEach(t),Pke.forEach(t),w8e=i(d),Tr=n(d,"DIV",{class:!0});var zl=s(Tr);m(ww.$$.fragment,zl),znr=i(zl),Ec=n(zl,"P",{});var tV=s(Ec);Vnr=r(tV,`This is a generic model class that will be instantiated as one of the model classes of the library (with a multiple choice head) when created
with the `),she=n(tV,"CODE",{});var Jnt=s(she);Wnr=r(Jnt,"from_pretrained()"),Jnt.forEach(t),Qnr=r(tV,"class method or the "),lhe=n(tV,"CODE",{});var Ynt=s(lhe);Hnr=r(Ynt,"from_config()"),Ynt.forEach(t),Unr=r(tV,`class
method.`),tV.forEach(t),Jnr=i(zl),Aw=n(zl,"P",{});var $ke=s(Aw);Ynr=r($ke,"This class cannot be instantiated directly using "),ihe=n($ke,"CODE",{});var Knt=s(ihe);Knr=r(Knt,"__init__()"),Knt.forEach(t),Znr=r($ke," (throws an error)."),$ke.forEach(t),esr=i(zl),gt=n(zl,"DIV",{class:!0});var Vl=s(gt);m(Lw.$$.fragment,Vl),osr=i(Vl),dhe=n(Vl,"P",{});var Znt=s(dhe);rsr=r(Znt,"Instantiates one of the model classes of the library (with a multiple choice head) from a configuration."),Znt.forEach(t),tsr=i(Vl),yc=n(Vl,"P",{});var aV=s(yc);asr=r(aV,`Note:
Loading a model from its configuration file does `),che=n(aV,"STRONG",{});var est=s(che);nsr=r(est,"not"),est.forEach(t),ssr=r(aV,` load the model weights. It only affects the
model\u2019s configuration. Use `),fhe=n(aV,"CODE",{});var ost=s(fhe);lsr=r(ost,"from_pretrained()"),ost.forEach(t),isr=r(aV,"to load the model weights."),aV.forEach(t),dsr=i(Vl),mhe=n(Vl,"P",{});var rst=s(mhe);csr=r(rst,"Examples:"),rst.forEach(t),fsr=i(Vl),m(Bw.$$.fragment,Vl),Vl.forEach(t),msr=i(zl),To=n(zl,"DIV",{class:!0});var ua=s(To);m(kw.$$.fragment,ua),gsr=i(ua),ghe=n(ua,"P",{});var tst=s(ghe);hsr=r(tst,"Instantiate one of the model classes of the library (with a multiple choice head) from a pretrained model."),tst.forEach(t),psr=i(ua),pn=n(ua,"P",{});var j4=s(pn);_sr=r(j4,"The model class to instantiate is selected based on the "),hhe=n(j4,"CODE",{});var ast=s(hhe);usr=r(ast,"model_type"),ast.forEach(t),bsr=r(j4,` property of the config object (either
passed as an argument or loaded from `),phe=n(j4,"CODE",{});var nst=s(phe);vsr=r(nst,"pretrained_model_name_or_path"),nst.forEach(t),Tsr=r(j4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),_he=n(j4,"CODE",{});var sst=s(_he);Fsr=r(sst,"pretrained_model_name_or_path"),sst.forEach(t),Csr=r(j4,":"),j4.forEach(t),Msr=i(ua),te=n(ua,"UL",{});var ne=s(te);K7=n(ne,"LI",{});var bAe=s(K7);uhe=n(bAe,"STRONG",{});var lst=s(uhe);Esr=r(lst,"albert"),lst.forEach(t),ysr=r(bAe," \u2014 "),tG=n(bAe,"A",{href:!0});var ist=s(tG);wsr=r(ist,"TFAlbertForMultipleChoice"),ist.forEach(t),Asr=r(bAe," (ALBERT model)"),bAe.forEach(t),Lsr=i(ne),Z7=n(ne,"LI",{});var vAe=s(Z7);bhe=n(vAe,"STRONG",{});var dst=s(bhe);Bsr=r(dst,"bert"),dst.forEach(t),ksr=r(vAe," \u2014 "),aG=n(vAe,"A",{href:!0});var cst=s(aG);xsr=r(cst,"TFBertForMultipleChoice"),cst.forEach(t),Rsr=r(vAe," (BERT model)"),vAe.forEach(t),Ssr=i(ne),eF=n(ne,"LI",{});var TAe=s(eF);vhe=n(TAe,"STRONG",{});var fst=s(vhe);Psr=r(fst,"camembert"),fst.forEach(t),$sr=r(TAe," \u2014 "),nG=n(TAe,"A",{href:!0});var mst=s(nG);Isr=r(mst,"TFCamembertForMultipleChoice"),mst.forEach(t),jsr=r(TAe," (CamemBERT model)"),TAe.forEach(t),Nsr=i(ne),oF=n(ne,"LI",{});var FAe=s(oF);The=n(FAe,"STRONG",{});var gst=s(The);Dsr=r(gst,"convbert"),gst.forEach(t),qsr=r(FAe," \u2014 "),sG=n(FAe,"A",{href:!0});var hst=s(sG);Gsr=r(hst,"TFConvBertForMultipleChoice"),hst.forEach(t),Osr=r(FAe," (ConvBERT model)"),FAe.forEach(t),Xsr=i(ne),rF=n(ne,"LI",{});var CAe=s(rF);Fhe=n(CAe,"STRONG",{});var pst=s(Fhe);zsr=r(pst,"distilbert"),pst.forEach(t),Vsr=r(CAe," \u2014 "),lG=n(CAe,"A",{href:!0});var _st=s(lG);Wsr=r(_st,"TFDistilBertForMultipleChoice"),_st.forEach(t),Qsr=r(CAe," (DistilBERT model)"),CAe.forEach(t),Hsr=i(ne),tF=n(ne,"LI",{});var MAe=s(tF);Che=n(MAe,"STRONG",{});var ust=s(Che);Usr=r(ust,"electra"),ust.forEach(t),Jsr=r(MAe," \u2014 "),iG=n(MAe,"A",{href:!0});var bst=s(iG);Ysr=r(bst,"TFElectraForMultipleChoice"),bst.forEach(t),Ksr=r(MAe," (ELECTRA model)"),MAe.forEach(t),Zsr=i(ne),aF=n(ne,"LI",{});var EAe=s(aF);Mhe=n(EAe,"STRONG",{});var vst=s(Mhe);elr=r(vst,"flaubert"),vst.forEach(t),olr=r(EAe," \u2014 "),dG=n(EAe,"A",{href:!0});var Tst=s(dG);rlr=r(Tst,"TFFlaubertForMultipleChoice"),Tst.forEach(t),tlr=r(EAe," (FlauBERT model)"),EAe.forEach(t),alr=i(ne),nF=n(ne,"LI",{});var yAe=s(nF);Ehe=n(yAe,"STRONG",{});var Fst=s(Ehe);nlr=r(Fst,"funnel"),Fst.forEach(t),slr=r(yAe," \u2014 "),cG=n(yAe,"A",{href:!0});var Cst=s(cG);llr=r(Cst,"TFFunnelForMultipleChoice"),Cst.forEach(t),ilr=r(yAe," (Funnel Transformer model)"),yAe.forEach(t),dlr=i(ne),sF=n(ne,"LI",{});var wAe=s(sF);yhe=n(wAe,"STRONG",{});var Mst=s(yhe);clr=r(Mst,"longformer"),Mst.forEach(t),flr=r(wAe," \u2014 "),fG=n(wAe,"A",{href:!0});var Est=s(fG);mlr=r(Est,"TFLongformerForMultipleChoice"),Est.forEach(t),glr=r(wAe," (Longformer model)"),wAe.forEach(t),hlr=i(ne),lF=n(ne,"LI",{});var AAe=s(lF);whe=n(AAe,"STRONG",{});var yst=s(whe);plr=r(yst,"mobilebert"),yst.forEach(t),_lr=r(AAe," \u2014 "),mG=n(AAe,"A",{href:!0});var wst=s(mG);ulr=r(wst,"TFMobileBertForMultipleChoice"),wst.forEach(t),blr=r(AAe," (MobileBERT model)"),AAe.forEach(t),vlr=i(ne),iF=n(ne,"LI",{});var LAe=s(iF);Ahe=n(LAe,"STRONG",{});var Ast=s(Ahe);Tlr=r(Ast,"mpnet"),Ast.forEach(t),Flr=r(LAe," \u2014 "),gG=n(LAe,"A",{href:!0});var Lst=s(gG);Clr=r(Lst,"TFMPNetForMultipleChoice"),Lst.forEach(t),Mlr=r(LAe," (MPNet model)"),LAe.forEach(t),Elr=i(ne),dF=n(ne,"LI",{});var BAe=s(dF);Lhe=n(BAe,"STRONG",{});var Bst=s(Lhe);ylr=r(Bst,"rembert"),Bst.forEach(t),wlr=r(BAe," \u2014 "),hG=n(BAe,"A",{href:!0});var kst=s(hG);Alr=r(kst,"TFRemBertForMultipleChoice"),kst.forEach(t),Llr=r(BAe," (RemBERT model)"),BAe.forEach(t),Blr=i(ne),cF=n(ne,"LI",{});var kAe=s(cF);Bhe=n(kAe,"STRONG",{});var xst=s(Bhe);klr=r(xst,"roberta"),xst.forEach(t),xlr=r(kAe," \u2014 "),pG=n(kAe,"A",{href:!0});var Rst=s(pG);Rlr=r(Rst,"TFRobertaForMultipleChoice"),Rst.forEach(t),Slr=r(kAe," (RoBERTa model)"),kAe.forEach(t),Plr=i(ne),fF=n(ne,"LI",{});var xAe=s(fF);khe=n(xAe,"STRONG",{});var Sst=s(khe);$lr=r(Sst,"roformer"),Sst.forEach(t),Ilr=r(xAe," \u2014 "),_G=n(xAe,"A",{href:!0});var Pst=s(_G);jlr=r(Pst,"TFRoFormerForMultipleChoice"),Pst.forEach(t),Nlr=r(xAe," (RoFormer model)"),xAe.forEach(t),Dlr=i(ne),mF=n(ne,"LI",{});var RAe=s(mF);xhe=n(RAe,"STRONG",{});var $st=s(xhe);qlr=r($st,"xlm"),$st.forEach(t),Glr=r(RAe," \u2014 "),uG=n(RAe,"A",{href:!0});var Ist=s(uG);Olr=r(Ist,"TFXLMForMultipleChoice"),Ist.forEach(t),Xlr=r(RAe," (XLM model)"),RAe.forEach(t),zlr=i(ne),gF=n(ne,"LI",{});var SAe=s(gF);Rhe=n(SAe,"STRONG",{});var jst=s(Rhe);Vlr=r(jst,"xlm-roberta"),jst.forEach(t),Wlr=r(SAe," \u2014 "),bG=n(SAe,"A",{href:!0});var Nst=s(bG);Qlr=r(Nst,"TFXLMRobertaForMultipleChoice"),Nst.forEach(t),Hlr=r(SAe," (XLM-RoBERTa model)"),SAe.forEach(t),Ulr=i(ne),hF=n(ne,"LI",{});var PAe=s(hF);She=n(PAe,"STRONG",{});var Dst=s(She);Jlr=r(Dst,"xlnet"),Dst.forEach(t),Ylr=r(PAe," \u2014 "),vG=n(PAe,"A",{href:!0});var qst=s(vG);Klr=r(qst,"TFXLNetForMultipleChoice"),qst.forEach(t),Zlr=r(PAe," (XLNet model)"),PAe.forEach(t),ne.forEach(t),eir=i(ua),Phe=n(ua,"P",{});var Gst=s(Phe);oir=r(Gst,"Examples:"),Gst.forEach(t),rir=i(ua),m(xw.$$.fragment,ua),ua.forEach(t),zl.forEach(t),A8e=i(d),wc=n(d,"H2",{class:!0});var Ike=s(wc);pF=n(Ike,"A",{id:!0,class:!0,href:!0});var Ost=s(pF);$he=n(Ost,"SPAN",{});var Xst=s($he);m(Rw.$$.fragment,Xst),Xst.forEach(t),Ost.forEach(t),tir=i(Ike),Ihe=n(Ike,"SPAN",{});var zst=s(Ihe);air=r(zst,"TFAutoModelForTableQuestionAnswering"),zst.forEach(t),Ike.forEach(t),L8e=i(d),Fr=n(d,"DIV",{class:!0});var Wl=s(Fr);m(Sw.$$.fragment,Wl),nir=i(Wl),Ac=n(Wl,"P",{});var nV=s(Ac);sir=r(nV,`This is a generic model class that will be instantiated as one of the model classes of the library (with a table question answering head) when created
with the `),jhe=n(nV,"CODE",{});var Vst=s(jhe);lir=r(Vst,"from_pretrained()"),Vst.forEach(t),iir=r(nV,"class method or the "),Nhe=n(nV,"CODE",{});var Wst=s(Nhe);dir=r(Wst,"from_config()"),Wst.forEach(t),cir=r(nV,`class
method.`),nV.forEach(t),fir=i(Wl),Pw=n(Wl,"P",{});var jke=s(Pw);mir=r(jke,"This class cannot be instantiated directly using "),Dhe=n(jke,"CODE",{});var Qst=s(Dhe);gir=r(Qst,"__init__()"),Qst.forEach(t),hir=r(jke," (throws an error)."),jke.forEach(t),pir=i(Wl),ht=n(Wl,"DIV",{class:!0});var Ql=s(ht);m($w.$$.fragment,Ql),_ir=i(Ql),qhe=n(Ql,"P",{});var Hst=s(qhe);uir=r(Hst,"Instantiates one of the model classes of the library (with a table question answering head) from a configuration."),Hst.forEach(t),bir=i(Ql),Lc=n(Ql,"P",{});var sV=s(Lc);vir=r(sV,`Note:
Loading a model from its configuration file does `),Ghe=n(sV,"STRONG",{});var Ust=s(Ghe);Tir=r(Ust,"not"),Ust.forEach(t),Fir=r(sV,` load the model weights. It only affects the
model\u2019s configuration. Use `),Ohe=n(sV,"CODE",{});var Jst=s(Ohe);Cir=r(Jst,"from_pretrained()"),Jst.forEach(t),Mir=r(sV,"to load the model weights."),sV.forEach(t),Eir=i(Ql),Xhe=n(Ql,"P",{});var Yst=s(Xhe);yir=r(Yst,"Examples:"),Yst.forEach(t),wir=i(Ql),m(Iw.$$.fragment,Ql),Ql.forEach(t),Air=i(Wl),Fo=n(Wl,"DIV",{class:!0});var ba=s(Fo);m(jw.$$.fragment,ba),Lir=i(ba),zhe=n(ba,"P",{});var Kst=s(zhe);Bir=r(Kst,"Instantiate one of the model classes of the library (with a table question answering head) from a pretrained model."),Kst.forEach(t),kir=i(ba),_n=n(ba,"P",{});var N4=s(_n);xir=r(N4,"The model class to instantiate is selected based on the "),Vhe=n(N4,"CODE",{});var Zst=s(Vhe);Rir=r(Zst,"model_type"),Zst.forEach(t),Sir=r(N4,` property of the config object (either
passed as an argument or loaded from `),Whe=n(N4,"CODE",{});var elt=s(Whe);Pir=r(elt,"pretrained_model_name_or_path"),elt.forEach(t),$ir=r(N4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Qhe=n(N4,"CODE",{});var olt=s(Qhe);Iir=r(olt,"pretrained_model_name_or_path"),olt.forEach(t),jir=r(N4,":"),N4.forEach(t),Nir=i(ba),Hhe=n(ba,"UL",{});var rlt=s(Hhe);_F=n(rlt,"LI",{});var $Ae=s(_F);Uhe=n($Ae,"STRONG",{});var tlt=s(Uhe);Dir=r(tlt,"tapas"),tlt.forEach(t),qir=r($Ae," \u2014 "),TG=n($Ae,"A",{href:!0});var alt=s(TG);Gir=r(alt,"TFTapasForQuestionAnswering"),alt.forEach(t),Oir=r($Ae," (TAPAS model)"),$Ae.forEach(t),rlt.forEach(t),Xir=i(ba),Jhe=n(ba,"P",{});var nlt=s(Jhe);zir=r(nlt,"Examples:"),nlt.forEach(t),Vir=i(ba),m(Nw.$$.fragment,ba),ba.forEach(t),Wl.forEach(t),B8e=i(d),Bc=n(d,"H2",{class:!0});var Nke=s(Bc);uF=n(Nke,"A",{id:!0,class:!0,href:!0});var slt=s(uF);Yhe=n(slt,"SPAN",{});var llt=s(Yhe);m(Dw.$$.fragment,llt),llt.forEach(t),slt.forEach(t),Wir=i(Nke),Khe=n(Nke,"SPAN",{});var ilt=s(Khe);Qir=r(ilt,"TFAutoModelForTokenClassification"),ilt.forEach(t),Nke.forEach(t),k8e=i(d),Cr=n(d,"DIV",{class:!0});var Hl=s(Cr);m(qw.$$.fragment,Hl),Hir=i(Hl),kc=n(Hl,"P",{});var lV=s(kc);Uir=r(lV,`This is a generic model class that will be instantiated as one of the model classes of the library (with a token classification head) when created
with the `),Zhe=n(lV,"CODE",{});var dlt=s(Zhe);Jir=r(dlt,"from_pretrained()"),dlt.forEach(t),Yir=r(lV,"class method or the "),epe=n(lV,"CODE",{});var clt=s(epe);Kir=r(clt,"from_config()"),clt.forEach(t),Zir=r(lV,`class
method.`),lV.forEach(t),edr=i(Hl),Gw=n(Hl,"P",{});var Dke=s(Gw);odr=r(Dke,"This class cannot be instantiated directly using "),ope=n(Dke,"CODE",{});var flt=s(ope);rdr=r(flt,"__init__()"),flt.forEach(t),tdr=r(Dke," (throws an error)."),Dke.forEach(t),adr=i(Hl),pt=n(Hl,"DIV",{class:!0});var Ul=s(pt);m(Ow.$$.fragment,Ul),ndr=i(Ul),rpe=n(Ul,"P",{});var mlt=s(rpe);sdr=r(mlt,"Instantiates one of the model classes of the library (with a token classification head) from a configuration."),mlt.forEach(t),ldr=i(Ul),xc=n(Ul,"P",{});var iV=s(xc);idr=r(iV,`Note:
Loading a model from its configuration file does `),tpe=n(iV,"STRONG",{});var glt=s(tpe);ddr=r(glt,"not"),glt.forEach(t),cdr=r(iV,` load the model weights. It only affects the
model\u2019s configuration. Use `),ape=n(iV,"CODE",{});var hlt=s(ape);fdr=r(hlt,"from_pretrained()"),hlt.forEach(t),mdr=r(iV,"to load the model weights."),iV.forEach(t),gdr=i(Ul),npe=n(Ul,"P",{});var plt=s(npe);hdr=r(plt,"Examples:"),plt.forEach(t),pdr=i(Ul),m(Xw.$$.fragment,Ul),Ul.forEach(t),_dr=i(Hl),Co=n(Hl,"DIV",{class:!0});var va=s(Co);m(zw.$$.fragment,va),udr=i(va),spe=n(va,"P",{});var _lt=s(spe);bdr=r(_lt,"Instantiate one of the model classes of the library (with a token classification head) from a pretrained model."),_lt.forEach(t),vdr=i(va),un=n(va,"P",{});var D4=s(un);Tdr=r(D4,"The model class to instantiate is selected based on the "),lpe=n(D4,"CODE",{});var ult=s(lpe);Fdr=r(ult,"model_type"),ult.forEach(t),Cdr=r(D4,` property of the config object (either
passed as an argument or loaded from `),ipe=n(D4,"CODE",{});var blt=s(ipe);Mdr=r(blt,"pretrained_model_name_or_path"),blt.forEach(t),Edr=r(D4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),dpe=n(D4,"CODE",{});var vlt=s(dpe);ydr=r(vlt,"pretrained_model_name_or_path"),vlt.forEach(t),wdr=r(D4,":"),D4.forEach(t),Adr=i(va),K=n(va,"UL",{});var oe=s(K);bF=n(oe,"LI",{});var IAe=s(bF);cpe=n(IAe,"STRONG",{});var Tlt=s(cpe);Ldr=r(Tlt,"albert"),Tlt.forEach(t),Bdr=r(IAe," \u2014 "),FG=n(IAe,"A",{href:!0});var Flt=s(FG);kdr=r(Flt,"TFAlbertForTokenClassification"),Flt.forEach(t),xdr=r(IAe," (ALBERT model)"),IAe.forEach(t),Rdr=i(oe),vF=n(oe,"LI",{});var jAe=s(vF);fpe=n(jAe,"STRONG",{});var Clt=s(fpe);Sdr=r(Clt,"bert"),Clt.forEach(t),Pdr=r(jAe," \u2014 "),CG=n(jAe,"A",{href:!0});var Mlt=s(CG);$dr=r(Mlt,"TFBertForTokenClassification"),Mlt.forEach(t),Idr=r(jAe," (BERT model)"),jAe.forEach(t),jdr=i(oe),TF=n(oe,"LI",{});var NAe=s(TF);mpe=n(NAe,"STRONG",{});var Elt=s(mpe);Ndr=r(Elt,"camembert"),Elt.forEach(t),Ddr=r(NAe," \u2014 "),MG=n(NAe,"A",{href:!0});var ylt=s(MG);qdr=r(ylt,"TFCamembertForTokenClassification"),ylt.forEach(t),Gdr=r(NAe," (CamemBERT model)"),NAe.forEach(t),Odr=i(oe),FF=n(oe,"LI",{});var DAe=s(FF);gpe=n(DAe,"STRONG",{});var wlt=s(gpe);Xdr=r(wlt,"convbert"),wlt.forEach(t),zdr=r(DAe," \u2014 "),EG=n(DAe,"A",{href:!0});var Alt=s(EG);Vdr=r(Alt,"TFConvBertForTokenClassification"),Alt.forEach(t),Wdr=r(DAe," (ConvBERT model)"),DAe.forEach(t),Qdr=i(oe),CF=n(oe,"LI",{});var qAe=s(CF);hpe=n(qAe,"STRONG",{});var Llt=s(hpe);Hdr=r(Llt,"deberta"),Llt.forEach(t),Udr=r(qAe," \u2014 "),yG=n(qAe,"A",{href:!0});var Blt=s(yG);Jdr=r(Blt,"TFDebertaForTokenClassification"),Blt.forEach(t),Ydr=r(qAe," (DeBERTa model)"),qAe.forEach(t),Kdr=i(oe),MF=n(oe,"LI",{});var GAe=s(MF);ppe=n(GAe,"STRONG",{});var klt=s(ppe);Zdr=r(klt,"deberta-v2"),klt.forEach(t),ecr=r(GAe," \u2014 "),wG=n(GAe,"A",{href:!0});var xlt=s(wG);ocr=r(xlt,"TFDebertaV2ForTokenClassification"),xlt.forEach(t),rcr=r(GAe," (DeBERTa-v2 model)"),GAe.forEach(t),tcr=i(oe),EF=n(oe,"LI",{});var OAe=s(EF);_pe=n(OAe,"STRONG",{});var Rlt=s(_pe);acr=r(Rlt,"distilbert"),Rlt.forEach(t),ncr=r(OAe," \u2014 "),AG=n(OAe,"A",{href:!0});var Slt=s(AG);scr=r(Slt,"TFDistilBertForTokenClassification"),Slt.forEach(t),lcr=r(OAe," (DistilBERT model)"),OAe.forEach(t),icr=i(oe),yF=n(oe,"LI",{});var XAe=s(yF);upe=n(XAe,"STRONG",{});var Plt=s(upe);dcr=r(Plt,"electra"),Plt.forEach(t),ccr=r(XAe," \u2014 "),LG=n(XAe,"A",{href:!0});var $lt=s(LG);fcr=r($lt,"TFElectraForTokenClassification"),$lt.forEach(t),mcr=r(XAe," (ELECTRA model)"),XAe.forEach(t),gcr=i(oe),wF=n(oe,"LI",{});var zAe=s(wF);bpe=n(zAe,"STRONG",{});var Ilt=s(bpe);hcr=r(Ilt,"flaubert"),Ilt.forEach(t),pcr=r(zAe," \u2014 "),BG=n(zAe,"A",{href:!0});var jlt=s(BG);_cr=r(jlt,"TFFlaubertForTokenClassification"),jlt.forEach(t),ucr=r(zAe," (FlauBERT model)"),zAe.forEach(t),bcr=i(oe),AF=n(oe,"LI",{});var VAe=s(AF);vpe=n(VAe,"STRONG",{});var Nlt=s(vpe);vcr=r(Nlt,"funnel"),Nlt.forEach(t),Tcr=r(VAe," \u2014 "),kG=n(VAe,"A",{href:!0});var Dlt=s(kG);Fcr=r(Dlt,"TFFunnelForTokenClassification"),Dlt.forEach(t),Ccr=r(VAe," (Funnel Transformer model)"),VAe.forEach(t),Mcr=i(oe),LF=n(oe,"LI",{});var WAe=s(LF);Tpe=n(WAe,"STRONG",{});var qlt=s(Tpe);Ecr=r(qlt,"layoutlm"),qlt.forEach(t),ycr=r(WAe," \u2014 "),xG=n(WAe,"A",{href:!0});var Glt=s(xG);wcr=r(Glt,"TFLayoutLMForTokenClassification"),Glt.forEach(t),Acr=r(WAe," (LayoutLM model)"),WAe.forEach(t),Lcr=i(oe),BF=n(oe,"LI",{});var QAe=s(BF);Fpe=n(QAe,"STRONG",{});var Olt=s(Fpe);Bcr=r(Olt,"longformer"),Olt.forEach(t),kcr=r(QAe," \u2014 "),RG=n(QAe,"A",{href:!0});var Xlt=s(RG);xcr=r(Xlt,"TFLongformerForTokenClassification"),Xlt.forEach(t),Rcr=r(QAe," (Longformer model)"),QAe.forEach(t),Scr=i(oe),kF=n(oe,"LI",{});var HAe=s(kF);Cpe=n(HAe,"STRONG",{});var zlt=s(Cpe);Pcr=r(zlt,"mobilebert"),zlt.forEach(t),$cr=r(HAe," \u2014 "),SG=n(HAe,"A",{href:!0});var Vlt=s(SG);Icr=r(Vlt,"TFMobileBertForTokenClassification"),Vlt.forEach(t),jcr=r(HAe," (MobileBERT model)"),HAe.forEach(t),Ncr=i(oe),xF=n(oe,"LI",{});var UAe=s(xF);Mpe=n(UAe,"STRONG",{});var Wlt=s(Mpe);Dcr=r(Wlt,"mpnet"),Wlt.forEach(t),qcr=r(UAe," \u2014 "),PG=n(UAe,"A",{href:!0});var Qlt=s(PG);Gcr=r(Qlt,"TFMPNetForTokenClassification"),Qlt.forEach(t),Ocr=r(UAe," (MPNet model)"),UAe.forEach(t),Xcr=i(oe),RF=n(oe,"LI",{});var JAe=s(RF);Epe=n(JAe,"STRONG",{});var Hlt=s(Epe);zcr=r(Hlt,"rembert"),Hlt.forEach(t),Vcr=r(JAe," \u2014 "),$G=n(JAe,"A",{href:!0});var Ult=s($G);Wcr=r(Ult,"TFRemBertForTokenClassification"),Ult.forEach(t),Qcr=r(JAe," (RemBERT model)"),JAe.forEach(t),Hcr=i(oe),SF=n(oe,"LI",{});var YAe=s(SF);ype=n(YAe,"STRONG",{});var Jlt=s(ype);Ucr=r(Jlt,"roberta"),Jlt.forEach(t),Jcr=r(YAe," \u2014 "),IG=n(YAe,"A",{href:!0});var Ylt=s(IG);Ycr=r(Ylt,"TFRobertaForTokenClassification"),Ylt.forEach(t),Kcr=r(YAe," (RoBERTa model)"),YAe.forEach(t),Zcr=i(oe),PF=n(oe,"LI",{});var KAe=s(PF);wpe=n(KAe,"STRONG",{});var Klt=s(wpe);efr=r(Klt,"roformer"),Klt.forEach(t),ofr=r(KAe," \u2014 "),jG=n(KAe,"A",{href:!0});var Zlt=s(jG);rfr=r(Zlt,"TFRoFormerForTokenClassification"),Zlt.forEach(t),tfr=r(KAe," (RoFormer model)"),KAe.forEach(t),afr=i(oe),$F=n(oe,"LI",{});var ZAe=s($F);Ape=n(ZAe,"STRONG",{});var eit=s(Ape);nfr=r(eit,"xlm"),eit.forEach(t),sfr=r(ZAe," \u2014 "),NG=n(ZAe,"A",{href:!0});var oit=s(NG);lfr=r(oit,"TFXLMForTokenClassification"),oit.forEach(t),ifr=r(ZAe," (XLM model)"),ZAe.forEach(t),dfr=i(oe),IF=n(oe,"LI",{});var e6e=s(IF);Lpe=n(e6e,"STRONG",{});var rit=s(Lpe);cfr=r(rit,"xlm-roberta"),rit.forEach(t),ffr=r(e6e," \u2014 "),DG=n(e6e,"A",{href:!0});var tit=s(DG);mfr=r(tit,"TFXLMRobertaForTokenClassification"),tit.forEach(t),gfr=r(e6e," (XLM-RoBERTa model)"),e6e.forEach(t),hfr=i(oe),jF=n(oe,"LI",{});var o6e=s(jF);Bpe=n(o6e,"STRONG",{});var ait=s(Bpe);pfr=r(ait,"xlnet"),ait.forEach(t),_fr=r(o6e," \u2014 "),qG=n(o6e,"A",{href:!0});var nit=s(qG);ufr=r(nit,"TFXLNetForTokenClassification"),nit.forEach(t),bfr=r(o6e," (XLNet model)"),o6e.forEach(t),oe.forEach(t),vfr=i(va),kpe=n(va,"P",{});var sit=s(kpe);Tfr=r(sit,"Examples:"),sit.forEach(t),Ffr=i(va),m(Vw.$$.fragment,va),va.forEach(t),Hl.forEach(t),x8e=i(d),Rc=n(d,"H2",{class:!0});var qke=s(Rc);NF=n(qke,"A",{id:!0,class:!0,href:!0});var lit=s(NF);xpe=n(lit,"SPAN",{});var iit=s(xpe);m(Ww.$$.fragment,iit),iit.forEach(t),lit.forEach(t),Cfr=i(qke),Rpe=n(qke,"SPAN",{});var dit=s(Rpe);Mfr=r(dit,"TFAutoModelForQuestionAnswering"),dit.forEach(t),qke.forEach(t),R8e=i(d),Mr=n(d,"DIV",{class:!0});var Jl=s(Mr);m(Qw.$$.fragment,Jl),Efr=i(Jl),Sc=n(Jl,"P",{});var dV=s(Sc);yfr=r(dV,`This is a generic model class that will be instantiated as one of the model classes of the library (with a question answering head) when created
with the `),Spe=n(dV,"CODE",{});var cit=s(Spe);wfr=r(cit,"from_pretrained()"),cit.forEach(t),Afr=r(dV,"class method or the "),Ppe=n(dV,"CODE",{});var fit=s(Ppe);Lfr=r(fit,"from_config()"),fit.forEach(t),Bfr=r(dV,`class
method.`),dV.forEach(t),kfr=i(Jl),Hw=n(Jl,"P",{});var Gke=s(Hw);xfr=r(Gke,"This class cannot be instantiated directly using "),$pe=n(Gke,"CODE",{});var mit=s($pe);Rfr=r(mit,"__init__()"),mit.forEach(t),Sfr=r(Gke," (throws an error)."),Gke.forEach(t),Pfr=i(Jl),_t=n(Jl,"DIV",{class:!0});var Yl=s(_t);m(Uw.$$.fragment,Yl),$fr=i(Yl),Ipe=n(Yl,"P",{});var git=s(Ipe);Ifr=r(git,"Instantiates one of the model classes of the library (with a question answering head) from a configuration."),git.forEach(t),jfr=i(Yl),Pc=n(Yl,"P",{});var cV=s(Pc);Nfr=r(cV,`Note:
Loading a model from its configuration file does `),jpe=n(cV,"STRONG",{});var hit=s(jpe);Dfr=r(hit,"not"),hit.forEach(t),qfr=r(cV,` load the model weights. It only affects the
model\u2019s configuration. Use `),Npe=n(cV,"CODE",{});var pit=s(Npe);Gfr=r(pit,"from_pretrained()"),pit.forEach(t),Ofr=r(cV,"to load the model weights."),cV.forEach(t),Xfr=i(Yl),Dpe=n(Yl,"P",{});var _it=s(Dpe);zfr=r(_it,"Examples:"),_it.forEach(t),Vfr=i(Yl),m(Jw.$$.fragment,Yl),Yl.forEach(t),Wfr=i(Jl),Mo=n(Jl,"DIV",{class:!0});var Ta=s(Mo);m(Yw.$$.fragment,Ta),Qfr=i(Ta),qpe=n(Ta,"P",{});var uit=s(qpe);Hfr=r(uit,"Instantiate one of the model classes of the library (with a question answering head) from a pretrained model."),uit.forEach(t),Ufr=i(Ta),bn=n(Ta,"P",{});var q4=s(bn);Jfr=r(q4,"The model class to instantiate is selected based on the "),Gpe=n(q4,"CODE",{});var bit=s(Gpe);Yfr=r(bit,"model_type"),bit.forEach(t),Kfr=r(q4,` property of the config object (either
passed as an argument or loaded from `),Ope=n(q4,"CODE",{});var vit=s(Ope);Zfr=r(vit,"pretrained_model_name_or_path"),vit.forEach(t),emr=r(q4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Xpe=n(q4,"CODE",{});var Tit=s(Xpe);omr=r(Tit,"pretrained_model_name_or_path"),Tit.forEach(t),rmr=r(q4,":"),q4.forEach(t),tmr=i(Ta),Z=n(Ta,"UL",{});var re=s(Z);DF=n(re,"LI",{});var r6e=s(DF);zpe=n(r6e,"STRONG",{});var Fit=s(zpe);amr=r(Fit,"albert"),Fit.forEach(t),nmr=r(r6e," \u2014 "),GG=n(r6e,"A",{href:!0});var Cit=s(GG);smr=r(Cit,"TFAlbertForQuestionAnswering"),Cit.forEach(t),lmr=r(r6e," (ALBERT model)"),r6e.forEach(t),imr=i(re),qF=n(re,"LI",{});var t6e=s(qF);Vpe=n(t6e,"STRONG",{});var Mit=s(Vpe);dmr=r(Mit,"bert"),Mit.forEach(t),cmr=r(t6e," \u2014 "),OG=n(t6e,"A",{href:!0});var Eit=s(OG);fmr=r(Eit,"TFBertForQuestionAnswering"),Eit.forEach(t),mmr=r(t6e," (BERT model)"),t6e.forEach(t),gmr=i(re),GF=n(re,"LI",{});var a6e=s(GF);Wpe=n(a6e,"STRONG",{});var yit=s(Wpe);hmr=r(yit,"camembert"),yit.forEach(t),pmr=r(a6e," \u2014 "),XG=n(a6e,"A",{href:!0});var wit=s(XG);_mr=r(wit,"TFCamembertForQuestionAnswering"),wit.forEach(t),umr=r(a6e," (CamemBERT model)"),a6e.forEach(t),bmr=i(re),OF=n(re,"LI",{});var n6e=s(OF);Qpe=n(n6e,"STRONG",{});var Ait=s(Qpe);vmr=r(Ait,"convbert"),Ait.forEach(t),Tmr=r(n6e," \u2014 "),zG=n(n6e,"A",{href:!0});var Lit=s(zG);Fmr=r(Lit,"TFConvBertForQuestionAnswering"),Lit.forEach(t),Cmr=r(n6e," (ConvBERT model)"),n6e.forEach(t),Mmr=i(re),XF=n(re,"LI",{});var s6e=s(XF);Hpe=n(s6e,"STRONG",{});var Bit=s(Hpe);Emr=r(Bit,"deberta"),Bit.forEach(t),ymr=r(s6e," \u2014 "),VG=n(s6e,"A",{href:!0});var kit=s(VG);wmr=r(kit,"TFDebertaForQuestionAnswering"),kit.forEach(t),Amr=r(s6e," (DeBERTa model)"),s6e.forEach(t),Lmr=i(re),zF=n(re,"LI",{});var l6e=s(zF);Upe=n(l6e,"STRONG",{});var xit=s(Upe);Bmr=r(xit,"deberta-v2"),xit.forEach(t),kmr=r(l6e," \u2014 "),WG=n(l6e,"A",{href:!0});var Rit=s(WG);xmr=r(Rit,"TFDebertaV2ForQuestionAnswering"),Rit.forEach(t),Rmr=r(l6e," (DeBERTa-v2 model)"),l6e.forEach(t),Smr=i(re),VF=n(re,"LI",{});var i6e=s(VF);Jpe=n(i6e,"STRONG",{});var Sit=s(Jpe);Pmr=r(Sit,"distilbert"),Sit.forEach(t),$mr=r(i6e," \u2014 "),QG=n(i6e,"A",{href:!0});var Pit=s(QG);Imr=r(Pit,"TFDistilBertForQuestionAnswering"),Pit.forEach(t),jmr=r(i6e," (DistilBERT model)"),i6e.forEach(t),Nmr=i(re),WF=n(re,"LI",{});var d6e=s(WF);Ype=n(d6e,"STRONG",{});var $it=s(Ype);Dmr=r($it,"electra"),$it.forEach(t),qmr=r(d6e," \u2014 "),HG=n(d6e,"A",{href:!0});var Iit=s(HG);Gmr=r(Iit,"TFElectraForQuestionAnswering"),Iit.forEach(t),Omr=r(d6e," (ELECTRA model)"),d6e.forEach(t),Xmr=i(re),QF=n(re,"LI",{});var c6e=s(QF);Kpe=n(c6e,"STRONG",{});var jit=s(Kpe);zmr=r(jit,"flaubert"),jit.forEach(t),Vmr=r(c6e," \u2014 "),UG=n(c6e,"A",{href:!0});var Nit=s(UG);Wmr=r(Nit,"TFFlaubertForQuestionAnsweringSimple"),Nit.forEach(t),Qmr=r(c6e," (FlauBERT model)"),c6e.forEach(t),Hmr=i(re),HF=n(re,"LI",{});var f6e=s(HF);Zpe=n(f6e,"STRONG",{});var Dit=s(Zpe);Umr=r(Dit,"funnel"),Dit.forEach(t),Jmr=r(f6e," \u2014 "),JG=n(f6e,"A",{href:!0});var qit=s(JG);Ymr=r(qit,"TFFunnelForQuestionAnswering"),qit.forEach(t),Kmr=r(f6e," (Funnel Transformer model)"),f6e.forEach(t),Zmr=i(re),UF=n(re,"LI",{});var m6e=s(UF);e_e=n(m6e,"STRONG",{});var Git=s(e_e);egr=r(Git,"longformer"),Git.forEach(t),ogr=r(m6e," \u2014 "),YG=n(m6e,"A",{href:!0});var Oit=s(YG);rgr=r(Oit,"TFLongformerForQuestionAnswering"),Oit.forEach(t),tgr=r(m6e," (Longformer model)"),m6e.forEach(t),agr=i(re),JF=n(re,"LI",{});var g6e=s(JF);o_e=n(g6e,"STRONG",{});var Xit=s(o_e);ngr=r(Xit,"mobilebert"),Xit.forEach(t),sgr=r(g6e," \u2014 "),KG=n(g6e,"A",{href:!0});var zit=s(KG);lgr=r(zit,"TFMobileBertForQuestionAnswering"),zit.forEach(t),igr=r(g6e," (MobileBERT model)"),g6e.forEach(t),dgr=i(re),YF=n(re,"LI",{});var h6e=s(YF);r_e=n(h6e,"STRONG",{});var Vit=s(r_e);cgr=r(Vit,"mpnet"),Vit.forEach(t),fgr=r(h6e," \u2014 "),ZG=n(h6e,"A",{href:!0});var Wit=s(ZG);mgr=r(Wit,"TFMPNetForQuestionAnswering"),Wit.forEach(t),ggr=r(h6e," (MPNet model)"),h6e.forEach(t),hgr=i(re),KF=n(re,"LI",{});var p6e=s(KF);t_e=n(p6e,"STRONG",{});var Qit=s(t_e);pgr=r(Qit,"rembert"),Qit.forEach(t),_gr=r(p6e," \u2014 "),eO=n(p6e,"A",{href:!0});var Hit=s(eO);ugr=r(Hit,"TFRemBertForQuestionAnswering"),Hit.forEach(t),bgr=r(p6e," (RemBERT model)"),p6e.forEach(t),vgr=i(re),ZF=n(re,"LI",{});var _6e=s(ZF);a_e=n(_6e,"STRONG",{});var Uit=s(a_e);Tgr=r(Uit,"roberta"),Uit.forEach(t),Fgr=r(_6e," \u2014 "),oO=n(_6e,"A",{href:!0});var Jit=s(oO);Cgr=r(Jit,"TFRobertaForQuestionAnswering"),Jit.forEach(t),Mgr=r(_6e," (RoBERTa model)"),_6e.forEach(t),Egr=i(re),e9=n(re,"LI",{});var u6e=s(e9);n_e=n(u6e,"STRONG",{});var Yit=s(n_e);ygr=r(Yit,"roformer"),Yit.forEach(t),wgr=r(u6e," \u2014 "),rO=n(u6e,"A",{href:!0});var Kit=s(rO);Agr=r(Kit,"TFRoFormerForQuestionAnswering"),Kit.forEach(t),Lgr=r(u6e," (RoFormer model)"),u6e.forEach(t),Bgr=i(re),o9=n(re,"LI",{});var b6e=s(o9);s_e=n(b6e,"STRONG",{});var Zit=s(s_e);kgr=r(Zit,"xlm"),Zit.forEach(t),xgr=r(b6e," \u2014 "),tO=n(b6e,"A",{href:!0});var edt=s(tO);Rgr=r(edt,"TFXLMForQuestionAnsweringSimple"),edt.forEach(t),Sgr=r(b6e," (XLM model)"),b6e.forEach(t),Pgr=i(re),r9=n(re,"LI",{});var v6e=s(r9);l_e=n(v6e,"STRONG",{});var odt=s(l_e);$gr=r(odt,"xlm-roberta"),odt.forEach(t),Igr=r(v6e," \u2014 "),aO=n(v6e,"A",{href:!0});var rdt=s(aO);jgr=r(rdt,"TFXLMRobertaForQuestionAnswering"),rdt.forEach(t),Ngr=r(v6e," (XLM-RoBERTa model)"),v6e.forEach(t),Dgr=i(re),t9=n(re,"LI",{});var T6e=s(t9);i_e=n(T6e,"STRONG",{});var tdt=s(i_e);qgr=r(tdt,"xlnet"),tdt.forEach(t),Ggr=r(T6e," \u2014 "),nO=n(T6e,"A",{href:!0});var adt=s(nO);Ogr=r(adt,"TFXLNetForQuestionAnsweringSimple"),adt.forEach(t),Xgr=r(T6e," (XLNet model)"),T6e.forEach(t),re.forEach(t),zgr=i(Ta),d_e=n(Ta,"P",{});var ndt=s(d_e);Vgr=r(ndt,"Examples:"),ndt.forEach(t),Wgr=i(Ta),m(Kw.$$.fragment,Ta),Ta.forEach(t),Jl.forEach(t),S8e=i(d),$c=n(d,"H2",{class:!0});var Oke=s($c);a9=n(Oke,"A",{id:!0,class:!0,href:!0});var sdt=s(a9);c_e=n(sdt,"SPAN",{});var ldt=s(c_e);m(Zw.$$.fragment,ldt),ldt.forEach(t),sdt.forEach(t),Qgr=i(Oke),f_e=n(Oke,"SPAN",{});var idt=s(f_e);Hgr=r(idt,"TFAutoModelForVision2Seq"),idt.forEach(t),Oke.forEach(t),P8e=i(d),Er=n(d,"DIV",{class:!0});var Kl=s(Er);m(eA.$$.fragment,Kl),Ugr=i(Kl),Ic=n(Kl,"P",{});var fV=s(Ic);Jgr=r(fV,`This is a generic model class that will be instantiated as one of the model classes of the library (with a vision-to-text modeling head) when created
with the `),m_e=n(fV,"CODE",{});var ddt=s(m_e);Ygr=r(ddt,"from_pretrained()"),ddt.forEach(t),Kgr=r(fV,"class method or the "),g_e=n(fV,"CODE",{});var cdt=s(g_e);Zgr=r(cdt,"from_config()"),cdt.forEach(t),ehr=r(fV,`class
method.`),fV.forEach(t),ohr=i(Kl),oA=n(Kl,"P",{});var Xke=s(oA);rhr=r(Xke,"This class cannot be instantiated directly using "),h_e=n(Xke,"CODE",{});var fdt=s(h_e);thr=r(fdt,"__init__()"),fdt.forEach(t),ahr=r(Xke," (throws an error)."),Xke.forEach(t),nhr=i(Kl),ut=n(Kl,"DIV",{class:!0});var Zl=s(ut);m(rA.$$.fragment,Zl),shr=i(Zl),p_e=n(Zl,"P",{});var mdt=s(p_e);lhr=r(mdt,"Instantiates one of the model classes of the library (with a vision-to-text modeling head) from a configuration."),mdt.forEach(t),ihr=i(Zl),jc=n(Zl,"P",{});var mV=s(jc);dhr=r(mV,`Note:
Loading a model from its configuration file does `),__e=n(mV,"STRONG",{});var gdt=s(__e);chr=r(gdt,"not"),gdt.forEach(t),fhr=r(mV,` load the model weights. It only affects the
model\u2019s configuration. Use `),u_e=n(mV,"CODE",{});var hdt=s(u_e);mhr=r(hdt,"from_pretrained()"),hdt.forEach(t),ghr=r(mV,"to load the model weights."),mV.forEach(t),hhr=i(Zl),b_e=n(Zl,"P",{});var pdt=s(b_e);phr=r(pdt,"Examples:"),pdt.forEach(t),_hr=i(Zl),m(tA.$$.fragment,Zl),Zl.forEach(t),uhr=i(Kl),Eo=n(Kl,"DIV",{class:!0});var Fa=s(Eo);m(aA.$$.fragment,Fa),bhr=i(Fa),v_e=n(Fa,"P",{});var _dt=s(v_e);vhr=r(_dt,"Instantiate one of the model classes of the library (with a vision-to-text modeling head) from a pretrained model."),_dt.forEach(t),Thr=i(Fa),vn=n(Fa,"P",{});var G4=s(vn);Fhr=r(G4,"The model class to instantiate is selected based on the "),T_e=n(G4,"CODE",{});var udt=s(T_e);Chr=r(udt,"model_type"),udt.forEach(t),Mhr=r(G4,` property of the config object (either
passed as an argument or loaded from `),F_e=n(G4,"CODE",{});var bdt=s(F_e);Ehr=r(bdt,"pretrained_model_name_or_path"),bdt.forEach(t),yhr=r(G4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),C_e=n(G4,"CODE",{});var vdt=s(C_e);whr=r(vdt,"pretrained_model_name_or_path"),vdt.forEach(t),Ahr=r(G4,":"),G4.forEach(t),Lhr=i(Fa),M_e=n(Fa,"UL",{});var Tdt=s(M_e);n9=n(Tdt,"LI",{});var F6e=s(n9);E_e=n(F6e,"STRONG",{});var Fdt=s(E_e);Bhr=r(Fdt,"vision-encoder-decoder"),Fdt.forEach(t),khr=r(F6e," \u2014 "),sO=n(F6e,"A",{href:!0});var Cdt=s(sO);xhr=r(Cdt,"TFVisionEncoderDecoderModel"),Cdt.forEach(t),Rhr=r(F6e," (Vision Encoder decoder model)"),F6e.forEach(t),Tdt.forEach(t),Shr=i(Fa),y_e=n(Fa,"P",{});var Mdt=s(y_e);Phr=r(Mdt,"Examples:"),Mdt.forEach(t),$hr=i(Fa),m(nA.$$.fragment,Fa),Fa.forEach(t),Kl.forEach(t),$8e=i(d),Nc=n(d,"H2",{class:!0});var zke=s(Nc);s9=n(zke,"A",{id:!0,class:!0,href:!0});var Edt=s(s9);w_e=n(Edt,"SPAN",{});var ydt=s(w_e);m(sA.$$.fragment,ydt),ydt.forEach(t),Edt.forEach(t),Ihr=i(zke),A_e=n(zke,"SPAN",{});var wdt=s(A_e);jhr=r(wdt,"TFAutoModelForSpeechSeq2Seq"),wdt.forEach(t),zke.forEach(t),I8e=i(d),yr=n(d,"DIV",{class:!0});var ei=s(yr);m(lA.$$.fragment,ei),Nhr=i(ei),Dc=n(ei,"P",{});var gV=s(Dc);Dhr=r(gV,`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) when created
with the `),L_e=n(gV,"CODE",{});var Adt=s(L_e);qhr=r(Adt,"from_pretrained()"),Adt.forEach(t),Ghr=r(gV,"class method or the "),B_e=n(gV,"CODE",{});var Ldt=s(B_e);Ohr=r(Ldt,"from_config()"),Ldt.forEach(t),Xhr=r(gV,`class
method.`),gV.forEach(t),zhr=i(ei),iA=n(ei,"P",{});var Vke=s(iA);Vhr=r(Vke,"This class cannot be instantiated directly using "),k_e=n(Vke,"CODE",{});var Bdt=s(k_e);Whr=r(Bdt,"__init__()"),Bdt.forEach(t),Qhr=r(Vke," (throws an error)."),Vke.forEach(t),Hhr=i(ei),bt=n(ei,"DIV",{class:!0});var oi=s(bt);m(dA.$$.fragment,oi),Uhr=i(oi),x_e=n(oi,"P",{});var kdt=s(x_e);Jhr=r(kdt,"Instantiates one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) from a configuration."),kdt.forEach(t),Yhr=i(oi),qc=n(oi,"P",{});var hV=s(qc);Khr=r(hV,`Note:
Loading a model from its configuration file does `),R_e=n(hV,"STRONG",{});var xdt=s(R_e);Zhr=r(xdt,"not"),xdt.forEach(t),epr=r(hV,` load the model weights. It only affects the
model\u2019s configuration. Use `),S_e=n(hV,"CODE",{});var Rdt=s(S_e);opr=r(Rdt,"from_pretrained()"),Rdt.forEach(t),rpr=r(hV,"to load the model weights."),hV.forEach(t),tpr=i(oi),P_e=n(oi,"P",{});var Sdt=s(P_e);apr=r(Sdt,"Examples:"),Sdt.forEach(t),npr=i(oi),m(cA.$$.fragment,oi),oi.forEach(t),spr=i(ei),yo=n(ei,"DIV",{class:!0});var Ca=s(yo);m(fA.$$.fragment,Ca),lpr=i(Ca),$_e=n(Ca,"P",{});var Pdt=s($_e);ipr=r(Pdt,"Instantiate one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) from a pretrained model."),Pdt.forEach(t),dpr=i(Ca),Tn=n(Ca,"P",{});var O4=s(Tn);cpr=r(O4,"The model class to instantiate is selected based on the "),I_e=n(O4,"CODE",{});var $dt=s(I_e);fpr=r($dt,"model_type"),$dt.forEach(t),mpr=r(O4,` property of the config object (either
passed as an argument or loaded from `),j_e=n(O4,"CODE",{});var Idt=s(j_e);gpr=r(Idt,"pretrained_model_name_or_path"),Idt.forEach(t),hpr=r(O4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),N_e=n(O4,"CODE",{});var jdt=s(N_e);ppr=r(jdt,"pretrained_model_name_or_path"),jdt.forEach(t),_pr=r(O4,":"),O4.forEach(t),upr=i(Ca),D_e=n(Ca,"UL",{});var Ndt=s(D_e);l9=n(Ndt,"LI",{});var C6e=s(l9);q_e=n(C6e,"STRONG",{});var Ddt=s(q_e);bpr=r(Ddt,"speech_to_text"),Ddt.forEach(t),vpr=r(C6e," \u2014 "),lO=n(C6e,"A",{href:!0});var qdt=s(lO);Tpr=r(qdt,"TFSpeech2TextForConditionalGeneration"),qdt.forEach(t),Fpr=r(C6e," (Speech2Text model)"),C6e.forEach(t),Ndt.forEach(t),Cpr=i(Ca),G_e=n(Ca,"P",{});var Gdt=s(G_e);Mpr=r(Gdt,"Examples:"),Gdt.forEach(t),Epr=i(Ca),m(mA.$$.fragment,Ca),Ca.forEach(t),ei.forEach(t),j8e=i(d),Gc=n(d,"H2",{class:!0});var Wke=s(Gc);i9=n(Wke,"A",{id:!0,class:!0,href:!0});var Odt=s(i9);O_e=n(Odt,"SPAN",{});var Xdt=s(O_e);m(gA.$$.fragment,Xdt),Xdt.forEach(t),Odt.forEach(t),ypr=i(Wke),X_e=n(Wke,"SPAN",{});var zdt=s(X_e);wpr=r(zdt,"FlaxAutoModel"),zdt.forEach(t),Wke.forEach(t),N8e=i(d),wr=n(d,"DIV",{class:!0});var ri=s(wr);m(hA.$$.fragment,ri),Apr=i(ri),Oc=n(ri,"P",{});var pV=s(Oc);Lpr=r(pV,`This is a generic model class that will be instantiated as one of the base model classes of the library when created
with the `),z_e=n(pV,"CODE",{});var Vdt=s(z_e);Bpr=r(Vdt,"from_pretrained()"),Vdt.forEach(t),kpr=r(pV,"class method or the "),V_e=n(pV,"CODE",{});var Wdt=s(V_e);xpr=r(Wdt,"from_config()"),Wdt.forEach(t),Rpr=r(pV,`class
method.`),pV.forEach(t),Spr=i(ri),pA=n(ri,"P",{});var Qke=s(pA);Ppr=r(Qke,"This class cannot be instantiated directly using "),W_e=n(Qke,"CODE",{});var Qdt=s(W_e);$pr=r(Qdt,"__init__()"),Qdt.forEach(t),Ipr=r(Qke," (throws an error)."),Qke.forEach(t),jpr=i(ri),vt=n(ri,"DIV",{class:!0});var ti=s(vt);m(_A.$$.fragment,ti),Npr=i(ti),Q_e=n(ti,"P",{});var Hdt=s(Q_e);Dpr=r(Hdt,"Instantiates one of the base model classes of the library from a configuration."),Hdt.forEach(t),qpr=i(ti),Xc=n(ti,"P",{});var _V=s(Xc);Gpr=r(_V,`Note:
Loading a model from its configuration file does `),H_e=n(_V,"STRONG",{});var Udt=s(H_e);Opr=r(Udt,"not"),Udt.forEach(t),Xpr=r(_V,` load the model weights. It only affects the
model\u2019s configuration. Use `),U_e=n(_V,"CODE",{});var Jdt=s(U_e);zpr=r(Jdt,"from_pretrained()"),Jdt.forEach(t),Vpr=r(_V,"to load the model weights."),_V.forEach(t),Wpr=i(ti),J_e=n(ti,"P",{});var Ydt=s(J_e);Qpr=r(Ydt,"Examples:"),Ydt.forEach(t),Hpr=i(ti),m(uA.$$.fragment,ti),ti.forEach(t),Upr=i(ri),wo=n(ri,"DIV",{class:!0});var Ma=s(wo);m(bA.$$.fragment,Ma),Jpr=i(Ma),Y_e=n(Ma,"P",{});var Kdt=s(Y_e);Ypr=r(Kdt,"Instantiate one of the base model classes of the library from a pretrained model."),Kdt.forEach(t),Kpr=i(Ma),Fn=n(Ma,"P",{});var X4=s(Fn);Zpr=r(X4,"The model class to instantiate is selected based on the "),K_e=n(X4,"CODE",{});var Zdt=s(K_e);e_r=r(Zdt,"model_type"),Zdt.forEach(t),o_r=r(X4,` property of the config object (either
passed as an argument or loaded from `),Z_e=n(X4,"CODE",{});var ect=s(Z_e);r_r=r(ect,"pretrained_model_name_or_path"),ect.forEach(t),t_r=r(X4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),eue=n(X4,"CODE",{});var oct=s(eue);a_r=r(oct,"pretrained_model_name_or_path"),oct.forEach(t),n_r=r(X4,":"),X4.forEach(t),s_r=i(Ma),V=n(Ma,"UL",{});var Q=s(V);d9=n(Q,"LI",{});var M6e=s(d9);oue=n(M6e,"STRONG",{});var rct=s(oue);l_r=r(rct,"albert"),rct.forEach(t),i_r=r(M6e," \u2014 "),iO=n(M6e,"A",{href:!0});var tct=s(iO);d_r=r(tct,"FlaxAlbertModel"),tct.forEach(t),c_r=r(M6e," (ALBERT model)"),M6e.forEach(t),f_r=i(Q),c9=n(Q,"LI",{});var E6e=s(c9);rue=n(E6e,"STRONG",{});var act=s(rue);m_r=r(act,"bart"),act.forEach(t),g_r=r(E6e," \u2014 "),dO=n(E6e,"A",{href:!0});var nct=s(dO);h_r=r(nct,"FlaxBartModel"),nct.forEach(t),p_r=r(E6e," (BART model)"),E6e.forEach(t),__r=i(Q),f9=n(Q,"LI",{});var y6e=s(f9);tue=n(y6e,"STRONG",{});var sct=s(tue);u_r=r(sct,"beit"),sct.forEach(t),b_r=r(y6e," \u2014 "),cO=n(y6e,"A",{href:!0});var lct=s(cO);v_r=r(lct,"FlaxBeitModel"),lct.forEach(t),T_r=r(y6e," (BEiT model)"),y6e.forEach(t),F_r=i(Q),m9=n(Q,"LI",{});var w6e=s(m9);aue=n(w6e,"STRONG",{});var ict=s(aue);C_r=r(ict,"bert"),ict.forEach(t),M_r=r(w6e," \u2014 "),fO=n(w6e,"A",{href:!0});var dct=s(fO);E_r=r(dct,"FlaxBertModel"),dct.forEach(t),y_r=r(w6e," (BERT model)"),w6e.forEach(t),w_r=i(Q),g9=n(Q,"LI",{});var A6e=s(g9);nue=n(A6e,"STRONG",{});var cct=s(nue);A_r=r(cct,"big_bird"),cct.forEach(t),L_r=r(A6e," \u2014 "),mO=n(A6e,"A",{href:!0});var fct=s(mO);B_r=r(fct,"FlaxBigBirdModel"),fct.forEach(t),k_r=r(A6e," (BigBird model)"),A6e.forEach(t),x_r=i(Q),h9=n(Q,"LI",{});var L6e=s(h9);sue=n(L6e,"STRONG",{});var mct=s(sue);R_r=r(mct,"blenderbot"),mct.forEach(t),S_r=r(L6e," \u2014 "),gO=n(L6e,"A",{href:!0});var gct=s(gO);P_r=r(gct,"FlaxBlenderbotModel"),gct.forEach(t),$_r=r(L6e," (Blenderbot model)"),L6e.forEach(t),I_r=i(Q),p9=n(Q,"LI",{});var B6e=s(p9);lue=n(B6e,"STRONG",{});var hct=s(lue);j_r=r(hct,"blenderbot-small"),hct.forEach(t),N_r=r(B6e," \u2014 "),hO=n(B6e,"A",{href:!0});var pct=s(hO);D_r=r(pct,"FlaxBlenderbotSmallModel"),pct.forEach(t),q_r=r(B6e," (BlenderbotSmall model)"),B6e.forEach(t),G_r=i(Q),_9=n(Q,"LI",{});var k6e=s(_9);iue=n(k6e,"STRONG",{});var _ct=s(iue);O_r=r(_ct,"clip"),_ct.forEach(t),X_r=r(k6e," \u2014 "),pO=n(k6e,"A",{href:!0});var uct=s(pO);z_r=r(uct,"FlaxCLIPModel"),uct.forEach(t),V_r=r(k6e," (CLIP model)"),k6e.forEach(t),W_r=i(Q),u9=n(Q,"LI",{});var x6e=s(u9);due=n(x6e,"STRONG",{});var bct=s(due);Q_r=r(bct,"distilbert"),bct.forEach(t),H_r=r(x6e," \u2014 "),_O=n(x6e,"A",{href:!0});var vct=s(_O);U_r=r(vct,"FlaxDistilBertModel"),vct.forEach(t),J_r=r(x6e," (DistilBERT model)"),x6e.forEach(t),Y_r=i(Q),b9=n(Q,"LI",{});var R6e=s(b9);cue=n(R6e,"STRONG",{});var Tct=s(cue);K_r=r(Tct,"electra"),Tct.forEach(t),Z_r=r(R6e," \u2014 "),uO=n(R6e,"A",{href:!0});var Fct=s(uO);eur=r(Fct,"FlaxElectraModel"),Fct.forEach(t),our=r(R6e," (ELECTRA model)"),R6e.forEach(t),rur=i(Q),v9=n(Q,"LI",{});var S6e=s(v9);fue=n(S6e,"STRONG",{});var Cct=s(fue);tur=r(Cct,"gpt2"),Cct.forEach(t),aur=r(S6e," \u2014 "),bO=n(S6e,"A",{href:!0});var Mct=s(bO);nur=r(Mct,"FlaxGPT2Model"),Mct.forEach(t),sur=r(S6e," (OpenAI GPT-2 model)"),S6e.forEach(t),lur=i(Q),T9=n(Q,"LI",{});var P6e=s(T9);mue=n(P6e,"STRONG",{});var Ect=s(mue);iur=r(Ect,"gpt_neo"),Ect.forEach(t),dur=r(P6e," \u2014 "),vO=n(P6e,"A",{href:!0});var yct=s(vO);cur=r(yct,"FlaxGPTNeoModel"),yct.forEach(t),fur=r(P6e," (GPT Neo model)"),P6e.forEach(t),mur=i(Q),F9=n(Q,"LI",{});var $6e=s(F9);gue=n($6e,"STRONG",{});var wct=s(gue);gur=r(wct,"gptj"),wct.forEach(t),hur=r($6e," \u2014 "),TO=n($6e,"A",{href:!0});var Act=s(TO);pur=r(Act,"FlaxGPTJModel"),Act.forEach(t),_ur=r($6e," (GPT-J model)"),$6e.forEach(t),uur=i(Q),C9=n(Q,"LI",{});var I6e=s(C9);hue=n(I6e,"STRONG",{});var Lct=s(hue);bur=r(Lct,"marian"),Lct.forEach(t),vur=r(I6e," \u2014 "),FO=n(I6e,"A",{href:!0});var Bct=s(FO);Tur=r(Bct,"FlaxMarianModel"),Bct.forEach(t),Fur=r(I6e," (Marian model)"),I6e.forEach(t),Cur=i(Q),M9=n(Q,"LI",{});var j6e=s(M9);pue=n(j6e,"STRONG",{});var kct=s(pue);Mur=r(kct,"mbart"),kct.forEach(t),Eur=r(j6e," \u2014 "),CO=n(j6e,"A",{href:!0});var xct=s(CO);yur=r(xct,"FlaxMBartModel"),xct.forEach(t),wur=r(j6e," (mBART model)"),j6e.forEach(t),Aur=i(Q),E9=n(Q,"LI",{});var N6e=s(E9);_ue=n(N6e,"STRONG",{});var Rct=s(_ue);Lur=r(Rct,"mt5"),Rct.forEach(t),Bur=r(N6e," \u2014 "),MO=n(N6e,"A",{href:!0});var Sct=s(MO);kur=r(Sct,"FlaxMT5Model"),Sct.forEach(t),xur=r(N6e," (mT5 model)"),N6e.forEach(t),Rur=i(Q),y9=n(Q,"LI",{});var D6e=s(y9);uue=n(D6e,"STRONG",{});var Pct=s(uue);Sur=r(Pct,"pegasus"),Pct.forEach(t),Pur=r(D6e," \u2014 "),EO=n(D6e,"A",{href:!0});var $ct=s(EO);$ur=r($ct,"FlaxPegasusModel"),$ct.forEach(t),Iur=r(D6e," (Pegasus model)"),D6e.forEach(t),jur=i(Q),w9=n(Q,"LI",{});var q6e=s(w9);bue=n(q6e,"STRONG",{});var Ict=s(bue);Nur=r(Ict,"roberta"),Ict.forEach(t),Dur=r(q6e," \u2014 "),yO=n(q6e,"A",{href:!0});var jct=s(yO);qur=r(jct,"FlaxRobertaModel"),jct.forEach(t),Gur=r(q6e," (RoBERTa model)"),q6e.forEach(t),Our=i(Q),A9=n(Q,"LI",{});var G6e=s(A9);vue=n(G6e,"STRONG",{});var Nct=s(vue);Xur=r(Nct,"roformer"),Nct.forEach(t),zur=r(G6e," \u2014 "),wO=n(G6e,"A",{href:!0});var Dct=s(wO);Vur=r(Dct,"FlaxRoFormerModel"),Dct.forEach(t),Wur=r(G6e," (RoFormer model)"),G6e.forEach(t),Qur=i(Q),L9=n(Q,"LI",{});var O6e=s(L9);Tue=n(O6e,"STRONG",{});var qct=s(Tue);Hur=r(qct,"t5"),qct.forEach(t),Uur=r(O6e," \u2014 "),AO=n(O6e,"A",{href:!0});var Gct=s(AO);Jur=r(Gct,"FlaxT5Model"),Gct.forEach(t),Yur=r(O6e," (T5 model)"),O6e.forEach(t),Kur=i(Q),B9=n(Q,"LI",{});var X6e=s(B9);Fue=n(X6e,"STRONG",{});var Oct=s(Fue);Zur=r(Oct,"vision-text-dual-encoder"),Oct.forEach(t),e2r=r(X6e," \u2014 "),LO=n(X6e,"A",{href:!0});var Xct=s(LO);o2r=r(Xct,"FlaxVisionTextDualEncoderModel"),Xct.forEach(t),r2r=r(X6e," (VisionTextDualEncoder model)"),X6e.forEach(t),t2r=i(Q),k9=n(Q,"LI",{});var z6e=s(k9);Cue=n(z6e,"STRONG",{});var zct=s(Cue);a2r=r(zct,"vit"),zct.forEach(t),n2r=r(z6e," \u2014 "),BO=n(z6e,"A",{href:!0});var Vct=s(BO);s2r=r(Vct,"FlaxViTModel"),Vct.forEach(t),l2r=r(z6e," (ViT model)"),z6e.forEach(t),i2r=i(Q),x9=n(Q,"LI",{});var V6e=s(x9);Mue=n(V6e,"STRONG",{});var Wct=s(Mue);d2r=r(Wct,"wav2vec2"),Wct.forEach(t),c2r=r(V6e," \u2014 "),kO=n(V6e,"A",{href:!0});var Qct=s(kO);f2r=r(Qct,"FlaxWav2Vec2Model"),Qct.forEach(t),m2r=r(V6e," (Wav2Vec2 model)"),V6e.forEach(t),g2r=i(Q),R9=n(Q,"LI",{});var W6e=s(R9);Eue=n(W6e,"STRONG",{});var Hct=s(Eue);h2r=r(Hct,"xglm"),Hct.forEach(t),p2r=r(W6e," \u2014 "),xO=n(W6e,"A",{href:!0});var Uct=s(xO);_2r=r(Uct,"FlaxXGLMModel"),Uct.forEach(t),u2r=r(W6e," (XGLM model)"),W6e.forEach(t),Q.forEach(t),b2r=i(Ma),yue=n(Ma,"P",{});var Jct=s(yue);v2r=r(Jct,"Examples:"),Jct.forEach(t),T2r=i(Ma),m(vA.$$.fragment,Ma),Ma.forEach(t),ri.forEach(t),D8e=i(d),zc=n(d,"H2",{class:!0});var Hke=s(zc);S9=n(Hke,"A",{id:!0,class:!0,href:!0});var Yct=s(S9);wue=n(Yct,"SPAN",{});var Kct=s(wue);m(TA.$$.fragment,Kct),Kct.forEach(t),Yct.forEach(t),F2r=i(Hke),Aue=n(Hke,"SPAN",{});var Zct=s(Aue);C2r=r(Zct,"FlaxAutoModelForCausalLM"),Zct.forEach(t),Hke.forEach(t),q8e=i(d),Ar=n(d,"DIV",{class:!0});var ai=s(Ar);m(FA.$$.fragment,ai),M2r=i(ai),Vc=n(ai,"P",{});var uV=s(Vc);E2r=r(uV,`This is a generic model class that will be instantiated as one of the model classes of the library (with a causal language modeling head) when created
with the `),Lue=n(uV,"CODE",{});var eft=s(Lue);y2r=r(eft,"from_pretrained()"),eft.forEach(t),w2r=r(uV,"class method or the "),Bue=n(uV,"CODE",{});var oft=s(Bue);A2r=r(oft,"from_config()"),oft.forEach(t),L2r=r(uV,`class
method.`),uV.forEach(t),B2r=i(ai),CA=n(ai,"P",{});var Uke=s(CA);k2r=r(Uke,"This class cannot be instantiated directly using "),kue=n(Uke,"CODE",{});var rft=s(kue);x2r=r(rft,"__init__()"),rft.forEach(t),R2r=r(Uke," (throws an error)."),Uke.forEach(t),S2r=i(ai),Tt=n(ai,"DIV",{class:!0});var ni=s(Tt);m(MA.$$.fragment,ni),P2r=i(ni),xue=n(ni,"P",{});var tft=s(xue);$2r=r(tft,"Instantiates one of the model classes of the library (with a causal language modeling head) from a configuration."),tft.forEach(t),I2r=i(ni),Wc=n(ni,"P",{});var bV=s(Wc);j2r=r(bV,`Note:
Loading a model from its configuration file does `),Rue=n(bV,"STRONG",{});var aft=s(Rue);N2r=r(aft,"not"),aft.forEach(t),D2r=r(bV,` load the model weights. It only affects the
model\u2019s configuration. Use `),Sue=n(bV,"CODE",{});var nft=s(Sue);q2r=r(nft,"from_pretrained()"),nft.forEach(t),G2r=r(bV,"to load the model weights."),bV.forEach(t),O2r=i(ni),Pue=n(ni,"P",{});var sft=s(Pue);X2r=r(sft,"Examples:"),sft.forEach(t),z2r=i(ni),m(EA.$$.fragment,ni),ni.forEach(t),V2r=i(ai),Ao=n(ai,"DIV",{class:!0});var Ea=s(Ao);m(yA.$$.fragment,Ea),W2r=i(Ea),$ue=n(Ea,"P",{});var lft=s($ue);Q2r=r(lft,"Instantiate one of the model classes of the library (with a causal language modeling head) from a pretrained model."),lft.forEach(t),H2r=i(Ea),Cn=n(Ea,"P",{});var z4=s(Cn);U2r=r(z4,"The model class to instantiate is selected based on the "),Iue=n(z4,"CODE",{});var ift=s(Iue);J2r=r(ift,"model_type"),ift.forEach(t),Y2r=r(z4,` property of the config object (either
passed as an argument or loaded from `),jue=n(z4,"CODE",{});var dft=s(jue);K2r=r(dft,"pretrained_model_name_or_path"),dft.forEach(t),Z2r=r(z4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Nue=n(z4,"CODE",{});var cft=s(Nue);e1r=r(cft,"pretrained_model_name_or_path"),cft.forEach(t),o1r=r(z4,":"),z4.forEach(t),r1r=i(Ea),Mn=n(Ea,"UL",{});var V4=s(Mn);P9=n(V4,"LI",{});var Q6e=s(P9);Due=n(Q6e,"STRONG",{});var fft=s(Due);t1r=r(fft,"gpt2"),fft.forEach(t),a1r=r(Q6e," \u2014 "),RO=n(Q6e,"A",{href:!0});var mft=s(RO);n1r=r(mft,"FlaxGPT2LMHeadModel"),mft.forEach(t),s1r=r(Q6e," (OpenAI GPT-2 model)"),Q6e.forEach(t),l1r=i(V4),$9=n(V4,"LI",{});var H6e=s($9);que=n(H6e,"STRONG",{});var gft=s(que);i1r=r(gft,"gpt_neo"),gft.forEach(t),d1r=r(H6e," \u2014 "),SO=n(H6e,"A",{href:!0});var hft=s(SO);c1r=r(hft,"FlaxGPTNeoForCausalLM"),hft.forEach(t),f1r=r(H6e," (GPT Neo model)"),H6e.forEach(t),m1r=i(V4),I9=n(V4,"LI",{});var U6e=s(I9);Gue=n(U6e,"STRONG",{});var pft=s(Gue);g1r=r(pft,"gptj"),pft.forEach(t),h1r=r(U6e," \u2014 "),PO=n(U6e,"A",{href:!0});var _ft=s(PO);p1r=r(_ft,"FlaxGPTJForCausalLM"),_ft.forEach(t),_1r=r(U6e," (GPT-J model)"),U6e.forEach(t),u1r=i(V4),j9=n(V4,"LI",{});var J6e=s(j9);Oue=n(J6e,"STRONG",{});var uft=s(Oue);b1r=r(uft,"xglm"),uft.forEach(t),v1r=r(J6e," \u2014 "),$O=n(J6e,"A",{href:!0});var bft=s($O);T1r=r(bft,"FlaxXGLMForCausalLM"),bft.forEach(t),F1r=r(J6e," (XGLM model)"),J6e.forEach(t),V4.forEach(t),C1r=i(Ea),Xue=n(Ea,"P",{});var vft=s(Xue);M1r=r(vft,"Examples:"),vft.forEach(t),E1r=i(Ea),m(wA.$$.fragment,Ea),Ea.forEach(t),ai.forEach(t),G8e=i(d),Qc=n(d,"H2",{class:!0});var Jke=s(Qc);N9=n(Jke,"A",{id:!0,class:!0,href:!0});var Tft=s(N9);zue=n(Tft,"SPAN",{});var Fft=s(zue);m(AA.$$.fragment,Fft),Fft.forEach(t),Tft.forEach(t),y1r=i(Jke),Vue=n(Jke,"SPAN",{});var Cft=s(Vue);w1r=r(Cft,"FlaxAutoModelForPreTraining"),Cft.forEach(t),Jke.forEach(t),O8e=i(d),Lr=n(d,"DIV",{class:!0});var si=s(Lr);m(LA.$$.fragment,si),A1r=i(si),Hc=n(si,"P",{});var vV=s(Hc);L1r=r(vV,`This is a generic model class that will be instantiated as one of the model classes of the library (with a pretraining head) when created
with the `),Wue=n(vV,"CODE",{});var Mft=s(Wue);B1r=r(Mft,"from_pretrained()"),Mft.forEach(t),k1r=r(vV,"class method or the "),Que=n(vV,"CODE",{});var Eft=s(Que);x1r=r(Eft,"from_config()"),Eft.forEach(t),R1r=r(vV,`class
method.`),vV.forEach(t),S1r=i(si),BA=n(si,"P",{});var Yke=s(BA);P1r=r(Yke,"This class cannot be instantiated directly using "),Hue=n(Yke,"CODE",{});var yft=s(Hue);$1r=r(yft,"__init__()"),yft.forEach(t),I1r=r(Yke," (throws an error)."),Yke.forEach(t),j1r=i(si),Ft=n(si,"DIV",{class:!0});var li=s(Ft);m(kA.$$.fragment,li),N1r=i(li),Uue=n(li,"P",{});var wft=s(Uue);D1r=r(wft,"Instantiates one of the model classes of the library (with a pretraining head) from a configuration."),wft.forEach(t),q1r=i(li),Uc=n(li,"P",{});var TV=s(Uc);G1r=r(TV,`Note:
Loading a model from its configuration file does `),Jue=n(TV,"STRONG",{});var Aft=s(Jue);O1r=r(Aft,"not"),Aft.forEach(t),X1r=r(TV,` load the model weights. It only affects the
model\u2019s configuration. Use `),Yue=n(TV,"CODE",{});var Lft=s(Yue);z1r=r(Lft,"from_pretrained()"),Lft.forEach(t),V1r=r(TV,"to load the model weights."),TV.forEach(t),W1r=i(li),Kue=n(li,"P",{});var Bft=s(Kue);Q1r=r(Bft,"Examples:"),Bft.forEach(t),H1r=i(li),m(xA.$$.fragment,li),li.forEach(t),U1r=i(si),Lo=n(si,"DIV",{class:!0});var ya=s(Lo);m(RA.$$.fragment,ya),J1r=i(ya),Zue=n(ya,"P",{});var kft=s(Zue);Y1r=r(kft,"Instantiate one of the model classes of the library (with a pretraining head) from a pretrained model."),kft.forEach(t),K1r=i(ya),En=n(ya,"P",{});var W4=s(En);Z1r=r(W4,"The model class to instantiate is selected based on the "),e2e=n(W4,"CODE",{});var xft=s(e2e);ebr=r(xft,"model_type"),xft.forEach(t),obr=r(W4,` property of the config object (either
passed as an argument or loaded from `),o2e=n(W4,"CODE",{});var Rft=s(o2e);rbr=r(Rft,"pretrained_model_name_or_path"),Rft.forEach(t),tbr=r(W4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),r2e=n(W4,"CODE",{});var Sft=s(r2e);abr=r(Sft,"pretrained_model_name_or_path"),Sft.forEach(t),nbr=r(W4,":"),W4.forEach(t),sbr=i(ya),fe=n(ya,"UL",{});var _e=s(fe);D9=n(_e,"LI",{});var Y6e=s(D9);t2e=n(Y6e,"STRONG",{});var Pft=s(t2e);lbr=r(Pft,"albert"),Pft.forEach(t),ibr=r(Y6e," \u2014 "),IO=n(Y6e,"A",{href:!0});var $ft=s(IO);dbr=r($ft,"FlaxAlbertForPreTraining"),$ft.forEach(t),cbr=r(Y6e," (ALBERT model)"),Y6e.forEach(t),fbr=i(_e),q9=n(_e,"LI",{});var K6e=s(q9);a2e=n(K6e,"STRONG",{});var Ift=s(a2e);mbr=r(Ift,"bart"),Ift.forEach(t),gbr=r(K6e," \u2014 "),jO=n(K6e,"A",{href:!0});var jft=s(jO);hbr=r(jft,"FlaxBartForConditionalGeneration"),jft.forEach(t),pbr=r(K6e," (BART model)"),K6e.forEach(t),_br=i(_e),G9=n(_e,"LI",{});var Z6e=s(G9);n2e=n(Z6e,"STRONG",{});var Nft=s(n2e);ubr=r(Nft,"bert"),Nft.forEach(t),bbr=r(Z6e," \u2014 "),NO=n(Z6e,"A",{href:!0});var Dft=s(NO);vbr=r(Dft,"FlaxBertForPreTraining"),Dft.forEach(t),Tbr=r(Z6e," (BERT model)"),Z6e.forEach(t),Fbr=i(_e),O9=n(_e,"LI",{});var e0e=s(O9);s2e=n(e0e,"STRONG",{});var qft=s(s2e);Cbr=r(qft,"big_bird"),qft.forEach(t),Mbr=r(e0e," \u2014 "),DO=n(e0e,"A",{href:!0});var Gft=s(DO);Ebr=r(Gft,"FlaxBigBirdForPreTraining"),Gft.forEach(t),ybr=r(e0e," (BigBird model)"),e0e.forEach(t),wbr=i(_e),X9=n(_e,"LI",{});var o0e=s(X9);l2e=n(o0e,"STRONG",{});var Oft=s(l2e);Abr=r(Oft,"electra"),Oft.forEach(t),Lbr=r(o0e," \u2014 "),qO=n(o0e,"A",{href:!0});var Xft=s(qO);Bbr=r(Xft,"FlaxElectraForPreTraining"),Xft.forEach(t),kbr=r(o0e," (ELECTRA model)"),o0e.forEach(t),xbr=i(_e),z9=n(_e,"LI",{});var r0e=s(z9);i2e=n(r0e,"STRONG",{});var zft=s(i2e);Rbr=r(zft,"mbart"),zft.forEach(t),Sbr=r(r0e," \u2014 "),GO=n(r0e,"A",{href:!0});var Vft=s(GO);Pbr=r(Vft,"FlaxMBartForConditionalGeneration"),Vft.forEach(t),$br=r(r0e," (mBART model)"),r0e.forEach(t),Ibr=i(_e),V9=n(_e,"LI",{});var t0e=s(V9);d2e=n(t0e,"STRONG",{});var Wft=s(d2e);jbr=r(Wft,"mt5"),Wft.forEach(t),Nbr=r(t0e," \u2014 "),OO=n(t0e,"A",{href:!0});var Qft=s(OO);Dbr=r(Qft,"FlaxMT5ForConditionalGeneration"),Qft.forEach(t),qbr=r(t0e," (mT5 model)"),t0e.forEach(t),Gbr=i(_e),W9=n(_e,"LI",{});var a0e=s(W9);c2e=n(a0e,"STRONG",{});var Hft=s(c2e);Obr=r(Hft,"roberta"),Hft.forEach(t),Xbr=r(a0e," \u2014 "),XO=n(a0e,"A",{href:!0});var Uft=s(XO);zbr=r(Uft,"FlaxRobertaForMaskedLM"),Uft.forEach(t),Vbr=r(a0e," (RoBERTa model)"),a0e.forEach(t),Wbr=i(_e),Q9=n(_e,"LI",{});var n0e=s(Q9);f2e=n(n0e,"STRONG",{});var Jft=s(f2e);Qbr=r(Jft,"roformer"),Jft.forEach(t),Hbr=r(n0e," \u2014 "),zO=n(n0e,"A",{href:!0});var Yft=s(zO);Ubr=r(Yft,"FlaxRoFormerForMaskedLM"),Yft.forEach(t),Jbr=r(n0e," (RoFormer model)"),n0e.forEach(t),Ybr=i(_e),H9=n(_e,"LI",{});var s0e=s(H9);m2e=n(s0e,"STRONG",{});var Kft=s(m2e);Kbr=r(Kft,"t5"),Kft.forEach(t),Zbr=r(s0e," \u2014 "),VO=n(s0e,"A",{href:!0});var Zft=s(VO);e5r=r(Zft,"FlaxT5ForConditionalGeneration"),Zft.forEach(t),o5r=r(s0e," (T5 model)"),s0e.forEach(t),r5r=i(_e),U9=n(_e,"LI",{});var l0e=s(U9);g2e=n(l0e,"STRONG",{});var emt=s(g2e);t5r=r(emt,"wav2vec2"),emt.forEach(t),a5r=r(l0e," \u2014 "),WO=n(l0e,"A",{href:!0});var omt=s(WO);n5r=r(omt,"FlaxWav2Vec2ForPreTraining"),omt.forEach(t),s5r=r(l0e," (Wav2Vec2 model)"),l0e.forEach(t),_e.forEach(t),l5r=i(ya),h2e=n(ya,"P",{});var rmt=s(h2e);i5r=r(rmt,"Examples:"),rmt.forEach(t),d5r=i(ya),m(SA.$$.fragment,ya),ya.forEach(t),si.forEach(t),X8e=i(d),Jc=n(d,"H2",{class:!0});var Kke=s(Jc);J9=n(Kke,"A",{id:!0,class:!0,href:!0});var tmt=s(J9);p2e=n(tmt,"SPAN",{});var amt=s(p2e);m(PA.$$.fragment,amt),amt.forEach(t),tmt.forEach(t),c5r=i(Kke),_2e=n(Kke,"SPAN",{});var nmt=s(_2e);f5r=r(nmt,"FlaxAutoModelForMaskedLM"),nmt.forEach(t),Kke.forEach(t),z8e=i(d),Br=n(d,"DIV",{class:!0});var ii=s(Br);m($A.$$.fragment,ii),m5r=i(ii),Yc=n(ii,"P",{});var FV=s(Yc);g5r=r(FV,`This is a generic model class that will be instantiated as one of the model classes of the library (with a masked language modeling head) when created
with the `),u2e=n(FV,"CODE",{});var smt=s(u2e);h5r=r(smt,"from_pretrained()"),smt.forEach(t),p5r=r(FV,"class method or the "),b2e=n(FV,"CODE",{});var lmt=s(b2e);_5r=r(lmt,"from_config()"),lmt.forEach(t),u5r=r(FV,`class
method.`),FV.forEach(t),b5r=i(ii),IA=n(ii,"P",{});var Zke=s(IA);v5r=r(Zke,"This class cannot be instantiated directly using "),v2e=n(Zke,"CODE",{});var imt=s(v2e);T5r=r(imt,"__init__()"),imt.forEach(t),F5r=r(Zke," (throws an error)."),Zke.forEach(t),C5r=i(ii),Ct=n(ii,"DIV",{class:!0});var di=s(Ct);m(jA.$$.fragment,di),M5r=i(di),T2e=n(di,"P",{});var dmt=s(T2e);E5r=r(dmt,"Instantiates one of the model classes of the library (with a masked language modeling head) from a configuration."),dmt.forEach(t),y5r=i(di),Kc=n(di,"P",{});var CV=s(Kc);w5r=r(CV,`Note:
Loading a model from its configuration file does `),F2e=n(CV,"STRONG",{});var cmt=s(F2e);A5r=r(cmt,"not"),cmt.forEach(t),L5r=r(CV,` load the model weights. It only affects the
model\u2019s configuration. Use `),C2e=n(CV,"CODE",{});var fmt=s(C2e);B5r=r(fmt,"from_pretrained()"),fmt.forEach(t),k5r=r(CV,"to load the model weights."),CV.forEach(t),x5r=i(di),M2e=n(di,"P",{});var mmt=s(M2e);R5r=r(mmt,"Examples:"),mmt.forEach(t),S5r=i(di),m(NA.$$.fragment,di),di.forEach(t),P5r=i(ii),Bo=n(ii,"DIV",{class:!0});var wa=s(Bo);m(DA.$$.fragment,wa),$5r=i(wa),E2e=n(wa,"P",{});var gmt=s(E2e);I5r=r(gmt,"Instantiate one of the model classes of the library (with a masked language modeling head) from a pretrained model."),gmt.forEach(t),j5r=i(wa),yn=n(wa,"P",{});var Q4=s(yn);N5r=r(Q4,"The model class to instantiate is selected based on the "),y2e=n(Q4,"CODE",{});var hmt=s(y2e);D5r=r(hmt,"model_type"),hmt.forEach(t),q5r=r(Q4,` property of the config object (either
passed as an argument or loaded from `),w2e=n(Q4,"CODE",{});var pmt=s(w2e);G5r=r(pmt,"pretrained_model_name_or_path"),pmt.forEach(t),O5r=r(Q4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),A2e=n(Q4,"CODE",{});var _mt=s(A2e);X5r=r(_mt,"pretrained_model_name_or_path"),_mt.forEach(t),z5r=r(Q4,":"),Q4.forEach(t),V5r=i(wa),ve=n(wa,"UL",{});var Ze=s(ve);Y9=n(Ze,"LI",{});var i0e=s(Y9);L2e=n(i0e,"STRONG",{});var umt=s(L2e);W5r=r(umt,"albert"),umt.forEach(t),Q5r=r(i0e," \u2014 "),QO=n(i0e,"A",{href:!0});var bmt=s(QO);H5r=r(bmt,"FlaxAlbertForMaskedLM"),bmt.forEach(t),U5r=r(i0e," (ALBERT model)"),i0e.forEach(t),J5r=i(Ze),K9=n(Ze,"LI",{});var d0e=s(K9);B2e=n(d0e,"STRONG",{});var vmt=s(B2e);Y5r=r(vmt,"bart"),vmt.forEach(t),K5r=r(d0e," \u2014 "),HO=n(d0e,"A",{href:!0});var Tmt=s(HO);Z5r=r(Tmt,"FlaxBartForConditionalGeneration"),Tmt.forEach(t),evr=r(d0e," (BART model)"),d0e.forEach(t),ovr=i(Ze),Z9=n(Ze,"LI",{});var c0e=s(Z9);k2e=n(c0e,"STRONG",{});var Fmt=s(k2e);rvr=r(Fmt,"bert"),Fmt.forEach(t),tvr=r(c0e," \u2014 "),UO=n(c0e,"A",{href:!0});var Cmt=s(UO);avr=r(Cmt,"FlaxBertForMaskedLM"),Cmt.forEach(t),nvr=r(c0e," (BERT model)"),c0e.forEach(t),svr=i(Ze),eC=n(Ze,"LI",{});var f0e=s(eC);x2e=n(f0e,"STRONG",{});var Mmt=s(x2e);lvr=r(Mmt,"big_bird"),Mmt.forEach(t),ivr=r(f0e," \u2014 "),JO=n(f0e,"A",{href:!0});var Emt=s(JO);dvr=r(Emt,"FlaxBigBirdForMaskedLM"),Emt.forEach(t),cvr=r(f0e," (BigBird model)"),f0e.forEach(t),fvr=i(Ze),oC=n(Ze,"LI",{});var m0e=s(oC);R2e=n(m0e,"STRONG",{});var ymt=s(R2e);mvr=r(ymt,"distilbert"),ymt.forEach(t),gvr=r(m0e," \u2014 "),YO=n(m0e,"A",{href:!0});var wmt=s(YO);hvr=r(wmt,"FlaxDistilBertForMaskedLM"),wmt.forEach(t),pvr=r(m0e," (DistilBERT model)"),m0e.forEach(t),_vr=i(Ze),rC=n(Ze,"LI",{});var g0e=s(rC);S2e=n(g0e,"STRONG",{});var Amt=s(S2e);uvr=r(Amt,"electra"),Amt.forEach(t),bvr=r(g0e," \u2014 "),KO=n(g0e,"A",{href:!0});var Lmt=s(KO);vvr=r(Lmt,"FlaxElectraForMaskedLM"),Lmt.forEach(t),Tvr=r(g0e," (ELECTRA model)"),g0e.forEach(t),Fvr=i(Ze),tC=n(Ze,"LI",{});var h0e=s(tC);P2e=n(h0e,"STRONG",{});var Bmt=s(P2e);Cvr=r(Bmt,"mbart"),Bmt.forEach(t),Mvr=r(h0e," \u2014 "),ZO=n(h0e,"A",{href:!0});var kmt=s(ZO);Evr=r(kmt,"FlaxMBartForConditionalGeneration"),kmt.forEach(t),yvr=r(h0e," (mBART model)"),h0e.forEach(t),wvr=i(Ze),aC=n(Ze,"LI",{});var p0e=s(aC);$2e=n(p0e,"STRONG",{});var xmt=s($2e);Avr=r(xmt,"roberta"),xmt.forEach(t),Lvr=r(p0e," \u2014 "),eX=n(p0e,"A",{href:!0});var Rmt=s(eX);Bvr=r(Rmt,"FlaxRobertaForMaskedLM"),Rmt.forEach(t),kvr=r(p0e," (RoBERTa model)"),p0e.forEach(t),xvr=i(Ze),nC=n(Ze,"LI",{});var _0e=s(nC);I2e=n(_0e,"STRONG",{});var Smt=s(I2e);Rvr=r(Smt,"roformer"),Smt.forEach(t),Svr=r(_0e," \u2014 "),oX=n(_0e,"A",{href:!0});var Pmt=s(oX);Pvr=r(Pmt,"FlaxRoFormerForMaskedLM"),Pmt.forEach(t),$vr=r(_0e," (RoFormer model)"),_0e.forEach(t),Ze.forEach(t),Ivr=i(wa),j2e=n(wa,"P",{});var $mt=s(j2e);jvr=r($mt,"Examples:"),$mt.forEach(t),Nvr=i(wa),m(qA.$$.fragment,wa),wa.forEach(t),ii.forEach(t),V8e=i(d),Zc=n(d,"H2",{class:!0});var exe=s(Zc);sC=n(exe,"A",{id:!0,class:!0,href:!0});var Imt=s(sC);N2e=n(Imt,"SPAN",{});var jmt=s(N2e);m(GA.$$.fragment,jmt),jmt.forEach(t),Imt.forEach(t),Dvr=i(exe),D2e=n(exe,"SPAN",{});var Nmt=s(D2e);qvr=r(Nmt,"FlaxAutoModelForSeq2SeqLM"),Nmt.forEach(t),exe.forEach(t),W8e=i(d),kr=n(d,"DIV",{class:!0});var ci=s(kr);m(OA.$$.fragment,ci),Gvr=i(ci),ef=n(ci,"P",{});var MV=s(ef);Ovr=r(MV,`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence language modeling head) when created
with the `),q2e=n(MV,"CODE",{});var Dmt=s(q2e);Xvr=r(Dmt,"from_pretrained()"),Dmt.forEach(t),zvr=r(MV,"class method or the "),G2e=n(MV,"CODE",{});var qmt=s(G2e);Vvr=r(qmt,"from_config()"),qmt.forEach(t),Wvr=r(MV,`class
method.`),MV.forEach(t),Qvr=i(ci),XA=n(ci,"P",{});var oxe=s(XA);Hvr=r(oxe,"This class cannot be instantiated directly using "),O2e=n(oxe,"CODE",{});var Gmt=s(O2e);Uvr=r(Gmt,"__init__()"),Gmt.forEach(t),Jvr=r(oxe," (throws an error)."),oxe.forEach(t),Yvr=i(ci),Mt=n(ci,"DIV",{class:!0});var fi=s(Mt);m(zA.$$.fragment,fi),Kvr=i(fi),X2e=n(fi,"P",{});var Omt=s(X2e);Zvr=r(Omt,"Instantiates one of the model classes of the library (with a sequence-to-sequence language modeling head) from a configuration."),Omt.forEach(t),eTr=i(fi),of=n(fi,"P",{});var EV=s(of);oTr=r(EV,`Note:
Loading a model from its configuration file does `),z2e=n(EV,"STRONG",{});var Xmt=s(z2e);rTr=r(Xmt,"not"),Xmt.forEach(t),tTr=r(EV,` load the model weights. It only affects the
model\u2019s configuration. Use `),V2e=n(EV,"CODE",{});var zmt=s(V2e);aTr=r(zmt,"from_pretrained()"),zmt.forEach(t),nTr=r(EV,"to load the model weights."),EV.forEach(t),sTr=i(fi),W2e=n(fi,"P",{});var Vmt=s(W2e);lTr=r(Vmt,"Examples:"),Vmt.forEach(t),iTr=i(fi),m(VA.$$.fragment,fi),fi.forEach(t),dTr=i(ci),ko=n(ci,"DIV",{class:!0});var Aa=s(ko);m(WA.$$.fragment,Aa),cTr=i(Aa),Q2e=n(Aa,"P",{});var Wmt=s(Q2e);fTr=r(Wmt,"Instantiate one of the model classes of the library (with a sequence-to-sequence language modeling head) from a pretrained model."),Wmt.forEach(t),mTr=i(Aa),wn=n(Aa,"P",{});var H4=s(wn);gTr=r(H4,"The model class to instantiate is selected based on the "),H2e=n(H4,"CODE",{});var Qmt=s(H2e);hTr=r(Qmt,"model_type"),Qmt.forEach(t),pTr=r(H4,` property of the config object (either
passed as an argument or loaded from `),U2e=n(H4,"CODE",{});var Hmt=s(U2e);_Tr=r(Hmt,"pretrained_model_name_or_path"),Hmt.forEach(t),uTr=r(H4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),J2e=n(H4,"CODE",{});var Umt=s(J2e);bTr=r(Umt,"pretrained_model_name_or_path"),Umt.forEach(t),vTr=r(H4,":"),H4.forEach(t),TTr=i(Aa),Te=n(Aa,"UL",{});var eo=s(Te);lC=n(eo,"LI",{});var u0e=s(lC);Y2e=n(u0e,"STRONG",{});var Jmt=s(Y2e);FTr=r(Jmt,"bart"),Jmt.forEach(t),CTr=r(u0e," \u2014 "),rX=n(u0e,"A",{href:!0});var Ymt=s(rX);MTr=r(Ymt,"FlaxBartForConditionalGeneration"),Ymt.forEach(t),ETr=r(u0e," (BART model)"),u0e.forEach(t),yTr=i(eo),iC=n(eo,"LI",{});var b0e=s(iC);K2e=n(b0e,"STRONG",{});var Kmt=s(K2e);wTr=r(Kmt,"blenderbot"),Kmt.forEach(t),ATr=r(b0e," \u2014 "),tX=n(b0e,"A",{href:!0});var Zmt=s(tX);LTr=r(Zmt,"FlaxBlenderbotForConditionalGeneration"),Zmt.forEach(t),BTr=r(b0e," (Blenderbot model)"),b0e.forEach(t),kTr=i(eo),dC=n(eo,"LI",{});var v0e=s(dC);Z2e=n(v0e,"STRONG",{});var egt=s(Z2e);xTr=r(egt,"blenderbot-small"),egt.forEach(t),RTr=r(v0e," \u2014 "),aX=n(v0e,"A",{href:!0});var ogt=s(aX);STr=r(ogt,"FlaxBlenderbotSmallForConditionalGeneration"),ogt.forEach(t),PTr=r(v0e," (BlenderbotSmall model)"),v0e.forEach(t),$Tr=i(eo),cC=n(eo,"LI",{});var T0e=s(cC);e1e=n(T0e,"STRONG",{});var rgt=s(e1e);ITr=r(rgt,"encoder-decoder"),rgt.forEach(t),jTr=r(T0e," \u2014 "),nX=n(T0e,"A",{href:!0});var tgt=s(nX);NTr=r(tgt,"FlaxEncoderDecoderModel"),tgt.forEach(t),DTr=r(T0e," (Encoder decoder model)"),T0e.forEach(t),qTr=i(eo),fC=n(eo,"LI",{});var F0e=s(fC);o1e=n(F0e,"STRONG",{});var agt=s(o1e);GTr=r(agt,"marian"),agt.forEach(t),OTr=r(F0e," \u2014 "),sX=n(F0e,"A",{href:!0});var ngt=s(sX);XTr=r(ngt,"FlaxMarianMTModel"),ngt.forEach(t),zTr=r(F0e," (Marian model)"),F0e.forEach(t),VTr=i(eo),mC=n(eo,"LI",{});var C0e=s(mC);r1e=n(C0e,"STRONG",{});var sgt=s(r1e);WTr=r(sgt,"mbart"),sgt.forEach(t),QTr=r(C0e," \u2014 "),lX=n(C0e,"A",{href:!0});var lgt=s(lX);HTr=r(lgt,"FlaxMBartForConditionalGeneration"),lgt.forEach(t),UTr=r(C0e," (mBART model)"),C0e.forEach(t),JTr=i(eo),gC=n(eo,"LI",{});var M0e=s(gC);t1e=n(M0e,"STRONG",{});var igt=s(t1e);YTr=r(igt,"mt5"),igt.forEach(t),KTr=r(M0e," \u2014 "),iX=n(M0e,"A",{href:!0});var dgt=s(iX);ZTr=r(dgt,"FlaxMT5ForConditionalGeneration"),dgt.forEach(t),e7r=r(M0e," (mT5 model)"),M0e.forEach(t),o7r=i(eo),hC=n(eo,"LI",{});var E0e=s(hC);a1e=n(E0e,"STRONG",{});var cgt=s(a1e);r7r=r(cgt,"pegasus"),cgt.forEach(t),t7r=r(E0e," \u2014 "),dX=n(E0e,"A",{href:!0});var fgt=s(dX);a7r=r(fgt,"FlaxPegasusForConditionalGeneration"),fgt.forEach(t),n7r=r(E0e," (Pegasus model)"),E0e.forEach(t),s7r=i(eo),pC=n(eo,"LI",{});var y0e=s(pC);n1e=n(y0e,"STRONG",{});var mgt=s(n1e);l7r=r(mgt,"t5"),mgt.forEach(t),i7r=r(y0e," \u2014 "),cX=n(y0e,"A",{href:!0});var ggt=s(cX);d7r=r(ggt,"FlaxT5ForConditionalGeneration"),ggt.forEach(t),c7r=r(y0e," (T5 model)"),y0e.forEach(t),eo.forEach(t),f7r=i(Aa),s1e=n(Aa,"P",{});var hgt=s(s1e);m7r=r(hgt,"Examples:"),hgt.forEach(t),g7r=i(Aa),m(QA.$$.fragment,Aa),Aa.forEach(t),ci.forEach(t),Q8e=i(d),rf=n(d,"H2",{class:!0});var rxe=s(rf);_C=n(rxe,"A",{id:!0,class:!0,href:!0});var pgt=s(_C);l1e=n(pgt,"SPAN",{});var _gt=s(l1e);m(HA.$$.fragment,_gt),_gt.forEach(t),pgt.forEach(t),h7r=i(rxe),i1e=n(rxe,"SPAN",{});var ugt=s(i1e);p7r=r(ugt,"FlaxAutoModelForSequenceClassification"),ugt.forEach(t),rxe.forEach(t),H8e=i(d),xr=n(d,"DIV",{class:!0});var mi=s(xr);m(UA.$$.fragment,mi),_7r=i(mi),tf=n(mi,"P",{});var yV=s(tf);u7r=r(yV,`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence classification head) when created
with the `),d1e=n(yV,"CODE",{});var bgt=s(d1e);b7r=r(bgt,"from_pretrained()"),bgt.forEach(t),v7r=r(yV,"class method or the "),c1e=n(yV,"CODE",{});var vgt=s(c1e);T7r=r(vgt,"from_config()"),vgt.forEach(t),F7r=r(yV,`class
method.`),yV.forEach(t),C7r=i(mi),JA=n(mi,"P",{});var txe=s(JA);M7r=r(txe,"This class cannot be instantiated directly using "),f1e=n(txe,"CODE",{});var Tgt=s(f1e);E7r=r(Tgt,"__init__()"),Tgt.forEach(t),y7r=r(txe," (throws an error)."),txe.forEach(t),w7r=i(mi),Et=n(mi,"DIV",{class:!0});var gi=s(Et);m(YA.$$.fragment,gi),A7r=i(gi),m1e=n(gi,"P",{});var Fgt=s(m1e);L7r=r(Fgt,"Instantiates one of the model classes of the library (with a sequence classification head) from a configuration."),Fgt.forEach(t),B7r=i(gi),af=n(gi,"P",{});var wV=s(af);k7r=r(wV,`Note:
Loading a model from its configuration file does `),g1e=n(wV,"STRONG",{});var Cgt=s(g1e);x7r=r(Cgt,"not"),Cgt.forEach(t),R7r=r(wV,` load the model weights. It only affects the
model\u2019s configuration. Use `),h1e=n(wV,"CODE",{});var Mgt=s(h1e);S7r=r(Mgt,"from_pretrained()"),Mgt.forEach(t),P7r=r(wV,"to load the model weights."),wV.forEach(t),$7r=i(gi),p1e=n(gi,"P",{});var Egt=s(p1e);I7r=r(Egt,"Examples:"),Egt.forEach(t),j7r=i(gi),m(KA.$$.fragment,gi),gi.forEach(t),N7r=i(mi),xo=n(mi,"DIV",{class:!0});var La=s(xo);m(ZA.$$.fragment,La),D7r=i(La),_1e=n(La,"P",{});var ygt=s(_1e);q7r=r(ygt,"Instantiate one of the model classes of the library (with a sequence classification head) from a pretrained model."),ygt.forEach(t),G7r=i(La),An=n(La,"P",{});var U4=s(An);O7r=r(U4,"The model class to instantiate is selected based on the "),u1e=n(U4,"CODE",{});var wgt=s(u1e);X7r=r(wgt,"model_type"),wgt.forEach(t),z7r=r(U4,` property of the config object (either
passed as an argument or loaded from `),b1e=n(U4,"CODE",{});var Agt=s(b1e);V7r=r(Agt,"pretrained_model_name_or_path"),Agt.forEach(t),W7r=r(U4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),v1e=n(U4,"CODE",{});var Lgt=s(v1e);Q7r=r(Lgt,"pretrained_model_name_or_path"),Lgt.forEach(t),H7r=r(U4,":"),U4.forEach(t),U7r=i(La),Fe=n(La,"UL",{});var oo=s(Fe);uC=n(oo,"LI",{});var w0e=s(uC);T1e=n(w0e,"STRONG",{});var Bgt=s(T1e);J7r=r(Bgt,"albert"),Bgt.forEach(t),Y7r=r(w0e," \u2014 "),fX=n(w0e,"A",{href:!0});var kgt=s(fX);K7r=r(kgt,"FlaxAlbertForSequenceClassification"),kgt.forEach(t),Z7r=r(w0e," (ALBERT model)"),w0e.forEach(t),eFr=i(oo),bC=n(oo,"LI",{});var A0e=s(bC);F1e=n(A0e,"STRONG",{});var xgt=s(F1e);oFr=r(xgt,"bart"),xgt.forEach(t),rFr=r(A0e," \u2014 "),mX=n(A0e,"A",{href:!0});var Rgt=s(mX);tFr=r(Rgt,"FlaxBartForSequenceClassification"),Rgt.forEach(t),aFr=r(A0e," (BART model)"),A0e.forEach(t),nFr=i(oo),vC=n(oo,"LI",{});var L0e=s(vC);C1e=n(L0e,"STRONG",{});var Sgt=s(C1e);sFr=r(Sgt,"bert"),Sgt.forEach(t),lFr=r(L0e," \u2014 "),gX=n(L0e,"A",{href:!0});var Pgt=s(gX);iFr=r(Pgt,"FlaxBertForSequenceClassification"),Pgt.forEach(t),dFr=r(L0e," (BERT model)"),L0e.forEach(t),cFr=i(oo),TC=n(oo,"LI",{});var B0e=s(TC);M1e=n(B0e,"STRONG",{});var $gt=s(M1e);fFr=r($gt,"big_bird"),$gt.forEach(t),mFr=r(B0e," \u2014 "),hX=n(B0e,"A",{href:!0});var Igt=s(hX);gFr=r(Igt,"FlaxBigBirdForSequenceClassification"),Igt.forEach(t),hFr=r(B0e," (BigBird model)"),B0e.forEach(t),pFr=i(oo),FC=n(oo,"LI",{});var k0e=s(FC);E1e=n(k0e,"STRONG",{});var jgt=s(E1e);_Fr=r(jgt,"distilbert"),jgt.forEach(t),uFr=r(k0e," \u2014 "),pX=n(k0e,"A",{href:!0});var Ngt=s(pX);bFr=r(Ngt,"FlaxDistilBertForSequenceClassification"),Ngt.forEach(t),vFr=r(k0e," (DistilBERT model)"),k0e.forEach(t),TFr=i(oo),CC=n(oo,"LI",{});var x0e=s(CC);y1e=n(x0e,"STRONG",{});var Dgt=s(y1e);FFr=r(Dgt,"electra"),Dgt.forEach(t),CFr=r(x0e," \u2014 "),_X=n(x0e,"A",{href:!0});var qgt=s(_X);MFr=r(qgt,"FlaxElectraForSequenceClassification"),qgt.forEach(t),EFr=r(x0e," (ELECTRA model)"),x0e.forEach(t),yFr=i(oo),MC=n(oo,"LI",{});var R0e=s(MC);w1e=n(R0e,"STRONG",{});var Ggt=s(w1e);wFr=r(Ggt,"mbart"),Ggt.forEach(t),AFr=r(R0e," \u2014 "),uX=n(R0e,"A",{href:!0});var Ogt=s(uX);LFr=r(Ogt,"FlaxMBartForSequenceClassification"),Ogt.forEach(t),BFr=r(R0e," (mBART model)"),R0e.forEach(t),kFr=i(oo),EC=n(oo,"LI",{});var S0e=s(EC);A1e=n(S0e,"STRONG",{});var Xgt=s(A1e);xFr=r(Xgt,"roberta"),Xgt.forEach(t),RFr=r(S0e," \u2014 "),bX=n(S0e,"A",{href:!0});var zgt=s(bX);SFr=r(zgt,"FlaxRobertaForSequenceClassification"),zgt.forEach(t),PFr=r(S0e," (RoBERTa model)"),S0e.forEach(t),$Fr=i(oo),yC=n(oo,"LI",{});var P0e=s(yC);L1e=n(P0e,"STRONG",{});var Vgt=s(L1e);IFr=r(Vgt,"roformer"),Vgt.forEach(t),jFr=r(P0e," \u2014 "),vX=n(P0e,"A",{href:!0});var Wgt=s(vX);NFr=r(Wgt,"FlaxRoFormerForSequenceClassification"),Wgt.forEach(t),DFr=r(P0e," (RoFormer model)"),P0e.forEach(t),oo.forEach(t),qFr=i(La),B1e=n(La,"P",{});var Qgt=s(B1e);GFr=r(Qgt,"Examples:"),Qgt.forEach(t),OFr=i(La),m(e6.$$.fragment,La),La.forEach(t),mi.forEach(t),U8e=i(d),nf=n(d,"H2",{class:!0});var axe=s(nf);wC=n(axe,"A",{id:!0,class:!0,href:!0});var Hgt=s(wC);k1e=n(Hgt,"SPAN",{});var Ugt=s(k1e);m(o6.$$.fragment,Ugt),Ugt.forEach(t),Hgt.forEach(t),XFr=i(axe),x1e=n(axe,"SPAN",{});var Jgt=s(x1e);zFr=r(Jgt,"FlaxAutoModelForQuestionAnswering"),Jgt.forEach(t),axe.forEach(t),J8e=i(d),Rr=n(d,"DIV",{class:!0});var hi=s(Rr);m(r6.$$.fragment,hi),VFr=i(hi),sf=n(hi,"P",{});var AV=s(sf);WFr=r(AV,`This is a generic model class that will be instantiated as one of the model classes of the library (with a question answering head) when created
with the `),R1e=n(AV,"CODE",{});var Ygt=s(R1e);QFr=r(Ygt,"from_pretrained()"),Ygt.forEach(t),HFr=r(AV,"class method or the "),S1e=n(AV,"CODE",{});var Kgt=s(S1e);UFr=r(Kgt,"from_config()"),Kgt.forEach(t),JFr=r(AV,`class
method.`),AV.forEach(t),YFr=i(hi),t6=n(hi,"P",{});var nxe=s(t6);KFr=r(nxe,"This class cannot be instantiated directly using "),P1e=n(nxe,"CODE",{});var Zgt=s(P1e);ZFr=r(Zgt,"__init__()"),Zgt.forEach(t),e9r=r(nxe," (throws an error)."),nxe.forEach(t),o9r=i(hi),yt=n(hi,"DIV",{class:!0});var pi=s(yt);m(a6.$$.fragment,pi),r9r=i(pi),$1e=n(pi,"P",{});var eht=s($1e);t9r=r(eht,"Instantiates one of the model classes of the library (with a question answering head) from a configuration."),eht.forEach(t),a9r=i(pi),lf=n(pi,"P",{});var LV=s(lf);n9r=r(LV,`Note:
Loading a model from its configuration file does `),I1e=n(LV,"STRONG",{});var oht=s(I1e);s9r=r(oht,"not"),oht.forEach(t),l9r=r(LV,` load the model weights. It only affects the
model\u2019s configuration. Use `),j1e=n(LV,"CODE",{});var rht=s(j1e);i9r=r(rht,"from_pretrained()"),rht.forEach(t),d9r=r(LV,"to load the model weights."),LV.forEach(t),c9r=i(pi),N1e=n(pi,"P",{});var tht=s(N1e);f9r=r(tht,"Examples:"),tht.forEach(t),m9r=i(pi),m(n6.$$.fragment,pi),pi.forEach(t),g9r=i(hi),Ro=n(hi,"DIV",{class:!0});var Ba=s(Ro);m(s6.$$.fragment,Ba),h9r=i(Ba),D1e=n(Ba,"P",{});var aht=s(D1e);p9r=r(aht,"Instantiate one of the model classes of the library (with a question answering head) from a pretrained model."),aht.forEach(t),_9r=i(Ba),Ln=n(Ba,"P",{});var J4=s(Ln);u9r=r(J4,"The model class to instantiate is selected based on the "),q1e=n(J4,"CODE",{});var nht=s(q1e);b9r=r(nht,"model_type"),nht.forEach(t),v9r=r(J4,` property of the config object (either
passed as an argument or loaded from `),G1e=n(J4,"CODE",{});var sht=s(G1e);T9r=r(sht,"pretrained_model_name_or_path"),sht.forEach(t),F9r=r(J4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),O1e=n(J4,"CODE",{});var lht=s(O1e);C9r=r(lht,"pretrained_model_name_or_path"),lht.forEach(t),M9r=r(J4,":"),J4.forEach(t),E9r=i(Ba),Ce=n(Ba,"UL",{});var ro=s(Ce);AC=n(ro,"LI",{});var $0e=s(AC);X1e=n($0e,"STRONG",{});var iht=s(X1e);y9r=r(iht,"albert"),iht.forEach(t),w9r=r($0e," \u2014 "),TX=n($0e,"A",{href:!0});var dht=s(TX);A9r=r(dht,"FlaxAlbertForQuestionAnswering"),dht.forEach(t),L9r=r($0e," (ALBERT model)"),$0e.forEach(t),B9r=i(ro),LC=n(ro,"LI",{});var I0e=s(LC);z1e=n(I0e,"STRONG",{});var cht=s(z1e);k9r=r(cht,"bart"),cht.forEach(t),x9r=r(I0e," \u2014 "),FX=n(I0e,"A",{href:!0});var fht=s(FX);R9r=r(fht,"FlaxBartForQuestionAnswering"),fht.forEach(t),S9r=r(I0e," (BART model)"),I0e.forEach(t),P9r=i(ro),BC=n(ro,"LI",{});var j0e=s(BC);V1e=n(j0e,"STRONG",{});var mht=s(V1e);$9r=r(mht,"bert"),mht.forEach(t),I9r=r(j0e," \u2014 "),CX=n(j0e,"A",{href:!0});var ght=s(CX);j9r=r(ght,"FlaxBertForQuestionAnswering"),ght.forEach(t),N9r=r(j0e," (BERT model)"),j0e.forEach(t),D9r=i(ro),kC=n(ro,"LI",{});var N0e=s(kC);W1e=n(N0e,"STRONG",{});var hht=s(W1e);q9r=r(hht,"big_bird"),hht.forEach(t),G9r=r(N0e," \u2014 "),MX=n(N0e,"A",{href:!0});var pht=s(MX);O9r=r(pht,"FlaxBigBirdForQuestionAnswering"),pht.forEach(t),X9r=r(N0e," (BigBird model)"),N0e.forEach(t),z9r=i(ro),xC=n(ro,"LI",{});var D0e=s(xC);Q1e=n(D0e,"STRONG",{});var _ht=s(Q1e);V9r=r(_ht,"distilbert"),_ht.forEach(t),W9r=r(D0e," \u2014 "),EX=n(D0e,"A",{href:!0});var uht=s(EX);Q9r=r(uht,"FlaxDistilBertForQuestionAnswering"),uht.forEach(t),H9r=r(D0e," (DistilBERT model)"),D0e.forEach(t),U9r=i(ro),RC=n(ro,"LI",{});var q0e=s(RC);H1e=n(q0e,"STRONG",{});var bht=s(H1e);J9r=r(bht,"electra"),bht.forEach(t),Y9r=r(q0e," \u2014 "),yX=n(q0e,"A",{href:!0});var vht=s(yX);K9r=r(vht,"FlaxElectraForQuestionAnswering"),vht.forEach(t),Z9r=r(q0e," (ELECTRA model)"),q0e.forEach(t),eCr=i(ro),SC=n(ro,"LI",{});var G0e=s(SC);U1e=n(G0e,"STRONG",{});var Tht=s(U1e);oCr=r(Tht,"mbart"),Tht.forEach(t),rCr=r(G0e," \u2014 "),wX=n(G0e,"A",{href:!0});var Fht=s(wX);tCr=r(Fht,"FlaxMBartForQuestionAnswering"),Fht.forEach(t),aCr=r(G0e," (mBART model)"),G0e.forEach(t),nCr=i(ro),PC=n(ro,"LI",{});var O0e=s(PC);J1e=n(O0e,"STRONG",{});var Cht=s(J1e);sCr=r(Cht,"roberta"),Cht.forEach(t),lCr=r(O0e," \u2014 "),AX=n(O0e,"A",{href:!0});var Mht=s(AX);iCr=r(Mht,"FlaxRobertaForQuestionAnswering"),Mht.forEach(t),dCr=r(O0e," (RoBERTa model)"),O0e.forEach(t),cCr=i(ro),$C=n(ro,"LI",{});var X0e=s($C);Y1e=n(X0e,"STRONG",{});var Eht=s(Y1e);fCr=r(Eht,"roformer"),Eht.forEach(t),mCr=r(X0e," \u2014 "),LX=n(X0e,"A",{href:!0});var yht=s(LX);gCr=r(yht,"FlaxRoFormerForQuestionAnswering"),yht.forEach(t),hCr=r(X0e," (RoFormer model)"),X0e.forEach(t),ro.forEach(t),pCr=i(Ba),K1e=n(Ba,"P",{});var wht=s(K1e);_Cr=r(wht,"Examples:"),wht.forEach(t),uCr=i(Ba),m(l6.$$.fragment,Ba),Ba.forEach(t),hi.forEach(t),Y8e=i(d),df=n(d,"H2",{class:!0});var sxe=s(df);IC=n(sxe,"A",{id:!0,class:!0,href:!0});var Aht=s(IC);Z1e=n(Aht,"SPAN",{});var Lht=s(Z1e);m(i6.$$.fragment,Lht),Lht.forEach(t),Aht.forEach(t),bCr=i(sxe),ebe=n(sxe,"SPAN",{});var Bht=s(ebe);vCr=r(Bht,"FlaxAutoModelForTokenClassification"),Bht.forEach(t),sxe.forEach(t),K8e=i(d),Sr=n(d,"DIV",{class:!0});var _i=s(Sr);m(d6.$$.fragment,_i),TCr=i(_i),cf=n(_i,"P",{});var BV=s(cf);FCr=r(BV,`This is a generic model class that will be instantiated as one of the model classes of the library (with a token classification head) when created
with the `),obe=n(BV,"CODE",{});var kht=s(obe);CCr=r(kht,"from_pretrained()"),kht.forEach(t),MCr=r(BV,"class method or the "),rbe=n(BV,"CODE",{});var xht=s(rbe);ECr=r(xht,"from_config()"),xht.forEach(t),yCr=r(BV,`class
method.`),BV.forEach(t),wCr=i(_i),c6=n(_i,"P",{});var lxe=s(c6);ACr=r(lxe,"This class cannot be instantiated directly using "),tbe=n(lxe,"CODE",{});var Rht=s(tbe);LCr=r(Rht,"__init__()"),Rht.forEach(t),BCr=r(lxe," (throws an error)."),lxe.forEach(t),kCr=i(_i),wt=n(_i,"DIV",{class:!0});var ui=s(wt);m(f6.$$.fragment,ui),xCr=i(ui),abe=n(ui,"P",{});var Sht=s(abe);RCr=r(Sht,"Instantiates one of the model classes of the library (with a token classification head) from a configuration."),Sht.forEach(t),SCr=i(ui),ff=n(ui,"P",{});var kV=s(ff);PCr=r(kV,`Note:
Loading a model from its configuration file does `),nbe=n(kV,"STRONG",{});var Pht=s(nbe);$Cr=r(Pht,"not"),Pht.forEach(t),ICr=r(kV,` load the model weights. It only affects the
model\u2019s configuration. Use `),sbe=n(kV,"CODE",{});var $ht=s(sbe);jCr=r($ht,"from_pretrained()"),$ht.forEach(t),NCr=r(kV,"to load the model weights."),kV.forEach(t),DCr=i(ui),lbe=n(ui,"P",{});var Iht=s(lbe);qCr=r(Iht,"Examples:"),Iht.forEach(t),GCr=i(ui),m(m6.$$.fragment,ui),ui.forEach(t),OCr=i(_i),So=n(_i,"DIV",{class:!0});var ka=s(So);m(g6.$$.fragment,ka),XCr=i(ka),ibe=n(ka,"P",{});var jht=s(ibe);zCr=r(jht,"Instantiate one of the model classes of the library (with a token classification head) from a pretrained model."),jht.forEach(t),VCr=i(ka),Bn=n(ka,"P",{});var Y4=s(Bn);WCr=r(Y4,"The model class to instantiate is selected based on the "),dbe=n(Y4,"CODE",{});var Nht=s(dbe);QCr=r(Nht,"model_type"),Nht.forEach(t),HCr=r(Y4,` property of the config object (either
passed as an argument or loaded from `),cbe=n(Y4,"CODE",{});var Dht=s(cbe);UCr=r(Dht,"pretrained_model_name_or_path"),Dht.forEach(t),JCr=r(Y4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),fbe=n(Y4,"CODE",{});var qht=s(fbe);YCr=r(qht,"pretrained_model_name_or_path"),qht.forEach(t),KCr=r(Y4,":"),Y4.forEach(t),ZCr=i(ka),so=n(ka,"UL",{});var ta=s(so);jC=n(ta,"LI",{});var z0e=s(jC);mbe=n(z0e,"STRONG",{});var Ght=s(mbe);e4r=r(Ght,"albert"),Ght.forEach(t),o4r=r(z0e," \u2014 "),BX=n(z0e,"A",{href:!0});var Oht=s(BX);r4r=r(Oht,"FlaxAlbertForTokenClassification"),Oht.forEach(t),t4r=r(z0e," (ALBERT model)"),z0e.forEach(t),a4r=i(ta),NC=n(ta,"LI",{});var V0e=s(NC);gbe=n(V0e,"STRONG",{});var Xht=s(gbe);n4r=r(Xht,"bert"),Xht.forEach(t),s4r=r(V0e," \u2014 "),kX=n(V0e,"A",{href:!0});var zht=s(kX);l4r=r(zht,"FlaxBertForTokenClassification"),zht.forEach(t),i4r=r(V0e," (BERT model)"),V0e.forEach(t),d4r=i(ta),DC=n(ta,"LI",{});var W0e=s(DC);hbe=n(W0e,"STRONG",{});var Vht=s(hbe);c4r=r(Vht,"big_bird"),Vht.forEach(t),f4r=r(W0e," \u2014 "),xX=n(W0e,"A",{href:!0});var Wht=s(xX);m4r=r(Wht,"FlaxBigBirdForTokenClassification"),Wht.forEach(t),g4r=r(W0e," (BigBird model)"),W0e.forEach(t),h4r=i(ta),qC=n(ta,"LI",{});var Q0e=s(qC);pbe=n(Q0e,"STRONG",{});var Qht=s(pbe);p4r=r(Qht,"distilbert"),Qht.forEach(t),_4r=r(Q0e," \u2014 "),RX=n(Q0e,"A",{href:!0});var Hht=s(RX);u4r=r(Hht,"FlaxDistilBertForTokenClassification"),Hht.forEach(t),b4r=r(Q0e," (DistilBERT model)"),Q0e.forEach(t),v4r=i(ta),GC=n(ta,"LI",{});var H0e=s(GC);_be=n(H0e,"STRONG",{});var Uht=s(_be);T4r=r(Uht,"electra"),Uht.forEach(t),F4r=r(H0e," \u2014 "),SX=n(H0e,"A",{href:!0});var Jht=s(SX);C4r=r(Jht,"FlaxElectraForTokenClassification"),Jht.forEach(t),M4r=r(H0e," (ELECTRA model)"),H0e.forEach(t),E4r=i(ta),OC=n(ta,"LI",{});var U0e=s(OC);ube=n(U0e,"STRONG",{});var Yht=s(ube);y4r=r(Yht,"roberta"),Yht.forEach(t),w4r=r(U0e," \u2014 "),PX=n(U0e,"A",{href:!0});var Kht=s(PX);A4r=r(Kht,"FlaxRobertaForTokenClassification"),Kht.forEach(t),L4r=r(U0e," (RoBERTa model)"),U0e.forEach(t),B4r=i(ta),XC=n(ta,"LI",{});var J0e=s(XC);bbe=n(J0e,"STRONG",{});var Zht=s(bbe);k4r=r(Zht,"roformer"),Zht.forEach(t),x4r=r(J0e," \u2014 "),$X=n(J0e,"A",{href:!0});var ept=s($X);R4r=r(ept,"FlaxRoFormerForTokenClassification"),ept.forEach(t),S4r=r(J0e," (RoFormer model)"),J0e.forEach(t),ta.forEach(t),P4r=i(ka),vbe=n(ka,"P",{});var opt=s(vbe);$4r=r(opt,"Examples:"),opt.forEach(t),I4r=i(ka),m(h6.$$.fragment,ka),ka.forEach(t),_i.forEach(t),Z8e=i(d),mf=n(d,"H2",{class:!0});var ixe=s(mf);zC=n(ixe,"A",{id:!0,class:!0,href:!0});var rpt=s(zC);Tbe=n(rpt,"SPAN",{});var tpt=s(Tbe);m(p6.$$.fragment,tpt),tpt.forEach(t),rpt.forEach(t),j4r=i(ixe),Fbe=n(ixe,"SPAN",{});var apt=s(Fbe);N4r=r(apt,"FlaxAutoModelForMultipleChoice"),apt.forEach(t),ixe.forEach(t),eBe=i(d),Pr=n(d,"DIV",{class:!0});var bi=s(Pr);m(_6.$$.fragment,bi),D4r=i(bi),gf=n(bi,"P",{});var xV=s(gf);q4r=r(xV,`This is a generic model class that will be instantiated as one of the model classes of the library (with a multiple choice head) when created
with the `),Cbe=n(xV,"CODE",{});var npt=s(Cbe);G4r=r(npt,"from_pretrained()"),npt.forEach(t),O4r=r(xV,"class method or the "),Mbe=n(xV,"CODE",{});var spt=s(Mbe);X4r=r(spt,"from_config()"),spt.forEach(t),z4r=r(xV,`class
method.`),xV.forEach(t),V4r=i(bi),u6=n(bi,"P",{});var dxe=s(u6);W4r=r(dxe,"This class cannot be instantiated directly using "),Ebe=n(dxe,"CODE",{});var lpt=s(Ebe);Q4r=r(lpt,"__init__()"),lpt.forEach(t),H4r=r(dxe," (throws an error)."),dxe.forEach(t),U4r=i(bi),At=n(bi,"DIV",{class:!0});var vi=s(At);m(b6.$$.fragment,vi),J4r=i(vi),ybe=n(vi,"P",{});var ipt=s(ybe);Y4r=r(ipt,"Instantiates one of the model classes of the library (with a multiple choice head) from a configuration."),ipt.forEach(t),K4r=i(vi),hf=n(vi,"P",{});var RV=s(hf);Z4r=r(RV,`Note:
Loading a model from its configuration file does `),wbe=n(RV,"STRONG",{});var dpt=s(wbe);eMr=r(dpt,"not"),dpt.forEach(t),oMr=r(RV,` load the model weights. It only affects the
model\u2019s configuration. Use `),Abe=n(RV,"CODE",{});var cpt=s(Abe);rMr=r(cpt,"from_pretrained()"),cpt.forEach(t),tMr=r(RV,"to load the model weights."),RV.forEach(t),aMr=i(vi),Lbe=n(vi,"P",{});var fpt=s(Lbe);nMr=r(fpt,"Examples:"),fpt.forEach(t),sMr=i(vi),m(v6.$$.fragment,vi),vi.forEach(t),lMr=i(bi),Po=n(bi,"DIV",{class:!0});var xa=s(Po);m(T6.$$.fragment,xa),iMr=i(xa),Bbe=n(xa,"P",{});var mpt=s(Bbe);dMr=r(mpt,"Instantiate one of the model classes of the library (with a multiple choice head) from a pretrained model."),mpt.forEach(t),cMr=i(xa),kn=n(xa,"P",{});var K4=s(kn);fMr=r(K4,"The model class to instantiate is selected based on the "),kbe=n(K4,"CODE",{});var gpt=s(kbe);mMr=r(gpt,"model_type"),gpt.forEach(t),gMr=r(K4,` property of the config object (either
passed as an argument or loaded from `),xbe=n(K4,"CODE",{});var hpt=s(xbe);hMr=r(hpt,"pretrained_model_name_or_path"),hpt.forEach(t),pMr=r(K4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Rbe=n(K4,"CODE",{});var ppt=s(Rbe);_Mr=r(ppt,"pretrained_model_name_or_path"),ppt.forEach(t),uMr=r(K4,":"),K4.forEach(t),bMr=i(xa),lo=n(xa,"UL",{});var aa=s(lo);VC=n(aa,"LI",{});var Y0e=s(VC);Sbe=n(Y0e,"STRONG",{});var _pt=s(Sbe);vMr=r(_pt,"albert"),_pt.forEach(t),TMr=r(Y0e," \u2014 "),IX=n(Y0e,"A",{href:!0});var upt=s(IX);FMr=r(upt,"FlaxAlbertForMultipleChoice"),upt.forEach(t),CMr=r(Y0e," (ALBERT model)"),Y0e.forEach(t),MMr=i(aa),WC=n(aa,"LI",{});var K0e=s(WC);Pbe=n(K0e,"STRONG",{});var bpt=s(Pbe);EMr=r(bpt,"bert"),bpt.forEach(t),yMr=r(K0e," \u2014 "),jX=n(K0e,"A",{href:!0});var vpt=s(jX);wMr=r(vpt,"FlaxBertForMultipleChoice"),vpt.forEach(t),AMr=r(K0e," (BERT model)"),K0e.forEach(t),LMr=i(aa),QC=n(aa,"LI",{});var Z0e=s(QC);$be=n(Z0e,"STRONG",{});var Tpt=s($be);BMr=r(Tpt,"big_bird"),Tpt.forEach(t),kMr=r(Z0e," \u2014 "),NX=n(Z0e,"A",{href:!0});var Fpt=s(NX);xMr=r(Fpt,"FlaxBigBirdForMultipleChoice"),Fpt.forEach(t),RMr=r(Z0e," (BigBird model)"),Z0e.forEach(t),SMr=i(aa),HC=n(aa,"LI",{});var eLe=s(HC);Ibe=n(eLe,"STRONG",{});var Cpt=s(Ibe);PMr=r(Cpt,"distilbert"),Cpt.forEach(t),$Mr=r(eLe," \u2014 "),DX=n(eLe,"A",{href:!0});var Mpt=s(DX);IMr=r(Mpt,"FlaxDistilBertForMultipleChoice"),Mpt.forEach(t),jMr=r(eLe," (DistilBERT model)"),eLe.forEach(t),NMr=i(aa),UC=n(aa,"LI",{});var oLe=s(UC);jbe=n(oLe,"STRONG",{});var Ept=s(jbe);DMr=r(Ept,"electra"),Ept.forEach(t),qMr=r(oLe," \u2014 "),qX=n(oLe,"A",{href:!0});var ypt=s(qX);GMr=r(ypt,"FlaxElectraForMultipleChoice"),ypt.forEach(t),OMr=r(oLe," (ELECTRA model)"),oLe.forEach(t),XMr=i(aa),JC=n(aa,"LI",{});var rLe=s(JC);Nbe=n(rLe,"STRONG",{});var wpt=s(Nbe);zMr=r(wpt,"roberta"),wpt.forEach(t),VMr=r(rLe," \u2014 "),GX=n(rLe,"A",{href:!0});var Apt=s(GX);WMr=r(Apt,"FlaxRobertaForMultipleChoice"),Apt.forEach(t),QMr=r(rLe," (RoBERTa model)"),rLe.forEach(t),HMr=i(aa),YC=n(aa,"LI",{});var tLe=s(YC);Dbe=n(tLe,"STRONG",{});var Lpt=s(Dbe);UMr=r(Lpt,"roformer"),Lpt.forEach(t),JMr=r(tLe," \u2014 "),OX=n(tLe,"A",{href:!0});var Bpt=s(OX);YMr=r(Bpt,"FlaxRoFormerForMultipleChoice"),Bpt.forEach(t),KMr=r(tLe," (RoFormer model)"),tLe.forEach(t),aa.forEach(t),ZMr=i(xa),qbe=n(xa,"P",{});var kpt=s(qbe);eEr=r(kpt,"Examples:"),kpt.forEach(t),oEr=i(xa),m(F6.$$.fragment,xa),xa.forEach(t),bi.forEach(t),oBe=i(d),pf=n(d,"H2",{class:!0});var cxe=s(pf);KC=n(cxe,"A",{id:!0,class:!0,href:!0});var xpt=s(KC);Gbe=n(xpt,"SPAN",{});var Rpt=s(Gbe);m(C6.$$.fragment,Rpt),Rpt.forEach(t),xpt.forEach(t),rEr=i(cxe),Obe=n(cxe,"SPAN",{});var Spt=s(Obe);tEr=r(Spt,"FlaxAutoModelForNextSentencePrediction"),Spt.forEach(t),cxe.forEach(t),rBe=i(d),$r=n(d,"DIV",{class:!0});var Ti=s($r);m(M6.$$.fragment,Ti),aEr=i(Ti),_f=n(Ti,"P",{});var SV=s(_f);nEr=r(SV,`This is a generic model class that will be instantiated as one of the model classes of the library (with a next sentence prediction head) when created
with the `),Xbe=n(SV,"CODE",{});var Ppt=s(Xbe);sEr=r(Ppt,"from_pretrained()"),Ppt.forEach(t),lEr=r(SV,"class method or the "),zbe=n(SV,"CODE",{});var $pt=s(zbe);iEr=r($pt,"from_config()"),$pt.forEach(t),dEr=r(SV,`class
method.`),SV.forEach(t),cEr=i(Ti),E6=n(Ti,"P",{});var fxe=s(E6);fEr=r(fxe,"This class cannot be instantiated directly using "),Vbe=n(fxe,"CODE",{});var Ipt=s(Vbe);mEr=r(Ipt,"__init__()"),Ipt.forEach(t),gEr=r(fxe," (throws an error)."),fxe.forEach(t),hEr=i(Ti),Lt=n(Ti,"DIV",{class:!0});var Fi=s(Lt);m(y6.$$.fragment,Fi),pEr=i(Fi),Wbe=n(Fi,"P",{});var jpt=s(Wbe);_Er=r(jpt,"Instantiates one of the model classes of the library (with a next sentence prediction head) from a configuration."),jpt.forEach(t),uEr=i(Fi),uf=n(Fi,"P",{});var PV=s(uf);bEr=r(PV,`Note:
Loading a model from its configuration file does `),Qbe=n(PV,"STRONG",{});var Npt=s(Qbe);vEr=r(Npt,"not"),Npt.forEach(t),TEr=r(PV,` load the model weights. It only affects the
model\u2019s configuration. Use `),Hbe=n(PV,"CODE",{});var Dpt=s(Hbe);FEr=r(Dpt,"from_pretrained()"),Dpt.forEach(t),CEr=r(PV,"to load the model weights."),PV.forEach(t),MEr=i(Fi),Ube=n(Fi,"P",{});var qpt=s(Ube);EEr=r(qpt,"Examples:"),qpt.forEach(t),yEr=i(Fi),m(w6.$$.fragment,Fi),Fi.forEach(t),wEr=i(Ti),$o=n(Ti,"DIV",{class:!0});var Ra=s($o);m(A6.$$.fragment,Ra),AEr=i(Ra),Jbe=n(Ra,"P",{});var Gpt=s(Jbe);LEr=r(Gpt,"Instantiate one of the model classes of the library (with a next sentence prediction head) from a pretrained model."),Gpt.forEach(t),BEr=i(Ra),xn=n(Ra,"P",{});var Z4=s(xn);kEr=r(Z4,"The model class to instantiate is selected based on the "),Ybe=n(Z4,"CODE",{});var Opt=s(Ybe);xEr=r(Opt,"model_type"),Opt.forEach(t),REr=r(Z4,` property of the config object (either
passed as an argument or loaded from `),Kbe=n(Z4,"CODE",{});var Xpt=s(Kbe);SEr=r(Xpt,"pretrained_model_name_or_path"),Xpt.forEach(t),PEr=r(Z4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Zbe=n(Z4,"CODE",{});var zpt=s(Zbe);$Er=r(zpt,"pretrained_model_name_or_path"),zpt.forEach(t),IEr=r(Z4,":"),Z4.forEach(t),jEr=i(Ra),e5e=n(Ra,"UL",{});var Vpt=s(e5e);ZC=n(Vpt,"LI",{});var aLe=s(ZC);o5e=n(aLe,"STRONG",{});var Wpt=s(o5e);NEr=r(Wpt,"bert"),Wpt.forEach(t),DEr=r(aLe," \u2014 "),XX=n(aLe,"A",{href:!0});var Qpt=s(XX);qEr=r(Qpt,"FlaxBertForNextSentencePrediction"),Qpt.forEach(t),GEr=r(aLe," (BERT model)"),aLe.forEach(t),Vpt.forEach(t),OEr=i(Ra),r5e=n(Ra,"P",{});var Hpt=s(r5e);XEr=r(Hpt,"Examples:"),Hpt.forEach(t),zEr=i(Ra),m(L6.$$.fragment,Ra),Ra.forEach(t),Ti.forEach(t),tBe=i(d),bf=n(d,"H2",{class:!0});var mxe=s(bf);e4=n(mxe,"A",{id:!0,class:!0,href:!0});var Upt=s(e4);t5e=n(Upt,"SPAN",{});var Jpt=s(t5e);m(B6.$$.fragment,Jpt),Jpt.forEach(t),Upt.forEach(t),VEr=i(mxe),a5e=n(mxe,"SPAN",{});var Ypt=s(a5e);WEr=r(Ypt,"FlaxAutoModelForImageClassification"),Ypt.forEach(t),mxe.forEach(t),aBe=i(d),Ir=n(d,"DIV",{class:!0});var Ci=s(Ir);m(k6.$$.fragment,Ci),QEr=i(Ci),vf=n(Ci,"P",{});var $V=s(vf);HEr=r($V,`This is a generic model class that will be instantiated as one of the model classes of the library (with a image classification head) when created
with the `),n5e=n($V,"CODE",{});var Kpt=s(n5e);UEr=r(Kpt,"from_pretrained()"),Kpt.forEach(t),JEr=r($V,"class method or the "),s5e=n($V,"CODE",{});var Zpt=s(s5e);YEr=r(Zpt,"from_config()"),Zpt.forEach(t),KEr=r($V,`class
method.`),$V.forEach(t),ZEr=i(Ci),x6=n(Ci,"P",{});var gxe=s(x6);e3r=r(gxe,"This class cannot be instantiated directly using "),l5e=n(gxe,"CODE",{});var e_t=s(l5e);o3r=r(e_t,"__init__()"),e_t.forEach(t),r3r=r(gxe," (throws an error)."),gxe.forEach(t),t3r=i(Ci),Bt=n(Ci,"DIV",{class:!0});var Mi=s(Bt);m(R6.$$.fragment,Mi),a3r=i(Mi),i5e=n(Mi,"P",{});var o_t=s(i5e);n3r=r(o_t,"Instantiates one of the model classes of the library (with a image classification head) from a configuration."),o_t.forEach(t),s3r=i(Mi),Tf=n(Mi,"P",{});var IV=s(Tf);l3r=r(IV,`Note:
Loading a model from its configuration file does `),d5e=n(IV,"STRONG",{});var r_t=s(d5e);i3r=r(r_t,"not"),r_t.forEach(t),d3r=r(IV,` load the model weights. It only affects the
model\u2019s configuration. Use `),c5e=n(IV,"CODE",{});var t_t=s(c5e);c3r=r(t_t,"from_pretrained()"),t_t.forEach(t),f3r=r(IV,"to load the model weights."),IV.forEach(t),m3r=i(Mi),f5e=n(Mi,"P",{});var a_t=s(f5e);g3r=r(a_t,"Examples:"),a_t.forEach(t),h3r=i(Mi),m(S6.$$.fragment,Mi),Mi.forEach(t),p3r=i(Ci),Io=n(Ci,"DIV",{class:!0});var Sa=s(Io);m(P6.$$.fragment,Sa),_3r=i(Sa),m5e=n(Sa,"P",{});var n_t=s(m5e);u3r=r(n_t,"Instantiate one of the model classes of the library (with a image classification head) from a pretrained model."),n_t.forEach(t),b3r=i(Sa),Rn=n(Sa,"P",{});var eM=s(Rn);v3r=r(eM,"The model class to instantiate is selected based on the "),g5e=n(eM,"CODE",{});var s_t=s(g5e);T3r=r(s_t,"model_type"),s_t.forEach(t),F3r=r(eM,` property of the config object (either
passed as an argument or loaded from `),h5e=n(eM,"CODE",{});var l_t=s(h5e);C3r=r(l_t,"pretrained_model_name_or_path"),l_t.forEach(t),M3r=r(eM,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),p5e=n(eM,"CODE",{});var i_t=s(p5e);E3r=r(i_t,"pretrained_model_name_or_path"),i_t.forEach(t),y3r=r(eM,":"),eM.forEach(t),w3r=i(Sa),$6=n(Sa,"UL",{});var hxe=s($6);o4=n(hxe,"LI",{});var nLe=s(o4);_5e=n(nLe,"STRONG",{});var d_t=s(_5e);A3r=r(d_t,"beit"),d_t.forEach(t),L3r=r(nLe," \u2014 "),zX=n(nLe,"A",{href:!0});var c_t=s(zX);B3r=r(c_t,"FlaxBeitForImageClassification"),c_t.forEach(t),k3r=r(nLe," (BEiT model)"),nLe.forEach(t),x3r=i(hxe),r4=n(hxe,"LI",{});var sLe=s(r4);u5e=n(sLe,"STRONG",{});var f_t=s(u5e);R3r=r(f_t,"vit"),f_t.forEach(t),S3r=r(sLe," \u2014 "),VX=n(sLe,"A",{href:!0});var m_t=s(VX);P3r=r(m_t,"FlaxViTForImageClassification"),m_t.forEach(t),$3r=r(sLe," (ViT model)"),sLe.forEach(t),hxe.forEach(t),I3r=i(Sa),b5e=n(Sa,"P",{});var g_t=s(b5e);j3r=r(g_t,"Examples:"),g_t.forEach(t),N3r=i(Sa),m(I6.$$.fragment,Sa),Sa.forEach(t),Ci.forEach(t),nBe=i(d),Ff=n(d,"H2",{class:!0});var pxe=s(Ff);t4=n(pxe,"A",{id:!0,class:!0,href:!0});var h_t=s(t4);v5e=n(h_t,"SPAN",{});var p_t=s(v5e);m(j6.$$.fragment,p_t),p_t.forEach(t),h_t.forEach(t),D3r=i(pxe),T5e=n(pxe,"SPAN",{});var __t=s(T5e);q3r=r(__t,"FlaxAutoModelForVision2Seq"),__t.forEach(t),pxe.forEach(t),sBe=i(d),jr=n(d,"DIV",{class:!0});var Ei=s(jr);m(N6.$$.fragment,Ei),G3r=i(Ei),Cf=n(Ei,"P",{});var jV=s(Cf);O3r=r(jV,`This is a generic model class that will be instantiated as one of the model classes of the library (with a vision-to-text modeling head) when created
with the `),F5e=n(jV,"CODE",{});var u_t=s(F5e);X3r=r(u_t,"from_pretrained()"),u_t.forEach(t),z3r=r(jV,"class method or the "),C5e=n(jV,"CODE",{});var b_t=s(C5e);V3r=r(b_t,"from_config()"),b_t.forEach(t),W3r=r(jV,`class
method.`),jV.forEach(t),Q3r=i(Ei),D6=n(Ei,"P",{});var _xe=s(D6);H3r=r(_xe,"This class cannot be instantiated directly using "),M5e=n(_xe,"CODE",{});var v_t=s(M5e);U3r=r(v_t,"__init__()"),v_t.forEach(t),J3r=r(_xe," (throws an error)."),_xe.forEach(t),Y3r=i(Ei),kt=n(Ei,"DIV",{class:!0});var yi=s(kt);m(q6.$$.fragment,yi),K3r=i(yi),E5e=n(yi,"P",{});var T_t=s(E5e);Z3r=r(T_t,"Instantiates one of the model classes of the library (with a vision-to-text modeling head) from a configuration."),T_t.forEach(t),eyr=i(yi),Mf=n(yi,"P",{});var NV=s(Mf);oyr=r(NV,`Note:
Loading a model from its configuration file does `),y5e=n(NV,"STRONG",{});var F_t=s(y5e);ryr=r(F_t,"not"),F_t.forEach(t),tyr=r(NV,` load the model weights. It only affects the
model\u2019s configuration. Use `),w5e=n(NV,"CODE",{});var C_t=s(w5e);ayr=r(C_t,"from_pretrained()"),C_t.forEach(t),nyr=r(NV,"to load the model weights."),NV.forEach(t),syr=i(yi),A5e=n(yi,"P",{});var M_t=s(A5e);lyr=r(M_t,"Examples:"),M_t.forEach(t),iyr=i(yi),m(G6.$$.fragment,yi),yi.forEach(t),dyr=i(Ei),jo=n(Ei,"DIV",{class:!0});var Pa=s(jo);m(O6.$$.fragment,Pa),cyr=i(Pa),L5e=n(Pa,"P",{});var E_t=s(L5e);fyr=r(E_t,"Instantiate one of the model classes of the library (with a vision-to-text modeling head) from a pretrained model."),E_t.forEach(t),myr=i(Pa),Sn=n(Pa,"P",{});var oM=s(Sn);gyr=r(oM,"The model class to instantiate is selected based on the "),B5e=n(oM,"CODE",{});var y_t=s(B5e);hyr=r(y_t,"model_type"),y_t.forEach(t),pyr=r(oM,` property of the config object (either
passed as an argument or loaded from `),k5e=n(oM,"CODE",{});var w_t=s(k5e);_yr=r(w_t,"pretrained_model_name_or_path"),w_t.forEach(t),uyr=r(oM,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),x5e=n(oM,"CODE",{});var A_t=s(x5e);byr=r(A_t,"pretrained_model_name_or_path"),A_t.forEach(t),vyr=r(oM,":"),oM.forEach(t),Tyr=i(Pa),R5e=n(Pa,"UL",{});var L_t=s(R5e);a4=n(L_t,"LI",{});var lLe=s(a4);S5e=n(lLe,"STRONG",{});var B_t=s(S5e);Fyr=r(B_t,"vision-encoder-decoder"),B_t.forEach(t),Cyr=r(lLe," \u2014 "),WX=n(lLe,"A",{href:!0});var k_t=s(WX);Myr=r(k_t,"FlaxVisionEncoderDecoderModel"),k_t.forEach(t),Eyr=r(lLe," (Vision Encoder decoder model)"),lLe.forEach(t),L_t.forEach(t),yyr=i(Pa),P5e=n(Pa,"P",{});var x_t=s(P5e);wyr=r(x_t,"Examples:"),x_t.forEach(t),Ayr=i(Pa),m(X6.$$.fragment,Pa),Pa.forEach(t),Ei.forEach(t),this.h()},h(){c(J,"name","hf:doc:metadata"),c(J,"content",JSON.stringify(q_t)),c(me,"id","auto-classes"),c(me,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(me,"href","#auto-classes"),c(ie,"class","relative group"),c(Pn,"href","/docs/transformers/pr_15297/en/model_doc/auto#transformers.AutoConfig"),c(In,"href","/docs/transformers/pr_15297/en/model_doc/auto#transformers.AutoModel"),c(jn,"href","/docs/transformers/pr_15297/en/model_doc/auto#transformers.AutoTokenizer"),c(Si,"href","/docs/transformers/pr_15297/en/model_doc/bert#transformers.BertModel"),c(Bf,"id","extending-the-auto-classes"),c(Bf,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Bf,"href","#extending-the-auto-classes"),c(Pi,"class","relative group"),c(xf,"id","transformers.AutoConfig"),c(xf,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(xf,"href","#transformers.AutoConfig"),c($i,"class","relative group"),c(Q0,"href","/docs/transformers/pr_15297/en/model_doc/auto#transformers.AutoConfig.from_pretrained"),c(H0,"href","/docs/transformers/pr_15297/en/model_doc/albert#transformers.AlbertConfig"),c(U0,"href","/docs/transformers/pr_15297/en/model_doc/bart#transformers.BartConfig"),c(J0,"href","/docs/transformers/pr_15297/en/model_doc/beit#transformers.BeitConfig"),c(Y0,"href","/docs/transformers/pr_15297/en/model_doc/bert#transformers.BertConfig"),c(K0,"href","/docs/transformers/pr_15297/en/model_doc/bert-generation#transformers.BertGenerationConfig"),c(Z0,"href","/docs/transformers/pr_15297/en/model_doc/big_bird#transformers.BigBirdConfig"),c(eL,"href","/docs/transformers/pr_15297/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusConfig"),c(oL,"href","/docs/transformers/pr_15297/en/model_doc/blenderbot#transformers.BlenderbotConfig"),c(rL,"href","/docs/transformers/pr_15297/en/model_doc/blenderbot-small#transformers.BlenderbotSmallConfig"),c(tL,"href","/docs/transformers/pr_15297/en/model_doc/camembert#transformers.CamembertConfig"),c(aL,"href","/docs/transformers/pr_15297/en/model_doc/canine#transformers.CanineConfig"),c(nL,"href","/docs/transformers/pr_15297/en/model_doc/clip#transformers.CLIPConfig"),c(sL,"href","/docs/transformers/pr_15297/en/model_doc/convbert#transformers.ConvBertConfig"),c(lL,"href","/docs/transformers/pr_15297/en/model_doc/convnext#transformers.ConvNextConfig"),c(iL,"href","/docs/transformers/pr_15297/en/model_doc/ctrl#transformers.CTRLConfig"),c(dL,"href","/docs/transformers/pr_15297/en/model_doc/deberta#transformers.DebertaConfig"),c(cL,"href","/docs/transformers/pr_15297/en/model_doc/deberta-v2#transformers.DebertaV2Config"),c(fL,"href","/docs/transformers/pr_15297/en/model_doc/deit#transformers.DeiTConfig"),c(mL,"href","/docs/transformers/pr_15297/en/model_doc/detr#transformers.DetrConfig"),c(gL,"href","/docs/transformers/pr_15297/en/model_doc/distilbert#transformers.DistilBertConfig"),c(hL,"href","/docs/transformers/pr_15297/en/model_doc/dpr#transformers.DPRConfig"),c(pL,"href","/docs/transformers/pr_15297/en/model_doc/electra#transformers.ElectraConfig"),c(_L,"href","/docs/transformers/pr_15297/en/model_doc/encoder-decoder#transformers.EncoderDecoderConfig"),c(uL,"href","/docs/transformers/pr_15297/en/model_doc/flaubert#transformers.FlaubertConfig"),c(bL,"href","/docs/transformers/pr_15297/en/model_doc/fnet#transformers.FNetConfig"),c(vL,"href","/docs/transformers/pr_15297/en/model_doc/fsmt#transformers.FSMTConfig"),c(TL,"href","/docs/transformers/pr_15297/en/model_doc/funnel#transformers.FunnelConfig"),c(FL,"href","/docs/transformers/pr_15297/en/model_doc/gpt2#transformers.GPT2Config"),c(CL,"href","/docs/transformers/pr_15297/en/model_doc/gpt_neo#transformers.GPTNeoConfig"),c(ML,"href","/docs/transformers/pr_15297/en/model_doc/gptj#transformers.GPTJConfig"),c(EL,"href","/docs/transformers/pr_15297/en/model_doc/hubert#transformers.HubertConfig"),c(yL,"href","/docs/transformers/pr_15297/en/model_doc/ibert#transformers.IBertConfig"),c(wL,"href","/docs/transformers/pr_15297/en/model_doc/imagegpt#transformers.ImageGPTConfig"),c(AL,"href","/docs/transformers/pr_15297/en/model_doc/layoutlm#transformers.LayoutLMConfig"),c(LL,"href","/docs/transformers/pr_15297/en/model_doc/layoutlmv2#transformers.LayoutLMv2Config"),c(BL,"href","/docs/transformers/pr_15297/en/model_doc/led#transformers.LEDConfig"),c(kL,"href","/docs/transformers/pr_15297/en/model_doc/longformer#transformers.LongformerConfig"),c(xL,"href","/docs/transformers/pr_15297/en/model_doc/luke#transformers.LukeConfig"),c(RL,"href","/docs/transformers/pr_15297/en/model_doc/lxmert#transformers.LxmertConfig"),c(SL,"href","/docs/transformers/pr_15297/en/model_doc/m2m_100#transformers.M2M100Config"),c(PL,"href","/docs/transformers/pr_15297/en/model_doc/marian#transformers.MarianConfig"),c($L,"href","/docs/transformers/pr_15297/en/model_doc/mbart#transformers.MBartConfig"),c(IL,"href","/docs/transformers/pr_15297/en/model_doc/megatron-bert#transformers.MegatronBertConfig"),c(jL,"href","/docs/transformers/pr_15297/en/model_doc/mobilebert#transformers.MobileBertConfig"),c(NL,"href","/docs/transformers/pr_15297/en/model_doc/mpnet#transformers.MPNetConfig"),c(DL,"href","/docs/transformers/pr_15297/en/model_doc/mt5#transformers.MT5Config"),c(qL,"href","/docs/transformers/pr_15297/en/model_doc/nystromformer#transformers.NystromformerConfig"),c(GL,"href","/docs/transformers/pr_15297/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig"),c(OL,"href","/docs/transformers/pr_15297/en/model_doc/pegasus#transformers.PegasusConfig"),c(XL,"href","/docs/transformers/pr_15297/en/model_doc/perceiver#transformers.PerceiverConfig"),c(zL,"href","/docs/transformers/pr_15297/en/model_doc/plbart#transformers.PLBartConfig"),c(VL,"href","/docs/transformers/pr_15297/en/model_doc/poolformer#transformers.PoolFormerConfig"),c(WL,"href","/docs/transformers/pr_15297/en/model_doc/prophetnet#transformers.ProphetNetConfig"),c(QL,"href","/docs/transformers/pr_15297/en/model_doc/qdqbert#transformers.QDQBertConfig"),c(HL,"href","/docs/transformers/pr_15297/en/model_doc/rag#transformers.RagConfig"),c(UL,"href","/docs/transformers/pr_15297/en/model_doc/realm#transformers.RealmConfig"),c(JL,"href","/docs/transformers/pr_15297/en/model_doc/reformer#transformers.ReformerConfig"),c(YL,"href","/docs/transformers/pr_15297/en/model_doc/rembert#transformers.RemBertConfig"),c(KL,"href","/docs/transformers/pr_15297/en/model_doc/retribert#transformers.RetriBertConfig"),c(ZL,"href","/docs/transformers/pr_15297/en/model_doc/roberta#transformers.RobertaConfig"),c(e8,"href","/docs/transformers/pr_15297/en/model_doc/roformer#transformers.RoFormerConfig"),c(o8,"href","/docs/transformers/pr_15297/en/model_doc/segformer#transformers.SegformerConfig"),c(r8,"href","/docs/transformers/pr_15297/en/model_doc/sew#transformers.SEWConfig"),c(t8,"href","/docs/transformers/pr_15297/en/model_doc/sew-d#transformers.SEWDConfig"),c(a8,"href","/docs/transformers/pr_15297/en/model_doc/speech-encoder-decoder#transformers.SpeechEncoderDecoderConfig"),c(n8,"href","/docs/transformers/pr_15297/en/model_doc/speech_to_text#transformers.Speech2TextConfig"),c(s8,"href","/docs/transformers/pr_15297/en/model_doc/speech_to_text_2#transformers.Speech2Text2Config"),c(l8,"href","/docs/transformers/pr_15297/en/model_doc/splinter#transformers.SplinterConfig"),c(i8,"href","/docs/transformers/pr_15297/en/model_doc/squeezebert#transformers.SqueezeBertConfig"),c(d8,"href","/docs/transformers/pr_15297/en/model_doc/swin#transformers.SwinConfig"),c(c8,"href","/docs/transformers/pr_15297/en/model_doc/t5#transformers.T5Config"),c(f8,"href","/docs/transformers/pr_15297/en/model_doc/tapas#transformers.TapasConfig"),c(m8,"href","/docs/transformers/pr_15297/en/model_doc/transfo-xl#transformers.TransfoXLConfig"),c(g8,"href","/docs/transformers/pr_15297/en/model_doc/trocr#transformers.TrOCRConfig"),c(h8,"href","/docs/transformers/pr_15297/en/model_doc/unispeech#transformers.UniSpeechConfig"),c(p8,"href","/docs/transformers/pr_15297/en/model_doc/unispeech-sat#transformers.UniSpeechSatConfig"),c(_8,"href","/docs/transformers/pr_15297/en/model_doc/vilt#transformers.ViltConfig"),c(u8,"href","/docs/transformers/pr_15297/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderConfig"),c(b8,"href","/docs/transformers/pr_15297/en/model_doc/vision-text-dual-encoder#transformers.VisionTextDualEncoderConfig"),c(v8,"href","/docs/transformers/pr_15297/en/model_doc/visual_bert#transformers.VisualBertConfig"),c(T8,"href","/docs/transformers/pr_15297/en/model_doc/vit#transformers.ViTConfig"),c(F8,"href","/docs/transformers/pr_15297/en/model_doc/vit_mae#transformers.ViTMAEConfig"),c(C8,"href","/docs/transformers/pr_15297/en/model_doc/wav2vec2#transformers.Wav2Vec2Config"),c(M8,"href","/docs/transformers/pr_15297/en/model_doc/wavlm#transformers.WavLMConfig"),c(E8,"href","/docs/transformers/pr_15297/en/model_doc/xglm#transformers.XGLMConfig"),c(y8,"href","/docs/transformers/pr_15297/en/model_doc/xlm#transformers.XLMConfig"),c(w8,"href","/docs/transformers/pr_15297/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetConfig"),c(A8,"href","/docs/transformers/pr_15297/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig"),c(L8,"href","/docs/transformers/pr_15297/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig"),c(B8,"href","/docs/transformers/pr_15297/en/model_doc/xlnet#transformers.XLNetConfig"),c(k8,"href","/docs/transformers/pr_15297/en/model_doc/yoso#transformers.YosoConfig"),c(fo,"class","docstring"),c(pg,"class","docstring"),c(Go,"class","docstring"),c(_g,"id","transformers.AutoTokenizer"),c(_g,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(_g,"href","#transformers.AutoTokenizer"),c(ji,"class","relative group"),c(x8,"href","/docs/transformers/pr_15297/en/model_doc/auto#transformers.AutoTokenizer.from_pretrained"),c(R8,"href","/docs/transformers/pr_15297/en/model_doc/albert#transformers.AlbertTokenizer"),c(S8,"href","/docs/transformers/pr_15297/en/model_doc/albert#transformers.AlbertTokenizerFast"),c(P8,"href","/docs/transformers/pr_15297/en/model_doc/bart#transformers.BartTokenizer"),c($8,"href","/docs/transformers/pr_15297/en/model_doc/bart#transformers.BartTokenizerFast"),c(I8,"href","/docs/transformers/pr_15297/en/model_doc/barthez#transformers.BarthezTokenizer"),c(j8,"href","/docs/transformers/pr_15297/en/model_doc/barthez#transformers.BarthezTokenizerFast"),c(N8,"href","/docs/transformers/pr_15297/en/model_doc/bartpho#transformers.BartphoTokenizer"),c(D8,"href","/docs/transformers/pr_15297/en/model_doc/bert#transformers.BertTokenizer"),c(q8,"href","/docs/transformers/pr_15297/en/model_doc/bert#transformers.BertTokenizerFast"),c(G8,"href","/docs/transformers/pr_15297/en/model_doc/bert-generation#transformers.BertGenerationTokenizer"),c(O8,"href","/docs/transformers/pr_15297/en/model_doc/bert-japanese#transformers.BertJapaneseTokenizer"),c(X8,"href","/docs/transformers/pr_15297/en/model_doc/bertweet#transformers.BertweetTokenizer"),c(z8,"href","/docs/transformers/pr_15297/en/model_doc/big_bird#transformers.BigBirdTokenizer"),c(V8,"href","/docs/transformers/pr_15297/en/model_doc/big_bird#transformers.BigBirdTokenizerFast"),c(W8,"href","/docs/transformers/pr_15297/en/model_doc/pegasus#transformers.PegasusTokenizer"),c(Q8,"href","/docs/transformers/pr_15297/en/model_doc/pegasus#transformers.PegasusTokenizerFast"),c(H8,"href","/docs/transformers/pr_15297/en/model_doc/blenderbot#transformers.BlenderbotTokenizer"),c(U8,"href","/docs/transformers/pr_15297/en/model_doc/blenderbot#transformers.BlenderbotTokenizerFast"),c(J8,"href","/docs/transformers/pr_15297/en/model_doc/blenderbot-small#transformers.BlenderbotSmallTokenizer"),c(Y8,"href","/docs/transformers/pr_15297/en/model_doc/byt5#transformers.ByT5Tokenizer"),c(K8,"href","/docs/transformers/pr_15297/en/model_doc/camembert#transformers.CamembertTokenizer"),c(Z8,"href","/docs/transformers/pr_15297/en/model_doc/camembert#transformers.CamembertTokenizerFast"),c(eB,"href","/docs/transformers/pr_15297/en/model_doc/canine#transformers.CanineTokenizer"),c(oB,"href","/docs/transformers/pr_15297/en/model_doc/clip#transformers.CLIPTokenizer"),c(rB,"href","/docs/transformers/pr_15297/en/model_doc/clip#transformers.CLIPTokenizerFast"),c(tB,"href","/docs/transformers/pr_15297/en/model_doc/convbert#transformers.ConvBertTokenizer"),c(aB,"href","/docs/transformers/pr_15297/en/model_doc/convbert#transformers.ConvBertTokenizerFast"),c(nB,"href","/docs/transformers/pr_15297/en/model_doc/cpm#transformers.CpmTokenizer"),c(sB,"href","/docs/transformers/pr_15297/en/model_doc/ctrl#transformers.CTRLTokenizer"),c(lB,"href","/docs/transformers/pr_15297/en/model_doc/deberta#transformers.DebertaTokenizer"),c(iB,"href","/docs/transformers/pr_15297/en/model_doc/deberta#transformers.DebertaTokenizerFast"),c(dB,"href","/docs/transformers/pr_15297/en/model_doc/deberta-v2#transformers.DebertaV2Tokenizer"),c(cB,"href","/docs/transformers/pr_15297/en/model_doc/distilbert#transformers.DistilBertTokenizer"),c(fB,"href","/docs/transformers/pr_15297/en/model_doc/distilbert#transformers.DistilBertTokenizerFast"),c(mB,"href","/docs/transformers/pr_15297/en/model_doc/dpr#transformers.DPRQuestionEncoderTokenizer"),c(gB,"href","/docs/transformers/pr_15297/en/model_doc/dpr#transformers.DPRQuestionEncoderTokenizerFast"),c(hB,"href","/docs/transformers/pr_15297/en/model_doc/electra#transformers.ElectraTokenizer"),c(pB,"href","/docs/transformers/pr_15297/en/model_doc/electra#transformers.ElectraTokenizerFast"),c(_B,"href","/docs/transformers/pr_15297/en/model_doc/flaubert#transformers.FlaubertTokenizer"),c(uB,"href","/docs/transformers/pr_15297/en/model_doc/fnet#transformers.FNetTokenizer"),c(bB,"href","/docs/transformers/pr_15297/en/model_doc/fnet#transformers.FNetTokenizerFast"),c(vB,"href","/docs/transformers/pr_15297/en/model_doc/fsmt#transformers.FSMTTokenizer"),c(TB,"href","/docs/transformers/pr_15297/en/model_doc/funnel#transformers.FunnelTokenizer"),c(FB,"href","/docs/transformers/pr_15297/en/model_doc/funnel#transformers.FunnelTokenizerFast"),c(CB,"href","/docs/transformers/pr_15297/en/model_doc/gpt2#transformers.GPT2Tokenizer"),c(MB,"href","/docs/transformers/pr_15297/en/model_doc/gpt2#transformers.GPT2TokenizerFast"),c(EB,"href","/docs/transformers/pr_15297/en/model_doc/gpt2#transformers.GPT2Tokenizer"),c(yB,"href","/docs/transformers/pr_15297/en/model_doc/gpt2#transformers.GPT2TokenizerFast"),c(wB,"href","/docs/transformers/pr_15297/en/model_doc/herbert#transformers.HerbertTokenizer"),c(AB,"href","/docs/transformers/pr_15297/en/model_doc/herbert#transformers.HerbertTokenizerFast"),c(LB,"href","/docs/transformers/pr_15297/en/model_doc/wav2vec2#transformers.Wav2Vec2CTCTokenizer"),c(BB,"href","/docs/transformers/pr_15297/en/model_doc/roberta#transformers.RobertaTokenizer"),c(kB,"href","/docs/transformers/pr_15297/en/model_doc/roberta#transformers.RobertaTokenizerFast"),c(xB,"href","/docs/transformers/pr_15297/en/model_doc/layoutlm#transformers.LayoutLMTokenizer"),c(RB,"href","/docs/transformers/pr_15297/en/model_doc/layoutlm#transformers.LayoutLMTokenizerFast"),c(SB,"href","/docs/transformers/pr_15297/en/model_doc/layoutlmv2#transformers.LayoutLMv2Tokenizer"),c(PB,"href","/docs/transformers/pr_15297/en/model_doc/layoutlmv2#transformers.LayoutLMv2TokenizerFast"),c($B,"href","/docs/transformers/pr_15297/en/model_doc/layoutxlm#transformers.LayoutXLMTokenizer"),c(IB,"href","/docs/transformers/pr_15297/en/model_doc/layoutxlm#transformers.LayoutXLMTokenizerFast"),c(jB,"href","/docs/transformers/pr_15297/en/model_doc/led#transformers.LEDTokenizer"),c(NB,"href","/docs/transformers/pr_15297/en/model_doc/led#transformers.LEDTokenizerFast"),c(DB,"href","/docs/transformers/pr_15297/en/model_doc/longformer#transformers.LongformerTokenizer"),c(qB,"href","/docs/transformers/pr_15297/en/model_doc/longformer#transformers.LongformerTokenizerFast"),c(GB,"href","/docs/transformers/pr_15297/en/model_doc/luke#transformers.LukeTokenizer"),c(OB,"href","/docs/transformers/pr_15297/en/model_doc/lxmert#transformers.LxmertTokenizer"),c(XB,"href","/docs/transformers/pr_15297/en/model_doc/lxmert#transformers.LxmertTokenizerFast"),c(zB,"href","/docs/transformers/pr_15297/en/model_doc/m2m_100#transformers.M2M100Tokenizer"),c(VB,"href","/docs/transformers/pr_15297/en/model_doc/marian#transformers.MarianTokenizer"),c(WB,"href","/docs/transformers/pr_15297/en/model_doc/mbart#transformers.MBartTokenizer"),c(QB,"href","/docs/transformers/pr_15297/en/model_doc/mbart#transformers.MBartTokenizerFast"),c(HB,"href","/docs/transformers/pr_15297/en/model_doc/mbart#transformers.MBart50Tokenizer"),c(UB,"href","/docs/transformers/pr_15297/en/model_doc/mbart#transformers.MBart50TokenizerFast"),c(JB,"href","/docs/transformers/pr_15297/en/model_doc/mluke#transformers.MLukeTokenizer"),c(YB,"href","/docs/transformers/pr_15297/en/model_doc/mobilebert#transformers.MobileBertTokenizer"),c(KB,"href","/docs/transformers/pr_15297/en/model_doc/mobilebert#transformers.MobileBertTokenizerFast"),c(ZB,"href","/docs/transformers/pr_15297/en/model_doc/mpnet#transformers.MPNetTokenizer"),c(ek,"href","/docs/transformers/pr_15297/en/model_doc/mpnet#transformers.MPNetTokenizerFast"),c(ok,"href","/docs/transformers/pr_15297/en/model_doc/mt5#transformers.T5Tokenizer"),c(rk,"href","/docs/transformers/pr_15297/en/model_doc/mt5#transformers.T5TokenizerFast"),c(tk,"href","/docs/transformers/pr_15297/en/model_doc/openai-gpt#transformers.OpenAIGPTTokenizer"),c(ak,"href","/docs/transformers/pr_15297/en/model_doc/openai-gpt#transformers.OpenAIGPTTokenizerFast"),c(nk,"href","/docs/transformers/pr_15297/en/model_doc/pegasus#transformers.PegasusTokenizer"),c(sk,"href","/docs/transformers/pr_15297/en/model_doc/pegasus#transformers.PegasusTokenizerFast"),c(lk,"href","/docs/transformers/pr_15297/en/model_doc/perceiver#transformers.PerceiverTokenizer"),c(ik,"href","/docs/transformers/pr_15297/en/model_doc/phobert#transformers.PhobertTokenizer"),c(dk,"href","/docs/transformers/pr_15297/en/model_doc/plbart#transformers.PLBartTokenizer"),c(ck,"href","/docs/transformers/pr_15297/en/model_doc/prophetnet#transformers.ProphetNetTokenizer"),c(fk,"href","/docs/transformers/pr_15297/en/model_doc/bert#transformers.BertTokenizer"),c(mk,"href","/docs/transformers/pr_15297/en/model_doc/bert#transformers.BertTokenizerFast"),c(gk,"href","/docs/transformers/pr_15297/en/model_doc/rag#transformers.RagTokenizer"),c(hk,"href","/docs/transformers/pr_15297/en/model_doc/realm#transformers.RealmTokenizer"),c(pk,"href","/docs/transformers/pr_15297/en/model_doc/realm#transformers.RealmTokenizerFast"),c(_k,"href","/docs/transformers/pr_15297/en/model_doc/reformer#transformers.ReformerTokenizer"),c(uk,"href","/docs/transformers/pr_15297/en/model_doc/reformer#transformers.ReformerTokenizerFast"),c(bk,"href","/docs/transformers/pr_15297/en/model_doc/rembert#transformers.RemBertTokenizer"),c(vk,"href","/docs/transformers/pr_15297/en/model_doc/rembert#transformers.RemBertTokenizerFast"),c(Tk,"href","/docs/transformers/pr_15297/en/model_doc/retribert#transformers.RetriBertTokenizer"),c(Fk,"href","/docs/transformers/pr_15297/en/model_doc/retribert#transformers.RetriBertTokenizerFast"),c(Ck,"href","/docs/transformers/pr_15297/en/model_doc/roberta#transformers.RobertaTokenizer"),c(Mk,"href","/docs/transformers/pr_15297/en/model_doc/roberta#transformers.RobertaTokenizerFast"),c(Ek,"href","/docs/transformers/pr_15297/en/model_doc/roformer#transformers.RoFormerTokenizer"),c(yk,"href","/docs/transformers/pr_15297/en/model_doc/roformer#transformers.RoFormerTokenizerFast"),c(wk,"href","/docs/transformers/pr_15297/en/model_doc/speech_to_text#transformers.Speech2TextTokenizer"),c(Ak,"href","/docs/transformers/pr_15297/en/model_doc/speech_to_text_2#transformers.Speech2Text2Tokenizer"),c(Lk,"href","/docs/transformers/pr_15297/en/model_doc/splinter#transformers.SplinterTokenizer"),c(Bk,"href","/docs/transformers/pr_15297/en/model_doc/splinter#transformers.SplinterTokenizerFast"),c(kk,"href","/docs/transformers/pr_15297/en/model_doc/squeezebert#transformers.SqueezeBertTokenizer"),c(xk,"href","/docs/transformers/pr_15297/en/model_doc/squeezebert#transformers.SqueezeBertTokenizerFast"),c(Rk,"href","/docs/transformers/pr_15297/en/model_doc/mt5#transformers.T5Tokenizer"),c(Sk,"href","/docs/transformers/pr_15297/en/model_doc/mt5#transformers.T5TokenizerFast"),c(Pk,"href","/docs/transformers/pr_15297/en/model_doc/tapas#transformers.TapasTokenizer"),c($k,"href","/docs/transformers/pr_15297/en/model_doc/transfo-xl#transformers.TransfoXLTokenizer"),c(Ik,"href","/docs/transformers/pr_15297/en/model_doc/wav2vec2#transformers.Wav2Vec2CTCTokenizer"),c(jk,"href","/docs/transformers/pr_15297/en/model_doc/wav2vec2_phoneme#transformers.Wav2Vec2PhonemeCTCTokenizer"),c(Nk,"href","/docs/transformers/pr_15297/en/model_doc/xglm#transformers.XGLMTokenizer"),c(Dk,"href","/docs/transformers/pr_15297/en/model_doc/xglm#transformers.XGLMTokenizerFast"),c(qk,"href","/docs/transformers/pr_15297/en/model_doc/xlm#transformers.XLMTokenizer"),c(Gk,"href","/docs/transformers/pr_15297/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetTokenizer"),c(Ok,"href","/docs/transformers/pr_15297/en/model_doc/xlm-roberta#transformers.XLMRobertaTokenizer"),c(Xk,"href","/docs/transformers/pr_15297/en/model_doc/xlm-roberta#transformers.XLMRobertaTokenizerFast"),c(zk,"href","/docs/transformers/pr_15297/en/model_doc/xlnet#transformers.XLNetTokenizer"),c(Vk,"href","/docs/transformers/pr_15297/en/model_doc/xlnet#transformers.XLNetTokenizerFast"),c(mo,"class","docstring"),c(Wg,"class","docstring"),c(Oo,"class","docstring"),c(Qg,"id","transformers.AutoFeatureExtractor"),c(Qg,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Qg,"href","#transformers.AutoFeatureExtractor"),c(Ni,"class","relative group"),c(Wk,"href","/docs/transformers/pr_15297/en/model_doc/auto#transformers.AutoFeatureExtractor.from_pretrained"),c(Qk,"href","/docs/transformers/pr_15297/en/model_doc/beit#transformers.BeitFeatureExtractor"),c(Hk,"href","/docs/transformers/pr_15297/en/model_doc/clip#transformers.CLIPFeatureExtractor"),c(Uk,"href","/docs/transformers/pr_15297/en/model_doc/convnext#transformers.ConvNextFeatureExtractor"),c(Jk,"href","/docs/transformers/pr_15297/en/model_doc/deit#transformers.DeiTFeatureExtractor"),c(Yk,"href","/docs/transformers/pr_15297/en/model_doc/detr#transformers.DetrFeatureExtractor"),c(Kk,"href","/docs/transformers/pr_15297/en/model_doc/wav2vec2#transformers.Wav2Vec2FeatureExtractor"),c(Zk,"href","/docs/transformers/pr_15297/en/model_doc/layoutlmv2#transformers.LayoutLMv2FeatureExtractor"),c(ex,"href","/docs/transformers/pr_15297/en/model_doc/perceiver#transformers.PerceiverFeatureExtractor"),c(ox,"href","/docs/transformers/pr_15297/en/model_doc/poolformer#transformers.PoolFormerFeatureExtractor"),c(rx,"href","/docs/transformers/pr_15297/en/model_doc/segformer#transformers.SegformerFeatureExtractor"),c(tx,"href","/docs/transformers/pr_15297/en/model_doc/speech_to_text#transformers.Speech2TextFeatureExtractor"),c(ax,"href","/docs/transformers/pr_15297/en/model_doc/vit#transformers.ViTFeatureExtractor"),c(nx,"href","/docs/transformers/pr_15297/en/model_doc/vit#transformers.ViTFeatureExtractor"),c(sx,"href","/docs/transformers/pr_15297/en/model_doc/vit#transformers.ViTFeatureExtractor"),c(lx,"href","/docs/transformers/pr_15297/en/model_doc/wav2vec2#transformers.Wav2Vec2FeatureExtractor"),c(Le,"class","docstring"),c(ch,"class","docstring"),c(Xo,"class","docstring"),c(fh,"id","transformers.AutoProcessor"),c(fh,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(fh,"href","#transformers.AutoProcessor"),c(Di,"class","relative group"),c(ix,"href","/docs/transformers/pr_15297/en/model_doc/auto#transformers.AutoProcessor.from_pretrained"),c(dx,"href","/docs/transformers/pr_15297/en/model_doc/clip#transformers.CLIPProcessor"),c(cx,"href","/docs/transformers/pr_15297/en/model_doc/layoutlmv2#transformers.LayoutLMv2Processor"),c(fx,"href","/docs/transformers/pr_15297/en/model_doc/layoutxlm#transformers.LayoutXLMProcessor"),c(mx,"href","/docs/transformers/pr_15297/en/model_doc/speech_to_text#transformers.Speech2TextProcessor"),c(gx,"href","/docs/transformers/pr_15297/en/model_doc/speech_to_text_2#transformers.Speech2Text2Processor"),c(hx,"href","/docs/transformers/pr_15297/en/model_doc/trocr#transformers.TrOCRProcessor"),c(px,"href","/docs/transformers/pr_15297/en/model_doc/vision-text-dual-encoder#transformers.VisionTextDualEncoderProcessor"),c(_x,"href","/docs/transformers/pr_15297/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor"),c(Be,"class","docstring"),c(Fh,"class","docstring"),c(zo,"class","docstring"),c(Ch,"id","transformers.AutoModel"),c(Ch,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Ch,"href","#transformers.AutoModel"),c(Gi,"class","relative group"),c(Nr,"class","docstring"),c(ux,"href","/docs/transformers/pr_15297/en/model_doc/albert#transformers.AlbertModel"),c(bx,"href","/docs/transformers/pr_15297/en/model_doc/bart#transformers.BartModel"),c(vx,"href","/docs/transformers/pr_15297/en/model_doc/beit#transformers.BeitModel"),c(Tx,"href","/docs/transformers/pr_15297/en/model_doc/bert#transformers.BertModel"),c(Fx,"href","/docs/transformers/pr_15297/en/model_doc/bert-generation#transformers.BertGenerationEncoder"),c(Cx,"href","/docs/transformers/pr_15297/en/model_doc/big_bird#transformers.BigBirdModel"),c(Mx,"href","/docs/transformers/pr_15297/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusModel"),c(Ex,"href","/docs/transformers/pr_15297/en/model_doc/blenderbot#transformers.BlenderbotModel"),c(yx,"href","/docs/transformers/pr_15297/en/model_doc/blenderbot-small#transformers.BlenderbotSmallModel"),c(wx,"href","/docs/transformers/pr_15297/en/model_doc/camembert#transformers.CamembertModel"),c(Ax,"href","/docs/transformers/pr_15297/en/model_doc/canine#transformers.CanineModel"),c(Lx,"href","/docs/transformers/pr_15297/en/model_doc/clip#transformers.CLIPModel"),c(Bx,"href","/docs/transformers/pr_15297/en/model_doc/convbert#transformers.ConvBertModel"),c(kx,"href","/docs/transformers/pr_15297/en/model_doc/convnext#transformers.ConvNextModel"),c(xx,"href","/docs/transformers/pr_15297/en/model_doc/ctrl#transformers.CTRLModel"),c(Rx,"href","/docs/transformers/pr_15297/en/model_doc/deberta#transformers.DebertaModel"),c(Sx,"href","/docs/transformers/pr_15297/en/model_doc/deberta-v2#transformers.DebertaV2Model"),c(Px,"href","/docs/transformers/pr_15297/en/model_doc/deit#transformers.DeiTModel"),c($x,"href","/docs/transformers/pr_15297/en/model_doc/detr#transformers.DetrModel"),c(Ix,"href","/docs/transformers/pr_15297/en/model_doc/distilbert#transformers.DistilBertModel"),c(jx,"href","/docs/transformers/pr_15297/en/model_doc/dpr#transformers.DPRQuestionEncoder"),c(Nx,"href","/docs/transformers/pr_15297/en/model_doc/electra#transformers.ElectraModel"),c(Dx,"href","/docs/transformers/pr_15297/en/model_doc/flaubert#transformers.FlaubertModel"),c(qx,"href","/docs/transformers/pr_15297/en/model_doc/fnet#transformers.FNetModel"),c(Gx,"href","/docs/transformers/pr_15297/en/model_doc/fsmt#transformers.FSMTModel"),c(Ox,"href","/docs/transformers/pr_15297/en/model_doc/funnel#transformers.FunnelModel"),c(Xx,"href","/docs/transformers/pr_15297/en/model_doc/funnel#transformers.FunnelBaseModel"),c(zx,"href","/docs/transformers/pr_15297/en/model_doc/gpt2#transformers.GPT2Model"),c(Vx,"href","/docs/transformers/pr_15297/en/model_doc/gpt_neo#transformers.GPTNeoModel"),c(Wx,"href","/docs/transformers/pr_15297/en/model_doc/gptj#transformers.GPTJModel"),c(Qx,"href","/docs/transformers/pr_15297/en/model_doc/hubert#transformers.HubertModel"),c(Hx,"href","/docs/transformers/pr_15297/en/model_doc/ibert#transformers.IBertModel"),c(Ux,"href","/docs/transformers/pr_15297/en/model_doc/imagegpt#transformers.ImageGPTModel"),c(Jx,"href","/docs/transformers/pr_15297/en/model_doc/layoutlm#transformers.LayoutLMModel"),c(Yx,"href","/docs/transformers/pr_15297/en/model_doc/layoutlmv2#transformers.LayoutLMv2Model"),c(Kx,"href","/docs/transformers/pr_15297/en/model_doc/led#transformers.LEDModel"),c(Zx,"href","/docs/transformers/pr_15297/en/model_doc/longformer#transformers.LongformerModel"),c(eR,"href","/docs/transformers/pr_15297/en/model_doc/luke#transformers.LukeModel"),c(oR,"href","/docs/transformers/pr_15297/en/model_doc/lxmert#transformers.LxmertModel"),c(rR,"href","/docs/transformers/pr_15297/en/model_doc/m2m_100#transformers.M2M100Model"),c(tR,"href","/docs/transformers/pr_15297/en/model_doc/marian#transformers.MarianModel"),c(aR,"href","/docs/transformers/pr_15297/en/model_doc/mbart#transformers.MBartModel"),c(nR,"href","/docs/transformers/pr_15297/en/model_doc/megatron-bert#transformers.MegatronBertModel"),c(sR,"href","/docs/transformers/pr_15297/en/model_doc/mobilebert#transformers.MobileBertModel"),c(lR,"href","/docs/transformers/pr_15297/en/model_doc/mpnet#transformers.MPNetModel"),c(iR,"href","/docs/transformers/pr_15297/en/model_doc/mt5#transformers.MT5Model"),c(dR,"href","/docs/transformers/pr_15297/en/model_doc/nystromformer#transformers.NystromformerModel"),c(cR,"href","/docs/transformers/pr_15297/en/model_doc/openai-gpt#transformers.OpenAIGPTModel"),c(fR,"href","/docs/transformers/pr_15297/en/model_doc/pegasus#transformers.PegasusModel"),c(mR,"href","/docs/transformers/pr_15297/en/model_doc/perceiver#transformers.PerceiverModel"),c(gR,"href","/docs/transformers/pr_15297/en/model_doc/plbart#transformers.PLBartModel"),c(hR,"href","/docs/transformers/pr_15297/en/model_doc/poolformer#transformers.PoolFormerModel"),c(pR,"href","/docs/transformers/pr_15297/en/model_doc/prophetnet#transformers.ProphetNetModel"),c(_R,"href","/docs/transformers/pr_15297/en/model_doc/qdqbert#transformers.QDQBertModel"),c(uR,"href","/docs/transformers/pr_15297/en/model_doc/reformer#transformers.ReformerModel"),c(bR,"href","/docs/transformers/pr_15297/en/model_doc/rembert#transformers.RemBertModel"),c(vR,"href","/docs/transformers/pr_15297/en/model_doc/retribert#transformers.RetriBertModel"),c(TR,"href","/docs/transformers/pr_15297/en/model_doc/roberta#transformers.RobertaModel"),c(FR,"href","/docs/transformers/pr_15297/en/model_doc/roformer#transformers.RoFormerModel"),c(CR,"href","/docs/transformers/pr_15297/en/model_doc/segformer#transformers.SegformerModel"),c(MR,"href","/docs/transformers/pr_15297/en/model_doc/sew#transformers.SEWModel"),c(ER,"href","/docs/transformers/pr_15297/en/model_doc/sew-d#transformers.SEWDModel"),c(yR,"href","/docs/transformers/pr_15297/en/model_doc/speech_to_text#transformers.Speech2TextModel"),c(wR,"href","/docs/transformers/pr_15297/en/model_doc/splinter#transformers.SplinterModel"),c(AR,"href","/docs/transformers/pr_15297/en/model_doc/squeezebert#transformers.SqueezeBertModel"),c(LR,"href","/docs/transformers/pr_15297/en/model_doc/swin#transformers.SwinModel"),c(BR,"href","/docs/transformers/pr_15297/en/model_doc/t5#transformers.T5Model"),c(kR,"href","/docs/transformers/pr_15297/en/model_doc/tapas#transformers.TapasModel"),c(xR,"href","/docs/transformers/pr_15297/en/model_doc/transfo-xl#transformers.TransfoXLModel"),c(RR,"href","/docs/transformers/pr_15297/en/model_doc/unispeech#transformers.UniSpeechModel"),c(SR,"href","/docs/transformers/pr_15297/en/model_doc/unispeech-sat#transformers.UniSpeechSatModel"),c(PR,"href","/docs/transformers/pr_15297/en/model_doc/vilt#transformers.ViltModel"),c($R,"href","/docs/transformers/pr_15297/en/model_doc/vision-text-dual-encoder#transformers.VisionTextDualEncoderModel"),c(IR,"href","/docs/transformers/pr_15297/en/model_doc/visual_bert#transformers.VisualBertModel"),c(jR,"href","/docs/transformers/pr_15297/en/model_doc/vit#transformers.ViTModel"),c(NR,"href","/docs/transformers/pr_15297/en/model_doc/vit_mae#transformers.ViTMAEModel"),c(DR,"href","/docs/transformers/pr_15297/en/model_doc/wav2vec2#transformers.Wav2Vec2Model"),c(qR,"href","/docs/transformers/pr_15297/en/model_doc/wavlm#transformers.WavLMModel"),c(GR,"href","/docs/transformers/pr_15297/en/model_doc/xglm#transformers.XGLMModel"),c(OR,"href","/docs/transformers/pr_15297/en/model_doc/xlm#transformers.XLMModel"),c(XR,"href","/docs/transformers/pr_15297/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetModel"),c(zR,"href","/docs/transformers/pr_15297/en/model_doc/xlm-roberta#transformers.XLMRobertaModel"),c(VR,"href","/docs/transformers/pr_15297/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLModel"),c(WR,"href","/docs/transformers/pr_15297/en/model_doc/xlnet#transformers.XLNetModel"),c(QR,"href","/docs/transformers/pr_15297/en/model_doc/yoso#transformers.YosoModel"),c(ke,"class","docstring"),c(Vo,"class","docstring"),c(Zp,"id","transformers.AutoModelForPreTraining"),c(Zp,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Zp,"href","#transformers.AutoModelForPreTraining"),c(zi,"class","relative group"),c(Dr,"class","docstring"),c(HR,"href","/docs/transformers/pr_15297/en/model_doc/albert#transformers.AlbertForPreTraining"),c(UR,"href","/docs/transformers/pr_15297/en/model_doc/bart#transformers.BartForConditionalGeneration"),c(JR,"href","/docs/transformers/pr_15297/en/model_doc/bert#transformers.BertForPreTraining"),c(YR,"href","/docs/transformers/pr_15297/en/model_doc/big_bird#transformers.BigBirdForPreTraining"),c(KR,"href","/docs/transformers/pr_15297/en/model_doc/camembert#transformers.CamembertForMaskedLM"),c(ZR,"href","/docs/transformers/pr_15297/en/model_doc/ctrl#transformers.CTRLLMHeadModel"),c(eS,"href","/docs/transformers/pr_15297/en/model_doc/deberta#transformers.DebertaForMaskedLM"),c(oS,"href","/docs/transformers/pr_15297/en/model_doc/deberta-v2#transformers.DebertaV2ForMaskedLM"),c(rS,"href","/docs/transformers/pr_15297/en/model_doc/distilbert#transformers.DistilBertForMaskedLM"),c(tS,"href","/docs/transformers/pr_15297/en/model_doc/electra#transformers.ElectraForPreTraining"),c(aS,"href","/docs/transformers/pr_15297/en/model_doc/flaubert#transformers.FlaubertWithLMHeadModel"),c(nS,"href","/docs/transformers/pr_15297/en/model_doc/fnet#transformers.FNetForPreTraining"),c(sS,"href","/docs/transformers/pr_15297/en/model_doc/fsmt#transformers.FSMTForConditionalGeneration"),c(lS,"href","/docs/transformers/pr_15297/en/model_doc/funnel#transformers.FunnelForPreTraining"),c(iS,"href","/docs/transformers/pr_15297/en/model_doc/gpt2#transformers.GPT2LMHeadModel"),c(dS,"href","/docs/transformers/pr_15297/en/model_doc/ibert#transformers.IBertForMaskedLM"),c(cS,"href","/docs/transformers/pr_15297/en/model_doc/layoutlm#transformers.LayoutLMForMaskedLM"),c(fS,"href","/docs/transformers/pr_15297/en/model_doc/longformer#transformers.LongformerForMaskedLM"),c(mS,"href","/docs/transformers/pr_15297/en/model_doc/lxmert#transformers.LxmertForPreTraining"),c(gS,"href","/docs/transformers/pr_15297/en/model_doc/megatron-bert#transformers.MegatronBertForPreTraining"),c(hS,"href","/docs/transformers/pr_15297/en/model_doc/mobilebert#transformers.MobileBertForPreTraining"),c(pS,"href","/docs/transformers/pr_15297/en/model_doc/mpnet#transformers.MPNetForMaskedLM"),c(_S,"href","/docs/transformers/pr_15297/en/model_doc/openai-gpt#transformers.OpenAIGPTLMHeadModel"),c(uS,"href","/docs/transformers/pr_15297/en/model_doc/retribert#transformers.RetriBertModel"),c(bS,"href","/docs/transformers/pr_15297/en/model_doc/roberta#transformers.RobertaForMaskedLM"),c(vS,"href","/docs/transformers/pr_15297/en/model_doc/squeezebert#transformers.SqueezeBertForMaskedLM"),c(TS,"href","/docs/transformers/pr_15297/en/model_doc/t5#transformers.T5ForConditionalGeneration"),c(FS,"href","/docs/transformers/pr_15297/en/model_doc/tapas#transformers.TapasForMaskedLM"),c(CS,"href","/docs/transformers/pr_15297/en/model_doc/transfo-xl#transformers.TransfoXLLMHeadModel"),c(MS,"href","/docs/transformers/pr_15297/en/model_doc/unispeech#transformers.UniSpeechForPreTraining"),c(ES,"href","/docs/transformers/pr_15297/en/model_doc/unispeech-sat#transformers.UniSpeechSatForPreTraining"),c(yS,"href","/docs/transformers/pr_15297/en/model_doc/visual_bert#transformers.VisualBertForPreTraining"),c(wS,"href","/docs/transformers/pr_15297/en/model_doc/vit_mae#transformers.ViTMAEForPreTraining"),c(AS,"href","/docs/transformers/pr_15297/en/model_doc/wav2vec2#transformers.Wav2Vec2ForPreTraining"),c(LS,"href","/docs/transformers/pr_15297/en/model_doc/xlm#transformers.XLMWithLMHeadModel"),c(BS,"href","/docs/transformers/pr_15297/en/model_doc/xlm-roberta#transformers.XLMRobertaForMaskedLM"),c(kS,"href","/docs/transformers/pr_15297/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForMaskedLM"),c(xS,"href","/docs/transformers/pr_15297/en/model_doc/xlnet#transformers.XLNetLMHeadModel"),c(xe,"class","docstring"),c(Wo,"class","docstring"),c(D_,"id","transformers.AutoModelForCausalLM"),c(D_,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(D_,"href","#transformers.AutoModelForCausalLM"),c(Qi,"class","relative group"),c(qr,"class","docstring"),c(RS,"href","/docs/transformers/pr_15297/en/model_doc/bart#transformers.BartForCausalLM"),c(SS,"href","/docs/transformers/pr_15297/en/model_doc/bert#transformers.BertLMHeadModel"),c(PS,"href","/docs/transformers/pr_15297/en/model_doc/bert-generation#transformers.BertGenerationDecoder"),c($S,"href","/docs/transformers/pr_15297/en/model_doc/big_bird#transformers.BigBirdForCausalLM"),c(IS,"href","/docs/transformers/pr_15297/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForCausalLM"),c(jS,"href","/docs/transformers/pr_15297/en/model_doc/blenderbot#transformers.BlenderbotForCausalLM"),c(NS,"href","/docs/transformers/pr_15297/en/model_doc/blenderbot-small#transformers.BlenderbotSmallForCausalLM"),c(DS,"href","/docs/transformers/pr_15297/en/model_doc/camembert#transformers.CamembertForCausalLM"),c(qS,"href","/docs/transformers/pr_15297/en/model_doc/ctrl#transformers.CTRLLMHeadModel"),c(GS,"href","/docs/transformers/pr_15297/en/model_doc/electra#transformers.ElectraForCausalLM"),c(OS,"href","/docs/transformers/pr_15297/en/model_doc/gpt2#transformers.GPT2LMHeadModel"),c(XS,"href","/docs/transformers/pr_15297/en/model_doc/gpt_neo#transformers.GPTNeoForCausalLM"),c(zS,"href","/docs/transformers/pr_15297/en/model_doc/gptj#transformers.GPTJForCausalLM"),c(VS,"href","/docs/transformers/pr_15297/en/model_doc/marian#transformers.MarianForCausalLM"),c(WS,"href","/docs/transformers/pr_15297/en/model_doc/mbart#transformers.MBartForCausalLM"),c(QS,"href","/docs/transformers/pr_15297/en/model_doc/megatron-bert#transformers.MegatronBertForCausalLM"),c(HS,"href","/docs/transformers/pr_15297/en/model_doc/openai-gpt#transformers.OpenAIGPTLMHeadModel"),c(US,"href","/docs/transformers/pr_15297/en/model_doc/pegasus#transformers.PegasusForCausalLM"),c(JS,"href","/docs/transformers/pr_15297/en/model_doc/plbart#transformers.PLBartForCausalLM"),c(YS,"href","/docs/transformers/pr_15297/en/model_doc/prophetnet#transformers.ProphetNetForCausalLM"),c(KS,"href","/docs/transformers/pr_15297/en/model_doc/qdqbert#transformers.QDQBertLMHeadModel"),c(ZS,"href","/docs/transformers/pr_15297/en/model_doc/reformer#transformers.ReformerModelWithLMHead"),c(eP,"href","/docs/transformers/pr_15297/en/model_doc/rembert#transformers.RemBertForCausalLM"),c(oP,"href","/docs/transformers/pr_15297/en/model_doc/roberta#transformers.RobertaForCausalLM"),c(rP,"href","/docs/transformers/pr_15297/en/model_doc/roformer#transformers.RoFormerForCausalLM"),c(tP,"href","/docs/transformers/pr_15297/en/model_doc/speech_to_text_2#transformers.Speech2Text2ForCausalLM"),c(aP,"href","/docs/transformers/pr_15297/en/model_doc/transfo-xl#transformers.TransfoXLLMHeadModel"),c(nP,"href","/docs/transformers/pr_15297/en/model_doc/trocr#transformers.TrOCRForCausalLM"),c(sP,"href","/docs/transformers/pr_15297/en/model_doc/xglm#transformers.XGLMForCausalLM"),c(lP,"href","/docs/transformers/pr_15297/en/model_doc/xlm#transformers.XLMWithLMHeadModel"),c(iP,"href","/docs/transformers/pr_15297/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetForCausalLM"),c(dP,"href","/docs/transformers/pr_15297/en/model_doc/xlm-roberta#transformers.XLMRobertaForCausalLM"),c(cP,"href","/docs/transformers/pr_15297/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForCausalLM"),c(fP,"href","/docs/transformers/pr_15297/en/model_doc/xlnet#transformers.XLNetLMHeadModel"),c(Re,"class","docstring"),c(Qo,"class","docstring"),c(Fu,"id","transformers.AutoModelForMaskedLM"),c(Fu,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Fu,"href","#transformers.AutoModelForMaskedLM"),c(Ji,"class","relative group"),c(Gr,"class","docstring"),c(mP,"href","/docs/transformers/pr_15297/en/model_doc/albert#transformers.AlbertForMaskedLM"),c(gP,"href","/docs/transformers/pr_15297/en/model_doc/bart#transformers.BartForConditionalGeneration"),c(hP,"href","/docs/transformers/pr_15297/en/model_doc/bert#transformers.BertForMaskedLM"),c(pP,"href","/docs/transformers/pr_15297/en/model_doc/big_bird#transformers.BigBirdForMaskedLM"),c(_P,"href","/docs/transformers/pr_15297/en/model_doc/camembert#transformers.CamembertForMaskedLM"),c(uP,"href","/docs/transformers/pr_15297/en/model_doc/convbert#transformers.ConvBertForMaskedLM"),c(bP,"href","/docs/transformers/pr_15297/en/model_doc/deberta#transformers.DebertaForMaskedLM"),c(vP,"href","/docs/transformers/pr_15297/en/model_doc/deberta-v2#transformers.DebertaV2ForMaskedLM"),c(TP,"href","/docs/transformers/pr_15297/en/model_doc/distilbert#transformers.DistilBertForMaskedLM"),c(FP,"href","/docs/transformers/pr_15297/en/model_doc/electra#transformers.ElectraForMaskedLM"),c(CP,"href","/docs/transformers/pr_15297/en/model_doc/flaubert#transformers.FlaubertWithLMHeadModel"),c(MP,"href","/docs/transformers/pr_15297/en/model_doc/fnet#transformers.FNetForMaskedLM"),c(EP,"href","/docs/transformers/pr_15297/en/model_doc/funnel#transformers.FunnelForMaskedLM"),c(yP,"href","/docs/transformers/pr_15297/en/model_doc/ibert#transformers.IBertForMaskedLM"),c(wP,"href","/docs/transformers/pr_15297/en/model_doc/layoutlm#transformers.LayoutLMForMaskedLM"),c(AP,"href","/docs/transformers/pr_15297/en/model_doc/longformer#transformers.LongformerForMaskedLM"),c(LP,"href","/docs/transformers/pr_15297/en/model_doc/mbart#transformers.MBartForConditionalGeneration"),c(BP,"href","/docs/transformers/pr_15297/en/model_doc/megatron-bert#transformers.MegatronBertForMaskedLM"),c(kP,"href","/docs/transformers/pr_15297/en/model_doc/mobilebert#transformers.MobileBertForMaskedLM"),c(xP,"href","/docs/transformers/pr_15297/en/model_doc/mpnet#transformers.MPNetForMaskedLM"),c(RP,"href","/docs/transformers/pr_15297/en/model_doc/nystromformer#transformers.NystromformerForMaskedLM"),c(SP,"href","/docs/transformers/pr_15297/en/model_doc/perceiver#transformers.PerceiverForMaskedLM"),c(PP,"href","/docs/transformers/pr_15297/en/model_doc/qdqbert#transformers.QDQBertForMaskedLM"),c($P,"href","/docs/transformers/pr_15297/en/model_doc/reformer#transformers.ReformerForMaskedLM"),c(IP,"href","/docs/transformers/pr_15297/en/model_doc/rembert#transformers.RemBertForMaskedLM"),c(jP,"href","/docs/transformers/pr_15297/en/model_doc/roberta#transformers.RobertaForMaskedLM"),c(NP,"href","/docs/transformers/pr_15297/en/model_doc/roformer#transformers.RoFormerForMaskedLM"),c(DP,"href","/docs/transformers/pr_15297/en/model_doc/squeezebert#transformers.SqueezeBertForMaskedLM"),c(qP,"href","/docs/transformers/pr_15297/en/model_doc/tapas#transformers.TapasForMaskedLM"),c(GP,"href","/docs/transformers/pr_15297/en/model_doc/xlm#transformers.XLMWithLMHeadModel"),c(OP,"href","/docs/transformers/pr_15297/en/model_doc/xlm-roberta#transformers.XLMRobertaForMaskedLM"),c(XP,"href","/docs/transformers/pr_15297/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForMaskedLM"),c(zP,"href","/docs/transformers/pr_15297/en/model_doc/yoso#transformers.YosoForMaskedLM"),c(Se,"class","docstring"),c(Ho,"class","docstring"),c(t2,"id","transformers.AutoModelForSeq2SeqLM"),c(t2,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(t2,"href","#transformers.AutoModelForSeq2SeqLM"),c(Zi,"class","relative group"),c(Or,"class","docstring"),c(VP,"href","/docs/transformers/pr_15297/en/model_doc/bart#transformers.BartForConditionalGeneration"),c(WP,"href","/docs/transformers/pr_15297/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForConditionalGeneration"),c(QP,"href","/docs/transformers/pr_15297/en/model_doc/blenderbot#transformers.BlenderbotForConditionalGeneration"),c(HP,"href","/docs/transformers/pr_15297/en/model_doc/blenderbot-small#transformers.BlenderbotSmallForConditionalGeneration"),c(UP,"href","/docs/transformers/pr_15297/en/model_doc/encoder-decoder#transformers.EncoderDecoderModel"),c(JP,"href","/docs/transformers/pr_15297/en/model_doc/fsmt#transformers.FSMTForConditionalGeneration"),c(YP,"href","/docs/transformers/pr_15297/en/model_doc/led#transformers.LEDForConditionalGeneration"),c(KP,"href","/docs/transformers/pr_15297/en/model_doc/m2m_100#transformers.M2M100ForConditionalGeneration"),c(ZP,"href","/docs/transformers/pr_15297/en/model_doc/marian#transformers.MarianMTModel"),c(e$,"href","/docs/transformers/pr_15297/en/model_doc/mbart#transformers.MBartForConditionalGeneration"),c(o$,"href","/docs/transformers/pr_15297/en/model_doc/mt5#transformers.MT5ForConditionalGeneration"),c(r$,"href","/docs/transformers/pr_15297/en/model_doc/pegasus#transformers.PegasusForConditionalGeneration"),c(t$,"href","/docs/transformers/pr_15297/en/model_doc/plbart#transformers.PLBartForConditionalGeneration"),c(a$,"href","/docs/transformers/pr_15297/en/model_doc/prophetnet#transformers.ProphetNetForConditionalGeneration"),c(n$,"href","/docs/transformers/pr_15297/en/model_doc/t5#transformers.T5ForConditionalGeneration"),c(s$,"href","/docs/transformers/pr_15297/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetForConditionalGeneration"),c(Pe,"class","docstring"),c(Uo,"class","docstring"),c(F2,"id","transformers.AutoModelForSequenceClassification"),c(F2,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(F2,"href","#transformers.AutoModelForSequenceClassification"),c(rd,"class","relative group"),c(Xr,"class","docstring"),c(l$,"href","/docs/transformers/pr_15297/en/model_doc/albert#transformers.AlbertForSequenceClassification"),c(i$,"href","/docs/transformers/pr_15297/en/model_doc/bart#transformers.BartForSequenceClassification"),c(d$,"href","/docs/transformers/pr_15297/en/model_doc/bert#transformers.BertForSequenceClassification"),c(c$,"href","/docs/transformers/pr_15297/en/model_doc/big_bird#transformers.BigBirdForSequenceClassification"),c(f$,"href","/docs/transformers/pr_15297/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForSequenceClassification"),c(m$,"href","/docs/transformers/pr_15297/en/model_doc/camembert#transformers.CamembertForSequenceClassification"),c(g$,"href","/docs/transformers/pr_15297/en/model_doc/canine#transformers.CanineForSequenceClassification"),c(h$,"href","/docs/transformers/pr_15297/en/model_doc/convbert#transformers.ConvBertForSequenceClassification"),c(p$,"href","/docs/transformers/pr_15297/en/model_doc/ctrl#transformers.CTRLForSequenceClassification"),c(_$,"href","/docs/transformers/pr_15297/en/model_doc/deberta#transformers.DebertaForSequenceClassification"),c(u$,"href","/docs/transformers/pr_15297/en/model_doc/deberta-v2#transformers.DebertaV2ForSequenceClassification"),c(b$,"href","/docs/transformers/pr_15297/en/model_doc/distilbert#transformers.DistilBertForSequenceClassification"),c(v$,"href","/docs/transformers/pr_15297/en/model_doc/electra#transformers.ElectraForSequenceClassification"),c(T$,"href","/docs/transformers/pr_15297/en/model_doc/flaubert#transformers.FlaubertForSequenceClassification"),c(F$,"href","/docs/transformers/pr_15297/en/model_doc/fnet#transformers.FNetForSequenceClassification"),c(C$,"href","/docs/transformers/pr_15297/en/model_doc/funnel#transformers.FunnelForSequenceClassification"),c(M$,"href","/docs/transformers/pr_15297/en/model_doc/gpt2#transformers.GPT2ForSequenceClassification"),c(E$,"href","/docs/transformers/pr_15297/en/model_doc/gpt_neo#transformers.GPTNeoForSequenceClassification"),c(y$,"href","/docs/transformers/pr_15297/en/model_doc/gptj#transformers.GPTJForSequenceClassification"),c(w$,"href","/docs/transformers/pr_15297/en/model_doc/ibert#transformers.IBertForSequenceClassification"),c(A$,"href","/docs/transformers/pr_15297/en/model_doc/layoutlm#transformers.LayoutLMForSequenceClassification"),c(L$,"href","/docs/transformers/pr_15297/en/model_doc/layoutlmv2#transformers.LayoutLMv2ForSequenceClassification"),c(B$,"href","/docs/transformers/pr_15297/en/model_doc/led#transformers.LEDForSequenceClassification"),c(k$,"href","/docs/transformers/pr_15297/en/model_doc/longformer#transformers.LongformerForSequenceClassification"),c(x$,"href","/docs/transformers/pr_15297/en/model_doc/mbart#transformers.MBartForSequenceClassification"),c(R$,"href","/docs/transformers/pr_15297/en/model_doc/megatron-bert#transformers.MegatronBertForSequenceClassification"),c(S$,"href","/docs/transformers/pr_15297/en/model_doc/mobilebert#transformers.MobileBertForSequenceClassification"),c(P$,"href","/docs/transformers/pr_15297/en/model_doc/mpnet#transformers.MPNetForSequenceClassification"),c($$,"href","/docs/transformers/pr_15297/en/model_doc/nystromformer#transformers.NystromformerForSequenceClassification"),c(I$,"href","/docs/transformers/pr_15297/en/model_doc/openai-gpt#transformers.OpenAIGPTForSequenceClassification"),c(j$,"href","/docs/transformers/pr_15297/en/model_doc/perceiver#transformers.PerceiverForSequenceClassification"),c(N$,"href","/docs/transformers/pr_15297/en/model_doc/plbart#transformers.PLBartForSequenceClassification"),c(D$,"href","/docs/transformers/pr_15297/en/model_doc/qdqbert#transformers.QDQBertForSequenceClassification"),c(q$,"href","/docs/transformers/pr_15297/en/model_doc/reformer#transformers.ReformerForSequenceClassification"),c(G$,"href","/docs/transformers/pr_15297/en/model_doc/rembert#transformers.RemBertForSequenceClassification"),c(O$,"href","/docs/transformers/pr_15297/en/model_doc/roberta#transformers.RobertaForSequenceClassification"),c(X$,"href","/docs/transformers/pr_15297/en/model_doc/roformer#transformers.RoFormerForSequenceClassification"),c(z$,"href","/docs/transformers/pr_15297/en/model_doc/squeezebert#transformers.SqueezeBertForSequenceClassification"),c(V$,"href","/docs/transformers/pr_15297/en/model_doc/tapas#transformers.TapasForSequenceClassification"),c(W$,"href","/docs/transformers/pr_15297/en/model_doc/transfo-xl#transformers.TransfoXLForSequenceClassification"),c(Q$,"href","/docs/transformers/pr_15297/en/model_doc/xlm#transformers.XLMForSequenceClassification"),c(H$,"href","/docs/transformers/pr_15297/en/model_doc/xlm-roberta#transformers.XLMRobertaForSequenceClassification"),c(U$,"href","/docs/transformers/pr_15297/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForSequenceClassification"),c(J$,"href","/docs/transformers/pr_15297/en/model_doc/xlnet#transformers.XLNetForSequenceClassification"),c(Y$,"href","/docs/transformers/pr_15297/en/model_doc/yoso#transformers.YosoForSequenceClassification"),c($e,"class","docstring"),c(Jo,"class","docstring"),c(h1,"id","transformers.AutoModelForMultipleChoice"),c(h1,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(h1,"href","#transformers.AutoModelForMultipleChoice"),c(nd,"class","relative group"),c(zr,"class","docstring"),c(K$,"href","/docs/transformers/pr_15297/en/model_doc/albert#transformers.AlbertForMultipleChoice"),c(Z$,"href","/docs/transformers/pr_15297/en/model_doc/bert#transformers.BertForMultipleChoice"),c(eI,"href","/docs/transformers/pr_15297/en/model_doc/big_bird#transformers.BigBirdForMultipleChoice"),c(oI,"href","/docs/transformers/pr_15297/en/model_doc/camembert#transformers.CamembertForMultipleChoice"),c(rI,"href","/docs/transformers/pr_15297/en/model_doc/canine#transformers.CanineForMultipleChoice"),c(tI,"href","/docs/transformers/pr_15297/en/model_doc/convbert#transformers.ConvBertForMultipleChoice"),c(aI,"href","/docs/transformers/pr_15297/en/model_doc/distilbert#transformers.DistilBertForMultipleChoice"),c(nI,"href","/docs/transformers/pr_15297/en/model_doc/electra#transformers.ElectraForMultipleChoice"),c(sI,"href","/docs/transformers/pr_15297/en/model_doc/flaubert#transformers.FlaubertForMultipleChoice"),c(lI,"href","/docs/transformers/pr_15297/en/model_doc/fnet#transformers.FNetForMultipleChoice"),c(iI,"href","/docs/transformers/pr_15297/en/model_doc/funnel#transformers.FunnelForMultipleChoice"),c(dI,"href","/docs/transformers/pr_15297/en/model_doc/ibert#transformers.IBertForMultipleChoice"),c(cI,"href","/docs/transformers/pr_15297/en/model_doc/longformer#transformers.LongformerForMultipleChoice"),c(fI,"href","/docs/transformers/pr_15297/en/model_doc/megatron-bert#transformers.MegatronBertForMultipleChoice"),c(mI,"href","/docs/transformers/pr_15297/en/model_doc/mobilebert#transformers.MobileBertForMultipleChoice"),c(gI,"href","/docs/transformers/pr_15297/en/model_doc/mpnet#transformers.MPNetForMultipleChoice"),c(hI,"href","/docs/transformers/pr_15297/en/model_doc/nystromformer#transformers.NystromformerForMultipleChoice"),c(pI,"href","/docs/transformers/pr_15297/en/model_doc/qdqbert#transformers.QDQBertForMultipleChoice"),c(_I,"href","/docs/transformers/pr_15297/en/model_doc/rembert#transformers.RemBertForMultipleChoice"),c(uI,"href","/docs/transformers/pr_15297/en/model_doc/roberta#transformers.RobertaForMultipleChoice"),c(bI,"href","/docs/transformers/pr_15297/en/model_doc/roformer#transformers.RoFormerForMultipleChoice"),c(vI,"href","/docs/transformers/pr_15297/en/model_doc/squeezebert#transformers.SqueezeBertForMultipleChoice"),c(TI,"href","/docs/transformers/pr_15297/en/model_doc/xlm#transformers.XLMForMultipleChoice"),c(FI,"href","/docs/transformers/pr_15297/en/model_doc/xlm-roberta#transformers.XLMRobertaForMultipleChoice"),c(CI,"href","/docs/transformers/pr_15297/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForMultipleChoice"),c(MI,"href","/docs/transformers/pr_15297/en/model_doc/xlnet#transformers.XLNetForMultipleChoice"),c(EI,"href","/docs/transformers/pr_15297/en/model_doc/yoso#transformers.YosoForMultipleChoice"),c(Ie,"class","docstring"),c(Yo,"class","docstring"),c(X1,"id","transformers.AutoModelForNextSentencePrediction"),c(X1,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(X1,"href","#transformers.AutoModelForNextSentencePrediction"),c(id,"class","relative group"),c(Vr,"class","docstring"),c(yI,"href","/docs/transformers/pr_15297/en/model_doc/bert#transformers.BertForNextSentencePrediction"),c(wI,"href","/docs/transformers/pr_15297/en/model_doc/fnet#transformers.FNetForNextSentencePrediction"),c(AI,"href","/docs/transformers/pr_15297/en/model_doc/megatron-bert#transformers.MegatronBertForNextSentencePrediction"),c(LI,"href","/docs/transformers/pr_15297/en/model_doc/mobilebert#transformers.MobileBertForNextSentencePrediction"),c(BI,"href","/docs/transformers/pr_15297/en/model_doc/qdqbert#transformers.QDQBertForNextSentencePrediction"),c(je,"class","docstring"),c(Ko,"class","docstring"),c(J1,"id","transformers.AutoModelForTokenClassification"),c(J1,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(J1,"href","#transformers.AutoModelForTokenClassification"),c(fd,"class","relative group"),c(Wr,"class","docstring"),c(kI,"href","/docs/transformers/pr_15297/en/model_doc/albert#transformers.AlbertForTokenClassification"),c(xI,"href","/docs/transformers/pr_15297/en/model_doc/bert#transformers.BertForTokenClassification"),c(RI,"href","/docs/transformers/pr_15297/en/model_doc/big_bird#transformers.BigBirdForTokenClassification"),c(SI,"href","/docs/transformers/pr_15297/en/model_doc/camembert#transformers.CamembertForTokenClassification"),c(PI,"href","/docs/transformers/pr_15297/en/model_doc/canine#transformers.CanineForTokenClassification"),c($I,"href","/docs/transformers/pr_15297/en/model_doc/convbert#transformers.ConvBertForTokenClassification"),c(II,"href","/docs/transformers/pr_15297/en/model_doc/deberta#transformers.DebertaForTokenClassification"),c(jI,"href","/docs/transformers/pr_15297/en/model_doc/deberta-v2#transformers.DebertaV2ForTokenClassification"),c(NI,"href","/docs/transformers/pr_15297/en/model_doc/distilbert#transformers.DistilBertForTokenClassification"),c(DI,"href","/docs/transformers/pr_15297/en/model_doc/electra#transformers.ElectraForTokenClassification"),c(qI,"href","/docs/transformers/pr_15297/en/model_doc/flaubert#transformers.FlaubertForTokenClassification"),c(GI,"href","/docs/transformers/pr_15297/en/model_doc/fnet#transformers.FNetForTokenClassification"),c(OI,"href","/docs/transformers/pr_15297/en/model_doc/funnel#transformers.FunnelForTokenClassification"),c(XI,"href","/docs/transformers/pr_15297/en/model_doc/gpt2#transformers.GPT2ForTokenClassification"),c(zI,"href","/docs/transformers/pr_15297/en/model_doc/ibert#transformers.IBertForTokenClassification"),c(VI,"href","/docs/transformers/pr_15297/en/model_doc/layoutlm#transformers.LayoutLMForTokenClassification"),c(WI,"href","/docs/transformers/pr_15297/en/model_doc/layoutlmv2#transformers.LayoutLMv2ForTokenClassification"),c(QI,"href","/docs/transformers/pr_15297/en/model_doc/longformer#transformers.LongformerForTokenClassification"),c(HI,"href","/docs/transformers/pr_15297/en/model_doc/megatron-bert#transformers.MegatronBertForTokenClassification"),c(UI,"href","/docs/transformers/pr_15297/en/model_doc/mobilebert#transformers.MobileBertForTokenClassification"),c(JI,"href","/docs/transformers/pr_15297/en/model_doc/mpnet#transformers.MPNetForTokenClassification"),c(YI,"href","/docs/transformers/pr_15297/en/model_doc/nystromformer#transformers.NystromformerForTokenClassification"),c(KI,"href","/docs/transformers/pr_15297/en/model_doc/qdqbert#transformers.QDQBertForTokenClassification"),c(ZI,"href","/docs/transformers/pr_15297/en/model_doc/rembert#transformers.RemBertForTokenClassification"),c(ej,"href","/docs/transformers/pr_15297/en/model_doc/roberta#transformers.RobertaForTokenClassification"),c(oj,"href","/docs/transformers/pr_15297/en/model_doc/roformer#transformers.RoFormerForTokenClassification"),c(rj,"href","/docs/transformers/pr_15297/en/model_doc/squeezebert#transformers.SqueezeBertForTokenClassification"),c(tj,"href","/docs/transformers/pr_15297/en/model_doc/xlm#transformers.XLMForTokenClassification"),c(aj,"href","/docs/transformers/pr_15297/en/model_doc/xlm-roberta#transformers.XLMRobertaForTokenClassification"),c(nj,"href","/docs/transformers/pr_15297/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForTokenClassification"),c(sj,"href","/docs/transformers/pr_15297/en/model_doc/xlnet#transformers.XLNetForTokenClassification"),c(lj,"href","/docs/transformers/pr_15297/en/model_doc/yoso#transformers.YosoForTokenClassification"),c(Ne,"class","docstring"),c(Zo,"class","docstring"),c(kb,"id","transformers.AutoModelForQuestionAnswering"),c(kb,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(kb,"href","#transformers.AutoModelForQuestionAnswering"),c(hd,"class","relative group"),c(Qr,"class","docstring"),c(ij,"href","/docs/transformers/pr_15297/en/model_doc/albert#transformers.AlbertForQuestionAnswering"),c(dj,"href","/docs/transformers/pr_15297/en/model_doc/bart#transformers.BartForQuestionAnswering"),c(cj,"href","/docs/transformers/pr_15297/en/model_doc/bert#transformers.BertForQuestionAnswering"),c(fj,"href","/docs/transformers/pr_15297/en/model_doc/big_bird#transformers.BigBirdForQuestionAnswering"),c(mj,"href","/docs/transformers/pr_15297/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForQuestionAnswering"),c(gj,"href","/docs/transformers/pr_15297/en/model_doc/camembert#transformers.CamembertForQuestionAnswering"),c(hj,"href","/docs/transformers/pr_15297/en/model_doc/canine#transformers.CanineForQuestionAnswering"),c(pj,"href","/docs/transformers/pr_15297/en/model_doc/convbert#transformers.ConvBertForQuestionAnswering"),c(_j,"href","/docs/transformers/pr_15297/en/model_doc/deberta#transformers.DebertaForQuestionAnswering"),c(uj,"href","/docs/transformers/pr_15297/en/model_doc/deberta-v2#transformers.DebertaV2ForQuestionAnswering"),c(bj,"href","/docs/transformers/pr_15297/en/model_doc/distilbert#transformers.DistilBertForQuestionAnswering"),c(vj,"href","/docs/transformers/pr_15297/en/model_doc/electra#transformers.ElectraForQuestionAnswering"),c(Tj,"href","/docs/transformers/pr_15297/en/model_doc/flaubert#transformers.FlaubertForQuestionAnsweringSimple"),c(Fj,"href","/docs/transformers/pr_15297/en/model_doc/fnet#transformers.FNetForQuestionAnswering"),c(Cj,"href","/docs/transformers/pr_15297/en/model_doc/funnel#transformers.FunnelForQuestionAnswering"),c(Mj,"href","/docs/transformers/pr_15297/en/model_doc/gptj#transformers.GPTJForQuestionAnswering"),c(Ej,"href","/docs/transformers/pr_15297/en/model_doc/ibert#transformers.IBertForQuestionAnswering"),c(yj,"href","/docs/transformers/pr_15297/en/model_doc/layoutlmv2#transformers.LayoutLMv2ForQuestionAnswering"),c(wj,"href","/docs/transformers/pr_15297/en/model_doc/led#transformers.LEDForQuestionAnswering"),c(Aj,"href","/docs/transformers/pr_15297/en/model_doc/longformer#transformers.LongformerForQuestionAnswering"),c(Lj,"href","/docs/transformers/pr_15297/en/model_doc/lxmert#transformers.LxmertForQuestionAnswering"),c(Bj,"href","/docs/transformers/pr_15297/en/model_doc/mbart#transformers.MBartForQuestionAnswering"),c(kj,"href","/docs/transformers/pr_15297/en/model_doc/megatron-bert#transformers.MegatronBertForQuestionAnswering"),c(xj,"href","/docs/transformers/pr_15297/en/model_doc/mobilebert#transformers.MobileBertForQuestionAnswering"),c(Rj,"href","/docs/transformers/pr_15297/en/model_doc/mpnet#transformers.MPNetForQuestionAnswering"),c(Sj,"href","/docs/transformers/pr_15297/en/model_doc/nystromformer#transformers.NystromformerForQuestionAnswering"),c(Pj,"href","/docs/transformers/pr_15297/en/model_doc/qdqbert#transformers.QDQBertForQuestionAnswering"),c($j,"href","/docs/transformers/pr_15297/en/model_doc/reformer#transformers.ReformerForQuestionAnswering"),c(Ij,"href","/docs/transformers/pr_15297/en/model_doc/rembert#transformers.RemBertForQuestionAnswering"),c(jj,"href","/docs/transformers/pr_15297/en/model_doc/roberta#transformers.RobertaForQuestionAnswering"),c(Nj,"href","/docs/transformers/pr_15297/en/model_doc/roformer#transformers.RoFormerForQuestionAnswering"),c(Dj,"href","/docs/transformers/pr_15297/en/model_doc/splinter#transformers.SplinterForQuestionAnswering"),c(qj,"href","/docs/transformers/pr_15297/en/model_doc/squeezebert#transformers.SqueezeBertForQuestionAnswering"),c(Gj,"href","/docs/transformers/pr_15297/en/model_doc/xlm#transformers.XLMForQuestionAnsweringSimple"),c(Oj,"href","/docs/transformers/pr_15297/en/model_doc/xlm-roberta#transformers.XLMRobertaForQuestionAnswering"),c(Xj,"href","/docs/transformers/pr_15297/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForQuestionAnswering"),c(zj,"href","/docs/transformers/pr_15297/en/model_doc/xlnet#transformers.XLNetForQuestionAnsweringSimple"),c(Vj,"href","/docs/transformers/pr_15297/en/model_doc/yoso#transformers.YosoForQuestionAnswering"),c(De,"class","docstring"),c(er,"class","docstring"),c(_5,"id","transformers.AutoModelForTableQuestionAnswering"),c(_5,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(_5,"href","#transformers.AutoModelForTableQuestionAnswering"),c(ud,"class","relative group"),c(Hr,"class","docstring"),c(Wj,"href","/docs/transformers/pr_15297/en/model_doc/tapas#transformers.TapasForQuestionAnswering"),c(qe,"class","docstring"),c(or,"class","docstring"),c(v5,"id","transformers.AutoModelForImageClassification"),c(v5,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(v5,"href","#transformers.AutoModelForImageClassification"),c(Td,"class","relative group"),c(Ur,"class","docstring"),c(Qj,"href","/docs/transformers/pr_15297/en/model_doc/beit#transformers.BeitForImageClassification"),c(Hj,"href","/docs/transformers/pr_15297/en/model_doc/convnext#transformers.ConvNextForImageClassification"),c(Uj,"href","/docs/transformers/pr_15297/en/model_doc/deit#transformers.DeiTForImageClassification"),c(Jj,"href","/docs/transformers/pr_15297/en/model_doc/deit#transformers.DeiTForImageClassificationWithTeacher"),c(Yj,"href","/docs/transformers/pr_15297/en/model_doc/imagegpt#transformers.ImageGPTForImageClassification"),c(Kj,"href","/docs/transformers/pr_15297/en/model_doc/perceiver#transformers.PerceiverForImageClassificationLearned"),c(Zj,"href","/docs/transformers/pr_15297/en/model_doc/perceiver#transformers.PerceiverForImageClassificationFourier"),c(eN,"href","/docs/transformers/pr_15297/en/model_doc/perceiver#transformers.PerceiverForImageClassificationConvProcessing"),c(oN,"href","/docs/transformers/pr_15297/en/model_doc/poolformer#transformers.PoolFormerForImageClassification"),c(rN,"href","/docs/transformers/pr_15297/en/model_doc/segformer#transformers.SegformerForImageClassification"),c(tN,"href","/docs/transformers/pr_15297/en/model_doc/swin#transformers.SwinForImageClassification"),c(aN,"href","/docs/transformers/pr_15297/en/model_doc/vit#transformers.ViTForImageClassification"),c(Ge,"class","docstring"),c(rr,"class","docstring"),c(L5,"id","transformers.AutoModelForVision2Seq"),c(L5,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(L5,"href","#transformers.AutoModelForVision2Seq"),c(Md,"class","relative group"),c(Jr,"class","docstring"),c(nN,"href","/docs/transformers/pr_15297/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderModel"),c(Oe,"class","docstring"),c(tr,"class","docstring"),c(x5,"id","transformers.AutoModelForAudioClassification"),c(x5,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(x5,"href","#transformers.AutoModelForAudioClassification"),c(wd,"class","relative group"),c(Yr,"class","docstring"),c(sN,"href","/docs/transformers/pr_15297/en/model_doc/hubert#transformers.HubertForSequenceClassification"),c(lN,"href","/docs/transformers/pr_15297/en/model_doc/sew#transformers.SEWForSequenceClassification"),c(iN,"href","/docs/transformers/pr_15297/en/model_doc/sew-d#transformers.SEWDForSequenceClassification"),c(dN,"href","/docs/transformers/pr_15297/en/model_doc/unispeech#transformers.UniSpeechForSequenceClassification"),c(cN,"href","/docs/transformers/pr_15297/en/model_doc/unispeech-sat#transformers.UniSpeechSatForSequenceClassification"),c(fN,"href","/docs/transformers/pr_15297/en/model_doc/wav2vec2#transformers.Wav2Vec2ForSequenceClassification"),c(mN,"href","/docs/transformers/pr_15297/en/model_doc/wavlm#transformers.WavLMForSequenceClassification"),c(Xe,"class","docstring"),c(ar,"class","docstring"),c(q5,"id","transformers.AutoModelForAudioFrameClassification"),c(q5,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(q5,"href","#transformers.AutoModelForAudioFrameClassification"),c(Bd,"class","relative group"),c(Kr,"class","docstring"),c(gN,"href","/docs/transformers/pr_15297/en/model_doc/unispeech-sat#transformers.UniSpeechSatForAudioFrameClassification"),c(hN,"href","/docs/transformers/pr_15297/en/model_doc/wav2vec2#transformers.Wav2Vec2ForAudioFrameClassification"),c(pN,"href","/docs/transformers/pr_15297/en/model_doc/wavlm#transformers.WavLMForAudioFrameClassification"),c(ze,"class","docstring"),c(nr,"class","docstring"),c(V5,"id","transformers.AutoModelForCTC"),c(V5,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(V5,"href","#transformers.AutoModelForCTC"),c(Sd,"class","relative group"),c(Zr,"class","docstring"),c(_N,"href","/docs/transformers/pr_15297/en/model_doc/hubert#transformers.HubertForCTC"),c(uN,"href","/docs/transformers/pr_15297/en/model_doc/sew#transformers.SEWForCTC"),c(bN,"href","/docs/transformers/pr_15297/en/model_doc/sew-d#transformers.SEWDForCTC"),c(vN,"href","/docs/transformers/pr_15297/en/model_doc/unispeech#transformers.UniSpeechForCTC"),c(TN,"href","/docs/transformers/pr_15297/en/model_doc/unispeech-sat#transformers.UniSpeechSatForCTC"),c(FN,"href","/docs/transformers/pr_15297/en/model_doc/wav2vec2#transformers.Wav2Vec2ForCTC"),c(CN,"href","/docs/transformers/pr_15297/en/model_doc/wavlm#transformers.WavLMForCTC"),c(Ve,"class","docstring"),c(sr,"class","docstring"),c(ev,"id","transformers.AutoModelForSpeechSeq2Seq"),c(ev,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(ev,"href","#transformers.AutoModelForSpeechSeq2Seq"),c(Id,"class","relative group"),c(et,"class","docstring"),c(MN,"href","/docs/transformers/pr_15297/en/model_doc/speech-encoder-decoder#transformers.SpeechEncoderDecoderModel"),c(EN,"href","/docs/transformers/pr_15297/en/model_doc/speech_to_text#transformers.Speech2TextForConditionalGeneration"),c(We,"class","docstring"),c(lr,"class","docstring"),c(av,"id","transformers.AutoModelForAudioXVector"),c(av,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(av,"href","#transformers.AutoModelForAudioXVector"),c(Dd,"class","relative group"),c(ot,"class","docstring"),c(yN,"href","/docs/transformers/pr_15297/en/model_doc/unispeech-sat#transformers.UniSpeechSatForXVector"),c(wN,"href","/docs/transformers/pr_15297/en/model_doc/wav2vec2#transformers.Wav2Vec2ForXVector"),c(AN,"href","/docs/transformers/pr_15297/en/model_doc/wavlm#transformers.WavLMForXVector"),c(Qe,"class","docstring"),c(ir,"class","docstring"),c(dv,"id","transformers.AutoModelForMaskedImageModeling"),c(dv,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(dv,"href","#transformers.AutoModelForMaskedImageModeling"),c(Xd,"class","relative group"),c(rt,"class","docstring"),c(LN,"href","/docs/transformers/pr_15297/en/model_doc/deit#transformers.DeiTForMaskedImageModeling"),c(BN,"href","/docs/transformers/pr_15297/en/model_doc/swin#transformers.SwinForMaskedImageModeling"),c(kN,"href","/docs/transformers/pr_15297/en/model_doc/vit#transformers.ViTForMaskedImageModeling"),c(He,"class","docstring"),c(dr,"class","docstring"),c(hv,"id","transformers.AutoModelForObjectDetection"),c(hv,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(hv,"href","#transformers.AutoModelForObjectDetection"),c(Qd,"class","relative group"),c(tt,"class","docstring"),c(xN,"href","/docs/transformers/pr_15297/en/model_doc/detr#transformers.DetrForObjectDetection"),c(Ue,"class","docstring"),c(cr,"class","docstring"),c(uv,"id","transformers.AutoModelForImageSegmentation"),c(uv,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(uv,"href","#transformers.AutoModelForImageSegmentation"),c(Jd,"class","relative group"),c(at,"class","docstring"),c(RN,"href","/docs/transformers/pr_15297/en/model_doc/detr#transformers.DetrForSegmentation"),c(Je,"class","docstring"),c(fr,"class","docstring"),c(Tv,"id","transformers.AutoModelForSemanticSegmentation"),c(Tv,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Tv,"href","#transformers.AutoModelForSemanticSegmentation"),c(Zd,"class","relative group"),c(nt,"class","docstring"),c(SN,"href","/docs/transformers/pr_15297/en/model_doc/beit#transformers.BeitForSemanticSegmentation"),c(PN,"href","/docs/transformers/pr_15297/en/model_doc/segformer#transformers.SegformerForSemanticSegmentation"),c(Ye,"class","docstring"),c(mr,"class","docstring"),c(Ev,"id","transformers.TFAutoModel"),c(Ev,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Ev,"href","#transformers.TFAutoModel"),c(rc,"class","relative group"),c(st,"class","docstring"),c($N,"href","/docs/transformers/pr_15297/en/model_doc/albert#transformers.TFAlbertModel"),c(IN,"href","/docs/transformers/pr_15297/en/model_doc/bart#transformers.TFBartModel"),c(jN,"href","/docs/transformers/pr_15297/en/model_doc/bert#transformers.TFBertModel"),c(NN,"href","/docs/transformers/pr_15297/en/model_doc/blenderbot#transformers.TFBlenderbotModel"),c(DN,"href","/docs/transformers/pr_15297/en/model_doc/blenderbot-small#transformers.TFBlenderbotSmallModel"),c(qN,"href","/docs/transformers/pr_15297/en/model_doc/camembert#transformers.TFCamembertModel"),c(GN,"href","/docs/transformers/pr_15297/en/model_doc/clip#transformers.TFCLIPModel"),c(ON,"href","/docs/transformers/pr_15297/en/model_doc/convbert#transformers.TFConvBertModel"),c(XN,"href","/docs/transformers/pr_15297/en/model_doc/ctrl#transformers.TFCTRLModel"),c(zN,"href","/docs/transformers/pr_15297/en/model_doc/deberta#transformers.TFDebertaModel"),c(VN,"href","/docs/transformers/pr_15297/en/model_doc/deberta-v2#transformers.TFDebertaV2Model"),c(WN,"href","/docs/transformers/pr_15297/en/model_doc/distilbert#transformers.TFDistilBertModel"),c(QN,"href","/docs/transformers/pr_15297/en/model_doc/dpr#transformers.TFDPRQuestionEncoder"),c(HN,"href","/docs/transformers/pr_15297/en/model_doc/electra#transformers.TFElectraModel"),c(UN,"href","/docs/transformers/pr_15297/en/model_doc/flaubert#transformers.TFFlaubertModel"),c(JN,"href","/docs/transformers/pr_15297/en/model_doc/funnel#transformers.TFFunnelModel"),c(YN,"href","/docs/transformers/pr_15297/en/model_doc/funnel#transformers.TFFunnelBaseModel"),c(KN,"href","/docs/transformers/pr_15297/en/model_doc/gpt2#transformers.TFGPT2Model"),c(ZN,"href","/docs/transformers/pr_15297/en/model_doc/hubert#transformers.TFHubertModel"),c(eD,"href","/docs/transformers/pr_15297/en/model_doc/layoutlm#transformers.TFLayoutLMModel"),c(oD,"href","/docs/transformers/pr_15297/en/model_doc/led#transformers.TFLEDModel"),c(rD,"href","/docs/transformers/pr_15297/en/model_doc/longformer#transformers.TFLongformerModel"),c(tD,"href","/docs/transformers/pr_15297/en/model_doc/lxmert#transformers.TFLxmertModel"),c(aD,"href","/docs/transformers/pr_15297/en/model_doc/marian#transformers.TFMarianModel"),c(nD,"href","/docs/transformers/pr_15297/en/model_doc/mbart#transformers.TFMBartModel"),c(sD,"href","/docs/transformers/pr_15297/en/model_doc/mobilebert#transformers.TFMobileBertModel"),c(lD,"href","/docs/transformers/pr_15297/en/model_doc/mpnet#transformers.TFMPNetModel"),c(iD,"href","/docs/transformers/pr_15297/en/model_doc/mt5#transformers.TFMT5Model"),c(dD,"href","/docs/transformers/pr_15297/en/model_doc/openai-gpt#transformers.TFOpenAIGPTModel"),c(cD,"href","/docs/transformers/pr_15297/en/model_doc/pegasus#transformers.TFPegasusModel"),c(fD,"href","/docs/transformers/pr_15297/en/model_doc/rembert#transformers.TFRemBertModel"),c(mD,"href","/docs/transformers/pr_15297/en/model_doc/roberta#transformers.TFRobertaModel"),c(gD,"href","/docs/transformers/pr_15297/en/model_doc/roformer#transformers.TFRoFormerModel"),c(hD,"href","/docs/transformers/pr_15297/en/model_doc/speech_to_text#transformers.TFSpeech2TextModel"),c(pD,"href","/docs/transformers/pr_15297/en/model_doc/t5#transformers.TFT5Model"),c(_D,"href","/docs/transformers/pr_15297/en/model_doc/tapas#transformers.TFTapasModel"),c(uD,"href","/docs/transformers/pr_15297/en/model_doc/transfo-xl#transformers.TFTransfoXLModel"),c(bD,"href","/docs/transformers/pr_15297/en/model_doc/vit#transformers.TFViTModel"),c(vD,"href","/docs/transformers/pr_15297/en/model_doc/wav2vec2#transformers.TFWav2Vec2Model"),c(TD,"href","/docs/transformers/pr_15297/en/model_doc/xlm#transformers.TFXLMModel"),c(FD,"href","/docs/transformers/pr_15297/en/model_doc/xlm-roberta#transformers.TFXLMRobertaModel"),c(CD,"href","/docs/transformers/pr_15297/en/model_doc/xlnet#transformers.TFXLNetModel"),c(go,"class","docstring"),c(gr,"class","docstring"),c(fT,"id","transformers.TFAutoModelForPreTraining"),c(fT,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(fT,"href","#transformers.TFAutoModelForPreTraining"),c(nc,"class","relative group"),c(lt,"class","docstring"),c(MD,"href","/docs/transformers/pr_15297/en/model_doc/albert#transformers.TFAlbertForPreTraining"),c(ED,"href","/docs/transformers/pr_15297/en/model_doc/bart#transformers.TFBartForConditionalGeneration"),c(yD,"href","/docs/transformers/pr_15297/en/model_doc/bert#transformers.TFBertForPreTraining"),c(wD,"href","/docs/transformers/pr_15297/en/model_doc/camembert#transformers.TFCamembertForMaskedLM"),c(AD,"href","/docs/transformers/pr_15297/en/model_doc/ctrl#transformers.TFCTRLLMHeadModel"),c(LD,"href","/docs/transformers/pr_15297/en/model_doc/distilbert#transformers.TFDistilBertForMaskedLM"),c(BD,"href","/docs/transformers/pr_15297/en/model_doc/electra#transformers.TFElectraForPreTraining"),c(kD,"href","/docs/transformers/pr_15297/en/model_doc/flaubert#transformers.TFFlaubertWithLMHeadModel"),c(xD,"href","/docs/transformers/pr_15297/en/model_doc/funnel#transformers.TFFunnelForPreTraining"),c(RD,"href","/docs/transformers/pr_15297/en/model_doc/gpt2#transformers.TFGPT2LMHeadModel"),c(SD,"href","/docs/transformers/pr_15297/en/model_doc/layoutlm#transformers.TFLayoutLMForMaskedLM"),c(PD,"href","/docs/transformers/pr_15297/en/model_doc/lxmert#transformers.TFLxmertForPreTraining"),c($D,"href","/docs/transformers/pr_15297/en/model_doc/mobilebert#transformers.TFMobileBertForPreTraining"),c(ID,"href","/docs/transformers/pr_15297/en/model_doc/mpnet#transformers.TFMPNetForMaskedLM"),c(jD,"href","/docs/transformers/pr_15297/en/model_doc/openai-gpt#transformers.TFOpenAIGPTLMHeadModel"),c(ND,"href","/docs/transformers/pr_15297/en/model_doc/roberta#transformers.TFRobertaForMaskedLM"),c(DD,"href","/docs/transformers/pr_15297/en/model_doc/t5#transformers.TFT5ForConditionalGeneration"),c(qD,"href","/docs/transformers/pr_15297/en/model_doc/tapas#transformers.TFTapasForMaskedLM"),c(GD,"href","/docs/transformers/pr_15297/en/model_doc/transfo-xl#transformers.TFTransfoXLLMHeadModel"),c(OD,"href","/docs/transformers/pr_15297/en/model_doc/xlm#transformers.TFXLMWithLMHeadModel"),c(XD,"href","/docs/transformers/pr_15297/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForMaskedLM"),c(zD,"href","/docs/transformers/pr_15297/en/model_doc/xlnet#transformers.TFXLNetLMHeadModel"),c(ho,"class","docstring"),c(hr,"class","docstring"),c(PT,"id","transformers.TFAutoModelForCausalLM"),c(PT,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(PT,"href","#transformers.TFAutoModelForCausalLM"),c(ic,"class","relative group"),c(it,"class","docstring"),c(VD,"href","/docs/transformers/pr_15297/en/model_doc/bert#transformers.TFBertLMHeadModel"),c(WD,"href","/docs/transformers/pr_15297/en/model_doc/ctrl#transformers.TFCTRLLMHeadModel"),c(QD,"href","/docs/transformers/pr_15297/en/model_doc/gpt2#transformers.TFGPT2LMHeadModel"),c(HD,"href","/docs/transformers/pr_15297/en/model_doc/openai-gpt#transformers.TFOpenAIGPTLMHeadModel"),c(UD,"href","/docs/transformers/pr_15297/en/model_doc/rembert#transformers.TFRemBertForCausalLM"),c(JD,"href","/docs/transformers/pr_15297/en/model_doc/roberta#transformers.TFRobertaForCausalLM"),c(YD,"href","/docs/transformers/pr_15297/en/model_doc/roformer#transformers.TFRoFormerForCausalLM"),c(KD,"href","/docs/transformers/pr_15297/en/model_doc/transfo-xl#transformers.TFTransfoXLLMHeadModel"),c(ZD,"href","/docs/transformers/pr_15297/en/model_doc/xlm#transformers.TFXLMWithLMHeadModel"),c(eq,"href","/docs/transformers/pr_15297/en/model_doc/xlnet#transformers.TFXLNetLMHeadModel"),c(po,"class","docstring"),c(pr,"class","docstring"),c(VT,"id","transformers.TFAutoModelForImageClassification"),c(VT,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(VT,"href","#transformers.TFAutoModelForImageClassification"),c(fc,"class","relative group"),c(dt,"class","docstring"),c(oq,"href","/docs/transformers/pr_15297/en/model_doc/vit#transformers.TFViTForImageClassification"),c(_o,"class","docstring"),c(_r,"class","docstring"),c(QT,"id","transformers.TFAutoModelForMaskedLM"),c(QT,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(QT,"href","#transformers.TFAutoModelForMaskedLM"),c(hc,"class","relative group"),c(ct,"class","docstring"),c(rq,"href","/docs/transformers/pr_15297/en/model_doc/albert#transformers.TFAlbertForMaskedLM"),c(tq,"href","/docs/transformers/pr_15297/en/model_doc/bert#transformers.TFBertForMaskedLM"),c(aq,"href","/docs/transformers/pr_15297/en/model_doc/camembert#transformers.TFCamembertForMaskedLM"),c(nq,"href","/docs/transformers/pr_15297/en/model_doc/convbert#transformers.TFConvBertForMaskedLM"),c(sq,"href","/docs/transformers/pr_15297/en/model_doc/deberta#transformers.TFDebertaForMaskedLM"),c(lq,"href","/docs/transformers/pr_15297/en/model_doc/deberta-v2#transformers.TFDebertaV2ForMaskedLM"),c(iq,"href","/docs/transformers/pr_15297/en/model_doc/distilbert#transformers.TFDistilBertForMaskedLM"),c(dq,"href","/docs/transformers/pr_15297/en/model_doc/electra#transformers.TFElectraForMaskedLM"),c(cq,"href","/docs/transformers/pr_15297/en/model_doc/flaubert#transformers.TFFlaubertWithLMHeadModel"),c(fq,"href","/docs/transformers/pr_15297/en/model_doc/funnel#transformers.TFFunnelForMaskedLM"),c(mq,"href","/docs/transformers/pr_15297/en/model_doc/layoutlm#transformers.TFLayoutLMForMaskedLM"),c(gq,"href","/docs/transformers/pr_15297/en/model_doc/longformer#transformers.TFLongformerForMaskedLM"),c(hq,"href","/docs/transformers/pr_15297/en/model_doc/mobilebert#transformers.TFMobileBertForMaskedLM"),c(pq,"href","/docs/transformers/pr_15297/en/model_doc/mpnet#transformers.TFMPNetForMaskedLM"),c(_q,"href","/docs/transformers/pr_15297/en/model_doc/rembert#transformers.TFRemBertForMaskedLM"),c(uq,"href","/docs/transformers/pr_15297/en/model_doc/roberta#transformers.TFRobertaForMaskedLM"),c(bq,"href","/docs/transformers/pr_15297/en/model_doc/roformer#transformers.TFRoFormerForMaskedLM"),c(vq,"href","/docs/transformers/pr_15297/en/model_doc/tapas#transformers.TFTapasForMaskedLM"),c(Tq,"href","/docs/transformers/pr_15297/en/model_doc/xlm#transformers.TFXLMWithLMHeadModel"),c(Fq,"href","/docs/transformers/pr_15297/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForMaskedLM"),c(uo,"class","docstring"),c(ur,"class","docstring"),c(h7,"id","transformers.TFAutoModelForSeq2SeqLM"),c(h7,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(h7,"href","#transformers.TFAutoModelForSeq2SeqLM"),c(uc,"class","relative group"),c(ft,"class","docstring"),c(Cq,"href","/docs/transformers/pr_15297/en/model_doc/bart#transformers.TFBartForConditionalGeneration"),c(Mq,"href","/docs/transformers/pr_15297/en/model_doc/blenderbot#transformers.TFBlenderbotForConditionalGeneration"),c(Eq,"href","/docs/transformers/pr_15297/en/model_doc/blenderbot-small#transformers.TFBlenderbotSmallForConditionalGeneration"),c(yq,"href","/docs/transformers/pr_15297/en/model_doc/encoder-decoder#transformers.TFEncoderDecoderModel"),c(wq,"href","/docs/transformers/pr_15297/en/model_doc/led#transformers.TFLEDForConditionalGeneration"),c(Aq,"href","/docs/transformers/pr_15297/en/model_doc/marian#transformers.TFMarianMTModel"),c(Lq,"href","/docs/transformers/pr_15297/en/model_doc/mbart#transformers.TFMBartForConditionalGeneration"),c(Bq,"href","/docs/transformers/pr_15297/en/model_doc/mt5#transformers.TFMT5ForConditionalGeneration"),c(kq,"href","/docs/transformers/pr_15297/en/model_doc/pegasus#transformers.TFPegasusForConditionalGeneration"),c(xq,"href","/docs/transformers/pr_15297/en/model_doc/t5#transformers.TFT5ForConditionalGeneration"),c(bo,"class","docstring"),c(br,"class","docstring"),c(y7,"id","transformers.TFAutoModelForSequenceClassification"),c(y7,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(y7,"href","#transformers.TFAutoModelForSequenceClassification"),c(Tc,"class","relative group"),c(mt,"class","docstring"),c(Rq,"href","/docs/transformers/pr_15297/en/model_doc/albert#transformers.TFAlbertForSequenceClassification"),c(Sq,"href","/docs/transformers/pr_15297/en/model_doc/bert#transformers.TFBertForSequenceClassification"),c(Pq,"href","/docs/transformers/pr_15297/en/model_doc/camembert#transformers.TFCamembertForSequenceClassification"),c($q,"href","/docs/transformers/pr_15297/en/model_doc/convbert#transformers.TFConvBertForSequenceClassification"),c(Iq,"href","/docs/transformers/pr_15297/en/model_doc/ctrl#transformers.TFCTRLForSequenceClassification"),c(jq,"href","/docs/transformers/pr_15297/en/model_doc/deberta#transformers.TFDebertaForSequenceClassification"),c(Nq,"href","/docs/transformers/pr_15297/en/model_doc/deberta-v2#transformers.TFDebertaV2ForSequenceClassification"),c(Dq,"href","/docs/transformers/pr_15297/en/model_doc/distilbert#transformers.TFDistilBertForSequenceClassification"),c(qq,"href","/docs/transformers/pr_15297/en/model_doc/electra#transformers.TFElectraForSequenceClassification"),c(Gq,"href","/docs/transformers/pr_15297/en/model_doc/flaubert#transformers.TFFlaubertForSequenceClassification"),c(Oq,"href","/docs/transformers/pr_15297/en/model_doc/funnel#transformers.TFFunnelForSequenceClassification"),c(Xq,"href","/docs/transformers/pr_15297/en/model_doc/gpt2#transformers.TFGPT2ForSequenceClassification"),c(zq,"href","/docs/transformers/pr_15297/en/model_doc/layoutlm#transformers.TFLayoutLMForSequenceClassification"),c(Vq,"href","/docs/transformers/pr_15297/en/model_doc/longformer#transformers.TFLongformerForSequenceClassification"),c(Wq,"href","/docs/transformers/pr_15297/en/model_doc/mobilebert#transformers.TFMobileBertForSequenceClassification"),c(Qq,"href","/docs/transformers/pr_15297/en/model_doc/mpnet#transformers.TFMPNetForSequenceClassification"),c(Hq,"href","/docs/transformers/pr_15297/en/model_doc/openai-gpt#transformers.TFOpenAIGPTForSequenceClassification"),c(Uq,"href","/docs/transformers/pr_15297/en/model_doc/rembert#transformers.TFRemBertForSequenceClassification"),c(Jq,"href","/docs/transformers/pr_15297/en/model_doc/roberta#transformers.TFRobertaForSequenceClassification"),c(Yq,"href","/docs/transformers/pr_15297/en/model_doc/roformer#transformers.TFRoFormerForSequenceClassification"),c(Kq,"href","/docs/transformers/pr_15297/en/model_doc/tapas#transformers.TFTapasForSequenceClassification"),c(Zq,"href","/docs/transformers/pr_15297/en/model_doc/transfo-xl#transformers.TFTransfoXLForSequenceClassification"),c(eG,"href","/docs/transformers/pr_15297/en/model_doc/xlm#transformers.TFXLMForSequenceClassification"),c(oG,"href","/docs/transformers/pr_15297/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForSequenceClassification"),c(rG,"href","/docs/transformers/pr_15297/en/model_doc/xlnet#transformers.TFXLNetForSequenceClassification"),c(vo,"class","docstring"),c(vr,"class","docstring"),c(Y7,"id","transformers.TFAutoModelForMultipleChoice"),c(Y7,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Y7,"href","#transformers.TFAutoModelForMultipleChoice"),c(Mc,"class","relative group"),c(gt,"class","docstring"),c(tG,"href","/docs/transformers/pr_15297/en/model_doc/albert#transformers.TFAlbertForMultipleChoice"),c(aG,"href","/docs/transformers/pr_15297/en/model_doc/bert#transformers.TFBertForMultipleChoice"),c(nG,"href","/docs/transformers/pr_15297/en/model_doc/camembert#transformers.TFCamembertForMultipleChoice"),c(sG,"href","/docs/transformers/pr_15297/en/model_doc/convbert#transformers.TFConvBertForMultipleChoice"),c(lG,"href","/docs/transformers/pr_15297/en/model_doc/distilbert#transformers.TFDistilBertForMultipleChoice"),c(iG,"href","/docs/transformers/pr_15297/en/model_doc/electra#transformers.TFElectraForMultipleChoice"),c(dG,"href","/docs/transformers/pr_15297/en/model_doc/flaubert#transformers.TFFlaubertForMultipleChoice"),c(cG,"href","/docs/transformers/pr_15297/en/model_doc/funnel#transformers.TFFunnelForMultipleChoice"),c(fG,"href","/docs/transformers/pr_15297/en/model_doc/longformer#transformers.TFLongformerForMultipleChoice"),c(mG,"href","/docs/transformers/pr_15297/en/model_doc/mobilebert#transformers.TFMobileBertForMultipleChoice"),c(gG,"href","/docs/transformers/pr_15297/en/model_doc/mpnet#transformers.TFMPNetForMultipleChoice"),c(hG,"href","/docs/transformers/pr_15297/en/model_doc/rembert#transformers.TFRemBertForMultipleChoice"),c(pG,"href","/docs/transformers/pr_15297/en/model_doc/roberta#transformers.TFRobertaForMultipleChoice"),c(_G,"href","/docs/transformers/pr_15297/en/model_doc/roformer#transformers.TFRoFormerForMultipleChoice"),c(uG,"href","/docs/transformers/pr_15297/en/model_doc/xlm#transformers.TFXLMForMultipleChoice"),c(bG,"href","/docs/transformers/pr_15297/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForMultipleChoice"),c(vG,"href","/docs/transformers/pr_15297/en/model_doc/xlnet#transformers.TFXLNetForMultipleChoice"),c(To,"class","docstring"),c(Tr,"class","docstring"),c(pF,"id","transformers.TFAutoModelForTableQuestionAnswering"),c(pF,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(pF,"href","#transformers.TFAutoModelForTableQuestionAnswering"),c(wc,"class","relative group"),c(ht,"class","docstring"),c(TG,"href","/docs/transformers/pr_15297/en/model_doc/tapas#transformers.TFTapasForQuestionAnswering"),c(Fo,"class","docstring"),c(Fr,"class","docstring"),c(uF,"id","transformers.TFAutoModelForTokenClassification"),c(uF,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(uF,"href","#transformers.TFAutoModelForTokenClassification"),c(Bc,"class","relative group"),c(pt,"class","docstring"),c(FG,"href","/docs/transformers/pr_15297/en/model_doc/albert#transformers.TFAlbertForTokenClassification"),c(CG,"href","/docs/transformers/pr_15297/en/model_doc/bert#transformers.TFBertForTokenClassification"),c(MG,"href","/docs/transformers/pr_15297/en/model_doc/camembert#transformers.TFCamembertForTokenClassification"),c(EG,"href","/docs/transformers/pr_15297/en/model_doc/convbert#transformers.TFConvBertForTokenClassification"),c(yG,"href","/docs/transformers/pr_15297/en/model_doc/deberta#transformers.TFDebertaForTokenClassification"),c(wG,"href","/docs/transformers/pr_15297/en/model_doc/deberta-v2#transformers.TFDebertaV2ForTokenClassification"),c(AG,"href","/docs/transformers/pr_15297/en/model_doc/distilbert#transformers.TFDistilBertForTokenClassification"),c(LG,"href","/docs/transformers/pr_15297/en/model_doc/electra#transformers.TFElectraForTokenClassification"),c(BG,"href","/docs/transformers/pr_15297/en/model_doc/flaubert#transformers.TFFlaubertForTokenClassification"),c(kG,"href","/docs/transformers/pr_15297/en/model_doc/funnel#transformers.TFFunnelForTokenClassification"),c(xG,"href","/docs/transformers/pr_15297/en/model_doc/layoutlm#transformers.TFLayoutLMForTokenClassification"),c(RG,"href","/docs/transformers/pr_15297/en/model_doc/longformer#transformers.TFLongformerForTokenClassification"),c(SG,"href","/docs/transformers/pr_15297/en/model_doc/mobilebert#transformers.TFMobileBertForTokenClassification"),c(PG,"href","/docs/transformers/pr_15297/en/model_doc/mpnet#transformers.TFMPNetForTokenClassification"),c($G,"href","/docs/transformers/pr_15297/en/model_doc/rembert#transformers.TFRemBertForTokenClassification"),c(IG,"href","/docs/transformers/pr_15297/en/model_doc/roberta#transformers.TFRobertaForTokenClassification"),c(jG,"href","/docs/transformers/pr_15297/en/model_doc/roformer#transformers.TFRoFormerForTokenClassification"),c(NG,"href","/docs/transformers/pr_15297/en/model_doc/xlm#transformers.TFXLMForTokenClassification"),c(DG,"href","/docs/transformers/pr_15297/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForTokenClassification"),c(qG,"href","/docs/transformers/pr_15297/en/model_doc/xlnet#transformers.TFXLNetForTokenClassification"),c(Co,"class","docstring"),c(Cr,"class","docstring"),c(NF,"id","transformers.TFAutoModelForQuestionAnswering"),c(NF,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(NF,"href","#transformers.TFAutoModelForQuestionAnswering"),c(Rc,"class","relative group"),c(_t,"class","docstring"),c(GG,"href","/docs/transformers/pr_15297/en/model_doc/albert#transformers.TFAlbertForQuestionAnswering"),c(OG,"href","/docs/transformers/pr_15297/en/model_doc/bert#transformers.TFBertForQuestionAnswering"),c(XG,"href","/docs/transformers/pr_15297/en/model_doc/camembert#transformers.TFCamembertForQuestionAnswering"),c(zG,"href","/docs/transformers/pr_15297/en/model_doc/convbert#transformers.TFConvBertForQuestionAnswering"),c(VG,"href","/docs/transformers/pr_15297/en/model_doc/deberta#transformers.TFDebertaForQuestionAnswering"),c(WG,"href","/docs/transformers/pr_15297/en/model_doc/deberta-v2#transformers.TFDebertaV2ForQuestionAnswering"),c(QG,"href","/docs/transformers/pr_15297/en/model_doc/distilbert#transformers.TFDistilBertForQuestionAnswering"),c(HG,"href","/docs/transformers/pr_15297/en/model_doc/electra#transformers.TFElectraForQuestionAnswering"),c(UG,"href","/docs/transformers/pr_15297/en/model_doc/flaubert#transformers.TFFlaubertForQuestionAnsweringSimple"),c(JG,"href","/docs/transformers/pr_15297/en/model_doc/funnel#transformers.TFFunnelForQuestionAnswering"),c(YG,"href","/docs/transformers/pr_15297/en/model_doc/longformer#transformers.TFLongformerForQuestionAnswering"),c(KG,"href","/docs/transformers/pr_15297/en/model_doc/mobilebert#transformers.TFMobileBertForQuestionAnswering"),c(ZG,"href","/docs/transformers/pr_15297/en/model_doc/mpnet#transformers.TFMPNetForQuestionAnswering"),c(eO,"href","/docs/transformers/pr_15297/en/model_doc/rembert#transformers.TFRemBertForQuestionAnswering"),c(oO,"href","/docs/transformers/pr_15297/en/model_doc/roberta#transformers.TFRobertaForQuestionAnswering"),c(rO,"href","/docs/transformers/pr_15297/en/model_doc/roformer#transformers.TFRoFormerForQuestionAnswering"),c(tO,"href","/docs/transformers/pr_15297/en/model_doc/xlm#transformers.TFXLMForQuestionAnsweringSimple"),c(aO,"href","/docs/transformers/pr_15297/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForQuestionAnswering"),c(nO,"href","/docs/transformers/pr_15297/en/model_doc/xlnet#transformers.TFXLNetForQuestionAnsweringSimple"),c(Mo,"class","docstring"),c(Mr,"class","docstring"),c(a9,"id","transformers.TFAutoModelForVision2Seq"),c(a9,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(a9,"href","#transformers.TFAutoModelForVision2Seq"),c($c,"class","relative group"),c(ut,"class","docstring"),c(sO,"href","/docs/transformers/pr_15297/en/model_doc/vision-encoder-decoder#transformers.TFVisionEncoderDecoderModel"),c(Eo,"class","docstring"),c(Er,"class","docstring"),c(s9,"id","transformers.TFAutoModelForSpeechSeq2Seq"),c(s9,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(s9,"href","#transformers.TFAutoModelForSpeechSeq2Seq"),c(Nc,"class","relative group"),c(bt,"class","docstring"),c(lO,"href","/docs/transformers/pr_15297/en/model_doc/speech_to_text#transformers.TFSpeech2TextForConditionalGeneration"),c(yo,"class","docstring"),c(yr,"class","docstring"),c(i9,"id","transformers.FlaxAutoModel"),c(i9,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(i9,"href","#transformers.FlaxAutoModel"),c(Gc,"class","relative group"),c(vt,"class","docstring"),c(iO,"href","/docs/transformers/pr_15297/en/model_doc/albert#transformers.FlaxAlbertModel"),c(dO,"href","/docs/transformers/pr_15297/en/model_doc/bart#transformers.FlaxBartModel"),c(cO,"href","/docs/transformers/pr_15297/en/model_doc/beit#transformers.FlaxBeitModel"),c(fO,"href","/docs/transformers/pr_15297/en/model_doc/bert#transformers.FlaxBertModel"),c(mO,"href","/docs/transformers/pr_15297/en/model_doc/big_bird#transformers.FlaxBigBirdModel"),c(gO,"href","/docs/transformers/pr_15297/en/model_doc/blenderbot#transformers.FlaxBlenderbotModel"),c(hO,"href","/docs/transformers/pr_15297/en/model_doc/blenderbot-small#transformers.FlaxBlenderbotSmallModel"),c(pO,"href","/docs/transformers/pr_15297/en/model_doc/clip#transformers.FlaxCLIPModel"),c(_O,"href","/docs/transformers/pr_15297/en/model_doc/distilbert#transformers.FlaxDistilBertModel"),c(uO,"href","/docs/transformers/pr_15297/en/model_doc/electra#transformers.FlaxElectraModel"),c(bO,"href","/docs/transformers/pr_15297/en/model_doc/gpt2#transformers.FlaxGPT2Model"),c(vO,"href","/docs/transformers/pr_15297/en/model_doc/gpt_neo#transformers.FlaxGPTNeoModel"),c(TO,"href","/docs/transformers/pr_15297/en/model_doc/gptj#transformers.FlaxGPTJModel"),c(FO,"href","/docs/transformers/pr_15297/en/model_doc/marian#transformers.FlaxMarianModel"),c(CO,"href","/docs/transformers/pr_15297/en/model_doc/mbart#transformers.FlaxMBartModel"),c(MO,"href","/docs/transformers/pr_15297/en/model_doc/mt5#transformers.FlaxMT5Model"),c(EO,"href","/docs/transformers/pr_15297/en/model_doc/pegasus#transformers.FlaxPegasusModel"),c(yO,"href","/docs/transformers/pr_15297/en/model_doc/roberta#transformers.FlaxRobertaModel"),c(wO,"href","/docs/transformers/pr_15297/en/model_doc/roformer#transformers.FlaxRoFormerModel"),c(AO,"href","/docs/transformers/pr_15297/en/model_doc/t5#transformers.FlaxT5Model"),c(LO,"href","/docs/transformers/pr_15297/en/model_doc/vision-text-dual-encoder#transformers.FlaxVisionTextDualEncoderModel"),c(BO,"href","/docs/transformers/pr_15297/en/model_doc/vit#transformers.FlaxViTModel"),c(kO,"href","/docs/transformers/pr_15297/en/model_doc/wav2vec2#transformers.FlaxWav2Vec2Model"),c(xO,"href","/docs/transformers/pr_15297/en/model_doc/xglm#transformers.FlaxXGLMModel"),c(wo,"class","docstring"),c(wr,"class","docstring"),c(S9,"id","transformers.FlaxAutoModelForCausalLM"),c(S9,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(S9,"href","#transformers.FlaxAutoModelForCausalLM"),c(zc,"class","relative group"),c(Tt,"class","docstring"),c(RO,"href","/docs/transformers/pr_15297/en/model_doc/gpt2#transformers.FlaxGPT2LMHeadModel"),c(SO,"href","/docs/transformers/pr_15297/en/model_doc/gpt_neo#transformers.FlaxGPTNeoForCausalLM"),c(PO,"href","/docs/transformers/pr_15297/en/model_doc/gptj#transformers.FlaxGPTJForCausalLM"),c($O,"href","/docs/transformers/pr_15297/en/model_doc/xglm#transformers.FlaxXGLMForCausalLM"),c(Ao,"class","docstring"),c(Ar,"class","docstring"),c(N9,"id","transformers.FlaxAutoModelForPreTraining"),c(N9,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(N9,"href","#transformers.FlaxAutoModelForPreTraining"),c(Qc,"class","relative group"),c(Ft,"class","docstring"),c(IO,"href","/docs/transformers/pr_15297/en/model_doc/albert#transformers.FlaxAlbertForPreTraining"),c(jO,"href","/docs/transformers/pr_15297/en/model_doc/bart#transformers.FlaxBartForConditionalGeneration"),c(NO,"href","/docs/transformers/pr_15297/en/model_doc/bert#transformers.FlaxBertForPreTraining"),c(DO,"href","/docs/transformers/pr_15297/en/model_doc/big_bird#transformers.FlaxBigBirdForPreTraining"),c(qO,"href","/docs/transformers/pr_15297/en/model_doc/electra#transformers.FlaxElectraForPreTraining"),c(GO,"href","/docs/transformers/pr_15297/en/model_doc/mbart#transformers.FlaxMBartForConditionalGeneration"),c(OO,"href","/docs/transformers/pr_15297/en/model_doc/mt5#transformers.FlaxMT5ForConditionalGeneration"),c(XO,"href","/docs/transformers/pr_15297/en/model_doc/roberta#transformers.FlaxRobertaForMaskedLM"),c(zO,"href","/docs/transformers/pr_15297/en/model_doc/roformer#transformers.FlaxRoFormerForMaskedLM"),c(VO,"href","/docs/transformers/pr_15297/en/model_doc/t5#transformers.FlaxT5ForConditionalGeneration"),c(WO,"href","/docs/transformers/pr_15297/en/model_doc/wav2vec2#transformers.FlaxWav2Vec2ForPreTraining"),c(Lo,"class","docstring"),c(Lr,"class","docstring"),c(J9,"id","transformers.FlaxAutoModelForMaskedLM"),c(J9,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(J9,"href","#transformers.FlaxAutoModelForMaskedLM"),c(Jc,"class","relative group"),c(Ct,"class","docstring"),c(QO,"href","/docs/transformers/pr_15297/en/model_doc/albert#transformers.FlaxAlbertForMaskedLM"),c(HO,"href","/docs/transformers/pr_15297/en/model_doc/bart#transformers.FlaxBartForConditionalGeneration"),c(UO,"href","/docs/transformers/pr_15297/en/model_doc/bert#transformers.FlaxBertForMaskedLM"),c(JO,"href","/docs/transformers/pr_15297/en/model_doc/big_bird#transformers.FlaxBigBirdForMaskedLM"),c(YO,"href","/docs/transformers/pr_15297/en/model_doc/distilbert#transformers.FlaxDistilBertForMaskedLM"),c(KO,"href","/docs/transformers/pr_15297/en/model_doc/electra#transformers.FlaxElectraForMaskedLM"),c(ZO,"href","/docs/transformers/pr_15297/en/model_doc/mbart#transformers.FlaxMBartForConditionalGeneration"),c(eX,"href","/docs/transformers/pr_15297/en/model_doc/roberta#transformers.FlaxRobertaForMaskedLM"),c(oX,"href","/docs/transformers/pr_15297/en/model_doc/roformer#transformers.FlaxRoFormerForMaskedLM"),c(Bo,"class","docstring"),c(Br,"class","docstring"),c(sC,"id","transformers.FlaxAutoModelForSeq2SeqLM"),c(sC,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(sC,"href","#transformers.FlaxAutoModelForSeq2SeqLM"),c(Zc,"class","relative group"),c(Mt,"class","docstring"),c(rX,"href","/docs/transformers/pr_15297/en/model_doc/bart#transformers.FlaxBartForConditionalGeneration"),c(tX,"href","/docs/transformers/pr_15297/en/model_doc/blenderbot#transformers.FlaxBlenderbotForConditionalGeneration"),c(aX,"href","/docs/transformers/pr_15297/en/model_doc/blenderbot-small#transformers.FlaxBlenderbotSmallForConditionalGeneration"),c(nX,"href","/docs/transformers/pr_15297/en/model_doc/encoder-decoder#transformers.FlaxEncoderDecoderModel"),c(sX,"href","/docs/transformers/pr_15297/en/model_doc/marian#transformers.FlaxMarianMTModel"),c(lX,"href","/docs/transformers/pr_15297/en/model_doc/mbart#transformers.FlaxMBartForConditionalGeneration"),c(iX,"href","/docs/transformers/pr_15297/en/model_doc/mt5#transformers.FlaxMT5ForConditionalGeneration"),c(dX,"href","/docs/transformers/pr_15297/en/model_doc/pegasus#transformers.FlaxPegasusForConditionalGeneration"),c(cX,"href","/docs/transformers/pr_15297/en/model_doc/t5#transformers.FlaxT5ForConditionalGeneration"),c(ko,"class","docstring"),c(kr,"class","docstring"),c(_C,"id","transformers.FlaxAutoModelForSequenceClassification"),c(_C,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(_C,"href","#transformers.FlaxAutoModelForSequenceClassification"),c(rf,"class","relative group"),c(Et,"class","docstring"),c(fX,"href","/docs/transformers/pr_15297/en/model_doc/albert#transformers.FlaxAlbertForSequenceClassification"),c(mX,"href","/docs/transformers/pr_15297/en/model_doc/bart#transformers.FlaxBartForSequenceClassification"),c(gX,"href","/docs/transformers/pr_15297/en/model_doc/bert#transformers.FlaxBertForSequenceClassification"),c(hX,"href","/docs/transformers/pr_15297/en/model_doc/big_bird#transformers.FlaxBigBirdForSequenceClassification"),c(pX,"href","/docs/transformers/pr_15297/en/model_doc/distilbert#transformers.FlaxDistilBertForSequenceClassification"),c(_X,"href","/docs/transformers/pr_15297/en/model_doc/electra#transformers.FlaxElectraForSequenceClassification"),c(uX,"href","/docs/transformers/pr_15297/en/model_doc/mbart#transformers.FlaxMBartForSequenceClassification"),c(bX,"href","/docs/transformers/pr_15297/en/model_doc/roberta#transformers.FlaxRobertaForSequenceClassification"),c(vX,"href","/docs/transformers/pr_15297/en/model_doc/roformer#transformers.FlaxRoFormerForSequenceClassification"),c(xo,"class","docstring"),c(xr,"class","docstring"),c(wC,"id","transformers.FlaxAutoModelForQuestionAnswering"),c(wC,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(wC,"href","#transformers.FlaxAutoModelForQuestionAnswering"),c(nf,"class","relative group"),c(yt,"class","docstring"),c(TX,"href","/docs/transformers/pr_15297/en/model_doc/albert#transformers.FlaxAlbertForQuestionAnswering"),c(FX,"href","/docs/transformers/pr_15297/en/model_doc/bart#transformers.FlaxBartForQuestionAnswering"),c(CX,"href","/docs/transformers/pr_15297/en/model_doc/bert#transformers.FlaxBertForQuestionAnswering"),c(MX,"href","/docs/transformers/pr_15297/en/model_doc/big_bird#transformers.FlaxBigBirdForQuestionAnswering"),c(EX,"href","/docs/transformers/pr_15297/en/model_doc/distilbert#transformers.FlaxDistilBertForQuestionAnswering"),c(yX,"href","/docs/transformers/pr_15297/en/model_doc/electra#transformers.FlaxElectraForQuestionAnswering"),c(wX,"href","/docs/transformers/pr_15297/en/model_doc/mbart#transformers.FlaxMBartForQuestionAnswering"),c(AX,"href","/docs/transformers/pr_15297/en/model_doc/roberta#transformers.FlaxRobertaForQuestionAnswering"),c(LX,"href","/docs/transformers/pr_15297/en/model_doc/roformer#transformers.FlaxRoFormerForQuestionAnswering"),c(Ro,"class","docstring"),c(Rr,"class","docstring"),c(IC,"id","transformers.FlaxAutoModelForTokenClassification"),c(IC,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(IC,"href","#transformers.FlaxAutoModelForTokenClassification"),c(df,"class","relative group"),c(wt,"class","docstring"),c(BX,"href","/docs/transformers/pr_15297/en/model_doc/albert#transformers.FlaxAlbertForTokenClassification"),c(kX,"href","/docs/transformers/pr_15297/en/model_doc/bert#transformers.FlaxBertForTokenClassification"),c(xX,"href","/docs/transformers/pr_15297/en/model_doc/big_bird#transformers.FlaxBigBirdForTokenClassification"),c(RX,"href","/docs/transformers/pr_15297/en/model_doc/distilbert#transformers.FlaxDistilBertForTokenClassification"),c(SX,"href","/docs/transformers/pr_15297/en/model_doc/electra#transformers.FlaxElectraForTokenClassification"),c(PX,"href","/docs/transformers/pr_15297/en/model_doc/roberta#transformers.FlaxRobertaForTokenClassification"),c($X,"href","/docs/transformers/pr_15297/en/model_doc/roformer#transformers.FlaxRoFormerForTokenClassification"),c(So,"class","docstring"),c(Sr,"class","docstring"),c(zC,"id","transformers.FlaxAutoModelForMultipleChoice"),c(zC,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(zC,"href","#transformers.FlaxAutoModelForMultipleChoice"),c(mf,"class","relative group"),c(At,"class","docstring"),c(IX,"href","/docs/transformers/pr_15297/en/model_doc/albert#transformers.FlaxAlbertForMultipleChoice"),c(jX,"href","/docs/transformers/pr_15297/en/model_doc/bert#transformers.FlaxBertForMultipleChoice"),c(NX,"href","/docs/transformers/pr_15297/en/model_doc/big_bird#transformers.FlaxBigBirdForMultipleChoice"),c(DX,"href","/docs/transformers/pr_15297/en/model_doc/distilbert#transformers.FlaxDistilBertForMultipleChoice"),c(qX,"href","/docs/transformers/pr_15297/en/model_doc/electra#transformers.FlaxElectraForMultipleChoice"),c(GX,"href","/docs/transformers/pr_15297/en/model_doc/roberta#transformers.FlaxRobertaForMultipleChoice"),c(OX,"href","/docs/transformers/pr_15297/en/model_doc/roformer#transformers.FlaxRoFormerForMultipleChoice"),c(Po,"class","docstring"),c(Pr,"class","docstring"),c(KC,"id","transformers.FlaxAutoModelForNextSentencePrediction"),c(KC,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(KC,"href","#transformers.FlaxAutoModelForNextSentencePrediction"),c(pf,"class","relative group"),c(Lt,"class","docstring"),c(XX,"href","/docs/transformers/pr_15297/en/model_doc/bert#transformers.FlaxBertForNextSentencePrediction"),c($o,"class","docstring"),c($r,"class","docstring"),c(e4,"id","transformers.FlaxAutoModelForImageClassification"),c(e4,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(e4,"href","#transformers.FlaxAutoModelForImageClassification"),c(bf,"class","relative group"),c(Bt,"class","docstring"),c(zX,"href","/docs/transformers/pr_15297/en/model_doc/beit#transformers.FlaxBeitForImageClassification"),c(VX,"href","/docs/transformers/pr_15297/en/model_doc/vit#transformers.FlaxViTForImageClassification"),c(Io,"class","docstring"),c(Ir,"class","docstring"),c(t4,"id","transformers.FlaxAutoModelForVision2Seq"),c(t4,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(t4,"href","#transformers.FlaxAutoModelForVision2Seq"),c(Ff,"class","relative group"),c(kt,"class","docstring"),c(WX,"href","/docs/transformers/pr_15297/en/model_doc/vision-encoder-decoder#transformers.FlaxVisionEncoderDecoderModel"),c(jo,"class","docstring"),c(jr,"class","docstring")},m(d,u){e(document.head,J),b(d,Ae,u),b(d,ie,u),e(ie,me),e(me,to),g(ce,to,null),e(ie,ue),e(ie,Do),e(Do,Ai),b(d,yf,u),b(d,sa,u),e(sa,Li),e(sa,Bi),e(Bi,rM),e(sa,wf),b(d,ye,u),b(d,io,u),e(io,ki),e(io,Pn),e(Pn,tM),e(io,$n),e(io,In),e(In,aM),e(io,xi),e(io,jn),e(jn,nM),e(io,Ri),b(d,Af,u),g($a,d,u),b(d,co,u),b(d,ge,u),e(ge,G0),e(ge,Si),e(Si,O0),e(ge,X0),b(d,qo,u),b(d,Ia,u),e(Ia,z0),e(Ia,Lf),e(Lf,V0),e(Ia,uxe),b(d,iLe,u),b(d,Pi,u),e(Pi,Bf),e(Bf,DV),g(sM,DV,null),e(Pi,bxe),e(Pi,qV),e(qV,vxe),b(d,dLe,u),b(d,Nn,u),e(Nn,Txe),e(Nn,GV),e(GV,Fxe),e(Nn,Cxe),e(Nn,OV),e(OV,Mxe),e(Nn,Exe),b(d,cLe,u),g(lM,d,u),b(d,fLe,u),b(d,W0,u),e(W0,yxe),b(d,mLe,u),g(kf,d,u),b(d,gLe,u),b(d,$i,u),e($i,xf),e(xf,XV),g(iM,XV,null),e($i,wxe),e($i,zV),e(zV,Axe),b(d,hLe,u),b(d,Go,u),g(dM,Go,null),e(Go,Lxe),e(Go,cM),e(cM,Bxe),e(cM,Q0),e(Q0,kxe),e(cM,xxe),e(Go,Rxe),e(Go,fM),e(fM,Sxe),e(fM,VV),e(VV,Pxe),e(fM,$xe),e(Go,Ixe),e(Go,fo),g(mM,fo,null),e(fo,jxe),e(fo,WV),e(WV,Nxe),e(fo,Dxe),e(fo,Ii),e(Ii,qxe),e(Ii,QV),e(QV,Gxe),e(Ii,Oxe),e(Ii,HV),e(HV,Xxe),e(Ii,zxe),e(fo,Vxe),e(fo,v),e(v,Rf),e(Rf,UV),e(UV,Wxe),e(Rf,Qxe),e(Rf,H0),e(H0,Hxe),e(Rf,Uxe),e(v,Jxe),e(v,Sf),e(Sf,JV),e(JV,Yxe),e(Sf,Kxe),e(Sf,U0),e(U0,Zxe),e(Sf,eRe),e(v,oRe),e(v,Pf),e(Pf,YV),e(YV,rRe),e(Pf,tRe),e(Pf,J0),e(J0,aRe),e(Pf,nRe),e(v,sRe),e(v,$f),e($f,KV),e(KV,lRe),e($f,iRe),e($f,Y0),e(Y0,dRe),e($f,cRe),e(v,fRe),e(v,If),e(If,ZV),e(ZV,mRe),e(If,gRe),e(If,K0),e(K0,hRe),e(If,pRe),e(v,_Re),e(v,jf),e(jf,eW),e(eW,uRe),e(jf,bRe),e(jf,Z0),e(Z0,vRe),e(jf,TRe),e(v,FRe),e(v,Nf),e(Nf,oW),e(oW,CRe),e(Nf,MRe),e(Nf,eL),e(eL,ERe),e(Nf,yRe),e(v,wRe),e(v,Df),e(Df,rW),e(rW,ARe),e(Df,LRe),e(Df,oL),e(oL,BRe),e(Df,kRe),e(v,xRe),e(v,qf),e(qf,tW),e(tW,RRe),e(qf,SRe),e(qf,rL),e(rL,PRe),e(qf,$Re),e(v,IRe),e(v,Gf),e(Gf,aW),e(aW,jRe),e(Gf,NRe),e(Gf,tL),e(tL,DRe),e(Gf,qRe),e(v,GRe),e(v,Of),e(Of,nW),e(nW,ORe),e(Of,XRe),e(Of,aL),e(aL,zRe),e(Of,VRe),e(v,WRe),e(v,Xf),e(Xf,sW),e(sW,QRe),e(Xf,HRe),e(Xf,nL),e(nL,URe),e(Xf,JRe),e(v,YRe),e(v,zf),e(zf,lW),e(lW,KRe),e(zf,ZRe),e(zf,sL),e(sL,eSe),e(zf,oSe),e(v,rSe),e(v,Vf),e(Vf,iW),e(iW,tSe),e(Vf,aSe),e(Vf,lL),e(lL,nSe),e(Vf,sSe),e(v,lSe),e(v,Wf),e(Wf,dW),e(dW,iSe),e(Wf,dSe),e(Wf,iL),e(iL,cSe),e(Wf,fSe),e(v,mSe),e(v,Qf),e(Qf,cW),e(cW,gSe),e(Qf,hSe),e(Qf,dL),e(dL,pSe),e(Qf,_Se),e(v,uSe),e(v,Hf),e(Hf,fW),e(fW,bSe),e(Hf,vSe),e(Hf,cL),e(cL,TSe),e(Hf,FSe),e(v,CSe),e(v,Uf),e(Uf,mW),e(mW,MSe),e(Uf,ESe),e(Uf,fL),e(fL,ySe),e(Uf,wSe),e(v,ASe),e(v,Jf),e(Jf,gW),e(gW,LSe),e(Jf,BSe),e(Jf,mL),e(mL,kSe),e(Jf,xSe),e(v,RSe),e(v,Yf),e(Yf,hW),e(hW,SSe),e(Yf,PSe),e(Yf,gL),e(gL,$Se),e(Yf,ISe),e(v,jSe),e(v,Kf),e(Kf,pW),e(pW,NSe),e(Kf,DSe),e(Kf,hL),e(hL,qSe),e(Kf,GSe),e(v,OSe),e(v,Zf),e(Zf,_W),e(_W,XSe),e(Zf,zSe),e(Zf,pL),e(pL,VSe),e(Zf,WSe),e(v,QSe),e(v,em),e(em,uW),e(uW,HSe),e(em,USe),e(em,_L),e(_L,JSe),e(em,YSe),e(v,KSe),e(v,om),e(om,bW),e(bW,ZSe),e(om,ePe),e(om,uL),e(uL,oPe),e(om,rPe),e(v,tPe),e(v,rm),e(rm,vW),e(vW,aPe),e(rm,nPe),e(rm,bL),e(bL,sPe),e(rm,lPe),e(v,iPe),e(v,tm),e(tm,TW),e(TW,dPe),e(tm,cPe),e(tm,vL),e(vL,fPe),e(tm,mPe),e(v,gPe),e(v,am),e(am,FW),e(FW,hPe),e(am,pPe),e(am,TL),e(TL,_Pe),e(am,uPe),e(v,bPe),e(v,nm),e(nm,CW),e(CW,vPe),e(nm,TPe),e(nm,FL),e(FL,FPe),e(nm,CPe),e(v,MPe),e(v,sm),e(sm,MW),e(MW,EPe),e(sm,yPe),e(sm,CL),e(CL,wPe),e(sm,APe),e(v,LPe),e(v,lm),e(lm,EW),e(EW,BPe),e(lm,kPe),e(lm,ML),e(ML,xPe),e(lm,RPe),e(v,SPe),e(v,im),e(im,yW),e(yW,PPe),e(im,$Pe),e(im,EL),e(EL,IPe),e(im,jPe),e(v,NPe),e(v,dm),e(dm,wW),e(wW,DPe),e(dm,qPe),e(dm,yL),e(yL,GPe),e(dm,OPe),e(v,XPe),e(v,cm),e(cm,AW),e(AW,zPe),e(cm,VPe),e(cm,wL),e(wL,WPe),e(cm,QPe),e(v,HPe),e(v,fm),e(fm,LW),e(LW,UPe),e(fm,JPe),e(fm,AL),e(AL,YPe),e(fm,KPe),e(v,ZPe),e(v,mm),e(mm,BW),e(BW,e$e),e(mm,o$e),e(mm,LL),e(LL,r$e),e(mm,t$e),e(v,a$e),e(v,gm),e(gm,kW),e(kW,n$e),e(gm,s$e),e(gm,BL),e(BL,l$e),e(gm,i$e),e(v,d$e),e(v,hm),e(hm,xW),e(xW,c$e),e(hm,f$e),e(hm,kL),e(kL,m$e),e(hm,g$e),e(v,h$e),e(v,pm),e(pm,RW),e(RW,p$e),e(pm,_$e),e(pm,xL),e(xL,u$e),e(pm,b$e),e(v,v$e),e(v,_m),e(_m,SW),e(SW,T$e),e(_m,F$e),e(_m,RL),e(RL,C$e),e(_m,M$e),e(v,E$e),e(v,um),e(um,PW),e(PW,y$e),e(um,w$e),e(um,SL),e(SL,A$e),e(um,L$e),e(v,B$e),e(v,bm),e(bm,$W),e($W,k$e),e(bm,x$e),e(bm,PL),e(PL,R$e),e(bm,S$e),e(v,P$e),e(v,vm),e(vm,IW),e(IW,$$e),e(vm,I$e),e(vm,$L),e($L,j$e),e(vm,N$e),e(v,D$e),e(v,Tm),e(Tm,jW),e(jW,q$e),e(Tm,G$e),e(Tm,IL),e(IL,O$e),e(Tm,X$e),e(v,z$e),e(v,Fm),e(Fm,NW),e(NW,V$e),e(Fm,W$e),e(Fm,jL),e(jL,Q$e),e(Fm,H$e),e(v,U$e),e(v,Cm),e(Cm,DW),e(DW,J$e),e(Cm,Y$e),e(Cm,NL),e(NL,K$e),e(Cm,Z$e),e(v,eIe),e(v,Mm),e(Mm,qW),e(qW,oIe),e(Mm,rIe),e(Mm,DL),e(DL,tIe),e(Mm,aIe),e(v,nIe),e(v,Em),e(Em,GW),e(GW,sIe),e(Em,lIe),e(Em,qL),e(qL,iIe),e(Em,dIe),e(v,cIe),e(v,ym),e(ym,OW),e(OW,fIe),e(ym,mIe),e(ym,GL),e(GL,gIe),e(ym,hIe),e(v,pIe),e(v,wm),e(wm,XW),e(XW,_Ie),e(wm,uIe),e(wm,OL),e(OL,bIe),e(wm,vIe),e(v,TIe),e(v,Am),e(Am,zW),e(zW,FIe),e(Am,CIe),e(Am,XL),e(XL,MIe),e(Am,EIe),e(v,yIe),e(v,Lm),e(Lm,VW),e(VW,wIe),e(Lm,AIe),e(Lm,zL),e(zL,LIe),e(Lm,BIe),e(v,kIe),e(v,Bm),e(Bm,WW),e(WW,xIe),e(Bm,RIe),e(Bm,VL),e(VL,SIe),e(Bm,PIe),e(v,$Ie),e(v,km),e(km,QW),e(QW,IIe),e(km,jIe),e(km,WL),e(WL,NIe),e(km,DIe),e(v,qIe),e(v,xm),e(xm,HW),e(HW,GIe),e(xm,OIe),e(xm,QL),e(QL,XIe),e(xm,zIe),e(v,VIe),e(v,Rm),e(Rm,UW),e(UW,WIe),e(Rm,QIe),e(Rm,HL),e(HL,HIe),e(Rm,UIe),e(v,JIe),e(v,Sm),e(Sm,JW),e(JW,YIe),e(Sm,KIe),e(Sm,UL),e(UL,ZIe),e(Sm,eje),e(v,oje),e(v,Pm),e(Pm,YW),e(YW,rje),e(Pm,tje),e(Pm,JL),e(JL,aje),e(Pm,nje),e(v,sje),e(v,$m),e($m,KW),e(KW,lje),e($m,ije),e($m,YL),e(YL,dje),e($m,cje),e(v,fje),e(v,Im),e(Im,ZW),e(ZW,mje),e(Im,gje),e(Im,KL),e(KL,hje),e(Im,pje),e(v,_je),e(v,jm),e(jm,eQ),e(eQ,uje),e(jm,bje),e(jm,ZL),e(ZL,vje),e(jm,Tje),e(v,Fje),e(v,Nm),e(Nm,oQ),e(oQ,Cje),e(Nm,Mje),e(Nm,e8),e(e8,Eje),e(Nm,yje),e(v,wje),e(v,Dm),e(Dm,rQ),e(rQ,Aje),e(Dm,Lje),e(Dm,o8),e(o8,Bje),e(Dm,kje),e(v,xje),e(v,qm),e(qm,tQ),e(tQ,Rje),e(qm,Sje),e(qm,r8),e(r8,Pje),e(qm,$je),e(v,Ije),e(v,Gm),e(Gm,aQ),e(aQ,jje),e(Gm,Nje),e(Gm,t8),e(t8,Dje),e(Gm,qje),e(v,Gje),e(v,Om),e(Om,nQ),e(nQ,Oje),e(Om,Xje),e(Om,a8),e(a8,zje),e(Om,Vje),e(v,Wje),e(v,Xm),e(Xm,sQ),e(sQ,Qje),e(Xm,Hje),e(Xm,n8),e(n8,Uje),e(Xm,Jje),e(v,Yje),e(v,zm),e(zm,lQ),e(lQ,Kje),e(zm,Zje),e(zm,s8),e(s8,eNe),e(zm,oNe),e(v,rNe),e(v,Vm),e(Vm,iQ),e(iQ,tNe),e(Vm,aNe),e(Vm,l8),e(l8,nNe),e(Vm,sNe),e(v,lNe),e(v,Wm),e(Wm,dQ),e(dQ,iNe),e(Wm,dNe),e(Wm,i8),e(i8,cNe),e(Wm,fNe),e(v,mNe),e(v,Qm),e(Qm,cQ),e(cQ,gNe),e(Qm,hNe),e(Qm,d8),e(d8,pNe),e(Qm,_Ne),e(v,uNe),e(v,Hm),e(Hm,fQ),e(fQ,bNe),e(Hm,vNe),e(Hm,c8),e(c8,TNe),e(Hm,FNe),e(v,CNe),e(v,Um),e(Um,mQ),e(mQ,MNe),e(Um,ENe),e(Um,f8),e(f8,yNe),e(Um,wNe),e(v,ANe),e(v,Jm),e(Jm,gQ),e(gQ,LNe),e(Jm,BNe),e(Jm,m8),e(m8,kNe),e(Jm,xNe),e(v,RNe),e(v,Ym),e(Ym,hQ),e(hQ,SNe),e(Ym,PNe),e(Ym,g8),e(g8,$Ne),e(Ym,INe),e(v,jNe),e(v,Km),e(Km,pQ),e(pQ,NNe),e(Km,DNe),e(Km,h8),e(h8,qNe),e(Km,GNe),e(v,ONe),e(v,Zm),e(Zm,_Q),e(_Q,XNe),e(Zm,zNe),e(Zm,p8),e(p8,VNe),e(Zm,WNe),e(v,QNe),e(v,eg),e(eg,uQ),e(uQ,HNe),e(eg,UNe),e(eg,_8),e(_8,JNe),e(eg,YNe),e(v,KNe),e(v,og),e(og,bQ),e(bQ,ZNe),e(og,eDe),e(og,u8),e(u8,oDe),e(og,rDe),e(v,tDe),e(v,rg),e(rg,vQ),e(vQ,aDe),e(rg,nDe),e(rg,b8),e(b8,sDe),e(rg,lDe),e(v,iDe),e(v,tg),e(tg,TQ),e(TQ,dDe),e(tg,cDe),e(tg,v8),e(v8,fDe),e(tg,mDe),e(v,gDe),e(v,ag),e(ag,FQ),e(FQ,hDe),e(ag,pDe),e(ag,T8),e(T8,_De),e(ag,uDe),e(v,bDe),e(v,ng),e(ng,CQ),e(CQ,vDe),e(ng,TDe),e(ng,F8),e(F8,FDe),e(ng,CDe),e(v,MDe),e(v,sg),e(sg,MQ),e(MQ,EDe),e(sg,yDe),e(sg,C8),e(C8,wDe),e(sg,ADe),e(v,LDe),e(v,lg),e(lg,EQ),e(EQ,BDe),e(lg,kDe),e(lg,M8),e(M8,xDe),e(lg,RDe),e(v,SDe),e(v,ig),e(ig,yQ),e(yQ,PDe),e(ig,$De),e(ig,E8),e(E8,IDe),e(ig,jDe),e(v,NDe),e(v,dg),e(dg,wQ),e(wQ,DDe),e(dg,qDe),e(dg,y8),e(y8,GDe),e(dg,ODe),e(v,XDe),e(v,cg),e(cg,AQ),e(AQ,zDe),e(cg,VDe),e(cg,w8),e(w8,WDe),e(cg,QDe),e(v,HDe),e(v,fg),e(fg,LQ),e(LQ,UDe),e(fg,JDe),e(fg,A8),e(A8,YDe),e(fg,KDe),e(v,ZDe),e(v,mg),e(mg,BQ),e(BQ,eqe),e(mg,oqe),e(mg,L8),e(L8,rqe),e(mg,tqe),e(v,aqe),e(v,gg),e(gg,kQ),e(kQ,nqe),e(gg,sqe),e(gg,B8),e(B8,lqe),e(gg,iqe),e(v,dqe),e(v,hg),e(hg,xQ),e(xQ,cqe),e(hg,fqe),e(hg,k8),e(k8,mqe),e(hg,gqe),e(fo,hqe),e(fo,RQ),e(RQ,pqe),e(fo,_qe),g(gM,fo,null),e(Go,uqe),e(Go,pg),g(hM,pg,null),e(pg,bqe),e(pg,SQ),e(SQ,vqe),b(d,pLe,u),b(d,ji,u),e(ji,_g),e(_g,PQ),g(pM,PQ,null),e(ji,Tqe),e(ji,$Q),e($Q,Fqe),b(d,_Le,u),b(d,Oo,u),g(_M,Oo,null),e(Oo,Cqe),e(Oo,uM),e(uM,Mqe),e(uM,x8),e(x8,Eqe),e(uM,yqe),e(Oo,wqe),e(Oo,bM),e(bM,Aqe),e(bM,IQ),e(IQ,Lqe),e(bM,Bqe),e(Oo,kqe),e(Oo,mo),g(vM,mo,null),e(mo,xqe),e(mo,jQ),e(jQ,Rqe),e(mo,Sqe),e(mo,ja),e(ja,Pqe),e(ja,NQ),e(NQ,$qe),e(ja,Iqe),e(ja,DQ),e(DQ,jqe),e(ja,Nqe),e(ja,qQ),e(qQ,Dqe),e(ja,qqe),e(mo,Gqe),e(mo,M),e(M,Dn),e(Dn,GQ),e(GQ,Oqe),e(Dn,Xqe),e(Dn,R8),e(R8,zqe),e(Dn,Vqe),e(Dn,S8),e(S8,Wqe),e(Dn,Qqe),e(M,Hqe),e(M,qn),e(qn,OQ),e(OQ,Uqe),e(qn,Jqe),e(qn,P8),e(P8,Yqe),e(qn,Kqe),e(qn,$8),e($8,Zqe),e(qn,eGe),e(M,oGe),e(M,Gn),e(Gn,XQ),e(XQ,rGe),e(Gn,tGe),e(Gn,I8),e(I8,aGe),e(Gn,nGe),e(Gn,j8),e(j8,sGe),e(Gn,lGe),e(M,iGe),e(M,ug),e(ug,zQ),e(zQ,dGe),e(ug,cGe),e(ug,N8),e(N8,fGe),e(ug,mGe),e(M,gGe),e(M,On),e(On,VQ),e(VQ,hGe),e(On,pGe),e(On,D8),e(D8,_Ge),e(On,uGe),e(On,q8),e(q8,bGe),e(On,vGe),e(M,TGe),e(M,bg),e(bg,WQ),e(WQ,FGe),e(bg,CGe),e(bg,G8),e(G8,MGe),e(bg,EGe),e(M,yGe),e(M,vg),e(vg,QQ),e(QQ,wGe),e(vg,AGe),e(vg,O8),e(O8,LGe),e(vg,BGe),e(M,kGe),e(M,Tg),e(Tg,HQ),e(HQ,xGe),e(Tg,RGe),e(Tg,X8),e(X8,SGe),e(Tg,PGe),e(M,$Ge),e(M,Xn),e(Xn,UQ),e(UQ,IGe),e(Xn,jGe),e(Xn,z8),e(z8,NGe),e(Xn,DGe),e(Xn,V8),e(V8,qGe),e(Xn,GGe),e(M,OGe),e(M,zn),e(zn,JQ),e(JQ,XGe),e(zn,zGe),e(zn,W8),e(W8,VGe),e(zn,WGe),e(zn,Q8),e(Q8,QGe),e(zn,HGe),e(M,UGe),e(M,Vn),e(Vn,YQ),e(YQ,JGe),e(Vn,YGe),e(Vn,H8),e(H8,KGe),e(Vn,ZGe),e(Vn,U8),e(U8,eOe),e(Vn,oOe),e(M,rOe),e(M,Fg),e(Fg,KQ),e(KQ,tOe),e(Fg,aOe),e(Fg,J8),e(J8,nOe),e(Fg,sOe),e(M,lOe),e(M,Cg),e(Cg,ZQ),e(ZQ,iOe),e(Cg,dOe),e(Cg,Y8),e(Y8,cOe),e(Cg,fOe),e(M,mOe),e(M,Wn),e(Wn,eH),e(eH,gOe),e(Wn,hOe),e(Wn,K8),e(K8,pOe),e(Wn,_Oe),e(Wn,Z8),e(Z8,uOe),e(Wn,bOe),e(M,vOe),e(M,Mg),e(Mg,oH),e(oH,TOe),e(Mg,FOe),e(Mg,eB),e(eB,COe),e(Mg,MOe),e(M,EOe),e(M,Qn),e(Qn,rH),e(rH,yOe),e(Qn,wOe),e(Qn,oB),e(oB,AOe),e(Qn,LOe),e(Qn,rB),e(rB,BOe),e(Qn,kOe),e(M,xOe),e(M,Hn),e(Hn,tH),e(tH,ROe),e(Hn,SOe),e(Hn,tB),e(tB,POe),e(Hn,$Oe),e(Hn,aB),e(aB,IOe),e(Hn,jOe),e(M,NOe),e(M,Un),e(Un,aH),e(aH,DOe),e(Un,qOe),e(Un,nB),e(nB,GOe),e(Un,OOe),e(Un,nH),e(nH,XOe),e(Un,zOe),e(M,VOe),e(M,Eg),e(Eg,sH),e(sH,WOe),e(Eg,QOe),e(Eg,sB),e(sB,HOe),e(Eg,UOe),e(M,JOe),e(M,Jn),e(Jn,lH),e(lH,YOe),e(Jn,KOe),e(Jn,lB),e(lB,ZOe),e(Jn,eXe),e(Jn,iB),e(iB,oXe),e(Jn,rXe),e(M,tXe),e(M,yg),e(yg,iH),e(iH,aXe),e(yg,nXe),e(yg,dB),e(dB,sXe),e(yg,lXe),e(M,iXe),e(M,Yn),e(Yn,dH),e(dH,dXe),e(Yn,cXe),e(Yn,cB),e(cB,fXe),e(Yn,mXe),e(Yn,fB),e(fB,gXe),e(Yn,hXe),e(M,pXe),e(M,Kn),e(Kn,cH),e(cH,_Xe),e(Kn,uXe),e(Kn,mB),e(mB,bXe),e(Kn,vXe),e(Kn,gB),e(gB,TXe),e(Kn,FXe),e(M,CXe),e(M,Zn),e(Zn,fH),e(fH,MXe),e(Zn,EXe),e(Zn,hB),e(hB,yXe),e(Zn,wXe),e(Zn,pB),e(pB,AXe),e(Zn,LXe),e(M,BXe),e(M,wg),e(wg,mH),e(mH,kXe),e(wg,xXe),e(wg,_B),e(_B,RXe),e(wg,SXe),e(M,PXe),e(M,es),e(es,gH),e(gH,$Xe),e(es,IXe),e(es,uB),e(uB,jXe),e(es,NXe),e(es,bB),e(bB,DXe),e(es,qXe),e(M,GXe),e(M,Ag),e(Ag,hH),e(hH,OXe),e(Ag,XXe),e(Ag,vB),e(vB,zXe),e(Ag,VXe),e(M,WXe),e(M,os),e(os,pH),e(pH,QXe),e(os,HXe),e(os,TB),e(TB,UXe),e(os,JXe),e(os,FB),e(FB,YXe),e(os,KXe),e(M,ZXe),e(M,rs),e(rs,_H),e(_H,eze),e(rs,oze),e(rs,CB),e(CB,rze),e(rs,tze),e(rs,MB),e(MB,aze),e(rs,nze),e(M,sze),e(M,ts),e(ts,uH),e(uH,lze),e(ts,ize),e(ts,EB),e(EB,dze),e(ts,cze),e(ts,yB),e(yB,fze),e(ts,mze),e(M,gze),e(M,as),e(as,bH),e(bH,hze),e(as,pze),e(as,wB),e(wB,_ze),e(as,uze),e(as,AB),e(AB,bze),e(as,vze),e(M,Tze),e(M,Lg),e(Lg,vH),e(vH,Fze),e(Lg,Cze),e(Lg,LB),e(LB,Mze),e(Lg,Eze),e(M,yze),e(M,ns),e(ns,TH),e(TH,wze),e(ns,Aze),e(ns,BB),e(BB,Lze),e(ns,Bze),e(ns,kB),e(kB,kze),e(ns,xze),e(M,Rze),e(M,ss),e(ss,FH),e(FH,Sze),e(ss,Pze),e(ss,xB),e(xB,$ze),e(ss,Ize),e(ss,RB),e(RB,jze),e(ss,Nze),e(M,Dze),e(M,ls),e(ls,CH),e(CH,qze),e(ls,Gze),e(ls,SB),e(SB,Oze),e(ls,Xze),e(ls,PB),e(PB,zze),e(ls,Vze),e(M,Wze),e(M,is),e(is,MH),e(MH,Qze),e(is,Hze),e(is,$B),e($B,Uze),e(is,Jze),e(is,IB),e(IB,Yze),e(is,Kze),e(M,Zze),e(M,ds),e(ds,EH),e(EH,eVe),e(ds,oVe),e(ds,jB),e(jB,rVe),e(ds,tVe),e(ds,NB),e(NB,aVe),e(ds,nVe),e(M,sVe),e(M,cs),e(cs,yH),e(yH,lVe),e(cs,iVe),e(cs,DB),e(DB,dVe),e(cs,cVe),e(cs,qB),e(qB,fVe),e(cs,mVe),e(M,gVe),e(M,Bg),e(Bg,wH),e(wH,hVe),e(Bg,pVe),e(Bg,GB),e(GB,_Ve),e(Bg,uVe),e(M,bVe),e(M,fs),e(fs,AH),e(AH,vVe),e(fs,TVe),e(fs,OB),e(OB,FVe),e(fs,CVe),e(fs,XB),e(XB,MVe),e(fs,EVe),e(M,yVe),e(M,kg),e(kg,LH),e(LH,wVe),e(kg,AVe),e(kg,zB),e(zB,LVe),e(kg,BVe),e(M,kVe),e(M,xg),e(xg,BH),e(BH,xVe),e(xg,RVe),e(xg,VB),e(VB,SVe),e(xg,PVe),e(M,$Ve),e(M,ms),e(ms,kH),e(kH,IVe),e(ms,jVe),e(ms,WB),e(WB,NVe),e(ms,DVe),e(ms,QB),e(QB,qVe),e(ms,GVe),e(M,OVe),e(M,gs),e(gs,xH),e(xH,XVe),e(gs,zVe),e(gs,HB),e(HB,VVe),e(gs,WVe),e(gs,UB),e(UB,QVe),e(gs,HVe),e(M,UVe),e(M,Rg),e(Rg,RH),e(RH,JVe),e(Rg,YVe),e(Rg,JB),e(JB,KVe),e(Rg,ZVe),e(M,eWe),e(M,hs),e(hs,SH),e(SH,oWe),e(hs,rWe),e(hs,YB),e(YB,tWe),e(hs,aWe),e(hs,KB),e(KB,nWe),e(hs,sWe),e(M,lWe),e(M,ps),e(ps,PH),e(PH,iWe),e(ps,dWe),e(ps,ZB),e(ZB,cWe),e(ps,fWe),e(ps,ek),e(ek,mWe),e(ps,gWe),e(M,hWe),e(M,_s),e(_s,$H),e($H,pWe),e(_s,_We),e(_s,ok),e(ok,uWe),e(_s,bWe),e(_s,rk),e(rk,vWe),e(_s,TWe),e(M,FWe),e(M,us),e(us,IH),e(IH,CWe),e(us,MWe),e(us,tk),e(tk,EWe),e(us,yWe),e(us,ak),e(ak,wWe),e(us,AWe),e(M,LWe),e(M,bs),e(bs,jH),e(jH,BWe),e(bs,kWe),e(bs,nk),e(nk,xWe),e(bs,RWe),e(bs,sk),e(sk,SWe),e(bs,PWe),e(M,$We),e(M,Sg),e(Sg,NH),e(NH,IWe),e(Sg,jWe),e(Sg,lk),e(lk,NWe),e(Sg,DWe),e(M,qWe),e(M,Pg),e(Pg,DH),e(DH,GWe),e(Pg,OWe),e(Pg,ik),e(ik,XWe),e(Pg,zWe),e(M,VWe),e(M,$g),e($g,qH),e(qH,WWe),e($g,QWe),e($g,dk),e(dk,HWe),e($g,UWe),e(M,JWe),e(M,Ig),e(Ig,GH),e(GH,YWe),e(Ig,KWe),e(Ig,ck),e(ck,ZWe),e(Ig,eQe),e(M,oQe),e(M,vs),e(vs,OH),e(OH,rQe),e(vs,tQe),e(vs,fk),e(fk,aQe),e(vs,nQe),e(vs,mk),e(mk,sQe),e(vs,lQe),e(M,iQe),e(M,jg),e(jg,XH),e(XH,dQe),e(jg,cQe),e(jg,gk),e(gk,fQe),e(jg,mQe),e(M,gQe),e(M,Ts),e(Ts,zH),e(zH,hQe),e(Ts,pQe),e(Ts,hk),e(hk,_Qe),e(Ts,uQe),e(Ts,pk),e(pk,bQe),e(Ts,vQe),e(M,TQe),e(M,Fs),e(Fs,VH),e(VH,FQe),e(Fs,CQe),e(Fs,_k),e(_k,MQe),e(Fs,EQe),e(Fs,uk),e(uk,yQe),e(Fs,wQe),e(M,AQe),e(M,Cs),e(Cs,WH),e(WH,LQe),e(Cs,BQe),e(Cs,bk),e(bk,kQe),e(Cs,xQe),e(Cs,vk),e(vk,RQe),e(Cs,SQe),e(M,PQe),e(M,Ms),e(Ms,QH),e(QH,$Qe),e(Ms,IQe),e(Ms,Tk),e(Tk,jQe),e(Ms,NQe),e(Ms,Fk),e(Fk,DQe),e(Ms,qQe),e(M,GQe),e(M,Es),e(Es,HH),e(HH,OQe),e(Es,XQe),e(Es,Ck),e(Ck,zQe),e(Es,VQe),e(Es,Mk),e(Mk,WQe),e(Es,QQe),e(M,HQe),e(M,ys),e(ys,UH),e(UH,UQe),e(ys,JQe),e(ys,Ek),e(Ek,YQe),e(ys,KQe),e(ys,yk),e(yk,ZQe),e(ys,eHe),e(M,oHe),e(M,Ng),e(Ng,JH),e(JH,rHe),e(Ng,tHe),e(Ng,wk),e(wk,aHe),e(Ng,nHe),e(M,sHe),e(M,Dg),e(Dg,YH),e(YH,lHe),e(Dg,iHe),e(Dg,Ak),e(Ak,dHe),e(Dg,cHe),e(M,fHe),e(M,ws),e(ws,KH),e(KH,mHe),e(ws,gHe),e(ws,Lk),e(Lk,hHe),e(ws,pHe),e(ws,Bk),e(Bk,_He),e(ws,uHe),e(M,bHe),e(M,As),e(As,ZH),e(ZH,vHe),e(As,THe),e(As,kk),e(kk,FHe),e(As,CHe),e(As,xk),e(xk,MHe),e(As,EHe),e(M,yHe),e(M,Ls),e(Ls,eU),e(eU,wHe),e(Ls,AHe),e(Ls,Rk),e(Rk,LHe),e(Ls,BHe),e(Ls,Sk),e(Sk,kHe),e(Ls,xHe),e(M,RHe),e(M,qg),e(qg,oU),e(oU,SHe),e(qg,PHe),e(qg,Pk),e(Pk,$He),e(qg,IHe),e(M,jHe),e(M,Gg),e(Gg,rU),e(rU,NHe),e(Gg,DHe),e(Gg,$k),e($k,qHe),e(Gg,GHe),e(M,OHe),e(M,Og),e(Og,tU),e(tU,XHe),e(Og,zHe),e(Og,Ik),e(Ik,VHe),e(Og,WHe),e(M,QHe),e(M,Xg),e(Xg,aU),e(aU,HHe),e(Xg,UHe),e(Xg,jk),e(jk,JHe),e(Xg,YHe),e(M,KHe),e(M,Bs),e(Bs,nU),e(nU,ZHe),e(Bs,eUe),e(Bs,Nk),e(Nk,oUe),e(Bs,rUe),e(Bs,Dk),e(Dk,tUe),e(Bs,aUe),e(M,nUe),e(M,zg),e(zg,sU),e(sU,sUe),e(zg,lUe),e(zg,qk),e(qk,iUe),e(zg,dUe),e(M,cUe),e(M,Vg),e(Vg,lU),e(lU,fUe),e(Vg,mUe),e(Vg,Gk),e(Gk,gUe),e(Vg,hUe),e(M,pUe),e(M,ks),e(ks,iU),e(iU,_Ue),e(ks,uUe),e(ks,Ok),e(Ok,bUe),e(ks,vUe),e(ks,Xk),e(Xk,TUe),e(ks,FUe),e(M,CUe),e(M,xs),e(xs,dU),e(dU,MUe),e(xs,EUe),e(xs,zk),e(zk,yUe),e(xs,wUe),e(xs,Vk),e(Vk,AUe),e(xs,LUe),e(mo,BUe),e(mo,cU),e(cU,kUe),e(mo,xUe),g(TM,mo,null),e(Oo,RUe),e(Oo,Wg),g(FM,Wg,null),e(Wg,SUe),e(Wg,fU),e(fU,PUe),b(d,uLe,u),b(d,Ni,u),e(Ni,Qg),e(Qg,mU),g(CM,mU,null),e(Ni,$Ue),e(Ni,gU),e(gU,IUe),b(d,bLe,u),b(d,Xo,u),g(MM,Xo,null),e(Xo,jUe),e(Xo,EM),e(EM,NUe),e(EM,Wk),e(Wk,DUe),e(EM,qUe),e(Xo,GUe),e(Xo,yM),e(yM,OUe),e(yM,hU),e(hU,XUe),e(yM,zUe),e(Xo,VUe),e(Xo,Le),g(wM,Le,null),e(Le,WUe),e(Le,pU),e(pU,QUe),e(Le,HUe),e(Le,Na),e(Na,UUe),e(Na,_U),e(_U,JUe),e(Na,YUe),e(Na,uU),e(uU,KUe),e(Na,ZUe),e(Na,bU),e(bU,eJe),e(Na,oJe),e(Le,rJe),e(Le,se),e(se,Hg),e(Hg,vU),e(vU,tJe),e(Hg,aJe),e(Hg,Qk),e(Qk,nJe),e(Hg,sJe),e(se,lJe),e(se,Ug),e(Ug,TU),e(TU,iJe),e(Ug,dJe),e(Ug,Hk),e(Hk,cJe),e(Ug,fJe),e(se,mJe),e(se,Jg),e(Jg,FU),e(FU,gJe),e(Jg,hJe),e(Jg,Uk),e(Uk,pJe),e(Jg,_Je),e(se,uJe),e(se,Yg),e(Yg,CU),e(CU,bJe),e(Yg,vJe),e(Yg,Jk),e(Jk,TJe),e(Yg,FJe),e(se,CJe),e(se,Kg),e(Kg,MU),e(MU,MJe),e(Kg,EJe),e(Kg,Yk),e(Yk,yJe),e(Kg,wJe),e(se,AJe),e(se,Zg),e(Zg,EU),e(EU,LJe),e(Zg,BJe),e(Zg,Kk),e(Kk,kJe),e(Zg,xJe),e(se,RJe),e(se,eh),e(eh,yU),e(yU,SJe),e(eh,PJe),e(eh,Zk),e(Zk,$Je),e(eh,IJe),e(se,jJe),e(se,oh),e(oh,wU),e(wU,NJe),e(oh,DJe),e(oh,ex),e(ex,qJe),e(oh,GJe),e(se,OJe),e(se,rh),e(rh,AU),e(AU,XJe),e(rh,zJe),e(rh,ox),e(ox,VJe),e(rh,WJe),e(se,QJe),e(se,th),e(th,LU),e(LU,HJe),e(th,UJe),e(th,rx),e(rx,JJe),e(th,YJe),e(se,KJe),e(se,ah),e(ah,BU),e(BU,ZJe),e(ah,eYe),e(ah,tx),e(tx,oYe),e(ah,rYe),e(se,tYe),e(se,nh),e(nh,kU),e(kU,aYe),e(nh,nYe),e(nh,ax),e(ax,sYe),e(nh,lYe),e(se,iYe),e(se,sh),e(sh,xU),e(xU,dYe),e(sh,cYe),e(sh,nx),e(nx,fYe),e(sh,mYe),e(se,gYe),e(se,lh),e(lh,RU),e(RU,hYe),e(lh,pYe),e(lh,sx),e(sx,_Ye),e(lh,uYe),e(se,bYe),e(se,ih),e(ih,SU),e(SU,vYe),e(ih,TYe),e(ih,lx),e(lx,FYe),e(ih,CYe),e(Le,MYe),g(dh,Le,null),e(Le,EYe),e(Le,PU),e(PU,yYe),e(Le,wYe),g(AM,Le,null),e(Xo,AYe),e(Xo,ch),g(LM,ch,null),e(ch,LYe),e(ch,$U),e($U,BYe),b(d,vLe,u),b(d,Di,u),e(Di,fh),e(fh,IU),g(BM,IU,null),e(Di,kYe),e(Di,jU),e(jU,xYe),b(d,TLe,u),b(d,zo,u),g(kM,zo,null),e(zo,RYe),e(zo,xM),e(xM,SYe),e(xM,ix),e(ix,PYe),e(xM,$Ye),e(zo,IYe),e(zo,RM),e(RM,jYe),e(RM,NU),e(NU,NYe),e(RM,DYe),e(zo,qYe),e(zo,Be),g(SM,Be,null),e(Be,GYe),e(Be,DU),e(DU,OYe),e(Be,XYe),e(Be,qi),e(qi,zYe),e(qi,qU),e(qU,VYe),e(qi,WYe),e(qi,GU),e(GU,QYe),e(qi,HYe),e(Be,UYe),e(Be,we),e(we,mh),e(mh,OU),e(OU,JYe),e(mh,YYe),e(mh,dx),e(dx,KYe),e(mh,ZYe),e(we,eKe),e(we,gh),e(gh,XU),e(XU,oKe),e(gh,rKe),e(gh,cx),e(cx,tKe),e(gh,aKe),e(we,nKe),e(we,hh),e(hh,zU),e(zU,sKe),e(hh,lKe),e(hh,fx),e(fx,iKe),e(hh,dKe),e(we,cKe),e(we,ph),e(ph,VU),e(VU,fKe),e(ph,mKe),e(ph,mx),e(mx,gKe),e(ph,hKe),e(we,pKe),e(we,_h),e(_h,WU),e(WU,_Ke),e(_h,uKe),e(_h,gx),e(gx,bKe),e(_h,vKe),e(we,TKe),e(we,uh),e(uh,QU),e(QU,FKe),e(uh,CKe),e(uh,hx),e(hx,MKe),e(uh,EKe),e(we,yKe),e(we,bh),e(bh,HU),e(HU,wKe),e(bh,AKe),e(bh,px),e(px,LKe),e(bh,BKe),e(we,kKe),e(we,vh),e(vh,UU),e(UU,xKe),e(vh,RKe),e(vh,_x),e(_x,SKe),e(vh,PKe),e(Be,$Ke),g(Th,Be,null),e(Be,IKe),e(Be,JU),e(JU,jKe),e(Be,NKe),g(PM,Be,null),e(zo,DKe),e(zo,Fh),g($M,Fh,null),e(Fh,qKe),e(Fh,YU),e(YU,GKe),b(d,FLe,u),b(d,Gi,u),e(Gi,Ch),e(Ch,KU),g(IM,KU,null),e(Gi,OKe),e(Gi,ZU),e(ZU,XKe),b(d,CLe,u),b(d,Vo,u),g(jM,Vo,null),e(Vo,zKe),e(Vo,Oi),e(Oi,VKe),e(Oi,eJ),e(eJ,WKe),e(Oi,QKe),e(Oi,oJ),e(oJ,HKe),e(Oi,UKe),e(Vo,JKe),e(Vo,NM),e(NM,YKe),e(NM,rJ),e(rJ,KKe),e(NM,ZKe),e(Vo,eZe),e(Vo,Nr),g(DM,Nr,null),e(Nr,oZe),e(Nr,tJ),e(tJ,rZe),e(Nr,tZe),e(Nr,Xi),e(Xi,aZe),e(Xi,aJ),e(aJ,nZe),e(Xi,sZe),e(Xi,nJ),e(nJ,lZe),e(Xi,iZe),e(Nr,dZe),e(Nr,sJ),e(sJ,cZe),e(Nr,fZe),g(qM,Nr,null),e(Vo,mZe),e(Vo,ke),g(GM,ke,null),e(ke,gZe),e(ke,lJ),e(lJ,hZe),e(ke,pZe),e(ke,Da),e(Da,_Ze),e(Da,iJ),e(iJ,uZe),e(Da,bZe),e(Da,dJ),e(dJ,vZe),e(Da,TZe),e(Da,cJ),e(cJ,FZe),e(Da,CZe),e(ke,MZe),e(ke,F),e(F,Mh),e(Mh,fJ),e(fJ,EZe),e(Mh,yZe),e(Mh,ux),e(ux,wZe),e(Mh,AZe),e(F,LZe),e(F,Eh),e(Eh,mJ),e(mJ,BZe),e(Eh,kZe),e(Eh,bx),e(bx,xZe),e(Eh,RZe),e(F,SZe),e(F,yh),e(yh,gJ),e(gJ,PZe),e(yh,$Ze),e(yh,vx),e(vx,IZe),e(yh,jZe),e(F,NZe),e(F,wh),e(wh,hJ),e(hJ,DZe),e(wh,qZe),e(wh,Tx),e(Tx,GZe),e(wh,OZe),e(F,XZe),e(F,Ah),e(Ah,pJ),e(pJ,zZe),e(Ah,VZe),e(Ah,Fx),e(Fx,WZe),e(Ah,QZe),e(F,HZe),e(F,Lh),e(Lh,_J),e(_J,UZe),e(Lh,JZe),e(Lh,Cx),e(Cx,YZe),e(Lh,KZe),e(F,ZZe),e(F,Bh),e(Bh,uJ),e(uJ,eeo),e(Bh,oeo),e(Bh,Mx),e(Mx,reo),e(Bh,teo),e(F,aeo),e(F,kh),e(kh,bJ),e(bJ,neo),e(kh,seo),e(kh,Ex),e(Ex,leo),e(kh,ieo),e(F,deo),e(F,xh),e(xh,vJ),e(vJ,ceo),e(xh,feo),e(xh,yx),e(yx,meo),e(xh,geo),e(F,heo),e(F,Rh),e(Rh,TJ),e(TJ,peo),e(Rh,_eo),e(Rh,wx),e(wx,ueo),e(Rh,beo),e(F,veo),e(F,Sh),e(Sh,FJ),e(FJ,Teo),e(Sh,Feo),e(Sh,Ax),e(Ax,Ceo),e(Sh,Meo),e(F,Eeo),e(F,Ph),e(Ph,CJ),e(CJ,yeo),e(Ph,weo),e(Ph,Lx),e(Lx,Aeo),e(Ph,Leo),e(F,Beo),e(F,$h),e($h,MJ),e(MJ,keo),e($h,xeo),e($h,Bx),e(Bx,Reo),e($h,Seo),e(F,Peo),e(F,Ih),e(Ih,EJ),e(EJ,$eo),e(Ih,Ieo),e(Ih,kx),e(kx,jeo),e(Ih,Neo),e(F,Deo),e(F,jh),e(jh,yJ),e(yJ,qeo),e(jh,Geo),e(jh,xx),e(xx,Oeo),e(jh,Xeo),e(F,zeo),e(F,Nh),e(Nh,wJ),e(wJ,Veo),e(Nh,Weo),e(Nh,Rx),e(Rx,Qeo),e(Nh,Heo),e(F,Ueo),e(F,Dh),e(Dh,AJ),e(AJ,Jeo),e(Dh,Yeo),e(Dh,Sx),e(Sx,Keo),e(Dh,Zeo),e(F,eoo),e(F,qh),e(qh,LJ),e(LJ,ooo),e(qh,roo),e(qh,Px),e(Px,too),e(qh,aoo),e(F,noo),e(F,Gh),e(Gh,BJ),e(BJ,soo),e(Gh,loo),e(Gh,$x),e($x,ioo),e(Gh,doo),e(F,coo),e(F,Oh),e(Oh,kJ),e(kJ,foo),e(Oh,moo),e(Oh,Ix),e(Ix,goo),e(Oh,hoo),e(F,poo),e(F,Xh),e(Xh,xJ),e(xJ,_oo),e(Xh,uoo),e(Xh,jx),e(jx,boo),e(Xh,voo),e(F,Too),e(F,zh),e(zh,RJ),e(RJ,Foo),e(zh,Coo),e(zh,Nx),e(Nx,Moo),e(zh,Eoo),e(F,yoo),e(F,Vh),e(Vh,SJ),e(SJ,woo),e(Vh,Aoo),e(Vh,Dx),e(Dx,Loo),e(Vh,Boo),e(F,koo),e(F,Wh),e(Wh,PJ),e(PJ,xoo),e(Wh,Roo),e(Wh,qx),e(qx,Soo),e(Wh,Poo),e(F,$oo),e(F,Qh),e(Qh,$J),e($J,Ioo),e(Qh,joo),e(Qh,Gx),e(Gx,Noo),e(Qh,Doo),e(F,qoo),e(F,Rs),e(Rs,IJ),e(IJ,Goo),e(Rs,Ooo),e(Rs,Ox),e(Ox,Xoo),e(Rs,zoo),e(Rs,Xx),e(Xx,Voo),e(Rs,Woo),e(F,Qoo),e(F,Hh),e(Hh,jJ),e(jJ,Hoo),e(Hh,Uoo),e(Hh,zx),e(zx,Joo),e(Hh,Yoo),e(F,Koo),e(F,Uh),e(Uh,NJ),e(NJ,Zoo),e(Uh,ero),e(Uh,Vx),e(Vx,oro),e(Uh,rro),e(F,tro),e(F,Jh),e(Jh,DJ),e(DJ,aro),e(Jh,nro),e(Jh,Wx),e(Wx,sro),e(Jh,lro),e(F,iro),e(F,Yh),e(Yh,qJ),e(qJ,dro),e(Yh,cro),e(Yh,Qx),e(Qx,fro),e(Yh,mro),e(F,gro),e(F,Kh),e(Kh,GJ),e(GJ,hro),e(Kh,pro),e(Kh,Hx),e(Hx,_ro),e(Kh,uro),e(F,bro),e(F,Zh),e(Zh,OJ),e(OJ,vro),e(Zh,Tro),e(Zh,Ux),e(Ux,Fro),e(Zh,Cro),e(F,Mro),e(F,ep),e(ep,XJ),e(XJ,Ero),e(ep,yro),e(ep,Jx),e(Jx,wro),e(ep,Aro),e(F,Lro),e(F,op),e(op,zJ),e(zJ,Bro),e(op,kro),e(op,Yx),e(Yx,xro),e(op,Rro),e(F,Sro),e(F,rp),e(rp,VJ),e(VJ,Pro),e(rp,$ro),e(rp,Kx),e(Kx,Iro),e(rp,jro),e(F,Nro),e(F,tp),e(tp,WJ),e(WJ,Dro),e(tp,qro),e(tp,Zx),e(Zx,Gro),e(tp,Oro),e(F,Xro),e(F,ap),e(ap,QJ),e(QJ,zro),e(ap,Vro),e(ap,eR),e(eR,Wro),e(ap,Qro),e(F,Hro),e(F,np),e(np,HJ),e(HJ,Uro),e(np,Jro),e(np,oR),e(oR,Yro),e(np,Kro),e(F,Zro),e(F,sp),e(sp,UJ),e(UJ,eto),e(sp,oto),e(sp,rR),e(rR,rto),e(sp,tto),e(F,ato),e(F,lp),e(lp,JJ),e(JJ,nto),e(lp,sto),e(lp,tR),e(tR,lto),e(lp,ito),e(F,dto),e(F,ip),e(ip,YJ),e(YJ,cto),e(ip,fto),e(ip,aR),e(aR,mto),e(ip,gto),e(F,hto),e(F,dp),e(dp,KJ),e(KJ,pto),e(dp,_to),e(dp,nR),e(nR,uto),e(dp,bto),e(F,vto),e(F,cp),e(cp,ZJ),e(ZJ,Tto),e(cp,Fto),e(cp,sR),e(sR,Cto),e(cp,Mto),e(F,Eto),e(F,fp),e(fp,eY),e(eY,yto),e(fp,wto),e(fp,lR),e(lR,Ato),e(fp,Lto),e(F,Bto),e(F,mp),e(mp,oY),e(oY,kto),e(mp,xto),e(mp,iR),e(iR,Rto),e(mp,Sto),e(F,Pto),e(F,gp),e(gp,rY),e(rY,$to),e(gp,Ito),e(gp,dR),e(dR,jto),e(gp,Nto),e(F,Dto),e(F,hp),e(hp,tY),e(tY,qto),e(hp,Gto),e(hp,cR),e(cR,Oto),e(hp,Xto),e(F,zto),e(F,pp),e(pp,aY),e(aY,Vto),e(pp,Wto),e(pp,fR),e(fR,Qto),e(pp,Hto),e(F,Uto),e(F,_p),e(_p,nY),e(nY,Jto),e(_p,Yto),e(_p,mR),e(mR,Kto),e(_p,Zto),e(F,eao),e(F,up),e(up,sY),e(sY,oao),e(up,rao),e(up,gR),e(gR,tao),e(up,aao),e(F,nao),e(F,bp),e(bp,lY),e(lY,sao),e(bp,lao),e(bp,hR),e(hR,iao),e(bp,dao),e(F,cao),e(F,vp),e(vp,iY),e(iY,fao),e(vp,mao),e(vp,pR),e(pR,gao),e(vp,hao),e(F,pao),e(F,Tp),e(Tp,dY),e(dY,_ao),e(Tp,uao),e(Tp,_R),e(_R,bao),e(Tp,vao),e(F,Tao),e(F,Fp),e(Fp,cY),e(cY,Fao),e(Fp,Cao),e(Fp,uR),e(uR,Mao),e(Fp,Eao),e(F,yao),e(F,Cp),e(Cp,fY),e(fY,wao),e(Cp,Aao),e(Cp,bR),e(bR,Lao),e(Cp,Bao),e(F,kao),e(F,Mp),e(Mp,mY),e(mY,xao),e(Mp,Rao),e(Mp,vR),e(vR,Sao),e(Mp,Pao),e(F,$ao),e(F,Ep),e(Ep,gY),e(gY,Iao),e(Ep,jao),e(Ep,TR),e(TR,Nao),e(Ep,Dao),e(F,qao),e(F,yp),e(yp,hY),e(hY,Gao),e(yp,Oao),e(yp,FR),e(FR,Xao),e(yp,zao),e(F,Vao),e(F,wp),e(wp,pY),e(pY,Wao),e(wp,Qao),e(wp,CR),e(CR,Hao),e(wp,Uao),e(F,Jao),e(F,Ap),e(Ap,_Y),e(_Y,Yao),e(Ap,Kao),e(Ap,MR),e(MR,Zao),e(Ap,eno),e(F,ono),e(F,Lp),e(Lp,uY),e(uY,rno),e(Lp,tno),e(Lp,ER),e(ER,ano),e(Lp,nno),e(F,sno),e(F,Bp),e(Bp,bY),e(bY,lno),e(Bp,ino),e(Bp,yR),e(yR,dno),e(Bp,cno),e(F,fno),e(F,kp),e(kp,vY),e(vY,mno),e(kp,gno),e(kp,wR),e(wR,hno),e(kp,pno),e(F,_no),e(F,xp),e(xp,TY),e(TY,uno),e(xp,bno),e(xp,AR),e(AR,vno),e(xp,Tno),e(F,Fno),e(F,Rp),e(Rp,FY),e(FY,Cno),e(Rp,Mno),e(Rp,LR),e(LR,Eno),e(Rp,yno),e(F,wno),e(F,Sp),e(Sp,CY),e(CY,Ano),e(Sp,Lno),e(Sp,BR),e(BR,Bno),e(Sp,kno),e(F,xno),e(F,Pp),e(Pp,MY),e(MY,Rno),e(Pp,Sno),e(Pp,kR),e(kR,Pno),e(Pp,$no),e(F,Ino),e(F,$p),e($p,EY),e(EY,jno),e($p,Nno),e($p,xR),e(xR,Dno),e($p,qno),e(F,Gno),e(F,Ip),e(Ip,yY),e(yY,Ono),e(Ip,Xno),e(Ip,RR),e(RR,zno),e(Ip,Vno),e(F,Wno),e(F,jp),e(jp,wY),e(wY,Qno),e(jp,Hno),e(jp,SR),e(SR,Uno),e(jp,Jno),e(F,Yno),e(F,Np),e(Np,AY),e(AY,Kno),e(Np,Zno),e(Np,PR),e(PR,eso),e(Np,oso),e(F,rso),e(F,Dp),e(Dp,LY),e(LY,tso),e(Dp,aso),e(Dp,$R),e($R,nso),e(Dp,sso),e(F,lso),e(F,qp),e(qp,BY),e(BY,iso),e(qp,dso),e(qp,IR),e(IR,cso),e(qp,fso),e(F,mso),e(F,Gp),e(Gp,kY),e(kY,gso),e(Gp,hso),e(Gp,jR),e(jR,pso),e(Gp,_so),e(F,uso),e(F,Op),e(Op,xY),e(xY,bso),e(Op,vso),e(Op,NR),e(NR,Tso),e(Op,Fso),e(F,Cso),e(F,Xp),e(Xp,RY),e(RY,Mso),e(Xp,Eso),e(Xp,DR),e(DR,yso),e(Xp,wso),e(F,Aso),e(F,zp),e(zp,SY),e(SY,Lso),e(zp,Bso),e(zp,qR),e(qR,kso),e(zp,xso),e(F,Rso),e(F,Vp),e(Vp,PY),e(PY,Sso),e(Vp,Pso),e(Vp,GR),e(GR,$so),e(Vp,Iso),e(F,jso),e(F,Wp),e(Wp,$Y),e($Y,Nso),e(Wp,Dso),e(Wp,OR),e(OR,qso),e(Wp,Gso),e(F,Oso),e(F,Qp),e(Qp,IY),e(IY,Xso),e(Qp,zso),e(Qp,XR),e(XR,Vso),e(Qp,Wso),e(F,Qso),e(F,Hp),e(Hp,jY),e(jY,Hso),e(Hp,Uso),e(Hp,zR),e(zR,Jso),e(Hp,Yso),e(F,Kso),e(F,Up),e(Up,NY),e(NY,Zso),e(Up,elo),e(Up,VR),e(VR,olo),e(Up,rlo),e(F,tlo),e(F,Jp),e(Jp,DY),e(DY,alo),e(Jp,nlo),e(Jp,WR),e(WR,slo),e(Jp,llo),e(F,ilo),e(F,Yp),e(Yp,qY),e(qY,dlo),e(Yp,clo),e(Yp,QR),e(QR,flo),e(Yp,mlo),e(ke,glo),e(ke,Kp),e(Kp,hlo),e(Kp,GY),e(GY,plo),e(Kp,_lo),e(Kp,OY),e(OY,ulo),e(ke,blo),e(ke,XY),e(XY,vlo),e(ke,Tlo),g(OM,ke,null),b(d,MLe,u),b(d,zi,u),e(zi,Zp),e(Zp,zY),g(XM,zY,null),e(zi,Flo),e(zi,VY),e(VY,Clo),b(d,ELe,u),b(d,Wo,u),g(zM,Wo,null),e(Wo,Mlo),e(Wo,Vi),e(Vi,Elo),e(Vi,WY),e(WY,ylo),e(Vi,wlo),e(Vi,QY),e(QY,Alo),e(Vi,Llo),e(Wo,Blo),e(Wo,VM),e(VM,klo),e(VM,HY),e(HY,xlo),e(VM,Rlo),e(Wo,Slo),e(Wo,Dr),g(WM,Dr,null),e(Dr,Plo),e(Dr,UY),e(UY,$lo),e(Dr,Ilo),e(Dr,Wi),e(Wi,jlo),e(Wi,JY),e(JY,Nlo),e(Wi,Dlo),e(Wi,YY),e(YY,qlo),e(Wi,Glo),e(Dr,Olo),e(Dr,KY),e(KY,Xlo),e(Dr,zlo),g(QM,Dr,null),e(Wo,Vlo),e(Wo,xe),g(HM,xe,null),e(xe,Wlo),e(xe,ZY),e(ZY,Qlo),e(xe,Hlo),e(xe,qa),e(qa,Ulo),e(qa,eK),e(eK,Jlo),e(qa,Ylo),e(qa,oK),e(oK,Klo),e(qa,Zlo),e(qa,rK),e(rK,eio),e(qa,oio),e(xe,rio),e(xe,x),e(x,e_),e(e_,tK),e(tK,tio),e(e_,aio),e(e_,HR),e(HR,nio),e(e_,sio),e(x,lio),e(x,o_),e(o_,aK),e(aK,iio),e(o_,dio),e(o_,UR),e(UR,cio),e(o_,fio),e(x,mio),e(x,r_),e(r_,nK),e(nK,gio),e(r_,hio),e(r_,JR),e(JR,pio),e(r_,_io),e(x,uio),e(x,t_),e(t_,sK),e(sK,bio),e(t_,vio),e(t_,YR),e(YR,Tio),e(t_,Fio),e(x,Cio),e(x,a_),e(a_,lK),e(lK,Mio),e(a_,Eio),e(a_,KR),e(KR,yio),e(a_,wio),e(x,Aio),e(x,n_),e(n_,iK),e(iK,Lio),e(n_,Bio),e(n_,ZR),e(ZR,kio),e(n_,xio),e(x,Rio),e(x,s_),e(s_,dK),e(dK,Sio),e(s_,Pio),e(s_,eS),e(eS,$io),e(s_,Iio),e(x,jio),e(x,l_),e(l_,cK),e(cK,Nio),e(l_,Dio),e(l_,oS),e(oS,qio),e(l_,Gio),e(x,Oio),e(x,i_),e(i_,fK),e(fK,Xio),e(i_,zio),e(i_,rS),e(rS,Vio),e(i_,Wio),e(x,Qio),e(x,d_),e(d_,mK),e(mK,Hio),e(d_,Uio),e(d_,tS),e(tS,Jio),e(d_,Yio),e(x,Kio),e(x,c_),e(c_,gK),e(gK,Zio),e(c_,edo),e(c_,aS),e(aS,odo),e(c_,rdo),e(x,tdo),e(x,f_),e(f_,hK),e(hK,ado),e(f_,ndo),e(f_,nS),e(nS,sdo),e(f_,ldo),e(x,ido),e(x,m_),e(m_,pK),e(pK,ddo),e(m_,cdo),e(m_,sS),e(sS,fdo),e(m_,mdo),e(x,gdo),e(x,g_),e(g_,_K),e(_K,hdo),e(g_,pdo),e(g_,lS),e(lS,_do),e(g_,udo),e(x,bdo),e(x,h_),e(h_,uK),e(uK,vdo),e(h_,Tdo),e(h_,iS),e(iS,Fdo),e(h_,Cdo),e(x,Mdo),e(x,p_),e(p_,bK),e(bK,Edo),e(p_,ydo),e(p_,dS),e(dS,wdo),e(p_,Ado),e(x,Ldo),e(x,__),e(__,vK),e(vK,Bdo),e(__,kdo),e(__,cS),e(cS,xdo),e(__,Rdo),e(x,Sdo),e(x,u_),e(u_,TK),e(TK,Pdo),e(u_,$do),e(u_,fS),e(fS,Ido),e(u_,jdo),e(x,Ndo),e(x,b_),e(b_,FK),e(FK,Ddo),e(b_,qdo),e(b_,mS),e(mS,Gdo),e(b_,Odo),e(x,Xdo),e(x,v_),e(v_,CK),e(CK,zdo),e(v_,Vdo),e(v_,gS),e(gS,Wdo),e(v_,Qdo),e(x,Hdo),e(x,T_),e(T_,MK),e(MK,Udo),e(T_,Jdo),e(T_,hS),e(hS,Ydo),e(T_,Kdo),e(x,Zdo),e(x,F_),e(F_,EK),e(EK,eco),e(F_,oco),e(F_,pS),e(pS,rco),e(F_,tco),e(x,aco),e(x,C_),e(C_,yK),e(yK,nco),e(C_,sco),e(C_,_S),e(_S,lco),e(C_,ico),e(x,dco),e(x,M_),e(M_,wK),e(wK,cco),e(M_,fco),e(M_,uS),e(uS,mco),e(M_,gco),e(x,hco),e(x,E_),e(E_,AK),e(AK,pco),e(E_,_co),e(E_,bS),e(bS,uco),e(E_,bco),e(x,vco),e(x,y_),e(y_,LK),e(LK,Tco),e(y_,Fco),e(y_,vS),e(vS,Cco),e(y_,Mco),e(x,Eco),e(x,w_),e(w_,BK),e(BK,yco),e(w_,wco),e(w_,TS),e(TS,Aco),e(w_,Lco),e(x,Bco),e(x,A_),e(A_,kK),e(kK,kco),e(A_,xco),e(A_,FS),e(FS,Rco),e(A_,Sco),e(x,Pco),e(x,L_),e(L_,xK),e(xK,$co),e(L_,Ico),e(L_,CS),e(CS,jco),e(L_,Nco),e(x,Dco),e(x,B_),e(B_,RK),e(RK,qco),e(B_,Gco),e(B_,MS),e(MS,Oco),e(B_,Xco),e(x,zco),e(x,k_),e(k_,SK),e(SK,Vco),e(k_,Wco),e(k_,ES),e(ES,Qco),e(k_,Hco),e(x,Uco),e(x,x_),e(x_,PK),e(PK,Jco),e(x_,Yco),e(x_,yS),e(yS,Kco),e(x_,Zco),e(x,efo),e(x,R_),e(R_,$K),e($K,ofo),e(R_,rfo),e(R_,wS),e(wS,tfo),e(R_,afo),e(x,nfo),e(x,S_),e(S_,IK),e(IK,sfo),e(S_,lfo),e(S_,AS),e(AS,ifo),e(S_,dfo),e(x,cfo),e(x,P_),e(P_,jK),e(jK,ffo),e(P_,mfo),e(P_,LS),e(LS,gfo),e(P_,hfo),e(x,pfo),e(x,$_),e($_,NK),e(NK,_fo),e($_,ufo),e($_,BS),e(BS,bfo),e($_,vfo),e(x,Tfo),e(x,I_),e(I_,DK),e(DK,Ffo),e(I_,Cfo),e(I_,kS),e(kS,Mfo),e(I_,Efo),e(x,yfo),e(x,j_),e(j_,qK),e(qK,wfo),e(j_,Afo),e(j_,xS),e(xS,Lfo),e(j_,Bfo),e(xe,kfo),e(xe,N_),e(N_,xfo),e(N_,GK),e(GK,Rfo),e(N_,Sfo),e(N_,OK),e(OK,Pfo),e(xe,$fo),e(xe,XK),e(XK,Ifo),e(xe,jfo),g(UM,xe,null),b(d,yLe,u),b(d,Qi,u),e(Qi,D_),e(D_,zK),g(JM,zK,null),e(Qi,Nfo),e(Qi,VK),e(VK,Dfo),b(d,wLe,u),b(d,Qo,u),g(YM,Qo,null),e(Qo,qfo),e(Qo,Hi),e(Hi,Gfo),e(Hi,WK),e(WK,Ofo),e(Hi,Xfo),e(Hi,QK),e(QK,zfo),e(Hi,Vfo),e(Qo,Wfo),e(Qo,KM),e(KM,Qfo),e(KM,HK),e(HK,Hfo),e(KM,Ufo),e(Qo,Jfo),e(Qo,qr),g(ZM,qr,null),e(qr,Yfo),e(qr,UK),e(UK,Kfo),e(qr,Zfo),e(qr,Ui),e(Ui,emo),e(Ui,JK),e(JK,omo),e(Ui,rmo),e(Ui,YK),e(YK,tmo),e(Ui,amo),e(qr,nmo),e(qr,KK),e(KK,smo),e(qr,lmo),g(eE,qr,null),e(Qo,imo),e(Qo,Re),g(oE,Re,null),e(Re,dmo),e(Re,ZK),e(ZK,cmo),e(Re,fmo),e(Re,Ga),e(Ga,mmo),e(Ga,eZ),e(eZ,gmo),e(Ga,hmo),e(Ga,oZ),e(oZ,pmo),e(Ga,_mo),e(Ga,rZ),e(rZ,umo),e(Ga,bmo),e(Re,vmo),e(Re,$),e($,q_),e(q_,tZ),e(tZ,Tmo),e(q_,Fmo),e(q_,RS),e(RS,Cmo),e(q_,Mmo),e($,Emo),e($,G_),e(G_,aZ),e(aZ,ymo),e(G_,wmo),e(G_,SS),e(SS,Amo),e(G_,Lmo),e($,Bmo),e($,O_),e(O_,nZ),e(nZ,kmo),e(O_,xmo),e(O_,PS),e(PS,Rmo),e(O_,Smo),e($,Pmo),e($,X_),e(X_,sZ),e(sZ,$mo),e(X_,Imo),e(X_,$S),e($S,jmo),e(X_,Nmo),e($,Dmo),e($,z_),e(z_,lZ),e(lZ,qmo),e(z_,Gmo),e(z_,IS),e(IS,Omo),e(z_,Xmo),e($,zmo),e($,V_),e(V_,iZ),e(iZ,Vmo),e(V_,Wmo),e(V_,jS),e(jS,Qmo),e(V_,Hmo),e($,Umo),e($,W_),e(W_,dZ),e(dZ,Jmo),e(W_,Ymo),e(W_,NS),e(NS,Kmo),e(W_,Zmo),e($,ego),e($,Q_),e(Q_,cZ),e(cZ,ogo),e(Q_,rgo),e(Q_,DS),e(DS,tgo),e(Q_,ago),e($,ngo),e($,H_),e(H_,fZ),e(fZ,sgo),e(H_,lgo),e(H_,qS),e(qS,igo),e(H_,dgo),e($,cgo),e($,U_),e(U_,mZ),e(mZ,fgo),e(U_,mgo),e(U_,GS),e(GS,ggo),e(U_,hgo),e($,pgo),e($,J_),e(J_,gZ),e(gZ,_go),e(J_,ugo),e(J_,OS),e(OS,bgo),e(J_,vgo),e($,Tgo),e($,Y_),e(Y_,hZ),e(hZ,Fgo),e(Y_,Cgo),e(Y_,XS),e(XS,Mgo),e(Y_,Ego),e($,ygo),e($,K_),e(K_,pZ),e(pZ,wgo),e(K_,Ago),e(K_,zS),e(zS,Lgo),e(K_,Bgo),e($,kgo),e($,Z_),e(Z_,_Z),e(_Z,xgo),e(Z_,Rgo),e(Z_,VS),e(VS,Sgo),e(Z_,Pgo),e($,$go),e($,eu),e(eu,uZ),e(uZ,Igo),e(eu,jgo),e(eu,WS),e(WS,Ngo),e(eu,Dgo),e($,qgo),e($,ou),e(ou,bZ),e(bZ,Ggo),e(ou,Ogo),e(ou,QS),e(QS,Xgo),e(ou,zgo),e($,Vgo),e($,ru),e(ru,vZ),e(vZ,Wgo),e(ru,Qgo),e(ru,HS),e(HS,Hgo),e(ru,Ugo),e($,Jgo),e($,tu),e(tu,TZ),e(TZ,Ygo),e(tu,Kgo),e(tu,US),e(US,Zgo),e(tu,eho),e($,oho),e($,au),e(au,FZ),e(FZ,rho),e(au,tho),e(au,JS),e(JS,aho),e(au,nho),e($,sho),e($,nu),e(nu,CZ),e(CZ,lho),e(nu,iho),e(nu,YS),e(YS,dho),e(nu,cho),e($,fho),e($,su),e(su,MZ),e(MZ,mho),e(su,gho),e(su,KS),e(KS,hho),e(su,pho),e($,_ho),e($,lu),e(lu,EZ),e(EZ,uho),e(lu,bho),e(lu,ZS),e(ZS,vho),e(lu,Tho),e($,Fho),e($,iu),e(iu,yZ),e(yZ,Cho),e(iu,Mho),e(iu,eP),e(eP,Eho),e(iu,yho),e($,who),e($,du),e(du,wZ),e(wZ,Aho),e(du,Lho),e(du,oP),e(oP,Bho),e(du,kho),e($,xho),e($,cu),e(cu,AZ),e(AZ,Rho),e(cu,Sho),e(cu,rP),e(rP,Pho),e(cu,$ho),e($,Iho),e($,fu),e(fu,LZ),e(LZ,jho),e(fu,Nho),e(fu,tP),e(tP,Dho),e(fu,qho),e($,Gho),e($,mu),e(mu,BZ),e(BZ,Oho),e(mu,Xho),e(mu,aP),e(aP,zho),e(mu,Vho),e($,Who),e($,gu),e(gu,kZ),e(kZ,Qho),e(gu,Hho),e(gu,nP),e(nP,Uho),e(gu,Jho),e($,Yho),e($,hu),e(hu,xZ),e(xZ,Kho),e(hu,Zho),e(hu,sP),e(sP,epo),e(hu,opo),e($,rpo),e($,pu),e(pu,RZ),e(RZ,tpo),e(pu,apo),e(pu,lP),e(lP,npo),e(pu,spo),e($,lpo),e($,_u),e(_u,SZ),e(SZ,ipo),e(_u,dpo),e(_u,iP),e(iP,cpo),e(_u,fpo),e($,mpo),e($,uu),e(uu,PZ),e(PZ,gpo),e(uu,hpo),e(uu,dP),e(dP,ppo),e(uu,_po),e($,upo),e($,bu),e(bu,$Z),e($Z,bpo),e(bu,vpo),e(bu,cP),e(cP,Tpo),e(bu,Fpo),e($,Cpo),e($,vu),e(vu,IZ),e(IZ,Mpo),e(vu,Epo),e(vu,fP),e(fP,ypo),e(vu,wpo),e(Re,Apo),e(Re,Tu),e(Tu,Lpo),e(Tu,jZ),e(jZ,Bpo),e(Tu,kpo),e(Tu,NZ),e(NZ,xpo),e(Re,Rpo),e(Re,DZ),e(DZ,Spo),e(Re,Ppo),g(rE,Re,null),b(d,ALe,u),b(d,Ji,u),e(Ji,Fu),e(Fu,qZ),g(tE,qZ,null),e(Ji,$po),e(Ji,GZ),e(GZ,Ipo),b(d,LLe,u),b(d,Ho,u),g(aE,Ho,null),e(Ho,jpo),e(Ho,Yi),e(Yi,Npo),e(Yi,OZ),e(OZ,Dpo),e(Yi,qpo),e(Yi,XZ),e(XZ,Gpo),e(Yi,Opo),e(Ho,Xpo),e(Ho,nE),e(nE,zpo),e(nE,zZ),e(zZ,Vpo),e(nE,Wpo),e(Ho,Qpo),e(Ho,Gr),g(sE,Gr,null),e(Gr,Hpo),e(Gr,VZ),e(VZ,Upo),e(Gr,Jpo),e(Gr,Ki),e(Ki,Ypo),e(Ki,WZ),e(WZ,Kpo),e(Ki,Zpo),e(Ki,QZ),e(QZ,e_o),e(Ki,o_o),e(Gr,r_o),e(Gr,HZ),e(HZ,t_o),e(Gr,a_o),g(lE,Gr,null),e(Ho,n_o),e(Ho,Se),g(iE,Se,null),e(Se,s_o),e(Se,UZ),e(UZ,l_o),e(Se,i_o),e(Se,Oa),e(Oa,d_o),e(Oa,JZ),e(JZ,c_o),e(Oa,f_o),e(Oa,YZ),e(YZ,m_o),e(Oa,g_o),e(Oa,KZ),e(KZ,h_o),e(Oa,p_o),e(Se,__o),e(Se,I),e(I,Cu),e(Cu,ZZ),e(ZZ,u_o),e(Cu,b_o),e(Cu,mP),e(mP,v_o),e(Cu,T_o),e(I,F_o),e(I,Mu),e(Mu,eee),e(eee,C_o),e(Mu,M_o),e(Mu,gP),e(gP,E_o),e(Mu,y_o),e(I,w_o),e(I,Eu),e(Eu,oee),e(oee,A_o),e(Eu,L_o),e(Eu,hP),e(hP,B_o),e(Eu,k_o),e(I,x_o),e(I,yu),e(yu,ree),e(ree,R_o),e(yu,S_o),e(yu,pP),e(pP,P_o),e(yu,$_o),e(I,I_o),e(I,wu),e(wu,tee),e(tee,j_o),e(wu,N_o),e(wu,_P),e(_P,D_o),e(wu,q_o),e(I,G_o),e(I,Au),e(Au,aee),e(aee,O_o),e(Au,X_o),e(Au,uP),e(uP,z_o),e(Au,V_o),e(I,W_o),e(I,Lu),e(Lu,nee),e(nee,Q_o),e(Lu,H_o),e(Lu,bP),e(bP,U_o),e(Lu,J_o),e(I,Y_o),e(I,Bu),e(Bu,see),e(see,K_o),e(Bu,Z_o),e(Bu,vP),e(vP,euo),e(Bu,ouo),e(I,ruo),e(I,ku),e(ku,lee),e(lee,tuo),e(ku,auo),e(ku,TP),e(TP,nuo),e(ku,suo),e(I,luo),e(I,xu),e(xu,iee),e(iee,iuo),e(xu,duo),e(xu,FP),e(FP,cuo),e(xu,fuo),e(I,muo),e(I,Ru),e(Ru,dee),e(dee,guo),e(Ru,huo),e(Ru,CP),e(CP,puo),e(Ru,_uo),e(I,uuo),e(I,Su),e(Su,cee),e(cee,buo),e(Su,vuo),e(Su,MP),e(MP,Tuo),e(Su,Fuo),e(I,Cuo),e(I,Pu),e(Pu,fee),e(fee,Muo),e(Pu,Euo),e(Pu,EP),e(EP,yuo),e(Pu,wuo),e(I,Auo),e(I,$u),e($u,mee),e(mee,Luo),e($u,Buo),e($u,yP),e(yP,kuo),e($u,xuo),e(I,Ruo),e(I,Iu),e(Iu,gee),e(gee,Suo),e(Iu,Puo),e(Iu,wP),e(wP,$uo),e(Iu,Iuo),e(I,juo),e(I,ju),e(ju,hee),e(hee,Nuo),e(ju,Duo),e(ju,AP),e(AP,quo),e(ju,Guo),e(I,Ouo),e(I,Nu),e(Nu,pee),e(pee,Xuo),e(Nu,zuo),e(Nu,LP),e(LP,Vuo),e(Nu,Wuo),e(I,Quo),e(I,Du),e(Du,_ee),e(_ee,Huo),e(Du,Uuo),e(Du,BP),e(BP,Juo),e(Du,Yuo),e(I,Kuo),e(I,qu),e(qu,uee),e(uee,Zuo),e(qu,e2o),e(qu,kP),e(kP,o2o),e(qu,r2o),e(I,t2o),e(I,Gu),e(Gu,bee),e(bee,a2o),e(Gu,n2o),e(Gu,xP),e(xP,s2o),e(Gu,l2o),e(I,i2o),e(I,Ou),e(Ou,vee),e(vee,d2o),e(Ou,c2o),e(Ou,RP),e(RP,f2o),e(Ou,m2o),e(I,g2o),e(I,Xu),e(Xu,Tee),e(Tee,h2o),e(Xu,p2o),e(Xu,SP),e(SP,_2o),e(Xu,u2o),e(I,b2o),e(I,zu),e(zu,Fee),e(Fee,v2o),e(zu,T2o),e(zu,PP),e(PP,F2o),e(zu,C2o),e(I,M2o),e(I,Vu),e(Vu,Cee),e(Cee,E2o),e(Vu,y2o),e(Vu,$P),e($P,w2o),e(Vu,A2o),e(I,L2o),e(I,Wu),e(Wu,Mee),e(Mee,B2o),e(Wu,k2o),e(Wu,IP),e(IP,x2o),e(Wu,R2o),e(I,S2o),e(I,Qu),e(Qu,Eee),e(Eee,P2o),e(Qu,$2o),e(Qu,jP),e(jP,I2o),e(Qu,j2o),e(I,N2o),e(I,Hu),e(Hu,yee),e(yee,D2o),e(Hu,q2o),e(Hu,NP),e(NP,G2o),e(Hu,O2o),e(I,X2o),e(I,Uu),e(Uu,wee),e(wee,z2o),e(Uu,V2o),e(Uu,DP),e(DP,W2o),e(Uu,Q2o),e(I,H2o),e(I,Ju),e(Ju,Aee),e(Aee,U2o),e(Ju,J2o),e(Ju,qP),e(qP,Y2o),e(Ju,K2o),e(I,Z2o),e(I,Yu),e(Yu,Lee),e(Lee,e1o),e(Yu,o1o),e(Yu,Bee),e(Bee,r1o),e(Yu,t1o),e(I,a1o),e(I,Ku),e(Ku,kee),e(kee,n1o),e(Ku,s1o),e(Ku,GP),e(GP,l1o),e(Ku,i1o),e(I,d1o),e(I,Zu),e(Zu,xee),e(xee,c1o),e(Zu,f1o),e(Zu,OP),e(OP,m1o),e(Zu,g1o),e(I,h1o),e(I,e2),e(e2,Ree),e(Ree,p1o),e(e2,_1o),e(e2,XP),e(XP,u1o),e(e2,b1o),e(I,v1o),e(I,o2),e(o2,See),e(See,T1o),e(o2,F1o),e(o2,zP),e(zP,C1o),e(o2,M1o),e(Se,E1o),e(Se,r2),e(r2,y1o),e(r2,Pee),e(Pee,w1o),e(r2,A1o),e(r2,$ee),e($ee,L1o),e(Se,B1o),e(Se,Iee),e(Iee,k1o),e(Se,x1o),g(dE,Se,null),b(d,BLe,u),b(d,Zi,u),e(Zi,t2),e(t2,jee),g(cE,jee,null),e(Zi,R1o),e(Zi,Nee),e(Nee,S1o),b(d,kLe,u),b(d,Uo,u),g(fE,Uo,null),e(Uo,P1o),e(Uo,ed),e(ed,$1o),e(ed,Dee),e(Dee,I1o),e(ed,j1o),e(ed,qee),e(qee,N1o),e(ed,D1o),e(Uo,q1o),e(Uo,mE),e(mE,G1o),e(mE,Gee),e(Gee,O1o),e(mE,X1o),e(Uo,z1o),e(Uo,Or),g(gE,Or,null),e(Or,V1o),e(Or,Oee),e(Oee,W1o),e(Or,Q1o),e(Or,od),e(od,H1o),e(od,Xee),e(Xee,U1o),e(od,J1o),e(od,zee),e(zee,Y1o),e(od,K1o),e(Or,Z1o),e(Or,Vee),e(Vee,ebo),e(Or,obo),g(hE,Or,null),e(Uo,rbo),e(Uo,Pe),g(pE,Pe,null),e(Pe,tbo),e(Pe,Wee),e(Wee,abo),e(Pe,nbo),e(Pe,Xa),e(Xa,sbo),e(Xa,Qee),e(Qee,lbo),e(Xa,ibo),e(Xa,Hee),e(Hee,dbo),e(Xa,cbo),e(Xa,Uee),e(Uee,fbo),e(Xa,mbo),e(Pe,gbo),e(Pe,ae),e(ae,a2),e(a2,Jee),e(Jee,hbo),e(a2,pbo),e(a2,VP),e(VP,_bo),e(a2,ubo),e(ae,bbo),e(ae,n2),e(n2,Yee),e(Yee,vbo),e(n2,Tbo),e(n2,WP),e(WP,Fbo),e(n2,Cbo),e(ae,Mbo),e(ae,s2),e(s2,Kee),e(Kee,Ebo),e(s2,ybo),e(s2,QP),e(QP,wbo),e(s2,Abo),e(ae,Lbo),e(ae,l2),e(l2,Zee),e(Zee,Bbo),e(l2,kbo),e(l2,HP),e(HP,xbo),e(l2,Rbo),e(ae,Sbo),e(ae,i2),e(i2,eoe),e(eoe,Pbo),e(i2,$bo),e(i2,UP),e(UP,Ibo),e(i2,jbo),e(ae,Nbo),e(ae,d2),e(d2,ooe),e(ooe,Dbo),e(d2,qbo),e(d2,JP),e(JP,Gbo),e(d2,Obo),e(ae,Xbo),e(ae,c2),e(c2,roe),e(roe,zbo),e(c2,Vbo),e(c2,YP),e(YP,Wbo),e(c2,Qbo),e(ae,Hbo),e(ae,f2),e(f2,toe),e(toe,Ubo),e(f2,Jbo),e(f2,KP),e(KP,Ybo),e(f2,Kbo),e(ae,Zbo),e(ae,m2),e(m2,aoe),e(aoe,e5o),e(m2,o5o),e(m2,ZP),e(ZP,r5o),e(m2,t5o),e(ae,a5o),e(ae,g2),e(g2,noe),e(noe,n5o),e(g2,s5o),e(g2,e$),e(e$,l5o),e(g2,i5o),e(ae,d5o),e(ae,h2),e(h2,soe),e(soe,c5o),e(h2,f5o),e(h2,o$),e(o$,m5o),e(h2,g5o),e(ae,h5o),e(ae,p2),e(p2,loe),e(loe,p5o),e(p2,_5o),e(p2,r$),e(r$,u5o),e(p2,b5o),e(ae,v5o),e(ae,_2),e(_2,ioe),e(ioe,T5o),e(_2,F5o),e(_2,t$),e(t$,C5o),e(_2,M5o),e(ae,E5o),e(ae,u2),e(u2,doe),e(doe,y5o),e(u2,w5o),e(u2,a$),e(a$,A5o),e(u2,L5o),e(ae,B5o),e(ae,b2),e(b2,coe),e(coe,k5o),e(b2,x5o),e(b2,n$),e(n$,R5o),e(b2,S5o),e(ae,P5o),e(ae,v2),e(v2,foe),e(foe,$5o),e(v2,I5o),e(v2,s$),e(s$,j5o),e(v2,N5o),e(Pe,D5o),e(Pe,T2),e(T2,q5o),e(T2,moe),e(moe,G5o),e(T2,O5o),e(T2,goe),e(goe,X5o),e(Pe,z5o),e(Pe,hoe),e(hoe,V5o),e(Pe,W5o),g(_E,Pe,null),b(d,xLe,u),b(d,rd,u),e(rd,F2),e(F2,poe),g(uE,poe,null),e(rd,Q5o),e(rd,_oe),e(_oe,H5o),b(d,RLe,u),b(d,Jo,u),g(bE,Jo,null),e(Jo,U5o),e(Jo,td),e(td,J5o),e(td,uoe),e(uoe,Y5o),e(td,K5o),e(td,boe),e(boe,Z5o),e(td,evo),e(Jo,ovo),e(Jo,vE),e(vE,rvo),e(vE,voe),e(voe,tvo),e(vE,avo),e(Jo,nvo),e(Jo,Xr),g(TE,Xr,null),e(Xr,svo),e(Xr,Toe),e(Toe,lvo),e(Xr,ivo),e(Xr,ad),e(ad,dvo),e(ad,Foe),e(Foe,cvo),e(ad,fvo),e(ad,Coe),e(Coe,mvo),e(ad,gvo),e(Xr,hvo),e(Xr,Moe),e(Moe,pvo),e(Xr,_vo),g(FE,Xr,null),e(Jo,uvo),e(Jo,$e),g(CE,$e,null),e($e,bvo),e($e,Eoe),e(Eoe,vvo),e($e,Tvo),e($e,za),e(za,Fvo),e(za,yoe),e(yoe,Cvo),e(za,Mvo),e(za,woe),e(woe,Evo),e(za,yvo),e(za,Aoe),e(Aoe,wvo),e(za,Avo),e($e,Lvo),e($e,A),e(A,C2),e(C2,Loe),e(Loe,Bvo),e(C2,kvo),e(C2,l$),e(l$,xvo),e(C2,Rvo),e(A,Svo),e(A,M2),e(M2,Boe),e(Boe,Pvo),e(M2,$vo),e(M2,i$),e(i$,Ivo),e(M2,jvo),e(A,Nvo),e(A,E2),e(E2,koe),e(koe,Dvo),e(E2,qvo),e(E2,d$),e(d$,Gvo),e(E2,Ovo),e(A,Xvo),e(A,y2),e(y2,xoe),e(xoe,zvo),e(y2,Vvo),e(y2,c$),e(c$,Wvo),e(y2,Qvo),e(A,Hvo),e(A,w2),e(w2,Roe),e(Roe,Uvo),e(w2,Jvo),e(w2,f$),e(f$,Yvo),e(w2,Kvo),e(A,Zvo),e(A,A2),e(A2,Soe),e(Soe,eTo),e(A2,oTo),e(A2,m$),e(m$,rTo),e(A2,tTo),e(A,aTo),e(A,L2),e(L2,Poe),e(Poe,nTo),e(L2,sTo),e(L2,g$),e(g$,lTo),e(L2,iTo),e(A,dTo),e(A,B2),e(B2,$oe),e($oe,cTo),e(B2,fTo),e(B2,h$),e(h$,mTo),e(B2,gTo),e(A,hTo),e(A,k2),e(k2,Ioe),e(Ioe,pTo),e(k2,_To),e(k2,p$),e(p$,uTo),e(k2,bTo),e(A,vTo),e(A,x2),e(x2,joe),e(joe,TTo),e(x2,FTo),e(x2,_$),e(_$,CTo),e(x2,MTo),e(A,ETo),e(A,R2),e(R2,Noe),e(Noe,yTo),e(R2,wTo),e(R2,u$),e(u$,ATo),e(R2,LTo),e(A,BTo),e(A,S2),e(S2,Doe),e(Doe,kTo),e(S2,xTo),e(S2,b$),e(b$,RTo),e(S2,STo),e(A,PTo),e(A,P2),e(P2,qoe),e(qoe,$To),e(P2,ITo),e(P2,v$),e(v$,jTo),e(P2,NTo),e(A,DTo),e(A,$2),e($2,Goe),e(Goe,qTo),e($2,GTo),e($2,T$),e(T$,OTo),e($2,XTo),e(A,zTo),e(A,I2),e(I2,Ooe),e(Ooe,VTo),e(I2,WTo),e(I2,F$),e(F$,QTo),e(I2,HTo),e(A,UTo),e(A,j2),e(j2,Xoe),e(Xoe,JTo),e(j2,YTo),e(j2,C$),e(C$,KTo),e(j2,ZTo),e(A,e7o),e(A,N2),e(N2,zoe),e(zoe,o7o),e(N2,r7o),e(N2,M$),e(M$,t7o),e(N2,a7o),e(A,n7o),e(A,D2),e(D2,Voe),e(Voe,s7o),e(D2,l7o),e(D2,E$),e(E$,i7o),e(D2,d7o),e(A,c7o),e(A,q2),e(q2,Woe),e(Woe,f7o),e(q2,m7o),e(q2,y$),e(y$,g7o),e(q2,h7o),e(A,p7o),e(A,G2),e(G2,Qoe),e(Qoe,_7o),e(G2,u7o),e(G2,w$),e(w$,b7o),e(G2,v7o),e(A,T7o),e(A,O2),e(O2,Hoe),e(Hoe,F7o),e(O2,C7o),e(O2,A$),e(A$,M7o),e(O2,E7o),e(A,y7o),e(A,X2),e(X2,Uoe),e(Uoe,w7o),e(X2,A7o),e(X2,L$),e(L$,L7o),e(X2,B7o),e(A,k7o),e(A,z2),e(z2,Joe),e(Joe,x7o),e(z2,R7o),e(z2,B$),e(B$,S7o),e(z2,P7o),e(A,$7o),e(A,V2),e(V2,Yoe),e(Yoe,I7o),e(V2,j7o),e(V2,k$),e(k$,N7o),e(V2,D7o),e(A,q7o),e(A,W2),e(W2,Koe),e(Koe,G7o),e(W2,O7o),e(W2,x$),e(x$,X7o),e(W2,z7o),e(A,V7o),e(A,Q2),e(Q2,Zoe),e(Zoe,W7o),e(Q2,Q7o),e(Q2,R$),e(R$,H7o),e(Q2,U7o),e(A,J7o),e(A,H2),e(H2,ere),e(ere,Y7o),e(H2,K7o),e(H2,S$),e(S$,Z7o),e(H2,eFo),e(A,oFo),e(A,U2),e(U2,ore),e(ore,rFo),e(U2,tFo),e(U2,P$),e(P$,aFo),e(U2,nFo),e(A,sFo),e(A,J2),e(J2,rre),e(rre,lFo),e(J2,iFo),e(J2,$$),e($$,dFo),e(J2,cFo),e(A,fFo),e(A,Y2),e(Y2,tre),e(tre,mFo),e(Y2,gFo),e(Y2,I$),e(I$,hFo),e(Y2,pFo),e(A,_Fo),e(A,K2),e(K2,are),e(are,uFo),e(K2,bFo),e(K2,j$),e(j$,vFo),e(K2,TFo),e(A,FFo),e(A,Z2),e(Z2,nre),e(nre,CFo),e(Z2,MFo),e(Z2,N$),e(N$,EFo),e(Z2,yFo),e(A,wFo),e(A,e1),e(e1,sre),e(sre,AFo),e(e1,LFo),e(e1,D$),e(D$,BFo),e(e1,kFo),e(A,xFo),e(A,o1),e(o1,lre),e(lre,RFo),e(o1,SFo),e(o1,q$),e(q$,PFo),e(o1,$Fo),e(A,IFo),e(A,r1),e(r1,ire),e(ire,jFo),e(r1,NFo),e(r1,G$),e(G$,DFo),e(r1,qFo),e(A,GFo),e(A,t1),e(t1,dre),e(dre,OFo),e(t1,XFo),e(t1,O$),e(O$,zFo),e(t1,VFo),e(A,WFo),e(A,a1),e(a1,cre),e(cre,QFo),e(a1,HFo),e(a1,X$),e(X$,UFo),e(a1,JFo),e(A,YFo),e(A,n1),e(n1,fre),e(fre,KFo),e(n1,ZFo),e(n1,z$),e(z$,e9o),e(n1,o9o),e(A,r9o),e(A,s1),e(s1,mre),e(mre,t9o),e(s1,a9o),e(s1,V$),e(V$,n9o),e(s1,s9o),e(A,l9o),e(A,l1),e(l1,gre),e(gre,i9o),e(l1,d9o),e(l1,W$),e(W$,c9o),e(l1,f9o),e(A,m9o),e(A,i1),e(i1,hre),e(hre,g9o),e(i1,h9o),e(i1,Q$),e(Q$,p9o),e(i1,_9o),e(A,u9o),e(A,d1),e(d1,pre),e(pre,b9o),e(d1,v9o),e(d1,H$),e(H$,T9o),e(d1,F9o),e(A,C9o),e(A,c1),e(c1,_re),e(_re,M9o),e(c1,E9o),e(c1,U$),e(U$,y9o),e(c1,w9o),e(A,A9o),e(A,f1),e(f1,ure),e(ure,L9o),e(f1,B9o),e(f1,J$),e(J$,k9o),e(f1,x9o),e(A,R9o),e(A,m1),e(m1,bre),e(bre,S9o),e(m1,P9o),e(m1,Y$),e(Y$,$9o),e(m1,I9o),e($e,j9o),e($e,g1),e(g1,N9o),e(g1,vre),e(vre,D9o),e(g1,q9o),e(g1,Tre),e(Tre,G9o),e($e,O9o),e($e,Fre),e(Fre,X9o),e($e,z9o),g(ME,$e,null),b(d,SLe,u),b(d,nd,u),e(nd,h1),e(h1,Cre),g(EE,Cre,null),e(nd,V9o),e(nd,Mre),e(Mre,W9o),b(d,PLe,u),b(d,Yo,u),g(yE,Yo,null),e(Yo,Q9o),e(Yo,sd),e(sd,H9o),e(sd,Ere),e(Ere,U9o),e(sd,J9o),e(sd,yre),e(yre,Y9o),e(sd,K9o),e(Yo,Z9o),e(Yo,wE),e(wE,eCo),e(wE,wre),e(wre,oCo),e(wE,rCo),e(Yo,tCo),e(Yo,zr),g(AE,zr,null),e(zr,aCo),e(zr,Are),e(Are,nCo),e(zr,sCo),e(zr,ld),e(ld,lCo),e(ld,Lre),e(Lre,iCo),e(ld,dCo),e(ld,Bre),e(Bre,cCo),e(ld,fCo),e(zr,mCo),e(zr,kre),e(kre,gCo),e(zr,hCo),g(LE,zr,null),e(Yo,pCo),e(Yo,Ie),g(BE,Ie,null),e(Ie,_Co),e(Ie,xre),e(xre,uCo),e(Ie,bCo),e(Ie,Va),e(Va,vCo),e(Va,Rre),e(Rre,TCo),e(Va,FCo),e(Va,Sre),e(Sre,CCo),e(Va,MCo),e(Va,Pre),e(Pre,ECo),e(Va,yCo),e(Ie,wCo),e(Ie,G),e(G,p1),e(p1,$re),e($re,ACo),e(p1,LCo),e(p1,K$),e(K$,BCo),e(p1,kCo),e(G,xCo),e(G,_1),e(_1,Ire),e(Ire,RCo),e(_1,SCo),e(_1,Z$),e(Z$,PCo),e(_1,$Co),e(G,ICo),e(G,u1),e(u1,jre),e(jre,jCo),e(u1,NCo),e(u1,eI),e(eI,DCo),e(u1,qCo),e(G,GCo),e(G,b1),e(b1,Nre),e(Nre,OCo),e(b1,XCo),e(b1,oI),e(oI,zCo),e(b1,VCo),e(G,WCo),e(G,v1),e(v1,Dre),e(Dre,QCo),e(v1,HCo),e(v1,rI),e(rI,UCo),e(v1,JCo),e(G,YCo),e(G,T1),e(T1,qre),e(qre,KCo),e(T1,ZCo),e(T1,tI),e(tI,e4o),e(T1,o4o),e(G,r4o),e(G,F1),e(F1,Gre),e(Gre,t4o),e(F1,a4o),e(F1,aI),e(aI,n4o),e(F1,s4o),e(G,l4o),e(G,C1),e(C1,Ore),e(Ore,i4o),e(C1,d4o),e(C1,nI),e(nI,c4o),e(C1,f4o),e(G,m4o),e(G,M1),e(M1,Xre),e(Xre,g4o),e(M1,h4o),e(M1,sI),e(sI,p4o),e(M1,_4o),e(G,u4o),e(G,E1),e(E1,zre),e(zre,b4o),e(E1,v4o),e(E1,lI),e(lI,T4o),e(E1,F4o),e(G,C4o),e(G,y1),e(y1,Vre),e(Vre,M4o),e(y1,E4o),e(y1,iI),e(iI,y4o),e(y1,w4o),e(G,A4o),e(G,w1),e(w1,Wre),e(Wre,L4o),e(w1,B4o),e(w1,dI),e(dI,k4o),e(w1,x4o),e(G,R4o),e(G,A1),e(A1,Qre),e(Qre,S4o),e(A1,P4o),e(A1,cI),e(cI,$4o),e(A1,I4o),e(G,j4o),e(G,L1),e(L1,Hre),e(Hre,N4o),e(L1,D4o),e(L1,fI),e(fI,q4o),e(L1,G4o),e(G,O4o),e(G,B1),e(B1,Ure),e(Ure,X4o),e(B1,z4o),e(B1,mI),e(mI,V4o),e(B1,W4o),e(G,Q4o),e(G,k1),e(k1,Jre),e(Jre,H4o),e(k1,U4o),e(k1,gI),e(gI,J4o),e(k1,Y4o),e(G,K4o),e(G,x1),e(x1,Yre),e(Yre,Z4o),e(x1,eMo),e(x1,hI),e(hI,oMo),e(x1,rMo),e(G,tMo),e(G,R1),e(R1,Kre),e(Kre,aMo),e(R1,nMo),e(R1,pI),e(pI,sMo),e(R1,lMo),e(G,iMo),e(G,S1),e(S1,Zre),e(Zre,dMo),e(S1,cMo),e(S1,_I),e(_I,fMo),e(S1,mMo),e(G,gMo),e(G,P1),e(P1,ete),e(ete,hMo),e(P1,pMo),e(P1,uI),e(uI,_Mo),e(P1,uMo),e(G,bMo),e(G,$1),e($1,ote),e(ote,vMo),e($1,TMo),e($1,bI),e(bI,FMo),e($1,CMo),e(G,MMo),e(G,I1),e(I1,rte),e(rte,EMo),e(I1,yMo),e(I1,vI),e(vI,wMo),e(I1,AMo),e(G,LMo),e(G,j1),e(j1,tte),e(tte,BMo),e(j1,kMo),e(j1,TI),e(TI,xMo),e(j1,RMo),e(G,SMo),e(G,N1),e(N1,ate),e(ate,PMo),e(N1,$Mo),e(N1,FI),e(FI,IMo),e(N1,jMo),e(G,NMo),e(G,D1),e(D1,nte),e(nte,DMo),e(D1,qMo),e(D1,CI),e(CI,GMo),e(D1,OMo),e(G,XMo),e(G,q1),e(q1,ste),e(ste,zMo),e(q1,VMo),e(q1,MI),e(MI,WMo),e(q1,QMo),e(G,HMo),e(G,G1),e(G1,lte),e(lte,UMo),e(G1,JMo),e(G1,EI),e(EI,YMo),e(G1,KMo),e(Ie,ZMo),e(Ie,O1),e(O1,eEo),e(O1,ite),e(ite,oEo),e(O1,rEo),e(O1,dte),e(dte,tEo),e(Ie,aEo),e(Ie,cte),e(cte,nEo),e(Ie,sEo),g(kE,Ie,null),b(d,$Le,u),b(d,id,u),e(id,X1),e(X1,fte),g(xE,fte,null),e(id,lEo),e(id,mte),e(mte,iEo),b(d,ILe,u),b(d,Ko,u),g(RE,Ko,null),e(Ko,dEo),e(Ko,dd),e(dd,cEo),e(dd,gte),e(gte,fEo),e(dd,mEo),e(dd,hte),e(hte,gEo),e(dd,hEo),e(Ko,pEo),e(Ko,SE),e(SE,_Eo),e(SE,pte),e(pte,uEo),e(SE,bEo),e(Ko,vEo),e(Ko,Vr),g(PE,Vr,null),e(Vr,TEo),e(Vr,_te),e(_te,FEo),e(Vr,CEo),e(Vr,cd),e(cd,MEo),e(cd,ute),e(ute,EEo),e(cd,yEo),e(cd,bte),e(bte,wEo),e(cd,AEo),e(Vr,LEo),e(Vr,vte),e(vte,BEo),e(Vr,kEo),g($E,Vr,null),e(Ko,xEo),e(Ko,je),g(IE,je,null),e(je,REo),e(je,Tte),e(Tte,SEo),e(je,PEo),e(je,Wa),e(Wa,$Eo),e(Wa,Fte),e(Fte,IEo),e(Wa,jEo),e(Wa,Cte),e(Cte,NEo),e(Wa,DEo),e(Wa,Mte),e(Mte,qEo),e(Wa,GEo),e(je,OEo),e(je,na),e(na,z1),e(z1,Ete),e(Ete,XEo),e(z1,zEo),e(z1,yI),e(yI,VEo),e(z1,WEo),e(na,QEo),e(na,V1),e(V1,yte),e(yte,HEo),e(V1,UEo),e(V1,wI),e(wI,JEo),e(V1,YEo),e(na,KEo),e(na,W1),e(W1,wte),e(wte,ZEo),e(W1,e3o),e(W1,AI),e(AI,o3o),e(W1,r3o),e(na,t3o),e(na,Q1),e(Q1,Ate),e(Ate,a3o),e(Q1,n3o),e(Q1,LI),e(LI,s3o),e(Q1,l3o),e(na,i3o),e(na,H1),e(H1,Lte),e(Lte,d3o),e(H1,c3o),e(H1,BI),e(BI,f3o),e(H1,m3o),e(je,g3o),e(je,U1),e(U1,h3o),e(U1,Bte),e(Bte,p3o),e(U1,_3o),e(U1,kte),e(kte,u3o),e(je,b3o),e(je,xte),e(xte,v3o),e(je,T3o),g(jE,je,null),b(d,jLe,u),b(d,fd,u),e(fd,J1),e(J1,Rte),g(NE,Rte,null),e(fd,F3o),e(fd,Ste),e(Ste,C3o),b(d,NLe,u),b(d,Zo,u),g(DE,Zo,null),e(Zo,M3o),e(Zo,md),e(md,E3o),e(md,Pte),e(Pte,y3o),e(md,w3o),e(md,$te),e($te,A3o),e(md,L3o),e(Zo,B3o),e(Zo,qE),e(qE,k3o),e(qE,Ite),e(Ite,x3o),e(qE,R3o),e(Zo,S3o),e(Zo,Wr),g(GE,Wr,null),e(Wr,P3o),e(Wr,jte),e(jte,$3o),e(Wr,I3o),e(Wr,gd),e(gd,j3o),e(gd,Nte),e(Nte,N3o),e(gd,D3o),e(gd,Dte),e(Dte,q3o),e(gd,G3o),e(Wr,O3o),e(Wr,qte),e(qte,X3o),e(Wr,z3o),g(OE,Wr,null),e(Zo,V3o),e(Zo,Ne),g(XE,Ne,null),e(Ne,W3o),e(Ne,Gte),e(Gte,Q3o),e(Ne,H3o),e(Ne,Qa),e(Qa,U3o),e(Qa,Ote),e(Ote,J3o),e(Qa,Y3o),e(Qa,Xte),e(Xte,K3o),e(Qa,Z3o),e(Qa,zte),e(zte,eyo),e(Qa,oyo),e(Ne,ryo),e(Ne,D),e(D,Y1),e(Y1,Vte),e(Vte,tyo),e(Y1,ayo),e(Y1,kI),e(kI,nyo),e(Y1,syo),e(D,lyo),e(D,K1),e(K1,Wte),e(Wte,iyo),e(K1,dyo),e(K1,xI),e(xI,cyo),e(K1,fyo),e(D,myo),e(D,Z1),e(Z1,Qte),e(Qte,gyo),e(Z1,hyo),e(Z1,RI),e(RI,pyo),e(Z1,_yo),e(D,uyo),e(D,eb),e(eb,Hte),e(Hte,byo),e(eb,vyo),e(eb,SI),e(SI,Tyo),e(eb,Fyo),e(D,Cyo),e(D,ob),e(ob,Ute),e(Ute,Myo),e(ob,Eyo),e(ob,PI),e(PI,yyo),e(ob,wyo),e(D,Ayo),e(D,rb),e(rb,Jte),e(Jte,Lyo),e(rb,Byo),e(rb,$I),e($I,kyo),e(rb,xyo),e(D,Ryo),e(D,tb),e(tb,Yte),e(Yte,Syo),e(tb,Pyo),e(tb,II),e(II,$yo),e(tb,Iyo),e(D,jyo),e(D,ab),e(ab,Kte),e(Kte,Nyo),e(ab,Dyo),e(ab,jI),e(jI,qyo),e(ab,Gyo),e(D,Oyo),e(D,nb),e(nb,Zte),e(Zte,Xyo),e(nb,zyo),e(nb,NI),e(NI,Vyo),e(nb,Wyo),e(D,Qyo),e(D,sb),e(sb,eae),e(eae,Hyo),e(sb,Uyo),e(sb,DI),e(DI,Jyo),e(sb,Yyo),e(D,Kyo),e(D,lb),e(lb,oae),e(oae,Zyo),e(lb,ewo),e(lb,qI),e(qI,owo),e(lb,rwo),e(D,two),e(D,ib),e(ib,rae),e(rae,awo),e(ib,nwo),e(ib,GI),e(GI,swo),e(ib,lwo),e(D,iwo),e(D,db),e(db,tae),e(tae,dwo),e(db,cwo),e(db,OI),e(OI,fwo),e(db,mwo),e(D,gwo),e(D,cb),e(cb,aae),e(aae,hwo),e(cb,pwo),e(cb,XI),e(XI,_wo),e(cb,uwo),e(D,bwo),e(D,fb),e(fb,nae),e(nae,vwo),e(fb,Two),e(fb,zI),e(zI,Fwo),e(fb,Cwo),e(D,Mwo),e(D,mb),e(mb,sae),e(sae,Ewo),e(mb,ywo),e(mb,VI),e(VI,wwo),e(mb,Awo),e(D,Lwo),e(D,gb),e(gb,lae),e(lae,Bwo),e(gb,kwo),e(gb,WI),e(WI,xwo),e(gb,Rwo),e(D,Swo),e(D,hb),e(hb,iae),e(iae,Pwo),e(hb,$wo),e(hb,QI),e(QI,Iwo),e(hb,jwo),e(D,Nwo),e(D,pb),e(pb,dae),e(dae,Dwo),e(pb,qwo),e(pb,HI),e(HI,Gwo),e(pb,Owo),e(D,Xwo),e(D,_b),e(_b,cae),e(cae,zwo),e(_b,Vwo),e(_b,UI),e(UI,Wwo),e(_b,Qwo),e(D,Hwo),e(D,ub),e(ub,fae),e(fae,Uwo),e(ub,Jwo),e(ub,JI),e(JI,Ywo),e(ub,Kwo),e(D,Zwo),e(D,bb),e(bb,mae),e(mae,eAo),e(bb,oAo),e(bb,YI),e(YI,rAo),e(bb,tAo),e(D,aAo),e(D,vb),e(vb,gae),e(gae,nAo),e(vb,sAo),e(vb,KI),e(KI,lAo),e(vb,iAo),e(D,dAo),e(D,Tb),e(Tb,hae),e(hae,cAo),e(Tb,fAo),e(Tb,ZI),e(ZI,mAo),e(Tb,gAo),e(D,hAo),e(D,Fb),e(Fb,pae),e(pae,pAo),e(Fb,_Ao),e(Fb,ej),e(ej,uAo),e(Fb,bAo),e(D,vAo),e(D,Cb),e(Cb,_ae),e(_ae,TAo),e(Cb,FAo),e(Cb,oj),e(oj,CAo),e(Cb,MAo),e(D,EAo),e(D,Mb),e(Mb,uae),e(uae,yAo),e(Mb,wAo),e(Mb,rj),e(rj,AAo),e(Mb,LAo),e(D,BAo),e(D,Eb),e(Eb,bae),e(bae,kAo),e(Eb,xAo),e(Eb,tj),e(tj,RAo),e(Eb,SAo),e(D,PAo),e(D,yb),e(yb,vae),e(vae,$Ao),e(yb,IAo),e(yb,aj),e(aj,jAo),e(yb,NAo),e(D,DAo),e(D,wb),e(wb,Tae),e(Tae,qAo),e(wb,GAo),e(wb,nj),e(nj,OAo),e(wb,XAo),e(D,zAo),e(D,Ab),e(Ab,Fae),e(Fae,VAo),e(Ab,WAo),e(Ab,sj),e(sj,QAo),e(Ab,HAo),e(D,UAo),e(D,Lb),e(Lb,Cae),e(Cae,JAo),e(Lb,YAo),e(Lb,lj),e(lj,KAo),e(Lb,ZAo),e(Ne,e6o),e(Ne,Bb),e(Bb,o6o),e(Bb,Mae),e(Mae,r6o),e(Bb,t6o),e(Bb,Eae),e(Eae,a6o),e(Ne,n6o),e(Ne,yae),e(yae,s6o),e(Ne,l6o),g(zE,Ne,null),b(d,DLe,u),b(d,hd,u),e(hd,kb),e(kb,wae),g(VE,wae,null),e(hd,i6o),e(hd,Aae),e(Aae,d6o),b(d,qLe,u),b(d,er,u),g(WE,er,null),e(er,c6o),e(er,pd),e(pd,f6o),e(pd,Lae),e(Lae,m6o),e(pd,g6o),e(pd,Bae),e(Bae,h6o),e(pd,p6o),e(er,_6o),e(er,QE),e(QE,u6o),e(QE,kae),e(kae,b6o),e(QE,v6o),e(er,T6o),e(er,Qr),g(HE,Qr,null),e(Qr,F6o),e(Qr,xae),e(xae,C6o),e(Qr,M6o),e(Qr,_d),e(_d,E6o),e(_d,Rae),e(Rae,y6o),e(_d,w6o),e(_d,Sae),e(Sae,A6o),e(_d,L6o),e(Qr,B6o),e(Qr,Pae),e(Pae,k6o),e(Qr,x6o),g(UE,Qr,null),e(er,R6o),e(er,De),g(JE,De,null),e(De,S6o),e(De,$ae),e($ae,P6o),e(De,$6o),e(De,Ha),e(Ha,I6o),e(Ha,Iae),e(Iae,j6o),e(Ha,N6o),e(Ha,jae),e(jae,D6o),e(Ha,q6o),e(Ha,Nae),e(Nae,G6o),e(Ha,O6o),e(De,X6o),e(De,R),e(R,xb),e(xb,Dae),e(Dae,z6o),e(xb,V6o),e(xb,ij),e(ij,W6o),e(xb,Q6o),e(R,H6o),e(R,Rb),e(Rb,qae),e(qae,U6o),e(Rb,J6o),e(Rb,dj),e(dj,Y6o),e(Rb,K6o),e(R,Z6o),e(R,Sb),e(Sb,Gae),e(Gae,e0o),e(Sb,o0o),e(Sb,cj),e(cj,r0o),e(Sb,t0o),e(R,a0o),e(R,Pb),e(Pb,Oae),e(Oae,n0o),e(Pb,s0o),e(Pb,fj),e(fj,l0o),e(Pb,i0o),e(R,d0o),e(R,$b),e($b,Xae),e(Xae,c0o),e($b,f0o),e($b,mj),e(mj,m0o),e($b,g0o),e(R,h0o),e(R,Ib),e(Ib,zae),e(zae,p0o),e(Ib,_0o),e(Ib,gj),e(gj,u0o),e(Ib,b0o),e(R,v0o),e(R,jb),e(jb,Vae),e(Vae,T0o),e(jb,F0o),e(jb,hj),e(hj,C0o),e(jb,M0o),e(R,E0o),e(R,Nb),e(Nb,Wae),e(Wae,y0o),e(Nb,w0o),e(Nb,pj),e(pj,A0o),e(Nb,L0o),e(R,B0o),e(R,Db),e(Db,Qae),e(Qae,k0o),e(Db,x0o),e(Db,_j),e(_j,R0o),e(Db,S0o),e(R,P0o),e(R,qb),e(qb,Hae),e(Hae,$0o),e(qb,I0o),e(qb,uj),e(uj,j0o),e(qb,N0o),e(R,D0o),e(R,Gb),e(Gb,Uae),e(Uae,q0o),e(Gb,G0o),e(Gb,bj),e(bj,O0o),e(Gb,X0o),e(R,z0o),e(R,Ob),e(Ob,Jae),e(Jae,V0o),e(Ob,W0o),e(Ob,vj),e(vj,Q0o),e(Ob,H0o),e(R,U0o),e(R,Xb),e(Xb,Yae),e(Yae,J0o),e(Xb,Y0o),e(Xb,Tj),e(Tj,K0o),e(Xb,Z0o),e(R,eLo),e(R,zb),e(zb,Kae),e(Kae,oLo),e(zb,rLo),e(zb,Fj),e(Fj,tLo),e(zb,aLo),e(R,nLo),e(R,Vb),e(Vb,Zae),e(Zae,sLo),e(Vb,lLo),e(Vb,Cj),e(Cj,iLo),e(Vb,dLo),e(R,cLo),e(R,Wb),e(Wb,ene),e(ene,fLo),e(Wb,mLo),e(Wb,Mj),e(Mj,gLo),e(Wb,hLo),e(R,pLo),e(R,Qb),e(Qb,one),e(one,_Lo),e(Qb,uLo),e(Qb,Ej),e(Ej,bLo),e(Qb,vLo),e(R,TLo),e(R,Hb),e(Hb,rne),e(rne,FLo),e(Hb,CLo),e(Hb,yj),e(yj,MLo),e(Hb,ELo),e(R,yLo),e(R,Ub),e(Ub,tne),e(tne,wLo),e(Ub,ALo),e(Ub,wj),e(wj,LLo),e(Ub,BLo),e(R,kLo),e(R,Jb),e(Jb,ane),e(ane,xLo),e(Jb,RLo),e(Jb,Aj),e(Aj,SLo),e(Jb,PLo),e(R,$Lo),e(R,Yb),e(Yb,nne),e(nne,ILo),e(Yb,jLo),e(Yb,Lj),e(Lj,NLo),e(Yb,DLo),e(R,qLo),e(R,Kb),e(Kb,sne),e(sne,GLo),e(Kb,OLo),e(Kb,Bj),e(Bj,XLo),e(Kb,zLo),e(R,VLo),e(R,Zb),e(Zb,lne),e(lne,WLo),e(Zb,QLo),e(Zb,kj),e(kj,HLo),e(Zb,ULo),e(R,JLo),e(R,e5),e(e5,ine),e(ine,YLo),e(e5,KLo),e(e5,xj),e(xj,ZLo),e(e5,e8o),e(R,o8o),e(R,o5),e(o5,dne),e(dne,r8o),e(o5,t8o),e(o5,Rj),e(Rj,a8o),e(o5,n8o),e(R,s8o),e(R,r5),e(r5,cne),e(cne,l8o),e(r5,i8o),e(r5,Sj),e(Sj,d8o),e(r5,c8o),e(R,f8o),e(R,t5),e(t5,fne),e(fne,m8o),e(t5,g8o),e(t5,Pj),e(Pj,h8o),e(t5,p8o),e(R,_8o),e(R,a5),e(a5,mne),e(mne,u8o),e(a5,b8o),e(a5,$j),e($j,v8o),e(a5,T8o),e(R,F8o),e(R,n5),e(n5,gne),e(gne,C8o),e(n5,M8o),e(n5,Ij),e(Ij,E8o),e(n5,y8o),e(R,w8o),e(R,s5),e(s5,hne),e(hne,A8o),e(s5,L8o),e(s5,jj),e(jj,B8o),e(s5,k8o),e(R,x8o),e(R,l5),e(l5,pne),e(pne,R8o),e(l5,S8o),e(l5,Nj),e(Nj,P8o),e(l5,$8o),e(R,I8o),e(R,i5),e(i5,_ne),e(_ne,j8o),e(i5,N8o),e(i5,Dj),e(Dj,D8o),e(i5,q8o),e(R,G8o),e(R,d5),e(d5,une),e(une,O8o),e(d5,X8o),e(d5,qj),e(qj,z8o),e(d5,V8o),e(R,W8o),e(R,c5),e(c5,bne),e(bne,Q8o),e(c5,H8o),e(c5,Gj),e(Gj,U8o),e(c5,J8o),e(R,Y8o),e(R,f5),e(f5,vne),e(vne,K8o),e(f5,Z8o),e(f5,Oj),e(Oj,eBo),e(f5,oBo),e(R,rBo),e(R,m5),e(m5,Tne),e(Tne,tBo),e(m5,aBo),e(m5,Xj),e(Xj,nBo),e(m5,sBo),e(R,lBo),e(R,g5),e(g5,Fne),e(Fne,iBo),e(g5,dBo),e(g5,zj),e(zj,cBo),e(g5,fBo),e(R,mBo),e(R,h5),e(h5,Cne),e(Cne,gBo),e(h5,hBo),e(h5,Vj),e(Vj,pBo),e(h5,_Bo),e(De,uBo),e(De,p5),e(p5,bBo),e(p5,Mne),e(Mne,vBo),e(p5,TBo),e(p5,Ene),e(Ene,FBo),e(De,CBo),e(De,yne),e(yne,MBo),e(De,EBo),g(YE,De,null),b(d,GLe,u),b(d,ud,u),e(ud,_5),e(_5,wne),g(KE,wne,null),e(ud,yBo),e(ud,Ane),e(Ane,wBo),b(d,OLe,u),b(d,or,u),g(ZE,or,null),e(or,ABo),e(or,bd),e(bd,LBo),e(bd,Lne),e(Lne,BBo),e(bd,kBo),e(bd,Bne),e(Bne,xBo),e(bd,RBo),e(or,SBo),e(or,e3),e(e3,PBo),e(e3,kne),e(kne,$Bo),e(e3,IBo),e(or,jBo),e(or,Hr),g(o3,Hr,null),e(Hr,NBo),e(Hr,xne),e(xne,DBo),e(Hr,qBo),e(Hr,vd),e(vd,GBo),e(vd,Rne),e(Rne,OBo),e(vd,XBo),e(vd,Sne),e(Sne,zBo),e(vd,VBo),e(Hr,WBo),e(Hr,Pne),e(Pne,QBo),e(Hr,HBo),g(r3,Hr,null),e(or,UBo),e(or,qe),g(t3,qe,null),e(qe,JBo),e(qe,$ne),e($ne,YBo),e(qe,KBo),e(qe,Ua),e(Ua,ZBo),e(Ua,Ine),e(Ine,eko),e(Ua,oko),e(Ua,jne),e(jne,rko),e(Ua,tko),e(Ua,Nne),e(Nne,ako),e(Ua,nko),e(qe,sko),e(qe,Dne),e(Dne,u5),e(u5,qne),e(qne,lko),e(u5,iko),e(u5,Wj),e(Wj,dko),e(u5,cko),e(qe,fko),e(qe,b5),e(b5,mko),e(b5,Gne),e(Gne,gko),e(b5,hko),e(b5,One),e(One,pko),e(qe,_ko),e(qe,Xne),e(Xne,uko),e(qe,bko),g(a3,qe,null),b(d,XLe,u),b(d,Td,u),e(Td,v5),e(v5,zne),g(n3,zne,null),e(Td,vko),e(Td,Vne),e(Vne,Tko),b(d,zLe,u),b(d,rr,u),g(s3,rr,null),e(rr,Fko),e(rr,Fd),e(Fd,Cko),e(Fd,Wne),e(Wne,Mko),e(Fd,Eko),e(Fd,Qne),e(Qne,yko),e(Fd,wko),e(rr,Ako),e(rr,l3),e(l3,Lko),e(l3,Hne),e(Hne,Bko),e(l3,kko),e(rr,xko),e(rr,Ur),g(i3,Ur,null),e(Ur,Rko),e(Ur,Une),e(Une,Sko),e(Ur,Pko),e(Ur,Cd),e(Cd,$ko),e(Cd,Jne),e(Jne,Iko),e(Cd,jko),e(Cd,Yne),e(Yne,Nko),e(Cd,Dko),e(Ur,qko),e(Ur,Kne),e(Kne,Gko),e(Ur,Oko),g(d3,Ur,null),e(rr,Xko),e(rr,Ge),g(c3,Ge,null),e(Ge,zko),e(Ge,Zne),e(Zne,Vko),e(Ge,Wko),e(Ge,Ja),e(Ja,Qko),e(Ja,ese),e(ese,Hko),e(Ja,Uko),e(Ja,ose),e(ose,Jko),e(Ja,Yko),e(Ja,rse),e(rse,Kko),e(Ja,Zko),e(Ge,exo),e(Ge,be),e(be,T5),e(T5,tse),e(tse,oxo),e(T5,rxo),e(T5,Qj),e(Qj,txo),e(T5,axo),e(be,nxo),e(be,F5),e(F5,ase),e(ase,sxo),e(F5,lxo),e(F5,Hj),e(Hj,ixo),e(F5,dxo),e(be,cxo),e(be,Ss),e(Ss,nse),e(nse,fxo),e(Ss,mxo),e(Ss,Uj),e(Uj,gxo),e(Ss,hxo),e(Ss,Jj),e(Jj,pxo),e(Ss,_xo),e(be,uxo),e(be,C5),e(C5,sse),e(sse,bxo),e(C5,vxo),e(C5,Yj),e(Yj,Txo),e(C5,Fxo),e(be,Cxo),e(be,la),e(la,lse),e(lse,Mxo),e(la,Exo),e(la,Kj),e(Kj,yxo),e(la,wxo),e(la,Zj),e(Zj,Axo),e(la,Lxo),e(la,eN),e(eN,Bxo),e(la,kxo),e(be,xxo),e(be,M5),e(M5,ise),e(ise,Rxo),e(M5,Sxo),e(M5,oN),e(oN,Pxo),e(M5,$xo),e(be,Ixo),e(be,E5),e(E5,dse),e(dse,jxo),e(E5,Nxo),e(E5,rN),e(rN,Dxo),e(E5,qxo),e(be,Gxo),e(be,y5),e(y5,cse),e(cse,Oxo),e(y5,Xxo),e(y5,tN),e(tN,zxo),e(y5,Vxo),e(be,Wxo),e(be,w5),e(w5,fse),e(fse,Qxo),e(w5,Hxo),e(w5,aN),e(aN,Uxo),e(w5,Jxo),e(Ge,Yxo),e(Ge,A5),e(A5,Kxo),e(A5,mse),e(mse,Zxo),e(A5,eRo),e(A5,gse),e(gse,oRo),e(Ge,rRo),e(Ge,hse),e(hse,tRo),e(Ge,aRo),g(f3,Ge,null),b(d,VLe,u),b(d,Md,u),e(Md,L5),e(L5,pse),g(m3,pse,null),e(Md,nRo),e(Md,_se),e(_se,sRo),b(d,WLe,u),b(d,tr,u),g(g3,tr,null),e(tr,lRo),e(tr,Ed),e(Ed,iRo),e(Ed,use),e(use,dRo),e(Ed,cRo),e(Ed,bse),e(bse,fRo),e(Ed,mRo),e(tr,gRo),e(tr,h3),e(h3,hRo),e(h3,vse),e(vse,pRo),e(h3,_Ro),e(tr,uRo),e(tr,Jr),g(p3,Jr,null),e(Jr,bRo),e(Jr,Tse),e(Tse,vRo),e(Jr,TRo),e(Jr,yd),e(yd,FRo),e(yd,Fse),e(Fse,CRo),e(yd,MRo),e(yd,Cse),e(Cse,ERo),e(yd,yRo),e(Jr,wRo),e(Jr,Mse),e(Mse,ARo),e(Jr,LRo),g(_3,Jr,null),e(tr,BRo),e(tr,Oe),g(u3,Oe,null),e(Oe,kRo),e(Oe,Ese),e(Ese,xRo),e(Oe,RRo),e(Oe,Ya),e(Ya,SRo),e(Ya,yse),e(yse,PRo),e(Ya,$Ro),e(Ya,wse),e(wse,IRo),e(Ya,jRo),e(Ya,Ase),e(Ase,NRo),e(Ya,DRo),e(Oe,qRo),e(Oe,Lse),e(Lse,B5),e(B5,Bse),e(Bse,GRo),e(B5,ORo),e(B5,nN),e(nN,XRo),e(B5,zRo),e(Oe,VRo),e(Oe,k5),e(k5,WRo),e(k5,kse),e(kse,QRo),e(k5,HRo),e(k5,xse),e(xse,URo),e(Oe,JRo),e(Oe,Rse),e(Rse,YRo),e(Oe,KRo),g(b3,Oe,null),b(d,QLe,u),b(d,wd,u),e(wd,x5),e(x5,Sse),g(v3,Sse,null),e(wd,ZRo),e(wd,Pse),e(Pse,eSo),b(d,HLe,u),b(d,ar,u),g(T3,ar,null),e(ar,oSo),e(ar,Ad),e(Ad,rSo),e(Ad,$se),e($se,tSo),e(Ad,aSo),e(Ad,Ise),e(Ise,nSo),e(Ad,sSo),e(ar,lSo),e(ar,F3),e(F3,iSo),e(F3,jse),e(jse,dSo),e(F3,cSo),e(ar,fSo),e(ar,Yr),g(C3,Yr,null),e(Yr,mSo),e(Yr,Nse),e(Nse,gSo),e(Yr,hSo),e(Yr,Ld),e(Ld,pSo),e(Ld,Dse),e(Dse,_So),e(Ld,uSo),e(Ld,qse),e(qse,bSo),e(Ld,vSo),e(Yr,TSo),e(Yr,Gse),e(Gse,FSo),e(Yr,CSo),g(M3,Yr,null),e(ar,MSo),e(ar,Xe),g(E3,Xe,null),e(Xe,ESo),e(Xe,Ose),e(Ose,ySo),e(Xe,wSo),e(Xe,Ka),e(Ka,ASo),e(Ka,Xse),e(Xse,LSo),e(Ka,BSo),e(Ka,zse),e(zse,kSo),e(Ka,xSo),e(Ka,Vse),e(Vse,RSo),e(Ka,SSo),e(Xe,PSo),e(Xe,ao),e(ao,R5),e(R5,Wse),e(Wse,$So),e(R5,ISo),e(R5,sN),e(sN,jSo),e(R5,NSo),e(ao,DSo),e(ao,S5),e(S5,Qse),e(Qse,qSo),e(S5,GSo),e(S5,lN),e(lN,OSo),e(S5,XSo),e(ao,zSo),e(ao,P5),e(P5,Hse),e(Hse,VSo),e(P5,WSo),e(P5,iN),e(iN,QSo),e(P5,HSo),e(ao,USo),e(ao,$5),e($5,Use),e(Use,JSo),e($5,YSo),e($5,dN),e(dN,KSo),e($5,ZSo),e(ao,ePo),e(ao,I5),e(I5,Jse),e(Jse,oPo),e(I5,rPo),e(I5,cN),e(cN,tPo),e(I5,aPo),e(ao,nPo),e(ao,j5),e(j5,Yse),e(Yse,sPo),e(j5,lPo),e(j5,fN),e(fN,iPo),e(j5,dPo),e(ao,cPo),e(ao,N5),e(N5,Kse),e(Kse,fPo),e(N5,mPo),e(N5,mN),e(mN,gPo),e(N5,hPo),e(Xe,pPo),e(Xe,D5),e(D5,_Po),e(D5,Zse),e(Zse,uPo),e(D5,bPo),e(D5,ele),e(ele,vPo),e(Xe,TPo),e(Xe,ole),e(ole,FPo),e(Xe,CPo),g(y3,Xe,null),b(d,ULe,u),b(d,Bd,u),e(Bd,q5),e(q5,rle),g(w3,rle,null),e(Bd,MPo),e(Bd,tle),e(tle,EPo),b(d,JLe,u),b(d,nr,u),g(A3,nr,null),e(nr,yPo),e(nr,kd),e(kd,wPo),e(kd,ale),e(ale,APo),e(kd,LPo),e(kd,nle),e(nle,BPo),e(kd,kPo),e(nr,xPo),e(nr,L3),e(L3,RPo),e(L3,sle),e(sle,SPo),e(L3,PPo),e(nr,$Po),e(nr,Kr),g(B3,Kr,null),e(Kr,IPo),e(Kr,lle),e(lle,jPo),e(Kr,NPo),e(Kr,xd),e(xd,DPo),e(xd,ile),e(ile,qPo),e(xd,GPo),e(xd,dle),e(dle,OPo),e(xd,XPo),e(Kr,zPo),e(Kr,cle),e(cle,VPo),e(Kr,WPo),g(k3,Kr,null),e(nr,QPo),e(nr,ze),g(x3,ze,null),e(ze,HPo),e(ze,fle),e(fle,UPo),e(ze,JPo),e(ze,Za),e(Za,YPo),e(Za,mle),e(mle,KPo),e(Za,ZPo),e(Za,gle),e(gle,e$o),e(Za,o$o),e(Za,hle),e(hle,r$o),e(Za,t$o),e(ze,a$o),e(ze,Rd),e(Rd,G5),e(G5,ple),e(ple,n$o),e(G5,s$o),e(G5,gN),e(gN,l$o),e(G5,i$o),e(Rd,d$o),e(Rd,O5),e(O5,_le),e(_le,c$o),e(O5,f$o),e(O5,hN),e(hN,m$o),e(O5,g$o),e(Rd,h$o),e(Rd,X5),e(X5,ule),e(ule,p$o),e(X5,_$o),e(X5,pN),e(pN,u$o),e(X5,b$o),e(ze,v$o),e(ze,z5),e(z5,T$o),e(z5,ble),e(ble,F$o),e(z5,C$o),e(z5,vle),e(vle,M$o),e(ze,E$o),e(ze,Tle),e(Tle,y$o),e(ze,w$o),g(R3,ze,null),b(d,YLe,u),b(d,Sd,u),e(Sd,V5),e(V5,Fle),g(S3,Fle,null),e(Sd,A$o),e(Sd,Cle),e(Cle,L$o),b(d,KLe,u),b(d,sr,u),g(P3,sr,null),e(sr,B$o),e(sr,Pd),e(Pd,k$o),e(Pd,Mle),e(Mle,x$o),e(Pd,R$o),e(Pd,Ele),e(Ele,S$o),e(Pd,P$o),e(sr,$$o),e(sr,$3),e($3,I$o),e($3,yle),e(yle,j$o),e($3,N$o),e(sr,D$o),e(sr,Zr),g(I3,Zr,null),e(Zr,q$o),e(Zr,wle),e(wle,G$o),e(Zr,O$o),e(Zr,$d),e($d,X$o),e($d,Ale),e(Ale,z$o),e($d,V$o),e($d,Lle),e(Lle,W$o),e($d,Q$o),e(Zr,H$o),e(Zr,Ble),e(Ble,U$o),e(Zr,J$o),g(j3,Zr,null),e(sr,Y$o),e(sr,Ve),g(N3,Ve,null),e(Ve,K$o),e(Ve,kle),e(kle,Z$o),e(Ve,eIo),e(Ve,en),e(en,oIo),e(en,xle),e(xle,rIo),e(en,tIo),e(en,Rle),e(Rle,aIo),e(en,nIo),e(en,Sle),e(Sle,sIo),e(en,lIo),e(Ve,iIo),e(Ve,no),e(no,W5),e(W5,Ple),e(Ple,dIo),e(W5,cIo),e(W5,_N),e(_N,fIo),e(W5,mIo),e(no,gIo),e(no,Q5),e(Q5,$le),e($le,hIo),e(Q5,pIo),e(Q5,uN),e(uN,_Io),e(Q5,uIo),e(no,bIo),e(no,H5),e(H5,Ile),e(Ile,vIo),e(H5,TIo),e(H5,bN),e(bN,FIo),e(H5,CIo),e(no,MIo),e(no,U5),e(U5,jle),e(jle,EIo),e(U5,yIo),e(U5,vN),e(vN,wIo),e(U5,AIo),e(no,LIo),e(no,J5),e(J5,Nle),e(Nle,BIo),e(J5,kIo),e(J5,TN),e(TN,xIo),e(J5,RIo),e(no,SIo),e(no,Y5),e(Y5,Dle),e(Dle,PIo),e(Y5,$Io),e(Y5,FN),e(FN,IIo),e(Y5,jIo),e(no,NIo),e(no,K5),e(K5,qle),e(qle,DIo),e(K5,qIo),e(K5,CN),e(CN,GIo),e(K5,OIo),e(Ve,XIo),e(Ve,Z5),e(Z5,zIo),e(Z5,Gle),e(Gle,VIo),e(Z5,WIo),e(Z5,Ole),e(Ole,QIo),e(Ve,HIo),e(Ve,Xle),e(Xle,UIo),e(Ve,JIo),g(D3,Ve,null),b(d,ZLe,u),b(d,Id,u),e(Id,ev),e(ev,zle),g(q3,zle,null),e(Id,YIo),e(Id,Vle),e(Vle,KIo),b(d,e8e,u),b(d,lr,u),g(G3,lr,null),e(lr,ZIo),e(lr,jd),e(jd,ejo),e(jd,Wle),e(Wle,ojo),e(jd,rjo),e(jd,Qle),e(Qle,tjo),e(jd,ajo),e(lr,njo),e(lr,O3),e(O3,sjo),e(O3,Hle),e(Hle,ljo),e(O3,ijo),e(lr,djo),e(lr,et),g(X3,et,null),e(et,cjo),e(et,Ule),e(Ule,fjo),e(et,mjo),e(et,Nd),e(Nd,gjo),e(Nd,Jle),e(Jle,hjo),e(Nd,pjo),e(Nd,Yle),e(Yle,_jo),e(Nd,ujo),e(et,bjo),e(et,Kle),e(Kle,vjo),e(et,Tjo),g(z3,et,null),e(lr,Fjo),e(lr,We),g(V3,We,null),e(We,Cjo),e(We,Zle),e(Zle,Mjo),e(We,Ejo),e(We,on),e(on,yjo),e(on,eie),e(eie,wjo),e(on,Ajo),e(on,oie),e(oie,Ljo),e(on,Bjo),e(on,rie),e(rie,kjo),e(on,xjo),e(We,Rjo),e(We,W3),e(W3,ov),e(ov,tie),e(tie,Sjo),e(ov,Pjo),e(ov,MN),e(MN,$jo),e(ov,Ijo),e(W3,jjo),e(W3,rv),e(rv,aie),e(aie,Njo),e(rv,Djo),e(rv,EN),e(EN,qjo),e(rv,Gjo),e(We,Ojo),e(We,tv),e(tv,Xjo),e(tv,nie),e(nie,zjo),e(tv,Vjo),e(tv,sie),e(sie,Wjo),e(We,Qjo),e(We,lie),e(lie,Hjo),e(We,Ujo),g(Q3,We,null),b(d,o8e,u),b(d,Dd,u),e(Dd,av),e(av,iie),g(H3,iie,null),e(Dd,Jjo),e(Dd,die),e(die,Yjo),b(d,r8e,u),b(d,ir,u),g(U3,ir,null),e(ir,Kjo),e(ir,qd),e(qd,Zjo),e(qd,cie),e(cie,eNo),e(qd,oNo),e(qd,fie),e(fie,rNo),e(qd,tNo),e(ir,aNo),e(ir,J3),e(J3,nNo),e(J3,mie),e(mie,sNo),e(J3,lNo),e(ir,iNo),e(ir,ot),g(Y3,ot,null),e(ot,dNo),e(ot,gie),e(gie,cNo),e(ot,fNo),e(ot,Gd),e(Gd,mNo),e(Gd,hie),e(hie,gNo),e(Gd,hNo),e(Gd,pie),e(pie,pNo),e(Gd,_No),e(ot,uNo),e(ot,_ie),e(_ie,bNo),e(ot,vNo),g(K3,ot,null),e(ir,TNo),e(ir,Qe),g(Z3,Qe,null),e(Qe,FNo),e(Qe,uie),e(uie,CNo),e(Qe,MNo),e(Qe,rn),e(rn,ENo),e(rn,bie),e(bie,yNo),e(rn,wNo),e(rn,vie),e(vie,ANo),e(rn,LNo),e(rn,Tie),e(Tie,BNo),e(rn,kNo),e(Qe,xNo),e(Qe,Od),e(Od,nv),e(nv,Fie),e(Fie,RNo),e(nv,SNo),e(nv,yN),e(yN,PNo),e(nv,$No),e(Od,INo),e(Od,sv),e(sv,Cie),e(Cie,jNo),e(sv,NNo),e(sv,wN),e(wN,DNo),e(sv,qNo),e(Od,GNo),e(Od,lv),e(lv,Mie),e(Mie,ONo),e(lv,XNo),e(lv,AN),e(AN,zNo),e(lv,VNo),e(Qe,WNo),e(Qe,iv),e(iv,QNo),e(iv,Eie),e(Eie,HNo),e(iv,UNo),e(iv,yie),e(yie,JNo),e(Qe,YNo),e(Qe,wie),e(wie,KNo),e(Qe,ZNo),g(ey,Qe,null),b(d,t8e,u),b(d,Xd,u),e(Xd,dv),e(dv,Aie),g(oy,Aie,null),e(Xd,eDo),e(Xd,Lie),e(Lie,oDo),b(d,a8e,u),b(d,dr,u),g(ry,dr,null),e(dr,rDo),e(dr,zd),e(zd,tDo),e(zd,Bie),e(Bie,aDo),e(zd,nDo),e(zd,kie),e(kie,sDo),e(zd,lDo),e(dr,iDo),e(dr,ty),e(ty,dDo),e(ty,xie),e(xie,cDo),e(ty,fDo),e(dr,mDo),e(dr,rt),g(ay,rt,null),e(rt,gDo),e(rt,Rie),e(Rie,hDo),e(rt,pDo),e(rt,Vd),e(Vd,_Do),e(Vd,Sie),e(Sie,uDo),e(Vd,bDo),e(Vd,Pie),e(Pie,vDo),e(Vd,TDo),e(rt,FDo),e(rt,$ie),e($ie,CDo),e(rt,MDo),g(ny,rt,null),e(dr,EDo),e(dr,He),g(sy,He,null),e(He,yDo),e(He,Iie),e(Iie,wDo),e(He,ADo),e(He,tn),e(tn,LDo),e(tn,jie),e(jie,BDo),e(tn,kDo),e(tn,Nie),e(Nie,xDo),e(tn,RDo),e(tn,Die),e(Die,SDo),e(tn,PDo),e(He,$Do),e(He,Wd),e(Wd,cv),e(cv,qie),e(qie,IDo),e(cv,jDo),e(cv,LN),e(LN,NDo),e(cv,DDo),e(Wd,qDo),e(Wd,fv),e(fv,Gie),e(Gie,GDo),e(fv,ODo),e(fv,BN),e(BN,XDo),e(fv,zDo),e(Wd,VDo),e(Wd,mv),e(mv,Oie),e(Oie,WDo),e(mv,QDo),e(mv,kN),e(kN,HDo),e(mv,UDo),e(He,JDo),e(He,gv),e(gv,YDo),e(gv,Xie),e(Xie,KDo),e(gv,ZDo),e(gv,zie),e(zie,eqo),e(He,oqo),e(He,Vie),e(Vie,rqo),e(He,tqo),g(ly,He,null),b(d,n8e,u),b(d,Qd,u),e(Qd,hv),e(hv,Wie),g(iy,Wie,null),e(Qd,aqo),e(Qd,Qie),e(Qie,nqo),b(d,s8e,u),b(d,cr,u),g(dy,cr,null),e(cr,sqo),e(cr,Hd),e(Hd,lqo),e(Hd,Hie),e(Hie,iqo),e(Hd,dqo),e(Hd,Uie),e(Uie,cqo),e(Hd,fqo),e(cr,mqo),e(cr,cy),e(cy,gqo),e(cy,Jie),e(Jie,hqo),e(cy,pqo),e(cr,_qo),e(cr,tt),g(fy,tt,null),e(tt,uqo),e(tt,Yie),e(Yie,bqo),e(tt,vqo),e(tt,Ud),e(Ud,Tqo),e(Ud,Kie),e(Kie,Fqo),e(Ud,Cqo),e(Ud,Zie),e(Zie,Mqo),e(Ud,Eqo),e(tt,yqo),e(tt,ede),e(ede,wqo),e(tt,Aqo),g(my,tt,null),e(cr,Lqo),e(cr,Ue),g(gy,Ue,null),e(Ue,Bqo),e(Ue,ode),e(ode,kqo),e(Ue,xqo),e(Ue,an),e(an,Rqo),e(an,rde),e(rde,Sqo),e(an,Pqo),e(an,tde),e(tde,$qo),e(an,Iqo),e(an,ade),e(ade,jqo),e(an,Nqo),e(Ue,Dqo),e(Ue,nde),e(nde,pv),e(pv,sde),e(sde,qqo),e(pv,Gqo),e(pv,xN),e(xN,Oqo),e(pv,Xqo),e(Ue,zqo),e(Ue,_v),e(_v,Vqo),e(_v,lde),e(lde,Wqo),e(_v,Qqo),e(_v,ide),e(ide,Hqo),e(Ue,Uqo),e(Ue,dde),e(dde,Jqo),e(Ue,Yqo),g(hy,Ue,null),b(d,l8e,u),b(d,Jd,u),e(Jd,uv),e(uv,cde),g(py,cde,null),e(Jd,Kqo),e(Jd,fde),e(fde,Zqo),b(d,i8e,u),b(d,fr,u),g(_y,fr,null),e(fr,eGo),e(fr,Yd),e(Yd,oGo),e(Yd,mde),e(mde,rGo),e(Yd,tGo),e(Yd,gde),e(gde,aGo),e(Yd,nGo),e(fr,sGo),e(fr,uy),e(uy,lGo),e(uy,hde),e(hde,iGo),e(uy,dGo),e(fr,cGo),e(fr,at),g(by,at,null),e(at,fGo),e(at,pde),e(pde,mGo),e(at,gGo),e(at,Kd),e(Kd,hGo),e(Kd,_de),e(_de,pGo),e(Kd,_Go),e(Kd,ude),e(ude,uGo),e(Kd,bGo),e(at,vGo),e(at,bde),e(bde,TGo),e(at,FGo),g(vy,at,null),e(fr,CGo),e(fr,Je),g(Ty,Je,null),e(Je,MGo),e(Je,vde),e(vde,EGo),e(Je,yGo),e(Je,nn),e(nn,wGo),e(nn,Tde),e(Tde,AGo),e(nn,LGo),e(nn,Fde),e(Fde,BGo),e(nn,kGo),e(nn,Cde),e(Cde,xGo),e(nn,RGo),e(Je,SGo),e(Je,Mde),e(Mde,bv),e(bv,Ede),e(Ede,PGo),e(bv,$Go),e(bv,RN),e(RN,IGo),e(bv,jGo),e(Je,NGo),e(Je,vv),e(vv,DGo),e(vv,yde),e(yde,qGo),e(vv,GGo),e(vv,wde),e(wde,OGo),e(Je,XGo),e(Je,Ade),e(Ade,zGo),e(Je,VGo),g(Fy,Je,null),b(d,d8e,u),b(d,Zd,u),e(Zd,Tv),e(Tv,Lde),g(Cy,Lde,null),e(Zd,WGo),e(Zd,Bde),e(Bde,QGo),b(d,c8e,u),b(d,mr,u),g(My,mr,null),e(mr,HGo),e(mr,ec),e(ec,UGo),e(ec,kde),e(kde,JGo),e(ec,YGo),e(ec,xde),e(xde,KGo),e(ec,ZGo),e(mr,eOo),e(mr,Ey),e(Ey,oOo),e(Ey,Rde),e(Rde,rOo),e(Ey,tOo),e(mr,aOo),e(mr,nt),g(yy,nt,null),e(nt,nOo),e(nt,Sde),e(Sde,sOo),e(nt,lOo),e(nt,oc),e(oc,iOo),e(oc,Pde),e(Pde,dOo),e(oc,cOo),e(oc,$de),e($de,fOo),e(oc,mOo),e(nt,gOo),e(nt,Ide),e(Ide,hOo),e(nt,pOo),g(wy,nt,null),e(mr,_Oo),e(mr,Ye),g(Ay,Ye,null),e(Ye,uOo),e(Ye,jde),e(jde,bOo),e(Ye,vOo),e(Ye,sn),e(sn,TOo),e(sn,Nde),e(Nde,FOo),e(sn,COo),e(sn,Dde),e(Dde,MOo),e(sn,EOo),e(sn,qde),e(qde,yOo),e(sn,wOo),e(Ye,AOo),e(Ye,Ly),e(Ly,Fv),e(Fv,Gde),e(Gde,LOo),e(Fv,BOo),e(Fv,SN),e(SN,kOo),e(Fv,xOo),e(Ly,ROo),e(Ly,Cv),e(Cv,Ode),e(Ode,SOo),e(Cv,POo),e(Cv,PN),e(PN,$Oo),e(Cv,IOo),e(Ye,jOo),e(Ye,Mv),e(Mv,NOo),e(Mv,Xde),e(Xde,DOo),e(Mv,qOo),e(Mv,zde),e(zde,GOo),e(Ye,OOo),e(Ye,Vde),e(Vde,XOo),e(Ye,zOo),g(By,Ye,null),b(d,f8e,u),b(d,rc,u),e(rc,Ev),e(Ev,Wde),g(ky,Wde,null),e(rc,VOo),e(rc,Qde),e(Qde,WOo),b(d,m8e,u),b(d,gr,u),g(xy,gr,null),e(gr,QOo),e(gr,tc),e(tc,HOo),e(tc,Hde),e(Hde,UOo),e(tc,JOo),e(tc,Ude),e(Ude,YOo),e(tc,KOo),e(gr,ZOo),e(gr,Ry),e(Ry,eXo),e(Ry,Jde),e(Jde,oXo),e(Ry,rXo),e(gr,tXo),e(gr,st),g(Sy,st,null),e(st,aXo),e(st,Yde),e(Yde,nXo),e(st,sXo),e(st,ac),e(ac,lXo),e(ac,Kde),e(Kde,iXo),e(ac,dXo),e(ac,Zde),e(Zde,cXo),e(ac,fXo),e(st,mXo),e(st,ece),e(ece,gXo),e(st,hXo),g(Py,st,null),e(gr,pXo),e(gr,go),g($y,go,null),e(go,_Xo),e(go,oce),e(oce,uXo),e(go,bXo),e(go,ln),e(ln,vXo),e(ln,rce),e(rce,TXo),e(ln,FXo),e(ln,tce),e(tce,CXo),e(ln,MXo),e(ln,ace),e(ace,EXo),e(ln,yXo),e(go,wXo),e(go,B),e(B,yv),e(yv,nce),e(nce,AXo),e(yv,LXo),e(yv,$N),e($N,BXo),e(yv,kXo),e(B,xXo),e(B,wv),e(wv,sce),e(sce,RXo),e(wv,SXo),e(wv,IN),e(IN,PXo),e(wv,$Xo),e(B,IXo),e(B,Av),e(Av,lce),e(lce,jXo),e(Av,NXo),e(Av,jN),e(jN,DXo),e(Av,qXo),e(B,GXo),e(B,Lv),e(Lv,ice),e(ice,OXo),e(Lv,XXo),e(Lv,NN),e(NN,zXo),e(Lv,VXo),e(B,WXo),e(B,Bv),e(Bv,dce),e(dce,QXo),e(Bv,HXo),e(Bv,DN),e(DN,UXo),e(Bv,JXo),e(B,YXo),e(B,kv),e(kv,cce),e(cce,KXo),e(kv,ZXo),e(kv,qN),e(qN,ezo),e(kv,ozo),e(B,rzo),e(B,xv),e(xv,fce),e(fce,tzo),e(xv,azo),e(xv,GN),e(GN,nzo),e(xv,szo),e(B,lzo),e(B,Rv),e(Rv,mce),e(mce,izo),e(Rv,dzo),e(Rv,ON),e(ON,czo),e(Rv,fzo),e(B,mzo),e(B,Sv),e(Sv,gce),e(gce,gzo),e(Sv,hzo),e(Sv,XN),e(XN,pzo),e(Sv,_zo),e(B,uzo),e(B,Pv),e(Pv,hce),e(hce,bzo),e(Pv,vzo),e(Pv,zN),e(zN,Tzo),e(Pv,Fzo),e(B,Czo),e(B,$v),e($v,pce),e(pce,Mzo),e($v,Ezo),e($v,VN),e(VN,yzo),e($v,wzo),e(B,Azo),e(B,Iv),e(Iv,_ce),e(_ce,Lzo),e(Iv,Bzo),e(Iv,WN),e(WN,kzo),e(Iv,xzo),e(B,Rzo),e(B,jv),e(jv,uce),e(uce,Szo),e(jv,Pzo),e(jv,QN),e(QN,$zo),e(jv,Izo),e(B,jzo),e(B,Nv),e(Nv,bce),e(bce,Nzo),e(Nv,Dzo),e(Nv,HN),e(HN,qzo),e(Nv,Gzo),e(B,Ozo),e(B,Dv),e(Dv,vce),e(vce,Xzo),e(Dv,zzo),e(Dv,UN),e(UN,Vzo),e(Dv,Wzo),e(B,Qzo),e(B,Ps),e(Ps,Tce),e(Tce,Hzo),e(Ps,Uzo),e(Ps,JN),e(JN,Jzo),e(Ps,Yzo),e(Ps,YN),e(YN,Kzo),e(Ps,Zzo),e(B,eVo),e(B,qv),e(qv,Fce),e(Fce,oVo),e(qv,rVo),e(qv,KN),e(KN,tVo),e(qv,aVo),e(B,nVo),e(B,Gv),e(Gv,Cce),e(Cce,sVo),e(Gv,lVo),e(Gv,ZN),e(ZN,iVo),e(Gv,dVo),e(B,cVo),e(B,Ov),e(Ov,Mce),e(Mce,fVo),e(Ov,mVo),e(Ov,eD),e(eD,gVo),e(Ov,hVo),e(B,pVo),e(B,Xv),e(Xv,Ece),e(Ece,_Vo),e(Xv,uVo),e(Xv,oD),e(oD,bVo),e(Xv,vVo),e(B,TVo),e(B,zv),e(zv,yce),e(yce,FVo),e(zv,CVo),e(zv,rD),e(rD,MVo),e(zv,EVo),e(B,yVo),e(B,Vv),e(Vv,wce),e(wce,wVo),e(Vv,AVo),e(Vv,tD),e(tD,LVo),e(Vv,BVo),e(B,kVo),e(B,Wv),e(Wv,Ace),e(Ace,xVo),e(Wv,RVo),e(Wv,aD),e(aD,SVo),e(Wv,PVo),e(B,$Vo),e(B,Qv),e(Qv,Lce),e(Lce,IVo),e(Qv,jVo),e(Qv,nD),e(nD,NVo),e(Qv,DVo),e(B,qVo),e(B,Hv),e(Hv,Bce),e(Bce,GVo),e(Hv,OVo),e(Hv,sD),e(sD,XVo),e(Hv,zVo),e(B,VVo),e(B,Uv),e(Uv,kce),e(kce,WVo),e(Uv,QVo),e(Uv,lD),e(lD,HVo),e(Uv,UVo),e(B,JVo),e(B,Jv),e(Jv,xce),e(xce,YVo),e(Jv,KVo),e(Jv,iD),e(iD,ZVo),e(Jv,eWo),e(B,oWo),e(B,Yv),e(Yv,Rce),e(Rce,rWo),e(Yv,tWo),e(Yv,dD),e(dD,aWo),e(Yv,nWo),e(B,sWo),e(B,Kv),e(Kv,Sce),e(Sce,lWo),e(Kv,iWo),e(Kv,cD),e(cD,dWo),e(Kv,cWo),e(B,fWo),e(B,Zv),e(Zv,Pce),e(Pce,mWo),e(Zv,gWo),e(Zv,fD),e(fD,hWo),e(Zv,pWo),e(B,_Wo),e(B,eT),e(eT,$ce),e($ce,uWo),e(eT,bWo),e(eT,mD),e(mD,vWo),e(eT,TWo),e(B,FWo),e(B,oT),e(oT,Ice),e(Ice,CWo),e(oT,MWo),e(oT,gD),e(gD,EWo),e(oT,yWo),e(B,wWo),e(B,rT),e(rT,jce),e(jce,AWo),e(rT,LWo),e(rT,hD),e(hD,BWo),e(rT,kWo),e(B,xWo),e(B,tT),e(tT,Nce),e(Nce,RWo),e(tT,SWo),e(tT,pD),e(pD,PWo),e(tT,$Wo),e(B,IWo),e(B,aT),e(aT,Dce),e(Dce,jWo),e(aT,NWo),e(aT,_D),e(_D,DWo),e(aT,qWo),e(B,GWo),e(B,nT),e(nT,qce),e(qce,OWo),e(nT,XWo),e(nT,uD),e(uD,zWo),e(nT,VWo),e(B,WWo),e(B,sT),e(sT,Gce),e(Gce,QWo),e(sT,HWo),e(sT,bD),e(bD,UWo),e(sT,JWo),e(B,YWo),e(B,lT),e(lT,Oce),e(Oce,KWo),e(lT,ZWo),e(lT,vD),e(vD,eQo),e(lT,oQo),e(B,rQo),e(B,iT),e(iT,Xce),e(Xce,tQo),e(iT,aQo),e(iT,TD),e(TD,nQo),e(iT,sQo),e(B,lQo),e(B,dT),e(dT,zce),e(zce,iQo),e(dT,dQo),e(dT,FD),e(FD,cQo),e(dT,fQo),e(B,mQo),e(B,cT),e(cT,Vce),e(Vce,gQo),e(cT,hQo),e(cT,CD),e(CD,pQo),e(cT,_Qo),e(go,uQo),e(go,Wce),e(Wce,bQo),e(go,vQo),g(Iy,go,null),b(d,g8e,u),b(d,nc,u),e(nc,fT),e(fT,Qce),g(jy,Qce,null),e(nc,TQo),e(nc,Hce),e(Hce,FQo),b(d,h8e,u),b(d,hr,u),g(Ny,hr,null),e(hr,CQo),e(hr,sc),e(sc,MQo),e(sc,Uce),e(Uce,EQo),e(sc,yQo),e(sc,Jce),e(Jce,wQo),e(sc,AQo),e(hr,LQo),e(hr,Dy),e(Dy,BQo),e(Dy,Yce),e(Yce,kQo),e(Dy,xQo),e(hr,RQo),e(hr,lt),g(qy,lt,null),e(lt,SQo),e(lt,Kce),e(Kce,PQo),e(lt,$Qo),e(lt,lc),e(lc,IQo),e(lc,Zce),e(Zce,jQo),e(lc,NQo),e(lc,efe),e(efe,DQo),e(lc,qQo),e(lt,GQo),e(lt,ofe),e(ofe,OQo),e(lt,XQo),g(Gy,lt,null),e(hr,zQo),e(hr,ho),g(Oy,ho,null),e(ho,VQo),e(ho,rfe),e(rfe,WQo),e(ho,QQo),e(ho,dn),e(dn,HQo),e(dn,tfe),e(tfe,UQo),e(dn,JQo),e(dn,afe),e(afe,YQo),e(dn,KQo),e(dn,nfe),e(nfe,ZQo),e(dn,eHo),e(ho,oHo),e(ho,H),e(H,mT),e(mT,sfe),e(sfe,rHo),e(mT,tHo),e(mT,MD),e(MD,aHo),e(mT,nHo),e(H,sHo),e(H,gT),e(gT,lfe),e(lfe,lHo),e(gT,iHo),e(gT,ED),e(ED,dHo),e(gT,cHo),e(H,fHo),e(H,hT),e(hT,ife),e(ife,mHo),e(hT,gHo),e(hT,yD),e(yD,hHo),e(hT,pHo),e(H,_Ho),e(H,pT),e(pT,dfe),e(dfe,uHo),e(pT,bHo),e(pT,wD),e(wD,vHo),e(pT,THo),e(H,FHo),e(H,_T),e(_T,cfe),e(cfe,CHo),e(_T,MHo),e(_T,AD),e(AD,EHo),e(_T,yHo),e(H,wHo),e(H,uT),e(uT,ffe),e(ffe,AHo),e(uT,LHo),e(uT,LD),e(LD,BHo),e(uT,kHo),e(H,xHo),e(H,bT),e(bT,mfe),e(mfe,RHo),e(bT,SHo),e(bT,BD),e(BD,PHo),e(bT,$Ho),e(H,IHo),e(H,vT),e(vT,gfe),e(gfe,jHo),e(vT,NHo),e(vT,kD),e(kD,DHo),e(vT,qHo),e(H,GHo),e(H,TT),e(TT,hfe),e(hfe,OHo),e(TT,XHo),e(TT,xD),e(xD,zHo),e(TT,VHo),e(H,WHo),e(H,FT),e(FT,pfe),e(pfe,QHo),e(FT,HHo),e(FT,RD),e(RD,UHo),e(FT,JHo),e(H,YHo),e(H,CT),e(CT,_fe),e(_fe,KHo),e(CT,ZHo),e(CT,SD),e(SD,eUo),e(CT,oUo),e(H,rUo),e(H,MT),e(MT,ufe),e(ufe,tUo),e(MT,aUo),e(MT,PD),e(PD,nUo),e(MT,sUo),e(H,lUo),e(H,ET),e(ET,bfe),e(bfe,iUo),e(ET,dUo),e(ET,$D),e($D,cUo),e(ET,fUo),e(H,mUo),e(H,yT),e(yT,vfe),e(vfe,gUo),e(yT,hUo),e(yT,ID),e(ID,pUo),e(yT,_Uo),e(H,uUo),e(H,wT),e(wT,Tfe),e(Tfe,bUo),e(wT,vUo),e(wT,jD),e(jD,TUo),e(wT,FUo),e(H,CUo),e(H,AT),e(AT,Ffe),e(Ffe,MUo),e(AT,EUo),e(AT,ND),e(ND,yUo),e(AT,wUo),e(H,AUo),e(H,LT),e(LT,Cfe),e(Cfe,LUo),e(LT,BUo),e(LT,DD),e(DD,kUo),e(LT,xUo),e(H,RUo),e(H,BT),e(BT,Mfe),e(Mfe,SUo),e(BT,PUo),e(BT,qD),e(qD,$Uo),e(BT,IUo),e(H,jUo),e(H,kT),e(kT,Efe),e(Efe,NUo),e(kT,DUo),e(kT,GD),e(GD,qUo),e(kT,GUo),e(H,OUo),e(H,xT),e(xT,yfe),e(yfe,XUo),e(xT,zUo),e(xT,OD),e(OD,VUo),e(xT,WUo),e(H,QUo),e(H,RT),e(RT,wfe),e(wfe,HUo),e(RT,UUo),e(RT,XD),e(XD,JUo),e(RT,YUo),e(H,KUo),e(H,ST),e(ST,Afe),e(Afe,ZUo),e(ST,eJo),e(ST,zD),e(zD,oJo),e(ST,rJo),e(ho,tJo),e(ho,Lfe),e(Lfe,aJo),e(ho,nJo),g(Xy,ho,null),b(d,p8e,u),b(d,ic,u),e(ic,PT),e(PT,Bfe),g(zy,Bfe,null),e(ic,sJo),e(ic,kfe),e(kfe,lJo),b(d,_8e,u),b(d,pr,u),g(Vy,pr,null),e(pr,iJo),e(pr,dc),e(dc,dJo),e(dc,xfe),e(xfe,cJo),e(dc,fJo),e(dc,Rfe),e(Rfe,mJo),e(dc,gJo),e(pr,hJo),e(pr,Wy),e(Wy,pJo),e(Wy,Sfe),e(Sfe,_Jo),e(Wy,uJo),e(pr,bJo),e(pr,it),g(Qy,it,null),e(it,vJo),e(it,Pfe),e(Pfe,TJo),e(it,FJo),e(it,cc),e(cc,CJo),e(cc,$fe),e($fe,MJo),e(cc,EJo),e(cc,Ife),e(Ife,yJo),e(cc,wJo),e(it,AJo),e(it,jfe),e(jfe,LJo),e(it,BJo),g(Hy,it,null),e(pr,kJo),e(pr,po),g(Uy,po,null),e(po,xJo),e(po,Nfe),e(Nfe,RJo),e(po,SJo),e(po,cn),e(cn,PJo),e(cn,Dfe),e(Dfe,$Jo),e(cn,IJo),e(cn,qfe),e(qfe,jJo),e(cn,NJo),e(cn,Gfe),e(Gfe,DJo),e(cn,qJo),e(po,GJo),e(po,he),e(he,$T),e($T,Ofe),e(Ofe,OJo),e($T,XJo),e($T,VD),e(VD,zJo),e($T,VJo),e(he,WJo),e(he,IT),e(IT,Xfe),e(Xfe,QJo),e(IT,HJo),e(IT,WD),e(WD,UJo),e(IT,JJo),e(he,YJo),e(he,jT),e(jT,zfe),e(zfe,KJo),e(jT,ZJo),e(jT,QD),e(QD,eYo),e(jT,oYo),e(he,rYo),e(he,NT),e(NT,Vfe),e(Vfe,tYo),e(NT,aYo),e(NT,HD),e(HD,nYo),e(NT,sYo),e(he,lYo),e(he,DT),e(DT,Wfe),e(Wfe,iYo),e(DT,dYo),e(DT,UD),e(UD,cYo),e(DT,fYo),e(he,mYo),e(he,qT),e(qT,Qfe),e(Qfe,gYo),e(qT,hYo),e(qT,JD),e(JD,pYo),e(qT,_Yo),e(he,uYo),e(he,GT),e(GT,Hfe),e(Hfe,bYo),e(GT,vYo),e(GT,YD),e(YD,TYo),e(GT,FYo),e(he,CYo),e(he,OT),e(OT,Ufe),e(Ufe,MYo),e(OT,EYo),e(OT,KD),e(KD,yYo),e(OT,wYo),e(he,AYo),e(he,XT),e(XT,Jfe),e(Jfe,LYo),e(XT,BYo),e(XT,ZD),e(ZD,kYo),e(XT,xYo),e(he,RYo),e(he,zT),e(zT,Yfe),e(Yfe,SYo),e(zT,PYo),e(zT,eq),e(eq,$Yo),e(zT,IYo),e(po,jYo),e(po,Kfe),e(Kfe,NYo),e(po,DYo),g(Jy,po,null),b(d,u8e,u),b(d,fc,u),e(fc,VT),e(VT,Zfe),g(Yy,Zfe,null),e(fc,qYo),e(fc,eme),e(eme,GYo),b(d,b8e,u),b(d,_r,u),g(Ky,_r,null),e(_r,OYo),e(_r,mc),e(mc,XYo),e(mc,ome),e(ome,zYo),e(mc,VYo),e(mc,rme),e(rme,WYo),e(mc,QYo),e(_r,HYo),e(_r,Zy),e(Zy,UYo),e(Zy,tme),e(tme,JYo),e(Zy,YYo),e(_r,KYo),e(_r,dt),g(ew,dt,null),e(dt,ZYo),e(dt,ame),e(ame,eKo),e(dt,oKo),e(dt,gc),e(gc,rKo),e(gc,nme),e(nme,tKo),e(gc,aKo),e(gc,sme),e(sme,nKo),e(gc,sKo),e(dt,lKo),e(dt,lme),e(lme,iKo),e(dt,dKo),g(ow,dt,null),e(_r,cKo),e(_r,_o),g(rw,_o,null),e(_o,fKo),e(_o,ime),e(ime,mKo),e(_o,gKo),e(_o,fn),e(fn,hKo),e(fn,dme),e(dme,pKo),e(fn,_Ko),e(fn,cme),e(cme,uKo),e(fn,bKo),e(fn,fme),e(fme,vKo),e(fn,TKo),e(_o,FKo),e(_o,mme),e(mme,WT),e(WT,gme),e(gme,CKo),e(WT,MKo),e(WT,oq),e(oq,EKo),e(WT,yKo),e(_o,wKo),e(_o,hme),e(hme,AKo),e(_o,LKo),g(tw,_o,null),b(d,v8e,u),b(d,hc,u),e(hc,QT),e(QT,pme),g(aw,pme,null),e(hc,BKo),e(hc,_me),e(_me,kKo),b(d,T8e,u),b(d,ur,u),g(nw,ur,null),e(ur,xKo),e(ur,pc),e(pc,RKo),e(pc,ume),e(ume,SKo),e(pc,PKo),e(pc,bme),e(bme,$Ko),e(pc,IKo),e(ur,jKo),e(ur,sw),e(sw,NKo),e(sw,vme),e(vme,DKo),e(sw,qKo),e(ur,GKo),e(ur,ct),g(lw,ct,null),e(ct,OKo),e(ct,Tme),e(Tme,XKo),e(ct,zKo),e(ct,_c),e(_c,VKo),e(_c,Fme),e(Fme,WKo),e(_c,QKo),e(_c,Cme),e(Cme,HKo),e(_c,UKo),e(ct,JKo),e(ct,Mme),e(Mme,YKo),e(ct,KKo),g(iw,ct,null),e(ur,ZKo),e(ur,uo),g(dw,uo,null),e(uo,eZo),e(uo,Eme),e(Eme,oZo),e(uo,rZo),e(uo,mn),e(mn,tZo),e(mn,yme),e(yme,aZo),e(mn,nZo),e(mn,wme),e(wme,sZo),e(mn,lZo),e(mn,Ame),e(Ame,iZo),e(mn,dZo),e(uo,cZo),e(uo,Y),e(Y,HT),e(HT,Lme),e(Lme,fZo),e(HT,mZo),e(HT,rq),e(rq,gZo),e(HT,hZo),e(Y,pZo),e(Y,UT),e(UT,Bme),e(Bme,_Zo),e(UT,uZo),e(UT,tq),e(tq,bZo),e(UT,vZo),e(Y,TZo),e(Y,JT),e(JT,kme),e(kme,FZo),e(JT,CZo),e(JT,aq),e(aq,MZo),e(JT,EZo),e(Y,yZo),e(Y,YT),e(YT,xme),e(xme,wZo),e(YT,AZo),e(YT,nq),e(nq,LZo),e(YT,BZo),e(Y,kZo),e(Y,KT),e(KT,Rme),e(Rme,xZo),e(KT,RZo),e(KT,sq),e(sq,SZo),e(KT,PZo),e(Y,$Zo),e(Y,ZT),e(ZT,Sme),e(Sme,IZo),e(ZT,jZo),e(ZT,lq),e(lq,NZo),e(ZT,DZo),e(Y,qZo),e(Y,e7),e(e7,Pme),e(Pme,GZo),e(e7,OZo),e(e7,iq),e(iq,XZo),e(e7,zZo),e(Y,VZo),e(Y,o7),e(o7,$me),e($me,WZo),e(o7,QZo),e(o7,dq),e(dq,HZo),e(o7,UZo),e(Y,JZo),e(Y,r7),e(r7,Ime),e(Ime,YZo),e(r7,KZo),e(r7,cq),e(cq,ZZo),e(r7,eer),e(Y,oer),e(Y,t7),e(t7,jme),e(jme,rer),e(t7,ter),e(t7,fq),e(fq,aer),e(t7,ner),e(Y,ser),e(Y,a7),e(a7,Nme),e(Nme,ler),e(a7,ier),e(a7,mq),e(mq,der),e(a7,cer),e(Y,fer),e(Y,n7),e(n7,Dme),e(Dme,mer),e(n7,ger),e(n7,gq),e(gq,her),e(n7,per),e(Y,_er),e(Y,s7),e(s7,qme),e(qme,uer),e(s7,ber),e(s7,hq),e(hq,ver),e(s7,Ter),e(Y,Fer),e(Y,l7),e(l7,Gme),e(Gme,Cer),e(l7,Mer),e(l7,pq),e(pq,Eer),e(l7,yer),e(Y,wer),e(Y,i7),e(i7,Ome),e(Ome,Aer),e(i7,Ler),e(i7,_q),e(_q,Ber),e(i7,ker),e(Y,xer),e(Y,d7),e(d7,Xme),e(Xme,Rer),e(d7,Ser),e(d7,uq),e(uq,Per),e(d7,$er),e(Y,Ier),e(Y,c7),e(c7,zme),e(zme,jer),e(c7,Ner),e(c7,bq),e(bq,Der),e(c7,qer),e(Y,Ger),e(Y,f7),e(f7,Vme),e(Vme,Oer),e(f7,Xer),e(f7,vq),e(vq,zer),e(f7,Ver),e(Y,Wer),e(Y,m7),e(m7,Wme),e(Wme,Qer),e(m7,Her),e(m7,Tq),e(Tq,Uer),e(m7,Jer),e(Y,Yer),e(Y,g7),e(g7,Qme),e(Qme,Ker),e(g7,Zer),e(g7,Fq),e(Fq,eor),e(g7,oor),e(uo,ror),e(uo,Hme),e(Hme,tor),e(uo,aor),g(cw,uo,null),b(d,F8e,u),b(d,uc,u),e(uc,h7),e(h7,Ume),g(fw,Ume,null),e(uc,nor),e(uc,Jme),e(Jme,sor),b(d,C8e,u),b(d,br,u),g(mw,br,null),e(br,lor),e(br,bc),e(bc,ior),e(bc,Yme),e(Yme,dor),e(bc,cor),e(bc,Kme),e(Kme,mor),e(bc,gor),e(br,hor),e(br,gw),e(gw,por),e(gw,Zme),e(Zme,_or),e(gw,uor),e(br,bor),e(br,ft),g(hw,ft,null),e(ft,vor),e(ft,ege),e(ege,Tor),e(ft,For),e(ft,vc),e(vc,Cor),e(vc,oge),e(oge,Mor),e(vc,Eor),e(vc,rge),e(rge,yor),e(vc,wor),e(ft,Aor),e(ft,tge),e(tge,Lor),e(ft,Bor),g(pw,ft,null),e(br,kor),e(br,bo),g(_w,bo,null),e(bo,xor),e(bo,age),e(age,Ror),e(bo,Sor),e(bo,gn),e(gn,Por),e(gn,nge),e(nge,$or),e(gn,Ior),e(gn,sge),e(sge,jor),e(gn,Nor),e(gn,lge),e(lge,Dor),e(gn,qor),e(bo,Gor),e(bo,pe),e(pe,p7),e(p7,ige),e(ige,Oor),e(p7,Xor),e(p7,Cq),e(Cq,zor),e(p7,Vor),e(pe,Wor),e(pe,_7),e(_7,dge),e(dge,Qor),e(_7,Hor),e(_7,Mq),e(Mq,Uor),e(_7,Jor),e(pe,Yor),e(pe,u7),e(u7,cge),e(cge,Kor),e(u7,Zor),e(u7,Eq),e(Eq,err),e(u7,orr),e(pe,rrr),e(pe,b7),e(b7,fge),e(fge,trr),e(b7,arr),e(b7,yq),e(yq,nrr),e(b7,srr),e(pe,lrr),e(pe,v7),e(v7,mge),e(mge,irr),e(v7,drr),e(v7,wq),e(wq,crr),e(v7,frr),e(pe,mrr),e(pe,T7),e(T7,gge),e(gge,grr),e(T7,hrr),e(T7,Aq),e(Aq,prr),e(T7,_rr),e(pe,urr),e(pe,F7),e(F7,hge),e(hge,brr),e(F7,vrr),e(F7,Lq),e(Lq,Trr),e(F7,Frr),e(pe,Crr),e(pe,C7),e(C7,pge),e(pge,Mrr),e(C7,Err),e(C7,Bq),e(Bq,yrr),e(C7,wrr),e(pe,Arr),e(pe,M7),e(M7,_ge),e(_ge,Lrr),e(M7,Brr),e(M7,kq),e(kq,krr),e(M7,xrr),e(pe,Rrr),e(pe,E7),e(E7,uge),e(uge,Srr),e(E7,Prr),e(E7,xq),e(xq,$rr),e(E7,Irr),e(bo,jrr),e(bo,bge),e(bge,Nrr),e(bo,Drr),g(uw,bo,null),b(d,M8e,u),b(d,Tc,u),e(Tc,y7),e(y7,vge),g(bw,vge,null),e(Tc,qrr),e(Tc,Tge),e(Tge,Grr),b(d,E8e,u),b(d,vr,u),g(vw,vr,null),e(vr,Orr),e(vr,Fc),e(Fc,Xrr),e(Fc,Fge),e(Fge,zrr),e(Fc,Vrr),e(Fc,Cge),e(Cge,Wrr),e(Fc,Qrr),e(vr,Hrr),e(vr,Tw),e(Tw,Urr),e(Tw,Mge),e(Mge,Jrr),e(Tw,Yrr),e(vr,Krr),e(vr,mt),g(Fw,mt,null),e(mt,Zrr),e(mt,Ege),e(Ege,etr),e(mt,otr),e(mt,Cc),e(Cc,rtr),e(Cc,yge),e(yge,ttr),e(Cc,atr),e(Cc,wge),e(wge,ntr),e(Cc,str),e(mt,ltr),e(mt,Age),e(Age,itr),e(mt,dtr),g(Cw,mt,null),e(vr,ctr),e(vr,vo),g(Mw,vo,null),e(vo,ftr),e(vo,Lge),e(Lge,mtr),e(vo,gtr),e(vo,hn),e(hn,htr),e(hn,Bge),e(Bge,ptr),e(hn,_tr),e(hn,kge),e(kge,utr),e(hn,btr),e(hn,xge),e(xge,vtr),e(hn,Ttr),e(vo,Ftr),e(vo,X),e(X,w7),e(w7,Rge),e(Rge,Ctr),e(w7,Mtr),e(w7,Rq),e(Rq,Etr),e(w7,ytr),e(X,wtr),e(X,A7),e(A7,Sge),e(Sge,Atr),e(A7,Ltr),e(A7,Sq),e(Sq,Btr),e(A7,ktr),e(X,xtr),e(X,L7),e(L7,Pge),e(Pge,Rtr),e(L7,Str),e(L7,Pq),e(Pq,Ptr),e(L7,$tr),e(X,Itr),e(X,B7),e(B7,$ge),e($ge,jtr),e(B7,Ntr),e(B7,$q),e($q,Dtr),e(B7,qtr),e(X,Gtr),e(X,k7),e(k7,Ige),e(Ige,Otr),e(k7,Xtr),e(k7,Iq),e(Iq,ztr),e(k7,Vtr),e(X,Wtr),e(X,x7),e(x7,jge),e(jge,Qtr),e(x7,Htr),e(x7,jq),e(jq,Utr),e(x7,Jtr),e(X,Ytr),e(X,R7),e(R7,Nge),e(Nge,Ktr),e(R7,Ztr),e(R7,Nq),e(Nq,ear),e(R7,oar),e(X,rar),e(X,S7),e(S7,Dge),e(Dge,tar),e(S7,aar),e(S7,Dq),e(Dq,nar),e(S7,sar),e(X,lar),e(X,P7),e(P7,qge),e(qge,iar),e(P7,dar),e(P7,qq),e(qq,car),e(P7,far),e(X,mar),e(X,$7),e($7,Gge),e(Gge,gar),e($7,har),e($7,Gq),e(Gq,par),e($7,_ar),e(X,uar),e(X,I7),e(I7,Oge),e(Oge,bar),e(I7,Tar),e(I7,Oq),e(Oq,Far),e(I7,Car),e(X,Mar),e(X,j7),e(j7,Xge),e(Xge,Ear),e(j7,yar),e(j7,Xq),e(Xq,war),e(j7,Aar),e(X,Lar),e(X,N7),e(N7,zge),e(zge,Bar),e(N7,kar),e(N7,zq),e(zq,xar),e(N7,Rar),e(X,Sar),e(X,D7),e(D7,Vge),e(Vge,Par),e(D7,$ar),e(D7,Vq),e(Vq,Iar),e(D7,jar),e(X,Nar),e(X,q7),e(q7,Wge),e(Wge,Dar),e(q7,qar),e(q7,Wq),e(Wq,Gar),e(q7,Oar),e(X,Xar),e(X,G7),e(G7,Qge),e(Qge,zar),e(G7,Var),e(G7,Qq),e(Qq,War),e(G7,Qar),e(X,Har),e(X,O7),e(O7,Hge),e(Hge,Uar),e(O7,Jar),e(O7,Hq),e(Hq,Yar),e(O7,Kar),e(X,Zar),e(X,X7),e(X7,Uge),e(Uge,enr),e(X7,onr),e(X7,Uq),e(Uq,rnr),e(X7,tnr),e(X,anr),e(X,z7),e(z7,Jge),e(Jge,nnr),e(z7,snr),e(z7,Jq),e(Jq,lnr),e(z7,inr),e(X,dnr),e(X,V7),e(V7,Yge),e(Yge,cnr),e(V7,fnr),e(V7,Yq),e(Yq,mnr),e(V7,gnr),e(X,hnr),e(X,W7),e(W7,Kge),e(Kge,pnr),e(W7,_nr),e(W7,Kq),e(Kq,unr),e(W7,bnr),e(X,vnr),e(X,Q7),e(Q7,Zge),e(Zge,Tnr),e(Q7,Fnr),e(Q7,Zq),e(Zq,Cnr),e(Q7,Mnr),e(X,Enr),e(X,H7),e(H7,ehe),e(ehe,ynr),e(H7,wnr),e(H7,eG),e(eG,Anr),e(H7,Lnr),e(X,Bnr),e(X,U7),e(U7,ohe),e(ohe,knr),e(U7,xnr),e(U7,oG),e(oG,Rnr),e(U7,Snr),e(X,Pnr),e(X,J7),e(J7,rhe),e(rhe,$nr),e(J7,Inr),e(J7,rG),e(rG,jnr),e(J7,Nnr),e(vo,Dnr),e(vo,the),e(the,qnr),e(vo,Gnr),g(Ew,vo,null),b(d,y8e,u),b(d,Mc,u),e(Mc,Y7),e(Y7,ahe),g(yw,ahe,null),e(Mc,Onr),e(Mc,nhe),e(nhe,Xnr),b(d,w8e,u),b(d,Tr,u),g(ww,Tr,null),e(Tr,znr),e(Tr,Ec),e(Ec,Vnr),e(Ec,she),e(she,Wnr),e(Ec,Qnr),e(Ec,lhe),e(lhe,Hnr),e(Ec,Unr),e(Tr,Jnr),e(Tr,Aw),e(Aw,Ynr),e(Aw,ihe),e(ihe,Knr),e(Aw,Znr),e(Tr,esr),e(Tr,gt),g(Lw,gt,null),e(gt,osr),e(gt,dhe),e(dhe,rsr),e(gt,tsr),e(gt,yc),e(yc,asr),e(yc,che),e(che,nsr),e(yc,ssr),e(yc,fhe),e(fhe,lsr),e(yc,isr),e(gt,dsr),e(gt,mhe),e(mhe,csr),e(gt,fsr),g(Bw,gt,null),e(Tr,msr),e(Tr,To),g(kw,To,null),e(To,gsr),e(To,ghe),e(ghe,hsr),e(To,psr),e(To,pn),e(pn,_sr),e(pn,hhe),e(hhe,usr),e(pn,bsr),e(pn,phe),e(phe,vsr),e(pn,Tsr),e(pn,_he),e(_he,Fsr),e(pn,Csr),e(To,Msr),e(To,te),e(te,K7),e(K7,uhe),e(uhe,Esr),e(K7,ysr),e(K7,tG),e(tG,wsr),e(K7,Asr),e(te,Lsr),e(te,Z7),e(Z7,bhe),e(bhe,Bsr),e(Z7,ksr),e(Z7,aG),e(aG,xsr),e(Z7,Rsr),e(te,Ssr),e(te,eF),e(eF,vhe),e(vhe,Psr),e(eF,$sr),e(eF,nG),e(nG,Isr),e(eF,jsr),e(te,Nsr),e(te,oF),e(oF,The),e(The,Dsr),e(oF,qsr),e(oF,sG),e(sG,Gsr),e(oF,Osr),e(te,Xsr),e(te,rF),e(rF,Fhe),e(Fhe,zsr),e(rF,Vsr),e(rF,lG),e(lG,Wsr),e(rF,Qsr),e(te,Hsr),e(te,tF),e(tF,Che),e(Che,Usr),e(tF,Jsr),e(tF,iG),e(iG,Ysr),e(tF,Ksr),e(te,Zsr),e(te,aF),e(aF,Mhe),e(Mhe,elr),e(aF,olr),e(aF,dG),e(dG,rlr),e(aF,tlr),e(te,alr),e(te,nF),e(nF,Ehe),e(Ehe,nlr),e(nF,slr),e(nF,cG),e(cG,llr),e(nF,ilr),e(te,dlr),e(te,sF),e(sF,yhe),e(yhe,clr),e(sF,flr),e(sF,fG),e(fG,mlr),e(sF,glr),e(te,hlr),e(te,lF),e(lF,whe),e(whe,plr),e(lF,_lr),e(lF,mG),e(mG,ulr),e(lF,blr),e(te,vlr),e(te,iF),e(iF,Ahe),e(Ahe,Tlr),e(iF,Flr),e(iF,gG),e(gG,Clr),e(iF,Mlr),e(te,Elr),e(te,dF),e(dF,Lhe),e(Lhe,ylr),e(dF,wlr),e(dF,hG),e(hG,Alr),e(dF,Llr),e(te,Blr),e(te,cF),e(cF,Bhe),e(Bhe,klr),e(cF,xlr),e(cF,pG),e(pG,Rlr),e(cF,Slr),e(te,Plr),e(te,fF),e(fF,khe),e(khe,$lr),e(fF,Ilr),e(fF,_G),e(_G,jlr),e(fF,Nlr),e(te,Dlr),e(te,mF),e(mF,xhe),e(xhe,qlr),e(mF,Glr),e(mF,uG),e(uG,Olr),e(mF,Xlr),e(te,zlr),e(te,gF),e(gF,Rhe),e(Rhe,Vlr),e(gF,Wlr),e(gF,bG),e(bG,Qlr),e(gF,Hlr),e(te,Ulr),e(te,hF),e(hF,She),e(She,Jlr),e(hF,Ylr),e(hF,vG),e(vG,Klr),e(hF,Zlr),e(To,eir),e(To,Phe),e(Phe,oir),e(To,rir),g(xw,To,null),b(d,A8e,u),b(d,wc,u),e(wc,pF),e(pF,$he),g(Rw,$he,null),e(wc,tir),e(wc,Ihe),e(Ihe,air),b(d,L8e,u),b(d,Fr,u),g(Sw,Fr,null),e(Fr,nir),e(Fr,Ac),e(Ac,sir),e(Ac,jhe),e(jhe,lir),e(Ac,iir),e(Ac,Nhe),e(Nhe,dir),e(Ac,cir),e(Fr,fir),e(Fr,Pw),e(Pw,mir),e(Pw,Dhe),e(Dhe,gir),e(Pw,hir),e(Fr,pir),e(Fr,ht),g($w,ht,null),e(ht,_ir),e(ht,qhe),e(qhe,uir),e(ht,bir),e(ht,Lc),e(Lc,vir),e(Lc,Ghe),e(Ghe,Tir),e(Lc,Fir),e(Lc,Ohe),e(Ohe,Cir),e(Lc,Mir),e(ht,Eir),e(ht,Xhe),e(Xhe,yir),e(ht,wir),g(Iw,ht,null),e(Fr,Air),e(Fr,Fo),g(jw,Fo,null),e(Fo,Lir),e(Fo,zhe),e(zhe,Bir),e(Fo,kir),e(Fo,_n),e(_n,xir),e(_n,Vhe),e(Vhe,Rir),e(_n,Sir),e(_n,Whe),e(Whe,Pir),e(_n,$ir),e(_n,Qhe),e(Qhe,Iir),e(_n,jir),e(Fo,Nir),e(Fo,Hhe),e(Hhe,_F),e(_F,Uhe),e(Uhe,Dir),e(_F,qir),e(_F,TG),e(TG,Gir),e(_F,Oir),e(Fo,Xir),e(Fo,Jhe),e(Jhe,zir),e(Fo,Vir),g(Nw,Fo,null),b(d,B8e,u),b(d,Bc,u),e(Bc,uF),e(uF,Yhe),g(Dw,Yhe,null),e(Bc,Wir),e(Bc,Khe),e(Khe,Qir),b(d,k8e,u),b(d,Cr,u),g(qw,Cr,null),e(Cr,Hir),e(Cr,kc),e(kc,Uir),e(kc,Zhe),e(Zhe,Jir),e(kc,Yir),e(kc,epe),e(epe,Kir),e(kc,Zir),e(Cr,edr),e(Cr,Gw),e(Gw,odr),e(Gw,ope),e(ope,rdr),e(Gw,tdr),e(Cr,adr),e(Cr,pt),g(Ow,pt,null),e(pt,ndr),e(pt,rpe),e(rpe,sdr),e(pt,ldr),e(pt,xc),e(xc,idr),e(xc,tpe),e(tpe,ddr),e(xc,cdr),e(xc,ape),e(ape,fdr),e(xc,mdr),e(pt,gdr),e(pt,npe),e(npe,hdr),e(pt,pdr),g(Xw,pt,null),e(Cr,_dr),e(Cr,Co),g(zw,Co,null),e(Co,udr),e(Co,spe),e(spe,bdr),e(Co,vdr),e(Co,un),e(un,Tdr),e(un,lpe),e(lpe,Fdr),e(un,Cdr),e(un,ipe),e(ipe,Mdr),e(un,Edr),e(un,dpe),e(dpe,ydr),e(un,wdr),e(Co,Adr),e(Co,K),e(K,bF),e(bF,cpe),e(cpe,Ldr),e(bF,Bdr),e(bF,FG),e(FG,kdr),e(bF,xdr),e(K,Rdr),e(K,vF),e(vF,fpe),e(fpe,Sdr),e(vF,Pdr),e(vF,CG),e(CG,$dr),e(vF,Idr),e(K,jdr),e(K,TF),e(TF,mpe),e(mpe,Ndr),e(TF,Ddr),e(TF,MG),e(MG,qdr),e(TF,Gdr),e(K,Odr),e(K,FF),e(FF,gpe),e(gpe,Xdr),e(FF,zdr),e(FF,EG),e(EG,Vdr),e(FF,Wdr),e(K,Qdr),e(K,CF),e(CF,hpe),e(hpe,Hdr),e(CF,Udr),e(CF,yG),e(yG,Jdr),e(CF,Ydr),e(K,Kdr),e(K,MF),e(MF,ppe),e(ppe,Zdr),e(MF,ecr),e(MF,wG),e(wG,ocr),e(MF,rcr),e(K,tcr),e(K,EF),e(EF,_pe),e(_pe,acr),e(EF,ncr),e(EF,AG),e(AG,scr),e(EF,lcr),e(K,icr),e(K,yF),e(yF,upe),e(upe,dcr),e(yF,ccr),e(yF,LG),e(LG,fcr),e(yF,mcr),e(K,gcr),e(K,wF),e(wF,bpe),e(bpe,hcr),e(wF,pcr),e(wF,BG),e(BG,_cr),e(wF,ucr),e(K,bcr),e(K,AF),e(AF,vpe),e(vpe,vcr),e(AF,Tcr),e(AF,kG),e(kG,Fcr),e(AF,Ccr),e(K,Mcr),e(K,LF),e(LF,Tpe),e(Tpe,Ecr),e(LF,ycr),e(LF,xG),e(xG,wcr),e(LF,Acr),e(K,Lcr),e(K,BF),e(BF,Fpe),e(Fpe,Bcr),e(BF,kcr),e(BF,RG),e(RG,xcr),e(BF,Rcr),e(K,Scr),e(K,kF),e(kF,Cpe),e(Cpe,Pcr),e(kF,$cr),e(kF,SG),e(SG,Icr),e(kF,jcr),e(K,Ncr),e(K,xF),e(xF,Mpe),e(Mpe,Dcr),e(xF,qcr),e(xF,PG),e(PG,Gcr),e(xF,Ocr),e(K,Xcr),e(K,RF),e(RF,Epe),e(Epe,zcr),e(RF,Vcr),e(RF,$G),e($G,Wcr),e(RF,Qcr),e(K,Hcr),e(K,SF),e(SF,ype),e(ype,Ucr),e(SF,Jcr),e(SF,IG),e(IG,Ycr),e(SF,Kcr),e(K,Zcr),e(K,PF),e(PF,wpe),e(wpe,efr),e(PF,ofr),e(PF,jG),e(jG,rfr),e(PF,tfr),e(K,afr),e(K,$F),e($F,Ape),e(Ape,nfr),e($F,sfr),e($F,NG),e(NG,lfr),e($F,ifr),e(K,dfr),e(K,IF),e(IF,Lpe),e(Lpe,cfr),e(IF,ffr),e(IF,DG),e(DG,mfr),e(IF,gfr),e(K,hfr),e(K,jF),e(jF,Bpe),e(Bpe,pfr),e(jF,_fr),e(jF,qG),e(qG,ufr),e(jF,bfr),e(Co,vfr),e(Co,kpe),e(kpe,Tfr),e(Co,Ffr),g(Vw,Co,null),b(d,x8e,u),b(d,Rc,u),e(Rc,NF),e(NF,xpe),g(Ww,xpe,null),e(Rc,Cfr),e(Rc,Rpe),e(Rpe,Mfr),b(d,R8e,u),b(d,Mr,u),g(Qw,Mr,null),e(Mr,Efr),e(Mr,Sc),e(Sc,yfr),e(Sc,Spe),e(Spe,wfr),e(Sc,Afr),e(Sc,Ppe),e(Ppe,Lfr),e(Sc,Bfr),e(Mr,kfr),e(Mr,Hw),e(Hw,xfr),e(Hw,$pe),e($pe,Rfr),e(Hw,Sfr),e(Mr,Pfr),e(Mr,_t),g(Uw,_t,null),e(_t,$fr),e(_t,Ipe),e(Ipe,Ifr),e(_t,jfr),e(_t,Pc),e(Pc,Nfr),e(Pc,jpe),e(jpe,Dfr),e(Pc,qfr),e(Pc,Npe),e(Npe,Gfr),e(Pc,Ofr),e(_t,Xfr),e(_t,Dpe),e(Dpe,zfr),e(_t,Vfr),g(Jw,_t,null),e(Mr,Wfr),e(Mr,Mo),g(Yw,Mo,null),e(Mo,Qfr),e(Mo,qpe),e(qpe,Hfr),e(Mo,Ufr),e(Mo,bn),e(bn,Jfr),e(bn,Gpe),e(Gpe,Yfr),e(bn,Kfr),e(bn,Ope),e(Ope,Zfr),e(bn,emr),e(bn,Xpe),e(Xpe,omr),e(bn,rmr),e(Mo,tmr),e(Mo,Z),e(Z,DF),e(DF,zpe),e(zpe,amr),e(DF,nmr),e(DF,GG),e(GG,smr),e(DF,lmr),e(Z,imr),e(Z,qF),e(qF,Vpe),e(Vpe,dmr),e(qF,cmr),e(qF,OG),e(OG,fmr),e(qF,mmr),e(Z,gmr),e(Z,GF),e(GF,Wpe),e(Wpe,hmr),e(GF,pmr),e(GF,XG),e(XG,_mr),e(GF,umr),e(Z,bmr),e(Z,OF),e(OF,Qpe),e(Qpe,vmr),e(OF,Tmr),e(OF,zG),e(zG,Fmr),e(OF,Cmr),e(Z,Mmr),e(Z,XF),e(XF,Hpe),e(Hpe,Emr),e(XF,ymr),e(XF,VG),e(VG,wmr),e(XF,Amr),e(Z,Lmr),e(Z,zF),e(zF,Upe),e(Upe,Bmr),e(zF,kmr),e(zF,WG),e(WG,xmr),e(zF,Rmr),e(Z,Smr),e(Z,VF),e(VF,Jpe),e(Jpe,Pmr),e(VF,$mr),e(VF,QG),e(QG,Imr),e(VF,jmr),e(Z,Nmr),e(Z,WF),e(WF,Ype),e(Ype,Dmr),e(WF,qmr),e(WF,HG),e(HG,Gmr),e(WF,Omr),e(Z,Xmr),e(Z,QF),e(QF,Kpe),e(Kpe,zmr),e(QF,Vmr),e(QF,UG),e(UG,Wmr),e(QF,Qmr),e(Z,Hmr),e(Z,HF),e(HF,Zpe),e(Zpe,Umr),e(HF,Jmr),e(HF,JG),e(JG,Ymr),e(HF,Kmr),e(Z,Zmr),e(Z,UF),e(UF,e_e),e(e_e,egr),e(UF,ogr),e(UF,YG),e(YG,rgr),e(UF,tgr),e(Z,agr),e(Z,JF),e(JF,o_e),e(o_e,ngr),e(JF,sgr),e(JF,KG),e(KG,lgr),e(JF,igr),e(Z,dgr),e(Z,YF),e(YF,r_e),e(r_e,cgr),e(YF,fgr),e(YF,ZG),e(ZG,mgr),e(YF,ggr),e(Z,hgr),e(Z,KF),e(KF,t_e),e(t_e,pgr),e(KF,_gr),e(KF,eO),e(eO,ugr),e(KF,bgr),e(Z,vgr),e(Z,ZF),e(ZF,a_e),e(a_e,Tgr),e(ZF,Fgr),e(ZF,oO),e(oO,Cgr),e(ZF,Mgr),e(Z,Egr),e(Z,e9),e(e9,n_e),e(n_e,ygr),e(e9,wgr),e(e9,rO),e(rO,Agr),e(e9,Lgr),e(Z,Bgr),e(Z,o9),e(o9,s_e),e(s_e,kgr),e(o9,xgr),e(o9,tO),e(tO,Rgr),e(o9,Sgr),e(Z,Pgr),e(Z,r9),e(r9,l_e),e(l_e,$gr),e(r9,Igr),e(r9,aO),e(aO,jgr),e(r9,Ngr),e(Z,Dgr),e(Z,t9),e(t9,i_e),e(i_e,qgr),e(t9,Ggr),e(t9,nO),e(nO,Ogr),e(t9,Xgr),e(Mo,zgr),e(Mo,d_e),e(d_e,Vgr),e(Mo,Wgr),g(Kw,Mo,null),b(d,S8e,u),b(d,$c,u),e($c,a9),e(a9,c_e),g(Zw,c_e,null),e($c,Qgr),e($c,f_e),e(f_e,Hgr),b(d,P8e,u),b(d,Er,u),g(eA,Er,null),e(Er,Ugr),e(Er,Ic),e(Ic,Jgr),e(Ic,m_e),e(m_e,Ygr),e(Ic,Kgr),e(Ic,g_e),e(g_e,Zgr),e(Ic,ehr),e(Er,ohr),e(Er,oA),e(oA,rhr),e(oA,h_e),e(h_e,thr),e(oA,ahr),e(Er,nhr),e(Er,ut),g(rA,ut,null),e(ut,shr),e(ut,p_e),e(p_e,lhr),e(ut,ihr),e(ut,jc),e(jc,dhr),e(jc,__e),e(__e,chr),e(jc,fhr),e(jc,u_e),e(u_e,mhr),e(jc,ghr),e(ut,hhr),e(ut,b_e),e(b_e,phr),e(ut,_hr),g(tA,ut,null),e(Er,uhr),e(Er,Eo),g(aA,Eo,null),e(Eo,bhr),e(Eo,v_e),e(v_e,vhr),e(Eo,Thr),e(Eo,vn),e(vn,Fhr),e(vn,T_e),e(T_e,Chr),e(vn,Mhr),e(vn,F_e),e(F_e,Ehr),e(vn,yhr),e(vn,C_e),e(C_e,whr),e(vn,Ahr),e(Eo,Lhr),e(Eo,M_e),e(M_e,n9),e(n9,E_e),e(E_e,Bhr),e(n9,khr),e(n9,sO),e(sO,xhr),e(n9,Rhr),e(Eo,Shr),e(Eo,y_e),e(y_e,Phr),e(Eo,$hr),g(nA,Eo,null),b(d,$8e,u),b(d,Nc,u),e(Nc,s9),e(s9,w_e),g(sA,w_e,null),e(Nc,Ihr),e(Nc,A_e),e(A_e,jhr),b(d,I8e,u),b(d,yr,u),g(lA,yr,null),e(yr,Nhr),e(yr,Dc),e(Dc,Dhr),e(Dc,L_e),e(L_e,qhr),e(Dc,Ghr),e(Dc,B_e),e(B_e,Ohr),e(Dc,Xhr),e(yr,zhr),e(yr,iA),e(iA,Vhr),e(iA,k_e),e(k_e,Whr),e(iA,Qhr),e(yr,Hhr),e(yr,bt),g(dA,bt,null),e(bt,Uhr),e(bt,x_e),e(x_e,Jhr),e(bt,Yhr),e(bt,qc),e(qc,Khr),e(qc,R_e),e(R_e,Zhr),e(qc,epr),e(qc,S_e),e(S_e,opr),e(qc,rpr),e(bt,tpr),e(bt,P_e),e(P_e,apr),e(bt,npr),g(cA,bt,null),e(yr,spr),e(yr,yo),g(fA,yo,null),e(yo,lpr),e(yo,$_e),e($_e,ipr),e(yo,dpr),e(yo,Tn),e(Tn,cpr),e(Tn,I_e),e(I_e,fpr),e(Tn,mpr),e(Tn,j_e),e(j_e,gpr),e(Tn,hpr),e(Tn,N_e),e(N_e,ppr),e(Tn,_pr),e(yo,upr),e(yo,D_e),e(D_e,l9),e(l9,q_e),e(q_e,bpr),e(l9,vpr),e(l9,lO),e(lO,Tpr),e(l9,Fpr),e(yo,Cpr),e(yo,G_e),e(G_e,Mpr),e(yo,Epr),g(mA,yo,null),b(d,j8e,u),b(d,Gc,u),e(Gc,i9),e(i9,O_e),g(gA,O_e,null),e(Gc,ypr),e(Gc,X_e),e(X_e,wpr),b(d,N8e,u),b(d,wr,u),g(hA,wr,null),e(wr,Apr),e(wr,Oc),e(Oc,Lpr),e(Oc,z_e),e(z_e,Bpr),e(Oc,kpr),e(Oc,V_e),e(V_e,xpr),e(Oc,Rpr),e(wr,Spr),e(wr,pA),e(pA,Ppr),e(pA,W_e),e(W_e,$pr),e(pA,Ipr),e(wr,jpr),e(wr,vt),g(_A,vt,null),e(vt,Npr),e(vt,Q_e),e(Q_e,Dpr),e(vt,qpr),e(vt,Xc),e(Xc,Gpr),e(Xc,H_e),e(H_e,Opr),e(Xc,Xpr),e(Xc,U_e),e(U_e,zpr),e(Xc,Vpr),e(vt,Wpr),e(vt,J_e),e(J_e,Qpr),e(vt,Hpr),g(uA,vt,null),e(wr,Upr),e(wr,wo),g(bA,wo,null),e(wo,Jpr),e(wo,Y_e),e(Y_e,Ypr),e(wo,Kpr),e(wo,Fn),e(Fn,Zpr),e(Fn,K_e),e(K_e,e_r),e(Fn,o_r),e(Fn,Z_e),e(Z_e,r_r),e(Fn,t_r),e(Fn,eue),e(eue,a_r),e(Fn,n_r),e(wo,s_r),e(wo,V),e(V,d9),e(d9,oue),e(oue,l_r),e(d9,i_r),e(d9,iO),e(iO,d_r),e(d9,c_r),e(V,f_r),e(V,c9),e(c9,rue),e(rue,m_r),e(c9,g_r),e(c9,dO),e(dO,h_r),e(c9,p_r),e(V,__r),e(V,f9),e(f9,tue),e(tue,u_r),e(f9,b_r),e(f9,cO),e(cO,v_r),e(f9,T_r),e(V,F_r),e(V,m9),e(m9,aue),e(aue,C_r),e(m9,M_r),e(m9,fO),e(fO,E_r),e(m9,y_r),e(V,w_r),e(V,g9),e(g9,nue),e(nue,A_r),e(g9,L_r),e(g9,mO),e(mO,B_r),e(g9,k_r),e(V,x_r),e(V,h9),e(h9,sue),e(sue,R_r),e(h9,S_r),e(h9,gO),e(gO,P_r),e(h9,$_r),e(V,I_r),e(V,p9),e(p9,lue),e(lue,j_r),e(p9,N_r),e(p9,hO),e(hO,D_r),e(p9,q_r),e(V,G_r),e(V,_9),e(_9,iue),e(iue,O_r),e(_9,X_r),e(_9,pO),e(pO,z_r),e(_9,V_r),e(V,W_r),e(V,u9),e(u9,due),e(due,Q_r),e(u9,H_r),e(u9,_O),e(_O,U_r),e(u9,J_r),e(V,Y_r),e(V,b9),e(b9,cue),e(cue,K_r),e(b9,Z_r),e(b9,uO),e(uO,eur),e(b9,our),e(V,rur),e(V,v9),e(v9,fue),e(fue,tur),e(v9,aur),e(v9,bO),e(bO,nur),e(v9,sur),e(V,lur),e(V,T9),e(T9,mue),e(mue,iur),e(T9,dur),e(T9,vO),e(vO,cur),e(T9,fur),e(V,mur),e(V,F9),e(F9,gue),e(gue,gur),e(F9,hur),e(F9,TO),e(TO,pur),e(F9,_ur),e(V,uur),e(V,C9),e(C9,hue),e(hue,bur),e(C9,vur),e(C9,FO),e(FO,Tur),e(C9,Fur),e(V,Cur),e(V,M9),e(M9,pue),e(pue,Mur),e(M9,Eur),e(M9,CO),e(CO,yur),e(M9,wur),e(V,Aur),e(V,E9),e(E9,_ue),e(_ue,Lur),e(E9,Bur),e(E9,MO),e(MO,kur),e(E9,xur),e(V,Rur),e(V,y9),e(y9,uue),e(uue,Sur),e(y9,Pur),e(y9,EO),e(EO,$ur),e(y9,Iur),e(V,jur),e(V,w9),e(w9,bue),e(bue,Nur),e(w9,Dur),e(w9,yO),e(yO,qur),e(w9,Gur),e(V,Our),e(V,A9),e(A9,vue),e(vue,Xur),e(A9,zur),e(A9,wO),e(wO,Vur),e(A9,Wur),e(V,Qur),e(V,L9),e(L9,Tue),e(Tue,Hur),e(L9,Uur),e(L9,AO),e(AO,Jur),e(L9,Yur),e(V,Kur),e(V,B9),e(B9,Fue),e(Fue,Zur),e(B9,e2r),e(B9,LO),e(LO,o2r),e(B9,r2r),e(V,t2r),e(V,k9),e(k9,Cue),e(Cue,a2r),e(k9,n2r),e(k9,BO),e(BO,s2r),e(k9,l2r),e(V,i2r),e(V,x9),e(x9,Mue),e(Mue,d2r),e(x9,c2r),e(x9,kO),e(kO,f2r),e(x9,m2r),e(V,g2r),e(V,R9),e(R9,Eue),e(Eue,h2r),e(R9,p2r),e(R9,xO),e(xO,_2r),e(R9,u2r),e(wo,b2r),e(wo,yue),e(yue,v2r),e(wo,T2r),g(vA,wo,null),b(d,D8e,u),b(d,zc,u),e(zc,S9),e(S9,wue),g(TA,wue,null),e(zc,F2r),e(zc,Aue),e(Aue,C2r),b(d,q8e,u),b(d,Ar,u),g(FA,Ar,null),e(Ar,M2r),e(Ar,Vc),e(Vc,E2r),e(Vc,Lue),e(Lue,y2r),e(Vc,w2r),e(Vc,Bue),e(Bue,A2r),e(Vc,L2r),e(Ar,B2r),e(Ar,CA),e(CA,k2r),e(CA,kue),e(kue,x2r),e(CA,R2r),e(Ar,S2r),e(Ar,Tt),g(MA,Tt,null),e(Tt,P2r),e(Tt,xue),e(xue,$2r),e(Tt,I2r),e(Tt,Wc),e(Wc,j2r),e(Wc,Rue),e(Rue,N2r),e(Wc,D2r),e(Wc,Sue),e(Sue,q2r),e(Wc,G2r),e(Tt,O2r),e(Tt,Pue),e(Pue,X2r),e(Tt,z2r),g(EA,Tt,null),e(Ar,V2r),e(Ar,Ao),g(yA,Ao,null),e(Ao,W2r),e(Ao,$ue),e($ue,Q2r),e(Ao,H2r),e(Ao,Cn),e(Cn,U2r),e(Cn,Iue),e(Iue,J2r),e(Cn,Y2r),e(Cn,jue),e(jue,K2r),e(Cn,Z2r),e(Cn,Nue),e(Nue,e1r),e(Cn,o1r),e(Ao,r1r),e(Ao,Mn),e(Mn,P9),e(P9,Due),e(Due,t1r),e(P9,a1r),e(P9,RO),e(RO,n1r),e(P9,s1r),e(Mn,l1r),e(Mn,$9),e($9,que),e(que,i1r),e($9,d1r),e($9,SO),e(SO,c1r),e($9,f1r),e(Mn,m1r),e(Mn,I9),e(I9,Gue),e(Gue,g1r),e(I9,h1r),e(I9,PO),e(PO,p1r),e(I9,_1r),e(Mn,u1r),e(Mn,j9),e(j9,Oue),e(Oue,b1r),e(j9,v1r),e(j9,$O),e($O,T1r),e(j9,F1r),e(Ao,C1r),e(Ao,Xue),e(Xue,M1r),e(Ao,E1r),g(wA,Ao,null),b(d,G8e,u),b(d,Qc,u),e(Qc,N9),e(N9,zue),g(AA,zue,null),e(Qc,y1r),e(Qc,Vue),e(Vue,w1r),b(d,O8e,u),b(d,Lr,u),g(LA,Lr,null),e(Lr,A1r),e(Lr,Hc),e(Hc,L1r),e(Hc,Wue),e(Wue,B1r),e(Hc,k1r),e(Hc,Que),e(Que,x1r),e(Hc,R1r),e(Lr,S1r),e(Lr,BA),e(BA,P1r),e(BA,Hue),e(Hue,$1r),e(BA,I1r),e(Lr,j1r),e(Lr,Ft),g(kA,Ft,null),e(Ft,N1r),e(Ft,Uue),e(Uue,D1r),e(Ft,q1r),e(Ft,Uc),e(Uc,G1r),e(Uc,Jue),e(Jue,O1r),e(Uc,X1r),e(Uc,Yue),e(Yue,z1r),e(Uc,V1r),e(Ft,W1r),e(Ft,Kue),e(Kue,Q1r),e(Ft,H1r),g(xA,Ft,null),e(Lr,U1r),e(Lr,Lo),g(RA,Lo,null),e(Lo,J1r),e(Lo,Zue),e(Zue,Y1r),e(Lo,K1r),e(Lo,En),e(En,Z1r),e(En,e2e),e(e2e,ebr),e(En,obr),e(En,o2e),e(o2e,rbr),e(En,tbr),e(En,r2e),e(r2e,abr),e(En,nbr),e(Lo,sbr),e(Lo,fe),e(fe,D9),e(D9,t2e),e(t2e,lbr),e(D9,ibr),e(D9,IO),e(IO,dbr),e(D9,cbr),e(fe,fbr),e(fe,q9),e(q9,a2e),e(a2e,mbr),e(q9,gbr),e(q9,jO),e(jO,hbr),e(q9,pbr),e(fe,_br),e(fe,G9),e(G9,n2e),e(n2e,ubr),e(G9,bbr),e(G9,NO),e(NO,vbr),e(G9,Tbr),e(fe,Fbr),e(fe,O9),e(O9,s2e),e(s2e,Cbr),e(O9,Mbr),e(O9,DO),e(DO,Ebr),e(O9,ybr),e(fe,wbr),e(fe,X9),e(X9,l2e),e(l2e,Abr),e(X9,Lbr),e(X9,qO),e(qO,Bbr),e(X9,kbr),e(fe,xbr),e(fe,z9),e(z9,i2e),e(i2e,Rbr),e(z9,Sbr),e(z9,GO),e(GO,Pbr),e(z9,$br),e(fe,Ibr),e(fe,V9),e(V9,d2e),e(d2e,jbr),e(V9,Nbr),e(V9,OO),e(OO,Dbr),e(V9,qbr),e(fe,Gbr),e(fe,W9),e(W9,c2e),e(c2e,Obr),e(W9,Xbr),e(W9,XO),e(XO,zbr),e(W9,Vbr),e(fe,Wbr),e(fe,Q9),e(Q9,f2e),e(f2e,Qbr),e(Q9,Hbr),e(Q9,zO),e(zO,Ubr),e(Q9,Jbr),e(fe,Ybr),e(fe,H9),e(H9,m2e),e(m2e,Kbr),e(H9,Zbr),e(H9,VO),e(VO,e5r),e(H9,o5r),e(fe,r5r),e(fe,U9),e(U9,g2e),e(g2e,t5r),e(U9,a5r),e(U9,WO),e(WO,n5r),e(U9,s5r),e(Lo,l5r),e(Lo,h2e),e(h2e,i5r),e(Lo,d5r),g(SA,Lo,null),b(d,X8e,u),b(d,Jc,u),e(Jc,J9),e(J9,p2e),g(PA,p2e,null),e(Jc,c5r),e(Jc,_2e),e(_2e,f5r),b(d,z8e,u),b(d,Br,u),g($A,Br,null),e(Br,m5r),e(Br,Yc),e(Yc,g5r),e(Yc,u2e),e(u2e,h5r),e(Yc,p5r),e(Yc,b2e),e(b2e,_5r),e(Yc,u5r),e(Br,b5r),e(Br,IA),e(IA,v5r),e(IA,v2e),e(v2e,T5r),e(IA,F5r),e(Br,C5r),e(Br,Ct),g(jA,Ct,null),e(Ct,M5r),e(Ct,T2e),e(T2e,E5r),e(Ct,y5r),e(Ct,Kc),e(Kc,w5r),e(Kc,F2e),e(F2e,A5r),e(Kc,L5r),e(Kc,C2e),e(C2e,B5r),e(Kc,k5r),e(Ct,x5r),e(Ct,M2e),e(M2e,R5r),e(Ct,S5r),g(NA,Ct,null),e(Br,P5r),e(Br,Bo),g(DA,Bo,null),e(Bo,$5r),e(Bo,E2e),e(E2e,I5r),e(Bo,j5r),e(Bo,yn),e(yn,N5r),e(yn,y2e),e(y2e,D5r),e(yn,q5r),e(yn,w2e),e(w2e,G5r),e(yn,O5r),e(yn,A2e),e(A2e,X5r),e(yn,z5r),e(Bo,V5r),e(Bo,ve),e(ve,Y9),e(Y9,L2e),e(L2e,W5r),e(Y9,Q5r),e(Y9,QO),e(QO,H5r),e(Y9,U5r),e(ve,J5r),e(ve,K9),e(K9,B2e),e(B2e,Y5r),e(K9,K5r),e(K9,HO),e(HO,Z5r),e(K9,evr),e(ve,ovr),e(ve,Z9),e(Z9,k2e),e(k2e,rvr),e(Z9,tvr),e(Z9,UO),e(UO,avr),e(Z9,nvr),e(ve,svr),e(ve,eC),e(eC,x2e),e(x2e,lvr),e(eC,ivr),e(eC,JO),e(JO,dvr),e(eC,cvr),e(ve,fvr),e(ve,oC),e(oC,R2e),e(R2e,mvr),e(oC,gvr),e(oC,YO),e(YO,hvr),e(oC,pvr),e(ve,_vr),e(ve,rC),e(rC,S2e),e(S2e,uvr),e(rC,bvr),e(rC,KO),e(KO,vvr),e(rC,Tvr),e(ve,Fvr),e(ve,tC),e(tC,P2e),e(P2e,Cvr),e(tC,Mvr),e(tC,ZO),e(ZO,Evr),e(tC,yvr),e(ve,wvr),e(ve,aC),e(aC,$2e),e($2e,Avr),e(aC,Lvr),e(aC,eX),e(eX,Bvr),e(aC,kvr),e(ve,xvr),e(ve,nC),e(nC,I2e),e(I2e,Rvr),e(nC,Svr),e(nC,oX),e(oX,Pvr),e(nC,$vr),e(Bo,Ivr),e(Bo,j2e),e(j2e,jvr),e(Bo,Nvr),g(qA,Bo,null),b(d,V8e,u),b(d,Zc,u),e(Zc,sC),e(sC,N2e),g(GA,N2e,null),e(Zc,Dvr),e(Zc,D2e),e(D2e,qvr),b(d,W8e,u),b(d,kr,u),g(OA,kr,null),e(kr,Gvr),e(kr,ef),e(ef,Ovr),e(ef,q2e),e(q2e,Xvr),e(ef,zvr),e(ef,G2e),e(G2e,Vvr),e(ef,Wvr),e(kr,Qvr),e(kr,XA),e(XA,Hvr),e(XA,O2e),e(O2e,Uvr),e(XA,Jvr),e(kr,Yvr),e(kr,Mt),g(zA,Mt,null),e(Mt,Kvr),e(Mt,X2e),e(X2e,Zvr),e(Mt,eTr),e(Mt,of),e(of,oTr),e(of,z2e),e(z2e,rTr),e(of,tTr),e(of,V2e),e(V2e,aTr),e(of,nTr),e(Mt,sTr),e(Mt,W2e),e(W2e,lTr),e(Mt,iTr),g(VA,Mt,null),e(kr,dTr),e(kr,ko),g(WA,ko,null),e(ko,cTr),e(ko,Q2e),e(Q2e,fTr),e(ko,mTr),e(ko,wn),e(wn,gTr),e(wn,H2e),e(H2e,hTr),e(wn,pTr),e(wn,U2e),e(U2e,_Tr),e(wn,uTr),e(wn,J2e),e(J2e,bTr),e(wn,vTr),e(ko,TTr),e(ko,Te),e(Te,lC),e(lC,Y2e),e(Y2e,FTr),e(lC,CTr),e(lC,rX),e(rX,MTr),e(lC,ETr),e(Te,yTr),e(Te,iC),e(iC,K2e),e(K2e,wTr),e(iC,ATr),e(iC,tX),e(tX,LTr),e(iC,BTr),e(Te,kTr),e(Te,dC),e(dC,Z2e),e(Z2e,xTr),e(dC,RTr),e(dC,aX),e(aX,STr),e(dC,PTr),e(Te,$Tr),e(Te,cC),e(cC,e1e),e(e1e,ITr),e(cC,jTr),e(cC,nX),e(nX,NTr),e(cC,DTr),e(Te,qTr),e(Te,fC),e(fC,o1e),e(o1e,GTr),e(fC,OTr),e(fC,sX),e(sX,XTr),e(fC,zTr),e(Te,VTr),e(Te,mC),e(mC,r1e),e(r1e,WTr),e(mC,QTr),e(mC,lX),e(lX,HTr),e(mC,UTr),e(Te,JTr),e(Te,gC),e(gC,t1e),e(t1e,YTr),e(gC,KTr),e(gC,iX),e(iX,ZTr),e(gC,e7r),e(Te,o7r),e(Te,hC),e(hC,a1e),e(a1e,r7r),e(hC,t7r),e(hC,dX),e(dX,a7r),e(hC,n7r),e(Te,s7r),e(Te,pC),e(pC,n1e),e(n1e,l7r),e(pC,i7r),e(pC,cX),e(cX,d7r),e(pC,c7r),e(ko,f7r),e(ko,s1e),e(s1e,m7r),e(ko,g7r),g(QA,ko,null),b(d,Q8e,u),b(d,rf,u),e(rf,_C),e(_C,l1e),g(HA,l1e,null),e(rf,h7r),e(rf,i1e),e(i1e,p7r),b(d,H8e,u),b(d,xr,u),g(UA,xr,null),e(xr,_7r),e(xr,tf),e(tf,u7r),e(tf,d1e),e(d1e,b7r),e(tf,v7r),e(tf,c1e),e(c1e,T7r),e(tf,F7r),e(xr,C7r),e(xr,JA),e(JA,M7r),e(JA,f1e),e(f1e,E7r),e(JA,y7r),e(xr,w7r),e(xr,Et),g(YA,Et,null),e(Et,A7r),e(Et,m1e),e(m1e,L7r),e(Et,B7r),e(Et,af),e(af,k7r),e(af,g1e),e(g1e,x7r),e(af,R7r),e(af,h1e),e(h1e,S7r),e(af,P7r),e(Et,$7r),e(Et,p1e),e(p1e,I7r),e(Et,j7r),g(KA,Et,null),e(xr,N7r),e(xr,xo),g(ZA,xo,null),e(xo,D7r),e(xo,_1e),e(_1e,q7r),e(xo,G7r),e(xo,An),e(An,O7r),e(An,u1e),e(u1e,X7r),e(An,z7r),e(An,b1e),e(b1e,V7r),e(An,W7r),e(An,v1e),e(v1e,Q7r),e(An,H7r),e(xo,U7r),e(xo,Fe),e(Fe,uC),e(uC,T1e),e(T1e,J7r),e(uC,Y7r),e(uC,fX),e(fX,K7r),e(uC,Z7r),e(Fe,eFr),e(Fe,bC),e(bC,F1e),e(F1e,oFr),e(bC,rFr),e(bC,mX),e(mX,tFr),e(bC,aFr),e(Fe,nFr),e(Fe,vC),e(vC,C1e),e(C1e,sFr),e(vC,lFr),e(vC,gX),e(gX,iFr),e(vC,dFr),e(Fe,cFr),e(Fe,TC),e(TC,M1e),e(M1e,fFr),e(TC,mFr),e(TC,hX),e(hX,gFr),e(TC,hFr),e(Fe,pFr),e(Fe,FC),e(FC,E1e),e(E1e,_Fr),e(FC,uFr),e(FC,pX),e(pX,bFr),e(FC,vFr),e(Fe,TFr),e(Fe,CC),e(CC,y1e),e(y1e,FFr),e(CC,CFr),e(CC,_X),e(_X,MFr),e(CC,EFr),e(Fe,yFr),e(Fe,MC),e(MC,w1e),e(w1e,wFr),e(MC,AFr),e(MC,uX),e(uX,LFr),e(MC,BFr),e(Fe,kFr),e(Fe,EC),e(EC,A1e),e(A1e,xFr),e(EC,RFr),e(EC,bX),e(bX,SFr),e(EC,PFr),e(Fe,$Fr),e(Fe,yC),e(yC,L1e),e(L1e,IFr),e(yC,jFr),e(yC,vX),e(vX,NFr),e(yC,DFr),e(xo,qFr),e(xo,B1e),e(B1e,GFr),e(xo,OFr),g(e6,xo,null),b(d,U8e,u),b(d,nf,u),e(nf,wC),e(wC,k1e),g(o6,k1e,null),e(nf,XFr),e(nf,x1e),e(x1e,zFr),b(d,J8e,u),b(d,Rr,u),g(r6,Rr,null),e(Rr,VFr),e(Rr,sf),e(sf,WFr),e(sf,R1e),e(R1e,QFr),e(sf,HFr),e(sf,S1e),e(S1e,UFr),e(sf,JFr),e(Rr,YFr),e(Rr,t6),e(t6,KFr),e(t6,P1e),e(P1e,ZFr),e(t6,e9r),e(Rr,o9r),e(Rr,yt),g(a6,yt,null),e(yt,r9r),e(yt,$1e),e($1e,t9r),e(yt,a9r),e(yt,lf),e(lf,n9r),e(lf,I1e),e(I1e,s9r),e(lf,l9r),e(lf,j1e),e(j1e,i9r),e(lf,d9r),e(yt,c9r),e(yt,N1e),e(N1e,f9r),e(yt,m9r),g(n6,yt,null),e(Rr,g9r),e(Rr,Ro),g(s6,Ro,null),e(Ro,h9r),e(Ro,D1e),e(D1e,p9r),e(Ro,_9r),e(Ro,Ln),e(Ln,u9r),e(Ln,q1e),e(q1e,b9r),e(Ln,v9r),e(Ln,G1e),e(G1e,T9r),e(Ln,F9r),e(Ln,O1e),e(O1e,C9r),e(Ln,M9r),e(Ro,E9r),e(Ro,Ce),e(Ce,AC),e(AC,X1e),e(X1e,y9r),e(AC,w9r),e(AC,TX),e(TX,A9r),e(AC,L9r),e(Ce,B9r),e(Ce,LC),e(LC,z1e),e(z1e,k9r),e(LC,x9r),e(LC,FX),e(FX,R9r),e(LC,S9r),e(Ce,P9r),e(Ce,BC),e(BC,V1e),e(V1e,$9r),e(BC,I9r),e(BC,CX),e(CX,j9r),e(BC,N9r),e(Ce,D9r),e(Ce,kC),e(kC,W1e),e(W1e,q9r),e(kC,G9r),e(kC,MX),e(MX,O9r),e(kC,X9r),e(Ce,z9r),e(Ce,xC),e(xC,Q1e),e(Q1e,V9r),e(xC,W9r),e(xC,EX),e(EX,Q9r),e(xC,H9r),e(Ce,U9r),e(Ce,RC),e(RC,H1e),e(H1e,J9r),e(RC,Y9r),e(RC,yX),e(yX,K9r),e(RC,Z9r),e(Ce,eCr),e(Ce,SC),e(SC,U1e),e(U1e,oCr),e(SC,rCr),e(SC,wX),e(wX,tCr),e(SC,aCr),e(Ce,nCr),e(Ce,PC),e(PC,J1e),e(J1e,sCr),e(PC,lCr),e(PC,AX),e(AX,iCr),e(PC,dCr),e(Ce,cCr),e(Ce,$C),e($C,Y1e),e(Y1e,fCr),e($C,mCr),e($C,LX),e(LX,gCr),e($C,hCr),e(Ro,pCr),e(Ro,K1e),e(K1e,_Cr),e(Ro,uCr),g(l6,Ro,null),b(d,Y8e,u),b(d,df,u),e(df,IC),e(IC,Z1e),g(i6,Z1e,null),e(df,bCr),e(df,ebe),e(ebe,vCr),b(d,K8e,u),b(d,Sr,u),g(d6,Sr,null),e(Sr,TCr),e(Sr,cf),e(cf,FCr),e(cf,obe),e(obe,CCr),e(cf,MCr),e(cf,rbe),e(rbe,ECr),e(cf,yCr),e(Sr,wCr),e(Sr,c6),e(c6,ACr),e(c6,tbe),e(tbe,LCr),e(c6,BCr),e(Sr,kCr),e(Sr,wt),g(f6,wt,null),e(wt,xCr),e(wt,abe),e(abe,RCr),e(wt,SCr),e(wt,ff),e(ff,PCr),e(ff,nbe),e(nbe,$Cr),e(ff,ICr),e(ff,sbe),e(sbe,jCr),e(ff,NCr),e(wt,DCr),e(wt,lbe),e(lbe,qCr),e(wt,GCr),g(m6,wt,null),e(Sr,OCr),e(Sr,So),g(g6,So,null),e(So,XCr),e(So,ibe),e(ibe,zCr),e(So,VCr),e(So,Bn),e(Bn,WCr),e(Bn,dbe),e(dbe,QCr),e(Bn,HCr),e(Bn,cbe),e(cbe,UCr),e(Bn,JCr),e(Bn,fbe),e(fbe,YCr),e(Bn,KCr),e(So,ZCr),e(So,so),e(so,jC),e(jC,mbe),e(mbe,e4r),e(jC,o4r),e(jC,BX),e(BX,r4r),e(jC,t4r),e(so,a4r),e(so,NC),e(NC,gbe),e(gbe,n4r),e(NC,s4r),e(NC,kX),e(kX,l4r),e(NC,i4r),e(so,d4r),e(so,DC),e(DC,hbe),e(hbe,c4r),e(DC,f4r),e(DC,xX),e(xX,m4r),e(DC,g4r),e(so,h4r),e(so,qC),e(qC,pbe),e(pbe,p4r),e(qC,_4r),e(qC,RX),e(RX,u4r),e(qC,b4r),e(so,v4r),e(so,GC),e(GC,_be),e(_be,T4r),e(GC,F4r),e(GC,SX),e(SX,C4r),e(GC,M4r),e(so,E4r),e(so,OC),e(OC,ube),e(ube,y4r),e(OC,w4r),e(OC,PX),e(PX,A4r),e(OC,L4r),e(so,B4r),e(so,XC),e(XC,bbe),e(bbe,k4r),e(XC,x4r),e(XC,$X),e($X,R4r),e(XC,S4r),e(So,P4r),e(So,vbe),e(vbe,$4r),e(So,I4r),g(h6,So,null),b(d,Z8e,u),b(d,mf,u),e(mf,zC),e(zC,Tbe),g(p6,Tbe,null),e(mf,j4r),e(mf,Fbe),e(Fbe,N4r),b(d,eBe,u),b(d,Pr,u),g(_6,Pr,null),e(Pr,D4r),e(Pr,gf),e(gf,q4r),e(gf,Cbe),e(Cbe,G4r),e(gf,O4r),e(gf,Mbe),e(Mbe,X4r),e(gf,z4r),e(Pr,V4r),e(Pr,u6),e(u6,W4r),e(u6,Ebe),e(Ebe,Q4r),e(u6,H4r),e(Pr,U4r),e(Pr,At),g(b6,At,null),e(At,J4r),e(At,ybe),e(ybe,Y4r),e(At,K4r),e(At,hf),e(hf,Z4r),e(hf,wbe),e(wbe,eMr),e(hf,oMr),e(hf,Abe),e(Abe,rMr),e(hf,tMr),e(At,aMr),e(At,Lbe),e(Lbe,nMr),e(At,sMr),g(v6,At,null),e(Pr,lMr),e(Pr,Po),g(T6,Po,null),e(Po,iMr),e(Po,Bbe),e(Bbe,dMr),e(Po,cMr),e(Po,kn),e(kn,fMr),e(kn,kbe),e(kbe,mMr),e(kn,gMr),e(kn,xbe),e(xbe,hMr),e(kn,pMr),e(kn,Rbe),e(Rbe,_Mr),e(kn,uMr),e(Po,bMr),e(Po,lo),e(lo,VC),e(VC,Sbe),e(Sbe,vMr),e(VC,TMr),e(VC,IX),e(IX,FMr),e(VC,CMr),e(lo,MMr),e(lo,WC),e(WC,Pbe),e(Pbe,EMr),e(WC,yMr),e(WC,jX),e(jX,wMr),e(WC,AMr),e(lo,LMr),e(lo,QC),e(QC,$be),e($be,BMr),e(QC,kMr),e(QC,NX),e(NX,xMr),e(QC,RMr),e(lo,SMr),e(lo,HC),e(HC,Ibe),e(Ibe,PMr),e(HC,$Mr),e(HC,DX),e(DX,IMr),e(HC,jMr),e(lo,NMr),e(lo,UC),e(UC,jbe),e(jbe,DMr),e(UC,qMr),e(UC,qX),e(qX,GMr),e(UC,OMr),e(lo,XMr),e(lo,JC),e(JC,Nbe),e(Nbe,zMr),e(JC,VMr),e(JC,GX),e(GX,WMr),e(JC,QMr),e(lo,HMr),e(lo,YC),e(YC,Dbe),e(Dbe,UMr),e(YC,JMr),e(YC,OX),e(OX,YMr),e(YC,KMr),e(Po,ZMr),e(Po,qbe),e(qbe,eEr),e(Po,oEr),g(F6,Po,null),b(d,oBe,u),b(d,pf,u),e(pf,KC),e(KC,Gbe),g(C6,Gbe,null),e(pf,rEr),e(pf,Obe),e(Obe,tEr),b(d,rBe,u),b(d,$r,u),g(M6,$r,null),e($r,aEr),e($r,_f),e(_f,nEr),e(_f,Xbe),e(Xbe,sEr),e(_f,lEr),e(_f,zbe),e(zbe,iEr),e(_f,dEr),e($r,cEr),e($r,E6),e(E6,fEr),e(E6,Vbe),e(Vbe,mEr),e(E6,gEr),e($r,hEr),e($r,Lt),g(y6,Lt,null),e(Lt,pEr),e(Lt,Wbe),e(Wbe,_Er),e(Lt,uEr),e(Lt,uf),e(uf,bEr),e(uf,Qbe),e(Qbe,vEr),e(uf,TEr),e(uf,Hbe),e(Hbe,FEr),e(uf,CEr),e(Lt,MEr),e(Lt,Ube),e(Ube,EEr),e(Lt,yEr),g(w6,Lt,null),e($r,wEr),e($r,$o),g(A6,$o,null),e($o,AEr),e($o,Jbe),e(Jbe,LEr),e($o,BEr),e($o,xn),e(xn,kEr),e(xn,Ybe),e(Ybe,xEr),e(xn,REr),e(xn,Kbe),e(Kbe,SEr),e(xn,PEr),e(xn,Zbe),e(Zbe,$Er),e(xn,IEr),e($o,jEr),e($o,e5e),e(e5e,ZC),e(ZC,o5e),e(o5e,NEr),e(ZC,DEr),e(ZC,XX),e(XX,qEr),e(ZC,GEr),e($o,OEr),e($o,r5e),e(r5e,XEr),e($o,zEr),g(L6,$o,null),b(d,tBe,u),b(d,bf,u),e(bf,e4),e(e4,t5e),g(B6,t5e,null),e(bf,VEr),e(bf,a5e),e(a5e,WEr),b(d,aBe,u),b(d,Ir,u),g(k6,Ir,null),e(Ir,QEr),e(Ir,vf),e(vf,HEr),e(vf,n5e),e(n5e,UEr),e(vf,JEr),e(vf,s5e),e(s5e,YEr),e(vf,KEr),e(Ir,ZEr),e(Ir,x6),e(x6,e3r),e(x6,l5e),e(l5e,o3r),e(x6,r3r),e(Ir,t3r),e(Ir,Bt),g(R6,Bt,null),e(Bt,a3r),e(Bt,i5e),e(i5e,n3r),e(Bt,s3r),e(Bt,Tf),e(Tf,l3r),e(Tf,d5e),e(d5e,i3r),e(Tf,d3r),e(Tf,c5e),e(c5e,c3r),e(Tf,f3r),e(Bt,m3r),e(Bt,f5e),e(f5e,g3r),e(Bt,h3r),g(S6,Bt,null),e(Ir,p3r),e(Ir,Io),g(P6,Io,null),e(Io,_3r),e(Io,m5e),e(m5e,u3r),e(Io,b3r),e(Io,Rn),e(Rn,v3r),e(Rn,g5e),e(g5e,T3r),e(Rn,F3r),e(Rn,h5e),e(h5e,C3r),e(Rn,M3r),e(Rn,p5e),e(p5e,E3r),e(Rn,y3r),e(Io,w3r),e(Io,$6),e($6,o4),e(o4,_5e),e(_5e,A3r),e(o4,L3r),e(o4,zX),e(zX,B3r),e(o4,k3r),e($6,x3r),e($6,r4),e(r4,u5e),e(u5e,R3r),e(r4,S3r),e(r4,VX),e(VX,P3r),e(r4,$3r),e(Io,I3r),e(Io,b5e),e(b5e,j3r),e(Io,N3r),g(I6,Io,null),b(d,nBe,u),b(d,Ff,u),e(Ff,t4),e(t4,v5e),g(j6,v5e,null),e(Ff,D3r),e(Ff,T5e),e(T5e,q3r),b(d,sBe,u),b(d,jr,u),g(N6,jr,null),e(jr,G3r),e(jr,Cf),e(Cf,O3r),e(Cf,F5e),e(F5e,X3r),e(Cf,z3r),e(Cf,C5e),e(C5e,V3r),e(Cf,W3r),e(jr,Q3r),e(jr,D6),e(D6,H3r),e(D6,M5e),e(M5e,U3r),e(D6,J3r),e(jr,Y3r),e(jr,kt),g(q6,kt,null),e(kt,K3r),e(kt,E5e),e(E5e,Z3r),e(kt,eyr),e(kt,Mf),e(Mf,oyr),e(Mf,y5e),e(y5e,ryr),e(Mf,tyr),e(Mf,w5e),e(w5e,ayr),e(Mf,nyr),e(kt,syr),e(kt,A5e),e(A5e,lyr),e(kt,iyr),g(G6,kt,null),e(jr,dyr),e(jr,jo),g(O6,jo,null),e(jo,cyr),e(jo,L5e),e(L5e,fyr),e(jo,myr),e(jo,Sn),e(Sn,gyr),e(Sn,B5e),e(B5e,hyr),e(Sn,pyr),e(Sn,k5e),e(k5e,_yr),e(Sn,uyr),e(Sn,x5e),e(x5e,byr),e(Sn,vyr),e(jo,Tyr),e(jo,R5e),e(R5e,a4),e(a4,S5e),e(S5e,Fyr),e(a4,Cyr),e(a4,WX),e(WX,Myr),e(a4,Eyr),e(jo,yyr),e(jo,P5e),e(P5e,wyr),e(jo,Ayr),g(X6,jo,null),lBe=!0},p(d,[u]){const z6={};u&2&&(z6.$$scope={dirty:u,ctx:d}),kf.$set(z6);const $5e={};u&2&&($5e.$$scope={dirty:u,ctx:d}),dh.$set($5e);const I5e={};u&2&&(I5e.$$scope={dirty:u,ctx:d}),Th.$set(I5e)},i(d){lBe||(h(ce.$$.fragment,d),h($a.$$.fragment,d),h(sM.$$.fragment,d),h(lM.$$.fragment,d),h(kf.$$.fragment,d),h(iM.$$.fragment,d),h(dM.$$.fragment,d),h(mM.$$.fragment,d),h(gM.$$.fragment,d),h(hM.$$.fragment,d),h(pM.$$.fragment,d),h(_M.$$.fragment,d),h(vM.$$.fragment,d),h(TM.$$.fragment,d),h(FM.$$.fragment,d),h(CM.$$.fragment,d),h(MM.$$.fragment,d),h(wM.$$.fragment,d),h(dh.$$.fragment,d),h(AM.$$.fragment,d),h(LM.$$.fragment,d),h(BM.$$.fragment,d),h(kM.$$.fragment,d),h(SM.$$.fragment,d),h(Th.$$.fragment,d),h(PM.$$.fragment,d),h($M.$$.fragment,d),h(IM.$$.fragment,d),h(jM.$$.fragment,d),h(DM.$$.fragment,d),h(qM.$$.fragment,d),h(GM.$$.fragment,d),h(OM.$$.fragment,d),h(XM.$$.fragment,d),h(zM.$$.fragment,d),h(WM.$$.fragment,d),h(QM.$$.fragment,d),h(HM.$$.fragment,d),h(UM.$$.fragment,d),h(JM.$$.fragment,d),h(YM.$$.fragment,d),h(ZM.$$.fragment,d),h(eE.$$.fragment,d),h(oE.$$.fragment,d),h(rE.$$.fragment,d),h(tE.$$.fragment,d),h(aE.$$.fragment,d),h(sE.$$.fragment,d),h(lE.$$.fragment,d),h(iE.$$.fragment,d),h(dE.$$.fragment,d),h(cE.$$.fragment,d),h(fE.$$.fragment,d),h(gE.$$.fragment,d),h(hE.$$.fragment,d),h(pE.$$.fragment,d),h(_E.$$.fragment,d),h(uE.$$.fragment,d),h(bE.$$.fragment,d),h(TE.$$.fragment,d),h(FE.$$.fragment,d),h(CE.$$.fragment,d),h(ME.$$.fragment,d),h(EE.$$.fragment,d),h(yE.$$.fragment,d),h(AE.$$.fragment,d),h(LE.$$.fragment,d),h(BE.$$.fragment,d),h(kE.$$.fragment,d),h(xE.$$.fragment,d),h(RE.$$.fragment,d),h(PE.$$.fragment,d),h($E.$$.fragment,d),h(IE.$$.fragment,d),h(jE.$$.fragment,d),h(NE.$$.fragment,d),h(DE.$$.fragment,d),h(GE.$$.fragment,d),h(OE.$$.fragment,d),h(XE.$$.fragment,d),h(zE.$$.fragment,d),h(VE.$$.fragment,d),h(WE.$$.fragment,d),h(HE.$$.fragment,d),h(UE.$$.fragment,d),h(JE.$$.fragment,d),h(YE.$$.fragment,d),h(KE.$$.fragment,d),h(ZE.$$.fragment,d),h(o3.$$.fragment,d),h(r3.$$.fragment,d),h(t3.$$.fragment,d),h(a3.$$.fragment,d),h(n3.$$.fragment,d),h(s3.$$.fragment,d),h(i3.$$.fragment,d),h(d3.$$.fragment,d),h(c3.$$.fragment,d),h(f3.$$.fragment,d),h(m3.$$.fragment,d),h(g3.$$.fragment,d),h(p3.$$.fragment,d),h(_3.$$.fragment,d),h(u3.$$.fragment,d),h(b3.$$.fragment,d),h(v3.$$.fragment,d),h(T3.$$.fragment,d),h(C3.$$.fragment,d),h(M3.$$.fragment,d),h(E3.$$.fragment,d),h(y3.$$.fragment,d),h(w3.$$.fragment,d),h(A3.$$.fragment,d),h(B3.$$.fragment,d),h(k3.$$.fragment,d),h(x3.$$.fragment,d),h(R3.$$.fragment,d),h(S3.$$.fragment,d),h(P3.$$.fragment,d),h(I3.$$.fragment,d),h(j3.$$.fragment,d),h(N3.$$.fragment,d),h(D3.$$.fragment,d),h(q3.$$.fragment,d),h(G3.$$.fragment,d),h(X3.$$.fragment,d),h(z3.$$.fragment,d),h(V3.$$.fragment,d),h(Q3.$$.fragment,d),h(H3.$$.fragment,d),h(U3.$$.fragment,d),h(Y3.$$.fragment,d),h(K3.$$.fragment,d),h(Z3.$$.fragment,d),h(ey.$$.fragment,d),h(oy.$$.fragment,d),h(ry.$$.fragment,d),h(ay.$$.fragment,d),h(ny.$$.fragment,d),h(sy.$$.fragment,d),h(ly.$$.fragment,d),h(iy.$$.fragment,d),h(dy.$$.fragment,d),h(fy.$$.fragment,d),h(my.$$.fragment,d),h(gy.$$.fragment,d),h(hy.$$.fragment,d),h(py.$$.fragment,d),h(_y.$$.fragment,d),h(by.$$.fragment,d),h(vy.$$.fragment,d),h(Ty.$$.fragment,d),h(Fy.$$.fragment,d),h(Cy.$$.fragment,d),h(My.$$.fragment,d),h(yy.$$.fragment,d),h(wy.$$.fragment,d),h(Ay.$$.fragment,d),h(By.$$.fragment,d),h(ky.$$.fragment,d),h(xy.$$.fragment,d),h(Sy.$$.fragment,d),h(Py.$$.fragment,d),h($y.$$.fragment,d),h(Iy.$$.fragment,d),h(jy.$$.fragment,d),h(Ny.$$.fragment,d),h(qy.$$.fragment,d),h(Gy.$$.fragment,d),h(Oy.$$.fragment,d),h(Xy.$$.fragment,d),h(zy.$$.fragment,d),h(Vy.$$.fragment,d),h(Qy.$$.fragment,d),h(Hy.$$.fragment,d),h(Uy.$$.fragment,d),h(Jy.$$.fragment,d),h(Yy.$$.fragment,d),h(Ky.$$.fragment,d),h(ew.$$.fragment,d),h(ow.$$.fragment,d),h(rw.$$.fragment,d),h(tw.$$.fragment,d),h(aw.$$.fragment,d),h(nw.$$.fragment,d),h(lw.$$.fragment,d),h(iw.$$.fragment,d),h(dw.$$.fragment,d),h(cw.$$.fragment,d),h(fw.$$.fragment,d),h(mw.$$.fragment,d),h(hw.$$.fragment,d),h(pw.$$.fragment,d),h(_w.$$.fragment,d),h(uw.$$.fragment,d),h(bw.$$.fragment,d),h(vw.$$.fragment,d),h(Fw.$$.fragment,d),h(Cw.$$.fragment,d),h(Mw.$$.fragment,d),h(Ew.$$.fragment,d),h(yw.$$.fragment,d),h(ww.$$.fragment,d),h(Lw.$$.fragment,d),h(Bw.$$.fragment,d),h(kw.$$.fragment,d),h(xw.$$.fragment,d),h(Rw.$$.fragment,d),h(Sw.$$.fragment,d),h($w.$$.fragment,d),h(Iw.$$.fragment,d),h(jw.$$.fragment,d),h(Nw.$$.fragment,d),h(Dw.$$.fragment,d),h(qw.$$.fragment,d),h(Ow.$$.fragment,d),h(Xw.$$.fragment,d),h(zw.$$.fragment,d),h(Vw.$$.fragment,d),h(Ww.$$.fragment,d),h(Qw.$$.fragment,d),h(Uw.$$.fragment,d),h(Jw.$$.fragment,d),h(Yw.$$.fragment,d),h(Kw.$$.fragment,d),h(Zw.$$.fragment,d),h(eA.$$.fragment,d),h(rA.$$.fragment,d),h(tA.$$.fragment,d),h(aA.$$.fragment,d),h(nA.$$.fragment,d),h(sA.$$.fragment,d),h(lA.$$.fragment,d),h(dA.$$.fragment,d),h(cA.$$.fragment,d),h(fA.$$.fragment,d),h(mA.$$.fragment,d),h(gA.$$.fragment,d),h(hA.$$.fragment,d),h(_A.$$.fragment,d),h(uA.$$.fragment,d),h(bA.$$.fragment,d),h(vA.$$.fragment,d),h(TA.$$.fragment,d),h(FA.$$.fragment,d),h(MA.$$.fragment,d),h(EA.$$.fragment,d),h(yA.$$.fragment,d),h(wA.$$.fragment,d),h(AA.$$.fragment,d),h(LA.$$.fragment,d),h(kA.$$.fragment,d),h(xA.$$.fragment,d),h(RA.$$.fragment,d),h(SA.$$.fragment,d),h(PA.$$.fragment,d),h($A.$$.fragment,d),h(jA.$$.fragment,d),h(NA.$$.fragment,d),h(DA.$$.fragment,d),h(qA.$$.fragment,d),h(GA.$$.fragment,d),h(OA.$$.fragment,d),h(zA.$$.fragment,d),h(VA.$$.fragment,d),h(WA.$$.fragment,d),h(QA.$$.fragment,d),h(HA.$$.fragment,d),h(UA.$$.fragment,d),h(YA.$$.fragment,d),h(KA.$$.fragment,d),h(ZA.$$.fragment,d),h(e6.$$.fragment,d),h(o6.$$.fragment,d),h(r6.$$.fragment,d),h(a6.$$.fragment,d),h(n6.$$.fragment,d),h(s6.$$.fragment,d),h(l6.$$.fragment,d),h(i6.$$.fragment,d),h(d6.$$.fragment,d),h(f6.$$.fragment,d),h(m6.$$.fragment,d),h(g6.$$.fragment,d),h(h6.$$.fragment,d),h(p6.$$.fragment,d),h(_6.$$.fragment,d),h(b6.$$.fragment,d),h(v6.$$.fragment,d),h(T6.$$.fragment,d),h(F6.$$.fragment,d),h(C6.$$.fragment,d),h(M6.$$.fragment,d),h(y6.$$.fragment,d),h(w6.$$.fragment,d),h(A6.$$.fragment,d),h(L6.$$.fragment,d),h(B6.$$.fragment,d),h(k6.$$.fragment,d),h(R6.$$.fragment,d),h(S6.$$.fragment,d),h(P6.$$.fragment,d),h(I6.$$.fragment,d),h(j6.$$.fragment,d),h(N6.$$.fragment,d),h(q6.$$.fragment,d),h(G6.$$.fragment,d),h(O6.$$.fragment,d),h(X6.$$.fragment,d),lBe=!0)},o(d){p(ce.$$.fragment,d),p($a.$$.fragment,d),p(sM.$$.fragment,d),p(lM.$$.fragment,d),p(kf.$$.fragment,d),p(iM.$$.fragment,d),p(dM.$$.fragment,d),p(mM.$$.fragment,d),p(gM.$$.fragment,d),p(hM.$$.fragment,d),p(pM.$$.fragment,d),p(_M.$$.fragment,d),p(vM.$$.fragment,d),p(TM.$$.fragment,d),p(FM.$$.fragment,d),p(CM.$$.fragment,d),p(MM.$$.fragment,d),p(wM.$$.fragment,d),p(dh.$$.fragment,d),p(AM.$$.fragment,d),p(LM.$$.fragment,d),p(BM.$$.fragment,d),p(kM.$$.fragment,d),p(SM.$$.fragment,d),p(Th.$$.fragment,d),p(PM.$$.fragment,d),p($M.$$.fragment,d),p(IM.$$.fragment,d),p(jM.$$.fragment,d),p(DM.$$.fragment,d),p(qM.$$.fragment,d),p(GM.$$.fragment,d),p(OM.$$.fragment,d),p(XM.$$.fragment,d),p(zM.$$.fragment,d),p(WM.$$.fragment,d),p(QM.$$.fragment,d),p(HM.$$.fragment,d),p(UM.$$.fragment,d),p(JM.$$.fragment,d),p(YM.$$.fragment,d),p(ZM.$$.fragment,d),p(eE.$$.fragment,d),p(oE.$$.fragment,d),p(rE.$$.fragment,d),p(tE.$$.fragment,d),p(aE.$$.fragment,d),p(sE.$$.fragment,d),p(lE.$$.fragment,d),p(iE.$$.fragment,d),p(dE.$$.fragment,d),p(cE.$$.fragment,d),p(fE.$$.fragment,d),p(gE.$$.fragment,d),p(hE.$$.fragment,d),p(pE.$$.fragment,d),p(_E.$$.fragment,d),p(uE.$$.fragment,d),p(bE.$$.fragment,d),p(TE.$$.fragment,d),p(FE.$$.fragment,d),p(CE.$$.fragment,d),p(ME.$$.fragment,d),p(EE.$$.fragment,d),p(yE.$$.fragment,d),p(AE.$$.fragment,d),p(LE.$$.fragment,d),p(BE.$$.fragment,d),p(kE.$$.fragment,d),p(xE.$$.fragment,d),p(RE.$$.fragment,d),p(PE.$$.fragment,d),p($E.$$.fragment,d),p(IE.$$.fragment,d),p(jE.$$.fragment,d),p(NE.$$.fragment,d),p(DE.$$.fragment,d),p(GE.$$.fragment,d),p(OE.$$.fragment,d),p(XE.$$.fragment,d),p(zE.$$.fragment,d),p(VE.$$.fragment,d),p(WE.$$.fragment,d),p(HE.$$.fragment,d),p(UE.$$.fragment,d),p(JE.$$.fragment,d),p(YE.$$.fragment,d),p(KE.$$.fragment,d),p(ZE.$$.fragment,d),p(o3.$$.fragment,d),p(r3.$$.fragment,d),p(t3.$$.fragment,d),p(a3.$$.fragment,d),p(n3.$$.fragment,d),p(s3.$$.fragment,d),p(i3.$$.fragment,d),p(d3.$$.fragment,d),p(c3.$$.fragment,d),p(f3.$$.fragment,d),p(m3.$$.fragment,d),p(g3.$$.fragment,d),p(p3.$$.fragment,d),p(_3.$$.fragment,d),p(u3.$$.fragment,d),p(b3.$$.fragment,d),p(v3.$$.fragment,d),p(T3.$$.fragment,d),p(C3.$$.fragment,d),p(M3.$$.fragment,d),p(E3.$$.fragment,d),p(y3.$$.fragment,d),p(w3.$$.fragment,d),p(A3.$$.fragment,d),p(B3.$$.fragment,d),p(k3.$$.fragment,d),p(x3.$$.fragment,d),p(R3.$$.fragment,d),p(S3.$$.fragment,d),p(P3.$$.fragment,d),p(I3.$$.fragment,d),p(j3.$$.fragment,d),p(N3.$$.fragment,d),p(D3.$$.fragment,d),p(q3.$$.fragment,d),p(G3.$$.fragment,d),p(X3.$$.fragment,d),p(z3.$$.fragment,d),p(V3.$$.fragment,d),p(Q3.$$.fragment,d),p(H3.$$.fragment,d),p(U3.$$.fragment,d),p(Y3.$$.fragment,d),p(K3.$$.fragment,d),p(Z3.$$.fragment,d),p(ey.$$.fragment,d),p(oy.$$.fragment,d),p(ry.$$.fragment,d),p(ay.$$.fragment,d),p(ny.$$.fragment,d),p(sy.$$.fragment,d),p(ly.$$.fragment,d),p(iy.$$.fragment,d),p(dy.$$.fragment,d),p(fy.$$.fragment,d),p(my.$$.fragment,d),p(gy.$$.fragment,d),p(hy.$$.fragment,d),p(py.$$.fragment,d),p(_y.$$.fragment,d),p(by.$$.fragment,d),p(vy.$$.fragment,d),p(Ty.$$.fragment,d),p(Fy.$$.fragment,d),p(Cy.$$.fragment,d),p(My.$$.fragment,d),p(yy.$$.fragment,d),p(wy.$$.fragment,d),p(Ay.$$.fragment,d),p(By.$$.fragment,d),p(ky.$$.fragment,d),p(xy.$$.fragment,d),p(Sy.$$.fragment,d),p(Py.$$.fragment,d),p($y.$$.fragment,d),p(Iy.$$.fragment,d),p(jy.$$.fragment,d),p(Ny.$$.fragment,d),p(qy.$$.fragment,d),p(Gy.$$.fragment,d),p(Oy.$$.fragment,d),p(Xy.$$.fragment,d),p(zy.$$.fragment,d),p(Vy.$$.fragment,d),p(Qy.$$.fragment,d),p(Hy.$$.fragment,d),p(Uy.$$.fragment,d),p(Jy.$$.fragment,d),p(Yy.$$.fragment,d),p(Ky.$$.fragment,d),p(ew.$$.fragment,d),p(ow.$$.fragment,d),p(rw.$$.fragment,d),p(tw.$$.fragment,d),p(aw.$$.fragment,d),p(nw.$$.fragment,d),p(lw.$$.fragment,d),p(iw.$$.fragment,d),p(dw.$$.fragment,d),p(cw.$$.fragment,d),p(fw.$$.fragment,d),p(mw.$$.fragment,d),p(hw.$$.fragment,d),p(pw.$$.fragment,d),p(_w.$$.fragment,d),p(uw.$$.fragment,d),p(bw.$$.fragment,d),p(vw.$$.fragment,d),p(Fw.$$.fragment,d),p(Cw.$$.fragment,d),p(Mw.$$.fragment,d),p(Ew.$$.fragment,d),p(yw.$$.fragment,d),p(ww.$$.fragment,d),p(Lw.$$.fragment,d),p(Bw.$$.fragment,d),p(kw.$$.fragment,d),p(xw.$$.fragment,d),p(Rw.$$.fragment,d),p(Sw.$$.fragment,d),p($w.$$.fragment,d),p(Iw.$$.fragment,d),p(jw.$$.fragment,d),p(Nw.$$.fragment,d),p(Dw.$$.fragment,d),p(qw.$$.fragment,d),p(Ow.$$.fragment,d),p(Xw.$$.fragment,d),p(zw.$$.fragment,d),p(Vw.$$.fragment,d),p(Ww.$$.fragment,d),p(Qw.$$.fragment,d),p(Uw.$$.fragment,d),p(Jw.$$.fragment,d),p(Yw.$$.fragment,d),p(Kw.$$.fragment,d),p(Zw.$$.fragment,d),p(eA.$$.fragment,d),p(rA.$$.fragment,d),p(tA.$$.fragment,d),p(aA.$$.fragment,d),p(nA.$$.fragment,d),p(sA.$$.fragment,d),p(lA.$$.fragment,d),p(dA.$$.fragment,d),p(cA.$$.fragment,d),p(fA.$$.fragment,d),p(mA.$$.fragment,d),p(gA.$$.fragment,d),p(hA.$$.fragment,d),p(_A.$$.fragment,d),p(uA.$$.fragment,d),p(bA.$$.fragment,d),p(vA.$$.fragment,d),p(TA.$$.fragment,d),p(FA.$$.fragment,d),p(MA.$$.fragment,d),p(EA.$$.fragment,d),p(yA.$$.fragment,d),p(wA.$$.fragment,d),p(AA.$$.fragment,d),p(LA.$$.fragment,d),p(kA.$$.fragment,d),p(xA.$$.fragment,d),p(RA.$$.fragment,d),p(SA.$$.fragment,d),p(PA.$$.fragment,d),p($A.$$.fragment,d),p(jA.$$.fragment,d),p(NA.$$.fragment,d),p(DA.$$.fragment,d),p(qA.$$.fragment,d),p(GA.$$.fragment,d),p(OA.$$.fragment,d),p(zA.$$.fragment,d),p(VA.$$.fragment,d),p(WA.$$.fragment,d),p(QA.$$.fragment,d),p(HA.$$.fragment,d),p(UA.$$.fragment,d),p(YA.$$.fragment,d),p(KA.$$.fragment,d),p(ZA.$$.fragment,d),p(e6.$$.fragment,d),p(o6.$$.fragment,d),p(r6.$$.fragment,d),p(a6.$$.fragment,d),p(n6.$$.fragment,d),p(s6.$$.fragment,d),p(l6.$$.fragment,d),p(i6.$$.fragment,d),p(d6.$$.fragment,d),p(f6.$$.fragment,d),p(m6.$$.fragment,d),p(g6.$$.fragment,d),p(h6.$$.fragment,d),p(p6.$$.fragment,d),p(_6.$$.fragment,d),p(b6.$$.fragment,d),p(v6.$$.fragment,d),p(T6.$$.fragment,d),p(F6.$$.fragment,d),p(C6.$$.fragment,d),p(M6.$$.fragment,d),p(y6.$$.fragment,d),p(w6.$$.fragment,d),p(A6.$$.fragment,d),p(L6.$$.fragment,d),p(B6.$$.fragment,d),p(k6.$$.fragment,d),p(R6.$$.fragment,d),p(S6.$$.fragment,d),p(P6.$$.fragment,d),p(I6.$$.fragment,d),p(j6.$$.fragment,d),p(N6.$$.fragment,d),p(q6.$$.fragment,d),p(G6.$$.fragment,d),p(O6.$$.fragment,d),p(X6.$$.fragment,d),lBe=!1},d(d){t(J),d&&t(Ae),d&&t(ie),_(ce),d&&t(yf),d&&t(sa),d&&t(ye),d&&t(io),d&&t(Af),_($a,d),d&&t(co),d&&t(ge),d&&t(qo),d&&t(Ia),d&&t(iLe),d&&t(Pi),_(sM),d&&t(dLe),d&&t(Nn),d&&t(cLe),_(lM,d),d&&t(fLe),d&&t(W0),d&&t(mLe),_(kf,d),d&&t(gLe),d&&t($i),_(iM),d&&t(hLe),d&&t(Go),_(dM),_(mM),_(gM),_(hM),d&&t(pLe),d&&t(ji),_(pM),d&&t(_Le),d&&t(Oo),_(_M),_(vM),_(TM),_(FM),d&&t(uLe),d&&t(Ni),_(CM),d&&t(bLe),d&&t(Xo),_(MM),_(wM),_(dh),_(AM),_(LM),d&&t(vLe),d&&t(Di),_(BM),d&&t(TLe),d&&t(zo),_(kM),_(SM),_(Th),_(PM),_($M),d&&t(FLe),d&&t(Gi),_(IM),d&&t(CLe),d&&t(Vo),_(jM),_(DM),_(qM),_(GM),_(OM),d&&t(MLe),d&&t(zi),_(XM),d&&t(ELe),d&&t(Wo),_(zM),_(WM),_(QM),_(HM),_(UM),d&&t(yLe),d&&t(Qi),_(JM),d&&t(wLe),d&&t(Qo),_(YM),_(ZM),_(eE),_(oE),_(rE),d&&t(ALe),d&&t(Ji),_(tE),d&&t(LLe),d&&t(Ho),_(aE),_(sE),_(lE),_(iE),_(dE),d&&t(BLe),d&&t(Zi),_(cE),d&&t(kLe),d&&t(Uo),_(fE),_(gE),_(hE),_(pE),_(_E),d&&t(xLe),d&&t(rd),_(uE),d&&t(RLe),d&&t(Jo),_(bE),_(TE),_(FE),_(CE),_(ME),d&&t(SLe),d&&t(nd),_(EE),d&&t(PLe),d&&t(Yo),_(yE),_(AE),_(LE),_(BE),_(kE),d&&t($Le),d&&t(id),_(xE),d&&t(ILe),d&&t(Ko),_(RE),_(PE),_($E),_(IE),_(jE),d&&t(jLe),d&&t(fd),_(NE),d&&t(NLe),d&&t(Zo),_(DE),_(GE),_(OE),_(XE),_(zE),d&&t(DLe),d&&t(hd),_(VE),d&&t(qLe),d&&t(er),_(WE),_(HE),_(UE),_(JE),_(YE),d&&t(GLe),d&&t(ud),_(KE),d&&t(OLe),d&&t(or),_(ZE),_(o3),_(r3),_(t3),_(a3),d&&t(XLe),d&&t(Td),_(n3),d&&t(zLe),d&&t(rr),_(s3),_(i3),_(d3),_(c3),_(f3),d&&t(VLe),d&&t(Md),_(m3),d&&t(WLe),d&&t(tr),_(g3),_(p3),_(_3),_(u3),_(b3),d&&t(QLe),d&&t(wd),_(v3),d&&t(HLe),d&&t(ar),_(T3),_(C3),_(M3),_(E3),_(y3),d&&t(ULe),d&&t(Bd),_(w3),d&&t(JLe),d&&t(nr),_(A3),_(B3),_(k3),_(x3),_(R3),d&&t(YLe),d&&t(Sd),_(S3),d&&t(KLe),d&&t(sr),_(P3),_(I3),_(j3),_(N3),_(D3),d&&t(ZLe),d&&t(Id),_(q3),d&&t(e8e),d&&t(lr),_(G3),_(X3),_(z3),_(V3),_(Q3),d&&t(o8e),d&&t(Dd),_(H3),d&&t(r8e),d&&t(ir),_(U3),_(Y3),_(K3),_(Z3),_(ey),d&&t(t8e),d&&t(Xd),_(oy),d&&t(a8e),d&&t(dr),_(ry),_(ay),_(ny),_(sy),_(ly),d&&t(n8e),d&&t(Qd),_(iy),d&&t(s8e),d&&t(cr),_(dy),_(fy),_(my),_(gy),_(hy),d&&t(l8e),d&&t(Jd),_(py),d&&t(i8e),d&&t(fr),_(_y),_(by),_(vy),_(Ty),_(Fy),d&&t(d8e),d&&t(Zd),_(Cy),d&&t(c8e),d&&t(mr),_(My),_(yy),_(wy),_(Ay),_(By),d&&t(f8e),d&&t(rc),_(ky),d&&t(m8e),d&&t(gr),_(xy),_(Sy),_(Py),_($y),_(Iy),d&&t(g8e),d&&t(nc),_(jy),d&&t(h8e),d&&t(hr),_(Ny),_(qy),_(Gy),_(Oy),_(Xy),d&&t(p8e),d&&t(ic),_(zy),d&&t(_8e),d&&t(pr),_(Vy),_(Qy),_(Hy),_(Uy),_(Jy),d&&t(u8e),d&&t(fc),_(Yy),d&&t(b8e),d&&t(_r),_(Ky),_(ew),_(ow),_(rw),_(tw),d&&t(v8e),d&&t(hc),_(aw),d&&t(T8e),d&&t(ur),_(nw),_(lw),_(iw),_(dw),_(cw),d&&t(F8e),d&&t(uc),_(fw),d&&t(C8e),d&&t(br),_(mw),_(hw),_(pw),_(_w),_(uw),d&&t(M8e),d&&t(Tc),_(bw),d&&t(E8e),d&&t(vr),_(vw),_(Fw),_(Cw),_(Mw),_(Ew),d&&t(y8e),d&&t(Mc),_(yw),d&&t(w8e),d&&t(Tr),_(ww),_(Lw),_(Bw),_(kw),_(xw),d&&t(A8e),d&&t(wc),_(Rw),d&&t(L8e),d&&t(Fr),_(Sw),_($w),_(Iw),_(jw),_(Nw),d&&t(B8e),d&&t(Bc),_(Dw),d&&t(k8e),d&&t(Cr),_(qw),_(Ow),_(Xw),_(zw),_(Vw),d&&t(x8e),d&&t(Rc),_(Ww),d&&t(R8e),d&&t(Mr),_(Qw),_(Uw),_(Jw),_(Yw),_(Kw),d&&t(S8e),d&&t($c),_(Zw),d&&t(P8e),d&&t(Er),_(eA),_(rA),_(tA),_(aA),_(nA),d&&t($8e),d&&t(Nc),_(sA),d&&t(I8e),d&&t(yr),_(lA),_(dA),_(cA),_(fA),_(mA),d&&t(j8e),d&&t(Gc),_(gA),d&&t(N8e),d&&t(wr),_(hA),_(_A),_(uA),_(bA),_(vA),d&&t(D8e),d&&t(zc),_(TA),d&&t(q8e),d&&t(Ar),_(FA),_(MA),_(EA),_(yA),_(wA),d&&t(G8e),d&&t(Qc),_(AA),d&&t(O8e),d&&t(Lr),_(LA),_(kA),_(xA),_(RA),_(SA),d&&t(X8e),d&&t(Jc),_(PA),d&&t(z8e),d&&t(Br),_($A),_(jA),_(NA),_(DA),_(qA),d&&t(V8e),d&&t(Zc),_(GA),d&&t(W8e),d&&t(kr),_(OA),_(zA),_(VA),_(WA),_(QA),d&&t(Q8e),d&&t(rf),_(HA),d&&t(H8e),d&&t(xr),_(UA),_(YA),_(KA),_(ZA),_(e6),d&&t(U8e),d&&t(nf),_(o6),d&&t(J8e),d&&t(Rr),_(r6),_(a6),_(n6),_(s6),_(l6),d&&t(Y8e),d&&t(df),_(i6),d&&t(K8e),d&&t(Sr),_(d6),_(f6),_(m6),_(g6),_(h6),d&&t(Z8e),d&&t(mf),_(p6),d&&t(eBe),d&&t(Pr),_(_6),_(b6),_(v6),_(T6),_(F6),d&&t(oBe),d&&t(pf),_(C6),d&&t(rBe),d&&t($r),_(M6),_(y6),_(w6),_(A6),_(L6),d&&t(tBe),d&&t(bf),_(B6),d&&t(aBe),d&&t(Ir),_(k6),_(R6),_(S6),_(P6),_(I6),d&&t(nBe),d&&t(Ff),_(j6),d&&t(sBe),d&&t(jr),_(N6),_(q6),_(G6),_(O6),_(X6)}}}const q_t={local:"auto-classes",sections:[{local:"extending-the-auto-classes",title:"Extending the Auto Classes"},{local:"transformers.AutoConfig",title:"AutoConfig"},{local:"transformers.AutoTokenizer",title:"AutoTokenizer"},{local:"transformers.AutoFeatureExtractor",title:"AutoFeatureExtractor"},{local:"transformers.AutoProcessor",title:"AutoProcessor"},{local:"transformers.AutoModel",title:"AutoModel"},{local:"transformers.AutoModelForPreTraining",title:"AutoModelForPreTraining"},{local:"transformers.AutoModelForCausalLM",title:"AutoModelForCausalLM"},{local:"transformers.AutoModelForMaskedLM",title:"AutoModelForMaskedLM"},{local:"transformers.AutoModelForSeq2SeqLM",title:"AutoModelForSeq2SeqLM"},{local:"transformers.AutoModelForSequenceClassification",title:"AutoModelForSequenceClassification"},{local:"transformers.AutoModelForMultipleChoice",title:"AutoModelForMultipleChoice"},{local:"transformers.AutoModelForNextSentencePrediction",title:"AutoModelForNextSentencePrediction"},{local:"transformers.AutoModelForTokenClassification",title:"AutoModelForTokenClassification"},{local:"transformers.AutoModelForQuestionAnswering",title:"AutoModelForQuestionAnswering"},{local:"transformers.AutoModelForTableQuestionAnswering",title:"AutoModelForTableQuestionAnswering"},{local:"transformers.AutoModelForImageClassification",title:"AutoModelForImageClassification"},{local:"transformers.AutoModelForVision2Seq",title:"AutoModelForVision2Seq"},{local:"transformers.AutoModelForAudioClassification",title:"AutoModelForAudioClassification"},{local:"transformers.AutoModelForAudioFrameClassification",title:"AutoModelForAudioFrameClassification"},{local:"transformers.AutoModelForCTC",title:"AutoModelForCTC"},{local:"transformers.AutoModelForSpeechSeq2Seq",title:"AutoModelForSpeechSeq2Seq"},{local:"transformers.AutoModelForAudioXVector",title:"AutoModelForAudioXVector"},{local:"transformers.AutoModelForMaskedImageModeling",title:"AutoModelForMaskedImageModeling"},{local:"transformers.AutoModelForObjectDetection",title:"AutoModelForObjectDetection"},{local:"transformers.AutoModelForImageSegmentation",title:"AutoModelForImageSegmentation"},{local:"transformers.AutoModelForSemanticSegmentation",title:"AutoModelForSemanticSegmentation"},{local:"transformers.TFAutoModel",title:"TFAutoModel"},{local:"transformers.TFAutoModelForPreTraining",title:"TFAutoModelForPreTraining"},{local:"transformers.TFAutoModelForCausalLM",title:"TFAutoModelForCausalLM"},{local:"transformers.TFAutoModelForImageClassification",title:"TFAutoModelForImageClassification"},{local:"transformers.TFAutoModelForMaskedLM",title:"TFAutoModelForMaskedLM"},{local:"transformers.TFAutoModelForSeq2SeqLM",title:"TFAutoModelForSeq2SeqLM"},{local:"transformers.TFAutoModelForSequenceClassification",title:"TFAutoModelForSequenceClassification"},{local:"transformers.TFAutoModelForMultipleChoice",title:"TFAutoModelForMultipleChoice"},{local:"transformers.TFAutoModelForTableQuestionAnswering",title:"TFAutoModelForTableQuestionAnswering"},{local:"transformers.TFAutoModelForTokenClassification",title:"TFAutoModelForTokenClassification"},{local:"transformers.TFAutoModelForQuestionAnswering",title:"TFAutoModelForQuestionAnswering"},{local:"transformers.TFAutoModelForVision2Seq",title:"TFAutoModelForVision2Seq"},{local:"transformers.TFAutoModelForSpeechSeq2Seq",title:"TFAutoModelForSpeechSeq2Seq"},{local:"transformers.FlaxAutoModel",title:"FlaxAutoModel"},{local:"transformers.FlaxAutoModelForCausalLM",title:"FlaxAutoModelForCausalLM"},{local:"transformers.FlaxAutoModelForPreTraining",title:"FlaxAutoModelForPreTraining"},{local:"transformers.FlaxAutoModelForMaskedLM",title:"FlaxAutoModelForMaskedLM"},{local:"transformers.FlaxAutoModelForSeq2SeqLM",title:"FlaxAutoModelForSeq2SeqLM"},{local:"transformers.FlaxAutoModelForSequenceClassification",title:"FlaxAutoModelForSequenceClassification"},{local:"transformers.FlaxAutoModelForQuestionAnswering",title:"FlaxAutoModelForQuestionAnswering"},{local:"transformers.FlaxAutoModelForTokenClassification",title:"FlaxAutoModelForTokenClassification"},{local:"transformers.FlaxAutoModelForMultipleChoice",title:"FlaxAutoModelForMultipleChoice"},{local:"transformers.FlaxAutoModelForNextSentencePrediction",title:"FlaxAutoModelForNextSentencePrediction"},{local:"transformers.FlaxAutoModelForImageClassification",title:"FlaxAutoModelForImageClassification"},{local:"transformers.FlaxAutoModelForVision2Seq",title:"FlaxAutoModelForVision2Seq"}],title:"Auto Classes"};function G_t(wi,J,Ae){let{fw:ie}=J;return wi.$$set=me=>{"fw"in me&&Ae(0,ie=me.fw)},[ie]}class H_t extends R_t{constructor(J){super();S_t(this,J,G_t,D_t,P_t,{fw:0})}}export{H_t as default,q_t as metadata};
