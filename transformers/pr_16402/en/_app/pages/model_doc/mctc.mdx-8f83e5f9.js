import{S as ul,i as gl,s as _l,e as r,k as c,w as _,t as a,M as vl,c as n,d as o,m as p,a as s,x as v,h as i,b as l,F as e,g as u,y as C,q as T,o as b,B as w,v as Cl,L as ir}from"../../chunks/vendor-6b77c823.js";import{T as Kr}from"../../chunks/Tip-39098574.js";import{D as E}from"../../chunks/Docstring-1088f2fb.js";import{C as lr}from"../../chunks/CodeBlock-3a8b25a8.js";import{I as ue}from"../../chunks/IconCopyLink-7a11ce68.js";import{E as ar}from"../../chunks/ExampleCodeBlock-5212b321.js";function Tl(z){let d,$;return d=new lr({props:{code:`from transformers import MCTCModel, MCTCConfig

# Initializing a mCTC mctc-large style configuration
configuration = MCTCConfig()

# Initializing a model from the mctc-large style configuration
model = MCTCModel(configuration)

# Accessing the model configuration
configuration = model.config`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> MCTCModel, MCTCConfig

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a mCTC mctc-large style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = MCTCConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a model from the mctc-large style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = MCTCModel(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`}}),{c(){_(d.$$.fragment)},l(m){v(d.$$.fragment,m)},m(m,g){C(d,m,g),$=!0},p:ir,i(m){$||(T(d.$$.fragment,m),$=!0)},o(m){b(d.$$.fragment,m),$=!1},d(m){w(d,m)}}}function bl(z){let d,$,m,g,y,f,k,F;return{c(){d=r("p"),$=a(`This class method is simply calling the feature extractor
`),m=r("a"),g=a("from_pretrained()"),y=a(` and the tokenizer
`),f=r("code"),k=a("from_pretrained"),F=a(` methods. Please refer to the docstrings of the
methods above for more information.`),this.h()},l(j){d=n(j,"P",{});var x=s(d);$=i(x,`This class method is simply calling the feature extractor
`),m=n(x,"A",{href:!0});var A=s(m);g=i(A,"from_pretrained()"),A.forEach(o),y=i(x,` and the tokenizer
`),f=n(x,"CODE",{});var L=s(f);k=i(L,"from_pretrained"),L.forEach(o),F=i(x,` methods. Please refer to the docstrings of the
methods above for more information.`),x.forEach(o),this.h()},h(){l(m,"href","/docs/transformers/pr_16402/en/main_classes/feature_extractor#transformers.FeatureExtractionMixin.from_pretrained")},m(j,x){u(j,d,x),e(d,$),e(d,m),e(m,g),e(d,y),e(d,f),e(f,k),e(d,F)},d(j){j&&o(d)}}}function wl(z){let d,$,m,g,y,f,k,F;return{c(){d=r("p"),$=a("This class method is simply calling "),m=r("a"),g=a("save_pretrained()"),y=a(` and
`),f=r("code"),k=a("save_pretrained"),F=a(`. Please refer to the docstrings of the methods
above for more information.`),this.h()},l(j){d=n(j,"P",{});var x=s(d);$=i(x,"This class method is simply calling "),m=n(x,"A",{href:!0});var A=s(m);g=i(A,"save_pretrained()"),A.forEach(o),y=i(x,` and
`),f=n(x,"CODE",{});var L=s(f);k=i(L,"save_pretrained"),L.forEach(o),F=i(x,`. Please refer to the docstrings of the methods
above for more information.`),x.forEach(o),this.h()},h(){l(m,"href","/docs/transformers/pr_16402/en/main_classes/feature_extractor#transformers.FeatureExtractionMixin.save_pretrained")},m(j,x){u(j,d,x),e(d,$),e(d,m),e(m,g),e(d,y),e(d,f),e(f,k),e(d,F)},d(j){j&&o(d)}}}function $l(z){let d,$,m,g,y;return{c(){d=r("p"),$=a("Although the recipe for forward pass needs to be defined within this function, one should call the "),m=r("code"),g=a("Module"),y=a(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(f){d=n(f,"P",{});var k=s(d);$=i(k,"Although the recipe for forward pass needs to be defined within this function, one should call the "),m=n(k,"CODE",{});var F=s(m);g=i(F,"Module"),F.forEach(o),y=i(k,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),k.forEach(o)},m(f,k){u(f,d,k),e(d,$),e(d,m),e(m,g),e(d,y)},d(f){f&&o(d)}}}function kl(z){let d,$,m,g,y;return g=new lr({props:{code:`from transformers import MCTCTokenizer, MCTCModel
from datasets import load_dataset
import torch

dataset = load_dataset("hf-internal-testing/librispeech_asr_demo", "clean", split="validation")
dataset = dataset.sort("id")
sampling_rate = dataset.features["audio"].sampling_rate

processor = MCTCTokenizer.from_pretrained("mctc-large")
model = MCTCModel.from_pretrained("mctc-large")

# audio file is decoded on the fly
inputs = processor(dataset[0]["audio"]["array"], sampling_rate=sampling_rate, return_tensors="pt")
with torch.no_grad():
    logits = model(**inputs).logits
predicted_ids = torch.argmax(logits, dim=-1)

# transcribe speech
transcription = processor.batch_decode(predicted_ids)
transcription[0]
`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> MCTCTokenizer, MCTCModel
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;hf-internal-testing/librispeech_asr_demo&quot;</span>, <span class="hljs-string">&quot;clean&quot;</span>, split=<span class="hljs-string">&quot;validation&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = dataset.sort(<span class="hljs-string">&quot;id&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>sampling_rate = dataset.features[<span class="hljs-string">&quot;audio&quot;</span>].sampling_rate

<span class="hljs-meta">&gt;&gt;&gt; </span>processor = MCTCTokenizer.from_pretrained(<span class="hljs-string">&quot;mctc-large&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = MCTCModel.from_pretrained(<span class="hljs-string">&quot;mctc-large&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># audio file is decoded on the fly</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(dataset[<span class="hljs-number">0</span>][<span class="hljs-string">&quot;audio&quot;</span>][<span class="hljs-string">&quot;array&quot;</span>], sampling_rate=sampling_rate, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    logits = model(**inputs).logits
<span class="hljs-meta">&gt;&gt;&gt; </span>predicted_ids = torch.argmax(logits, dim=-<span class="hljs-number">1</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># transcribe speech</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>transcription = processor.batch_decode(predicted_ids)
<span class="hljs-meta">&gt;&gt;&gt; </span>transcription[<span class="hljs-number">0</span>]
`}}),{c(){d=r("p"),$=a("Example:"),m=c(),_(g.$$.fragment)},l(f){d=n(f,"P",{});var k=s(d);$=i(k,"Example:"),k.forEach(o),m=p(f),v(g.$$.fragment,f)},m(f,k){u(f,d,k),e(d,$),u(f,m,k),C(g,f,k),y=!0},p:ir,i(f){y||(T(g.$$.fragment,f),y=!0)},o(f){b(g.$$.fragment,f),y=!1},d(f){f&&o(d),f&&o(m),w(g,f)}}}function yl(z){let d,$;return d=new lr({props:{code:`with processor.as_target_processor():
    inputs["labels"] = processor(dataset[0]["text"], return_tensors="pt").input_ids

# compute loss
loss = model(**inputs).loss
round(loss.item(), 2)
`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> processor.as_target_processor():
<span class="hljs-meta">... </span>    inputs[<span class="hljs-string">&quot;labels&quot;</span>] = processor(dataset[<span class="hljs-number">0</span>][<span class="hljs-string">&quot;text&quot;</span>], return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).input_ids

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># compute loss</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = model(**inputs).loss
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">round</span>(loss.item(), <span class="hljs-number">2</span>)
`}}),{c(){_(d.$$.fragment)},l(m){v(d.$$.fragment,m)},m(m,g){C(d,m,g),$=!0},p:ir,i(m){$||(T(d.$$.fragment,m),$=!0)},o(m){b(d.$$.fragment,m),$=!1},d(m){w(d,m)}}}function Ml(z){let d,$,m,g,y;return{c(){d=r("p"),$=a("Although the recipe for forward pass needs to be defined within this function, one should call the "),m=r("code"),g=a("Module"),y=a(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(f){d=n(f,"P",{});var k=s(d);$=i(k,"Although the recipe for forward pass needs to be defined within this function, one should call the "),m=n(k,"CODE",{});var F=s(m);g=i(F,"Module"),F.forEach(o),y=i(k,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),k.forEach(o)},m(f,k){u(f,d,k),e(d,$),e(d,m),e(m,g),e(d,y)},d(f){f&&o(d)}}}function xl(z){let d,$,m,g,y;return g=new lr({props:{code:`from transformers import MCTCProcessor, MCTCForCTC
from datasets import load_dataset
import torch

dataset = load_dataset("hf-internal-testing/librispeech_asr_demo", "clean", split="validation")
dataset = dataset.sort("id")
sampling_rate = dataset.features["audio"].sampling_rate

processor = MCTCProcessor.from_pretrained("mctc-large")
model = MCTCForCTC.from_pretrained("mctc-large")

# audio file is decoded on the fly
inputs = processor(dataset[0]["audio"]["array"], sampling_rate=sampling_rate, return_tensors="pt")
with torch.no_grad():
    logits = model(**inputs).logits
predicted_ids = torch.argmax(logits, dim=-1)

# transcribe speech
transcription = processor.batch_decode(predicted_ids)
transcription[0]`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> MCTCProcessor, MCTCForCTC
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;hf-internal-testing/librispeech_asr_demo&quot;</span>, <span class="hljs-string">&quot;clean&quot;</span>, split=<span class="hljs-string">&quot;validation&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = dataset.sort(<span class="hljs-string">&quot;id&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>sampling_rate = dataset.features[<span class="hljs-string">&quot;audio&quot;</span>].sampling_rate

<span class="hljs-meta">&gt;&gt;&gt; </span>processor = MCTCProcessor.from_pretrained(<span class="hljs-string">&quot;mctc-large&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = MCTCForCTC.from_pretrained(<span class="hljs-string">&quot;mctc-large&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># audio file is decoded on the fly</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(dataset[<span class="hljs-number">0</span>][<span class="hljs-string">&quot;audio&quot;</span>][<span class="hljs-string">&quot;array&quot;</span>], sampling_rate=sampling_rate, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    logits = model(**inputs).logits
<span class="hljs-meta">&gt;&gt;&gt; </span>predicted_ids = torch.argmax(logits, dim=-<span class="hljs-number">1</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># transcribe speech</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>transcription = processor.batch_decode(predicted_ids)
<span class="hljs-meta">&gt;&gt;&gt; </span>transcription[<span class="hljs-number">0</span>]
<span class="hljs-string">&#x27;MISTER QUILTER IS THE APOSTLE OF THE MIDDLE CLASSES AND WE ARE GLAD TO WELCOME HIS GOSPEL&#x27;</span>`}}),{c(){d=r("p"),$=a("Example:"),m=c(),_(g.$$.fragment)},l(f){d=n(f,"P",{});var k=s(d);$=i(k,"Example:"),k.forEach(o),m=p(f),v(g.$$.fragment,f)},m(f,k){u(f,d,k),e(d,$),u(f,m,k),C(g,f,k),y=!0},p:ir,i(f){y||(T(g.$$.fragment,f),y=!0)},o(f){b(g.$$.fragment,f),y=!1},d(f){f&&o(d),f&&o(m),w(g,f)}}}function El(z){let d,$;return d=new lr({props:{code:`with processor.as_target_processor():
    inputs["labels"] = processor(dataset[0]["text"], return_tensors="pt").input_ids

# compute loss
loss = model(**inputs).loss
round(loss.item(), 2)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> processor.as_target_processor():
<span class="hljs-meta">... </span>    inputs[<span class="hljs-string">&quot;labels&quot;</span>] = processor(dataset[<span class="hljs-number">0</span>][<span class="hljs-string">&quot;text&quot;</span>], return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).input_ids

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># compute loss</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = model(**inputs).loss
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">round</span>(loss.item(), <span class="hljs-number">2</span>)
<span class="hljs-number">53.48</span>`}}),{c(){_(d.$$.fragment)},l(m){v(d.$$.fragment,m)},m(m,g){C(d,m,g),$=!0},p:ir,i(m){$||(T(d.$$.fragment,m),$=!0)},o(m){b(d.$$.fragment,m),$=!1},d(m){w(d,m)}}}function zl(z){let d,$,m,g,y,f,k,F,j,x,A,L,Co,We,Xr,To,Zr,dr,ge,en,Re,tn,on,cr,Ot,rn,pr,Dt,bo,nn,mr,U,sn,St,an,ln,Be,dn,cn,hr,Z,_e,wo,He,pn,$o,mn,fr,N,Ue,hn,ee,fn,It,un,gn,Ge,_n,vn,Cn,te,Tn,Nt,bn,wn,Vt,$n,kn,yn,ve,ur,oe,Ce,ko,Je,Mn,yo,xn,gr,P,Qe,En,Mo,zn,Fn,Ye,Pn,Wt,qn,jn,An,G,Ke,Ln,xo,On,Dn,Eo,Sn,In,Te,Xe,Nn,re,Vn,zo,Wn,Rn,Fo,Bn,Hn,Un,J,Ze,Gn,Rt,Jn,Bt,Qn,Yn,Po,Kn,Xn,Ht,et,_r,ne,be,qo,tt,Zn,jo,es,vr,V,ot,ts,Ao,os,rs,se,ns,Ut,ss,as,rt,is,ls,ds,we,nt,cs,Lo,ps,Cr,ae,$e,Oo,st,ms,Do,hs,Tr,M,at,fs,So,us,gs,O,Gt,_s,vs,Jt,Cs,Ts,Qt,bs,ws,it,Io,$s,ks,ys,Yt,Ms,xs,Es,ke,lt,zs,W,Fs,dt,No,Ps,qs,js,Kt,As,Ls,ct,Vo,Os,Ds,Ss,Is,Q,pt,Ns,Wo,Vs,Ws,ye,Rs,Y,mt,Bs,ht,Hs,Xt,Us,Gs,Js,Me,Qs,xe,ft,Ys,ut,Ks,Zt,Xs,Zs,ea,Ee,gt,ta,_t,oa,eo,ra,na,sa,ze,vt,aa,Ro,ia,br,ie,Fe,Bo,Ct,la,Ho,da,wr,le,Tt,ca,bt,pa,Uo,ma,ha,$r,de,wt,fa,$t,ua,to,ga,_a,kr,ce,Pe,Go,kt,va,Jo,Ca,yr,R,yt,Ta,Mt,ba,xt,wa,$a,ka,D,Et,ya,pe,Ma,oo,xa,Ea,Qo,za,Fa,Pa,qe,qa,je,ja,Ae,Mr,me,Le,Yo,zt,Aa,Ko,La,xr,B,Ft,Oa,he,Da,Xo,Sa,Ia,Pt,Na,Va,Wa,S,qt,Ra,fe,Ba,ro,Ha,Ua,Zo,Ga,Ja,Qa,Oe,Ya,De,Ka,Se,Er;return f=new ue({}),We=new ue({}),He=new ue({}),Ue=new E({props:{name:"class transformers.MCTCConfig",anchor:"transformers.MCTCConfig",parameters:[{name:"vocab_size",val:" = 8065"},{name:"hidden_size",val:" = 1536"},{name:"num_hidden_layers",val:" = 36"},{name:"intermediate_size",val:" = 6144"},{name:"num_attention_heads",val:" = 4"},{name:"attention_head_dim",val:" = 384"},{name:"position_embedding_type",val:" = 'relative_key'"},{name:"max_position_embeddings",val:" = 920"},{name:"layer_norm_eps",val:" = 1e-05"},{name:"layerdrop",val:" = 0.3"},{name:"hidden_act",val:" = 'relu'"},{name:"initializer_range",val:" = 0.02"},{name:"hidden_dropout_prob",val:" = 0.3"},{name:"attention_probs_dropout_prob",val:" = 0.3"},{name:"use_cache",val:" = True"},{name:"is_encoder_decoder",val:" = False"},{name:"pad_token_id",val:" = 1"},{name:"bos_token_id",val:" = 0"},{name:"eos_token_id",val:" = 2"},{name:"conv_glu_dim",val:" = 1"},{name:"conv_dropout",val:" = 0.3"},{name:"num_conv_layers",val:" = 1"},{name:"conv_kernel",val:" = [7]"},{name:"conv_stride",val:" = [3]"},{name:"input_feat_per_channel",val:" = 80"},{name:"input_channels",val:" = 1"},{name:"conv_channels",val:" = None"},{name:"ctc_loss_reduction",val:" = 'sum'"},{name:"ctc_zero_infinity",val:" = False"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.MCTCConfig.vocab_size",description:`<strong>vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 8065) &#x2014;
Vocabulary size of the mCTC model. Defines the number of different tokens that can be represented by the
<code>inputs_ids</code> passed when calling <a href="/docs/transformers/pr_16402/en/model_doc/mctc#transformers.MCTCModel">~MCTCModel</a> or <code>~TFMCTCModel</code>.`,name:"vocab_size"},{anchor:"transformers.MCTCConfig.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 768) &#x2014;
Dimension of the encoder layers and the pooler layer.`,name:"hidden_size"},{anchor:"transformers.MCTCConfig.num_hidden_layers",description:`<strong>num_hidden_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of hidden layers in the Transformer encoder.`,name:"num_hidden_layers"},{anchor:"transformers.MCTCConfig.num_attention_heads",description:`<strong>num_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of attention heads for each attention layer in the Transformer encoder.`,name:"num_attention_heads"},{anchor:"transformers.MCTCConfig.intermediate_size",description:`<strong>intermediate_size</strong> (<code>int</code>, <em>optional</em>, defaults to 3072) &#x2014;
Dimension of the &#x201C;intermediate&#x201D; (i.e., feed-forward) layer in the Transformer encoder.`,name:"intermediate_size"},{anchor:"transformers.MCTCConfig.hidden_act",description:`<strong>hidden_act</strong> (<code>str</code> or <code>function</code>, <em>optional</em>, defaults to <code>&quot;gelu&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the encoder and pooler. If string, <code>&quot;gelu&quot;</code>,
<code>&quot;relu&quot;</code>, <code>&quot;selu&quot;</code> and <code>&quot;gelu_new&quot;</code> are supported.`,name:"hidden_act"},{anchor:"transformers.MCTCConfig.hidden_dropout_prob",description:`<strong>hidden_dropout_prob</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout probabilitiy for all fully connected layers in the embeddings, encoder, and pooler.`,name:"hidden_dropout_prob"},{anchor:"transformers.MCTCConfig.attention_probs_dropout_prob",description:`<strong>attention_probs_dropout_prob</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout ratio for the attention probabilities.`,name:"attention_probs_dropout_prob"},{anchor:"transformers.MCTCConfig.max_position_embeddings",description:`<strong>max_position_embeddings</strong> (<code>int</code>, <em>optional</em>, defaults to 512) &#x2014;
The maximum sequence length that this model might ever be used with. Typically set this to something large
just in case (e.g., 512 or 1024 or 2048).`,name:"max_position_embeddings"},{anchor:"transformers.MCTCConfig.type_vocab_size",description:`<strong>type_vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 2) &#x2014;
The vocabulary size of the <code>token_type_ids</code> passed when calling <a href="/docs/transformers/pr_16402/en/model_doc/mctc#transformers.MCTCModel">~MCTCModel</a> or <code>~TFMCTCModel</code>.`,name:"type_vocab_size"},{anchor:"transformers.MCTCConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.MCTCConfig.layer_norm_eps",description:`<strong>layer_norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-12) &#x2014;
The epsilon used by the layer normalization layers.`,name:"layer_norm_eps"},{anchor:"transformers.MCTCConfig.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not the model should return the last key/values attentions (not used by all models). Only
relevant if <code>config.is_decoder=True</code>.
Example &#x2014;`,name:"use_cache"}],source:"https://github.com/huggingface/transformers/blob/vr_16402/src/transformers/models/mctc/configuration_mctc.py#L29"}}),ve=new ar({props:{anchor:"transformers.MCTCConfig.example",$$slots:{default:[Tl]},$$scope:{ctx:z}}}),Je=new ue({}),Qe=new E({props:{name:"class transformers.MCTCTokenizer",anchor:"transformers.MCTCTokenizer",parameters:[{name:"vocab_file",val:""},{name:"bos_token",val:" = '<s>'"},{name:"eos_token",val:" = '</s>'"},{name:"unk_token",val:" = '<unk>'"},{name:"pad_token",val:" = '<pad>'"},{name:"word_delimiter_token",val:" = '|'"},{name:"replace_word_delimiter_char",val:" = ' '"},{name:"do_lower_case",val:" = False"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.MCTCTokenizer.vocab_file",description:`<strong>vocab_file</strong> (<code>str</code>) &#x2014;
File containing the vocabulary.`,name:"vocab_file"},{anchor:"transformers.MCTCTokenizer.bos_token",description:`<strong>bos_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;s&gt;&quot;</code>) &#x2014;
The beginning of sentence token.`,name:"bos_token"},{anchor:"transformers.MCTCTokenizer.eos_token",description:`<strong>eos_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;/s&gt;&quot;</code>) &#x2014;
The end of sentence token.`,name:"eos_token"},{anchor:"transformers.MCTCTokenizer.unk_token",description:`<strong>unk_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;unk&gt;&quot;</code>) &#x2014;
The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this
token instead.`,name:"unk_token"},{anchor:"transformers.MCTCTokenizer.pad_token",description:`<strong>pad_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;pad&gt;&quot;</code>) &#x2014;
The token used for padding, for example when batching sequences of different lengths.`,name:"pad_token"},{anchor:"transformers.MCTCTokenizer.word_delimiter_token",description:`<strong>word_delimiter_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;|&quot;</code>) &#x2014;
The token used for defining the end of a word.`,name:"word_delimiter_token"},{anchor:"transformers.MCTCTokenizer.do_lower_case",description:`<strong>do_lower_case</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to accept lowercase input and lowercase the output when decoding.</p>
<p>**kwargs &#x2014;
Additional keyword arguments passed along to <a href="/docs/transformers/pr_16402/en/main_classes/tokenizer#transformers.PreTrainedTokenizer">PreTrainedTokenizer</a>`,name:"do_lower_case"}],source:"https://github.com/huggingface/transformers/blob/vr_16402/src/transformers/models/mctc/tokenization_mctc.py#L121"}}),Ke=new E({props:{name:"build_inputs_with_special_tokens",anchor:"transformers.MCTCTokenizer.build_inputs_with_special_tokens",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"}],parametersDescription:[{anchor:"transformers.MCTCTokenizer.build_inputs_with_special_tokens.token_ids_0",description:"<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014; The first tokenized sequence.",name:"token_ids_0"},{anchor:"transformers.MCTCTokenizer.build_inputs_with_special_tokens.token_ids_1",description:"<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014; The second tokenized sequence.",name:"token_ids_1"}],source:"https://github.com/huggingface/transformers/blob/vr_16402/src/transformers/tokenization_utils_base.py#L2880",returnDescription:`
<p>The model input with special tokens.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),Xe=new E({props:{name:"get_special_tokens_mask",anchor:"transformers.MCTCTokenizer.get_special_tokens_mask",parameters:[{name:"token_ids_0",val:": typing.List"},{name:"token_ids_1",val:": typing.Optional[typing.List] = None"},{name:"already_has_special_tokens",val:": bool = False"}],parametersDescription:[{anchor:"transformers.MCTCTokenizer.get_special_tokens_mask.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of ids of the first sequence.`,name:"token_ids_0"},{anchor:"transformers.MCTCTokenizer.get_special_tokens_mask.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
List of ids of the second sequence.`,name:"token_ids_1"},{anchor:"transformers.MCTCTokenizer.get_special_tokens_mask.already_has_special_tokens",description:`<strong>already_has_special_tokens</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not the token list is already formatted with special tokens for the model.`,name:"already_has_special_tokens"}],source:"https://github.com/huggingface/transformers/blob/vr_16402/src/transformers/tokenization_utils.py#L842",returnDescription:`
<p>1 for a special token, 0 for a sequence token.</p>
`,returnType:`
<p>A list of integers in the range [0, 1]</p>
`}}),Ze=new E({props:{name:"create_token_type_ids_from_sequences",anchor:"transformers.MCTCTokenizer.create_token_type_ids_from_sequences",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"}],parametersDescription:[{anchor:"transformers.MCTCTokenizer.create_token_type_ids_from_sequences.token_ids_0",description:"<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014; The first tokenized sequence.",name:"token_ids_0"},{anchor:"transformers.MCTCTokenizer.create_token_type_ids_from_sequences.token_ids_1",description:"<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014; The second tokenized sequence.",name:"token_ids_1"}],source:"https://github.com/huggingface/transformers/blob/vr_16402/src/transformers/tokenization_utils_base.py#L2860",returnDescription:`
<p>The token type ids.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),et=new E({props:{name:"save_vocabulary",anchor:"transformers.MCTCTokenizer.save_vocabulary",parameters:[{name:"save_directory",val:": str"},{name:"filename_prefix",val:": typing.Optional[str] = None"}],source:"https://github.com/huggingface/transformers/blob/vr_16402/src/transformers/models/mctc/tokenization_mctc.py#L587"}}),tt=new ue({}),ot=new E({props:{name:"class transformers.MCTCFeatureExtractor",anchor:"transformers.MCTCFeatureExtractor",parameters:[{name:"feature_size",val:" = 80"},{name:"sampling_rate",val:" = 16000"},{name:"padding_value",val:" = 0.0"},{name:"hop_length",val:" = 10"},{name:"win_length",val:" = 25"},{name:"win_function",val:" = 'hamming_window'"},{name:"frame_signal_scale",val:" = 32768.0"},{name:"preemphasis_coeff",val:" = 0.97"},{name:"mel_floor",val:" = 1.0"},{name:"normalize_means",val:" = True"},{name:"normalize_vars",val:" = True"},{name:"return_attention_mask",val:" = False"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.MCTCFeatureExtractor.feature_size",description:`<strong>feature_size</strong> (<code>int</code>, defaults to 80) &#x2014;
The feature dimension of the extracted features. This is the number of mel_frequency`,name:"feature_size"},{anchor:"transformers.MCTCFeatureExtractor.sampling_rate",description:`<strong>sampling_rate</strong> (<code>int</code>, defaults to 16000) &#x2014;
The sampling rate at which the audio files should be digitalized expressed in Hertz per second (Hz).`,name:"sampling_rate"},{anchor:"transformers.MCTCFeatureExtractor.padding_value",description:`<strong>padding_value</strong> (<code>float</code>, defaults to 0.0) &#x2014;
The value that is used to fill the padding values.`,name:"padding_value"},{anchor:"transformers.MCTCFeatureExtractor.hop_length",description:`<strong>hop_length</strong> (<code>int</code>, defaults to 10) &#x2014;
Number of audio samples between windows. Otherwise referred to as &#x201C;shift&#x201D; in many papers.`,name:"hop_length"},{anchor:"transformers.MCTCFeatureExtractor.win_length",description:`<strong>win_length</strong> (<code>int</code>, defaults to 25) &#x2014;
Number of ms per window`,name:"win_length"},{anchor:"transformers.MCTCFeatureExtractor.win_function",description:`<strong>win_function</strong> (<code>str</code>, defaults to <code>hamming_window</code>) &#x2014;
Name for the window function used for windowing, must be accessible via <code>torch.{win_function}</code>`,name:"win_function"},{anchor:"transformers.MCTCFeatureExtractor.frame_signal_scale",description:`<strong>frame_signal_scale</strong> (<code>float</code>, defaults to 32768.0) &#x2014;
Constant multiplied in creating the frames before applying DFT.`,name:"frame_signal_scale"},{anchor:"transformers.MCTCFeatureExtractor.preemphasis_coeff",description:`<strong>preemphasis_coeff</strong> (<code>float</code>, defaults to 0.97) &#x2014;
Constant multiplied in applying Pre-emphasis before DFT.`,name:"preemphasis_coeff"},{anchor:"transformers.MCTCFeatureExtractor.mel_floor",description:`<strong>mel_floor</strong> (<code>float</code> defaults to 1.0) &#x2014;
Minimum value of mel frequency banks.`,name:"mel_floor"},{anchor:"transformers.MCTCFeatureExtractor.normalize_means",description:`<strong>normalize_means</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to zero-mean normalize the extracted features.`,name:"normalize_means"},{anchor:"transformers.MCTCFeatureExtractor.normalize_vars",description:`<strong>normalize_vars</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to unit-variance normalize the extracted features.`,name:"normalize_vars"}],source:"https://github.com/huggingface/transformers/blob/vr_16402/src/transformers/models/mctc/feature_extraction_mctc.py#L34"}}),nt=new E({props:{name:"__call__",anchor:"transformers.MCTCFeatureExtractor.__call__",parameters:[{name:"raw_speech",val:": typing.Union[numpy.ndarray, typing.List[float], typing.List[numpy.ndarray], typing.List[typing.List[float]]]"},{name:"padding",val:": typing.Union[bool, str, transformers.utils.generic.PaddingStrategy] = False"},{name:"max_length",val:": typing.Optional[int] = None"},{name:"truncation",val:": bool = False"},{name:"pad_to_multiple_of",val:": typing.Optional[int] = None"},{name:"return_attention_mask",val:": typing.Optional[bool] = None"},{name:"return_tensors",val:": typing.Union[str, transformers.utils.generic.TensorType, NoneType] = None"},{name:"sampling_rate",val:": typing.Optional[int] = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.MCTCFeatureExtractor.__call__.raw_speech",description:`<strong>raw_speech</strong> (<code>torch.Tensor</code>, <code>np.ndarray</code>, <code>List[float]</code>, <code>List[torch.Tensor]</code>, <code>List[np.ndarray]</code>, <code>List[List[float]]</code>) &#x2014;
The sequence or batch of sequences to be padded. Each sequence can be a tensor, a numpy array, a list
of float values, a list of tensors, a list of numpy arrays or a list of list of float values.`,name:"raw_speech"},{anchor:"transformers.MCTCFeatureExtractor.__call__.padding",description:`<strong>padding</strong> (<code>bool</code>, <code>str</code> or <a href="/docs/transformers/pr_16402/en/internal/file_utils#transformers.utils.PaddingStrategy">PaddingStrategy</a>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Select a strategy to pad the returned sequences (according to the model&#x2019;s padding side and padding
index) among:</p>
<ul>
<li><code>True</code> or <code>&apos;longest&apos;</code>: Pad to the longest sequence in the batch (or no padding if only a single
sequence if provided).</li>
<li><code>&apos;max_length&apos;</code>: Pad to a maximum length specified with the argument <code>max_length</code> or to the maximum
acceptable input length for the model if that argument is not provided.</li>
<li><code>False</code> or <code>&apos;do_not_pad&apos;</code> (default): No padding (i.e., can output a batch with sequences of different
lengths).</li>
</ul>`,name:"padding"},{anchor:"transformers.MCTCFeatureExtractor.__call__.max_length",description:`<strong>max_length</strong> (<code>int</code>, <em>optional</em>) &#x2014;
Maximum length of the returned list and optionally padding length (see above).`,name:"max_length"},{anchor:"transformers.MCTCFeatureExtractor.__call__.truncation",description:`<strong>truncation</strong> (<code>bool</code>) &#x2014;
Activates truncation to cut input sequences longer than <em>max_length</em> to <em>max_length</em>.`,name:"truncation"},{anchor:"transformers.MCTCFeatureExtractor.__call__.pad_to_multiple_of",description:`<strong>pad_to_multiple_of</strong> (<code>int</code>, <em>optional</em>) &#x2014;
If set will pad the sequence to a multiple of the provided value.</p>
<p>This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability</p>
<blockquote>
<p>= 7.5 (Volta), or on TPUs which benefit from having sequence lengths be a multiple of 128.</p>
</blockquote>`,name:"pad_to_multiple_of"},{anchor:"transformers.MCTCFeatureExtractor.__call__.return_attention_mask",description:`<strong>return_attention_mask</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether to return the attention mask. If left to the default, will return the attention mask according
to the specific feature_extractor&#x2019;s default.</p>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"return_attention_mask"},{anchor:"transformers.MCTCFeatureExtractor.__call__.return_tensors",description:`<strong>return_tensors</strong> (<code>str</code> or <a href="/docs/transformers/pr_16402/en/internal/file_utils#transformers.TensorType">TensorType</a>, <em>optional</em>) &#x2014;
If set, will return tensors instead of list of python integers. Acceptable values are:</p>
<ul>
<li><code>&apos;tf&apos;</code>: Return TensorFlow <code>tf.constant</code> objects.</li>
<li><code>&apos;pt&apos;</code>: Return PyTorch <code>torch.Tensor</code> objects.</li>
<li><code>&apos;np&apos;</code>: Return Numpy <code>np.ndarray</code> objects.</li>
</ul>`,name:"return_tensors"},{anchor:"transformers.MCTCFeatureExtractor.__call__.sampling_rate",description:`<strong>sampling_rate</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The sampling rate at which the <code>raw_speech</code> input was sampled. It is strongly recommended to pass
<code>sampling_rate</code> at the forward call to prevent silent errors.`,name:"sampling_rate"},{anchor:"transformers.MCTCFeatureExtractor.__call__.padding_value",description:"<strong>padding_value</strong> (<code>float</code>, defaults to 0.0) &#x2014;",name:"padding_value"}],source:"https://github.com/huggingface/transformers/blob/vr_16402/src/transformers/models/mctc/feature_extraction_mctc.py#L233"}}),st=new ue({}),at=new E({props:{name:"class transformers.MCTCProcessor",anchor:"transformers.MCTCProcessor",parameters:[{name:"feature_extractor",val:""},{name:"tokenizer",val:""}],parametersDescription:[{anchor:"transformers.MCTCProcessor.feature_extractor",description:`<strong>feature_extractor</strong> (<code>MCTCFeatureExtractor</code>) &#x2014;
An instance of <a href="/docs/transformers/pr_16402/en/model_doc/mctc#transformers.MCTCFeatureExtractor">MCTCFeatureExtractor</a>. The feature extractor is a required input.`,name:"feature_extractor"},{anchor:"transformers.MCTCProcessor.tokenizer",description:`<strong>tokenizer</strong> (<code>MCTCTokenizer</code>) &#x2014;
An instance of <a href="/docs/transformers/pr_16402/en/model_doc/mctc#transformers.MCTCTokenizer">MCTCTokenizer</a>. The tokenizer is a required input.`,name:"tokenizer"}],source:"https://github.com/huggingface/transformers/blob/vr_16402/src/transformers/models/mctc/processing_mctc.py#L23"}}),lt=new E({props:{name:"__call__",anchor:"transformers.MCTCProcessor.__call__",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/vr_16402/src/transformers/models/mctc/processing_mctc.py#L43"}}),pt=new E({props:{name:"from_pretrained",anchor:"transformers.MCTCProcessor.from_pretrained",parameters:[{name:"pretrained_model_name_or_path",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.MCTCProcessor.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
This can be either:</p>
<ul>
<li>a string, the <em>model id</em> of a pretrained feature_extractor hosted inside a model repo on
huggingface.co. Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or
namespaced under a user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>a path to a <em>directory</em> containing a feature extractor file saved using the
<a href="/docs/transformers/pr_16402/en/main_classes/feature_extractor#transformers.FeatureExtractionMixin.save_pretrained">save_pretrained()</a> method, e.g., <code>./my_model_directory/</code>.</li>
<li>a path or url to a saved feature extractor JSON <em>file</em>, e.g.,
<code>./my_model_directory/preprocessor_config.json</code>.
**kwargs &#x2014;
Additional keyword arguments passed along to both
<a href="/docs/transformers/pr_16402/en/main_classes/feature_extractor#transformers.FeatureExtractionMixin.from_pretrained">from_pretrained()</a> and
<code>from_pretrained</code>.</li>
</ul>`,name:"pretrained_model_name_or_path"}],source:"https://github.com/huggingface/transformers/blob/vr_16402/src/transformers/processing_utils.py#L156"}}),ye=new Kr({props:{$$slots:{default:[bl]},$$scope:{ctx:z}}}),mt=new E({props:{name:"save_pretrained",anchor:"transformers.MCTCProcessor.save_pretrained",parameters:[{name:"save_directory",val:""},{name:"push_to_hub",val:": bool = False"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.MCTCProcessor.save_pretrained.save_directory",description:`<strong>save_directory</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Directory where the feature extractor JSON file and the tokenizer files will be saved (directory will
be created if it does not exist).`,name:"save_directory"},{anchor:"transformers.MCTCProcessor.save_pretrained.push_to_hub",description:`<strong>push_to_hub</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to push your processor to the Hugging Face model hub after saving it.</p>
<div class="course-tip course-tip-orange bg-gradient-to-br dark:bg-gradient-to-r before:border-orange-500 dark:before:border-orange-800 from-orange-50 dark:from-gray-900 to-white dark:to-gray-950 border border-orange-50 text-orange-700 dark:text-gray-400">
						
<p>Using <code>push_to_hub=True</code> will synchronize the repository you are pushing to with <code>save_directory</code>,
which requires <code>save_directory</code> to be a local clone of the repo you are pushing to if it&#x2019;s an existing
folder. Pass along <code>temp_dir=True</code> to use a temporary directory instead.</p>

					</div>
<p>kwargs &#x2014;
Additional key word arguments passed along to the <a href="/docs/transformers/pr_16402/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.push_to_hub">push_to_hub()</a> method.`,name:"push_to_hub"}],source:"https://github.com/huggingface/transformers/blob/vr_16402/src/transformers/processing_utils.py#L94"}}),Me=new Kr({props:{$$slots:{default:[wl]},$$scope:{ctx:z}}}),ft=new E({props:{name:"batch_decode",anchor:"transformers.MCTCProcessor.batch_decode",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/vr_16402/src/transformers/models/mctc/processing_mctc.py#L52"}}),gt=new E({props:{name:"decode",anchor:"transformers.MCTCProcessor.decode",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/vr_16402/src/transformers/models/mctc/processing_mctc.py#L59"}}),vt=new E({props:{name:"as_target_processor",anchor:"transformers.MCTCProcessor.as_target_processor",parameters:[],source:"https://github.com/huggingface/transformers/blob/vr_16402/src/transformers/models/mctc/processing_mctc.py#L66"}}),Ct=new ue({}),Tt=new E({props:{name:"class transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2BaseModelOutput",anchor:"transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2BaseModelOutput",parameters:[{name:"last_hidden_state",val:": FloatTensor = None"},{name:"extract_features",val:": FloatTensor = None"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}],parametersDescription:[{anchor:"transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2BaseModelOutput.last_hidden_state",description:`<strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) &#x2014;
Sequence of hidden-states at the output of the last layer of the model.`,name:"last_hidden_state"},{anchor:"transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2BaseModelOutput.extract_features",description:`<strong>extract_features</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, conv_dim[-1])</code>) &#x2014;
Sequence of extracted feature vectors of the last convolutional layer of the model.`,name:"extract_features"},{anchor:"transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2BaseModelOutput.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2BaseModelOutput.attentions",description:`<strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"}],source:"https://github.com/huggingface/transformers/blob/vr_16402/src/transformers/models/wav2vec2/modeling_wav2vec2.py#L92"}}),wt=new E({props:{name:"class transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2ForPreTrainingOutput",anchor:"transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2ForPreTrainingOutput",parameters:[{name:"loss",val:": typing.Optional[torch.FloatTensor] = None"},{name:"projected_states",val:": FloatTensor = None"},{name:"projected_quantized_states",val:": FloatTensor = None"},{name:"codevector_perplexity",val:": FloatTensor = None"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"contrastive_loss",val:": typing.Optional[torch.FloatTensor] = None"},{name:"diversity_loss",val:": typing.Optional[torch.FloatTensor] = None"}],parametersDescription:[{anchor:"transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2ForPreTrainingOutput.loss",description:`<strong>loss</strong> (<em>optional</em>, returned when <code>sample_negative_indices</code> are passed, <code>torch.FloatTensor</code> of shape <code>(1,)</code>) &#x2014;
Total loss as the sum of the contrastive loss (L_m) and the diversity loss (L_d) as stated in the <a href="https://arxiv.org/pdf/2006.11477.pdf" rel="nofollow">official
paper</a> . (classification) loss.`,name:"loss"},{anchor:"transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2ForPreTrainingOutput.projected_states",description:`<strong>projected_states</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, config.proj_codevector_dim)</code>) &#x2014;
Hidden-states of the model projected to <em>config.proj_codevector_dim</em> that can be used to predict the masked
projected quantized states.`,name:"projected_states"},{anchor:"transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2ForPreTrainingOutput.projected_quantized_states",description:`<strong>projected_quantized_states</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, config.proj_codevector_dim)</code>) &#x2014;
Quantized extracted feature vectors projected to <em>config.proj_codevector_dim</em> representing the positive
target vectors for contrastive loss.`,name:"projected_quantized_states"},{anchor:"transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2ForPreTrainingOutput.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2ForPreTrainingOutput.attentions",description:`<strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"},{anchor:"transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2ForPreTrainingOutput.contrastive_loss",description:`<strong>contrastive_loss</strong> (<em>optional</em>, returned when <code>sample_negative_indices</code> are passed, <code>torch.FloatTensor</code> of shape <code>(1,)</code>) &#x2014;
The contrastive loss (L_m) as stated in the <a href="https://arxiv.org/pdf/2006.11477.pdf" rel="nofollow">official paper</a> .`,name:"contrastive_loss"},{anchor:"transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2ForPreTrainingOutput.diversity_loss",description:`<strong>diversity_loss</strong> (<em>optional</em>, returned when <code>sample_negative_indices</code> are passed, <code>torch.FloatTensor</code> of shape <code>(1,)</code>) &#x2014;
The diversity loss (L_d) as stated in the <a href="https://arxiv.org/pdf/2006.11477.pdf" rel="nofollow">official paper</a> .`,name:"diversity_loss"}],source:"https://github.com/huggingface/transformers/blob/vr_16402/src/transformers/models/wav2vec2/modeling_wav2vec2.py#L121"}}),kt=new ue({}),yt=new E({props:{name:"class transformers.MCTCModel",anchor:"transformers.MCTCModel",parameters:[{name:"config",val:""}],parametersDescription:[{anchor:"transformers.MCTCModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_16402/en/model_doc/mctc#transformers.MCTCConfig">~MCTCConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_16402/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/vr_16402/src/transformers/models/mctc/modeling_mctc.py#L717"}}),Et=new E({props:{name:"forward",anchor:"transformers.MCTCModel.forward",parameters:[{name:"input_features",val:""},{name:"attention_mask",val:" = None"},{name:"head_mask",val:" = None"},{name:"use_cache",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"}],parametersDescription:[{anchor:"transformers.MCTCModel.forward.input_features",description:`<strong>input_features</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/pr_16402/en/model_doc/mctc#transformers.MCTCTokenizer">MCTCTokenizer</a>. See <a href="/docs/transformers/pr_16402/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/pr_16402/en/model_doc/wav2vec2_phoneme#transformers.Wav2Vec2PhonemeCTCTokenizer.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_features"},{anchor:"transformers.MCTCModel.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.MCTCModel.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.MCTCModel.forward.position_ids",description:`<strong>position_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.max_position_embeddings - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>`,name:"position_ids"},{anchor:"transformers.MCTCModel.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.MCTCModel.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_features</code> you can choose to directly pass an embedded representation.
This is useful if you want more control over how to convert <em>input_features</em> indices into associated
vectors than the model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.MCTCModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.MCTCModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.MCTCModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_16402/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/vr_16402/src/transformers/models/mctc/modeling_mctc.py#L727",returnDescription:`
<p>A <a
  href="/docs/transformers/pr_16402/en/main_classes/output#transformers.modeling_outputs.BaseModelOutput"
>transformers.modeling_outputs.BaseModelOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/pr_16402/en/model_doc/mctc#transformers.MCTCConfig"
>MCTCConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) \u2014 Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/pr_16402/en/main_classes/output#transformers.modeling_outputs.BaseModelOutput"
>transformers.modeling_outputs.BaseModelOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),qe=new Kr({props:{$$slots:{default:[$l]},$$scope:{ctx:z}}}),je=new ar({props:{anchor:"transformers.MCTCModel.forward.example",$$slots:{default:[kl]},$$scope:{ctx:z}}}),Ae=new ar({props:{anchor:"transformers.MCTCModel.forward.example-2",$$slots:{default:[yl]},$$scope:{ctx:z}}}),zt=new ue({}),Ft=new E({props:{name:"class transformers.MCTCForCTC",anchor:"transformers.MCTCForCTC",parameters:[{name:"config",val:""}],parametersDescription:[{anchor:"transformers.MCTCForCTC.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_16402/en/model_doc/mctc#transformers.MCTCConfig">~MCTCConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_16402/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/vr_16402/src/transformers/models/mctc/modeling_mctc.py#L785"}}),qt=new E({props:{name:"forward",anchor:"transformers.MCTCForCTC.forward",parameters:[{name:"input_features",val:""},{name:"attention_mask",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"},{name:"labels",val:" = None"}],parametersDescription:[{anchor:"transformers.MCTCForCTC.forward.input_features",description:`<strong>input_features</strong> (<code>torch.LongTensor</code> of shape <code>({0})</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/pr_16402/en/model_doc/mctc#transformers.MCTCTokenizer">MCTCTokenizer</a>. See <a href="/docs/transformers/pr_16402/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/pr_16402/en/model_doc/wav2vec2_phoneme#transformers.Wav2Vec2PhonemeCTCTokenizer.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_features"},{anchor:"transformers.MCTCForCTC.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>({0})</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.MCTCForCTC.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>({0})</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.MCTCForCTC.forward.position_ids",description:`<strong>position_ids</strong> (<code>torch.LongTensor</code> of shape <code>({0})</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.max_position_embeddings - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>`,name:"position_ids"},{anchor:"transformers.MCTCForCTC.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.MCTCForCTC.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>({0}, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_features</code> you can choose to directly pass an embedded representation.
This is useful if you want more control over how to convert <em>input_features</em> indices into associated
vectors than the model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.MCTCForCTC.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.MCTCForCTC.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.MCTCForCTC.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_16402/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.MCTCForCTC.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, target_length)</code>, <em>optional</em>) &#x2014;
Labels for connectionist temporal classification. Note that <code>target_length</code> has to be smaller or equal to
the sequence length of the output logits. Indices are selected in <code>[-100, 0, ..., config.vocab_size - 1]</code>.
All labels set to <code>-100</code> are ignored (masked), the loss is only computed for labels in <code>[0, ..., config.vocab_size - 1]</code>.`,name:"labels"}],source:"https://github.com/huggingface/transformers/blob/vr_16402/src/transformers/models/mctc/modeling_mctc.py#L805",returnDescription:`
<p>A <a
  href="/docs/transformers/pr_16402/en/main_classes/output#transformers.modeling_outputs.CausalLMOutput"
>transformers.modeling_outputs.CausalLMOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/pr_16402/en/model_doc/mctc#transformers.MCTCConfig"
>MCTCConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) \u2014 Language modeling loss (for next-token prediction).</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, config.vocab_size)</code>) \u2014 Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/pr_16402/en/main_classes/output#transformers.modeling_outputs.CausalLMOutput"
>transformers.modeling_outputs.CausalLMOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Oe=new Kr({props:{$$slots:{default:[Ml]},$$scope:{ctx:z}}}),De=new ar({props:{anchor:"transformers.MCTCForCTC.forward.example",$$slots:{default:[xl]},$$scope:{ctx:z}}}),Se=new ar({props:{anchor:"transformers.MCTCForCTC.forward.example-2",$$slots:{default:[El]},$$scope:{ctx:z}}}),{c(){d=r("meta"),$=c(),m=r("h1"),g=r("a"),y=r("span"),_(f.$$.fragment),k=c(),F=r("span"),j=a("MCTC"),x=c(),A=r("h2"),L=r("a"),Co=r("span"),_(We.$$.fragment),Xr=c(),To=r("span"),Zr=a("Overview"),dr=c(),ge=r("p"),en=a("The MCTC model was proposed in "),Re=r("a"),tn=a("Pseudo-Labeling For Massively Multilingual Speech Recognition"),on=a(` by Loren Lugosch, Tatiana Likhomanenko, Gabriel Synnaeve, and Ronan Collobert.
<INSERT SHORT SUMMARY HERE>`),cr=c(),Ot=r("p"),rn=a("The abstract from the paper is the following:"),pr=c(),Dt=r("p"),bo=r("em"),nn=a(`Semi-supervised learning through pseudo-labeling has become a staple of state-of-the-art monolingual
speech recognition systems. In this work, we extend pseudo-labeling to massively multilingual speech
recognition with 60 languages. We propose a simple pseudo-labeling recipe that works well even
with low-resource languages: train a supervised multilingual model, fine-tune it with semi-supervised
learning on a target language, generate pseudo-labels for that language, and train a final model using
pseudo-labels for all languages, either from scratch or by fine-tuning. Experiments on the labeled
Common Voice and unlabeled VoxPopuli datasets show that our recipe can yield a model with better
performance for many languages that also transfers well to LibriSpeech.`),mr=c(),U=r("p"),sn=a("This model was contributed by "),St=r("a"),an=a("cwkeam"),ln=a(". The original code can be found "),Be=r("a"),dn=a("here"),cn=a("."),hr=c(),Z=r("h2"),_e=r("a"),wo=r("span"),_(He.$$.fragment),pn=c(),$o=r("span"),mn=a("MCTCConfig"),fr=c(),N=r("div"),_(Ue.$$.fragment),hn=c(),ee=r("p"),fn=a("This is the configuration class to store the configuration of a "),It=r("a"),un=a("~MCTCModel"),gn=a(`. It is used to instantiate an mCTC
model according to the specified arguments, defining the model architecture. Instantiating a configuration with the
defaults will yield a similar configuration to that of the mCTC `),Ge=r("a"),_n=a("mctc-large"),vn=a(`
architecture.`),Cn=c(),te=r("p"),Tn=a("Configuration objects inherit from "),Nt=r("a"),bn=a("PretrainedConfig"),wn=a(` and can be used to control the model outputs. Read the
documentation from `),Vt=r("a"),$n=a("PretrainedConfig"),kn=a(" for more information."),yn=c(),_(ve.$$.fragment),ur=c(),oe=r("h2"),Ce=r("a"),ko=r("span"),_(Je.$$.fragment),Mn=c(),yo=r("span"),xn=a("MCTCTokenizer"),gr=c(),P=r("div"),_(Qe.$$.fragment),En=c(),Mo=r("p"),zn=a("Constructs a MCTC tokenizer."),Fn=c(),Ye=r("p"),Pn=a("This tokenizer inherits from "),Wt=r("a"),qn=a("PreTrainedTokenizer"),jn=a(` which contains some of the main methods. Users should refer to
the superclass for more information regarding such methods.`),An=c(),G=r("div"),_(Ke.$$.fragment),Ln=c(),xo=r("p"),On=a(`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens.`),Dn=c(),Eo=r("p"),Sn=a("This implementation does not add special tokens and this method should be overridden in a subclass."),In=c(),Te=r("div"),_(Xe.$$.fragment),Nn=c(),re=r("p"),Vn=a(`Retrieves sequence ids from a token list that has no special tokens added. This method is called when adding
special tokens using the tokenizer `),zo=r("code"),Wn=a("prepare_for_model"),Rn=a(" or "),Fo=r("code"),Bn=a("encode_plus"),Hn=a(" methods."),Un=c(),J=r("div"),_(Ze.$$.fragment),Gn=c(),Rt=r("p"),Jn=a("Create the token type IDs corresponding to the sequences passed. "),Bt=r("a"),Qn=a(`What are token type
IDs?`),Yn=c(),Po=r("p"),Kn=a("Should be overridden in a subclass if the model has a special way of building those."),Xn=c(),Ht=r("div"),_(et.$$.fragment),_r=c(),ne=r("h2"),be=r("a"),qo=r("span"),_(tt.$$.fragment),Zn=c(),jo=r("span"),es=a("MCTCFeatureExtractor"),vr=c(),V=r("div"),_(ot.$$.fragment),ts=c(),Ao=r("p"),os=a("Constructs a mCTC feature extractor."),rs=c(),se=r("p"),ns=a("This feature extractor inherits from "),Ut=r("a"),ss=a("SequenceFeatureExtractor"),as=a(` which contains
most of the main methods. Users should refer to this superclass for more information regarding those methods. This
code has been adapted from Flashlight\u2019s C++ code. For more information about the implementation, one can refer to
this `),rt=r("a"),is=a("notebook"),ls=a(`
that takes the user step-by-step in the implementation.`),ds=c(),we=r("div"),_(nt.$$.fragment),cs=c(),Lo=r("p"),ps=a("Main method to featurize and prepare for the model one or several sequence(s). sequences."),Cr=c(),ae=r("h2"),$e=r("a"),Oo=r("span"),_(st.$$.fragment),ms=c(),Do=r("span"),hs=a("MCTCProcessor"),Tr=c(),M=r("div"),_(at.$$.fragment),fs=c(),So=r("p"),us=a("Constructs a MCTC processor which wraps a MCTC feature extractor and a MCTC tokenizer into a single processor."),gs=c(),O=r("p"),Gt=r("a"),_s=a("MCTCProcessor"),vs=a(" offers all the functionalities of "),Jt=r("a"),Cs=a("MCTCFeatureExtractor"),Ts=a(" and "),Qt=r("a"),bs=a("MCTCTokenizer"),ws=a(`. See the
`),it=r("a"),Io=r("strong"),$s=a("call"),ks=a("()"),ys=a(" and "),Yt=r("a"),Ms=a("decode()"),xs=a(" for more information."),Es=c(),ke=r("div"),_(lt.$$.fragment),zs=c(),W=r("p"),Fs=a(`When used in normal mode, this method forwards all its arguments to MCTCFeatureExtractor\u2019s
`),dt=r("a"),No=r("strong"),Ps=a("call"),qs=a("()"),js=a(` and returns its output. If used in the context
`),Kt=r("a"),As=a("as_target_processor()"),Ls=a(` this method forwards all its arguments to MCTCTokenizer\u2019s
`),ct=r("a"),Vo=r("strong"),Os=a("call"),Ds=a("()"),Ss=a(". Please refer to the doctsring of the above two methods for more information."),Is=c(),Q=r("div"),_(pt.$$.fragment),Ns=c(),Wo=r("p"),Vs=a("Instantiate a processor associated with a pretrained model."),Ws=c(),_(ye.$$.fragment),Rs=c(),Y=r("div"),_(mt.$$.fragment),Bs=c(),ht=r("p"),Hs=a(`Saves the attributes of this processor (feature extractor, tokenizer\u2026) in the specified directory so that it
can be reloaded using the `),Xt=r("a"),Us=a("from_pretrained()"),Gs=a(" method."),Js=c(),_(Me.$$.fragment),Qs=c(),xe=r("div"),_(ft.$$.fragment),Ys=c(),ut=r("p"),Ks=a("This method forwards all its arguments to MCTCTokenizer\u2019s "),Zt=r("a"),Xs=a("batch_decode()"),Zs=a(`. Please refer
to the docstring of this method for more information.`),ea=c(),Ee=r("div"),_(gt.$$.fragment),ta=c(),_t=r("p"),oa=a("This method forwards all its arguments to MCTCTokenizer\u2019s "),eo=r("a"),ra=a("decode()"),na=a(`. Please refer to the
docstring of this method for more information.`),sa=c(),ze=r("div"),_(vt.$$.fragment),aa=c(),Ro=r("p"),ia=a("Temporarily sets the tokenizer for processing the input. Useful for encoding the labels when fine-tuning MCTC."),br=c(),ie=r("h2"),Fe=r("a"),Bo=r("span"),_(Ct.$$.fragment),la=c(),Ho=r("span"),da=a("Wav2Vec2 specific outputs"),wr=c(),le=r("div"),_(Tt.$$.fragment),ca=c(),bt=r("p"),pa=a("Output type of "),Uo=r("code"),ma=a("Wav2Vec2BaseModelOutput"),ha=a(", with potential hidden states and attentions."),$r=c(),de=r("div"),_(wt.$$.fragment),fa=c(),$t=r("p"),ua=a("Output type of "),to=r("a"),ga=a("Wav2Vec2ForPreTraining"),_a=a(", with potential hidden states and attentions."),kr=c(),ce=r("h2"),Pe=r("a"),Go=r("span"),_(kt.$$.fragment),va=c(),Jo=r("span"),Ca=a("MCTCModel"),yr=c(),R=r("div"),_(yt.$$.fragment),Ta=c(),Mt=r("p"),ba=a(`The bare mCTC Model transformer outputting raw hidden-states without any specific head on top.
This model is a PyTorch `),xt=r("a"),wa=a("torch.nn.Module"),$a=a(` sub-class. Use
it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),ka=c(),D=r("div"),_(Et.$$.fragment),ya=c(),pe=r("p"),Ma=a("The "),oo=r("a"),xa=a("MCTCModel"),Ea=a(" forward method, overrides the "),Qo=r("code"),za=a("__call__"),Fa=a(" special method."),Pa=c(),_(qe.$$.fragment),qa=c(),_(je.$$.fragment),ja=c(),_(Ae.$$.fragment),Mr=c(),me=r("h2"),Le=r("a"),Yo=r("span"),_(zt.$$.fragment),Aa=c(),Ko=r("span"),La=a("MCTCForCTC"),xr=c(),B=r("div"),_(Ft.$$.fragment),Oa=c(),he=r("p"),Da=a("MCTC Model with a "),Xo=r("code"),Sa=a("language modeling"),Ia=a(` head on top for Connectionist Temporal Classification (CTC).
This model is a PyTorch `),Pt=r("a"),Na=a("torch.nn.Module"),Va=a(` sub-class. Use
it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),Wa=c(),S=r("div"),_(qt.$$.fragment),Ra=c(),fe=r("p"),Ba=a("The "),ro=r("a"),Ha=a("MCTCForCTC"),Ua=a(" forward method, overrides the "),Zo=r("code"),Ga=a("__call__"),Ja=a(" special method."),Qa=c(),_(Oe.$$.fragment),Ya=c(),_(De.$$.fragment),Ka=c(),_(Se.$$.fragment),this.h()},l(t){const h=vl('[data-svelte="svelte-1phssyn"]',document.head);d=n(h,"META",{name:!0,content:!0}),h.forEach(o),$=p(t),m=n(t,"H1",{class:!0});var jt=s(m);g=n(jt,"A",{id:!0,class:!0,href:!0});var er=s(g);y=n(er,"SPAN",{});var tr=s(y);v(f.$$.fragment,tr),tr.forEach(o),er.forEach(o),k=p(jt),F=n(jt,"SPAN",{});var or=s(F);j=i(or,"MCTC"),or.forEach(o),jt.forEach(o),x=p(t),A=n(t,"H2",{class:!0});var At=s(A);L=n(At,"A",{id:!0,class:!0,href:!0});var rr=s(L);Co=n(rr,"SPAN",{});var nr=s(Co);v(We.$$.fragment,nr),nr.forEach(o),rr.forEach(o),Xr=p(At),To=n(At,"SPAN",{});var sr=s(To);Zr=i(sr,"Overview"),sr.forEach(o),At.forEach(o),dr=p(t),ge=n(t,"P",{});var Lt=s(ge);en=i(Lt,"The MCTC model was proposed in "),Re=n(Lt,"A",{href:!0,rel:!0});var oi=s(Re);tn=i(oi,"Pseudo-Labeling For Massively Multilingual Speech Recognition"),oi.forEach(o),on=i(Lt,` by Loren Lugosch, Tatiana Likhomanenko, Gabriel Synnaeve, and Ronan Collobert.
<INSERT SHORT SUMMARY HERE>`),Lt.forEach(o),cr=p(t),Ot=n(t,"P",{});var ri=s(Ot);rn=i(ri,"The abstract from the paper is the following:"),ri.forEach(o),pr=p(t),Dt=n(t,"P",{});var ni=s(Dt);bo=n(ni,"EM",{});var si=s(bo);nn=i(si,`Semi-supervised learning through pseudo-labeling has become a staple of state-of-the-art monolingual
speech recognition systems. In this work, we extend pseudo-labeling to massively multilingual speech
recognition with 60 languages. We propose a simple pseudo-labeling recipe that works well even
with low-resource languages: train a supervised multilingual model, fine-tune it with semi-supervised
learning on a target language, generate pseudo-labels for that language, and train a final model using
pseudo-labels for all languages, either from scratch or by fine-tuning. Experiments on the labeled
Common Voice and unlabeled VoxPopuli datasets show that our recipe can yield a model with better
performance for many languages that also transfers well to LibriSpeech.`),si.forEach(o),ni.forEach(o),mr=p(t),U=n(t,"P",{});var no=s(U);sn=i(no,"This model was contributed by "),St=n(no,"A",{href:!0});var ai=s(St);an=i(ai,"cwkeam"),ai.forEach(o),ln=i(no,". The original code can be found "),Be=n(no,"A",{href:!0,rel:!0});var ii=s(Be);dn=i(ii,"here"),ii.forEach(o),cn=i(no,"."),no.forEach(o),hr=p(t),Z=n(t,"H2",{class:!0});var zr=s(Z);_e=n(zr,"A",{id:!0,class:!0,href:!0});var li=s(_e);wo=n(li,"SPAN",{});var di=s(wo);v(He.$$.fragment,di),di.forEach(o),li.forEach(o),pn=p(zr),$o=n(zr,"SPAN",{});var ci=s($o);mn=i(ci,"MCTCConfig"),ci.forEach(o),zr.forEach(o),fr=p(t),N=n(t,"DIV",{class:!0});var Ie=s(N);v(Ue.$$.fragment,Ie),hn=p(Ie),ee=n(Ie,"P",{});var so=s(ee);fn=i(so,"This is the configuration class to store the configuration of a "),It=n(so,"A",{href:!0});var pi=s(It);un=i(pi,"~MCTCModel"),pi.forEach(o),gn=i(so,`. It is used to instantiate an mCTC
model according to the specified arguments, defining the model architecture. Instantiating a configuration with the
defaults will yield a similar configuration to that of the mCTC `),Ge=n(so,"A",{href:!0,rel:!0});var mi=s(Ge);_n=i(mi,"mctc-large"),mi.forEach(o),vn=i(so,`
architecture.`),so.forEach(o),Cn=p(Ie),te=n(Ie,"P",{});var ao=s(te);Tn=i(ao,"Configuration objects inherit from "),Nt=n(ao,"A",{href:!0});var hi=s(Nt);bn=i(hi,"PretrainedConfig"),hi.forEach(o),wn=i(ao,` and can be used to control the model outputs. Read the
documentation from `),Vt=n(ao,"A",{href:!0});var fi=s(Vt);$n=i(fi,"PretrainedConfig"),fi.forEach(o),kn=i(ao," for more information."),ao.forEach(o),yn=p(Ie),v(ve.$$.fragment,Ie),Ie.forEach(o),ur=p(t),oe=n(t,"H2",{class:!0});var Fr=s(oe);Ce=n(Fr,"A",{id:!0,class:!0,href:!0});var ui=s(Ce);ko=n(ui,"SPAN",{});var gi=s(ko);v(Je.$$.fragment,gi),gi.forEach(o),ui.forEach(o),Mn=p(Fr),yo=n(Fr,"SPAN",{});var _i=s(yo);xn=i(_i,"MCTCTokenizer"),_i.forEach(o),Fr.forEach(o),gr=p(t),P=n(t,"DIV",{class:!0});var I=s(P);v(Qe.$$.fragment,I),En=p(I),Mo=n(I,"P",{});var vi=s(Mo);zn=i(vi,"Constructs a MCTC tokenizer."),vi.forEach(o),Fn=p(I),Ye=n(I,"P",{});var Pr=s(Ye);Pn=i(Pr,"This tokenizer inherits from "),Wt=n(Pr,"A",{href:!0});var Ci=s(Wt);qn=i(Ci,"PreTrainedTokenizer"),Ci.forEach(o),jn=i(Pr,` which contains some of the main methods. Users should refer to
the superclass for more information regarding such methods.`),Pr.forEach(o),An=p(I),G=n(I,"DIV",{class:!0});var io=s(G);v(Ke.$$.fragment,io),Ln=p(io),xo=n(io,"P",{});var Ti=s(xo);On=i(Ti,`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens.`),Ti.forEach(o),Dn=p(io),Eo=n(io,"P",{});var bi=s(Eo);Sn=i(bi,"This implementation does not add special tokens and this method should be overridden in a subclass."),bi.forEach(o),io.forEach(o),In=p(I),Te=n(I,"DIV",{class:!0});var qr=s(Te);v(Xe.$$.fragment,qr),Nn=p(qr),re=n(qr,"P",{});var lo=s(re);Vn=i(lo,`Retrieves sequence ids from a token list that has no special tokens added. This method is called when adding
special tokens using the tokenizer `),zo=n(lo,"CODE",{});var wi=s(zo);Wn=i(wi,"prepare_for_model"),wi.forEach(o),Rn=i(lo," or "),Fo=n(lo,"CODE",{});var $i=s(Fo);Bn=i($i,"encode_plus"),$i.forEach(o),Hn=i(lo," methods."),lo.forEach(o),qr.forEach(o),Un=p(I),J=n(I,"DIV",{class:!0});var co=s(J);v(Ze.$$.fragment,co),Gn=p(co),Rt=n(co,"P",{});var Xa=s(Rt);Jn=i(Xa,"Create the token type IDs corresponding to the sequences passed. "),Bt=n(Xa,"A",{href:!0});var ki=s(Bt);Qn=i(ki,`What are token type
IDs?`),ki.forEach(o),Xa.forEach(o),Yn=p(co),Po=n(co,"P",{});var yi=s(Po);Kn=i(yi,"Should be overridden in a subclass if the model has a special way of building those."),yi.forEach(o),co.forEach(o),Xn=p(I),Ht=n(I,"DIV",{class:!0});var Mi=s(Ht);v(et.$$.fragment,Mi),Mi.forEach(o),I.forEach(o),_r=p(t),ne=n(t,"H2",{class:!0});var jr=s(ne);be=n(jr,"A",{id:!0,class:!0,href:!0});var xi=s(be);qo=n(xi,"SPAN",{});var Ei=s(qo);v(tt.$$.fragment,Ei),Ei.forEach(o),xi.forEach(o),Zn=p(jr),jo=n(jr,"SPAN",{});var zi=s(jo);es=i(zi,"MCTCFeatureExtractor"),zi.forEach(o),jr.forEach(o),vr=p(t),V=n(t,"DIV",{class:!0});var Ne=s(V);v(ot.$$.fragment,Ne),ts=p(Ne),Ao=n(Ne,"P",{});var Fi=s(Ao);os=i(Fi,"Constructs a mCTC feature extractor."),Fi.forEach(o),rs=p(Ne),se=n(Ne,"P",{});var po=s(se);ns=i(po,"This feature extractor inherits from "),Ut=n(po,"A",{href:!0});var Pi=s(Ut);ss=i(Pi,"SequenceFeatureExtractor"),Pi.forEach(o),as=i(po,` which contains
most of the main methods. Users should refer to this superclass for more information regarding those methods. This
code has been adapted from Flashlight\u2019s C++ code. For more information about the implementation, one can refer to
this `),rt=n(po,"A",{href:!0,rel:!0});var qi=s(rt);is=i(qi,"notebook"),qi.forEach(o),ls=i(po,`
that takes the user step-by-step in the implementation.`),po.forEach(o),ds=p(Ne),we=n(Ne,"DIV",{class:!0});var Ar=s(we);v(nt.$$.fragment,Ar),cs=p(Ar),Lo=n(Ar,"P",{});var ji=s(Lo);ps=i(ji,"Main method to featurize and prepare for the model one or several sequence(s). sequences."),ji.forEach(o),Ar.forEach(o),Ne.forEach(o),Cr=p(t),ae=n(t,"H2",{class:!0});var Lr=s(ae);$e=n(Lr,"A",{id:!0,class:!0,href:!0});var Ai=s($e);Oo=n(Ai,"SPAN",{});var Li=s(Oo);v(st.$$.fragment,Li),Li.forEach(o),Ai.forEach(o),ms=p(Lr),Do=n(Lr,"SPAN",{});var Oi=s(Do);hs=i(Oi,"MCTCProcessor"),Oi.forEach(o),Lr.forEach(o),Tr=p(t),M=n(t,"DIV",{class:!0});var q=s(M);v(at.$$.fragment,q),fs=p(q),So=n(q,"P",{});var Di=s(So);us=i(Di,"Constructs a MCTC processor which wraps a MCTC feature extractor and a MCTC tokenizer into a single processor."),Di.forEach(o),gs=p(q),O=n(q,"P",{});var H=s(O);Gt=n(H,"A",{href:!0});var Si=s(Gt);_s=i(Si,"MCTCProcessor"),Si.forEach(o),vs=i(H," offers all the functionalities of "),Jt=n(H,"A",{href:!0});var Ii=s(Jt);Cs=i(Ii,"MCTCFeatureExtractor"),Ii.forEach(o),Ts=i(H," and "),Qt=n(H,"A",{href:!0});var Ni=s(Qt);bs=i(Ni,"MCTCTokenizer"),Ni.forEach(o),ws=i(H,`. See the
`),it=n(H,"A",{href:!0});var Za=s(it);Io=n(Za,"STRONG",{});var Vi=s(Io);$s=i(Vi,"call"),Vi.forEach(o),ks=i(Za,"()"),Za.forEach(o),ys=i(H," and "),Yt=n(H,"A",{href:!0});var Wi=s(Yt);Ms=i(Wi,"decode()"),Wi.forEach(o),xs=i(H," for more information."),H.forEach(o),Es=p(q),ke=n(q,"DIV",{class:!0});var Or=s(ke);v(lt.$$.fragment,Or),zs=p(Or),W=n(Or,"P",{});var Ve=s(W);Fs=i(Ve,`When used in normal mode, this method forwards all its arguments to MCTCFeatureExtractor\u2019s
`),dt=n(Ve,"A",{href:!0});var ei=s(dt);No=n(ei,"STRONG",{});var Ri=s(No);Ps=i(Ri,"call"),Ri.forEach(o),qs=i(ei,"()"),ei.forEach(o),js=i(Ve,` and returns its output. If used in the context
`),Kt=n(Ve,"A",{href:!0});var Bi=s(Kt);As=i(Bi,"as_target_processor()"),Bi.forEach(o),Ls=i(Ve,` this method forwards all its arguments to MCTCTokenizer\u2019s
`),ct=n(Ve,"A",{href:!0});var ti=s(ct);Vo=n(ti,"STRONG",{});var Hi=s(Vo);Os=i(Hi,"call"),Hi.forEach(o),Ds=i(ti,"()"),ti.forEach(o),Ss=i(Ve,". Please refer to the doctsring of the above two methods for more information."),Ve.forEach(o),Or.forEach(o),Is=p(q),Q=n(q,"DIV",{class:!0});var mo=s(Q);v(pt.$$.fragment,mo),Ns=p(mo),Wo=n(mo,"P",{});var Ui=s(Wo);Vs=i(Ui,"Instantiate a processor associated with a pretrained model."),Ui.forEach(o),Ws=p(mo),v(ye.$$.fragment,mo),mo.forEach(o),Rs=p(q),Y=n(q,"DIV",{class:!0});var ho=s(Y);v(mt.$$.fragment,ho),Bs=p(ho),ht=n(ho,"P",{});var Dr=s(ht);Hs=i(Dr,`Saves the attributes of this processor (feature extractor, tokenizer\u2026) in the specified directory so that it
can be reloaded using the `),Xt=n(Dr,"A",{href:!0});var Gi=s(Xt);Us=i(Gi,"from_pretrained()"),Gi.forEach(o),Gs=i(Dr," method."),Dr.forEach(o),Js=p(ho),v(Me.$$.fragment,ho),ho.forEach(o),Qs=p(q),xe=n(q,"DIV",{class:!0});var Sr=s(xe);v(ft.$$.fragment,Sr),Ys=p(Sr),ut=n(Sr,"P",{});var Ir=s(ut);Ks=i(Ir,"This method forwards all its arguments to MCTCTokenizer\u2019s "),Zt=n(Ir,"A",{href:!0});var Ji=s(Zt);Xs=i(Ji,"batch_decode()"),Ji.forEach(o),Zs=i(Ir,`. Please refer
to the docstring of this method for more information.`),Ir.forEach(o),Sr.forEach(o),ea=p(q),Ee=n(q,"DIV",{class:!0});var Nr=s(Ee);v(gt.$$.fragment,Nr),ta=p(Nr),_t=n(Nr,"P",{});var Vr=s(_t);oa=i(Vr,"This method forwards all its arguments to MCTCTokenizer\u2019s "),eo=n(Vr,"A",{href:!0});var Qi=s(eo);ra=i(Qi,"decode()"),Qi.forEach(o),na=i(Vr,`. Please refer to the
docstring of this method for more information.`),Vr.forEach(o),Nr.forEach(o),sa=p(q),ze=n(q,"DIV",{class:!0});var Wr=s(ze);v(vt.$$.fragment,Wr),aa=p(Wr),Ro=n(Wr,"P",{});var Yi=s(Ro);ia=i(Yi,"Temporarily sets the tokenizer for processing the input. Useful for encoding the labels when fine-tuning MCTC."),Yi.forEach(o),Wr.forEach(o),q.forEach(o),br=p(t),ie=n(t,"H2",{class:!0});var Rr=s(ie);Fe=n(Rr,"A",{id:!0,class:!0,href:!0});var Ki=s(Fe);Bo=n(Ki,"SPAN",{});var Xi=s(Bo);v(Ct.$$.fragment,Xi),Xi.forEach(o),Ki.forEach(o),la=p(Rr),Ho=n(Rr,"SPAN",{});var Zi=s(Ho);da=i(Zi,"Wav2Vec2 specific outputs"),Zi.forEach(o),Rr.forEach(o),wr=p(t),le=n(t,"DIV",{class:!0});var Br=s(le);v(Tt.$$.fragment,Br),ca=p(Br),bt=n(Br,"P",{});var Hr=s(bt);pa=i(Hr,"Output type of "),Uo=n(Hr,"CODE",{});var el=s(Uo);ma=i(el,"Wav2Vec2BaseModelOutput"),el.forEach(o),ha=i(Hr,", with potential hidden states and attentions."),Hr.forEach(o),Br.forEach(o),$r=p(t),de=n(t,"DIV",{class:!0});var Ur=s(de);v(wt.$$.fragment,Ur),fa=p(Ur),$t=n(Ur,"P",{});var Gr=s($t);ua=i(Gr,"Output type of "),to=n(Gr,"A",{href:!0});var tl=s(to);ga=i(tl,"Wav2Vec2ForPreTraining"),tl.forEach(o),_a=i(Gr,", with potential hidden states and attentions."),Gr.forEach(o),Ur.forEach(o),kr=p(t),ce=n(t,"H2",{class:!0});var Jr=s(ce);Pe=n(Jr,"A",{id:!0,class:!0,href:!0});var ol=s(Pe);Go=n(ol,"SPAN",{});var rl=s(Go);v(kt.$$.fragment,rl),rl.forEach(o),ol.forEach(o),va=p(Jr),Jo=n(Jr,"SPAN",{});var nl=s(Jo);Ca=i(nl,"MCTCModel"),nl.forEach(o),Jr.forEach(o),yr=p(t),R=n(t,"DIV",{class:!0});var fo=s(R);v(yt.$$.fragment,fo),Ta=p(fo),Mt=n(fo,"P",{});var Qr=s(Mt);ba=i(Qr,`The bare mCTC Model transformer outputting raw hidden-states without any specific head on top.
This model is a PyTorch `),xt=n(Qr,"A",{href:!0,rel:!0});var sl=s(xt);wa=i(sl,"torch.nn.Module"),sl.forEach(o),$a=i(Qr,` sub-class. Use
it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),Qr.forEach(o),ka=p(fo),D=n(fo,"DIV",{class:!0});var K=s(D);v(Et.$$.fragment,K),ya=p(K),pe=n(K,"P",{});var uo=s(pe);Ma=i(uo,"The "),oo=n(uo,"A",{href:!0});var al=s(oo);xa=i(al,"MCTCModel"),al.forEach(o),Ea=i(uo," forward method, overrides the "),Qo=n(uo,"CODE",{});var il=s(Qo);za=i(il,"__call__"),il.forEach(o),Fa=i(uo," special method."),uo.forEach(o),Pa=p(K),v(qe.$$.fragment,K),qa=p(K),v(je.$$.fragment,K),ja=p(K),v(Ae.$$.fragment,K),K.forEach(o),fo.forEach(o),Mr=p(t),me=n(t,"H2",{class:!0});var Yr=s(me);Le=n(Yr,"A",{id:!0,class:!0,href:!0});var ll=s(Le);Yo=n(ll,"SPAN",{});var dl=s(Yo);v(zt.$$.fragment,dl),dl.forEach(o),ll.forEach(o),Aa=p(Yr),Ko=n(Yr,"SPAN",{});var cl=s(Ko);La=i(cl,"MCTCForCTC"),cl.forEach(o),Yr.forEach(o),xr=p(t),B=n(t,"DIV",{class:!0});var go=s(B);v(Ft.$$.fragment,go),Oa=p(go),he=n(go,"P",{});var _o=s(he);Da=i(_o,"MCTC Model with a "),Xo=n(_o,"CODE",{});var pl=s(Xo);Sa=i(pl,"language modeling"),pl.forEach(o),Ia=i(_o,` head on top for Connectionist Temporal Classification (CTC).
This model is a PyTorch `),Pt=n(_o,"A",{href:!0,rel:!0});var ml=s(Pt);Na=i(ml,"torch.nn.Module"),ml.forEach(o),Va=i(_o,` sub-class. Use
it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),_o.forEach(o),Wa=p(go),S=n(go,"DIV",{class:!0});var X=s(S);v(qt.$$.fragment,X),Ra=p(X),fe=n(X,"P",{});var vo=s(fe);Ba=i(vo,"The "),ro=n(vo,"A",{href:!0});var hl=s(ro);Ha=i(hl,"MCTCForCTC"),hl.forEach(o),Ua=i(vo," forward method, overrides the "),Zo=n(vo,"CODE",{});var fl=s(Zo);Ga=i(fl,"__call__"),fl.forEach(o),Ja=i(vo," special method."),vo.forEach(o),Qa=p(X),v(Oe.$$.fragment,X),Ya=p(X),v(De.$$.fragment,X),Ka=p(X),v(Se.$$.fragment,X),X.forEach(o),go.forEach(o),this.h()},h(){l(d,"name","hf:doc:metadata"),l(d,"content",JSON.stringify(Fl)),l(g,"id","mctc"),l(g,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(g,"href","#mctc"),l(m,"class","relative group"),l(L,"id","overview"),l(L,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(L,"href","#overview"),l(A,"class","relative group"),l(Re,"href","https://arxiv.org/abs/2111.00161"),l(Re,"rel","nofollow"),l(St,"href","<https://huggingface.co/cwkeam"),l(Be,"href","https://github.com/flashlight/wav2letter/tree/main/recipes/mling_pl"),l(Be,"rel","nofollow"),l(_e,"id","transformers.MCTCConfig"),l(_e,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(_e,"href","#transformers.MCTCConfig"),l(Z,"class","relative group"),l(It,"href","/docs/transformers/pr_16402/en/model_doc/mctc#transformers.MCTCModel"),l(Ge,"href","https://huggingface.co/mctc-large"),l(Ge,"rel","nofollow"),l(Nt,"href","/docs/transformers/pr_16402/en/main_classes/configuration#transformers.PretrainedConfig"),l(Vt,"href","/docs/transformers/pr_16402/en/main_classes/configuration#transformers.PretrainedConfig"),l(N,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(Ce,"id","transformers.MCTCTokenizer"),l(Ce,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(Ce,"href","#transformers.MCTCTokenizer"),l(oe,"class","relative group"),l(Wt,"href","/docs/transformers/pr_16402/en/main_classes/tokenizer#transformers.PreTrainedTokenizer"),l(G,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(Te,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(Bt,"href","../glossary#token-type-ids"),l(J,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(Ht,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(P,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(be,"id","transformers.MCTCFeatureExtractor"),l(be,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(be,"href","#transformers.MCTCFeatureExtractor"),l(ne,"class","relative group"),l(Ut,"href","/docs/transformers/pr_16402/en/main_classes/feature_extractor#transformers.SequenceFeatureExtractor"),l(rt,"href","https://colab.research.google.com/drive/1GLtINkkhzms-IsdcGy_-tVCkv0qNF-Gt#scrollTo=pMCRGMmUC_an"),l(rt,"rel","nofollow"),l(we,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(V,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l($e,"id","transformers.MCTCProcessor"),l($e,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l($e,"href","#transformers.MCTCProcessor"),l(ae,"class","relative group"),l(Gt,"href","/docs/transformers/pr_16402/en/model_doc/mctc#transformers.MCTCProcessor"),l(Jt,"href","/docs/transformers/pr_16402/en/model_doc/mctc#transformers.MCTCFeatureExtractor"),l(Qt,"href","/docs/transformers/pr_16402/en/model_doc/mctc#transformers.MCTCTokenizer"),l(it,"href","/docs/transformers/pr_16402/en/model_doc/mctc#transformers.MCTCProcessor.__call__"),l(Yt,"href","/docs/transformers/pr_16402/en/model_doc/mctc#transformers.MCTCProcessor.decode"),l(dt,"href","/docs/transformers/pr_16402/en/model_doc/mctc#transformers.MCTCFeatureExtractor.__call__"),l(Kt,"href","/docs/transformers/pr_16402/en/model_doc/mctc#transformers.MCTCProcessor.as_target_processor"),l(ct,"href","/docs/transformers/pr_16402/en/model_doc/wav2vec2_phoneme#transformers.Wav2Vec2PhonemeCTCTokenizer.__call__"),l(ke,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(Q,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(Xt,"href","/docs/transformers/pr_16402/en/model_doc/mctc#transformers.MCTCProcessor.from_pretrained"),l(Y,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(Zt,"href","/docs/transformers/pr_16402/en/model_doc/speech_to_text_2#transformers.Speech2Text2Tokenizer.batch_decode"),l(xe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(eo,"href","/docs/transformers/pr_16402/en/model_doc/speech_to_text_2#transformers.Speech2Text2Tokenizer.decode"),l(Ee,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(ze,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(M,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(Fe,"id","transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2BaseModelOutput"),l(Fe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(Fe,"href","#transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2BaseModelOutput"),l(ie,"class","relative group"),l(le,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(to,"href","/docs/transformers/pr_16402/en/model_doc/wav2vec2#transformers.Wav2Vec2ForPreTraining"),l(de,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(Pe,"id","transformers.MCTCModel"),l(Pe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(Pe,"href","#transformers.MCTCModel"),l(ce,"class","relative group"),l(xt,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),l(xt,"rel","nofollow"),l(oo,"href","/docs/transformers/pr_16402/en/model_doc/mctc#transformers.MCTCModel"),l(D,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(R,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(Le,"id","transformers.MCTCForCTC"),l(Le,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(Le,"href","#transformers.MCTCForCTC"),l(me,"class","relative group"),l(Pt,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),l(Pt,"rel","nofollow"),l(ro,"href","/docs/transformers/pr_16402/en/model_doc/mctc#transformers.MCTCForCTC"),l(S,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(B,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(t,h){e(document.head,d),u(t,$,h),u(t,m,h),e(m,g),e(g,y),C(f,y,null),e(m,k),e(m,F),e(F,j),u(t,x,h),u(t,A,h),e(A,L),e(L,Co),C(We,Co,null),e(A,Xr),e(A,To),e(To,Zr),u(t,dr,h),u(t,ge,h),e(ge,en),e(ge,Re),e(Re,tn),e(ge,on),u(t,cr,h),u(t,Ot,h),e(Ot,rn),u(t,pr,h),u(t,Dt,h),e(Dt,bo),e(bo,nn),u(t,mr,h),u(t,U,h),e(U,sn),e(U,St),e(St,an),e(U,ln),e(U,Be),e(Be,dn),e(U,cn),u(t,hr,h),u(t,Z,h),e(Z,_e),e(_e,wo),C(He,wo,null),e(Z,pn),e(Z,$o),e($o,mn),u(t,fr,h),u(t,N,h),C(Ue,N,null),e(N,hn),e(N,ee),e(ee,fn),e(ee,It),e(It,un),e(ee,gn),e(ee,Ge),e(Ge,_n),e(ee,vn),e(N,Cn),e(N,te),e(te,Tn),e(te,Nt),e(Nt,bn),e(te,wn),e(te,Vt),e(Vt,$n),e(te,kn),e(N,yn),C(ve,N,null),u(t,ur,h),u(t,oe,h),e(oe,Ce),e(Ce,ko),C(Je,ko,null),e(oe,Mn),e(oe,yo),e(yo,xn),u(t,gr,h),u(t,P,h),C(Qe,P,null),e(P,En),e(P,Mo),e(Mo,zn),e(P,Fn),e(P,Ye),e(Ye,Pn),e(Ye,Wt),e(Wt,qn),e(Ye,jn),e(P,An),e(P,G),C(Ke,G,null),e(G,Ln),e(G,xo),e(xo,On),e(G,Dn),e(G,Eo),e(Eo,Sn),e(P,In),e(P,Te),C(Xe,Te,null),e(Te,Nn),e(Te,re),e(re,Vn),e(re,zo),e(zo,Wn),e(re,Rn),e(re,Fo),e(Fo,Bn),e(re,Hn),e(P,Un),e(P,J),C(Ze,J,null),e(J,Gn),e(J,Rt),e(Rt,Jn),e(Rt,Bt),e(Bt,Qn),e(J,Yn),e(J,Po),e(Po,Kn),e(P,Xn),e(P,Ht),C(et,Ht,null),u(t,_r,h),u(t,ne,h),e(ne,be),e(be,qo),C(tt,qo,null),e(ne,Zn),e(ne,jo),e(jo,es),u(t,vr,h),u(t,V,h),C(ot,V,null),e(V,ts),e(V,Ao),e(Ao,os),e(V,rs),e(V,se),e(se,ns),e(se,Ut),e(Ut,ss),e(se,as),e(se,rt),e(rt,is),e(se,ls),e(V,ds),e(V,we),C(nt,we,null),e(we,cs),e(we,Lo),e(Lo,ps),u(t,Cr,h),u(t,ae,h),e(ae,$e),e($e,Oo),C(st,Oo,null),e(ae,ms),e(ae,Do),e(Do,hs),u(t,Tr,h),u(t,M,h),C(at,M,null),e(M,fs),e(M,So),e(So,us),e(M,gs),e(M,O),e(O,Gt),e(Gt,_s),e(O,vs),e(O,Jt),e(Jt,Cs),e(O,Ts),e(O,Qt),e(Qt,bs),e(O,ws),e(O,it),e(it,Io),e(Io,$s),e(it,ks),e(O,ys),e(O,Yt),e(Yt,Ms),e(O,xs),e(M,Es),e(M,ke),C(lt,ke,null),e(ke,zs),e(ke,W),e(W,Fs),e(W,dt),e(dt,No),e(No,Ps),e(dt,qs),e(W,js),e(W,Kt),e(Kt,As),e(W,Ls),e(W,ct),e(ct,Vo),e(Vo,Os),e(ct,Ds),e(W,Ss),e(M,Is),e(M,Q),C(pt,Q,null),e(Q,Ns),e(Q,Wo),e(Wo,Vs),e(Q,Ws),C(ye,Q,null),e(M,Rs),e(M,Y),C(mt,Y,null),e(Y,Bs),e(Y,ht),e(ht,Hs),e(ht,Xt),e(Xt,Us),e(ht,Gs),e(Y,Js),C(Me,Y,null),e(M,Qs),e(M,xe),C(ft,xe,null),e(xe,Ys),e(xe,ut),e(ut,Ks),e(ut,Zt),e(Zt,Xs),e(ut,Zs),e(M,ea),e(M,Ee),C(gt,Ee,null),e(Ee,ta),e(Ee,_t),e(_t,oa),e(_t,eo),e(eo,ra),e(_t,na),e(M,sa),e(M,ze),C(vt,ze,null),e(ze,aa),e(ze,Ro),e(Ro,ia),u(t,br,h),u(t,ie,h),e(ie,Fe),e(Fe,Bo),C(Ct,Bo,null),e(ie,la),e(ie,Ho),e(Ho,da),u(t,wr,h),u(t,le,h),C(Tt,le,null),e(le,ca),e(le,bt),e(bt,pa),e(bt,Uo),e(Uo,ma),e(bt,ha),u(t,$r,h),u(t,de,h),C(wt,de,null),e(de,fa),e(de,$t),e($t,ua),e($t,to),e(to,ga),e($t,_a),u(t,kr,h),u(t,ce,h),e(ce,Pe),e(Pe,Go),C(kt,Go,null),e(ce,va),e(ce,Jo),e(Jo,Ca),u(t,yr,h),u(t,R,h),C(yt,R,null),e(R,Ta),e(R,Mt),e(Mt,ba),e(Mt,xt),e(xt,wa),e(Mt,$a),e(R,ka),e(R,D),C(Et,D,null),e(D,ya),e(D,pe),e(pe,Ma),e(pe,oo),e(oo,xa),e(pe,Ea),e(pe,Qo),e(Qo,za),e(pe,Fa),e(D,Pa),C(qe,D,null),e(D,qa),C(je,D,null),e(D,ja),C(Ae,D,null),u(t,Mr,h),u(t,me,h),e(me,Le),e(Le,Yo),C(zt,Yo,null),e(me,Aa),e(me,Ko),e(Ko,La),u(t,xr,h),u(t,B,h),C(Ft,B,null),e(B,Oa),e(B,he),e(he,Da),e(he,Xo),e(Xo,Sa),e(he,Ia),e(he,Pt),e(Pt,Na),e(he,Va),e(B,Wa),e(B,S),C(qt,S,null),e(S,Ra),e(S,fe),e(fe,Ba),e(fe,ro),e(ro,Ha),e(fe,Ua),e(fe,Zo),e(Zo,Ga),e(fe,Ja),e(S,Qa),C(Oe,S,null),e(S,Ya),C(De,S,null),e(S,Ka),C(Se,S,null),Er=!0},p(t,[h]){const jt={};h&2&&(jt.$$scope={dirty:h,ctx:t}),ve.$set(jt);const er={};h&2&&(er.$$scope={dirty:h,ctx:t}),ye.$set(er);const tr={};h&2&&(tr.$$scope={dirty:h,ctx:t}),Me.$set(tr);const or={};h&2&&(or.$$scope={dirty:h,ctx:t}),qe.$set(or);const At={};h&2&&(At.$$scope={dirty:h,ctx:t}),je.$set(At);const rr={};h&2&&(rr.$$scope={dirty:h,ctx:t}),Ae.$set(rr);const nr={};h&2&&(nr.$$scope={dirty:h,ctx:t}),Oe.$set(nr);const sr={};h&2&&(sr.$$scope={dirty:h,ctx:t}),De.$set(sr);const Lt={};h&2&&(Lt.$$scope={dirty:h,ctx:t}),Se.$set(Lt)},i(t){Er||(T(f.$$.fragment,t),T(We.$$.fragment,t),T(He.$$.fragment,t),T(Ue.$$.fragment,t),T(ve.$$.fragment,t),T(Je.$$.fragment,t),T(Qe.$$.fragment,t),T(Ke.$$.fragment,t),T(Xe.$$.fragment,t),T(Ze.$$.fragment,t),T(et.$$.fragment,t),T(tt.$$.fragment,t),T(ot.$$.fragment,t),T(nt.$$.fragment,t),T(st.$$.fragment,t),T(at.$$.fragment,t),T(lt.$$.fragment,t),T(pt.$$.fragment,t),T(ye.$$.fragment,t),T(mt.$$.fragment,t),T(Me.$$.fragment,t),T(ft.$$.fragment,t),T(gt.$$.fragment,t),T(vt.$$.fragment,t),T(Ct.$$.fragment,t),T(Tt.$$.fragment,t),T(wt.$$.fragment,t),T(kt.$$.fragment,t),T(yt.$$.fragment,t),T(Et.$$.fragment,t),T(qe.$$.fragment,t),T(je.$$.fragment,t),T(Ae.$$.fragment,t),T(zt.$$.fragment,t),T(Ft.$$.fragment,t),T(qt.$$.fragment,t),T(Oe.$$.fragment,t),T(De.$$.fragment,t),T(Se.$$.fragment,t),Er=!0)},o(t){b(f.$$.fragment,t),b(We.$$.fragment,t),b(He.$$.fragment,t),b(Ue.$$.fragment,t),b(ve.$$.fragment,t),b(Je.$$.fragment,t),b(Qe.$$.fragment,t),b(Ke.$$.fragment,t),b(Xe.$$.fragment,t),b(Ze.$$.fragment,t),b(et.$$.fragment,t),b(tt.$$.fragment,t),b(ot.$$.fragment,t),b(nt.$$.fragment,t),b(st.$$.fragment,t),b(at.$$.fragment,t),b(lt.$$.fragment,t),b(pt.$$.fragment,t),b(ye.$$.fragment,t),b(mt.$$.fragment,t),b(Me.$$.fragment,t),b(ft.$$.fragment,t),b(gt.$$.fragment,t),b(vt.$$.fragment,t),b(Ct.$$.fragment,t),b(Tt.$$.fragment,t),b(wt.$$.fragment,t),b(kt.$$.fragment,t),b(yt.$$.fragment,t),b(Et.$$.fragment,t),b(qe.$$.fragment,t),b(je.$$.fragment,t),b(Ae.$$.fragment,t),b(zt.$$.fragment,t),b(Ft.$$.fragment,t),b(qt.$$.fragment,t),b(Oe.$$.fragment,t),b(De.$$.fragment,t),b(Se.$$.fragment,t),Er=!1},d(t){o(d),t&&o($),t&&o(m),w(f),t&&o(x),t&&o(A),w(We),t&&o(dr),t&&o(ge),t&&o(cr),t&&o(Ot),t&&o(pr),t&&o(Dt),t&&o(mr),t&&o(U),t&&o(hr),t&&o(Z),w(He),t&&o(fr),t&&o(N),w(Ue),w(ve),t&&o(ur),t&&o(oe),w(Je),t&&o(gr),t&&o(P),w(Qe),w(Ke),w(Xe),w(Ze),w(et),t&&o(_r),t&&o(ne),w(tt),t&&o(vr),t&&o(V),w(ot),w(nt),t&&o(Cr),t&&o(ae),w(st),t&&o(Tr),t&&o(M),w(at),w(lt),w(pt),w(ye),w(mt),w(Me),w(ft),w(gt),w(vt),t&&o(br),t&&o(ie),w(Ct),t&&o(wr),t&&o(le),w(Tt),t&&o($r),t&&o(de),w(wt),t&&o(kr),t&&o(ce),w(kt),t&&o(yr),t&&o(R),w(yt),w(Et),w(qe),w(je),w(Ae),t&&o(Mr),t&&o(me),w(zt),t&&o(xr),t&&o(B),w(Ft),w(qt),w(Oe),w(De),w(Se)}}}const Fl={local:"mctc",sections:[{local:"overview",title:"Overview"},{local:"transformers.MCTCConfig",title:"MCTCConfig"},{local:"transformers.MCTCTokenizer",title:"MCTCTokenizer"},{local:"transformers.MCTCFeatureExtractor",title:"MCTCFeatureExtractor"},{local:"transformers.MCTCProcessor",title:"MCTCProcessor"},{local:"transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2BaseModelOutput",title:"Wav2Vec2 specific outputs"},{local:"transformers.MCTCModel",title:"MCTCModel"},{local:"transformers.MCTCForCTC",title:"MCTCForCTC"}],title:"MCTC"};function Pl(z){return Cl(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Sl extends ul{constructor(d){super();gl(this,d,Pl,zl,_l,{})}}export{Sl as default,Fl as metadata};
