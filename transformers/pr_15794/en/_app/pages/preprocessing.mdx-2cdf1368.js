import{S as ym,i as km,s as Tm,e as a,k as p,w as j,t as o,M as $m,c as t,d as n,m as c,a as l,x as _,h as r,b as m,F as s,g as h,y as g,q as v,o as E,B as w}from"../chunks/vendor-22ad994f.js";import{T as Em}from"../chunks/Tip-540f533b.js";import{Y as wm}from"../chunks/Youtube-d2f2adc2.js";import{I as It}from"../chunks/IconCopyLink-2eb9a001.js";import{C as Y}from"../chunks/CodeBlock-03069293.js";import{C as Hh}from"../chunks/CodeBlockFw-b3c04121.js";import{D as xm}from"../chunks/DocNotebookDropdown-48171ded.js";import"../chunks/CopyButton-f539c482.js";function qm(Ms){let b,T,f,y,K;return{c(){b=a("p"),T=o(`If you plan on using a pretrained model, it\u2019s important to use the associated pretrained tokenizer: it will split
the text you give it in tokens the same way for the pretraining corpus, and it will use the same correspondence
token to index (that we usually call a `),f=a("em"),y=o("vocab"),K=o(") as during pretraining.")},l(z){b=t(z,"P",{});var H=l(b);T=r(H,`If you plan on using a pretrained model, it\u2019s important to use the associated pretrained tokenizer: it will split
the text you give it in tokens the same way for the pretraining corpus, and it will use the same correspondence
token to index (that we usually call a `),f=t(H,"EM",{});var ks=l(f);y=r(ks,"vocab"),ks.forEach(n),K=r(H,") as during pretraining."),H.forEach(n)},m(z,H){h(z,b,H),s(b,T),s(b,f),s(f,y),s(b,K)},d(z){z&&n(b)}}}function zm(Ms){let b,T;return{c(){b=a("p"),T=o(`Pre-tokenized does not mean your inputs are already tokenized (you wouldn\u2019t need to pass them through the tokenizer
if that was the case) but just split into words (which is often the first step in subword tokenization algorithms
like BPE).`)},l(f){b=t(f,"P",{});var y=l(b);T=r(y,`Pre-tokenized does not mean your inputs are already tokenized (you wouldn\u2019t need to pass them through the tokenizer
if that was the case) but just split into words (which is often the first step in subword tokenization algorithms
like BPE).`),y.forEach(n)},m(f,y){h(f,b,y),s(b,T)},d(f){f&&n(b)}}}function Dm(Ms){let b,T,f,y,K,z,H,ks,vo,Ct,Ws,Ot,L,Eo,ve,wo,yo,Ee,ko,To,St,D,$o,we,xo,qo,xn,zo,Do,qn,Ao,Po,Nt,Ts,Rt,$s,Io,ye,Co,Oo,Yt,Us,Ht,Q,xs,zn,Js,So,Dn,No,Lt,Ks,Ft,F,Ro,ke,Yo,Ho,An,Lo,Fo,Bt,Qs,Gt,$,Bo,Te,Go,Mo,$e,Wo,Uo,xe,Jo,Ko,qe,Qo,Vo,Mt,ze,Xo,Wt,Vs,Ut,A,Zo,Pn,sr,er,In,nr,ar,Cn,tr,lr,Jt,De,or,Kt,Xs,Qt,Ae,rr,Vt,Pe,pr,Xt,B,On,cr,ir,Sn,hr,ur,Nn,dr,Zt,Ie,mr,sl,Zs,el,qs,br,Ce,fr,jr,nl,zs,_r,Rn,gr,vr,al,Oe,tl,V,Ds,Yn,se,Er,Hn,wr,ll,ee,ol,ne,yr,Ln,kr,rl,Se,Tr,pl,ae,cl,x,$r,Ne,xr,qr,Fn,zr,Dr,Bn,Ar,Pr,Gn,Ir,Cr,il,Re,Or,hl,te,ul,Ye,Sr,dl,le,ml,He,Nr,bl,As,Rr,Mn,Yr,Hr,fl,oe,jl,Le,Lr,_l,re,gl,X,Ps,Wn,pe,Fr,Un,Br,vl,P,Gr,Jn,Mr,Wr,Kn,Ur,Jr,Qn,Kr,Qr,El,G,ce,Fe,Vn,Vr,Xr,Zr,Z,Is,Xn,sp,ep,Zn,np,ap,tp,I,sa,lp,op,ea,rp,pp,na,cp,ip,aa,hp,up,dp,Cs,ta,mp,bp,la,fp,jp,_p,ie,Be,oa,gp,vp,Ep,N,q,ra,wp,yp,pa,kp,Tp,ca,$p,xp,ia,qp,zp,ha,Dp,Ap,Pp,C,ua,Ip,Cp,da,Op,Sp,ma,Np,Rp,ba,Yp,Hp,Lp,O,fa,Fp,Bp,ja,Gp,Mp,_a,Wp,Up,ga,Jp,Kp,Qp,Os,va,Vp,Xp,Ea,Zp,sc,ec,wa,M,ya,nc,ac,ka,tc,lc,Ta,oc,rc,wl,k,pc,$a,cc,ic,xa,hc,uc,qa,dc,mc,za,bc,fc,Da,jc,_c,yl,Ss,Aa,ss,Pa,gc,vc,Ia,Ec,wc,Ca,yc,kc,u,es,Oa,Tc,$c,Sa,xc,qc,Na,Ra,zc,Dc,ns,kl,Ac,Ya,Pc,Ic,Ge,Ha,Cc,Oc,Sc,as,Tl,Nc,$l,Rc,La,Fa,Yc,Hc,ts,xl,Lc,Ba,Fc,Bc,Ga,Ma,Gc,Mc,ls,ql,Wc,Wa,Uc,Jc,Ua,Ja,Kc,Qc,os,Ka,Vc,Xc,Qa,Zc,si,Me,Va,ei,ni,ai,rs,zl,ti,Dl,li,Xa,Za,oi,ri,ps,Al,pi,st,ci,ii,We,et,hi,ui,di,cs,Pl,mi,Il,bi,nt,at,fi,ji,is,Cl,_i,tt,gi,vi,Ue,lt,Ei,wi,yi,hs,Ol,ki,Sl,Ti,ot,rt,$i,xi,us,Nl,qi,pt,zi,Di,ct,Ai,Pi,ds,it,Ii,Ci,ht,Oi,Si,Je,ut,Ni,Ri,Yi,ms,Rl,Hi,Yl,Li,dt,mt,Fi,Bi,bs,Hl,Gi,bt,Mi,Wi,Ke,ft,Ui,Ji,Ki,fs,Ll,Qi,Fl,Vi,jt,_t,Xi,Zi,js,Bl,sh,gt,eh,nh,vt,ah,th,_s,Gl,lh,Et,oh,rh,Qe,wt,ph,ch,ih,gs,Ml,hh,Wl,uh,yt,kt,dh,Ul,vs,Ns,Tt,he,mh,$t,bh,Jl,W,fh,ue,jh,_h,de,gh,vh,Kl,Rs,Ql,Ys,Eh,xt,wh,yh,Vl,me,Xl,Hs,kh,qt,Th,$h,Zl,Ve,xh,so,be,eo,Xe,qh,no,fe,ao,Ze,zh,to,je,lo;return z=new It({}),Ws=new xm({props:{classNames:"absolute z-10 right-0 top-0",options:[{label:"Mixed",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/master/transformers_doc/preprocessing.ipynb"},{label:"PyTorch",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/master/transformers_doc/pytorch/preprocessing.ipynb"},{label:"TensorFlow",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/master/transformers_doc/tensorflow/preprocessing.ipynb"},{label:"Mixed",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/transformers_doc/preprocessing.ipynb"},{label:"PyTorch",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/transformers_doc/pytorch/preprocessing.ipynb"},{label:"TensorFlow",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/transformers_doc/tensorflow/preprocessing.ipynb"}]}}),Ts=new Em({props:{$$slots:{default:[qm]},$$scope:{ctx:Ms}}}),Us=new Y({props:{code:`from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased"),`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)`}}),Js=new It({}),Ks=new wm({props:{id:"Yffk5aydLzg"}}),Qs=new Y({props:{code:`encoded_input = tokenizer("Hello, I'm a single sentence!")
print(encoded_input),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>encoded_input = tokenizer(<span class="hljs-string">&quot;Hello, I&#x27;m a single sentence!&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(encoded_input)
{<span class="hljs-string">&#x27;input_ids&#x27;</span>: [<span class="hljs-number">101</span>, <span class="hljs-number">138</span>, <span class="hljs-number">18696</span>, <span class="hljs-number">155</span>, <span class="hljs-number">1942</span>, <span class="hljs-number">3190</span>, <span class="hljs-number">1144</span>, <span class="hljs-number">1572</span>, <span class="hljs-number">13745</span>, <span class="hljs-number">1104</span>, <span class="hljs-number">159</span>, <span class="hljs-number">9664</span>, <span class="hljs-number">2107</span>, <span class="hljs-number">102</span>], 
 <span class="hljs-string">&#x27;token_type_ids&#x27;</span>: [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>], 
 <span class="hljs-string">&#x27;attention_mask&#x27;</span>: [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]}`}}),Vs=new Y({props:{code:'tokenizer.decode(encoded_input["input_ids"]),',highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.decode(encoded_input[<span class="hljs-string">&quot;input_ids&quot;</span>])
<span class="hljs-string">&quot;[CLS] Hello, I&#x27;m a single sentence! [SEP]&quot;</span>`}}),Xs=new Y({props:{code:`batch_sentences = ["Hello I'm a single sentence", "And another sentence", "And the very very last one"]
encoded_inputs = tokenizer(batch_sentences)
print(encoded_inputs),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>batch_sentences = [<span class="hljs-string">&quot;Hello I&#x27;m a single sentence&quot;</span>, <span class="hljs-string">&quot;And another sentence&quot;</span>, <span class="hljs-string">&quot;And the very very last one&quot;</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>encoded_inputs = tokenizer(batch_sentences)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(encoded_inputs)
{<span class="hljs-string">&#x27;input_ids&#x27;</span>: [[<span class="hljs-number">101</span>, <span class="hljs-number">8667</span>, <span class="hljs-number">146</span>, <span class="hljs-number">112</span>, <span class="hljs-number">182</span>, <span class="hljs-number">170</span>, <span class="hljs-number">1423</span>, <span class="hljs-number">5650</span>, <span class="hljs-number">102</span>],
               [<span class="hljs-number">101</span>, <span class="hljs-number">1262</span>, <span class="hljs-number">1330</span>, <span class="hljs-number">5650</span>, <span class="hljs-number">102</span>],
               [<span class="hljs-number">101</span>, <span class="hljs-number">1262</span>, <span class="hljs-number">1103</span>, <span class="hljs-number">1304</span>, <span class="hljs-number">1304</span>, <span class="hljs-number">1314</span>, <span class="hljs-number">1141</span>, <span class="hljs-number">102</span>]],
 <span class="hljs-string">&#x27;token_type_ids&#x27;</span>: [[<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
                    [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
                    [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>]],
 <span class="hljs-string">&#x27;attention_mask&#x27;</span>: [[<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>],
                    [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>],
                    [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]]}`}}),Zs=new Hh({props:{pt:{code:`batch = tokenizer(batch_sentences, padding=True, truncation=True, return_tensors="pt")
print(batch)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>batch = tokenizer(batch_sentences, padding=<span class="hljs-literal">True</span>, truncation=<span class="hljs-literal">True</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(batch)
{<span class="hljs-string">&#x27;input_ids&#x27;</span>: tensor([[ <span class="hljs-number">101</span>, <span class="hljs-number">8667</span>,  <span class="hljs-number">146</span>,  <span class="hljs-number">112</span>,  <span class="hljs-number">182</span>,  <span class="hljs-number">170</span>, <span class="hljs-number">1423</span>, <span class="hljs-number">5650</span>,  <span class="hljs-number">102</span>],
                      [ <span class="hljs-number">101</span>, <span class="hljs-number">1262</span>, <span class="hljs-number">1330</span>, <span class="hljs-number">5650</span>,  <span class="hljs-number">102</span>,    <span class="hljs-number">0</span>,    <span class="hljs-number">0</span>,    <span class="hljs-number">0</span>,    <span class="hljs-number">0</span>],
                      [ <span class="hljs-number">101</span>, <span class="hljs-number">1262</span>, <span class="hljs-number">1103</span>, <span class="hljs-number">1304</span>, <span class="hljs-number">1304</span>, <span class="hljs-number">1314</span>, <span class="hljs-number">1141</span>,  <span class="hljs-number">102</span>,    <span class="hljs-number">0</span>]]),
 <span class="hljs-string">&#x27;token_type_ids&#x27;</span>: tensor([[<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
                           [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
                           [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>]]), 
 <span class="hljs-string">&#x27;attention_mask&#x27;</span>: tensor([[<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>],
                           [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
                           [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>]])}`},tf:{code:`batch = tokenizer(batch_sentences, padding=True, truncation=True, return_tensors="tf")
print(batch)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>batch = tokenizer(batch_sentences, padding=<span class="hljs-literal">True</span>, truncation=<span class="hljs-literal">True</span>, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(batch)
{<span class="hljs-string">&#x27;input_ids&#x27;</span>: tf.Tensor([[ <span class="hljs-number">101</span>, <span class="hljs-number">8667</span>,  <span class="hljs-number">146</span>,  <span class="hljs-number">112</span>,  <span class="hljs-number">182</span>,  <span class="hljs-number">170</span>, <span class="hljs-number">1423</span>, <span class="hljs-number">5650</span>,  <span class="hljs-number">102</span>],
                      [ <span class="hljs-number">101</span>, <span class="hljs-number">1262</span>, <span class="hljs-number">1330</span>, <span class="hljs-number">5650</span>,  <span class="hljs-number">102</span>,    <span class="hljs-number">0</span>,    <span class="hljs-number">0</span>,    <span class="hljs-number">0</span>,    <span class="hljs-number">0</span>],
                      [ <span class="hljs-number">101</span>, <span class="hljs-number">1262</span>, <span class="hljs-number">1103</span>, <span class="hljs-number">1304</span>, <span class="hljs-number">1304</span>, <span class="hljs-number">1314</span>, <span class="hljs-number">1141</span>,  <span class="hljs-number">102</span>,    <span class="hljs-number">0</span>]]),
 <span class="hljs-string">&#x27;token_type_ids&#x27;</span>: tf.Tensor([[<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
                           [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
                           [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>]]), 
 <span class="hljs-string">&#x27;attention_mask&#x27;</span>: tf.Tensor([[<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>],
                           [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
                           [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>]])}`}}}),se=new It({}),ee=new wm({props:{id:"0u3ioSwev3s"}}),ae=new Y({props:{code:`encoded_input = tokenizer("How old are you?", "I'm 6 years old")
print(encoded_input),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>encoded_input = tokenizer(<span class="hljs-string">&quot;How old are you?&quot;</span>, <span class="hljs-string">&quot;I&#x27;m 6 years old&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(encoded_input)
{<span class="hljs-string">&#x27;input_ids&#x27;</span>: [<span class="hljs-number">101</span>, <span class="hljs-number">1731</span>, <span class="hljs-number">1385</span>, <span class="hljs-number">1132</span>, <span class="hljs-number">1128</span>, <span class="hljs-number">136</span>, <span class="hljs-number">102</span>, <span class="hljs-number">146</span>, <span class="hljs-number">112</span>, <span class="hljs-number">182</span>, <span class="hljs-number">127</span>, <span class="hljs-number">1201</span>, <span class="hljs-number">1385</span>, <span class="hljs-number">102</span>], 
 <span class="hljs-string">&#x27;token_type_ids&#x27;</span>: [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>], 
 <span class="hljs-string">&#x27;attention_mask&#x27;</span>: [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]}`}}),te=new Y({props:{code:'tokenizer.decode(encoded_input["input_ids"]),',highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.decode(encoded_input[<span class="hljs-string">&quot;input_ids&quot;</span>])
<span class="hljs-string">&quot;[CLS] How old are you? [SEP] I&#x27;m 6 years old [SEP]&quot;</span>`}}),le=new Y({props:{code:`batch_sentences = ["Hello I'm a single sentence", "And another sentence", "And the very very last one"]
batch_of_second_sentences = [
    "I'm a sentence that goes with the first sentence",
    "And I should be encoded with the second sentence",
    "And I go with the very last one",
]
encoded_inputs = tokenizer(batch_sentences, batch_of_second_sentences)
print(encoded_inputs),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>batch_sentences = [<span class="hljs-string">&quot;Hello I&#x27;m a single sentence&quot;</span>, <span class="hljs-string">&quot;And another sentence&quot;</span>, <span class="hljs-string">&quot;And the very very last one&quot;</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>batch_of_second_sentences = [
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;I&#x27;m a sentence that goes with the first sentence&quot;</span>,
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;And I should be encoded with the second sentence&quot;</span>,
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;And I go with the very last one&quot;</span>,
<span class="hljs-meta">... </span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>encoded_inputs = tokenizer(batch_sentences, batch_of_second_sentences)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(encoded_inputs)
{<span class="hljs-string">&#x27;input_ids&#x27;</span>: [[<span class="hljs-number">101</span>, <span class="hljs-number">8667</span>, <span class="hljs-number">146</span>, <span class="hljs-number">112</span>, <span class="hljs-number">182</span>, <span class="hljs-number">170</span>, <span class="hljs-number">1423</span>, <span class="hljs-number">5650</span>, <span class="hljs-number">102</span>, <span class="hljs-number">146</span>, <span class="hljs-number">112</span>, <span class="hljs-number">182</span>, <span class="hljs-number">170</span>, <span class="hljs-number">5650</span>, <span class="hljs-number">1115</span>, <span class="hljs-number">2947</span>, <span class="hljs-number">1114</span>, <span class="hljs-number">1103</span>, <span class="hljs-number">1148</span>, <span class="hljs-number">5650</span>, <span class="hljs-number">102</span>], 
               [<span class="hljs-number">101</span>, <span class="hljs-number">1262</span>, <span class="hljs-number">1330</span>, <span class="hljs-number">5650</span>, <span class="hljs-number">102</span>, <span class="hljs-number">1262</span>, <span class="hljs-number">146</span>, <span class="hljs-number">1431</span>, <span class="hljs-number">1129</span>, <span class="hljs-number">12544</span>, <span class="hljs-number">1114</span>, <span class="hljs-number">1103</span>, <span class="hljs-number">1248</span>, <span class="hljs-number">5650</span>, <span class="hljs-number">102</span>], 
               [<span class="hljs-number">101</span>, <span class="hljs-number">1262</span>, <span class="hljs-number">1103</span>, <span class="hljs-number">1304</span>, <span class="hljs-number">1304</span>, <span class="hljs-number">1314</span>, <span class="hljs-number">1141</span>, <span class="hljs-number">102</span>, <span class="hljs-number">1262</span>, <span class="hljs-number">146</span>, <span class="hljs-number">1301</span>, <span class="hljs-number">1114</span>, <span class="hljs-number">1103</span>, <span class="hljs-number">1304</span>, <span class="hljs-number">1314</span>, <span class="hljs-number">1141</span>, <span class="hljs-number">102</span>]], 
<span class="hljs-string">&#x27;token_type_ids&#x27;</span>: [[<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>], 
                   [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>], 
                   [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]], 
<span class="hljs-string">&#x27;attention_mask&#x27;</span>: [[<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>], 
                   [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>], 
                   [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]]}`}}),oe=new Y({props:{code:`for ids in encoded_inputs["input_ids"]:
    print(tokenizer.decode(ids)),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">for</span> ids <span class="hljs-keyword">in</span> encoded_inputs[<span class="hljs-string">&quot;input_ids&quot;</span>]:
<span class="hljs-meta">... </span>    <span class="hljs-built_in">print</span>(tokenizer.decode(ids))
[CLS] Hello I<span class="hljs-string">&#x27;m a single sentence [SEP] I&#x27;</span>m a sentence that goes <span class="hljs-keyword">with</span> the first sentence [SEP]
[CLS] And another sentence [SEP] And I should be encoded <span class="hljs-keyword">with</span> the second sentence [SEP]
[CLS] And the very very last one [SEP] And I go <span class="hljs-keyword">with</span> the very last one [SEP]`}}),re=new Hh({props:{pt:{code:'batch = tokenizer(batch_sentences, batch_of_second_sentences, padding=True, truncation=True, return_tensors="pt")',highlighted:'batch = tokenizer(batch_sentences, batch_of_second_sentences, padding=<span class="hljs-literal">True</span>, truncation=<span class="hljs-literal">True</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)'},tf:{code:'batch = tokenizer(batch_sentences, batch_of_second_sentences, padding=True, truncation=True, return_tensors="tf")',highlighted:'batch = tokenizer(batch_sentences, batch_of_second_sentences, padding=<span class="hljs-literal">True</span>, truncation=<span class="hljs-literal">True</span>, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)'}}}),pe=new It({}),he=new It({}),Rs=new Em({props:{warning:"&lcub;true}",$$slots:{default:[zm]},$$scope:{ctx:Ms}}}),me=new Y({props:{code:`encoded_input = tokenizer(["Hello", "I'm", "a", "single", "sentence"], is_split_into_words=True)
print(encoded_input),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>encoded_input = tokenizer([<span class="hljs-string">&quot;Hello&quot;</span>, <span class="hljs-string">&quot;I&#x27;m&quot;</span>, <span class="hljs-string">&quot;a&quot;</span>, <span class="hljs-string">&quot;single&quot;</span>, <span class="hljs-string">&quot;sentence&quot;</span>], is_split_into_words=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(encoded_input)
{<span class="hljs-string">&#x27;input_ids&#x27;</span>: [<span class="hljs-number">101</span>, <span class="hljs-number">8667</span>, <span class="hljs-number">146</span>, <span class="hljs-number">112</span>, <span class="hljs-number">182</span>, <span class="hljs-number">170</span>, <span class="hljs-number">1423</span>, <span class="hljs-number">5650</span>, <span class="hljs-number">102</span>],
 <span class="hljs-string">&#x27;token_type_ids&#x27;</span>: [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>], 
 <span class="hljs-string">&#x27;attention_mask&#x27;</span>: [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]}`}}),be=new Y({props:{code:`batch_sentences = [
    ["Hello", "I'm", "a", "single", "sentence"],
    ["And", "another", "sentence"],
    ["And", "the", "very", "very", "last", "one"],
]
encoded_inputs = tokenizer(batch_sentences, is_split_into_words=True),`,highlighted:`batch_sentences = [
    [<span class="hljs-string">&quot;Hello&quot;</span>, <span class="hljs-string">&quot;I&#x27;m&quot;</span>, <span class="hljs-string">&quot;a&quot;</span>, <span class="hljs-string">&quot;single&quot;</span>, <span class="hljs-string">&quot;sentence&quot;</span>],
    [<span class="hljs-string">&quot;And&quot;</span>, <span class="hljs-string">&quot;another&quot;</span>, <span class="hljs-string">&quot;sentence&quot;</span>],
    [<span class="hljs-string">&quot;And&quot;</span>, <span class="hljs-string">&quot;the&quot;</span>, <span class="hljs-string">&quot;very&quot;</span>, <span class="hljs-string">&quot;very&quot;</span>, <span class="hljs-string">&quot;last&quot;</span>, <span class="hljs-string">&quot;one&quot;</span>],
]
encoded_inputs = tokenizer(batch_sentences, is_split_into_words=<span class="hljs-literal">True</span>)`}}),fe=new Y({props:{code:`batch_of_second_sentences = [
    ["I'm", "a", "sentence", "that", "goes", "with", "the", "first", "sentence"],
    ["And", "I", "should", "be", "encoded", "with", "the", "second", "sentence"],
    ["And", "I", "go", "with", "the", "very", "last", "one"],
]
encoded_inputs = tokenizer(batch_sentences, batch_of_second_sentences, is_split_into_words=True),`,highlighted:`batch_of_second_sentences = [
    [<span class="hljs-string">&quot;I&#x27;m&quot;</span>, <span class="hljs-string">&quot;a&quot;</span>, <span class="hljs-string">&quot;sentence&quot;</span>, <span class="hljs-string">&quot;that&quot;</span>, <span class="hljs-string">&quot;goes&quot;</span>, <span class="hljs-string">&quot;with&quot;</span>, <span class="hljs-string">&quot;the&quot;</span>, <span class="hljs-string">&quot;first&quot;</span>, <span class="hljs-string">&quot;sentence&quot;</span>],
    [<span class="hljs-string">&quot;And&quot;</span>, <span class="hljs-string">&quot;I&quot;</span>, <span class="hljs-string">&quot;should&quot;</span>, <span class="hljs-string">&quot;be&quot;</span>, <span class="hljs-string">&quot;encoded&quot;</span>, <span class="hljs-string">&quot;with&quot;</span>, <span class="hljs-string">&quot;the&quot;</span>, <span class="hljs-string">&quot;second&quot;</span>, <span class="hljs-string">&quot;sentence&quot;</span>],
    [<span class="hljs-string">&quot;And&quot;</span>, <span class="hljs-string">&quot;I&quot;</span>, <span class="hljs-string">&quot;go&quot;</span>, <span class="hljs-string">&quot;with&quot;</span>, <span class="hljs-string">&quot;the&quot;</span>, <span class="hljs-string">&quot;very&quot;</span>, <span class="hljs-string">&quot;last&quot;</span>, <span class="hljs-string">&quot;one&quot;</span>],
]
encoded_inputs = tokenizer(batch_sentences, batch_of_second_sentences, is_split_into_words=<span class="hljs-literal">True</span>)`}}),je=new Hh({props:{pt:{code:`batch = tokenizer(
    batch_sentences,
    batch_of_second_sentences,
    is_split_into_words=True,
    padding=True,
    truncation=True,
    return_tensors="pt",
)`,highlighted:`batch = tokenizer(
    batch_sentences,
    batch_of_second_sentences,
    is_split_into_words=<span class="hljs-literal">True</span>,
    padding=<span class="hljs-literal">True</span>,
    truncation=<span class="hljs-literal">True</span>,
    return_tensors=<span class="hljs-string">&quot;pt&quot;</span>,
)`},tf:{code:`batch = tokenizer(
    batch_sentences,
    batch_of_second_sentences,
    is_split_into_words=True,
    padding=True,
    truncation=True,
    return_tensors="tf",
)`,highlighted:`batch = tokenizer(
    batch_sentences,
    batch_of_second_sentences,
    is_split_into_words=<span class="hljs-literal">True</span>,
    padding=<span class="hljs-literal">True</span>,
    truncation=<span class="hljs-literal">True</span>,
    return_tensors=<span class="hljs-string">&quot;tf&quot;</span>,
)`}}}),{c(){b=a("meta"),T=p(),f=a("h1"),y=a("a"),K=a("span"),j(z.$$.fragment),H=p(),ks=a("span"),vo=o("Preprocessing data"),Ct=p(),j(Ws.$$.fragment),Ot=p(),L=a("p"),Eo=o(`In this tutorial, we\u2019ll explore how to preprocess your data using \u{1F917} Transformers. The main tool for this is what we
call a `),ve=a("a"),wo=o("tokenizer"),yo=o(`. You can build one using the tokenizer class associated to the model
you would like to use, or directly with the `),Ee=a("a"),ko=o("AutoTokenizer"),To=o(" class."),St=p(),D=a("p"),$o=o("As we saw in the "),we=a("a"),xo=o("quick tour"),qo=o(`, the tokenizer will first split a given text in words (or part of
words, punctuation symbols, etc.) usually called `),xn=a("em"),zo=o("tokens"),Do=o(". Then it will convert those "),qn=a("em"),Ao=o("tokens"),Po=o(` into numbers, to be able
to build a tensor out of them and feed them to the model. It will also add any additional inputs the model might expect
to work properly.`),Nt=p(),j(Ts.$$.fragment),Rt=p(),$s=a("p"),Io=o(`To automatically download the vocab used during pretraining or fine-tuning a given model, you can use the
`),ye=a("a"),Co=o("AutoTokenizer.from_pretrained()"),Oo=o(" method:"),Yt=p(),j(Us.$$.fragment),Ht=p(),Q=a("h2"),xs=a("a"),zn=a("span"),j(Js.$$.fragment),So=p(),Dn=a("span"),No=o("Base use"),Lt=p(),j(Ks.$$.fragment),Ft=p(),F=a("p"),Ro=o("A "),ke=a("a"),Yo=o("PreTrainedTokenizer"),Ho=o(` has many methods, but the only one you need to remember for preprocessing
is its `),An=a("code"),Lo=o("__call__"),Fo=o(": you just need to feed your sentence to your tokenizer object."),Bt=p(),j(Qs.$$.fragment),Gt=p(),$=a("p"),Bo=o("This returns a dictionary string to list of ints. The "),Te=a("a"),Go=o("input_ids"),Mo=o(` are the indices corresponding
to each token in our sentence. We will see below what the `),$e=a("a"),Wo=o("attention_mask"),Uo=o(` is used for and
in `),xe=a("a"),Jo=o("the next section"),Ko=o(" the goal of "),qe=a("a"),Qo=o("token_type_ids"),Vo=o("."),Mt=p(),ze=a("p"),Xo=o("The tokenizer can decode a list of token ids in a proper sentence:"),Wt=p(),j(Vs.$$.fragment),Ut=p(),A=a("p"),Zo=o(`As you can see, the tokenizer automatically added some special tokens that the model expects. Not all models need
special tokens; for instance, if we had used `),Pn=a("em"),sr=o("gpt2-medium"),er=o(" instead of "),In=a("em"),nr=o("bert-base-cased"),ar=o(` to create our tokenizer, we
would have seen the same sentence as the original one here. You can disable this behavior (which is only advised if you
have added those special tokens yourself) by passing `),Cn=a("code"),tr=o("add_special_tokens=False"),lr=o("."),Jt=p(),De=a("p"),or=o(`If you have several sentences you want to process, you can do this efficiently by sending them as a list to the
tokenizer:`),Kt=p(),j(Xs.$$.fragment),Qt=p(),Ae=a("p"),rr=o("We get back a dictionary once again, this time with values being lists of lists of ints."),Vt=p(),Pe=a("p"),pr=o(`If the purpose of sending several sentences at a time to the tokenizer is to build a batch to feed the model, you will
probably want:`),Xt=p(),B=a("ul"),On=a("li"),cr=o("To pad each sentence to the maximum length there is in your batch."),ir=p(),Sn=a("li"),hr=o("To truncate each sentence to the maximum length the model can accept (if applicable)."),ur=p(),Nn=a("li"),dr=o("To return tensors."),Zt=p(),Ie=a("p"),mr=o("You can do all of this by using the following options when feeding your list of sentences to the tokenizer:"),sl=p(),j(Zs.$$.fragment),el=p(),qs=a("p"),br=o("It returns a dictionary with string keys and tensor values. We can now see what the "),Ce=a("a"),fr=o("attention_mask"),jr=o(` is all about: it points out which tokens the model should pay attention to and which ones
it should not (because they represent padding in this case).`),nl=p(),zs=a("p"),_r=o(`Note that if your model does not have a maximum length associated to it, the command above will throw a warning. You
can safely ignore it. You can also pass `),Rn=a("code"),gr=o("verbose=False"),vr=o(" to stop the tokenizer from throwing those kinds of warnings."),al=p(),Oe=a("a"),tl=p(),V=a("h2"),Ds=a("a"),Yn=a("span"),j(se.$$.fragment),Er=p(),Hn=a("span"),wr=o("Preprocessing pairs of sentences"),ll=p(),j(ee.$$.fragment),ol=p(),ne=a("p"),yr=o(`Sometimes you need to feed a pair of sentences to your model. For instance, if you want to classify if two sentences in
a pair are similar, or for question-answering models, which take a context and a question. For BERT models, the input
is then represented like this: `),Ln=a("code"),kr=o("[CLS] Sequence A [SEP] Sequence B [SEP]"),rl=p(),Se=a("p"),Tr=o(`You can encode a pair of sentences in the format expected by your model by supplying the two sentences as two arguments
(not a list since a list of two sentences will be interpreted as a batch of two single sentences, as we saw before).
This will once again return a dict string to list of ints:`),pl=p(),j(ae.$$.fragment),cl=p(),x=a("p"),$r=o("This shows us what the "),Ne=a("a"),xr=o("token_type_ids"),qr=o(` are for: they indicate to the model which part of
the inputs correspond to the first sentence and which part corresponds to the second sentence. Note that
`),Fn=a("em"),zr=o("token_type_ids"),Dr=o(` are not required or handled by all models. By default, a tokenizer will only return the inputs that
its associated model expects. You can force the return (or the non-return) of any of those special arguments by using
`),Bn=a("code"),Ar=o("return_input_ids"),Pr=o(" or "),Gn=a("code"),Ir=o("return_token_type_ids"),Cr=o("."),il=p(),Re=a("p"),Or=o("If we decode the token ids we obtained, we will see that the special tokens have been properly added."),hl=p(),j(te.$$.fragment),ul=p(),Ye=a("p"),Sr=o(`If you have a list of pairs of sequences you want to process, you should feed them as two lists to your tokenizer: the
list of first sentences and the list of second sentences:`),dl=p(),j(le.$$.fragment),ml=p(),He=a("p"),Nr=o("As we can see, it returns a dictionary where each value is a list of lists of ints."),bl=p(),As=a("p"),Rr=o("To double-check what is fed to the model, we can decode each list in "),Mn=a("em"),Yr=o("input_ids"),Hr=o(" one by one:"),fl=p(),j(oe.$$.fragment),jl=p(),Le=a("p"),Lr=o(`Once again, you can automatically pad your inputs to the maximum sentence length in the batch, truncate to the maximum
length the model can accept and return tensors directly with the following:`),_l=p(),j(re.$$.fragment),gl=p(),X=a("h2"),Ps=a("a"),Wn=a("span"),j(pe.$$.fragment),Fr=p(),Un=a("span"),Br=o("Everything you always wanted to know about padding and truncation"),vl=p(),P=a("p"),Gr=o(`We have seen the commands that will work for most cases (pad your batch to the length of the maximum sentence and
truncate to the maximum length the model can accept). However, the API supports more strategies if you need them. The
three arguments you need to know for this are `),Jn=a("code"),Mr=o("padding"),Wr=o(", "),Kn=a("code"),Ur=o("truncation"),Jr=o(" and "),Qn=a("code"),Kr=o("max_length"),Qr=o("."),El=p(),G=a("ul"),ce=a("li"),Fe=a("p"),Vn=a("code"),Vr=o("padding"),Xr=o(" controls the padding. It can be a boolean or a string which should be:"),Zr=p(),Z=a("ul"),Is=a("li"),Xn=a("code"),sp=o("True"),ep=o(" or "),Zn=a("code"),np=o("'longest'"),ap=o(` to pad to the longest sequence in the batch (doing no padding if you only provide
a single sequence).`),tp=p(),I=a("li"),sa=a("code"),lp=o("'max_length'"),op=o(" to pad to a length specified by the "),ea=a("code"),rp=o("max_length"),pp=o(` argument or the maximum length accepted
by the model if no `),na=a("code"),cp=o("max_length"),ip=o(" is provided ("),aa=a("code"),hp=o("max_length=None"),up=o(`). If you only provide a single sequence,
padding will still be applied to it.`),dp=p(),Cs=a("li"),ta=a("code"),mp=o("False"),bp=o(" or "),la=a("code"),fp=o("'do_not_pad'"),jp=o(` to not pad the sequences. As we have seen before, this is the default
behavior.`),_p=p(),ie=a("li"),Be=a("p"),oa=a("code"),gp=o("truncation"),vp=o(" controls the truncation. It can be a boolean or a string which should be:"),Ep=p(),N=a("ul"),q=a("li"),ra=a("code"),wp=o("True"),yp=o(" or "),pa=a("code"),kp=o("'longest_first'"),Tp=o(" truncate to a maximum length specified by the "),ca=a("code"),$p=o("max_length"),xp=o(` argument or
the maximum length accepted by the model if no `),ia=a("code"),qp=o("max_length"),zp=o(" is provided ("),ha=a("code"),Dp=o("max_length=None"),Ap=o(`). This will
truncate token by token, removing a token from the longest sequence in the pair until the proper length is
reached.`),Pp=p(),C=a("li"),ua=a("code"),Ip=o("'only_second'"),Cp=o(" truncate to a maximum length specified by the "),da=a("code"),Op=o("max_length"),Sp=o(` argument or the maximum
length accepted by the model if no `),ma=a("code"),Np=o("max_length"),Rp=o(" is provided ("),ba=a("code"),Yp=o("max_length=None"),Hp=o(`). This will only truncate
the second sentence of a pair if a pair of sequence (or a batch of pairs of sequences) is provided.`),Lp=p(),O=a("li"),fa=a("code"),Fp=o("'only_first'"),Bp=o(" truncate to a maximum length specified by the "),ja=a("code"),Gp=o("max_length"),Mp=o(` argument or the maximum
length accepted by the model if no `),_a=a("code"),Wp=o("max_length"),Up=o(" is provided ("),ga=a("code"),Jp=o("max_length=None"),Kp=o(`). This will only truncate
the first sentence of a pair if a pair of sequence (or a batch of pairs of sequences) is provided.`),Qp=p(),Os=a("li"),va=a("code"),Vp=o("False"),Xp=o(" or "),Ea=a("code"),Zp=o("'do_not_truncate'"),sc=o(` to not truncate the sequences. As we have seen before, this is the
default behavior.`),ec=p(),wa=a("li"),M=a("p"),ya=a("code"),nc=o("max_length"),ac=o(" to control the length of the padding/truncation. It can be an integer or "),ka=a("code"),tc=o("None"),lc=o(`, in which case
it will default to the maximum length the model can accept. If the model has no specific maximum input length,
truncation/padding to `),Ta=a("code"),oc=o("max_length"),rc=o(" is deactivated."),wl=p(),k=a("p"),pc=o(`Here is a table summarizing the recommend way to setup padding and truncation. If you use pair of inputs sequence in
any of the following examples, you can replace `),$a=a("code"),cc=o("truncation=True"),ic=o(" by a "),xa=a("code"),hc=o("STRATEGY"),uc=o(` selected in
`),qa=a("code"),dc=o("['only_first', 'only_second', 'longest_first']"),mc=o(", i.e. "),za=a("code"),bc=o("truncation='only_second'"),fc=o(" or "),Da=a("code"),jc=o("truncation= 'longest_first'"),_c=o(" to control how both sequence in the pair are truncated as detailed before."),yl=p(),Ss=a("table"),Aa=a("thead"),ss=a("tr"),Pa=a("th"),gc=o("Truncation"),vc=p(),Ia=a("th"),Ec=o("Padding"),wc=p(),Ca=a("th"),yc=o("Instruction"),kc=p(),u=a("tbody"),es=a("tr"),Oa=a("td"),Tc=o("no truncation"),$c=p(),Sa=a("td"),xc=o("no padding"),qc=p(),Na=a("td"),Ra=a("code"),zc=o("tokenizer(batch_sentences)"),Dc=p(),ns=a("tr"),kl=a("td"),Ac=p(),Ya=a("td"),Pc=o("padding to max sequence in batch"),Ic=p(),Ge=a("td"),Ha=a("code"),Cc=o("tokenizer(batch_sentences, padding=True)"),Oc=o(" or"),Sc=p(),as=a("tr"),Tl=a("td"),Nc=p(),$l=a("td"),Rc=p(),La=a("td"),Fa=a("code"),Yc=o("tokenizer(batch_sentences, padding='longest')"),Hc=p(),ts=a("tr"),xl=a("td"),Lc=p(),Ba=a("td"),Fc=o("padding to max model input length"),Bc=p(),Ga=a("td"),Ma=a("code"),Gc=o("tokenizer(batch_sentences, padding='max_length')"),Mc=p(),ls=a("tr"),ql=a("td"),Wc=p(),Wa=a("td"),Uc=o("padding to specific length"),Jc=p(),Ua=a("td"),Ja=a("code"),Kc=o("tokenizer(batch_sentences, padding='max_length', max_length=42)"),Qc=p(),os=a("tr"),Ka=a("td"),Vc=o("truncation to max model input length"),Xc=p(),Qa=a("td"),Zc=o("no padding"),si=p(),Me=a("td"),Va=a("code"),ei=o("tokenizer(batch_sentences, truncation=True)"),ni=o(" or"),ai=p(),rs=a("tr"),zl=a("td"),ti=p(),Dl=a("td"),li=p(),Xa=a("td"),Za=a("code"),oi=o("tokenizer(batch_sentences, truncation=STRATEGY)"),ri=p(),ps=a("tr"),Al=a("td"),pi=p(),st=a("td"),ci=o("padding to max sequence in batch"),ii=p(),We=a("td"),et=a("code"),hi=o("tokenizer(batch_sentences, padding=True, truncation=True)"),ui=o(" or"),di=p(),cs=a("tr"),Pl=a("td"),mi=p(),Il=a("td"),bi=p(),nt=a("td"),at=a("code"),fi=o("tokenizer(batch_sentences, padding=True, truncation=STRATEGY)"),ji=p(),is=a("tr"),Cl=a("td"),_i=p(),tt=a("td"),gi=o("padding to max model input length"),vi=p(),Ue=a("td"),lt=a("code"),Ei=o("tokenizer(batch_sentences, padding='max_length', truncation=True)"),wi=o(" or"),yi=p(),hs=a("tr"),Ol=a("td"),ki=p(),Sl=a("td"),Ti=p(),ot=a("td"),rt=a("code"),$i=o("tokenizer(batch_sentences, padding='max_length', truncation=STRATEGY)"),xi=p(),us=a("tr"),Nl=a("td"),qi=p(),pt=a("td"),zi=o("padding to specific length"),Di=p(),ct=a("td"),Ai=o("Not possible"),Pi=p(),ds=a("tr"),it=a("td"),Ii=o("truncation to specific length"),Ci=p(),ht=a("td"),Oi=o("no padding"),Si=p(),Je=a("td"),ut=a("code"),Ni=o("tokenizer(batch_sentences, truncation=True, max_length=42)"),Ri=o(" or"),Yi=p(),ms=a("tr"),Rl=a("td"),Hi=p(),Yl=a("td"),Li=p(),dt=a("td"),mt=a("code"),Fi=o("tokenizer(batch_sentences, truncation=STRATEGY, max_length=42)"),Bi=p(),bs=a("tr"),Hl=a("td"),Gi=p(),bt=a("td"),Mi=o("padding to max sequence in batch"),Wi=p(),Ke=a("td"),ft=a("code"),Ui=o("tokenizer(batch_sentences, padding=True, truncation=True, max_length=42)"),Ji=o(" or"),Ki=p(),fs=a("tr"),Ll=a("td"),Qi=p(),Fl=a("td"),Vi=p(),jt=a("td"),_t=a("code"),Xi=o("tokenizer(batch_sentences, padding=True, truncation=STRATEGY, max_length=42)"),Zi=p(),js=a("tr"),Bl=a("td"),sh=p(),gt=a("td"),eh=o("padding to max model input length"),nh=p(),vt=a("td"),ah=o("Not possible"),th=p(),_s=a("tr"),Gl=a("td"),lh=p(),Et=a("td"),oh=o("padding to specific length"),rh=p(),Qe=a("td"),wt=a("code"),ph=o("tokenizer(batch_sentences, padding='max_length', truncation=True, max_length=42)"),ch=o(" or"),ih=p(),gs=a("tr"),Ml=a("td"),hh=p(),Wl=a("td"),uh=p(),yt=a("td"),kt=a("code"),dh=o("tokenizer(batch_sentences, padding='max_length', truncation=STRATEGY, max_length=42)"),Ul=p(),vs=a("h2"),Ns=a("a"),Tt=a("span"),j(he.$$.fragment),mh=p(),$t=a("span"),bh=o("Pre-tokenized inputs"),Jl=p(),W=a("p"),fh=o(`The tokenizer also accept pre-tokenized inputs. This is particularly useful when you want to compute labels and extract
predictions in `),ue=a("a"),jh=o("named entity recognition (NER)"),_h=o(` or
`),de=a("a"),gh=o("part-of-speech tagging (POS tagging)"),vh=o("."),Kl=p(),j(Rs.$$.fragment),Ql=p(),Ys=a("p"),Eh=o("If you want to use pre-tokenized inputs, just set "),xt=a("code"),wh=o("is_split_into_words=True"),yh=o(` when passing your inputs to the
tokenizer. For instance, we have:`),Vl=p(),j(me.$$.fragment),Xl=p(),Hs=a("p"),kh=o(`Note that the tokenizer still adds the ids of special tokens (if applicable) unless you pass
`),qt=a("code"),Th=o("add_special_tokens=False"),$h=o("."),Zl=p(),Ve=a("p"),xh=o(`This works exactly as before for batch of sentences or batch of pairs of sentences. You can encode a batch of sentences
like this:`),so=p(),j(be.$$.fragment),eo=p(),Xe=a("p"),qh=o("or a batch of pair sentences like this:"),no=p(),j(fe.$$.fragment),ao=p(),Ze=a("p"),zh=o("And you can add padding, truncation as well as directly return tensors like before:"),to=p(),j(je.$$.fragment),this.h()},l(e){const i=$m('[data-svelte="svelte-1phssyn"]',document.head);b=t(i,"META",{name:!0,content:!0}),i.forEach(n),T=c(e),f=t(e,"H1",{class:!0});var _e=l(f);y=t(_e,"A",{id:!0,class:!0,href:!0});var zt=l(y);K=t(zt,"SPAN",{});var Lh=l(K);_(z.$$.fragment,Lh),Lh.forEach(n),zt.forEach(n),H=c(_e),ks=t(_e,"SPAN",{});var Fh=l(ks);vo=r(Fh,"Preprocessing data"),Fh.forEach(n),_e.forEach(n),Ct=c(e),_(Ws.$$.fragment,e),Ot=c(e),L=t(e,"P",{});var sn=l(L);Eo=r(sn,`In this tutorial, we\u2019ll explore how to preprocess your data using \u{1F917} Transformers. The main tool for this is what we
call a `),ve=t(sn,"A",{href:!0});var Bh=l(ve);wo=r(Bh,"tokenizer"),Bh.forEach(n),yo=r(sn,`. You can build one using the tokenizer class associated to the model
you would like to use, or directly with the `),Ee=t(sn,"A",{href:!0});var Gh=l(Ee);ko=r(Gh,"AutoTokenizer"),Gh.forEach(n),To=r(sn," class."),sn.forEach(n),St=c(e),D=t(e,"P",{});var Ls=l(D);$o=r(Ls,"As we saw in the "),we=t(Ls,"A",{href:!0});var Mh=l(we);xo=r(Mh,"quick tour"),Mh.forEach(n),qo=r(Ls,`, the tokenizer will first split a given text in words (or part of
words, punctuation symbols, etc.) usually called `),xn=t(Ls,"EM",{});var Wh=l(xn);zo=r(Wh,"tokens"),Wh.forEach(n),Do=r(Ls,". Then it will convert those "),qn=t(Ls,"EM",{});var Uh=l(qn);Ao=r(Uh,"tokens"),Uh.forEach(n),Po=r(Ls,` into numbers, to be able
to build a tensor out of them and feed them to the model. It will also add any additional inputs the model might expect
to work properly.`),Ls.forEach(n),Nt=c(e),_(Ts.$$.fragment,e),Rt=c(e),$s=t(e,"P",{});var oo=l($s);Io=r(oo,`To automatically download the vocab used during pretraining or fine-tuning a given model, you can use the
`),ye=t(oo,"A",{href:!0});var Jh=l(ye);Co=r(Jh,"AutoTokenizer.from_pretrained()"),Jh.forEach(n),Oo=r(oo," method:"),oo.forEach(n),Yt=c(e),_(Us.$$.fragment,e),Ht=c(e),Q=t(e,"H2",{class:!0});var ro=l(Q);xs=t(ro,"A",{id:!0,class:!0,href:!0});var Kh=l(xs);zn=t(Kh,"SPAN",{});var Qh=l(zn);_(Js.$$.fragment,Qh),Qh.forEach(n),Kh.forEach(n),So=c(ro),Dn=t(ro,"SPAN",{});var Vh=l(Dn);No=r(Vh,"Base use"),Vh.forEach(n),ro.forEach(n),Lt=c(e),_(Ks.$$.fragment,e),Ft=c(e),F=t(e,"P",{});var en=l(F);Ro=r(en,"A "),ke=t(en,"A",{href:!0});var Xh=l(ke);Yo=r(Xh,"PreTrainedTokenizer"),Xh.forEach(n),Ho=r(en,` has many methods, but the only one you need to remember for preprocessing
is its `),An=t(en,"CODE",{});var Zh=l(An);Lo=r(Zh,"__call__"),Zh.forEach(n),Fo=r(en,": you just need to feed your sentence to your tokenizer object."),en.forEach(n),Bt=c(e),_(Qs.$$.fragment,e),Gt=c(e),$=t(e,"P",{});var U=l($);Bo=r(U,"This returns a dictionary string to list of ints. The "),Te=t(U,"A",{href:!0});var su=l(Te);Go=r(su,"input_ids"),su.forEach(n),Mo=r(U,` are the indices corresponding
to each token in our sentence. We will see below what the `),$e=t(U,"A",{href:!0});var eu=l($e);Wo=r(eu,"attention_mask"),eu.forEach(n),Uo=r(U,` is used for and
in `),xe=t(U,"A",{href:!0});var nu=l(xe);Jo=r(nu,"the next section"),nu.forEach(n),Ko=r(U," the goal of "),qe=t(U,"A",{href:!0});var au=l(qe);Qo=r(au,"token_type_ids"),au.forEach(n),Vo=r(U,"."),U.forEach(n),Mt=c(e),ze=t(e,"P",{});var tu=l(ze);Xo=r(tu,"The tokenizer can decode a list of token ids in a proper sentence:"),tu.forEach(n),Wt=c(e),_(Vs.$$.fragment,e),Ut=c(e),A=t(e,"P",{});var Fs=l(A);Zo=r(Fs,`As you can see, the tokenizer automatically added some special tokens that the model expects. Not all models need
special tokens; for instance, if we had used `),Pn=t(Fs,"EM",{});var lu=l(Pn);sr=r(lu,"gpt2-medium"),lu.forEach(n),er=r(Fs," instead of "),In=t(Fs,"EM",{});var ou=l(In);nr=r(ou,"bert-base-cased"),ou.forEach(n),ar=r(Fs,` to create our tokenizer, we
would have seen the same sentence as the original one here. You can disable this behavior (which is only advised if you
have added those special tokens yourself) by passing `),Cn=t(Fs,"CODE",{});var ru=l(Cn);tr=r(ru,"add_special_tokens=False"),ru.forEach(n),lr=r(Fs,"."),Fs.forEach(n),Jt=c(e),De=t(e,"P",{});var pu=l(De);or=r(pu,`If you have several sentences you want to process, you can do this efficiently by sending them as a list to the
tokenizer:`),pu.forEach(n),Kt=c(e),_(Xs.$$.fragment,e),Qt=c(e),Ae=t(e,"P",{});var cu=l(Ae);rr=r(cu,"We get back a dictionary once again, this time with values being lists of lists of ints."),cu.forEach(n),Vt=c(e),Pe=t(e,"P",{});var iu=l(Pe);pr=r(iu,`If the purpose of sending several sentences at a time to the tokenizer is to build a batch to feed the model, you will
probably want:`),iu.forEach(n),Xt=c(e),B=t(e,"UL",{});var nn=l(B);On=t(nn,"LI",{});var hu=l(On);cr=r(hu,"To pad each sentence to the maximum length there is in your batch."),hu.forEach(n),ir=c(nn),Sn=t(nn,"LI",{});var uu=l(Sn);hr=r(uu,"To truncate each sentence to the maximum length the model can accept (if applicable)."),uu.forEach(n),ur=c(nn),Nn=t(nn,"LI",{});var du=l(Nn);dr=r(du,"To return tensors."),du.forEach(n),nn.forEach(n),Zt=c(e),Ie=t(e,"P",{});var mu=l(Ie);mr=r(mu,"You can do all of this by using the following options when feeding your list of sentences to the tokenizer:"),mu.forEach(n),sl=c(e),_(Zs.$$.fragment,e),el=c(e),qs=t(e,"P",{});var po=l(qs);br=r(po,"It returns a dictionary with string keys and tensor values. We can now see what the "),Ce=t(po,"A",{href:!0});var bu=l(Ce);fr=r(bu,"attention_mask"),bu.forEach(n),jr=r(po,` is all about: it points out which tokens the model should pay attention to and which ones
it should not (because they represent padding in this case).`),po.forEach(n),nl=c(e),zs=t(e,"P",{});var co=l(zs);_r=r(co,`Note that if your model does not have a maximum length associated to it, the command above will throw a warning. You
can safely ignore it. You can also pass `),Rn=t(co,"CODE",{});var fu=l(Rn);gr=r(fu,"verbose=False"),fu.forEach(n),vr=r(co," to stop the tokenizer from throwing those kinds of warnings."),co.forEach(n),al=c(e),Oe=t(e,"A",{id:!0}),l(Oe).forEach(n),tl=c(e),V=t(e,"H2",{class:!0});var io=l(V);Ds=t(io,"A",{id:!0,class:!0,href:!0});var ju=l(Ds);Yn=t(ju,"SPAN",{});var _u=l(Yn);_(se.$$.fragment,_u),_u.forEach(n),ju.forEach(n),Er=c(io),Hn=t(io,"SPAN",{});var gu=l(Hn);wr=r(gu,"Preprocessing pairs of sentences"),gu.forEach(n),io.forEach(n),ll=c(e),_(ee.$$.fragment,e),ol=c(e),ne=t(e,"P",{});var Dh=l(ne);yr=r(Dh,`Sometimes you need to feed a pair of sentences to your model. For instance, if you want to classify if two sentences in
a pair are similar, or for question-answering models, which take a context and a question. For BERT models, the input
is then represented like this: `),Ln=t(Dh,"CODE",{});var vu=l(Ln);kr=r(vu,"[CLS] Sequence A [SEP] Sequence B [SEP]"),vu.forEach(n),Dh.forEach(n),rl=c(e),Se=t(e,"P",{});var Eu=l(Se);Tr=r(Eu,`You can encode a pair of sentences in the format expected by your model by supplying the two sentences as two arguments
(not a list since a list of two sentences will be interpreted as a batch of two single sentences, as we saw before).
This will once again return a dict string to list of ints:`),Eu.forEach(n),pl=c(e),_(ae.$$.fragment,e),cl=c(e),x=t(e,"P",{});var J=l(x);$r=r(J,"This shows us what the "),Ne=t(J,"A",{href:!0});var wu=l(Ne);xr=r(wu,"token_type_ids"),wu.forEach(n),qr=r(J,` are for: they indicate to the model which part of
the inputs correspond to the first sentence and which part corresponds to the second sentence. Note that
`),Fn=t(J,"EM",{});var yu=l(Fn);zr=r(yu,"token_type_ids"),yu.forEach(n),Dr=r(J,` are not required or handled by all models. By default, a tokenizer will only return the inputs that
its associated model expects. You can force the return (or the non-return) of any of those special arguments by using
`),Bn=t(J,"CODE",{});var ku=l(Bn);Ar=r(ku,"return_input_ids"),ku.forEach(n),Pr=r(J," or "),Gn=t(J,"CODE",{});var Tu=l(Gn);Ir=r(Tu,"return_token_type_ids"),Tu.forEach(n),Cr=r(J,"."),J.forEach(n),il=c(e),Re=t(e,"P",{});var $u=l(Re);Or=r($u,"If we decode the token ids we obtained, we will see that the special tokens have been properly added."),$u.forEach(n),hl=c(e),_(te.$$.fragment,e),ul=c(e),Ye=t(e,"P",{});var xu=l(Ye);Sr=r(xu,`If you have a list of pairs of sequences you want to process, you should feed them as two lists to your tokenizer: the
list of first sentences and the list of second sentences:`),xu.forEach(n),dl=c(e),_(le.$$.fragment,e),ml=c(e),He=t(e,"P",{});var qu=l(He);Nr=r(qu,"As we can see, it returns a dictionary where each value is a list of lists of ints."),qu.forEach(n),bl=c(e),As=t(e,"P",{});var ho=l(As);Rr=r(ho,"To double-check what is fed to the model, we can decode each list in "),Mn=t(ho,"EM",{});var zu=l(Mn);Yr=r(zu,"input_ids"),zu.forEach(n),Hr=r(ho," one by one:"),ho.forEach(n),fl=c(e),_(oe.$$.fragment,e),jl=c(e),Le=t(e,"P",{});var Du=l(Le);Lr=r(Du,`Once again, you can automatically pad your inputs to the maximum sentence length in the batch, truncate to the maximum
length the model can accept and return tensors directly with the following:`),Du.forEach(n),_l=c(e),_(re.$$.fragment,e),gl=c(e),X=t(e,"H2",{class:!0});var uo=l(X);Ps=t(uo,"A",{id:!0,class:!0,href:!0});var Au=l(Ps);Wn=t(Au,"SPAN",{});var Pu=l(Wn);_(pe.$$.fragment,Pu),Pu.forEach(n),Au.forEach(n),Fr=c(uo),Un=t(uo,"SPAN",{});var Iu=l(Un);Br=r(Iu,"Everything you always wanted to know about padding and truncation"),Iu.forEach(n),uo.forEach(n),vl=c(e),P=t(e,"P",{});var Bs=l(P);Gr=r(Bs,`We have seen the commands that will work for most cases (pad your batch to the length of the maximum sentence and
truncate to the maximum length the model can accept). However, the API supports more strategies if you need them. The
three arguments you need to know for this are `),Jn=t(Bs,"CODE",{});var Cu=l(Jn);Mr=r(Cu,"padding"),Cu.forEach(n),Wr=r(Bs,", "),Kn=t(Bs,"CODE",{});var Ou=l(Kn);Ur=r(Ou,"truncation"),Ou.forEach(n),Jr=r(Bs," and "),Qn=t(Bs,"CODE",{});var Su=l(Qn);Kr=r(Su,"max_length"),Su.forEach(n),Qr=r(Bs,"."),Bs.forEach(n),El=c(e),G=t(e,"UL",{});var an=l(G);ce=t(an,"LI",{});var mo=l(ce);Fe=t(mo,"P",{});var Ah=l(Fe);Vn=t(Ah,"CODE",{});var Nu=l(Vn);Vr=r(Nu,"padding"),Nu.forEach(n),Xr=r(Ah," controls the padding. It can be a boolean or a string which should be:"),Ah.forEach(n),Zr=c(mo),Z=t(mo,"UL",{});var tn=l(Z);Is=t(tn,"LI",{});var Dt=l(Is);Xn=t(Dt,"CODE",{});var Ru=l(Xn);sp=r(Ru,"True"),Ru.forEach(n),ep=r(Dt," or "),Zn=t(Dt,"CODE",{});var Yu=l(Zn);np=r(Yu,"'longest'"),Yu.forEach(n),ap=r(Dt,` to pad to the longest sequence in the batch (doing no padding if you only provide
a single sequence).`),Dt.forEach(n),tp=c(tn),I=t(tn,"LI",{});var Es=l(I);sa=t(Es,"CODE",{});var Hu=l(sa);lp=r(Hu,"'max_length'"),Hu.forEach(n),op=r(Es," to pad to a length specified by the "),ea=t(Es,"CODE",{});var Lu=l(ea);rp=r(Lu,"max_length"),Lu.forEach(n),pp=r(Es,` argument or the maximum length accepted
by the model if no `),na=t(Es,"CODE",{});var Fu=l(na);cp=r(Fu,"max_length"),Fu.forEach(n),ip=r(Es," is provided ("),aa=t(Es,"CODE",{});var Bu=l(aa);hp=r(Bu,"max_length=None"),Bu.forEach(n),up=r(Es,`). If you only provide a single sequence,
padding will still be applied to it.`),Es.forEach(n),dp=c(tn),Cs=t(tn,"LI",{});var At=l(Cs);ta=t(At,"CODE",{});var Gu=l(ta);mp=r(Gu,"False"),Gu.forEach(n),bp=r(At," or "),la=t(At,"CODE",{});var Mu=l(la);fp=r(Mu,"'do_not_pad'"),Mu.forEach(n),jp=r(At,` to not pad the sequences. As we have seen before, this is the default
behavior.`),At.forEach(n),tn.forEach(n),mo.forEach(n),_p=c(an),ie=t(an,"LI",{});var bo=l(ie);Be=t(bo,"P",{});var Ph=l(Be);oa=t(Ph,"CODE",{});var Wu=l(oa);gp=r(Wu,"truncation"),Wu.forEach(n),vp=r(Ph," controls the truncation. It can be a boolean or a string which should be:"),Ph.forEach(n),Ep=c(bo),N=t(bo,"UL",{});var Gs=l(N);q=t(Gs,"LI",{});var R=l(q);ra=t(R,"CODE",{});var Uu=l(ra);wp=r(Uu,"True"),Uu.forEach(n),yp=r(R," or "),pa=t(R,"CODE",{});var Ju=l(pa);kp=r(Ju,"'longest_first'"),Ju.forEach(n),Tp=r(R," truncate to a maximum length specified by the "),ca=t(R,"CODE",{});var Ku=l(ca);$p=r(Ku,"max_length"),Ku.forEach(n),xp=r(R,` argument or
the maximum length accepted by the model if no `),ia=t(R,"CODE",{});var Qu=l(ia);qp=r(Qu,"max_length"),Qu.forEach(n),zp=r(R," is provided ("),ha=t(R,"CODE",{});var Vu=l(ha);Dp=r(Vu,"max_length=None"),Vu.forEach(n),Ap=r(R,`). This will
truncate token by token, removing a token from the longest sequence in the pair until the proper length is
reached.`),R.forEach(n),Pp=c(Gs),C=t(Gs,"LI",{});var ws=l(C);ua=t(ws,"CODE",{});var Xu=l(ua);Ip=r(Xu,"'only_second'"),Xu.forEach(n),Cp=r(ws," truncate to a maximum length specified by the "),da=t(ws,"CODE",{});var Zu=l(da);Op=r(Zu,"max_length"),Zu.forEach(n),Sp=r(ws,` argument or the maximum
length accepted by the model if no `),ma=t(ws,"CODE",{});var sd=l(ma);Np=r(sd,"max_length"),sd.forEach(n),Rp=r(ws," is provided ("),ba=t(ws,"CODE",{});var ed=l(ba);Yp=r(ed,"max_length=None"),ed.forEach(n),Hp=r(ws,`). This will only truncate
the second sentence of a pair if a pair of sequence (or a batch of pairs of sequences) is provided.`),ws.forEach(n),Lp=c(Gs),O=t(Gs,"LI",{});var ys=l(O);fa=t(ys,"CODE",{});var nd=l(fa);Fp=r(nd,"'only_first'"),nd.forEach(n),Bp=r(ys," truncate to a maximum length specified by the "),ja=t(ys,"CODE",{});var ad=l(ja);Gp=r(ad,"max_length"),ad.forEach(n),Mp=r(ys,` argument or the maximum
length accepted by the model if no `),_a=t(ys,"CODE",{});var td=l(_a);Wp=r(td,"max_length"),td.forEach(n),Up=r(ys," is provided ("),ga=t(ys,"CODE",{});var ld=l(ga);Jp=r(ld,"max_length=None"),ld.forEach(n),Kp=r(ys,`). This will only truncate
the first sentence of a pair if a pair of sequence (or a batch of pairs of sequences) is provided.`),ys.forEach(n),Qp=c(Gs),Os=t(Gs,"LI",{});var Pt=l(Os);va=t(Pt,"CODE",{});var od=l(va);Vp=r(od,"False"),od.forEach(n),Xp=r(Pt," or "),Ea=t(Pt,"CODE",{});var rd=l(Ea);Zp=r(rd,"'do_not_truncate'"),rd.forEach(n),sc=r(Pt,` to not truncate the sequences. As we have seen before, this is the
default behavior.`),Pt.forEach(n),Gs.forEach(n),bo.forEach(n),ec=c(an),wa=t(an,"LI",{});var pd=l(wa);M=t(pd,"P",{});var ge=l(M);ya=t(ge,"CODE",{});var cd=l(ya);nc=r(cd,"max_length"),cd.forEach(n),ac=r(ge," to control the length of the padding/truncation. It can be an integer or "),ka=t(ge,"CODE",{});var id=l(ka);tc=r(id,"None"),id.forEach(n),lc=r(ge,`, in which case
it will default to the maximum length the model can accept. If the model has no specific maximum input length,
truncation/padding to `),Ta=t(ge,"CODE",{});var hd=l(Ta);oc=r(hd,"max_length"),hd.forEach(n),rc=r(ge," is deactivated."),ge.forEach(n),pd.forEach(n),an.forEach(n),wl=c(e),k=t(e,"P",{});var S=l(k);pc=r(S,`Here is a table summarizing the recommend way to setup padding and truncation. If you use pair of inputs sequence in
any of the following examples, you can replace `),$a=t(S,"CODE",{});var ud=l($a);cc=r(ud,"truncation=True"),ud.forEach(n),ic=r(S," by a "),xa=t(S,"CODE",{});var dd=l(xa);hc=r(dd,"STRATEGY"),dd.forEach(n),uc=r(S,` selected in
`),qa=t(S,"CODE",{});var md=l(qa);dc=r(md,"['only_first', 'only_second', 'longest_first']"),md.forEach(n),mc=r(S,", i.e. "),za=t(S,"CODE",{});var bd=l(za);bc=r(bd,"truncation='only_second'"),bd.forEach(n),fc=r(S," or "),Da=t(S,"CODE",{});var fd=l(Da);jc=r(fd,"truncation= 'longest_first'"),fd.forEach(n),_c=r(S," to control how both sequence in the pair are truncated as detailed before."),S.forEach(n),yl=c(e),Ss=t(e,"TABLE",{});var fo=l(Ss);Aa=t(fo,"THEAD",{});var jd=l(Aa);ss=t(jd,"TR",{});var ln=l(ss);Pa=t(ln,"TH",{});var _d=l(Pa);gc=r(_d,"Truncation"),_d.forEach(n),vc=c(ln),Ia=t(ln,"TH",{});var gd=l(Ia);Ec=r(gd,"Padding"),gd.forEach(n),wc=c(ln),Ca=t(ln,"TH",{});var vd=l(Ca);yc=r(vd,"Instruction"),vd.forEach(n),ln.forEach(n),jd.forEach(n),kc=c(fo),u=t(fo,"TBODY",{});var d=l(u);es=t(d,"TR",{});var on=l(es);Oa=t(on,"TD",{});var Ed=l(Oa);Tc=r(Ed,"no truncation"),Ed.forEach(n),$c=c(on),Sa=t(on,"TD",{});var wd=l(Sa);xc=r(wd,"no padding"),wd.forEach(n),qc=c(on),Na=t(on,"TD",{});var yd=l(Na);Ra=t(yd,"CODE",{});var kd=l(Ra);zc=r(kd,"tokenizer(batch_sentences)"),kd.forEach(n),yd.forEach(n),on.forEach(n),Dc=c(d),ns=t(d,"TR",{});var rn=l(ns);kl=t(rn,"TD",{}),l(kl).forEach(n),Ac=c(rn),Ya=t(rn,"TD",{});var Td=l(Ya);Pc=r(Td,"padding to max sequence in batch"),Td.forEach(n),Ic=c(rn),Ge=t(rn,"TD",{});var Ih=l(Ge);Ha=t(Ih,"CODE",{});var $d=l(Ha);Cc=r($d,"tokenizer(batch_sentences, padding=True)"),$d.forEach(n),Oc=r(Ih," or"),Ih.forEach(n),rn.forEach(n),Sc=c(d),as=t(d,"TR",{});var pn=l(as);Tl=t(pn,"TD",{}),l(Tl).forEach(n),Nc=c(pn),$l=t(pn,"TD",{}),l($l).forEach(n),Rc=c(pn),La=t(pn,"TD",{});var xd=l(La);Fa=t(xd,"CODE",{});var qd=l(Fa);Yc=r(qd,"tokenizer(batch_sentences, padding='longest')"),qd.forEach(n),xd.forEach(n),pn.forEach(n),Hc=c(d),ts=t(d,"TR",{});var cn=l(ts);xl=t(cn,"TD",{}),l(xl).forEach(n),Lc=c(cn),Ba=t(cn,"TD",{});var zd=l(Ba);Fc=r(zd,"padding to max model input length"),zd.forEach(n),Bc=c(cn),Ga=t(cn,"TD",{});var Dd=l(Ga);Ma=t(Dd,"CODE",{});var Ad=l(Ma);Gc=r(Ad,"tokenizer(batch_sentences, padding='max_length')"),Ad.forEach(n),Dd.forEach(n),cn.forEach(n),Mc=c(d),ls=t(d,"TR",{});var hn=l(ls);ql=t(hn,"TD",{}),l(ql).forEach(n),Wc=c(hn),Wa=t(hn,"TD",{});var Pd=l(Wa);Uc=r(Pd,"padding to specific length"),Pd.forEach(n),Jc=c(hn),Ua=t(hn,"TD",{});var Id=l(Ua);Ja=t(Id,"CODE",{});var Cd=l(Ja);Kc=r(Cd,"tokenizer(batch_sentences, padding='max_length', max_length=42)"),Cd.forEach(n),Id.forEach(n),hn.forEach(n),Qc=c(d),os=t(d,"TR",{});var un=l(os);Ka=t(un,"TD",{});var Od=l(Ka);Vc=r(Od,"truncation to max model input length"),Od.forEach(n),Xc=c(un),Qa=t(un,"TD",{});var Sd=l(Qa);Zc=r(Sd,"no padding"),Sd.forEach(n),si=c(un),Me=t(un,"TD",{});var Ch=l(Me);Va=t(Ch,"CODE",{});var Nd=l(Va);ei=r(Nd,"tokenizer(batch_sentences, truncation=True)"),Nd.forEach(n),ni=r(Ch," or"),Ch.forEach(n),un.forEach(n),ai=c(d),rs=t(d,"TR",{});var dn=l(rs);zl=t(dn,"TD",{}),l(zl).forEach(n),ti=c(dn),Dl=t(dn,"TD",{}),l(Dl).forEach(n),li=c(dn),Xa=t(dn,"TD",{});var Rd=l(Xa);Za=t(Rd,"CODE",{});var Yd=l(Za);oi=r(Yd,"tokenizer(batch_sentences, truncation=STRATEGY)"),Yd.forEach(n),Rd.forEach(n),dn.forEach(n),ri=c(d),ps=t(d,"TR",{});var mn=l(ps);Al=t(mn,"TD",{}),l(Al).forEach(n),pi=c(mn),st=t(mn,"TD",{});var Hd=l(st);ci=r(Hd,"padding to max sequence in batch"),Hd.forEach(n),ii=c(mn),We=t(mn,"TD",{});var Oh=l(We);et=t(Oh,"CODE",{});var Ld=l(et);hi=r(Ld,"tokenizer(batch_sentences, padding=True, truncation=True)"),Ld.forEach(n),ui=r(Oh," or"),Oh.forEach(n),mn.forEach(n),di=c(d),cs=t(d,"TR",{});var bn=l(cs);Pl=t(bn,"TD",{}),l(Pl).forEach(n),mi=c(bn),Il=t(bn,"TD",{}),l(Il).forEach(n),bi=c(bn),nt=t(bn,"TD",{});var Fd=l(nt);at=t(Fd,"CODE",{});var Bd=l(at);fi=r(Bd,"tokenizer(batch_sentences, padding=True, truncation=STRATEGY)"),Bd.forEach(n),Fd.forEach(n),bn.forEach(n),ji=c(d),is=t(d,"TR",{});var fn=l(is);Cl=t(fn,"TD",{}),l(Cl).forEach(n),_i=c(fn),tt=t(fn,"TD",{});var Gd=l(tt);gi=r(Gd,"padding to max model input length"),Gd.forEach(n),vi=c(fn),Ue=t(fn,"TD",{});var Sh=l(Ue);lt=t(Sh,"CODE",{});var Md=l(lt);Ei=r(Md,"tokenizer(batch_sentences, padding='max_length', truncation=True)"),Md.forEach(n),wi=r(Sh," or"),Sh.forEach(n),fn.forEach(n),yi=c(d),hs=t(d,"TR",{});var jn=l(hs);Ol=t(jn,"TD",{}),l(Ol).forEach(n),ki=c(jn),Sl=t(jn,"TD",{}),l(Sl).forEach(n),Ti=c(jn),ot=t(jn,"TD",{});var Wd=l(ot);rt=t(Wd,"CODE",{});var Ud=l(rt);$i=r(Ud,"tokenizer(batch_sentences, padding='max_length', truncation=STRATEGY)"),Ud.forEach(n),Wd.forEach(n),jn.forEach(n),xi=c(d),us=t(d,"TR",{});var _n=l(us);Nl=t(_n,"TD",{}),l(Nl).forEach(n),qi=c(_n),pt=t(_n,"TD",{});var Jd=l(pt);zi=r(Jd,"padding to specific length"),Jd.forEach(n),Di=c(_n),ct=t(_n,"TD",{});var Kd=l(ct);Ai=r(Kd,"Not possible"),Kd.forEach(n),_n.forEach(n),Pi=c(d),ds=t(d,"TR",{});var gn=l(ds);it=t(gn,"TD",{});var Qd=l(it);Ii=r(Qd,"truncation to specific length"),Qd.forEach(n),Ci=c(gn),ht=t(gn,"TD",{});var Vd=l(ht);Oi=r(Vd,"no padding"),Vd.forEach(n),Si=c(gn),Je=t(gn,"TD",{});var Nh=l(Je);ut=t(Nh,"CODE",{});var Xd=l(ut);Ni=r(Xd,"tokenizer(batch_sentences, truncation=True, max_length=42)"),Xd.forEach(n),Ri=r(Nh," or"),Nh.forEach(n),gn.forEach(n),Yi=c(d),ms=t(d,"TR",{});var vn=l(ms);Rl=t(vn,"TD",{}),l(Rl).forEach(n),Hi=c(vn),Yl=t(vn,"TD",{}),l(Yl).forEach(n),Li=c(vn),dt=t(vn,"TD",{});var Zd=l(dt);mt=t(Zd,"CODE",{});var sm=l(mt);Fi=r(sm,"tokenizer(batch_sentences, truncation=STRATEGY, max_length=42)"),sm.forEach(n),Zd.forEach(n),vn.forEach(n),Bi=c(d),bs=t(d,"TR",{});var En=l(bs);Hl=t(En,"TD",{}),l(Hl).forEach(n),Gi=c(En),bt=t(En,"TD",{});var em=l(bt);Mi=r(em,"padding to max sequence in batch"),em.forEach(n),Wi=c(En),Ke=t(En,"TD",{});var Rh=l(Ke);ft=t(Rh,"CODE",{});var nm=l(ft);Ui=r(nm,"tokenizer(batch_sentences, padding=True, truncation=True, max_length=42)"),nm.forEach(n),Ji=r(Rh," or"),Rh.forEach(n),En.forEach(n),Ki=c(d),fs=t(d,"TR",{});var wn=l(fs);Ll=t(wn,"TD",{}),l(Ll).forEach(n),Qi=c(wn),Fl=t(wn,"TD",{}),l(Fl).forEach(n),Vi=c(wn),jt=t(wn,"TD",{});var am=l(jt);_t=t(am,"CODE",{});var tm=l(_t);Xi=r(tm,"tokenizer(batch_sentences, padding=True, truncation=STRATEGY, max_length=42)"),tm.forEach(n),am.forEach(n),wn.forEach(n),Zi=c(d),js=t(d,"TR",{});var yn=l(js);Bl=t(yn,"TD",{}),l(Bl).forEach(n),sh=c(yn),gt=t(yn,"TD",{});var lm=l(gt);eh=r(lm,"padding to max model input length"),lm.forEach(n),nh=c(yn),vt=t(yn,"TD",{});var om=l(vt);ah=r(om,"Not possible"),om.forEach(n),yn.forEach(n),th=c(d),_s=t(d,"TR",{});var kn=l(_s);Gl=t(kn,"TD",{}),l(Gl).forEach(n),lh=c(kn),Et=t(kn,"TD",{});var rm=l(Et);oh=r(rm,"padding to specific length"),rm.forEach(n),rh=c(kn),Qe=t(kn,"TD",{});var Yh=l(Qe);wt=t(Yh,"CODE",{});var pm=l(wt);ph=r(pm,"tokenizer(batch_sentences, padding='max_length', truncation=True, max_length=42)"),pm.forEach(n),ch=r(Yh," or"),Yh.forEach(n),kn.forEach(n),ih=c(d),gs=t(d,"TR",{});var Tn=l(gs);Ml=t(Tn,"TD",{}),l(Ml).forEach(n),hh=c(Tn),Wl=t(Tn,"TD",{}),l(Wl).forEach(n),uh=c(Tn),yt=t(Tn,"TD",{});var cm=l(yt);kt=t(cm,"CODE",{});var im=l(kt);dh=r(im,"tokenizer(batch_sentences, padding='max_length', truncation=STRATEGY, max_length=42)"),im.forEach(n),cm.forEach(n),Tn.forEach(n),d.forEach(n),fo.forEach(n),Ul=c(e),vs=t(e,"H2",{class:!0});var jo=l(vs);Ns=t(jo,"A",{id:!0,class:!0,href:!0});var hm=l(Ns);Tt=t(hm,"SPAN",{});var um=l(Tt);_(he.$$.fragment,um),um.forEach(n),hm.forEach(n),mh=c(jo),$t=t(jo,"SPAN",{});var dm=l($t);bh=r(dm,"Pre-tokenized inputs"),dm.forEach(n),jo.forEach(n),Jl=c(e),W=t(e,"P",{});var $n=l(W);fh=r($n,`The tokenizer also accept pre-tokenized inputs. This is particularly useful when you want to compute labels and extract
predictions in `),ue=t($n,"A",{href:!0,rel:!0});var mm=l(ue);jh=r(mm,"named entity recognition (NER)"),mm.forEach(n),_h=r($n,` or
`),de=t($n,"A",{href:!0,rel:!0});var bm=l(de);gh=r(bm,"part-of-speech tagging (POS tagging)"),bm.forEach(n),vh=r($n,"."),$n.forEach(n),Kl=c(e),_(Rs.$$.fragment,e),Ql=c(e),Ys=t(e,"P",{});var _o=l(Ys);Eh=r(_o,"If you want to use pre-tokenized inputs, just set "),xt=t(_o,"CODE",{});var fm=l(xt);wh=r(fm,"is_split_into_words=True"),fm.forEach(n),yh=r(_o,` when passing your inputs to the
tokenizer. For instance, we have:`),_o.forEach(n),Vl=c(e),_(me.$$.fragment,e),Xl=c(e),Hs=t(e,"P",{});var go=l(Hs);kh=r(go,`Note that the tokenizer still adds the ids of special tokens (if applicable) unless you pass
`),qt=t(go,"CODE",{});var jm=l(qt);Th=r(jm,"add_special_tokens=False"),jm.forEach(n),$h=r(go,"."),go.forEach(n),Zl=c(e),Ve=t(e,"P",{});var _m=l(Ve);xh=r(_m,`This works exactly as before for batch of sentences or batch of pairs of sentences. You can encode a batch of sentences
like this:`),_m.forEach(n),so=c(e),_(be.$$.fragment,e),eo=c(e),Xe=t(e,"P",{});var gm=l(Xe);qh=r(gm,"or a batch of pair sentences like this:"),gm.forEach(n),no=c(e),_(fe.$$.fragment,e),ao=c(e),Ze=t(e,"P",{});var vm=l(Ze);zh=r(vm,"And you can add padding, truncation as well as directly return tensors like before:"),vm.forEach(n),to=c(e),_(je.$$.fragment,e),this.h()},h(){m(b,"name","hf:doc:metadata"),m(b,"content",JSON.stringify(Am)),m(y,"id","preprocessing-data"),m(y,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(y,"href","#preprocessing-data"),m(f,"class","relative group"),m(ve,"href","main_classes/tokenizer"),m(Ee,"href","/docs/transformers/pr_15794/en/model_doc/auto#transformers.AutoTokenizer"),m(we,"href","quicktour"),m(ye,"href","/docs/transformers/pr_15794/en/model_doc/auto#transformers.AutoTokenizer.from_pretrained"),m(xs,"id","base-use"),m(xs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(xs,"href","#base-use"),m(Q,"class","relative group"),m(ke,"href","/docs/transformers/pr_15794/en/main_classes/tokenizer#transformers.PreTrainedTokenizer"),m(Te,"href","glossary#input-ids"),m($e,"href","glossary#attention-mask"),m(xe,"href","#preprocessing-pairs-of-sentences"),m(qe,"href","glossary#token-type-ids"),m(Ce,"href","glossary#attention-mask"),m(Oe,"id","sentence-pairs"),m(Ds,"id","preprocessing-pairs-of-sentences"),m(Ds,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(Ds,"href","#preprocessing-pairs-of-sentences"),m(V,"class","relative group"),m(Ne,"href","glossary#token-type-ids"),m(Ps,"id","everything-you-always-wanted-to-know-about-padding-and-truncation"),m(Ps,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(Ps,"href","#everything-you-always-wanted-to-know-about-padding-and-truncation"),m(X,"class","relative group"),m(Ns,"id","pretokenized-inputs"),m(Ns,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(Ns,"href","#pretokenized-inputs"),m(vs,"class","relative group"),m(ue,"href","https://en.wikipedia.org/wiki/Named-entity_recognition"),m(ue,"rel","nofollow"),m(de,"href","https://en.wikipedia.org/wiki/Part-of-speech_tagging"),m(de,"rel","nofollow")},m(e,i){s(document.head,b),h(e,T,i),h(e,f,i),s(f,y),s(y,K),g(z,K,null),s(f,H),s(f,ks),s(ks,vo),h(e,Ct,i),g(Ws,e,i),h(e,Ot,i),h(e,L,i),s(L,Eo),s(L,ve),s(ve,wo),s(L,yo),s(L,Ee),s(Ee,ko),s(L,To),h(e,St,i),h(e,D,i),s(D,$o),s(D,we),s(we,xo),s(D,qo),s(D,xn),s(xn,zo),s(D,Do),s(D,qn),s(qn,Ao),s(D,Po),h(e,Nt,i),g(Ts,e,i),h(e,Rt,i),h(e,$s,i),s($s,Io),s($s,ye),s(ye,Co),s($s,Oo),h(e,Yt,i),g(Us,e,i),h(e,Ht,i),h(e,Q,i),s(Q,xs),s(xs,zn),g(Js,zn,null),s(Q,So),s(Q,Dn),s(Dn,No),h(e,Lt,i),g(Ks,e,i),h(e,Ft,i),h(e,F,i),s(F,Ro),s(F,ke),s(ke,Yo),s(F,Ho),s(F,An),s(An,Lo),s(F,Fo),h(e,Bt,i),g(Qs,e,i),h(e,Gt,i),h(e,$,i),s($,Bo),s($,Te),s(Te,Go),s($,Mo),s($,$e),s($e,Wo),s($,Uo),s($,xe),s(xe,Jo),s($,Ko),s($,qe),s(qe,Qo),s($,Vo),h(e,Mt,i),h(e,ze,i),s(ze,Xo),h(e,Wt,i),g(Vs,e,i),h(e,Ut,i),h(e,A,i),s(A,Zo),s(A,Pn),s(Pn,sr),s(A,er),s(A,In),s(In,nr),s(A,ar),s(A,Cn),s(Cn,tr),s(A,lr),h(e,Jt,i),h(e,De,i),s(De,or),h(e,Kt,i),g(Xs,e,i),h(e,Qt,i),h(e,Ae,i),s(Ae,rr),h(e,Vt,i),h(e,Pe,i),s(Pe,pr),h(e,Xt,i),h(e,B,i),s(B,On),s(On,cr),s(B,ir),s(B,Sn),s(Sn,hr),s(B,ur),s(B,Nn),s(Nn,dr),h(e,Zt,i),h(e,Ie,i),s(Ie,mr),h(e,sl,i),g(Zs,e,i),h(e,el,i),h(e,qs,i),s(qs,br),s(qs,Ce),s(Ce,fr),s(qs,jr),h(e,nl,i),h(e,zs,i),s(zs,_r),s(zs,Rn),s(Rn,gr),s(zs,vr),h(e,al,i),h(e,Oe,i),h(e,tl,i),h(e,V,i),s(V,Ds),s(Ds,Yn),g(se,Yn,null),s(V,Er),s(V,Hn),s(Hn,wr),h(e,ll,i),g(ee,e,i),h(e,ol,i),h(e,ne,i),s(ne,yr),s(ne,Ln),s(Ln,kr),h(e,rl,i),h(e,Se,i),s(Se,Tr),h(e,pl,i),g(ae,e,i),h(e,cl,i),h(e,x,i),s(x,$r),s(x,Ne),s(Ne,xr),s(x,qr),s(x,Fn),s(Fn,zr),s(x,Dr),s(x,Bn),s(Bn,Ar),s(x,Pr),s(x,Gn),s(Gn,Ir),s(x,Cr),h(e,il,i),h(e,Re,i),s(Re,Or),h(e,hl,i),g(te,e,i),h(e,ul,i),h(e,Ye,i),s(Ye,Sr),h(e,dl,i),g(le,e,i),h(e,ml,i),h(e,He,i),s(He,Nr),h(e,bl,i),h(e,As,i),s(As,Rr),s(As,Mn),s(Mn,Yr),s(As,Hr),h(e,fl,i),g(oe,e,i),h(e,jl,i),h(e,Le,i),s(Le,Lr),h(e,_l,i),g(re,e,i),h(e,gl,i),h(e,X,i),s(X,Ps),s(Ps,Wn),g(pe,Wn,null),s(X,Fr),s(X,Un),s(Un,Br),h(e,vl,i),h(e,P,i),s(P,Gr),s(P,Jn),s(Jn,Mr),s(P,Wr),s(P,Kn),s(Kn,Ur),s(P,Jr),s(P,Qn),s(Qn,Kr),s(P,Qr),h(e,El,i),h(e,G,i),s(G,ce),s(ce,Fe),s(Fe,Vn),s(Vn,Vr),s(Fe,Xr),s(ce,Zr),s(ce,Z),s(Z,Is),s(Is,Xn),s(Xn,sp),s(Is,ep),s(Is,Zn),s(Zn,np),s(Is,ap),s(Z,tp),s(Z,I),s(I,sa),s(sa,lp),s(I,op),s(I,ea),s(ea,rp),s(I,pp),s(I,na),s(na,cp),s(I,ip),s(I,aa),s(aa,hp),s(I,up),s(Z,dp),s(Z,Cs),s(Cs,ta),s(ta,mp),s(Cs,bp),s(Cs,la),s(la,fp),s(Cs,jp),s(G,_p),s(G,ie),s(ie,Be),s(Be,oa),s(oa,gp),s(Be,vp),s(ie,Ep),s(ie,N),s(N,q),s(q,ra),s(ra,wp),s(q,yp),s(q,pa),s(pa,kp),s(q,Tp),s(q,ca),s(ca,$p),s(q,xp),s(q,ia),s(ia,qp),s(q,zp),s(q,ha),s(ha,Dp),s(q,Ap),s(N,Pp),s(N,C),s(C,ua),s(ua,Ip),s(C,Cp),s(C,da),s(da,Op),s(C,Sp),s(C,ma),s(ma,Np),s(C,Rp),s(C,ba),s(ba,Yp),s(C,Hp),s(N,Lp),s(N,O),s(O,fa),s(fa,Fp),s(O,Bp),s(O,ja),s(ja,Gp),s(O,Mp),s(O,_a),s(_a,Wp),s(O,Up),s(O,ga),s(ga,Jp),s(O,Kp),s(N,Qp),s(N,Os),s(Os,va),s(va,Vp),s(Os,Xp),s(Os,Ea),s(Ea,Zp),s(Os,sc),s(G,ec),s(G,wa),s(wa,M),s(M,ya),s(ya,nc),s(M,ac),s(M,ka),s(ka,tc),s(M,lc),s(M,Ta),s(Ta,oc),s(M,rc),h(e,wl,i),h(e,k,i),s(k,pc),s(k,$a),s($a,cc),s(k,ic),s(k,xa),s(xa,hc),s(k,uc),s(k,qa),s(qa,dc),s(k,mc),s(k,za),s(za,bc),s(k,fc),s(k,Da),s(Da,jc),s(k,_c),h(e,yl,i),h(e,Ss,i),s(Ss,Aa),s(Aa,ss),s(ss,Pa),s(Pa,gc),s(ss,vc),s(ss,Ia),s(Ia,Ec),s(ss,wc),s(ss,Ca),s(Ca,yc),s(Ss,kc),s(Ss,u),s(u,es),s(es,Oa),s(Oa,Tc),s(es,$c),s(es,Sa),s(Sa,xc),s(es,qc),s(es,Na),s(Na,Ra),s(Ra,zc),s(u,Dc),s(u,ns),s(ns,kl),s(ns,Ac),s(ns,Ya),s(Ya,Pc),s(ns,Ic),s(ns,Ge),s(Ge,Ha),s(Ha,Cc),s(Ge,Oc),s(u,Sc),s(u,as),s(as,Tl),s(as,Nc),s(as,$l),s(as,Rc),s(as,La),s(La,Fa),s(Fa,Yc),s(u,Hc),s(u,ts),s(ts,xl),s(ts,Lc),s(ts,Ba),s(Ba,Fc),s(ts,Bc),s(ts,Ga),s(Ga,Ma),s(Ma,Gc),s(u,Mc),s(u,ls),s(ls,ql),s(ls,Wc),s(ls,Wa),s(Wa,Uc),s(ls,Jc),s(ls,Ua),s(Ua,Ja),s(Ja,Kc),s(u,Qc),s(u,os),s(os,Ka),s(Ka,Vc),s(os,Xc),s(os,Qa),s(Qa,Zc),s(os,si),s(os,Me),s(Me,Va),s(Va,ei),s(Me,ni),s(u,ai),s(u,rs),s(rs,zl),s(rs,ti),s(rs,Dl),s(rs,li),s(rs,Xa),s(Xa,Za),s(Za,oi),s(u,ri),s(u,ps),s(ps,Al),s(ps,pi),s(ps,st),s(st,ci),s(ps,ii),s(ps,We),s(We,et),s(et,hi),s(We,ui),s(u,di),s(u,cs),s(cs,Pl),s(cs,mi),s(cs,Il),s(cs,bi),s(cs,nt),s(nt,at),s(at,fi),s(u,ji),s(u,is),s(is,Cl),s(is,_i),s(is,tt),s(tt,gi),s(is,vi),s(is,Ue),s(Ue,lt),s(lt,Ei),s(Ue,wi),s(u,yi),s(u,hs),s(hs,Ol),s(hs,ki),s(hs,Sl),s(hs,Ti),s(hs,ot),s(ot,rt),s(rt,$i),s(u,xi),s(u,us),s(us,Nl),s(us,qi),s(us,pt),s(pt,zi),s(us,Di),s(us,ct),s(ct,Ai),s(u,Pi),s(u,ds),s(ds,it),s(it,Ii),s(ds,Ci),s(ds,ht),s(ht,Oi),s(ds,Si),s(ds,Je),s(Je,ut),s(ut,Ni),s(Je,Ri),s(u,Yi),s(u,ms),s(ms,Rl),s(ms,Hi),s(ms,Yl),s(ms,Li),s(ms,dt),s(dt,mt),s(mt,Fi),s(u,Bi),s(u,bs),s(bs,Hl),s(bs,Gi),s(bs,bt),s(bt,Mi),s(bs,Wi),s(bs,Ke),s(Ke,ft),s(ft,Ui),s(Ke,Ji),s(u,Ki),s(u,fs),s(fs,Ll),s(fs,Qi),s(fs,Fl),s(fs,Vi),s(fs,jt),s(jt,_t),s(_t,Xi),s(u,Zi),s(u,js),s(js,Bl),s(js,sh),s(js,gt),s(gt,eh),s(js,nh),s(js,vt),s(vt,ah),s(u,th),s(u,_s),s(_s,Gl),s(_s,lh),s(_s,Et),s(Et,oh),s(_s,rh),s(_s,Qe),s(Qe,wt),s(wt,ph),s(Qe,ch),s(u,ih),s(u,gs),s(gs,Ml),s(gs,hh),s(gs,Wl),s(gs,uh),s(gs,yt),s(yt,kt),s(kt,dh),h(e,Ul,i),h(e,vs,i),s(vs,Ns),s(Ns,Tt),g(he,Tt,null),s(vs,mh),s(vs,$t),s($t,bh),h(e,Jl,i),h(e,W,i),s(W,fh),s(W,ue),s(ue,jh),s(W,_h),s(W,de),s(de,gh),s(W,vh),h(e,Kl,i),g(Rs,e,i),h(e,Ql,i),h(e,Ys,i),s(Ys,Eh),s(Ys,xt),s(xt,wh),s(Ys,yh),h(e,Vl,i),g(me,e,i),h(e,Xl,i),h(e,Hs,i),s(Hs,kh),s(Hs,qt),s(qt,Th),s(Hs,$h),h(e,Zl,i),h(e,Ve,i),s(Ve,xh),h(e,so,i),g(be,e,i),h(e,eo,i),h(e,Xe,i),s(Xe,qh),h(e,no,i),g(fe,e,i),h(e,ao,i),h(e,Ze,i),s(Ze,zh),h(e,to,i),g(je,e,i),lo=!0},p(e,[i]){const _e={};i&2&&(_e.$$scope={dirty:i,ctx:e}),Ts.$set(_e);const zt={};i&2&&(zt.$$scope={dirty:i,ctx:e}),Rs.$set(zt)},i(e){lo||(v(z.$$.fragment,e),v(Ws.$$.fragment,e),v(Ts.$$.fragment,e),v(Us.$$.fragment,e),v(Js.$$.fragment,e),v(Ks.$$.fragment,e),v(Qs.$$.fragment,e),v(Vs.$$.fragment,e),v(Xs.$$.fragment,e),v(Zs.$$.fragment,e),v(se.$$.fragment,e),v(ee.$$.fragment,e),v(ae.$$.fragment,e),v(te.$$.fragment,e),v(le.$$.fragment,e),v(oe.$$.fragment,e),v(re.$$.fragment,e),v(pe.$$.fragment,e),v(he.$$.fragment,e),v(Rs.$$.fragment,e),v(me.$$.fragment,e),v(be.$$.fragment,e),v(fe.$$.fragment,e),v(je.$$.fragment,e),lo=!0)},o(e){E(z.$$.fragment,e),E(Ws.$$.fragment,e),E(Ts.$$.fragment,e),E(Us.$$.fragment,e),E(Js.$$.fragment,e),E(Ks.$$.fragment,e),E(Qs.$$.fragment,e),E(Vs.$$.fragment,e),E(Xs.$$.fragment,e),E(Zs.$$.fragment,e),E(se.$$.fragment,e),E(ee.$$.fragment,e),E(ae.$$.fragment,e),E(te.$$.fragment,e),E(le.$$.fragment,e),E(oe.$$.fragment,e),E(re.$$.fragment,e),E(pe.$$.fragment,e),E(he.$$.fragment,e),E(Rs.$$.fragment,e),E(me.$$.fragment,e),E(be.$$.fragment,e),E(fe.$$.fragment,e),E(je.$$.fragment,e),lo=!1},d(e){n(b),e&&n(T),e&&n(f),w(z),e&&n(Ct),w(Ws,e),e&&n(Ot),e&&n(L),e&&n(St),e&&n(D),e&&n(Nt),w(Ts,e),e&&n(Rt),e&&n($s),e&&n(Yt),w(Us,e),e&&n(Ht),e&&n(Q),w(Js),e&&n(Lt),w(Ks,e),e&&n(Ft),e&&n(F),e&&n(Bt),w(Qs,e),e&&n(Gt),e&&n($),e&&n(Mt),e&&n(ze),e&&n(Wt),w(Vs,e),e&&n(Ut),e&&n(A),e&&n(Jt),e&&n(De),e&&n(Kt),w(Xs,e),e&&n(Qt),e&&n(Ae),e&&n(Vt),e&&n(Pe),e&&n(Xt),e&&n(B),e&&n(Zt),e&&n(Ie),e&&n(sl),w(Zs,e),e&&n(el),e&&n(qs),e&&n(nl),e&&n(zs),e&&n(al),e&&n(Oe),e&&n(tl),e&&n(V),w(se),e&&n(ll),w(ee,e),e&&n(ol),e&&n(ne),e&&n(rl),e&&n(Se),e&&n(pl),w(ae,e),e&&n(cl),e&&n(x),e&&n(il),e&&n(Re),e&&n(hl),w(te,e),e&&n(ul),e&&n(Ye),e&&n(dl),w(le,e),e&&n(ml),e&&n(He),e&&n(bl),e&&n(As),e&&n(fl),w(oe,e),e&&n(jl),e&&n(Le),e&&n(_l),w(re,e),e&&n(gl),e&&n(X),w(pe),e&&n(vl),e&&n(P),e&&n(El),e&&n(G),e&&n(wl),e&&n(k),e&&n(yl),e&&n(Ss),e&&n(Ul),e&&n(vs),w(he),e&&n(Jl),e&&n(W),e&&n(Kl),w(Rs,e),e&&n(Ql),e&&n(Ys),e&&n(Vl),w(me,e),e&&n(Xl),e&&n(Hs),e&&n(Zl),e&&n(Ve),e&&n(so),w(be,e),e&&n(eo),e&&n(Xe),e&&n(no),w(fe,e),e&&n(ao),e&&n(Ze),e&&n(to),w(je,e)}}}const Am={local:"preprocessing-data",sections:[{local:"base-use",title:"Base use"},{local:"preprocessing-pairs-of-sentences",title:"Preprocessing pairs of sentences"},{local:"everything-you-always-wanted-to-know-about-padding-and-truncation",title:"Everything you always wanted to know about padding and truncation"},{local:"pretokenized-inputs",title:"Pre-tokenized inputs"}],title:"Preprocessing data"};function Pm(Ms,b,T){let{fw:f}=b;return Ms.$$set=y=>{"fw"in y&&T(0,f=y.fw)},[f]}class Lm extends ym{constructor(b){super();km(this,b,Pm,Dm,Tm,{fw:0})}}export{Lm as default,Am as metadata};
