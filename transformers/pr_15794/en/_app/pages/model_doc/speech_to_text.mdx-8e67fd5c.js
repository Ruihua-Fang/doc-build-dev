import{S as ol,i as nl,s as sl,e as r,k as d,w as u,t as n,M as rl,c as a,d as o,m as l,a as i,x as _,h as s,b as c,F as e,g as h,y as g,q as v,o as T,B as x}from"../../chunks/vendor-22ad994f.js";import{T as ws}from"../../chunks/Tip-540f533b.js";import{D as F}from"../../chunks/Docstring-3bc3620c.js";import{C as Sn}from"../../chunks/CodeBlock-03069293.js";import{I as Te}from"../../chunks/IconCopyLink-2eb9a001.js";import"../../chunks/CopyButton-f539c482.js";function al(Y){let m,S,f,k,z,b,y,E;return{c(){m=r("p"),S=n(`This class method is simply calling the feature extractor
`),f=r("a"),k=n("from_pretrained()"),z=n(` and the tokenizer
`),b=r("code"),y=n("from_pretrained"),E=n(` methods. Please refer to the docstrings of the
methods above for more information.`),this.h()},l(C){m=a(C,"P",{});var $=i(m);S=s($,`This class method is simply calling the feature extractor
`),f=a($,"A",{href:!0});var M=i(f);k=s(M,"from_pretrained()"),M.forEach(o),z=s($,` and the tokenizer
`),b=a($,"CODE",{});var L=i(b);y=s(L,"from_pretrained"),L.forEach(o),E=s($,` methods. Please refer to the docstrings of the
methods above for more information.`),$.forEach(o),this.h()},h(){c(f,"href","/docs/transformers/pr_15794/en/main_classes/feature_extractor#transformers.FeatureExtractionMixin.from_pretrained")},m(C,$){h(C,m,$),e(m,S),e(m,f),e(f,k),e(m,z),e(m,b),e(b,y),e(m,E)},d(C){C&&o(m)}}}function il(Y){let m,S,f,k,z,b,y,E;return{c(){m=r("p"),S=n("This class method is simply calling "),f=r("a"),k=n("save_pretrained()"),z=n(` and
`),b=r("code"),y=n("save_pretrained"),E=n(`. Please refer to the docstrings of the methods
above for more information.`),this.h()},l(C){m=a(C,"P",{});var $=i(m);S=s($,"This class method is simply calling "),f=a($,"A",{href:!0});var M=i(f);k=s(M,"save_pretrained()"),M.forEach(o),z=s($,` and
`),b=a($,"CODE",{});var L=i(b);y=s(L,"save_pretrained"),L.forEach(o),E=s($,`. Please refer to the docstrings of the methods
above for more information.`),$.forEach(o),this.h()},h(){c(f,"href","/docs/transformers/pr_15794/en/main_classes/feature_extractor#transformers.FeatureExtractionMixin.save_pretrained")},m(C,$){h(C,m,$),e(m,S),e(m,f),e(f,k),e(m,z),e(m,b),e(b,y),e(m,E)},d(C){C&&o(m)}}}function cl(Y){let m,S,f,k,z;return{c(){m=r("p"),S=n("Although the recipe for forward pass needs to be defined within this function, one should call the "),f=r("code"),k=n("Module"),z=n(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(b){m=a(b,"P",{});var y=i(m);S=s(y,"Although the recipe for forward pass needs to be defined within this function, one should call the "),f=a(y,"CODE",{});var E=i(f);k=s(E,"Module"),E.forEach(o),z=s(y,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),y.forEach(o)},m(b,y){h(b,m,y),e(m,S),e(m,f),e(f,k),e(m,z)},d(b){b&&o(m)}}}function dl(Y){let m,S,f,k,z;return{c(){m=r("p"),S=n("Although the recipe for forward pass needs to be defined within this function, one should call the "),f=r("code"),k=n("Module"),z=n(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(b){m=a(b,"P",{});var y=i(m);S=s(y,"Although the recipe for forward pass needs to be defined within this function, one should call the "),f=a(y,"CODE",{});var E=i(f);k=s(E,"Module"),E.forEach(o),z=s(y,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),y.forEach(o)},m(b,y){h(b,m,y),e(m,S),e(m,f),e(f,k),e(m,z)},d(b){b&&o(m)}}}function ll(Y){let m,S,f,k,z,b,y,E,C,$,M,L,So,Ve,Ss,$o,$s,$n,O,zs,Ue,Es,qs,Be,Ps,js,Re,Fs,Cs,He,Ms,As,zn,Q,Ds,Je,Is,Ns,Ye,Ls,Os,En,ce,xe,zo,Xe,Gs,Eo,Ws,qn,be,Vs,qo,Us,Bs,Pn,G,Rs,Rt,Hs,Js,Ht,Ys,Xs,Jt,Ks,Qs,Yt,Zs,er,jn,q,tr,Po,or,nr,jo,sr,rr,Fo,ar,ir,Co,cr,dr,Mo,lr,pr,Ke,hr,mr,Ao,fr,Fn,Xt,Do,ur,Cn,Qe,Mn,Kt,Ze,Io,_r,gr,A,vr,No,Tr,xr,Lo,br,kr,Oo,yr,wr,Go,Sr,$r,Wo,zr,Er,An,et,Dn,ke,qr,tt,Pr,jr,In,de,ye,Vo,ot,Fr,Uo,Cr,Nn,D,nt,Mr,le,Ar,Qt,Dr,Ir,st,Nr,Lr,Or,pe,Gr,Zt,Wr,Vr,eo,Ur,Br,Rr,Bo,Hr,Jr,rt,Ln,he,we,Ro,at,Yr,Ho,Xr,On,P,it,Kr,Jo,Qr,Zr,ct,ea,to,ta,oa,na,Se,dt,sa,Yo,ra,aa,$e,lt,ia,pt,ca,Xo,da,la,pa,Z,ht,ha,oo,ma,no,fa,ua,Ko,_a,ga,Qo,Gn,me,ze,Zo,mt,va,en,Ta,Wn,I,ft,xa,tn,ba,ka,ut,ya,so,wa,Sa,$a,on,za,Ea,Ee,_t,qa,nn,Pa,Vn,fe,qe,sn,gt,ja,rn,Fa,Un,w,vt,Ca,an,Ma,Aa,W,ro,Da,Ia,ao,Na,La,io,Oa,Ga,Tt,cn,Wa,Va,Ua,co,Ba,Ra,Ha,Pe,xt,Ja,X,Ya,bt,dn,Xa,Ka,Qa,lo,Za,ei,kt,ln,ti,oi,ni,si,ee,yt,ri,pn,ai,ii,je,ci,te,wt,di,St,li,hn,pi,hi,mi,Fe,fi,Ce,$t,ui,zt,_i,po,gi,vi,Ti,Me,Et,xi,qt,bi,ho,ki,yi,wi,Ae,Pt,Si,mn,$i,Bn,ue,De,fn,jt,zi,un,Ei,Rn,R,Ft,qi,Ct,Pi,mo,ji,Fi,Ci,Mt,Mi,At,Ai,Di,Ii,V,Dt,Ni,_e,Li,fo,Oi,Gi,_n,Wi,Vi,Ui,Ie,Bi,gn,Ri,Hi,It,Hn,ge,Ne,vn,Nt,Ji,Tn,Yi,Jn,H,Lt,Xi,Ot,Ki,uo,Qi,Zi,ec,Gt,tc,Wt,oc,nc,sc,U,Vt,rc,ve,ac,_o,ic,cc,xn,dc,lc,pc,Le,hc,bn,mc,fc,Ut,Yn;return b=new Te({}),Ve=new Te({}),Xe=new Te({}),Qe=new Sn({props:{code:`import torch
from transformers import Speech2TextProcessor, Speech2TextForConditionalGeneration
from datasets import load_dataset
import soundfile as sf

model = Speech2TextForConditionalGeneration.from_pretrained("facebook/s2t-small-librispeech-asr")
processor = Speech2TextProcessor.from_pretrained("facebook/s2t-small-librispeech-asr")


def map_to_array(batch):
    speech, _ = sf.read(batch["file"])
    batch["speech"] = speech
    return batch


ds = load_dataset("hf-internal-testing/librispeech_asr_dummy", "clean", split="validation")
ds = ds.map(map_to_array)

inputs = processor(ds["speech"][0], sampling_rate=16_000, return_tensors="pt")
generated_ids = model.generate(input_ids=inputs["input_features"], attention_mask=inputs["attention_mask"])

transcription = processor.batch_decode(generated_ids),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> Speech2TextProcessor, Speech2TextForConditionalGeneration
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> soundfile <span class="hljs-keyword">as</span> sf

<span class="hljs-meta">&gt;&gt;&gt; </span>model = Speech2TextForConditionalGeneration.from_pretrained(<span class="hljs-string">&quot;facebook/s2t-small-librispeech-asr&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>processor = Speech2TextProcessor.from_pretrained(<span class="hljs-string">&quot;facebook/s2t-small-librispeech-asr&quot;</span>)


<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">def</span> <span class="hljs-title function_">map_to_array</span>(<span class="hljs-params">batch</span>):
<span class="hljs-meta">... </span>    speech, _ = sf.read(batch[<span class="hljs-string">&quot;file&quot;</span>])
<span class="hljs-meta">... </span>    batch[<span class="hljs-string">&quot;speech&quot;</span>] = speech
<span class="hljs-meta">... </span>    <span class="hljs-keyword">return</span> batch


<span class="hljs-meta">&gt;&gt;&gt; </span>ds = load_dataset(<span class="hljs-string">&quot;hf-internal-testing/librispeech_asr_dummy&quot;</span>, <span class="hljs-string">&quot;clean&quot;</span>, split=<span class="hljs-string">&quot;validation&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>ds = ds.<span class="hljs-built_in">map</span>(map_to_array)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(ds[<span class="hljs-string">&quot;speech&quot;</span>][<span class="hljs-number">0</span>], sampling_rate=<span class="hljs-number">16_000</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>generated_ids = model.generate(input_ids=inputs[<span class="hljs-string">&quot;input_features&quot;</span>], attention_mask=inputs[<span class="hljs-string">&quot;attention_mask&quot;</span>])

<span class="hljs-meta">&gt;&gt;&gt; </span>transcription = processor.batch_decode(generated_ids)`}}),et=new Sn({props:{code:`import torch
from transformers import Speech2TextProcessor, Speech2TextForConditionalGeneration
from datasets import load_dataset
import soundfile as sf

model = Speech2TextForConditionalGeneration.from_pretrained("facebook/s2t-medium-mustc-multilingual-st")
processor = Speech2TextProcessor.from_pretrained("facebook/s2t-medium-mustc-multilingual-st")


def map_to_array(batch):
    speech, _ = sf.read(batch["file"])
    batch["speech"] = speech
    return batch


ds = load_dataset("hf-internal-testing/librispeech_asr_dummy", "clean", split="validation")
ds = ds.map(map_to_array)

inputs = processor(ds["speech"][0], sampling_rate=16_000, return_tensors="pt")
generated_ids = model.generate(
    input_ids=inputs["input_features"],
    attention_mask=inputs["attention_mask"],
    forced_bos_token_id=processor.tokenizer.lang_code_to_id["fr"],
)

translation = processor.batch_decode(generated_ids),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> Speech2TextProcessor, Speech2TextForConditionalGeneration
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> soundfile <span class="hljs-keyword">as</span> sf

<span class="hljs-meta">&gt;&gt;&gt; </span>model = Speech2TextForConditionalGeneration.from_pretrained(<span class="hljs-string">&quot;facebook/s2t-medium-mustc-multilingual-st&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>processor = Speech2TextProcessor.from_pretrained(<span class="hljs-string">&quot;facebook/s2t-medium-mustc-multilingual-st&quot;</span>)


<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">def</span> <span class="hljs-title function_">map_to_array</span>(<span class="hljs-params">batch</span>):
<span class="hljs-meta">... </span>    speech, _ = sf.read(batch[<span class="hljs-string">&quot;file&quot;</span>])
<span class="hljs-meta">... </span>    batch[<span class="hljs-string">&quot;speech&quot;</span>] = speech
<span class="hljs-meta">... </span>    <span class="hljs-keyword">return</span> batch


<span class="hljs-meta">&gt;&gt;&gt; </span>ds = load_dataset(<span class="hljs-string">&quot;hf-internal-testing/librispeech_asr_dummy&quot;</span>, <span class="hljs-string">&quot;clean&quot;</span>, split=<span class="hljs-string">&quot;validation&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>ds = ds.<span class="hljs-built_in">map</span>(map_to_array)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(ds[<span class="hljs-string">&quot;speech&quot;</span>][<span class="hljs-number">0</span>], sampling_rate=<span class="hljs-number">16_000</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>generated_ids = model.generate(
<span class="hljs-meta">... </span>    input_ids=inputs[<span class="hljs-string">&quot;input_features&quot;</span>],
<span class="hljs-meta">... </span>    attention_mask=inputs[<span class="hljs-string">&quot;attention_mask&quot;</span>],
<span class="hljs-meta">... </span>    forced_bos_token_id=processor.tokenizer.lang_code_to_id[<span class="hljs-string">&quot;fr&quot;</span>],
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>translation = processor.batch_decode(generated_ids)`}}),ot=new Te({}),nt=new F({props:{name:"class transformers.Speech2TextConfig",anchor:"transformers.Speech2TextConfig",parameters:[{name:"vocab_size",val:" = 10000"},{name:"encoder_layers",val:" = 12"},{name:"encoder_ffn_dim",val:" = 2048"},{name:"encoder_attention_heads",val:" = 4"},{name:"decoder_layers",val:" = 6"},{name:"decoder_ffn_dim",val:" = 2048"},{name:"decoder_attention_heads",val:" = 4"},{name:"encoder_layerdrop",val:" = 0.0"},{name:"decoder_layerdrop",val:" = 0.0"},{name:"use_cache",val:" = True"},{name:"is_encoder_decoder",val:" = True"},{name:"activation_function",val:" = 'relu'"},{name:"d_model",val:" = 256"},{name:"dropout",val:" = 0.1"},{name:"attention_dropout",val:" = 0.0"},{name:"activation_dropout",val:" = 0.0"},{name:"init_std",val:" = 0.02"},{name:"decoder_start_token_id",val:" = 2"},{name:"classifier_dropout",val:" = 0.0"},{name:"scale_embedding",val:" = True"},{name:"pad_token_id",val:" = 1"},{name:"bos_token_id",val:" = 0"},{name:"eos_token_id",val:" = 2"},{name:"max_source_positions",val:" = 6000"},{name:"max_target_positions",val:" = 1024"},{name:"num_conv_layers",val:" = 2"},{name:"conv_kernel_sizes",val:" = (5, 5)"},{name:"conv_channels",val:" = 1024"},{name:"input_feat_per_channel",val:" = 80"},{name:"input_channels",val:" = 1"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15794/src/transformers/models/speech_to_text/configuration_speech_to_text.py#L29",parametersDescription:[{anchor:"transformers.Speech2TextConfig.vocab_size",description:`<strong>vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 50265) &#x2014;
Vocabulary size of the Speech2Text model. Defines the number of different tokens that can be represented by
the <code>inputs_ids</code> passed when calling <a href="/docs/transformers/pr_15794/en/model_doc/speech_to_text#transformers.Speech2TextModel">Speech2TextModel</a>`,name:"vocab_size"},{anchor:"transformers.Speech2TextConfig.d_model",description:`<strong>d_model</strong> (<code>int</code>, <em>optional</em>, defaults to 1024) &#x2014;
Dimensionality of the layers and the pooler layer.`,name:"d_model"},{anchor:"transformers.Speech2TextConfig.encoder_layers",description:`<strong>encoder_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of encoder layers.`,name:"encoder_layers"},{anchor:"transformers.Speech2TextConfig.decoder_layers",description:`<strong>decoder_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of decoder layers.`,name:"decoder_layers"},{anchor:"transformers.Speech2TextConfig.encoder_attention_heads",description:`<strong>encoder_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 16) &#x2014;
Number of attention heads for each attention layer in the Transformer encoder.`,name:"encoder_attention_heads"},{anchor:"transformers.Speech2TextConfig.decoder_attention_heads",description:`<strong>decoder_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 16) &#x2014;
Number of attention heads for each attention layer in the Transformer decoder.`,name:"decoder_attention_heads"},{anchor:"transformers.Speech2TextConfig.decoder_ffn_dim",description:`<strong>decoder_ffn_dim</strong> (<code>int</code>, <em>optional</em>, defaults to 4096) &#x2014;
Dimensionality of the &#x201C;intermediate&#x201D; (often named feed-forward) layer in decoder.`,name:"decoder_ffn_dim"},{anchor:"transformers.Speech2TextConfig.encoder_ffn_dim",description:`<strong>encoder_ffn_dim</strong> (<code>int</code>, <em>optional</em>, defaults to 4096) &#x2014;
Dimensionality of the &#x201C;intermediate&#x201D; (often named feed-forward) layer in decoder.`,name:"encoder_ffn_dim"},{anchor:"transformers.Speech2TextConfig.activation_function",description:`<strong>activation_function</strong> (<code>str</code> or <code>function</code>, <em>optional</em>, defaults to <code>&quot;gelu&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the encoder and pooler. If string, <code>&quot;gelu&quot;</code>,
<code>&quot;relu&quot;</code>, <code>&quot;silu&quot;</code> and <code>&quot;gelu_new&quot;</code> are supported.`,name:"activation_function"},{anchor:"transformers.Speech2TextConfig.dropout",description:`<strong>dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.`,name:"dropout"},{anchor:"transformers.Speech2TextConfig.attention_dropout",description:`<strong>attention_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout ratio for the attention probabilities.`,name:"attention_dropout"},{anchor:"transformers.Speech2TextConfig.activation_dropout",description:`<strong>activation_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout ratio for activations inside the fully connected layer.`,name:"activation_dropout"},{anchor:"transformers.Speech2TextConfig.classifier_dropout",description:`<strong>classifier_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout ratio for classifier.`,name:"classifier_dropout"},{anchor:"transformers.Speech2TextConfig.init_std",description:`<strong>init_std</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.
encoder_layerdrop &#x2014; (<code>float</code>, <em>optional</em>, defaults to 0.0):
The LayerDrop probability for the encoder. See the [LayerDrop paper](see <a href="https://arxiv.org/abs/1909.11556" rel="nofollow">https://arxiv.org/abs/1909.11556</a>)
for more details.
decoder_layerdrop &#x2014; (<code>float</code>, <em>optional</em>, defaults to 0.0):
The LayerDrop probability for the decoder. See the [LayerDrop paper](see <a href="https://arxiv.org/abs/1909.11556" rel="nofollow">https://arxiv.org/abs/1909.11556</a>)
for more details.`,name:"init_std"},{anchor:"transformers.Speech2TextConfig.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not the model should return the last key/values attentions (not used by all models).`,name:"use_cache"},{anchor:"transformers.Speech2TextConfig.max_source_positions",description:`<strong>max_source_positions</strong> (<code>int</code>, <em>optional</em>, defaults to 6000) &#x2014;
The maximum sequence length of log-mel filter-bank features that this model might ever be used with.`,name:"max_source_positions"},{anchor:"transformers.Speech2TextConfig.max_target_positions",description:`<strong>max_target_positions</strong> (<code>int</code>, <em>optional</em>, defaults to 1024) &#x2014;
The maximum sequence length that this model might ever be used with. Typically set this to something large
just in case (e.g., 512 or 1024 or 2048).`,name:"max_target_positions"},{anchor:"transformers.Speech2TextConfig.num_conv_layers",description:`<strong>num_conv_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 2) &#x2014;
Number of 1D convolutional layers in the conv module.`,name:"num_conv_layers"},{anchor:"transformers.Speech2TextConfig.conv_kernel_sizes",description:`<strong>conv_kernel_sizes</strong> (<code>Tuple[int]</code>, <em>optional</em>, defaults to <code>(5, 5)</code>) &#x2014;
A tuple of integers defining the kernel size of each 1D convolutional layer in the conv module. The length
of <code>conv_kernel_sizes</code> has to match <code>num_conv_layers</code>.`,name:"conv_kernel_sizes"},{anchor:"transformers.Speech2TextConfig.conv_channels",description:`<strong>conv_channels</strong> (<code>int</code>, <em>optional</em>, defaults to 1024) &#x2014;
An integer defining the number of output channels of each convolution layers except the final one in the
conv module.`,name:"conv_channels"},{anchor:"transformers.Speech2TextConfig.input_feat_per_channel",description:`<strong>input_feat_per_channel</strong> (<code>int</code>, <em>optional</em>, defaults to 80) &#x2014;
An integer specifying the size of feature vector. This is also the dimensions of log-mel filter-bank
features.`,name:"input_feat_per_channel"},{anchor:"transformers.Speech2TextConfig.input_channels",description:`<strong>input_channels</strong> (<code>int</code>, <em>optional</em>, defaults to 1) &#x2014;
An integer specifying number of input channels of the input feature vector.`,name:"input_channels"}]}}),rt=new Sn({props:{code:`from transformers import Speech2TextModel, Speech2TextConfig

# Initializing a Speech2Text s2t_transformer_s style configuration
configuration = Speech2TextConfig()

# Initializing a model from the s2t_transformer_s style configuration
model = Speech2TextModel(configuration)

# Accessing the model configuration
configuration = model.config,`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> Speech2TextModel, Speech2TextConfig

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a Speech2Text s2t_transformer_s style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = Speech2TextConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a model from the s2t_transformer_s style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = Speech2TextModel(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`}}),at=new Te({}),it=new F({props:{name:"class transformers.Speech2TextTokenizer",anchor:"transformers.Speech2TextTokenizer",parameters:[{name:"vocab_file",val:""},{name:"spm_file",val:""},{name:"bos_token",val:" = '<s>'"},{name:"eos_token",val:" = '</s>'"},{name:"pad_token",val:" = '<pad>'"},{name:"unk_token",val:" = '<unk>'"},{name:"do_upper_case",val:" = False"},{name:"do_lower_case",val:" = False"},{name:"tgt_lang",val:" = None"},{name:"lang_codes",val:" = None"},{name:"sp_model_kwargs",val:": typing.Union[typing.Dict[str, typing.Any], NoneType] = None"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15794/src/transformers/models/speech_to_text/tokenization_speech_to_text.py#L55",parametersDescription:[{anchor:"transformers.Speech2TextTokenizer.vocab_file",description:`<strong>vocab_file</strong> (<code>str</code>) &#x2014;
File containing the vocabulary.`,name:"vocab_file"},{anchor:"transformers.Speech2TextTokenizer.spm_file",description:`<strong>spm_file</strong> (<code>str</code>) &#x2014;
Path to the <a href="https://github.com/google/sentencepiece" rel="nofollow">SentencePiece</a> model file`,name:"spm_file"},{anchor:"transformers.Speech2TextTokenizer.bos_token",description:`<strong>bos_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;s&gt;&quot;</code>) &#x2014;
The beginning of sentence token.`,name:"bos_token"},{anchor:"transformers.Speech2TextTokenizer.eos_token",description:`<strong>eos_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;/s&gt;&quot;</code>) &#x2014;
The end of sentence token.`,name:"eos_token"},{anchor:"transformers.Speech2TextTokenizer.unk_token",description:`<strong>unk_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;unk&gt;&quot;</code>) &#x2014;
The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this
token instead.`,name:"unk_token"},{anchor:"transformers.Speech2TextTokenizer.pad_token",description:`<strong>pad_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;pad&gt;&quot;</code>) &#x2014;
The token used for padding, for example when batching sequences of different lengths.`,name:"pad_token"},{anchor:"transformers.Speech2TextTokenizer.do_upper_case",description:`<strong>do_upper_case</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to uppercase the output when decoding.`,name:"do_upper_case"},{anchor:"transformers.Speech2TextTokenizer.do_lower_case",description:`<strong>do_lower_case</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to lowercase the input when tokenizing.`,name:"do_lower_case"},{anchor:"transformers.Speech2TextTokenizer.tgt_lang",description:`<strong>tgt_lang</strong> (<code>str</code>, <em>optional</em>) &#x2014;
A string representing the target language.`,name:"tgt_lang"},{anchor:"transformers.Speech2TextTokenizer.sp_model_kwargs",description:`<strong>sp_model_kwargs</strong> (<code>dict</code>, <em>optional</em>) &#x2014;
Will be passed to the <code>SentencePieceProcessor.__init__()</code> method. The <a href="https://github.com/google/sentencepiece/tree/master/python" rel="nofollow">Python wrapper for
SentencePiece</a> can be used, among other things,
to set:</p>
<ul>
<li>
<p><code>enable_sampling</code>: Enable subword regularization.</p>
</li>
<li>
<p><code>nbest_size</code>: Sampling parameters for unigram. Invalid for BPE-Dropout.</p>
<ul>
<li><code>nbest_size = {0,1}</code>: No sampling is performed.</li>
<li><code>nbest_size &gt; 1</code>: samples from the nbest_size results.</li>
<li><code>nbest_size &lt; 0</code>: assuming that nbest_size is infinite and samples from the all hypothesis (lattice)
using forward-filtering-and-backward-sampling algorithm.</li>
</ul>
</li>
<li>
<p><code>alpha</code>: Smoothing parameter for unigram sampling, and dropout probability of merge operations for
BPE-dropout.</p>
</li>
</ul>`,name:"sp_model_kwargs"}]}}),dt=new F({props:{name:"build_inputs_with_special_tokens",anchor:"transformers.Speech2TextTokenizer.build_inputs_with_special_tokens",parameters:[{name:"token_ids_0",val:""},{name:"token_ids_1",val:" = None"}],source:"https://github.com/huggingface/transformers/blob/pr_15794/src/transformers/models/speech_to_text/tokenization_speech_to_text.py#L195"}}),lt=new F({props:{name:"get_special_tokens_mask",anchor:"transformers.Speech2TextTokenizer.get_special_tokens_mask",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"},{name:"already_has_special_tokens",val:": bool = False"}],source:"https://github.com/huggingface/transformers/blob/pr_15794/src/transformers/models/speech_to_text/tokenization_speech_to_text.py#L202",parametersDescription:[{anchor:"transformers.Speech2TextTokenizer.get_special_tokens_mask.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.Speech2TextTokenizer.get_special_tokens_mask.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"},{anchor:"transformers.Speech2TextTokenizer.get_special_tokens_mask.already_has_special_tokens",description:`<strong>already_has_special_tokens</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not the token list is already formatted with special tokens for the model.`,name:"already_has_special_tokens"}],returnDescription:`
<p>A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),ht=new F({props:{name:"create_token_type_ids_from_sequences",anchor:"transformers.PreTrainedTokenizerBase.create_token_type_ids_from_sequences",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"}],source:"https://github.com/huggingface/transformers/blob/pr_15794/src/transformers/tokenization_utils_base.py#L2864",parametersDescription:[{anchor:"transformers.PreTrainedTokenizerBase.create_token_type_ids_from_sequences.token_ids_0",description:"<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014; The first tokenized sequence.",name:"token_ids_0"},{anchor:"transformers.PreTrainedTokenizerBase.create_token_type_ids_from_sequences.token_ids_1",description:"<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014; The second tokenized sequence.",name:"token_ids_1"}],returnDescription:`
<p>The token type ids.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),mt=new Te({}),ft=new F({props:{name:"class transformers.Speech2TextFeatureExtractor",anchor:"transformers.Speech2TextFeatureExtractor",parameters:[{name:"feature_size",val:" = 80"},{name:"sampling_rate",val:" = 16000"},{name:"num_mel_bins",val:" = 80"},{name:"padding_value",val:" = 0.0"},{name:"do_ceptral_normalize",val:" = True"},{name:"normalize_means",val:" = True"},{name:"normalize_vars",val:" = True"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15794/src/transformers/models/speech_to_text/feature_extraction_speech_to_text.py#L34",parametersDescription:[{anchor:"transformers.Speech2TextFeatureExtractor.feature_size",description:`<strong>feature_size</strong> (<code>int</code>, defaults to 80) &#x2014;
The feature dimension of the extracted features.`,name:"feature_size"},{anchor:"transformers.Speech2TextFeatureExtractor.sampling_rate",description:`<strong>sampling_rate</strong> (<code>int</code>, defaults to 16000) &#x2014;
The sampling rate at which the audio files should be digitalized expressed in Hertz per second (Hz).`,name:"sampling_rate"},{anchor:"transformers.Speech2TextFeatureExtractor.num_mel_bins",description:`<strong>num_mel_bins</strong> (<code>int</code>, defaults to 80) &#x2014;
Number of Mel-frequency bins.`,name:"num_mel_bins"},{anchor:"transformers.Speech2TextFeatureExtractor.padding_value",description:`<strong>padding_value</strong> (<code>float</code>, defaults to 0.0) &#x2014;
The value that is used to fill the padding vectors.`,name:"padding_value"},{anchor:"transformers.Speech2TextFeatureExtractor.do_ceptral_normalize",description:`<strong>do_ceptral_normalize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to apply utterance-level cepstral mean and variance normalization to extracted features.`,name:"do_ceptral_normalize"},{anchor:"transformers.Speech2TextFeatureExtractor.normalize_means",description:`<strong>normalize_means</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to zero-mean normalize the extracted features.`,name:"normalize_means"},{anchor:"transformers.Speech2TextFeatureExtractor.normalize_vars",description:`<strong>normalize_vars</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to unit-variance normalize the extracted features.`,name:"normalize_vars"}]}}),_t=new F({props:{name:"__call__",anchor:"transformers.Speech2TextFeatureExtractor.__call__",parameters:[{name:"raw_speech",val:": typing.Union[numpy.ndarray, typing.List[float], typing.List[numpy.ndarray], typing.List[typing.List[float]]]"},{name:"padding",val:": typing.Union[bool, str, transformers.file_utils.PaddingStrategy] = False"},{name:"max_length",val:": typing.Optional[int] = None"},{name:"truncation",val:": bool = False"},{name:"pad_to_multiple_of",val:": typing.Optional[int] = None"},{name:"return_tensors",val:": typing.Union[str, transformers.file_utils.TensorType, NoneType] = None"},{name:"sampling_rate",val:": typing.Optional[int] = None"},{name:"return_attention_mask",val:": typing.Optional[bool] = None"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15794/src/transformers/models/speech_to_text/feature_extraction_speech_to_text.py#L127",parametersDescription:[{anchor:"transformers.Speech2TextFeatureExtractor.__call__.raw_speech",description:`<strong>raw_speech</strong> (<code>np.ndarray</code>, <code>List[float]</code>, <code>List[np.ndarray]</code>, <code>List[List[float]]</code>) &#x2014;
The sequence or batch of sequences to be padded. Each sequence can be a numpy array, a list of float
values, a list of numpy arrays or a list of list of float values.`,name:"raw_speech"},{anchor:"transformers.Speech2TextFeatureExtractor.__call__.padding",description:`<strong>padding</strong> (<code>bool</code>, <code>str</code> or <a href="/docs/transformers/pr_15794/en/internal/file_utils#transformers.file_utils.PaddingStrategy">PaddingStrategy</a>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Select a strategy to pad the returned sequences (according to the model&#x2019;s padding side and padding
index) among:</p>
<ul>
<li><code>True</code> or <code>&apos;longest&apos;</code>: Pad to the longest sequence in the batch (or no padding if only a single
sequence if provided).</li>
<li><code>&apos;max_length&apos;</code>: Pad to a maximum length specified with the argument <code>max_length</code> or to the maximum
acceptable input length for the model if that argument is not provided.</li>
<li><code>False</code> or <code>&apos;do_not_pad&apos;</code> (default): No padding (i.e., can output a batch with sequences of different
lengths).</li>
</ul>`,name:"padding"},{anchor:"transformers.Speech2TextFeatureExtractor.__call__.max_length",description:`<strong>max_length</strong> (<code>int</code>, <em>optional</em>) &#x2014;
Maximum length of the returned list and optionally padding length (see above).`,name:"max_length"},{anchor:"transformers.Speech2TextFeatureExtractor.__call__.truncation",description:`<strong>truncation</strong> (<code>bool</code>) &#x2014;
Activates truncation to cut input sequences longer than <em>max_length</em> to <em>max_length</em>.`,name:"truncation"},{anchor:"transformers.Speech2TextFeatureExtractor.__call__.pad_to_multiple_of",description:`<strong>pad_to_multiple_of</strong> (<code>int</code>, <em>optional</em>) &#x2014;
If set will pad the sequence to a multiple of the provided value.</p>
<p>This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability</p>
<blockquote>
<p>= 7.5 (Volta), or on TPUs which benefit from having sequence lengths be a multiple of 128.</p>
</blockquote>`,name:"pad_to_multiple_of"},{anchor:"transformers.Speech2TextFeatureExtractor.__call__.return_attention_mask",description:`<strong>return_attention_mask</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether to return the attention mask. If left to the default, will return the attention mask according
to the specific feature_extractor&#x2019;s default.</p>
<p><a href="../glossary#attention-mask">What are attention masks?</a></p>
<div class="course-tip  bg-gradient-to-br dark:bg-gradient-to-r before:border-green-500 dark:before:border-green-800 from-green-50 dark:from-gray-900 to-white dark:to-gray-950 border border-green-50 text-green-700 dark:text-gray-400">
						
<p>For Speech2TextTransoformer models, <code>attention_mask</code> should alwys be passed for batched inference, to
avoid subtle bugs.</p>

					</div>`,name:"return_attention_mask"},{anchor:"transformers.Speech2TextFeatureExtractor.__call__.return_tensors",description:`<strong>return_tensors</strong> (<code>str</code> or <a href="/docs/transformers/pr_15794/en/internal/file_utils#transformers.TensorType">TensorType</a>, <em>optional</em>) &#x2014;
If set, will return tensors instead of list of python integers. Acceptable values are:</p>
<ul>
<li><code>&apos;tf&apos;</code>: Return TensorFlow <code>tf.constant</code> objects.</li>
<li><code>&apos;pt&apos;</code>: Return PyTorch <code>torch.Tensor</code> objects.</li>
<li><code>&apos;np&apos;</code>: Return Numpy <code>np.ndarray</code> objects.</li>
</ul>`,name:"return_tensors"},{anchor:"transformers.Speech2TextFeatureExtractor.__call__.sampling_rate",description:`<strong>sampling_rate</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The sampling rate at which the <code>raw_speech</code> input was sampled. It is strongly recommended to pass
<code>sampling_rate</code> at the forward call to prevent silent errors.`,name:"sampling_rate"},{anchor:"transformers.Speech2TextFeatureExtractor.__call__.padding_value",description:`<strong>padding_value</strong> (<code>float</code>, defaults to 0.0) &#x2014;
The value that is used to fill the padding values / vectors.`,name:"padding_value"}]}}),gt=new Te({}),vt=new F({props:{name:"class transformers.Speech2TextProcessor",anchor:"transformers.Speech2TextProcessor",parameters:[{name:"feature_extractor",val:""},{name:"tokenizer",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15794/src/transformers/models/speech_to_text/processing_speech_to_text.py#L23",parametersDescription:[{anchor:"transformers.Speech2TextProcessor.feature_extractor",description:`<strong>feature_extractor</strong> (<code>Speech2TextFeatureExtractor</code>) &#x2014;
An instance of <a href="/docs/transformers/pr_15794/en/model_doc/speech_to_text#transformers.Speech2TextFeatureExtractor">Speech2TextFeatureExtractor</a>. The feature extractor is a required input.`,name:"feature_extractor"},{anchor:"transformers.Speech2TextProcessor.tokenizer",description:`<strong>tokenizer</strong> (<code>Speech2TextTokenizer</code>) &#x2014;
An instance of <a href="/docs/transformers/pr_15794/en/model_doc/speech_to_text#transformers.Speech2TextTokenizer">Speech2TextTokenizer</a>. The tokenizer is a required input.`,name:"tokenizer"}]}}),xt=new F({props:{name:"__call__",anchor:"transformers.Speech2TextProcessor.__call__",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15794/src/transformers/models/speech_to_text/processing_speech_to_text.py#L45"}}),yt=new F({props:{name:"from_pretrained",anchor:"transformers.ProcessorMixin.from_pretrained",parameters:[{name:"pretrained_model_name_or_path",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15794/src/transformers/processing_utils.py#L157",parametersDescription:[{anchor:"transformers.ProcessorMixin.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
This can be either:</p>
<ul>
<li>a string, the <em>model id</em> of a pretrained feature_extractor hosted inside a model repo on
huggingface.co. Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or
namespaced under a user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>a path to a <em>directory</em> containing a feature extractor file saved using the
<a href="/docs/transformers/pr_15794/en/main_classes/feature_extractor#transformers.FeatureExtractionMixin.save_pretrained">save_pretrained()</a> method, e.g., <code>./my_model_directory/</code>.</li>
<li>a path or url to a saved feature extractor JSON <em>file</em>, e.g.,
<code>./my_model_directory/preprocessor_config.json</code>.
**kwargs &#x2014;
Additional keyword arguments passed along to both
<a href="/docs/transformers/pr_15794/en/main_classes/feature_extractor#transformers.FeatureExtractionMixin.from_pretrained">from_pretrained()</a> and
<code>from_pretrained</code>.</li>
</ul>`,name:"pretrained_model_name_or_path"}]}}),je=new ws({props:{$$slots:{default:[al]},$$scope:{ctx:Y}}}),wt=new F({props:{name:"save_pretrained",anchor:"transformers.ProcessorMixin.save_pretrained",parameters:[{name:"save_directory",val:""},{name:"push_to_hub",val:": bool = False"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15794/src/transformers/processing_utils.py#L95",parametersDescription:[{anchor:"transformers.ProcessorMixin.save_pretrained.save_directory",description:`<strong>save_directory</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Directory where the feature extractor JSON file and the tokenizer files will be saved (directory will
be created if it does not exist).`,name:"save_directory"},{anchor:"transformers.ProcessorMixin.save_pretrained.push_to_hub",description:`<strong>push_to_hub</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to push your processor to the Hugging Face model hub after saving it.</p>
<div class="course-tip course-tip-orange bg-gradient-to-br dark:bg-gradient-to-r before:border-orange-500 dark:before:border-orange-800 from-orange-50 dark:from-gray-900 to-white dark:to-gray-950 border border-orange-50 text-orange-700 dark:text-gray-400">
						
<p>Using <code>push_to_hub=True</code> will synchronize the repository you are pushing to with <code>save_directory</code>,
which requires <code>save_directory</code> to be a local clone of the repo you are pushing to if it&#x2019;s an existing
folder. Pass along <code>temp_dir=True</code> to use a temporary directory instead.</p>

					</div>
<p>kwargs &#x2014;
Additional key word arguments passed along to the <a href="/docs/transformers/pr_15794/en/main_classes/model#transformers.file_utils.PushToHubMixin.push_to_hub">push_to_hub()</a> method.`,name:"push_to_hub"}]}}),Fe=new ws({props:{$$slots:{default:[il]},$$scope:{ctx:Y}}}),$t=new F({props:{name:"batch_decode",anchor:"transformers.Speech2TextProcessor.batch_decode",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15794/src/transformers/models/speech_to_text/processing_speech_to_text.py#L55"}}),Et=new F({props:{name:"decode",anchor:"transformers.Speech2TextProcessor.decode",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15794/src/transformers/models/speech_to_text/processing_speech_to_text.py#L62"}}),Pt=new F({props:{name:"as_target_processor",anchor:"transformers.Speech2TextProcessor.as_target_processor",parameters:[],source:"https://github.com/huggingface/transformers/blob/pr_15794/src/transformers/models/speech_to_text/processing_speech_to_text.py#L69"}}),jt=new Te({}),Ft=new F({props:{name:"class transformers.Speech2TextModel",anchor:"transformers.Speech2TextModel",parameters:[{name:"config",val:": Speech2TextConfig"}],source:"https://github.com/huggingface/transformers/blob/pr_15794/src/transformers/models/speech_to_text/modeling_speech_to_text.py#L1124",parametersDescription:[{anchor:"transformers.Speech2TextModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15794/en/model_doc/speech_to_text#transformers.Speech2TextConfig">Speech2TextConfig</a>) &#x2014;
Model configuration class with all the parameters of the model. Initializing with a config file does not
load the weights associated with the model, only the configuration. Check out the
<a href="/docs/transformers/pr_15794/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),Dt=new F({props:{name:"forward",anchor:"transformers.Speech2TextModel.forward",parameters:[{name:"input_features",val:" = None"},{name:"attention_mask",val:" = None"},{name:"decoder_input_ids",val:" = None"},{name:"decoder_attention_mask",val:" = None"},{name:"head_mask",val:" = None"},{name:"decoder_head_mask",val:" = None"},{name:"cross_attn_head_mask",val:" = None"},{name:"encoder_outputs",val:" = None"},{name:"past_key_values",val:" = None"},{name:"decoder_inputs_embeds",val:" = None"},{name:"use_cache",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"}],source:"https://github.com/huggingface/transformers/blob/pr_15794/src/transformers/models/speech_to_text/modeling_speech_to_text.py#L1146",parametersDescription:[{anchor:"transformers.Speech2TextModel.forward.input_features",description:`<strong>input_features</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length, feature_size)</code>) &#x2014;
Float values of fbank features extracted from the raw speech waveform. Raw speech waveform can be obtained
by loading a <code>.flac</code> or <code>.wav</code> audio file into an array of type <code>List[float]</code> or a <code>numpy.ndarray</code>, <em>e.g.</em>
via the soundfile library (<code>pip install soundfile</code>). To prepare the array into <code>input_features</code>, the
<a href="/docs/transformers/pr_15794/en/model_doc/speech_to_text#transformers.Speech2TextTokenizer">Speech2TextTokenizer</a> should be used for extracting the fbank features, padding and conversion into a
tensor of type <code>torch.FloatTensor</code>. See <a href="/docs/transformers/pr_15794/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__"><strong>call</strong>()</a>`,name:"input_features"},{anchor:"transformers.Speech2TextModel.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing convolution and attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.Speech2TextModel.forward.decoder_input_ids",description:`<strong>decoder_input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, target_sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of decoder input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <code>SpeechToTextTokenizer</code>. See <a href="/docs/transformers/pr_15794/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/pr_15794/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#decoder-input-ids">What are decoder input IDs?</a></p>
<p>SpeechToText uses the <code>eos_token_id</code> as the starting token for <code>decoder_input_ids</code> generation. If
<code>past_key_values</code> is used, optionally only the last <code>decoder_input_ids</code> have to be input (see
<code>past_key_values</code>).`,name:"decoder_input_ids"},{anchor:"transformers.Speech2TextModel.forward.decoder_attention_mask",description:`<strong>decoder_attention_mask</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, target_sequence_length)</code>, <em>optional</em>) &#x2014;
Default behavior: generate a tensor that ignores pad tokens in <code>decoder_input_ids</code>. Causal mask will also
be used by default.</p>
<p>If you want to change padding behavior, you should read <code>modeling_speech_to_text._prepare_decoder_inputs</code>
and modify to your needs. See diagram 1 in <a href="https://arxiv.org/abs/1910.13461" rel="nofollow">the paper</a> for more
information on the default strategy.`,name:"decoder_attention_mask"},{anchor:"transformers.Speech2TextModel.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.Tensor</code> of shape <code>(encoder_layers, encoder_attention_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the attention modules in the encoder. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.Speech2TextModel.forward.decoder_head_mask",description:`<strong>decoder_head_mask</strong> (<code>torch.Tensor</code> of shape <code>(decoder_layers, decoder_attention_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the attention modules in the decoder. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"decoder_head_mask"},{anchor:"transformers.Speech2TextModel.forward.cross_attn_head_mask",description:`<strong>cross_attn_head_mask</strong> (<code>torch.Tensor</code> of shape <code>(decoder_layers, decoder_attention_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the cross-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"cross_attn_head_mask"},{anchor:"transformers.Speech2TextModel.forward.encoder_outputs",description:`<strong>encoder_outputs</strong> (<code>tuple(tuple(torch.FloatTensor)</code>, <em>optional</em>) &#x2014;
Tuple consists of (<code>last_hidden_state</code>, <em>optional</em>: <code>hidden_states</code>, <em>optional</em>: <code>attentions</code>)
<code>last_hidden_state</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) is a sequence of
hidden-states at the output of the last layer of the encoder. Used in the cross-attention of the decoder.`,name:"encoder_outputs"},{anchor:"transformers.Speech2TextModel.forward.past_key_values",description:`<strong>past_key_values</strong> (<code>tuple(tuple(torch.FloatTensor))</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) &#x2014;
Tuple of <code>tuple(torch.FloatTensor)</code> of length <code>config.n_layers</code>, with each tuple having 2 tensors of shape
<code>(batch_size, num_heads, sequence_length, embed_size_per_head)</code>) and 2 additional tensors of shape
<code>(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)</code>.</p>
<p>Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
blocks) that can be used (see <code>past_key_values</code> input) to speed up sequential decoding.</p>
<p>If <code>past_key_values</code> are used, the user can optionally input only the last <code>decoder_input_ids</code> (those that
don&#x2019;t have their past key value states given to this model) of shape <code>(batch_size, 1)</code> instead of all
\`<code>decoder_input_ids\`\`\` of shape </code>(batch_size, sequence_length)<code>. decoder_inputs_embeds (</code>torch.FloatTensor<code>of shape</code>(batch_size, target_sequence_length, hidden_size)<code>, *optional*): Optionally, instead of passing </code>decoder_input_ids<code>you can choose to directly pass an embedded representation. If</code>past_key_values<code>is used, optionally only the last</code>decoder_inputs_embeds<code>have to be input (see</code>past_key_values<code>). This is useful if you want more control over how to convert </code>decoder_input_ids\` indices into associated vectors
than the model&#x2019;s internal embedding lookup matrix.</p>
<p>If <code>decoder_input_ids</code> and <code>decoder_inputs_embeds</code> are both unset, <code>decoder_inputs_embeds</code> takes the value
of <code>inputs_embeds</code>.`,name:"past_key_values"},{anchor:"transformers.Speech2TextModel.forward.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
If set to <code>True</code>, <code>past_key_values</code> key value states are returned and can be used to speed up decoding (see
<code>past_key_values</code>).`,name:"use_cache"},{anchor:"transformers.Speech2TextModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.Speech2TextModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.Speech2TextModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_15794/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],returnDescription:`
<p>A <a
  href="/docs/transformers/pr_15794/en/main_classes/output#transformers.modeling_outputs.Seq2SeqModelOutput"
>transformers.modeling_outputs.Seq2SeqModelOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/pr_15794/en/model_doc/speech_to_text#transformers.Speech2TextConfig"
>Speech2TextConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) \u2014 Sequence of hidden-states at the output of the last layer of the decoder of the model.</p>
<p>If <code>past_key_values</code> is used only the last hidden-state of the sequences of shape <code>(batch_size, 1, hidden_size)</code> is output.</p>
</li>
<li>
<p><strong>past_key_values</strong> (<code>tuple(tuple(torch.FloatTensor))</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) \u2014 Tuple of <code>tuple(torch.FloatTensor)</code> of length <code>config.n_layers</code>, with each tuple having 2 tensors of shape
<code>(batch_size, num_heads, sequence_length, embed_size_per_head)</code>) and 2 additional tensors of shape
<code>(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)</code>.</p>
<p>Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
blocks) that can be used (see <code>past_key_values</code> input) to speed up sequential decoding.</p>
</li>
<li>
<p><strong>decoder_hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the decoder at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>decoder_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the decoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.</p>
</li>
<li>
<p><strong>cross_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the decoder\u2019s cross-attention layer, after the attention softmax, used to compute the
weighted average in the cross-attention heads.</p>
</li>
<li>
<p><strong>encoder_last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) \u2014 Sequence of hidden-states at the output of the last layer of the encoder of the model.</p>
</li>
<li>
<p><strong>encoder_hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the encoder at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>encoder_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the encoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/pr_15794/en/main_classes/output#transformers.modeling_outputs.Seq2SeqModelOutput"
>transformers.modeling_outputs.Seq2SeqModelOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Ie=new ws({props:{$$slots:{default:[cl]},$$scope:{ctx:Y}}}),It=new Sn({props:{code:`from transformers import Speech2TextTokenizer, Speech2TextModel
import torch

tokenizer = Speech2TextTokenizer.from_pretrained("facebook/s2t-small-librispeech-asr")
model = Speech2TextModel.from_pretrained("facebook/s2t-small-librispeech-asr")

inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
outputs = model(**inputs)

last_hidden_states = outputs.last_hidden_state,`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> Speech2TextTokenizer, Speech2TextModel
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = Speech2TextTokenizer.from_pretrained(<span class="hljs-string">&quot;facebook/s2t-small-librispeech-asr&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = Speech2TextModel.from_pretrained(<span class="hljs-string">&quot;facebook/s2t-small-librispeech-asr&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state`}}),Nt=new Te({}),Lt=new F({props:{name:"class transformers.Speech2TextForConditionalGeneration",anchor:"transformers.Speech2TextForConditionalGeneration",parameters:[{name:"config",val:": Speech2TextConfig"}],source:"https://github.com/huggingface/transformers/blob/pr_15794/src/transformers/models/speech_to_text/modeling_speech_to_text.py#L1237",parametersDescription:[{anchor:"transformers.Speech2TextForConditionalGeneration.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15794/en/model_doc/speech_to_text#transformers.Speech2TextConfig">Speech2TextConfig</a>) &#x2014;
Model configuration class with all the parameters of the model. Initializing with a config file does not
load the weights associated with the model, only the configuration. Check out the
<a href="/docs/transformers/pr_15794/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),Vt=new F({props:{name:"forward",anchor:"transformers.Speech2TextForConditionalGeneration.forward",parameters:[{name:"input_features",val:" = None"},{name:"attention_mask",val:" = None"},{name:"decoder_input_ids",val:" = None"},{name:"decoder_attention_mask",val:" = None"},{name:"head_mask",val:" = None"},{name:"decoder_head_mask",val:" = None"},{name:"cross_attn_head_mask",val:" = None"},{name:"encoder_outputs",val:" = None"},{name:"past_key_values",val:" = None"},{name:"decoder_inputs_embeds",val:" = None"},{name:"labels",val:" = None"},{name:"use_cache",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"}],source:"https://github.com/huggingface/transformers/blob/pr_15794/src/transformers/models/speech_to_text/modeling_speech_to_text.py#L1274",parametersDescription:[{anchor:"transformers.Speech2TextForConditionalGeneration.forward.input_features",description:`<strong>input_features</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length, feature_size)</code>) &#x2014;
Float values of fbank features extracted from the raw speech waveform. Raw speech waveform can be obtained
by loading a <code>.flac</code> or <code>.wav</code> audio file into an array of type <code>List[float]</code> or a <code>numpy.ndarray</code>, <em>e.g.</em>
via the soundfile library (<code>pip install soundfile</code>). To prepare the array into <code>input_features</code>, the
<a href="/docs/transformers/pr_15794/en/model_doc/speech_to_text#transformers.Speech2TextTokenizer">Speech2TextTokenizer</a> should be used for extracting the fbank features, padding and conversion into a
tensor of type <code>torch.FloatTensor</code>. See <a href="/docs/transformers/pr_15794/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__"><strong>call</strong>()</a>`,name:"input_features"},{anchor:"transformers.Speech2TextForConditionalGeneration.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing convolution and attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.Speech2TextForConditionalGeneration.forward.decoder_input_ids",description:`<strong>decoder_input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, target_sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of decoder input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <code>SpeechToTextTokenizer</code>. See <a href="/docs/transformers/pr_15794/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/pr_15794/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#decoder-input-ids">What are decoder input IDs?</a></p>
<p>SpeechToText uses the <code>eos_token_id</code> as the starting token for <code>decoder_input_ids</code> generation. If
<code>past_key_values</code> is used, optionally only the last <code>decoder_input_ids</code> have to be input (see
<code>past_key_values</code>).`,name:"decoder_input_ids"},{anchor:"transformers.Speech2TextForConditionalGeneration.forward.decoder_attention_mask",description:`<strong>decoder_attention_mask</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, target_sequence_length)</code>, <em>optional</em>) &#x2014;
Default behavior: generate a tensor that ignores pad tokens in <code>decoder_input_ids</code>. Causal mask will also
be used by default.</p>
<p>If you want to change padding behavior, you should read <code>modeling_speech_to_text._prepare_decoder_inputs</code>
and modify to your needs. See diagram 1 in <a href="https://arxiv.org/abs/1910.13461" rel="nofollow">the paper</a> for more
information on the default strategy.`,name:"decoder_attention_mask"},{anchor:"transformers.Speech2TextForConditionalGeneration.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.Tensor</code> of shape <code>(encoder_layers, encoder_attention_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the attention modules in the encoder. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.Speech2TextForConditionalGeneration.forward.decoder_head_mask",description:`<strong>decoder_head_mask</strong> (<code>torch.Tensor</code> of shape <code>(decoder_layers, decoder_attention_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the attention modules in the decoder. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"decoder_head_mask"},{anchor:"transformers.Speech2TextForConditionalGeneration.forward.cross_attn_head_mask",description:`<strong>cross_attn_head_mask</strong> (<code>torch.Tensor</code> of shape <code>(decoder_layers, decoder_attention_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the cross-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"cross_attn_head_mask"},{anchor:"transformers.Speech2TextForConditionalGeneration.forward.encoder_outputs",description:`<strong>encoder_outputs</strong> (<code>tuple(tuple(torch.FloatTensor)</code>, <em>optional</em>) &#x2014;
Tuple consists of (<code>last_hidden_state</code>, <em>optional</em>: <code>hidden_states</code>, <em>optional</em>: <code>attentions</code>)
<code>last_hidden_state</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) is a sequence of
hidden-states at the output of the last layer of the encoder. Used in the cross-attention of the decoder.`,name:"encoder_outputs"},{anchor:"transformers.Speech2TextForConditionalGeneration.forward.past_key_values",description:`<strong>past_key_values</strong> (<code>tuple(tuple(torch.FloatTensor))</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) &#x2014;
Tuple of <code>tuple(torch.FloatTensor)</code> of length <code>config.n_layers</code>, with each tuple having 2 tensors of shape
<code>(batch_size, num_heads, sequence_length, embed_size_per_head)</code>) and 2 additional tensors of shape
<code>(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)</code>.</p>
<p>Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
blocks) that can be used (see <code>past_key_values</code> input) to speed up sequential decoding.</p>
<p>If <code>past_key_values</code> are used, the user can optionally input only the last <code>decoder_input_ids</code> (those that
don&#x2019;t have their past key value states given to this model) of shape <code>(batch_size, 1)</code> instead of all
\`<code>decoder_input_ids\`\`\` of shape </code>(batch_size, sequence_length)<code>. decoder_inputs_embeds (</code>torch.FloatTensor<code>of shape</code>(batch_size, target_sequence_length, hidden_size)<code>, *optional*): Optionally, instead of passing </code>decoder_input_ids<code>you can choose to directly pass an embedded representation. If</code>past_key_values<code>is used, optionally only the last</code>decoder_inputs_embeds<code>have to be input (see</code>past_key_values<code>). This is useful if you want more control over how to convert </code>decoder_input_ids\` indices into associated vectors
than the model&#x2019;s internal embedding lookup matrix.</p>
<p>If <code>decoder_input_ids</code> and <code>decoder_inputs_embeds</code> are both unset, <code>decoder_inputs_embeds</code> takes the value
of <code>inputs_embeds</code>.`,name:"past_key_values"},{anchor:"transformers.Speech2TextForConditionalGeneration.forward.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
If set to <code>True</code>, <code>past_key_values</code> key value states are returned and can be used to speed up decoding (see
<code>past_key_values</code>).`,name:"use_cache"},{anchor:"transformers.Speech2TextForConditionalGeneration.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.Speech2TextForConditionalGeneration.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.Speech2TextForConditionalGeneration.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_15794/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.Speech2TextForConditionalGeneration.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Labels for computing the language modeling loss. Indices should either be in <code>[0, ..., config.vocab_size]</code>
or -100 (see <code>input_ids</code> docstring). Tokens with indices set to <code>-100</code> are ignored (masked), the loss is
only computed for the tokens with labels in <code>[0, ..., config.vocab_size]</code>.`,name:"labels"}],returnDescription:`
<p>A <a
  href="/docs/transformers/pr_15794/en/main_classes/output#transformers.modeling_outputs.Seq2SeqLMOutput"
>transformers.modeling_outputs.Seq2SeqLMOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/pr_15794/en/model_doc/speech_to_text#transformers.Speech2TextConfig"
>Speech2TextConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) \u2014 Language modeling loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, config.vocab_size)</code>) \u2014 Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).</p>
</li>
<li>
<p><strong>past_key_values</strong> (<code>tuple(tuple(torch.FloatTensor))</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) \u2014 Tuple of <code>tuple(torch.FloatTensor)</code> of length <code>config.n_layers</code>, with each tuple having 2 tensors of shape
<code>(batch_size, num_heads, sequence_length, embed_size_per_head)</code>) and 2 additional tensors of shape
<code>(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)</code>.</p>
<p>Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
blocks) that can be used (see <code>past_key_values</code> input) to speed up sequential decoding.</p>
</li>
<li>
<p><strong>decoder_hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the decoder at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>decoder_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the decoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.</p>
</li>
<li>
<p><strong>cross_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the decoder\u2019s cross-attention layer, after the attention softmax, used to compute the
weighted average in the cross-attention heads.</p>
</li>
<li>
<p><strong>encoder_last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) \u2014 Sequence of hidden-states at the output of the last layer of the encoder of the model.</p>
</li>
<li>
<p><strong>encoder_hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the encoder at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>encoder_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the encoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/pr_15794/en/main_classes/output#transformers.modeling_outputs.Seq2SeqLMOutput"
>transformers.modeling_outputs.Seq2SeqLMOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Le=new ws({props:{$$slots:{default:[dl]},$$scope:{ctx:Y}}}),Ut=new Sn({props:{code:`import torch
from transformers import Speech2TextProcessor, Speech2TextForConditionalGeneration
from datasets import load_dataset
import soundfile as sf

model = Speech2TextForConditionalGeneration.from_pretrained("facebook/s2t-small-librispeech-asr")
processor = Speech2TextProcessor.from_pretrained("facebook/s2t-small-librispeech-asr")


def map_to_array(batch):
    speech, _ = sf.read(batch["file"])
    batch["speech"] = speech
    return batch


ds = load_dataset("hf-internal-testing/librispeech_asr_dummy", "clean", split="validation")
ds = ds.map(map_to_array)

input_features = processor(
    ds["speech"][0], sampling_rate=16000, return_tensors="pt"
).input_features  # Batch size 1
generated_ids = model.generate(inputs=input_features)

transcription = processor.batch_decode(generated_ids),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> Speech2TextProcessor, Speech2TextForConditionalGeneration
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> soundfile <span class="hljs-keyword">as</span> sf

<span class="hljs-meta">&gt;&gt;&gt; </span>model = Speech2TextForConditionalGeneration.from_pretrained(<span class="hljs-string">&quot;facebook/s2t-small-librispeech-asr&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>processor = Speech2TextProcessor.from_pretrained(<span class="hljs-string">&quot;facebook/s2t-small-librispeech-asr&quot;</span>)


<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">def</span> <span class="hljs-title function_">map_to_array</span>(<span class="hljs-params">batch</span>):
<span class="hljs-meta">... </span>    speech, _ = sf.read(batch[<span class="hljs-string">&quot;file&quot;</span>])
<span class="hljs-meta">... </span>    batch[<span class="hljs-string">&quot;speech&quot;</span>] = speech
<span class="hljs-meta">... </span>    <span class="hljs-keyword">return</span> batch


<span class="hljs-meta">&gt;&gt;&gt; </span>ds = load_dataset(<span class="hljs-string">&quot;hf-internal-testing/librispeech_asr_dummy&quot;</span>, <span class="hljs-string">&quot;clean&quot;</span>, split=<span class="hljs-string">&quot;validation&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>ds = ds.<span class="hljs-built_in">map</span>(map_to_array)

<span class="hljs-meta">&gt;&gt;&gt; </span>input_features = processor(
<span class="hljs-meta">... </span>    ds[<span class="hljs-string">&quot;speech&quot;</span>][<span class="hljs-number">0</span>], sampling_rate=<span class="hljs-number">16000</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>).input_features  <span class="hljs-comment"># Batch size 1</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>generated_ids = model.generate(inputs=input_features)

<span class="hljs-meta">&gt;&gt;&gt; </span>transcription = processor.batch_decode(generated_ids)`}}),{c(){m=r("meta"),S=d(),f=r("h1"),k=r("a"),z=r("span"),u(b.$$.fragment),y=d(),E=r("span"),C=n("Speech2Text"),$=d(),M=r("h2"),L=r("a"),So=r("span"),u(Ve.$$.fragment),Ss=d(),$o=r("span"),$s=n("Overview"),$n=d(),O=r("p"),zs=n("The Speech2Text model was proposed in "),Ue=r("a"),Es=n("fairseq S2T: Fast Speech-to-Text Modeling with fairseq"),qs=n(` by Changhan Wang, Yun Tang, Xutai Ma, Anne Wu, Dmytro Okhonko, Juan Pino. It\u2019s a
transformer-based seq2seq (encoder-decoder) model designed for end-to-end Automatic Speech Recognition (ASR) and Speech
Translation (ST). It uses a convolutional downsampler to reduce the length of speech inputs by 3/4th before they are
fed into the encoder. The model is trained with standard autoregressive cross-entropy loss and generates the
transcripts/translations autoregressively. Speech2Text has been fine-tuned on several datasets for ASR and ST:
`),Be=r("a"),Ps=n("LibriSpeech"),js=n(", "),Re=r("a"),Fs=n("CoVoST 2"),Cs=n(", "),He=r("a"),Ms=n("MuST-C"),As=n("."),zn=d(),Q=r("p"),Ds=n("This model was contributed by "),Je=r("a"),Is=n("valhalla"),Ns=n(". The original code can be found "),Ye=r("a"),Ls=n("here"),Os=n("."),En=d(),ce=r("h2"),xe=r("a"),zo=r("span"),u(Xe.$$.fragment),Gs=d(),Eo=r("span"),Ws=n("Inference"),qn=d(),be=r("p"),Vs=n(`Speech2Text is a speech model that accepts a float tensor of log-mel filter-bank features extracted from the speech
signal. It\u2019s a transformer-based seq2seq model, so the transcripts/translations are generated autoregressively. The
`),qo=r("code"),Us=n("generate()"),Bs=n(" method can be used for inference."),Pn=d(),G=r("p"),Rs=n("The "),Rt=r("a"),Hs=n("Speech2TextFeatureExtractor"),Js=n(` class is responsible for extracting the log-mel filter-bank
features. The `),Ht=r("a"),Ys=n("Speech2TextProcessor"),Xs=n(" wraps "),Jt=r("a"),Ks=n("Speech2TextFeatureExtractor"),Qs=n(` and
`),Yt=r("a"),Zs=n("Speech2TextTokenizer"),er=n(` into a single instance to both extract the input features and decode the
predicted token ids.`),jn=d(),q=r("p"),tr=n("The feature extractor depends on "),Po=r("code"),or=n("torchaudio"),nr=n(" and the tokenizer depends on "),jo=r("code"),sr=n("sentencepiece"),rr=n(` so be sure to
install those packages before running the examples. You could either install those as extra speech dependencies with
`),Fo=r("code"),ar=n('pip install transformers"[speech, sentencepiece]"'),ir=n(" or install the packages seperately with "),Co=r("code"),cr=n("pip install torchaudio sentencepiece"),dr=n(". Also "),Mo=r("code"),lr=n("torchaudio"),pr=n(" requires the development version of the "),Ke=r("a"),hr=n("libsndfile"),mr=n(` package which can be installed via a system package manager. On Ubuntu it can
be installed as follows: `),Ao=r("code"),fr=n("apt install libsndfile1-dev"),Fn=d(),Xt=r("ul"),Do=r("li"),ur=n("ASR and Speech Translation"),Cn=d(),u(Qe.$$.fragment),Mn=d(),Kt=r("ul"),Ze=r("li"),Io=r("p"),_r=n("Multilingual speech translation"),gr=d(),A=r("p"),vr=n("For multilingual speech translation models, "),No=r("code"),Tr=n("eos_token_id"),xr=n(" is used as the "),Lo=r("code"),br=n("decoder_start_token_id"),kr=n(` and
the target language id is forced as the first generated token. To force the target language id as the first
generated token, pass the `),Oo=r("code"),yr=n("forced_bos_token_id"),wr=n(" parameter to the "),Go=r("code"),Sr=n("generate()"),$r=n(` method. The following
example shows how to transate English speech to French text using the `),Wo=r("em"),zr=n("facebook/s2t-medium-mustc-multilingual-st"),Er=n(`
checkpoint.`),An=d(),u(et.$$.fragment),Dn=d(),ke=r("p"),qr=n("See the "),tt=r("a"),Pr=n("model hub"),jr=n(" to look for Speech2Text checkpoints."),In=d(),de=r("h2"),ye=r("a"),Vo=r("span"),u(ot.$$.fragment),Fr=d(),Uo=r("span"),Cr=n("Speech2TextConfig"),Nn=d(),D=r("div"),u(nt.$$.fragment),Mr=d(),le=r("p"),Ar=n("This is the configuration class to store the configuration of a "),Qt=r("a"),Dr=n("Speech2TextModel"),Ir=n(`. It is used to instantiate an
Speech2Text model according to the specified arguments, defining the model architecture. Instantiating a
configuration with the defaults will yield a similar configuration to that of the Speech2Text
`),st=r("a"),Nr=n("facebook/s2t-small-librispeech-asr"),Lr=n(" architecture."),Or=d(),pe=r("p"),Gr=n("Configuration objects inherit from "),Zt=r("a"),Wr=n("PretrainedConfig"),Vr=n(` and can be used to control the model outputs. Read the
documentation from `),eo=r("a"),Ur=n("PretrainedConfig"),Br=n(" for more information."),Rr=d(),Bo=r("p"),Hr=n("Example:"),Jr=d(),u(rt.$$.fragment),Ln=d(),he=r("h2"),we=r("a"),Ro=r("span"),u(at.$$.fragment),Yr=d(),Ho=r("span"),Xr=n("Speech2TextTokenizer"),On=d(),P=r("div"),u(it.$$.fragment),Kr=d(),Jo=r("p"),Qr=n("Construct an Speech2Text tokenizer."),Zr=d(),ct=r("p"),ea=n("This tokenizer inherits from "),to=r("a"),ta=n("PreTrainedTokenizer"),oa=n(` which contains some of the main methods. Users should refer to
the superclass for more information regarding such methods.`),na=d(),Se=r("div"),u(dt.$$.fragment),sa=d(),Yo=r("p"),ra=n("Build model inputs from a sequence by appending eos_token_id."),aa=d(),$e=r("div"),u(lt.$$.fragment),ia=d(),pt=r("p"),ca=n(`Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding
special tokens using the tokenizer `),Xo=r("code"),da=n("prepare_for_model"),la=n(" method."),pa=d(),Z=r("div"),u(ht.$$.fragment),ha=d(),oo=r("p"),ma=n("Create the token type IDs corresponding to the sequences passed. "),no=r("a"),fa=n(`What are token type
IDs?`),ua=d(),Ko=r("p"),_a=n("Should be overridden in a subclass if the model has a special way of building those."),ga=d(),Qo=r("div"),Gn=d(),me=r("h2"),ze=r("a"),Zo=r("span"),u(mt.$$.fragment),va=d(),en=r("span"),Ta=n("Speech2TextFeatureExtractor"),Wn=d(),I=r("div"),u(ft.$$.fragment),xa=d(),tn=r("p"),ba=n("Constructs a Speech2Text feature extractor."),ka=d(),ut=r("p"),ya=n("This feature extractor inherits from "),so=r("a"),wa=n("Speech2TextFeatureExtractor"),Sa=n(` which contains most of the main methods. Users
should refer to this superclass for more information regarding those methods.`),$a=d(),on=r("p"),za=n(`This class extracts mel-filter bank features from raw speech using TorchAudio and applies utterance-level cepstral
mean and variance normalization to the extracted features.`),Ea=d(),Ee=r("div"),u(_t.$$.fragment),qa=d(),nn=r("p"),Pa=n("Main method to featurize and prepare for the model one or several sequence(s). sequences."),Vn=d(),fe=r("h2"),qe=r("a"),sn=r("span"),u(gt.$$.fragment),ja=d(),rn=r("span"),Fa=n("Speech2TextProcessor"),Un=d(),w=r("div"),u(vt.$$.fragment),Ca=d(),an=r("p"),Ma=n(`Constructs a Speech2Text processor which wraps a Speech2Text feature extractor and a Speech2Text tokenizer into a
single processor.`),Aa=d(),W=r("p"),ro=r("a"),Da=n("Speech2TextProcessor"),Ia=n(" offers all the functionalities of "),ao=r("a"),Na=n("Speech2TextFeatureExtractor"),La=n(` and
`),io=r("a"),Oa=n("Speech2TextTokenizer"),Ga=n(". See the "),Tt=r("a"),cn=r("strong"),Wa=n("call"),Va=n("()"),Ua=n(" and "),co=r("a"),Ba=n("decode()"),Ra=n(` for more
information.`),Ha=d(),Pe=r("div"),u(xt.$$.fragment),Ja=d(),X=r("p"),Ya=n(`When used in normal mode, this method forwards all its arguments to Speech2TextFeatureExtractor\u2019s
`),bt=r("a"),dn=r("strong"),Xa=n("call"),Ka=n("()"),Qa=n(` and returns its output. If used in the context
`),lo=r("a"),Za=n("as_target_processor()"),ei=n(` this method forwards all its arguments to Speech2TextTokenizer\u2019s
`),kt=r("a"),ln=r("strong"),ti=n("call"),oi=n("()"),ni=n(`. Please refer to the doctsring of the above two methods for more
information.`),si=d(),ee=r("div"),u(yt.$$.fragment),ri=d(),pn=r("p"),ai=n("Instantiate a processor associated with a pretrained model."),ii=d(),u(je.$$.fragment),ci=d(),te=r("div"),u(wt.$$.fragment),di=d(),St=r("p"),li=n(`Saves the attributes of this processor (feature extractor, tokenizer\u2026) in the specified directory so that it
can be reloaded using the `),hn=r("code"),pi=n("from_pretrained()"),hi=n("method."),mi=d(),u(Fe.$$.fragment),fi=d(),Ce=r("div"),u($t.$$.fragment),ui=d(),zt=r("p"),_i=n("This method forwards all its arguments to Speech2TextTokenizer\u2019s "),po=r("a"),gi=n("batch_decode()"),vi=n(`. Please
refer to the docstring of this method for more information.`),Ti=d(),Me=r("div"),u(Et.$$.fragment),xi=d(),qt=r("p"),bi=n("This method forwards all its arguments to Speech2TextTokenizer\u2019s "),ho=r("a"),ki=n("decode()"),yi=n(`. Please refer
to the docstring of this method for more information.`),wi=d(),Ae=r("div"),u(Pt.$$.fragment),Si=d(),mn=r("p"),$i=n(`Temporarily sets the tokenizer for processing the input. Useful for encoding the labels when fine-tuning
Speech2Text.`),Bn=d(),ue=r("h2"),De=r("a"),fn=r("span"),u(jt.$$.fragment),zi=d(),un=r("span"),Ei=n("Speech2TextModel"),Rn=d(),R=r("div"),u(Ft.$$.fragment),qi=d(),Ct=r("p"),Pi=n(`The bare Speech2Text Model outputting raw hidden-states without any specific head on top.
This model inherits from `),mo=r("a"),ji=n("PreTrainedModel"),Fi=n(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Ci=d(),Mt=r("p"),Mi=n("This model is also a PyTorch "),At=r("a"),Ai=n("torch.nn.Module"),Di=n(` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),Ii=d(),V=r("div"),u(Dt.$$.fragment),Ni=d(),_e=r("p"),Li=n("The "),fo=r("a"),Oi=n("Speech2TextModel"),Gi=n(" forward method, overrides the "),_n=r("code"),Wi=n("__call__"),Vi=n(" special method."),Ui=d(),u(Ie.$$.fragment),Bi=d(),gn=r("p"),Ri=n("Example:"),Hi=d(),u(It.$$.fragment),Hn=d(),ge=r("h2"),Ne=r("a"),vn=r("span"),u(Nt.$$.fragment),Ji=d(),Tn=r("span"),Yi=n("Speech2TextForConditionalGeneration"),Jn=d(),H=r("div"),u(Lt.$$.fragment),Xi=d(),Ot=r("p"),Ki=n(`The Speech2Text Model with a language modeling head. Can be used for summarization.
This model inherits from `),uo=r("a"),Qi=n("PreTrainedModel"),Zi=n(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),ec=d(),Gt=r("p"),tc=n("This model is also a PyTorch "),Wt=r("a"),oc=n("torch.nn.Module"),nc=n(` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),sc=d(),U=r("div"),u(Vt.$$.fragment),rc=d(),ve=r("p"),ac=n("The "),_o=r("a"),ic=n("Speech2TextForConditionalGeneration"),cc=n(" forward method, overrides the "),xn=r("code"),dc=n("__call__"),lc=n(" special method."),pc=d(),u(Le.$$.fragment),hc=d(),bn=r("p"),mc=n("Example:"),fc=d(),u(Ut.$$.fragment),this.h()},l(t){const p=rl('[data-svelte="svelte-1phssyn"]',document.head);m=a(p,"META",{name:!0,content:!0}),p.forEach(o),S=l(t),f=a(t,"H1",{class:!0});var Bt=i(f);k=a(Bt,"A",{id:!0,class:!0,href:!0});var kn=i(k);z=a(kn,"SPAN",{});var yn=i(z);_(b.$$.fragment,yn),yn.forEach(o),kn.forEach(o),y=l(Bt),E=a(Bt,"SPAN",{});var wn=i(E);C=s(wn,"Speech2Text"),wn.forEach(o),Bt.forEach(o),$=l(t),M=a(t,"H2",{class:!0});var Xn=i(M);L=a(Xn,"A",{id:!0,class:!0,href:!0});var Tc=i(L);So=a(Tc,"SPAN",{});var xc=i(So);_(Ve.$$.fragment,xc),xc.forEach(o),Tc.forEach(o),Ss=l(Xn),$o=a(Xn,"SPAN",{});var bc=i($o);$s=s(bc,"Overview"),bc.forEach(o),Xn.forEach(o),$n=l(t),O=a(t,"P",{});var oe=i(O);zs=s(oe,"The Speech2Text model was proposed in "),Ue=a(oe,"A",{href:!0,rel:!0});var kc=i(Ue);Es=s(kc,"fairseq S2T: Fast Speech-to-Text Modeling with fairseq"),kc.forEach(o),qs=s(oe,` by Changhan Wang, Yun Tang, Xutai Ma, Anne Wu, Dmytro Okhonko, Juan Pino. It\u2019s a
transformer-based seq2seq (encoder-decoder) model designed for end-to-end Automatic Speech Recognition (ASR) and Speech
Translation (ST). It uses a convolutional downsampler to reduce the length of speech inputs by 3/4th before they are
fed into the encoder. The model is trained with standard autoregressive cross-entropy loss and generates the
transcripts/translations autoregressively. Speech2Text has been fine-tuned on several datasets for ASR and ST:
`),Be=a(oe,"A",{href:!0,rel:!0});var yc=i(Be);Ps=s(yc,"LibriSpeech"),yc.forEach(o),js=s(oe,", "),Re=a(oe,"A",{href:!0,rel:!0});var wc=i(Re);Fs=s(wc,"CoVoST 2"),wc.forEach(o),Cs=s(oe,", "),He=a(oe,"A",{href:!0,rel:!0});var Sc=i(He);Ms=s(Sc,"MuST-C"),Sc.forEach(o),As=s(oe,"."),oe.forEach(o),zn=l(t),Q=a(t,"P",{});var go=i(Q);Ds=s(go,"This model was contributed by "),Je=a(go,"A",{href:!0,rel:!0});var $c=i(Je);Is=s($c,"valhalla"),$c.forEach(o),Ns=s(go,". The original code can be found "),Ye=a(go,"A",{href:!0,rel:!0});var zc=i(Ye);Ls=s(zc,"here"),zc.forEach(o),Os=s(go,"."),go.forEach(o),En=l(t),ce=a(t,"H2",{class:!0});var Kn=i(ce);xe=a(Kn,"A",{id:!0,class:!0,href:!0});var Ec=i(xe);zo=a(Ec,"SPAN",{});var qc=i(zo);_(Xe.$$.fragment,qc),qc.forEach(o),Ec.forEach(o),Gs=l(Kn),Eo=a(Kn,"SPAN",{});var Pc=i(Eo);Ws=s(Pc,"Inference"),Pc.forEach(o),Kn.forEach(o),qn=l(t),be=a(t,"P",{});var Qn=i(be);Vs=s(Qn,`Speech2Text is a speech model that accepts a float tensor of log-mel filter-bank features extracted from the speech
signal. It\u2019s a transformer-based seq2seq model, so the transcripts/translations are generated autoregressively. The
`),qo=a(Qn,"CODE",{});var jc=i(qo);Us=s(jc,"generate()"),jc.forEach(o),Bs=s(Qn," method can be used for inference."),Qn.forEach(o),Pn=l(t),G=a(t,"P",{});var ne=i(G);Rs=s(ne,"The "),Rt=a(ne,"A",{href:!0});var Fc=i(Rt);Hs=s(Fc,"Speech2TextFeatureExtractor"),Fc.forEach(o),Js=s(ne,` class is responsible for extracting the log-mel filter-bank
features. The `),Ht=a(ne,"A",{href:!0});var Cc=i(Ht);Ys=s(Cc,"Speech2TextProcessor"),Cc.forEach(o),Xs=s(ne," wraps "),Jt=a(ne,"A",{href:!0});var Mc=i(Jt);Ks=s(Mc,"Speech2TextFeatureExtractor"),Mc.forEach(o),Qs=s(ne,` and
`),Yt=a(ne,"A",{href:!0});var Ac=i(Yt);Zs=s(Ac,"Speech2TextTokenizer"),Ac.forEach(o),er=s(ne,` into a single instance to both extract the input features and decode the
predicted token ids.`),ne.forEach(o),jn=l(t),q=a(t,"P",{});var N=i(q);tr=s(N,"The feature extractor depends on "),Po=a(N,"CODE",{});var Dc=i(Po);or=s(Dc,"torchaudio"),Dc.forEach(o),nr=s(N," and the tokenizer depends on "),jo=a(N,"CODE",{});var Ic=i(jo);sr=s(Ic,"sentencepiece"),Ic.forEach(o),rr=s(N,` so be sure to
install those packages before running the examples. You could either install those as extra speech dependencies with
`),Fo=a(N,"CODE",{});var Nc=i(Fo);ar=s(Nc,'pip install transformers"[speech, sentencepiece]"'),Nc.forEach(o),ir=s(N," or install the packages seperately with "),Co=a(N,"CODE",{});var Lc=i(Co);cr=s(Lc,"pip install torchaudio sentencepiece"),Lc.forEach(o),dr=s(N,". Also "),Mo=a(N,"CODE",{});var Oc=i(Mo);lr=s(Oc,"torchaudio"),Oc.forEach(o),pr=s(N," requires the development version of the "),Ke=a(N,"A",{href:!0,rel:!0});var Gc=i(Ke);hr=s(Gc,"libsndfile"),Gc.forEach(o),mr=s(N,` package which can be installed via a system package manager. On Ubuntu it can
be installed as follows: `),Ao=a(N,"CODE",{});var Wc=i(Ao);fr=s(Wc,"apt install libsndfile1-dev"),Wc.forEach(o),N.forEach(o),Fn=l(t),Xt=a(t,"UL",{});var Vc=i(Xt);Do=a(Vc,"LI",{});var Uc=i(Do);ur=s(Uc,"ASR and Speech Translation"),Uc.forEach(o),Vc.forEach(o),Cn=l(t),_(Qe.$$.fragment,t),Mn=l(t),Kt=a(t,"UL",{});var Bc=i(Kt);Ze=a(Bc,"LI",{});var Zn=i(Ze);Io=a(Zn,"P",{});var Rc=i(Io);_r=s(Rc,"Multilingual speech translation"),Rc.forEach(o),gr=l(Zn),A=a(Zn,"P",{});var J=i(A);vr=s(J,"For multilingual speech translation models, "),No=a(J,"CODE",{});var Hc=i(No);Tr=s(Hc,"eos_token_id"),Hc.forEach(o),xr=s(J," is used as the "),Lo=a(J,"CODE",{});var Jc=i(Lo);br=s(Jc,"decoder_start_token_id"),Jc.forEach(o),kr=s(J,` and
the target language id is forced as the first generated token. To force the target language id as the first
generated token, pass the `),Oo=a(J,"CODE",{});var Yc=i(Oo);yr=s(Yc,"forced_bos_token_id"),Yc.forEach(o),wr=s(J," parameter to the "),Go=a(J,"CODE",{});var Xc=i(Go);Sr=s(Xc,"generate()"),Xc.forEach(o),$r=s(J,` method. The following
example shows how to transate English speech to French text using the `),Wo=a(J,"EM",{});var Kc=i(Wo);zr=s(Kc,"facebook/s2t-medium-mustc-multilingual-st"),Kc.forEach(o),Er=s(J,`
checkpoint.`),J.forEach(o),Zn.forEach(o),Bc.forEach(o),An=l(t),_(et.$$.fragment,t),Dn=l(t),ke=a(t,"P",{});var es=i(ke);qr=s(es,"See the "),tt=a(es,"A",{href:!0,rel:!0});var Qc=i(tt);Pr=s(Qc,"model hub"),Qc.forEach(o),jr=s(es," to look for Speech2Text checkpoints."),es.forEach(o),In=l(t),de=a(t,"H2",{class:!0});var ts=i(de);ye=a(ts,"A",{id:!0,class:!0,href:!0});var Zc=i(ye);Vo=a(Zc,"SPAN",{});var ed=i(Vo);_(ot.$$.fragment,ed),ed.forEach(o),Zc.forEach(o),Fr=l(ts),Uo=a(ts,"SPAN",{});var td=i(Uo);Cr=s(td,"Speech2TextConfig"),td.forEach(o),ts.forEach(o),Nn=l(t),D=a(t,"DIV",{class:!0});var se=i(D);_(nt.$$.fragment,se),Mr=l(se),le=a(se,"P",{});var vo=i(le);Ar=s(vo,"This is the configuration class to store the configuration of a "),Qt=a(vo,"A",{href:!0});var od=i(Qt);Dr=s(od,"Speech2TextModel"),od.forEach(o),Ir=s(vo,`. It is used to instantiate an
Speech2Text model according to the specified arguments, defining the model architecture. Instantiating a
configuration with the defaults will yield a similar configuration to that of the Speech2Text
`),st=a(vo,"A",{href:!0,rel:!0});var nd=i(st);Nr=s(nd,"facebook/s2t-small-librispeech-asr"),nd.forEach(o),Lr=s(vo," architecture."),vo.forEach(o),Or=l(se),pe=a(se,"P",{});var To=i(pe);Gr=s(To,"Configuration objects inherit from "),Zt=a(To,"A",{href:!0});var sd=i(Zt);Wr=s(sd,"PretrainedConfig"),sd.forEach(o),Vr=s(To,` and can be used to control the model outputs. Read the
documentation from `),eo=a(To,"A",{href:!0});var rd=i(eo);Ur=s(rd,"PretrainedConfig"),rd.forEach(o),Br=s(To," for more information."),To.forEach(o),Rr=l(se),Bo=a(se,"P",{});var ad=i(Bo);Hr=s(ad,"Example:"),ad.forEach(o),Jr=l(se),_(rt.$$.fragment,se),se.forEach(o),Ln=l(t),he=a(t,"H2",{class:!0});var os=i(he);we=a(os,"A",{id:!0,class:!0,href:!0});var id=i(we);Ro=a(id,"SPAN",{});var cd=i(Ro);_(at.$$.fragment,cd),cd.forEach(o),id.forEach(o),Yr=l(os),Ho=a(os,"SPAN",{});var dd=i(Ho);Xr=s(dd,"Speech2TextTokenizer"),dd.forEach(o),os.forEach(o),On=l(t),P=a(t,"DIV",{class:!0});var B=i(P);_(it.$$.fragment,B),Kr=l(B),Jo=a(B,"P",{});var ld=i(Jo);Qr=s(ld,"Construct an Speech2Text tokenizer."),ld.forEach(o),Zr=l(B),ct=a(B,"P",{});var ns=i(ct);ea=s(ns,"This tokenizer inherits from "),to=a(ns,"A",{href:!0});var pd=i(to);ta=s(pd,"PreTrainedTokenizer"),pd.forEach(o),oa=s(ns,` which contains some of the main methods. Users should refer to
the superclass for more information regarding such methods.`),ns.forEach(o),na=l(B),Se=a(B,"DIV",{class:!0});var ss=i(Se);_(dt.$$.fragment,ss),sa=l(ss),Yo=a(ss,"P",{});var hd=i(Yo);ra=s(hd,"Build model inputs from a sequence by appending eos_token_id."),hd.forEach(o),ss.forEach(o),aa=l(B),$e=a(B,"DIV",{class:!0});var rs=i($e);_(lt.$$.fragment,rs),ia=l(rs),pt=a(rs,"P",{});var as=i(pt);ca=s(as,`Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding
special tokens using the tokenizer `),Xo=a(as,"CODE",{});var md=i(Xo);da=s(md,"prepare_for_model"),md.forEach(o),la=s(as," method."),as.forEach(o),rs.forEach(o),pa=l(B),Z=a(B,"DIV",{class:!0});var xo=i(Z);_(ht.$$.fragment,xo),ha=l(xo),oo=a(xo,"P",{});var uc=i(oo);ma=s(uc,"Create the token type IDs corresponding to the sequences passed. "),no=a(uc,"A",{href:!0});var fd=i(no);fa=s(fd,`What are token type
IDs?`),fd.forEach(o),uc.forEach(o),ua=l(xo),Ko=a(xo,"P",{});var ud=i(Ko);_a=s(ud,"Should be overridden in a subclass if the model has a special way of building those."),ud.forEach(o),xo.forEach(o),ga=l(B),Qo=a(B,"DIV",{class:!0}),i(Qo).forEach(o),B.forEach(o),Gn=l(t),me=a(t,"H2",{class:!0});var is=i(me);ze=a(is,"A",{id:!0,class:!0,href:!0});var _d=i(ze);Zo=a(_d,"SPAN",{});var gd=i(Zo);_(mt.$$.fragment,gd),gd.forEach(o),_d.forEach(o),va=l(is),en=a(is,"SPAN",{});var vd=i(en);Ta=s(vd,"Speech2TextFeatureExtractor"),vd.forEach(o),is.forEach(o),Wn=l(t),I=a(t,"DIV",{class:!0});var re=i(I);_(ft.$$.fragment,re),xa=l(re),tn=a(re,"P",{});var Td=i(tn);ba=s(Td,"Constructs a Speech2Text feature extractor."),Td.forEach(o),ka=l(re),ut=a(re,"P",{});var cs=i(ut);ya=s(cs,"This feature extractor inherits from "),so=a(cs,"A",{href:!0});var xd=i(so);wa=s(xd,"Speech2TextFeatureExtractor"),xd.forEach(o),Sa=s(cs,` which contains most of the main methods. Users
should refer to this superclass for more information regarding those methods.`),cs.forEach(o),$a=l(re),on=a(re,"P",{});var bd=i(on);za=s(bd,`This class extracts mel-filter bank features from raw speech using TorchAudio and applies utterance-level cepstral
mean and variance normalization to the extracted features.`),bd.forEach(o),Ea=l(re),Ee=a(re,"DIV",{class:!0});var ds=i(Ee);_(_t.$$.fragment,ds),qa=l(ds),nn=a(ds,"P",{});var kd=i(nn);Pa=s(kd,"Main method to featurize and prepare for the model one or several sequence(s). sequences."),kd.forEach(o),ds.forEach(o),re.forEach(o),Vn=l(t),fe=a(t,"H2",{class:!0});var ls=i(fe);qe=a(ls,"A",{id:!0,class:!0,href:!0});var yd=i(qe);sn=a(yd,"SPAN",{});var wd=i(sn);_(gt.$$.fragment,wd),wd.forEach(o),yd.forEach(o),ja=l(ls),rn=a(ls,"SPAN",{});var Sd=i(rn);Fa=s(Sd,"Speech2TextProcessor"),Sd.forEach(o),ls.forEach(o),Un=l(t),w=a(t,"DIV",{class:!0});var j=i(w);_(vt.$$.fragment,j),Ca=l(j),an=a(j,"P",{});var $d=i(an);Ma=s($d,`Constructs a Speech2Text processor which wraps a Speech2Text feature extractor and a Speech2Text tokenizer into a
single processor.`),$d.forEach(o),Aa=l(j),W=a(j,"P",{});var K=i(W);ro=a(K,"A",{href:!0});var zd=i(ro);Da=s(zd,"Speech2TextProcessor"),zd.forEach(o),Ia=s(K," offers all the functionalities of "),ao=a(K,"A",{href:!0});var Ed=i(ao);Na=s(Ed,"Speech2TextFeatureExtractor"),Ed.forEach(o),La=s(K,` and
`),io=a(K,"A",{href:!0});var qd=i(io);Oa=s(qd,"Speech2TextTokenizer"),qd.forEach(o),Ga=s(K,". See the "),Tt=a(K,"A",{href:!0});var _c=i(Tt);cn=a(_c,"STRONG",{});var Pd=i(cn);Wa=s(Pd,"call"),Pd.forEach(o),Va=s(_c,"()"),_c.forEach(o),Ua=s(K," and "),co=a(K,"A",{href:!0});var jd=i(co);Ba=s(jd,"decode()"),jd.forEach(o),Ra=s(K,` for more
information.`),K.forEach(o),Ha=l(j),Pe=a(j,"DIV",{class:!0});var ps=i(Pe);_(xt.$$.fragment,ps),Ja=l(ps),X=a(ps,"P",{});var Oe=i(X);Ya=s(Oe,`When used in normal mode, this method forwards all its arguments to Speech2TextFeatureExtractor\u2019s
`),bt=a(Oe,"A",{href:!0});var gc=i(bt);dn=a(gc,"STRONG",{});var Fd=i(dn);Xa=s(Fd,"call"),Fd.forEach(o),Ka=s(gc,"()"),gc.forEach(o),Qa=s(Oe,` and returns its output. If used in the context
`),lo=a(Oe,"A",{href:!0});var Cd=i(lo);Za=s(Cd,"as_target_processor()"),Cd.forEach(o),ei=s(Oe,` this method forwards all its arguments to Speech2TextTokenizer\u2019s
`),kt=a(Oe,"A",{href:!0});var vc=i(kt);ln=a(vc,"STRONG",{});var Md=i(ln);ti=s(Md,"call"),Md.forEach(o),oi=s(vc,"()"),vc.forEach(o),ni=s(Oe,`. Please refer to the doctsring of the above two methods for more
information.`),Oe.forEach(o),ps.forEach(o),si=l(j),ee=a(j,"DIV",{class:!0});var bo=i(ee);_(yt.$$.fragment,bo),ri=l(bo),pn=a(bo,"P",{});var Ad=i(pn);ai=s(Ad,"Instantiate a processor associated with a pretrained model."),Ad.forEach(o),ii=l(bo),_(je.$$.fragment,bo),bo.forEach(o),ci=l(j),te=a(j,"DIV",{class:!0});var ko=i(te);_(wt.$$.fragment,ko),di=l(ko),St=a(ko,"P",{});var hs=i(St);li=s(hs,`Saves the attributes of this processor (feature extractor, tokenizer\u2026) in the specified directory so that it
can be reloaded using the `),hn=a(hs,"CODE",{});var Dd=i(hn);pi=s(Dd,"from_pretrained()"),Dd.forEach(o),hi=s(hs,"method."),hs.forEach(o),mi=l(ko),_(Fe.$$.fragment,ko),ko.forEach(o),fi=l(j),Ce=a(j,"DIV",{class:!0});var ms=i(Ce);_($t.$$.fragment,ms),ui=l(ms),zt=a(ms,"P",{});var fs=i(zt);_i=s(fs,"This method forwards all its arguments to Speech2TextTokenizer\u2019s "),po=a(fs,"A",{href:!0});var Id=i(po);gi=s(Id,"batch_decode()"),Id.forEach(o),vi=s(fs,`. Please
refer to the docstring of this method for more information.`),fs.forEach(o),ms.forEach(o),Ti=l(j),Me=a(j,"DIV",{class:!0});var us=i(Me);_(Et.$$.fragment,us),xi=l(us),qt=a(us,"P",{});var _s=i(qt);bi=s(_s,"This method forwards all its arguments to Speech2TextTokenizer\u2019s "),ho=a(_s,"A",{href:!0});var Nd=i(ho);ki=s(Nd,"decode()"),Nd.forEach(o),yi=s(_s,`. Please refer
to the docstring of this method for more information.`),_s.forEach(o),us.forEach(o),wi=l(j),Ae=a(j,"DIV",{class:!0});var gs=i(Ae);_(Pt.$$.fragment,gs),Si=l(gs),mn=a(gs,"P",{});var Ld=i(mn);$i=s(Ld,`Temporarily sets the tokenizer for processing the input. Useful for encoding the labels when fine-tuning
Speech2Text.`),Ld.forEach(o),gs.forEach(o),j.forEach(o),Bn=l(t),ue=a(t,"H2",{class:!0});var vs=i(ue);De=a(vs,"A",{id:!0,class:!0,href:!0});var Od=i(De);fn=a(Od,"SPAN",{});var Gd=i(fn);_(jt.$$.fragment,Gd),Gd.forEach(o),Od.forEach(o),zi=l(vs),un=a(vs,"SPAN",{});var Wd=i(un);Ei=s(Wd,"Speech2TextModel"),Wd.forEach(o),vs.forEach(o),Rn=l(t),R=a(t,"DIV",{class:!0});var Ge=i(R);_(Ft.$$.fragment,Ge),qi=l(Ge),Ct=a(Ge,"P",{});var Ts=i(Ct);Pi=s(Ts,`The bare Speech2Text Model outputting raw hidden-states without any specific head on top.
This model inherits from `),mo=a(Ts,"A",{href:!0});var Vd=i(mo);ji=s(Vd,"PreTrainedModel"),Vd.forEach(o),Fi=s(Ts,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Ts.forEach(o),Ci=l(Ge),Mt=a(Ge,"P",{});var xs=i(Mt);Mi=s(xs,"This model is also a PyTorch "),At=a(xs,"A",{href:!0,rel:!0});var Ud=i(At);Ai=s(Ud,"torch.nn.Module"),Ud.forEach(o),Di=s(xs,` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),xs.forEach(o),Ii=l(Ge),V=a(Ge,"DIV",{class:!0});var ae=i(V);_(Dt.$$.fragment,ae),Ni=l(ae),_e=a(ae,"P",{});var yo=i(_e);Li=s(yo,"The "),fo=a(yo,"A",{href:!0});var Bd=i(fo);Oi=s(Bd,"Speech2TextModel"),Bd.forEach(o),Gi=s(yo," forward method, overrides the "),_n=a(yo,"CODE",{});var Rd=i(_n);Wi=s(Rd,"__call__"),Rd.forEach(o),Vi=s(yo," special method."),yo.forEach(o),Ui=l(ae),_(Ie.$$.fragment,ae),Bi=l(ae),gn=a(ae,"P",{});var Hd=i(gn);Ri=s(Hd,"Example:"),Hd.forEach(o),Hi=l(ae),_(It.$$.fragment,ae),ae.forEach(o),Ge.forEach(o),Hn=l(t),ge=a(t,"H2",{class:!0});var bs=i(ge);Ne=a(bs,"A",{id:!0,class:!0,href:!0});var Jd=i(Ne);vn=a(Jd,"SPAN",{});var Yd=i(vn);_(Nt.$$.fragment,Yd),Yd.forEach(o),Jd.forEach(o),Ji=l(bs),Tn=a(bs,"SPAN",{});var Xd=i(Tn);Yi=s(Xd,"Speech2TextForConditionalGeneration"),Xd.forEach(o),bs.forEach(o),Jn=l(t),H=a(t,"DIV",{class:!0});var We=i(H);_(Lt.$$.fragment,We),Xi=l(We),Ot=a(We,"P",{});var ks=i(Ot);Ki=s(ks,`The Speech2Text Model with a language modeling head. Can be used for summarization.
This model inherits from `),uo=a(ks,"A",{href:!0});var Kd=i(uo);Qi=s(Kd,"PreTrainedModel"),Kd.forEach(o),Zi=s(ks,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),ks.forEach(o),ec=l(We),Gt=a(We,"P",{});var ys=i(Gt);tc=s(ys,"This model is also a PyTorch "),Wt=a(ys,"A",{href:!0,rel:!0});var Qd=i(Wt);oc=s(Qd,"torch.nn.Module"),Qd.forEach(o),nc=s(ys,` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),ys.forEach(o),sc=l(We),U=a(We,"DIV",{class:!0});var ie=i(U);_(Vt.$$.fragment,ie),rc=l(ie),ve=a(ie,"P",{});var wo=i(ve);ac=s(wo,"The "),_o=a(wo,"A",{href:!0});var Zd=i(_o);ic=s(Zd,"Speech2TextForConditionalGeneration"),Zd.forEach(o),cc=s(wo," forward method, overrides the "),xn=a(wo,"CODE",{});var el=i(xn);dc=s(el,"__call__"),el.forEach(o),lc=s(wo," special method."),wo.forEach(o),pc=l(ie),_(Le.$$.fragment,ie),hc=l(ie),bn=a(ie,"P",{});var tl=i(bn);mc=s(tl,"Example:"),tl.forEach(o),fc=l(ie),_(Ut.$$.fragment,ie),ie.forEach(o),We.forEach(o),this.h()},h(){c(m,"name","hf:doc:metadata"),c(m,"content",JSON.stringify(pl)),c(k,"id","speech2text"),c(k,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(k,"href","#speech2text"),c(f,"class","relative group"),c(L,"id","overview"),c(L,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(L,"href","#overview"),c(M,"class","relative group"),c(Ue,"href","https://arxiv.org/abs/2010.05171"),c(Ue,"rel","nofollow"),c(Be,"href","http://www.openslr.org/12"),c(Be,"rel","nofollow"),c(Re,"href","https://github.com/facebookresearch/covost"),c(Re,"rel","nofollow"),c(He,"href","https://ict.fbk.eu/must-c/"),c(He,"rel","nofollow"),c(Je,"href","https://huggingface.co/valhalla"),c(Je,"rel","nofollow"),c(Ye,"href","https://github.com/pytorch/fairseq/tree/master/examples/speech_to_text"),c(Ye,"rel","nofollow"),c(xe,"id","inference"),c(xe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(xe,"href","#inference"),c(ce,"class","relative group"),c(Rt,"href","/docs/transformers/pr_15794/en/model_doc/speech_to_text#transformers.Speech2TextFeatureExtractor"),c(Ht,"href","/docs/transformers/pr_15794/en/model_doc/speech_to_text#transformers.Speech2TextProcessor"),c(Jt,"href","/docs/transformers/pr_15794/en/model_doc/speech_to_text#transformers.Speech2TextFeatureExtractor"),c(Yt,"href","/docs/transformers/pr_15794/en/model_doc/speech_to_text#transformers.Speech2TextTokenizer"),c(Ke,"href","http://www.mega-nerd.com/libsndfile/"),c(Ke,"rel","nofollow"),c(tt,"href","https://huggingface.co/models?filter=speech_to_text"),c(tt,"rel","nofollow"),c(ye,"id","transformers.Speech2TextConfig"),c(ye,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(ye,"href","#transformers.Speech2TextConfig"),c(de,"class","relative group"),c(Qt,"href","/docs/transformers/pr_15794/en/model_doc/speech_to_text#transformers.Speech2TextModel"),c(st,"href","https://huggingface.co/facebook/s2t-small-librispeech-asr"),c(st,"rel","nofollow"),c(Zt,"href","/docs/transformers/pr_15794/en/main_classes/configuration#transformers.PretrainedConfig"),c(eo,"href","/docs/transformers/pr_15794/en/main_classes/configuration#transformers.PretrainedConfig"),c(D,"class","docstring"),c(we,"id","transformers.Speech2TextTokenizer"),c(we,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(we,"href","#transformers.Speech2TextTokenizer"),c(he,"class","relative group"),c(to,"href","/docs/transformers/pr_15794/en/main_classes/tokenizer#transformers.PreTrainedTokenizer"),c(Se,"class","docstring"),c($e,"class","docstring"),c(no,"href","../glossary#token-type-ids"),c(Z,"class","docstring"),c(Qo,"class","docstring"),c(P,"class","docstring"),c(ze,"id","transformers.Speech2TextFeatureExtractor"),c(ze,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(ze,"href","#transformers.Speech2TextFeatureExtractor"),c(me,"class","relative group"),c(so,"href","/docs/transformers/pr_15794/en/model_doc/speech_to_text#transformers.Speech2TextFeatureExtractor"),c(Ee,"class","docstring"),c(I,"class","docstring"),c(qe,"id","transformers.Speech2TextProcessor"),c(qe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(qe,"href","#transformers.Speech2TextProcessor"),c(fe,"class","relative group"),c(ro,"href","/docs/transformers/pr_15794/en/model_doc/speech_to_text#transformers.Speech2TextProcessor"),c(ao,"href","/docs/transformers/pr_15794/en/model_doc/speech_to_text#transformers.Speech2TextFeatureExtractor"),c(io,"href","/docs/transformers/pr_15794/en/model_doc/speech_to_text#transformers.Speech2TextTokenizer"),c(Tt,"href","/docs/transformers/pr_15794/en/model_doc/speech_to_text#transformers.Speech2TextProcessor.__call__"),c(co,"href","/docs/transformers/pr_15794/en/model_doc/speech_to_text#transformers.Speech2TextProcessor.decode"),c(bt,"href","/docs/transformers/pr_15794/en/model_doc/speech_to_text#transformers.Speech2TextFeatureExtractor.__call__"),c(lo,"href","/docs/transformers/pr_15794/en/model_doc/speech_to_text#transformers.Speech2TextProcessor.as_target_processor"),c(kt,"href","/docs/transformers/pr_15794/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__"),c(Pe,"class","docstring"),c(ee,"class","docstring"),c(te,"class","docstring"),c(po,"href","/docs/transformers/pr_15794/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.batch_decode"),c(Ce,"class","docstring"),c(ho,"href","/docs/transformers/pr_15794/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.decode"),c(Me,"class","docstring"),c(Ae,"class","docstring"),c(w,"class","docstring"),c(De,"id","transformers.Speech2TextModel"),c(De,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(De,"href","#transformers.Speech2TextModel"),c(ue,"class","relative group"),c(mo,"href","/docs/transformers/pr_15794/en/main_classes/model#transformers.PreTrainedModel"),c(At,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(At,"rel","nofollow"),c(fo,"href","/docs/transformers/pr_15794/en/model_doc/speech_to_text#transformers.Speech2TextModel"),c(V,"class","docstring"),c(R,"class","docstring"),c(Ne,"id","transformers.Speech2TextForConditionalGeneration"),c(Ne,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Ne,"href","#transformers.Speech2TextForConditionalGeneration"),c(ge,"class","relative group"),c(uo,"href","/docs/transformers/pr_15794/en/main_classes/model#transformers.PreTrainedModel"),c(Wt,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(Wt,"rel","nofollow"),c(_o,"href","/docs/transformers/pr_15794/en/model_doc/speech_to_text#transformers.Speech2TextForConditionalGeneration"),c(U,"class","docstring"),c(H,"class","docstring")},m(t,p){e(document.head,m),h(t,S,p),h(t,f,p),e(f,k),e(k,z),g(b,z,null),e(f,y),e(f,E),e(E,C),h(t,$,p),h(t,M,p),e(M,L),e(L,So),g(Ve,So,null),e(M,Ss),e(M,$o),e($o,$s),h(t,$n,p),h(t,O,p),e(O,zs),e(O,Ue),e(Ue,Es),e(O,qs),e(O,Be),e(Be,Ps),e(O,js),e(O,Re),e(Re,Fs),e(O,Cs),e(O,He),e(He,Ms),e(O,As),h(t,zn,p),h(t,Q,p),e(Q,Ds),e(Q,Je),e(Je,Is),e(Q,Ns),e(Q,Ye),e(Ye,Ls),e(Q,Os),h(t,En,p),h(t,ce,p),e(ce,xe),e(xe,zo),g(Xe,zo,null),e(ce,Gs),e(ce,Eo),e(Eo,Ws),h(t,qn,p),h(t,be,p),e(be,Vs),e(be,qo),e(qo,Us),e(be,Bs),h(t,Pn,p),h(t,G,p),e(G,Rs),e(G,Rt),e(Rt,Hs),e(G,Js),e(G,Ht),e(Ht,Ys),e(G,Xs),e(G,Jt),e(Jt,Ks),e(G,Qs),e(G,Yt),e(Yt,Zs),e(G,er),h(t,jn,p),h(t,q,p),e(q,tr),e(q,Po),e(Po,or),e(q,nr),e(q,jo),e(jo,sr),e(q,rr),e(q,Fo),e(Fo,ar),e(q,ir),e(q,Co),e(Co,cr),e(q,dr),e(q,Mo),e(Mo,lr),e(q,pr),e(q,Ke),e(Ke,hr),e(q,mr),e(q,Ao),e(Ao,fr),h(t,Fn,p),h(t,Xt,p),e(Xt,Do),e(Do,ur),h(t,Cn,p),g(Qe,t,p),h(t,Mn,p),h(t,Kt,p),e(Kt,Ze),e(Ze,Io),e(Io,_r),e(Ze,gr),e(Ze,A),e(A,vr),e(A,No),e(No,Tr),e(A,xr),e(A,Lo),e(Lo,br),e(A,kr),e(A,Oo),e(Oo,yr),e(A,wr),e(A,Go),e(Go,Sr),e(A,$r),e(A,Wo),e(Wo,zr),e(A,Er),h(t,An,p),g(et,t,p),h(t,Dn,p),h(t,ke,p),e(ke,qr),e(ke,tt),e(tt,Pr),e(ke,jr),h(t,In,p),h(t,de,p),e(de,ye),e(ye,Vo),g(ot,Vo,null),e(de,Fr),e(de,Uo),e(Uo,Cr),h(t,Nn,p),h(t,D,p),g(nt,D,null),e(D,Mr),e(D,le),e(le,Ar),e(le,Qt),e(Qt,Dr),e(le,Ir),e(le,st),e(st,Nr),e(le,Lr),e(D,Or),e(D,pe),e(pe,Gr),e(pe,Zt),e(Zt,Wr),e(pe,Vr),e(pe,eo),e(eo,Ur),e(pe,Br),e(D,Rr),e(D,Bo),e(Bo,Hr),e(D,Jr),g(rt,D,null),h(t,Ln,p),h(t,he,p),e(he,we),e(we,Ro),g(at,Ro,null),e(he,Yr),e(he,Ho),e(Ho,Xr),h(t,On,p),h(t,P,p),g(it,P,null),e(P,Kr),e(P,Jo),e(Jo,Qr),e(P,Zr),e(P,ct),e(ct,ea),e(ct,to),e(to,ta),e(ct,oa),e(P,na),e(P,Se),g(dt,Se,null),e(Se,sa),e(Se,Yo),e(Yo,ra),e(P,aa),e(P,$e),g(lt,$e,null),e($e,ia),e($e,pt),e(pt,ca),e(pt,Xo),e(Xo,da),e(pt,la),e(P,pa),e(P,Z),g(ht,Z,null),e(Z,ha),e(Z,oo),e(oo,ma),e(oo,no),e(no,fa),e(Z,ua),e(Z,Ko),e(Ko,_a),e(P,ga),e(P,Qo),h(t,Gn,p),h(t,me,p),e(me,ze),e(ze,Zo),g(mt,Zo,null),e(me,va),e(me,en),e(en,Ta),h(t,Wn,p),h(t,I,p),g(ft,I,null),e(I,xa),e(I,tn),e(tn,ba),e(I,ka),e(I,ut),e(ut,ya),e(ut,so),e(so,wa),e(ut,Sa),e(I,$a),e(I,on),e(on,za),e(I,Ea),e(I,Ee),g(_t,Ee,null),e(Ee,qa),e(Ee,nn),e(nn,Pa),h(t,Vn,p),h(t,fe,p),e(fe,qe),e(qe,sn),g(gt,sn,null),e(fe,ja),e(fe,rn),e(rn,Fa),h(t,Un,p),h(t,w,p),g(vt,w,null),e(w,Ca),e(w,an),e(an,Ma),e(w,Aa),e(w,W),e(W,ro),e(ro,Da),e(W,Ia),e(W,ao),e(ao,Na),e(W,La),e(W,io),e(io,Oa),e(W,Ga),e(W,Tt),e(Tt,cn),e(cn,Wa),e(Tt,Va),e(W,Ua),e(W,co),e(co,Ba),e(W,Ra),e(w,Ha),e(w,Pe),g(xt,Pe,null),e(Pe,Ja),e(Pe,X),e(X,Ya),e(X,bt),e(bt,dn),e(dn,Xa),e(bt,Ka),e(X,Qa),e(X,lo),e(lo,Za),e(X,ei),e(X,kt),e(kt,ln),e(ln,ti),e(kt,oi),e(X,ni),e(w,si),e(w,ee),g(yt,ee,null),e(ee,ri),e(ee,pn),e(pn,ai),e(ee,ii),g(je,ee,null),e(w,ci),e(w,te),g(wt,te,null),e(te,di),e(te,St),e(St,li),e(St,hn),e(hn,pi),e(St,hi),e(te,mi),g(Fe,te,null),e(w,fi),e(w,Ce),g($t,Ce,null),e(Ce,ui),e(Ce,zt),e(zt,_i),e(zt,po),e(po,gi),e(zt,vi),e(w,Ti),e(w,Me),g(Et,Me,null),e(Me,xi),e(Me,qt),e(qt,bi),e(qt,ho),e(ho,ki),e(qt,yi),e(w,wi),e(w,Ae),g(Pt,Ae,null),e(Ae,Si),e(Ae,mn),e(mn,$i),h(t,Bn,p),h(t,ue,p),e(ue,De),e(De,fn),g(jt,fn,null),e(ue,zi),e(ue,un),e(un,Ei),h(t,Rn,p),h(t,R,p),g(Ft,R,null),e(R,qi),e(R,Ct),e(Ct,Pi),e(Ct,mo),e(mo,ji),e(Ct,Fi),e(R,Ci),e(R,Mt),e(Mt,Mi),e(Mt,At),e(At,Ai),e(Mt,Di),e(R,Ii),e(R,V),g(Dt,V,null),e(V,Ni),e(V,_e),e(_e,Li),e(_e,fo),e(fo,Oi),e(_e,Gi),e(_e,_n),e(_n,Wi),e(_e,Vi),e(V,Ui),g(Ie,V,null),e(V,Bi),e(V,gn),e(gn,Ri),e(V,Hi),g(It,V,null),h(t,Hn,p),h(t,ge,p),e(ge,Ne),e(Ne,vn),g(Nt,vn,null),e(ge,Ji),e(ge,Tn),e(Tn,Yi),h(t,Jn,p),h(t,H,p),g(Lt,H,null),e(H,Xi),e(H,Ot),e(Ot,Ki),e(Ot,uo),e(uo,Qi),e(Ot,Zi),e(H,ec),e(H,Gt),e(Gt,tc),e(Gt,Wt),e(Wt,oc),e(Gt,nc),e(H,sc),e(H,U),g(Vt,U,null),e(U,rc),e(U,ve),e(ve,ac),e(ve,_o),e(_o,ic),e(ve,cc),e(ve,xn),e(xn,dc),e(ve,lc),e(U,pc),g(Le,U,null),e(U,hc),e(U,bn),e(bn,mc),e(U,fc),g(Ut,U,null),Yn=!0},p(t,[p]){const Bt={};p&2&&(Bt.$$scope={dirty:p,ctx:t}),je.$set(Bt);const kn={};p&2&&(kn.$$scope={dirty:p,ctx:t}),Fe.$set(kn);const yn={};p&2&&(yn.$$scope={dirty:p,ctx:t}),Ie.$set(yn);const wn={};p&2&&(wn.$$scope={dirty:p,ctx:t}),Le.$set(wn)},i(t){Yn||(v(b.$$.fragment,t),v(Ve.$$.fragment,t),v(Xe.$$.fragment,t),v(Qe.$$.fragment,t),v(et.$$.fragment,t),v(ot.$$.fragment,t),v(nt.$$.fragment,t),v(rt.$$.fragment,t),v(at.$$.fragment,t),v(it.$$.fragment,t),v(dt.$$.fragment,t),v(lt.$$.fragment,t),v(ht.$$.fragment,t),v(mt.$$.fragment,t),v(ft.$$.fragment,t),v(_t.$$.fragment,t),v(gt.$$.fragment,t),v(vt.$$.fragment,t),v(xt.$$.fragment,t),v(yt.$$.fragment,t),v(je.$$.fragment,t),v(wt.$$.fragment,t),v(Fe.$$.fragment,t),v($t.$$.fragment,t),v(Et.$$.fragment,t),v(Pt.$$.fragment,t),v(jt.$$.fragment,t),v(Ft.$$.fragment,t),v(Dt.$$.fragment,t),v(Ie.$$.fragment,t),v(It.$$.fragment,t),v(Nt.$$.fragment,t),v(Lt.$$.fragment,t),v(Vt.$$.fragment,t),v(Le.$$.fragment,t),v(Ut.$$.fragment,t),Yn=!0)},o(t){T(b.$$.fragment,t),T(Ve.$$.fragment,t),T(Xe.$$.fragment,t),T(Qe.$$.fragment,t),T(et.$$.fragment,t),T(ot.$$.fragment,t),T(nt.$$.fragment,t),T(rt.$$.fragment,t),T(at.$$.fragment,t),T(it.$$.fragment,t),T(dt.$$.fragment,t),T(lt.$$.fragment,t),T(ht.$$.fragment,t),T(mt.$$.fragment,t),T(ft.$$.fragment,t),T(_t.$$.fragment,t),T(gt.$$.fragment,t),T(vt.$$.fragment,t),T(xt.$$.fragment,t),T(yt.$$.fragment,t),T(je.$$.fragment,t),T(wt.$$.fragment,t),T(Fe.$$.fragment,t),T($t.$$.fragment,t),T(Et.$$.fragment,t),T(Pt.$$.fragment,t),T(jt.$$.fragment,t),T(Ft.$$.fragment,t),T(Dt.$$.fragment,t),T(Ie.$$.fragment,t),T(It.$$.fragment,t),T(Nt.$$.fragment,t),T(Lt.$$.fragment,t),T(Vt.$$.fragment,t),T(Le.$$.fragment,t),T(Ut.$$.fragment,t),Yn=!1},d(t){o(m),t&&o(S),t&&o(f),x(b),t&&o($),t&&o(M),x(Ve),t&&o($n),t&&o(O),t&&o(zn),t&&o(Q),t&&o(En),t&&o(ce),x(Xe),t&&o(qn),t&&o(be),t&&o(Pn),t&&o(G),t&&o(jn),t&&o(q),t&&o(Fn),t&&o(Xt),t&&o(Cn),x(Qe,t),t&&o(Mn),t&&o(Kt),t&&o(An),x(et,t),t&&o(Dn),t&&o(ke),t&&o(In),t&&o(de),x(ot),t&&o(Nn),t&&o(D),x(nt),x(rt),t&&o(Ln),t&&o(he),x(at),t&&o(On),t&&o(P),x(it),x(dt),x(lt),x(ht),t&&o(Gn),t&&o(me),x(mt),t&&o(Wn),t&&o(I),x(ft),x(_t),t&&o(Vn),t&&o(fe),x(gt),t&&o(Un),t&&o(w),x(vt),x(xt),x(yt),x(je),x(wt),x(Fe),x($t),x(Et),x(Pt),t&&o(Bn),t&&o(ue),x(jt),t&&o(Rn),t&&o(R),x(Ft),x(Dt),x(Ie),x(It),t&&o(Hn),t&&o(ge),x(Nt),t&&o(Jn),t&&o(H),x(Lt),x(Vt),x(Le),x(Ut)}}}const pl={local:"speech2text",sections:[{local:"overview",title:"Overview"},{local:"inference",title:"Inference"},{local:"transformers.Speech2TextConfig",title:"Speech2TextConfig"},{local:"transformers.Speech2TextTokenizer",title:"Speech2TextTokenizer"},{local:"transformers.Speech2TextFeatureExtractor",title:"Speech2TextFeatureExtractor"},{local:"transformers.Speech2TextProcessor",title:"Speech2TextProcessor"},{local:"transformers.Speech2TextModel",title:"Speech2TextModel"},{local:"transformers.Speech2TextForConditionalGeneration",title:"Speech2TextForConditionalGeneration"}],title:"Speech2Text"};function hl(Y,m,S){let{fw:f}=m;return Y.$$set=k=>{"fw"in k&&S(0,f=k.fw)},[f]}class Tl extends ol{constructor(m){super();nl(this,m,hl,ll,sl,{fw:0})}}export{Tl as default,pl as metadata};
