import{S as VA,i as HA,s as BA,e as s,k as h,w as d,t as i,M as FA,c as r,d as t,m as f,a as l,x as u,h as o,b as c,N as zA,F as a,g as p,y as m,L as qA,q as v,o as w,B as _}from"../chunks/vendor-22ad994f.js";import{I as b}from"../chunks/IconCopyLink-2eb9a001.js";import{C as q}from"../chunks/CodeBlock-03069293.js";import"../chunks/CopyButton-f539c482.js";function RA(Tv){let R,Tr,j,G,Io,ya,Iv,So,Sv,sf,Ir,Nv,rf,ne,Qe,No,ga,Cv,Co,Lv,lf,Sr,Gv,of,pe,Ze,Lo,Ea,Uv,Go,Mv,nf,Nr,Ov,pf,Cr,Lr,zv,Pa,Uo,Vv,Hv,Mo,Bv,hf,Gr,Fv,ff,Je,Oo,qv,Rv,zo,Wv,cf,he,Ke,Vo,ka,Xv,Ho,Yv,df,Ur,Qv,uf,W,Bo,Zv,Jv,Fo,Kv,e1,$a,t1,Mr,a1,s1,mf,Or,r1,vf,y,qo,l1,i1,Ro,o1,n1,Wo,p1,h1,Xo,f1,c1,Yo,d1,u1,Qo,m1,v1,Zo,w1,wf,fe,et,Jo,Aa,_1,Ko,b1,_f,ce,tt,en,xa,y1,tn,g1,bf,zr,E1,yf,ja,an,P1,k1,gf,Vr,$1,Ef,Hr,A1,Pf,Br,x1,kf,Fr,j1,$f,qr,D1,Af,Rr,T1,xf,Da,sn,I1,S1,jf,Wr,N1,Df,Xr,C1,Tf,de,at,rn,Ta,L1,ln,G1,If,Yr,U1,Sf,Qr,M1,Nf,Ia,Cf,Zr,O1,Lf,Jr,z1,Gf,Sa,Uf,Kr,V1,Mf,Na,Of,el,H1,zf,Ca,Vf,X,B1,on,F1,q1,nn,R1,W1,Hf,tl,X1,Bf,al,Y1,Ff,ue,st,pn,La,Q1,hn,Z1,qf,Ga,Ua,J1,K1,Rf,rt,ew,Ma,tw,aw,Wf,sl,fn,sw,Xf,D,rw,cn,lw,iw,dn,ow,nw,un,pw,hw,Yf,rl,fw,Qf,ll,cw,Zf,lt,mn,Oa,vn,dw,uw,il,mw,vw,za,Va,wn,ww,_w,ol,bw,yw,Ha,_n,gw,Ew,nl,Pw,Jf,pl,kw,Kf,it,$w,bn,Aw,xw,ec,hl,jw,tc,Ba,ac,P,Dw,yn,Tw,Iw,gn,Sw,Nw,En,Cw,Lw,Pn,Gw,Uw,kn,Mw,sc,me,ot,$n,Fa,Ow,An,zw,rc,ve,nt,xn,qa,Vw,jn,Hw,lc,fl,Bw,ic,Ra,Fw,cl,qw,oc,we,pt,Dn,Wa,Rw,Tn,Ww,nc,dl,Xw,pc,Y,Xa,In,Sn,Yw,Qw,Ya,Zw,Nn,Jw,Kw,e_,Qa,Cn,Ln,t_,a_,Za,s_,Gn,r_,l_,i_,Ja,Un,Mn,o_,n_,Ka,p_,On,h_,f_,hc,ul,c_,fc,es,d_,ts,u_,cc,_e,ht,zn,as,m_,Vn,v_,dc,ml,w_,uc,E,Hn,__,b_,Bn,y_,g_,Fn,E_,P_,qn,k_,$_,Rn,A_,x_,Wn,j_,mc,vl,D_,vc,wl,T_,wc,_l,I_,_c,be,ft,Xn,ss,S_,Yn,N_,bc,ct,Qn,C_,L_,Zn,G_,yc,ye,dt,Jn,rs,U_,Kn,M_,gc,Q,ep,O_,z_,bl,V_,ls,H_,B_,tp,F_,Ec,ge,ut,ap,is,q_,sp,R_,Pc,yl,rp,W_,kc,Ee,mt,lp,os,X_,ip,Y_,$c,gl,op,Q_,Ac,El,Z_,xc,Pe,vt,np,ns,J_,pp,K_,jc,Pl,eb,Dc,ke,wt,hp,ps,tb,fp,ab,Tc,kl,sb,Ic,$e,_t,cp,hs,rb,bt,dp,lb,ib,up,ob,nb,Sc,$l,pb,Nc,Ae,yt,mp,fs,hb,vp,fb,Cc,Al,cb,Lc,T,cs,db,wp,ub,mb,vb,ds,wb,_p,_b,bb,yb,us,gb,bp,Eb,Pb,kb,yp,$b,Gc,xl,Ab,Uc,jl,Dl,E0,Mc,gt,xb,ms,jb,Db,Oc,Tl,Tb,zc,xe,Et,gp,vs,Ib,Ep,Sb,Vc,Il,Nb,Hc,Sl,Cb,Bc,I,Pp,Lb,Gb,kp,Ub,Mb,ws,Ob,$p,zb,Vb,Hb,Ap,Bb,Fc,Nl,Fb,qc,Pt,qb,xp,Rb,Wb,Rc,Cl,Xb,Wc,_s,Xc,kt,Yb,jp,Qb,Zb,Yc,$t,Dp,U,Ll,Jb,Kb,Gl,ey,ty,Ul,ay,sy,Ml,ry,ly,M,O,Ol,iy,oy,zl,ny,py,Vl,hy,fy,Hl,cy,dy,z,Bl,uy,my,Fl,vy,wy,ql,_y,by,Rl,yy,gy,V,Wl,Ey,Py,Xl,ky,$y,Yl,Ay,xy,Ql,jy,Dy,H,Zl,Ty,Iy,Jl,Sy,Ny,Kl,Cy,Ly,ei,Gy,Qc,At,Uy,Tp,My,Oy,Zc,ti,zy,Jc,ai,Vy,Kc,xt,Hy,bs,By,Fy,ed,si,qy,td,jt,ri,Ry,ys,Wy,Xy,li,Yy,gs,Qy,ad,Z,Zy,Es,Jy,Ky,Ps,e2,t2,sd,je,Dt,Ip,ks,a2,Sp,s2,rd,J,r2,Np,l2,i2,$s,o2,n2,ld,ii,p2,id,De,Tt,Cp,As,h2,Lp,f2,od,oi,c2,nd,ni,d2,pd,pi,u2,hd,It,m2,Gp,v2,w2,fd,Te,St,Up,xs,_2,Mp,b2,cd,K,y2,Op,g2,E2,zp,P2,k2,dd,hi,$2,ud,fi,A2,md,ci,x2,vd,di,j2,wd,Nt,D2,Vp,T2,I2,_d,ui,S2,bd,js,yd,Ds,N2,Hp,C2,gd,mi,L2,Ed,Ts,Pd,vi,G2,kd,ee,U2,Is,M2,O2,Ss,z2,V2,$d,Ie,Ct,Bp,Ns,H2,Fp,B2,Ad,Lt,F2,wi,q2,R2,xd,Gt,W2,qp,X2,Y2,jd,Se,Ut,Rp,Cs,Q2,Wp,Z2,Dd,_i,J2,Td,bi,K2,Id,Ls,Sd,yi,eg,Nd,Mt,tg,Gs,ag,sg,Cd,gi,rg,Ld,S,lg,Xp,ig,og,Yp,ng,pg,Qp,hg,fg,Gd,te,cg,Zp,dg,ug,Jp,mg,vg,Ud,Ot,wg,Kp,_g,bg,Md,ae,yg,Us,gg,Eg,Ms,Pg,kg,Od,Ne,zt,eh,Os,$g,th,Ag,zd,se,xg,zs,jg,Dg,Vs,Tg,Ig,Vd,Vt,Sg,ah,Ng,Cg,Hd,Ei,Lg,Bd,Ce,Ht,sh,Hs,Gg,rh,Ug,Fd,Pi,Mg,qd,N,Og,Bs,zg,Vg,lh,Hg,Bg,ih,Fg,qg,Rd,ki,Rg,Wd,Fs,Xd,Bt,Wg,oh,Xg,Yg,Yd,Le,Ft,nh,qs,Qg,ph,Zg,Qd,$i,Jg,Zd,re,Kg,Rs,e3,t3,Ws,a3,s3,Jd,Xs,Ys,r3,l3,Kd,qt,i3,Qs,o3,n3,eu,C,p3,Ai,h3,f3,Zs,c3,d3,Js,u3,m3,tu,Ge,Rt,hh,Ks,v3,fh,w3,au,Ue,ch,_3,b3,dh,y3,g3,su,Wt,uh,E3,P3,mh,k3,ru,xi,$3,lu,er,tr,A3,x3,iu,Xt,vh,j3,D3,ji,T3,ar,wh,I3,S3,sr,N3,_h,C3,L3,ou,rr,lr,G3,U3,nu,Di,M3,pu,$,bh,O3,z3,yh,V3,H3,ir,B3,gh,F3,q3,R3,Ti,W3,Eh,X3,Y3,Ph,Q3,hu,Ii,Z3,fu,Yt,J3,or,K3,eE,cu,Si,tE,du,Ni,aE,uu,Ci,sE,mu,Qt,rE,nr,lE,iE,vu,Li,oE,wu,Zt,kh,Me,Gi,nE,pE,$h,hE,fE,Ui,cE,dE,Oe,ze,Mi,uE,mE,Ah,vE,wE,Oi,_E,bE,Ve,zi,yE,gE,xh,EE,PE,Vi,kE,$E,He,Hi,AE,xE,jh,jE,DE,Bi,TE,_u,Fi,IE,bu,qi,SE,yu,Ri,NE,gu,Wi,CE,Eu,pr,Dh,LE,GE,Pu,hr,ku,k,UE,Th,ME,OE,Ih,zE,VE,Sh,HE,BE,Nh,FE,qE,Ch,RE,$u,Be,Jt,Lh,fr,WE,Gh,XE,Au,Xi,YE,xu,Kt,Yi,Uh,QE,ZE,JE,Qi,Mh,KE,e6,ju,Fe,ea,Oh,cr,t6,zh,a6,Du,qe,s6,Vh,r6,l6,dr,i6,Tu,Re,ta,Hh,ur,o6,Bh,n6,Iu,We,aa,Fh,mr,p6,qh,h6,Su,Zi,f6,Nu,Ji,c6,Cu,Ki,d6,Lu,eo,to,P0,Gu,sa,u6,vr,m6,v6,Uu,ao,w6,Mu,so,_6,Ou,ro,b6,zu,lo,y6,Vu,le,Rh,wr,g6,E6,Wh,_r,P6,k6,Xh,br,$6,Hu,g,A6,yr,x6,j6,gr,D6,T6,Er,I6,S6,Pr,N6,C6,kr,L6,G6,io,U6,M6,Bu,Xe,ra,Yh,$r,O6,Qh,z6,Fu,la,V6,Ar,H6,B6,qu,ia,F6,Zh,q6,R6,Ru,oo,W6,Wu,ie,X6,xr,Y6,Q6,jr,Z6,J6,Xu,no,K6,Yu,Ye,oa,Jh,Dr,e0,Kh,t0,Qu,po,a0,Zu,ho,s0,Ju;return ya=new b({}),ga=new b({}),Ea=new b({}),ka=new b({}),Aa=new b({}),xa=new b({}),Ta=new b({}),Ia=new q({props:{code:"nvidia-smi topo -m,",highlighted:'<span class="hljs-symbol">nvidia</span>-<span class="hljs-keyword">smi</span> topo -m'}}),Sa=new q({props:{code:`        GPU0    GPU1    CPU Affinity    NUMA Affinity
GPU0     X      NV2     0-23            N/A
GPU1    NV2      X      0-23            N/A,`,highlighted:`        <span class="hljs-attribute">GPU0</span>    GPU1    CPU Affinity    NUMA Affinity
<span class="hljs-attribute">GPU0</span>     X      NV2     <span class="hljs-number">0</span>-<span class="hljs-number">23</span>            N/A
<span class="hljs-attribute">GPU1</span>    NV2      X      <span class="hljs-number">0</span>-<span class="hljs-number">23</span>            N/A`}}),Na=new q({props:{code:`        GPU0    GPU1    CPU Affinity    NUMA Affinity
GPU0     X      PHB     0-11            N/A
GPU1    PHB      X      0-11            N/A,`,highlighted:`        <span class="hljs-attribute">GPU0</span>    GPU1    CPU Affinity    NUMA Affinity
<span class="hljs-attribute">GPU0</span>     X      PHB     <span class="hljs-number">0</span>-<span class="hljs-number">11</span>            N/A
<span class="hljs-attribute">GPU1</span>    PHB      X      <span class="hljs-number">0</span>-<span class="hljs-number">11</span>            N/A`}}),Ca=new q({props:{code:`  X    = Self
  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)
  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node
  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)
  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)
  PIX  = Connection traversing at most a single PCIe bridge
  NV#  = Connection traversing a bonded set of # NVLinks,`,highlighted:`  X    = Self
  SYS  = Connection traversing PCIe <span class="hljs-keyword">as</span> well <span class="hljs-keyword">as</span> <span class="hljs-keyword">the</span> SMP interconnect between NUMA nodes (e.g., QPI/UPI)
  NODE = Connection traversing PCIe <span class="hljs-keyword">as</span> well <span class="hljs-keyword">as</span> <span class="hljs-keyword">the</span> interconnect between PCIe Host Bridges <span class="hljs-keyword">within</span> <span class="hljs-keyword">a</span> NUMA node
  PHB  = Connection traversing PCIe <span class="hljs-keyword">as</span> well <span class="hljs-keyword">as</span> <span class="hljs-keyword">a</span> PCIe Host Bridge (typically <span class="hljs-keyword">the</span> CPU)
  PXB  = Connection traversing multiple PCIe bridges (<span class="hljs-keyword">without</span> traversing <span class="hljs-keyword">the</span> PCIe Host Bridge)
  PIX  = Connection traversing <span class="hljs-keyword">at</span> most <span class="hljs-keyword">a</span> single PCIe bridge
  NV<span class="hljs-comment">#  = Connection traversing a bonded set of # NVLinks</span>`}}),La=new b({}),Ba=new q({props:{code:`# DDP w/ NVLink

rm -r /tmp/test-clm; CUDA_VISIBLE_DEVICES=0,1 python -m torch.distributed.launch \\
--nproc_per_node 2 examples/pytorch/language-modeling/run_clm.py --model_name_or_path gpt2 \\
--dataset_name wikitext --dataset_config_name wikitext-2-raw-v1 --do_train \\
--output_dir /tmp/test-clm --per_device_train_batch_size 4 --max_steps 200

{'train_runtime': 101.9003, 'train_samples_per_second': 1.963, 'epoch': 0.69}

# DDP w/o NVLink

rm -r /tmp/test-clm; CUDA_VISIBLE_DEVICES=0,1 NCCL_P2P_DISABLE=1 python -m torch.distributed.launch \\
--nproc_per_node 2 examples/pytorch/language-modeling/run_clm.py --model_name_or_path gpt2 \\
--dataset_name wikitext --dataset_config_name wikitext-2-raw-v1 --do_train
--output_dir /tmp/test-clm --per_device_train_batch_size 4 --max_steps 200

{'train_runtime': 131.4367, 'train_samples_per_second': 1.522, 'epoch': 0.69},`,highlighted:`<span class="hljs-comment"># DDP w/ NVLink</span>

<span class="hljs-string">rm</span> -<span class="hljs-string">r</span> /<span class="hljs-string">tmp</span>/<span class="hljs-string">test-clm</span>; <span class="hljs-string">CUDA_VISIBLE_DEVICES</span>=<span class="hljs-string">0</span>,<span class="hljs-string">1</span> <span class="hljs-string">python</span> -<span class="hljs-string">m</span> <span class="hljs-string">torch</span>.<span class="hljs-string">distributed</span>.<span class="hljs-string">launch</span> \\
<span class="hljs-built_in">--nproc_per_node</span> <span class="hljs-string">2</span> <span class="hljs-string">examples</span>/<span class="hljs-string">pytorch</span>/<span class="hljs-string">language-modeling</span>/<span class="hljs-string">run_clm</span>.<span class="hljs-string">py</span> <span class="hljs-built_in">--model_name_or_path</span> <span class="hljs-string">gpt2</span> \\
<span class="hljs-built_in">--dataset_name</span> <span class="hljs-string">wikitext</span> <span class="hljs-built_in">--dataset_config_name</span> <span class="hljs-string">wikitext-2-raw-v1</span> <span class="hljs-built_in">--do_train</span> \\
<span class="hljs-built_in">--output_dir</span> /<span class="hljs-string">tmp</span>/<span class="hljs-string">test-clm</span> <span class="hljs-built_in">--per_device_train_batch_size</span> <span class="hljs-string">4</span> <span class="hljs-built_in">--max_steps</span> <span class="hljs-string">200</span>

{<span class="hljs-string">&#x27;train_runtime&#x27;</span>: <span class="hljs-string">101</span>.<span class="hljs-string">9003</span>, <span class="hljs-string">&#x27;train_samples_per_second&#x27;</span>: <span class="hljs-string">1</span>.<span class="hljs-string">963</span>, <span class="hljs-string">&#x27;epoch&#x27;</span>: <span class="hljs-string">0</span>.<span class="hljs-string">69</span>}

<span class="hljs-comment"># DDP w/o NVLink</span>

<span class="hljs-string">rm</span> -<span class="hljs-string">r</span> /<span class="hljs-string">tmp</span>/<span class="hljs-string">test-clm</span>; <span class="hljs-string">CUDA_VISIBLE_DEVICES</span>=<span class="hljs-string">0</span>,<span class="hljs-string">1</span> <span class="hljs-string">NCCL_P2P_DISABLE</span>=<span class="hljs-string">1</span> <span class="hljs-string">python</span> -<span class="hljs-string">m</span> <span class="hljs-string">torch</span>.<span class="hljs-string">distributed</span>.<span class="hljs-string">launch</span> \\
<span class="hljs-built_in">--nproc_per_node</span> <span class="hljs-string">2</span> <span class="hljs-string">examples</span>/<span class="hljs-string">pytorch</span>/<span class="hljs-string">language-modeling</span>/<span class="hljs-string">run_clm</span>.<span class="hljs-string">py</span> <span class="hljs-built_in">--model_name_or_path</span> <span class="hljs-string">gpt2</span> \\
<span class="hljs-built_in">--dataset_name</span> <span class="hljs-string">wikitext</span> <span class="hljs-built_in">--dataset_config_name</span> <span class="hljs-string">wikitext-2-raw-v1</span> <span class="hljs-built_in">--do_train</span>
<span class="hljs-built_in">--output_dir</span> /<span class="hljs-string">tmp</span>/<span class="hljs-string">test-clm</span> <span class="hljs-built_in">--per_device_train_batch_size</span> <span class="hljs-string">4</span> <span class="hljs-built_in">--max_steps</span> <span class="hljs-string">200</span>

{<span class="hljs-string">&#x27;train_runtime&#x27;</span>: <span class="hljs-string">131</span>.<span class="hljs-string">4367</span>, <span class="hljs-string">&#x27;train_samples_per_second&#x27;</span>: <span class="hljs-string">1</span>.<span class="hljs-string">522</span>, <span class="hljs-string">&#x27;epoch&#x27;</span>: <span class="hljs-string">0</span>.<span class="hljs-string">69</span>}`}}),Fa=new b({}),qa=new b({}),Wa=new b({}),as=new b({}),ss=new b({}),rs=new b({}),is=new b({}),os=new b({}),ns=new b({}),ps=new b({}),hs=new b({}),fs=new b({}),vs=new b({}),_s=new q({props:{code:`export BS=16
python -m torch.distributed.launch \\
    --nproc_per_node 2 examples/pytorch/text-classification/run_glue.py \\
    --model_name_or_path bert-base-cased \\
    --task_name mrpc \\
    --do_train \\
    --do_eval \\
    --max_seq_length 128 \\
    --per_device_train_batch_size $BS \\
    --learning_rate 2e-5 \\
    --num_train_epochs 3.0 \\
    --output_dir /tmp/mrpc \\
    --overwrite_output_dir \\
    --fp16,`,highlighted:`export BS=<span class="hljs-number">16</span>
python -m torch<span class="hljs-selector-class">.distributed</span><span class="hljs-selector-class">.launch</span> \\
    <span class="hljs-attr">--nproc_per_node</span> <span class="hljs-number">2</span> examples/pytorch/text-classification/run_glue<span class="hljs-selector-class">.py</span> \\
    <span class="hljs-attr">--model_name_or_path</span> bert-base-cased \\
    <span class="hljs-attr">--task_name</span> mrpc \\
    <span class="hljs-attr">--do_train</span> \\
    <span class="hljs-attr">--do_eval</span> \\
    <span class="hljs-attr">--max_seq_length</span> <span class="hljs-number">128</span> \\
    <span class="hljs-attr">--per_device_train_batch_size</span> <span class="hljs-variable">$BS</span> \\
    <span class="hljs-attr">--learning_rate</span> <span class="hljs-number">2</span>e-<span class="hljs-number">5</span> \\
    <span class="hljs-attr">--num_train_epochs</span> <span class="hljs-number">3.0</span> \\
    <span class="hljs-attr">--output_dir</span> /tmp/mrpc \\
    <span class="hljs-attr">--overwrite_output_dir</span> \\
    <span class="hljs-attr">--fp16</span>`}}),ks=new b({}),As=new b({}),xs=new b({}),js=new q({props:{code:`from torch.cuda.amp import autocast
with autocast(dtype=torch.bfloat16):
    loss, outputs = ...,`,highlighted:`<span class="hljs-keyword">from</span> torch.cuda.amp <span class="hljs-keyword">import</span> <span class="hljs-built_in">auto</span><span class="hljs-keyword">cast</span>
with <span class="hljs-built_in">auto</span><span class="hljs-keyword">cast</span>(dtype=torch.bfloat16):
    loss, outputs = ...`}}),Ts=new q({props:{code:`python -c 'import transformers; print(f"BF16 support is {transformers.file_utils.is_torch_bf16_available()}")',`,highlighted:'python -c &#x27;<span class="hljs-keyword">import</span> transformers; <span class="hljs-keyword">print</span>(f<span class="hljs-string">&quot;BF16 support is {transformers.file_utils.is_torch_bf16_available()}&quot;</span>)&#x27;'}}),Ns=new b({}),Cs=new b({}),Ls=new q({props:{code:`import torch
torch.backends.cuda.matmul.allow_tf32 = True,`,highlighted:`import torch
torch<span class="hljs-selector-class">.backends</span><span class="hljs-selector-class">.cuda</span><span class="hljs-selector-class">.matmul</span><span class="hljs-selector-class">.allow_tf32</span> = True`}}),Os=new b({}),Hs=new b({}),Fs=new q({props:{code:"model.gradient_checkpointing_enable(),",highlighted:"model.gradient_checkpointing_enable()"}}),qs=new b({}),Ks=new b({}),hr=new q({props:{code:`
# DP
rm -r /tmp/test-clm; CUDA_VISIBLE_DEVICES=0,1 \\
python examples/pytorch/language-modeling/run_clm.py \\
--model_name_or_path gpt2 --dataset_name wikitext --dataset_config_name wikitext-2-raw-v1 \\
--do_train --output_dir /tmp/test-clm --per_device_train_batch_size 4 --max_steps 200

{'train_runtime': 110.5948, 'train_samples_per_second': 1.808, 'epoch': 0.69}

# DDP w/ NVlink
rm -r /tmp/test-clm; CUDA_VISIBLE_DEVICES=0,1 \\
python -m torch.distributed.launch --nproc_per_node 2 examples/pytorch/language-modeling/run_clm.py \\
--model_name_or_path gpt2 --dataset_name wikitext --dataset_config_name wikitext-2-raw-v1 \\
--do_train --output_dir /tmp/test-clm --per_device_train_batch_size 4 --max_steps 200

{'train_runtime': 101.9003, 'train_samples_per_second': 1.963, 'epoch': 0.69}

# DDP w/o NVlink
rm -r /tmp/test-clm; NCCL_P2P_DISABLE=1 CUDA_VISIBLE_DEVICES=0,1 \\
python -m torch.distributed.launch --nproc_per_node 2 examples/pytorch/language-modeling/run_clm.py \\
--model_name_or_path gpt2 --dataset_name wikitext --dataset_config_name wikitext-2-raw-v1 \\
--do_train --output_dir /tmp/test-clm --per_device_train_batch_size 4 --max_steps 200

{'train_runtime': 131.4367, 'train_samples_per_second': 1.522, 'epoch': 0.69},`,highlighted:`
<span class="hljs-comment"># DP</span>
<span class="hljs-string">rm</span> -<span class="hljs-string">r</span> /<span class="hljs-string">tmp</span>/<span class="hljs-string">test-clm</span>; <span class="hljs-string">CUDA_VISIBLE_DEVICES</span>=<span class="hljs-string">0</span>,<span class="hljs-string">1</span> \\
<span class="hljs-string">python</span> <span class="hljs-string">examples</span>/<span class="hljs-string">pytorch</span>/<span class="hljs-string">language-modeling</span>/<span class="hljs-string">run_clm</span>.<span class="hljs-string">py</span> \\
<span class="hljs-built_in">--model_name_or_path</span> <span class="hljs-string">gpt2</span> <span class="hljs-built_in">--dataset_name</span> <span class="hljs-string">wikitext</span> <span class="hljs-built_in">--dataset_config_name</span> <span class="hljs-string">wikitext-2-raw-v1</span> \\
<span class="hljs-built_in">--do_train</span> <span class="hljs-built_in">--output_dir</span> /<span class="hljs-string">tmp</span>/<span class="hljs-string">test-clm</span> <span class="hljs-built_in">--per_device_train_batch_size</span> <span class="hljs-string">4</span> <span class="hljs-built_in">--max_steps</span> <span class="hljs-string">200</span>

{<span class="hljs-string">&#x27;train_runtime&#x27;</span>: <span class="hljs-string">110</span>.<span class="hljs-string">5948</span>, <span class="hljs-string">&#x27;train_samples_per_second&#x27;</span>: <span class="hljs-string">1</span>.<span class="hljs-string">808</span>, <span class="hljs-string">&#x27;epoch&#x27;</span>: <span class="hljs-string">0</span>.<span class="hljs-string">69</span>}

<span class="hljs-comment"># DDP w/ NVlink</span>
<span class="hljs-string">rm</span> -<span class="hljs-string">r</span> /<span class="hljs-string">tmp</span>/<span class="hljs-string">test-clm</span>; <span class="hljs-string">CUDA_VISIBLE_DEVICES</span>=<span class="hljs-string">0</span>,<span class="hljs-string">1</span> \\
<span class="hljs-string">python</span> -<span class="hljs-string">m</span> <span class="hljs-string">torch</span>.<span class="hljs-string">distributed</span>.<span class="hljs-string">launch</span> <span class="hljs-built_in">--nproc_per_node</span> <span class="hljs-string">2</span> <span class="hljs-string">examples</span>/<span class="hljs-string">pytorch</span>/<span class="hljs-string">language-modeling</span>/<span class="hljs-string">run_clm</span>.<span class="hljs-string">py</span> \\
<span class="hljs-built_in">--model_name_or_path</span> <span class="hljs-string">gpt2</span> <span class="hljs-built_in">--dataset_name</span> <span class="hljs-string">wikitext</span> <span class="hljs-built_in">--dataset_config_name</span> <span class="hljs-string">wikitext-2-raw-v1</span> \\
<span class="hljs-built_in">--do_train</span> <span class="hljs-built_in">--output_dir</span> /<span class="hljs-string">tmp</span>/<span class="hljs-string">test-clm</span> <span class="hljs-built_in">--per_device_train_batch_size</span> <span class="hljs-string">4</span> <span class="hljs-built_in">--max_steps</span> <span class="hljs-string">200</span>

{<span class="hljs-string">&#x27;train_runtime&#x27;</span>: <span class="hljs-string">101</span>.<span class="hljs-string">9003</span>, <span class="hljs-string">&#x27;train_samples_per_second&#x27;</span>: <span class="hljs-string">1</span>.<span class="hljs-string">963</span>, <span class="hljs-string">&#x27;epoch&#x27;</span>: <span class="hljs-string">0</span>.<span class="hljs-string">69</span>}

<span class="hljs-comment"># DDP w/o NVlink</span>
<span class="hljs-string">rm</span> -<span class="hljs-string">r</span> /<span class="hljs-string">tmp</span>/<span class="hljs-string">test-clm</span>; <span class="hljs-string">NCCL_P2P_DISABLE</span>=<span class="hljs-string">1</span> <span class="hljs-string">CUDA_VISIBLE_DEVICES</span>=<span class="hljs-string">0</span>,<span class="hljs-string">1</span> \\
<span class="hljs-string">python</span> -<span class="hljs-string">m</span> <span class="hljs-string">torch</span>.<span class="hljs-string">distributed</span>.<span class="hljs-string">launch</span> <span class="hljs-built_in">--nproc_per_node</span> <span class="hljs-string">2</span> <span class="hljs-string">examples</span>/<span class="hljs-string">pytorch</span>/<span class="hljs-string">language-modeling</span>/<span class="hljs-string">run_clm</span>.<span class="hljs-string">py</span> \\
<span class="hljs-built_in">--model_name_or_path</span> <span class="hljs-string">gpt2</span> <span class="hljs-built_in">--dataset_name</span> <span class="hljs-string">wikitext</span> <span class="hljs-built_in">--dataset_config_name</span> <span class="hljs-string">wikitext-2-raw-v1</span> \\
<span class="hljs-built_in">--do_train</span> <span class="hljs-built_in">--output_dir</span> /<span class="hljs-string">tmp</span>/<span class="hljs-string">test-clm</span> <span class="hljs-built_in">--per_device_train_batch_size</span> <span class="hljs-string">4</span> <span class="hljs-built_in">--max_steps</span> <span class="hljs-string">200</span>

{<span class="hljs-string">&#x27;train_runtime&#x27;</span>: <span class="hljs-string">131</span>.<span class="hljs-string">4367</span>, <span class="hljs-string">&#x27;train_samples_per_second&#x27;</span>: <span class="hljs-string">1</span>.<span class="hljs-string">522</span>, <span class="hljs-string">&#x27;epoch&#x27;</span>: <span class="hljs-string">0</span>.<span class="hljs-string">69</span>}`}}),fr=new b({}),cr=new b({}),ur=new b({}),mr=new b({}),$r=new b({}),Dr=new b({}),{c(){R=s("meta"),Tr=h(),j=s("h1"),G=s("a"),Io=s("span"),d(ya.$$.fragment),Iv=h(),So=s("span"),Sv=i("Performance and Scalability: How To Fit a Bigger Model and Train It Faster"),sf=h(),Ir=s("p"),Nv=i("For now the software sections of this document are mainly Pytorch-specific, but the guide can be extended to other frameworks in the future."),rf=h(),ne=s("h2"),Qe=s("a"),No=s("span"),d(ga.$$.fragment),Cv=h(),Co=s("span"),Lv=i("Quick notes"),lf=h(),Sr=s("p"),Gv=i("This section gives brief ideas on how to make training faster and support bigger models. Later sections will expand, demonstrate and elucidate each of these."),of=h(),pe=s("h3"),Ze=s("a"),Lo=s("span"),d(Ea.$$.fragment),Uv=h(),Go=s("span"),Mv=i("Faster Training"),nf=h(),Nr=s("p"),Ov=i("Hardware:"),pf=h(),Cr=s("ul"),Lr=s("li"),zv=i("fast connectivity between GPUs"),Pa=s("ul"),Uo=s("li"),Vv=i("intra-node: NVLink"),Hv=h(),Mo=s("li"),Bv=i("inter-node: Infiniband / Intel OPA"),hf=h(),Gr=s("p"),Fv=i("Software:"),ff=h(),Je=s("ul"),Oo=s("li"),qv=i("Data Parallel / Distributed Data Parallel"),Rv=h(),zo=s("li"),Wv=i("fp16 (autocast caching)"),cf=h(),he=s("h3"),Ke=s("a"),Vo=s("span"),d(ka.$$.fragment),Xv=h(),Ho=s("span"),Yv=i("Bigger Models"),df=h(),Ur=s("p"),Qv=i("Hardware:"),uf=h(),W=s("ul"),Bo=s("li"),Zv=i("bigger GPUs"),Jv=h(),Fo=s("li"),Kv=i("more GPUs"),e1=h(),$a=s("li"),t1=i("more CPU and NVMe (offloaded to by "),Mr=s("a"),a1=i("DeepSpeed-Infinity"),s1=i(")"),mf=h(),Or=s("p"),r1=i("Software:"),vf=h(),y=s("ul"),qo=s("li"),l1=i("Model Scalability (ZeRO and 3D Parallelism)"),i1=h(),Ro=s("li"),o1=i("Low-memory Optimizers"),n1=h(),Wo=s("li"),p1=i("fp16/bf16 (smaller data/faster throughput)"),h1=h(),Xo=s("li"),f1=i("tf32 (faster throughput)"),c1=h(),Yo=s("li"),d1=i("Gradient accumulation"),u1=h(),Qo=s("li"),m1=i("Gradient checkpointing"),v1=h(),Zo=s("li"),w1=i("Sparsity"),wf=h(),fe=s("h2"),et=s("a"),Jo=s("span"),d(Aa.$$.fragment),_1=h(),Ko=s("span"),b1=i("Hardware"),_f=h(),ce=s("h3"),tt=s("a"),en=s("span"),d(xa.$$.fragment),y1=h(),tn=s("span"),g1=i("Power and Cooling"),bf=h(),zr=s("p"),E1=i("If you bought an expensive high end GPU make sure you give it the correct power and sufficient cooling."),yf=h(),ja=s("p"),an=s("strong"),P1=i("Power"),k1=i(":"),gf=h(),Vr=s("p"),$1=i("Some high end consumer GPU cards have 2 and sometimes 3 PCI-E 8-Pin power sockets. Make sure you have as many independent 12V PCI-E 8-Pin cables plugged into the card as there are sockets. Do not use the 2 splits at one end of the same cable (also known as pigtail cable). That is if you have 2 sockets on the GPU, you want 2 PCI-E 8-Pin cables going from your PSU to the card and not one that has 2 PCI-E 8-Pin connectors at the end! You won\u2019t get the full performance out of your card otherwise."),Ef=h(),Hr=s("p"),A1=i("Each PCI-E 8-Pin power cable needs to be plugged into a 12V rail on the PSU side and can supply up to 150W of power."),Pf=h(),Br=s("p"),x1=i("Some other cards may use a PCI-E 12-Pin connectors, and these can deliver up to 500-600W of power."),kf=h(),Fr=s("p"),j1=i("Low end cards may use 6-Pin connectors, which supply up to 75W of power."),$f=h(),qr=s("p"),D1=i("Additionally you want the high-end PSU that has stable voltage. Some lower quality ones may not give the card the stable voltage it needs to function at its peak."),Af=h(),Rr=s("p"),T1=i("And of course the PSU needs to have enough unused Watts to power the card."),xf=h(),Da=s("p"),sn=s("strong"),I1=i("Cooling"),S1=i(":"),jf=h(),Wr=s("p"),N1=i("When a GPU gets overheated it would start throttling down and will not deliver full performance. And it will shutdown if it gets too hot."),Df=h(),Xr=s("p"),C1=i("It\u2019s hard to tell the exact best temperature to strive for when a GPU is heavily loaded, but probably anything under +80C is good, but lower is better - perhaps 70-75C is an excellent range to be in. The throttling down is likely to start at around 84-90C. But other than throttling performance a prolonged very higher temperature is likely to reduce the lifespan of a GPU."),Tf=h(),de=s("h3"),at=s("a"),rn=s("span"),d(Ta.$$.fragment),L1=h(),ln=s("span"),G1=i("Multi-GPU Connectivity"),If=h(),Yr=s("p"),U1=i("If you use multiple GPUs the way cards are inter-connected can have a huge impact on the total training time."),Sf=h(),Qr=s("p"),M1=i("If the GPUs are on the same physical node, you can run:"),Nf=h(),d(Ia.$$.fragment),Cf=h(),Zr=s("p"),O1=i("and it will tell you how the GPUs are inter-connected."),Lf=h(),Jr=s("p"),z1=i("On a machine with dual-GPU and which are connected with NVLink, you will most likely see something like:"),Gf=h(),d(Sa.$$.fragment),Uf=h(),Kr=s("p"),V1=i("on a different machine w/o NVLink we may see:"),Mf=h(),d(Na.$$.fragment),Of=h(),el=s("p"),H1=i("The report includes this legend:"),zf=h(),d(Ca.$$.fragment),Vf=h(),X=s("p"),B1=i("So the first report "),on=s("code"),F1=i("NV2"),q1=i(" tells us the GPUs are interconnected with 2 NVLinks, and the second report "),nn=s("code"),R1=i("PHB"),W1=i(" we have a typical consumer-level PCIe+Bridge setup."),Hf=h(),tl=s("p"),X1=i("Check what type of connectivity you have on your setup. Some of these will make the communication between cards faster (e.g. NVLink), others slower (e.g. PHB)."),Bf=h(),al=s("p"),Y1=i("Depending on the type of scalability solution used, the connectivity speed could have a major or a minor impact. If the GPUs need to sync rarely, as in DDP, the impact of a slower connection will be less significant. If the GPUs need to send messages to each other often, as in ZeRO-DP, then faster connectivity becomes super important to achieve faster training."),Ff=h(),ue=s("h3"),st=s("a"),pn=s("span"),d(La.$$.fragment),Q1=h(),hn=s("span"),Z1=i("NVlink"),qf=h(),Ga=s("p"),Ua=s("a"),J1=i("NVLink"),K1=i(" is a wire-based serial multi-lane near-range communications link developed by Nvidia."),Rf=h(),rt=s("p"),ew=i("Each new generation provides a faster bandwidth, e.g. here is a quote from "),Ma=s("a"),tw=i("Nvidia Ampere GA102 GPU Architecture"),aw=i(":"),Wf=h(),sl=s("blockquote"),fn=s("p"),sw=i(`Third-Generation NVLink\xAE
GA102 GPUs utilize NVIDIA\u2019s third-generation NVLink interface, which includes four x4 links,
with each link providing 14.0625 GB/sec bandwidth in each direction between two GPUs. Four
links provide 56.25 GB/sec bandwidth in each direction, and 112.5 GB/sec total bandwidth
between two GPUs. Two RTX 3090 GPUs can be connected together for SLI using NVLink.
(Note that 3-Way and 4-Way SLI configurations are not supported.)`),Xf=h(),D=s("p"),rw=i("So the higher "),cn=s("code"),lw=i("X"),iw=i(" you get in the report of "),dn=s("code"),ow=i("NVX"),nw=i(" in the output of "),un=s("code"),pw=i("nvidia-smi topo -m"),hw=i(" the better. The generation will depend on your GPU architecture."),Yf=h(),rl=s("p"),fw=i("Let\u2019s compare the execution of a gpt2 language model training over a small sample of wikitext."),Qf=h(),ll=s("p"),cw=i("The results are:"),Zf=h(),lt=s("table"),mn=s("thead"),Oa=s("tr"),vn=s("th"),dw=i("NVlink"),uw=h(),il=s("th"),mw=i("Time"),vw=h(),za=s("tbody"),Va=s("tr"),wn=s("td"),ww=i("Y"),_w=h(),ol=s("td"),bw=i("101s"),yw=h(),Ha=s("tr"),_n=s("td"),gw=i("N"),Ew=h(),nl=s("td"),Pw=i("131s"),Jf=h(),pl=s("p"),kw=i("You can see that NVLink completes the training ~23% faster."),Kf=h(),it=s("p"),$w=i("In the second benchmark we use "),bn=s("code"),Aw=i("NCCL_P2P_DISABLE=1"),xw=i(" to tell the GPUs not to use NVLink."),ec=h(),hl=s("p"),jw=i("Here is the full benchmark code and outputs:"),tc=h(),d(Ba.$$.fragment),ac=h(),P=s("p"),Dw=i("Hardware: 2x TITAN RTX 24GB each + NVlink with 2 NVLinks ("),yn=s("code"),Tw=i("NV2"),Iw=i(" in "),gn=s("code"),Sw=i("nvidia-smi topo -m"),Nw=i(`)
Software: `),En=s("code"),Cw=i("pytorch-1.8-to-be"),Lw=i(" + "),Pn=s("code"),Gw=i("cuda-11.0"),Uw=i(" / "),kn=s("code"),Mw=i("transformers==4.3.0.dev0"),sc=h(),me=s("h2"),ot=s("a"),$n=s("span"),d(Fa.$$.fragment),Ow=h(),An=s("span"),zw=i("Software"),rc=h(),ve=s("h3"),nt=s("a"),xn=s("span"),d(qa.$$.fragment),Vw=h(),jn=s("span"),Hw=i("Model Scalability"),lc=h(),fl=s("p"),Bw=i("When you can\u2019t fit a model into the available GPU memory, you need to start using a solution that allows you to scale a large model to use multiple GPUs in parallel."),ic=h(),Ra=s("p"),Fw=i("For indepth details on ZeRO and various other model parallelism protocols please see: "),cl=s("a"),qw=i("Model Parallelism"),oc=h(),we=s("h3"),pt=s("a"),Dn=s("span"),d(Wa.$$.fragment),Rw=h(),Tn=s("span"),Ww=i("Anatomy of Model's Operations"),nc=h(),dl=s("p"),Xw=i("Transformers architecture includes 3 main groups of operations grouped below by compute-intensity."),pc=h(),Y=s("ol"),Xa=s("li"),In=s("p"),Sn=s("strong"),Yw=i("Tensor Contractions"),Qw=h(),Ya=s("p"),Zw=i("Linear layers and components of Multi-Head Attention all do batched "),Nn=s("strong"),Jw=i("matrix-matrix multiplications"),Kw=i(". These operations are the most compute-intensive part of training a transformer."),e_=h(),Qa=s("li"),Cn=s("p"),Ln=s("strong"),t_=i("Statistical Normalizations"),a_=h(),Za=s("p"),s_=i("Softmax and layer normalization are less compute-intensive than tensor contractions, and involve one or more "),Gn=s("strong"),r_=i("reduction operations"),l_=i(", the result of which is then applied via a map."),i_=h(),Ja=s("li"),Un=s("p"),Mn=s("strong"),o_=i("Element-wise Operators"),n_=h(),Ka=s("p"),p_=i("These are the remaining operators: "),On=s("strong"),h_=i("biases, dropout, activations, and residual connections"),f_=i(". These are the least compute-intensive operations."),hc=h(),ul=s("p"),c_=i("This knowledge can be helpful to know when analyzing performance bottlenecks."),fc=h(),es=s("p"),d_=i("This summary is derived from "),ts=s("a"),u_=i("Data Movement Is All You Need: A Case Study on Optimizing Transformers 2020"),cc=h(),_e=s("h3"),ht=s("a"),zn=s("span"),d(as.$$.fragment),m_=h(),Vn=s("span"),v_=i("Anatomy of Model's Memory"),dc=h(),ml=s("p"),w_=i("The components on GPU memory are the following:"),uc=h(),E=s("ol"),Hn=s("li"),__=i("model weights"),b_=h(),Bn=s("li"),y_=i("optimizer states"),g_=h(),Fn=s("li"),E_=i("gradients"),P_=h(),qn=s("li"),k_=i("forward activations saved for gradient computation"),$_=h(),Rn=s("li"),A_=i("temporary buffers"),x_=h(),Wn=s("li"),j_=i("functionality-specific memory"),mc=h(),vl=s("p"),D_=i("A typical model trained in mixed precision with AdamW requires 18 bytes per model parameter plus activation memory."),vc=h(),wl=s("p"),T_=i("For inference there are no optimizer states and gradients, so we can subtract those. And thus we end up with 6 bytes per model parameter for mixed precision inference, plus activation memory."),wc=h(),_l=s("p"),I_=i("Let\u2019s look at the details."),_c=h(),be=s("h4"),ft=s("a"),Xn=s("span"),d(ss.$$.fragment),S_=h(),Yn=s("span"),N_=i("Model Weights"),bc=h(),ct=s("ul"),Qn=s("li"),C_=i("4 bytes * number of parameters for fp32 training"),L_=h(),Zn=s("li"),G_=i("6 bytes * number of parameters for mixed precision training"),yc=h(),ye=s("h4"),dt=s("a"),Jn=s("span"),d(rs.$$.fragment),U_=h(),Kn=s("span"),M_=i("Optimizer States"),gc=h(),Q=s("ul"),ep=s("li"),O_=i("8 bytes * number of parameters for normal AdamW (maintains 2 states)"),z_=h(),bl=s("li"),V_=i("2 bytes * number of parameters for 8-bit AdamW optimizers like "),ls=s("a"),H_=i("bitsandbytes"),B_=h(),tp=s("li"),F_=i("4 bytes * number of parameters for optimizers like SGD (maintains only 1 state)"),Ec=h(),ge=s("h4"),ut=s("a"),ap=s("span"),d(is.$$.fragment),q_=h(),sp=s("span"),R_=i("Gradients"),Pc=h(),yl=s("ul"),rp=s("li"),W_=i("4 bytes * number of parameters for either fp32 or mixed precision training"),kc=h(),Ee=s("h4"),mt=s("a"),lp=s("span"),d(os.$$.fragment),X_=h(),ip=s("span"),Y_=i("Forward Activations"),$c=h(),gl=s("ul"),op=s("li"),Q_=i("size depends on many factors, the key ones being sequence length, hidden size and batch size."),Ac=h(),El=s("p"),Z_=i("There are the input and output that are being passed and returned by the forward and the backward functions and the forward activations saved for gradient computation."),xc=h(),Pe=s("h4"),vt=s("a"),np=s("span"),d(ns.$$.fragment),J_=h(),pp=s("span"),K_=i("Temporary Memory"),jc=h(),Pl=s("p"),eb=i("Additionally there are all kinds of temporary variables which get released once the calculation is done, but in the moment these could require additional memory and could push to OOM. Therefore when coding it\u2019s crucial to think strategically about such temporary variables and sometimes to explicitly free those as soon as they are no longer needed."),Dc=h(),ke=s("h4"),wt=s("a"),hp=s("span"),d(ps.$$.fragment),tb=h(),fp=s("span"),ab=i("Functionality-specific memory"),Tc=h(),kl=s("p"),sb=i("Then your software could have special memory needs. For example, when generating text using beam search, the software needs to maintain multiple copies of inputs and outputs."),Ic=h(),$e=s("h3"),_t=s("a"),cp=s("span"),d(hs.$$.fragment),rb=h(),bt=s("span"),dp=s("code"),lb=i("forward"),ib=i(" vs "),up=s("code"),ob=i("backward"),nb=i(" Execution Speed"),Sc=h(),$l=s("p"),pb=i("For convolutions and linear layers there are 2x flops in the backward compared to the forward, which generally translates into ~2x slower (sometimes more, because sizes in the backward tend to be more awkward). Activations are usually bandwidth-limited, and it\u2019s typical for an activation to have to read more data in the backward than in the forward (e.g. activation forward reads once, writes once, activation backward reads twice, gradOutput and output of the forward, and writes once, gradInput)."),Nc=h(),Ae=s("h3"),yt=s("a"),mp=s("span"),d(fs.$$.fragment),hb=h(),vp=s("span"),fb=i("Floating Data Types"),Cc=h(),Al=s("p"),cb=i("Here are the commonly used floating point data types choice of which impacts both memory usage and throughput:"),Lc=h(),T=s("ul"),cs=s("li"),db=i("fp32 ("),wp=s("code"),ub=i("float32"),mb=i(")"),vb=h(),ds=s("li"),wb=i("fp16 ("),_p=s("code"),_b=i("float16"),bb=i(")"),yb=h(),us=s("li"),gb=i("bf16 ("),bp=s("code"),Eb=i("bfloat16"),Pb=i(")"),kb=h(),yp=s("li"),$b=i("tf32 (CUDA internal data type)"),Gc=h(),xl=s("p"),Ab=i("Here is a diagram that shows how these data types correlate to each other."),Uc=h(),jl=s("p"),Dl=s("img"),Mc=h(),gt=s("p"),xb=i("(source: "),ms=s("a"),jb=i("NVIDIA Blog"),Db=i(")"),Oc=h(),Tl=s("p"),Tb=i("While fp16 and fp32 have been around for quite some time, bf16 and tf32 are only available on the Ampere architecture GPUS. TPUs support bf16 as well."),zc=h(),xe=s("h4"),Et=s("a"),gp=s("span"),d(vs.$$.fragment),Ib=h(),Ep=s("span"),Sb=i("fp16"),Vc=h(),Il=s("p"),Nb=i("AMP = Automatic Mixed Precision"),Hc=h(),Sl=s("p"),Cb=i("If we look at what\u2019s happening with FP16 training (mixed precision) we have:"),Bc=h(),I=s("ul"),Pp=s("li"),Lb=i("the model has two copies in memory: one in half-precision for the forward/backward computations and one in full precision - no memory saved here"),Gb=h(),kp=s("li"),Ub=i("the forward activations saved for gradient computation are in half-precision - memory is saved here"),Mb=h(),ws=s("li"),Ob=i("the gradients are computed in half-precision "),$p=s("em"),zb=i("but"),Vb=i(" converted to full-precision for the update, no saving there"),Hb=h(),Ap=s("li"),Bb=i("the optimizer states are in full precision as all the updates are done in full-precision"),Fc=h(),Nl=s("p"),Fb=i("So the savings only happen for the forward activations saved for the backward computation, and there is a slight overhead because the model weights are stored both in half- and full-precision."),qc=h(),Pt=s("p"),qb=i("In \u{1F917} Transformers fp16 mixed precision is enabled by passing "),xp=s("code"),Rb=i("--fp16"),Wb=i(" to the \u{1F917} Trainer."),Rc=h(),Cl=s("p"),Xb=i("Now let\u2019s look at a simple text-classification fine-tuning on 2 GPUs (I\u2019m giving the command for reference):"),Wc=h(),d(_s.$$.fragment),Xc=h(),kt=s("p"),Yb=i("Since the only savings we get are in the model activations saved for the backward passed, it\u2019s logical that the bigger those activations are, the bigger the saving will be. If we try different batch sizes, I indeed get (this is with "),jp=s("code"),Qb=i("nvidia-smi"),Zb=i(" so not completely reliable as said above but it will be a fair comparison):"),Yc=h(),$t=s("table"),Dp=s("thead"),U=s("tr"),Ll=s("th"),Jb=i("batch size"),Kb=h(),Gl=s("th"),ey=i("w/o \u2014fp16"),ty=h(),Ul=s("th"),ay=i("w/ \u2014fp16"),sy=h(),Ml=s("th"),ry=i("savings"),ly=h(),M=s("tbody"),O=s("tr"),Ol=s("td"),iy=i("8"),oy=h(),zl=s("td"),ny=i("4247"),py=h(),Vl=s("td"),hy=i("4163"),fy=h(),Hl=s("td"),cy=i("84"),dy=h(),z=s("tr"),Bl=s("td"),uy=i("16"),my=h(),Fl=s("td"),vy=i("4971"),wy=h(),ql=s("td"),_y=i("4793"),by=h(),Rl=s("td"),yy=i("178"),gy=h(),V=s("tr"),Wl=s("td"),Ey=i("32"),Py=h(),Xl=s("td"),ky=i("6827"),$y=h(),Yl=s("td"),Ay=i("6207"),xy=h(),Ql=s("td"),jy=i("620"),Dy=h(),H=s("tr"),Zl=s("td"),Ty=i("64"),Iy=h(),Jl=s("td"),Sy=i("10037"),Ny=h(),Kl=s("td"),Cy=i("8061"),Ly=h(),ei=s("td"),Gy=i("1976"),Qc=h(),At=s("p"),Uy=i("So there is only a real memory saving if we train at a high batch size (and it\u2019s not half) and at batch sizes lower than 8, you actually get a bigger memory footprint (because of the overhead mentioned above). The gain for FP16 training is that in each of those cases, the training with the flag "),Tp=s("code"),My=i("--fp16"),Oy=i(" is twice as fast, which does require every tensor to have every dimension be a multiple of 8 (examples pad the tensors to a sequence length that is a multiple of 8)."),Zc=h(),ti=s("p"),zy=i("Summary: FP16 with apex or AMP will only give you some memory savings with a reasonably high batch size."),Jc=h(),ai=s("p"),Vy=i("Additionally, under mixed precision when possible, it\u2019s important that the batch size is a multiple of 8 to efficiently use tensor cores."),Kc=h(),xt=s("p"),Hy=i("Note that in some situations the speed up can be as big as 5x when using mixed precision. e.g. we have observed that while using "),bs=s("a"),By=i("Megatron-Deepspeed"),Fy=i("."),ed=h(),si=s("p"),qy=i("Some amazing tutorials to read on mixed precision:"),td=h(),jt=s("ul"),ri=s("li"),Ry=i("@sgugger wrote a great explanation of mixed precision "),ys=s("a"),Wy=i("here"),Xy=h(),li=s("li"),Yy=i("Aleksey Bilogur\u2019s "),gs=s("a"),Qy=i("A developer-friendly guide to mixed precision training with PyTorch"),ad=h(),Z=s("p"),Zy=i(`You can also see a variety of benchmarks on fp16 vs other precisions:
`),Es=s("a"),Jy=i("RTX-3090"),Ky=i(` and
`),Ps=s("a"),e2=i("A100"),t2=i("."),sd=h(),je=s("h5"),Dt=s("a"),Ip=s("span"),d(ks.$$.fragment),a2=h(),Sp=s("span"),s2=i("fp16 caching"),rd=h(),J=s("p"),r2=i("pytorch "),Np=s("code"),l2=i("autocast"),i2=i(" which performs AMP include a caching feature, which speed things up by caching fp16-converted values. Here is the full description from this "),$s=s("a"),o2=i("comment"),n2=i(":"),ld=h(),ii=s("p"),p2=i("Autocast maintains a cache of the FP16 casts of model parameters (leaves). This helps streamline parameter reuse: if the same FP32 param is used in several different FP16list ops, like several matmuls, instead of re-casting the param to FP16 on entering each matmul, the cast will occur on the first matmul, the casted FP16 copy will be cached, and for all later matmuls the FP16 copy will be reused. The cache is maintained only within a particular outermost autocast context. When you exit the autocast context the cache is dropped. For recommended usage, in which autocast wraps the forward pass, and then you exit the context before calling backward(), this means the cache only lasts the duration of the forward pass each iteration, and will be rebuilt next iteration. (The cache of FP16-casted copies MUST be rebuilt each iteration. The FP32 parameters get updated by the optimizer, so the FP16 copies must be recreated, otherwise the FP16 values will be stale.)"),id=h(),De=s("h5"),Tt=s("a"),Cp=s("span"),d(As.$$.fragment),h2=h(),Lp=s("span"),f2=i("fp16 Inference"),od=h(),oi=s("p"),c2=i("While normally inference is done with fp16/amp as with training, it\u2019s also possible to use the full fp16 mode without using mixed precision. This is especially a good fit if the pretrained model weights are already in fp16. So a lot less memory is used: 2 bytes per parameter vs 6 bytes with mixed precision!"),nd=h(),ni=s("p"),d2=i("How good the results this will deliver will depend on the model. If it can handle fp16 without overflows and accuracy issues, then it\u2019ll definitely better to use the full fp16 mode."),pd=h(),pi=s("p"),u2=i("For example, LayerNorm has to be done in fp32 and recent pytorch (1.10+) has been fixed to do that regardless of the input types, but earlier pytorch versions accumulate in the input type which can be an issue."),hd=h(),It=s("p"),m2=i("In \u{1F917} Transformers the full fp16 inference is enabled by passing "),Gp=s("code"),v2=i("--fp16_full_eval"),w2=i(" to the \u{1F917} Trainer."),fd=h(),Te=s("h4"),St=s("a"),Up=s("span"),d(xs.$$.fragment),_2=h(),Mp=s("span"),b2=i("bf16"),cd=h(),K=s("p"),y2=i("If you own Ampere or newer hardware you can start using bf16 for your training and evaluation. While bf16 has a worse precision than fp16, it has a much much bigger dynamic range. Therefore, if in the past you were experiencing overflow issues while training the model, bf16 will prevent this from happening most of the time. Remember that in fp16 the biggest number you can have is "),Op=s("code"),g2=i("65535"),E2=i(" and any number above that will overflow. A bf16 number can be as large as "),zp=s("code"),P2=i("3.39e+38"),k2=i(" (!) which is about the same as fp32 - because both have 8-bits used for the numerical range."),dd=h(),hi=s("p"),$2=i("Automatic Mixed Precision (AMP) is the same as with fp16, except it\u2019ll use bf16."),ud=h(),fi=s("p"),A2=i("Thanks to the fp32-like dynamic range with bf16 mixed precision loss scaling is no longer needed."),md=h(),ci=s("p"),x2=i("If you have tried to finetune models pre-trained under bf16 mixed precision (e.g. T5) it\u2019s very likely that you have encountered overflow issues. Now you should be able to finetune those models without any issues."),vd=h(),di=s("p"),j2=i("That said, also be aware that if you pre-trained a model in bf16, it\u2019s likely to have overflow issues if someone tries to finetune it in fp16 down the road. So once started on the bf16-mode path it\u2019s best to remain on it and not switch to fp16."),wd=h(),Nt=s("p"),D2=i("In \u{1F917} Transformers bf16 mixed precision is enabled by passing "),Vp=s("code"),T2=i("--bf16"),I2=i(" to the \u{1F917} Trainer."),_d=h(),ui=s("p"),S2=i("If you use your own trainer, this is just:"),bd=h(),d(js.$$.fragment),yd=h(),Ds=s("p"),N2=i("If you need to switch a tensor to bf16, it\u2019s just: "),Hp=s("code"),C2=i("t.to(dtype=torch.bfloat16)"),gd=h(),mi=s("p"),L2=i("Here is how you can check if your setup supports bf16:"),Ed=h(),d(Ts.$$.fragment),Pd=h(),vi=s("p"),G2=i("On the other hand bf16 has a much worse precision than fp16, so there are certain situations where you\u2019d still want to use fp16 and not bf16."),kd=h(),ee=s("p"),U2=i(`You can also see a variety of benchmarks on bf16 vs other precisions:
`),Is=s("a"),M2=i("RTX-3090"),O2=i(` and
`),Ss=s("a"),z2=i("A100"),V2=i("."),$d=h(),Ie=s("h5"),Ct=s("a"),Bp=s("span"),d(Ns.$$.fragment),H2=h(),Fp=s("span"),B2=i("bf16 Inference"),Ad=h(),Lt=s("p"),F2=i("Same as with fp16, you can do inference in either the mixed precision bf16 or using the full bf16 mode. The same caveats apply. For details see "),wi=s("a"),q2=i("fp16 Inference"),R2=i("."),xd=h(),Gt=s("p"),W2=i("In \u{1F917} Transformers the full bf16 inference is enabled by passing "),qp=s("code"),X2=i("--bf16_full_eval"),Y2=i(" to the \u{1F917} Trainer."),jd=h(),Se=s("h4"),Ut=s("a"),Rp=s("span"),d(Cs.$$.fragment),Q2=h(),Wp=s("span"),Z2=i("tf32"),Dd=h(),_i=s("p"),J2=i("The Ampere hardware uses a magical data type called tf32. It has the same numerical range as fp32 (8-bits), but instead of 23 bits precision it has only 10 bits (same as fp16). In total it uses only 19 bits."),Td=h(),bi=s("p"),K2=i("It\u2019s magical in the sense that you can use the normal fp32 training and/or inference code and by enabling tf32 support you can get up to 3x throughput improvement. All you need to do is to add this to your code:"),Id=h(),d(Ls.$$.fragment),Sd=h(),yi=s("p"),eg=i("When this is done CUDA will automatically switch to using tf32 instead of fp32 where it\u2019s possible. This, of course, assumes that the used GPU is from the Ampere series."),Nd=h(),Mt=s("p"),tg=i("Like all cases with reduced precision this may or may not be satisfactory for your needs, so you have to experiment and see. According to "),Gs=s("a"),ag=i("NVIDIA research"),sg=i(" the majority of machine learning training shouldn\u2019t be impacted and showed the same perplexity and convergence as the fp32 training."),Cd=h(),gi=s("p"),rg=i("If you\u2019re already using fp16 or bf16 mixed precision it may help with the throughput as well."),Ld=h(),S=s("p"),lg=i("You can enable this mode in the \u{1F917} Trainer with "),Xp=s("code"),ig=i("--tf32"),og=i(", or disable it with "),Yp=s("code"),ng=i("--tf32 0"),pg=i(" or "),Qp=s("code"),hg=i("--no_tf32"),fg=i(`.
By default the PyTorch default is used.`),Gd=h(),te=s("p"),cg=i("Note: tf32 mode is internal to CUDA and can\u2019t be accessed directly via "),Zp=s("code"),dg=i("tensor.to(dtype=torch.tf32)"),ug=i(" as "),Jp=s("code"),mg=i("torch.tf32"),vg=i(" doesn\u2019t exit."),Ud=h(),Ot=s("p"),wg=i("Note: you need "),Kp=s("code"),_g=i("torch>=1.7"),bg=i(" to enjoy this feature."),Md=h(),ae=s("p"),yg=i(`You can also see a variety of benchmarks on tf32 vs other precisions:
`),Us=s("a"),gg=i("RTX-3090"),Eg=i(` and
`),Ms=s("a"),Pg=i("A100"),kg=i("."),Od=h(),Ne=s("h3"),zt=s("a"),eh=s("span"),d(Os.$$.fragment),$g=h(),th=s("span"),Ag=i("Gradient Accumulation"),zd=h(),se=s("p"),xg=i("Since gradient accumulation essentially is identical to having a larger batch size, just as with the larger batch size here you are likely to see a 20-30% speedup due to the optimizer running less often. For example, see benchmarks for "),zs=s("a"),jg=i("RTX-3090"),Dg=i(`
and `),Vs=s("a"),Tg=i("A100"),Ig=i("."),Vd=h(),Vt=s("p"),Sg=i("To activate this feature in \u{1F917} Trainer add "),ah=s("code"),Ng=i("--gradient_accumulation_steps 4"),Cg=i(" to its arguments (experiment with the value to get the best performance)."),Hd=h(),Ei=s("p"),Lg=i("It\u2019s important to remember that using gradient accumulation you may end up with a much larger effective batch size, so you may need to adjust the learning rate, its warm up and for very short datasets it\u2019ll impact the loss as the training will end up doing less steps than normal."),Bd=h(),Ce=s("h3"),Ht=s("a"),sh=s("span"),d(Hs.$$.fragment),Gg=h(),rh=s("span"),Ug=i("Gradient Checkpointing"),Fd=h(),Pi=s("p"),Mg=i("One way to use significantly less GPU memory is to enabled \u201CGradient Checkpointing\u201D (also known as \u201Cactivation checkpointing\u201D). When enabled, a lot of memory can be freed at the cost of small decrease in the training speed due to recomputing parts of the graph during back-propagation. The slowdown will depend on the model but quite often it is around 20-30%."),qd=h(),N=s("p"),Og=i("This technique was first shared in the paper: "),Bs=s("a"),zg=i("Training Deep Nets with Sublinear Memory Cost"),Vg=i(". The paper will also give you the exact details on the savings, but it\u2019s in the ballpark of "),lh=s("code"),Hg=i("O(sqrt(n))"),Bg=i(", where "),ih=s("code"),Fg=i("n"),qg=i(" is the number of feed-forward layers."),Rd=h(),ki=s("p"),Rg=i("To activate this feature in \u{1F917} Transformers for models that support it, use:"),Wd=h(),d(Fs.$$.fragment),Xd=h(),Bt=s("p"),Wg=i("or add "),oh=s("code"),Xg=i("--gradient_checkpointing"),Yg=i(" to the Trainer arguments."),Yd=h(),Le=s("h3"),Ft=s("a"),nh=s("span"),d(qs.$$.fragment),Qg=h(),ph=s("span"),Zg=i("Batch sizes"),Qd=h(),$i=s("p"),Jg=i("One gets the most efficient performance when batch sizes and input/output neuron counts are divisible by a certain number, which typically starts at 8, but can be much higher as well. That number varies a lot depending on the specific hardware being used and the dtype of the model."),Zd=h(),re=s("p"),Kg=i("For example for fully connected layers (which correspond to GEMMs), NVIDIA provides recommendations for "),Rs=s("a"),e3=i("input/output neuron counts"),t3=i(" and "),Ws=s("a"),a3=i("batch size"),s3=i("."),Jd=h(),Xs=s("p"),Ys=s("a"),r3=i("Tensor Core Requirements"),l3=i(" define the multiplier based on the dtype and the hardware. For example, for fp16 a multiple of 8 is recommended, but on A100 it\u2019s 64!"),Kd=h(),qt=s("p"),i3=i("For parameters that are small, there is also "),Qs=s("a"),o3=i("Dimension Quantization Effects"),n3=i(" to consider, this is where tiling happens and the right multiplier can have a significant speedup."),eu=h(),C=s("p"),p3=i("Additionally, as explained in the "),Ai=s("a"),h3=i("Gradient Accumulation"),f3=i(` section, the bigger the batch size the less often the optimizer is run, the faster the training is (considering the same dataset length). See benchmarks
for `),Zs=s("a"),c3=i("RTX-3090"),d3=i(`
and `),Js=s("a"),u3=i("A100"),m3=i("."),tu=h(),Ge=s("h3"),Rt=s("a"),hh=s("span"),d(Ks.$$.fragment),v3=h(),fh=s("span"),w3=i("DP vs DDP"),au=h(),Ue=s("p"),ch=s("code"),_3=i("DistributedDataParallel"),b3=i(" (DDP) is typically faster than "),dh=s("code"),y3=i("DataParallel"),g3=i(" (DP), but it is not always the case:"),su=h(),Wt=s("ul"),uh=s("li"),E3=i("while DP is python threads-based, DDP is multiprocess-based - and as such it has no python threads limitations, such as GIL"),P3=h(),mh=s("li"),k3=i("on the other hand a slow inter-connectivity between the GPU cards could lead to an actual slower outcome with DDP"),ru=h(),xi=s("p"),$3=i("Here are the main differences in the inter-GPU communication overhead between the two modes:"),lu=h(),er=s("p"),tr=s("a"),A3=i("DDP"),x3=i(":"),iu=h(),Xt=s("ul"),vh=s("li"),j3=i("At the start time the main process replicates the model once from gpu 0 to the rest of gpus"),D3=h(),ji=s("li"),T3=i("Then for each batch:"),ar=s("ol"),wh=s("li"),I3=i("each gpu consumes each own mini-batch of data directly"),S3=h(),sr=s("li"),N3=i("during "),_h=s("code"),C3=i("backward"),L3=i(", once the local gradients are ready, they are then averaged across all processes"),ou=h(),rr=s("p"),lr=s("a"),G3=i("DP"),U3=i(":"),nu=h(),Di=s("p"),M3=i("For each batch:"),pu=h(),$=s("ol"),bh=s("li"),O3=i("gpu 0 reads the batch of data and then sends a mini-batch to each gpu"),z3=h(),yh=s("li"),V3=i("replicates the up-to-date model from gpu 0 to each gpu"),H3=h(),ir=s("li"),B3=i("runs "),gh=s("code"),F3=i("forward"),q3=i(" and sends output from each gpu to gpu 0, computes loss"),R3=h(),Ti=s("li"),W3=i("scatters loss from gpu 0 to all gpus, runs "),Eh=s("code"),X3=i("backward"),Y3=h(),Ph=s("li"),Q3=i("sends gradients from each gpu to gpu 0 and averages those"),hu=h(),Ii=s("p"),Z3=i("The only communication DDP performs per batch is sending gradients, whereas DP does 5 different data exchanges per batch."),fu=h(),Yt=s("p"),J3=i("DP copies data within the process via python threads, whereas DDP copies data via "),or=s("a"),K3=i("torch.distributed"),eE=i("."),cu=h(),Si=s("p"),tE=i("Under DP gpu 0 performs a lot more work than the rest of the gpus, thus resulting in under-utilization of gpus."),du=h(),Ni=s("p"),aE=i("You can use DDP across multiple machines, but this is not the case with DP."),uu=h(),Ci=s("p"),sE=i("There are other differences between DP and DDP but they aren\u2019t relevant to this discussion."),mu=h(),Qt=s("p"),rE=i("If you want to go really deep into understanding these 2 modes, this "),nr=s("a"),lE=i("article"),iE=i(" is highly recommended, as it has great diagrams, includes multiple benchmarks and profiler outputs on various hardware, explains all the nuances that you may need to know."),vu=h(),Li=s("p"),oE=i("Let\u2019s look at an actual benchmark:"),wu=h(),Zt=s("table"),kh=s("thead"),Me=s("tr"),Gi=s("th"),nE=i("Type"),pE=h(),$h=s("th"),hE=i("NVlink"),fE=h(),Ui=s("th"),cE=i("Time"),dE=h(),Oe=s("tbody"),ze=s("tr"),Mi=s("td"),uE=i("2:DP"),mE=h(),Ah=s("td"),vE=i("Y"),wE=h(),Oi=s("td"),_E=i("110s"),bE=h(),Ve=s("tr"),zi=s("td"),yE=i("2:DDP"),gE=h(),xh=s("td"),EE=i("Y"),PE=h(),Vi=s("td"),kE=i("101s"),$E=h(),He=s("tr"),Hi=s("td"),AE=i("2:DDP"),xE=h(),jh=s("td"),jE=i("N"),DE=h(),Bi=s("td"),TE=i("131s"),_u=h(),Fi=s("p"),IE=i("Analysis:"),bu=h(),qi=s("p"),SE=i("Here DP is ~10% slower than DDP w/ NVlink, but ~15% faster than DDP w/o NVlink"),yu=h(),Ri=s("p"),NE=i("The real difference will depend on how much data each GPU needs to sync with the others - the more there is to sync, the more a slow link will slow down the total runtime."),gu=h(),Wi=s("p"),CE=i("Here is the full benchmark code and outputs:"),Eu=h(),pr=s("p"),Dh=s("code"),LE=i("NCCL_P2P_DISABLE=1"),GE=i(" was used to disable the NVLink feature on the corresponding benchmark."),Pu=h(),d(hr.$$.fragment),ku=h(),k=s("p"),UE=i("Hardware: 2x TITAN RTX 24GB each + NVlink with 2 NVLinks ("),Th=s("code"),ME=i("NV2"),OE=i(" in "),Ih=s("code"),zE=i("nvidia-smi topo -m"),VE=i(`)
Software: `),Sh=s("code"),HE=i("pytorch-1.8-to-be"),BE=i(" + "),Nh=s("code"),FE=i("cuda-11.0"),qE=i(" / "),Ch=s("code"),RE=i("transformers==4.3.0.dev0"),$u=h(),Be=s("h3"),Jt=s("a"),Lh=s("span"),d(fr.$$.fragment),WE=h(),Gh=s("span"),XE=i("DataLoader"),Au=h(),Xi=s("p"),YE=i("One of the important requirements to reach great training speed is the ability to feed the GPU at the maximum speed it can handle. By default everything happens in the main process and it might not be able to read the data from disk fast enough, and thus create a bottleneck, leading to GPU under-utilization."),xu=h(),Kt=s("ul"),Yi=s("li"),Uh=s("code"),QE=i("DataLoader(pin_memory=True, ...)"),ZE=i(" which ensures that the data gets preloaded into the pinned memory on CPU and typically leads to much faster transfers from CPU to GPU memory."),JE=h(),Qi=s("li"),Mh=s("code"),KE=i("DataLoader(num_workers=4, ...)"),e6=i(" - spawn several workers to pre-load data faster - during training watch the GPU utilization stats and if it\u2019s far from 100% experiment with raising the number of workers. Of course, the problem could be elsewhere so a very big number of workers won\u2019t necessarily lead to a better performance."),ju=h(),Fe=s("h3"),ea=s("a"),Oh=s("span"),d(cr.$$.fragment),t6=h(),zh=s("span"),a6=i("Faster optimizer"),Du=h(),qe=s("p"),s6=i("pytorch-nightly introduced "),Vh=s("code"),r6=i("torch.optim._multi_tensor"),l6=i(" which should significantly speed up the optimizers for situations with lots of small feature tensors. It should eventually become the default, but if you want to experiment with it sooner and don\u2019t mind using the bleed-edge, see: "),dr=s("a"),i6=i("https://github.com/huggingface/transformers/issues/9965"),Tu=h(),Re=s("h3"),ta=s("a"),Hh=s("span"),d(ur.$$.fragment),o6=h(),Bh=s("span"),n6=i("Sparsity"),Iu=h(),We=s("h4"),aa=s("a"),Fh=s("span"),d(mr.$$.fragment),p6=h(),qh=s("span"),h6=i("Mixture of Experts"),Su=h(),Zi=s("p"),f6=i(`Quite a few of the recent papers reported a 4-5x training speedup and a faster inference by integrating
Mixture of Experts (MoE) into the Transformer models.`),Nu=h(),Ji=s("p"),c6=i("Since it has been discovered that more parameters lead to better performance, this technique allows to increase the number of parameters by an order of magnitude without increasing training costs."),Cu=h(),Ki=s("p"),d6=i("In this approach every other FFN layer is replaced with a MoE Layer which consists of many experts, with a gated function that trains each expert in a balanced way depending on the input token\u2019s position in a sequence."),Lu=h(),eo=s("p"),to=s("img"),Gu=h(),sa=s("p"),u6=i("(source: "),vr=s("a"),m6=i("GLAM"),v6=i(")"),Uu=h(),ao=s("p"),w6=i("You can find exhaustive details and comparison tables in the papers listed at the end of this section."),Mu=h(),so=s("p"),_6=i("The main drawback of this approach is that it requires staggering amounts of GPU memory - almost an order of magnitude larger than its dense equivalent. Various distillation and approaches are proposed to how to overcome the much higher memory requirements."),Ou=h(),ro=s("p"),b6=i("There is direct trade-off though, you can use just a few experts with a 2-3x smaller base model instead of dozens or hundreds experts leading to a 5x smaller model and thus increase the training speed moderately while increasing the memory requirements moderately as well."),zu=h(),lo=s("p"),y6=i("Most related papers and implementations are built around Tensorflow/TPUs:"),Vu=h(),le=s("ul"),Rh=s("li"),wr=s("a"),g6=i("GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding"),E6=h(),Wh=s("li"),_r=s("a"),P6=i("Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity"),k6=h(),Xh=s("li"),br=s("a"),$6=i("GLaM: Generalist Language Model (GLaM)"),Hu=h(),g=s("p"),A6=i("And for Pytorch DeepSpeed has built one as well: "),yr=s("a"),x6=i("DeepSpeed-MoE: Advancing Mixture-of-Experts Inference and Training to Power Next-Generation AI Scale"),j6=i(", "),gr=s("a"),D6=i("Mixture of Experts"),T6=i(" - blog posts:  "),Er=s("a"),I6=i("1"),S6=i(", "),Pr=s("a"),N6=i("2"),C6=i(" and specific deployment with large transformer-based natural language generation models: "),kr=s("a"),L6=i("blog post"),G6=i(", "),io=s("a"),U6=i("Megatron-Deepspeed branch"),M6=i("."),Bu=h(),Xe=s("h3"),ra=s("a"),Yh=s("span"),d($r.$$.fragment),O6=h(),Qh=s("span"),z6=i("Efficient Software Prebuilds"),Fu=h(),la=s("p"),V6=i("PyTorch\u2019s "),Ar=s("a"),H6=i("pip and conda builds"),B6=i(" come prebuit with the cuda toolkit which is enough to run PyTorch, but it is insufficient if you need to build cuda extensions."),qu=h(),ia=s("p"),F6=i("At times it may take an additional effort to pre-build some components, e.g., if you\u2019re using libraries like "),Zh=s("code"),q6=i("apex"),R6=i(" that don\u2019t come pre-compiled. In other situations figuring out how to install the right cuda toolkit system-wide can be complicated. To address these users\u2019 needs PyTorch and NVIDIA release a new version of NGC docker container which already comes with everything prebuilt and you just need to install your programs on it and it will run out of the box."),Ru=h(),oo=s("p"),W6=i("This approach is also useful if you want to tweak the pytorch source and/or make a new customized build."),Wu=h(),ie=s("p"),X6=i("To find the docker image version you want start "),xr=s("a"),Y6=i("here"),Q6=i(", choose one of the latest monthly releases. Go into the release\u2019s notes for the desired release, check that the environment\u2019s components are matching your needs (including NVIDIA Driver requirements!) and then at the very top of that document go to the corresponding NGC page. If for some reason you get lost, here is "),jr=s("a"),Z6=i("the index of all PyTorch NGC images"),J6=i("."),Xu=h(),no=s("p"),K6=i("Next follow the instructions to download and deploy the docker image."),Yu=h(),Ye=s("h2"),oa=s("a"),Jh=s("span"),d(Dr.$$.fragment),e0=h(),Kh=s("span"),t0=i("Contribute"),Qu=h(),po=s("p"),a0=i("This document is far from being complete and a lot more needs to be added, so if you have additions or corrections to make please don\u2019t hesitate to open a PR or if you aren\u2019t sure start an Issue and we can discuss the details there."),Zu=h(),ho=s("p"),s0=i("When making contributions that A is better than B, please try to include a reproducible benchmark and/or a link to the source of that information (unless it comes directly from you)."),this.h()},l(e){const n=FA('[data-svelte="svelte-1phssyn"]',document.head);R=r(n,"META",{name:!0,content:!0}),n.forEach(t),Tr=f(e),j=r(e,"H1",{class:!0});var Ku=l(j);G=r(Ku,"A",{id:!0,class:!0,href:!0});var k0=l(G);Io=r(k0,"SPAN",{});var $0=l(Io);u(ya.$$.fragment,$0),$0.forEach(t),k0.forEach(t),Iv=f(Ku),So=r(Ku,"SPAN",{});var A0=l(So);Sv=o(A0,"Performance and Scalability: How To Fit a Bigger Model and Train It Faster"),A0.forEach(t),Ku.forEach(t),sf=f(e),Ir=r(e,"P",{});var x0=l(Ir);Nv=o(x0,"For now the software sections of this document are mainly Pytorch-specific, but the guide can be extended to other frameworks in the future."),x0.forEach(t),rf=f(e),ne=r(e,"H2",{class:!0});var em=l(ne);Qe=r(em,"A",{id:!0,class:!0,href:!0});var j0=l(Qe);No=r(j0,"SPAN",{});var D0=l(No);u(ga.$$.fragment,D0),D0.forEach(t),j0.forEach(t),Cv=f(em),Co=r(em,"SPAN",{});var T0=l(Co);Lv=o(T0,"Quick notes"),T0.forEach(t),em.forEach(t),lf=f(e),Sr=r(e,"P",{});var I0=l(Sr);Gv=o(I0,"This section gives brief ideas on how to make training faster and support bigger models. Later sections will expand, demonstrate and elucidate each of these."),I0.forEach(t),of=f(e),pe=r(e,"H3",{class:!0});var tm=l(pe);Ze=r(tm,"A",{id:!0,class:!0,href:!0});var S0=l(Ze);Lo=r(S0,"SPAN",{});var N0=l(Lo);u(Ea.$$.fragment,N0),N0.forEach(t),S0.forEach(t),Uv=f(tm),Go=r(tm,"SPAN",{});var C0=l(Go);Mv=o(C0,"Faster Training"),C0.forEach(t),tm.forEach(t),nf=f(e),Nr=r(e,"P",{});var L0=l(Nr);Ov=o(L0,"Hardware:"),L0.forEach(t),pf=f(e),Cr=r(e,"UL",{});var G0=l(Cr);Lr=r(G0,"LI",{});var r0=l(Lr);zv=o(r0,"fast connectivity between GPUs"),Pa=r(r0,"UL",{});var am=l(Pa);Uo=r(am,"LI",{});var U0=l(Uo);Vv=o(U0,"intra-node: NVLink"),U0.forEach(t),Hv=f(am),Mo=r(am,"LI",{});var M0=l(Mo);Bv=o(M0,"inter-node: Infiniband / Intel OPA"),M0.forEach(t),am.forEach(t),r0.forEach(t),G0.forEach(t),hf=f(e),Gr=r(e,"P",{});var O0=l(Gr);Fv=o(O0,"Software:"),O0.forEach(t),ff=f(e),Je=r(e,"UL",{});var sm=l(Je);Oo=r(sm,"LI",{});var z0=l(Oo);qv=o(z0,"Data Parallel / Distributed Data Parallel"),z0.forEach(t),Rv=f(sm),zo=r(sm,"LI",{});var V0=l(zo);Wv=o(V0,"fp16 (autocast caching)"),V0.forEach(t),sm.forEach(t),cf=f(e),he=r(e,"H3",{class:!0});var rm=l(he);Ke=r(rm,"A",{id:!0,class:!0,href:!0});var H0=l(Ke);Vo=r(H0,"SPAN",{});var B0=l(Vo);u(ka.$$.fragment,B0),B0.forEach(t),H0.forEach(t),Xv=f(rm),Ho=r(rm,"SPAN",{});var F0=l(Ho);Yv=o(F0,"Bigger Models"),F0.forEach(t),rm.forEach(t),df=f(e),Ur=r(e,"P",{});var q0=l(Ur);Qv=o(q0,"Hardware:"),q0.forEach(t),uf=f(e),W=r(e,"UL",{});var fo=l(W);Bo=r(fo,"LI",{});var R0=l(Bo);Zv=o(R0,"bigger GPUs"),R0.forEach(t),Jv=f(fo),Fo=r(fo,"LI",{});var W0=l(Fo);Kv=o(W0,"more GPUs"),W0.forEach(t),e1=f(fo),$a=r(fo,"LI",{});var lm=l($a);t1=o(lm,"more CPU and NVMe (offloaded to by "),Mr=r(lm,"A",{href:!0});var X0=l(Mr);a1=o(X0,"DeepSpeed-Infinity"),X0.forEach(t),s1=o(lm,")"),lm.forEach(t),fo.forEach(t),mf=f(e),Or=r(e,"P",{});var Y0=l(Or);r1=o(Y0,"Software:"),Y0.forEach(t),vf=f(e),y=r(e,"UL",{});var A=l(y);qo=r(A,"LI",{});var Q0=l(qo);l1=o(Q0,"Model Scalability (ZeRO and 3D Parallelism)"),Q0.forEach(t),i1=f(A),Ro=r(A,"LI",{});var Z0=l(Ro);o1=o(Z0,"Low-memory Optimizers"),Z0.forEach(t),n1=f(A),Wo=r(A,"LI",{});var J0=l(Wo);p1=o(J0,"fp16/bf16 (smaller data/faster throughput)"),J0.forEach(t),h1=f(A),Xo=r(A,"LI",{});var K0=l(Xo);f1=o(K0,"tf32 (faster throughput)"),K0.forEach(t),c1=f(A),Yo=r(A,"LI",{});var e4=l(Yo);d1=o(e4,"Gradient accumulation"),e4.forEach(t),u1=f(A),Qo=r(A,"LI",{});var t4=l(Qo);m1=o(t4,"Gradient checkpointing"),t4.forEach(t),v1=f(A),Zo=r(A,"LI",{});var a4=l(Zo);w1=o(a4,"Sparsity"),a4.forEach(t),A.forEach(t),wf=f(e),fe=r(e,"H2",{class:!0});var im=l(fe);et=r(im,"A",{id:!0,class:!0,href:!0});var s4=l(et);Jo=r(s4,"SPAN",{});var r4=l(Jo);u(Aa.$$.fragment,r4),r4.forEach(t),s4.forEach(t),_1=f(im),Ko=r(im,"SPAN",{});var l4=l(Ko);b1=o(l4,"Hardware"),l4.forEach(t),im.forEach(t),_f=f(e),ce=r(e,"H3",{class:!0});var om=l(ce);tt=r(om,"A",{id:!0,class:!0,href:!0});var i4=l(tt);en=r(i4,"SPAN",{});var o4=l(en);u(xa.$$.fragment,o4),o4.forEach(t),i4.forEach(t),y1=f(om),tn=r(om,"SPAN",{});var n4=l(tn);g1=o(n4,"Power and Cooling"),n4.forEach(t),om.forEach(t),bf=f(e),zr=r(e,"P",{});var p4=l(zr);E1=o(p4,"If you bought an expensive high end GPU make sure you give it the correct power and sufficient cooling."),p4.forEach(t),yf=f(e),ja=r(e,"P",{});var l0=l(ja);an=r(l0,"STRONG",{});var h4=l(an);P1=o(h4,"Power"),h4.forEach(t),k1=o(l0,":"),l0.forEach(t),gf=f(e),Vr=r(e,"P",{});var f4=l(Vr);$1=o(f4,"Some high end consumer GPU cards have 2 and sometimes 3 PCI-E 8-Pin power sockets. Make sure you have as many independent 12V PCI-E 8-Pin cables plugged into the card as there are sockets. Do not use the 2 splits at one end of the same cable (also known as pigtail cable). That is if you have 2 sockets on the GPU, you want 2 PCI-E 8-Pin cables going from your PSU to the card and not one that has 2 PCI-E 8-Pin connectors at the end! You won\u2019t get the full performance out of your card otherwise."),f4.forEach(t),Ef=f(e),Hr=r(e,"P",{});var c4=l(Hr);A1=o(c4,"Each PCI-E 8-Pin power cable needs to be plugged into a 12V rail on the PSU side and can supply up to 150W of power."),c4.forEach(t),Pf=f(e),Br=r(e,"P",{});var d4=l(Br);x1=o(d4,"Some other cards may use a PCI-E 12-Pin connectors, and these can deliver up to 500-600W of power."),d4.forEach(t),kf=f(e),Fr=r(e,"P",{});var u4=l(Fr);j1=o(u4,"Low end cards may use 6-Pin connectors, which supply up to 75W of power."),u4.forEach(t),$f=f(e),qr=r(e,"P",{});var m4=l(qr);D1=o(m4,"Additionally you want the high-end PSU that has stable voltage. Some lower quality ones may not give the card the stable voltage it needs to function at its peak."),m4.forEach(t),Af=f(e),Rr=r(e,"P",{});var v4=l(Rr);T1=o(v4,"And of course the PSU needs to have enough unused Watts to power the card."),v4.forEach(t),xf=f(e),Da=r(e,"P",{});var i0=l(Da);sn=r(i0,"STRONG",{});var w4=l(sn);I1=o(w4,"Cooling"),w4.forEach(t),S1=o(i0,":"),i0.forEach(t),jf=f(e),Wr=r(e,"P",{});var _4=l(Wr);N1=o(_4,"When a GPU gets overheated it would start throttling down and will not deliver full performance. And it will shutdown if it gets too hot."),_4.forEach(t),Df=f(e),Xr=r(e,"P",{});var b4=l(Xr);C1=o(b4,"It\u2019s hard to tell the exact best temperature to strive for when a GPU is heavily loaded, but probably anything under +80C is good, but lower is better - perhaps 70-75C is an excellent range to be in. The throttling down is likely to start at around 84-90C. But other than throttling performance a prolonged very higher temperature is likely to reduce the lifespan of a GPU."),b4.forEach(t),Tf=f(e),de=r(e,"H3",{class:!0});var nm=l(de);at=r(nm,"A",{id:!0,class:!0,href:!0});var y4=l(at);rn=r(y4,"SPAN",{});var g4=l(rn);u(Ta.$$.fragment,g4),g4.forEach(t),y4.forEach(t),L1=f(nm),ln=r(nm,"SPAN",{});var E4=l(ln);G1=o(E4,"Multi-GPU Connectivity"),E4.forEach(t),nm.forEach(t),If=f(e),Yr=r(e,"P",{});var P4=l(Yr);U1=o(P4,"If you use multiple GPUs the way cards are inter-connected can have a huge impact on the total training time."),P4.forEach(t),Sf=f(e),Qr=r(e,"P",{});var k4=l(Qr);M1=o(k4,"If the GPUs are on the same physical node, you can run:"),k4.forEach(t),Nf=f(e),u(Ia.$$.fragment,e),Cf=f(e),Zr=r(e,"P",{});var $4=l(Zr);O1=o($4,"and it will tell you how the GPUs are inter-connected."),$4.forEach(t),Lf=f(e),Jr=r(e,"P",{});var A4=l(Jr);z1=o(A4,"On a machine with dual-GPU and which are connected with NVLink, you will most likely see something like:"),A4.forEach(t),Gf=f(e),u(Sa.$$.fragment,e),Uf=f(e),Kr=r(e,"P",{});var x4=l(Kr);V1=o(x4,"on a different machine w/o NVLink we may see:"),x4.forEach(t),Mf=f(e),u(Na.$$.fragment,e),Of=f(e),el=r(e,"P",{});var j4=l(el);H1=o(j4,"The report includes this legend:"),j4.forEach(t),zf=f(e),u(Ca.$$.fragment,e),Vf=f(e),X=r(e,"P",{});var co=l(X);B1=o(co,"So the first report "),on=r(co,"CODE",{});var D4=l(on);F1=o(D4,"NV2"),D4.forEach(t),q1=o(co," tells us the GPUs are interconnected with 2 NVLinks, and the second report "),nn=r(co,"CODE",{});var T4=l(nn);R1=o(T4,"PHB"),T4.forEach(t),W1=o(co," we have a typical consumer-level PCIe+Bridge setup."),co.forEach(t),Hf=f(e),tl=r(e,"P",{});var I4=l(tl);X1=o(I4,"Check what type of connectivity you have on your setup. Some of these will make the communication between cards faster (e.g. NVLink), others slower (e.g. PHB)."),I4.forEach(t),Bf=f(e),al=r(e,"P",{});var S4=l(al);Y1=o(S4,"Depending on the type of scalability solution used, the connectivity speed could have a major or a minor impact. If the GPUs need to sync rarely, as in DDP, the impact of a slower connection will be less significant. If the GPUs need to send messages to each other often, as in ZeRO-DP, then faster connectivity becomes super important to achieve faster training."),S4.forEach(t),Ff=f(e),ue=r(e,"H3",{class:!0});var pm=l(ue);st=r(pm,"A",{id:!0,class:!0,href:!0});var N4=l(st);pn=r(N4,"SPAN",{});var C4=l(pn);u(La.$$.fragment,C4),C4.forEach(t),N4.forEach(t),Q1=f(pm),hn=r(pm,"SPAN",{});var L4=l(hn);Z1=o(L4,"NVlink"),L4.forEach(t),pm.forEach(t),qf=f(e),Ga=r(e,"P",{});var o0=l(Ga);Ua=r(o0,"A",{href:!0,rel:!0});var G4=l(Ua);J1=o(G4,"NVLink"),G4.forEach(t),K1=o(o0," is a wire-based serial multi-lane near-range communications link developed by Nvidia."),o0.forEach(t),Rf=f(e),rt=r(e,"P",{});var hm=l(rt);ew=o(hm,"Each new generation provides a faster bandwidth, e.g. here is a quote from "),Ma=r(hm,"A",{href:!0,rel:!0});var U4=l(Ma);tw=o(U4,"Nvidia Ampere GA102 GPU Architecture"),U4.forEach(t),aw=o(hm,":"),hm.forEach(t),Wf=f(e),sl=r(e,"BLOCKQUOTE",{});var M4=l(sl);fn=r(M4,"P",{});var O4=l(fn);sw=o(O4,`Third-Generation NVLink\xAE
GA102 GPUs utilize NVIDIA\u2019s third-generation NVLink interface, which includes four x4 links,
with each link providing 14.0625 GB/sec bandwidth in each direction between two GPUs. Four
links provide 56.25 GB/sec bandwidth in each direction, and 112.5 GB/sec total bandwidth
between two GPUs. Two RTX 3090 GPUs can be connected together for SLI using NVLink.
(Note that 3-Way and 4-Way SLI configurations are not supported.)`),O4.forEach(t),M4.forEach(t),Xf=f(e),D=r(e,"P",{});var na=l(D);rw=o(na,"So the higher "),cn=r(na,"CODE",{});var z4=l(cn);lw=o(z4,"X"),z4.forEach(t),iw=o(na," you get in the report of "),dn=r(na,"CODE",{});var V4=l(dn);ow=o(V4,"NVX"),V4.forEach(t),nw=o(na," in the output of "),un=r(na,"CODE",{});var H4=l(un);pw=o(H4,"nvidia-smi topo -m"),H4.forEach(t),hw=o(na," the better. The generation will depend on your GPU architecture."),na.forEach(t),Yf=f(e),rl=r(e,"P",{});var B4=l(rl);fw=o(B4,"Let\u2019s compare the execution of a gpt2 language model training over a small sample of wikitext."),B4.forEach(t),Qf=f(e),ll=r(e,"P",{});var F4=l(ll);cw=o(F4,"The results are:"),F4.forEach(t),Zf=f(e),lt=r(e,"TABLE",{});var fm=l(lt);mn=r(fm,"THEAD",{});var q4=l(mn);Oa=r(q4,"TR",{});var cm=l(Oa);vn=r(cm,"TH",{});var R4=l(vn);dw=o(R4,"NVlink"),R4.forEach(t),uw=f(cm),il=r(cm,"TH",{align:!0});var W4=l(il);mw=o(W4,"Time"),W4.forEach(t),cm.forEach(t),q4.forEach(t),vw=f(fm),za=r(fm,"TBODY",{});var dm=l(za);Va=r(dm,"TR",{});var um=l(Va);wn=r(um,"TD",{});var X4=l(wn);ww=o(X4,"Y"),X4.forEach(t),_w=f(um),ol=r(um,"TD",{align:!0});var Y4=l(ol);bw=o(Y4,"101s"),Y4.forEach(t),um.forEach(t),yw=f(dm),Ha=r(dm,"TR",{});var mm=l(Ha);_n=r(mm,"TD",{});var Q4=l(_n);gw=o(Q4,"N"),Q4.forEach(t),Ew=f(mm),nl=r(mm,"TD",{align:!0});var Z4=l(nl);Pw=o(Z4,"131s"),Z4.forEach(t),mm.forEach(t),dm.forEach(t),fm.forEach(t),Jf=f(e),pl=r(e,"P",{});var J4=l(pl);kw=o(J4,"You can see that NVLink completes the training ~23% faster."),J4.forEach(t),Kf=f(e),it=r(e,"P",{});var vm=l(it);$w=o(vm,"In the second benchmark we use "),bn=r(vm,"CODE",{});var K4=l(bn);Aw=o(K4,"NCCL_P2P_DISABLE=1"),K4.forEach(t),xw=o(vm," to tell the GPUs not to use NVLink."),vm.forEach(t),ec=f(e),hl=r(e,"P",{});var e5=l(hl);jw=o(e5,"Here is the full benchmark code and outputs:"),e5.forEach(t),tc=f(e),u(Ba.$$.fragment,e),ac=f(e),P=r(e,"P",{});var B=l(P);Dw=o(B,"Hardware: 2x TITAN RTX 24GB each + NVlink with 2 NVLinks ("),yn=r(B,"CODE",{});var t5=l(yn);Tw=o(t5,"NV2"),t5.forEach(t),Iw=o(B," in "),gn=r(B,"CODE",{});var a5=l(gn);Sw=o(a5,"nvidia-smi topo -m"),a5.forEach(t),Nw=o(B,`)
Software: `),En=r(B,"CODE",{});var s5=l(En);Cw=o(s5,"pytorch-1.8-to-be"),s5.forEach(t),Lw=o(B," + "),Pn=r(B,"CODE",{});var r5=l(Pn);Gw=o(r5,"cuda-11.0"),r5.forEach(t),Uw=o(B," / "),kn=r(B,"CODE",{});var l5=l(kn);Mw=o(l5,"transformers==4.3.0.dev0"),l5.forEach(t),B.forEach(t),sc=f(e),me=r(e,"H2",{class:!0});var wm=l(me);ot=r(wm,"A",{id:!0,class:!0,href:!0});var i5=l(ot);$n=r(i5,"SPAN",{});var o5=l($n);u(Fa.$$.fragment,o5),o5.forEach(t),i5.forEach(t),Ow=f(wm),An=r(wm,"SPAN",{});var n5=l(An);zw=o(n5,"Software"),n5.forEach(t),wm.forEach(t),rc=f(e),ve=r(e,"H3",{class:!0});var _m=l(ve);nt=r(_m,"A",{id:!0,class:!0,href:!0});var p5=l(nt);xn=r(p5,"SPAN",{});var h5=l(xn);u(qa.$$.fragment,h5),h5.forEach(t),p5.forEach(t),Vw=f(_m),jn=r(_m,"SPAN",{});var f5=l(jn);Hw=o(f5,"Model Scalability"),f5.forEach(t),_m.forEach(t),lc=f(e),fl=r(e,"P",{});var c5=l(fl);Bw=o(c5,"When you can\u2019t fit a model into the available GPU memory, you need to start using a solution that allows you to scale a large model to use multiple GPUs in parallel."),c5.forEach(t),ic=f(e),Ra=r(e,"P",{});var n0=l(Ra);Fw=o(n0,"For indepth details on ZeRO and various other model parallelism protocols please see: "),cl=r(n0,"A",{href:!0});var d5=l(cl);qw=o(d5,"Model Parallelism"),d5.forEach(t),n0.forEach(t),oc=f(e),we=r(e,"H3",{class:!0});var bm=l(we);pt=r(bm,"A",{id:!0,class:!0,href:!0});var u5=l(pt);Dn=r(u5,"SPAN",{});var m5=l(Dn);u(Wa.$$.fragment,m5),m5.forEach(t),u5.forEach(t),Rw=f(bm),Tn=r(bm,"SPAN",{});var v5=l(Tn);Ww=o(v5,"Anatomy of Model's Operations"),v5.forEach(t),bm.forEach(t),nc=f(e),dl=r(e,"P",{});var w5=l(dl);Xw=o(w5,"Transformers architecture includes 3 main groups of operations grouped below by compute-intensity."),w5.forEach(t),pc=f(e),Y=r(e,"OL",{});var uo=l(Y);Xa=r(uo,"LI",{});var ym=l(Xa);In=r(ym,"P",{});var _5=l(In);Sn=r(_5,"STRONG",{});var b5=l(Sn);Yw=o(b5,"Tensor Contractions"),b5.forEach(t),_5.forEach(t),Qw=f(ym),Ya=r(ym,"P",{});var gm=l(Ya);Zw=o(gm,"Linear layers and components of Multi-Head Attention all do batched "),Nn=r(gm,"STRONG",{});var y5=l(Nn);Jw=o(y5,"matrix-matrix multiplications"),y5.forEach(t),Kw=o(gm,". These operations are the most compute-intensive part of training a transformer."),gm.forEach(t),ym.forEach(t),e_=f(uo),Qa=r(uo,"LI",{});var Em=l(Qa);Cn=r(Em,"P",{});var g5=l(Cn);Ln=r(g5,"STRONG",{});var E5=l(Ln);t_=o(E5,"Statistical Normalizations"),E5.forEach(t),g5.forEach(t),a_=f(Em),Za=r(Em,"P",{});var Pm=l(Za);s_=o(Pm,"Softmax and layer normalization are less compute-intensive than tensor contractions, and involve one or more "),Gn=r(Pm,"STRONG",{});var P5=l(Gn);r_=o(P5,"reduction operations"),P5.forEach(t),l_=o(Pm,", the result of which is then applied via a map."),Pm.forEach(t),Em.forEach(t),i_=f(uo),Ja=r(uo,"LI",{});var km=l(Ja);Un=r(km,"P",{});var k5=l(Un);Mn=r(k5,"STRONG",{});var $5=l(Mn);o_=o($5,"Element-wise Operators"),$5.forEach(t),k5.forEach(t),n_=f(km),Ka=r(km,"P",{});var $m=l(Ka);p_=o($m,"These are the remaining operators: "),On=r($m,"STRONG",{});var A5=l(On);h_=o(A5,"biases, dropout, activations, and residual connections"),A5.forEach(t),f_=o($m,". These are the least compute-intensive operations."),$m.forEach(t),km.forEach(t),uo.forEach(t),hc=f(e),ul=r(e,"P",{});var x5=l(ul);c_=o(x5,"This knowledge can be helpful to know when analyzing performance bottlenecks."),x5.forEach(t),fc=f(e),es=r(e,"P",{});var p0=l(es);d_=o(p0,"This summary is derived from "),ts=r(p0,"A",{href:!0,rel:!0});var j5=l(ts);u_=o(j5,"Data Movement Is All You Need: A Case Study on Optimizing Transformers 2020"),j5.forEach(t),p0.forEach(t),cc=f(e),_e=r(e,"H3",{class:!0});var Am=l(_e);ht=r(Am,"A",{id:!0,class:!0,href:!0});var D5=l(ht);zn=r(D5,"SPAN",{});var T5=l(zn);u(as.$$.fragment,T5),T5.forEach(t),D5.forEach(t),m_=f(Am),Vn=r(Am,"SPAN",{});var I5=l(Vn);v_=o(I5,"Anatomy of Model's Memory"),I5.forEach(t),Am.forEach(t),dc=f(e),ml=r(e,"P",{});var S5=l(ml);w_=o(S5,"The components on GPU memory are the following:"),S5.forEach(t),uc=f(e),E=r(e,"OL",{});var L=l(E);Hn=r(L,"LI",{});var N5=l(Hn);__=o(N5,"model weights"),N5.forEach(t),b_=f(L),Bn=r(L,"LI",{});var C5=l(Bn);y_=o(C5,"optimizer states"),C5.forEach(t),g_=f(L),Fn=r(L,"LI",{});var L5=l(Fn);E_=o(L5,"gradients"),L5.forEach(t),P_=f(L),qn=r(L,"LI",{});var G5=l(qn);k_=o(G5,"forward activations saved for gradient computation"),G5.forEach(t),$_=f(L),Rn=r(L,"LI",{});var U5=l(Rn);A_=o(U5,"temporary buffers"),U5.forEach(t),x_=f(L),Wn=r(L,"LI",{});var M5=l(Wn);j_=o(M5,"functionality-specific memory"),M5.forEach(t),L.forEach(t),mc=f(e),vl=r(e,"P",{});var O5=l(vl);D_=o(O5,"A typical model trained in mixed precision with AdamW requires 18 bytes per model parameter plus activation memory."),O5.forEach(t),vc=f(e),wl=r(e,"P",{});var z5=l(wl);T_=o(z5,"For inference there are no optimizer states and gradients, so we can subtract those. And thus we end up with 6 bytes per model parameter for mixed precision inference, plus activation memory."),z5.forEach(t),wc=f(e),_l=r(e,"P",{});var V5=l(_l);I_=o(V5,"Let\u2019s look at the details."),V5.forEach(t),_c=f(e),be=r(e,"H4",{class:!0});var xm=l(be);ft=r(xm,"A",{id:!0,class:!0,href:!0});var H5=l(ft);Xn=r(H5,"SPAN",{});var B5=l(Xn);u(ss.$$.fragment,B5),B5.forEach(t),H5.forEach(t),S_=f(xm),Yn=r(xm,"SPAN",{});var F5=l(Yn);N_=o(F5,"Model Weights"),F5.forEach(t),xm.forEach(t),bc=f(e),ct=r(e,"UL",{});var jm=l(ct);Qn=r(jm,"LI",{});var q5=l(Qn);C_=o(q5,"4 bytes * number of parameters for fp32 training"),q5.forEach(t),L_=f(jm),Zn=r(jm,"LI",{});var R5=l(Zn);G_=o(R5,"6 bytes * number of parameters for mixed precision training"),R5.forEach(t),jm.forEach(t),yc=f(e),ye=r(e,"H4",{class:!0});var Dm=l(ye);dt=r(Dm,"A",{id:!0,class:!0,href:!0});var W5=l(dt);Jn=r(W5,"SPAN",{});var X5=l(Jn);u(rs.$$.fragment,X5),X5.forEach(t),W5.forEach(t),U_=f(Dm),Kn=r(Dm,"SPAN",{});var Y5=l(Kn);M_=o(Y5,"Optimizer States"),Y5.forEach(t),Dm.forEach(t),gc=f(e),Q=r(e,"UL",{});var mo=l(Q);ep=r(mo,"LI",{});var Q5=l(ep);O_=o(Q5,"8 bytes * number of parameters for normal AdamW (maintains 2 states)"),Q5.forEach(t),z_=f(mo),bl=r(mo,"LI",{});var h0=l(bl);V_=o(h0,"2 bytes * number of parameters for 8-bit AdamW optimizers like "),ls=r(h0,"A",{href:!0,rel:!0});var Z5=l(ls);H_=o(Z5,"bitsandbytes"),Z5.forEach(t),h0.forEach(t),B_=f(mo),tp=r(mo,"LI",{});var J5=l(tp);F_=o(J5,"4 bytes * number of parameters for optimizers like SGD (maintains only 1 state)"),J5.forEach(t),mo.forEach(t),Ec=f(e),ge=r(e,"H4",{class:!0});var Tm=l(ge);ut=r(Tm,"A",{id:!0,class:!0,href:!0});var K5=l(ut);ap=r(K5,"SPAN",{});var eP=l(ap);u(is.$$.fragment,eP),eP.forEach(t),K5.forEach(t),q_=f(Tm),sp=r(Tm,"SPAN",{});var tP=l(sp);R_=o(tP,"Gradients"),tP.forEach(t),Tm.forEach(t),Pc=f(e),yl=r(e,"UL",{});var aP=l(yl);rp=r(aP,"LI",{});var sP=l(rp);W_=o(sP,"4 bytes * number of parameters for either fp32 or mixed precision training"),sP.forEach(t),aP.forEach(t),kc=f(e),Ee=r(e,"H4",{class:!0});var Im=l(Ee);mt=r(Im,"A",{id:!0,class:!0,href:!0});var rP=l(mt);lp=r(rP,"SPAN",{});var lP=l(lp);u(os.$$.fragment,lP),lP.forEach(t),rP.forEach(t),X_=f(Im),ip=r(Im,"SPAN",{});var iP=l(ip);Y_=o(iP,"Forward Activations"),iP.forEach(t),Im.forEach(t),$c=f(e),gl=r(e,"UL",{});var oP=l(gl);op=r(oP,"LI",{});var nP=l(op);Q_=o(nP,"size depends on many factors, the key ones being sequence length, hidden size and batch size."),nP.forEach(t),oP.forEach(t),Ac=f(e),El=r(e,"P",{});var pP=l(El);Z_=o(pP,"There are the input and output that are being passed and returned by the forward and the backward functions and the forward activations saved for gradient computation."),pP.forEach(t),xc=f(e),Pe=r(e,"H4",{class:!0});var Sm=l(Pe);vt=r(Sm,"A",{id:!0,class:!0,href:!0});var hP=l(vt);np=r(hP,"SPAN",{});var fP=l(np);u(ns.$$.fragment,fP),fP.forEach(t),hP.forEach(t),J_=f(Sm),pp=r(Sm,"SPAN",{});var cP=l(pp);K_=o(cP,"Temporary Memory"),cP.forEach(t),Sm.forEach(t),jc=f(e),Pl=r(e,"P",{});var dP=l(Pl);eb=o(dP,"Additionally there are all kinds of temporary variables which get released once the calculation is done, but in the moment these could require additional memory and could push to OOM. Therefore when coding it\u2019s crucial to think strategically about such temporary variables and sometimes to explicitly free those as soon as they are no longer needed."),dP.forEach(t),Dc=f(e),ke=r(e,"H4",{class:!0});var Nm=l(ke);wt=r(Nm,"A",{id:!0,class:!0,href:!0});var uP=l(wt);hp=r(uP,"SPAN",{});var mP=l(hp);u(ps.$$.fragment,mP),mP.forEach(t),uP.forEach(t),tb=f(Nm),fp=r(Nm,"SPAN",{});var vP=l(fp);ab=o(vP,"Functionality-specific memory"),vP.forEach(t),Nm.forEach(t),Tc=f(e),kl=r(e,"P",{});var wP=l(kl);sb=o(wP,"Then your software could have special memory needs. For example, when generating text using beam search, the software needs to maintain multiple copies of inputs and outputs."),wP.forEach(t),Ic=f(e),$e=r(e,"H3",{class:!0});var Cm=l($e);_t=r(Cm,"A",{id:!0,class:!0,href:!0});var _P=l(_t);cp=r(_P,"SPAN",{});var bP=l(cp);u(hs.$$.fragment,bP),bP.forEach(t),_P.forEach(t),rb=f(Cm),bt=r(Cm,"SPAN",{});var ef=l(bt);dp=r(ef,"CODE",{});var yP=l(dp);lb=o(yP,"forward"),yP.forEach(t),ib=o(ef," vs "),up=r(ef,"CODE",{});var gP=l(up);ob=o(gP,"backward"),gP.forEach(t),nb=o(ef," Execution Speed"),ef.forEach(t),Cm.forEach(t),Sc=f(e),$l=r(e,"P",{});var EP=l($l);pb=o(EP,"For convolutions and linear layers there are 2x flops in the backward compared to the forward, which generally translates into ~2x slower (sometimes more, because sizes in the backward tend to be more awkward). Activations are usually bandwidth-limited, and it\u2019s typical for an activation to have to read more data in the backward than in the forward (e.g. activation forward reads once, writes once, activation backward reads twice, gradOutput and output of the forward, and writes once, gradInput)."),EP.forEach(t),Nc=f(e),Ae=r(e,"H3",{class:!0});var Lm=l(Ae);yt=r(Lm,"A",{id:!0,class:!0,href:!0});var PP=l(yt);mp=r(PP,"SPAN",{});var kP=l(mp);u(fs.$$.fragment,kP),kP.forEach(t),PP.forEach(t),hb=f(Lm),vp=r(Lm,"SPAN",{});var $P=l(vp);fb=o($P,"Floating Data Types"),$P.forEach(t),Lm.forEach(t),Cc=f(e),Al=r(e,"P",{});var AP=l(Al);cb=o(AP,"Here are the commonly used floating point data types choice of which impacts both memory usage and throughput:"),AP.forEach(t),Lc=f(e),T=r(e,"UL",{});var pa=l(T);cs=r(pa,"LI",{});var Gm=l(cs);db=o(Gm,"fp32 ("),wp=r(Gm,"CODE",{});var xP=l(wp);ub=o(xP,"float32"),xP.forEach(t),mb=o(Gm,")"),Gm.forEach(t),vb=f(pa),ds=r(pa,"LI",{});var Um=l(ds);wb=o(Um,"fp16 ("),_p=r(Um,"CODE",{});var jP=l(_p);_b=o(jP,"float16"),jP.forEach(t),bb=o(Um,")"),Um.forEach(t),yb=f(pa),us=r(pa,"LI",{});var Mm=l(us);gb=o(Mm,"bf16 ("),bp=r(Mm,"CODE",{});var DP=l(bp);Eb=o(DP,"bfloat16"),DP.forEach(t),Pb=o(Mm,")"),Mm.forEach(t),kb=f(pa),yp=r(pa,"LI",{});var TP=l(yp);$b=o(TP,"tf32 (CUDA internal data type)"),TP.forEach(t),pa.forEach(t),Gc=f(e),xl=r(e,"P",{});var IP=l(xl);Ab=o(IP,"Here is a diagram that shows how these data types correlate to each other."),IP.forEach(t),Uc=f(e),jl=r(e,"P",{});var SP=l(jl);Dl=r(SP,"IMG",{src:!0,alt:!0}),SP.forEach(t),Mc=f(e),gt=r(e,"P",{});var Om=l(gt);xb=o(Om,"(source: "),ms=r(Om,"A",{href:!0,rel:!0});var NP=l(ms);jb=o(NP,"NVIDIA Blog"),NP.forEach(t),Db=o(Om,")"),Om.forEach(t),Oc=f(e),Tl=r(e,"P",{});var CP=l(Tl);Tb=o(CP,"While fp16 and fp32 have been around for quite some time, bf16 and tf32 are only available on the Ampere architecture GPUS. TPUs support bf16 as well."),CP.forEach(t),zc=f(e),xe=r(e,"H4",{class:!0});var zm=l(xe);Et=r(zm,"A",{id:!0,class:!0,href:!0});var LP=l(Et);gp=r(LP,"SPAN",{});var GP=l(gp);u(vs.$$.fragment,GP),GP.forEach(t),LP.forEach(t),Ib=f(zm),Ep=r(zm,"SPAN",{});var UP=l(Ep);Sb=o(UP,"fp16"),UP.forEach(t),zm.forEach(t),Vc=f(e),Il=r(e,"P",{});var MP=l(Il);Nb=o(MP,"AMP = Automatic Mixed Precision"),MP.forEach(t),Hc=f(e),Sl=r(e,"P",{});var OP=l(Sl);Cb=o(OP,"If we look at what\u2019s happening with FP16 training (mixed precision) we have:"),OP.forEach(t),Bc=f(e),I=r(e,"UL",{});var ha=l(I);Pp=r(ha,"LI",{});var zP=l(Pp);Lb=o(zP,"the model has two copies in memory: one in half-precision for the forward/backward computations and one in full precision - no memory saved here"),zP.forEach(t),Gb=f(ha),kp=r(ha,"LI",{});var VP=l(kp);Ub=o(VP,"the forward activations saved for gradient computation are in half-precision - memory is saved here"),VP.forEach(t),Mb=f(ha),ws=r(ha,"LI",{});var Vm=l(ws);Ob=o(Vm,"the gradients are computed in half-precision "),$p=r(Vm,"EM",{});var HP=l($p);zb=o(HP,"but"),HP.forEach(t),Vb=o(Vm," converted to full-precision for the update, no saving there"),Vm.forEach(t),Hb=f(ha),Ap=r(ha,"LI",{});var BP=l(Ap);Bb=o(BP,"the optimizer states are in full precision as all the updates are done in full-precision"),BP.forEach(t),ha.forEach(t),Fc=f(e),Nl=r(e,"P",{});var FP=l(Nl);Fb=o(FP,"So the savings only happen for the forward activations saved for the backward computation, and there is a slight overhead because the model weights are stored both in half- and full-precision."),FP.forEach(t),qc=f(e),Pt=r(e,"P",{});var Hm=l(Pt);qb=o(Hm,"In \u{1F917} Transformers fp16 mixed precision is enabled by passing "),xp=r(Hm,"CODE",{});var qP=l(xp);Rb=o(qP,"--fp16"),qP.forEach(t),Wb=o(Hm," to the \u{1F917} Trainer."),Hm.forEach(t),Rc=f(e),Cl=r(e,"P",{});var RP=l(Cl);Xb=o(RP,"Now let\u2019s look at a simple text-classification fine-tuning on 2 GPUs (I\u2019m giving the command for reference):"),RP.forEach(t),Wc=f(e),u(_s.$$.fragment,e),Xc=f(e),kt=r(e,"P",{});var Bm=l(kt);Yb=o(Bm,"Since the only savings we get are in the model activations saved for the backward passed, it\u2019s logical that the bigger those activations are, the bigger the saving will be. If we try different batch sizes, I indeed get (this is with "),jp=r(Bm,"CODE",{});var WP=l(jp);Qb=o(WP,"nvidia-smi"),WP.forEach(t),Zb=o(Bm," so not completely reliable as said above but it will be a fair comparison):"),Bm.forEach(t),Yc=f(e),$t=r(e,"TABLE",{});var Fm=l($t);Dp=r(Fm,"THEAD",{});var XP=l(Dp);U=r(XP,"TR",{});var fa=l(U);Ll=r(fa,"TH",{align:!0});var YP=l(Ll);Jb=o(YP,"batch size"),YP.forEach(t),Kb=f(fa),Gl=r(fa,"TH",{align:!0});var QP=l(Gl);ey=o(QP,"w/o \u2014fp16"),QP.forEach(t),ty=f(fa),Ul=r(fa,"TH",{align:!0});var ZP=l(Ul);ay=o(ZP,"w/ \u2014fp16"),ZP.forEach(t),sy=f(fa),Ml=r(fa,"TH",{align:!0});var JP=l(Ml);ry=o(JP,"savings"),JP.forEach(t),fa.forEach(t),XP.forEach(t),ly=f(Fm),M=r(Fm,"TBODY",{});var ca=l(M);O=r(ca,"TR",{});var da=l(O);Ol=r(da,"TD",{align:!0});var KP=l(Ol);iy=o(KP,"8"),KP.forEach(t),oy=f(da),zl=r(da,"TD",{align:!0});var ek=l(zl);ny=o(ek,"4247"),ek.forEach(t),py=f(da),Vl=r(da,"TD",{align:!0});var tk=l(Vl);hy=o(tk,"4163"),tk.forEach(t),fy=f(da),Hl=r(da,"TD",{align:!0});var ak=l(Hl);cy=o(ak,"84"),ak.forEach(t),da.forEach(t),dy=f(ca),z=r(ca,"TR",{});var ua=l(z);Bl=r(ua,"TD",{align:!0});var sk=l(Bl);uy=o(sk,"16"),sk.forEach(t),my=f(ua),Fl=r(ua,"TD",{align:!0});var rk=l(Fl);vy=o(rk,"4971"),rk.forEach(t),wy=f(ua),ql=r(ua,"TD",{align:!0});var lk=l(ql);_y=o(lk,"4793"),lk.forEach(t),by=f(ua),Rl=r(ua,"TD",{align:!0});var ik=l(Rl);yy=o(ik,"178"),ik.forEach(t),ua.forEach(t),gy=f(ca),V=r(ca,"TR",{});var ma=l(V);Wl=r(ma,"TD",{align:!0});var ok=l(Wl);Ey=o(ok,"32"),ok.forEach(t),Py=f(ma),Xl=r(ma,"TD",{align:!0});var nk=l(Xl);ky=o(nk,"6827"),nk.forEach(t),$y=f(ma),Yl=r(ma,"TD",{align:!0});var pk=l(Yl);Ay=o(pk,"6207"),pk.forEach(t),xy=f(ma),Ql=r(ma,"TD",{align:!0});var hk=l(Ql);jy=o(hk,"620"),hk.forEach(t),ma.forEach(t),Dy=f(ca),H=r(ca,"TR",{});var va=l(H);Zl=r(va,"TD",{align:!0});var fk=l(Zl);Ty=o(fk,"64"),fk.forEach(t),Iy=f(va),Jl=r(va,"TD",{align:!0});var ck=l(Jl);Sy=o(ck,"10037"),ck.forEach(t),Ny=f(va),Kl=r(va,"TD",{align:!0});var dk=l(Kl);Cy=o(dk,"8061"),dk.forEach(t),Ly=f(va),ei=r(va,"TD",{align:!0});var uk=l(ei);Gy=o(uk,"1976"),uk.forEach(t),va.forEach(t),ca.forEach(t),Fm.forEach(t),Qc=f(e),At=r(e,"P",{});var qm=l(At);Uy=o(qm,"So there is only a real memory saving if we train at a high batch size (and it\u2019s not half) and at batch sizes lower than 8, you actually get a bigger memory footprint (because of the overhead mentioned above). The gain for FP16 training is that in each of those cases, the training with the flag "),Tp=r(qm,"CODE",{});var mk=l(Tp);My=o(mk,"--fp16"),mk.forEach(t),Oy=o(qm," is twice as fast, which does require every tensor to have every dimension be a multiple of 8 (examples pad the tensors to a sequence length that is a multiple of 8)."),qm.forEach(t),Zc=f(e),ti=r(e,"P",{});var vk=l(ti);zy=o(vk,"Summary: FP16 with apex or AMP will only give you some memory savings with a reasonably high batch size."),vk.forEach(t),Jc=f(e),ai=r(e,"P",{});var wk=l(ai);Vy=o(wk,"Additionally, under mixed precision when possible, it\u2019s important that the batch size is a multiple of 8 to efficiently use tensor cores."),wk.forEach(t),Kc=f(e),xt=r(e,"P",{});var Rm=l(xt);Hy=o(Rm,"Note that in some situations the speed up can be as big as 5x when using mixed precision. e.g. we have observed that while using "),bs=r(Rm,"A",{href:!0,rel:!0});var _k=l(bs);By=o(_k,"Megatron-Deepspeed"),_k.forEach(t),Fy=o(Rm,"."),Rm.forEach(t),ed=f(e),si=r(e,"P",{});var bk=l(si);qy=o(bk,"Some amazing tutorials to read on mixed precision:"),bk.forEach(t),td=f(e),jt=r(e,"UL",{});var Wm=l(jt);ri=r(Wm,"LI",{});var f0=l(ri);Ry=o(f0,"@sgugger wrote a great explanation of mixed precision "),ys=r(f0,"A",{href:!0,rel:!0});var yk=l(ys);Wy=o(yk,"here"),yk.forEach(t),f0.forEach(t),Xy=f(Wm),li=r(Wm,"LI",{});var c0=l(li);Yy=o(c0,"Aleksey Bilogur\u2019s "),gs=r(c0,"A",{href:!0,rel:!0});var gk=l(gs);Qy=o(gk,"A developer-friendly guide to mixed precision training with PyTorch"),gk.forEach(t),c0.forEach(t),Wm.forEach(t),ad=f(e),Z=r(e,"P",{});var vo=l(Z);Zy=o(vo,`You can also see a variety of benchmarks on fp16 vs other precisions:
`),Es=r(vo,"A",{href:!0,rel:!0});var Ek=l(Es);Jy=o(Ek,"RTX-3090"),Ek.forEach(t),Ky=o(vo,` and
`),Ps=r(vo,"A",{href:!0,rel:!0});var Pk=l(Ps);e2=o(Pk,"A100"),Pk.forEach(t),t2=o(vo,"."),vo.forEach(t),sd=f(e),je=r(e,"H5",{class:!0});var Xm=l(je);Dt=r(Xm,"A",{id:!0,class:!0,href:!0});var kk=l(Dt);Ip=r(kk,"SPAN",{});var $k=l(Ip);u(ks.$$.fragment,$k),$k.forEach(t),kk.forEach(t),a2=f(Xm),Sp=r(Xm,"SPAN",{});var Ak=l(Sp);s2=o(Ak,"fp16 caching"),Ak.forEach(t),Xm.forEach(t),rd=f(e),J=r(e,"P",{});var wo=l(J);r2=o(wo,"pytorch "),Np=r(wo,"CODE",{});var xk=l(Np);l2=o(xk,"autocast"),xk.forEach(t),i2=o(wo," which performs AMP include a caching feature, which speed things up by caching fp16-converted values. Here is the full description from this "),$s=r(wo,"A",{href:!0,rel:!0});var jk=l($s);o2=o(jk,"comment"),jk.forEach(t),n2=o(wo,":"),wo.forEach(t),ld=f(e),ii=r(e,"P",{});var Dk=l(ii);p2=o(Dk,"Autocast maintains a cache of the FP16 casts of model parameters (leaves). This helps streamline parameter reuse: if the same FP32 param is used in several different FP16list ops, like several matmuls, instead of re-casting the param to FP16 on entering each matmul, the cast will occur on the first matmul, the casted FP16 copy will be cached, and for all later matmuls the FP16 copy will be reused. The cache is maintained only within a particular outermost autocast context. When you exit the autocast context the cache is dropped. For recommended usage, in which autocast wraps the forward pass, and then you exit the context before calling backward(), this means the cache only lasts the duration of the forward pass each iteration, and will be rebuilt next iteration. (The cache of FP16-casted copies MUST be rebuilt each iteration. The FP32 parameters get updated by the optimizer, so the FP16 copies must be recreated, otherwise the FP16 values will be stale.)"),Dk.forEach(t),id=f(e),De=r(e,"H5",{class:!0});var Ym=l(De);Tt=r(Ym,"A",{id:!0,class:!0,href:!0});var Tk=l(Tt);Cp=r(Tk,"SPAN",{});var Ik=l(Cp);u(As.$$.fragment,Ik),Ik.forEach(t),Tk.forEach(t),h2=f(Ym),Lp=r(Ym,"SPAN",{});var Sk=l(Lp);f2=o(Sk,"fp16 Inference"),Sk.forEach(t),Ym.forEach(t),od=f(e),oi=r(e,"P",{});var Nk=l(oi);c2=o(Nk,"While normally inference is done with fp16/amp as with training, it\u2019s also possible to use the full fp16 mode without using mixed precision. This is especially a good fit if the pretrained model weights are already in fp16. So a lot less memory is used: 2 bytes per parameter vs 6 bytes with mixed precision!"),Nk.forEach(t),nd=f(e),ni=r(e,"P",{});var Ck=l(ni);d2=o(Ck,"How good the results this will deliver will depend on the model. If it can handle fp16 without overflows and accuracy issues, then it\u2019ll definitely better to use the full fp16 mode."),Ck.forEach(t),pd=f(e),pi=r(e,"P",{});var Lk=l(pi);u2=o(Lk,"For example, LayerNorm has to be done in fp32 and recent pytorch (1.10+) has been fixed to do that regardless of the input types, but earlier pytorch versions accumulate in the input type which can be an issue."),Lk.forEach(t),hd=f(e),It=r(e,"P",{});var Qm=l(It);m2=o(Qm,"In \u{1F917} Transformers the full fp16 inference is enabled by passing "),Gp=r(Qm,"CODE",{});var Gk=l(Gp);v2=o(Gk,"--fp16_full_eval"),Gk.forEach(t),w2=o(Qm," to the \u{1F917} Trainer."),Qm.forEach(t),fd=f(e),Te=r(e,"H4",{class:!0});var Zm=l(Te);St=r(Zm,"A",{id:!0,class:!0,href:!0});var Uk=l(St);Up=r(Uk,"SPAN",{});var Mk=l(Up);u(xs.$$.fragment,Mk),Mk.forEach(t),Uk.forEach(t),_2=f(Zm),Mp=r(Zm,"SPAN",{});var Ok=l(Mp);b2=o(Ok,"bf16"),Ok.forEach(t),Zm.forEach(t),cd=f(e),K=r(e,"P",{});var _o=l(K);y2=o(_o,"If you own Ampere or newer hardware you can start using bf16 for your training and evaluation. While bf16 has a worse precision than fp16, it has a much much bigger dynamic range. Therefore, if in the past you were experiencing overflow issues while training the model, bf16 will prevent this from happening most of the time. Remember that in fp16 the biggest number you can have is "),Op=r(_o,"CODE",{});var zk=l(Op);g2=o(zk,"65535"),zk.forEach(t),E2=o(_o," and any number above that will overflow. A bf16 number can be as large as "),zp=r(_o,"CODE",{});var Vk=l(zp);P2=o(Vk,"3.39e+38"),Vk.forEach(t),k2=o(_o," (!) which is about the same as fp32 - because both have 8-bits used for the numerical range."),_o.forEach(t),dd=f(e),hi=r(e,"P",{});var Hk=l(hi);$2=o(Hk,"Automatic Mixed Precision (AMP) is the same as with fp16, except it\u2019ll use bf16."),Hk.forEach(t),ud=f(e),fi=r(e,"P",{});var Bk=l(fi);A2=o(Bk,"Thanks to the fp32-like dynamic range with bf16 mixed precision loss scaling is no longer needed."),Bk.forEach(t),md=f(e),ci=r(e,"P",{});var Fk=l(ci);x2=o(Fk,"If you have tried to finetune models pre-trained under bf16 mixed precision (e.g. T5) it\u2019s very likely that you have encountered overflow issues. Now you should be able to finetune those models without any issues."),Fk.forEach(t),vd=f(e),di=r(e,"P",{});var qk=l(di);j2=o(qk,"That said, also be aware that if you pre-trained a model in bf16, it\u2019s likely to have overflow issues if someone tries to finetune it in fp16 down the road. So once started on the bf16-mode path it\u2019s best to remain on it and not switch to fp16."),qk.forEach(t),wd=f(e),Nt=r(e,"P",{});var Jm=l(Nt);D2=o(Jm,"In \u{1F917} Transformers bf16 mixed precision is enabled by passing "),Vp=r(Jm,"CODE",{});var Rk=l(Vp);T2=o(Rk,"--bf16"),Rk.forEach(t),I2=o(Jm," to the \u{1F917} Trainer."),Jm.forEach(t),_d=f(e),ui=r(e,"P",{});var Wk=l(ui);S2=o(Wk,"If you use your own trainer, this is just:"),Wk.forEach(t),bd=f(e),u(js.$$.fragment,e),yd=f(e),Ds=r(e,"P",{});var d0=l(Ds);N2=o(d0,"If you need to switch a tensor to bf16, it\u2019s just: "),Hp=r(d0,"CODE",{});var Xk=l(Hp);C2=o(Xk,"t.to(dtype=torch.bfloat16)"),Xk.forEach(t),d0.forEach(t),gd=f(e),mi=r(e,"P",{});var Yk=l(mi);L2=o(Yk,"Here is how you can check if your setup supports bf16:"),Yk.forEach(t),Ed=f(e),u(Ts.$$.fragment,e),Pd=f(e),vi=r(e,"P",{});var Qk=l(vi);G2=o(Qk,"On the other hand bf16 has a much worse precision than fp16, so there are certain situations where you\u2019d still want to use fp16 and not bf16."),Qk.forEach(t),kd=f(e),ee=r(e,"P",{});var bo=l(ee);U2=o(bo,`You can also see a variety of benchmarks on bf16 vs other precisions:
`),Is=r(bo,"A",{href:!0,rel:!0});var Zk=l(Is);M2=o(Zk,"RTX-3090"),Zk.forEach(t),O2=o(bo,` and
`),Ss=r(bo,"A",{href:!0,rel:!0});var Jk=l(Ss);z2=o(Jk,"A100"),Jk.forEach(t),V2=o(bo,"."),bo.forEach(t),$d=f(e),Ie=r(e,"H5",{class:!0});var Km=l(Ie);Ct=r(Km,"A",{id:!0,class:!0,href:!0});var Kk=l(Ct);Bp=r(Kk,"SPAN",{});var e7=l(Bp);u(Ns.$$.fragment,e7),e7.forEach(t),Kk.forEach(t),H2=f(Km),Fp=r(Km,"SPAN",{});var t7=l(Fp);B2=o(t7,"bf16 Inference"),t7.forEach(t),Km.forEach(t),Ad=f(e),Lt=r(e,"P",{});var ev=l(Lt);F2=o(ev,"Same as with fp16, you can do inference in either the mixed precision bf16 or using the full bf16 mode. The same caveats apply. For details see "),wi=r(ev,"A",{href:!0});var a7=l(wi);q2=o(a7,"fp16 Inference"),a7.forEach(t),R2=o(ev,"."),ev.forEach(t),xd=f(e),Gt=r(e,"P",{});var tv=l(Gt);W2=o(tv,"In \u{1F917} Transformers the full bf16 inference is enabled by passing "),qp=r(tv,"CODE",{});var s7=l(qp);X2=o(s7,"--bf16_full_eval"),s7.forEach(t),Y2=o(tv," to the \u{1F917} Trainer."),tv.forEach(t),jd=f(e),Se=r(e,"H4",{class:!0});var av=l(Se);Ut=r(av,"A",{id:!0,class:!0,href:!0});var r7=l(Ut);Rp=r(r7,"SPAN",{});var l7=l(Rp);u(Cs.$$.fragment,l7),l7.forEach(t),r7.forEach(t),Q2=f(av),Wp=r(av,"SPAN",{});var i7=l(Wp);Z2=o(i7,"tf32"),i7.forEach(t),av.forEach(t),Dd=f(e),_i=r(e,"P",{});var o7=l(_i);J2=o(o7,"The Ampere hardware uses a magical data type called tf32. It has the same numerical range as fp32 (8-bits), but instead of 23 bits precision it has only 10 bits (same as fp16). In total it uses only 19 bits."),o7.forEach(t),Td=f(e),bi=r(e,"P",{});var n7=l(bi);K2=o(n7,"It\u2019s magical in the sense that you can use the normal fp32 training and/or inference code and by enabling tf32 support you can get up to 3x throughput improvement. All you need to do is to add this to your code:"),n7.forEach(t),Id=f(e),u(Ls.$$.fragment,e),Sd=f(e),yi=r(e,"P",{});var p7=l(yi);eg=o(p7,"When this is done CUDA will automatically switch to using tf32 instead of fp32 where it\u2019s possible. This, of course, assumes that the used GPU is from the Ampere series."),p7.forEach(t),Nd=f(e),Mt=r(e,"P",{});var sv=l(Mt);tg=o(sv,"Like all cases with reduced precision this may or may not be satisfactory for your needs, so you have to experiment and see. According to "),Gs=r(sv,"A",{href:!0,rel:!0});var h7=l(Gs);ag=o(h7,"NVIDIA research"),h7.forEach(t),sg=o(sv," the majority of machine learning training shouldn\u2019t be impacted and showed the same perplexity and convergence as the fp32 training."),sv.forEach(t),Cd=f(e),gi=r(e,"P",{});var f7=l(gi);rg=o(f7,"If you\u2019re already using fp16 or bf16 mixed precision it may help with the throughput as well."),f7.forEach(t),Ld=f(e),S=r(e,"P",{});var wa=l(S);lg=o(wa,"You can enable this mode in the \u{1F917} Trainer with "),Xp=r(wa,"CODE",{});var c7=l(Xp);ig=o(c7,"--tf32"),c7.forEach(t),og=o(wa,", or disable it with "),Yp=r(wa,"CODE",{});var d7=l(Yp);ng=o(d7,"--tf32 0"),d7.forEach(t),pg=o(wa," or "),Qp=r(wa,"CODE",{});var u7=l(Qp);hg=o(u7,"--no_tf32"),u7.forEach(t),fg=o(wa,`.
By default the PyTorch default is used.`),wa.forEach(t),Gd=f(e),te=r(e,"P",{});var yo=l(te);cg=o(yo,"Note: tf32 mode is internal to CUDA and can\u2019t be accessed directly via "),Zp=r(yo,"CODE",{});var m7=l(Zp);dg=o(m7,"tensor.to(dtype=torch.tf32)"),m7.forEach(t),ug=o(yo," as "),Jp=r(yo,"CODE",{});var v7=l(Jp);mg=o(v7,"torch.tf32"),v7.forEach(t),vg=o(yo," doesn\u2019t exit."),yo.forEach(t),Ud=f(e),Ot=r(e,"P",{});var rv=l(Ot);wg=o(rv,"Note: you need "),Kp=r(rv,"CODE",{});var w7=l(Kp);_g=o(w7,"torch>=1.7"),w7.forEach(t),bg=o(rv," to enjoy this feature."),rv.forEach(t),Md=f(e),ae=r(e,"P",{});var go=l(ae);yg=o(go,`You can also see a variety of benchmarks on tf32 vs other precisions:
`),Us=r(go,"A",{href:!0,rel:!0});var _7=l(Us);gg=o(_7,"RTX-3090"),_7.forEach(t),Eg=o(go,` and
`),Ms=r(go,"A",{href:!0,rel:!0});var b7=l(Ms);Pg=o(b7,"A100"),b7.forEach(t),kg=o(go,"."),go.forEach(t),Od=f(e),Ne=r(e,"H3",{class:!0});var lv=l(Ne);zt=r(lv,"A",{id:!0,class:!0,href:!0});var y7=l(zt);eh=r(y7,"SPAN",{});var g7=l(eh);u(Os.$$.fragment,g7),g7.forEach(t),y7.forEach(t),$g=f(lv),th=r(lv,"SPAN",{});var E7=l(th);Ag=o(E7,"Gradient Accumulation"),E7.forEach(t),lv.forEach(t),zd=f(e),se=r(e,"P",{});var Eo=l(se);xg=o(Eo,"Since gradient accumulation essentially is identical to having a larger batch size, just as with the larger batch size here you are likely to see a 20-30% speedup due to the optimizer running less often. For example, see benchmarks for "),zs=r(Eo,"A",{href:!0,rel:!0});var P7=l(zs);jg=o(P7,"RTX-3090"),P7.forEach(t),Dg=o(Eo,`
and `),Vs=r(Eo,"A",{href:!0,rel:!0});var k7=l(Vs);Tg=o(k7,"A100"),k7.forEach(t),Ig=o(Eo,"."),Eo.forEach(t),Vd=f(e),Vt=r(e,"P",{});var iv=l(Vt);Sg=o(iv,"To activate this feature in \u{1F917} Trainer add "),ah=r(iv,"CODE",{});var $7=l(ah);Ng=o($7,"--gradient_accumulation_steps 4"),$7.forEach(t),Cg=o(iv," to its arguments (experiment with the value to get the best performance)."),iv.forEach(t),Hd=f(e),Ei=r(e,"P",{});var A7=l(Ei);Lg=o(A7,"It\u2019s important to remember that using gradient accumulation you may end up with a much larger effective batch size, so you may need to adjust the learning rate, its warm up and for very short datasets it\u2019ll impact the loss as the training will end up doing less steps than normal."),A7.forEach(t),Bd=f(e),Ce=r(e,"H3",{class:!0});var ov=l(Ce);Ht=r(ov,"A",{id:!0,class:!0,href:!0});var x7=l(Ht);sh=r(x7,"SPAN",{});var j7=l(sh);u(Hs.$$.fragment,j7),j7.forEach(t),x7.forEach(t),Gg=f(ov),rh=r(ov,"SPAN",{});var D7=l(rh);Ug=o(D7,"Gradient Checkpointing"),D7.forEach(t),ov.forEach(t),Fd=f(e),Pi=r(e,"P",{});var T7=l(Pi);Mg=o(T7,"One way to use significantly less GPU memory is to enabled \u201CGradient Checkpointing\u201D (also known as \u201Cactivation checkpointing\u201D). When enabled, a lot of memory can be freed at the cost of small decrease in the training speed due to recomputing parts of the graph during back-propagation. The slowdown will depend on the model but quite often it is around 20-30%."),T7.forEach(t),qd=f(e),N=r(e,"P",{});var _a=l(N);Og=o(_a,"This technique was first shared in the paper: "),Bs=r(_a,"A",{href:!0,rel:!0});var I7=l(Bs);zg=o(I7,"Training Deep Nets with Sublinear Memory Cost"),I7.forEach(t),Vg=o(_a,". The paper will also give you the exact details on the savings, but it\u2019s in the ballpark of "),lh=r(_a,"CODE",{});var S7=l(lh);Hg=o(S7,"O(sqrt(n))"),S7.forEach(t),Bg=o(_a,", where "),ih=r(_a,"CODE",{});var N7=l(ih);Fg=o(N7,"n"),N7.forEach(t),qg=o(_a," is the number of feed-forward layers."),_a.forEach(t),Rd=f(e),ki=r(e,"P",{});var C7=l(ki);Rg=o(C7,"To activate this feature in \u{1F917} Transformers for models that support it, use:"),C7.forEach(t),Wd=f(e),u(Fs.$$.fragment,e),Xd=f(e),Bt=r(e,"P",{});var nv=l(Bt);Wg=o(nv,"or add "),oh=r(nv,"CODE",{});var L7=l(oh);Xg=o(L7,"--gradient_checkpointing"),L7.forEach(t),Yg=o(nv," to the Trainer arguments."),nv.forEach(t),Yd=f(e),Le=r(e,"H3",{class:!0});var pv=l(Le);Ft=r(pv,"A",{id:!0,class:!0,href:!0});var G7=l(Ft);nh=r(G7,"SPAN",{});var U7=l(nh);u(qs.$$.fragment,U7),U7.forEach(t),G7.forEach(t),Qg=f(pv),ph=r(pv,"SPAN",{});var M7=l(ph);Zg=o(M7,"Batch sizes"),M7.forEach(t),pv.forEach(t),Qd=f(e),$i=r(e,"P",{});var O7=l($i);Jg=o(O7,"One gets the most efficient performance when batch sizes and input/output neuron counts are divisible by a certain number, which typically starts at 8, but can be much higher as well. That number varies a lot depending on the specific hardware being used and the dtype of the model."),O7.forEach(t),Zd=f(e),re=r(e,"P",{});var Po=l(re);Kg=o(Po,"For example for fully connected layers (which correspond to GEMMs), NVIDIA provides recommendations for "),Rs=r(Po,"A",{href:!0,rel:!0});var z7=l(Rs);e3=o(z7,"input/output neuron counts"),z7.forEach(t),t3=o(Po," and "),Ws=r(Po,"A",{href:!0,rel:!0});var V7=l(Ws);a3=o(V7,"batch size"),V7.forEach(t),s3=o(Po,"."),Po.forEach(t),Jd=f(e),Xs=r(e,"P",{});var u0=l(Xs);Ys=r(u0,"A",{href:!0,rel:!0});var H7=l(Ys);r3=o(H7,"Tensor Core Requirements"),H7.forEach(t),l3=o(u0," define the multiplier based on the dtype and the hardware. For example, for fp16 a multiple of 8 is recommended, but on A100 it\u2019s 64!"),u0.forEach(t),Kd=f(e),qt=r(e,"P",{});var hv=l(qt);i3=o(hv,"For parameters that are small, there is also "),Qs=r(hv,"A",{href:!0,rel:!0});var B7=l(Qs);o3=o(B7,"Dimension Quantization Effects"),B7.forEach(t),n3=o(hv," to consider, this is where tiling happens and the right multiplier can have a significant speedup."),hv.forEach(t),eu=f(e),C=r(e,"P",{});var ba=l(C);p3=o(ba,"Additionally, as explained in the "),Ai=r(ba,"A",{href:!0});var F7=l(Ai);h3=o(F7,"Gradient Accumulation"),F7.forEach(t),f3=o(ba,` section, the bigger the batch size the less often the optimizer is run, the faster the training is (considering the same dataset length). See benchmarks
for `),Zs=r(ba,"A",{href:!0,rel:!0});var q7=l(Zs);c3=o(q7,"RTX-3090"),q7.forEach(t),d3=o(ba,`
and `),Js=r(ba,"A",{href:!0,rel:!0});var R7=l(Js);u3=o(R7,"A100"),R7.forEach(t),m3=o(ba,"."),ba.forEach(t),tu=f(e),Ge=r(e,"H3",{class:!0});var fv=l(Ge);Rt=r(fv,"A",{id:!0,class:!0,href:!0});var W7=l(Rt);hh=r(W7,"SPAN",{});var X7=l(hh);u(Ks.$$.fragment,X7),X7.forEach(t),W7.forEach(t),v3=f(fv),fh=r(fv,"SPAN",{});var Y7=l(fh);w3=o(Y7,"DP vs DDP"),Y7.forEach(t),fv.forEach(t),au=f(e),Ue=r(e,"P",{});var tf=l(Ue);ch=r(tf,"CODE",{});var Q7=l(ch);_3=o(Q7,"DistributedDataParallel"),Q7.forEach(t),b3=o(tf," (DDP) is typically faster than "),dh=r(tf,"CODE",{});var Z7=l(dh);y3=o(Z7,"DataParallel"),Z7.forEach(t),g3=o(tf," (DP), but it is not always the case:"),tf.forEach(t),su=f(e),Wt=r(e,"UL",{});var cv=l(Wt);uh=r(cv,"LI",{});var J7=l(uh);E3=o(J7,"while DP is python threads-based, DDP is multiprocess-based - and as such it has no python threads limitations, such as GIL"),J7.forEach(t),P3=f(cv),mh=r(cv,"LI",{});var K7=l(mh);k3=o(K7,"on the other hand a slow inter-connectivity between the GPU cards could lead to an actual slower outcome with DDP"),K7.forEach(t),cv.forEach(t),ru=f(e),xi=r(e,"P",{});var e$=l(xi);$3=o(e$,"Here are the main differences in the inter-GPU communication overhead between the two modes:"),e$.forEach(t),lu=f(e),er=r(e,"P",{});var m0=l(er);tr=r(m0,"A",{href:!0,rel:!0});var t$=l(tr);A3=o(t$,"DDP"),t$.forEach(t),x3=o(m0,":"),m0.forEach(t),iu=f(e),Xt=r(e,"UL",{});var dv=l(Xt);vh=r(dv,"LI",{});var a$=l(vh);j3=o(a$,"At the start time the main process replicates the model once from gpu 0 to the rest of gpus"),a$.forEach(t),D3=f(dv),ji=r(dv,"LI",{});var v0=l(ji);T3=o(v0,"Then for each batch:"),ar=r(v0,"OL",{});var uv=l(ar);wh=r(uv,"LI",{});var s$=l(wh);I3=o(s$,"each gpu consumes each own mini-batch of data directly"),s$.forEach(t),S3=f(uv),sr=r(uv,"LI",{});var mv=l(sr);N3=o(mv,"during "),_h=r(mv,"CODE",{});var r$=l(_h);C3=o(r$,"backward"),r$.forEach(t),L3=o(mv,", once the local gradients are ready, they are then averaged across all processes"),mv.forEach(t),uv.forEach(t),v0.forEach(t),dv.forEach(t),ou=f(e),rr=r(e,"P",{});var w0=l(rr);lr=r(w0,"A",{href:!0,rel:!0});var l$=l(lr);G3=o(l$,"DP"),l$.forEach(t),U3=o(w0,":"),w0.forEach(t),nu=f(e),Di=r(e,"P",{});var i$=l(Di);M3=o(i$,"For each batch:"),i$.forEach(t),pu=f(e),$=r(e,"OL",{});var oe=l($);bh=r(oe,"LI",{});var o$=l(bh);O3=o(o$,"gpu 0 reads the batch of data and then sends a mini-batch to each gpu"),o$.forEach(t),z3=f(oe),yh=r(oe,"LI",{});var n$=l(yh);V3=o(n$,"replicates the up-to-date model from gpu 0 to each gpu"),n$.forEach(t),H3=f(oe),ir=r(oe,"LI",{});var vv=l(ir);B3=o(vv,"runs "),gh=r(vv,"CODE",{});var p$=l(gh);F3=o(p$,"forward"),p$.forEach(t),q3=o(vv," and sends output from each gpu to gpu 0, computes loss"),vv.forEach(t),R3=f(oe),Ti=r(oe,"LI",{});var _0=l(Ti);W3=o(_0,"scatters loss from gpu 0 to all gpus, runs "),Eh=r(_0,"CODE",{});var h$=l(Eh);X3=o(h$,"backward"),h$.forEach(t),_0.forEach(t),Y3=f(oe),Ph=r(oe,"LI",{});var f$=l(Ph);Q3=o(f$,"sends gradients from each gpu to gpu 0 and averages those"),f$.forEach(t),oe.forEach(t),hu=f(e),Ii=r(e,"P",{});var c$=l(Ii);Z3=o(c$,"The only communication DDP performs per batch is sending gradients, whereas DP does 5 different data exchanges per batch."),c$.forEach(t),fu=f(e),Yt=r(e,"P",{});var wv=l(Yt);J3=o(wv,"DP copies data within the process via python threads, whereas DDP copies data via "),or=r(wv,"A",{href:!0,rel:!0});var d$=l(or);K3=o(d$,"torch.distributed"),d$.forEach(t),eE=o(wv,"."),wv.forEach(t),cu=f(e),Si=r(e,"P",{});var u$=l(Si);tE=o(u$,"Under DP gpu 0 performs a lot more work than the rest of the gpus, thus resulting in under-utilization of gpus."),u$.forEach(t),du=f(e),Ni=r(e,"P",{});var m$=l(Ni);aE=o(m$,"You can use DDP across multiple machines, but this is not the case with DP."),m$.forEach(t),uu=f(e),Ci=r(e,"P",{});var v$=l(Ci);sE=o(v$,"There are other differences between DP and DDP but they aren\u2019t relevant to this discussion."),v$.forEach(t),mu=f(e),Qt=r(e,"P",{});var _v=l(Qt);rE=o(_v,"If you want to go really deep into understanding these 2 modes, this "),nr=r(_v,"A",{href:!0,rel:!0});var w$=l(nr);lE=o(w$,"article"),w$.forEach(t),iE=o(_v," is highly recommended, as it has great diagrams, includes multiple benchmarks and profiler outputs on various hardware, explains all the nuances that you may need to know."),_v.forEach(t),vu=f(e),Li=r(e,"P",{});var _$=l(Li);oE=o(_$,"Let\u2019s look at an actual benchmark:"),_$.forEach(t),wu=f(e),Zt=r(e,"TABLE",{});var bv=l(Zt);kh=r(bv,"THEAD",{});var b$=l(kh);Me=r(b$,"TR",{});var ko=l(Me);Gi=r(ko,"TH",{align:!0});var y$=l(Gi);nE=o(y$,"Type"),y$.forEach(t),pE=f(ko),$h=r(ko,"TH",{});var g$=l($h);hE=o(g$,"NVlink"),g$.forEach(t),fE=f(ko),Ui=r(ko,"TH",{align:!0});var E$=l(Ui);cE=o(E$,"Time"),E$.forEach(t),ko.forEach(t),b$.forEach(t),dE=f(bv),Oe=r(bv,"TBODY",{});var $o=l(Oe);ze=r($o,"TR",{});var Ao=l(ze);Mi=r(Ao,"TD",{align:!0});var P$=l(Mi);uE=o(P$,"2:DP"),P$.forEach(t),mE=f(Ao),Ah=r(Ao,"TD",{});var k$=l(Ah);vE=o(k$,"Y"),k$.forEach(t),wE=f(Ao),Oi=r(Ao,"TD",{align:!0});var $$=l(Oi);_E=o($$,"110s"),$$.forEach(t),Ao.forEach(t),bE=f($o),Ve=r($o,"TR",{});var xo=l(Ve);zi=r(xo,"TD",{align:!0});var A$=l(zi);yE=o(A$,"2:DDP"),A$.forEach(t),gE=f(xo),xh=r(xo,"TD",{});var x$=l(xh);EE=o(x$,"Y"),x$.forEach(t),PE=f(xo),Vi=r(xo,"TD",{align:!0});var j$=l(Vi);kE=o(j$,"101s"),j$.forEach(t),xo.forEach(t),$E=f($o),He=r($o,"TR",{});var jo=l(He);Hi=r(jo,"TD",{align:!0});var D$=l(Hi);AE=o(D$,"2:DDP"),D$.forEach(t),xE=f(jo),jh=r(jo,"TD",{});var T$=l(jh);jE=o(T$,"N"),T$.forEach(t),DE=f(jo),Bi=r(jo,"TD",{align:!0});var I$=l(Bi);TE=o(I$,"131s"),I$.forEach(t),jo.forEach(t),$o.forEach(t),bv.forEach(t),_u=f(e),Fi=r(e,"P",{});var S$=l(Fi);IE=o(S$,"Analysis:"),S$.forEach(t),bu=f(e),qi=r(e,"P",{});var N$=l(qi);SE=o(N$,"Here DP is ~10% slower than DDP w/ NVlink, but ~15% faster than DDP w/o NVlink"),N$.forEach(t),yu=f(e),Ri=r(e,"P",{});var C$=l(Ri);NE=o(C$,"The real difference will depend on how much data each GPU needs to sync with the others - the more there is to sync, the more a slow link will slow down the total runtime."),C$.forEach(t),gu=f(e),Wi=r(e,"P",{});var L$=l(Wi);CE=o(L$,"Here is the full benchmark code and outputs:"),L$.forEach(t),Eu=f(e),pr=r(e,"P",{});var b0=l(pr);Dh=r(b0,"CODE",{});var G$=l(Dh);LE=o(G$,"NCCL_P2P_DISABLE=1"),G$.forEach(t),GE=o(b0," was used to disable the NVLink feature on the corresponding benchmark."),b0.forEach(t),Pu=f(e),u(hr.$$.fragment,e),ku=f(e),k=r(e,"P",{});var F=l(k);UE=o(F,"Hardware: 2x TITAN RTX 24GB each + NVlink with 2 NVLinks ("),Th=r(F,"CODE",{});var U$=l(Th);ME=o(U$,"NV2"),U$.forEach(t),OE=o(F," in "),Ih=r(F,"CODE",{});var M$=l(Ih);zE=o(M$,"nvidia-smi topo -m"),M$.forEach(t),VE=o(F,`)
Software: `),Sh=r(F,"CODE",{});var O$=l(Sh);HE=o(O$,"pytorch-1.8-to-be"),O$.forEach(t),BE=o(F," + "),Nh=r(F,"CODE",{});var z$=l(Nh);FE=o(z$,"cuda-11.0"),z$.forEach(t),qE=o(F," / "),Ch=r(F,"CODE",{});var V$=l(Ch);RE=o(V$,"transformers==4.3.0.dev0"),V$.forEach(t),F.forEach(t),$u=f(e),Be=r(e,"H3",{class:!0});var yv=l(Be);Jt=r(yv,"A",{id:!0,class:!0,href:!0});var H$=l(Jt);Lh=r(H$,"SPAN",{});var B$=l(Lh);u(fr.$$.fragment,B$),B$.forEach(t),H$.forEach(t),WE=f(yv),Gh=r(yv,"SPAN",{});var F$=l(Gh);XE=o(F$,"DataLoader"),F$.forEach(t),yv.forEach(t),Au=f(e),Xi=r(e,"P",{});var q$=l(Xi);YE=o(q$,"One of the important requirements to reach great training speed is the ability to feed the GPU at the maximum speed it can handle. By default everything happens in the main process and it might not be able to read the data from disk fast enough, and thus create a bottleneck, leading to GPU under-utilization."),q$.forEach(t),xu=f(e),Kt=r(e,"UL",{});var gv=l(Kt);Yi=r(gv,"LI",{});var y0=l(Yi);Uh=r(y0,"CODE",{});var R$=l(Uh);QE=o(R$,"DataLoader(pin_memory=True, ...)"),R$.forEach(t),ZE=o(y0," which ensures that the data gets preloaded into the pinned memory on CPU and typically leads to much faster transfers from CPU to GPU memory."),y0.forEach(t),JE=f(gv),Qi=r(gv,"LI",{});var g0=l(Qi);Mh=r(g0,"CODE",{});var W$=l(Mh);KE=o(W$,"DataLoader(num_workers=4, ...)"),W$.forEach(t),e6=o(g0," - spawn several workers to pre-load data faster - during training watch the GPU utilization stats and if it\u2019s far from 100% experiment with raising the number of workers. Of course, the problem could be elsewhere so a very big number of workers won\u2019t necessarily lead to a better performance."),g0.forEach(t),gv.forEach(t),ju=f(e),Fe=r(e,"H3",{class:!0});var Ev=l(Fe);ea=r(Ev,"A",{id:!0,class:!0,href:!0});var X$=l(ea);Oh=r(X$,"SPAN",{});var Y$=l(Oh);u(cr.$$.fragment,Y$),Y$.forEach(t),X$.forEach(t),t6=f(Ev),zh=r(Ev,"SPAN",{});var Q$=l(zh);a6=o(Q$,"Faster optimizer"),Q$.forEach(t),Ev.forEach(t),Du=f(e),qe=r(e,"P",{});var af=l(qe);s6=o(af,"pytorch-nightly introduced "),Vh=r(af,"CODE",{});var Z$=l(Vh);r6=o(Z$,"torch.optim._multi_tensor"),Z$.forEach(t),l6=o(af," which should significantly speed up the optimizers for situations with lots of small feature tensors. It should eventually become the default, but if you want to experiment with it sooner and don\u2019t mind using the bleed-edge, see: "),dr=r(af,"A",{href:!0,rel:!0});var J$=l(dr);i6=o(J$,"https://github.com/huggingface/transformers/issues/9965"),J$.forEach(t),af.forEach(t),Tu=f(e),Re=r(e,"H3",{class:!0});var Pv=l(Re);ta=r(Pv,"A",{id:!0,class:!0,href:!0});var K$=l(ta);Hh=r(K$,"SPAN",{});var eA=l(Hh);u(ur.$$.fragment,eA),eA.forEach(t),K$.forEach(t),o6=f(Pv),Bh=r(Pv,"SPAN",{});var tA=l(Bh);n6=o(tA,"Sparsity"),tA.forEach(t),Pv.forEach(t),Iu=f(e),We=r(e,"H4",{class:!0});var kv=l(We);aa=r(kv,"A",{id:!0,class:!0,href:!0});var aA=l(aa);Fh=r(aA,"SPAN",{});var sA=l(Fh);u(mr.$$.fragment,sA),sA.forEach(t),aA.forEach(t),p6=f(kv),qh=r(kv,"SPAN",{});var rA=l(qh);h6=o(rA,"Mixture of Experts"),rA.forEach(t),kv.forEach(t),Su=f(e),Zi=r(e,"P",{});var lA=l(Zi);f6=o(lA,`Quite a few of the recent papers reported a 4-5x training speedup and a faster inference by integrating
Mixture of Experts (MoE) into the Transformer models.`),lA.forEach(t),Nu=f(e),Ji=r(e,"P",{});var iA=l(Ji);c6=o(iA,"Since it has been discovered that more parameters lead to better performance, this technique allows to increase the number of parameters by an order of magnitude without increasing training costs."),iA.forEach(t),Cu=f(e),Ki=r(e,"P",{});var oA=l(Ki);d6=o(oA,"In this approach every other FFN layer is replaced with a MoE Layer which consists of many experts, with a gated function that trains each expert in a balanced way depending on the input token\u2019s position in a sequence."),oA.forEach(t),Lu=f(e),eo=r(e,"P",{});var nA=l(eo);to=r(nA,"IMG",{src:!0,alt:!0}),nA.forEach(t),Gu=f(e),sa=r(e,"P",{});var $v=l(sa);u6=o($v,"(source: "),vr=r($v,"A",{href:!0,rel:!0});var pA=l(vr);m6=o(pA,"GLAM"),pA.forEach(t),v6=o($v,")"),$v.forEach(t),Uu=f(e),ao=r(e,"P",{});var hA=l(ao);w6=o(hA,"You can find exhaustive details and comparison tables in the papers listed at the end of this section."),hA.forEach(t),Mu=f(e),so=r(e,"P",{});var fA=l(so);_6=o(fA,"The main drawback of this approach is that it requires staggering amounts of GPU memory - almost an order of magnitude larger than its dense equivalent. Various distillation and approaches are proposed to how to overcome the much higher memory requirements."),fA.forEach(t),Ou=f(e),ro=r(e,"P",{});var cA=l(ro);b6=o(cA,"There is direct trade-off though, you can use just a few experts with a 2-3x smaller base model instead of dozens or hundreds experts leading to a 5x smaller model and thus increase the training speed moderately while increasing the memory requirements moderately as well."),cA.forEach(t),zu=f(e),lo=r(e,"P",{});var dA=l(lo);y6=o(dA,"Most related papers and implementations are built around Tensorflow/TPUs:"),dA.forEach(t),Vu=f(e),le=r(e,"UL",{});var Do=l(le);Rh=r(Do,"LI",{});var uA=l(Rh);wr=r(uA,"A",{href:!0,rel:!0});var mA=l(wr);g6=o(mA,"GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding"),mA.forEach(t),uA.forEach(t),E6=f(Do),Wh=r(Do,"LI",{});var vA=l(Wh);_r=r(vA,"A",{href:!0,rel:!0});var wA=l(_r);P6=o(wA,"Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity"),wA.forEach(t),vA.forEach(t),k6=f(Do),Xh=r(Do,"LI",{});var _A=l(Xh);br=r(_A,"A",{href:!0,rel:!0});var bA=l(br);$6=o(bA,"GLaM: Generalist Language Model (GLaM)"),bA.forEach(t),_A.forEach(t),Do.forEach(t),Hu=f(e),g=r(e,"P",{});var x=l(g);A6=o(x,"And for Pytorch DeepSpeed has built one as well: "),yr=r(x,"A",{href:!0,rel:!0});var yA=l(yr);x6=o(yA,"DeepSpeed-MoE: Advancing Mixture-of-Experts Inference and Training to Power Next-Generation AI Scale"),yA.forEach(t),j6=o(x,", "),gr=r(x,"A",{href:!0,rel:!0});var gA=l(gr);D6=o(gA,"Mixture of Experts"),gA.forEach(t),T6=o(x," - blog posts:  "),Er=r(x,"A",{href:!0,rel:!0});var EA=l(Er);I6=o(EA,"1"),EA.forEach(t),S6=o(x,", "),Pr=r(x,"A",{href:!0,rel:!0});var PA=l(Pr);N6=o(PA,"2"),PA.forEach(t),C6=o(x," and specific deployment with large transformer-based natural language generation models: "),kr=r(x,"A",{href:!0,rel:!0});var kA=l(kr);L6=o(kA,"blog post"),kA.forEach(t),G6=o(x,", "),io=r(x,"A",{href:!0});var $A=l(io);U6=o($A,"Megatron-Deepspeed branch"),$A.forEach(t),M6=o(x,"."),x.forEach(t),Bu=f(e),Xe=r(e,"H3",{class:!0});var Av=l(Xe);ra=r(Av,"A",{id:!0,class:!0,href:!0});var AA=l(ra);Yh=r(AA,"SPAN",{});var xA=l(Yh);u($r.$$.fragment,xA),xA.forEach(t),AA.forEach(t),O6=f(Av),Qh=r(Av,"SPAN",{});var jA=l(Qh);z6=o(jA,"Efficient Software Prebuilds"),jA.forEach(t),Av.forEach(t),Fu=f(e),la=r(e,"P",{});var xv=l(la);V6=o(xv,"PyTorch\u2019s "),Ar=r(xv,"A",{href:!0,rel:!0});var DA=l(Ar);H6=o(DA,"pip and conda builds"),DA.forEach(t),B6=o(xv," come prebuit with the cuda toolkit which is enough to run PyTorch, but it is insufficient if you need to build cuda extensions."),xv.forEach(t),qu=f(e),ia=r(e,"P",{});var jv=l(ia);F6=o(jv,"At times it may take an additional effort to pre-build some components, e.g., if you\u2019re using libraries like "),Zh=r(jv,"CODE",{});var TA=l(Zh);q6=o(TA,"apex"),TA.forEach(t),R6=o(jv," that don\u2019t come pre-compiled. In other situations figuring out how to install the right cuda toolkit system-wide can be complicated. To address these users\u2019 needs PyTorch and NVIDIA release a new version of NGC docker container which already comes with everything prebuilt and you just need to install your programs on it and it will run out of the box."),jv.forEach(t),Ru=f(e),oo=r(e,"P",{});var IA=l(oo);W6=o(IA,"This approach is also useful if you want to tweak the pytorch source and/or make a new customized build."),IA.forEach(t),Wu=f(e),ie=r(e,"P",{});var To=l(ie);X6=o(To,"To find the docker image version you want start "),xr=r(To,"A",{href:!0,rel:!0});var SA=l(xr);Y6=o(SA,"here"),SA.forEach(t),Q6=o(To,", choose one of the latest monthly releases. Go into the release\u2019s notes for the desired release, check that the environment\u2019s components are matching your needs (including NVIDIA Driver requirements!) and then at the very top of that document go to the corresponding NGC page. If for some reason you get lost, here is "),jr=r(To,"A",{href:!0,rel:!0});var NA=l(jr);Z6=o(NA,"the index of all PyTorch NGC images"),NA.forEach(t),J6=o(To,"."),To.forEach(t),Xu=f(e),no=r(e,"P",{});var CA=l(no);K6=o(CA,"Next follow the instructions to download and deploy the docker image."),CA.forEach(t),Yu=f(e),Ye=r(e,"H2",{class:!0});var Dv=l(Ye);oa=r(Dv,"A",{id:!0,class:!0,href:!0});var LA=l(oa);Jh=r(LA,"SPAN",{});var GA=l(Jh);u(Dr.$$.fragment,GA),GA.forEach(t),LA.forEach(t),e0=f(Dv),Kh=r(Dv,"SPAN",{});var UA=l(Kh);t0=o(UA,"Contribute"),UA.forEach(t),Dv.forEach(t),Qu=f(e),po=r(e,"P",{});var MA=l(po);a0=o(MA,"This document is far from being complete and a lot more needs to be added, so if you have additions or corrections to make please don\u2019t hesitate to open a PR or if you aren\u2019t sure start an Issue and we can discuss the details there."),MA.forEach(t),Zu=f(e),ho=r(e,"P",{});var OA=l(ho);s0=o(OA,"When making contributions that A is better than B, please try to include a reproducible benchmark and/or a link to the source of that information (unless it comes directly from you)."),OA.forEach(t),this.h()},h(){c(R,"name","hf:doc:metadata"),c(R,"content",JSON.stringify(WA)),c(G,"id","performance-and-scalability-how-to-fit-a-bigger-model-and-train-it-faster"),c(G,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(G,"href","#performance-and-scalability-how-to-fit-a-bigger-model-and-train-it-faster"),c(j,"class","relative group"),c(Qe,"id","quick-notes"),c(Qe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Qe,"href","#quick-notes"),c(ne,"class","relative group"),c(Ze,"id","faster-training"),c(Ze,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Ze,"href","#faster-training"),c(pe,"class","relative group"),c(Ke,"id","bigger-models"),c(Ke,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Ke,"href","#bigger-models"),c(he,"class","relative group"),c(Mr,"href","deepspeed#nvme-support"),c(et,"id","hardware"),c(et,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(et,"href","#hardware"),c(fe,"class","relative group"),c(tt,"id","power-and-cooling"),c(tt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(tt,"href","#power-and-cooling"),c(ce,"class","relative group"),c(at,"id","multigpu-connectivity"),c(at,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(at,"href","#multigpu-connectivity"),c(de,"class","relative group"),c(st,"id","nvlink"),c(st,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(st,"href","#nvlink"),c(ue,"class","relative group"),c(Ua,"href","https://en.wikipedia.org/wiki/NVLink"),c(Ua,"rel","nofollow"),c(Ma,"href","https://www.nvidia.com/content/dam/en-zz/Solutions/geforce/ampere/pdf/NVIDIA-ampere-GA102-GPU-Architecture-Whitepaper-V1.pdf"),c(Ma,"rel","nofollow"),c(il,"align","right"),c(ol,"align","right"),c(nl,"align","right"),c(ot,"id","software"),c(ot,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(ot,"href","#software"),c(me,"class","relative group"),c(nt,"id","model-scalability"),c(nt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(nt,"href","#model-scalability"),c(ve,"class","relative group"),c(cl,"href","parallelism"),c(pt,"id","anatomy-of-models-operations"),c(pt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(pt,"href","#anatomy-of-models-operations"),c(we,"class","relative group"),c(ts,"href","https://arxiv.org/abs/2007.00072"),c(ts,"rel","nofollow"),c(ht,"id","anatomy-of-models-memory"),c(ht,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(ht,"href","#anatomy-of-models-memory"),c(_e,"class","relative group"),c(ft,"id","model-weights"),c(ft,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(ft,"href","#model-weights"),c(be,"class","relative group"),c(dt,"id","optimizer-states"),c(dt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(dt,"href","#optimizer-states"),c(ye,"class","relative group"),c(ls,"href","https://github.com/facebookresearch/bitsandbytes"),c(ls,"rel","nofollow"),c(ut,"id","gradients"),c(ut,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(ut,"href","#gradients"),c(ge,"class","relative group"),c(mt,"id","forward-activations"),c(mt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(mt,"href","#forward-activations"),c(Ee,"class","relative group"),c(vt,"id","temporary-memory"),c(vt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(vt,"href","#temporary-memory"),c(Pe,"class","relative group"),c(wt,"id","functionalityspecific-memory"),c(wt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(wt,"href","#functionalityspecific-memory"),c(ke,"class","relative group"),c(_t,"id","forward-vs-backward-execution-speed"),c(_t,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(_t,"href","#forward-vs-backward-execution-speed"),c($e,"class","relative group"),c(yt,"id","floating-data-types"),c(yt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(yt,"href","#floating-data-types"),c(Ae,"class","relative group"),zA(Dl.src,E0="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/tf32-bf16-fp16-fp32.png")||c(Dl,"src",E0),c(Dl,"alt","data types"),c(ms,"href","https://developer.nvidia.com/blog/accelerating-ai-training-with-tf32-tensor-cores/"),c(ms,"rel","nofollow"),c(Et,"id","fp16"),c(Et,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Et,"href","#fp16"),c(xe,"class","relative group"),c(Ll,"align","right"),c(Gl,"align","right"),c(Ul,"align","right"),c(Ml,"align","right"),c(Ol,"align","right"),c(zl,"align","right"),c(Vl,"align","right"),c(Hl,"align","right"),c(Bl,"align","right"),c(Fl,"align","right"),c(ql,"align","right"),c(Rl,"align","right"),c(Wl,"align","right"),c(Xl,"align","right"),c(Yl,"align","right"),c(Ql,"align","right"),c(Zl,"align","right"),c(Jl,"align","right"),c(Kl,"align","right"),c(ei,"align","right"),c(bs,"href","https://github.com/bigscience-workshop/Megatron-DeepSpeed"),c(bs,"rel","nofollow"),c(ys,"href","https://docs.fast.ai/callback.fp16.html#A-little-bit-of-theory"),c(ys,"rel","nofollow"),c(gs,"href","https://spell.ml/blog/mixed-precision-training-with-pytorch-Xuk7YBEAACAASJam"),c(gs,"rel","nofollow"),c(Es,"href","https://github.com/huggingface/transformers/issues/14608#issuecomment-1004390803"),c(Es,"rel","nofollow"),c(Ps,"href","https://github.com/huggingface/transformers/issues/15026#issuecomment-1004543189"),c(Ps,"rel","nofollow"),c(Dt,"id","fp16-caching"),c(Dt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Dt,"href","#fp16-caching"),c(je,"class","relative group"),c($s,"href","https://discuss.pytorch.org/t/autocast-and-torch-no-grad-unexpected-behaviour/93475/3"),c($s,"rel","nofollow"),c(Tt,"id","fp16-inference"),c(Tt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Tt,"href","#fp16-inference"),c(De,"class","relative group"),c(St,"id","bf16"),c(St,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(St,"href","#bf16"),c(Te,"class","relative group"),c(Is,"href","https://github.com/huggingface/transformers/issues/14608#issuecomment-1004390803"),c(Is,"rel","nofollow"),c(Ss,"href","https://github.com/huggingface/transformers/issues/15026#issuecomment-1004543189"),c(Ss,"rel","nofollow"),c(Ct,"id","bf16-inference"),c(Ct,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Ct,"href","#bf16-inference"),c(Ie,"class","relative group"),c(wi,"href","#fp16-inference"),c(Ut,"id","tf32"),c(Ut,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Ut,"href","#tf32"),c(Se,"class","relative group"),c(Gs,"href","https://developer.nvidia.com/blog/accelerating-ai-training-with-tf32-tensor-cores/"),c(Gs,"rel","nofollow"),c(Us,"href","https://github.com/huggingface/transformers/issues/14608#issuecomment-1004390803"),c(Us,"rel","nofollow"),c(Ms,"href","https://github.com/huggingface/transformers/issues/15026#issuecomment-1004543189"),c(Ms,"rel","nofollow"),c(zt,"id","gradient-accumulation"),c(zt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(zt,"href","#gradient-accumulation"),c(Ne,"class","relative group"),c(zs,"href","https://github.com/huggingface/transformers/issues/14608#issuecomment-1004392537"),c(zs,"rel","nofollow"),c(Vs,"href","https://github.com/huggingface/transformers/issues/15026#issuecomment-1004592231"),c(Vs,"rel","nofollow"),c(Ht,"id","gradient-checkpointing"),c(Ht,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Ht,"href","#gradient-checkpointing"),c(Ce,"class","relative group"),c(Bs,"href","https://arxiv.org/abs/1604.06174"),c(Bs,"rel","nofollow"),c(Ft,"id","batch-sizes"),c(Ft,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Ft,"href","#batch-sizes"),c(Le,"class","relative group"),c(Rs,"href","https://docs.nvidia.com/deeplearning/performance/dl-performance-fully-connected/index.html#input-features"),c(Rs,"rel","nofollow"),c(Ws,"href","https://docs.nvidia.com/deeplearning/performance/dl-performance-fully-connected/index.html#batch-size"),c(Ws,"rel","nofollow"),c(Ys,"href","https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc"),c(Ys,"rel","nofollow"),c(Qs,"href","https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#dim-quantization"),c(Qs,"rel","nofollow"),c(Ai,"href","#gradient-accumulation"),c(Zs,"href","https://github.com/huggingface/transformers/issues/14608#issuecomment-1004392537"),c(Zs,"rel","nofollow"),c(Js,"href","https://github.com/huggingface/transformers/issues/15026#issuecomment-1005033957"),c(Js,"rel","nofollow"),c(Rt,"id","dp-vs-ddp"),c(Rt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Rt,"href","#dp-vs-ddp"),c(Ge,"class","relative group"),c(tr,"href","https://pytorch.org/docs/master/notes/ddp.html"),c(tr,"rel","nofollow"),c(lr,"href","https://pytorch.org/docs/master/generated/torch.nn.DataParallel.html"),c(lr,"rel","nofollow"),c(or,"href","https://pytorch.org/docs/master/distributed.html"),c(or,"rel","nofollow"),c(nr,"href","https://www.telesens.co/2019/04/04/distributed-data-parallel-training-using-pytorch-on-aws/"),c(nr,"rel","nofollow"),c(Gi,"align","left"),c(Ui,"align","right"),c(Mi,"align","left"),c(Oi,"align","right"),c(zi,"align","left"),c(Vi,"align","right"),c(Hi,"align","left"),c(Bi,"align","right"),c(Jt,"id","dataloader"),c(Jt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Jt,"href","#dataloader"),c(Be,"class","relative group"),c(ea,"id","faster-optimizer"),c(ea,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(ea,"href","#faster-optimizer"),c(Fe,"class","relative group"),c(dr,"href","https://github.com/huggingface/transformers/issues/9965"),c(dr,"rel","nofollow"),c(ta,"id","sparsity"),c(ta,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(ta,"href","#sparsity"),c(Re,"class","relative group"),c(aa,"id","mixture-of-experts"),c(aa,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(aa,"href","#mixture-of-experts"),c(We,"class","relative group"),zA(to.src,P0="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/perf-moe-transformer.png")||c(to,"src",P0),c(to,"alt","MoE Transformer 2x block"),c(vr,"href","https://ai.googleblog.com/2021/12/more-efficient-in-context-learning-with.html"),c(vr,"rel","nofollow"),c(wr,"href","https://arxiv.org/abs/2006.16668"),c(wr,"rel","nofollow"),c(_r,"href","https://arxiv.org/abs/2101.03961"),c(_r,"rel","nofollow"),c(br,"href","https://ai.googleblog.com/2021/12/more-efficient-in-context-learning-with.html"),c(br,"rel","nofollow"),c(yr,"href","https://arxiv.org/abs/2201.05596"),c(yr,"rel","nofollow"),c(gr,"href","https://www.deepspeed.ai/tutorials/mixture-of-experts/"),c(gr,"rel","nofollow"),c(Er,"href","https://www.microsoft.com/en-us/research/blog/deepspeed-powers-8x-larger-moe-model-training-with-high-performance/"),c(Er,"rel","nofollow"),c(Pr,"href","https://www.microsoft.com/en-us/research/publication/scalable-and-efficient-moe-training-for-multitask-multilingual-models/"),c(Pr,"rel","nofollow"),c(kr,"href","https://www.deepspeed.ai/news/2021/12/09/deepspeed-moe-nlg.html"),c(kr,"rel","nofollow"),c(io,"href","Thttps://github.com/microsoft/Megatron-DeepSpeed/tree/moe-training"),c(ra,"id","efficient-software-prebuilds"),c(ra,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(ra,"href","#efficient-software-prebuilds"),c(Xe,"class","relative group"),c(Ar,"href","https://pytorch.org/get-started/locally/#start-locally"),c(Ar,"rel","nofollow"),c(xr,"href","https://docs.nvidia.com/deeplearning/frameworks/pytorch-release-notes/"),c(xr,"rel","nofollow"),c(jr,"href","https://ngc.nvidia.com/catalog/containers/nvidia:pytorch"),c(jr,"rel","nofollow"),c(oa,"id","contribute"),c(oa,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(oa,"href","#contribute"),c(Ye,"class","relative group")},m(e,n){a(document.head,R),p(e,Tr,n),p(e,j,n),a(j,G),a(G,Io),m(ya,Io,null),a(j,Iv),a(j,So),a(So,Sv),p(e,sf,n),p(e,Ir,n),a(Ir,Nv),p(e,rf,n),p(e,ne,n),a(ne,Qe),a(Qe,No),m(ga,No,null),a(ne,Cv),a(ne,Co),a(Co,Lv),p(e,lf,n),p(e,Sr,n),a(Sr,Gv),p(e,of,n),p(e,pe,n),a(pe,Ze),a(Ze,Lo),m(Ea,Lo,null),a(pe,Uv),a(pe,Go),a(Go,Mv),p(e,nf,n),p(e,Nr,n),a(Nr,Ov),p(e,pf,n),p(e,Cr,n),a(Cr,Lr),a(Lr,zv),a(Lr,Pa),a(Pa,Uo),a(Uo,Vv),a(Pa,Hv),a(Pa,Mo),a(Mo,Bv),p(e,hf,n),p(e,Gr,n),a(Gr,Fv),p(e,ff,n),p(e,Je,n),a(Je,Oo),a(Oo,qv),a(Je,Rv),a(Je,zo),a(zo,Wv),p(e,cf,n),p(e,he,n),a(he,Ke),a(Ke,Vo),m(ka,Vo,null),a(he,Xv),a(he,Ho),a(Ho,Yv),p(e,df,n),p(e,Ur,n),a(Ur,Qv),p(e,uf,n),p(e,W,n),a(W,Bo),a(Bo,Zv),a(W,Jv),a(W,Fo),a(Fo,Kv),a(W,e1),a(W,$a),a($a,t1),a($a,Mr),a(Mr,a1),a($a,s1),p(e,mf,n),p(e,Or,n),a(Or,r1),p(e,vf,n),p(e,y,n),a(y,qo),a(qo,l1),a(y,i1),a(y,Ro),a(Ro,o1),a(y,n1),a(y,Wo),a(Wo,p1),a(y,h1),a(y,Xo),a(Xo,f1),a(y,c1),a(y,Yo),a(Yo,d1),a(y,u1),a(y,Qo),a(Qo,m1),a(y,v1),a(y,Zo),a(Zo,w1),p(e,wf,n),p(e,fe,n),a(fe,et),a(et,Jo),m(Aa,Jo,null),a(fe,_1),a(fe,Ko),a(Ko,b1),p(e,_f,n),p(e,ce,n),a(ce,tt),a(tt,en),m(xa,en,null),a(ce,y1),a(ce,tn),a(tn,g1),p(e,bf,n),p(e,zr,n),a(zr,E1),p(e,yf,n),p(e,ja,n),a(ja,an),a(an,P1),a(ja,k1),p(e,gf,n),p(e,Vr,n),a(Vr,$1),p(e,Ef,n),p(e,Hr,n),a(Hr,A1),p(e,Pf,n),p(e,Br,n),a(Br,x1),p(e,kf,n),p(e,Fr,n),a(Fr,j1),p(e,$f,n),p(e,qr,n),a(qr,D1),p(e,Af,n),p(e,Rr,n),a(Rr,T1),p(e,xf,n),p(e,Da,n),a(Da,sn),a(sn,I1),a(Da,S1),p(e,jf,n),p(e,Wr,n),a(Wr,N1),p(e,Df,n),p(e,Xr,n),a(Xr,C1),p(e,Tf,n),p(e,de,n),a(de,at),a(at,rn),m(Ta,rn,null),a(de,L1),a(de,ln),a(ln,G1),p(e,If,n),p(e,Yr,n),a(Yr,U1),p(e,Sf,n),p(e,Qr,n),a(Qr,M1),p(e,Nf,n),m(Ia,e,n),p(e,Cf,n),p(e,Zr,n),a(Zr,O1),p(e,Lf,n),p(e,Jr,n),a(Jr,z1),p(e,Gf,n),m(Sa,e,n),p(e,Uf,n),p(e,Kr,n),a(Kr,V1),p(e,Mf,n),m(Na,e,n),p(e,Of,n),p(e,el,n),a(el,H1),p(e,zf,n),m(Ca,e,n),p(e,Vf,n),p(e,X,n),a(X,B1),a(X,on),a(on,F1),a(X,q1),a(X,nn),a(nn,R1),a(X,W1),p(e,Hf,n),p(e,tl,n),a(tl,X1),p(e,Bf,n),p(e,al,n),a(al,Y1),p(e,Ff,n),p(e,ue,n),a(ue,st),a(st,pn),m(La,pn,null),a(ue,Q1),a(ue,hn),a(hn,Z1),p(e,qf,n),p(e,Ga,n),a(Ga,Ua),a(Ua,J1),a(Ga,K1),p(e,Rf,n),p(e,rt,n),a(rt,ew),a(rt,Ma),a(Ma,tw),a(rt,aw),p(e,Wf,n),p(e,sl,n),a(sl,fn),a(fn,sw),p(e,Xf,n),p(e,D,n),a(D,rw),a(D,cn),a(cn,lw),a(D,iw),a(D,dn),a(dn,ow),a(D,nw),a(D,un),a(un,pw),a(D,hw),p(e,Yf,n),p(e,rl,n),a(rl,fw),p(e,Qf,n),p(e,ll,n),a(ll,cw),p(e,Zf,n),p(e,lt,n),a(lt,mn),a(mn,Oa),a(Oa,vn),a(vn,dw),a(Oa,uw),a(Oa,il),a(il,mw),a(lt,vw),a(lt,za),a(za,Va),a(Va,wn),a(wn,ww),a(Va,_w),a(Va,ol),a(ol,bw),a(za,yw),a(za,Ha),a(Ha,_n),a(_n,gw),a(Ha,Ew),a(Ha,nl),a(nl,Pw),p(e,Jf,n),p(e,pl,n),a(pl,kw),p(e,Kf,n),p(e,it,n),a(it,$w),a(it,bn),a(bn,Aw),a(it,xw),p(e,ec,n),p(e,hl,n),a(hl,jw),p(e,tc,n),m(Ba,e,n),p(e,ac,n),p(e,P,n),a(P,Dw),a(P,yn),a(yn,Tw),a(P,Iw),a(P,gn),a(gn,Sw),a(P,Nw),a(P,En),a(En,Cw),a(P,Lw),a(P,Pn),a(Pn,Gw),a(P,Uw),a(P,kn),a(kn,Mw),p(e,sc,n),p(e,me,n),a(me,ot),a(ot,$n),m(Fa,$n,null),a(me,Ow),a(me,An),a(An,zw),p(e,rc,n),p(e,ve,n),a(ve,nt),a(nt,xn),m(qa,xn,null),a(ve,Vw),a(ve,jn),a(jn,Hw),p(e,lc,n),p(e,fl,n),a(fl,Bw),p(e,ic,n),p(e,Ra,n),a(Ra,Fw),a(Ra,cl),a(cl,qw),p(e,oc,n),p(e,we,n),a(we,pt),a(pt,Dn),m(Wa,Dn,null),a(we,Rw),a(we,Tn),a(Tn,Ww),p(e,nc,n),p(e,dl,n),a(dl,Xw),p(e,pc,n),p(e,Y,n),a(Y,Xa),a(Xa,In),a(In,Sn),a(Sn,Yw),a(Xa,Qw),a(Xa,Ya),a(Ya,Zw),a(Ya,Nn),a(Nn,Jw),a(Ya,Kw),a(Y,e_),a(Y,Qa),a(Qa,Cn),a(Cn,Ln),a(Ln,t_),a(Qa,a_),a(Qa,Za),a(Za,s_),a(Za,Gn),a(Gn,r_),a(Za,l_),a(Y,i_),a(Y,Ja),a(Ja,Un),a(Un,Mn),a(Mn,o_),a(Ja,n_),a(Ja,Ka),a(Ka,p_),a(Ka,On),a(On,h_),a(Ka,f_),p(e,hc,n),p(e,ul,n),a(ul,c_),p(e,fc,n),p(e,es,n),a(es,d_),a(es,ts),a(ts,u_),p(e,cc,n),p(e,_e,n),a(_e,ht),a(ht,zn),m(as,zn,null),a(_e,m_),a(_e,Vn),a(Vn,v_),p(e,dc,n),p(e,ml,n),a(ml,w_),p(e,uc,n),p(e,E,n),a(E,Hn),a(Hn,__),a(E,b_),a(E,Bn),a(Bn,y_),a(E,g_),a(E,Fn),a(Fn,E_),a(E,P_),a(E,qn),a(qn,k_),a(E,$_),a(E,Rn),a(Rn,A_),a(E,x_),a(E,Wn),a(Wn,j_),p(e,mc,n),p(e,vl,n),a(vl,D_),p(e,vc,n),p(e,wl,n),a(wl,T_),p(e,wc,n),p(e,_l,n),a(_l,I_),p(e,_c,n),p(e,be,n),a(be,ft),a(ft,Xn),m(ss,Xn,null),a(be,S_),a(be,Yn),a(Yn,N_),p(e,bc,n),p(e,ct,n),a(ct,Qn),a(Qn,C_),a(ct,L_),a(ct,Zn),a(Zn,G_),p(e,yc,n),p(e,ye,n),a(ye,dt),a(dt,Jn),m(rs,Jn,null),a(ye,U_),a(ye,Kn),a(Kn,M_),p(e,gc,n),p(e,Q,n),a(Q,ep),a(ep,O_),a(Q,z_),a(Q,bl),a(bl,V_),a(bl,ls),a(ls,H_),a(Q,B_),a(Q,tp),a(tp,F_),p(e,Ec,n),p(e,ge,n),a(ge,ut),a(ut,ap),m(is,ap,null),a(ge,q_),a(ge,sp),a(sp,R_),p(e,Pc,n),p(e,yl,n),a(yl,rp),a(rp,W_),p(e,kc,n),p(e,Ee,n),a(Ee,mt),a(mt,lp),m(os,lp,null),a(Ee,X_),a(Ee,ip),a(ip,Y_),p(e,$c,n),p(e,gl,n),a(gl,op),a(op,Q_),p(e,Ac,n),p(e,El,n),a(El,Z_),p(e,xc,n),p(e,Pe,n),a(Pe,vt),a(vt,np),m(ns,np,null),a(Pe,J_),a(Pe,pp),a(pp,K_),p(e,jc,n),p(e,Pl,n),a(Pl,eb),p(e,Dc,n),p(e,ke,n),a(ke,wt),a(wt,hp),m(ps,hp,null),a(ke,tb),a(ke,fp),a(fp,ab),p(e,Tc,n),p(e,kl,n),a(kl,sb),p(e,Ic,n),p(e,$e,n),a($e,_t),a(_t,cp),m(hs,cp,null),a($e,rb),a($e,bt),a(bt,dp),a(dp,lb),a(bt,ib),a(bt,up),a(up,ob),a(bt,nb),p(e,Sc,n),p(e,$l,n),a($l,pb),p(e,Nc,n),p(e,Ae,n),a(Ae,yt),a(yt,mp),m(fs,mp,null),a(Ae,hb),a(Ae,vp),a(vp,fb),p(e,Cc,n),p(e,Al,n),a(Al,cb),p(e,Lc,n),p(e,T,n),a(T,cs),a(cs,db),a(cs,wp),a(wp,ub),a(cs,mb),a(T,vb),a(T,ds),a(ds,wb),a(ds,_p),a(_p,_b),a(ds,bb),a(T,yb),a(T,us),a(us,gb),a(us,bp),a(bp,Eb),a(us,Pb),a(T,kb),a(T,yp),a(yp,$b),p(e,Gc,n),p(e,xl,n),a(xl,Ab),p(e,Uc,n),p(e,jl,n),a(jl,Dl),p(e,Mc,n),p(e,gt,n),a(gt,xb),a(gt,ms),a(ms,jb),a(gt,Db),p(e,Oc,n),p(e,Tl,n),a(Tl,Tb),p(e,zc,n),p(e,xe,n),a(xe,Et),a(Et,gp),m(vs,gp,null),a(xe,Ib),a(xe,Ep),a(Ep,Sb),p(e,Vc,n),p(e,Il,n),a(Il,Nb),p(e,Hc,n),p(e,Sl,n),a(Sl,Cb),p(e,Bc,n),p(e,I,n),a(I,Pp),a(Pp,Lb),a(I,Gb),a(I,kp),a(kp,Ub),a(I,Mb),a(I,ws),a(ws,Ob),a(ws,$p),a($p,zb),a(ws,Vb),a(I,Hb),a(I,Ap),a(Ap,Bb),p(e,Fc,n),p(e,Nl,n),a(Nl,Fb),p(e,qc,n),p(e,Pt,n),a(Pt,qb),a(Pt,xp),a(xp,Rb),a(Pt,Wb),p(e,Rc,n),p(e,Cl,n),a(Cl,Xb),p(e,Wc,n),m(_s,e,n),p(e,Xc,n),p(e,kt,n),a(kt,Yb),a(kt,jp),a(jp,Qb),a(kt,Zb),p(e,Yc,n),p(e,$t,n),a($t,Dp),a(Dp,U),a(U,Ll),a(Ll,Jb),a(U,Kb),a(U,Gl),a(Gl,ey),a(U,ty),a(U,Ul),a(Ul,ay),a(U,sy),a(U,Ml),a(Ml,ry),a($t,ly),a($t,M),a(M,O),a(O,Ol),a(Ol,iy),a(O,oy),a(O,zl),a(zl,ny),a(O,py),a(O,Vl),a(Vl,hy),a(O,fy),a(O,Hl),a(Hl,cy),a(M,dy),a(M,z),a(z,Bl),a(Bl,uy),a(z,my),a(z,Fl),a(Fl,vy),a(z,wy),a(z,ql),a(ql,_y),a(z,by),a(z,Rl),a(Rl,yy),a(M,gy),a(M,V),a(V,Wl),a(Wl,Ey),a(V,Py),a(V,Xl),a(Xl,ky),a(V,$y),a(V,Yl),a(Yl,Ay),a(V,xy),a(V,Ql),a(Ql,jy),a(M,Dy),a(M,H),a(H,Zl),a(Zl,Ty),a(H,Iy),a(H,Jl),a(Jl,Sy),a(H,Ny),a(H,Kl),a(Kl,Cy),a(H,Ly),a(H,ei),a(ei,Gy),p(e,Qc,n),p(e,At,n),a(At,Uy),a(At,Tp),a(Tp,My),a(At,Oy),p(e,Zc,n),p(e,ti,n),a(ti,zy),p(e,Jc,n),p(e,ai,n),a(ai,Vy),p(e,Kc,n),p(e,xt,n),a(xt,Hy),a(xt,bs),a(bs,By),a(xt,Fy),p(e,ed,n),p(e,si,n),a(si,qy),p(e,td,n),p(e,jt,n),a(jt,ri),a(ri,Ry),a(ri,ys),a(ys,Wy),a(jt,Xy),a(jt,li),a(li,Yy),a(li,gs),a(gs,Qy),p(e,ad,n),p(e,Z,n),a(Z,Zy),a(Z,Es),a(Es,Jy),a(Z,Ky),a(Z,Ps),a(Ps,e2),a(Z,t2),p(e,sd,n),p(e,je,n),a(je,Dt),a(Dt,Ip),m(ks,Ip,null),a(je,a2),a(je,Sp),a(Sp,s2),p(e,rd,n),p(e,J,n),a(J,r2),a(J,Np),a(Np,l2),a(J,i2),a(J,$s),a($s,o2),a(J,n2),p(e,ld,n),p(e,ii,n),a(ii,p2),p(e,id,n),p(e,De,n),a(De,Tt),a(Tt,Cp),m(As,Cp,null),a(De,h2),a(De,Lp),a(Lp,f2),p(e,od,n),p(e,oi,n),a(oi,c2),p(e,nd,n),p(e,ni,n),a(ni,d2),p(e,pd,n),p(e,pi,n),a(pi,u2),p(e,hd,n),p(e,It,n),a(It,m2),a(It,Gp),a(Gp,v2),a(It,w2),p(e,fd,n),p(e,Te,n),a(Te,St),a(St,Up),m(xs,Up,null),a(Te,_2),a(Te,Mp),a(Mp,b2),p(e,cd,n),p(e,K,n),a(K,y2),a(K,Op),a(Op,g2),a(K,E2),a(K,zp),a(zp,P2),a(K,k2),p(e,dd,n),p(e,hi,n),a(hi,$2),p(e,ud,n),p(e,fi,n),a(fi,A2),p(e,md,n),p(e,ci,n),a(ci,x2),p(e,vd,n),p(e,di,n),a(di,j2),p(e,wd,n),p(e,Nt,n),a(Nt,D2),a(Nt,Vp),a(Vp,T2),a(Nt,I2),p(e,_d,n),p(e,ui,n),a(ui,S2),p(e,bd,n),m(js,e,n),p(e,yd,n),p(e,Ds,n),a(Ds,N2),a(Ds,Hp),a(Hp,C2),p(e,gd,n),p(e,mi,n),a(mi,L2),p(e,Ed,n),m(Ts,e,n),p(e,Pd,n),p(e,vi,n),a(vi,G2),p(e,kd,n),p(e,ee,n),a(ee,U2),a(ee,Is),a(Is,M2),a(ee,O2),a(ee,Ss),a(Ss,z2),a(ee,V2),p(e,$d,n),p(e,Ie,n),a(Ie,Ct),a(Ct,Bp),m(Ns,Bp,null),a(Ie,H2),a(Ie,Fp),a(Fp,B2),p(e,Ad,n),p(e,Lt,n),a(Lt,F2),a(Lt,wi),a(wi,q2),a(Lt,R2),p(e,xd,n),p(e,Gt,n),a(Gt,W2),a(Gt,qp),a(qp,X2),a(Gt,Y2),p(e,jd,n),p(e,Se,n),a(Se,Ut),a(Ut,Rp),m(Cs,Rp,null),a(Se,Q2),a(Se,Wp),a(Wp,Z2),p(e,Dd,n),p(e,_i,n),a(_i,J2),p(e,Td,n),p(e,bi,n),a(bi,K2),p(e,Id,n),m(Ls,e,n),p(e,Sd,n),p(e,yi,n),a(yi,eg),p(e,Nd,n),p(e,Mt,n),a(Mt,tg),a(Mt,Gs),a(Gs,ag),a(Mt,sg),p(e,Cd,n),p(e,gi,n),a(gi,rg),p(e,Ld,n),p(e,S,n),a(S,lg),a(S,Xp),a(Xp,ig),a(S,og),a(S,Yp),a(Yp,ng),a(S,pg),a(S,Qp),a(Qp,hg),a(S,fg),p(e,Gd,n),p(e,te,n),a(te,cg),a(te,Zp),a(Zp,dg),a(te,ug),a(te,Jp),a(Jp,mg),a(te,vg),p(e,Ud,n),p(e,Ot,n),a(Ot,wg),a(Ot,Kp),a(Kp,_g),a(Ot,bg),p(e,Md,n),p(e,ae,n),a(ae,yg),a(ae,Us),a(Us,gg),a(ae,Eg),a(ae,Ms),a(Ms,Pg),a(ae,kg),p(e,Od,n),p(e,Ne,n),a(Ne,zt),a(zt,eh),m(Os,eh,null),a(Ne,$g),a(Ne,th),a(th,Ag),p(e,zd,n),p(e,se,n),a(se,xg),a(se,zs),a(zs,jg),a(se,Dg),a(se,Vs),a(Vs,Tg),a(se,Ig),p(e,Vd,n),p(e,Vt,n),a(Vt,Sg),a(Vt,ah),a(ah,Ng),a(Vt,Cg),p(e,Hd,n),p(e,Ei,n),a(Ei,Lg),p(e,Bd,n),p(e,Ce,n),a(Ce,Ht),a(Ht,sh),m(Hs,sh,null),a(Ce,Gg),a(Ce,rh),a(rh,Ug),p(e,Fd,n),p(e,Pi,n),a(Pi,Mg),p(e,qd,n),p(e,N,n),a(N,Og),a(N,Bs),a(Bs,zg),a(N,Vg),a(N,lh),a(lh,Hg),a(N,Bg),a(N,ih),a(ih,Fg),a(N,qg),p(e,Rd,n),p(e,ki,n),a(ki,Rg),p(e,Wd,n),m(Fs,e,n),p(e,Xd,n),p(e,Bt,n),a(Bt,Wg),a(Bt,oh),a(oh,Xg),a(Bt,Yg),p(e,Yd,n),p(e,Le,n),a(Le,Ft),a(Ft,nh),m(qs,nh,null),a(Le,Qg),a(Le,ph),a(ph,Zg),p(e,Qd,n),p(e,$i,n),a($i,Jg),p(e,Zd,n),p(e,re,n),a(re,Kg),a(re,Rs),a(Rs,e3),a(re,t3),a(re,Ws),a(Ws,a3),a(re,s3),p(e,Jd,n),p(e,Xs,n),a(Xs,Ys),a(Ys,r3),a(Xs,l3),p(e,Kd,n),p(e,qt,n),a(qt,i3),a(qt,Qs),a(Qs,o3),a(qt,n3),p(e,eu,n),p(e,C,n),a(C,p3),a(C,Ai),a(Ai,h3),a(C,f3),a(C,Zs),a(Zs,c3),a(C,d3),a(C,Js),a(Js,u3),a(C,m3),p(e,tu,n),p(e,Ge,n),a(Ge,Rt),a(Rt,hh),m(Ks,hh,null),a(Ge,v3),a(Ge,fh),a(fh,w3),p(e,au,n),p(e,Ue,n),a(Ue,ch),a(ch,_3),a(Ue,b3),a(Ue,dh),a(dh,y3),a(Ue,g3),p(e,su,n),p(e,Wt,n),a(Wt,uh),a(uh,E3),a(Wt,P3),a(Wt,mh),a(mh,k3),p(e,ru,n),p(e,xi,n),a(xi,$3),p(e,lu,n),p(e,er,n),a(er,tr),a(tr,A3),a(er,x3),p(e,iu,n),p(e,Xt,n),a(Xt,vh),a(vh,j3),a(Xt,D3),a(Xt,ji),a(ji,T3),a(ji,ar),a(ar,wh),a(wh,I3),a(ar,S3),a(ar,sr),a(sr,N3),a(sr,_h),a(_h,C3),a(sr,L3),p(e,ou,n),p(e,rr,n),a(rr,lr),a(lr,G3),a(rr,U3),p(e,nu,n),p(e,Di,n),a(Di,M3),p(e,pu,n),p(e,$,n),a($,bh),a(bh,O3),a($,z3),a($,yh),a(yh,V3),a($,H3),a($,ir),a(ir,B3),a(ir,gh),a(gh,F3),a(ir,q3),a($,R3),a($,Ti),a(Ti,W3),a(Ti,Eh),a(Eh,X3),a($,Y3),a($,Ph),a(Ph,Q3),p(e,hu,n),p(e,Ii,n),a(Ii,Z3),p(e,fu,n),p(e,Yt,n),a(Yt,J3),a(Yt,or),a(or,K3),a(Yt,eE),p(e,cu,n),p(e,Si,n),a(Si,tE),p(e,du,n),p(e,Ni,n),a(Ni,aE),p(e,uu,n),p(e,Ci,n),a(Ci,sE),p(e,mu,n),p(e,Qt,n),a(Qt,rE),a(Qt,nr),a(nr,lE),a(Qt,iE),p(e,vu,n),p(e,Li,n),a(Li,oE),p(e,wu,n),p(e,Zt,n),a(Zt,kh),a(kh,Me),a(Me,Gi),a(Gi,nE),a(Me,pE),a(Me,$h),a($h,hE),a(Me,fE),a(Me,Ui),a(Ui,cE),a(Zt,dE),a(Zt,Oe),a(Oe,ze),a(ze,Mi),a(Mi,uE),a(ze,mE),a(ze,Ah),a(Ah,vE),a(ze,wE),a(ze,Oi),a(Oi,_E),a(Oe,bE),a(Oe,Ve),a(Ve,zi),a(zi,yE),a(Ve,gE),a(Ve,xh),a(xh,EE),a(Ve,PE),a(Ve,Vi),a(Vi,kE),a(Oe,$E),a(Oe,He),a(He,Hi),a(Hi,AE),a(He,xE),a(He,jh),a(jh,jE),a(He,DE),a(He,Bi),a(Bi,TE),p(e,_u,n),p(e,Fi,n),a(Fi,IE),p(e,bu,n),p(e,qi,n),a(qi,SE),p(e,yu,n),p(e,Ri,n),a(Ri,NE),p(e,gu,n),p(e,Wi,n),a(Wi,CE),p(e,Eu,n),p(e,pr,n),a(pr,Dh),a(Dh,LE),a(pr,GE),p(e,Pu,n),m(hr,e,n),p(e,ku,n),p(e,k,n),a(k,UE),a(k,Th),a(Th,ME),a(k,OE),a(k,Ih),a(Ih,zE),a(k,VE),a(k,Sh),a(Sh,HE),a(k,BE),a(k,Nh),a(Nh,FE),a(k,qE),a(k,Ch),a(Ch,RE),p(e,$u,n),p(e,Be,n),a(Be,Jt),a(Jt,Lh),m(fr,Lh,null),a(Be,WE),a(Be,Gh),a(Gh,XE),p(e,Au,n),p(e,Xi,n),a(Xi,YE),p(e,xu,n),p(e,Kt,n),a(Kt,Yi),a(Yi,Uh),a(Uh,QE),a(Yi,ZE),a(Kt,JE),a(Kt,Qi),a(Qi,Mh),a(Mh,KE),a(Qi,e6),p(e,ju,n),p(e,Fe,n),a(Fe,ea),a(ea,Oh),m(cr,Oh,null),a(Fe,t6),a(Fe,zh),a(zh,a6),p(e,Du,n),p(e,qe,n),a(qe,s6),a(qe,Vh),a(Vh,r6),a(qe,l6),a(qe,dr),a(dr,i6),p(e,Tu,n),p(e,Re,n),a(Re,ta),a(ta,Hh),m(ur,Hh,null),a(Re,o6),a(Re,Bh),a(Bh,n6),p(e,Iu,n),p(e,We,n),a(We,aa),a(aa,Fh),m(mr,Fh,null),a(We,p6),a(We,qh),a(qh,h6),p(e,Su,n),p(e,Zi,n),a(Zi,f6),p(e,Nu,n),p(e,Ji,n),a(Ji,c6),p(e,Cu,n),p(e,Ki,n),a(Ki,d6),p(e,Lu,n),p(e,eo,n),a(eo,to),p(e,Gu,n),p(e,sa,n),a(sa,u6),a(sa,vr),a(vr,m6),a(sa,v6),p(e,Uu,n),p(e,ao,n),a(ao,w6),p(e,Mu,n),p(e,so,n),a(so,_6),p(e,Ou,n),p(e,ro,n),a(ro,b6),p(e,zu,n),p(e,lo,n),a(lo,y6),p(e,Vu,n),p(e,le,n),a(le,Rh),a(Rh,wr),a(wr,g6),a(le,E6),a(le,Wh),a(Wh,_r),a(_r,P6),a(le,k6),a(le,Xh),a(Xh,br),a(br,$6),p(e,Hu,n),p(e,g,n),a(g,A6),a(g,yr),a(yr,x6),a(g,j6),a(g,gr),a(gr,D6),a(g,T6),a(g,Er),a(Er,I6),a(g,S6),a(g,Pr),a(Pr,N6),a(g,C6),a(g,kr),a(kr,L6),a(g,G6),a(g,io),a(io,U6),a(g,M6),p(e,Bu,n),p(e,Xe,n),a(Xe,ra),a(ra,Yh),m($r,Yh,null),a(Xe,O6),a(Xe,Qh),a(Qh,z6),p(e,Fu,n),p(e,la,n),a(la,V6),a(la,Ar),a(Ar,H6),a(la,B6),p(e,qu,n),p(e,ia,n),a(ia,F6),a(ia,Zh),a(Zh,q6),a(ia,R6),p(e,Ru,n),p(e,oo,n),a(oo,W6),p(e,Wu,n),p(e,ie,n),a(ie,X6),a(ie,xr),a(xr,Y6),a(ie,Q6),a(ie,jr),a(jr,Z6),a(ie,J6),p(e,Xu,n),p(e,no,n),a(no,K6),p(e,Yu,n),p(e,Ye,n),a(Ye,oa),a(oa,Jh),m(Dr,Jh,null),a(Ye,e0),a(Ye,Kh),a(Kh,t0),p(e,Qu,n),p(e,po,n),a(po,a0),p(e,Zu,n),p(e,ho,n),a(ho,s0),Ju=!0},p:qA,i(e){Ju||(v(ya.$$.fragment,e),v(ga.$$.fragment,e),v(Ea.$$.fragment,e),v(ka.$$.fragment,e),v(Aa.$$.fragment,e),v(xa.$$.fragment,e),v(Ta.$$.fragment,e),v(Ia.$$.fragment,e),v(Sa.$$.fragment,e),v(Na.$$.fragment,e),v(Ca.$$.fragment,e),v(La.$$.fragment,e),v(Ba.$$.fragment,e),v(Fa.$$.fragment,e),v(qa.$$.fragment,e),v(Wa.$$.fragment,e),v(as.$$.fragment,e),v(ss.$$.fragment,e),v(rs.$$.fragment,e),v(is.$$.fragment,e),v(os.$$.fragment,e),v(ns.$$.fragment,e),v(ps.$$.fragment,e),v(hs.$$.fragment,e),v(fs.$$.fragment,e),v(vs.$$.fragment,e),v(_s.$$.fragment,e),v(ks.$$.fragment,e),v(As.$$.fragment,e),v(xs.$$.fragment,e),v(js.$$.fragment,e),v(Ts.$$.fragment,e),v(Ns.$$.fragment,e),v(Cs.$$.fragment,e),v(Ls.$$.fragment,e),v(Os.$$.fragment,e),v(Hs.$$.fragment,e),v(Fs.$$.fragment,e),v(qs.$$.fragment,e),v(Ks.$$.fragment,e),v(hr.$$.fragment,e),v(fr.$$.fragment,e),v(cr.$$.fragment,e),v(ur.$$.fragment,e),v(mr.$$.fragment,e),v($r.$$.fragment,e),v(Dr.$$.fragment,e),Ju=!0)},o(e){w(ya.$$.fragment,e),w(ga.$$.fragment,e),w(Ea.$$.fragment,e),w(ka.$$.fragment,e),w(Aa.$$.fragment,e),w(xa.$$.fragment,e),w(Ta.$$.fragment,e),w(Ia.$$.fragment,e),w(Sa.$$.fragment,e),w(Na.$$.fragment,e),w(Ca.$$.fragment,e),w(La.$$.fragment,e),w(Ba.$$.fragment,e),w(Fa.$$.fragment,e),w(qa.$$.fragment,e),w(Wa.$$.fragment,e),w(as.$$.fragment,e),w(ss.$$.fragment,e),w(rs.$$.fragment,e),w(is.$$.fragment,e),w(os.$$.fragment,e),w(ns.$$.fragment,e),w(ps.$$.fragment,e),w(hs.$$.fragment,e),w(fs.$$.fragment,e),w(vs.$$.fragment,e),w(_s.$$.fragment,e),w(ks.$$.fragment,e),w(As.$$.fragment,e),w(xs.$$.fragment,e),w(js.$$.fragment,e),w(Ts.$$.fragment,e),w(Ns.$$.fragment,e),w(Cs.$$.fragment,e),w(Ls.$$.fragment,e),w(Os.$$.fragment,e),w(Hs.$$.fragment,e),w(Fs.$$.fragment,e),w(qs.$$.fragment,e),w(Ks.$$.fragment,e),w(hr.$$.fragment,e),w(fr.$$.fragment,e),w(cr.$$.fragment,e),w(ur.$$.fragment,e),w(mr.$$.fragment,e),w($r.$$.fragment,e),w(Dr.$$.fragment,e),Ju=!1},d(e){t(R),e&&t(Tr),e&&t(j),_(ya),e&&t(sf),e&&t(Ir),e&&t(rf),e&&t(ne),_(ga),e&&t(lf),e&&t(Sr),e&&t(of),e&&t(pe),_(Ea),e&&t(nf),e&&t(Nr),e&&t(pf),e&&t(Cr),e&&t(hf),e&&t(Gr),e&&t(ff),e&&t(Je),e&&t(cf),e&&t(he),_(ka),e&&t(df),e&&t(Ur),e&&t(uf),e&&t(W),e&&t(mf),e&&t(Or),e&&t(vf),e&&t(y),e&&t(wf),e&&t(fe),_(Aa),e&&t(_f),e&&t(ce),_(xa),e&&t(bf),e&&t(zr),e&&t(yf),e&&t(ja),e&&t(gf),e&&t(Vr),e&&t(Ef),e&&t(Hr),e&&t(Pf),e&&t(Br),e&&t(kf),e&&t(Fr),e&&t($f),e&&t(qr),e&&t(Af),e&&t(Rr),e&&t(xf),e&&t(Da),e&&t(jf),e&&t(Wr),e&&t(Df),e&&t(Xr),e&&t(Tf),e&&t(de),_(Ta),e&&t(If),e&&t(Yr),e&&t(Sf),e&&t(Qr),e&&t(Nf),_(Ia,e),e&&t(Cf),e&&t(Zr),e&&t(Lf),e&&t(Jr),e&&t(Gf),_(Sa,e),e&&t(Uf),e&&t(Kr),e&&t(Mf),_(Na,e),e&&t(Of),e&&t(el),e&&t(zf),_(Ca,e),e&&t(Vf),e&&t(X),e&&t(Hf),e&&t(tl),e&&t(Bf),e&&t(al),e&&t(Ff),e&&t(ue),_(La),e&&t(qf),e&&t(Ga),e&&t(Rf),e&&t(rt),e&&t(Wf),e&&t(sl),e&&t(Xf),e&&t(D),e&&t(Yf),e&&t(rl),e&&t(Qf),e&&t(ll),e&&t(Zf),e&&t(lt),e&&t(Jf),e&&t(pl),e&&t(Kf),e&&t(it),e&&t(ec),e&&t(hl),e&&t(tc),_(Ba,e),e&&t(ac),e&&t(P),e&&t(sc),e&&t(me),_(Fa),e&&t(rc),e&&t(ve),_(qa),e&&t(lc),e&&t(fl),e&&t(ic),e&&t(Ra),e&&t(oc),e&&t(we),_(Wa),e&&t(nc),e&&t(dl),e&&t(pc),e&&t(Y),e&&t(hc),e&&t(ul),e&&t(fc),e&&t(es),e&&t(cc),e&&t(_e),_(as),e&&t(dc),e&&t(ml),e&&t(uc),e&&t(E),e&&t(mc),e&&t(vl),e&&t(vc),e&&t(wl),e&&t(wc),e&&t(_l),e&&t(_c),e&&t(be),_(ss),e&&t(bc),e&&t(ct),e&&t(yc),e&&t(ye),_(rs),e&&t(gc),e&&t(Q),e&&t(Ec),e&&t(ge),_(is),e&&t(Pc),e&&t(yl),e&&t(kc),e&&t(Ee),_(os),e&&t($c),e&&t(gl),e&&t(Ac),e&&t(El),e&&t(xc),e&&t(Pe),_(ns),e&&t(jc),e&&t(Pl),e&&t(Dc),e&&t(ke),_(ps),e&&t(Tc),e&&t(kl),e&&t(Ic),e&&t($e),_(hs),e&&t(Sc),e&&t($l),e&&t(Nc),e&&t(Ae),_(fs),e&&t(Cc),e&&t(Al),e&&t(Lc),e&&t(T),e&&t(Gc),e&&t(xl),e&&t(Uc),e&&t(jl),e&&t(Mc),e&&t(gt),e&&t(Oc),e&&t(Tl),e&&t(zc),e&&t(xe),_(vs),e&&t(Vc),e&&t(Il),e&&t(Hc),e&&t(Sl),e&&t(Bc),e&&t(I),e&&t(Fc),e&&t(Nl),e&&t(qc),e&&t(Pt),e&&t(Rc),e&&t(Cl),e&&t(Wc),_(_s,e),e&&t(Xc),e&&t(kt),e&&t(Yc),e&&t($t),e&&t(Qc),e&&t(At),e&&t(Zc),e&&t(ti),e&&t(Jc),e&&t(ai),e&&t(Kc),e&&t(xt),e&&t(ed),e&&t(si),e&&t(td),e&&t(jt),e&&t(ad),e&&t(Z),e&&t(sd),e&&t(je),_(ks),e&&t(rd),e&&t(J),e&&t(ld),e&&t(ii),e&&t(id),e&&t(De),_(As),e&&t(od),e&&t(oi),e&&t(nd),e&&t(ni),e&&t(pd),e&&t(pi),e&&t(hd),e&&t(It),e&&t(fd),e&&t(Te),_(xs),e&&t(cd),e&&t(K),e&&t(dd),e&&t(hi),e&&t(ud),e&&t(fi),e&&t(md),e&&t(ci),e&&t(vd),e&&t(di),e&&t(wd),e&&t(Nt),e&&t(_d),e&&t(ui),e&&t(bd),_(js,e),e&&t(yd),e&&t(Ds),e&&t(gd),e&&t(mi),e&&t(Ed),_(Ts,e),e&&t(Pd),e&&t(vi),e&&t(kd),e&&t(ee),e&&t($d),e&&t(Ie),_(Ns),e&&t(Ad),e&&t(Lt),e&&t(xd),e&&t(Gt),e&&t(jd),e&&t(Se),_(Cs),e&&t(Dd),e&&t(_i),e&&t(Td),e&&t(bi),e&&t(Id),_(Ls,e),e&&t(Sd),e&&t(yi),e&&t(Nd),e&&t(Mt),e&&t(Cd),e&&t(gi),e&&t(Ld),e&&t(S),e&&t(Gd),e&&t(te),e&&t(Ud),e&&t(Ot),e&&t(Md),e&&t(ae),e&&t(Od),e&&t(Ne),_(Os),e&&t(zd),e&&t(se),e&&t(Vd),e&&t(Vt),e&&t(Hd),e&&t(Ei),e&&t(Bd),e&&t(Ce),_(Hs),e&&t(Fd),e&&t(Pi),e&&t(qd),e&&t(N),e&&t(Rd),e&&t(ki),e&&t(Wd),_(Fs,e),e&&t(Xd),e&&t(Bt),e&&t(Yd),e&&t(Le),_(qs),e&&t(Qd),e&&t($i),e&&t(Zd),e&&t(re),e&&t(Jd),e&&t(Xs),e&&t(Kd),e&&t(qt),e&&t(eu),e&&t(C),e&&t(tu),e&&t(Ge),_(Ks),e&&t(au),e&&t(Ue),e&&t(su),e&&t(Wt),e&&t(ru),e&&t(xi),e&&t(lu),e&&t(er),e&&t(iu),e&&t(Xt),e&&t(ou),e&&t(rr),e&&t(nu),e&&t(Di),e&&t(pu),e&&t($),e&&t(hu),e&&t(Ii),e&&t(fu),e&&t(Yt),e&&t(cu),e&&t(Si),e&&t(du),e&&t(Ni),e&&t(uu),e&&t(Ci),e&&t(mu),e&&t(Qt),e&&t(vu),e&&t(Li),e&&t(wu),e&&t(Zt),e&&t(_u),e&&t(Fi),e&&t(bu),e&&t(qi),e&&t(yu),e&&t(Ri),e&&t(gu),e&&t(Wi),e&&t(Eu),e&&t(pr),e&&t(Pu),_(hr,e),e&&t(ku),e&&t(k),e&&t($u),e&&t(Be),_(fr),e&&t(Au),e&&t(Xi),e&&t(xu),e&&t(Kt),e&&t(ju),e&&t(Fe),_(cr),e&&t(Du),e&&t(qe),e&&t(Tu),e&&t(Re),_(ur),e&&t(Iu),e&&t(We),_(mr),e&&t(Su),e&&t(Zi),e&&t(Nu),e&&t(Ji),e&&t(Cu),e&&t(Ki),e&&t(Lu),e&&t(eo),e&&t(Gu),e&&t(sa),e&&t(Uu),e&&t(ao),e&&t(Mu),e&&t(so),e&&t(Ou),e&&t(ro),e&&t(zu),e&&t(lo),e&&t(Vu),e&&t(le),e&&t(Hu),e&&t(g),e&&t(Bu),e&&t(Xe),_($r),e&&t(Fu),e&&t(la),e&&t(qu),e&&t(ia),e&&t(Ru),e&&t(oo),e&&t(Wu),e&&t(ie),e&&t(Xu),e&&t(no),e&&t(Yu),e&&t(Ye),_(Dr),e&&t(Qu),e&&t(po),e&&t(Zu),e&&t(ho)}}}const WA={local:"performance-and-scalability-how-to-fit-a-bigger-model-and-train-it-faster",sections:[{local:"quick-notes",sections:[{local:"faster-training",title:"Faster Training"},{local:"bigger-models",title:"Bigger Models"}],title:"Quick notes"},{local:"hardware",sections:[{local:"power-and-cooling",title:"Power and Cooling"},{local:"multigpu-connectivity",title:"Multi-GPU Connectivity"},{local:"nvlink",title:"NVlink"}],title:"Hardware"},{local:"software",sections:[{local:"model-scalability",title:"Model Scalability"},{local:"anatomy-of-models-operations",title:"Anatomy of Model's Operations"},{local:"anatomy-of-models-memory",sections:[{local:"model-weights",title:"Model Weights"},{local:"optimizer-states",title:"Optimizer States"},{local:"gradients",title:"Gradients"},{local:"forward-activations",title:"Forward Activations"},{local:"temporary-memory",title:"Temporary Memory"},{local:"functionalityspecific-memory",title:"Functionality-specific memory"}],title:"Anatomy of Model's Memory"},{local:"forward-vs-backward-execution-speed",title:"`forward` vs `backward` Execution Speed"},{local:"floating-data-types",sections:[{local:"fp16",sections:[{local:"fp16-caching",title:"fp16 caching"},{local:"fp16-inference",title:"fp16 Inference"}],title:"fp16"},{local:"bf16",sections:[{local:"bf16-inference",title:"bf16 Inference"}],title:"bf16"},{local:"tf32",title:"tf32"}],title:"Floating Data Types"},{local:"gradient-accumulation",title:"Gradient Accumulation"},{local:"gradient-checkpointing",title:"Gradient Checkpointing"},{local:"batch-sizes",title:"Batch sizes"},{local:"dp-vs-ddp",title:"DP vs DDP"},{local:"dataloader",title:"DataLoader"},{local:"faster-optimizer",title:"Faster optimizer"},{local:"sparsity",sections:[{local:"mixture-of-experts",title:"Mixture of Experts"}],title:"Sparsity"},{local:"efficient-software-prebuilds",title:"Efficient Software Prebuilds"}],title:"Software"},{local:"contribute",title:"Contribute"}],title:"Performance and Scalability: How To Fit a Bigger Model and Train It Faster"};function XA(Tv,R,Tr){let{fw:j}=R;return Tv.$$set=G=>{"fw"in G&&Tr(0,j=G.fw)},[j]}class KA extends VA{constructor(R){super();HA(this,R,XA,RA,BA,{fw:0})}}export{KA as default,WA as metadata};
