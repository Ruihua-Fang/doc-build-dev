import{S as zu,i as qu,s as Fu,e as n,k as d,w as T,t as a,M as Cu,c as r,d as t,m as l,a as s,x as k,h as i,b as c,F as e,g as p,y as b,q as P,o as w,B as E}from"../../chunks/vendor-4833417e.js";import{T as Ct}from"../../chunks/Tip-fffd6df1.js";import{D as W}from"../../chunks/Docstring-7b52c3d4.js";import{C as Qn}from"../../chunks/CodeBlock-6a3d1b46.js";import{I as ge}from"../../chunks/IconCopyLink-4b81c553.js";import"../../chunks/CopyButton-dacfbfaf.js";function Au(V){let u,D,m,g,R;return{c(){u=n("p"),D=a("Although the recipe for forward pass needs to be defined within this function, one should call the "),m=n("code"),g=a("Module"),R=a(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(v){u=r(v,"P",{});var _=s(u);D=i(_,"Although the recipe for forward pass needs to be defined within this function, one should call the "),m=r(_,"CODE",{});var x=s(m);g=i(x,"Module"),x.forEach(t),R=i(_,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),_.forEach(t)},m(v,_){p(v,u,_),e(u,D),e(u,m),e(m,g),e(u,R)},d(v){v&&t(u)}}}function Nu(V){let u,D,m,g,R;return{c(){u=n("p"),D=a("Although the recipe for forward pass needs to be defined within this function, one should call the "),m=n("code"),g=a("Module"),R=a(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(v){u=r(v,"P",{});var _=s(u);D=i(_,"Although the recipe for forward pass needs to be defined within this function, one should call the "),m=r(_,"CODE",{});var x=s(m);g=i(x,"Module"),x.forEach(t),R=i(_,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),_.forEach(t)},m(v,_){p(v,u,_),e(u,D),e(u,m),e(m,g),e(u,R)},d(v){v&&t(u)}}}function Ou(V){let u,D,m,g,R;return{c(){u=n("p"),D=a("Although the recipe for forward pass needs to be defined within this function, one should call the "),m=n("code"),g=a("Module"),R=a(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(v){u=r(v,"P",{});var _=s(u);D=i(_,"Although the recipe for forward pass needs to be defined within this function, one should call the "),m=r(_,"CODE",{});var x=s(m);g=i(x,"Module"),x.forEach(t),R=i(_,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),_.forEach(t)},m(v,_){p(v,u,_),e(u,D),e(u,m),e(m,g),e(u,R)},d(v){v&&t(u)}}}function ju(V){let u,D,m,g,R,v,_,x,_e,Z,$,J,L,ee,ve,I,Te,he,S,Q,te,oe,q,C,ne,U,fe,re,H,ke,ue,z,be,O,Pe,we,j,Ee,Re,M,K,N,se;return{c(){u=n("p"),D=a("TF 2.0 models accepts two formats as inputs:"),m=d(),g=n("ul"),R=n("li"),v=a("having all inputs as keyword arguments (like PyTorch models), or"),_=d(),x=n("li"),_e=a("having all inputs as a list, tuple or dict in the first positional arguments."),Z=d(),$=n("p"),J=a("This second option is useful when using "),L=n("code"),ee=a("tf.keras.Model.fit"),ve=a(` method which currently requires having all the
tensors in the first argument of the model call function: `),I=n("code"),Te=a("model(inputs)"),he=a("."),S=d(),Q=n("p"),te=a(`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),oe=d(),q=n("ul"),C=n("li"),ne=a("a single Tensor with "),U=n("code"),fe=a("input_ids"),re=a(" only and nothing else: "),H=n("code"),ke=a("model(inputs_ids)"),ue=d(),z=n("li"),be=a(`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),O=n("code"),Pe=a("model([input_ids, attention_mask])"),we=a(" or "),j=n("code"),Ee=a("model([input_ids, attention_mask, token_type_ids])"),Re=d(),M=n("li"),K=a(`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),N=n("code"),se=a('model({"input_ids": input_ids, "token_type_ids": token_type_ids})')},l(h){u=r(h,"P",{});var y=s(u);D=i(y,"TF 2.0 models accepts two formats as inputs:"),y.forEach(t),m=l(h),g=r(h,"UL",{});var G=s(g);R=r(G,"LI",{});var Ie=s(R);v=i(Ie,"having all inputs as keyword arguments (like PyTorch models), or"),Ie.forEach(t),_=l(G),x=r(G,"LI",{});var Oe=s(x);_e=i(Oe,"having all inputs as a list, tuple or dict in the first positional arguments."),Oe.forEach(t),G.forEach(t),Z=l(h),$=r(h,"P",{});var F=s($);J=i(F,"This second option is useful when using "),L=r(F,"CODE",{});var me=s(L);ee=i(me,"tf.keras.Model.fit"),me.forEach(t),ve=i(F,` method which currently requires having all the
tensors in the first argument of the model call function: `),I=r(F,"CODE",{});var Se=s(I);Te=i(Se,"model(inputs)"),Se.forEach(t),he=i(F,"."),F.forEach(t),S=l(h),Q=r(h,"P",{});var ae=s(Q);te=i(ae,`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),ae.forEach(t),oe=l(h),q=r(h,"UL",{});var A=s(q);C=r(A,"LI",{});var Y=s(C);ne=i(Y,"a single Tensor with "),U=r(Y,"CODE",{});var He=s(U);fe=i(He,"input_ids"),He.forEach(t),re=i(Y," only and nothing else: "),H=r(Y,"CODE",{});var xe=s(H);ke=i(xe,"model(inputs_ids)"),xe.forEach(t),Y.forEach(t),ue=l(A),z=r(A,"LI",{});var X=s(z);be=i(X,`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),O=r(X,"CODE",{});var Be=s(O);Pe=i(Be,"model([input_ids, attention_mask])"),Be.forEach(t),we=i(X," or "),j=r(X,"CODE",{});var We=s(j);Ee=i(We,"model([input_ids, attention_mask, token_type_ids])"),We.forEach(t),X.forEach(t),Re=l(A),M=r(A,"LI",{});var B=s(M);K=i(B,`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),N=r(B,"CODE",{});var Ve=s(N);se=i(Ve,'model({"input_ids": input_ids, "token_type_ids": token_type_ids})'),Ve.forEach(t),B.forEach(t),A.forEach(t)},m(h,y){p(h,u,y),e(u,D),p(h,m,y),p(h,g,y),e(g,R),e(R,v),e(g,_),e(g,x),e(x,_e),p(h,Z,y),p(h,$,y),e($,J),e($,L),e(L,ee),e($,ve),e($,I),e(I,Te),e($,he),p(h,S,y),p(h,Q,y),e(Q,te),p(h,oe,y),p(h,q,y),e(q,C),e(C,ne),e(C,U),e(U,fe),e(C,re),e(C,H),e(H,ke),e(q,ue),e(q,z),e(z,be),e(z,O),e(O,Pe),e(z,we),e(z,j),e(j,Ee),e(q,Re),e(q,M),e(M,K),e(M,N),e(N,se)},d(h){h&&t(u),h&&t(m),h&&t(g),h&&t(Z),h&&t($),h&&t(S),h&&t(Q),h&&t(oe),h&&t(q)}}}function Mu(V){let u,D,m,g,R;return{c(){u=n("p"),D=a("Although the recipe for forward pass needs to be defined within this function, one should call the "),m=n("code"),g=a("Module"),R=a(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(v){u=r(v,"P",{});var _=s(u);D=i(_,"Although the recipe for forward pass needs to be defined within this function, one should call the "),m=r(_,"CODE",{});var x=s(m);g=i(x,"Module"),x.forEach(t),R=i(_,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),_.forEach(t)},m(v,_){p(v,u,_),e(u,D),e(u,m),e(m,g),e(u,R)},d(v){v&&t(u)}}}function Qu(V){let u,D,m,g,R,v,_,x,_e,Z,$,J,L,ee,ve,I,Te,he,S,Q,te,oe,q,C,ne,U,fe,re,H,ke,ue,z,be,O,Pe,we,j,Ee,Re,M,K,N,se;return{c(){u=n("p"),D=a("TF 2.0 models accepts two formats as inputs:"),m=d(),g=n("ul"),R=n("li"),v=a("having all inputs as keyword arguments (like PyTorch models), or"),_=d(),x=n("li"),_e=a("having all inputs as a list, tuple or dict in the first positional arguments."),Z=d(),$=n("p"),J=a("This second option is useful when using "),L=n("code"),ee=a("tf.keras.Model.fit"),ve=a(` method which currently requires having all the
tensors in the first argument of the model call function: `),I=n("code"),Te=a("model(inputs)"),he=a("."),S=d(),Q=n("p"),te=a(`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),oe=d(),q=n("ul"),C=n("li"),ne=a("a single Tensor with "),U=n("code"),fe=a("input_ids"),re=a(" only and nothing else: "),H=n("code"),ke=a("model(inputs_ids)"),ue=d(),z=n("li"),be=a(`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),O=n("code"),Pe=a("model([input_ids, attention_mask])"),we=a(" or "),j=n("code"),Ee=a("model([input_ids, attention_mask, token_type_ids])"),Re=d(),M=n("li"),K=a(`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),N=n("code"),se=a('model({"input_ids": input_ids, "token_type_ids": token_type_ids})')},l(h){u=r(h,"P",{});var y=s(u);D=i(y,"TF 2.0 models accepts two formats as inputs:"),y.forEach(t),m=l(h),g=r(h,"UL",{});var G=s(g);R=r(G,"LI",{});var Ie=s(R);v=i(Ie,"having all inputs as keyword arguments (like PyTorch models), or"),Ie.forEach(t),_=l(G),x=r(G,"LI",{});var Oe=s(x);_e=i(Oe,"having all inputs as a list, tuple or dict in the first positional arguments."),Oe.forEach(t),G.forEach(t),Z=l(h),$=r(h,"P",{});var F=s($);J=i(F,"This second option is useful when using "),L=r(F,"CODE",{});var me=s(L);ee=i(me,"tf.keras.Model.fit"),me.forEach(t),ve=i(F,` method which currently requires having all the
tensors in the first argument of the model call function: `),I=r(F,"CODE",{});var Se=s(I);Te=i(Se,"model(inputs)"),Se.forEach(t),he=i(F,"."),F.forEach(t),S=l(h),Q=r(h,"P",{});var ae=s(Q);te=i(ae,`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),ae.forEach(t),oe=l(h),q=r(h,"UL",{});var A=s(q);C=r(A,"LI",{});var Y=s(C);ne=i(Y,"a single Tensor with "),U=r(Y,"CODE",{});var He=s(U);fe=i(He,"input_ids"),He.forEach(t),re=i(Y," only and nothing else: "),H=r(Y,"CODE",{});var xe=s(H);ke=i(xe,"model(inputs_ids)"),xe.forEach(t),Y.forEach(t),ue=l(A),z=r(A,"LI",{});var X=s(z);be=i(X,`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),O=r(X,"CODE",{});var Be=s(O);Pe=i(Be,"model([input_ids, attention_mask])"),Be.forEach(t),we=i(X," or "),j=r(X,"CODE",{});var We=s(j);Ee=i(We,"model([input_ids, attention_mask, token_type_ids])"),We.forEach(t),X.forEach(t),Re=l(A),M=r(A,"LI",{});var B=s(M);K=i(B,`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),N=r(B,"CODE",{});var Ve=s(N);se=i(Ve,'model({"input_ids": input_ids, "token_type_ids": token_type_ids})'),Ve.forEach(t),B.forEach(t),A.forEach(t)},m(h,y){p(h,u,y),e(u,D),p(h,m,y),p(h,g,y),e(g,R),e(R,v),e(g,_),e(g,x),e(x,_e),p(h,Z,y),p(h,$,y),e($,J),e($,L),e(L,ee),e($,ve),e($,I),e(I,Te),e($,he),p(h,S,y),p(h,Q,y),e(Q,te),p(h,oe,y),p(h,q,y),e(q,C),e(C,ne),e(C,U),e(U,fe),e(C,re),e(C,H),e(H,ke),e(q,ue),e(q,z),e(z,be),e(z,O),e(O,Pe),e(z,we),e(z,j),e(j,Ee),e(q,Re),e(q,M),e(M,K),e(M,N),e(N,se)},d(h){h&&t(u),h&&t(m),h&&t(g),h&&t(Z),h&&t($),h&&t(S),h&&t(Q),h&&t(oe),h&&t(q)}}}function Lu(V){let u,D,m,g,R;return{c(){u=n("p"),D=a("Although the recipe for forward pass needs to be defined within this function, one should call the "),m=n("code"),g=a("Module"),R=a(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(v){u=r(v,"P",{});var _=s(u);D=i(_,"Although the recipe for forward pass needs to be defined within this function, one should call the "),m=r(_,"CODE",{});var x=s(m);g=i(x,"Module"),x.forEach(t),R=i(_,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),_.forEach(t)},m(v,_){p(v,u,_),e(u,D),e(u,m),e(m,g),e(u,R)},d(v){v&&t(u)}}}function Iu(V){let u,D,m,g,R,v,_,x,_e,Z,$,J,L,ee,ve,I,Te,he,S,Q,te,oe,q,C,ne,U,fe,re,H,ke,ue,z,be,O,Pe,we,j,Ee,Re,M,K,N,se;return{c(){u=n("p"),D=a("TF 2.0 models accepts two formats as inputs:"),m=d(),g=n("ul"),R=n("li"),v=a("having all inputs as keyword arguments (like PyTorch models), or"),_=d(),x=n("li"),_e=a("having all inputs as a list, tuple or dict in the first positional arguments."),Z=d(),$=n("p"),J=a("This second option is useful when using "),L=n("code"),ee=a("tf.keras.Model.fit"),ve=a(` method which currently requires having all the
tensors in the first argument of the model call function: `),I=n("code"),Te=a("model(inputs)"),he=a("."),S=d(),Q=n("p"),te=a(`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),oe=d(),q=n("ul"),C=n("li"),ne=a("a single Tensor with "),U=n("code"),fe=a("input_ids"),re=a(" only and nothing else: "),H=n("code"),ke=a("model(inputs_ids)"),ue=d(),z=n("li"),be=a(`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),O=n("code"),Pe=a("model([input_ids, attention_mask])"),we=a(" or "),j=n("code"),Ee=a("model([input_ids, attention_mask, token_type_ids])"),Re=d(),M=n("li"),K=a(`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),N=n("code"),se=a('model({"input_ids": input_ids, "token_type_ids": token_type_ids})')},l(h){u=r(h,"P",{});var y=s(u);D=i(y,"TF 2.0 models accepts two formats as inputs:"),y.forEach(t),m=l(h),g=r(h,"UL",{});var G=s(g);R=r(G,"LI",{});var Ie=s(R);v=i(Ie,"having all inputs as keyword arguments (like PyTorch models), or"),Ie.forEach(t),_=l(G),x=r(G,"LI",{});var Oe=s(x);_e=i(Oe,"having all inputs as a list, tuple or dict in the first positional arguments."),Oe.forEach(t),G.forEach(t),Z=l(h),$=r(h,"P",{});var F=s($);J=i(F,"This second option is useful when using "),L=r(F,"CODE",{});var me=s(L);ee=i(me,"tf.keras.Model.fit"),me.forEach(t),ve=i(F,` method which currently requires having all the
tensors in the first argument of the model call function: `),I=r(F,"CODE",{});var Se=s(I);Te=i(Se,"model(inputs)"),Se.forEach(t),he=i(F,"."),F.forEach(t),S=l(h),Q=r(h,"P",{});var ae=s(Q);te=i(ae,`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),ae.forEach(t),oe=l(h),q=r(h,"UL",{});var A=s(q);C=r(A,"LI",{});var Y=s(C);ne=i(Y,"a single Tensor with "),U=r(Y,"CODE",{});var He=s(U);fe=i(He,"input_ids"),He.forEach(t),re=i(Y," only and nothing else: "),H=r(Y,"CODE",{});var xe=s(H);ke=i(xe,"model(inputs_ids)"),xe.forEach(t),Y.forEach(t),ue=l(A),z=r(A,"LI",{});var X=s(z);be=i(X,`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),O=r(X,"CODE",{});var Be=s(O);Pe=i(Be,"model([input_ids, attention_mask])"),Be.forEach(t),we=i(X," or "),j=r(X,"CODE",{});var We=s(j);Ee=i(We,"model([input_ids, attention_mask, token_type_ids])"),We.forEach(t),X.forEach(t),Re=l(A),M=r(A,"LI",{});var B=s(M);K=i(B,`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),N=r(B,"CODE",{});var Ve=s(N);se=i(Ve,'model({"input_ids": input_ids, "token_type_ids": token_type_ids})'),Ve.forEach(t),B.forEach(t),A.forEach(t)},m(h,y){p(h,u,y),e(u,D),p(h,m,y),p(h,g,y),e(g,R),e(R,v),e(g,_),e(g,x),e(x,_e),p(h,Z,y),p(h,$,y),e($,J),e($,L),e(L,ee),e($,ve),e($,I),e(I,Te),e($,he),p(h,S,y),p(h,Q,y),e(Q,te),p(h,oe,y),p(h,q,y),e(q,C),e(C,ne),e(C,U),e(U,fe),e(C,re),e(C,H),e(H,ke),e(q,ue),e(q,z),e(z,be),e(z,O),e(O,Pe),e(z,we),e(z,j),e(j,Ee),e(q,Re),e(q,M),e(M,K),e(M,N),e(N,se)},d(h){h&&t(u),h&&t(m),h&&t(g),h&&t(Z),h&&t($),h&&t(S),h&&t(Q),h&&t(oe),h&&t(q)}}}function Su(V){let u,D,m,g,R;return{c(){u=n("p"),D=a("Although the recipe for forward pass needs to be defined within this function, one should call the "),m=n("code"),g=a("Module"),R=a(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(v){u=r(v,"P",{});var _=s(u);D=i(_,"Although the recipe for forward pass needs to be defined within this function, one should call the "),m=r(_,"CODE",{});var x=s(m);g=i(x,"Module"),x.forEach(t),R=i(_,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),_.forEach(t)},m(v,_){p(v,u,_),e(u,D),e(u,m),e(m,g),e(u,R)},d(v){v&&t(u)}}}function Hu(V){let u,D,m,g,R,v,_,x,_e,Z,$,J,L,ee,ve,I,Te,he,S,Q,te,oe,q,C,ne,U,fe,re,H,ke,ue,z,be,O,Pe,we,j,Ee,Re,M,K,N,se,h,y,G,Ie,Oe,F,me,Se,ae,A,Y,He,xe,X,Be,We,B,Ve,Ln,li,ci,In,pi,hi,Sn,fi,ui,mi,mo,gi,Hn,_i,vi,Vs,pt,At,Fr,go,Ti,Cr,ki,Us,je,_o,bi,Ar,Pi,wi,Nt,Bn,Ei,Ri,Wn,Di,$i,yi,vo,xi,Vn,zi,qi,Ks,ht,Ot,Nr,To,Fi,Or,Ci,Ys,Me,ko,Ai,bo,Ni,jr,Oi,ji,Mi,jt,Un,Qi,Li,Kn,Ii,Si,Hi,Po,Bi,Yn,Wi,Vi,Xs,ft,Mt,Mr,wo,Ui,Qr,Ki,Js,Qe,Eo,Yi,Lr,Xi,Ji,Qt,Xn,Gi,Zi,Jn,ed,td,od,Ro,nd,Gn,rd,sd,Gs,ut,Lt,Ir,Do,ad,Sr,id,Zs,Le,$o,dd,yo,ld,Hr,cd,pd,hd,It,Zn,fd,ud,er,md,gd,_d,xo,vd,tr,Td,kd,ea,mt,St,Br,zo,bd,Wr,Pd,ta,ie,qo,wd,Vr,Ed,Rd,et,or,Dd,$d,nr,yd,xd,rr,zd,qd,Fd,Fo,Cd,sr,Ad,Nd,Od,Ge,jd,Ur,Md,Qd,Kr,Ld,Id,Yr,Sd,Hd,Bd,Co,oa,gt,Ht,Xr,Ao,Wd,Jr,Vd,na,de,No,Ud,Oo,Kd,Gr,Yd,Xd,Jd,tt,ar,Gd,Zd,ir,el,tl,dr,ol,nl,rl,jo,sl,lr,al,il,dl,Ze,ll,Zr,cl,pl,es,hl,fl,ts,ul,ml,gl,os,_l,ra,_t,Bt,ns,Mo,vl,rs,Tl,sa,vt,Qo,kl,Lo,bl,cr,Pl,wl,aa,Tt,Io,El,So,Rl,pr,Dl,$l,ia,kt,Ho,yl,Bo,xl,hr,zl,ql,da,bt,Wt,ss,Wo,Fl,as,Cl,la,De,Vo,Al,is,Nl,Ol,Uo,jl,fr,Ml,Ql,Ll,Ko,Il,Yo,Sl,Hl,Bl,ze,Xo,Wl,Pt,Vl,ur,Ul,Kl,ds,Yl,Xl,Jl,Vt,Gl,ls,Zl,ec,Jo,ca,wt,Ut,cs,Go,tc,ps,oc,pa,$e,Zo,nc,hs,rc,sc,en,ac,mr,ic,dc,lc,tn,cc,on,pc,hc,fc,qe,nn,uc,Et,mc,gr,gc,_c,fs,vc,Tc,kc,Kt,bc,us,Pc,wc,rn,ha,Rt,Yt,ms,sn,Ec,gs,Rc,fa,ye,an,Dc,_s,$c,yc,dn,xc,_r,zc,qc,Fc,ln,Cc,cn,Ac,Nc,Oc,Fe,pn,jc,Dt,Mc,vr,Qc,Lc,vs,Ic,Sc,Hc,Xt,Bc,Ts,Wc,Vc,hn,ua,$t,Jt,ks,fn,Uc,bs,Kc,ma,le,un,Yc,Ps,Xc,Jc,mn,Gc,Tr,Zc,ep,tp,gn,op,_n,np,rp,sp,Gt,ap,Ce,vn,ip,yt,dp,kr,lp,cp,ws,pp,hp,fp,Zt,up,Es,mp,gp,Tn,ga,xt,eo,Rs,kn,_p,Ds,vp,_a,ce,bn,Tp,$s,kp,bp,Pn,Pp,br,wp,Ep,Rp,wn,Dp,En,$p,yp,xp,to,zp,Ae,Rn,qp,zt,Fp,Pr,Cp,Ap,ys,Np,Op,jp,oo,Mp,xs,Qp,Lp,Dn,va,qt,no,zs,$n,Ip,qs,Sp,Ta,pe,yn,Hp,Fs,Bp,Wp,xn,Vp,wr,Up,Kp,Yp,zn,Xp,qn,Jp,Gp,Zp,ro,eh,Ne,Fn,th,Ft,oh,Er,nh,rh,Cs,sh,ah,ih,so,dh,As,lh,ch,Cn,ka;return v=new ge({}),ee=new ge({}),h=new ge({}),me=new W({props:{name:"class transformers.DPRConfig",anchor:"transformers.DPRConfig",parameters:[{name:"vocab_size",val:" = 30522"},{name:"hidden_size",val:" = 768"},{name:"num_hidden_layers",val:" = 12"},{name:"num_attention_heads",val:" = 12"},{name:"intermediate_size",val:" = 3072"},{name:"hidden_act",val:" = 'gelu'"},{name:"hidden_dropout_prob",val:" = 0.1"},{name:"attention_probs_dropout_prob",val:" = 0.1"},{name:"max_position_embeddings",val:" = 512"},{name:"type_vocab_size",val:" = 2"},{name:"initializer_range",val:" = 0.02"},{name:"layer_norm_eps",val:" = 1e-12"},{name:"pad_token_id",val:" = 0"},{name:"position_embedding_type",val:" = 'absolute'"},{name:"projection_dim",val:": int = 0"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15943/src/transformers/models/dpr/configuration_dpr.py#L33",parametersDescription:[{anchor:"transformers.DPRConfig.vocab_size",description:`<strong>vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 30522) &#x2014;
Vocabulary size of the DPR model. Defines the different tokens that can be represented by the <em>inputs_ids</em>
passed to the forward method of <a href="/docs/transformers/pr_15943/en/model_doc/bert#transformers.BertModel">BertModel</a>.`,name:"vocab_size"},{anchor:"transformers.DPRConfig.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 768) &#x2014;
Dimensionality of the encoder layers and the pooler layer.`,name:"hidden_size"},{anchor:"transformers.DPRConfig.num_hidden_layers",description:`<strong>num_hidden_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of hidden layers in the Transformer encoder.`,name:"num_hidden_layers"},{anchor:"transformers.DPRConfig.num_attention_heads",description:`<strong>num_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of attention heads for each attention layer in the Transformer encoder.`,name:"num_attention_heads"},{anchor:"transformers.DPRConfig.intermediate_size",description:`<strong>intermediate_size</strong> (<code>int</code>, <em>optional</em>, defaults to 3072) &#x2014;
Dimensionality of the &#x201C;intermediate&#x201D; (i.e., feed-forward) layer in the Transformer encoder.`,name:"intermediate_size"},{anchor:"transformers.DPRConfig.hidden_act",description:`<strong>hidden_act</strong> (<code>str</code> or <code>function</code>, <em>optional</em>, defaults to <code>&quot;gelu&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the encoder and pooler. If string, <code>&quot;gelu&quot;</code>,
<code>&quot;relu&quot;</code>, <code>&quot;silu&quot;</code> and <code>&quot;gelu_new&quot;</code> are supported.`,name:"hidden_act"},{anchor:"transformers.DPRConfig.hidden_dropout_prob",description:`<strong>hidden_dropout_prob</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.`,name:"hidden_dropout_prob"},{anchor:"transformers.DPRConfig.attention_probs_dropout_prob",description:`<strong>attention_probs_dropout_prob</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout ratio for the attention probabilities.`,name:"attention_probs_dropout_prob"},{anchor:"transformers.DPRConfig.max_position_embeddings",description:`<strong>max_position_embeddings</strong> (<code>int</code>, <em>optional</em>, defaults to 512) &#x2014;
The maximum sequence length that this model might ever be used with. Typically set this to something large
just in case (e.g., 512 or 1024 or 2048).`,name:"max_position_embeddings"},{anchor:"transformers.DPRConfig.type_vocab_size",description:`<strong>type_vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 2) &#x2014;
The vocabulary size of the <em>token_type_ids</em> passed into <a href="/docs/transformers/pr_15943/en/model_doc/bert#transformers.BertModel">BertModel</a>.`,name:"type_vocab_size"},{anchor:"transformers.DPRConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.DPRConfig.layer_norm_eps",description:`<strong>layer_norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-12) &#x2014;
The epsilon used by the layer normalization layers.`,name:"layer_norm_eps"},{anchor:"transformers.DPRConfig.position_embedding_type",description:`<strong>position_embedding_type</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;absolute&quot;</code>) &#x2014;
Type of position embedding. Choose one of <code>&quot;absolute&quot;</code>, <code>&quot;relative_key&quot;</code>, <code>&quot;relative_key_query&quot;</code>. For
positional embeddings use <code>&quot;absolute&quot;</code>. For more information on <code>&quot;relative_key&quot;</code>, please refer to
<a href="https://arxiv.org/abs/1803.02155" rel="nofollow">Self-Attention with Relative Position Representations (Shaw et al.)</a>.
For more information on <code>&quot;relative_key_query&quot;</code>, please refer to <em>Method 4</em> in <a href="https://arxiv.org/abs/2009.13658" rel="nofollow">Improve Transformer Models
with Better Relative Position Embeddings (Huang et al.)</a>.`,name:"position_embedding_type"},{anchor:"transformers.DPRConfig.projection_dim",description:`<strong>projection_dim</strong> (<code>int</code>, <em>optional</em>, defaults to 0) &#x2014;
Dimension of the projection for the context and question encoders. If it is set to zero (default), then no
projection is done.`,name:"projection_dim"}]}}),go=new ge({}),_o=new W({props:{name:"class transformers.DPRContextEncoderTokenizer",anchor:"transformers.DPRContextEncoderTokenizer",parameters:[{name:"vocab_file",val:""},{name:"do_lower_case",val:" = True"},{name:"do_basic_tokenize",val:" = True"},{name:"never_split",val:" = None"},{name:"unk_token",val:" = '[UNK]'"},{name:"sep_token",val:" = '[SEP]'"},{name:"pad_token",val:" = '[PAD]'"},{name:"cls_token",val:" = '[CLS]'"},{name:"mask_token",val:" = '[MASK]'"},{name:"tokenize_chinese_chars",val:" = True"},{name:"strip_accents",val:" = None"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15943/src/transformers/models/dpr/tokenization_dpr.py#L90"}}),To=new ge({}),ko=new W({props:{name:"class transformers.DPRContextEncoderTokenizerFast",anchor:"transformers.DPRContextEncoderTokenizerFast",parameters:[{name:"vocab_file",val:" = None"},{name:"tokenizer_file",val:" = None"},{name:"do_lower_case",val:" = True"},{name:"unk_token",val:" = '[UNK]'"},{name:"sep_token",val:" = '[SEP]'"},{name:"pad_token",val:" = '[PAD]'"},{name:"cls_token",val:" = '[CLS]'"},{name:"mask_token",val:" = '[MASK]'"},{name:"tokenize_chinese_chars",val:" = True"},{name:"strip_accents",val:" = None"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15943/src/transformers/models/dpr/tokenization_dpr_fast.py#L91"}}),wo=new ge({}),Eo=new W({props:{name:"class transformers.DPRQuestionEncoderTokenizer",anchor:"transformers.DPRQuestionEncoderTokenizer",parameters:[{name:"vocab_file",val:""},{name:"do_lower_case",val:" = True"},{name:"do_basic_tokenize",val:" = True"},{name:"never_split",val:" = None"},{name:"unk_token",val:" = '[UNK]'"},{name:"sep_token",val:" = '[SEP]'"},{name:"pad_token",val:" = '[PAD]'"},{name:"cls_token",val:" = '[CLS]'"},{name:"mask_token",val:" = '[MASK]'"},{name:"tokenize_chinese_chars",val:" = True"},{name:"strip_accents",val:" = None"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15943/src/transformers/models/dpr/tokenization_dpr.py#L106"}}),Do=new ge({}),$o=new W({props:{name:"class transformers.DPRQuestionEncoderTokenizerFast",anchor:"transformers.DPRQuestionEncoderTokenizerFast",parameters:[{name:"vocab_file",val:" = None"},{name:"tokenizer_file",val:" = None"},{name:"do_lower_case",val:" = True"},{name:"unk_token",val:" = '[UNK]'"},{name:"sep_token",val:" = '[SEP]'"},{name:"pad_token",val:" = '[PAD]'"},{name:"cls_token",val:" = '[CLS]'"},{name:"mask_token",val:" = '[MASK]'"},{name:"tokenize_chinese_chars",val:" = True"},{name:"strip_accents",val:" = None"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15943/src/transformers/models/dpr/tokenization_dpr_fast.py#L108"}}),zo=new ge({}),qo=new W({props:{name:"class transformers.DPRReaderTokenizer",anchor:"transformers.DPRReaderTokenizer",parameters:[{name:"vocab_file",val:""},{name:"do_lower_case",val:" = True"},{name:"do_basic_tokenize",val:" = True"},{name:"never_split",val:" = None"},{name:"unk_token",val:" = '[UNK]'"},{name:"sep_token",val:" = '[SEP]'"},{name:"pad_token",val:" = '[PAD]'"},{name:"cls_token",val:" = '[CLS]'"},{name:"mask_token",val:" = '[MASK]'"},{name:"tokenize_chinese_chars",val:" = True"},{name:"strip_accents",val:" = None"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15943/src/transformers/models/dpr/tokenization_dpr.py#L370",parametersDescription:[{anchor:"transformers.DPRReaderTokenizer.questions",description:`<strong>questions</strong> (<code>str</code> or <code>List[str]</code>) &#x2014;
The questions to be encoded. You can specify one question for many passages. In this case, the question
will be duplicated like <code>[questions] * n_passages</code>. Otherwise you have to specify as many questions as in
<code>titles</code> or <code>texts</code>.`,name:"questions"},{anchor:"transformers.DPRReaderTokenizer.titles",description:`<strong>titles</strong> (<code>str</code> or <code>List[str]</code>) &#x2014;
The passages titles to be encoded. This can be a string or a list of strings if there are several passages.`,name:"titles"},{anchor:"transformers.DPRReaderTokenizer.texts",description:`<strong>texts</strong> (<code>str</code> or <code>List[str]</code>) &#x2014;
The passages texts to be encoded. This can be a string or a list of strings if there are several passages.`,name:"texts"},{anchor:"transformers.DPRReaderTokenizer.padding",description:`<strong>padding</strong> (<code>bool</code>, <code>str</code> or <a href="/docs/transformers/pr_15943/en/internal/file_utils#transformers.file_utils.PaddingStrategy">PaddingStrategy</a>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Activates and controls padding. Accepts the following values:</p>
<ul>
<li><code>True</code> or <code>&apos;longest&apos;</code>: Pad to the longest sequence in the batch (or no padding if only a single sequence
if provided).</li>
<li><code>&apos;max_length&apos;</code>: Pad to a maximum length specified with the argument <code>max_length</code> or to the maximum
acceptable input length for the model if that argument is not provided.</li>
<li><code>False</code> or <code>&apos;do_not_pad&apos;</code> (default): No padding (i.e., can output a batch with sequences of different
lengths).</li>
</ul>`,name:"padding"},{anchor:"transformers.DPRReaderTokenizer.truncation",description:`<strong>truncation</strong> (<code>bool</code>, <code>str</code> or <a href="/docs/transformers/pr_15943/en/internal/tokenization_utils#transformers.tokenization_utils_base.TruncationStrategy">TruncationStrategy</a>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Activates and controls truncation. Accepts the following values:</p>
<ul>
<li><code>True</code> or <code>&apos;longest_first&apos;</code>: Truncate to a maximum length specified with the argument <code>max_length</code> or to
the maximum acceptable input length for the model if that argument is not provided. This will truncate
token by token, removing a token from the longest sequence in the pair if a pair of sequences (or a batch
of pairs) is provided.</li>
<li><code>&apos;only_first&apos;</code>: Truncate to a maximum length specified with the argument <code>max_length</code> or to the maximum
acceptable input length for the model if that argument is not provided. This will only truncate the first
sequence of a pair if a pair of sequences (or a batch of pairs) is provided.</li>
<li><code>&apos;only_second&apos;</code>: Truncate to a maximum length specified with the argument <code>max_length</code> or to the maximum
acceptable input length for the model if that argument is not provided. This will only truncate the
second sequence of a pair if a pair of sequences (or a batch of pairs) is provided.</li>
<li><code>False</code> or <code>&apos;do_not_truncate&apos;</code> (default): No truncation (i.e., can output batch with sequence lengths
greater than the model maximum admissible input size).</li>
</ul>`,name:"truncation"},{anchor:"transformers.DPRReaderTokenizer.max_length",description:`<strong>max_length</strong> (<code>int</code>, <em>optional</em>) &#x2014;
Controls the maximum length to use by one of the truncation/padding parameters.</p>
<p>If left unset or set to <code>None</code>, this will use the predefined model maximum length if a maximum length
is required by one of the truncation/padding parameters. If the model has no specific maximum input
length (like XLNet) truncation/padding to a maximum length will be deactivated.`,name:"max_length"},{anchor:"transformers.DPRReaderTokenizer.return_tensors",description:`<strong>return_tensors</strong> (<code>str</code> or <a href="/docs/transformers/pr_15943/en/internal/file_utils#transformers.TensorType">TensorType</a>, <em>optional</em>) &#x2014;
If set, will return tensors instead of list of python integers. Acceptable values are:</p>
<ul>
<li><code>&apos;tf&apos;</code>: Return TensorFlow <code>tf.constant</code> objects.</li>
<li><code>&apos;pt&apos;</code>: Return PyTorch <code>torch.Tensor</code> objects.</li>
<li><code>&apos;np&apos;</code>: Return Numpy <code>np.ndarray</code> objects.</li>
</ul>`,name:"return_tensors"},{anchor:"transformers.DPRReaderTokenizer.return_attention_mask",description:`<strong>return_attention_mask</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attention mask. If not set, will return the attention mask according to the
specific tokenizer&#x2019;s default, defined by the <code>return_outputs</code> attribute.</p>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"return_attention_mask"}],returnDescription:`
<p>A dictionary with the following keys:</p>
<p>List of token ids to be fed to a model.
List of indices specifying which tokens should be attended to by the model.</p>
`,returnType:`
<ul>
<li><code>attention_mask</code></li>
</ul>
`}}),Co=new Qn({props:{code:"[CLS] <question token ids> [SEP] <titles ids> [SEP] <texts ids>",highlighted:'[CLS] <span class="hljs-tag">&lt;<span class="hljs-name">question</span> <span class="hljs-attr">token</span> <span class="hljs-attr">ids</span>&gt;</span> [SEP] <span class="hljs-tag">&lt;<span class="hljs-name">titles</span> <span class="hljs-attr">ids</span>&gt;</span> [SEP] <span class="hljs-tag">&lt;<span class="hljs-name">texts</span> <span class="hljs-attr">ids</span>&gt;</span>'}}),Ao=new ge({}),No=new W({props:{name:"class transformers.DPRReaderTokenizerFast",anchor:"transformers.DPRReaderTokenizerFast",parameters:[{name:"vocab_file",val:" = None"},{name:"tokenizer_file",val:" = None"},{name:"do_lower_case",val:" = True"},{name:"unk_token",val:" = '[UNK]'"},{name:"sep_token",val:" = '[SEP]'"},{name:"pad_token",val:" = '[PAD]'"},{name:"cls_token",val:" = '[CLS]'"},{name:"mask_token",val:" = '[MASK]'"},{name:"tokenize_chinese_chars",val:" = True"},{name:"strip_accents",val:" = None"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15943/src/transformers/models/dpr/tokenization_dpr_fast.py#L371",parametersDescription:[{anchor:"transformers.DPRReaderTokenizerFast.questions",description:`<strong>questions</strong> (<code>str</code> or <code>List[str]</code>) &#x2014;
The questions to be encoded. You can specify one question for many passages. In this case, the question
will be duplicated like <code>[questions] * n_passages</code>. Otherwise you have to specify as many questions as in
<code>titles</code> or <code>texts</code>.`,name:"questions"},{anchor:"transformers.DPRReaderTokenizerFast.titles",description:`<strong>titles</strong> (<code>str</code> or <code>List[str]</code>) &#x2014;
The passages titles to be encoded. This can be a string or a list of strings if there are several passages.`,name:"titles"},{anchor:"transformers.DPRReaderTokenizerFast.texts",description:`<strong>texts</strong> (<code>str</code> or <code>List[str]</code>) &#x2014;
The passages texts to be encoded. This can be a string or a list of strings if there are several passages.`,name:"texts"},{anchor:"transformers.DPRReaderTokenizerFast.padding",description:`<strong>padding</strong> (<code>bool</code>, <code>str</code> or <a href="/docs/transformers/pr_15943/en/internal/file_utils#transformers.file_utils.PaddingStrategy">PaddingStrategy</a>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Activates and controls padding. Accepts the following values:</p>
<ul>
<li><code>True</code> or <code>&apos;longest&apos;</code>: Pad to the longest sequence in the batch (or no padding if only a single sequence
if provided).</li>
<li><code>&apos;max_length&apos;</code>: Pad to a maximum length specified with the argument <code>max_length</code> or to the maximum
acceptable input length for the model if that argument is not provided.</li>
<li><code>False</code> or <code>&apos;do_not_pad&apos;</code> (default): No padding (i.e., can output a batch with sequences of different
lengths).</li>
</ul>`,name:"padding"},{anchor:"transformers.DPRReaderTokenizerFast.truncation",description:`<strong>truncation</strong> (<code>bool</code>, <code>str</code> or <a href="/docs/transformers/pr_15943/en/internal/tokenization_utils#transformers.tokenization_utils_base.TruncationStrategy">TruncationStrategy</a>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Activates and controls truncation. Accepts the following values:</p>
<ul>
<li><code>True</code> or <code>&apos;longest_first&apos;</code>: Truncate to a maximum length specified with the argument <code>max_length</code> or to
the maximum acceptable input length for the model if that argument is not provided. This will truncate
token by token, removing a token from the longest sequence in the pair if a pair of sequences (or a batch
of pairs) is provided.</li>
<li><code>&apos;only_first&apos;</code>: Truncate to a maximum length specified with the argument <code>max_length</code> or to the maximum
acceptable input length for the model if that argument is not provided. This will only truncate the first
sequence of a pair if a pair of sequences (or a batch of pairs) is provided.</li>
<li><code>&apos;only_second&apos;</code>: Truncate to a maximum length specified with the argument <code>max_length</code> or to the maximum
acceptable input length for the model if that argument is not provided. This will only truncate the
second sequence of a pair if a pair of sequences (or a batch of pairs) is provided.</li>
<li><code>False</code> or <code>&apos;do_not_truncate&apos;</code> (default): No truncation (i.e., can output batch with sequence lengths
greater than the model maximum admissible input size).</li>
</ul>`,name:"truncation"},{anchor:"transformers.DPRReaderTokenizerFast.max_length",description:`<strong>max_length</strong> (<code>int</code>, <em>optional</em>) &#x2014;
Controls the maximum length to use by one of the truncation/padding parameters.</p>
<p>If left unset or set to <code>None</code>, this will use the predefined model maximum length if a maximum length
is required by one of the truncation/padding parameters. If the model has no specific maximum input
length (like XLNet) truncation/padding to a maximum length will be deactivated.`,name:"max_length"},{anchor:"transformers.DPRReaderTokenizerFast.return_tensors",description:`<strong>return_tensors</strong> (<code>str</code> or <a href="/docs/transformers/pr_15943/en/internal/file_utils#transformers.TensorType">TensorType</a>, <em>optional</em>) &#x2014;
If set, will return tensors instead of list of python integers. Acceptable values are:</p>
<ul>
<li><code>&apos;tf&apos;</code>: Return TensorFlow <code>tf.constant</code> objects.</li>
<li><code>&apos;pt&apos;</code>: Return PyTorch <code>torch.Tensor</code> objects.</li>
<li><code>&apos;np&apos;</code>: Return Numpy <code>np.ndarray</code> objects.</li>
</ul>`,name:"return_tensors"},{anchor:"transformers.DPRReaderTokenizerFast.return_attention_mask",description:`<strong>return_attention_mask</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attention mask. If not set, will return the attention mask according to the
specific tokenizer&#x2019;s default, defined by the <code>return_outputs</code> attribute.</p>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"return_attention_mask"}],returnDescription:`
<p>A dictionary with the following keys:</p>
<p>List of token ids to be fed to a model.
List of indices specifying which tokens should be attended to by the model.</p>
`,returnType:`
<ul>
<li><code>attention_mask</code></li>
</ul>
`}}),Mo=new ge({}),Qo=new W({props:{name:"class transformers.models.dpr.modeling_dpr.DPRContextEncoderOutput",anchor:"transformers.models.dpr.modeling_dpr.DPRContextEncoderOutput",parameters:[{name:"pooler_output",val:": FloatTensor"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}],source:"https://github.com/huggingface/transformers/blob/pr_15943/src/transformers/models/dpr/modeling_dpr.py#L62",parametersDescription:[{anchor:"transformers.models.dpr.modeling_dpr.DPRContextEncoderOutput.pooler_output",description:`<strong>pooler_output</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, embeddings_size)</code>) &#x2014;
The DPR encoder outputs the <em>pooler_output</em> that corresponds to the context representation. Last layer
hidden-state of the first token of the sequence (classification token) further processed by a Linear layer.
This output is to be used to embed contexts for nearest neighbors queries with questions embeddings.`,name:"pooler_output"},{anchor:"transformers.models.dpr.modeling_dpr.DPRContextEncoderOutput.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.models.dpr.modeling_dpr.DPRContextEncoderOutput.attentions",description:`<strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"}]}}),Io=new W({props:{name:"class transformers.models.dpr.modeling_dpr.DPRQuestionEncoderOutput",anchor:"transformers.models.dpr.modeling_dpr.DPRQuestionEncoderOutput",parameters:[{name:"pooler_output",val:": FloatTensor"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}],source:"https://github.com/huggingface/transformers/blob/pr_15943/src/transformers/models/dpr/modeling_dpr.py#L90",parametersDescription:[{anchor:"transformers.models.dpr.modeling_dpr.DPRQuestionEncoderOutput.pooler_output",description:`<strong>pooler_output</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, embeddings_size)</code>) &#x2014;
The DPR encoder outputs the <em>pooler_output</em> that corresponds to the question representation. Last layer
hidden-state of the first token of the sequence (classification token) further processed by a Linear layer.
This output is to be used to embed questions for nearest neighbors queries with context embeddings.`,name:"pooler_output"},{anchor:"transformers.models.dpr.modeling_dpr.DPRQuestionEncoderOutput.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.models.dpr.modeling_dpr.DPRQuestionEncoderOutput.attentions",description:`<strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"}]}}),Ho=new W({props:{name:"class transformers.DPRReaderOutput",anchor:"transformers.DPRReaderOutput",parameters:[{name:"start_logits",val:": FloatTensor"},{name:"end_logits",val:": FloatTensor = None"},{name:"relevance_logits",val:": FloatTensor = None"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}],source:"https://github.com/huggingface/transformers/blob/pr_15943/src/transformers/models/dpr/modeling_dpr.py#L118",parametersDescription:[{anchor:"transformers.DPRReaderOutput.start_logits",description:`<strong>start_logits</strong> (<code>torch.FloatTensor</code> of shape <code>(n_passages, sequence_length)</code>) &#x2014;
Logits of the start index of the span for each passage.`,name:"start_logits"},{anchor:"transformers.DPRReaderOutput.end_logits",description:`<strong>end_logits</strong> (<code>torch.FloatTensor</code> of shape <code>(n_passages, sequence_length)</code>) &#x2014;
Logits of the end index of the span for each passage.`,name:"end_logits"},{anchor:"transformers.DPRReaderOutput.relevance_logits",description:`<strong>relevance_logits</strong> (<code>torch.FloatTensor</code> of shape <code>(n_passages, )</code>) &#x2014;
Outputs of the QA classifier of the DPRReader that corresponds to the scores of each passage to answer the
question, compared to all the other passages.`,name:"relevance_logits"},{anchor:"transformers.DPRReaderOutput.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.DPRReaderOutput.attentions",description:`<strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"}]}}),Wo=new ge({}),Vo=new W({props:{name:"class transformers.DPRContextEncoder",anchor:"transformers.DPRContextEncoder",parameters:[{name:"config",val:": DPRConfig"}],source:"https://github.com/huggingface/transformers/blob/pr_15943/src/transformers/models/dpr/modeling_dpr.py#L445",parametersDescription:[{anchor:"transformers.DPRContextEncoder.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15943/en/model_doc/dpr#transformers.DPRConfig">DPRConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_15943/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),Xo=new W({props:{name:"forward",anchor:"transformers.DPRContextEncoder.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"token_type_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"inputs_embeds",val:": typing.Optional[torch.Tensor] = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"}],source:"https://github.com/huggingface/transformers/blob/pr_15943/src/transformers/models/dpr/modeling_dpr.py#L453",parametersDescription:[{anchor:"transformers.DPRContextEncoder.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. To match pretraining, DPR input sequence should be
formatted with [CLS] and [SEP] tokens as follows:</p>
<p>(a) For sequence pairs (for a pair title+text for example):`,name:"input_ids"}],returnDescription:`
<p>A <a
  href="/docs/transformers/pr_15943/en/model_doc/dpr#transformers.models.dpr.modeling_dpr.DPRContextEncoderOutput"
>transformers.models.dpr.modeling_dpr.DPRContextEncoderOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/pr_15943/en/model_doc/dpr#transformers.DPRConfig"
>DPRConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>pooler_output</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, embeddings_size)</code>) \u2014 The DPR encoder outputs the <em>pooler_output</em> that corresponds to the context representation. Last layer
hidden-state of the first token of the sequence (classification token) further processed by a Linear layer.
This output is to be used to embed contexts for nearest neighbors queries with questions embeddings.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`}}),Vt=new Ct({props:{$$slots:{default:[Au]},$$scope:{ctx:V}}}),Jo=new Qn({props:{code:`from transformers import DPRContextEncoder, DPRContextEncoderTokenizer

tokenizer = DPRContextEncoderTokenizer.from_pretrained("facebook/dpr-ctx_encoder-single-nq-base")
model = DPRContextEncoder.from_pretrained("facebook/dpr-ctx_encoder-single-nq-base")
input_ids = tokenizer("Hello, is my dog cute ?", return_tensors="pt")["input_ids"]
embeddings = model(input_ids).pooler_output`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DPRContextEncoder, DPRContextEncoderTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DPRContextEncoderTokenizer.from_pretrained(<span class="hljs-string">&quot;facebook/dpr-ctx_encoder-single-nq-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = DPRContextEncoder.from_pretrained(<span class="hljs-string">&quot;facebook/dpr-ctx_encoder-single-nq-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = tokenizer(<span class="hljs-string">&quot;Hello, is my dog cute ?&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)[<span class="hljs-string">&quot;input_ids&quot;</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>embeddings = model(input_ids).pooler_output`}}),Go=new ge({}),Zo=new W({props:{name:"class transformers.DPRQuestionEncoder",anchor:"transformers.DPRQuestionEncoder",parameters:[{name:"config",val:": DPRConfig"}],source:"https://github.com/huggingface/transformers/blob/pr_15943/src/transformers/models/dpr/modeling_dpr.py#L526",parametersDescription:[{anchor:"transformers.DPRQuestionEncoder.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15943/en/model_doc/dpr#transformers.DPRConfig">DPRConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_15943/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),nn=new W({props:{name:"forward",anchor:"transformers.DPRQuestionEncoder.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"token_type_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"inputs_embeds",val:": typing.Optional[torch.Tensor] = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"}],source:"https://github.com/huggingface/transformers/blob/pr_15943/src/transformers/models/dpr/modeling_dpr.py#L534",parametersDescription:[{anchor:"transformers.DPRQuestionEncoder.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. To match pretraining, DPR input sequence should be
formatted with [CLS] and [SEP] tokens as follows:</p>
<p>(a) For sequence pairs (for a pair title+text for example):`,name:"input_ids"}],returnDescription:`
<p>A <a
  href="/docs/transformers/pr_15943/en/model_doc/dpr#transformers.models.dpr.modeling_dpr.DPRQuestionEncoderOutput"
>transformers.models.dpr.modeling_dpr.DPRQuestionEncoderOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/pr_15943/en/model_doc/dpr#transformers.DPRConfig"
>DPRConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>pooler_output</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, embeddings_size)</code>) \u2014 The DPR encoder outputs the <em>pooler_output</em> that corresponds to the question representation. Last layer
hidden-state of the first token of the sequence (classification token) further processed by a Linear layer.
This output is to be used to embed questions for nearest neighbors queries with context embeddings.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`}}),Kt=new Ct({props:{$$slots:{default:[Nu]},$$scope:{ctx:V}}}),rn=new Qn({props:{code:`from transformers import DPRQuestionEncoder, DPRQuestionEncoderTokenizer

tokenizer = DPRQuestionEncoderTokenizer.from_pretrained("facebook/dpr-question_encoder-single-nq-base")
model = DPRQuestionEncoder.from_pretrained("facebook/dpr-question_encoder-single-nq-base")
input_ids = tokenizer("Hello, is my dog cute ?", return_tensors="pt")["input_ids"]
embeddings = model(input_ids).pooler_output`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DPRQuestionEncoder, DPRQuestionEncoderTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DPRQuestionEncoderTokenizer.from_pretrained(<span class="hljs-string">&quot;facebook/dpr-question_encoder-single-nq-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = DPRQuestionEncoder.from_pretrained(<span class="hljs-string">&quot;facebook/dpr-question_encoder-single-nq-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = tokenizer(<span class="hljs-string">&quot;Hello, is my dog cute ?&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)[<span class="hljs-string">&quot;input_ids&quot;</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>embeddings = model(input_ids).pooler_output`}}),sn=new ge({}),an=new W({props:{name:"class transformers.DPRReader",anchor:"transformers.DPRReader",parameters:[{name:"config",val:": DPRConfig"}],source:"https://github.com/huggingface/transformers/blob/pr_15943/src/transformers/models/dpr/modeling_dpr.py#L607",parametersDescription:[{anchor:"transformers.DPRReader.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15943/en/model_doc/dpr#transformers.DPRConfig">DPRConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_15943/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),pn=new W({props:{name:"forward",anchor:"transformers.DPRReader.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"inputs_embeds",val:": typing.Optional[torch.Tensor] = None"},{name:"output_attentions",val:": bool = None"},{name:"output_hidden_states",val:": bool = None"},{name:"return_dict",val:" = None"}],source:"https://github.com/huggingface/transformers/blob/pr_15943/src/transformers/models/dpr/modeling_dpr.py#L615",parametersDescription:[{anchor:"transformers.DPRReader.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(n_passages, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.DPRReader.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(n_passages, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.DPRReader.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.DPRReader.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.DPRReader.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_15943/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],returnDescription:`
<p>A <a
  href="/docs/transformers/pr_15943/en/model_doc/dpr#transformers.DPRReaderOutput"
>transformers.models.dpr.modeling_dpr.DPRReaderOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/pr_15943/en/model_doc/dpr#transformers.DPRConfig"
>DPRConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>start_logits</strong> (<code>torch.FloatTensor</code> of shape <code>(n_passages, sequence_length)</code>) \u2014 Logits of the start index of the span for each passage.</p>
</li>
<li>
<p><strong>end_logits</strong> (<code>torch.FloatTensor</code> of shape <code>(n_passages, sequence_length)</code>) \u2014 Logits of the end index of the span for each passage.</p>
</li>
<li>
<p><strong>relevance_logits</strong> (<code>torch.FloatTensor</code> of shape <code>(n_passages, )</code>) \u2014 Outputs of the QA classifier of the DPRReader that corresponds to the scores of each passage to answer the
question, compared to all the other passages.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`}}),Xt=new Ct({props:{$$slots:{default:[Ou]},$$scope:{ctx:V}}}),hn=new Qn({props:{code:`from transformers import DPRReader, DPRReaderTokenizer

tokenizer = DPRReaderTokenizer.from_pretrained("facebook/dpr-reader-single-nq-base")
model = DPRReader.from_pretrained("facebook/dpr-reader-single-nq-base")
encoded_inputs = tokenizer(
    questions=["What is love ?"],
    titles=["Haddaway"],
    texts=["'What Is Love' is a song recorded by the artist Haddaway"],
    return_tensors="pt",
)
outputs = model(**encoded_inputs)
start_logits = outputs.start_logits
end_logits = outputs.end_logits
relevance_logits = outputs.relevance_logits`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DPRReader, DPRReaderTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DPRReaderTokenizer.from_pretrained(<span class="hljs-string">&quot;facebook/dpr-reader-single-nq-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = DPRReader.from_pretrained(<span class="hljs-string">&quot;facebook/dpr-reader-single-nq-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>encoded_inputs = tokenizer(
<span class="hljs-meta">... </span>    questions=[<span class="hljs-string">&quot;What is love ?&quot;</span>],
<span class="hljs-meta">... </span>    titles=[<span class="hljs-string">&quot;Haddaway&quot;</span>],
<span class="hljs-meta">... </span>    texts=[<span class="hljs-string">&quot;&#x27;What Is Love&#x27; is a song recorded by the artist Haddaway&quot;</span>],
<span class="hljs-meta">... </span>    return_tensors=<span class="hljs-string">&quot;pt&quot;</span>,
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**encoded_inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>start_logits = outputs.start_logits
<span class="hljs-meta">&gt;&gt;&gt; </span>end_logits = outputs.end_logits
<span class="hljs-meta">&gt;&gt;&gt; </span>relevance_logits = outputs.relevance_logits`}}),fn=new ge({}),un=new W({props:{name:"class transformers.TFDPRContextEncoder",anchor:"transformers.TFDPRContextEncoder",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15943/src/transformers/models/dpr/modeling_tf_dpr.py#L585",parametersDescription:[{anchor:"transformers.TFDPRContextEncoder.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15943/en/model_doc/dpr#transformers.DPRConfig">DPRConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_15943/en/main_classes/model#transformers.TFPreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),Gt=new Ct({props:{$$slots:{default:[ju]},$$scope:{ctx:V}}}),vn=new W({props:{name:"call",anchor:"transformers.TFDPRContextEncoder.call",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:": typing.Optional[tensorflow.python.framework.ops.Tensor] = None"},{name:"token_type_ids",val:": typing.Optional[tensorflow.python.framework.ops.Tensor] = None"},{name:"inputs_embeds",val:": typing.Optional[tensorflow.python.framework.ops.Tensor] = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"},{name:"training",val:": bool = False"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15943/src/transformers/models/dpr/modeling_tf_dpr.py#L597",parametersDescription:[{anchor:"transformers.TFDPRContextEncoder.call.input_ids",description:`<strong>input_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. To match pretraining, DPR input sequence should be
formatted with [CLS] and [SEP] tokens as follows:</p>
<p>(a) For sequence pairs (for a pair title+text for example):`,name:"input_ids"}],returnDescription:`
<p>A <code>transformers.models.dpr.modeling_tf_dpr.TFDPRContextEncoderOutput</code>or a tuple of <code>tf.Tensor</code> (if
<code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various elements depending on the
configuration (<a
  href="/docs/transformers/pr_15943/en/model_doc/dpr#transformers.DPRConfig"
>DPRConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>pooler_output</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, embeddings_size)</code>) \u2014 The DPR encoder outputs the <em>pooler_output</em> that corresponds to the context representation. Last layer
hidden-state of the first token of the sequence (classification token) further processed by a Linear layer.
This output is to be used to embed contexts for nearest neighbors queries with questions embeddings.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`}}),Zt=new Ct({props:{$$slots:{default:[Mu]},$$scope:{ctx:V}}}),Tn=new Qn({props:{code:`from transformers import TFDPRContextEncoder, DPRContextEncoderTokenizer

tokenizer = DPRContextEncoderTokenizer.from_pretrained("facebook/dpr-ctx_encoder-single-nq-base")
model = TFDPRContextEncoder.from_pretrained("facebook/dpr-ctx_encoder-single-nq-base", from_pt=True)
input_ids = tokenizer("Hello, is my dog cute ?", return_tensors="tf")["input_ids"]
embeddings = model(input_ids).pooler_output`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFDPRContextEncoder, DPRContextEncoderTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DPRContextEncoderTokenizer.from_pretrained(<span class="hljs-string">&quot;facebook/dpr-ctx_encoder-single-nq-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFDPRContextEncoder.from_pretrained(<span class="hljs-string">&quot;facebook/dpr-ctx_encoder-single-nq-base&quot;</span>, from_pt=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = tokenizer(<span class="hljs-string">&quot;Hello, is my dog cute ?&quot;</span>, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)[<span class="hljs-string">&quot;input_ids&quot;</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>embeddings = model(input_ids).pooler_output`}}),kn=new ge({}),bn=new W({props:{name:"class transformers.TFDPRQuestionEncoder",anchor:"transformers.TFDPRQuestionEncoder",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15943/src/transformers/models/dpr/modeling_tf_dpr.py#L686",parametersDescription:[{anchor:"transformers.TFDPRQuestionEncoder.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15943/en/model_doc/dpr#transformers.DPRConfig">DPRConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_15943/en/main_classes/model#transformers.TFPreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),to=new Ct({props:{$$slots:{default:[Qu]},$$scope:{ctx:V}}}),Rn=new W({props:{name:"call",anchor:"transformers.TFDPRQuestionEncoder.call",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:": typing.Optional[tensorflow.python.framework.ops.Tensor] = None"},{name:"token_type_ids",val:": typing.Optional[tensorflow.python.framework.ops.Tensor] = None"},{name:"inputs_embeds",val:": typing.Optional[tensorflow.python.framework.ops.Tensor] = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"},{name:"training",val:": bool = False"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15943/src/transformers/models/dpr/modeling_tf_dpr.py#L698",parametersDescription:[{anchor:"transformers.TFDPRQuestionEncoder.call.input_ids",description:`<strong>input_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. To match pretraining, DPR input sequence should be
formatted with [CLS] and [SEP] tokens as follows:</p>
<p>(a) For sequence pairs (for a pair title+text for example):`,name:"input_ids"}],returnDescription:`
<p>A <code>transformers.models.dpr.modeling_tf_dpr.TFDPRQuestionEncoderOutput</code>or a tuple of <code>tf.Tensor</code> (if
<code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various elements depending on the
configuration (<a
  href="/docs/transformers/pr_15943/en/model_doc/dpr#transformers.DPRConfig"
>DPRConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>pooler_output</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, embeddings_size)</code>) \u2014 The DPR encoder outputs the <em>pooler_output</em> that corresponds to the question representation. Last layer
hidden-state of the first token of the sequence (classification token) further processed by a Linear layer.
This output is to be used to embed questions for nearest neighbors queries with context embeddings.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`}}),oo=new Ct({props:{$$slots:{default:[Lu]},$$scope:{ctx:V}}}),Dn=new Qn({props:{code:`from transformers import TFDPRQuestionEncoder, DPRQuestionEncoderTokenizer

tokenizer = DPRQuestionEncoderTokenizer.from_pretrained("facebook/dpr-question_encoder-single-nq-base")
model = TFDPRQuestionEncoder.from_pretrained("facebook/dpr-question_encoder-single-nq-base", from_pt=True)
input_ids = tokenizer("Hello, is my dog cute ?", return_tensors="tf")["input_ids"]
embeddings = model(input_ids).pooler_output`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFDPRQuestionEncoder, DPRQuestionEncoderTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DPRQuestionEncoderTokenizer.from_pretrained(<span class="hljs-string">&quot;facebook/dpr-question_encoder-single-nq-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFDPRQuestionEncoder.from_pretrained(<span class="hljs-string">&quot;facebook/dpr-question_encoder-single-nq-base&quot;</span>, from_pt=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = tokenizer(<span class="hljs-string">&quot;Hello, is my dog cute ?&quot;</span>, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)[<span class="hljs-string">&quot;input_ids&quot;</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>embeddings = model(input_ids).pooler_output`}}),$n=new ge({}),yn=new W({props:{name:"class transformers.TFDPRReader",anchor:"transformers.TFDPRReader",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15943/src/transformers/models/dpr/modeling_tf_dpr.py#L786",parametersDescription:[{anchor:"transformers.TFDPRReader.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15943/en/model_doc/dpr#transformers.DPRConfig">DPRConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_15943/en/main_classes/model#transformers.TFPreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),ro=new Ct({props:{$$slots:{default:[Iu]},$$scope:{ctx:V}}}),Fn=new W({props:{name:"call",anchor:"transformers.TFDPRReader.call",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:": typing.Optional[tensorflow.python.framework.ops.Tensor] = None"},{name:"inputs_embeds",val:": typing.Optional[tensorflow.python.framework.ops.Tensor] = None"},{name:"output_attentions",val:": bool = None"},{name:"output_hidden_states",val:": bool = None"},{name:"return_dict",val:" = None"},{name:"training",val:": bool = False"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15943/src/transformers/models/dpr/modeling_tf_dpr.py#L798",parametersDescription:[{anchor:"transformers.TFDPRReader.call.attention_mask",description:`<strong>attention_mask</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(n_passages, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.TFDPRReader.call.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(n_passages, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.TFDPRReader.call.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail. This argument can be used only in eager mode, in graph mode the value in the config will be
used instead.`,name:"output_hidden_states"},{anchor:"transformers.TFDPRReader.call.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_15943/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple. This argument can be used
in eager mode, in graph mode the value will always be set to True.`,name:"return_dict"},{anchor:"transformers.TFDPRReader.call.training",description:`<strong>training</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to use the model in training mode (some modules like dropout modules have different
behaviors between training and evaluation).`,name:"training"}],returnDescription:`
<p>A <code>transformers.models.dpr.modeling_tf_dpr.TFDPRReaderOutput</code>or a tuple of <code>tf.Tensor</code> (if
<code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various elements depending on the
configuration (<a
  href="/docs/transformers/pr_15943/en/model_doc/dpr#transformers.DPRConfig"
>DPRConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>start_logits</strong> (<code>tf.Tensor</code> of shape <code>(n_passages, sequence_length)</code>) \u2014 Logits of the start index of the span for each passage.</p>
</li>
<li>
<p><strong>end_logits</strong> (<code>tf.Tensor</code> of shape <code>(n_passages, sequence_length)</code>) \u2014 Logits of the end index of the span for each passage.</p>
</li>
<li>
<p><strong>relevance_logits</strong> (<code>tf.Tensor</code> of shape <code>(n_passages, )</code>) \u2014 Outputs of the QA classifier of the DPRReader that corresponds to the scores of each passage to answer the
question, compared to all the other passages.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`}}),so=new Ct({props:{$$slots:{default:[Su]},$$scope:{ctx:V}}}),Cn=new Qn({props:{code:`from transformers import TFDPRReader, DPRReaderTokenizer

tokenizer = DPRReaderTokenizer.from_pretrained("facebook/dpr-reader-single-nq-base")
model = TFDPRReader.from_pretrained("facebook/dpr-reader-single-nq-base", from_pt=True)
encoded_inputs = tokenizer(
    questions=["What is love ?"],
    titles=["Haddaway"],
    texts=["'What Is Love' is a song recorded by the artist Haddaway"],
    return_tensors="tf",
)
outputs = model(encoded_inputs)
start_logits = outputs.start_logits
end_logits = outputs.end_logits
relevance_logits = outputs.relevance_logits`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFDPRReader, DPRReaderTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DPRReaderTokenizer.from_pretrained(<span class="hljs-string">&quot;facebook/dpr-reader-single-nq-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFDPRReader.from_pretrained(<span class="hljs-string">&quot;facebook/dpr-reader-single-nq-base&quot;</span>, from_pt=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>encoded_inputs = tokenizer(
<span class="hljs-meta">... </span>    questions=[<span class="hljs-string">&quot;What is love ?&quot;</span>],
<span class="hljs-meta">... </span>    titles=[<span class="hljs-string">&quot;Haddaway&quot;</span>],
<span class="hljs-meta">... </span>    texts=[<span class="hljs-string">&quot;&#x27;What Is Love&#x27; is a song recorded by the artist Haddaway&quot;</span>],
<span class="hljs-meta">... </span>    return_tensors=<span class="hljs-string">&quot;tf&quot;</span>,
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(encoded_inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>start_logits = outputs.start_logits
<span class="hljs-meta">&gt;&gt;&gt; </span>end_logits = outputs.end_logits
<span class="hljs-meta">&gt;&gt;&gt; </span>relevance_logits = outputs.relevance_logits`}}),{c(){u=n("meta"),D=d(),m=n("h1"),g=n("a"),R=n("span"),T(v.$$.fragment),_=d(),x=n("span"),_e=a("DPR"),Z=d(),$=n("h2"),J=n("a"),L=n("span"),T(ee.$$.fragment),ve=d(),I=n("span"),Te=a("Overview"),he=d(),S=n("p"),Q=a(`Dense Passage Retrieval (DPR) is a set of tools and models for state-of-the-art open-domain Q&A research. It was
introduced in `),te=n("a"),oe=a("Dense Passage Retrieval for Open-Domain Question Answering"),q=a(` by
Vladimir Karpukhin, Barlas O\u011Fuz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, Wen-tau Yih.`),C=d(),ne=n("p"),U=a("The abstract from the paper is the following:"),fe=d(),re=n("p"),H=n("em"),ke=a(`Open-domain question answering relies on efficient passage retrieval to select candidate contexts, where traditional
sparse vector space models, such as TF-IDF or BM25, are the de facto method. In this work, we show that retrieval can
be practically implemented using dense representations alone, where embeddings are learned from a small number of
questions and passages by a simple dual-encoder framework. When evaluated on a wide range of open-domain QA datasets,
our dense retriever outperforms a strong Lucene-BM25 system largely by 9%-19% absolute in terms of top-20 passage
retrieval accuracy, and helps our end-to-end QA system establish new state-of-the-art on multiple open-domain QA
benchmarks.`),ue=d(),z=n("p"),be=a("This model was contributed by "),O=n("a"),Pe=a("lhoestq"),we=a(". The original code can be found "),j=n("a"),Ee=a("here"),Re=a("."),M=d(),K=n("h2"),N=n("a"),se=n("span"),T(h.$$.fragment),y=d(),G=n("span"),Ie=a("DPRConfig"),Oe=d(),F=n("div"),T(me.$$.fragment),Se=d(),ae=n("p"),A=n("a"),Y=a("DPRConfig"),He=a(" is the configuration class to store the configuration of a "),xe=n("em"),X=a("DPRModel"),Be=a("."),We=d(),B=n("p"),Ve=a("This is the configuration class to store the configuration of a "),Ln=n("a"),li=a("DPRContextEncoder"),ci=a(", "),In=n("a"),pi=a("DPRQuestionEncoder"),hi=a(`, or a
`),Sn=n("a"),fi=a("DPRReader"),ui=a(". It is used to instantiate the components of the DPR model."),mi=d(),mo=n("p"),gi=a("This class is a subclass of "),Hn=n("a"),_i=a("BertConfig"),vi=a(". Please check the superclass for the documentation of all kwargs."),Vs=d(),pt=n("h2"),At=n("a"),Fr=n("span"),T(go.$$.fragment),Ti=d(),Cr=n("span"),ki=a("DPRContextEncoderTokenizer"),Us=d(),je=n("div"),T(_o.$$.fragment),bi=d(),Ar=n("p"),Pi=a("Construct a DPRContextEncoder tokenizer."),wi=d(),Nt=n("p"),Bn=n("a"),Ei=a("DPRContextEncoderTokenizer"),Ri=a(" is identical to "),Wn=n("a"),Di=a("BertTokenizer"),$i=a(` and runs end-to-end tokenization: punctuation
splitting and wordpiece.`),yi=d(),vo=n("p"),xi=a("Refer to superclass "),Vn=n("a"),zi=a("BertTokenizer"),qi=a(" for usage examples and documentation concerning parameters."),Ks=d(),ht=n("h2"),Ot=n("a"),Nr=n("span"),T(To.$$.fragment),Fi=d(),Or=n("span"),Ci=a("DPRContextEncoderTokenizerFast"),Ys=d(),Me=n("div"),T(ko.$$.fragment),Ai=d(),bo=n("p"),Ni=a("Construct a \u201Cfast\u201D DPRContextEncoder tokenizer (backed by HuggingFace\u2019s "),jr=n("em"),Oi=a("tokenizers"),ji=a(" library)."),Mi=d(),jt=n("p"),Un=n("a"),Qi=a("DPRContextEncoderTokenizerFast"),Li=a(" is identical to "),Kn=n("a"),Ii=a("BertTokenizerFast"),Si=a(` and runs end-to-end tokenization:
punctuation splitting and wordpiece.`),Hi=d(),Po=n("p"),Bi=a("Refer to superclass "),Yn=n("a"),Wi=a("BertTokenizerFast"),Vi=a(" for usage examples and documentation concerning parameters."),Xs=d(),ft=n("h2"),Mt=n("a"),Mr=n("span"),T(wo.$$.fragment),Ui=d(),Qr=n("span"),Ki=a("DPRQuestionEncoderTokenizer"),Js=d(),Qe=n("div"),T(Eo.$$.fragment),Yi=d(),Lr=n("p"),Xi=a("Constructs a DPRQuestionEncoder tokenizer."),Ji=d(),Qt=n("p"),Xn=n("a"),Gi=a("DPRQuestionEncoderTokenizer"),Zi=a(" is identical to "),Jn=n("a"),ed=a("BertTokenizer"),td=a(` and runs end-to-end tokenization: punctuation
splitting and wordpiece.`),od=d(),Ro=n("p"),nd=a("Refer to superclass "),Gn=n("a"),rd=a("BertTokenizer"),sd=a(" for usage examples and documentation concerning parameters."),Gs=d(),ut=n("h2"),Lt=n("a"),Ir=n("span"),T(Do.$$.fragment),ad=d(),Sr=n("span"),id=a("DPRQuestionEncoderTokenizerFast"),Zs=d(),Le=n("div"),T($o.$$.fragment),dd=d(),yo=n("p"),ld=a("Constructs a \u201Cfast\u201D DPRQuestionEncoder tokenizer (backed by HuggingFace\u2019s "),Hr=n("em"),cd=a("tokenizers"),pd=a(" library)."),hd=d(),It=n("p"),Zn=n("a"),fd=a("DPRQuestionEncoderTokenizerFast"),ud=a(" is identical to "),er=n("a"),md=a("BertTokenizerFast"),gd=a(` and runs end-to-end tokenization:
punctuation splitting and wordpiece.`),_d=d(),xo=n("p"),vd=a("Refer to superclass "),tr=n("a"),Td=a("BertTokenizerFast"),kd=a(" for usage examples and documentation concerning parameters."),ea=d(),mt=n("h2"),St=n("a"),Br=n("span"),T(zo.$$.fragment),bd=d(),Wr=n("span"),Pd=a("DPRReaderTokenizer"),ta=d(),ie=n("div"),T(qo.$$.fragment),wd=d(),Vr=n("p"),Ed=a("Construct a DPRReader tokenizer."),Rd=d(),et=n("p"),or=n("a"),Dd=a("DPRReaderTokenizer"),$d=a(" is almost identical to "),nr=n("a"),yd=a("BertTokenizer"),xd=a(` and runs end-to-end tokenization: punctuation
splitting and wordpiece. The difference is that is has three inputs strings: question, titles and texts that are
combined to be fed to the `),rr=n("a"),zd=a("DPRReader"),qd=a(" model."),Fd=d(),Fo=n("p"),Cd=a("Refer to superclass "),sr=n("a"),Ad=a("BertTokenizer"),Nd=a(" for usage examples and documentation concerning parameters."),Od=d(),Ge=n("p"),jd=a("Return a dictionary with the token ids of the input strings and other information to give to "),Ur=n("code"),Md=a(".decode_best_spans"),Qd=a(`.
It converts the strings of a question and different passages (title and text) in a sequence of IDs (integers),
using the tokenizer and vocabulary. The resulting `),Kr=n("code"),Ld=a("input_ids"),Id=a(" is a matrix of size "),Yr=n("code"),Sd=a("(n_passages, sequence_length)"),Hd=a(`
with the format:`),Bd=d(),T(Co.$$.fragment),oa=d(),gt=n("h2"),Ht=n("a"),Xr=n("span"),T(Ao.$$.fragment),Wd=d(),Jr=n("span"),Vd=a("DPRReaderTokenizerFast"),na=d(),de=n("div"),T(No.$$.fragment),Ud=d(),Oo=n("p"),Kd=a("Constructs a \u201Cfast\u201D DPRReader tokenizer (backed by HuggingFace\u2019s "),Gr=n("em"),Yd=a("tokenizers"),Xd=a(" library)."),Jd=d(),tt=n("p"),ar=n("a"),Gd=a("DPRReaderTokenizerFast"),Zd=a(" is almost identical to "),ir=n("a"),el=a("BertTokenizerFast"),tl=a(` and runs end-to-end tokenization:
punctuation splitting and wordpiece. The difference is that is has three inputs strings: question, titles and texts
that are combined to be fed to the `),dr=n("a"),ol=a("DPRReader"),nl=a(" model."),rl=d(),jo=n("p"),sl=a("Refer to superclass "),lr=n("a"),al=a("BertTokenizerFast"),il=a(" for usage examples and documentation concerning parameters."),dl=d(),Ze=n("p"),ll=a("Return a dictionary with the token ids of the input strings and other information to give to "),Zr=n("code"),cl=a(".decode_best_spans"),pl=a(`.
It converts the strings of a question and different passages (title and text) in a sequence of IDs (integers),
using the tokenizer and vocabulary. The resulting `),es=n("code"),hl=a("input_ids"),fl=a(" is a matrix of size "),ts=n("code"),ul=a("(n_passages, sequence_length)"),ml=a(`
with the format:`),gl=d(),os=n("p"),_l=a("[CLS] <question token ids> [SEP] <titles ids> [SEP] <texts ids>"),ra=d(),_t=n("h2"),Bt=n("a"),ns=n("span"),T(Mo.$$.fragment),vl=d(),rs=n("span"),Tl=a("DPR specific outputs"),sa=d(),vt=n("div"),T(Qo.$$.fragment),kl=d(),Lo=n("p"),bl=a("Class for outputs of "),cr=n("a"),Pl=a("DPRQuestionEncoder"),wl=a("."),aa=d(),Tt=n("div"),T(Io.$$.fragment),El=d(),So=n("p"),Rl=a("Class for outputs of "),pr=n("a"),Dl=a("DPRQuestionEncoder"),$l=a("."),ia=d(),kt=n("div"),T(Ho.$$.fragment),yl=d(),Bo=n("p"),xl=a("Class for outputs of "),hr=n("a"),zl=a("DPRQuestionEncoder"),ql=a("."),da=d(),bt=n("h2"),Wt=n("a"),ss=n("span"),T(Wo.$$.fragment),Fl=d(),as=n("span"),Cl=a("DPRContextEncoder"),la=d(),De=n("div"),T(Vo.$$.fragment),Al=d(),is=n("p"),Nl=a("The bare DPRContextEncoder transformer outputting pooler outputs as context representations."),Ol=d(),Uo=n("p"),jl=a("This model inherits from "),fr=n("a"),Ml=a("PreTrainedModel"),Ql=a(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Ll=d(),Ko=n("p"),Il=a("This model is also a PyTorch "),Yo=n("a"),Sl=a("torch.nn.Module"),Hl=a(` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),Bl=d(),ze=n("div"),T(Xo.$$.fragment),Wl=d(),Pt=n("p"),Vl=a("The "),ur=n("a"),Ul=a("DPRContextEncoder"),Kl=a(" forward method, overrides the "),ds=n("code"),Yl=a("__call__"),Xl=a(" special method."),Jl=d(),T(Vt.$$.fragment),Gl=d(),ls=n("p"),Zl=a("Examples:"),ec=d(),T(Jo.$$.fragment),ca=d(),wt=n("h2"),Ut=n("a"),cs=n("span"),T(Go.$$.fragment),tc=d(),ps=n("span"),oc=a("DPRQuestionEncoder"),pa=d(),$e=n("div"),T(Zo.$$.fragment),nc=d(),hs=n("p"),rc=a("The bare DPRQuestionEncoder transformer outputting pooler outputs as question representations."),sc=d(),en=n("p"),ac=a("This model inherits from "),mr=n("a"),ic=a("PreTrainedModel"),dc=a(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),lc=d(),tn=n("p"),cc=a("This model is also a PyTorch "),on=n("a"),pc=a("torch.nn.Module"),hc=a(` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),fc=d(),qe=n("div"),T(nn.$$.fragment),uc=d(),Et=n("p"),mc=a("The "),gr=n("a"),gc=a("DPRQuestionEncoder"),_c=a(" forward method, overrides the "),fs=n("code"),vc=a("__call__"),Tc=a(" special method."),kc=d(),T(Kt.$$.fragment),bc=d(),us=n("p"),Pc=a("Examples:"),wc=d(),T(rn.$$.fragment),ha=d(),Rt=n("h2"),Yt=n("a"),ms=n("span"),T(sn.$$.fragment),Ec=d(),gs=n("span"),Rc=a("DPRReader"),fa=d(),ye=n("div"),T(an.$$.fragment),Dc=d(),_s=n("p"),$c=a("The bare DPRReader transformer outputting span predictions."),yc=d(),dn=n("p"),xc=a("This model inherits from "),_r=n("a"),zc=a("PreTrainedModel"),qc=a(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Fc=d(),ln=n("p"),Cc=a("This model is also a PyTorch "),cn=n("a"),Ac=a("torch.nn.Module"),Nc=a(` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),Oc=d(),Fe=n("div"),T(pn.$$.fragment),jc=d(),Dt=n("p"),Mc=a("The "),vr=n("a"),Qc=a("DPRReader"),Lc=a(" forward method, overrides the "),vs=n("code"),Ic=a("__call__"),Sc=a(" special method."),Hc=d(),T(Xt.$$.fragment),Bc=d(),Ts=n("p"),Wc=a("Examples:"),Vc=d(),T(hn.$$.fragment),ua=d(),$t=n("h2"),Jt=n("a"),ks=n("span"),T(fn.$$.fragment),Uc=d(),bs=n("span"),Kc=a("TFDPRContextEncoder"),ma=d(),le=n("div"),T(un.$$.fragment),Yc=d(),Ps=n("p"),Xc=a("The bare DPRContextEncoder transformer outputting pooler outputs as context representations."),Jc=d(),mn=n("p"),Gc=a("This model inherits from "),Tr=n("a"),Zc=a("TFPreTrainedModel"),ep=a(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),tp=d(),gn=n("p"),op=a("This model is also a Tensorflow "),_n=n("a"),np=a("tf.keras.Model"),rp=a(`
subclass. Use it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to
general usage and behavior.`),sp=d(),T(Gt.$$.fragment),ap=d(),Ce=n("div"),T(vn.$$.fragment),ip=d(),yt=n("p"),dp=a("The "),kr=n("a"),lp=a("TFDPRContextEncoder"),cp=a(" forward method, overrides the "),ws=n("code"),pp=a("__call__"),hp=a(" special method."),fp=d(),T(Zt.$$.fragment),up=d(),Es=n("p"),mp=a("Examples:"),gp=d(),T(Tn.$$.fragment),ga=d(),xt=n("h2"),eo=n("a"),Rs=n("span"),T(kn.$$.fragment),_p=d(),Ds=n("span"),vp=a("TFDPRQuestionEncoder"),_a=d(),ce=n("div"),T(bn.$$.fragment),Tp=d(),$s=n("p"),kp=a("The bare DPRQuestionEncoder transformer outputting pooler outputs as question representations."),bp=d(),Pn=n("p"),Pp=a("This model inherits from "),br=n("a"),wp=a("TFPreTrainedModel"),Ep=a(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Rp=d(),wn=n("p"),Dp=a("This model is also a Tensorflow "),En=n("a"),$p=a("tf.keras.Model"),yp=a(`
subclass. Use it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to
general usage and behavior.`),xp=d(),T(to.$$.fragment),zp=d(),Ae=n("div"),T(Rn.$$.fragment),qp=d(),zt=n("p"),Fp=a("The "),Pr=n("a"),Cp=a("TFDPRQuestionEncoder"),Ap=a(" forward method, overrides the "),ys=n("code"),Np=a("__call__"),Op=a(" special method."),jp=d(),T(oo.$$.fragment),Mp=d(),xs=n("p"),Qp=a("Examples:"),Lp=d(),T(Dn.$$.fragment),va=d(),qt=n("h2"),no=n("a"),zs=n("span"),T($n.$$.fragment),Ip=d(),qs=n("span"),Sp=a("TFDPRReader"),Ta=d(),pe=n("div"),T(yn.$$.fragment),Hp=d(),Fs=n("p"),Bp=a("The bare DPRReader transformer outputting span predictions."),Wp=d(),xn=n("p"),Vp=a("This model inherits from "),wr=n("a"),Up=a("TFPreTrainedModel"),Kp=a(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Yp=d(),zn=n("p"),Xp=a("This model is also a Tensorflow "),qn=n("a"),Jp=a("tf.keras.Model"),Gp=a(`
subclass. Use it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to
general usage and behavior.`),Zp=d(),T(ro.$$.fragment),eh=d(),Ne=n("div"),T(Fn.$$.fragment),th=d(),Ft=n("p"),oh=a("The "),Er=n("a"),nh=a("TFDPRReader"),rh=a(" forward method, overrides the "),Cs=n("code"),sh=a("__call__"),ah=a(" special method."),ih=d(),T(so.$$.fragment),dh=d(),As=n("p"),lh=a("Examples:"),ch=d(),T(Cn.$$.fragment),this.h()},l(o){const f=Cu('[data-svelte="svelte-1phssyn"]',document.head);u=r(f,"META",{name:!0,content:!0}),f.forEach(t),D=l(o),m=r(o,"H1",{class:!0});var An=s(m);g=r(An,"A",{id:!0,class:!0,href:!0});var Ns=s(g);R=r(Ns,"SPAN",{});var Os=s(R);k(v.$$.fragment,Os),Os.forEach(t),Ns.forEach(t),_=l(An),x=r(An,"SPAN",{});var js=s(x);_e=i(js,"DPR"),js.forEach(t),An.forEach(t),Z=l(o),$=r(o,"H2",{class:!0});var Nn=s($);J=r(Nn,"A",{id:!0,class:!0,href:!0});var Ms=s(J);L=r(Ms,"SPAN",{});var Qs=s(L);k(ee.$$.fragment,Qs),Qs.forEach(t),Ms.forEach(t),ve=l(Nn),I=r(Nn,"SPAN",{});var Ls=s(I);Te=i(Ls,"Overview"),Ls.forEach(t),Nn.forEach(t),he=l(o),S=r(o,"P",{});var On=s(S);Q=i(On,`Dense Passage Retrieval (DPR) is a set of tools and models for state-of-the-art open-domain Q&A research. It was
introduced in `),te=r(On,"A",{href:!0,rel:!0});var ph=s(te);oe=i(ph,"Dense Passage Retrieval for Open-Domain Question Answering"),ph.forEach(t),q=i(On,` by
Vladimir Karpukhin, Barlas O\u011Fuz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, Wen-tau Yih.`),On.forEach(t),C=l(o),ne=r(o,"P",{});var hh=s(ne);U=i(hh,"The abstract from the paper is the following:"),hh.forEach(t),fe=l(o),re=r(o,"P",{});var fh=s(re);H=r(fh,"EM",{});var uh=s(H);ke=i(uh,`Open-domain question answering relies on efficient passage retrieval to select candidate contexts, where traditional
sparse vector space models, such as TF-IDF or BM25, are the de facto method. In this work, we show that retrieval can
be practically implemented using dense representations alone, where embeddings are learned from a small number of
questions and passages by a simple dual-encoder framework. When evaluated on a wide range of open-domain QA datasets,
our dense retriever outperforms a strong Lucene-BM25 system largely by 9%-19% absolute in terms of top-20 passage
retrieval accuracy, and helps our end-to-end QA system establish new state-of-the-art on multiple open-domain QA
benchmarks.`),uh.forEach(t),fh.forEach(t),ue=l(o),z=r(o,"P",{});var Rr=s(z);be=i(Rr,"This model was contributed by "),O=r(Rr,"A",{href:!0,rel:!0});var mh=s(O);Pe=i(mh,"lhoestq"),mh.forEach(t),we=i(Rr,". The original code can be found "),j=r(Rr,"A",{href:!0,rel:!0});var gh=s(j);Ee=i(gh,"here"),gh.forEach(t),Re=i(Rr,"."),Rr.forEach(t),M=l(o),K=r(o,"H2",{class:!0});var ba=s(K);N=r(ba,"A",{id:!0,class:!0,href:!0});var _h=s(N);se=r(_h,"SPAN",{});var vh=s(se);k(h.$$.fragment,vh),vh.forEach(t),_h.forEach(t),y=l(ba),G=r(ba,"SPAN",{});var Th=s(G);Ie=i(Th,"DPRConfig"),Th.forEach(t),ba.forEach(t),Oe=l(o),F=r(o,"DIV",{class:!0});var ao=s(F);k(me.$$.fragment,ao),Se=l(ao),ae=r(ao,"P",{});var Is=s(ae);A=r(Is,"A",{href:!0});var kh=s(A);Y=i(kh,"DPRConfig"),kh.forEach(t),He=i(Is," is the configuration class to store the configuration of a "),xe=r(Is,"EM",{});var bh=s(xe);X=i(bh,"DPRModel"),bh.forEach(t),Be=i(Is,"."),Is.forEach(t),We=l(ao),B=r(ao,"P",{});var io=s(B);Ve=i(io,"This is the configuration class to store the configuration of a "),Ln=r(io,"A",{href:!0});var Ph=s(Ln);li=i(Ph,"DPRContextEncoder"),Ph.forEach(t),ci=i(io,", "),In=r(io,"A",{href:!0});var wh=s(In);pi=i(wh,"DPRQuestionEncoder"),wh.forEach(t),hi=i(io,`, or a
`),Sn=r(io,"A",{href:!0});var Eh=s(Sn);fi=i(Eh,"DPRReader"),Eh.forEach(t),ui=i(io,". It is used to instantiate the components of the DPR model."),io.forEach(t),mi=l(ao),mo=r(ao,"P",{});var Pa=s(mo);gi=i(Pa,"This class is a subclass of "),Hn=r(Pa,"A",{href:!0});var Rh=s(Hn);_i=i(Rh,"BertConfig"),Rh.forEach(t),vi=i(Pa,". Please check the superclass for the documentation of all kwargs."),Pa.forEach(t),ao.forEach(t),Vs=l(o),pt=r(o,"H2",{class:!0});var wa=s(pt);At=r(wa,"A",{id:!0,class:!0,href:!0});var Dh=s(At);Fr=r(Dh,"SPAN",{});var $h=s(Fr);k(go.$$.fragment,$h),$h.forEach(t),Dh.forEach(t),Ti=l(wa),Cr=r(wa,"SPAN",{});var yh=s(Cr);ki=i(yh,"DPRContextEncoderTokenizer"),yh.forEach(t),wa.forEach(t),Us=l(o),je=r(o,"DIV",{class:!0});var lo=s(je);k(_o.$$.fragment,lo),bi=l(lo),Ar=r(lo,"P",{});var xh=s(Ar);Pi=i(xh,"Construct a DPRContextEncoder tokenizer."),xh.forEach(t),wi=l(lo),Nt=r(lo,"P",{});var Ss=s(Nt);Bn=r(Ss,"A",{href:!0});var zh=s(Bn);Ei=i(zh,"DPRContextEncoderTokenizer"),zh.forEach(t),Ri=i(Ss," is identical to "),Wn=r(Ss,"A",{href:!0});var qh=s(Wn);Di=i(qh,"BertTokenizer"),qh.forEach(t),$i=i(Ss,` and runs end-to-end tokenization: punctuation
splitting and wordpiece.`),Ss.forEach(t),yi=l(lo),vo=r(lo,"P",{});var Ea=s(vo);xi=i(Ea,"Refer to superclass "),Vn=r(Ea,"A",{href:!0});var Fh=s(Vn);zi=i(Fh,"BertTokenizer"),Fh.forEach(t),qi=i(Ea," for usage examples and documentation concerning parameters."),Ea.forEach(t),lo.forEach(t),Ks=l(o),ht=r(o,"H2",{class:!0});var Ra=s(ht);Ot=r(Ra,"A",{id:!0,class:!0,href:!0});var Ch=s(Ot);Nr=r(Ch,"SPAN",{});var Ah=s(Nr);k(To.$$.fragment,Ah),Ah.forEach(t),Ch.forEach(t),Fi=l(Ra),Or=r(Ra,"SPAN",{});var Nh=s(Or);Ci=i(Nh,"DPRContextEncoderTokenizerFast"),Nh.forEach(t),Ra.forEach(t),Ys=l(o),Me=r(o,"DIV",{class:!0});var co=s(Me);k(ko.$$.fragment,co),Ai=l(co),bo=r(co,"P",{});var Da=s(bo);Ni=i(Da,"Construct a \u201Cfast\u201D DPRContextEncoder tokenizer (backed by HuggingFace\u2019s "),jr=r(Da,"EM",{});var Oh=s(jr);Oi=i(Oh,"tokenizers"),Oh.forEach(t),ji=i(Da," library)."),Da.forEach(t),Mi=l(co),jt=r(co,"P",{});var Hs=s(jt);Un=r(Hs,"A",{href:!0});var jh=s(Un);Qi=i(jh,"DPRContextEncoderTokenizerFast"),jh.forEach(t),Li=i(Hs," is identical to "),Kn=r(Hs,"A",{href:!0});var Mh=s(Kn);Ii=i(Mh,"BertTokenizerFast"),Mh.forEach(t),Si=i(Hs,` and runs end-to-end tokenization:
punctuation splitting and wordpiece.`),Hs.forEach(t),Hi=l(co),Po=r(co,"P",{});var $a=s(Po);Bi=i($a,"Refer to superclass "),Yn=r($a,"A",{href:!0});var Qh=s(Yn);Wi=i(Qh,"BertTokenizerFast"),Qh.forEach(t),Vi=i($a," for usage examples and documentation concerning parameters."),$a.forEach(t),co.forEach(t),Xs=l(o),ft=r(o,"H2",{class:!0});var ya=s(ft);Mt=r(ya,"A",{id:!0,class:!0,href:!0});var Lh=s(Mt);Mr=r(Lh,"SPAN",{});var Ih=s(Mr);k(wo.$$.fragment,Ih),Ih.forEach(t),Lh.forEach(t),Ui=l(ya),Qr=r(ya,"SPAN",{});var Sh=s(Qr);Ki=i(Sh,"DPRQuestionEncoderTokenizer"),Sh.forEach(t),ya.forEach(t),Js=l(o),Qe=r(o,"DIV",{class:!0});var po=s(Qe);k(Eo.$$.fragment,po),Yi=l(po),Lr=r(po,"P",{});var Hh=s(Lr);Xi=i(Hh,"Constructs a DPRQuestionEncoder tokenizer."),Hh.forEach(t),Ji=l(po),Qt=r(po,"P",{});var Bs=s(Qt);Xn=r(Bs,"A",{href:!0});var Bh=s(Xn);Gi=i(Bh,"DPRQuestionEncoderTokenizer"),Bh.forEach(t),Zi=i(Bs," is identical to "),Jn=r(Bs,"A",{href:!0});var Wh=s(Jn);ed=i(Wh,"BertTokenizer"),Wh.forEach(t),td=i(Bs,` and runs end-to-end tokenization: punctuation
splitting and wordpiece.`),Bs.forEach(t),od=l(po),Ro=r(po,"P",{});var xa=s(Ro);nd=i(xa,"Refer to superclass "),Gn=r(xa,"A",{href:!0});var Vh=s(Gn);rd=i(Vh,"BertTokenizer"),Vh.forEach(t),sd=i(xa," for usage examples and documentation concerning parameters."),xa.forEach(t),po.forEach(t),Gs=l(o),ut=r(o,"H2",{class:!0});var za=s(ut);Lt=r(za,"A",{id:!0,class:!0,href:!0});var Uh=s(Lt);Ir=r(Uh,"SPAN",{});var Kh=s(Ir);k(Do.$$.fragment,Kh),Kh.forEach(t),Uh.forEach(t),ad=l(za),Sr=r(za,"SPAN",{});var Yh=s(Sr);id=i(Yh,"DPRQuestionEncoderTokenizerFast"),Yh.forEach(t),za.forEach(t),Zs=l(o),Le=r(o,"DIV",{class:!0});var ho=s(Le);k($o.$$.fragment,ho),dd=l(ho),yo=r(ho,"P",{});var qa=s(yo);ld=i(qa,"Constructs a \u201Cfast\u201D DPRQuestionEncoder tokenizer (backed by HuggingFace\u2019s "),Hr=r(qa,"EM",{});var Xh=s(Hr);cd=i(Xh,"tokenizers"),Xh.forEach(t),pd=i(qa," library)."),qa.forEach(t),hd=l(ho),It=r(ho,"P",{});var Ws=s(It);Zn=r(Ws,"A",{href:!0});var Jh=s(Zn);fd=i(Jh,"DPRQuestionEncoderTokenizerFast"),Jh.forEach(t),ud=i(Ws," is identical to "),er=r(Ws,"A",{href:!0});var Gh=s(er);md=i(Gh,"BertTokenizerFast"),Gh.forEach(t),gd=i(Ws,` and runs end-to-end tokenization:
punctuation splitting and wordpiece.`),Ws.forEach(t),_d=l(ho),xo=r(ho,"P",{});var Fa=s(xo);vd=i(Fa,"Refer to superclass "),tr=r(Fa,"A",{href:!0});var Zh=s(tr);Td=i(Zh,"BertTokenizerFast"),Zh.forEach(t),kd=i(Fa," for usage examples and documentation concerning parameters."),Fa.forEach(t),ho.forEach(t),ea=l(o),mt=r(o,"H2",{class:!0});var Ca=s(mt);St=r(Ca,"A",{id:!0,class:!0,href:!0});var ef=s(St);Br=r(ef,"SPAN",{});var tf=s(Br);k(zo.$$.fragment,tf),tf.forEach(t),ef.forEach(t),bd=l(Ca),Wr=r(Ca,"SPAN",{});var of=s(Wr);Pd=i(of,"DPRReaderTokenizer"),of.forEach(t),Ca.forEach(t),ta=l(o),ie=r(o,"DIV",{class:!0});var Ue=s(ie);k(qo.$$.fragment,Ue),wd=l(Ue),Vr=r(Ue,"P",{});var nf=s(Vr);Ed=i(nf,"Construct a DPRReader tokenizer."),nf.forEach(t),Rd=l(Ue),et=r(Ue,"P",{});var jn=s(et);or=r(jn,"A",{href:!0});var rf=s(or);Dd=i(rf,"DPRReaderTokenizer"),rf.forEach(t),$d=i(jn," is almost identical to "),nr=r(jn,"A",{href:!0});var sf=s(nr);yd=i(sf,"BertTokenizer"),sf.forEach(t),xd=i(jn,` and runs end-to-end tokenization: punctuation
splitting and wordpiece. The difference is that is has three inputs strings: question, titles and texts that are
combined to be fed to the `),rr=r(jn,"A",{href:!0});var af=s(rr);zd=i(af,"DPRReader"),af.forEach(t),qd=i(jn," model."),jn.forEach(t),Fd=l(Ue),Fo=r(Ue,"P",{});var Aa=s(Fo);Cd=i(Aa,"Refer to superclass "),sr=r(Aa,"A",{href:!0});var df=s(sr);Ad=i(df,"BertTokenizer"),df.forEach(t),Nd=i(Aa," for usage examples and documentation concerning parameters."),Aa.forEach(t),Od=l(Ue),Ge=r(Ue,"P",{});var fo=s(Ge);jd=i(fo,"Return a dictionary with the token ids of the input strings and other information to give to "),Ur=r(fo,"CODE",{});var lf=s(Ur);Md=i(lf,".decode_best_spans"),lf.forEach(t),Qd=i(fo,`.
It converts the strings of a question and different passages (title and text) in a sequence of IDs (integers),
using the tokenizer and vocabulary. The resulting `),Kr=r(fo,"CODE",{});var cf=s(Kr);Ld=i(cf,"input_ids"),cf.forEach(t),Id=i(fo," is a matrix of size "),Yr=r(fo,"CODE",{});var pf=s(Yr);Sd=i(pf,"(n_passages, sequence_length)"),pf.forEach(t),Hd=i(fo,`
with the format:`),fo.forEach(t),Bd=l(Ue),k(Co.$$.fragment,Ue),Ue.forEach(t),oa=l(o),gt=r(o,"H2",{class:!0});var Na=s(gt);Ht=r(Na,"A",{id:!0,class:!0,href:!0});var hf=s(Ht);Xr=r(hf,"SPAN",{});var ff=s(Xr);k(Ao.$$.fragment,ff),ff.forEach(t),hf.forEach(t),Wd=l(Na),Jr=r(Na,"SPAN",{});var uf=s(Jr);Vd=i(uf,"DPRReaderTokenizerFast"),uf.forEach(t),Na.forEach(t),na=l(o),de=r(o,"DIV",{class:!0});var Ke=s(de);k(No.$$.fragment,Ke),Ud=l(Ke),Oo=r(Ke,"P",{});var Oa=s(Oo);Kd=i(Oa,"Constructs a \u201Cfast\u201D DPRReader tokenizer (backed by HuggingFace\u2019s "),Gr=r(Oa,"EM",{});var mf=s(Gr);Yd=i(mf,"tokenizers"),mf.forEach(t),Xd=i(Oa," library)."),Oa.forEach(t),Jd=l(Ke),tt=r(Ke,"P",{});var Mn=s(tt);ar=r(Mn,"A",{href:!0});var gf=s(ar);Gd=i(gf,"DPRReaderTokenizerFast"),gf.forEach(t),Zd=i(Mn," is almost identical to "),ir=r(Mn,"A",{href:!0});var _f=s(ir);el=i(_f,"BertTokenizerFast"),_f.forEach(t),tl=i(Mn,` and runs end-to-end tokenization:
punctuation splitting and wordpiece. The difference is that is has three inputs strings: question, titles and texts
that are combined to be fed to the `),dr=r(Mn,"A",{href:!0});var vf=s(dr);ol=i(vf,"DPRReader"),vf.forEach(t),nl=i(Mn," model."),Mn.forEach(t),rl=l(Ke),jo=r(Ke,"P",{});var ja=s(jo);sl=i(ja,"Refer to superclass "),lr=r(ja,"A",{href:!0});var Tf=s(lr);al=i(Tf,"BertTokenizerFast"),Tf.forEach(t),il=i(ja," for usage examples and documentation concerning parameters."),ja.forEach(t),dl=l(Ke),Ze=r(Ke,"P",{});var uo=s(Ze);ll=i(uo,"Return a dictionary with the token ids of the input strings and other information to give to "),Zr=r(uo,"CODE",{});var kf=s(Zr);cl=i(kf,".decode_best_spans"),kf.forEach(t),pl=i(uo,`.
It converts the strings of a question and different passages (title and text) in a sequence of IDs (integers),
using the tokenizer and vocabulary. The resulting `),es=r(uo,"CODE",{});var bf=s(es);hl=i(bf,"input_ids"),bf.forEach(t),fl=i(uo," is a matrix of size "),ts=r(uo,"CODE",{});var Pf=s(ts);ul=i(Pf,"(n_passages, sequence_length)"),Pf.forEach(t),ml=i(uo,`
with the format:`),uo.forEach(t),gl=l(Ke),os=r(Ke,"P",{});var wf=s(os);_l=i(wf,"[CLS] <question token ids> [SEP] <titles ids> [SEP] <texts ids>"),wf.forEach(t),Ke.forEach(t),ra=l(o),_t=r(o,"H2",{class:!0});var Ma=s(_t);Bt=r(Ma,"A",{id:!0,class:!0,href:!0});var Ef=s(Bt);ns=r(Ef,"SPAN",{});var Rf=s(ns);k(Mo.$$.fragment,Rf),Rf.forEach(t),Ef.forEach(t),vl=l(Ma),rs=r(Ma,"SPAN",{});var Df=s(rs);Tl=i(Df,"DPR specific outputs"),Df.forEach(t),Ma.forEach(t),sa=l(o),vt=r(o,"DIV",{class:!0});var Qa=s(vt);k(Qo.$$.fragment,Qa),kl=l(Qa),Lo=r(Qa,"P",{});var La=s(Lo);bl=i(La,"Class for outputs of "),cr=r(La,"A",{href:!0});var $f=s(cr);Pl=i($f,"DPRQuestionEncoder"),$f.forEach(t),wl=i(La,"."),La.forEach(t),Qa.forEach(t),aa=l(o),Tt=r(o,"DIV",{class:!0});var Ia=s(Tt);k(Io.$$.fragment,Ia),El=l(Ia),So=r(Ia,"P",{});var Sa=s(So);Rl=i(Sa,"Class for outputs of "),pr=r(Sa,"A",{href:!0});var yf=s(pr);Dl=i(yf,"DPRQuestionEncoder"),yf.forEach(t),$l=i(Sa,"."),Sa.forEach(t),Ia.forEach(t),ia=l(o),kt=r(o,"DIV",{class:!0});var Ha=s(kt);k(Ho.$$.fragment,Ha),yl=l(Ha),Bo=r(Ha,"P",{});var Ba=s(Bo);xl=i(Ba,"Class for outputs of "),hr=r(Ba,"A",{href:!0});var xf=s(hr);zl=i(xf,"DPRQuestionEncoder"),xf.forEach(t),ql=i(Ba,"."),Ba.forEach(t),Ha.forEach(t),da=l(o),bt=r(o,"H2",{class:!0});var Wa=s(bt);Wt=r(Wa,"A",{id:!0,class:!0,href:!0});var zf=s(Wt);ss=r(zf,"SPAN",{});var qf=s(ss);k(Wo.$$.fragment,qf),qf.forEach(t),zf.forEach(t),Fl=l(Wa),as=r(Wa,"SPAN",{});var Ff=s(as);Cl=i(Ff,"DPRContextEncoder"),Ff.forEach(t),Wa.forEach(t),la=l(o),De=r(o,"DIV",{class:!0});var ot=s(De);k(Vo.$$.fragment,ot),Al=l(ot),is=r(ot,"P",{});var Cf=s(is);Nl=i(Cf,"The bare DPRContextEncoder transformer outputting pooler outputs as context representations."),Cf.forEach(t),Ol=l(ot),Uo=r(ot,"P",{});var Va=s(Uo);jl=i(Va,"This model inherits from "),fr=r(Va,"A",{href:!0});var Af=s(fr);Ml=i(Af,"PreTrainedModel"),Af.forEach(t),Ql=i(Va,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Va.forEach(t),Ll=l(ot),Ko=r(ot,"P",{});var Ua=s(Ko);Il=i(Ua,"This model is also a PyTorch "),Yo=r(Ua,"A",{href:!0,rel:!0});var Nf=s(Yo);Sl=i(Nf,"torch.nn.Module"),Nf.forEach(t),Hl=i(Ua,` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),Ua.forEach(t),Bl=l(ot),ze=r(ot,"DIV",{class:!0});var nt=s(ze);k(Xo.$$.fragment,nt),Wl=l(nt),Pt=r(nt,"P",{});var Dr=s(Pt);Vl=i(Dr,"The "),ur=r(Dr,"A",{href:!0});var Of=s(ur);Ul=i(Of,"DPRContextEncoder"),Of.forEach(t),Kl=i(Dr," forward method, overrides the "),ds=r(Dr,"CODE",{});var jf=s(ds);Yl=i(jf,"__call__"),jf.forEach(t),Xl=i(Dr," special method."),Dr.forEach(t),Jl=l(nt),k(Vt.$$.fragment,nt),Gl=l(nt),ls=r(nt,"P",{});var Mf=s(ls);Zl=i(Mf,"Examples:"),Mf.forEach(t),ec=l(nt),k(Jo.$$.fragment,nt),nt.forEach(t),ot.forEach(t),ca=l(o),wt=r(o,"H2",{class:!0});var Ka=s(wt);Ut=r(Ka,"A",{id:!0,class:!0,href:!0});var Qf=s(Ut);cs=r(Qf,"SPAN",{});var Lf=s(cs);k(Go.$$.fragment,Lf),Lf.forEach(t),Qf.forEach(t),tc=l(Ka),ps=r(Ka,"SPAN",{});var If=s(ps);oc=i(If,"DPRQuestionEncoder"),If.forEach(t),Ka.forEach(t),pa=l(o),$e=r(o,"DIV",{class:!0});var rt=s($e);k(Zo.$$.fragment,rt),nc=l(rt),hs=r(rt,"P",{});var Sf=s(hs);rc=i(Sf,"The bare DPRQuestionEncoder transformer outputting pooler outputs as question representations."),Sf.forEach(t),sc=l(rt),en=r(rt,"P",{});var Ya=s(en);ac=i(Ya,"This model inherits from "),mr=r(Ya,"A",{href:!0});var Hf=s(mr);ic=i(Hf,"PreTrainedModel"),Hf.forEach(t),dc=i(Ya,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Ya.forEach(t),lc=l(rt),tn=r(rt,"P",{});var Xa=s(tn);cc=i(Xa,"This model is also a PyTorch "),on=r(Xa,"A",{href:!0,rel:!0});var Bf=s(on);pc=i(Bf,"torch.nn.Module"),Bf.forEach(t),hc=i(Xa,` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),Xa.forEach(t),fc=l(rt),qe=r(rt,"DIV",{class:!0});var st=s(qe);k(nn.$$.fragment,st),uc=l(st),Et=r(st,"P",{});var $r=s(Et);mc=i($r,"The "),gr=r($r,"A",{href:!0});var Wf=s(gr);gc=i(Wf,"DPRQuestionEncoder"),Wf.forEach(t),_c=i($r," forward method, overrides the "),fs=r($r,"CODE",{});var Vf=s(fs);vc=i(Vf,"__call__"),Vf.forEach(t),Tc=i($r," special method."),$r.forEach(t),kc=l(st),k(Kt.$$.fragment,st),bc=l(st),us=r(st,"P",{});var Uf=s(us);Pc=i(Uf,"Examples:"),Uf.forEach(t),wc=l(st),k(rn.$$.fragment,st),st.forEach(t),rt.forEach(t),ha=l(o),Rt=r(o,"H2",{class:!0});var Ja=s(Rt);Yt=r(Ja,"A",{id:!0,class:!0,href:!0});var Kf=s(Yt);ms=r(Kf,"SPAN",{});var Yf=s(ms);k(sn.$$.fragment,Yf),Yf.forEach(t),Kf.forEach(t),Ec=l(Ja),gs=r(Ja,"SPAN",{});var Xf=s(gs);Rc=i(Xf,"DPRReader"),Xf.forEach(t),Ja.forEach(t),fa=l(o),ye=r(o,"DIV",{class:!0});var at=s(ye);k(an.$$.fragment,at),Dc=l(at),_s=r(at,"P",{});var Jf=s(_s);$c=i(Jf,"The bare DPRReader transformer outputting span predictions."),Jf.forEach(t),yc=l(at),dn=r(at,"P",{});var Ga=s(dn);xc=i(Ga,"This model inherits from "),_r=r(Ga,"A",{href:!0});var Gf=s(_r);zc=i(Gf,"PreTrainedModel"),Gf.forEach(t),qc=i(Ga,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Ga.forEach(t),Fc=l(at),ln=r(at,"P",{});var Za=s(ln);Cc=i(Za,"This model is also a PyTorch "),cn=r(Za,"A",{href:!0,rel:!0});var Zf=s(cn);Ac=i(Zf,"torch.nn.Module"),Zf.forEach(t),Nc=i(Za,` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),Za.forEach(t),Oc=l(at),Fe=r(at,"DIV",{class:!0});var it=s(Fe);k(pn.$$.fragment,it),jc=l(it),Dt=r(it,"P",{});var yr=s(Dt);Mc=i(yr,"The "),vr=r(yr,"A",{href:!0});var eu=s(vr);Qc=i(eu,"DPRReader"),eu.forEach(t),Lc=i(yr," forward method, overrides the "),vs=r(yr,"CODE",{});var tu=s(vs);Ic=i(tu,"__call__"),tu.forEach(t),Sc=i(yr," special method."),yr.forEach(t),Hc=l(it),k(Xt.$$.fragment,it),Bc=l(it),Ts=r(it,"P",{});var ou=s(Ts);Wc=i(ou,"Examples:"),ou.forEach(t),Vc=l(it),k(hn.$$.fragment,it),it.forEach(t),at.forEach(t),ua=l(o),$t=r(o,"H2",{class:!0});var ei=s($t);Jt=r(ei,"A",{id:!0,class:!0,href:!0});var nu=s(Jt);ks=r(nu,"SPAN",{});var ru=s(ks);k(fn.$$.fragment,ru),ru.forEach(t),nu.forEach(t),Uc=l(ei),bs=r(ei,"SPAN",{});var su=s(bs);Kc=i(su,"TFDPRContextEncoder"),su.forEach(t),ei.forEach(t),ma=l(o),le=r(o,"DIV",{class:!0});var Ye=s(le);k(un.$$.fragment,Ye),Yc=l(Ye),Ps=r(Ye,"P",{});var au=s(Ps);Xc=i(au,"The bare DPRContextEncoder transformer outputting pooler outputs as context representations."),au.forEach(t),Jc=l(Ye),mn=r(Ye,"P",{});var ti=s(mn);Gc=i(ti,"This model inherits from "),Tr=r(ti,"A",{href:!0});var iu=s(Tr);Zc=i(iu,"TFPreTrainedModel"),iu.forEach(t),ep=i(ti,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),ti.forEach(t),tp=l(Ye),gn=r(Ye,"P",{});var oi=s(gn);op=i(oi,"This model is also a Tensorflow "),_n=r(oi,"A",{href:!0,rel:!0});var du=s(_n);np=i(du,"tf.keras.Model"),du.forEach(t),rp=i(oi,`
subclass. Use it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to
general usage and behavior.`),oi.forEach(t),sp=l(Ye),k(Gt.$$.fragment,Ye),ap=l(Ye),Ce=r(Ye,"DIV",{class:!0});var dt=s(Ce);k(vn.$$.fragment,dt),ip=l(dt),yt=r(dt,"P",{});var xr=s(yt);dp=i(xr,"The "),kr=r(xr,"A",{href:!0});var lu=s(kr);lp=i(lu,"TFDPRContextEncoder"),lu.forEach(t),cp=i(xr," forward method, overrides the "),ws=r(xr,"CODE",{});var cu=s(ws);pp=i(cu,"__call__"),cu.forEach(t),hp=i(xr," special method."),xr.forEach(t),fp=l(dt),k(Zt.$$.fragment,dt),up=l(dt),Es=r(dt,"P",{});var pu=s(Es);mp=i(pu,"Examples:"),pu.forEach(t),gp=l(dt),k(Tn.$$.fragment,dt),dt.forEach(t),Ye.forEach(t),ga=l(o),xt=r(o,"H2",{class:!0});var ni=s(xt);eo=r(ni,"A",{id:!0,class:!0,href:!0});var hu=s(eo);Rs=r(hu,"SPAN",{});var fu=s(Rs);k(kn.$$.fragment,fu),fu.forEach(t),hu.forEach(t),_p=l(ni),Ds=r(ni,"SPAN",{});var uu=s(Ds);vp=i(uu,"TFDPRQuestionEncoder"),uu.forEach(t),ni.forEach(t),_a=l(o),ce=r(o,"DIV",{class:!0});var Xe=s(ce);k(bn.$$.fragment,Xe),Tp=l(Xe),$s=r(Xe,"P",{});var mu=s($s);kp=i(mu,"The bare DPRQuestionEncoder transformer outputting pooler outputs as question representations."),mu.forEach(t),bp=l(Xe),Pn=r(Xe,"P",{});var ri=s(Pn);Pp=i(ri,"This model inherits from "),br=r(ri,"A",{href:!0});var gu=s(br);wp=i(gu,"TFPreTrainedModel"),gu.forEach(t),Ep=i(ri,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),ri.forEach(t),Rp=l(Xe),wn=r(Xe,"P",{});var si=s(wn);Dp=i(si,"This model is also a Tensorflow "),En=r(si,"A",{href:!0,rel:!0});var _u=s(En);$p=i(_u,"tf.keras.Model"),_u.forEach(t),yp=i(si,`
subclass. Use it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to
general usage and behavior.`),si.forEach(t),xp=l(Xe),k(to.$$.fragment,Xe),zp=l(Xe),Ae=r(Xe,"DIV",{class:!0});var lt=s(Ae);k(Rn.$$.fragment,lt),qp=l(lt),zt=r(lt,"P",{});var zr=s(zt);Fp=i(zr,"The "),Pr=r(zr,"A",{href:!0});var vu=s(Pr);Cp=i(vu,"TFDPRQuestionEncoder"),vu.forEach(t),Ap=i(zr," forward method, overrides the "),ys=r(zr,"CODE",{});var Tu=s(ys);Np=i(Tu,"__call__"),Tu.forEach(t),Op=i(zr," special method."),zr.forEach(t),jp=l(lt),k(oo.$$.fragment,lt),Mp=l(lt),xs=r(lt,"P",{});var ku=s(xs);Qp=i(ku,"Examples:"),ku.forEach(t),Lp=l(lt),k(Dn.$$.fragment,lt),lt.forEach(t),Xe.forEach(t),va=l(o),qt=r(o,"H2",{class:!0});var ai=s(qt);no=r(ai,"A",{id:!0,class:!0,href:!0});var bu=s(no);zs=r(bu,"SPAN",{});var Pu=s(zs);k($n.$$.fragment,Pu),Pu.forEach(t),bu.forEach(t),Ip=l(ai),qs=r(ai,"SPAN",{});var wu=s(qs);Sp=i(wu,"TFDPRReader"),wu.forEach(t),ai.forEach(t),Ta=l(o),pe=r(o,"DIV",{class:!0});var Je=s(pe);k(yn.$$.fragment,Je),Hp=l(Je),Fs=r(Je,"P",{});var Eu=s(Fs);Bp=i(Eu,"The bare DPRReader transformer outputting span predictions."),Eu.forEach(t),Wp=l(Je),xn=r(Je,"P",{});var ii=s(xn);Vp=i(ii,"This model inherits from "),wr=r(ii,"A",{href:!0});var Ru=s(wr);Up=i(Ru,"TFPreTrainedModel"),Ru.forEach(t),Kp=i(ii,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),ii.forEach(t),Yp=l(Je),zn=r(Je,"P",{});var di=s(zn);Xp=i(di,"This model is also a Tensorflow "),qn=r(di,"A",{href:!0,rel:!0});var Du=s(qn);Jp=i(Du,"tf.keras.Model"),Du.forEach(t),Gp=i(di,`
subclass. Use it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to
general usage and behavior.`),di.forEach(t),Zp=l(Je),k(ro.$$.fragment,Je),eh=l(Je),Ne=r(Je,"DIV",{class:!0});var ct=s(Ne);k(Fn.$$.fragment,ct),th=l(ct),Ft=r(ct,"P",{});var qr=s(Ft);oh=i(qr,"The "),Er=r(qr,"A",{href:!0});var $u=s(Er);nh=i($u,"TFDPRReader"),$u.forEach(t),rh=i(qr," forward method, overrides the "),Cs=r(qr,"CODE",{});var yu=s(Cs);sh=i(yu,"__call__"),yu.forEach(t),ah=i(qr," special method."),qr.forEach(t),ih=l(ct),k(so.$$.fragment,ct),dh=l(ct),As=r(ct,"P",{});var xu=s(As);lh=i(xu,"Examples:"),xu.forEach(t),ch=l(ct),k(Cn.$$.fragment,ct),ct.forEach(t),Je.forEach(t),this.h()},h(){c(u,"name","hf:doc:metadata"),c(u,"content",JSON.stringify(Bu)),c(g,"id","dpr"),c(g,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(g,"href","#dpr"),c(m,"class","relative group"),c(J,"id","overview"),c(J,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(J,"href","#overview"),c($,"class","relative group"),c(te,"href","https://arxiv.org/abs/2004.04906"),c(te,"rel","nofollow"),c(O,"href","https://huggingface.co/lhoestq"),c(O,"rel","nofollow"),c(j,"href","https://github.com/facebookresearch/DPR"),c(j,"rel","nofollow"),c(N,"id","transformers.DPRConfig"),c(N,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(N,"href","#transformers.DPRConfig"),c(K,"class","relative group"),c(A,"href","/docs/transformers/pr_15943/en/model_doc/dpr#transformers.DPRConfig"),c(Ln,"href","/docs/transformers/pr_15943/en/model_doc/dpr#transformers.DPRContextEncoder"),c(In,"href","/docs/transformers/pr_15943/en/model_doc/dpr#transformers.DPRQuestionEncoder"),c(Sn,"href","/docs/transformers/pr_15943/en/model_doc/dpr#transformers.DPRReader"),c(Hn,"href","/docs/transformers/pr_15943/en/model_doc/bert#transformers.BertConfig"),c(F,"class","docstring"),c(At,"id","transformers.DPRContextEncoderTokenizer"),c(At,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(At,"href","#transformers.DPRContextEncoderTokenizer"),c(pt,"class","relative group"),c(Bn,"href","/docs/transformers/pr_15943/en/model_doc/dpr#transformers.DPRContextEncoderTokenizer"),c(Wn,"href","/docs/transformers/pr_15943/en/model_doc/bert#transformers.BertTokenizer"),c(Vn,"href","/docs/transformers/pr_15943/en/model_doc/bert#transformers.BertTokenizer"),c(je,"class","docstring"),c(Ot,"id","transformers.DPRContextEncoderTokenizerFast"),c(Ot,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Ot,"href","#transformers.DPRContextEncoderTokenizerFast"),c(ht,"class","relative group"),c(Un,"href","/docs/transformers/pr_15943/en/model_doc/dpr#transformers.DPRContextEncoderTokenizerFast"),c(Kn,"href","/docs/transformers/pr_15943/en/model_doc/bert#transformers.BertTokenizerFast"),c(Yn,"href","/docs/transformers/pr_15943/en/model_doc/bert#transformers.BertTokenizerFast"),c(Me,"class","docstring"),c(Mt,"id","transformers.DPRQuestionEncoderTokenizer"),c(Mt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Mt,"href","#transformers.DPRQuestionEncoderTokenizer"),c(ft,"class","relative group"),c(Xn,"href","/docs/transformers/pr_15943/en/model_doc/dpr#transformers.DPRQuestionEncoderTokenizer"),c(Jn,"href","/docs/transformers/pr_15943/en/model_doc/bert#transformers.BertTokenizer"),c(Gn,"href","/docs/transformers/pr_15943/en/model_doc/bert#transformers.BertTokenizer"),c(Qe,"class","docstring"),c(Lt,"id","transformers.DPRQuestionEncoderTokenizerFast"),c(Lt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Lt,"href","#transformers.DPRQuestionEncoderTokenizerFast"),c(ut,"class","relative group"),c(Zn,"href","/docs/transformers/pr_15943/en/model_doc/dpr#transformers.DPRQuestionEncoderTokenizerFast"),c(er,"href","/docs/transformers/pr_15943/en/model_doc/bert#transformers.BertTokenizerFast"),c(tr,"href","/docs/transformers/pr_15943/en/model_doc/bert#transformers.BertTokenizerFast"),c(Le,"class","docstring"),c(St,"id","transformers.DPRReaderTokenizer"),c(St,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(St,"href","#transformers.DPRReaderTokenizer"),c(mt,"class","relative group"),c(or,"href","/docs/transformers/pr_15943/en/model_doc/dpr#transformers.DPRReaderTokenizer"),c(nr,"href","/docs/transformers/pr_15943/en/model_doc/bert#transformers.BertTokenizer"),c(rr,"href","/docs/transformers/pr_15943/en/model_doc/dpr#transformers.DPRReader"),c(sr,"href","/docs/transformers/pr_15943/en/model_doc/bert#transformers.BertTokenizer"),c(ie,"class","docstring"),c(Ht,"id","transformers.DPRReaderTokenizerFast"),c(Ht,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Ht,"href","#transformers.DPRReaderTokenizerFast"),c(gt,"class","relative group"),c(ar,"href","/docs/transformers/pr_15943/en/model_doc/dpr#transformers.DPRReaderTokenizerFast"),c(ir,"href","/docs/transformers/pr_15943/en/model_doc/bert#transformers.BertTokenizerFast"),c(dr,"href","/docs/transformers/pr_15943/en/model_doc/dpr#transformers.DPRReader"),c(lr,"href","/docs/transformers/pr_15943/en/model_doc/bert#transformers.BertTokenizerFast"),c(de,"class","docstring"),c(Bt,"id","transformers.models.dpr.modeling_dpr.DPRContextEncoderOutput"),c(Bt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Bt,"href","#transformers.models.dpr.modeling_dpr.DPRContextEncoderOutput"),c(_t,"class","relative group"),c(cr,"href","/docs/transformers/pr_15943/en/model_doc/dpr#transformers.DPRQuestionEncoder"),c(vt,"class","docstring"),c(pr,"href","/docs/transformers/pr_15943/en/model_doc/dpr#transformers.DPRQuestionEncoder"),c(Tt,"class","docstring"),c(hr,"href","/docs/transformers/pr_15943/en/model_doc/dpr#transformers.DPRQuestionEncoder"),c(kt,"class","docstring"),c(Wt,"id","transformers.DPRContextEncoder"),c(Wt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Wt,"href","#transformers.DPRContextEncoder"),c(bt,"class","relative group"),c(fr,"href","/docs/transformers/pr_15943/en/main_classes/model#transformers.PreTrainedModel"),c(Yo,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(Yo,"rel","nofollow"),c(ur,"href","/docs/transformers/pr_15943/en/model_doc/dpr#transformers.DPRContextEncoder"),c(ze,"class","docstring"),c(De,"class","docstring"),c(Ut,"id","transformers.DPRQuestionEncoder"),c(Ut,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Ut,"href","#transformers.DPRQuestionEncoder"),c(wt,"class","relative group"),c(mr,"href","/docs/transformers/pr_15943/en/main_classes/model#transformers.PreTrainedModel"),c(on,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(on,"rel","nofollow"),c(gr,"href","/docs/transformers/pr_15943/en/model_doc/dpr#transformers.DPRQuestionEncoder"),c(qe,"class","docstring"),c($e,"class","docstring"),c(Yt,"id","transformers.DPRReader"),c(Yt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Yt,"href","#transformers.DPRReader"),c(Rt,"class","relative group"),c(_r,"href","/docs/transformers/pr_15943/en/main_classes/model#transformers.PreTrainedModel"),c(cn,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(cn,"rel","nofollow"),c(vr,"href","/docs/transformers/pr_15943/en/model_doc/dpr#transformers.DPRReader"),c(Fe,"class","docstring"),c(ye,"class","docstring"),c(Jt,"id","transformers.TFDPRContextEncoder"),c(Jt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Jt,"href","#transformers.TFDPRContextEncoder"),c($t,"class","relative group"),c(Tr,"href","/docs/transformers/pr_15943/en/main_classes/model#transformers.TFPreTrainedModel"),c(_n,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),c(_n,"rel","nofollow"),c(kr,"href","/docs/transformers/pr_15943/en/model_doc/dpr#transformers.TFDPRContextEncoder"),c(Ce,"class","docstring"),c(le,"class","docstring"),c(eo,"id","transformers.TFDPRQuestionEncoder"),c(eo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(eo,"href","#transformers.TFDPRQuestionEncoder"),c(xt,"class","relative group"),c(br,"href","/docs/transformers/pr_15943/en/main_classes/model#transformers.TFPreTrainedModel"),c(En,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),c(En,"rel","nofollow"),c(Pr,"href","/docs/transformers/pr_15943/en/model_doc/dpr#transformers.TFDPRQuestionEncoder"),c(Ae,"class","docstring"),c(ce,"class","docstring"),c(no,"id","transformers.TFDPRReader"),c(no,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(no,"href","#transformers.TFDPRReader"),c(qt,"class","relative group"),c(wr,"href","/docs/transformers/pr_15943/en/main_classes/model#transformers.TFPreTrainedModel"),c(qn,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),c(qn,"rel","nofollow"),c(Er,"href","/docs/transformers/pr_15943/en/model_doc/dpr#transformers.TFDPRReader"),c(Ne,"class","docstring"),c(pe,"class","docstring")},m(o,f){e(document.head,u),p(o,D,f),p(o,m,f),e(m,g),e(g,R),b(v,R,null),e(m,_),e(m,x),e(x,_e),p(o,Z,f),p(o,$,f),e($,J),e(J,L),b(ee,L,null),e($,ve),e($,I),e(I,Te),p(o,he,f),p(o,S,f),e(S,Q),e(S,te),e(te,oe),e(S,q),p(o,C,f),p(o,ne,f),e(ne,U),p(o,fe,f),p(o,re,f),e(re,H),e(H,ke),p(o,ue,f),p(o,z,f),e(z,be),e(z,O),e(O,Pe),e(z,we),e(z,j),e(j,Ee),e(z,Re),p(o,M,f),p(o,K,f),e(K,N),e(N,se),b(h,se,null),e(K,y),e(K,G),e(G,Ie),p(o,Oe,f),p(o,F,f),b(me,F,null),e(F,Se),e(F,ae),e(ae,A),e(A,Y),e(ae,He),e(ae,xe),e(xe,X),e(ae,Be),e(F,We),e(F,B),e(B,Ve),e(B,Ln),e(Ln,li),e(B,ci),e(B,In),e(In,pi),e(B,hi),e(B,Sn),e(Sn,fi),e(B,ui),e(F,mi),e(F,mo),e(mo,gi),e(mo,Hn),e(Hn,_i),e(mo,vi),p(o,Vs,f),p(o,pt,f),e(pt,At),e(At,Fr),b(go,Fr,null),e(pt,Ti),e(pt,Cr),e(Cr,ki),p(o,Us,f),p(o,je,f),b(_o,je,null),e(je,bi),e(je,Ar),e(Ar,Pi),e(je,wi),e(je,Nt),e(Nt,Bn),e(Bn,Ei),e(Nt,Ri),e(Nt,Wn),e(Wn,Di),e(Nt,$i),e(je,yi),e(je,vo),e(vo,xi),e(vo,Vn),e(Vn,zi),e(vo,qi),p(o,Ks,f),p(o,ht,f),e(ht,Ot),e(Ot,Nr),b(To,Nr,null),e(ht,Fi),e(ht,Or),e(Or,Ci),p(o,Ys,f),p(o,Me,f),b(ko,Me,null),e(Me,Ai),e(Me,bo),e(bo,Ni),e(bo,jr),e(jr,Oi),e(bo,ji),e(Me,Mi),e(Me,jt),e(jt,Un),e(Un,Qi),e(jt,Li),e(jt,Kn),e(Kn,Ii),e(jt,Si),e(Me,Hi),e(Me,Po),e(Po,Bi),e(Po,Yn),e(Yn,Wi),e(Po,Vi),p(o,Xs,f),p(o,ft,f),e(ft,Mt),e(Mt,Mr),b(wo,Mr,null),e(ft,Ui),e(ft,Qr),e(Qr,Ki),p(o,Js,f),p(o,Qe,f),b(Eo,Qe,null),e(Qe,Yi),e(Qe,Lr),e(Lr,Xi),e(Qe,Ji),e(Qe,Qt),e(Qt,Xn),e(Xn,Gi),e(Qt,Zi),e(Qt,Jn),e(Jn,ed),e(Qt,td),e(Qe,od),e(Qe,Ro),e(Ro,nd),e(Ro,Gn),e(Gn,rd),e(Ro,sd),p(o,Gs,f),p(o,ut,f),e(ut,Lt),e(Lt,Ir),b(Do,Ir,null),e(ut,ad),e(ut,Sr),e(Sr,id),p(o,Zs,f),p(o,Le,f),b($o,Le,null),e(Le,dd),e(Le,yo),e(yo,ld),e(yo,Hr),e(Hr,cd),e(yo,pd),e(Le,hd),e(Le,It),e(It,Zn),e(Zn,fd),e(It,ud),e(It,er),e(er,md),e(It,gd),e(Le,_d),e(Le,xo),e(xo,vd),e(xo,tr),e(tr,Td),e(xo,kd),p(o,ea,f),p(o,mt,f),e(mt,St),e(St,Br),b(zo,Br,null),e(mt,bd),e(mt,Wr),e(Wr,Pd),p(o,ta,f),p(o,ie,f),b(qo,ie,null),e(ie,wd),e(ie,Vr),e(Vr,Ed),e(ie,Rd),e(ie,et),e(et,or),e(or,Dd),e(et,$d),e(et,nr),e(nr,yd),e(et,xd),e(et,rr),e(rr,zd),e(et,qd),e(ie,Fd),e(ie,Fo),e(Fo,Cd),e(Fo,sr),e(sr,Ad),e(Fo,Nd),e(ie,Od),e(ie,Ge),e(Ge,jd),e(Ge,Ur),e(Ur,Md),e(Ge,Qd),e(Ge,Kr),e(Kr,Ld),e(Ge,Id),e(Ge,Yr),e(Yr,Sd),e(Ge,Hd),e(ie,Bd),b(Co,ie,null),p(o,oa,f),p(o,gt,f),e(gt,Ht),e(Ht,Xr),b(Ao,Xr,null),e(gt,Wd),e(gt,Jr),e(Jr,Vd),p(o,na,f),p(o,de,f),b(No,de,null),e(de,Ud),e(de,Oo),e(Oo,Kd),e(Oo,Gr),e(Gr,Yd),e(Oo,Xd),e(de,Jd),e(de,tt),e(tt,ar),e(ar,Gd),e(tt,Zd),e(tt,ir),e(ir,el),e(tt,tl),e(tt,dr),e(dr,ol),e(tt,nl),e(de,rl),e(de,jo),e(jo,sl),e(jo,lr),e(lr,al),e(jo,il),e(de,dl),e(de,Ze),e(Ze,ll),e(Ze,Zr),e(Zr,cl),e(Ze,pl),e(Ze,es),e(es,hl),e(Ze,fl),e(Ze,ts),e(ts,ul),e(Ze,ml),e(de,gl),e(de,os),e(os,_l),p(o,ra,f),p(o,_t,f),e(_t,Bt),e(Bt,ns),b(Mo,ns,null),e(_t,vl),e(_t,rs),e(rs,Tl),p(o,sa,f),p(o,vt,f),b(Qo,vt,null),e(vt,kl),e(vt,Lo),e(Lo,bl),e(Lo,cr),e(cr,Pl),e(Lo,wl),p(o,aa,f),p(o,Tt,f),b(Io,Tt,null),e(Tt,El),e(Tt,So),e(So,Rl),e(So,pr),e(pr,Dl),e(So,$l),p(o,ia,f),p(o,kt,f),b(Ho,kt,null),e(kt,yl),e(kt,Bo),e(Bo,xl),e(Bo,hr),e(hr,zl),e(Bo,ql),p(o,da,f),p(o,bt,f),e(bt,Wt),e(Wt,ss),b(Wo,ss,null),e(bt,Fl),e(bt,as),e(as,Cl),p(o,la,f),p(o,De,f),b(Vo,De,null),e(De,Al),e(De,is),e(is,Nl),e(De,Ol),e(De,Uo),e(Uo,jl),e(Uo,fr),e(fr,Ml),e(Uo,Ql),e(De,Ll),e(De,Ko),e(Ko,Il),e(Ko,Yo),e(Yo,Sl),e(Ko,Hl),e(De,Bl),e(De,ze),b(Xo,ze,null),e(ze,Wl),e(ze,Pt),e(Pt,Vl),e(Pt,ur),e(ur,Ul),e(Pt,Kl),e(Pt,ds),e(ds,Yl),e(Pt,Xl),e(ze,Jl),b(Vt,ze,null),e(ze,Gl),e(ze,ls),e(ls,Zl),e(ze,ec),b(Jo,ze,null),p(o,ca,f),p(o,wt,f),e(wt,Ut),e(Ut,cs),b(Go,cs,null),e(wt,tc),e(wt,ps),e(ps,oc),p(o,pa,f),p(o,$e,f),b(Zo,$e,null),e($e,nc),e($e,hs),e(hs,rc),e($e,sc),e($e,en),e(en,ac),e(en,mr),e(mr,ic),e(en,dc),e($e,lc),e($e,tn),e(tn,cc),e(tn,on),e(on,pc),e(tn,hc),e($e,fc),e($e,qe),b(nn,qe,null),e(qe,uc),e(qe,Et),e(Et,mc),e(Et,gr),e(gr,gc),e(Et,_c),e(Et,fs),e(fs,vc),e(Et,Tc),e(qe,kc),b(Kt,qe,null),e(qe,bc),e(qe,us),e(us,Pc),e(qe,wc),b(rn,qe,null),p(o,ha,f),p(o,Rt,f),e(Rt,Yt),e(Yt,ms),b(sn,ms,null),e(Rt,Ec),e(Rt,gs),e(gs,Rc),p(o,fa,f),p(o,ye,f),b(an,ye,null),e(ye,Dc),e(ye,_s),e(_s,$c),e(ye,yc),e(ye,dn),e(dn,xc),e(dn,_r),e(_r,zc),e(dn,qc),e(ye,Fc),e(ye,ln),e(ln,Cc),e(ln,cn),e(cn,Ac),e(ln,Nc),e(ye,Oc),e(ye,Fe),b(pn,Fe,null),e(Fe,jc),e(Fe,Dt),e(Dt,Mc),e(Dt,vr),e(vr,Qc),e(Dt,Lc),e(Dt,vs),e(vs,Ic),e(Dt,Sc),e(Fe,Hc),b(Xt,Fe,null),e(Fe,Bc),e(Fe,Ts),e(Ts,Wc),e(Fe,Vc),b(hn,Fe,null),p(o,ua,f),p(o,$t,f),e($t,Jt),e(Jt,ks),b(fn,ks,null),e($t,Uc),e($t,bs),e(bs,Kc),p(o,ma,f),p(o,le,f),b(un,le,null),e(le,Yc),e(le,Ps),e(Ps,Xc),e(le,Jc),e(le,mn),e(mn,Gc),e(mn,Tr),e(Tr,Zc),e(mn,ep),e(le,tp),e(le,gn),e(gn,op),e(gn,_n),e(_n,np),e(gn,rp),e(le,sp),b(Gt,le,null),e(le,ap),e(le,Ce),b(vn,Ce,null),e(Ce,ip),e(Ce,yt),e(yt,dp),e(yt,kr),e(kr,lp),e(yt,cp),e(yt,ws),e(ws,pp),e(yt,hp),e(Ce,fp),b(Zt,Ce,null),e(Ce,up),e(Ce,Es),e(Es,mp),e(Ce,gp),b(Tn,Ce,null),p(o,ga,f),p(o,xt,f),e(xt,eo),e(eo,Rs),b(kn,Rs,null),e(xt,_p),e(xt,Ds),e(Ds,vp),p(o,_a,f),p(o,ce,f),b(bn,ce,null),e(ce,Tp),e(ce,$s),e($s,kp),e(ce,bp),e(ce,Pn),e(Pn,Pp),e(Pn,br),e(br,wp),e(Pn,Ep),e(ce,Rp),e(ce,wn),e(wn,Dp),e(wn,En),e(En,$p),e(wn,yp),e(ce,xp),b(to,ce,null),e(ce,zp),e(ce,Ae),b(Rn,Ae,null),e(Ae,qp),e(Ae,zt),e(zt,Fp),e(zt,Pr),e(Pr,Cp),e(zt,Ap),e(zt,ys),e(ys,Np),e(zt,Op),e(Ae,jp),b(oo,Ae,null),e(Ae,Mp),e(Ae,xs),e(xs,Qp),e(Ae,Lp),b(Dn,Ae,null),p(o,va,f),p(o,qt,f),e(qt,no),e(no,zs),b($n,zs,null),e(qt,Ip),e(qt,qs),e(qs,Sp),p(o,Ta,f),p(o,pe,f),b(yn,pe,null),e(pe,Hp),e(pe,Fs),e(Fs,Bp),e(pe,Wp),e(pe,xn),e(xn,Vp),e(xn,wr),e(wr,Up),e(xn,Kp),e(pe,Yp),e(pe,zn),e(zn,Xp),e(zn,qn),e(qn,Jp),e(zn,Gp),e(pe,Zp),b(ro,pe,null),e(pe,eh),e(pe,Ne),b(Fn,Ne,null),e(Ne,th),e(Ne,Ft),e(Ft,oh),e(Ft,Er),e(Er,nh),e(Ft,rh),e(Ft,Cs),e(Cs,sh),e(Ft,ah),e(Ne,ih),b(so,Ne,null),e(Ne,dh),e(Ne,As),e(As,lh),e(Ne,ch),b(Cn,Ne,null),ka=!0},p(o,[f]){const An={};f&2&&(An.$$scope={dirty:f,ctx:o}),Vt.$set(An);const Ns={};f&2&&(Ns.$$scope={dirty:f,ctx:o}),Kt.$set(Ns);const Os={};f&2&&(Os.$$scope={dirty:f,ctx:o}),Xt.$set(Os);const js={};f&2&&(js.$$scope={dirty:f,ctx:o}),Gt.$set(js);const Nn={};f&2&&(Nn.$$scope={dirty:f,ctx:o}),Zt.$set(Nn);const Ms={};f&2&&(Ms.$$scope={dirty:f,ctx:o}),to.$set(Ms);const Qs={};f&2&&(Qs.$$scope={dirty:f,ctx:o}),oo.$set(Qs);const Ls={};f&2&&(Ls.$$scope={dirty:f,ctx:o}),ro.$set(Ls);const On={};f&2&&(On.$$scope={dirty:f,ctx:o}),so.$set(On)},i(o){ka||(P(v.$$.fragment,o),P(ee.$$.fragment,o),P(h.$$.fragment,o),P(me.$$.fragment,o),P(go.$$.fragment,o),P(_o.$$.fragment,o),P(To.$$.fragment,o),P(ko.$$.fragment,o),P(wo.$$.fragment,o),P(Eo.$$.fragment,o),P(Do.$$.fragment,o),P($o.$$.fragment,o),P(zo.$$.fragment,o),P(qo.$$.fragment,o),P(Co.$$.fragment,o),P(Ao.$$.fragment,o),P(No.$$.fragment,o),P(Mo.$$.fragment,o),P(Qo.$$.fragment,o),P(Io.$$.fragment,o),P(Ho.$$.fragment,o),P(Wo.$$.fragment,o),P(Vo.$$.fragment,o),P(Xo.$$.fragment,o),P(Vt.$$.fragment,o),P(Jo.$$.fragment,o),P(Go.$$.fragment,o),P(Zo.$$.fragment,o),P(nn.$$.fragment,o),P(Kt.$$.fragment,o),P(rn.$$.fragment,o),P(sn.$$.fragment,o),P(an.$$.fragment,o),P(pn.$$.fragment,o),P(Xt.$$.fragment,o),P(hn.$$.fragment,o),P(fn.$$.fragment,o),P(un.$$.fragment,o),P(Gt.$$.fragment,o),P(vn.$$.fragment,o),P(Zt.$$.fragment,o),P(Tn.$$.fragment,o),P(kn.$$.fragment,o),P(bn.$$.fragment,o),P(to.$$.fragment,o),P(Rn.$$.fragment,o),P(oo.$$.fragment,o),P(Dn.$$.fragment,o),P($n.$$.fragment,o),P(yn.$$.fragment,o),P(ro.$$.fragment,o),P(Fn.$$.fragment,o),P(so.$$.fragment,o),P(Cn.$$.fragment,o),ka=!0)},o(o){w(v.$$.fragment,o),w(ee.$$.fragment,o),w(h.$$.fragment,o),w(me.$$.fragment,o),w(go.$$.fragment,o),w(_o.$$.fragment,o),w(To.$$.fragment,o),w(ko.$$.fragment,o),w(wo.$$.fragment,o),w(Eo.$$.fragment,o),w(Do.$$.fragment,o),w($o.$$.fragment,o),w(zo.$$.fragment,o),w(qo.$$.fragment,o),w(Co.$$.fragment,o),w(Ao.$$.fragment,o),w(No.$$.fragment,o),w(Mo.$$.fragment,o),w(Qo.$$.fragment,o),w(Io.$$.fragment,o),w(Ho.$$.fragment,o),w(Wo.$$.fragment,o),w(Vo.$$.fragment,o),w(Xo.$$.fragment,o),w(Vt.$$.fragment,o),w(Jo.$$.fragment,o),w(Go.$$.fragment,o),w(Zo.$$.fragment,o),w(nn.$$.fragment,o),w(Kt.$$.fragment,o),w(rn.$$.fragment,o),w(sn.$$.fragment,o),w(an.$$.fragment,o),w(pn.$$.fragment,o),w(Xt.$$.fragment,o),w(hn.$$.fragment,o),w(fn.$$.fragment,o),w(un.$$.fragment,o),w(Gt.$$.fragment,o),w(vn.$$.fragment,o),w(Zt.$$.fragment,o),w(Tn.$$.fragment,o),w(kn.$$.fragment,o),w(bn.$$.fragment,o),w(to.$$.fragment,o),w(Rn.$$.fragment,o),w(oo.$$.fragment,o),w(Dn.$$.fragment,o),w($n.$$.fragment,o),w(yn.$$.fragment,o),w(ro.$$.fragment,o),w(Fn.$$.fragment,o),w(so.$$.fragment,o),w(Cn.$$.fragment,o),ka=!1},d(o){t(u),o&&t(D),o&&t(m),E(v),o&&t(Z),o&&t($),E(ee),o&&t(he),o&&t(S),o&&t(C),o&&t(ne),o&&t(fe),o&&t(re),o&&t(ue),o&&t(z),o&&t(M),o&&t(K),E(h),o&&t(Oe),o&&t(F),E(me),o&&t(Vs),o&&t(pt),E(go),o&&t(Us),o&&t(je),E(_o),o&&t(Ks),o&&t(ht),E(To),o&&t(Ys),o&&t(Me),E(ko),o&&t(Xs),o&&t(ft),E(wo),o&&t(Js),o&&t(Qe),E(Eo),o&&t(Gs),o&&t(ut),E(Do),o&&t(Zs),o&&t(Le),E($o),o&&t(ea),o&&t(mt),E(zo),o&&t(ta),o&&t(ie),E(qo),E(Co),o&&t(oa),o&&t(gt),E(Ao),o&&t(na),o&&t(de),E(No),o&&t(ra),o&&t(_t),E(Mo),o&&t(sa),o&&t(vt),E(Qo),o&&t(aa),o&&t(Tt),E(Io),o&&t(ia),o&&t(kt),E(Ho),o&&t(da),o&&t(bt),E(Wo),o&&t(la),o&&t(De),E(Vo),E(Xo),E(Vt),E(Jo),o&&t(ca),o&&t(wt),E(Go),o&&t(pa),o&&t($e),E(Zo),E(nn),E(Kt),E(rn),o&&t(ha),o&&t(Rt),E(sn),o&&t(fa),o&&t(ye),E(an),E(pn),E(Xt),E(hn),o&&t(ua),o&&t($t),E(fn),o&&t(ma),o&&t(le),E(un),E(Gt),E(vn),E(Zt),E(Tn),o&&t(ga),o&&t(xt),E(kn),o&&t(_a),o&&t(ce),E(bn),E(to),E(Rn),E(oo),E(Dn),o&&t(va),o&&t(qt),E($n),o&&t(Ta),o&&t(pe),E(yn),E(ro),E(Fn),E(so),E(Cn)}}}const Bu={local:"dpr",sections:[{local:"overview",title:"Overview"},{local:"transformers.DPRConfig",title:"DPRConfig"},{local:"transformers.DPRContextEncoderTokenizer",title:"DPRContextEncoderTokenizer"},{local:"transformers.DPRContextEncoderTokenizerFast",title:"DPRContextEncoderTokenizerFast"},{local:"transformers.DPRQuestionEncoderTokenizer",title:"DPRQuestionEncoderTokenizer"},{local:"transformers.DPRQuestionEncoderTokenizerFast",title:"DPRQuestionEncoderTokenizerFast"},{local:"transformers.DPRReaderTokenizer",title:"DPRReaderTokenizer"},{local:"transformers.DPRReaderTokenizerFast",title:"DPRReaderTokenizerFast"},{local:"transformers.models.dpr.modeling_dpr.DPRContextEncoderOutput",title:"DPR specific outputs"},{local:"transformers.DPRContextEncoder",title:"DPRContextEncoder"},{local:"transformers.DPRQuestionEncoder",title:"DPRQuestionEncoder"},{local:"transformers.DPRReader",title:"DPRReader"},{local:"transformers.TFDPRContextEncoder",title:"TFDPRContextEncoder"},{local:"transformers.TFDPRQuestionEncoder",title:"TFDPRQuestionEncoder"},{local:"transformers.TFDPRReader",title:"TFDPRReader"}],title:"DPR"};function Wu(V,u,D){let{fw:m}=u;return V.$$set=g=>{"fw"in g&&D(0,m=g.fw)},[m]}class Gu extends zu{constructor(u){super();qu(this,u,Wu,Hu,Fu,{fw:0})}}export{Gu as default,Bu as metadata};
