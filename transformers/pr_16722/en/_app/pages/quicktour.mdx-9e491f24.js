import{S as kp,i as yp,s as bp,e as l,k as h,w as b,t as o,M as Ep,c as i,d as s,m as d,a as p,x as E,h as n,b as $,F as t,g as u,y as A,q as T,o as j,B as x,v as Ap,L as qe}from"../chunks/vendor-6b77c823.js";import{T as Ms}from"../chunks/Tip-39098574.js";import{Y as wp}from"../chunks/Youtube-5c6e11e6.js";import{I as mt}from"../chunks/IconCopyLink-7a11ce68.js";import{C as W}from"../chunks/CodeBlock-3a8b25a8.js";import{D as Tp}from"../chunks/DocNotebookDropdown-f2b55cd8.js";import{F as Fs,M as _e}from"../chunks/Markdown-4489c441.js";function jp(P){let a,m;return{c(){a=l("p"),m=o(`All code examples presented in the documentation have a toggle on the top left for PyTorch and TensorFlow. If
not, the code is expected to work for both backends without any change.`)},l(r){a=i(r,"P",{});var c=p(a);m=n(c,`All code examples presented in the documentation have a toggle on the top left for PyTorch and TensorFlow. If
not, the code is expected to work for both backends without any change.`),c.forEach(s)},m(r,c){u(r,a,c),t(a,m)},d(r){r&&s(a)}}}function xp(P){let a,m,r,c,_,v,q,I;return{c(){a=l("p"),m=o("For more details about the "),r=l("a"),c=o("pipeline()"),_=o(" and associated tasks, refer to the documentation "),v=l("a"),q=o("here"),I=o("."),this.h()},l(w){a=i(w,"P",{});var M=p(a);m=n(M,"For more details about the "),r=i(M,"A",{href:!0});var C=p(r);c=n(C,"pipeline()"),C.forEach(s),_=n(M," and associated tasks, refer to the documentation "),v=i(M,"A",{href:!0});var O=p(v);q=n(O,"here"),O.forEach(s),I=n(M,"."),M.forEach(s),this.h()},h(){$(r,"href","/docs/transformers/pr_16722/en/main_classes/pipelines#transformers.pipeline"),$(v,"href","./main_classes/pipelines")},m(w,M){u(w,a,M),t(a,m),t(a,r),t(r,c),t(a,_),t(a,v),t(v,q),t(a,I)},d(w){w&&s(a)}}}function zp(P){let a,m;return a=new W({props:{code:"pip install torch",highlighted:"pip install torch"}}),{c(){b(a.$$.fragment)},l(r){E(a.$$.fragment,r)},m(r,c){A(a,r,c),m=!0},p:qe,i(r){m||(T(a.$$.fragment,r),m=!0)},o(r){j(a.$$.fragment,r),m=!1},d(r){x(a,r)}}}function qp(P){let a,m;return a=new _e({props:{$$slots:{default:[zp]},$$scope:{ctx:P}}}),{c(){b(a.$$.fragment)},l(r){E(a.$$.fragment,r)},m(r,c){A(a,r,c),m=!0},p(r,c){const _={};c&2&&(_.$$scope={dirty:c,ctx:r}),a.$set(_)},i(r){m||(T(a.$$.fragment,r),m=!0)},o(r){j(a.$$.fragment,r),m=!1},d(r){x(a,r)}}}function Pp(P){let a,m;return a=new W({props:{code:"pip install tensorflow",highlighted:"pip install tensorflow"}}),{c(){b(a.$$.fragment)},l(r){E(a.$$.fragment,r)},m(r,c){A(a,r,c),m=!0},p:qe,i(r){m||(T(a.$$.fragment,r),m=!0)},o(r){j(a.$$.fragment,r),m=!1},d(r){x(a,r)}}}function Fp(P){let a,m;return a=new _e({props:{$$slots:{default:[Pp]},$$scope:{ctx:P}}}),{c(){b(a.$$.fragment)},l(r){E(a.$$.fragment,r)},m(r,c){A(a,r,c),m=!0},p(r,c){const _={};c&2&&(_.$$scope={dirty:c,ctx:r}),a.$set(_)},i(r){m||(T(a.$$.fragment,r),m=!0)},o(r){j(a.$$.fragment,r),m=!1},d(r){x(a,r)}}}function Mp(P){let a,m,r,c,_,v,q,I,w,M,C,O,D,L;return D=new W({props:{code:`from transformers import AutoTokenizer, AutoModelForSequenceClassification

model = AutoModelForSequenceClassification.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, AutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSequenceClassification.from_pretrained(model_name)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(model_name)`}}),{c(){a=l("p"),m=o("Use the "),r=l("a"),c=o("AutoModelForSequenceClassification"),_=o(" and "),v=l("a"),q=o("AutoTokenizer"),I=o(" to load the pretrained model and it\u2019s associated tokenizer (more on an "),w=l("code"),M=o("AutoClass"),C=o(" below):"),O=h(),b(D.$$.fragment),this.h()},l(y){a=i(y,"P",{});var S=p(a);m=n(S,"Use the "),r=i(S,"A",{href:!0});var g=p(r);c=n(g,"AutoModelForSequenceClassification"),g.forEach(s),_=n(S," and "),v=i(S,"A",{href:!0});var F=p(v);q=n(F,"AutoTokenizer"),F.forEach(s),I=n(S," to load the pretrained model and it\u2019s associated tokenizer (more on an "),w=i(S,"CODE",{});var R=p(w);M=n(R,"AutoClass"),R.forEach(s),C=n(S," below):"),S.forEach(s),O=d(y),E(D.$$.fragment,y),this.h()},h(){$(r,"href","/docs/transformers/pr_16722/en/model_doc/auto#transformers.AutoModelForSequenceClassification"),$(v,"href","/docs/transformers/pr_16722/en/model_doc/auto#transformers.AutoTokenizer")},m(y,S){u(y,a,S),t(a,m),t(a,r),t(r,c),t(a,_),t(a,v),t(v,q),t(a,I),t(a,w),t(w,M),t(a,C),u(y,O,S),A(D,y,S),L=!0},p:qe,i(y){L||(T(D.$$.fragment,y),L=!0)},o(y){j(D.$$.fragment,y),L=!1},d(y){y&&s(a),y&&s(O),x(D,y)}}}function Sp(P){let a,m;return a=new _e({props:{$$slots:{default:[Mp]},$$scope:{ctx:P}}}),{c(){b(a.$$.fragment)},l(r){E(a.$$.fragment,r)},m(r,c){A(a,r,c),m=!0},p(r,c){const _={};c&2&&(_.$$scope={dirty:c,ctx:r}),a.$set(_)},i(r){m||(T(a.$$.fragment,r),m=!0)},o(r){j(a.$$.fragment,r),m=!1},d(r){x(a,r)}}}function Ip(P){let a,m,r,c,_,v,q,I,w,M,C,O,D,L;return D=new W({props:{code:`from transformers import AutoTokenizer, TFAutoModelForSequenceClassification

model = TFAutoModelForSequenceClassification.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, TFAutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSequenceClassification.from_pretrained(model_name)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(model_name)`}}),{c(){a=l("p"),m=o("Use the "),r=l("a"),c=o("TFAutoModelForSequenceClassification"),_=o(" and "),v=l("a"),q=o("AutoTokenizer"),I=o(" to load the pretrained model and it\u2019s associated tokenizer (more on an "),w=l("code"),M=o("TFAutoClass"),C=o(" below):"),O=h(),b(D.$$.fragment),this.h()},l(y){a=i(y,"P",{});var S=p(a);m=n(S,"Use the "),r=i(S,"A",{href:!0});var g=p(r);c=n(g,"TFAutoModelForSequenceClassification"),g.forEach(s),_=n(S," and "),v=i(S,"A",{href:!0});var F=p(v);q=n(F,"AutoTokenizer"),F.forEach(s),I=n(S," to load the pretrained model and it\u2019s associated tokenizer (more on an "),w=i(S,"CODE",{});var R=p(w);M=n(R,"TFAutoClass"),R.forEach(s),C=n(S," below):"),S.forEach(s),O=d(y),E(D.$$.fragment,y),this.h()},h(){$(r,"href","/docs/transformers/pr_16722/en/model_doc/auto#transformers.TFAutoModelForSequenceClassification"),$(v,"href","/docs/transformers/pr_16722/en/model_doc/auto#transformers.AutoTokenizer")},m(y,S){u(y,a,S),t(a,m),t(a,r),t(r,c),t(a,_),t(a,v),t(v,q),t(a,I),t(a,w),t(w,M),t(a,C),u(y,O,S),A(D,y,S),L=!0},p:qe,i(y){L||(T(D.$$.fragment,y),L=!0)},o(y){j(D.$$.fragment,y),L=!1},d(y){y&&s(a),y&&s(O),x(D,y)}}}function Cp(P){let a,m;return a=new _e({props:{$$slots:{default:[Ip]},$$scope:{ctx:P}}}),{c(){b(a.$$.fragment)},l(r){E(a.$$.fragment,r)},m(r,c){A(a,r,c),m=!0},p(r,c){const _={};c&2&&(_.$$scope={dirty:c,ctx:r}),a.$set(_)},i(r){m||(T(a.$$.fragment,r),m=!0)},o(r){j(a.$$.fragment,r),m=!1},d(r){x(a,r)}}}function Op(P){let a,m;return a=new W({props:{code:`pt_batch = tokenizer(
    ["We are very happy to show you the \u{1F917} Transformers library.", "We hope you don't hate it."],
    padding=True,
    truncation=True,
    max_length=512,
    return_tensors="pt",
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>pt_batch = tokenizer(
<span class="hljs-meta">... </span>    [<span class="hljs-string">&quot;We are very happy to show you the \u{1F917} Transformers library.&quot;</span>, <span class="hljs-string">&quot;We hope you don&#x27;t hate it.&quot;</span>],
<span class="hljs-meta">... </span>    padding=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    truncation=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    max_length=<span class="hljs-number">512</span>,
<span class="hljs-meta">... </span>    return_tensors=<span class="hljs-string">&quot;pt&quot;</span>,
<span class="hljs-meta">... </span>)`}}),{c(){b(a.$$.fragment)},l(r){E(a.$$.fragment,r)},m(r,c){A(a,r,c),m=!0},p:qe,i(r){m||(T(a.$$.fragment,r),m=!0)},o(r){j(a.$$.fragment,r),m=!1},d(r){x(a,r)}}}function Np(P){let a,m;return a=new _e({props:{$$slots:{default:[Op]},$$scope:{ctx:P}}}),{c(){b(a.$$.fragment)},l(r){E(a.$$.fragment,r)},m(r,c){A(a,r,c),m=!0},p(r,c){const _={};c&2&&(_.$$scope={dirty:c,ctx:r}),a.$set(_)},i(r){m||(T(a.$$.fragment,r),m=!0)},o(r){j(a.$$.fragment,r),m=!1},d(r){x(a,r)}}}function Dp(P){let a,m;return a=new W({props:{code:`tf_batch = tokenizer(
    ["We are very happy to show you the \u{1F917} Transformers library.", "We hope you don't hate it."],
    padding=True,
    truncation=True,
    max_length=512,
    return_tensors="tf",
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>tf_batch = tokenizer(
<span class="hljs-meta">... </span>    [<span class="hljs-string">&quot;We are very happy to show you the \u{1F917} Transformers library.&quot;</span>, <span class="hljs-string">&quot;We hope you don&#x27;t hate it.&quot;</span>],
<span class="hljs-meta">... </span>    padding=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    truncation=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    max_length=<span class="hljs-number">512</span>,
<span class="hljs-meta">... </span>    return_tensors=<span class="hljs-string">&quot;tf&quot;</span>,
<span class="hljs-meta">... </span>)`}}),{c(){b(a.$$.fragment)},l(r){E(a.$$.fragment,r)},m(r,c){A(a,r,c),m=!0},p:qe,i(r){m||(T(a.$$.fragment,r),m=!0)},o(r){j(a.$$.fragment,r),m=!1},d(r){x(a,r)}}}function Hp(P){let a,m;return a=new _e({props:{$$slots:{default:[Dp]},$$scope:{ctx:P}}}),{c(){b(a.$$.fragment)},l(r){E(a.$$.fragment,r)},m(r,c){A(a,r,c),m=!0},p(r,c){const _={};c&2&&(_.$$scope={dirty:c,ctx:r}),a.$set(_)},i(r){m||(T(a.$$.fragment,r),m=!0)},o(r){j(a.$$.fragment,r),m=!1},d(r){x(a,r)}}}function Lp(P){let a,m,r,c,_,v,q,I;return{c(){a=l("p"),m=o("See the "),r=l("a"),c=o("task summary"),_=o(" for which "),v=l("a"),q=o("AutoModel"),I=o(" class to use for which task."),this.h()},l(w){a=i(w,"P",{});var M=p(a);m=n(M,"See the "),r=i(M,"A",{href:!0});var C=p(r);c=n(C,"task summary"),C.forEach(s),_=n(M," for which "),v=i(M,"A",{href:!0});var O=p(v);q=n(O,"AutoModel"),O.forEach(s),I=n(M," class to use for which task."),M.forEach(s),this.h()},h(){$(r,"href","./task_summary"),$(v,"href","/docs/transformers/pr_16722/en/model_doc/auto#transformers.AutoModel")},m(w,M){u(w,a,M),t(a,m),t(a,r),t(r,c),t(a,_),t(a,v),t(v,q),t(a,I)},d(w){w&&s(a)}}}function Up(P){let a,m,r,c,_,v,q,I,w,M,C,O,D,L,y,S,g,F,R,U,Q,J,se,B,G,ee,V,K,ce,re,de,oe,te,ne,$e,z,N,le;return S=new W({props:{code:`from transformers import AutoModelForSequenceClassification

model_name = "nlptown/bert-base-multilingual-uncased-sentiment"
pt_model = AutoModelForSequenceClassification.from_pretrained(model_name)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>model_name = <span class="hljs-string">&quot;nlptown/bert-base-multilingual-uncased-sentiment&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>pt_model = AutoModelForSequenceClassification.from_pretrained(model_name)`}}),F=new Ms({props:{$$slots:{default:[Lp]},$$scope:{ctx:P}}}),ee=new W({props:{code:"pt_outputs = pt_model(**pt_batch)",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>pt_outputs = pt_model(**pt_batch)'}}),N=new W({props:{code:`from torch import nn

pt_predictions = nn.functional.softmax(pt_outputs.logits, dim=-1)
print(pt_predictions)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn

<span class="hljs-meta">&gt;&gt;&gt; </span>pt_predictions = nn.functional.softmax(pt_outputs.logits, dim=-<span class="hljs-number">1</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(pt_predictions)
tensor([[<span class="hljs-number">0.0021</span>, <span class="hljs-number">0.0018</span>, <span class="hljs-number">0.0115</span>, <span class="hljs-number">0.2121</span>, <span class="hljs-number">0.7725</span>],
        [<span class="hljs-number">0.2084</span>, <span class="hljs-number">0.1826</span>, <span class="hljs-number">0.1969</span>, <span class="hljs-number">0.1755</span>, <span class="hljs-number">0.2365</span>]], grad_fn=&lt;SoftmaxBackward0&gt;)`}}),{c(){a=l("p"),m=o("\u{1F917} Transformers provides a simple and unified way to load pretrained instances. This means you can load an "),r=l("a"),c=o("AutoModel"),_=o(" like you would load an "),v=l("a"),q=o("AutoTokenizer"),I=o(". The only difference is selecting the correct "),w=l("a"),M=o("AutoModel"),C=o(" for the task. Since you are doing text - or sequence - classification, load "),O=l("a"),D=o("AutoModelForSequenceClassification"),L=o(":"),y=h(),b(S.$$.fragment),g=h(),b(F.$$.fragment),R=h(),U=l("p"),Q=o("Now you can pass your preprocessed batch of inputs directly to the model. You just have to unpack the dictionary by adding "),J=l("code"),se=o("**"),B=o(":"),G=h(),b(ee.$$.fragment),V=h(),K=l("p"),ce=o("The model outputs the final activations in the "),re=l("code"),de=o("logits"),oe=o(" attribute. Apply the softmax function to the "),te=l("code"),ne=o("logits"),$e=o(" to retrieve the probabilities:"),z=h(),b(N.$$.fragment),this.h()},l(k){a=i(k,"P",{});var H=p(a);m=n(H,"\u{1F917} Transformers provides a simple and unified way to load pretrained instances. This means you can load an "),r=i(H,"A",{href:!0});var ie=p(r);c=n(ie,"AutoModel"),ie.forEach(s),_=n(H," like you would load an "),v=i(H,"A",{href:!0});var Pe=p(v);q=n(Pe,"AutoTokenizer"),Pe.forEach(s),I=n(H,". The only difference is selecting the correct "),w=i(H,"A",{href:!0});var he=p(w);M=n(he,"AutoModel"),he.forEach(s),C=n(H," for the task. Since you are doing text - or sequence - classification, load "),O=i(H,"A",{href:!0});var ge=p(O);D=n(ge,"AutoModelForSequenceClassification"),ge.forEach(s),L=n(H,":"),H.forEach(s),y=d(k),E(S.$$.fragment,k),g=d(k),E(F.$$.fragment,k),R=d(k),U=i(k,"P",{});var pe=p(U);Q=n(pe,"Now you can pass your preprocessed batch of inputs directly to the model. You just have to unpack the dictionary by adding "),J=i(pe,"CODE",{});var De=p(J);se=n(De,"**"),De.forEach(s),B=n(pe,":"),pe.forEach(s),G=d(k),E(ee.$$.fragment,k),V=d(k),K=i(k,"P",{});var ve=p(K);ce=n(ve,"The model outputs the final activations in the "),re=i(ve,"CODE",{});var Gt=p(re);de=n(Gt,"logits"),Gt.forEach(s),oe=n(ve," attribute. Apply the softmax function to the "),te=i(ve,"CODE",{});var ct=p(te);ne=n(ct,"logits"),ct.forEach(s),$e=n(ve," to retrieve the probabilities:"),ve.forEach(s),z=d(k),E(N.$$.fragment,k),this.h()},h(){$(r,"href","/docs/transformers/pr_16722/en/model_doc/auto#transformers.AutoModel"),$(v,"href","/docs/transformers/pr_16722/en/model_doc/auto#transformers.AutoTokenizer"),$(w,"href","/docs/transformers/pr_16722/en/model_doc/auto#transformers.AutoModel"),$(O,"href","/docs/transformers/pr_16722/en/model_doc/auto#transformers.AutoModelForSequenceClassification")},m(k,H){u(k,a,H),t(a,m),t(a,r),t(r,c),t(a,_),t(a,v),t(v,q),t(a,I),t(a,w),t(w,M),t(a,C),t(a,O),t(O,D),t(a,L),u(k,y,H),A(S,k,H),u(k,g,H),A(F,k,H),u(k,R,H),u(k,U,H),t(U,Q),t(U,J),t(J,se),t(U,B),u(k,G,H),A(ee,k,H),u(k,V,H),u(k,K,H),t(K,ce),t(K,re),t(re,de),t(K,oe),t(K,te),t(te,ne),t(K,$e),u(k,z,H),A(N,k,H),le=!0},p(k,H){const ie={};H&2&&(ie.$$scope={dirty:H,ctx:k}),F.$set(ie)},i(k){le||(T(S.$$.fragment,k),T(F.$$.fragment,k),T(ee.$$.fragment,k),T(N.$$.fragment,k),le=!0)},o(k){j(S.$$.fragment,k),j(F.$$.fragment,k),j(ee.$$.fragment,k),j(N.$$.fragment,k),le=!1},d(k){k&&s(a),k&&s(y),x(S,k),k&&s(g),x(F,k),k&&s(R),k&&s(U),k&&s(G),x(ee,k),k&&s(V),k&&s(K),k&&s(z),x(N,k)}}}function Wp(P){let a,m;return a=new _e({props:{$$slots:{default:[Up]},$$scope:{ctx:P}}}),{c(){b(a.$$.fragment)},l(r){E(a.$$.fragment,r)},m(r,c){A(a,r,c),m=!0},p(r,c){const _={};c&2&&(_.$$scope={dirty:c,ctx:r}),a.$set(_)},i(r){m||(T(a.$$.fragment,r),m=!0)},o(r){j(a.$$.fragment,r),m=!1},d(r){x(a,r)}}}function Rp(P){let a,m,r,c,_,v,q,I;return{c(){a=l("p"),m=o("See the "),r=l("a"),c=o("task summary"),_=o(" for which "),v=l("a"),q=o("AutoModel"),I=o(" class to use for which task."),this.h()},l(w){a=i(w,"P",{});var M=p(a);m=n(M,"See the "),r=i(M,"A",{href:!0});var C=p(r);c=n(C,"task summary"),C.forEach(s),_=n(M," for which "),v=i(M,"A",{href:!0});var O=p(v);q=n(O,"AutoModel"),O.forEach(s),I=n(M," class to use for which task."),M.forEach(s),this.h()},h(){$(r,"href","./task_summary"),$(v,"href","/docs/transformers/pr_16722/en/model_doc/auto#transformers.AutoModel")},m(w,M){u(w,a,M),t(a,m),t(a,r),t(r,c),t(a,_),t(a,v),t(v,q),t(a,I)},d(w){w&&s(a)}}}function Gp(P){let a,m,r,c,_,v,q,I,w,M,C,O,D,L,y,S,g,F,R,U,Q,J,se,B,G,ee,V,K,ce,re,de,oe,te,ne,$e;return S=new W({props:{code:`from transformers import TFAutoModelForSequenceClassification

model_name = "nlptown/bert-base-multilingual-uncased-sentiment"
tf_model = TFAutoModelForSequenceClassification.from_pretrained(model_name)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFAutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>model_name = <span class="hljs-string">&quot;nlptown/bert-base-multilingual-uncased-sentiment&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tf_model = TFAutoModelForSequenceClassification.from_pretrained(model_name)`}}),F=new Ms({props:{$$slots:{default:[Rp]},$$scope:{ctx:P}}}),se=new W({props:{code:"tf_outputs = tf_model(tf_batch)",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>tf_outputs = tf_model(tf_batch)'}}),ne=new W({props:{code:`import tensorflow as tf

tf_predictions = tf.nn.softmax(tf_outputs.logits, axis=-1)
tf_predictions`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-meta">&gt;&gt;&gt; </span>tf_predictions = tf.nn.softmax(tf_outputs.logits, axis=-<span class="hljs-number">1</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tf_predictions`}}),{c(){a=l("p"),m=o("\u{1F917} Transformers provides a simple and unified way to load pretrained instances. This means you can load an "),r=l("a"),c=o("TFAutoModel"),_=o(" like you would load an "),v=l("a"),q=o("AutoTokenizer"),I=o(". The only difference is selecting the correct "),w=l("a"),M=o("TFAutoModel"),C=o(" for the task. Since you are doing text - or sequence - classification, load "),O=l("a"),D=o("TFAutoModelForSequenceClassification"),L=o(":"),y=h(),b(S.$$.fragment),g=h(),b(F.$$.fragment),R=h(),U=l("p"),Q=o("Now you can pass your preprocessed batch of inputs directly to the model by passing the dictionary keys directly to the tensors:"),J=h(),b(se.$$.fragment),B=h(),G=l("p"),ee=o("The model outputs the final activations in the "),V=l("code"),K=o("logits"),ce=o(" attribute. Apply the softmax function to the "),re=l("code"),de=o("logits"),oe=o(" to retrieve the probabilities:"),te=h(),b(ne.$$.fragment),this.h()},l(z){a=i(z,"P",{});var N=p(a);m=n(N,"\u{1F917} Transformers provides a simple and unified way to load pretrained instances. This means you can load an "),r=i(N,"A",{href:!0});var le=p(r);c=n(le,"TFAutoModel"),le.forEach(s),_=n(N," like you would load an "),v=i(N,"A",{href:!0});var k=p(v);q=n(k,"AutoTokenizer"),k.forEach(s),I=n(N,". The only difference is selecting the correct "),w=i(N,"A",{href:!0});var H=p(w);M=n(H,"TFAutoModel"),H.forEach(s),C=n(N," for the task. Since you are doing text - or sequence - classification, load "),O=i(N,"A",{href:!0});var ie=p(O);D=n(ie,"TFAutoModelForSequenceClassification"),ie.forEach(s),L=n(N,":"),N.forEach(s),y=d(z),E(S.$$.fragment,z),g=d(z),E(F.$$.fragment,z),R=d(z),U=i(z,"P",{});var Pe=p(U);Q=n(Pe,"Now you can pass your preprocessed batch of inputs directly to the model by passing the dictionary keys directly to the tensors:"),Pe.forEach(s),J=d(z),E(se.$$.fragment,z),B=d(z),G=i(z,"P",{});var he=p(G);ee=n(he,"The model outputs the final activations in the "),V=i(he,"CODE",{});var ge=p(V);K=n(ge,"logits"),ge.forEach(s),ce=n(he," attribute. Apply the softmax function to the "),re=i(he,"CODE",{});var pe=p(re);de=n(pe,"logits"),pe.forEach(s),oe=n(he," to retrieve the probabilities:"),he.forEach(s),te=d(z),E(ne.$$.fragment,z),this.h()},h(){$(r,"href","/docs/transformers/pr_16722/en/model_doc/auto#transformers.TFAutoModel"),$(v,"href","/docs/transformers/pr_16722/en/model_doc/auto#transformers.AutoTokenizer"),$(w,"href","/docs/transformers/pr_16722/en/model_doc/auto#transformers.TFAutoModel"),$(O,"href","/docs/transformers/pr_16722/en/model_doc/auto#transformers.TFAutoModelForSequenceClassification")},m(z,N){u(z,a,N),t(a,m),t(a,r),t(r,c),t(a,_),t(a,v),t(v,q),t(a,I),t(a,w),t(w,M),t(a,C),t(a,O),t(O,D),t(a,L),u(z,y,N),A(S,z,N),u(z,g,N),A(F,z,N),u(z,R,N),u(z,U,N),t(U,Q),u(z,J,N),A(se,z,N),u(z,B,N),u(z,G,N),t(G,ee),t(G,V),t(V,K),t(G,ce),t(G,re),t(re,de),t(G,oe),u(z,te,N),A(ne,z,N),$e=!0},p(z,N){const le={};N&2&&(le.$$scope={dirty:N,ctx:z}),F.$set(le)},i(z){$e||(T(S.$$.fragment,z),T(F.$$.fragment,z),T(se.$$.fragment,z),T(ne.$$.fragment,z),$e=!0)},o(z){j(S.$$.fragment,z),j(F.$$.fragment,z),j(se.$$.fragment,z),j(ne.$$.fragment,z),$e=!1},d(z){z&&s(a),z&&s(y),x(S,z),z&&s(g),x(F,z),z&&s(R),z&&s(U),z&&s(J),x(se,z),z&&s(B),z&&s(G),z&&s(te),x(ne,z)}}}function Yp(P){let a,m;return a=new _e({props:{$$slots:{default:[Gp]},$$scope:{ctx:P}}}),{c(){b(a.$$.fragment)},l(r){E(a.$$.fragment,r)},m(r,c){A(a,r,c),m=!0},p(r,c){const _={};c&2&&(_.$$scope={dirty:c,ctx:r}),a.$set(_)},i(r){m||(T(a.$$.fragment,r),m=!0)},o(r){j(a.$$.fragment,r),m=!1},d(r){x(a,r)}}}function Jp(P){let a,m,r,c,_;return{c(){a=l("p"),m=o("All \u{1F917} Transformers models (PyTorch or TensorFlow) outputs the tensors "),r=l("em"),c=o("before"),_=o(` the final activation
function (like softmax) because the final activation function is often fused with the loss.`)},l(v){a=i(v,"P",{});var q=p(a);m=n(q,"All \u{1F917} Transformers models (PyTorch or TensorFlow) outputs the tensors "),r=i(q,"EM",{});var I=p(r);c=n(I,"before"),I.forEach(s),_=n(q,` the final activation
function (like softmax) because the final activation function is often fused with the loss.`),q.forEach(s)},m(v,q){u(v,a,q),t(a,m),t(a,r),t(r,c),t(a,_)},d(v){v&&s(a)}}}function Qp(P){let a,m,r,c,_;return{c(){a=l("p"),m=o(`\u{1F917} Transformers model outputs are special dataclasses so their attributes are autocompleted in an IDE.
The model outputs also behave like a tuple or a dictionary (e.g., you can index with an integer, a slice or a string) in which case the attributes that are `),r=l("code"),c=o("None"),_=o(" are ignored.")},l(v){a=i(v,"P",{});var q=p(a);m=n(q,`\u{1F917} Transformers model outputs are special dataclasses so their attributes are autocompleted in an IDE.
The model outputs also behave like a tuple or a dictionary (e.g., you can index with an integer, a slice or a string) in which case the attributes that are `),r=i(q,"CODE",{});var I=p(r);c=n(I,"None"),I.forEach(s),_=n(q," are ignored."),q.forEach(s)},m(v,q){u(v,a,q),t(a,m),t(a,r),t(r,c),t(a,_)},d(v){v&&s(a)}}}function Bp(P){let a,m,r,c,_,v,q,I,w,M,C,O,D,L,y,S;return q=new W({props:{code:`pt_save_directory = "./pt_save_pretrained"
tokenizer.save_pretrained(pt_save_directory)
pt_model.save_pretrained(pt_save_directory)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>pt_save_directory = <span class="hljs-string">&quot;./pt_save_pretrained&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.save_pretrained(pt_save_directory)
<span class="hljs-meta">&gt;&gt;&gt; </span>pt_model.save_pretrained(pt_save_directory)`}}),y=new W({props:{code:'pt_model = AutoModelForSequenceClassification.from_pretrained("./pt_save_pretrained")',highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>pt_model = AutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;./pt_save_pretrained&quot;</span>)'}}),{c(){a=l("p"),m=o("Once your model is fine-tuned, you can save it with its tokenizer using "),r=l("a"),c=o("PreTrainedModel.save_pretrained()"),_=o(":"),v=h(),b(q.$$.fragment),I=h(),w=l("p"),M=o("When you are ready to use the model again, reload it with "),C=l("a"),O=o("PreTrainedModel.from_pretrained()"),D=o(":"),L=h(),b(y.$$.fragment),this.h()},l(g){a=i(g,"P",{});var F=p(a);m=n(F,"Once your model is fine-tuned, you can save it with its tokenizer using "),r=i(F,"A",{href:!0});var R=p(r);c=n(R,"PreTrainedModel.save_pretrained()"),R.forEach(s),_=n(F,":"),F.forEach(s),v=d(g),E(q.$$.fragment,g),I=d(g),w=i(g,"P",{});var U=p(w);M=n(U,"When you are ready to use the model again, reload it with "),C=i(U,"A",{href:!0});var Q=p(C);O=n(Q,"PreTrainedModel.from_pretrained()"),Q.forEach(s),D=n(U,":"),U.forEach(s),L=d(g),E(y.$$.fragment,g),this.h()},h(){$(r,"href","/docs/transformers/pr_16722/en/main_classes/model#transformers.PreTrainedModel.save_pretrained"),$(C,"href","/docs/transformers/pr_16722/en/main_classes/model#transformers.PreTrainedModel.from_pretrained")},m(g,F){u(g,a,F),t(a,m),t(a,r),t(r,c),t(a,_),u(g,v,F),A(q,g,F),u(g,I,F),u(g,w,F),t(w,M),t(w,C),t(C,O),t(w,D),u(g,L,F),A(y,g,F),S=!0},p:qe,i(g){S||(T(q.$$.fragment,g),T(y.$$.fragment,g),S=!0)},o(g){j(q.$$.fragment,g),j(y.$$.fragment,g),S=!1},d(g){g&&s(a),g&&s(v),x(q,g),g&&s(I),g&&s(w),g&&s(L),x(y,g)}}}function Vp(P){let a,m;return a=new _e({props:{$$slots:{default:[Bp]},$$scope:{ctx:P}}}),{c(){b(a.$$.fragment)},l(r){E(a.$$.fragment,r)},m(r,c){A(a,r,c),m=!0},p(r,c){const _={};c&2&&(_.$$scope={dirty:c,ctx:r}),a.$set(_)},i(r){m||(T(a.$$.fragment,r),m=!0)},o(r){j(a.$$.fragment,r),m=!1},d(r){x(a,r)}}}function Kp(P){let a,m,r,c,_,v,q,I,w,M,C,O,D,L,y,S;return q=new W({props:{code:`tf_save_directory = "./tf_save_pretrained"
tokenizer.save_pretrained(tf_save_directory)
tf_model.save_pretrained(tf_save_directory)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>tf_save_directory = <span class="hljs-string">&quot;./tf_save_pretrained&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.save_pretrained(tf_save_directory)
<span class="hljs-meta">&gt;&gt;&gt; </span>tf_model.save_pretrained(tf_save_directory)`}}),y=new W({props:{code:'tf_model = TFAutoModelForSequenceClassification.from_pretrained("./tf_save_pretrained")',highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>tf_model = TFAutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;./tf_save_pretrained&quot;</span>)'}}),{c(){a=l("p"),m=o("Once your model is fine-tuned, you can save it with its tokenizer using "),r=l("a"),c=o("TFPreTrainedModel.save_pretrained()"),_=o(":"),v=h(),b(q.$$.fragment),I=h(),w=l("p"),M=o("When you are ready to use the model again, reload it with "),C=l("a"),O=o("TFPreTrainedModel.from_pretrained()"),D=o(":"),L=h(),b(y.$$.fragment),this.h()},l(g){a=i(g,"P",{});var F=p(a);m=n(F,"Once your model is fine-tuned, you can save it with its tokenizer using "),r=i(F,"A",{href:!0});var R=p(r);c=n(R,"TFPreTrainedModel.save_pretrained()"),R.forEach(s),_=n(F,":"),F.forEach(s),v=d(g),E(q.$$.fragment,g),I=d(g),w=i(g,"P",{});var U=p(w);M=n(U,"When you are ready to use the model again, reload it with "),C=i(U,"A",{href:!0});var Q=p(C);O=n(Q,"TFPreTrainedModel.from_pretrained()"),Q.forEach(s),D=n(U,":"),U.forEach(s),L=d(g),E(y.$$.fragment,g),this.h()},h(){$(r,"href","/docs/transformers/pr_16722/en/main_classes/model#transformers.TFPreTrainedModel.save_pretrained"),$(C,"href","/docs/transformers/pr_16722/en/main_classes/model#transformers.TFPreTrainedModel.from_pretrained")},m(g,F){u(g,a,F),t(a,m),t(a,r),t(r,c),t(a,_),u(g,v,F),A(q,g,F),u(g,I,F),u(g,w,F),t(w,M),t(w,C),t(C,O),t(w,D),u(g,L,F),A(y,g,F),S=!0},p:qe,i(g){S||(T(q.$$.fragment,g),T(y.$$.fragment,g),S=!0)},o(g){j(q.$$.fragment,g),j(y.$$.fragment,g),S=!1},d(g){g&&s(a),g&&s(v),x(q,g),g&&s(I),g&&s(w),g&&s(L),x(y,g)}}}function Zp(P){let a,m;return a=new _e({props:{$$slots:{default:[Kp]},$$scope:{ctx:P}}}),{c(){b(a.$$.fragment)},l(r){E(a.$$.fragment,r)},m(r,c){A(a,r,c),m=!0},p(r,c){const _={};c&2&&(_.$$scope={dirty:c,ctx:r}),a.$set(_)},i(r){m||(T(a.$$.fragment,r),m=!0)},o(r){j(a.$$.fragment,r),m=!1},d(r){x(a,r)}}}function Xp(P){let a,m;return a=new W({props:{code:`from transformers import AutoModel

tokenizer = AutoTokenizer.from_pretrained(tf_save_directory)
pt_model = AutoModelForSequenceClassification.from_pretrained(tf_save_directory, from_tf=True)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModel

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(tf_save_directory)
<span class="hljs-meta">&gt;&gt;&gt; </span>pt_model = AutoModelForSequenceClassification.from_pretrained(tf_save_directory, from_tf=<span class="hljs-literal">True</span>)`}}),{c(){b(a.$$.fragment)},l(r){E(a.$$.fragment,r)},m(r,c){A(a,r,c),m=!0},p:qe,i(r){m||(T(a.$$.fragment,r),m=!0)},o(r){j(a.$$.fragment,r),m=!1},d(r){x(a,r)}}}function ef(P){let a,m;return a=new _e({props:{$$slots:{default:[Xp]},$$scope:{ctx:P}}}),{c(){b(a.$$.fragment)},l(r){E(a.$$.fragment,r)},m(r,c){A(a,r,c),m=!0},p(r,c){const _={};c&2&&(_.$$scope={dirty:c,ctx:r}),a.$set(_)},i(r){m||(T(a.$$.fragment,r),m=!0)},o(r){j(a.$$.fragment,r),m=!1},d(r){x(a,r)}}}function tf(P){let a,m;return a=new W({props:{code:`from transformers import TFAutoModel

tokenizer = AutoTokenizer.from_pretrained(pt_save_directory)
tf_model = TFAutoModelForSequenceClassification.from_pretrained(pt_save_directory, from_pt=True)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFAutoModel

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(pt_save_directory)
<span class="hljs-meta">&gt;&gt;&gt; </span>tf_model = TFAutoModelForSequenceClassification.from_pretrained(pt_save_directory, from_pt=<span class="hljs-literal">True</span>)`}}),{c(){b(a.$$.fragment)},l(r){E(a.$$.fragment,r)},m(r,c){A(a,r,c),m=!0},p:qe,i(r){m||(T(a.$$.fragment,r),m=!0)},o(r){j(a.$$.fragment,r),m=!1},d(r){x(a,r)}}}function sf(P){let a,m;return a=new _e({props:{$$slots:{default:[tf]},$$scope:{ctx:P}}}),{c(){b(a.$$.fragment)},l(r){E(a.$$.fragment,r)},m(r,c){A(a,r,c),m=!0},p(r,c){const _={};c&2&&(_.$$scope={dirty:c,ctx:r}),a.$set(_)},i(r){m||(T(a.$$.fragment,r),m=!0)},o(r){j(a.$$.fragment,r),m=!1},d(r){x(a,r)}}}function af(P){let a,m,r,c,_,v,q,I,w,M,C,O,D,L,y,S,g,F,R,U,Q,J,se,B,G,ee,V,K,ce,re,de,oe,te,ne,$e,z,N,le,k,H,ie,Pe,he,ge,pe,De,ve,Gt,ct,Y,Ss,Yr,Jr,Is,Qr,Br,Cs,Vr,Kr,Os,Zr,Xr,Ns,eo,to,Ds,so,ao,Hs,ro,oo,Ls,no,Aa,ht,Us,lo,io,Ta,we,Ws,po,fo,Rs,uo,mo,Gs,co,ja,dt,Ys,ho,$o,xa,He,Js,_o,go,Qs,vo,za,Le,qa,Fe,Ue,Bs,$t,wo,Vs,ko,Pa,We,yo,Yt,bo,Eo,Fa,Jt,Ao,Ma,Re,Sa,Ge,To,Qt,jo,xo,Ia,_t,Ca,ke,zo,gt,qo,Po,Ks,Fo,Mo,Oa,vt,Na,Ye,So,Bt,Io,Co,Da,wt,Ha,ye,Oo,Vt,No,Do,kt,Ho,Lo,La,yt,Ua,Je,Uo,Kt,Wo,Ro,Wa,bt,Ra,be,Go,Et,Yo,Jo,At,Qo,Bo,Ga,Tt,Ya,Zt,Vo,Ja,jt,Qa,Qe,Ko,Xt,Zo,Xo,Ba,Me,Be,Zs,xt,en,Xs,tn,Va,fe,sn,es,an,rn,zt,on,nn,ts,ln,pn,qt,fn,un,Ka,Pt,Za,Ve,Xa,Ee,mn,ss,cn,hn,ea,dn,$n,er,Ft,tr,Ae,_n,as,gn,vn,rs,wn,kn,sr,Se,Ke,ta,Mt,yn,sa,bn,ar,St,rr,Z,En,os,An,Tn,ns,jn,xn,ls,zn,qn,is,Pn,Fn,aa,Mn,Sn,ps,In,Cn,or,Te,On,ra,Nn,Dn,fs,Hn,Ln,nr,Ie,Ze,oa,It,Un,na,Wn,lr,je,Rn,la,Gn,Yn,us,Jn,Qn,ir,Xe,Bn,ms,Vn,Kn,pr,Ct,fr,et,Zn,ia,Xn,el,ur,cs,tl,mr,Ot,cr,hs,sl,hr,tt,ds,$s,al,rl,ol,_s,gs,nl,ll,dr,st,il,vs,pl,fl,$r,at,_r,rt,ul,ws,ml,cl,gr,Ce,ot,pa,Nt,hl,fa,dl,vr,nt,wr,lt,kr,X,$l,Dt,ua,_l,gl,Ht,ma,vl,wl,ks,kl,yl,ca,bl,El,Lt,Al,Tl,ys,jl,xl,yr,it,br,Oe,pt,ha,Ut,zl,da,ql,Er,ft,Ar,xe,Pl,$a,Fl,Ml,_a,Sl,Il,Tr,ut,jr;return v=new mt({}),C=new Tp({props:{classNames:"absolute z-10 right-0 top-0",options:[{label:"Mixed",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/en/quicktour.ipynb"},{label:"PyTorch",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/en/pytorch/quicktour.ipynb"},{label:"TensorFlow",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/en/tensorflow/quicktour.ipynb"},{label:"Mixed",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/transformers_doc/en/quicktour.ipynb"},{label:"PyTorch",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/transformers_doc/en/pytorch/quicktour.ipynb"},{label:"TensorFlow",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/transformers_doc/en/tensorflow/quicktour.ipynb"}]}}),J=new Ms({props:{$$slots:{default:[jp]},$$scope:{ctx:P}}}),V=new mt({}),N=new wp({props:{id:"tiZFewofSLM"}}),Le=new Ms({props:{$$slots:{default:[xp]},$$scope:{ctx:P}}}),$t=new mt({}),Re=new Fs({props:{pytorch:!0,tensorflow:!0,jax:!1,$$slots:{tensorflow:[Fp],pytorch:[qp]},$$scope:{ctx:P}}}),_t=new W({props:{code:`from transformers import pipeline

classifier = pipeline("sentiment-analysis")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline

<span class="hljs-meta">&gt;&gt;&gt; </span>classifier = pipeline(<span class="hljs-string">&quot;sentiment-analysis&quot;</span>)`}}),vt=new W({props:{code:'classifier("We are very happy to show you the \u{1F917} Transformers library.")',highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>classifier(<span class="hljs-string">&quot;We are very happy to show you the \u{1F917} Transformers library.&quot;</span>)
[{<span class="hljs-string">&#x27;label&#x27;</span>: <span class="hljs-string">&#x27;POSITIVE&#x27;</span>, <span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.9998</span>}]`}}),wt=new W({props:{code:`results = classifier(["We are very happy to show you the \u{1F917} Transformers library.", "We hope you don't hate it."])
for result in results:
    print(f"label: {result['label']}, with score: {round(result['score'], 4)}")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>results = classifier([<span class="hljs-string">&quot;We are very happy to show you the \u{1F917} Transformers library.&quot;</span>, <span class="hljs-string">&quot;We hope you don&#x27;t hate it.&quot;</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">for</span> result <span class="hljs-keyword">in</span> results:
<span class="hljs-meta">... </span>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;label: <span class="hljs-subst">{result[<span class="hljs-string">&#x27;label&#x27;</span>]}</span>, with score: <span class="hljs-subst">{<span class="hljs-built_in">round</span>(result[<span class="hljs-string">&#x27;score&#x27;</span>], <span class="hljs-number">4</span>)}</span>&quot;</span>)
label: POSITIVE, <span class="hljs-keyword">with</span> score: <span class="hljs-number">0.9998</span>
label: NEGATIVE, <span class="hljs-keyword">with</span> score: <span class="hljs-number">0.5309</span>`}}),yt=new W({props:{code:"pip install datasets ",highlighted:"pip install datasets "}}),bt=new W({props:{code:`import torch
from transformers import pipeline

speech_recognizer = pipeline("automatic-speech-recognition", model="facebook/wav2vec2-base-960h")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline

<span class="hljs-meta">&gt;&gt;&gt; </span>speech_recognizer = pipeline(<span class="hljs-string">&quot;automatic-speech-recognition&quot;</span>, model=<span class="hljs-string">&quot;facebook/wav2vec2-base-960h&quot;</span>)`}}),Tt=new W({props:{code:`from datasets import load_dataset

dataset = load_dataset("PolyAI/minds14", name="en-US", split="train")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;PolyAI/minds14&quot;</span>, name=<span class="hljs-string">&quot;en-US&quot;</span>, split=<span class="hljs-string">&quot;train&quot;</span>)`}}),jt=new W({props:{code:`files = dataset["path"]
speech_recognizer(files[:4])`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>files = dataset[<span class="hljs-string">&quot;path&quot;</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>speech_recognizer(files[:<span class="hljs-number">4</span>])
[{<span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;I WOULD LIKE TO SET UP A JOINT ACCOUNT WITH MY PARTNER HOW DO I PROCEED WITH DOING THAT&#x27;</span>}, 
 {<span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&quot;FONDERING HOW I&#x27;D SET UP A JOIN TO HELL T WITH MY WIFE AND WHERE THE AP MIGHT BE&quot;</span>}, 
 {<span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&quot;I I&#x27;D LIKE TOY SET UP A JOINT ACCOUNT WITH MY PARTNER I&#x27;M NOT SEEING THE OPTION TO DO IT ON THE APSO I CALLED IN TO GET SOME HELP CAN I JUST DO IT OVER THE PHONE WITH YOU AND GIVE YOU THE INFORMATION OR SHOULD I DO IT IN THE AP AN I&#x27;M MISSING SOMETHING UQUETTE HAD PREFERRED TO JUST DO IT OVER THE PHONE OF POSSIBLE THINGS&quot;</span>}, 
 {<span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;HOW DO I FURN A JOINA COUT&#x27;</span>}]`}}),xt=new mt({}),Pt=new W({props:{code:'model_name = "nlptown/bert-base-multilingual-uncased-sentiment"',highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>model_name = <span class="hljs-string">&quot;nlptown/bert-base-multilingual-uncased-sentiment&quot;</span>'}}),Ve=new Fs({props:{pytorch:!0,tensorflow:!0,jax:!1,$$slots:{tensorflow:[Cp],pytorch:[Sp]},$$scope:{ctx:P}}}),Ft=new W({props:{code:`classifier = pipeline("sentiment-analysis", model=model, tokenizer=tokenizer)
classifier("Nous sommes tr\xE8s heureux de vous pr\xE9senter la biblioth\xE8que \u{1F917} Transformers.")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>classifier = pipeline(<span class="hljs-string">&quot;sentiment-analysis&quot;</span>, model=model, tokenizer=tokenizer)
<span class="hljs-meta">&gt;&gt;&gt; </span>classifier(<span class="hljs-string">&quot;Nous sommes tr\xE8s heureux de vous pr\xE9senter la biblioth\xE8que \u{1F917} Transformers.&quot;</span>)
[{<span class="hljs-string">&#x27;label&#x27;</span>: <span class="hljs-string">&#x27;5 stars&#x27;</span>, <span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.7273</span>}]`}}),Mt=new mt({}),St=new wp({props:{id:"AhChOFRegn4"}}),It=new mt({}),Ct=new W({props:{code:`from transformers import AutoTokenizer

model_name = "nlptown/bert-base-multilingual-uncased-sentiment"
tokenizer = AutoTokenizer.from_pretrained(model_name)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>model_name = <span class="hljs-string">&quot;nlptown/bert-base-multilingual-uncased-sentiment&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(model_name)`}}),Ot=new W({props:{code:`encoding = tokenizer("We are very happy to show you the \u{1F917} Transformers library.")
print(encoding)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>encoding = tokenizer(<span class="hljs-string">&quot;We are very happy to show you the \u{1F917} Transformers library.&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(encoding)
{<span class="hljs-string">&#x27;input_ids&#x27;</span>: [<span class="hljs-number">101</span>, <span class="hljs-number">11312</span>, <span class="hljs-number">10320</span>, <span class="hljs-number">12495</span>, <span class="hljs-number">19308</span>, <span class="hljs-number">10114</span>, <span class="hljs-number">11391</span>, <span class="hljs-number">10855</span>, <span class="hljs-number">10103</span>, <span class="hljs-number">100</span>, <span class="hljs-number">58263</span>, <span class="hljs-number">13299</span>, <span class="hljs-number">119</span>, <span class="hljs-number">102</span>],
 <span class="hljs-string">&#x27;token_type_ids&#x27;</span>: [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
 <span class="hljs-string">&#x27;attention_mask&#x27;</span>: [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]}`}}),at=new Fs({props:{pytorch:!0,tensorflow:!0,jax:!1,$$slots:{tensorflow:[Hp],pytorch:[Np]},$$scope:{ctx:P}}}),Nt=new mt({}),nt=new Fs({props:{pytorch:!0,tensorflow:!0,jax:!1,$$slots:{tensorflow:[Yp],pytorch:[Wp]},$$scope:{ctx:P}}}),lt=new Ms({props:{$$slots:{default:[Jp]},$$scope:{ctx:P}}}),it=new Ms({props:{$$slots:{default:[Qp]},$$scope:{ctx:P}}}),Ut=new mt({}),ft=new Fs({props:{pytorch:!0,tensorflow:!0,jax:!1,$$slots:{tensorflow:[Zp],pytorch:[Vp]},$$scope:{ctx:P}}}),ut=new Fs({props:{pytorch:!0,tensorflow:!0,jax:!1,$$slots:{tensorflow:[sf],pytorch:[ef]},$$scope:{ctx:P}}}),{c(){a=l("meta"),m=h(),r=l("h1"),c=l("a"),_=l("span"),b(v.$$.fragment),q=h(),I=l("span"),w=o("Quick tour"),M=h(),b(C.$$.fragment),O=h(),D=l("p"),L=o("Get up and running with \u{1F917} Transformers! Start using the "),y=l("a"),S=o("pipeline()"),g=o(" for rapid inference, and quickly load a pretrained model and tokenizer with an "),F=l("a"),R=o("AutoClass"),U=o(" to solve your text, vision or audio task."),Q=h(),b(J.$$.fragment),se=h(),B=l("h2"),G=l("a"),ee=l("span"),b(V.$$.fragment),K=h(),ce=l("span"),re=o("Pipeline"),de=h(),oe=l("p"),te=l("a"),ne=o("pipeline()"),$e=o(" is the easiest way to use a pretrained model for a given task."),z=h(),b(N.$$.fragment),le=h(),k=l("p"),H=o("The "),ie=l("a"),Pe=o("pipeline()"),he=o(" supports many common tasks out-of-the-box:"),ge=h(),pe=l("p"),De=l("strong"),ve=o("Text"),Gt=o(":"),ct=h(),Y=l("ul"),Ss=l("li"),Yr=o("Sentiment analysis: classify the polarity of a given text."),Jr=h(),Is=l("li"),Qr=o("Text generation (in English): generate text from a given input."),Br=h(),Cs=l("li"),Vr=o("Name entity recognition (NER): label each word with the entity it represents (person, date, location, etc.)."),Kr=h(),Os=l("li"),Zr=o("Question answering: extract the answer from the context, given some context and a question."),Xr=h(),Ns=l("li"),eo=o("Fill-mask: fill in the blank given a text with masked words."),to=h(),Ds=l("li"),so=o("Summarization: generate a summary of a long sequence of text or document."),ao=h(),Hs=l("li"),ro=o("Translation: translate text into another language."),oo=h(),Ls=l("li"),no=o("Feature extraction: create a tensor representation of the text."),Aa=h(),ht=l("p"),Us=l("strong"),lo=o("Image"),io=o(":"),Ta=h(),we=l("ul"),Ws=l("li"),po=o("Image classification: classify an image."),fo=h(),Rs=l("li"),uo=o("Image segmentation: classify every pixel in an image."),mo=h(),Gs=l("li"),co=o("Object detection: detect objects within an image."),ja=h(),dt=l("p"),Ys=l("strong"),ho=o("Audio"),$o=o(":"),xa=h(),He=l("ul"),Js=l("li"),_o=o("Audio classification: assign a label to a given segment of audio."),go=h(),Qs=l("li"),vo=o("Automatic speech recognition (ASR): transcribe audio data into text."),za=h(),b(Le.$$.fragment),qa=h(),Fe=l("h3"),Ue=l("a"),Bs=l("span"),b($t.$$.fragment),wo=h(),Vs=l("span"),ko=o("Pipeline usage"),Pa=h(),We=l("p"),yo=o("In the following example, you will use the "),Yt=l("a"),bo=o("pipeline()"),Eo=o(" for sentiment analysis."),Fa=h(),Jt=l("p"),Ao=o("Install the following dependencies if you haven\u2019t already:"),Ma=h(),b(Re.$$.fragment),Sa=h(),Ge=l("p"),To=o("Import "),Qt=l("a"),jo=o("pipeline()"),xo=o(" and specify the task you want to complete:"),Ia=h(),b(_t.$$.fragment),Ca=h(),ke=l("p"),zo=o("The pipeline downloads and caches a default "),gt=l("a"),qo=o("pretrained model"),Po=o(" and tokenizer for sentiment analysis. Now you can use the "),Ks=l("code"),Fo=o("classifier"),Mo=o(" on your target text:"),Oa=h(),b(vt.$$.fragment),Na=h(),Ye=l("p"),So=o("For more than one sentence, pass a list of sentences to the "),Bt=l("a"),Io=o("pipeline()"),Co=o(" which returns a list of dictionaries:"),Da=h(),b(wt.$$.fragment),Ha=h(),ye=l("p"),Oo=o("The "),Vt=l("a"),No=o("pipeline()"),Do=o(" can also iterate over an entire dataset. Start by installing the "),kt=l("a"),Ho=o("\u{1F917} Datasets"),Lo=o(" library:"),La=h(),b(yt.$$.fragment),Ua=h(),Je=l("p"),Uo=o("Create a "),Kt=l("a"),Wo=o("pipeline()"),Ro=o(" with the task you want to solve for and the model you want to use."),Wa=h(),b(bt.$$.fragment),Ra=h(),be=l("p"),Go=o("Next, load a dataset (see the \u{1F917} Datasets "),Et=l("a"),Yo=o("Quick Start"),Jo=o(" for more details) you\u2019d like to iterate over. For example, let\u2019s load the "),At=l("a"),Qo=o("MInDS-14"),Bo=o(" dataset:"),Ga=h(),b(Tt.$$.fragment),Ya=h(),Zt=l("p"),Vo=o("You can pass a whole dataset pipeline:"),Ja=h(),b(jt.$$.fragment),Qa=h(),Qe=l("p"),Ko=o("For a larger dataset where the inputs are big (like in speech or vision), you will want to pass along a generator instead of a list that loads all the inputs in memory. See the "),Xt=l("a"),Zo=o("pipeline documentation"),Xo=o(" for more information."),Ba=h(),Me=l("h3"),Be=l("a"),Zs=l("span"),b(xt.$$.fragment),en=h(),Xs=l("span"),tn=o("Use another model and tokenizer in the pipeline"),Va=h(),fe=l("p"),sn=o("The "),es=l("a"),an=o("pipeline()"),rn=o(" can accommodate any model from the "),zt=l("a"),on=o("Model Hub"),nn=o(", making it easy to adapt the "),ts=l("a"),ln=o("pipeline()"),pn=o(" for other use-cases. For example, if you\u2019d like a model capable of handling French text, use the tags on the Model Hub to filter for an appropriate model. The top filtered result returns a multilingual "),qt=l("a"),fn=o("BERT model"),un=o(" fine-tuned for sentiment analysis. Great, let\u2019s use this model!"),Ka=h(),b(Pt.$$.fragment),Za=h(),b(Ve.$$.fragment),Xa=h(),Ee=l("p"),mn=o("Then you can specify the model and tokenizer in the "),ss=l("a"),cn=o("pipeline()"),hn=o(", and apply the "),ea=l("code"),dn=o("classifier"),$n=o(" on your target text:"),er=h(),b(Ft.$$.fragment),tr=h(),Ae=l("p"),_n=o("If you can\u2019t find a model for your use-case, you will need to fine-tune a pretrained model on your data. Take a look at our "),as=l("a"),gn=o("fine-tuning tutorial"),vn=o(" to learn how. Finally, after you\u2019ve fine-tuned your pretrained model, please consider sharing it (see tutorial "),rs=l("a"),wn=o("here"),kn=o(") with the community on the Model Hub to democratize NLP for everyone! \u{1F917}"),sr=h(),Se=l("h2"),Ke=l("a"),ta=l("span"),b(Mt.$$.fragment),yn=h(),sa=l("span"),bn=o("AutoClass"),ar=h(),b(St.$$.fragment),rr=h(),Z=l("p"),En=o("Under the hood, the "),os=l("a"),An=o("AutoModelForSequenceClassification"),Tn=o(" and "),ns=l("a"),jn=o("AutoTokenizer"),xn=o(" classes work together to power the "),ls=l("a"),zn=o("pipeline()"),qn=o(". An "),is=l("a"),Pn=o("AutoClass"),Fn=o(" is a shortcut that automatically retrieves the architecture of a pretrained model from it\u2019s name or path. You only need to select the appropriate "),aa=l("code"),Mn=o("AutoClass"),Sn=o(" for your task and it\u2019s associated tokenizer with "),ps=l("a"),In=o("AutoTokenizer"),Cn=o("."),or=h(),Te=l("p"),On=o("Let\u2019s return to our example and see how you can use the "),ra=l("code"),Nn=o("AutoClass"),Dn=o(" to replicate the results of the "),fs=l("a"),Hn=o("pipeline()"),Ln=o("."),nr=h(),Ie=l("h3"),Ze=l("a"),oa=l("span"),b(It.$$.fragment),Un=h(),na=l("span"),Wn=o("AutoTokenizer"),lr=h(),je=l("p"),Rn=o("A tokenizer is responsible for preprocessing text into a format that is understandable to the model. First, the tokenizer will split the text into words called "),la=l("em"),Gn=o("tokens"),Yn=o(". There are multiple rules that govern the tokenization process, including how to split a word and at what level (learn more about tokenization "),us=l("a"),Jn=o("here"),Qn=o("). The most important thing to remember though is you need to instantiate the tokenizer with the same model name to ensure you\u2019re using the same tokenization rules a model was pretrained with."),ir=h(),Xe=l("p"),Bn=o("Load a tokenizer with "),ms=l("a"),Vn=o("AutoTokenizer"),Kn=o(":"),pr=h(),b(Ct.$$.fragment),fr=h(),et=l("p"),Zn=o("Next, the tokenizer converts the tokens into numbers in order to construct a tensor as input to the model. This is known as the model\u2019s "),ia=l("em"),Xn=o("vocabulary"),el=o("."),ur=h(),cs=l("p"),tl=o("Pass your text to the tokenizer:"),mr=h(),b(Ot.$$.fragment),cr=h(),hs=l("p"),sl=o("The tokenizer will return a dictionary containing:"),hr=h(),tt=l("ul"),ds=l("li"),$s=l("a"),al=o("input_ids"),rl=o(": numerical representions of your tokens."),ol=h(),_s=l("li"),gs=l("a"),nl=o("atttention_mask"),ll=o(": indicates which tokens should be attended to."),dr=h(),st=l("p"),il=o("Just like the "),vs=l("a"),pl=o("pipeline()"),fl=o(", the tokenizer will accept a list of inputs. In addition, the tokenizer can also pad and truncate the text to return a batch with uniform length:"),$r=h(),b(at.$$.fragment),_r=h(),rt=l("p"),ul=o("Read the "),ws=l("a"),ml=o("preprocessing"),cl=o(" tutorial for more details about tokenization."),gr=h(),Ce=l("h3"),ot=l("a"),pa=l("span"),b(Nt.$$.fragment),hl=h(),fa=l("span"),dl=o("AutoModel"),vr=h(),b(nt.$$.fragment),wr=h(),b(lt.$$.fragment),kr=h(),X=l("p"),$l=o("Models are a standard "),Dt=l("a"),ua=l("code"),_l=o("torch.nn.Module"),gl=o(" or a "),Ht=l("a"),ma=l("code"),vl=o("tf.keras.Model"),wl=o(" so you can use them in your usual training loop. However, to make things easier, \u{1F917} Transformers provides a "),ks=l("a"),kl=o("Trainer"),yl=o(" class for PyTorch that adds functionality for distributed training, mixed precision, and more. For TensorFlow, you can use the "),ca=l("code"),bl=o("fit"),El=o(" method from "),Lt=l("a"),Al=o("Keras"),Tl=o(". Refer to the "),ys=l("a"),jl=o("training tutorial"),xl=o(" for more details."),yr=h(),b(it.$$.fragment),br=h(),Oe=l("h3"),pt=l("a"),ha=l("span"),b(Ut.$$.fragment),zl=h(),da=l("span"),ql=o("Save a model"),Er=h(),b(ft.$$.fragment),Ar=h(),xe=l("p"),Pl=o("One particularly cool \u{1F917} Transformers feature is the ability to save a model and reload it as either a PyTorch or TensorFlow model. The "),$a=l("code"),Fl=o("from_pt"),Ml=o(" or "),_a=l("code"),Sl=o("from_tf"),Il=o(" parameter can convert the model from one framework to the other:"),Tr=h(),b(ut.$$.fragment),this.h()},l(e){const f=Ep('[data-svelte="svelte-1phssyn"]',document.head);a=i(f,"META",{name:!0,content:!0}),f.forEach(s),m=d(e),r=i(e,"H1",{class:!0});var Wt=p(r);c=i(Wt,"A",{id:!0,class:!0,href:!0});var ga=p(c);_=i(ga,"SPAN",{});var va=p(_);E(v.$$.fragment,va),va.forEach(s),ga.forEach(s),q=d(Wt),I=i(Wt,"SPAN",{});var wa=p(I);w=n(wa,"Quick tour"),wa.forEach(s),Wt.forEach(s),M=d(e),E(C.$$.fragment,e),O=d(e),D=i(e,"P",{});var Ne=p(D);L=n(Ne,"Get up and running with \u{1F917} Transformers! Start using the "),y=i(Ne,"A",{href:!0});var ka=p(y);S=n(ka,"pipeline()"),ka.forEach(s),g=n(Ne," for rapid inference, and quickly load a pretrained model and tokenizer with an "),F=i(Ne,"A",{href:!0});var ya=p(F);R=n(ya,"AutoClass"),ya.forEach(s),U=n(Ne," to solve your text, vision or audio task."),Ne.forEach(s),Q=d(e),E(J.$$.fragment,e),se=d(e),B=i(e,"H2",{class:!0});var Rt=p(B);G=i(Rt,"A",{id:!0,class:!0,href:!0});var ba=p(G);ee=i(ba,"SPAN",{});var Ea=p(ee);E(V.$$.fragment,Ea),Ea.forEach(s),ba.forEach(s),K=d(Rt),ce=i(Rt,"SPAN",{});var Ul=p(ce);re=n(Ul,"Pipeline"),Ul.forEach(s),Rt.forEach(s),de=d(e),oe=i(e,"P",{});var Cl=p(oe);te=i(Cl,"A",{href:!0});var Wl=p(te);ne=n(Wl,"pipeline()"),Wl.forEach(s),$e=n(Cl," is the easiest way to use a pretrained model for a given task."),Cl.forEach(s),z=d(e),E(N.$$.fragment,e),le=d(e),k=i(e,"P",{});var xr=p(k);H=n(xr,"The "),ie=i(xr,"A",{href:!0});var Rl=p(ie);Pe=n(Rl,"pipeline()"),Rl.forEach(s),he=n(xr," supports many common tasks out-of-the-box:"),xr.forEach(s),ge=d(e),pe=i(e,"P",{});var Ol=p(pe);De=i(Ol,"STRONG",{});var Gl=p(De);ve=n(Gl,"Text"),Gl.forEach(s),Gt=n(Ol,":"),Ol.forEach(s),ct=d(e),Y=i(e,"UL",{});var ae=p(Y);Ss=i(ae,"LI",{});var Yl=p(Ss);Yr=n(Yl,"Sentiment analysis: classify the polarity of a given text."),Yl.forEach(s),Jr=d(ae),Is=i(ae,"LI",{});var Jl=p(Is);Qr=n(Jl,"Text generation (in English): generate text from a given input."),Jl.forEach(s),Br=d(ae),Cs=i(ae,"LI",{});var Ql=p(Cs);Vr=n(Ql,"Name entity recognition (NER): label each word with the entity it represents (person, date, location, etc.)."),Ql.forEach(s),Kr=d(ae),Os=i(ae,"LI",{});var Bl=p(Os);Zr=n(Bl,"Question answering: extract the answer from the context, given some context and a question."),Bl.forEach(s),Xr=d(ae),Ns=i(ae,"LI",{});var Vl=p(Ns);eo=n(Vl,"Fill-mask: fill in the blank given a text with masked words."),Vl.forEach(s),to=d(ae),Ds=i(ae,"LI",{});var Kl=p(Ds);so=n(Kl,"Summarization: generate a summary of a long sequence of text or document."),Kl.forEach(s),ao=d(ae),Hs=i(ae,"LI",{});var Zl=p(Hs);ro=n(Zl,"Translation: translate text into another language."),Zl.forEach(s),oo=d(ae),Ls=i(ae,"LI",{});var Xl=p(Ls);no=n(Xl,"Feature extraction: create a tensor representation of the text."),Xl.forEach(s),ae.forEach(s),Aa=d(e),ht=i(e,"P",{});var Nl=p(ht);Us=i(Nl,"STRONG",{});var ei=p(Us);lo=n(ei,"Image"),ei.forEach(s),io=n(Nl,":"),Nl.forEach(s),Ta=d(e),we=i(e,"UL",{});var bs=p(we);Ws=i(bs,"LI",{});var ti=p(Ws);po=n(ti,"Image classification: classify an image."),ti.forEach(s),fo=d(bs),Rs=i(bs,"LI",{});var si=p(Rs);uo=n(si,"Image segmentation: classify every pixel in an image."),si.forEach(s),mo=d(bs),Gs=i(bs,"LI",{});var ai=p(Gs);co=n(ai,"Object detection: detect objects within an image."),ai.forEach(s),bs.forEach(s),ja=d(e),dt=i(e,"P",{});var Dl=p(dt);Ys=i(Dl,"STRONG",{});var ri=p(Ys);ho=n(ri,"Audio"),ri.forEach(s),$o=n(Dl,":"),Dl.forEach(s),xa=d(e),He=i(e,"UL",{});var zr=p(He);Js=i(zr,"LI",{});var oi=p(Js);_o=n(oi,"Audio classification: assign a label to a given segment of audio."),oi.forEach(s),go=d(zr),Qs=i(zr,"LI",{});var ni=p(Qs);vo=n(ni,"Automatic speech recognition (ASR): transcribe audio data into text."),ni.forEach(s),zr.forEach(s),za=d(e),E(Le.$$.fragment,e),qa=d(e),Fe=i(e,"H3",{class:!0});var qr=p(Fe);Ue=i(qr,"A",{id:!0,class:!0,href:!0});var li=p(Ue);Bs=i(li,"SPAN",{});var ii=p(Bs);E($t.$$.fragment,ii),ii.forEach(s),li.forEach(s),wo=d(qr),Vs=i(qr,"SPAN",{});var pi=p(Vs);ko=n(pi,"Pipeline usage"),pi.forEach(s),qr.forEach(s),Pa=d(e),We=i(e,"P",{});var Pr=p(We);yo=n(Pr,"In the following example, you will use the "),Yt=i(Pr,"A",{href:!0});var fi=p(Yt);bo=n(fi,"pipeline()"),fi.forEach(s),Eo=n(Pr," for sentiment analysis."),Pr.forEach(s),Fa=d(e),Jt=i(e,"P",{});var ui=p(Jt);Ao=n(ui,"Install the following dependencies if you haven\u2019t already:"),ui.forEach(s),Ma=d(e),E(Re.$$.fragment,e),Sa=d(e),Ge=i(e,"P",{});var Fr=p(Ge);To=n(Fr,"Import "),Qt=i(Fr,"A",{href:!0});var mi=p(Qt);jo=n(mi,"pipeline()"),mi.forEach(s),xo=n(Fr," and specify the task you want to complete:"),Fr.forEach(s),Ia=d(e),E(_t.$$.fragment,e),Ca=d(e),ke=i(e,"P",{});var Es=p(ke);zo=n(Es,"The pipeline downloads and caches a default "),gt=i(Es,"A",{href:!0,rel:!0});var ci=p(gt);qo=n(ci,"pretrained model"),ci.forEach(s),Po=n(Es," and tokenizer for sentiment analysis. Now you can use the "),Ks=i(Es,"CODE",{});var hi=p(Ks);Fo=n(hi,"classifier"),hi.forEach(s),Mo=n(Es," on your target text:"),Es.forEach(s),Oa=d(e),E(vt.$$.fragment,e),Na=d(e),Ye=i(e,"P",{});var Mr=p(Ye);So=n(Mr,"For more than one sentence, pass a list of sentences to the "),Bt=i(Mr,"A",{href:!0});var di=p(Bt);Io=n(di,"pipeline()"),di.forEach(s),Co=n(Mr," which returns a list of dictionaries:"),Mr.forEach(s),Da=d(e),E(wt.$$.fragment,e),Ha=d(e),ye=i(e,"P",{});var As=p(ye);Oo=n(As,"The "),Vt=i(As,"A",{href:!0});var $i=p(Vt);No=n($i,"pipeline()"),$i.forEach(s),Do=n(As," can also iterate over an entire dataset. Start by installing the "),kt=i(As,"A",{href:!0,rel:!0});var _i=p(kt);Ho=n(_i,"\u{1F917} Datasets"),_i.forEach(s),Lo=n(As," library:"),As.forEach(s),La=d(e),E(yt.$$.fragment,e),Ua=d(e),Je=i(e,"P",{});var Sr=p(Je);Uo=n(Sr,"Create a "),Kt=i(Sr,"A",{href:!0});var gi=p(Kt);Wo=n(gi,"pipeline()"),gi.forEach(s),Ro=n(Sr," with the task you want to solve for and the model you want to use."),Sr.forEach(s),Wa=d(e),E(bt.$$.fragment,e),Ra=d(e),be=i(e,"P",{});var Ts=p(be);Go=n(Ts,"Next, load a dataset (see the \u{1F917} Datasets "),Et=i(Ts,"A",{href:!0,rel:!0});var vi=p(Et);Yo=n(vi,"Quick Start"),vi.forEach(s),Jo=n(Ts," for more details) you\u2019d like to iterate over. For example, let\u2019s load the "),At=i(Ts,"A",{href:!0,rel:!0});var wi=p(At);Qo=n(wi,"MInDS-14"),wi.forEach(s),Bo=n(Ts," dataset:"),Ts.forEach(s),Ga=d(e),E(Tt.$$.fragment,e),Ya=d(e),Zt=i(e,"P",{});var ki=p(Zt);Vo=n(ki,"You can pass a whole dataset pipeline:"),ki.forEach(s),Ja=d(e),E(jt.$$.fragment,e),Qa=d(e),Qe=i(e,"P",{});var Ir=p(Qe);Ko=n(Ir,"For a larger dataset where the inputs are big (like in speech or vision), you will want to pass along a generator instead of a list that loads all the inputs in memory. See the "),Xt=i(Ir,"A",{href:!0});var yi=p(Xt);Zo=n(yi,"pipeline documentation"),yi.forEach(s),Xo=n(Ir," for more information."),Ir.forEach(s),Ba=d(e),Me=i(e,"H3",{class:!0});var Cr=p(Me);Be=i(Cr,"A",{id:!0,class:!0,href:!0});var bi=p(Be);Zs=i(bi,"SPAN",{});var Ei=p(Zs);E(xt.$$.fragment,Ei),Ei.forEach(s),bi.forEach(s),en=d(Cr),Xs=i(Cr,"SPAN",{});var Ai=p(Xs);tn=n(Ai,"Use another model and tokenizer in the pipeline"),Ai.forEach(s),Cr.forEach(s),Va=d(e),fe=i(e,"P",{});var ze=p(fe);sn=n(ze,"The "),es=i(ze,"A",{href:!0});var Ti=p(es);an=n(Ti,"pipeline()"),Ti.forEach(s),rn=n(ze," can accommodate any model from the "),zt=i(ze,"A",{href:!0,rel:!0});var ji=p(zt);on=n(ji,"Model Hub"),ji.forEach(s),nn=n(ze,", making it easy to adapt the "),ts=i(ze,"A",{href:!0});var xi=p(ts);ln=n(xi,"pipeline()"),xi.forEach(s),pn=n(ze," for other use-cases. For example, if you\u2019d like a model capable of handling French text, use the tags on the Model Hub to filter for an appropriate model. The top filtered result returns a multilingual "),qt=i(ze,"A",{href:!0,rel:!0});var zi=p(qt);fn=n(zi,"BERT model"),zi.forEach(s),un=n(ze," fine-tuned for sentiment analysis. Great, let\u2019s use this model!"),ze.forEach(s),Ka=d(e),E(Pt.$$.fragment,e),Za=d(e),E(Ve.$$.fragment,e),Xa=d(e),Ee=i(e,"P",{});var js=p(Ee);mn=n(js,"Then you can specify the model and tokenizer in the "),ss=i(js,"A",{href:!0});var qi=p(ss);cn=n(qi,"pipeline()"),qi.forEach(s),hn=n(js,", and apply the "),ea=i(js,"CODE",{});var Pi=p(ea);dn=n(Pi,"classifier"),Pi.forEach(s),$n=n(js," on your target text:"),js.forEach(s),er=d(e),E(Ft.$$.fragment,e),tr=d(e),Ae=i(e,"P",{});var xs=p(Ae);_n=n(xs,"If you can\u2019t find a model for your use-case, you will need to fine-tune a pretrained model on your data. Take a look at our "),as=i(xs,"A",{href:!0});var Fi=p(as);gn=n(Fi,"fine-tuning tutorial"),Fi.forEach(s),vn=n(xs," to learn how. Finally, after you\u2019ve fine-tuned your pretrained model, please consider sharing it (see tutorial "),rs=i(xs,"A",{href:!0});var Mi=p(rs);wn=n(Mi,"here"),Mi.forEach(s),kn=n(xs,") with the community on the Model Hub to democratize NLP for everyone! \u{1F917}"),xs.forEach(s),sr=d(e),Se=i(e,"H2",{class:!0});var Or=p(Se);Ke=i(Or,"A",{id:!0,class:!0,href:!0});var Si=p(Ke);ta=i(Si,"SPAN",{});var Ii=p(ta);E(Mt.$$.fragment,Ii),Ii.forEach(s),Si.forEach(s),yn=d(Or),sa=i(Or,"SPAN",{});var Ci=p(sa);bn=n(Ci,"AutoClass"),Ci.forEach(s),Or.forEach(s),ar=d(e),E(St.$$.fragment,e),rr=d(e),Z=i(e,"P",{});var ue=p(Z);En=n(ue,"Under the hood, the "),os=i(ue,"A",{href:!0});var Oi=p(os);An=n(Oi,"AutoModelForSequenceClassification"),Oi.forEach(s),Tn=n(ue," and "),ns=i(ue,"A",{href:!0});var Ni=p(ns);jn=n(Ni,"AutoTokenizer"),Ni.forEach(s),xn=n(ue," classes work together to power the "),ls=i(ue,"A",{href:!0});var Di=p(ls);zn=n(Di,"pipeline()"),Di.forEach(s),qn=n(ue,". An "),is=i(ue,"A",{href:!0});var Hi=p(is);Pn=n(Hi,"AutoClass"),Hi.forEach(s),Fn=n(ue," is a shortcut that automatically retrieves the architecture of a pretrained model from it\u2019s name or path. You only need to select the appropriate "),aa=i(ue,"CODE",{});var Li=p(aa);Mn=n(Li,"AutoClass"),Li.forEach(s),Sn=n(ue," for your task and it\u2019s associated tokenizer with "),ps=i(ue,"A",{href:!0});var Ui=p(ps);In=n(Ui,"AutoTokenizer"),Ui.forEach(s),Cn=n(ue,"."),ue.forEach(s),or=d(e),Te=i(e,"P",{});var zs=p(Te);On=n(zs,"Let\u2019s return to our example and see how you can use the "),ra=i(zs,"CODE",{});var Wi=p(ra);Nn=n(Wi,"AutoClass"),Wi.forEach(s),Dn=n(zs," to replicate the results of the "),fs=i(zs,"A",{href:!0});var Ri=p(fs);Hn=n(Ri,"pipeline()"),Ri.forEach(s),Ln=n(zs,"."),zs.forEach(s),nr=d(e),Ie=i(e,"H3",{class:!0});var Nr=p(Ie);Ze=i(Nr,"A",{id:!0,class:!0,href:!0});var Gi=p(Ze);oa=i(Gi,"SPAN",{});var Yi=p(oa);E(It.$$.fragment,Yi),Yi.forEach(s),Gi.forEach(s),Un=d(Nr),na=i(Nr,"SPAN",{});var Ji=p(na);Wn=n(Ji,"AutoTokenizer"),Ji.forEach(s),Nr.forEach(s),lr=d(e),je=i(e,"P",{});var qs=p(je);Rn=n(qs,"A tokenizer is responsible for preprocessing text into a format that is understandable to the model. First, the tokenizer will split the text into words called "),la=i(qs,"EM",{});var Qi=p(la);Gn=n(Qi,"tokens"),Qi.forEach(s),Yn=n(qs,". There are multiple rules that govern the tokenization process, including how to split a word and at what level (learn more about tokenization "),us=i(qs,"A",{href:!0});var Bi=p(us);Jn=n(Bi,"here"),Bi.forEach(s),Qn=n(qs,"). The most important thing to remember though is you need to instantiate the tokenizer with the same model name to ensure you\u2019re using the same tokenization rules a model was pretrained with."),qs.forEach(s),ir=d(e),Xe=i(e,"P",{});var Dr=p(Xe);Bn=n(Dr,"Load a tokenizer with "),ms=i(Dr,"A",{href:!0});var Vi=p(ms);Vn=n(Vi,"AutoTokenizer"),Vi.forEach(s),Kn=n(Dr,":"),Dr.forEach(s),pr=d(e),E(Ct.$$.fragment,e),fr=d(e),et=i(e,"P",{});var Hr=p(et);Zn=n(Hr,"Next, the tokenizer converts the tokens into numbers in order to construct a tensor as input to the model. This is known as the model\u2019s "),ia=i(Hr,"EM",{});var Ki=p(ia);Xn=n(Ki,"vocabulary"),Ki.forEach(s),el=n(Hr,"."),Hr.forEach(s),ur=d(e),cs=i(e,"P",{});var Zi=p(cs);tl=n(Zi,"Pass your text to the tokenizer:"),Zi.forEach(s),mr=d(e),E(Ot.$$.fragment,e),cr=d(e),hs=i(e,"P",{});var Xi=p(hs);sl=n(Xi,"The tokenizer will return a dictionary containing:"),Xi.forEach(s),hr=d(e),tt=i(e,"UL",{});var Lr=p(tt);ds=i(Lr,"LI",{});var Hl=p(ds);$s=i(Hl,"A",{href:!0});var ep=p($s);al=n(ep,"input_ids"),ep.forEach(s),rl=n(Hl,": numerical representions of your tokens."),Hl.forEach(s),ol=d(Lr),_s=i(Lr,"LI",{});var Ll=p(_s);gs=i(Ll,"A",{href:!0});var tp=p(gs);nl=n(tp,"atttention_mask"),tp.forEach(s),ll=n(Ll,": indicates which tokens should be attended to."),Ll.forEach(s),Lr.forEach(s),dr=d(e),st=i(e,"P",{});var Ur=p(st);il=n(Ur,"Just like the "),vs=i(Ur,"A",{href:!0});var sp=p(vs);pl=n(sp,"pipeline()"),sp.forEach(s),fl=n(Ur,", the tokenizer will accept a list of inputs. In addition, the tokenizer can also pad and truncate the text to return a batch with uniform length:"),Ur.forEach(s),$r=d(e),E(at.$$.fragment,e),_r=d(e),rt=i(e,"P",{});var Wr=p(rt);ul=n(Wr,"Read the "),ws=i(Wr,"A",{href:!0});var ap=p(ws);ml=n(ap,"preprocessing"),ap.forEach(s),cl=n(Wr," tutorial for more details about tokenization."),Wr.forEach(s),gr=d(e),Ce=i(e,"H3",{class:!0});var Rr=p(Ce);ot=i(Rr,"A",{id:!0,class:!0,href:!0});var rp=p(ot);pa=i(rp,"SPAN",{});var op=p(pa);E(Nt.$$.fragment,op),op.forEach(s),rp.forEach(s),hl=d(Rr),fa=i(Rr,"SPAN",{});var np=p(fa);dl=n(np,"AutoModel"),np.forEach(s),Rr.forEach(s),vr=d(e),E(nt.$$.fragment,e),wr=d(e),E(lt.$$.fragment,e),kr=d(e),X=i(e,"P",{});var me=p(X);$l=n(me,"Models are a standard "),Dt=i(me,"A",{href:!0,rel:!0});var lp=p(Dt);ua=i(lp,"CODE",{});var ip=p(ua);_l=n(ip,"torch.nn.Module"),ip.forEach(s),lp.forEach(s),gl=n(me," or a "),Ht=i(me,"A",{href:!0,rel:!0});var pp=p(Ht);ma=i(pp,"CODE",{});var fp=p(ma);vl=n(fp,"tf.keras.Model"),fp.forEach(s),pp.forEach(s),wl=n(me," so you can use them in your usual training loop. However, to make things easier, \u{1F917} Transformers provides a "),ks=i(me,"A",{href:!0});var up=p(ks);kl=n(up,"Trainer"),up.forEach(s),yl=n(me," class for PyTorch that adds functionality for distributed training, mixed precision, and more. For TensorFlow, you can use the "),ca=i(me,"CODE",{});var mp=p(ca);bl=n(mp,"fit"),mp.forEach(s),El=n(me," method from "),Lt=i(me,"A",{href:!0,rel:!0});var cp=p(Lt);Al=n(cp,"Keras"),cp.forEach(s),Tl=n(me,". Refer to the "),ys=i(me,"A",{href:!0});var hp=p(ys);jl=n(hp,"training tutorial"),hp.forEach(s),xl=n(me," for more details."),me.forEach(s),yr=d(e),E(it.$$.fragment,e),br=d(e),Oe=i(e,"H3",{class:!0});var Gr=p(Oe);pt=i(Gr,"A",{id:!0,class:!0,href:!0});var dp=p(pt);ha=i(dp,"SPAN",{});var $p=p(ha);E(Ut.$$.fragment,$p),$p.forEach(s),dp.forEach(s),zl=d(Gr),da=i(Gr,"SPAN",{});var _p=p(da);ql=n(_p,"Save a model"),_p.forEach(s),Gr.forEach(s),Er=d(e),E(ft.$$.fragment,e),Ar=d(e),xe=i(e,"P",{});var Ps=p(xe);Pl=n(Ps,"One particularly cool \u{1F917} Transformers feature is the ability to save a model and reload it as either a PyTorch or TensorFlow model. The "),$a=i(Ps,"CODE",{});var gp=p($a);Fl=n(gp,"from_pt"),gp.forEach(s),Ml=n(Ps," or "),_a=i(Ps,"CODE",{});var vp=p(_a);Sl=n(vp,"from_tf"),vp.forEach(s),Il=n(Ps," parameter can convert the model from one framework to the other:"),Ps.forEach(s),Tr=d(e),E(ut.$$.fragment,e),this.h()},h(){$(a,"name","hf:doc:metadata"),$(a,"content",JSON.stringify(rf)),$(c,"id","quick-tour"),$(c,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),$(c,"href","#quick-tour"),$(r,"class","relative group"),$(y,"href","/docs/transformers/pr_16722/en/main_classes/pipelines#transformers.pipeline"),$(F,"href","./model_doc/auto"),$(G,"id","pipeline"),$(G,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),$(G,"href","#pipeline"),$(B,"class","relative group"),$(te,"href","/docs/transformers/pr_16722/en/main_classes/pipelines#transformers.pipeline"),$(ie,"href","/docs/transformers/pr_16722/en/main_classes/pipelines#transformers.pipeline"),$(Ue,"id","pipeline-usage"),$(Ue,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),$(Ue,"href","#pipeline-usage"),$(Fe,"class","relative group"),$(Yt,"href","/docs/transformers/pr_16722/en/main_classes/pipelines#transformers.pipeline"),$(Qt,"href","/docs/transformers/pr_16722/en/main_classes/pipelines#transformers.pipeline"),$(gt,"href","https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english"),$(gt,"rel","nofollow"),$(Bt,"href","/docs/transformers/pr_16722/en/main_classes/pipelines#transformers.pipeline"),$(Vt,"href","/docs/transformers/pr_16722/en/main_classes/pipelines#transformers.pipeline"),$(kt,"href","https://huggingface.co/docs/datasets/"),$(kt,"rel","nofollow"),$(Kt,"href","/docs/transformers/pr_16722/en/main_classes/pipelines#transformers.pipeline"),$(Et,"href","https://huggingface.co/docs/datasets/quickstart.html"),$(Et,"rel","nofollow"),$(At,"href","https://huggingface.co/datasets/PolyAI/minds14"),$(At,"rel","nofollow"),$(Xt,"href","./main_classes/pipelines"),$(Be,"id","use-another-model-and-tokenizer-in-the-pipeline"),$(Be,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),$(Be,"href","#use-another-model-and-tokenizer-in-the-pipeline"),$(Me,"class","relative group"),$(es,"href","/docs/transformers/pr_16722/en/main_classes/pipelines#transformers.pipeline"),$(zt,"href","https://huggingface.co/models"),$(zt,"rel","nofollow"),$(ts,"href","/docs/transformers/pr_16722/en/main_classes/pipelines#transformers.pipeline"),$(qt,"href","https://huggingface.co/nlptown/bert-base-multilingual-uncased-sentiment"),$(qt,"rel","nofollow"),$(ss,"href","/docs/transformers/pr_16722/en/main_classes/pipelines#transformers.pipeline"),$(as,"href","./training"),$(rs,"href","./model_sharing"),$(Ke,"id","autoclass"),$(Ke,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),$(Ke,"href","#autoclass"),$(Se,"class","relative group"),$(os,"href","/docs/transformers/pr_16722/en/model_doc/auto#transformers.AutoModelForSequenceClassification"),$(ns,"href","/docs/transformers/pr_16722/en/model_doc/auto#transformers.AutoTokenizer"),$(ls,"href","/docs/transformers/pr_16722/en/main_classes/pipelines#transformers.pipeline"),$(is,"href","./model_doc/auto"),$(ps,"href","/docs/transformers/pr_16722/en/model_doc/auto#transformers.AutoTokenizer"),$(fs,"href","/docs/transformers/pr_16722/en/main_classes/pipelines#transformers.pipeline"),$(Ze,"id","autotokenizer"),$(Ze,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),$(Ze,"href","#autotokenizer"),$(Ie,"class","relative group"),$(us,"href","./tokenizer_summary"),$(ms,"href","/docs/transformers/pr_16722/en/model_doc/auto#transformers.AutoTokenizer"),$($s,"href","./glossary#input-ids"),$(gs,"href",".glossary#attention-mask"),$(vs,"href","/docs/transformers/pr_16722/en/main_classes/pipelines#transformers.pipeline"),$(ws,"href","./preprocessing"),$(ot,"id","automodel"),$(ot,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),$(ot,"href","#automodel"),$(Ce,"class","relative group"),$(Dt,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),$(Dt,"rel","nofollow"),$(Ht,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),$(Ht,"rel","nofollow"),$(ks,"href","/docs/transformers/pr_16722/en/main_classes/trainer#transformers.Trainer"),$(Lt,"href","https://keras.io/"),$(Lt,"rel","nofollow"),$(ys,"href","./training"),$(pt,"id","save-a-model"),$(pt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),$(pt,"href","#save-a-model"),$(Oe,"class","relative group")},m(e,f){t(document.head,a),u(e,m,f),u(e,r,f),t(r,c),t(c,_),A(v,_,null),t(r,q),t(r,I),t(I,w),u(e,M,f),A(C,e,f),u(e,O,f),u(e,D,f),t(D,L),t(D,y),t(y,S),t(D,g),t(D,F),t(F,R),t(D,U),u(e,Q,f),A(J,e,f),u(e,se,f),u(e,B,f),t(B,G),t(G,ee),A(V,ee,null),t(B,K),t(B,ce),t(ce,re),u(e,de,f),u(e,oe,f),t(oe,te),t(te,ne),t(oe,$e),u(e,z,f),A(N,e,f),u(e,le,f),u(e,k,f),t(k,H),t(k,ie),t(ie,Pe),t(k,he),u(e,ge,f),u(e,pe,f),t(pe,De),t(De,ve),t(pe,Gt),u(e,ct,f),u(e,Y,f),t(Y,Ss),t(Ss,Yr),t(Y,Jr),t(Y,Is),t(Is,Qr),t(Y,Br),t(Y,Cs),t(Cs,Vr),t(Y,Kr),t(Y,Os),t(Os,Zr),t(Y,Xr),t(Y,Ns),t(Ns,eo),t(Y,to),t(Y,Ds),t(Ds,so),t(Y,ao),t(Y,Hs),t(Hs,ro),t(Y,oo),t(Y,Ls),t(Ls,no),u(e,Aa,f),u(e,ht,f),t(ht,Us),t(Us,lo),t(ht,io),u(e,Ta,f),u(e,we,f),t(we,Ws),t(Ws,po),t(we,fo),t(we,Rs),t(Rs,uo),t(we,mo),t(we,Gs),t(Gs,co),u(e,ja,f),u(e,dt,f),t(dt,Ys),t(Ys,ho),t(dt,$o),u(e,xa,f),u(e,He,f),t(He,Js),t(Js,_o),t(He,go),t(He,Qs),t(Qs,vo),u(e,za,f),A(Le,e,f),u(e,qa,f),u(e,Fe,f),t(Fe,Ue),t(Ue,Bs),A($t,Bs,null),t(Fe,wo),t(Fe,Vs),t(Vs,ko),u(e,Pa,f),u(e,We,f),t(We,yo),t(We,Yt),t(Yt,bo),t(We,Eo),u(e,Fa,f),u(e,Jt,f),t(Jt,Ao),u(e,Ma,f),A(Re,e,f),u(e,Sa,f),u(e,Ge,f),t(Ge,To),t(Ge,Qt),t(Qt,jo),t(Ge,xo),u(e,Ia,f),A(_t,e,f),u(e,Ca,f),u(e,ke,f),t(ke,zo),t(ke,gt),t(gt,qo),t(ke,Po),t(ke,Ks),t(Ks,Fo),t(ke,Mo),u(e,Oa,f),A(vt,e,f),u(e,Na,f),u(e,Ye,f),t(Ye,So),t(Ye,Bt),t(Bt,Io),t(Ye,Co),u(e,Da,f),A(wt,e,f),u(e,Ha,f),u(e,ye,f),t(ye,Oo),t(ye,Vt),t(Vt,No),t(ye,Do),t(ye,kt),t(kt,Ho),t(ye,Lo),u(e,La,f),A(yt,e,f),u(e,Ua,f),u(e,Je,f),t(Je,Uo),t(Je,Kt),t(Kt,Wo),t(Je,Ro),u(e,Wa,f),A(bt,e,f),u(e,Ra,f),u(e,be,f),t(be,Go),t(be,Et),t(Et,Yo),t(be,Jo),t(be,At),t(At,Qo),t(be,Bo),u(e,Ga,f),A(Tt,e,f),u(e,Ya,f),u(e,Zt,f),t(Zt,Vo),u(e,Ja,f),A(jt,e,f),u(e,Qa,f),u(e,Qe,f),t(Qe,Ko),t(Qe,Xt),t(Xt,Zo),t(Qe,Xo),u(e,Ba,f),u(e,Me,f),t(Me,Be),t(Be,Zs),A(xt,Zs,null),t(Me,en),t(Me,Xs),t(Xs,tn),u(e,Va,f),u(e,fe,f),t(fe,sn),t(fe,es),t(es,an),t(fe,rn),t(fe,zt),t(zt,on),t(fe,nn),t(fe,ts),t(ts,ln),t(fe,pn),t(fe,qt),t(qt,fn),t(fe,un),u(e,Ka,f),A(Pt,e,f),u(e,Za,f),A(Ve,e,f),u(e,Xa,f),u(e,Ee,f),t(Ee,mn),t(Ee,ss),t(ss,cn),t(Ee,hn),t(Ee,ea),t(ea,dn),t(Ee,$n),u(e,er,f),A(Ft,e,f),u(e,tr,f),u(e,Ae,f),t(Ae,_n),t(Ae,as),t(as,gn),t(Ae,vn),t(Ae,rs),t(rs,wn),t(Ae,kn),u(e,sr,f),u(e,Se,f),t(Se,Ke),t(Ke,ta),A(Mt,ta,null),t(Se,yn),t(Se,sa),t(sa,bn),u(e,ar,f),A(St,e,f),u(e,rr,f),u(e,Z,f),t(Z,En),t(Z,os),t(os,An),t(Z,Tn),t(Z,ns),t(ns,jn),t(Z,xn),t(Z,ls),t(ls,zn),t(Z,qn),t(Z,is),t(is,Pn),t(Z,Fn),t(Z,aa),t(aa,Mn),t(Z,Sn),t(Z,ps),t(ps,In),t(Z,Cn),u(e,or,f),u(e,Te,f),t(Te,On),t(Te,ra),t(ra,Nn),t(Te,Dn),t(Te,fs),t(fs,Hn),t(Te,Ln),u(e,nr,f),u(e,Ie,f),t(Ie,Ze),t(Ze,oa),A(It,oa,null),t(Ie,Un),t(Ie,na),t(na,Wn),u(e,lr,f),u(e,je,f),t(je,Rn),t(je,la),t(la,Gn),t(je,Yn),t(je,us),t(us,Jn),t(je,Qn),u(e,ir,f),u(e,Xe,f),t(Xe,Bn),t(Xe,ms),t(ms,Vn),t(Xe,Kn),u(e,pr,f),A(Ct,e,f),u(e,fr,f),u(e,et,f),t(et,Zn),t(et,ia),t(ia,Xn),t(et,el),u(e,ur,f),u(e,cs,f),t(cs,tl),u(e,mr,f),A(Ot,e,f),u(e,cr,f),u(e,hs,f),t(hs,sl),u(e,hr,f),u(e,tt,f),t(tt,ds),t(ds,$s),t($s,al),t(ds,rl),t(tt,ol),t(tt,_s),t(_s,gs),t(gs,nl),t(_s,ll),u(e,dr,f),u(e,st,f),t(st,il),t(st,vs),t(vs,pl),t(st,fl),u(e,$r,f),A(at,e,f),u(e,_r,f),u(e,rt,f),t(rt,ul),t(rt,ws),t(ws,ml),t(rt,cl),u(e,gr,f),u(e,Ce,f),t(Ce,ot),t(ot,pa),A(Nt,pa,null),t(Ce,hl),t(Ce,fa),t(fa,dl),u(e,vr,f),A(nt,e,f),u(e,wr,f),A(lt,e,f),u(e,kr,f),u(e,X,f),t(X,$l),t(X,Dt),t(Dt,ua),t(ua,_l),t(X,gl),t(X,Ht),t(Ht,ma),t(ma,vl),t(X,wl),t(X,ks),t(ks,kl),t(X,yl),t(X,ca),t(ca,bl),t(X,El),t(X,Lt),t(Lt,Al),t(X,Tl),t(X,ys),t(ys,jl),t(X,xl),u(e,yr,f),A(it,e,f),u(e,br,f),u(e,Oe,f),t(Oe,pt),t(pt,ha),A(Ut,ha,null),t(Oe,zl),t(Oe,da),t(da,ql),u(e,Er,f),A(ft,e,f),u(e,Ar,f),u(e,xe,f),t(xe,Pl),t(xe,$a),t($a,Fl),t(xe,Ml),t(xe,_a),t(_a,Sl),t(xe,Il),u(e,Tr,f),A(ut,e,f),jr=!0},p(e,[f]){const Wt={};f&2&&(Wt.$$scope={dirty:f,ctx:e}),J.$set(Wt);const ga={};f&2&&(ga.$$scope={dirty:f,ctx:e}),Le.$set(ga);const va={};f&2&&(va.$$scope={dirty:f,ctx:e}),Re.$set(va);const wa={};f&2&&(wa.$$scope={dirty:f,ctx:e}),Ve.$set(wa);const Ne={};f&2&&(Ne.$$scope={dirty:f,ctx:e}),at.$set(Ne);const ka={};f&2&&(ka.$$scope={dirty:f,ctx:e}),nt.$set(ka);const ya={};f&2&&(ya.$$scope={dirty:f,ctx:e}),lt.$set(ya);const Rt={};f&2&&(Rt.$$scope={dirty:f,ctx:e}),it.$set(Rt);const ba={};f&2&&(ba.$$scope={dirty:f,ctx:e}),ft.$set(ba);const Ea={};f&2&&(Ea.$$scope={dirty:f,ctx:e}),ut.$set(Ea)},i(e){jr||(T(v.$$.fragment,e),T(C.$$.fragment,e),T(J.$$.fragment,e),T(V.$$.fragment,e),T(N.$$.fragment,e),T(Le.$$.fragment,e),T($t.$$.fragment,e),T(Re.$$.fragment,e),T(_t.$$.fragment,e),T(vt.$$.fragment,e),T(wt.$$.fragment,e),T(yt.$$.fragment,e),T(bt.$$.fragment,e),T(Tt.$$.fragment,e),T(jt.$$.fragment,e),T(xt.$$.fragment,e),T(Pt.$$.fragment,e),T(Ve.$$.fragment,e),T(Ft.$$.fragment,e),T(Mt.$$.fragment,e),T(St.$$.fragment,e),T(It.$$.fragment,e),T(Ct.$$.fragment,e),T(Ot.$$.fragment,e),T(at.$$.fragment,e),T(Nt.$$.fragment,e),T(nt.$$.fragment,e),T(lt.$$.fragment,e),T(it.$$.fragment,e),T(Ut.$$.fragment,e),T(ft.$$.fragment,e),T(ut.$$.fragment,e),jr=!0)},o(e){j(v.$$.fragment,e),j(C.$$.fragment,e),j(J.$$.fragment,e),j(V.$$.fragment,e),j(N.$$.fragment,e),j(Le.$$.fragment,e),j($t.$$.fragment,e),j(Re.$$.fragment,e),j(_t.$$.fragment,e),j(vt.$$.fragment,e),j(wt.$$.fragment,e),j(yt.$$.fragment,e),j(bt.$$.fragment,e),j(Tt.$$.fragment,e),j(jt.$$.fragment,e),j(xt.$$.fragment,e),j(Pt.$$.fragment,e),j(Ve.$$.fragment,e),j(Ft.$$.fragment,e),j(Mt.$$.fragment,e),j(St.$$.fragment,e),j(It.$$.fragment,e),j(Ct.$$.fragment,e),j(Ot.$$.fragment,e),j(at.$$.fragment,e),j(Nt.$$.fragment,e),j(nt.$$.fragment,e),j(lt.$$.fragment,e),j(it.$$.fragment,e),j(Ut.$$.fragment,e),j(ft.$$.fragment,e),j(ut.$$.fragment,e),jr=!1},d(e){s(a),e&&s(m),e&&s(r),x(v),e&&s(M),x(C,e),e&&s(O),e&&s(D),e&&s(Q),x(J,e),e&&s(se),e&&s(B),x(V),e&&s(de),e&&s(oe),e&&s(z),x(N,e),e&&s(le),e&&s(k),e&&s(ge),e&&s(pe),e&&s(ct),e&&s(Y),e&&s(Aa),e&&s(ht),e&&s(Ta),e&&s(we),e&&s(ja),e&&s(dt),e&&s(xa),e&&s(He),e&&s(za),x(Le,e),e&&s(qa),e&&s(Fe),x($t),e&&s(Pa),e&&s(We),e&&s(Fa),e&&s(Jt),e&&s(Ma),x(Re,e),e&&s(Sa),e&&s(Ge),e&&s(Ia),x(_t,e),e&&s(Ca),e&&s(ke),e&&s(Oa),x(vt,e),e&&s(Na),e&&s(Ye),e&&s(Da),x(wt,e),e&&s(Ha),e&&s(ye),e&&s(La),x(yt,e),e&&s(Ua),e&&s(Je),e&&s(Wa),x(bt,e),e&&s(Ra),e&&s(be),e&&s(Ga),x(Tt,e),e&&s(Ya),e&&s(Zt),e&&s(Ja),x(jt,e),e&&s(Qa),e&&s(Qe),e&&s(Ba),e&&s(Me),x(xt),e&&s(Va),e&&s(fe),e&&s(Ka),x(Pt,e),e&&s(Za),x(Ve,e),e&&s(Xa),e&&s(Ee),e&&s(er),x(Ft,e),e&&s(tr),e&&s(Ae),e&&s(sr),e&&s(Se),x(Mt),e&&s(ar),x(St,e),e&&s(rr),e&&s(Z),e&&s(or),e&&s(Te),e&&s(nr),e&&s(Ie),x(It),e&&s(lr),e&&s(je),e&&s(ir),e&&s(Xe),e&&s(pr),x(Ct,e),e&&s(fr),e&&s(et),e&&s(ur),e&&s(cs),e&&s(mr),x(Ot,e),e&&s(cr),e&&s(hs),e&&s(hr),e&&s(tt),e&&s(dr),e&&s(st),e&&s($r),x(at,e),e&&s(_r),e&&s(rt),e&&s(gr),e&&s(Ce),x(Nt),e&&s(vr),x(nt,e),e&&s(wr),x(lt,e),e&&s(kr),e&&s(X),e&&s(yr),x(it,e),e&&s(br),e&&s(Oe),x(Ut),e&&s(Er),x(ft,e),e&&s(Ar),e&&s(xe),e&&s(Tr),x(ut,e)}}}const rf={local:"quick-tour",sections:[{local:"pipeline",sections:[{local:"pipeline-usage",title:"Pipeline usage"},{local:"use-another-model-and-tokenizer-in-the-pipeline",title:"Use another model and tokenizer in the pipeline"}],title:"Pipeline"},{local:"autoclass",sections:[{local:"autotokenizer",title:"AutoTokenizer"},{local:"automodel",title:"AutoModel"},{local:"save-a-model",title:"Save a model"}],title:"AutoClass"}],title:"Quick tour"};function of(P){return Ap(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class hf extends kp{constructor(a){super();yp(this,a,of,af,bp,{})}}export{hf as default,rf as metadata};
