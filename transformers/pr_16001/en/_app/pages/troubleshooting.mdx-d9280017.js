import{S as Bs,i as Gs,s as Os,e as o,k as m,w as d,t as n,M as Hs,c as s,d as t,m as p,a,x as v,h as i,b as h,F as r,g as f,y as g,q as w,o as _,B as y}from"../chunks/vendor-4833417e.js";import{T as Rs}from"../chunks/Tip-fffd6df1.js";import{I as We}from"../chunks/IconCopyLink-4b81c553.js";import{C}from"../chunks/CodeBlock-6a3d1b46.js";import"../chunks/CopyButton-dacfbfaf.js";function Ys(Je){let u,A,c,$,I;return{c(){u=o("p"),A=n("Refer to the Performance "),c=o("a"),$=n("guide"),I=n(" for more details about memory-saving techniques."),this.h()},l(b){u=s(b,"P",{});var j=a(u);A=i(j,"Refer to the Performance "),c=s(j,"A",{href:!0});var M=a(c);$=i(M,"guide"),M.forEach(t),I=i(j," for more details about memory-saving techniques."),j.forEach(t),this.h()},h(){h(c,"href","performance")},m(b,j){f(b,u,j),r(u,A),r(u,c),r(c,$),r(u,I)},d(b){b&&t(u)}}}function Ks(Je){let u,A,c,$,I,b,j,M,cr,gt,Pe,dr,wt,P,k,vr,Z,gr,wr,ee,_r,yr,te,$r,br,Er,re,kr,oe,Tr,Ar,jr,se,Pr,xe,xr,Cr,_t,L,Ir,ae,Ur,Fr,yt,U,N,Qe,le,Dr,Xe,qr,$t,Ce,Sr,bt,ne,Et,B,zr,Ie,Mr,Lr,kt,F,G,Ze,ie,Nr,et,Br,Tt,Ue,Gr,At,fe,jt,Fe,Or,Pt,E,he,Hr,De,tt,Rr,Yr,Kr,me,Vr,qe,Wr,Jr,Qr,pe,Xr,Se,rt,Zr,eo,to,ue,ro,ze,ot,oo,so,xt,O,Ct,D,H,st,ce,ao,at,lo,It,R,no,de,io,fo,Ut,Me,T,ho,lt,mo,po,ve,nt,uo,co,Le,vo,go,Ft,ge,Dt,Ne,q,wo,it,_o,yo,Be,$o,bo,qt,we,St,S,Y,ft,_e,Eo,ht,ko,zt,K,To,mt,Ao,jo,Mt,ye,Lt,Ge,Po,Nt,$e,Bt,z,V,pt,be,xo,ut,Co,Gt,Oe,Io,Ot,Ee,Ht,He,Uo,Rt,ke,Yt,x,Fo,ct,Do,qo,dt,So,zo,Kt,Re,Mo,Vt,Te,Wt,W,Lo,vt,No,Bo,Jt,Ae,Qt;return b=new We({}),le=new We({}),ne=new C({props:{code:`ValueError: Connection error, and we cannot find the requested files in the cached path.
Please try again or make sure your Internet connection is on.`,highlighted:`ValueError: Connection error, <span class="hljs-built_in">and</span> we cannot <span class="hljs-keyword">find</span> the requested <span class="hljs-keyword">files</span> in the cached path.
Please <span class="hljs-keyword">try</span> again <span class="hljs-built_in">or</span> <span class="hljs-keyword">make</span> sure your Internet connection <span class="hljs-keyword">is</span> <span class="hljs-keyword">on</span>.`}}),ie=new We({}),fe=new C({props:{code:"CUDA out of memory. Tried to allocate 256.00 MiB (GPU 0; 11.17 GiB total capacity; 9.70 GiB already allocated; 179.81 MiB free; 9.85 GiB reserved in total by PyTorch)",highlighted:'<span class="hljs-attribute">CUDA</span> out of memory. Tried to allocate <span class="hljs-number">256</span>.<span class="hljs-number">00</span> MiB (GPU <span class="hljs-number">0</span>; <span class="hljs-number">11</span>.<span class="hljs-number">17</span> GiB total capacity; <span class="hljs-number">9</span>.<span class="hljs-number">70</span> GiB already allocated; <span class="hljs-number">179</span>.<span class="hljs-number">81</span> MiB free; <span class="hljs-number">9</span>.<span class="hljs-number">85</span> GiB reserved in total by PyTorch)'}}),O=new Rs({props:{$$slots:{default:[Ys]},$$scope:{ctx:Je}}}),ce=new We({}),ge=new C({props:{code:`from transformers import TFPreTrainedModel
from tensorflow import keras

model.save_weights("some_folder/tf_model.h5")
model = TFPreTrainedModel.from_pretrained("some_folder")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFPreTrainedModel
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> tensorflow <span class="hljs-keyword">import</span> keras

<span class="hljs-meta">&gt;&gt;&gt; </span>model.save_weights(<span class="hljs-string">&quot;some_folder/tf_model.h5&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFPreTrainedModel.from_pretrained(<span class="hljs-string">&quot;some_folder&quot;</span>)`}}),we=new C({props:{code:`from transformers import TFPreTrainedModel

model.save_pretrained("path_to/model")
model = TFPreTrainedModel.from_pretrained("path_to/model")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFPreTrainedModel

<span class="hljs-meta">&gt;&gt;&gt; </span>model.save_pretrained(<span class="hljs-string">&quot;path_to/model&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFPreTrainedModel.from_pretrained(<span class="hljs-string">&quot;path_to/model&quot;</span>)`}}),_e=new We({}),ye=new C({props:{code:"ImportError: cannot import name 'ImageGPTFeatureExtractor' from 'transformers' (unknown location)",highlighted:'ImportError: cannot <span class="hljs-keyword">import</span> <span class="hljs-type">name</span> <span class="hljs-string">&#x27;ImageGPTFeatureExtractor&#x27;</span> <span class="hljs-keyword">from</span> <span class="hljs-string">&#x27;transformers&#x27;</span> (<span class="hljs-type">unknown</span> <span class="hljs-keyword">location</span>)'}}),$e=new C({props:{code:"pip install transformers",highlighted:"pip install transformers"}}),be=new We({}),Ee=new C({props:{code:"RuntimeError: CUDA error: device-side assert triggered",highlighted:'RuntimeError: CUDA <span class="hljs-literal">error</span>: device-<span class="hljs-literal">side</span> <span class="hljs-keyword">assert</span> triggered'}}),ke=new C({props:{code:`import os

os.environ["CUDA_LAUNCH_BLOCKING"] = "1"`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> os

<span class="hljs-meta">&gt;&gt;&gt; </span>os.environ[<span class="hljs-string">&quot;CUDA_LAUNCH_BLOCKING&quot;</span>] = <span class="hljs-string">&quot;1&quot;</span>`}}),Te=new C({props:{code:"--max_source_length 512",highlighted:"--max_source_length 512"}}),Ae=new C({props:{code:`from transformers import T5Tokenizer

tokenizer = T5Tokenizer.from_pretrained("t5-small")
max_source_length = 512

encoded_input = tokenizer(sentences, padding=True, truncation=True, max_length=max_source_length)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> T5Tokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = T5Tokenizer.from_pretrained(<span class="hljs-string">&quot;t5-small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>max_source_length = <span class="hljs-number">512</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>encoded_input = tokenizer(sentences, padding=<span class="hljs-literal">True</span>, truncation=<span class="hljs-literal">True</span>, max_length=max_source_length)`}}),{c(){u=o("meta"),A=m(),c=o("h1"),$=o("a"),I=o("span"),d(b.$$.fragment),j=m(),M=o("span"),cr=n("Troubleshoot"),gt=m(),Pe=o("p"),dr=n("Sometimes errors occur, but we are here to help! This guide covers some of the most common issues we\u2019ve seen and how you can resolve them. However, this guide isn\u2019t meant to be a comprehensive collection of every \u{1F917} Transformers issue. For more help with troubleshooting your issue, try:"),wt=m(),P=o("ol"),k=o("li"),vr=n("Asking for help on the "),Z=o("a"),gr=n("forums"),wr=n(". There are specific categories you can post your question to, like "),ee=o("a"),_r=n("Beginners"),yr=n(" or "),te=o("a"),$r=n("\u{1F917} Transformers"),br=n(". Make sure you write a good descriptive forum post with some reproducible code to maximize the likelihood that your problem is solved!"),Er=m(),re=o("li"),kr=n("Create an "),oe=o("a"),Tr=n("Issue"),Ar=n(" on the \u{1F917} Transformers repository if it is a bug related to the library. Try to include as much information describing the bug as possible to help us better figure out what\u2019s wrong and how we can fix it."),jr=m(),se=o("li"),Pr=n("Check the "),xe=o("a"),xr=n("Migration"),Cr=n(" guide if you use an older version of \u{1F917} Transformers since some important changes have been introduced between versions."),_t=m(),L=o("p"),Ir=n("For more details about troubleshooting and getting help, take a look at "),ae=o("a"),Ur=n("Chapter 8"),Fr=n(" of the Hugging Face course."),yt=m(),U=o("h2"),N=o("a"),Qe=o("span"),d(le.$$.fragment),Dr=m(),Xe=o("span"),qr=n("Firewalled environments"),$t=m(),Ce=o("p"),Sr=n("Some GPU instances on cloud and intranet setups are firewalled to external connections, resulting in a connection error. When your script attempts to download model weights or datasets, the download will hang and then timeout with the following message:"),bt=m(),d(ne.$$.fragment),Et=m(),B=o("p"),zr=n("In this case, you should try to run \u{1F917} Transformers on "),Ie=o("a"),Mr=n("offline mode"),Lr=n(" to avoid the connection error."),kt=m(),F=o("h2"),G=o("a"),Ze=o("span"),d(ie.$$.fragment),Nr=m(),et=o("span"),Br=n("CUDA out of memory"),Tt=m(),Ue=o("p"),Gr=n("Training large models with millions of parameters can be challenging without the appropriate hardware. A common error you may encounter when the GPU runs out of memory is:"),At=m(),d(fe.$$.fragment),jt=m(),Fe=o("p"),Or=n("There are several potential solutions you can try:"),Pt=m(),E=o("ul"),he=o("li"),Hr=n("Reduce the "),De=o("a"),tt=o("code"),Rr=n("per_device_train_batch_size"),Yr=n(" value in [`TrainingArguments]."),Kr=m(),me=o("li"),Vr=n("Use an alternative optimizer like "),qe=o("a"),Wr=n("Adafactor"),Jr=n(" to reduce your memory footprint."),Qr=m(),pe=o("li"),Xr=n("Try using "),Se=o("a"),rt=o("code"),Zr=n("gradient_accumulation_steps"),eo=n(" in [`TrainingArguments] to effectively increase overall batch size."),to=m(),ue=o("li"),ro=n("Try mixed precision training with "),ze=o("a"),ot=o("code"),oo=n("fp16"),so=n("."),xt=m(),d(O.$$.fragment),Ct=m(),D=o("h2"),H=o("a"),st=o("span"),d(ce.$$.fragment),ao=m(),at=o("span"),lo=n("Unable to load a saved TensorFlow model"),It=m(),R=o("p"),no=n("TensorFlow\u2019s "),de=o("a"),io=n("model.save"),fo=n(" method will save the entire model - architecture, weights, training configuration - in a single file. However, when you load the model file again, you may run into an error because \u{1F917} Transformers may not load all the TensorFlow-related objects in the model file. To avoid issues with saving and loading TensorFlow models, we recommend you:"),Ut=m(),Me=o("ul"),T=o("li"),ho=n("Save the model weights as a "),lt=o("code"),mo=n("h5"),po=n(" file extension with "),ve=o("a"),nt=o("code"),uo=n("model.save_weights"),co=n(" and then reload the model with "),Le=o("a"),vo=n("from_pretrained()"),go=n(":"),Ft=m(),d(ge.$$.fragment),Dt=m(),Ne=o("ul"),q=o("li"),wo=n("Save the model with "),it=o("code"),_o=n("save_pretrained"),yo=n(" and load it again with "),Be=o("a"),$o=n("from_pretrained()"),bo=n(":"),qt=m(),d(we.$$.fragment),St=m(),S=o("h2"),Y=o("a"),ft=o("span"),d(_e.$$.fragment),Eo=m(),ht=o("span"),ko=n("ImportError"),zt=m(),K=o("p"),To=n("Another common error you may encounter, especially if it is a newly released model, is "),mt=o("code"),Ao=n("ImportError"),jo=n(":"),Mt=m(),d(ye.$$.fragment),Lt=m(),Ge=o("p"),Po=n("For these error types, check to make sure you have the latest version of \u{1F917} Transformers installed to access the most recent models:"),Nt=m(),d($e.$$.fragment),Bt=m(),z=o("h2"),V=o("a"),pt=o("span"),d(be.$$.fragment),xo=m(),ut=o("span"),Co=n("CUDA error: device-side assert triggered"),Gt=m(),Oe=o("p"),Io=n("Sometimes you may run into a generic CUDA error about an error in the device code."),Ot=m(),d(Ee.$$.fragment),Ht=m(),He=o("p"),Uo=n("You should try to run the code on a CPU first to get a more descriptive error message. Add the following environment variable to the beginning of your code to get a better traceback:"),Rt=m(),d(ke.$$.fragment),Yt=m(),x=o("p"),Fo=n("Often, the error occurs when there is a mismatch between the number of labels and outputs, which produces an "),ct=o("code"),Do=n("IndexError"),qo=n(". You can try to fix this by truncating sequences to the "),dt=o("code"),So=n("max_source_length"),zo=n(" supported by the model you\u2019re using."),Kt=m(),Re=o("p"),Mo=n("If you are using a script, add the following argument to your script:"),Vt=m(),d(Te.$$.fragment),Wt=m(),W=o("p"),Lo=n("If using a notebook, specify the maximum length in the tokenizer\u2019s "),vt=o("code"),No=n("max_length"),Bo=n(" argument. You should also restart your notebook to flush the earlier CUDA assertions."),Jt=m(),d(Ae.$$.fragment),this.h()},l(e){const l=Hs('[data-svelte="svelte-1phssyn"]',document.head);u=s(l,"META",{name:!0,content:!0}),l.forEach(t),A=p(e),c=s(e,"H1",{class:!0});var je=a(c);$=s(je,"A",{id:!0,class:!0,href:!0});var Go=a($);I=s(Go,"SPAN",{});var Oo=a(I);v(b.$$.fragment,Oo),Oo.forEach(t),Go.forEach(t),j=p(je),M=s(je,"SPAN",{});var Ho=a(M);cr=i(Ho,"Troubleshoot"),Ho.forEach(t),je.forEach(t),gt=p(e),Pe=s(e,"P",{});var Ro=a(Pe);dr=i(Ro,"Sometimes errors occur, but we are here to help! This guide covers some of the most common issues we\u2019ve seen and how you can resolve them. However, this guide isn\u2019t meant to be a comprehensive collection of every \u{1F917} Transformers issue. For more help with troubleshooting your issue, try:"),Ro.forEach(t),wt=p(e),P=s(e,"OL",{});var Ye=a(P);k=s(Ye,"LI",{});var J=a(k);vr=i(J,"Asking for help on the "),Z=s(J,"A",{href:!0,rel:!0});var Yo=a(Z);gr=i(Yo,"forums"),Yo.forEach(t),wr=i(J,". There are specific categories you can post your question to, like "),ee=s(J,"A",{href:!0,rel:!0});var Ko=a(ee);_r=i(Ko,"Beginners"),Ko.forEach(t),yr=i(J," or "),te=s(J,"A",{href:!0,rel:!0});var Vo=a(te);$r=i(Vo,"\u{1F917} Transformers"),Vo.forEach(t),br=i(J,". Make sure you write a good descriptive forum post with some reproducible code to maximize the likelihood that your problem is solved!"),J.forEach(t),Er=p(Ye),re=s(Ye,"LI",{});var Xt=a(re);kr=i(Xt,"Create an "),oe=s(Xt,"A",{href:!0,rel:!0});var Wo=a(oe);Tr=i(Wo,"Issue"),Wo.forEach(t),Ar=i(Xt," on the \u{1F917} Transformers repository if it is a bug related to the library. Try to include as much information describing the bug as possible to help us better figure out what\u2019s wrong and how we can fix it."),Xt.forEach(t),jr=p(Ye),se=s(Ye,"LI",{});var Zt=a(se);Pr=i(Zt,"Check the "),xe=s(Zt,"A",{href:!0});var Jo=a(xe);xr=i(Jo,"Migration"),Jo.forEach(t),Cr=i(Zt," guide if you use an older version of \u{1F917} Transformers since some important changes have been introduced between versions."),Zt.forEach(t),Ye.forEach(t),_t=p(e),L=s(e,"P",{});var er=a(L);Ir=i(er,"For more details about troubleshooting and getting help, take a look at "),ae=s(er,"A",{href:!0,rel:!0});var Qo=a(ae);Ur=i(Qo,"Chapter 8"),Qo.forEach(t),Fr=i(er," of the Hugging Face course."),er.forEach(t),yt=p(e),U=s(e,"H2",{class:!0});var tr=a(U);N=s(tr,"A",{id:!0,class:!0,href:!0});var Xo=a(N);Qe=s(Xo,"SPAN",{});var Zo=a(Qe);v(le.$$.fragment,Zo),Zo.forEach(t),Xo.forEach(t),Dr=p(tr),Xe=s(tr,"SPAN",{});var es=a(Xe);qr=i(es,"Firewalled environments"),es.forEach(t),tr.forEach(t),$t=p(e),Ce=s(e,"P",{});var ts=a(Ce);Sr=i(ts,"Some GPU instances on cloud and intranet setups are firewalled to external connections, resulting in a connection error. When your script attempts to download model weights or datasets, the download will hang and then timeout with the following message:"),ts.forEach(t),bt=p(e),v(ne.$$.fragment,e),Et=p(e),B=s(e,"P",{});var rr=a(B);zr=i(rr,"In this case, you should try to run \u{1F917} Transformers on "),Ie=s(rr,"A",{href:!0});var rs=a(Ie);Mr=i(rs,"offline mode"),rs.forEach(t),Lr=i(rr," to avoid the connection error."),rr.forEach(t),kt=p(e),F=s(e,"H2",{class:!0});var or=a(F);G=s(or,"A",{id:!0,class:!0,href:!0});var os=a(G);Ze=s(os,"SPAN",{});var ss=a(Ze);v(ie.$$.fragment,ss),ss.forEach(t),os.forEach(t),Nr=p(or),et=s(or,"SPAN",{});var as=a(et);Br=i(as,"CUDA out of memory"),as.forEach(t),or.forEach(t),Tt=p(e),Ue=s(e,"P",{});var ls=a(Ue);Gr=i(ls,"Training large models with millions of parameters can be challenging without the appropriate hardware. A common error you may encounter when the GPU runs out of memory is:"),ls.forEach(t),At=p(e),v(fe.$$.fragment,e),jt=p(e),Fe=s(e,"P",{});var ns=a(Fe);Or=i(ns,"There are several potential solutions you can try:"),ns.forEach(t),Pt=p(e),E=s(e,"UL",{});var Q=a(E);he=s(Q,"LI",{});var sr=a(he);Hr=i(sr,"Reduce the "),De=s(sr,"A",{href:!0});var is=a(De);tt=s(is,"CODE",{});var fs=a(tt);Rr=i(fs,"per_device_train_batch_size"),fs.forEach(t),is.forEach(t),Yr=i(sr," value in [`TrainingArguments]."),sr.forEach(t),Kr=p(Q),me=s(Q,"LI",{});var ar=a(me);Vr=i(ar,"Use an alternative optimizer like "),qe=s(ar,"A",{href:!0});var hs=a(qe);Wr=i(hs,"Adafactor"),hs.forEach(t),Jr=i(ar," to reduce your memory footprint."),ar.forEach(t),Qr=p(Q),pe=s(Q,"LI",{});var lr=a(pe);Xr=i(lr,"Try using "),Se=s(lr,"A",{href:!0});var ms=a(Se);rt=s(ms,"CODE",{});var ps=a(rt);Zr=i(ps,"gradient_accumulation_steps"),ps.forEach(t),ms.forEach(t),eo=i(lr," in [`TrainingArguments] to effectively increase overall batch size."),lr.forEach(t),to=p(Q),ue=s(Q,"LI",{});var nr=a(ue);ro=i(nr,"Try mixed precision training with "),ze=s(nr,"A",{href:!0});var us=a(ze);ot=s(us,"CODE",{});var cs=a(ot);oo=i(cs,"fp16"),cs.forEach(t),us.forEach(t),so=i(nr,"."),nr.forEach(t),Q.forEach(t),xt=p(e),v(O.$$.fragment,e),Ct=p(e),D=s(e,"H2",{class:!0});var ir=a(D);H=s(ir,"A",{id:!0,class:!0,href:!0});var ds=a(H);st=s(ds,"SPAN",{});var vs=a(st);v(ce.$$.fragment,vs),vs.forEach(t),ds.forEach(t),ao=p(ir),at=s(ir,"SPAN",{});var gs=a(at);lo=i(gs,"Unable to load a saved TensorFlow model"),gs.forEach(t),ir.forEach(t),It=p(e),R=s(e,"P",{});var fr=a(R);no=i(fr,"TensorFlow\u2019s "),de=s(fr,"A",{href:!0,rel:!0});var ws=a(de);io=i(ws,"model.save"),ws.forEach(t),fo=i(fr," method will save the entire model - architecture, weights, training configuration - in a single file. However, when you load the model file again, you may run into an error because \u{1F917} Transformers may not load all the TensorFlow-related objects in the model file. To avoid issues with saving and loading TensorFlow models, we recommend you:"),fr.forEach(t),Ut=p(e),Me=s(e,"UL",{});var _s=a(Me);T=s(_s,"LI",{});var X=a(T);ho=i(X,"Save the model weights as a "),lt=s(X,"CODE",{});var ys=a(lt);mo=i(ys,"h5"),ys.forEach(t),po=i(X," file extension with "),ve=s(X,"A",{href:!0,rel:!0});var $s=a(ve);nt=s($s,"CODE",{});var bs=a(nt);uo=i(bs,"model.save_weights"),bs.forEach(t),$s.forEach(t),co=i(X," and then reload the model with "),Le=s(X,"A",{href:!0});var Es=a(Le);vo=i(Es,"from_pretrained()"),Es.forEach(t),go=i(X,":"),X.forEach(t),_s.forEach(t),Ft=p(e),v(ge.$$.fragment,e),Dt=p(e),Ne=s(e,"UL",{});var ks=a(Ne);q=s(ks,"LI",{});var Ke=a(q);wo=i(Ke,"Save the model with "),it=s(Ke,"CODE",{});var Ts=a(it);_o=i(Ts,"save_pretrained"),Ts.forEach(t),yo=i(Ke," and load it again with "),Be=s(Ke,"A",{href:!0});var As=a(Be);$o=i(As,"from_pretrained()"),As.forEach(t),bo=i(Ke,":"),Ke.forEach(t),ks.forEach(t),qt=p(e),v(we.$$.fragment,e),St=p(e),S=s(e,"H2",{class:!0});var hr=a(S);Y=s(hr,"A",{id:!0,class:!0,href:!0});var js=a(Y);ft=s(js,"SPAN",{});var Ps=a(ft);v(_e.$$.fragment,Ps),Ps.forEach(t),js.forEach(t),Eo=p(hr),ht=s(hr,"SPAN",{});var xs=a(ht);ko=i(xs,"ImportError"),xs.forEach(t),hr.forEach(t),zt=p(e),K=s(e,"P",{});var mr=a(K);To=i(mr,"Another common error you may encounter, especially if it is a newly released model, is "),mt=s(mr,"CODE",{});var Cs=a(mt);Ao=i(Cs,"ImportError"),Cs.forEach(t),jo=i(mr,":"),mr.forEach(t),Mt=p(e),v(ye.$$.fragment,e),Lt=p(e),Ge=s(e,"P",{});var Is=a(Ge);Po=i(Is,"For these error types, check to make sure you have the latest version of \u{1F917} Transformers installed to access the most recent models:"),Is.forEach(t),Nt=p(e),v($e.$$.fragment,e),Bt=p(e),z=s(e,"H2",{class:!0});var pr=a(z);V=s(pr,"A",{id:!0,class:!0,href:!0});var Us=a(V);pt=s(Us,"SPAN",{});var Fs=a(pt);v(be.$$.fragment,Fs),Fs.forEach(t),Us.forEach(t),xo=p(pr),ut=s(pr,"SPAN",{});var Ds=a(ut);Co=i(Ds,"CUDA error: device-side assert triggered"),Ds.forEach(t),pr.forEach(t),Gt=p(e),Oe=s(e,"P",{});var qs=a(Oe);Io=i(qs,"Sometimes you may run into a generic CUDA error about an error in the device code."),qs.forEach(t),Ot=p(e),v(Ee.$$.fragment,e),Ht=p(e),He=s(e,"P",{});var Ss=a(He);Uo=i(Ss,"You should try to run the code on a CPU first to get a more descriptive error message. Add the following environment variable to the beginning of your code to get a better traceback:"),Ss.forEach(t),Rt=p(e),v(ke.$$.fragment,e),Yt=p(e),x=s(e,"P",{});var Ve=a(x);Fo=i(Ve,"Often, the error occurs when there is a mismatch between the number of labels and outputs, which produces an "),ct=s(Ve,"CODE",{});var zs=a(ct);Do=i(zs,"IndexError"),zs.forEach(t),qo=i(Ve,". You can try to fix this by truncating sequences to the "),dt=s(Ve,"CODE",{});var Ms=a(dt);So=i(Ms,"max_source_length"),Ms.forEach(t),zo=i(Ve," supported by the model you\u2019re using."),Ve.forEach(t),Kt=p(e),Re=s(e,"P",{});var Ls=a(Re);Mo=i(Ls,"If you are using a script, add the following argument to your script:"),Ls.forEach(t),Vt=p(e),v(Te.$$.fragment,e),Wt=p(e),W=s(e,"P",{});var ur=a(W);Lo=i(ur,"If using a notebook, specify the maximum length in the tokenizer\u2019s "),vt=s(ur,"CODE",{});var Ns=a(vt);No=i(Ns,"max_length"),Ns.forEach(t),Bo=i(ur," argument. You should also restart your notebook to flush the earlier CUDA assertions."),ur.forEach(t),Jt=p(e),v(Ae.$$.fragment,e),this.h()},h(){h(u,"name","hf:doc:metadata"),h(u,"content",JSON.stringify(Vs)),h($,"id","troubleshoot"),h($,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h($,"href","#troubleshoot"),h(c,"class","relative group"),h(Z,"href","https://discuss.huggingface.co/"),h(Z,"rel","nofollow"),h(ee,"href","https://discuss.huggingface.co/c/beginners/5"),h(ee,"rel","nofollow"),h(te,"href","https://discuss.huggingface.co/c/transformers/9"),h(te,"rel","nofollow"),h(oe,"href","https://github.com/huggingface/transformers/issues/new/choose"),h(oe,"rel","nofollow"),h(xe,"href","migration"),h(ae,"href","https://huggingface.co/course/chapter8/1?fw=pt"),h(ae,"rel","nofollow"),h(N,"id","firewalled-environments"),h(N,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(N,"href","#firewalled-environments"),h(U,"class","relative group"),h(Ie,"href","installation#offline-mode"),h(G,"id","cuda-out-of-memory"),h(G,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(G,"href","#cuda-out-of-memory"),h(F,"class","relative group"),h(De,"href","main_classes/trainer#transformers.TrainingArguments.per_device_train_batch_size"),h(qe,"href","/docs/transformers/pr_16001/en/main_classes/optimizer_schedules#transformers.Adafactor"),h(Se,"href","main_classes/trainer#transformers.TrainingArguments.gradient_accumulation_steps"),h(ze,"href","main_classes/trainer#transformers.TrainingArguments.fp16"),h(H,"id","unable-to-load-a-saved-tensorflow-model"),h(H,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(H,"href","#unable-to-load-a-saved-tensorflow-model"),h(D,"class","relative group"),h(de,"href","https://www.tensorflow.org/tutorials/keras/save_and_load#save_the_entire_model"),h(de,"rel","nofollow"),h(ve,"href","https://www.tensorflow.org/tutorials/keras/save_and_load#save_the_entire_model"),h(ve,"rel","nofollow"),h(Le,"href","/docs/transformers/pr_16001/en/main_classes/model#transformers.TFPreTrainedModel.from_pretrained"),h(Be,"href","/docs/transformers/pr_16001/en/main_classes/model#transformers.TFPreTrainedModel.from_pretrained"),h(Y,"id","importerror"),h(Y,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(Y,"href","#importerror"),h(S,"class","relative group"),h(V,"id","cuda-error-deviceside-assert-triggered"),h(V,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(V,"href","#cuda-error-deviceside-assert-triggered"),h(z,"class","relative group")},m(e,l){r(document.head,u),f(e,A,l),f(e,c,l),r(c,$),r($,I),g(b,I,null),r(c,j),r(c,M),r(M,cr),f(e,gt,l),f(e,Pe,l),r(Pe,dr),f(e,wt,l),f(e,P,l),r(P,k),r(k,vr),r(k,Z),r(Z,gr),r(k,wr),r(k,ee),r(ee,_r),r(k,yr),r(k,te),r(te,$r),r(k,br),r(P,Er),r(P,re),r(re,kr),r(re,oe),r(oe,Tr),r(re,Ar),r(P,jr),r(P,se),r(se,Pr),r(se,xe),r(xe,xr),r(se,Cr),f(e,_t,l),f(e,L,l),r(L,Ir),r(L,ae),r(ae,Ur),r(L,Fr),f(e,yt,l),f(e,U,l),r(U,N),r(N,Qe),g(le,Qe,null),r(U,Dr),r(U,Xe),r(Xe,qr),f(e,$t,l),f(e,Ce,l),r(Ce,Sr),f(e,bt,l),g(ne,e,l),f(e,Et,l),f(e,B,l),r(B,zr),r(B,Ie),r(Ie,Mr),r(B,Lr),f(e,kt,l),f(e,F,l),r(F,G),r(G,Ze),g(ie,Ze,null),r(F,Nr),r(F,et),r(et,Br),f(e,Tt,l),f(e,Ue,l),r(Ue,Gr),f(e,At,l),g(fe,e,l),f(e,jt,l),f(e,Fe,l),r(Fe,Or),f(e,Pt,l),f(e,E,l),r(E,he),r(he,Hr),r(he,De),r(De,tt),r(tt,Rr),r(he,Yr),r(E,Kr),r(E,me),r(me,Vr),r(me,qe),r(qe,Wr),r(me,Jr),r(E,Qr),r(E,pe),r(pe,Xr),r(pe,Se),r(Se,rt),r(rt,Zr),r(pe,eo),r(E,to),r(E,ue),r(ue,ro),r(ue,ze),r(ze,ot),r(ot,oo),r(ue,so),f(e,xt,l),g(O,e,l),f(e,Ct,l),f(e,D,l),r(D,H),r(H,st),g(ce,st,null),r(D,ao),r(D,at),r(at,lo),f(e,It,l),f(e,R,l),r(R,no),r(R,de),r(de,io),r(R,fo),f(e,Ut,l),f(e,Me,l),r(Me,T),r(T,ho),r(T,lt),r(lt,mo),r(T,po),r(T,ve),r(ve,nt),r(nt,uo),r(T,co),r(T,Le),r(Le,vo),r(T,go),f(e,Ft,l),g(ge,e,l),f(e,Dt,l),f(e,Ne,l),r(Ne,q),r(q,wo),r(q,it),r(it,_o),r(q,yo),r(q,Be),r(Be,$o),r(q,bo),f(e,qt,l),g(we,e,l),f(e,St,l),f(e,S,l),r(S,Y),r(Y,ft),g(_e,ft,null),r(S,Eo),r(S,ht),r(ht,ko),f(e,zt,l),f(e,K,l),r(K,To),r(K,mt),r(mt,Ao),r(K,jo),f(e,Mt,l),g(ye,e,l),f(e,Lt,l),f(e,Ge,l),r(Ge,Po),f(e,Nt,l),g($e,e,l),f(e,Bt,l),f(e,z,l),r(z,V),r(V,pt),g(be,pt,null),r(z,xo),r(z,ut),r(ut,Co),f(e,Gt,l),f(e,Oe,l),r(Oe,Io),f(e,Ot,l),g(Ee,e,l),f(e,Ht,l),f(e,He,l),r(He,Uo),f(e,Rt,l),g(ke,e,l),f(e,Yt,l),f(e,x,l),r(x,Fo),r(x,ct),r(ct,Do),r(x,qo),r(x,dt),r(dt,So),r(x,zo),f(e,Kt,l),f(e,Re,l),r(Re,Mo),f(e,Vt,l),g(Te,e,l),f(e,Wt,l),f(e,W,l),r(W,Lo),r(W,vt),r(vt,No),r(W,Bo),f(e,Jt,l),g(Ae,e,l),Qt=!0},p(e,[l]){const je={};l&2&&(je.$$scope={dirty:l,ctx:e}),O.$set(je)},i(e){Qt||(w(b.$$.fragment,e),w(le.$$.fragment,e),w(ne.$$.fragment,e),w(ie.$$.fragment,e),w(fe.$$.fragment,e),w(O.$$.fragment,e),w(ce.$$.fragment,e),w(ge.$$.fragment,e),w(we.$$.fragment,e),w(_e.$$.fragment,e),w(ye.$$.fragment,e),w($e.$$.fragment,e),w(be.$$.fragment,e),w(Ee.$$.fragment,e),w(ke.$$.fragment,e),w(Te.$$.fragment,e),w(Ae.$$.fragment,e),Qt=!0)},o(e){_(b.$$.fragment,e),_(le.$$.fragment,e),_(ne.$$.fragment,e),_(ie.$$.fragment,e),_(fe.$$.fragment,e),_(O.$$.fragment,e),_(ce.$$.fragment,e),_(ge.$$.fragment,e),_(we.$$.fragment,e),_(_e.$$.fragment,e),_(ye.$$.fragment,e),_($e.$$.fragment,e),_(be.$$.fragment,e),_(Ee.$$.fragment,e),_(ke.$$.fragment,e),_(Te.$$.fragment,e),_(Ae.$$.fragment,e),Qt=!1},d(e){t(u),e&&t(A),e&&t(c),y(b),e&&t(gt),e&&t(Pe),e&&t(wt),e&&t(P),e&&t(_t),e&&t(L),e&&t(yt),e&&t(U),y(le),e&&t($t),e&&t(Ce),e&&t(bt),y(ne,e),e&&t(Et),e&&t(B),e&&t(kt),e&&t(F),y(ie),e&&t(Tt),e&&t(Ue),e&&t(At),y(fe,e),e&&t(jt),e&&t(Fe),e&&t(Pt),e&&t(E),e&&t(xt),y(O,e),e&&t(Ct),e&&t(D),y(ce),e&&t(It),e&&t(R),e&&t(Ut),e&&t(Me),e&&t(Ft),y(ge,e),e&&t(Dt),e&&t(Ne),e&&t(qt),y(we,e),e&&t(St),e&&t(S),y(_e),e&&t(zt),e&&t(K),e&&t(Mt),y(ye,e),e&&t(Lt),e&&t(Ge),e&&t(Nt),y($e,e),e&&t(Bt),e&&t(z),y(be),e&&t(Gt),e&&t(Oe),e&&t(Ot),y(Ee,e),e&&t(Ht),e&&t(He),e&&t(Rt),y(ke,e),e&&t(Yt),e&&t(x),e&&t(Kt),e&&t(Re),e&&t(Vt),y(Te,e),e&&t(Wt),e&&t(W),e&&t(Jt),y(Ae,e)}}}const Vs={local:"troubleshoot",sections:[{local:"firewalled-environments",title:"Firewalled environments"},{local:"cuda-out-of-memory",title:"CUDA out of memory"},{local:"unable-to-load-a-saved-tensorflow-model",title:"Unable to load a saved TensorFlow model"},{local:"importerror",title:"ImportError"},{local:"cuda-error-deviceside-assert-triggered",title:"CUDA error: device-side assert triggered"}],title:"Troubleshoot"};function Ws(Je,u,A){let{fw:c}=u;return Je.$$set=$=>{"fw"in $&&A(0,c=$.fw)},[c]}class ta extends Bs{constructor(u){super();Gs(this,u,Ws,Ks,Os,{fw:0})}}export{ta as default,Vs as metadata};
