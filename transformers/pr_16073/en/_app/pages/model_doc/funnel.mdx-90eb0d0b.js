import{S as by,i as yy,s as $y,e as r,k as l,w as k,t,M as Ey,c as a,d as n,m as d,a as i,x as w,h as o,b as c,F as e,g as h,y as b,q as y,o as $,B as E}from"../../chunks/vendor-4833417e.js";import{T as ze}from"../../chunks/Tip-fffd6df1.js";import{D as X}from"../../chunks/Docstring-4f315ed9.js";import{C as Ne}from"../../chunks/CodeBlock-6a3d1b46.js";import{I as qe}from"../../chunks/IconCopyLink-4b81c553.js";import"../../chunks/CopyButton-dacfbfaf.js";function My(W){let p,M,m,g,F;return{c(){p=r("p"),M=t("Although the recipe for forward pass needs to be defined within this function, one should call the "),m=r("code"),g=t("Module"),F=t(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(T){p=a(T,"P",{});var _=i(p);M=o(_,"Although the recipe for forward pass needs to be defined within this function, one should call the "),m=a(_,"CODE",{});var z=i(m);g=o(z,"Module"),z.forEach(n),F=o(_,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),_.forEach(n)},m(T,_){h(T,p,_),e(p,M),e(p,m),e(m,g),e(p,F)},d(T){T&&n(p)}}}function zy(W){let p,M,m,g,F;return{c(){p=r("p"),M=t("Although the recipe for forward pass needs to be defined within this function, one should call the "),m=r("code"),g=t("Module"),F=t(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(T){p=a(T,"P",{});var _=i(p);M=o(_,"Although the recipe for forward pass needs to be defined within this function, one should call the "),m=a(_,"CODE",{});var z=i(m);g=o(z,"Module"),z.forEach(n),F=o(_,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),_.forEach(n)},m(T,_){h(T,p,_),e(p,M),e(p,m),e(m,g),e(p,F)},d(T){T&&n(p)}}}function qy(W){let p,M,m,g,F;return{c(){p=r("p"),M=t("Although the recipe for forward pass needs to be defined within this function, one should call the "),m=r("code"),g=t("Module"),F=t(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(T){p=a(T,"P",{});var _=i(p);M=o(_,"Although the recipe for forward pass needs to be defined within this function, one should call the "),m=a(_,"CODE",{});var z=i(m);g=o(z,"Module"),z.forEach(n),F=o(_,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),_.forEach(n)},m(T,_){h(T,p,_),e(p,M),e(p,m),e(m,g),e(p,F)},d(T){T&&n(p)}}}function Py(W){let p,M,m,g,F;return{c(){p=r("p"),M=t("Although the recipe for forward pass needs to be defined within this function, one should call the "),m=r("code"),g=t("Module"),F=t(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(T){p=a(T,"P",{});var _=i(p);M=o(_,"Although the recipe for forward pass needs to be defined within this function, one should call the "),m=a(_,"CODE",{});var z=i(m);g=o(z,"Module"),z.forEach(n),F=o(_,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),_.forEach(n)},m(T,_){h(T,p,_),e(p,M),e(p,m),e(m,g),e(p,F)},d(T){T&&n(p)}}}function Cy(W){let p,M,m,g,F;return{c(){p=r("p"),M=t("Although the recipe for forward pass needs to be defined within this function, one should call the "),m=r("code"),g=t("Module"),F=t(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(T){p=a(T,"P",{});var _=i(p);M=o(_,"Although the recipe for forward pass needs to be defined within this function, one should call the "),m=a(_,"CODE",{});var z=i(m);g=o(z,"Module"),z.forEach(n),F=o(_,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),_.forEach(n)},m(T,_){h(T,p,_),e(p,M),e(p,m),e(m,g),e(p,F)},d(T){T&&n(p)}}}function jy(W){let p,M,m,g,F;return{c(){p=r("p"),M=t("Although the recipe for forward pass needs to be defined within this function, one should call the "),m=r("code"),g=t("Module"),F=t(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(T){p=a(T,"P",{});var _=i(p);M=o(_,"Although the recipe for forward pass needs to be defined within this function, one should call the "),m=a(_,"CODE",{});var z=i(m);g=o(z,"Module"),z.forEach(n),F=o(_,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),_.forEach(n)},m(T,_){h(T,p,_),e(p,M),e(p,m),e(m,g),e(p,F)},d(T){T&&n(p)}}}function xy(W){let p,M,m,g,F;return{c(){p=r("p"),M=t("Although the recipe for forward pass needs to be defined within this function, one should call the "),m=r("code"),g=t("Module"),F=t(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(T){p=a(T,"P",{});var _=i(p);M=o(_,"Although the recipe for forward pass needs to be defined within this function, one should call the "),m=a(_,"CODE",{});var z=i(m);g=o(z,"Module"),z.forEach(n),F=o(_,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),_.forEach(n)},m(T,_){h(T,p,_),e(p,M),e(p,m),e(m,g),e(p,F)},d(T){T&&n(p)}}}function Ly(W){let p,M,m,g,F;return{c(){p=r("p"),M=t("Although the recipe for forward pass needs to be defined within this function, one should call the "),m=r("code"),g=t("Module"),F=t(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(T){p=a(T,"P",{});var _=i(p);M=o(_,"Although the recipe for forward pass needs to be defined within this function, one should call the "),m=a(_,"CODE",{});var z=i(m);g=o(z,"Module"),z.forEach(n),F=o(_,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),_.forEach(n)},m(T,_){h(T,p,_),e(p,M),e(p,m),e(m,g),e(p,F)},d(T){T&&n(p)}}}function Ay(W){let p,M,m,g,F,T,_,z,ce,G,q,J,I,ne,ue,O,pe,ie,U,L,te,Z,P,j,oe,Q,le,se,S,he,de,C,fe,B,ee,ae,R,me,N,A,re,H,ge;return{c(){p=r("p"),M=t("TF 2.0 models accepts two formats as inputs:"),m=l(),g=r("ul"),F=r("li"),T=t("having all inputs as keyword arguments (like PyTorch models), or"),_=l(),z=r("li"),ce=t("having all inputs as a list, tuple or dict in the first positional arguments."),G=l(),q=r("p"),J=t("This second option is useful when using "),I=r("code"),ne=t("tf.keras.Model.fit"),ue=t(` method which currently requires having all the
tensors in the first argument of the model call function: `),O=r("code"),pe=t("model(inputs)"),ie=t("."),U=l(),L=r("p"),te=t(`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),Z=l(),P=r("ul"),j=r("li"),oe=t("a single Tensor with "),Q=r("code"),le=t("input_ids"),se=t(" only and nothing else: "),S=r("code"),he=t("model(inputs_ids)"),de=l(),C=r("li"),fe=t(`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=r("code"),ee=t("model([input_ids, attention_mask])"),ae=t(" or "),R=r("code"),me=t("model([input_ids, attention_mask, token_type_ids])"),N=l(),A=r("li"),re=t(`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),H=r("code"),ge=t('model({"input_ids": input_ids, "token_type_ids": token_type_ids})')},l(u){p=a(u,"P",{});var v=i(p);M=o(v,"TF 2.0 models accepts two formats as inputs:"),v.forEach(n),m=d(u),g=a(u,"UL",{});var K=i(g);F=a(K,"LI",{});var Te=i(F);T=o(Te,"having all inputs as keyword arguments (like PyTorch models), or"),Te.forEach(n),_=d(K),z=a(K,"LI",{});var we=i(z);ce=o(we,"having all inputs as a list, tuple or dict in the first positional arguments."),we.forEach(n),K.forEach(n),G=d(u),q=a(u,"P",{});var D=i(q);J=o(D,"This second option is useful when using "),I=a(D,"CODE",{});var Fe=i(I);ne=o(Fe,"tf.keras.Model.fit"),Fe.forEach(n),ue=o(D,` method which currently requires having all the
tensors in the first argument of the model call function: `),O=a(D,"CODE",{});var be=i(O);pe=o(be,"model(inputs)"),be.forEach(n),ie=o(D,"."),D.forEach(n),U=d(u),L=a(u,"P",{});var ye=i(L);te=o(ye,`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),ye.forEach(n),Z=d(u),P=a(u,"UL",{});var x=i(P);j=a(x,"LI",{});var V=i(j);oe=o(V,"a single Tensor with "),Q=a(V,"CODE",{});var $e=i(Q);le=o($e,"input_ids"),$e.forEach(n),se=o(V," only and nothing else: "),S=a(V,"CODE",{});var ve=i(S);he=o(ve,"model(inputs_ids)"),ve.forEach(n),V.forEach(n),de=d(x),C=a(x,"LI",{});var Y=i(C);fe=o(Y,`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=a(Y,"CODE",{});var Ee=i(B);ee=o(Ee,"model([input_ids, attention_mask])"),Ee.forEach(n),ae=o(Y," or "),R=a(Y,"CODE",{});var ke=i(R);me=o(ke,"model([input_ids, attention_mask, token_type_ids])"),ke.forEach(n),Y.forEach(n),N=d(x),A=a(x,"LI",{});var _e=i(A);re=o(_e,`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),H=a(_e,"CODE",{});var Me=i(H);ge=o(Me,'model({"input_ids": input_ids, "token_type_ids": token_type_ids})'),Me.forEach(n),_e.forEach(n),x.forEach(n)},m(u,v){h(u,p,v),e(p,M),h(u,m,v),h(u,g,v),e(g,F),e(F,T),e(g,_),e(g,z),e(z,ce),h(u,G,v),h(u,q,v),e(q,J),e(q,I),e(I,ne),e(q,ue),e(q,O),e(O,pe),e(q,ie),h(u,U,v),h(u,L,v),e(L,te),h(u,Z,v),h(u,P,v),e(P,j),e(j,oe),e(j,Q),e(Q,le),e(j,se),e(j,S),e(S,he),e(P,de),e(P,C),e(C,fe),e(C,B),e(B,ee),e(C,ae),e(C,R),e(R,me),e(P,N),e(P,A),e(A,re),e(A,H),e(H,ge)},d(u){u&&n(p),u&&n(m),u&&n(g),u&&n(G),u&&n(q),u&&n(U),u&&n(L),u&&n(Z),u&&n(P)}}}function Dy(W){let p,M,m,g,F;return{c(){p=r("p"),M=t("Although the recipe for forward pass needs to be defined within this function, one should call the "),m=r("code"),g=t("Module"),F=t(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(T){p=a(T,"P",{});var _=i(p);M=o(_,"Although the recipe for forward pass needs to be defined within this function, one should call the "),m=a(_,"CODE",{});var z=i(m);g=o(z,"Module"),z.forEach(n),F=o(_,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),_.forEach(n)},m(T,_){h(T,p,_),e(p,M),e(p,m),e(m,g),e(p,F)},d(T){T&&n(p)}}}function Iy(W){let p,M,m,g,F,T,_,z,ce,G,q,J,I,ne,ue,O,pe,ie,U,L,te,Z,P,j,oe,Q,le,se,S,he,de,C,fe,B,ee,ae,R,me,N,A,re,H,ge;return{c(){p=r("p"),M=t("TF 2.0 models accepts two formats as inputs:"),m=l(),g=r("ul"),F=r("li"),T=t("having all inputs as keyword arguments (like PyTorch models), or"),_=l(),z=r("li"),ce=t("having all inputs as a list, tuple or dict in the first positional arguments."),G=l(),q=r("p"),J=t("This second option is useful when using "),I=r("code"),ne=t("tf.keras.Model.fit"),ue=t(` method which currently requires having all the
tensors in the first argument of the model call function: `),O=r("code"),pe=t("model(inputs)"),ie=t("."),U=l(),L=r("p"),te=t(`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),Z=l(),P=r("ul"),j=r("li"),oe=t("a single Tensor with "),Q=r("code"),le=t("input_ids"),se=t(" only and nothing else: "),S=r("code"),he=t("model(inputs_ids)"),de=l(),C=r("li"),fe=t(`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=r("code"),ee=t("model([input_ids, attention_mask])"),ae=t(" or "),R=r("code"),me=t("model([input_ids, attention_mask, token_type_ids])"),N=l(),A=r("li"),re=t(`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),H=r("code"),ge=t('model({"input_ids": input_ids, "token_type_ids": token_type_ids})')},l(u){p=a(u,"P",{});var v=i(p);M=o(v,"TF 2.0 models accepts two formats as inputs:"),v.forEach(n),m=d(u),g=a(u,"UL",{});var K=i(g);F=a(K,"LI",{});var Te=i(F);T=o(Te,"having all inputs as keyword arguments (like PyTorch models), or"),Te.forEach(n),_=d(K),z=a(K,"LI",{});var we=i(z);ce=o(we,"having all inputs as a list, tuple or dict in the first positional arguments."),we.forEach(n),K.forEach(n),G=d(u),q=a(u,"P",{});var D=i(q);J=o(D,"This second option is useful when using "),I=a(D,"CODE",{});var Fe=i(I);ne=o(Fe,"tf.keras.Model.fit"),Fe.forEach(n),ue=o(D,` method which currently requires having all the
tensors in the first argument of the model call function: `),O=a(D,"CODE",{});var be=i(O);pe=o(be,"model(inputs)"),be.forEach(n),ie=o(D,"."),D.forEach(n),U=d(u),L=a(u,"P",{});var ye=i(L);te=o(ye,`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),ye.forEach(n),Z=d(u),P=a(u,"UL",{});var x=i(P);j=a(x,"LI",{});var V=i(j);oe=o(V,"a single Tensor with "),Q=a(V,"CODE",{});var $e=i(Q);le=o($e,"input_ids"),$e.forEach(n),se=o(V," only and nothing else: "),S=a(V,"CODE",{});var ve=i(S);he=o(ve,"model(inputs_ids)"),ve.forEach(n),V.forEach(n),de=d(x),C=a(x,"LI",{});var Y=i(C);fe=o(Y,`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=a(Y,"CODE",{});var Ee=i(B);ee=o(Ee,"model([input_ids, attention_mask])"),Ee.forEach(n),ae=o(Y," or "),R=a(Y,"CODE",{});var ke=i(R);me=o(ke,"model([input_ids, attention_mask, token_type_ids])"),ke.forEach(n),Y.forEach(n),N=d(x),A=a(x,"LI",{});var _e=i(A);re=o(_e,`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),H=a(_e,"CODE",{});var Me=i(H);ge=o(Me,'model({"input_ids": input_ids, "token_type_ids": token_type_ids})'),Me.forEach(n),_e.forEach(n),x.forEach(n)},m(u,v){h(u,p,v),e(p,M),h(u,m,v),h(u,g,v),e(g,F),e(F,T),e(g,_),e(g,z),e(z,ce),h(u,G,v),h(u,q,v),e(q,J),e(q,I),e(I,ne),e(q,ue),e(q,O),e(O,pe),e(q,ie),h(u,U,v),h(u,L,v),e(L,te),h(u,Z,v),h(u,P,v),e(P,j),e(j,oe),e(j,Q),e(Q,le),e(j,se),e(j,S),e(S,he),e(P,de),e(P,C),e(C,fe),e(C,B),e(B,ee),e(C,ae),e(C,R),e(R,me),e(P,N),e(P,A),e(A,re),e(A,H),e(H,ge)},d(u){u&&n(p),u&&n(m),u&&n(g),u&&n(G),u&&n(q),u&&n(U),u&&n(L),u&&n(Z),u&&n(P)}}}function Oy(W){let p,M,m,g,F;return{c(){p=r("p"),M=t("Although the recipe for forward pass needs to be defined within this function, one should call the "),m=r("code"),g=t("Module"),F=t(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(T){p=a(T,"P",{});var _=i(p);M=o(_,"Although the recipe for forward pass needs to be defined within this function, one should call the "),m=a(_,"CODE",{});var z=i(m);g=o(z,"Module"),z.forEach(n),F=o(_,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),_.forEach(n)},m(T,_){h(T,p,_),e(p,M),e(p,m),e(m,g),e(p,F)},d(T){T&&n(p)}}}function Sy(W){let p,M,m,g,F,T,_,z,ce,G,q,J,I,ne,ue,O,pe,ie,U,L,te,Z,P,j,oe,Q,le,se,S,he,de,C,fe,B,ee,ae,R,me,N,A,re,H,ge;return{c(){p=r("p"),M=t("TF 2.0 models accepts two formats as inputs:"),m=l(),g=r("ul"),F=r("li"),T=t("having all inputs as keyword arguments (like PyTorch models), or"),_=l(),z=r("li"),ce=t("having all inputs as a list, tuple or dict in the first positional arguments."),G=l(),q=r("p"),J=t("This second option is useful when using "),I=r("code"),ne=t("tf.keras.Model.fit"),ue=t(` method which currently requires having all the
tensors in the first argument of the model call function: `),O=r("code"),pe=t("model(inputs)"),ie=t("."),U=l(),L=r("p"),te=t(`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),Z=l(),P=r("ul"),j=r("li"),oe=t("a single Tensor with "),Q=r("code"),le=t("input_ids"),se=t(" only and nothing else: "),S=r("code"),he=t("model(inputs_ids)"),de=l(),C=r("li"),fe=t(`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=r("code"),ee=t("model([input_ids, attention_mask])"),ae=t(" or "),R=r("code"),me=t("model([input_ids, attention_mask, token_type_ids])"),N=l(),A=r("li"),re=t(`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),H=r("code"),ge=t('model({"input_ids": input_ids, "token_type_ids": token_type_ids})')},l(u){p=a(u,"P",{});var v=i(p);M=o(v,"TF 2.0 models accepts two formats as inputs:"),v.forEach(n),m=d(u),g=a(u,"UL",{});var K=i(g);F=a(K,"LI",{});var Te=i(F);T=o(Te,"having all inputs as keyword arguments (like PyTorch models), or"),Te.forEach(n),_=d(K),z=a(K,"LI",{});var we=i(z);ce=o(we,"having all inputs as a list, tuple or dict in the first positional arguments."),we.forEach(n),K.forEach(n),G=d(u),q=a(u,"P",{});var D=i(q);J=o(D,"This second option is useful when using "),I=a(D,"CODE",{});var Fe=i(I);ne=o(Fe,"tf.keras.Model.fit"),Fe.forEach(n),ue=o(D,` method which currently requires having all the
tensors in the first argument of the model call function: `),O=a(D,"CODE",{});var be=i(O);pe=o(be,"model(inputs)"),be.forEach(n),ie=o(D,"."),D.forEach(n),U=d(u),L=a(u,"P",{});var ye=i(L);te=o(ye,`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),ye.forEach(n),Z=d(u),P=a(u,"UL",{});var x=i(P);j=a(x,"LI",{});var V=i(j);oe=o(V,"a single Tensor with "),Q=a(V,"CODE",{});var $e=i(Q);le=o($e,"input_ids"),$e.forEach(n),se=o(V," only and nothing else: "),S=a(V,"CODE",{});var ve=i(S);he=o(ve,"model(inputs_ids)"),ve.forEach(n),V.forEach(n),de=d(x),C=a(x,"LI",{});var Y=i(C);fe=o(Y,`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=a(Y,"CODE",{});var Ee=i(B);ee=o(Ee,"model([input_ids, attention_mask])"),Ee.forEach(n),ae=o(Y," or "),R=a(Y,"CODE",{});var ke=i(R);me=o(ke,"model([input_ids, attention_mask, token_type_ids])"),ke.forEach(n),Y.forEach(n),N=d(x),A=a(x,"LI",{});var _e=i(A);re=o(_e,`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),H=a(_e,"CODE",{});var Me=i(H);ge=o(Me,'model({"input_ids": input_ids, "token_type_ids": token_type_ids})'),Me.forEach(n),_e.forEach(n),x.forEach(n)},m(u,v){h(u,p,v),e(p,M),h(u,m,v),h(u,g,v),e(g,F),e(F,T),e(g,_),e(g,z),e(z,ce),h(u,G,v),h(u,q,v),e(q,J),e(q,I),e(I,ne),e(q,ue),e(q,O),e(O,pe),e(q,ie),h(u,U,v),h(u,L,v),e(L,te),h(u,Z,v),h(u,P,v),e(P,j),e(j,oe),e(j,Q),e(Q,le),e(j,se),e(j,S),e(S,he),e(P,de),e(P,C),e(C,fe),e(C,B),e(B,ee),e(C,ae),e(C,R),e(R,me),e(P,N),e(P,A),e(A,re),e(A,H),e(H,ge)},d(u){u&&n(p),u&&n(m),u&&n(g),u&&n(G),u&&n(q),u&&n(U),u&&n(L),u&&n(Z),u&&n(P)}}}function Ny(W){let p,M,m,g,F;return{c(){p=r("p"),M=t("Although the recipe for forward pass needs to be defined within this function, one should call the "),m=r("code"),g=t("Module"),F=t(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(T){p=a(T,"P",{});var _=i(p);M=o(_,"Although the recipe for forward pass needs to be defined within this function, one should call the "),m=a(_,"CODE",{});var z=i(m);g=o(z,"Module"),z.forEach(n),F=o(_,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),_.forEach(n)},m(T,_){h(T,p,_),e(p,M),e(p,m),e(m,g),e(p,F)},d(T){T&&n(p)}}}function By(W){let p,M,m,g,F,T,_,z,ce,G,q,J,I,ne,ue,O,pe,ie,U,L,te,Z,P,j,oe,Q,le,se,S,he,de,C,fe,B,ee,ae,R,me,N,A,re,H,ge;return{c(){p=r("p"),M=t("TF 2.0 models accepts two formats as inputs:"),m=l(),g=r("ul"),F=r("li"),T=t("having all inputs as keyword arguments (like PyTorch models), or"),_=l(),z=r("li"),ce=t("having all inputs as a list, tuple or dict in the first positional arguments."),G=l(),q=r("p"),J=t("This second option is useful when using "),I=r("code"),ne=t("tf.keras.Model.fit"),ue=t(` method which currently requires having all the
tensors in the first argument of the model call function: `),O=r("code"),pe=t("model(inputs)"),ie=t("."),U=l(),L=r("p"),te=t(`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),Z=l(),P=r("ul"),j=r("li"),oe=t("a single Tensor with "),Q=r("code"),le=t("input_ids"),se=t(" only and nothing else: "),S=r("code"),he=t("model(inputs_ids)"),de=l(),C=r("li"),fe=t(`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=r("code"),ee=t("model([input_ids, attention_mask])"),ae=t(" or "),R=r("code"),me=t("model([input_ids, attention_mask, token_type_ids])"),N=l(),A=r("li"),re=t(`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),H=r("code"),ge=t('model({"input_ids": input_ids, "token_type_ids": token_type_ids})')},l(u){p=a(u,"P",{});var v=i(p);M=o(v,"TF 2.0 models accepts two formats as inputs:"),v.forEach(n),m=d(u),g=a(u,"UL",{});var K=i(g);F=a(K,"LI",{});var Te=i(F);T=o(Te,"having all inputs as keyword arguments (like PyTorch models), or"),Te.forEach(n),_=d(K),z=a(K,"LI",{});var we=i(z);ce=o(we,"having all inputs as a list, tuple or dict in the first positional arguments."),we.forEach(n),K.forEach(n),G=d(u),q=a(u,"P",{});var D=i(q);J=o(D,"This second option is useful when using "),I=a(D,"CODE",{});var Fe=i(I);ne=o(Fe,"tf.keras.Model.fit"),Fe.forEach(n),ue=o(D,` method which currently requires having all the
tensors in the first argument of the model call function: `),O=a(D,"CODE",{});var be=i(O);pe=o(be,"model(inputs)"),be.forEach(n),ie=o(D,"."),D.forEach(n),U=d(u),L=a(u,"P",{});var ye=i(L);te=o(ye,`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),ye.forEach(n),Z=d(u),P=a(u,"UL",{});var x=i(P);j=a(x,"LI",{});var V=i(j);oe=o(V,"a single Tensor with "),Q=a(V,"CODE",{});var $e=i(Q);le=o($e,"input_ids"),$e.forEach(n),se=o(V," only and nothing else: "),S=a(V,"CODE",{});var ve=i(S);he=o(ve,"model(inputs_ids)"),ve.forEach(n),V.forEach(n),de=d(x),C=a(x,"LI",{});var Y=i(C);fe=o(Y,`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=a(Y,"CODE",{});var Ee=i(B);ee=o(Ee,"model([input_ids, attention_mask])"),Ee.forEach(n),ae=o(Y," or "),R=a(Y,"CODE",{});var ke=i(R);me=o(ke,"model([input_ids, attention_mask, token_type_ids])"),ke.forEach(n),Y.forEach(n),N=d(x),A=a(x,"LI",{});var _e=i(A);re=o(_e,`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),H=a(_e,"CODE",{});var Me=i(H);ge=o(Me,'model({"input_ids": input_ids, "token_type_ids": token_type_ids})'),Me.forEach(n),_e.forEach(n),x.forEach(n)},m(u,v){h(u,p,v),e(p,M),h(u,m,v),h(u,g,v),e(g,F),e(F,T),e(g,_),e(g,z),e(z,ce),h(u,G,v),h(u,q,v),e(q,J),e(q,I),e(I,ne),e(q,ue),e(q,O),e(O,pe),e(q,ie),h(u,U,v),h(u,L,v),e(L,te),h(u,Z,v),h(u,P,v),e(P,j),e(j,oe),e(j,Q),e(Q,le),e(j,se),e(j,S),e(S,he),e(P,de),e(P,C),e(C,fe),e(C,B),e(B,ee),e(C,ae),e(C,R),e(R,me),e(P,N),e(P,A),e(A,re),e(A,H),e(H,ge)},d(u){u&&n(p),u&&n(m),u&&n(g),u&&n(G),u&&n(q),u&&n(U),u&&n(L),u&&n(Z),u&&n(P)}}}function Wy(W){let p,M,m,g,F;return{c(){p=r("p"),M=t("Although the recipe for forward pass needs to be defined within this function, one should call the "),m=r("code"),g=t("Module"),F=t(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(T){p=a(T,"P",{});var _=i(p);M=o(_,"Although the recipe for forward pass needs to be defined within this function, one should call the "),m=a(_,"CODE",{});var z=i(m);g=o(z,"Module"),z.forEach(n),F=o(_,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),_.forEach(n)},m(T,_){h(T,p,_),e(p,M),e(p,m),e(m,g),e(p,F)},d(T){T&&n(p)}}}function Qy(W){let p,M,m,g,F,T,_,z,ce,G,q,J,I,ne,ue,O,pe,ie,U,L,te,Z,P,j,oe,Q,le,se,S,he,de,C,fe,B,ee,ae,R,me,N,A,re,H,ge;return{c(){p=r("p"),M=t("TF 2.0 models accepts two formats as inputs:"),m=l(),g=r("ul"),F=r("li"),T=t("having all inputs as keyword arguments (like PyTorch models), or"),_=l(),z=r("li"),ce=t("having all inputs as a list, tuple or dict in the first positional arguments."),G=l(),q=r("p"),J=t("This second option is useful when using "),I=r("code"),ne=t("tf.keras.Model.fit"),ue=t(` method which currently requires having all the
tensors in the first argument of the model call function: `),O=r("code"),pe=t("model(inputs)"),ie=t("."),U=l(),L=r("p"),te=t(`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),Z=l(),P=r("ul"),j=r("li"),oe=t("a single Tensor with "),Q=r("code"),le=t("input_ids"),se=t(" only and nothing else: "),S=r("code"),he=t("model(inputs_ids)"),de=l(),C=r("li"),fe=t(`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=r("code"),ee=t("model([input_ids, attention_mask])"),ae=t(" or "),R=r("code"),me=t("model([input_ids, attention_mask, token_type_ids])"),N=l(),A=r("li"),re=t(`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),H=r("code"),ge=t('model({"input_ids": input_ids, "token_type_ids": token_type_ids})')},l(u){p=a(u,"P",{});var v=i(p);M=o(v,"TF 2.0 models accepts two formats as inputs:"),v.forEach(n),m=d(u),g=a(u,"UL",{});var K=i(g);F=a(K,"LI",{});var Te=i(F);T=o(Te,"having all inputs as keyword arguments (like PyTorch models), or"),Te.forEach(n),_=d(K),z=a(K,"LI",{});var we=i(z);ce=o(we,"having all inputs as a list, tuple or dict in the first positional arguments."),we.forEach(n),K.forEach(n),G=d(u),q=a(u,"P",{});var D=i(q);J=o(D,"This second option is useful when using "),I=a(D,"CODE",{});var Fe=i(I);ne=o(Fe,"tf.keras.Model.fit"),Fe.forEach(n),ue=o(D,` method which currently requires having all the
tensors in the first argument of the model call function: `),O=a(D,"CODE",{});var be=i(O);pe=o(be,"model(inputs)"),be.forEach(n),ie=o(D,"."),D.forEach(n),U=d(u),L=a(u,"P",{});var ye=i(L);te=o(ye,`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),ye.forEach(n),Z=d(u),P=a(u,"UL",{});var x=i(P);j=a(x,"LI",{});var V=i(j);oe=o(V,"a single Tensor with "),Q=a(V,"CODE",{});var $e=i(Q);le=o($e,"input_ids"),$e.forEach(n),se=o(V," only and nothing else: "),S=a(V,"CODE",{});var ve=i(S);he=o(ve,"model(inputs_ids)"),ve.forEach(n),V.forEach(n),de=d(x),C=a(x,"LI",{});var Y=i(C);fe=o(Y,`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=a(Y,"CODE",{});var Ee=i(B);ee=o(Ee,"model([input_ids, attention_mask])"),Ee.forEach(n),ae=o(Y," or "),R=a(Y,"CODE",{});var ke=i(R);me=o(ke,"model([input_ids, attention_mask, token_type_ids])"),ke.forEach(n),Y.forEach(n),N=d(x),A=a(x,"LI",{});var _e=i(A);re=o(_e,`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),H=a(_e,"CODE",{});var Me=i(H);ge=o(Me,'model({"input_ids": input_ids, "token_type_ids": token_type_ids})'),Me.forEach(n),_e.forEach(n),x.forEach(n)},m(u,v){h(u,p,v),e(p,M),h(u,m,v),h(u,g,v),e(g,F),e(F,T),e(g,_),e(g,z),e(z,ce),h(u,G,v),h(u,q,v),e(q,J),e(q,I),e(I,ne),e(q,ue),e(q,O),e(O,pe),e(q,ie),h(u,U,v),h(u,L,v),e(L,te),h(u,Z,v),h(u,P,v),e(P,j),e(j,oe),e(j,Q),e(Q,le),e(j,se),e(j,S),e(S,he),e(P,de),e(P,C),e(C,fe),e(C,B),e(B,ee),e(C,ae),e(C,R),e(R,me),e(P,N),e(P,A),e(A,re),e(A,H),e(H,ge)},d(u){u&&n(p),u&&n(m),u&&n(g),u&&n(G),u&&n(q),u&&n(U),u&&n(L),u&&n(Z),u&&n(P)}}}function Ry(W){let p,M,m,g,F;return{c(){p=r("p"),M=t("Although the recipe for forward pass needs to be defined within this function, one should call the "),m=r("code"),g=t("Module"),F=t(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(T){p=a(T,"P",{});var _=i(p);M=o(_,"Although the recipe for forward pass needs to be defined within this function, one should call the "),m=a(_,"CODE",{});var z=i(m);g=o(z,"Module"),z.forEach(n),F=o(_,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),_.forEach(n)},m(T,_){h(T,p,_),e(p,M),e(p,m),e(m,g),e(p,F)},d(T){T&&n(p)}}}function Hy(W){let p,M,m,g,F,T,_,z,ce,G,q,J,I,ne,ue,O,pe,ie,U,L,te,Z,P,j,oe,Q,le,se,S,he,de,C,fe,B,ee,ae,R,me,N,A,re,H,ge;return{c(){p=r("p"),M=t("TF 2.0 models accepts two formats as inputs:"),m=l(),g=r("ul"),F=r("li"),T=t("having all inputs as keyword arguments (like PyTorch models), or"),_=l(),z=r("li"),ce=t("having all inputs as a list, tuple or dict in the first positional arguments."),G=l(),q=r("p"),J=t("This second option is useful when using "),I=r("code"),ne=t("tf.keras.Model.fit"),ue=t(` method which currently requires having all the
tensors in the first argument of the model call function: `),O=r("code"),pe=t("model(inputs)"),ie=t("."),U=l(),L=r("p"),te=t(`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),Z=l(),P=r("ul"),j=r("li"),oe=t("a single Tensor with "),Q=r("code"),le=t("input_ids"),se=t(" only and nothing else: "),S=r("code"),he=t("model(inputs_ids)"),de=l(),C=r("li"),fe=t(`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=r("code"),ee=t("model([input_ids, attention_mask])"),ae=t(" or "),R=r("code"),me=t("model([input_ids, attention_mask, token_type_ids])"),N=l(),A=r("li"),re=t(`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),H=r("code"),ge=t('model({"input_ids": input_ids, "token_type_ids": token_type_ids})')},l(u){p=a(u,"P",{});var v=i(p);M=o(v,"TF 2.0 models accepts two formats as inputs:"),v.forEach(n),m=d(u),g=a(u,"UL",{});var K=i(g);F=a(K,"LI",{});var Te=i(F);T=o(Te,"having all inputs as keyword arguments (like PyTorch models), or"),Te.forEach(n),_=d(K),z=a(K,"LI",{});var we=i(z);ce=o(we,"having all inputs as a list, tuple or dict in the first positional arguments."),we.forEach(n),K.forEach(n),G=d(u),q=a(u,"P",{});var D=i(q);J=o(D,"This second option is useful when using "),I=a(D,"CODE",{});var Fe=i(I);ne=o(Fe,"tf.keras.Model.fit"),Fe.forEach(n),ue=o(D,` method which currently requires having all the
tensors in the first argument of the model call function: `),O=a(D,"CODE",{});var be=i(O);pe=o(be,"model(inputs)"),be.forEach(n),ie=o(D,"."),D.forEach(n),U=d(u),L=a(u,"P",{});var ye=i(L);te=o(ye,`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),ye.forEach(n),Z=d(u),P=a(u,"UL",{});var x=i(P);j=a(x,"LI",{});var V=i(j);oe=o(V,"a single Tensor with "),Q=a(V,"CODE",{});var $e=i(Q);le=o($e,"input_ids"),$e.forEach(n),se=o(V," only and nothing else: "),S=a(V,"CODE",{});var ve=i(S);he=o(ve,"model(inputs_ids)"),ve.forEach(n),V.forEach(n),de=d(x),C=a(x,"LI",{});var Y=i(C);fe=o(Y,`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=a(Y,"CODE",{});var Ee=i(B);ee=o(Ee,"model([input_ids, attention_mask])"),Ee.forEach(n),ae=o(Y," or "),R=a(Y,"CODE",{});var ke=i(R);me=o(ke,"model([input_ids, attention_mask, token_type_ids])"),ke.forEach(n),Y.forEach(n),N=d(x),A=a(x,"LI",{});var _e=i(A);re=o(_e,`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),H=a(_e,"CODE",{});var Me=i(H);ge=o(Me,'model({"input_ids": input_ids, "token_type_ids": token_type_ids})'),Me.forEach(n),_e.forEach(n),x.forEach(n)},m(u,v){h(u,p,v),e(p,M),h(u,m,v),h(u,g,v),e(g,F),e(F,T),e(g,_),e(g,z),e(z,ce),h(u,G,v),h(u,q,v),e(q,J),e(q,I),e(I,ne),e(q,ue),e(q,O),e(O,pe),e(q,ie),h(u,U,v),h(u,L,v),e(L,te),h(u,Z,v),h(u,P,v),e(P,j),e(j,oe),e(j,Q),e(Q,le),e(j,se),e(j,S),e(S,he),e(P,de),e(P,C),e(C,fe),e(C,B),e(B,ee),e(C,ae),e(C,R),e(R,me),e(P,N),e(P,A),e(A,re),e(A,H),e(H,ge)},d(u){u&&n(p),u&&n(m),u&&n(g),u&&n(G),u&&n(q),u&&n(U),u&&n(L),u&&n(Z),u&&n(P)}}}function Vy(W){let p,M,m,g,F;return{c(){p=r("p"),M=t("Although the recipe for forward pass needs to be defined within this function, one should call the "),m=r("code"),g=t("Module"),F=t(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(T){p=a(T,"P",{});var _=i(p);M=o(_,"Although the recipe for forward pass needs to be defined within this function, one should call the "),m=a(_,"CODE",{});var z=i(m);g=o(z,"Module"),z.forEach(n),F=o(_,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),_.forEach(n)},m(T,_){h(T,p,_),e(p,M),e(p,m),e(m,g),e(p,F)},d(T){T&&n(p)}}}function Yy(W){let p,M,m,g,F,T,_,z,ce,G,q,J,I,ne,ue,O,pe,ie,U,L,te,Z,P,j,oe,Q,le,se,S,he,de,C,fe,B,ee,ae,R,me,N,A,re,H,ge;return{c(){p=r("p"),M=t("TF 2.0 models accepts two formats as inputs:"),m=l(),g=r("ul"),F=r("li"),T=t("having all inputs as keyword arguments (like PyTorch models), or"),_=l(),z=r("li"),ce=t("having all inputs as a list, tuple or dict in the first positional arguments."),G=l(),q=r("p"),J=t("This second option is useful when using "),I=r("code"),ne=t("tf.keras.Model.fit"),ue=t(` method which currently requires having all the
tensors in the first argument of the model call function: `),O=r("code"),pe=t("model(inputs)"),ie=t("."),U=l(),L=r("p"),te=t(`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),Z=l(),P=r("ul"),j=r("li"),oe=t("a single Tensor with "),Q=r("code"),le=t("input_ids"),se=t(" only and nothing else: "),S=r("code"),he=t("model(inputs_ids)"),de=l(),C=r("li"),fe=t(`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=r("code"),ee=t("model([input_ids, attention_mask])"),ae=t(" or "),R=r("code"),me=t("model([input_ids, attention_mask, token_type_ids])"),N=l(),A=r("li"),re=t(`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),H=r("code"),ge=t('model({"input_ids": input_ids, "token_type_ids": token_type_ids})')},l(u){p=a(u,"P",{});var v=i(p);M=o(v,"TF 2.0 models accepts two formats as inputs:"),v.forEach(n),m=d(u),g=a(u,"UL",{});var K=i(g);F=a(K,"LI",{});var Te=i(F);T=o(Te,"having all inputs as keyword arguments (like PyTorch models), or"),Te.forEach(n),_=d(K),z=a(K,"LI",{});var we=i(z);ce=o(we,"having all inputs as a list, tuple or dict in the first positional arguments."),we.forEach(n),K.forEach(n),G=d(u),q=a(u,"P",{});var D=i(q);J=o(D,"This second option is useful when using "),I=a(D,"CODE",{});var Fe=i(I);ne=o(Fe,"tf.keras.Model.fit"),Fe.forEach(n),ue=o(D,` method which currently requires having all the
tensors in the first argument of the model call function: `),O=a(D,"CODE",{});var be=i(O);pe=o(be,"model(inputs)"),be.forEach(n),ie=o(D,"."),D.forEach(n),U=d(u),L=a(u,"P",{});var ye=i(L);te=o(ye,`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),ye.forEach(n),Z=d(u),P=a(u,"UL",{});var x=i(P);j=a(x,"LI",{});var V=i(j);oe=o(V,"a single Tensor with "),Q=a(V,"CODE",{});var $e=i(Q);le=o($e,"input_ids"),$e.forEach(n),se=o(V," only and nothing else: "),S=a(V,"CODE",{});var ve=i(S);he=o(ve,"model(inputs_ids)"),ve.forEach(n),V.forEach(n),de=d(x),C=a(x,"LI",{});var Y=i(C);fe=o(Y,`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=a(Y,"CODE",{});var Ee=i(B);ee=o(Ee,"model([input_ids, attention_mask])"),Ee.forEach(n),ae=o(Y," or "),R=a(Y,"CODE",{});var ke=i(R);me=o(ke,"model([input_ids, attention_mask, token_type_ids])"),ke.forEach(n),Y.forEach(n),N=d(x),A=a(x,"LI",{});var _e=i(A);re=o(_e,`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),H=a(_e,"CODE",{});var Me=i(H);ge=o(Me,'model({"input_ids": input_ids, "token_type_ids": token_type_ids})'),Me.forEach(n),_e.forEach(n),x.forEach(n)},m(u,v){h(u,p,v),e(p,M),h(u,m,v),h(u,g,v),e(g,F),e(F,T),e(g,_),e(g,z),e(z,ce),h(u,G,v),h(u,q,v),e(q,J),e(q,I),e(I,ne),e(q,ue),e(q,O),e(O,pe),e(q,ie),h(u,U,v),h(u,L,v),e(L,te),h(u,Z,v),h(u,P,v),e(P,j),e(j,oe),e(j,Q),e(Q,le),e(j,se),e(j,S),e(S,he),e(P,de),e(P,C),e(C,fe),e(C,B),e(B,ee),e(C,ae),e(C,R),e(R,me),e(P,N),e(P,A),e(A,re),e(A,H),e(H,ge)},d(u){u&&n(p),u&&n(m),u&&n(g),u&&n(G),u&&n(q),u&&n(U),u&&n(L),u&&n(Z),u&&n(P)}}}function Uy(W){let p,M,m,g,F;return{c(){p=r("p"),M=t("Although the recipe for forward pass needs to be defined within this function, one should call the "),m=r("code"),g=t("Module"),F=t(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(T){p=a(T,"P",{});var _=i(p);M=o(_,"Although the recipe for forward pass needs to be defined within this function, one should call the "),m=a(_,"CODE",{});var z=i(m);g=o(z,"Module"),z.forEach(n),F=o(_,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),_.forEach(n)},m(T,_){h(T,p,_),e(p,M),e(p,m),e(m,g),e(p,F)},d(T){T&&n(p)}}}function Gy(W){let p,M,m,g,F,T,_,z,ce,G,q,J,I,ne,ue,O,pe,ie,U,L,te,Z,P,j,oe,Q,le,se,S,he,de,C,fe,B,ee,ae,R,me,N,A,re,H,ge;return{c(){p=r("p"),M=t("TF 2.0 models accepts two formats as inputs:"),m=l(),g=r("ul"),F=r("li"),T=t("having all inputs as keyword arguments (like PyTorch models), or"),_=l(),z=r("li"),ce=t("having all inputs as a list, tuple or dict in the first positional arguments."),G=l(),q=r("p"),J=t("This second option is useful when using "),I=r("code"),ne=t("tf.keras.Model.fit"),ue=t(` method which currently requires having all the
tensors in the first argument of the model call function: `),O=r("code"),pe=t("model(inputs)"),ie=t("."),U=l(),L=r("p"),te=t(`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),Z=l(),P=r("ul"),j=r("li"),oe=t("a single Tensor with "),Q=r("code"),le=t("input_ids"),se=t(" only and nothing else: "),S=r("code"),he=t("model(inputs_ids)"),de=l(),C=r("li"),fe=t(`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=r("code"),ee=t("model([input_ids, attention_mask])"),ae=t(" or "),R=r("code"),me=t("model([input_ids, attention_mask, token_type_ids])"),N=l(),A=r("li"),re=t(`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),H=r("code"),ge=t('model({"input_ids": input_ids, "token_type_ids": token_type_ids})')},l(u){p=a(u,"P",{});var v=i(p);M=o(v,"TF 2.0 models accepts two formats as inputs:"),v.forEach(n),m=d(u),g=a(u,"UL",{});var K=i(g);F=a(K,"LI",{});var Te=i(F);T=o(Te,"having all inputs as keyword arguments (like PyTorch models), or"),Te.forEach(n),_=d(K),z=a(K,"LI",{});var we=i(z);ce=o(we,"having all inputs as a list, tuple or dict in the first positional arguments."),we.forEach(n),K.forEach(n),G=d(u),q=a(u,"P",{});var D=i(q);J=o(D,"This second option is useful when using "),I=a(D,"CODE",{});var Fe=i(I);ne=o(Fe,"tf.keras.Model.fit"),Fe.forEach(n),ue=o(D,` method which currently requires having all the
tensors in the first argument of the model call function: `),O=a(D,"CODE",{});var be=i(O);pe=o(be,"model(inputs)"),be.forEach(n),ie=o(D,"."),D.forEach(n),U=d(u),L=a(u,"P",{});var ye=i(L);te=o(ye,`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),ye.forEach(n),Z=d(u),P=a(u,"UL",{});var x=i(P);j=a(x,"LI",{});var V=i(j);oe=o(V,"a single Tensor with "),Q=a(V,"CODE",{});var $e=i(Q);le=o($e,"input_ids"),$e.forEach(n),se=o(V," only and nothing else: "),S=a(V,"CODE",{});var ve=i(S);he=o(ve,"model(inputs_ids)"),ve.forEach(n),V.forEach(n),de=d(x),C=a(x,"LI",{});var Y=i(C);fe=o(Y,`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=a(Y,"CODE",{});var Ee=i(B);ee=o(Ee,"model([input_ids, attention_mask])"),Ee.forEach(n),ae=o(Y," or "),R=a(Y,"CODE",{});var ke=i(R);me=o(ke,"model([input_ids, attention_mask, token_type_ids])"),ke.forEach(n),Y.forEach(n),N=d(x),A=a(x,"LI",{});var _e=i(A);re=o(_e,`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),H=a(_e,"CODE",{});var Me=i(H);ge=o(Me,'model({"input_ids": input_ids, "token_type_ids": token_type_ids})'),Me.forEach(n),_e.forEach(n),x.forEach(n)},m(u,v){h(u,p,v),e(p,M),h(u,m,v),h(u,g,v),e(g,F),e(F,T),e(g,_),e(g,z),e(z,ce),h(u,G,v),h(u,q,v),e(q,J),e(q,I),e(I,ne),e(q,ue),e(q,O),e(O,pe),e(q,ie),h(u,U,v),h(u,L,v),e(L,te),h(u,Z,v),h(u,P,v),e(P,j),e(j,oe),e(j,Q),e(Q,le),e(j,se),e(j,S),e(S,he),e(P,de),e(P,C),e(C,fe),e(C,B),e(B,ee),e(C,ae),e(C,R),e(R,me),e(P,N),e(P,A),e(A,re),e(A,H),e(H,ge)},d(u){u&&n(p),u&&n(m),u&&n(g),u&&n(G),u&&n(q),u&&n(U),u&&n(L),u&&n(Z),u&&n(P)}}}function Zy(W){let p,M,m,g,F;return{c(){p=r("p"),M=t("Although the recipe for forward pass needs to be defined within this function, one should call the "),m=r("code"),g=t("Module"),F=t(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(T){p=a(T,"P",{});var _=i(p);M=o(_,"Although the recipe for forward pass needs to be defined within this function, one should call the "),m=a(_,"CODE",{});var z=i(m);g=o(z,"Module"),z.forEach(n),F=o(_,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),_.forEach(n)},m(T,_){h(T,p,_),e(p,M),e(p,m),e(m,g),e(p,F)},d(T){T&&n(p)}}}function Ky(W){let p,M,m,g,F,T,_,z,ce,G,q,J,I,ne,ue,O,pe,ie,U,L,te,Z,P,j,oe,Q,le,se,S,he,de,C,fe,B,ee,ae,R,me,N,A,re,H,ge,u,v,K,Te,we,D,Fe,be,ye,x,V,$e,ve,Y,Ee,ke,_e,Me,Ua,Sp,Np,Ec,xn,Bp,Oo,Wp,Qp,So,Rp,Hp,Mc,Kn,Bt,ul,No,Vp,pl,Yp,zc,Cn,Bo,Up,jn,Gp,Ga,Zp,Kp,Za,Xp,Jp,Wo,eh,nh,th,Xn,oh,Ka,sh,rh,Xa,ah,ih,qc,Jn,Wt,hl,Qo,lh,fl,dh,Pc,Pe,Ro,ch,ml,uh,ph,Qt,Ja,hh,fh,ei,mh,gh,_h,Ho,Th,ni,Fh,vh,kh,Ln,Vo,wh,gl,bh,yh,Yo,ti,$h,_l,Eh,Mh,oi,zh,Tl,qh,Ph,Rt,Uo,Ch,Go,jh,Fl,xh,Lh,Ah,wn,Zo,Dh,vl,Ih,Oh,Ko,Sh,et,Nh,kl,Bh,Wh,wl,Qh,Rh,Hh,si,Xo,Cc,nt,Ht,bl,Jo,Vh,yl,Yh,jc,Ze,es,Uh,ns,Gh,$l,Zh,Kh,Xh,Vt,ri,Jh,ef,ai,nf,tf,of,ts,sf,ii,rf,af,lf,bn,os,df,El,cf,uf,ss,pf,tt,hf,Ml,ff,mf,zl,gf,_f,xc,ot,Yt,ql,rs,Tf,Pl,Ff,Lc,st,as,vf,is,kf,li,wf,bf,Ac,rt,ls,yf,ds,$f,di,Ef,Mf,Dc,at,Ut,Cl,cs,zf,jl,qf,Ic,We,us,Pf,xl,Cf,jf,ps,xf,hs,Lf,Af,Df,fs,If,ci,Of,Sf,Nf,ms,Bf,gs,Wf,Qf,Rf,Ke,_s,Hf,it,Vf,ui,Yf,Uf,Ll,Gf,Zf,Kf,Gt,Xf,Al,Jf,em,Ts,Oc,lt,Zt,Dl,Fs,nm,Il,tm,Sc,Qe,vs,om,Ol,sm,rm,ks,am,ws,im,lm,dm,bs,cm,pi,um,pm,hm,ys,fm,$s,mm,gm,_m,Xe,Es,Tm,dt,Fm,hi,vm,km,Sl,wm,bm,ym,Kt,$m,Nl,Em,Mm,Ms,Nc,ct,Xt,Bl,zs,zm,Wl,qm,Bc,ut,qs,Pm,Je,Ps,Cm,pt,jm,fi,xm,Lm,Ql,Am,Dm,Im,Jt,Om,Rl,Sm,Nm,Cs,Wc,ht,eo,Hl,js,Bm,Vl,Wm,Qc,Re,xs,Qm,Ls,Rm,Yl,Hm,Vm,Ym,As,Um,Ds,Gm,Zm,Km,Is,Xm,mi,Jm,eg,ng,Os,tg,Ss,og,sg,rg,en,Ns,ag,ft,ig,gi,lg,dg,Ul,cg,ug,pg,no,hg,Gl,fg,mg,Bs,Rc,mt,to,Zl,Ws,gg,Kl,_g,Hc,He,Qs,Tg,Xl,Fg,vg,Rs,kg,Hs,wg,bg,yg,Vs,$g,_i,Eg,Mg,zg,Ys,qg,Us,Pg,Cg,jg,Be,Gs,xg,gt,Lg,Ti,Ag,Dg,Jl,Ig,Og,Sg,oo,Ng,ed,Bg,Wg,Zs,Qg,nd,Rg,Hg,Ks,Vc,_t,so,td,Xs,Vg,od,Yg,Yc,Ve,Js,Ug,sd,Gg,Zg,er,Kg,nr,Xg,Jg,e_,tr,n_,Fi,t_,o_,s_,or,r_,sr,a_,i_,l_,nn,rr,d_,Tt,c_,vi,u_,p_,rd,h_,f_,m_,ro,g_,ad,__,T_,ar,Uc,Ft,ao,id,ir,F_,ld,v_,Gc,Ye,lr,k_,dd,w_,b_,dr,y_,cr,$_,E_,M_,ur,z_,ki,q_,P_,C_,pr,j_,hr,x_,L_,A_,tn,fr,D_,vt,I_,wi,O_,S_,cd,N_,B_,W_,io,Q_,ud,R_,H_,mr,Zc,kt,lo,pd,gr,V_,hd,Y_,Kc,Ue,_r,U_,wt,G_,fd,Z_,K_,md,X_,J_,eT,Tr,nT,Fr,tT,oT,sT,vr,rT,bi,aT,iT,lT,kr,dT,wr,cT,uT,pT,on,br,hT,bt,fT,yi,mT,gT,gd,_T,TT,FT,co,vT,_d,kT,wT,yr,Xc,yt,uo,Td,$r,bT,Fd,yT,Jc,je,Er,$T,vd,ET,MT,Mr,zT,zr,qT,PT,CT,qr,jT,$i,xT,LT,AT,Pr,DT,Cr,IT,OT,ST,po,NT,sn,jr,BT,$t,WT,Ei,QT,RT,kd,HT,VT,YT,ho,UT,wd,GT,ZT,xr,eu,Et,fo,bd,Lr,KT,yd,XT,nu,xe,Ar,JT,$d,eF,nF,Dr,tF,Ir,oF,sF,rF,Or,aF,Mi,iF,lF,dF,Sr,cF,Nr,uF,pF,hF,mo,fF,rn,Br,mF,Mt,gF,zi,_F,TF,Ed,FF,vF,kF,go,wF,Md,bF,yF,Wr,tu,zt,_o,zd,Qr,$F,qd,EF,ou,Le,Rr,MF,Pd,zF,qF,Hr,PF,Vr,CF,jF,xF,Yr,LF,qi,AF,DF,IF,Ur,OF,Gr,SF,NF,BF,To,WF,an,Zr,QF,qt,RF,Pi,HF,VF,Cd,YF,UF,GF,Fo,ZF,jd,KF,XF,Kr,su,Pt,vo,xd,Xr,JF,Ld,ev,ru,Ae,Jr,nv,ea,tv,Ad,ov,sv,rv,na,av,ta,iv,lv,dv,oa,cv,Ci,uv,pv,hv,sa,fv,ra,mv,gv,_v,ko,Tv,ln,aa,Fv,Ct,vv,ji,kv,wv,Dd,bv,yv,$v,wo,Ev,Id,Mv,zv,ia,au,jt,bo,Od,la,qv,Sd,Pv,iu,De,da,Cv,Nd,jv,xv,ca,Lv,ua,Av,Dv,Iv,pa,Ov,xi,Sv,Nv,Bv,ha,Wv,fa,Qv,Rv,Hv,yo,Vv,dn,ma,Yv,xt,Uv,Li,Gv,Zv,Bd,Kv,Xv,Jv,$o,ek,Wd,nk,tk,ga,lu,Lt,Eo,Qd,_a,ok,Rd,sk,du,Ie,Ta,rk,Hd,ak,ik,Fa,lk,va,dk,ck,uk,ka,pk,Ai,hk,fk,mk,wa,gk,ba,_k,Tk,Fk,Mo,vk,cn,ya,kk,At,wk,Di,bk,yk,Vd,$k,Ek,Mk,zo,zk,Yd,qk,Pk,$a,cu,Dt,qo,Ud,Ea,Ck,Gd,jk,uu,Oe,Ma,xk,Zd,Lk,Ak,za,Dk,qa,Ik,Ok,Sk,Pa,Nk,Ii,Bk,Wk,Qk,Ca,Rk,ja,Hk,Vk,Yk,Po,Uk,un,xa,Gk,It,Zk,Oi,Kk,Xk,Kd,Jk,e1,n1,Co,t1,Xd,o1,s1,La,pu,Ot,jo,Jd,Aa,r1,ec,a1,hu,Se,Da,i1,St,l1,nc,d1,c1,tc,u1,p1,h1,Ia,f1,Oa,m1,g1,_1,Sa,T1,Si,F1,v1,k1,Na,w1,Ba,b1,y1,$1,xo,E1,pn,Wa,M1,Nt,z1,Ni,q1,P1,oc,C1,j1,x1,Lo,L1,sc,A1,D1,Qa,fu;return T=new qe({}),ne=new qe({}),No=new qe({}),Bo=new X({props:{name:"class transformers.FunnelConfig",anchor:"transformers.FunnelConfig",parameters:[{name:"vocab_size",val:" = 30522"},{name:"block_sizes",val:" = [4, 4, 4]"},{name:"block_repeats",val:" = None"},{name:"num_decoder_layers",val:" = 2"},{name:"d_model",val:" = 768"},{name:"n_head",val:" = 12"},{name:"d_head",val:" = 64"},{name:"d_inner",val:" = 3072"},{name:"hidden_act",val:" = 'gelu_new'"},{name:"hidden_dropout",val:" = 0.1"},{name:"attention_dropout",val:" = 0.1"},{name:"activation_dropout",val:" = 0.0"},{name:"max_position_embeddings",val:" = 512"},{name:"type_vocab_size",val:" = 3"},{name:"initializer_range",val:" = 0.1"},{name:"initializer_std",val:" = None"},{name:"layer_norm_eps",val:" = 1e-09"},{name:"pooling_type",val:" = 'mean'"},{name:"attention_type",val:" = 'relative_shift'"},{name:"separate_cls",val:" = True"},{name:"truncate_seq",val:" = True"},{name:"pool_q_only",val:" = True"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_16073/src/transformers/models/funnel/configuration_funnel.py#L37",parametersDescription:[{anchor:"transformers.FunnelConfig.vocab_size",description:`<strong>vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 30522) &#x2014;
Vocabulary size of the Funnel transformer. Defines the number of different tokens that can be represented
by the <code>inputs_ids</code> passed when calling <a href="/docs/transformers/pr_16073/en/model_doc/funnel#transformers.FunnelModel">FunnelModel</a> or <a href="/docs/transformers/pr_16073/en/model_doc/funnel#transformers.TFFunnelModel">TFFunnelModel</a>.`,name:"vocab_size"},{anchor:"transformers.FunnelConfig.block_sizes",description:`<strong>block_sizes</strong> (<code>List[int]</code>, <em>optional</em>, defaults to <code>[4, 4, 4]</code>) &#x2014;
The sizes of the blocks used in the model.`,name:"block_sizes"},{anchor:"transformers.FunnelConfig.block_repeats",description:`<strong>block_repeats</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
If passed along, each layer of each block is repeated the number of times indicated.`,name:"block_repeats"},{anchor:"transformers.FunnelConfig.num_decoder_layers",description:`<strong>num_decoder_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 2) &#x2014;
The number of layers in the decoder (when not using the base model).`,name:"num_decoder_layers"},{anchor:"transformers.FunnelConfig.d_model",description:`<strong>d_model</strong> (<code>int</code>, <em>optional</em>, defaults to 768) &#x2014;
Dimensionality of the model&#x2019;s hidden states.`,name:"d_model"},{anchor:"transformers.FunnelConfig.n_head",description:`<strong>n_head</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of attention heads for each attention layer in the Transformer encoder.`,name:"n_head"},{anchor:"transformers.FunnelConfig.d_head",description:`<strong>d_head</strong> (<code>int</code>, <em>optional</em>, defaults to 64) &#x2014;
Dimensionality of the model&#x2019;s heads.`,name:"d_head"},{anchor:"transformers.FunnelConfig.d_inner",description:`<strong>d_inner</strong> (<code>int</code>, <em>optional</em>, defaults to 3072) &#x2014;
Inner dimension in the feed-forward blocks.`,name:"d_inner"},{anchor:"transformers.FunnelConfig.hidden_act",description:`<strong>hidden_act</strong> (<code>str</code> or <code>callable</code>, <em>optional</em>, defaults to <code>&quot;gelu_new&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the encoder and pooler. If string, <code>&quot;gelu&quot;</code>,
<code>&quot;relu&quot;</code>, <code>&quot;silu&quot;</code> and <code>&quot;gelu_new&quot;</code> are supported.`,name:"hidden_act"},{anchor:"transformers.FunnelConfig.hidden_dropout",description:`<strong>hidden_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.`,name:"hidden_dropout"},{anchor:"transformers.FunnelConfig.attention_dropout",description:`<strong>attention_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout probability for the attention probabilities.`,name:"attention_dropout"},{anchor:"transformers.FunnelConfig.activation_dropout",description:`<strong>activation_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout probability used between the two layers of the feed-forward blocks.`,name:"activation_dropout"},{anchor:"transformers.FunnelConfig.max_position_embeddings",description:`<strong>max_position_embeddings</strong> (<code>int</code>, <em>optional</em>, defaults to 512) &#x2014;
The maximum sequence length that this model might ever be used with. Typically set this to something large
just in case (e.g., 512 or 1024 or 2048).`,name:"max_position_embeddings"},{anchor:"transformers.FunnelConfig.type_vocab_size",description:`<strong>type_vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 3) &#x2014;
The vocabulary size of the <code>token_type_ids</code> passed when calling <a href="/docs/transformers/pr_16073/en/model_doc/funnel#transformers.FunnelModel">FunnelModel</a> or <a href="/docs/transformers/pr_16073/en/model_doc/funnel#transformers.TFFunnelModel">TFFunnelModel</a>.`,name:"type_vocab_size"},{anchor:"transformers.FunnelConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The upper bound of the <em>uniform initializer</em> for initializing all weight matrices in attention layers.`,name:"initializer_range"},{anchor:"transformers.FunnelConfig.initializer_std",description:`<strong>initializer_std</strong> (<code>float</code>, <em>optional</em>) &#x2014;
The standard deviation of the <em>normal initializer</em> for initializing the embedding matrix and the weight of
linear layers. Will default to 1 for the embedding matrix and the value given by Xavier initialization for
linear layers.`,name:"initializer_std"},{anchor:"transformers.FunnelConfig.layer_norm_eps",description:`<strong>layer_norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-9) &#x2014;
The epsilon used by the layer normalization layers.`,name:"layer_norm_eps"},{anchor:"transformers.FunnelConfig.pooling_type",description:`<strong>pooling_type</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;mean&quot;</code>) &#x2014;
Possible values are <code>&quot;mean&quot;</code> or <code>&quot;max&quot;</code>. The way pooling is performed at the beginning of each block.`,name:"pooling_type"},{anchor:"transformers.FunnelConfig.attention_type",description:`<strong>attention_type</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;relative_shift&quot;</code>) &#x2014;
Possible values are <code>&quot;relative_shift&quot;</code> or <code>&quot;factorized&quot;</code>. The former is faster on CPU/GPU while the latter
is faster on TPU.`,name:"attention_type"},{anchor:"transformers.FunnelConfig.separate_cls",description:`<strong>separate_cls</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to separate the cls token when applying pooling.`,name:"separate_cls"},{anchor:"transformers.FunnelConfig.truncate_seq",description:`<strong>truncate_seq</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
When using <code>separate_cls</code>, whether or not to truncate the last token when pooling, to avoid getting a
sequence length that is not a multiple of 2.`,name:"truncate_seq"},{anchor:"transformers.FunnelConfig.pool_q_only",description:`<strong>pool_q_only</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to apply the pooling only to the query or to query, key and values for the attention layers.`,name:"pool_q_only"}]}}),Qo=new qe({}),Ro=new X({props:{name:"class transformers.FunnelTokenizer",anchor:"transformers.FunnelTokenizer",parameters:[{name:"vocab_file",val:""},{name:"do_lower_case",val:" = True"},{name:"do_basic_tokenize",val:" = True"},{name:"never_split",val:" = None"},{name:"unk_token",val:" = '<unk>'"},{name:"sep_token",val:" = '<sep>'"},{name:"pad_token",val:" = '<pad>'"},{name:"cls_token",val:" = '<cls>'"},{name:"mask_token",val:" = '<mask>'"},{name:"bos_token",val:" = '<s>'"},{name:"eos_token",val:" = '</s>'"},{name:"tokenize_chinese_chars",val:" = True"},{name:"strip_accents",val:" = None"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_16073/src/transformers/models/funnel/tokenization_funnel.py#L58"}}),Vo=new X({props:{name:"build_inputs_with_special_tokens",anchor:"transformers.BertTokenizer.build_inputs_with_special_tokens",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"}],source:"https://github.com/huggingface/transformers/blob/pr_16073/src/transformers/models/bert/tokenization_bert.py#L248",parametersDescription:[{anchor:"transformers.BertTokenizer.build_inputs_with_special_tokens.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs to which the special tokens will be added.`,name:"token_ids_0"},{anchor:"transformers.BertTokenizer.build_inputs_with_special_tokens.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],returnDescription:`
<p>List of <a href="../glossary#input-ids">input IDs</a> with the appropriate special tokens.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),Uo=new X({props:{name:"get_special_tokens_mask",anchor:"transformers.BertTokenizer.get_special_tokens_mask",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"},{name:"already_has_special_tokens",val:": bool = False"}],source:"https://github.com/huggingface/transformers/blob/pr_16073/src/transformers/models/bert/tokenization_bert.py#L273",parametersDescription:[{anchor:"transformers.BertTokenizer.get_special_tokens_mask.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.BertTokenizer.get_special_tokens_mask.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"},{anchor:"transformers.BertTokenizer.get_special_tokens_mask.already_has_special_tokens",description:`<strong>already_has_special_tokens</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not the token list is already formatted with special tokens for the model.`,name:"already_has_special_tokens"}],returnDescription:`
<p>A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),Zo=new X({props:{name:"create_token_type_ids_from_sequences",anchor:"transformers.FunnelTokenizer.create_token_type_ids_from_sequences",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"}],source:"https://github.com/huggingface/transformers/blob/pr_16073/src/transformers/models/funnel/tokenization_funnel.py#L108",parametersDescription:[{anchor:"transformers.FunnelTokenizer.create_token_type_ids_from_sequences.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.FunnelTokenizer.create_token_type_ids_from_sequences.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],returnDescription:`
<p>List of <a href="../glossary#token-type-ids">token type IDs</a> according to the given sequence(s).</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),Ko=new Ne({props:{code:`2 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1
| first sequence    | second sequence |`,highlighted:`2<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 1 </span>1<span class="hljs-number"> 1 </span>1<span class="hljs-number"> 1 </span>1<span class="hljs-number"> 1 </span>1 1
| first sequence    | second sequence |`}}),Xo=new X({props:{name:"save_vocabulary",anchor:"transformers.BertTokenizer.save_vocabulary",parameters:[{name:"save_directory",val:": str"},{name:"filename_prefix",val:": typing.Optional[str] = None"}],source:"https://github.com/huggingface/transformers/blob/pr_16073/src/transformers/models/bert/tokenization_bert.py#L330"}}),Jo=new qe({}),es=new X({props:{name:"class transformers.FunnelTokenizerFast",anchor:"transformers.FunnelTokenizerFast",parameters:[{name:"vocab_file",val:" = None"},{name:"tokenizer_file",val:" = None"},{name:"do_lower_case",val:" = True"},{name:"unk_token",val:" = '<unk>'"},{name:"sep_token",val:" = '<sep>'"},{name:"pad_token",val:" = '<pad>'"},{name:"cls_token",val:" = '<cls>'"},{name:"mask_token",val:" = '<mask>'"},{name:"bos_token",val:" = '<s>'"},{name:"eos_token",val:" = '</s>'"},{name:"clean_text",val:" = True"},{name:"tokenize_chinese_chars",val:" = True"},{name:"strip_accents",val:" = None"},{name:"wordpieces_prefix",val:" = '##'"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_16073/src/transformers/models/funnel/tokenization_funnel_fast.py#L71"}}),os=new X({props:{name:"create_token_type_ids_from_sequences",anchor:"transformers.FunnelTokenizerFast.create_token_type_ids_from_sequences",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"}],source:"https://github.com/huggingface/transformers/blob/pr_16073/src/transformers/models/funnel/tokenization_funnel_fast.py#L124",parametersDescription:[{anchor:"transformers.FunnelTokenizerFast.create_token_type_ids_from_sequences.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.FunnelTokenizerFast.create_token_type_ids_from_sequences.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],returnDescription:`
<p>List of <a href="../glossary#token-type-ids">token type IDs</a> according to the given sequence(s).</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),ss=new Ne({props:{code:`2 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1
| first sequence    | second sequence |`,highlighted:`2<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 1 </span>1<span class="hljs-number"> 1 </span>1<span class="hljs-number"> 1 </span>1<span class="hljs-number"> 1 </span>1 1
| first sequence    | second sequence |`}}),rs=new qe({}),as=new X({props:{name:"class transformers.models.funnel.modeling_funnel.FunnelForPreTrainingOutput",anchor:"transformers.models.funnel.modeling_funnel.FunnelForPreTrainingOutput",parameters:[{name:"loss",val:": typing.Optional[torch.FloatTensor] = None"},{name:"logits",val:": FloatTensor = None"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}],source:"https://github.com/huggingface/transformers/blob/pr_16073/src/transformers/models/funnel/modeling_funnel.py#L801",parametersDescription:[{anchor:"transformers.models.funnel.modeling_funnel.FunnelForPreTrainingOutput.loss",description:`<strong>loss</strong> (<em>optional</em>, returned when <code>labels</code> is provided, <code>torch.FloatTensor</code> of shape <code>(1,)</code>) &#x2014;
Total loss of the ELECTRA-style objective.`,name:"loss"},{anchor:"transformers.models.funnel.modeling_funnel.FunnelForPreTrainingOutput.logits",description:`<strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Prediction scores of the head (scores for each token before SoftMax).`,name:"logits"},{anchor:"transformers.models.funnel.modeling_funnel.FunnelForPreTrainingOutput.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.models.funnel.modeling_funnel.FunnelForPreTrainingOutput.attentions",description:`<strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"}]}}),ls=new X({props:{name:"class transformers.models.funnel.modeling_tf_funnel.TFFunnelForPreTrainingOutput",anchor:"transformers.models.funnel.modeling_tf_funnel.TFFunnelForPreTrainingOutput",parameters:[{name:"logits",val:": Tensor = None"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"}],source:"https://github.com/huggingface/transformers/blob/pr_16073/src/transformers/models/funnel/modeling_tf_funnel.py#L1005",parametersDescription:[{anchor:"transformers.models.funnel.modeling_tf_funnel.TFFunnelForPreTrainingOutput.logits",description:`<strong>logits</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Prediction scores of the head (scores for each token before SoftMax).`,name:"logits"},{anchor:"transformers.models.funnel.modeling_tf_funnel.TFFunnelForPreTrainingOutput.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.models.funnel.modeling_tf_funnel.TFFunnelForPreTrainingOutput.attentions",description:`<strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"}]}}),cs=new qe({}),us=new X({props:{name:"class transformers.FunnelBaseModel",anchor:"transformers.FunnelBaseModel",parameters:[{name:"config",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_16073/src/transformers/models/funnel/modeling_funnel.py#L894",parametersDescription:[{anchor:"transformers.FunnelBaseModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_16073/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_16073/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),_s=new X({props:{name:"forward",anchor:"transformers.FunnelBaseModel.forward",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:" = None"},{name:"token_type_ids",val:" = None"},{name:"position_ids",val:" = None"},{name:"head_mask",val:" = None"},{name:"inputs_embeds",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"}],source:"https://github.com/huggingface/transformers/blob/pr_16073/src/transformers/models/funnel/modeling_funnel.py#L910",parametersDescription:[{anchor:"transformers.FunnelBaseModel.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/pr_16073/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a>. See <a href="/docs/transformers/pr_16073/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/pr_16073/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.FunnelBaseModel.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.FunnelBaseModel.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.FunnelBaseModel.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.FunnelBaseModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FunnelBaseModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FunnelBaseModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_16073/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],returnDescription:`
<p>A <a
  href="/docs/transformers/pr_16073/en/main_classes/output#transformers.modeling_outputs.BaseModelOutput"
>transformers.modeling_outputs.BaseModelOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/pr_16073/en/model_doc/funnel#transformers.FunnelConfig"
>FunnelConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) \u2014 Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/pr_16073/en/main_classes/output#transformers.modeling_outputs.BaseModelOutput"
>transformers.modeling_outputs.BaseModelOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Gt=new ze({props:{$$slots:{default:[My]},$$scope:{ctx:W}}}),Ts=new Ne({props:{code:`from transformers import FunnelTokenizer, FunnelBaseModel
import torch

tokenizer = FunnelTokenizer.from_pretrained("funnel-transformer/small-base")
model = FunnelBaseModel.from_pretrained("funnel-transformer/small-base")

inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
outputs = model(**inputs)

last_hidden_states = outputs.last_hidden_state`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FunnelTokenizer, FunnelBaseModel
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = FunnelTokenizer.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FunnelBaseModel.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state`}}),Fs=new qe({}),vs=new X({props:{name:"class transformers.FunnelModel",anchor:"transformers.FunnelModel",parameters:[{name:"config",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_16073/src/transformers/models/funnel/modeling_funnel.py#L971",parametersDescription:[{anchor:"transformers.FunnelModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_16073/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_16073/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),Es=new X({props:{name:"forward",anchor:"transformers.FunnelModel.forward",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:" = None"},{name:"token_type_ids",val:" = None"},{name:"inputs_embeds",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"}],source:"https://github.com/huggingface/transformers/blob/pr_16073/src/transformers/models/funnel/modeling_funnel.py#L988",parametersDescription:[{anchor:"transformers.FunnelModel.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/pr_16073/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a>. See <a href="/docs/transformers/pr_16073/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/pr_16073/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.FunnelModel.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.FunnelModel.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.FunnelModel.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.FunnelModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FunnelModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FunnelModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_16073/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],returnDescription:`
<p>A <a
  href="/docs/transformers/pr_16073/en/main_classes/output#transformers.modeling_outputs.BaseModelOutput"
>transformers.modeling_outputs.BaseModelOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/pr_16073/en/model_doc/funnel#transformers.FunnelConfig"
>FunnelConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) \u2014 Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/pr_16073/en/main_classes/output#transformers.modeling_outputs.BaseModelOutput"
>transformers.modeling_outputs.BaseModelOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Kt=new ze({props:{$$slots:{default:[zy]},$$scope:{ctx:W}}}),Ms=new Ne({props:{code:`from transformers import FunnelTokenizer, FunnelModel
import torch

tokenizer = FunnelTokenizer.from_pretrained("funnel-transformer/small")
model = FunnelModel.from_pretrained("funnel-transformer/small")

inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
outputs = model(**inputs)

last_hidden_states = outputs.last_hidden_state`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FunnelTokenizer, FunnelModel
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = FunnelTokenizer.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FunnelModel.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state`}}),zs=new qe({}),qs=new X({props:{name:"class transformers.FunnelForPreTraining",anchor:"transformers.FunnelForPreTraining",parameters:[{name:"config",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_16073/src/transformers/models/funnel/modeling_funnel.py#L1079"}}),Ps=new X({props:{name:"forward",anchor:"transformers.FunnelForPreTraining.forward",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:" = None"},{name:"token_type_ids",val:" = None"},{name:"inputs_embeds",val:" = None"},{name:"labels",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"}],source:"https://github.com/huggingface/transformers/blob/pr_16073/src/transformers/models/funnel/modeling_funnel.py#L1088",parametersDescription:[{anchor:"transformers.FunnelForPreTraining.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/pr_16073/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a>. See <a href="/docs/transformers/pr_16073/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/pr_16073/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.FunnelForPreTraining.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.FunnelForPreTraining.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.FunnelForPreTraining.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.FunnelForPreTraining.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FunnelForPreTraining.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FunnelForPreTraining.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_16073/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.FunnelForPreTraining.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Labels for computing the ELECTRA-style loss. Input should be a sequence of tokens (see <code>input_ids</code>
docstring) Indices should be in <code>[0, 1]</code>:</p>
<ul>
<li>0 indicates the token is an original token,</li>
<li>1 indicates the token was replaced.</li>
</ul>`,name:"labels"}],returnDescription:`
<p>A <a
  href="/docs/transformers/pr_16073/en/model_doc/funnel#transformers.models.funnel.modeling_funnel.FunnelForPreTrainingOutput"
>transformers.models.funnel.modeling_funnel.FunnelForPreTrainingOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/pr_16073/en/model_doc/funnel#transformers.FunnelConfig"
>FunnelConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<em>optional</em>, returned when <code>labels</code> is provided, <code>torch.FloatTensor</code> of shape <code>(1,)</code>) \u2014 Total loss of the ELECTRA-style objective.</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>) \u2014 Prediction scores of the head (scores for each token before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/pr_16073/en/model_doc/funnel#transformers.models.funnel.modeling_funnel.FunnelForPreTrainingOutput"
>transformers.models.funnel.modeling_funnel.FunnelForPreTrainingOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Jt=new ze({props:{$$slots:{default:[qy]},$$scope:{ctx:W}}}),Cs=new Ne({props:{code:`from transformers import FunnelTokenizer, FunnelForPreTraining
import torch

tokenizer = FunnelTokenizer.from_pretrained("funnel-transformer/small")
model = FunnelForPreTraining.from_pretrained("funnel-transformer/small")

inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
logits = model(**inputs).logits`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FunnelTokenizer, FunnelForPreTraining
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = FunnelTokenizer.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FunnelForPreTraining.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = model(**inputs).logits`}}),js=new qe({}),xs=new X({props:{name:"class transformers.FunnelForMaskedLM",anchor:"transformers.FunnelForMaskedLM",parameters:[{name:"config",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_16073/src/transformers/models/funnel/modeling_funnel.py#L1162",parametersDescription:[{anchor:"transformers.FunnelForMaskedLM.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_16073/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_16073/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),Ns=new X({props:{name:"forward",anchor:"transformers.FunnelForMaskedLM.forward",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:" = None"},{name:"token_type_ids",val:" = None"},{name:"inputs_embeds",val:" = None"},{name:"labels",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"}],source:"https://github.com/huggingface/transformers/blob/pr_16073/src/transformers/models/funnel/modeling_funnel.py#L1178",parametersDescription:[{anchor:"transformers.FunnelForMaskedLM.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/pr_16073/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a>. See <a href="/docs/transformers/pr_16073/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/pr_16073/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.FunnelForMaskedLM.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.FunnelForMaskedLM.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.FunnelForMaskedLM.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.FunnelForMaskedLM.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FunnelForMaskedLM.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FunnelForMaskedLM.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_16073/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.FunnelForMaskedLM.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Labels for computing the masked language modeling loss. Indices should be in <code>[-100, 0, ..., config.vocab_size]</code> (see <code>input_ids</code> docstring) Tokens with indices set to <code>-100</code> are ignored (masked), the
loss is only computed for the tokens with labels in <code>[0, ..., config.vocab_size]</code>`,name:"labels"}],returnDescription:`
<p>A <a
  href="/docs/transformers/pr_16073/en/main_classes/output#transformers.modeling_outputs.MaskedLMOutput"
>transformers.modeling_outputs.MaskedLMOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/pr_16073/en/model_doc/funnel#transformers.FunnelConfig"
>FunnelConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) \u2014 Masked language modeling (MLM) loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, config.vocab_size)</code>) \u2014 Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/pr_16073/en/main_classes/output#transformers.modeling_outputs.MaskedLMOutput"
>transformers.modeling_outputs.MaskedLMOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),no=new ze({props:{$$slots:{default:[Py]},$$scope:{ctx:W}}}),Bs=new Ne({props:{code:`from transformers import FunnelTokenizer, FunnelForMaskedLM
import torch

tokenizer = FunnelTokenizer.from_pretrained("funnel-transformer/small")
model = FunnelForMaskedLM.from_pretrained("funnel-transformer/small")

inputs = tokenizer("The capital of France is <mask>.", return_tensors="pt")
labels = tokenizer("The capital of France is Paris.", return_tensors="pt")["input_ids"]

outputs = model(**inputs, labels=labels)
loss = outputs.loss
logits = outputs.logits`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FunnelTokenizer, FunnelForMaskedLM
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = FunnelTokenizer.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FunnelForMaskedLM.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;The capital of France is &lt;mask&gt;.&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>labels = tokenizer(<span class="hljs-string">&quot;The capital of France is Paris.&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)[<span class="hljs-string">&quot;input_ids&quot;</span>]

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs, labels=labels)
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = outputs.loss
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits`}}),Ws=new qe({}),Qs=new X({props:{name:"class transformers.FunnelForSequenceClassification",anchor:"transformers.FunnelForSequenceClassification",parameters:[{name:"config",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_16073/src/transformers/models/funnel/modeling_funnel.py#L1242",parametersDescription:[{anchor:"transformers.FunnelForSequenceClassification.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_16073/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_16073/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),Gs=new X({props:{name:"forward",anchor:"transformers.FunnelForSequenceClassification.forward",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:" = None"},{name:"token_type_ids",val:" = None"},{name:"inputs_embeds",val:" = None"},{name:"labels",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"}],source:"https://github.com/huggingface/transformers/blob/pr_16073/src/transformers/models/funnel/modeling_funnel.py#L1253",parametersDescription:[{anchor:"transformers.FunnelForSequenceClassification.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/pr_16073/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a>. See <a href="/docs/transformers/pr_16073/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/pr_16073/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.FunnelForSequenceClassification.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.FunnelForSequenceClassification.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.FunnelForSequenceClassification.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.FunnelForSequenceClassification.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FunnelForSequenceClassification.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FunnelForSequenceClassification.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_16073/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.FunnelForSequenceClassification.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for computing the sequence classification/regression loss. Indices should be in <code>[0, ..., config.num_labels - 1]</code>. If <code>config.num_labels == 1</code> a regression loss is computed (Mean-Square loss), If
<code>config.num_labels &gt; 1</code> a classification loss is computed (Cross-Entropy).`,name:"labels"}],returnDescription:`
<p>A <a
  href="/docs/transformers/pr_16073/en/main_classes/output#transformers.modeling_outputs.SequenceClassifierOutput"
>transformers.modeling_outputs.SequenceClassifierOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/pr_16073/en/model_doc/funnel#transformers.FunnelConfig"
>FunnelConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) \u2014 Classification (or regression if config.num_labels==1) loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, config.num_labels)</code>) \u2014 Classification (or regression if config.num_labels==1) scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/pr_16073/en/main_classes/output#transformers.modeling_outputs.SequenceClassifierOutput"
>transformers.modeling_outputs.SequenceClassifierOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),oo=new ze({props:{$$slots:{default:[Cy]},$$scope:{ctx:W}}}),Zs=new Ne({props:{code:`import torch
from transformers import FunnelTokenizer, FunnelForSequenceClassification

torch.manual_seed(0)
tokenizer = FunnelTokenizer.from_pretrained("funnel-transformer/small-base")
model = FunnelForSequenceClassification.from_pretrained("funnel-transformer/small-base", num_labels=2)

inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
labels = torch.tensor([1]).unsqueeze(0)  # Batch size 1
outputs = model(**inputs, labels=labels)
loss = outputs.loss
logits = outputs.logits
list(logits.shape)
`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FunnelTokenizer, FunnelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>torch.manual_seed(<span class="hljs-number">0</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = FunnelTokenizer.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FunnelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small-base&quot;</span>, num_labels=<span class="hljs-number">2</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>labels = torch.tensor([<span class="hljs-number">1</span>]).unsqueeze(<span class="hljs-number">0</span>)  <span class="hljs-comment"># Batch size 1</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs, labels=labels)
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = outputs.loss
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">list</span>(logits.shape)
`}}),Ks=new Ne({props:{code:`import torch
from transformers import FunnelTokenizer, FunnelForSequenceClassification

torch.manual_seed(0)
tokenizer = FunnelTokenizer.from_pretrained("funnel-transformer/small-base")
model = FunnelForSequenceClassification.from_pretrained("funnel-transformer/small-base", problem_type="multi_label_classification", num_labels=2)

inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
labels = torch.tensor([[1, 1]], dtype=torch.float)  # need dtype=float for BCEWithLogitsLoss
outputs = model(**inputs, labels=labels)
loss = outputs.loss
list(logits.shape)
`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FunnelTokenizer, FunnelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>torch.manual_seed(<span class="hljs-number">0</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = FunnelTokenizer.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FunnelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small-base&quot;</span>, problem_type=<span class="hljs-string">&quot;multi_label_classification&quot;</span>, num_labels=<span class="hljs-number">2</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>labels = torch.tensor([[<span class="hljs-number">1</span>, <span class="hljs-number">1</span>]], dtype=torch.<span class="hljs-built_in">float</span>)  <span class="hljs-comment"># need dtype=float for BCEWithLogitsLoss</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs, labels=labels)
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = outputs.loss
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">list</span>(logits.shape)
`}}),Xs=new qe({}),Js=new X({props:{name:"class transformers.FunnelForMultipleChoice",anchor:"transformers.FunnelForMultipleChoice",parameters:[{name:"config",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_16073/src/transformers/models/funnel/modeling_funnel.py#L1335",parametersDescription:[{anchor:"transformers.FunnelForMultipleChoice.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_16073/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_16073/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),rr=new X({props:{name:"forward",anchor:"transformers.FunnelForMultipleChoice.forward",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:" = None"},{name:"token_type_ids",val:" = None"},{name:"inputs_embeds",val:" = None"},{name:"labels",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"}],source:"https://github.com/huggingface/transformers/blob/pr_16073/src/transformers/models/funnel/modeling_funnel.py#L1344",parametersDescription:[{anchor:"transformers.FunnelForMultipleChoice.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, num_choices, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/pr_16073/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a>. See <a href="/docs/transformers/pr_16073/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/pr_16073/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.FunnelForMultipleChoice.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_choices, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.FunnelForMultipleChoice.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, num_choices, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.FunnelForMultipleChoice.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_choices, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.FunnelForMultipleChoice.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FunnelForMultipleChoice.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FunnelForMultipleChoice.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_16073/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.FunnelForMultipleChoice.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for computing the multiple choice classification loss. Indices should be in <code>[0, ..., num_choices-1]</code> where <code>num_choices</code> is the size of the second dimension of the input tensors. (See
<code>input_ids</code> above)`,name:"labels"}],returnDescription:`
<p>A <a
  href="/docs/transformers/pr_16073/en/main_classes/output#transformers.modeling_outputs.MultipleChoiceModelOutput"
>transformers.modeling_outputs.MultipleChoiceModelOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/pr_16073/en/model_doc/funnel#transformers.FunnelConfig"
>FunnelConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <em>(1,)</em>, <em>optional</em>, returned when <code>labels</code> is provided) \u2014 Classification loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_choices)</code>) \u2014 <em>num_choices</em> is the second dimension of the input tensors. (see <em>input_ids</em> above).</p>
<p>Classification scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/pr_16073/en/main_classes/output#transformers.modeling_outputs.MultipleChoiceModelOutput"
>transformers.modeling_outputs.MultipleChoiceModelOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),ro=new ze({props:{$$slots:{default:[jy]},$$scope:{ctx:W}}}),ar=new Ne({props:{code:`from transformers import FunnelTokenizer, FunnelForMultipleChoice
import torch

tokenizer = FunnelTokenizer.from_pretrained("funnel-transformer/small-base")
model = FunnelForMultipleChoice.from_pretrained("funnel-transformer/small-base")

prompt = "In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced."
choice0 = "It is eaten with a fork and a knife."
choice1 = "It is eaten while held in the hand."
labels = torch.tensor(0).unsqueeze(0)  # choice0 is correct (according to Wikipedia ;)), batch size 1

encoding = tokenizer([prompt, prompt], [choice0, choice1], return_tensors="pt", padding=True)
outputs = model(**{k: v.unsqueeze(0) for k, v in encoding.items()}, labels=labels)  # batch size is 1

# the linear classifier still needs to be trained
loss = outputs.loss
logits = outputs.logits`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FunnelTokenizer, FunnelForMultipleChoice
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = FunnelTokenizer.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FunnelForMultipleChoice.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>prompt = <span class="hljs-string">&quot;In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>choice0 = <span class="hljs-string">&quot;It is eaten with a fork and a knife.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>choice1 = <span class="hljs-string">&quot;It is eaten while held in the hand.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>labels = torch.tensor(<span class="hljs-number">0</span>).unsqueeze(<span class="hljs-number">0</span>)  <span class="hljs-comment"># choice0 is correct (according to Wikipedia ;)), batch size 1</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>encoding = tokenizer([prompt, prompt], [choice0, choice1], return_tensors=<span class="hljs-string">&quot;pt&quot;</span>, padding=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**{k: v.unsqueeze(<span class="hljs-number">0</span>) <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> encoding.items()}, labels=labels)  <span class="hljs-comment"># batch size is 1</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># the linear classifier still needs to be trained</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = outputs.loss
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits`}}),ir=new qe({}),lr=new X({props:{name:"class transformers.FunnelForTokenClassification",anchor:"transformers.FunnelForTokenClassification",parameters:[{name:"config",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_16073/src/transformers/models/funnel/modeling_funnel.py#L1419",parametersDescription:[{anchor:"transformers.FunnelForTokenClassification.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_16073/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_16073/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),fr=new X({props:{name:"forward",anchor:"transformers.FunnelForTokenClassification.forward",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:" = None"},{name:"token_type_ids",val:" = None"},{name:"inputs_embeds",val:" = None"},{name:"labels",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"}],source:"https://github.com/huggingface/transformers/blob/pr_16073/src/transformers/models/funnel/modeling_funnel.py#L1431",parametersDescription:[{anchor:"transformers.FunnelForTokenClassification.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/pr_16073/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a>. See <a href="/docs/transformers/pr_16073/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/pr_16073/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.FunnelForTokenClassification.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.FunnelForTokenClassification.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.FunnelForTokenClassification.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.FunnelForTokenClassification.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FunnelForTokenClassification.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FunnelForTokenClassification.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_16073/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.FunnelForTokenClassification.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Labels for computing the token classification loss. Indices should be in <code>[0, ..., config.num_labels - 1]</code>.`,name:"labels"}],returnDescription:`
<p>A <a
  href="/docs/transformers/pr_16073/en/main_classes/output#transformers.modeling_outputs.TokenClassifierOutput"
>transformers.modeling_outputs.TokenClassifierOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/pr_16073/en/model_doc/funnel#transformers.FunnelConfig"
>FunnelConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided)  \u2014 Classification loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, config.num_labels)</code>) \u2014 Classification scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/pr_16073/en/main_classes/output#transformers.modeling_outputs.TokenClassifierOutput"
>transformers.modeling_outputs.TokenClassifierOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),io=new ze({props:{$$slots:{default:[xy]},$$scope:{ctx:W}}}),mr=new Ne({props:{code:`from transformers import FunnelTokenizer, FunnelForTokenClassification
import torch

tokenizer = FunnelTokenizer.from_pretrained("funnel-transformer/small")
model = FunnelForTokenClassification.from_pretrained("funnel-transformer/small")

inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
labels = torch.tensor([1] * inputs["input_ids"].size(1)).unsqueeze(0)  # Batch size 1

outputs = model(**inputs, labels=labels)
loss = outputs.loss
logits = outputs.logits`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FunnelTokenizer, FunnelForTokenClassification
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = FunnelTokenizer.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FunnelForTokenClassification.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>labels = torch.tensor([<span class="hljs-number">1</span>] * inputs[<span class="hljs-string">&quot;input_ids&quot;</span>].size(<span class="hljs-number">1</span>)).unsqueeze(<span class="hljs-number">0</span>)  <span class="hljs-comment"># Batch size 1</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs, labels=labels)
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = outputs.loss
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits`}}),gr=new qe({}),_r=new X({props:{name:"class transformers.FunnelForQuestionAnswering",anchor:"transformers.FunnelForQuestionAnswering",parameters:[{name:"config",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_16073/src/transformers/models/funnel/modeling_funnel.py#L1493",parametersDescription:[{anchor:"transformers.FunnelForQuestionAnswering.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_16073/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_16073/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),br=new X({props:{name:"forward",anchor:"transformers.FunnelForQuestionAnswering.forward",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:" = None"},{name:"token_type_ids",val:" = None"},{name:"inputs_embeds",val:" = None"},{name:"start_positions",val:" = None"},{name:"end_positions",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"}],source:"https://github.com/huggingface/transformers/blob/pr_16073/src/transformers/models/funnel/modeling_funnel.py#L1504",parametersDescription:[{anchor:"transformers.FunnelForQuestionAnswering.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/pr_16073/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a>. See <a href="/docs/transformers/pr_16073/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/pr_16073/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.FunnelForQuestionAnswering.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.FunnelForQuestionAnswering.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.FunnelForQuestionAnswering.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.FunnelForQuestionAnswering.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FunnelForQuestionAnswering.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FunnelForQuestionAnswering.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_16073/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.FunnelForQuestionAnswering.forward.start_positions",description:`<strong>start_positions</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for position (index) of the start of the labelled span for computing the token classification loss.
Positions are clamped to the length of the sequence (<code>sequence_length</code>). Position outside of the sequence
are not taken into account for computing the loss.`,name:"start_positions"},{anchor:"transformers.FunnelForQuestionAnswering.forward.end_positions",description:`<strong>end_positions</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for position (index) of the end of the labelled span for computing the token classification loss.
Positions are clamped to the length of the sequence (<code>sequence_length</code>). Position outside of the sequence
are not taken into account for computing the loss.`,name:"end_positions"}],returnDescription:`
<p>A <a
  href="/docs/transformers/pr_16073/en/main_classes/output#transformers.modeling_outputs.QuestionAnsweringModelOutput"
>transformers.modeling_outputs.QuestionAnsweringModelOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/pr_16073/en/model_doc/funnel#transformers.FunnelConfig"
>FunnelConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) \u2014 Total span extraction loss is the sum of a Cross-Entropy for the start and end positions.</p>
</li>
<li>
<p><strong>start_logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>) \u2014 Span-start scores (before SoftMax).</p>
</li>
<li>
<p><strong>end_logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>) \u2014 Span-end scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/pr_16073/en/main_classes/output#transformers.modeling_outputs.QuestionAnsweringModelOutput"
>transformers.modeling_outputs.QuestionAnsweringModelOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),co=new ze({props:{$$slots:{default:[Ly]},$$scope:{ctx:W}}}),yr=new Ne({props:{code:`from transformers import FunnelTokenizer, FunnelForQuestionAnswering
import torch

torch.manual_seed(0)
tokenizer = FunnelTokenizer.from_pretrained("funnel-transformer/small")
model = FunnelForQuestionAnswering.from_pretrained("funnel-transformer/small")

question, text = "Who was Jim Henson?", "Jim Henson was a nice puppet"
inputs = tokenizer(question, text, return_tensors="pt")
start_positions = torch.tensor([1])
end_positions = torch.tensor([3])

outputs = model(**inputs, start_positions=start_positions, end_positions=end_positions)
loss = outputs.loss
round(loss.item(), 2)


start_scores = outputs.start_logits
list(start_scores.shape)


end_scores = outputs.end_logits
list(end_scores.shape)
`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FunnelTokenizer, FunnelForQuestionAnswering
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>torch.manual_seed(<span class="hljs-number">0</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = FunnelTokenizer.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FunnelForQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>question, text = <span class="hljs-string">&quot;Who was Jim Henson?&quot;</span>, <span class="hljs-string">&quot;Jim Henson was a nice puppet&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(question, text, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>start_positions = torch.tensor([<span class="hljs-number">1</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>end_positions = torch.tensor([<span class="hljs-number">3</span>])

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs, start_positions=start_positions, end_positions=end_positions)
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = outputs.loss
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">round</span>(loss.item(), <span class="hljs-number">2</span>)


<span class="hljs-meta">&gt;&gt;&gt; </span>start_scores = outputs.start_logits
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">list</span>(start_scores.shape)


<span class="hljs-meta">&gt;&gt;&gt; </span>end_scores = outputs.end_logits
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">list</span>(end_scores.shape)
`}}),$r=new qe({}),Er=new X({props:{name:"class transformers.TFFunnelBaseModel",anchor:"transformers.TFFunnelBaseModel",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_16073/src/transformers/models/funnel/modeling_tf_funnel.py#L1122",parametersDescription:[{anchor:"transformers.TFFunnelBaseModel.config",description:`<strong>config</strong> (<code>XxxConfig</code>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_16073/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),po=new ze({props:{$$slots:{default:[Ay]},$$scope:{ctx:W}}}),jr=new X({props:{name:"call",anchor:"transformers.TFFunnelBaseModel.call",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:" = None"},{name:"token_type_ids",val:" = None"},{name:"inputs_embeds",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"},{name:"training",val:" = False"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_16073/src/transformers/models/funnel/modeling_tf_funnel.py#L1127",parametersDescription:[{anchor:"transformers.TFFunnelBaseModel.call.input_ids",description:`<strong>input_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/pr_16073/en/model_doc/funnel#transformers.FunnelTokenizer">FunnelTokenizer</a>. See <a href="/docs/transformers/pr_16073/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> and
<a href="/docs/transformers/pr_16073/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.TFFunnelBaseModel.call.attention_mask",description:`<strong>attention_mask</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.TFFunnelBaseModel.call.token_type_ids",description:`<strong>token_type_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.TFFunnelBaseModel.call.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.TFFunnelBaseModel.call.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail. This argument can be used only in eager mode, in graph mode the value in the
config will be used instead.`,name:"output_attentions"},{anchor:"transformers.TFFunnelBaseModel.call.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail. This argument can be used only in eager mode, in graph mode the value in the config will be
used instead.`,name:"output_hidden_states"},{anchor:"transformers.TFFunnelBaseModel.call.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_16073/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple. This argument can be used
in eager mode, in graph mode the value will always be set to True.`,name:"return_dict"},{anchor:"transformers.TFFunnelBaseModel.call.training",description:`<strong>training</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to use the model in training mode (some modules like dropout modules have different
behaviors between training and evaluation).`,name:"training"}],returnDescription:`
<p>A <a
  href="/docs/transformers/pr_16073/en/main_classes/output#transformers.modeling_tf_outputs.TFBaseModelOutput"
>transformers.modeling_tf_outputs.TFBaseModelOutput</a> or a tuple of <code>tf.Tensor</code> (if
<code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various elements depending on the
configuration (<a
  href="/docs/transformers/pr_16073/en/model_doc/funnel#transformers.FunnelConfig"
>FunnelConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) \u2014 Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(tf.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/pr_16073/en/main_classes/output#transformers.modeling_tf_outputs.TFBaseModelOutput"
>transformers.modeling_tf_outputs.TFBaseModelOutput</a> or <code>tuple(tf.Tensor)</code></p>
`}}),ho=new ze({props:{$$slots:{default:[Dy]},$$scope:{ctx:W}}}),xr=new Ne({props:{code:`from transformers import FunnelTokenizer, TFFunnelBaseModel
import tensorflow as tf

tokenizer = FunnelTokenizer.from_pretrained("funnel-transformer/small-base")
model = TFFunnelBaseModel.from_pretrained("funnel-transformer/small-base")

inputs = tokenizer("Hello, my dog is cute", return_tensors="tf")
outputs = model(inputs)

last_hidden_states = outputs.last_hidden_state`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FunnelTokenizer, TFFunnelBaseModel
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = FunnelTokenizer.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFFunnelBaseModel.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state`}}),Lr=new qe({}),Ar=new X({props:{name:"class transformers.TFFunnelModel",anchor:"transformers.TFFunnelModel",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_16073/src/transformers/models/funnel/modeling_tf_funnel.py#L1183",parametersDescription:[{anchor:"transformers.TFFunnelModel.config",description:`<strong>config</strong> (<code>XxxConfig</code>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_16073/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),mo=new ze({props:{$$slots:{default:[Iy]},$$scope:{ctx:W}}}),Br=new X({props:{name:"call",anchor:"transformers.TFFunnelModel.call",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:" = None"},{name:"token_type_ids",val:" = None"},{name:"inputs_embeds",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"},{name:"training",val:" = False"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_16073/src/transformers/models/funnel/modeling_tf_funnel.py#L1188",parametersDescription:[{anchor:"transformers.TFFunnelModel.call.input_ids",description:`<strong>input_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/pr_16073/en/model_doc/funnel#transformers.FunnelTokenizer">FunnelTokenizer</a>. See <a href="/docs/transformers/pr_16073/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> and
<a href="/docs/transformers/pr_16073/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.TFFunnelModel.call.attention_mask",description:`<strong>attention_mask</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.TFFunnelModel.call.token_type_ids",description:`<strong>token_type_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.TFFunnelModel.call.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.TFFunnelModel.call.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail. This argument can be used only in eager mode, in graph mode the value in the
config will be used instead.`,name:"output_attentions"},{anchor:"transformers.TFFunnelModel.call.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail. This argument can be used only in eager mode, in graph mode the value in the config will be
used instead.`,name:"output_hidden_states"},{anchor:"transformers.TFFunnelModel.call.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_16073/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple. This argument can be used
in eager mode, in graph mode the value will always be set to True.`,name:"return_dict"},{anchor:"transformers.TFFunnelModel.call.training",description:`<strong>training</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to use the model in training mode (some modules like dropout modules have different
behaviors between training and evaluation).`,name:"training"}],returnDescription:`
<p>A <a
  href="/docs/transformers/pr_16073/en/main_classes/output#transformers.modeling_tf_outputs.TFBaseModelOutput"
>transformers.modeling_tf_outputs.TFBaseModelOutput</a> or a tuple of <code>tf.Tensor</code> (if
<code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various elements depending on the
configuration (<a
  href="/docs/transformers/pr_16073/en/model_doc/funnel#transformers.FunnelConfig"
>FunnelConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) \u2014 Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(tf.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/pr_16073/en/main_classes/output#transformers.modeling_tf_outputs.TFBaseModelOutput"
>transformers.modeling_tf_outputs.TFBaseModelOutput</a> or <code>tuple(tf.Tensor)</code></p>
`}}),go=new ze({props:{$$slots:{default:[Oy]},$$scope:{ctx:W}}}),Wr=new Ne({props:{code:`from transformers import FunnelTokenizer, TFFunnelModel
import tensorflow as tf

tokenizer = FunnelTokenizer.from_pretrained("funnel-transformer/small")
model = TFFunnelModel.from_pretrained("funnel-transformer/small")

inputs = tokenizer("Hello, my dog is cute", return_tensors="tf")
outputs = model(inputs)

last_hidden_states = outputs.last_hidden_state`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FunnelTokenizer, TFFunnelModel
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = FunnelTokenizer.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFFunnelModel.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state`}}),Qr=new qe({}),Rr=new X({props:{name:"class transformers.TFFunnelForPreTraining",anchor:"transformers.TFFunnelForPreTraining",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_16073/src/transformers/models/funnel/modeling_tf_funnel.py#L1246",parametersDescription:[{anchor:"transformers.TFFunnelForPreTraining.config",description:`<strong>config</strong> (<code>XxxConfig</code>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_16073/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),To=new ze({props:{$$slots:{default:[Sy]},$$scope:{ctx:W}}}),Zr=new X({props:{name:"call",anchor:"transformers.TFFunnelForPreTraining.call",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:" = None"},{name:"token_type_ids",val:" = None"},{name:"inputs_embeds",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"},{name:"training",val:" = False"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_16073/src/transformers/models/funnel/modeling_tf_funnel.py#L1253",parametersDescription:[{anchor:"transformers.TFFunnelForPreTraining.call.input_ids",description:`<strong>input_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/pr_16073/en/model_doc/funnel#transformers.FunnelTokenizer">FunnelTokenizer</a>. See <a href="/docs/transformers/pr_16073/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> and
<a href="/docs/transformers/pr_16073/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.TFFunnelForPreTraining.call.attention_mask",description:`<strong>attention_mask</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.TFFunnelForPreTraining.call.token_type_ids",description:`<strong>token_type_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.TFFunnelForPreTraining.call.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.TFFunnelForPreTraining.call.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail. This argument can be used only in eager mode, in graph mode the value in the
config will be used instead.`,name:"output_attentions"},{anchor:"transformers.TFFunnelForPreTraining.call.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail. This argument can be used only in eager mode, in graph mode the value in the config will be
used instead.`,name:"output_hidden_states"},{anchor:"transformers.TFFunnelForPreTraining.call.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_16073/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple. This argument can be used
in eager mode, in graph mode the value will always be set to True.`,name:"return_dict"},{anchor:"transformers.TFFunnelForPreTraining.call.training",description:`<strong>training</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to use the model in training mode (some modules like dropout modules have different
behaviors between training and evaluation).`,name:"training"}],returnDescription:`
<p>A <a
  href="/docs/transformers/pr_16073/en/model_doc/funnel#transformers.models.funnel.modeling_tf_funnel.TFFunnelForPreTrainingOutput"
>transformers.models.funnel.modeling_tf_funnel.TFFunnelForPreTrainingOutput</a> or a tuple of <code>tf.Tensor</code> (if
<code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various elements depending on the
configuration (<a
  href="/docs/transformers/pr_16073/en/model_doc/funnel#transformers.FunnelConfig"
>FunnelConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>logits</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) \u2014 Prediction scores of the head (scores for each token before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/pr_16073/en/model_doc/funnel#transformers.models.funnel.modeling_tf_funnel.TFFunnelForPreTrainingOutput"
>transformers.models.funnel.modeling_tf_funnel.TFFunnelForPreTrainingOutput</a> or <code>tuple(tf.Tensor)</code></p>
`}}),Fo=new ze({props:{$$slots:{default:[Ny]},$$scope:{ctx:W}}}),Kr=new Ne({props:{code:`from transformers import FunnelTokenizer, TFFunnelForPreTraining
import torch

tokenizer = FunnelTokenizer.from_pretrained("funnel-transformer/small")
model = TFFunnelForPreTraining.from_pretrained("funnel-transformer/small")

inputs = tokenizer("Hello, my dog is cute", return_tensors="tf")
logits = model(inputs).logits`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FunnelTokenizer, TFFunnelForPreTraining
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = FunnelTokenizer.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFFunnelForPreTraining.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = model(inputs).logits`}}),Xr=new qe({}),Jr=new X({props:{name:"class transformers.TFFunnelForMaskedLM",anchor:"transformers.TFFunnelForMaskedLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_16073/src/transformers/models/funnel/modeling_tf_funnel.py#L1325",parametersDescription:[{anchor:"transformers.TFFunnelForMaskedLM.config",description:`<strong>config</strong> (<code>XxxConfig</code>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_16073/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),ko=new ze({props:{$$slots:{default:[By]},$$scope:{ctx:W}}}),aa=new X({props:{name:"call",anchor:"transformers.TFFunnelForMaskedLM.call",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:" = None"},{name:"token_type_ids",val:" = None"},{name:"inputs_embeds",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"},{name:"labels",val:" = None"},{name:"training",val:" = False"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_16073/src/transformers/models/funnel/modeling_tf_funnel.py#L1339",parametersDescription:[{anchor:"transformers.TFFunnelForMaskedLM.call.input_ids",description:`<strong>input_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/pr_16073/en/model_doc/funnel#transformers.FunnelTokenizer">FunnelTokenizer</a>. See <a href="/docs/transformers/pr_16073/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> and
<a href="/docs/transformers/pr_16073/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.TFFunnelForMaskedLM.call.attention_mask",description:`<strong>attention_mask</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.TFFunnelForMaskedLM.call.token_type_ids",description:`<strong>token_type_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.TFFunnelForMaskedLM.call.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.TFFunnelForMaskedLM.call.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail. This argument can be used only in eager mode, in graph mode the value in the
config will be used instead.`,name:"output_attentions"},{anchor:"transformers.TFFunnelForMaskedLM.call.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail. This argument can be used only in eager mode, in graph mode the value in the config will be
used instead.`,name:"output_hidden_states"},{anchor:"transformers.TFFunnelForMaskedLM.call.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_16073/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple. This argument can be used
in eager mode, in graph mode the value will always be set to True.`,name:"return_dict"},{anchor:"transformers.TFFunnelForMaskedLM.call.training",description:`<strong>training</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to use the model in training mode (some modules like dropout modules have different
behaviors between training and evaluation).`,name:"training"},{anchor:"transformers.TFFunnelForMaskedLM.call.labels",description:`<strong>labels</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Labels for computing the masked language modeling loss. Indices should be in <code>[-100, 0, ..., config.vocab_size]</code> (see <code>input_ids</code> docstring) Tokens with indices set to <code>-100</code> are ignored (masked), the
loss is only computed for the tokens with labels in <code>[0, ..., config.vocab_size]</code>`,name:"labels"}],returnDescription:`
<p>A <a
  href="/docs/transformers/pr_16073/en/main_classes/output#transformers.modeling_tf_outputs.TFMaskedLMOutput"
>transformers.modeling_tf_outputs.TFMaskedLMOutput</a> or a tuple of <code>tf.Tensor</code> (if
<code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various elements depending on the
configuration (<a
  href="/docs/transformers/pr_16073/en/model_doc/funnel#transformers.FunnelConfig"
>FunnelConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>tf.Tensor</code> of shape <code>(n,)</code>, <em>optional</em>, where n is the number of non-masked labels, returned when <code>labels</code> is provided) \u2014 Masked language modeling (MLM) loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, config.vocab_size)</code>) \u2014 Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/pr_16073/en/main_classes/output#transformers.modeling_tf_outputs.TFMaskedLMOutput"
>transformers.modeling_tf_outputs.TFMaskedLMOutput</a> or <code>tuple(tf.Tensor)</code></p>
`}}),wo=new ze({props:{$$slots:{default:[Wy]},$$scope:{ctx:W}}}),ia=new Ne({props:{code:`from transformers import FunnelTokenizer, TFFunnelForMaskedLM
import tensorflow as tf

tokenizer = FunnelTokenizer.from_pretrained("funnel-transformer/small")
model = TFFunnelForMaskedLM.from_pretrained("funnel-transformer/small")

inputs = tokenizer("The capital of France is [MASK].", return_tensors="tf")
inputs["labels"] = tokenizer("The capital of France is Paris.", return_tensors="tf")["input_ids"]

outputs = model(inputs)
loss = outputs.loss
logits = outputs.logits`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FunnelTokenizer, TFFunnelForMaskedLM
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = FunnelTokenizer.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFFunnelForMaskedLM.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;The capital of France is [MASK].&quot;</span>, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs[<span class="hljs-string">&quot;labels&quot;</span>] = tokenizer(<span class="hljs-string">&quot;The capital of France is Paris.&quot;</span>, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)[<span class="hljs-string">&quot;input_ids&quot;</span>]

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = outputs.loss
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits`}}),la=new qe({}),da=new X({props:{name:"class transformers.TFFunnelForSequenceClassification",anchor:"transformers.TFFunnelForSequenceClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_16073/src/transformers/models/funnel/modeling_tf_funnel.py#L1420",parametersDescription:[{anchor:"transformers.TFFunnelForSequenceClassification.config",description:`<strong>config</strong> (<code>XxxConfig</code>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_16073/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),yo=new ze({props:{$$slots:{default:[Qy]},$$scope:{ctx:W}}}),ma=new X({props:{name:"call",anchor:"transformers.TFFunnelForSequenceClassification.call",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:" = None"},{name:"token_type_ids",val:" = None"},{name:"inputs_embeds",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"},{name:"labels",val:" = None"},{name:"training",val:" = False"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_16073/src/transformers/models/funnel/modeling_tf_funnel.py#L1428",parametersDescription:[{anchor:"transformers.TFFunnelForSequenceClassification.call.input_ids",description:`<strong>input_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/pr_16073/en/model_doc/funnel#transformers.FunnelTokenizer">FunnelTokenizer</a>. See <a href="/docs/transformers/pr_16073/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> and
<a href="/docs/transformers/pr_16073/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.TFFunnelForSequenceClassification.call.attention_mask",description:`<strong>attention_mask</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.TFFunnelForSequenceClassification.call.token_type_ids",description:`<strong>token_type_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.TFFunnelForSequenceClassification.call.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.TFFunnelForSequenceClassification.call.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail. This argument can be used only in eager mode, in graph mode the value in the
config will be used instead.`,name:"output_attentions"},{anchor:"transformers.TFFunnelForSequenceClassification.call.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail. This argument can be used only in eager mode, in graph mode the value in the config will be
used instead.`,name:"output_hidden_states"},{anchor:"transformers.TFFunnelForSequenceClassification.call.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_16073/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple. This argument can be used
in eager mode, in graph mode the value will always be set to True.`,name:"return_dict"},{anchor:"transformers.TFFunnelForSequenceClassification.call.training",description:`<strong>training</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to use the model in training mode (some modules like dropout modules have different
behaviors between training and evaluation).`,name:"training"},{anchor:"transformers.TFFunnelForSequenceClassification.call.labels",description:`<strong>labels</strong> (<code>tf.Tensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for computing the sequence classification/regression loss. Indices should be in <code>[0, ..., config.num_labels - 1]</code>. If <code>config.num_labels == 1</code> a regression loss is computed (Mean-Square loss), If
<code>config.num_labels &gt; 1</code> a classification loss is computed (Cross-Entropy).`,name:"labels"}],returnDescription:`
<p>A <a
  href="/docs/transformers/pr_16073/en/main_classes/output#transformers.modeling_tf_outputs.TFSequenceClassifierOutput"
>transformers.modeling_tf_outputs.TFSequenceClassifierOutput</a> or a tuple of <code>tf.Tensor</code> (if
<code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various elements depending on the
configuration (<a
  href="/docs/transformers/pr_16073/en/model_doc/funnel#transformers.FunnelConfig"
>FunnelConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, )</code>, <em>optional</em>, returned when <code>labels</code> is provided) \u2014 Classification (or regression if config.num_labels==1) loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, config.num_labels)</code>) \u2014 Classification (or regression if config.num_labels==1) scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/pr_16073/en/main_classes/output#transformers.modeling_tf_outputs.TFSequenceClassifierOutput"
>transformers.modeling_tf_outputs.TFSequenceClassifierOutput</a> or <code>tuple(tf.Tensor)</code></p>
`}}),$o=new ze({props:{$$slots:{default:[Ry]},$$scope:{ctx:W}}}),ga=new Ne({props:{code:`from transformers import FunnelTokenizer, TFFunnelForSequenceClassification
import tensorflow as tf

tokenizer = FunnelTokenizer.from_pretrained("funnel-transformer/small-base")
model = TFFunnelForSequenceClassification.from_pretrained("funnel-transformer/small-base")

inputs = tokenizer("Hello, my dog is cute", return_tensors="tf")
inputs["labels"] = tf.reshape(tf.constant(1), (-1, 1))  # Batch size 1

outputs = model(inputs)
loss = outputs.loss
logits = outputs.logits`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FunnelTokenizer, TFFunnelForSequenceClassification
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = FunnelTokenizer.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFFunnelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs[<span class="hljs-string">&quot;labels&quot;</span>] = tf.reshape(tf.constant(<span class="hljs-number">1</span>), (-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>))  <span class="hljs-comment"># Batch size 1</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = outputs.loss
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits`}}),_a=new qe({}),Ta=new X({props:{name:"class transformers.TFFunnelForMultipleChoice",anchor:"transformers.TFFunnelForMultipleChoice",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_16073/src/transformers/models/funnel/modeling_tf_funnel.py#L1510",parametersDescription:[{anchor:"transformers.TFFunnelForMultipleChoice.config",description:`<strong>config</strong> (<code>XxxConfig</code>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_16073/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),Mo=new ze({props:{$$slots:{default:[Hy]},$$scope:{ctx:W}}}),ya=new X({props:{name:"call",anchor:"transformers.TFFunnelForMultipleChoice.call",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:" = None"},{name:"token_type_ids",val:" = None"},{name:"inputs_embeds",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"},{name:"labels",val:" = None"},{name:"training",val:" = False"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_16073/src/transformers/models/funnel/modeling_tf_funnel.py#L1527",parametersDescription:[{anchor:"transformers.TFFunnelForMultipleChoice.call.input_ids",description:`<strong>input_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, num_choices, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/pr_16073/en/model_doc/funnel#transformers.FunnelTokenizer">FunnelTokenizer</a>. See <a href="/docs/transformers/pr_16073/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> and
<a href="/docs/transformers/pr_16073/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.TFFunnelForMultipleChoice.call.attention_mask",description:`<strong>attention_mask</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, num_choices, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.TFFunnelForMultipleChoice.call.token_type_ids",description:`<strong>token_type_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, num_choices, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.TFFunnelForMultipleChoice.call.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, num_choices, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.TFFunnelForMultipleChoice.call.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail. This argument can be used only in eager mode, in graph mode the value in the
config will be used instead.`,name:"output_attentions"},{anchor:"transformers.TFFunnelForMultipleChoice.call.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail. This argument can be used only in eager mode, in graph mode the value in the config will be
used instead.`,name:"output_hidden_states"},{anchor:"transformers.TFFunnelForMultipleChoice.call.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_16073/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple. This argument can be used
in eager mode, in graph mode the value will always be set to True.`,name:"return_dict"},{anchor:"transformers.TFFunnelForMultipleChoice.call.training",description:`<strong>training</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to use the model in training mode (some modules like dropout modules have different
behaviors between training and evaluation).`,name:"training"},{anchor:"transformers.TFFunnelForMultipleChoice.call.labels",description:`<strong>labels</strong> (<code>tf.Tensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for computing the multiple choice classification loss. Indices should be in <code>[0, ..., num_choices]</code>
where <code>num_choices</code> is the size of the second dimension of the input tensors. (See <code>input_ids</code> above)`,name:"labels"}],returnDescription:`
<p>A <a
  href="/docs/transformers/pr_16073/en/main_classes/output#transformers.modeling_tf_outputs.TFMultipleChoiceModelOutput"
>transformers.modeling_tf_outputs.TFMultipleChoiceModelOutput</a> or a tuple of <code>tf.Tensor</code> (if
<code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various elements depending on the
configuration (<a
  href="/docs/transformers/pr_16073/en/model_doc/funnel#transformers.FunnelConfig"
>FunnelConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>tf.Tensor</code> of shape <em>(batch_size, )</em>, <em>optional</em>, returned when <code>labels</code> is provided) \u2014 Classification loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, num_choices)</code>) \u2014 <em>num_choices</em> is the second dimension of the input tensors. (see <em>input_ids</em> above).</p>
<p>Classification scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/pr_16073/en/main_classes/output#transformers.modeling_tf_outputs.TFMultipleChoiceModelOutput"
>transformers.modeling_tf_outputs.TFMultipleChoiceModelOutput</a> or <code>tuple(tf.Tensor)</code></p>
`}}),zo=new ze({props:{$$slots:{default:[Vy]},$$scope:{ctx:W}}}),$a=new Ne({props:{code:`from transformers import FunnelTokenizer, TFFunnelForMultipleChoice
import tensorflow as tf

tokenizer = FunnelTokenizer.from_pretrained("funnel-transformer/small-base")
model = TFFunnelForMultipleChoice.from_pretrained("funnel-transformer/small-base")

prompt = "In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced."
choice0 = "It is eaten with a fork and a knife."
choice1 = "It is eaten while held in the hand."

encoding = tokenizer([prompt, prompt], [choice0, choice1], return_tensors="tf", padding=True)
inputs = {k: tf.expand_dims(v, 0) for k, v in encoding.items()}
outputs = model(inputs)  # batch size is 1

# the linear classifier still needs to be trained
logits = outputs.logits`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FunnelTokenizer, TFFunnelForMultipleChoice
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = FunnelTokenizer.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFFunnelForMultipleChoice.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>prompt = <span class="hljs-string">&quot;In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>choice0 = <span class="hljs-string">&quot;It is eaten with a fork and a knife.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>choice1 = <span class="hljs-string">&quot;It is eaten while held in the hand.&quot;</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>encoding = tokenizer([prompt, prompt], [choice0, choice1], return_tensors=<span class="hljs-string">&quot;tf&quot;</span>, padding=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = {k: tf.expand_dims(v, <span class="hljs-number">0</span>) <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> encoding.items()}
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(inputs)  <span class="hljs-comment"># batch size is 1</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># the linear classifier still needs to be trained</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits`}}),Ea=new qe({}),Ma=new X({props:{name:"class transformers.TFFunnelForTokenClassification",anchor:"transformers.TFFunnelForTokenClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_16073/src/transformers/models/funnel/modeling_tf_funnel.py#L1645",parametersDescription:[{anchor:"transformers.TFFunnelForTokenClassification.config",description:`<strong>config</strong> (<code>XxxConfig</code>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_16073/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),Po=new ze({props:{$$slots:{default:[Yy]},$$scope:{ctx:W}}}),xa=new X({props:{name:"call",anchor:"transformers.TFFunnelForTokenClassification.call",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:" = None"},{name:"token_type_ids",val:" = None"},{name:"inputs_embeds",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"},{name:"labels",val:" = None"},{name:"training",val:" = False"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_16073/src/transformers/models/funnel/modeling_tf_funnel.py#L1656",parametersDescription:[{anchor:"transformers.TFFunnelForTokenClassification.call.input_ids",description:`<strong>input_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/pr_16073/en/model_doc/funnel#transformers.FunnelTokenizer">FunnelTokenizer</a>. See <a href="/docs/transformers/pr_16073/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> and
<a href="/docs/transformers/pr_16073/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.TFFunnelForTokenClassification.call.attention_mask",description:`<strong>attention_mask</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.TFFunnelForTokenClassification.call.token_type_ids",description:`<strong>token_type_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.TFFunnelForTokenClassification.call.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.TFFunnelForTokenClassification.call.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail. This argument can be used only in eager mode, in graph mode the value in the
config will be used instead.`,name:"output_attentions"},{anchor:"transformers.TFFunnelForTokenClassification.call.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail. This argument can be used only in eager mode, in graph mode the value in the config will be
used instead.`,name:"output_hidden_states"},{anchor:"transformers.TFFunnelForTokenClassification.call.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_16073/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple. This argument can be used
in eager mode, in graph mode the value will always be set to True.`,name:"return_dict"},{anchor:"transformers.TFFunnelForTokenClassification.call.training",description:`<strong>training</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to use the model in training mode (some modules like dropout modules have different
behaviors between training and evaluation).`,name:"training"},{anchor:"transformers.TFFunnelForTokenClassification.call.labels",description:`<strong>labels</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Labels for computing the token classification loss. Indices should be in <code>[0, ..., config.num_labels - 1]</code>.`,name:"labels"}],returnDescription:`
<p>A <a
  href="/docs/transformers/pr_16073/en/main_classes/output#transformers.modeling_tf_outputs.TFTokenClassifierOutput"
>transformers.modeling_tf_outputs.TFTokenClassifierOutput</a> or a tuple of <code>tf.Tensor</code> (if
<code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various elements depending on the
configuration (<a
  href="/docs/transformers/pr_16073/en/model_doc/funnel#transformers.FunnelConfig"
>FunnelConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>tf.Tensor</code> of shape <code>(n,)</code>, <em>optional</em>, where n is the number of unmasked labels, returned when <code>labels</code> is provided)  \u2014 Classification loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, config.num_labels)</code>) \u2014 Classification scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/pr_16073/en/main_classes/output#transformers.modeling_tf_outputs.TFTokenClassifierOutput"
>transformers.modeling_tf_outputs.TFTokenClassifierOutput</a> or <code>tuple(tf.Tensor)</code></p>
`}}),Co=new ze({props:{$$slots:{default:[Uy]},$$scope:{ctx:W}}}),La=new Ne({props:{code:`from transformers import FunnelTokenizer, TFFunnelForTokenClassification
import tensorflow as tf

tokenizer = FunnelTokenizer.from_pretrained("funnel-transformer/small")
model = TFFunnelForTokenClassification.from_pretrained("funnel-transformer/small")

inputs = tokenizer("Hello, my dog is cute", return_tensors="tf")
input_ids = inputs["input_ids"]
inputs["labels"] = tf.reshape(
    tf.constant([1] * tf.size(input_ids).numpy()), (-1, tf.size(input_ids))
)  # Batch size 1

outputs = model(inputs)
loss = outputs.loss
logits = outputs.logits`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FunnelTokenizer, TFFunnelForTokenClassification
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = FunnelTokenizer.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFFunnelForTokenClassification.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = inputs[<span class="hljs-string">&quot;input_ids&quot;</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs[<span class="hljs-string">&quot;labels&quot;</span>] = tf.reshape(
<span class="hljs-meta">... </span>    tf.constant([<span class="hljs-number">1</span>] * tf.size(input_ids).numpy()), (-<span class="hljs-number">1</span>, tf.size(input_ids))
<span class="hljs-meta">&gt;&gt;&gt; </span>)  <span class="hljs-comment"># Batch size 1</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = outputs.loss
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits`}}),Aa=new qe({}),Da=new X({props:{name:"class transformers.TFFunnelForQuestionAnswering",anchor:"transformers.TFFunnelForQuestionAnswering",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_16073/src/transformers/models/funnel/modeling_tf_funnel.py#L1737",parametersDescription:[{anchor:"transformers.TFFunnelForQuestionAnswering.config",description:`<strong>config</strong> (<code>XxxConfig</code>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_16073/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),xo=new ze({props:{$$slots:{default:[Gy]},$$scope:{ctx:W}}}),Wa=new X({props:{name:"call",anchor:"transformers.TFFunnelForQuestionAnswering.call",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:" = None"},{name:"token_type_ids",val:" = None"},{name:"inputs_embeds",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"},{name:"start_positions",val:" = None"},{name:"end_positions",val:" = None"},{name:"training",val:" = False"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_16073/src/transformers/models/funnel/modeling_tf_funnel.py#L1747",parametersDescription:[{anchor:"transformers.TFFunnelForQuestionAnswering.call.input_ids",description:`<strong>input_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/pr_16073/en/model_doc/funnel#transformers.FunnelTokenizer">FunnelTokenizer</a>. See <a href="/docs/transformers/pr_16073/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> and
<a href="/docs/transformers/pr_16073/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.TFFunnelForQuestionAnswering.call.attention_mask",description:`<strong>attention_mask</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.TFFunnelForQuestionAnswering.call.token_type_ids",description:`<strong>token_type_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.TFFunnelForQuestionAnswering.call.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.TFFunnelForQuestionAnswering.call.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail. This argument can be used only in eager mode, in graph mode the value in the
config will be used instead.`,name:"output_attentions"},{anchor:"transformers.TFFunnelForQuestionAnswering.call.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail. This argument can be used only in eager mode, in graph mode the value in the config will be
used instead.`,name:"output_hidden_states"},{anchor:"transformers.TFFunnelForQuestionAnswering.call.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_16073/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple. This argument can be used
in eager mode, in graph mode the value will always be set to True.`,name:"return_dict"},{anchor:"transformers.TFFunnelForQuestionAnswering.call.training",description:`<strong>training</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to use the model in training mode (some modules like dropout modules have different
behaviors between training and evaluation).`,name:"training"},{anchor:"transformers.TFFunnelForQuestionAnswering.call.start_positions",description:`<strong>start_positions</strong> (<code>tf.Tensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for position (index) of the start of the labelled span for computing the token classification loss.
Positions are clamped to the length of the sequence (<code>sequence_length</code>). Position outside of the sequence
are not taken into account for computing the loss.`,name:"start_positions"},{anchor:"transformers.TFFunnelForQuestionAnswering.call.end_positions",description:`<strong>end_positions</strong> (<code>tf.Tensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for position (index) of the end of the labelled span for computing the token classification loss.
Positions are clamped to the length of the sequence (<code>sequence_length</code>). Position outside of the sequence
are not taken into account for computing the loss.`,name:"end_positions"}],returnDescription:`
<p>A <a
  href="/docs/transformers/pr_16073/en/main_classes/output#transformers.modeling_tf_outputs.TFQuestionAnsweringModelOutput"
>transformers.modeling_tf_outputs.TFQuestionAnsweringModelOutput</a> or a tuple of <code>tf.Tensor</code> (if
<code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various elements depending on the
configuration (<a
  href="/docs/transformers/pr_16073/en/model_doc/funnel#transformers.FunnelConfig"
>FunnelConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, )</code>, <em>optional</em>, returned when <code>start_positions</code> and <code>end_positions</code> are provided) \u2014 Total span extraction loss is the sum of a Cross-Entropy for the start and end positions.</p>
</li>
<li>
<p><strong>start_logits</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) \u2014 Span-start scores (before SoftMax).</p>
</li>
<li>
<p><strong>end_logits</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) \u2014 Span-end scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/pr_16073/en/main_classes/output#transformers.modeling_tf_outputs.TFQuestionAnsweringModelOutput"
>transformers.modeling_tf_outputs.TFQuestionAnsweringModelOutput</a> or <code>tuple(tf.Tensor)</code></p>
`}}),Lo=new ze({props:{$$slots:{default:[Zy]},$$scope:{ctx:W}}}),Qa=new Ne({props:{code:`from transformers import FunnelTokenizer, TFFunnelForQuestionAnswering
import tensorflow as tf

tokenizer = FunnelTokenizer.from_pretrained("funnel-transformer/small")
model = TFFunnelForQuestionAnswering.from_pretrained("funnel-transformer/small")

question, text = "Who was Jim Henson?", "Jim Henson was a nice puppet"
input_dict = tokenizer(question, text, return_tensors="tf")
outputs = model(input_dict)
start_logits = outputs.start_logits
end_logits = outputs.end_logits

all_tokens = tokenizer.convert_ids_to_tokens(input_dict["input_ids"].numpy()[0])
answer = " ".join(all_tokens[tf.math.argmax(start_logits, 1)[0] : tf.math.argmax(end_logits, 1)[0] + 1])`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FunnelTokenizer, TFFunnelForQuestionAnswering
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = FunnelTokenizer.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFFunnelForQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>question, text = <span class="hljs-string">&quot;Who was Jim Henson?&quot;</span>, <span class="hljs-string">&quot;Jim Henson was a nice puppet&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>input_dict = tokenizer(question, text, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(input_dict)
<span class="hljs-meta">&gt;&gt;&gt; </span>start_logits = outputs.start_logits
<span class="hljs-meta">&gt;&gt;&gt; </span>end_logits = outputs.end_logits

<span class="hljs-meta">&gt;&gt;&gt; </span>all_tokens = tokenizer.convert_ids_to_tokens(input_dict[<span class="hljs-string">&quot;input_ids&quot;</span>].numpy()[<span class="hljs-number">0</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>answer = <span class="hljs-string">&quot; &quot;</span>.join(all_tokens[tf.math.argmax(start_logits, <span class="hljs-number">1</span>)[<span class="hljs-number">0</span>] : tf.math.argmax(end_logits, <span class="hljs-number">1</span>)[<span class="hljs-number">0</span>] + <span class="hljs-number">1</span>])`}}),{c(){p=r("meta"),M=l(),m=r("h1"),g=r("a"),F=r("span"),k(T.$$.fragment),_=l(),z=r("span"),ce=t("Funnel Transformer"),G=l(),q=r("h2"),J=r("a"),I=r("span"),k(ne.$$.fragment),ue=l(),O=r("span"),pe=t("Overview"),ie=l(),U=r("p"),L=t("The Funnel Transformer model was proposed in the paper "),te=r("a"),Z=t(`Funnel-Transformer: Filtering out Sequential Redundancy for
Efficient Language Processing`),P=t(`. It is a bidirectional transformer model, like
BERT, but with a pooling operation after each block of layers, a bit like in traditional convolutional neural networks
(CNN) in computer vision.`),j=l(),oe=r("p"),Q=t("The abstract from the paper is the following:"),le=l(),se=r("p"),S=r("em"),he=t(`With the success of language pretraining, it is highly desirable to develop more efficient architectures of good
scalability that can exploit the abundant unlabeled data at a lower cost. To improve the efficiency, we examine the
much-overlooked redundancy in maintaining a full-length token-level presentation, especially for tasks that only
require a single-vector presentation of the sequence. With this intuition, we propose Funnel-Transformer which
gradually compresses the sequence of hidden states to a shorter one and hence reduces the computation cost. More
importantly, by re-investing the saved FLOPs from length reduction in constructing a deeper or wider model, we further
improve the model capacity. In addition, to perform token-level predictions as required by common pretraining
objectives, Funnel-Transformer is able to recover a deep representation for each token from the reduced hidden sequence
via a decoder. Empirically, with comparable or fewer FLOPs, Funnel-Transformer outperforms the standard Transformer on
a wide variety of sequence-level prediction tasks, including text classification, language understanding, and reading
comprehension.`),de=l(),C=r("p"),fe=t("Tips:"),B=l(),ee=r("ul"),ae=r("li"),R=t(`Since Funnel Transformer uses pooling, the sequence length of the hidden states changes after each block of layers.
The base model therefore has a final sequence length that is a quarter of the original one. This model can be used
directly for tasks that just require a sentence summary (like sequence classification or multiple choice). For other
tasks, the full model is used; this full model has a decoder that upsamples the final hidden states to the same
sequence length as the input.`),me=l(),N=r("li"),A=t(`The Funnel Transformer checkpoints are all available with a full version and a base version. The first ones should be
used for `),re=r("a"),H=t("FunnelModel"),ge=t(", "),u=r("a"),v=t("FunnelForPreTraining"),K=t(`,
`),Te=r("a"),we=t("FunnelForMaskedLM"),D=t(", "),Fe=r("a"),be=t("FunnelForTokenClassification"),ye=t(` and
class:`),x=r("em"),V=t("~transformers.FunnelForQuestionAnswering"),$e=t(`. The second ones should be used for
`),ve=r("a"),Y=t("FunnelBaseModel"),Ee=t(", "),ke=r("a"),_e=t("FunnelForSequenceClassification"),Me=t(` and
`),Ua=r("a"),Sp=t("FunnelForMultipleChoice"),Np=t("."),Ec=l(),xn=r("p"),Bp=t("This model was contributed by "),Oo=r("a"),Wp=t("sgugger"),Qp=t(". The original code can be found "),So=r("a"),Rp=t("here"),Hp=t("."),Mc=l(),Kn=r("h2"),Bt=r("a"),ul=r("span"),k(No.$$.fragment),Vp=l(),pl=r("span"),Yp=t("FunnelConfig"),zc=l(),Cn=r("div"),k(Bo.$$.fragment),Up=l(),jn=r("p"),Gp=t("This is the configuration class to store the configuration of a "),Ga=r("a"),Zp=t("FunnelModel"),Kp=t(" or a "),Za=r("a"),Xp=t("TFBertModel"),Jp=t(`. It is used to
instantiate a Funnel Transformer model according to the specified arguments, defining the model architecture.
Instantiating a configuration with the defaults will yield a similar configuration to that of the Funnel
Transformer `),Wo=r("a"),eh=t("funnel-transformer/small"),nh=t(" architecture."),th=l(),Xn=r("p"),oh=t("Configuration objects inherit from "),Ka=r("a"),sh=t("PretrainedConfig"),rh=t(` and can be used to control the model outputs. Read the
documentation from `),Xa=r("a"),ah=t("PretrainedConfig"),ih=t(" for more information."),qc=l(),Jn=r("h2"),Wt=r("a"),hl=r("span"),k(Qo.$$.fragment),lh=l(),fl=r("span"),dh=t("FunnelTokenizer"),Pc=l(),Pe=r("div"),k(Ro.$$.fragment),ch=l(),ml=r("p"),uh=t("Construct a Funnel Transformer tokenizer."),ph=l(),Qt=r("p"),Ja=r("a"),hh=t("FunnelTokenizer"),fh=t(" is identical to "),ei=r("a"),mh=t("BertTokenizer"),gh=t(` and runs end-to-end tokenization: punctuation splitting and
wordpiece.`),_h=l(),Ho=r("p"),Th=t("Refer to superclass "),ni=r("a"),Fh=t("BertTokenizer"),vh=t(" for usage examples and documentation concerning parameters."),kh=l(),Ln=r("div"),k(Vo.$$.fragment),wh=l(),gl=r("p"),bh=t(`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. A BERT sequence has the following format:`),yh=l(),Yo=r("ul"),ti=r("li"),$h=t("single sequence: "),_l=r("code"),Eh=t("[CLS] X [SEP]"),Mh=l(),oi=r("li"),zh=t("pair of sequences: "),Tl=r("code"),qh=t("[CLS] A [SEP] B [SEP]"),Ph=l(),Rt=r("div"),k(Uo.$$.fragment),Ch=l(),Go=r("p"),jh=t(`Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding
special tokens using the tokenizer `),Fl=r("code"),xh=t("prepare_for_model"),Lh=t(" method."),Ah=l(),wn=r("div"),k(Zo.$$.fragment),Dh=l(),vl=r("p"),Ih=t(`Create a mask from the two sequences passed to be used in a sequence-pair classification task. A Funnel
Transformer sequence pair mask has the following format:`),Oh=l(),k(Ko.$$.fragment),Sh=l(),et=r("p"),Nh=t("If "),kl=r("code"),Bh=t("token_ids_1"),Wh=t(" is "),wl=r("code"),Qh=t("None"),Rh=t(", this method only returns the first portion of the mask (0s)."),Hh=l(),si=r("div"),k(Xo.$$.fragment),Cc=l(),nt=r("h2"),Ht=r("a"),bl=r("span"),k(Jo.$$.fragment),Vh=l(),yl=r("span"),Yh=t("FunnelTokenizerFast"),jc=l(),Ze=r("div"),k(es.$$.fragment),Uh=l(),ns=r("p"),Gh=t("Construct a \u201Cfast\u201D Funnel Transformer tokenizer (backed by HuggingFace\u2019s "),$l=r("em"),Zh=t("tokenizers"),Kh=t(" library)."),Xh=l(),Vt=r("p"),ri=r("a"),Jh=t("FunnelTokenizerFast"),ef=t(" is identical to "),ai=r("a"),nf=t("BertTokenizerFast"),tf=t(` and runs end-to-end tokenization: punctuation
splitting and wordpiece.`),of=l(),ts=r("p"),sf=t("Refer to superclass "),ii=r("a"),rf=t("BertTokenizerFast"),af=t(" for usage examples and documentation concerning parameters."),lf=l(),bn=r("div"),k(os.$$.fragment),df=l(),El=r("p"),cf=t(`Create a mask from the two sequences passed to be used in a sequence-pair classification task. A Funnel
Transformer sequence pair mask has the following format:`),uf=l(),k(ss.$$.fragment),pf=l(),tt=r("p"),hf=t("If "),Ml=r("code"),ff=t("token_ids_1"),mf=t(" is "),zl=r("code"),gf=t("None"),_f=t(", this method only returns the first portion of the mask (0s)."),xc=l(),ot=r("h2"),Yt=r("a"),ql=r("span"),k(rs.$$.fragment),Tf=l(),Pl=r("span"),Ff=t("Funnel specific outputs"),Lc=l(),st=r("div"),k(as.$$.fragment),vf=l(),is=r("p"),kf=t("Output type of "),li=r("a"),wf=t("FunnelForPreTraining"),bf=t("."),Ac=l(),rt=r("div"),k(ls.$$.fragment),yf=l(),ds=r("p"),$f=t("Output type of "),di=r("a"),Ef=t("FunnelForPreTraining"),Mf=t("."),Dc=l(),at=r("h2"),Ut=r("a"),Cl=r("span"),k(cs.$$.fragment),zf=l(),jl=r("span"),qf=t("FunnelBaseModel"),Ic=l(),We=r("div"),k(us.$$.fragment),Pf=l(),xl=r("p"),Cf=t(`The base Funnel Transformer Model transformer outputting raw hidden-states without upsampling head (also called
decoder) or any task-specific head on top.`),jf=l(),ps=r("p"),xf=t("The Funnel Transformer model was proposed in "),hs=r("a"),Lf=t(`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),Af=t(" by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),Df=l(),fs=r("p"),If=t("This model inherits from "),ci=r("a"),Of=t("PreTrainedModel"),Sf=t(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Nf=l(),ms=r("p"),Bf=t("This model is also a PyTorch "),gs=r("a"),Wf=t("torch.nn.Module"),Qf=t(` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),Rf=l(),Ke=r("div"),k(_s.$$.fragment),Hf=l(),it=r("p"),Vf=t("The "),ui=r("a"),Yf=t("FunnelBaseModel"),Uf=t(" forward method, overrides the "),Ll=r("code"),Gf=t("__call__"),Zf=t(" special method."),Kf=l(),k(Gt.$$.fragment),Xf=l(),Al=r("p"),Jf=t("Example:"),em=l(),k(Ts.$$.fragment),Oc=l(),lt=r("h2"),Zt=r("a"),Dl=r("span"),k(Fs.$$.fragment),nm=l(),Il=r("span"),tm=t("FunnelModel"),Sc=l(),Qe=r("div"),k(vs.$$.fragment),om=l(),Ol=r("p"),sm=t("The bare Funnel Transformer Model transformer outputting raw hidden-states without any specific head on top."),rm=l(),ks=r("p"),am=t("The Funnel Transformer model was proposed in "),ws=r("a"),im=t(`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),lm=t(" by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),dm=l(),bs=r("p"),cm=t("This model inherits from "),pi=r("a"),um=t("PreTrainedModel"),pm=t(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),hm=l(),ys=r("p"),fm=t("This model is also a PyTorch "),$s=r("a"),mm=t("torch.nn.Module"),gm=t(` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),_m=l(),Xe=r("div"),k(Es.$$.fragment),Tm=l(),dt=r("p"),Fm=t("The "),hi=r("a"),vm=t("FunnelModel"),km=t(" forward method, overrides the "),Sl=r("code"),wm=t("__call__"),bm=t(" special method."),ym=l(),k(Kt.$$.fragment),$m=l(),Nl=r("p"),Em=t("Example:"),Mm=l(),k(Ms.$$.fragment),Nc=l(),ct=r("h2"),Xt=r("a"),Bl=r("span"),k(zs.$$.fragment),zm=l(),Wl=r("span"),qm=t("FunnelModelForPreTraining"),Bc=l(),ut=r("div"),k(qs.$$.fragment),Pm=l(),Je=r("div"),k(Ps.$$.fragment),Cm=l(),pt=r("p"),jm=t("The "),fi=r("a"),xm=t("FunnelForPreTraining"),Lm=t(" forward method, overrides the "),Ql=r("code"),Am=t("__call__"),Dm=t(" special method."),Im=l(),k(Jt.$$.fragment),Om=l(),Rl=r("p"),Sm=t("Examples:"),Nm=l(),k(Cs.$$.fragment),Wc=l(),ht=r("h2"),eo=r("a"),Hl=r("span"),k(js.$$.fragment),Bm=l(),Vl=r("span"),Wm=t("FunnelForMaskedLM"),Qc=l(),Re=r("div"),k(xs.$$.fragment),Qm=l(),Ls=r("p"),Rm=t("Funnel Transformer Model with a "),Yl=r("code"),Hm=t("language modeling"),Vm=t(" head on top."),Ym=l(),As=r("p"),Um=t("The Funnel Transformer model was proposed in "),Ds=r("a"),Gm=t(`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),Zm=t(" by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),Km=l(),Is=r("p"),Xm=t("This model inherits from "),mi=r("a"),Jm=t("PreTrainedModel"),eg=t(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),ng=l(),Os=r("p"),tg=t("This model is also a PyTorch "),Ss=r("a"),og=t("torch.nn.Module"),sg=t(` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),rg=l(),en=r("div"),k(Ns.$$.fragment),ag=l(),ft=r("p"),ig=t("The "),gi=r("a"),lg=t("FunnelForMaskedLM"),dg=t(" forward method, overrides the "),Ul=r("code"),cg=t("__call__"),ug=t(" special method."),pg=l(),k(no.$$.fragment),hg=l(),Gl=r("p"),fg=t("Example:"),mg=l(),k(Bs.$$.fragment),Rc=l(),mt=r("h2"),to=r("a"),Zl=r("span"),k(Ws.$$.fragment),gg=l(),Kl=r("span"),_g=t("FunnelForSequenceClassification"),Hc=l(),He=r("div"),k(Qs.$$.fragment),Tg=l(),Xl=r("p"),Fg=t(`Funnel Transformer Model with a sequence classification/regression head on top (two linear layer on top of the
first timestep of the last hidden state) e.g. for GLUE tasks.`),vg=l(),Rs=r("p"),kg=t("The Funnel Transformer model was proposed in "),Hs=r("a"),wg=t(`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),bg=t(" by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),yg=l(),Vs=r("p"),$g=t("This model inherits from "),_i=r("a"),Eg=t("PreTrainedModel"),Mg=t(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),zg=l(),Ys=r("p"),qg=t("This model is also a PyTorch "),Us=r("a"),Pg=t("torch.nn.Module"),Cg=t(` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),jg=l(),Be=r("div"),k(Gs.$$.fragment),xg=l(),gt=r("p"),Lg=t("The "),Ti=r("a"),Ag=t("FunnelForSequenceClassification"),Dg=t(" forward method, overrides the "),Jl=r("code"),Ig=t("__call__"),Og=t(" special method."),Sg=l(),k(oo.$$.fragment),Ng=l(),ed=r("p"),Bg=t("Example of single-label classification:"),Wg=l(),k(Zs.$$.fragment),Qg=l(),nd=r("p"),Rg=t("Example of multi-label classification:"),Hg=l(),k(Ks.$$.fragment),Vc=l(),_t=r("h2"),so=r("a"),td=r("span"),k(Xs.$$.fragment),Vg=l(),od=r("span"),Yg=t("FunnelForMultipleChoice"),Yc=l(),Ve=r("div"),k(Js.$$.fragment),Ug=l(),sd=r("p"),Gg=t(`Funnel Transformer Model with a multiple choice classification head on top (two linear layer on top of the first
timestep of the last hidden state, and a softmax) e.g. for RocStories/SWAG tasks.`),Zg=l(),er=r("p"),Kg=t("The Funnel Transformer model was proposed in "),nr=r("a"),Xg=t(`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),Jg=t(" by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),e_=l(),tr=r("p"),n_=t("This model inherits from "),Fi=r("a"),t_=t("PreTrainedModel"),o_=t(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),s_=l(),or=r("p"),r_=t("This model is also a PyTorch "),sr=r("a"),a_=t("torch.nn.Module"),i_=t(` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),l_=l(),nn=r("div"),k(rr.$$.fragment),d_=l(),Tt=r("p"),c_=t("The "),vi=r("a"),u_=t("FunnelForMultipleChoice"),p_=t(" forward method, overrides the "),rd=r("code"),h_=t("__call__"),f_=t(" special method."),m_=l(),k(ro.$$.fragment),g_=l(),ad=r("p"),__=t("Example:"),T_=l(),k(ar.$$.fragment),Uc=l(),Ft=r("h2"),ao=r("a"),id=r("span"),k(ir.$$.fragment),F_=l(),ld=r("span"),v_=t("FunnelForTokenClassification"),Gc=l(),Ye=r("div"),k(lr.$$.fragment),k_=l(),dd=r("p"),w_=t(`Funnel Transformer Model with a token classification head on top (a linear layer on top of the hidden-states
output) e.g. for Named-Entity-Recognition (NER) tasks.`),b_=l(),dr=r("p"),y_=t("The Funnel Transformer model was proposed in "),cr=r("a"),$_=t(`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),E_=t(" by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),M_=l(),ur=r("p"),z_=t("This model inherits from "),ki=r("a"),q_=t("PreTrainedModel"),P_=t(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),C_=l(),pr=r("p"),j_=t("This model is also a PyTorch "),hr=r("a"),x_=t("torch.nn.Module"),L_=t(` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),A_=l(),tn=r("div"),k(fr.$$.fragment),D_=l(),vt=r("p"),I_=t("The "),wi=r("a"),O_=t("FunnelForTokenClassification"),S_=t(" forward method, overrides the "),cd=r("code"),N_=t("__call__"),B_=t(" special method."),W_=l(),k(io.$$.fragment),Q_=l(),ud=r("p"),R_=t("Example:"),H_=l(),k(mr.$$.fragment),Zc=l(),kt=r("h2"),lo=r("a"),pd=r("span"),k(gr.$$.fragment),V_=l(),hd=r("span"),Y_=t("FunnelForQuestionAnswering"),Kc=l(),Ue=r("div"),k(_r.$$.fragment),U_=l(),wt=r("p"),G_=t(`Funnel Transformer Model with a span classification head on top for extractive question-answering tasks like SQuAD
(a linear layer on top of the hidden-states output to compute `),fd=r("code"),Z_=t("span start logits"),K_=t(" and "),md=r("code"),X_=t("span end logits"),J_=t(")."),eT=l(),Tr=r("p"),nT=t("The Funnel Transformer model was proposed in "),Fr=r("a"),tT=t(`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),oT=t(" by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),sT=l(),vr=r("p"),rT=t("This model inherits from "),bi=r("a"),aT=t("PreTrainedModel"),iT=t(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),lT=l(),kr=r("p"),dT=t("This model is also a PyTorch "),wr=r("a"),cT=t("torch.nn.Module"),uT=t(` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),pT=l(),on=r("div"),k(br.$$.fragment),hT=l(),bt=r("p"),fT=t("The "),yi=r("a"),mT=t("FunnelForQuestionAnswering"),gT=t(" forward method, overrides the "),gd=r("code"),_T=t("__call__"),TT=t(" special method."),FT=l(),k(co.$$.fragment),vT=l(),_d=r("p"),kT=t("Example:"),wT=l(),k(yr.$$.fragment),Xc=l(),yt=r("h2"),uo=r("a"),Td=r("span"),k($r.$$.fragment),bT=l(),Fd=r("span"),yT=t("TFFunnelBaseModel"),Jc=l(),je=r("div"),k(Er.$$.fragment),$T=l(),vd=r("p"),ET=t(`The base Funnel Transformer Model transformer outputting raw hidden-states without upsampling head (also called
decoder) or any task-specific head on top.`),MT=l(),Mr=r("p"),zT=t("The Funnel Transformer model was proposed in "),zr=r("a"),qT=t(`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),PT=t(" by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),CT=l(),qr=r("p"),jT=t("This model inherits from "),$i=r("a"),xT=t("TFPreTrainedModel"),LT=t(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),AT=l(),Pr=r("p"),DT=t("This model is also a "),Cr=r("a"),IT=t("tf.keras.Model"),OT=t(` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),ST=l(),k(po.$$.fragment),NT=l(),sn=r("div"),k(jr.$$.fragment),BT=l(),$t=r("p"),WT=t("The "),Ei=r("a"),QT=t("TFFunnelBaseModel"),RT=t(" forward method, overrides the "),kd=r("code"),HT=t("__call__"),VT=t(" special method."),YT=l(),k(ho.$$.fragment),UT=l(),wd=r("p"),GT=t("Example:"),ZT=l(),k(xr.$$.fragment),eu=l(),Et=r("h2"),fo=r("a"),bd=r("span"),k(Lr.$$.fragment),KT=l(),yd=r("span"),XT=t("TFFunnelModel"),nu=l(),xe=r("div"),k(Ar.$$.fragment),JT=l(),$d=r("p"),eF=t("The bare Funnel Transformer Model transformer outputting raw hidden-states without any specific head on top."),nF=l(),Dr=r("p"),tF=t("The Funnel Transformer model was proposed in "),Ir=r("a"),oF=t(`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),sF=t(" by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),rF=l(),Or=r("p"),aF=t("This model inherits from "),Mi=r("a"),iF=t("TFPreTrainedModel"),lF=t(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),dF=l(),Sr=r("p"),cF=t("This model is also a "),Nr=r("a"),uF=t("tf.keras.Model"),pF=t(` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),hF=l(),k(mo.$$.fragment),fF=l(),rn=r("div"),k(Br.$$.fragment),mF=l(),Mt=r("p"),gF=t("The "),zi=r("a"),_F=t("TFFunnelModel"),TF=t(" forward method, overrides the "),Ed=r("code"),FF=t("__call__"),vF=t(" special method."),kF=l(),k(go.$$.fragment),wF=l(),Md=r("p"),bF=t("Example:"),yF=l(),k(Wr.$$.fragment),tu=l(),zt=r("h2"),_o=r("a"),zd=r("span"),k(Qr.$$.fragment),$F=l(),qd=r("span"),EF=t("TFFunnelModelForPreTraining"),ou=l(),Le=r("div"),k(Rr.$$.fragment),MF=l(),Pd=r("p"),zF=t("Funnel model with a binary classification head on top as used during pretraining for identifying generated tokens."),qF=l(),Hr=r("p"),PF=t("The Funnel Transformer model was proposed in "),Vr=r("a"),CF=t(`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),jF=t(" by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),xF=l(),Yr=r("p"),LF=t("This model inherits from "),qi=r("a"),AF=t("TFPreTrainedModel"),DF=t(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),IF=l(),Ur=r("p"),OF=t("This model is also a "),Gr=r("a"),SF=t("tf.keras.Model"),NF=t(` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),BF=l(),k(To.$$.fragment),WF=l(),an=r("div"),k(Zr.$$.fragment),QF=l(),qt=r("p"),RF=t("The "),Pi=r("a"),HF=t("TFFunnelForPreTraining"),VF=t(" forward method, overrides the "),Cd=r("code"),YF=t("__call__"),UF=t(" special method."),GF=l(),k(Fo.$$.fragment),ZF=l(),jd=r("p"),KF=t("Examples:"),XF=l(),k(Kr.$$.fragment),su=l(),Pt=r("h2"),vo=r("a"),xd=r("span"),k(Xr.$$.fragment),JF=l(),Ld=r("span"),ev=t("TFFunnelForMaskedLM"),ru=l(),Ae=r("div"),k(Jr.$$.fragment),nv=l(),ea=r("p"),tv=t("Funnel Model with a "),Ad=r("code"),ov=t("language modeling"),sv=t(" head on top."),rv=l(),na=r("p"),av=t("The Funnel Transformer model was proposed in "),ta=r("a"),iv=t(`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),lv=t(" by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),dv=l(),oa=r("p"),cv=t("This model inherits from "),Ci=r("a"),uv=t("TFPreTrainedModel"),pv=t(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),hv=l(),sa=r("p"),fv=t("This model is also a "),ra=r("a"),mv=t("tf.keras.Model"),gv=t(` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),_v=l(),k(ko.$$.fragment),Tv=l(),ln=r("div"),k(aa.$$.fragment),Fv=l(),Ct=r("p"),vv=t("The "),ji=r("a"),kv=t("TFFunnelForMaskedLM"),wv=t(" forward method, overrides the "),Dd=r("code"),bv=t("__call__"),yv=t(" special method."),$v=l(),k(wo.$$.fragment),Ev=l(),Id=r("p"),Mv=t("Example:"),zv=l(),k(ia.$$.fragment),au=l(),jt=r("h2"),bo=r("a"),Od=r("span"),k(la.$$.fragment),qv=l(),Sd=r("span"),Pv=t("TFFunnelForSequenceClassification"),iu=l(),De=r("div"),k(da.$$.fragment),Cv=l(),Nd=r("p"),jv=t(`Funnel Model transformer with a sequence classification/regression head on top (a linear layer on top of the pooled
output) e.g. for GLUE tasks.`),xv=l(),ca=r("p"),Lv=t("The Funnel Transformer model was proposed in "),ua=r("a"),Av=t(`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),Dv=t(" by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),Iv=l(),pa=r("p"),Ov=t("This model inherits from "),xi=r("a"),Sv=t("TFPreTrainedModel"),Nv=t(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Bv=l(),ha=r("p"),Wv=t("This model is also a "),fa=r("a"),Qv=t("tf.keras.Model"),Rv=t(` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),Hv=l(),k(yo.$$.fragment),Vv=l(),dn=r("div"),k(ma.$$.fragment),Yv=l(),xt=r("p"),Uv=t("The "),Li=r("a"),Gv=t("TFFunnelForSequenceClassification"),Zv=t(" forward method, overrides the "),Bd=r("code"),Kv=t("__call__"),Xv=t(" special method."),Jv=l(),k($o.$$.fragment),ek=l(),Wd=r("p"),nk=t("Example:"),tk=l(),k(ga.$$.fragment),lu=l(),Lt=r("h2"),Eo=r("a"),Qd=r("span"),k(_a.$$.fragment),ok=l(),Rd=r("span"),sk=t("TFFunnelForMultipleChoice"),du=l(),Ie=r("div"),k(Ta.$$.fragment),rk=l(),Hd=r("p"),ak=t(`Funnel Model with a multiple choice classification head on top (a linear layer on top of the pooled output and a
softmax) e.g. for RocStories/SWAG tasks.`),ik=l(),Fa=r("p"),lk=t("The Funnel Transformer model was proposed in "),va=r("a"),dk=t(`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),ck=t(" by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),uk=l(),ka=r("p"),pk=t("This model inherits from "),Ai=r("a"),hk=t("TFPreTrainedModel"),fk=t(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),mk=l(),wa=r("p"),gk=t("This model is also a "),ba=r("a"),_k=t("tf.keras.Model"),Tk=t(` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),Fk=l(),k(Mo.$$.fragment),vk=l(),cn=r("div"),k(ya.$$.fragment),kk=l(),At=r("p"),wk=t("The "),Di=r("a"),bk=t("TFFunnelForMultipleChoice"),yk=t(" forward method, overrides the "),Vd=r("code"),$k=t("__call__"),Ek=t(" special method."),Mk=l(),k(zo.$$.fragment),zk=l(),Yd=r("p"),qk=t("Example:"),Pk=l(),k($a.$$.fragment),cu=l(),Dt=r("h2"),qo=r("a"),Ud=r("span"),k(Ea.$$.fragment),Ck=l(),Gd=r("span"),jk=t("TFFunnelForTokenClassification"),uu=l(),Oe=r("div"),k(Ma.$$.fragment),xk=l(),Zd=r("p"),Lk=t(`Funnel Model with a token classification head on top (a linear layer on top of the hidden-states output) e.g. for
Named-Entity-Recognition (NER) tasks.`),Ak=l(),za=r("p"),Dk=t("The Funnel Transformer model was proposed in "),qa=r("a"),Ik=t(`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),Ok=t(" by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),Sk=l(),Pa=r("p"),Nk=t("This model inherits from "),Ii=r("a"),Bk=t("TFPreTrainedModel"),Wk=t(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Qk=l(),Ca=r("p"),Rk=t("This model is also a "),ja=r("a"),Hk=t("tf.keras.Model"),Vk=t(` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),Yk=l(),k(Po.$$.fragment),Uk=l(),un=r("div"),k(xa.$$.fragment),Gk=l(),It=r("p"),Zk=t("The "),Oi=r("a"),Kk=t("TFFunnelForTokenClassification"),Xk=t(" forward method, overrides the "),Kd=r("code"),Jk=t("__call__"),e1=t(" special method."),n1=l(),k(Co.$$.fragment),t1=l(),Xd=r("p"),o1=t("Example:"),s1=l(),k(La.$$.fragment),pu=l(),Ot=r("h2"),jo=r("a"),Jd=r("span"),k(Aa.$$.fragment),r1=l(),ec=r("span"),a1=t("TFFunnelForQuestionAnswering"),hu=l(),Se=r("div"),k(Da.$$.fragment),i1=l(),St=r("p"),l1=t(`Funnel Model with a span classification head on top for extractive question-answering tasks like SQuAD (a linear
layers on top of the hidden-states output to compute `),nc=r("code"),d1=t("span start logits"),c1=t(" and "),tc=r("code"),u1=t("span end logits"),p1=t(")."),h1=l(),Ia=r("p"),f1=t("The Funnel Transformer model was proposed in "),Oa=r("a"),m1=t(`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),g1=t(" by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),_1=l(),Sa=r("p"),T1=t("This model inherits from "),Si=r("a"),F1=t("TFPreTrainedModel"),v1=t(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),k1=l(),Na=r("p"),w1=t("This model is also a "),Ba=r("a"),b1=t("tf.keras.Model"),y1=t(` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),$1=l(),k(xo.$$.fragment),E1=l(),pn=r("div"),k(Wa.$$.fragment),M1=l(),Nt=r("p"),z1=t("The "),Ni=r("a"),q1=t("TFFunnelForQuestionAnswering"),P1=t(" forward method, overrides the "),oc=r("code"),C1=t("__call__"),j1=t(" special method."),x1=l(),k(Lo.$$.fragment),L1=l(),sc=r("p"),A1=t("Example:"),D1=l(),k(Qa.$$.fragment),this.h()},l(s){const f=Ey('[data-svelte="svelte-1phssyn"]',document.head);p=a(f,"META",{name:!0,content:!0}),f.forEach(n),M=d(s),m=a(s,"H1",{class:!0});var Ra=i(m);g=a(Ra,"A",{id:!0,class:!0,href:!0});var rc=i(g);F=a(rc,"SPAN",{});var ac=i(F);w(T.$$.fragment,ac),ac.forEach(n),rc.forEach(n),_=d(Ra),z=a(Ra,"SPAN",{});var ic=i(z);ce=o(ic,"Funnel Transformer"),ic.forEach(n),Ra.forEach(n),G=d(s),q=a(s,"H2",{class:!0});var Ha=i(q);J=a(Ha,"A",{id:!0,class:!0,href:!0});var lc=i(J);I=a(lc,"SPAN",{});var dc=i(I);w(ne.$$.fragment,dc),dc.forEach(n),lc.forEach(n),ue=d(Ha),O=a(Ha,"SPAN",{});var cc=i(O);pe=o(cc,"Overview"),cc.forEach(n),Ha.forEach(n),ie=d(s),U=a(s,"P",{});var Va=i(U);L=o(Va,"The Funnel Transformer model was proposed in the paper "),te=a(Va,"A",{href:!0,rel:!0});var uc=i(te);Z=o(uc,`Funnel-Transformer: Filtering out Sequential Redundancy for
Efficient Language Processing`),uc.forEach(n),P=o(Va,`. It is a bidirectional transformer model, like
BERT, but with a pooling operation after each block of layers, a bit like in traditional convolutional neural networks
(CNN) in computer vision.`),Va.forEach(n),j=d(s),oe=a(s,"P",{});var pc=i(oe);Q=o(pc,"The abstract from the paper is the following:"),pc.forEach(n),le=d(s),se=a(s,"P",{});var hc=i(se);S=a(hc,"EM",{});var fc=i(S);he=o(fc,`With the success of language pretraining, it is highly desirable to develop more efficient architectures of good
scalability that can exploit the abundant unlabeled data at a lower cost. To improve the efficiency, we examine the
much-overlooked redundancy in maintaining a full-length token-level presentation, especially for tasks that only
require a single-vector presentation of the sequence. With this intuition, we propose Funnel-Transformer which
gradually compresses the sequence of hidden states to a shorter one and hence reduces the computation cost. More
importantly, by re-investing the saved FLOPs from length reduction in constructing a deeper or wider model, we further
improve the model capacity. In addition, to perform token-level predictions as required by common pretraining
objectives, Funnel-Transformer is able to recover a deep representation for each token from the reduced hidden sequence
via a decoder. Empirically, with comparable or fewer FLOPs, Funnel-Transformer outperforms the standard Transformer on
a wide variety of sequence-level prediction tasks, including text classification, language understanding, and reading
comprehension.`),fc.forEach(n),hc.forEach(n),de=d(s),C=a(s,"P",{});var mc=i(C);fe=o(mc,"Tips:"),mc.forEach(n),B=d(s),ee=a(s,"UL",{});var Ya=i(ee);ae=a(Ya,"LI",{});var gc=i(ae);R=o(gc,`Since Funnel Transformer uses pooling, the sequence length of the hidden states changes after each block of layers.
The base model therefore has a final sequence length that is a quarter of the original one. This model can be used
directly for tasks that just require a sentence summary (like sequence classification or multiple choice). For other
tasks, the full model is used; this full model has a decoder that upsamples the final hidden states to the same
sequence length as the input.`),gc.forEach(n),me=d(Ya),N=a(Ya,"LI",{});var Ce=i(N);A=o(Ce,`The Funnel Transformer checkpoints are all available with a full version and a base version. The first ones should be
used for `),re=a(Ce,"A",{href:!0});var _c=i(re);H=o(_c,"FunnelModel"),_c.forEach(n),ge=o(Ce,", "),u=a(Ce,"A",{href:!0});var Tc=i(u);v=o(Tc,"FunnelForPreTraining"),Tc.forEach(n),K=o(Ce,`,
`),Te=a(Ce,"A",{href:!0});var Fc=i(Te);we=o(Fc,"FunnelForMaskedLM"),Fc.forEach(n),D=o(Ce,", "),Fe=a(Ce,"A",{href:!0});var vc=i(Fe);be=o(vc,"FunnelForTokenClassification"),vc.forEach(n),ye=o(Ce,` and
class:`),x=a(Ce,"EM",{});var kc=i(x);V=o(kc,"~transformers.FunnelForQuestionAnswering"),kc.forEach(n),$e=o(Ce,`. The second ones should be used for
`),ve=a(Ce,"A",{href:!0});var wc=i(ve);Y=o(wc,"FunnelBaseModel"),wc.forEach(n),Ee=o(Ce,", "),ke=a(Ce,"A",{href:!0});var bc=i(ke);_e=o(bc,"FunnelForSequenceClassification"),bc.forEach(n),Me=o(Ce,` and
`),Ua=a(Ce,"A",{href:!0});var S1=i(Ua);Sp=o(S1,"FunnelForMultipleChoice"),S1.forEach(n),Np=o(Ce,"."),Ce.forEach(n),Ya.forEach(n),Ec=d(s),xn=a(s,"P",{});var Bi=i(xn);Bp=o(Bi,"This model was contributed by "),Oo=a(Bi,"A",{href:!0,rel:!0});var N1=i(Oo);Wp=o(N1,"sgugger"),N1.forEach(n),Qp=o(Bi,". The original code can be found "),So=a(Bi,"A",{href:!0,rel:!0});var B1=i(So);Rp=o(B1,"here"),B1.forEach(n),Hp=o(Bi,"."),Bi.forEach(n),Mc=d(s),Kn=a(s,"H2",{class:!0});var mu=i(Kn);Bt=a(mu,"A",{id:!0,class:!0,href:!0});var W1=i(Bt);ul=a(W1,"SPAN",{});var Q1=i(ul);w(No.$$.fragment,Q1),Q1.forEach(n),W1.forEach(n),Vp=d(mu),pl=a(mu,"SPAN",{});var R1=i(pl);Yp=o(R1,"FunnelConfig"),R1.forEach(n),mu.forEach(n),zc=d(s),Cn=a(s,"DIV",{class:!0});var Wi=i(Cn);w(Bo.$$.fragment,Wi),Up=d(Wi),jn=a(Wi,"P",{});var Ao=i(jn);Gp=o(Ao,"This is the configuration class to store the configuration of a "),Ga=a(Ao,"A",{href:!0});var H1=i(Ga);Zp=o(H1,"FunnelModel"),H1.forEach(n),Kp=o(Ao," or a "),Za=a(Ao,"A",{href:!0});var V1=i(Za);Xp=o(V1,"TFBertModel"),V1.forEach(n),Jp=o(Ao,`. It is used to
instantiate a Funnel Transformer model according to the specified arguments, defining the model architecture.
Instantiating a configuration with the defaults will yield a similar configuration to that of the Funnel
Transformer `),Wo=a(Ao,"A",{href:!0,rel:!0});var Y1=i(Wo);eh=o(Y1,"funnel-transformer/small"),Y1.forEach(n),nh=o(Ao," architecture."),Ao.forEach(n),th=d(Wi),Xn=a(Wi,"P",{});var Qi=i(Xn);oh=o(Qi,"Configuration objects inherit from "),Ka=a(Qi,"A",{href:!0});var U1=i(Ka);sh=o(U1,"PretrainedConfig"),U1.forEach(n),rh=o(Qi,` and can be used to control the model outputs. Read the
documentation from `),Xa=a(Qi,"A",{href:!0});var G1=i(Xa);ah=o(G1,"PretrainedConfig"),G1.forEach(n),ih=o(Qi," for more information."),Qi.forEach(n),Wi.forEach(n),qc=d(s),Jn=a(s,"H2",{class:!0});var gu=i(Jn);Wt=a(gu,"A",{id:!0,class:!0,href:!0});var Z1=i(Wt);hl=a(Z1,"SPAN",{});var K1=i(hl);w(Qo.$$.fragment,K1),K1.forEach(n),Z1.forEach(n),lh=d(gu),fl=a(gu,"SPAN",{});var X1=i(fl);dh=o(X1,"FunnelTokenizer"),X1.forEach(n),gu.forEach(n),Pc=d(s),Pe=a(s,"DIV",{class:!0});var Ge=i(Pe);w(Ro.$$.fragment,Ge),ch=d(Ge),ml=a(Ge,"P",{});var J1=i(ml);uh=o(J1,"Construct a Funnel Transformer tokenizer."),J1.forEach(n),ph=d(Ge),Qt=a(Ge,"P",{});var yc=i(Qt);Ja=a(yc,"A",{href:!0});var ew=i(Ja);hh=o(ew,"FunnelTokenizer"),ew.forEach(n),fh=o(yc," is identical to "),ei=a(yc,"A",{href:!0});var nw=i(ei);mh=o(nw,"BertTokenizer"),nw.forEach(n),gh=o(yc,` and runs end-to-end tokenization: punctuation splitting and
wordpiece.`),yc.forEach(n),_h=d(Ge),Ho=a(Ge,"P",{});var _u=i(Ho);Th=o(_u,"Refer to superclass "),ni=a(_u,"A",{href:!0});var tw=i(ni);Fh=o(tw,"BertTokenizer"),tw.forEach(n),vh=o(_u," for usage examples and documentation concerning parameters."),_u.forEach(n),kh=d(Ge),Ln=a(Ge,"DIV",{class:!0});var Ri=i(Ln);w(Vo.$$.fragment,Ri),wh=d(Ri),gl=a(Ri,"P",{});var ow=i(gl);bh=o(ow,`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. A BERT sequence has the following format:`),ow.forEach(n),yh=d(Ri),Yo=a(Ri,"UL",{});var Tu=i(Yo);ti=a(Tu,"LI",{});var I1=i(ti);$h=o(I1,"single sequence: "),_l=a(I1,"CODE",{});var sw=i(_l);Eh=o(sw,"[CLS] X [SEP]"),sw.forEach(n),I1.forEach(n),Mh=d(Tu),oi=a(Tu,"LI",{});var O1=i(oi);zh=o(O1,"pair of sequences: "),Tl=a(O1,"CODE",{});var rw=i(Tl);qh=o(rw,"[CLS] A [SEP] B [SEP]"),rw.forEach(n),O1.forEach(n),Tu.forEach(n),Ri.forEach(n),Ph=d(Ge),Rt=a(Ge,"DIV",{class:!0});var Fu=i(Rt);w(Uo.$$.fragment,Fu),Ch=d(Fu),Go=a(Fu,"P",{});var vu=i(Go);jh=o(vu,`Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding
special tokens using the tokenizer `),Fl=a(vu,"CODE",{});var aw=i(Fl);xh=o(aw,"prepare_for_model"),aw.forEach(n),Lh=o(vu," method."),vu.forEach(n),Fu.forEach(n),Ah=d(Ge),wn=a(Ge,"DIV",{class:!0});var Do=i(wn);w(Zo.$$.fragment,Do),Dh=d(Do),vl=a(Do,"P",{});var iw=i(vl);Ih=o(iw,`Create a mask from the two sequences passed to be used in a sequence-pair classification task. A Funnel
Transformer sequence pair mask has the following format:`),iw.forEach(n),Oh=d(Do),w(Ko.$$.fragment,Do),Sh=d(Do),et=a(Do,"P",{});var Hi=i(et);Nh=o(Hi,"If "),kl=a(Hi,"CODE",{});var lw=i(kl);Bh=o(lw,"token_ids_1"),lw.forEach(n),Wh=o(Hi," is "),wl=a(Hi,"CODE",{});var dw=i(wl);Qh=o(dw,"None"),dw.forEach(n),Rh=o(Hi,", this method only returns the first portion of the mask (0s)."),Hi.forEach(n),Do.forEach(n),Hh=d(Ge),si=a(Ge,"DIV",{class:!0});var cw=i(si);w(Xo.$$.fragment,cw),cw.forEach(n),Ge.forEach(n),Cc=d(s),nt=a(s,"H2",{class:!0});var ku=i(nt);Ht=a(ku,"A",{id:!0,class:!0,href:!0});var uw=i(Ht);bl=a(uw,"SPAN",{});var pw=i(bl);w(Jo.$$.fragment,pw),pw.forEach(n),uw.forEach(n),Vh=d(ku),yl=a(ku,"SPAN",{});var hw=i(yl);Yh=o(hw,"FunnelTokenizerFast"),hw.forEach(n),ku.forEach(n),jc=d(s),Ze=a(s,"DIV",{class:!0});var An=i(Ze);w(es.$$.fragment,An),Uh=d(An),ns=a(An,"P",{});var wu=i(ns);Gh=o(wu,"Construct a \u201Cfast\u201D Funnel Transformer tokenizer (backed by HuggingFace\u2019s "),$l=a(wu,"EM",{});var fw=i($l);Zh=o(fw,"tokenizers"),fw.forEach(n),Kh=o(wu," library)."),wu.forEach(n),Xh=d(An),Vt=a(An,"P",{});var $c=i(Vt);ri=a($c,"A",{href:!0});var mw=i(ri);Jh=o(mw,"FunnelTokenizerFast"),mw.forEach(n),ef=o($c," is identical to "),ai=a($c,"A",{href:!0});var gw=i(ai);nf=o(gw,"BertTokenizerFast"),gw.forEach(n),tf=o($c,` and runs end-to-end tokenization: punctuation
splitting and wordpiece.`),$c.forEach(n),of=d(An),ts=a(An,"P",{});var bu=i(ts);sf=o(bu,"Refer to superclass "),ii=a(bu,"A",{href:!0});var _w=i(ii);rf=o(_w,"BertTokenizerFast"),_w.forEach(n),af=o(bu," for usage examples and documentation concerning parameters."),bu.forEach(n),lf=d(An),bn=a(An,"DIV",{class:!0});var Io=i(bn);w(os.$$.fragment,Io),df=d(Io),El=a(Io,"P",{});var Tw=i(El);cf=o(Tw,`Create a mask from the two sequences passed to be used in a sequence-pair classification task. A Funnel
Transformer sequence pair mask has the following format:`),Tw.forEach(n),uf=d(Io),w(ss.$$.fragment,Io),pf=d(Io),tt=a(Io,"P",{});var Vi=i(tt);hf=o(Vi,"If "),Ml=a(Vi,"CODE",{});var Fw=i(Ml);ff=o(Fw,"token_ids_1"),Fw.forEach(n),mf=o(Vi," is "),zl=a(Vi,"CODE",{});var vw=i(zl);gf=o(vw,"None"),vw.forEach(n),_f=o(Vi,", this method only returns the first portion of the mask (0s)."),Vi.forEach(n),Io.forEach(n),An.forEach(n),xc=d(s),ot=a(s,"H2",{class:!0});var yu=i(ot);Yt=a(yu,"A",{id:!0,class:!0,href:!0});var kw=i(Yt);ql=a(kw,"SPAN",{});var ww=i(ql);w(rs.$$.fragment,ww),ww.forEach(n),kw.forEach(n),Tf=d(yu),Pl=a(yu,"SPAN",{});var bw=i(Pl);Ff=o(bw,"Funnel specific outputs"),bw.forEach(n),yu.forEach(n),Lc=d(s),st=a(s,"DIV",{class:!0});var $u=i(st);w(as.$$.fragment,$u),vf=d($u),is=a($u,"P",{});var Eu=i(is);kf=o(Eu,"Output type of "),li=a(Eu,"A",{href:!0});var yw=i(li);wf=o(yw,"FunnelForPreTraining"),yw.forEach(n),bf=o(Eu,"."),Eu.forEach(n),$u.forEach(n),Ac=d(s),rt=a(s,"DIV",{class:!0});var Mu=i(rt);w(ls.$$.fragment,Mu),yf=d(Mu),ds=a(Mu,"P",{});var zu=i(ds);$f=o(zu,"Output type of "),di=a(zu,"A",{href:!0});var $w=i(di);Ef=o($w,"FunnelForPreTraining"),$w.forEach(n),Mf=o(zu,"."),zu.forEach(n),Mu.forEach(n),Dc=d(s),at=a(s,"H2",{class:!0});var qu=i(at);Ut=a(qu,"A",{id:!0,class:!0,href:!0});var Ew=i(Ut);Cl=a(Ew,"SPAN",{});var Mw=i(Cl);w(cs.$$.fragment,Mw),Mw.forEach(n),Ew.forEach(n),zf=d(qu),jl=a(qu,"SPAN",{});var zw=i(jl);qf=o(zw,"FunnelBaseModel"),zw.forEach(n),qu.forEach(n),Ic=d(s),We=a(s,"DIV",{class:!0});var yn=i(We);w(us.$$.fragment,yn),Pf=d(yn),xl=a(yn,"P",{});var qw=i(xl);Cf=o(qw,`The base Funnel Transformer Model transformer outputting raw hidden-states without upsampling head (also called
decoder) or any task-specific head on top.`),qw.forEach(n),jf=d(yn),ps=a(yn,"P",{});var Pu=i(ps);xf=o(Pu,"The Funnel Transformer model was proposed in "),hs=a(Pu,"A",{href:!0,rel:!0});var Pw=i(hs);Lf=o(Pw,`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),Pw.forEach(n),Af=o(Pu," by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),Pu.forEach(n),Df=d(yn),fs=a(yn,"P",{});var Cu=i(fs);If=o(Cu,"This model inherits from "),ci=a(Cu,"A",{href:!0});var Cw=i(ci);Of=o(Cw,"PreTrainedModel"),Cw.forEach(n),Sf=o(Cu,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Cu.forEach(n),Nf=d(yn),ms=a(yn,"P",{});var ju=i(ms);Bf=o(ju,"This model is also a PyTorch "),gs=a(ju,"A",{href:!0,rel:!0});var jw=i(gs);Wf=o(jw,"torch.nn.Module"),jw.forEach(n),Qf=o(ju,` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),ju.forEach(n),Rf=d(yn),Ke=a(yn,"DIV",{class:!0});var Dn=i(Ke);w(_s.$$.fragment,Dn),Hf=d(Dn),it=a(Dn,"P",{});var Yi=i(it);Vf=o(Yi,"The "),ui=a(Yi,"A",{href:!0});var xw=i(ui);Yf=o(xw,"FunnelBaseModel"),xw.forEach(n),Uf=o(Yi," forward method, overrides the "),Ll=a(Yi,"CODE",{});var Lw=i(Ll);Gf=o(Lw,"__call__"),Lw.forEach(n),Zf=o(Yi," special method."),Yi.forEach(n),Kf=d(Dn),w(Gt.$$.fragment,Dn),Xf=d(Dn),Al=a(Dn,"P",{});var Aw=i(Al);Jf=o(Aw,"Example:"),Aw.forEach(n),em=d(Dn),w(Ts.$$.fragment,Dn),Dn.forEach(n),yn.forEach(n),Oc=d(s),lt=a(s,"H2",{class:!0});var xu=i(lt);Zt=a(xu,"A",{id:!0,class:!0,href:!0});var Dw=i(Zt);Dl=a(Dw,"SPAN",{});var Iw=i(Dl);w(Fs.$$.fragment,Iw),Iw.forEach(n),Dw.forEach(n),nm=d(xu),Il=a(xu,"SPAN",{});var Ow=i(Il);tm=o(Ow,"FunnelModel"),Ow.forEach(n),xu.forEach(n),Sc=d(s),Qe=a(s,"DIV",{class:!0});var $n=i(Qe);w(vs.$$.fragment,$n),om=d($n),Ol=a($n,"P",{});var Sw=i(Ol);sm=o(Sw,"The bare Funnel Transformer Model transformer outputting raw hidden-states without any specific head on top."),Sw.forEach(n),rm=d($n),ks=a($n,"P",{});var Lu=i(ks);am=o(Lu,"The Funnel Transformer model was proposed in "),ws=a(Lu,"A",{href:!0,rel:!0});var Nw=i(ws);im=o(Nw,`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),Nw.forEach(n),lm=o(Lu," by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),Lu.forEach(n),dm=d($n),bs=a($n,"P",{});var Au=i(bs);cm=o(Au,"This model inherits from "),pi=a(Au,"A",{href:!0});var Bw=i(pi);um=o(Bw,"PreTrainedModel"),Bw.forEach(n),pm=o(Au,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Au.forEach(n),hm=d($n),ys=a($n,"P",{});var Du=i(ys);fm=o(Du,"This model is also a PyTorch "),$s=a(Du,"A",{href:!0,rel:!0});var Ww=i($s);mm=o(Ww,"torch.nn.Module"),Ww.forEach(n),gm=o(Du,` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),Du.forEach(n),_m=d($n),Xe=a($n,"DIV",{class:!0});var In=i(Xe);w(Es.$$.fragment,In),Tm=d(In),dt=a(In,"P",{});var Ui=i(dt);Fm=o(Ui,"The "),hi=a(Ui,"A",{href:!0});var Qw=i(hi);vm=o(Qw,"FunnelModel"),Qw.forEach(n),km=o(Ui," forward method, overrides the "),Sl=a(Ui,"CODE",{});var Rw=i(Sl);wm=o(Rw,"__call__"),Rw.forEach(n),bm=o(Ui," special method."),Ui.forEach(n),ym=d(In),w(Kt.$$.fragment,In),$m=d(In),Nl=a(In,"P",{});var Hw=i(Nl);Em=o(Hw,"Example:"),Hw.forEach(n),Mm=d(In),w(Ms.$$.fragment,In),In.forEach(n),$n.forEach(n),Nc=d(s),ct=a(s,"H2",{class:!0});var Iu=i(ct);Xt=a(Iu,"A",{id:!0,class:!0,href:!0});var Vw=i(Xt);Bl=a(Vw,"SPAN",{});var Yw=i(Bl);w(zs.$$.fragment,Yw),Yw.forEach(n),Vw.forEach(n),zm=d(Iu),Wl=a(Iu,"SPAN",{});var Uw=i(Wl);qm=o(Uw,"FunnelModelForPreTraining"),Uw.forEach(n),Iu.forEach(n),Bc=d(s),ut=a(s,"DIV",{class:!0});var Ou=i(ut);w(qs.$$.fragment,Ou),Pm=d(Ou),Je=a(Ou,"DIV",{class:!0});var On=i(Je);w(Ps.$$.fragment,On),Cm=d(On),pt=a(On,"P",{});var Gi=i(pt);jm=o(Gi,"The "),fi=a(Gi,"A",{href:!0});var Gw=i(fi);xm=o(Gw,"FunnelForPreTraining"),Gw.forEach(n),Lm=o(Gi," forward method, overrides the "),Ql=a(Gi,"CODE",{});var Zw=i(Ql);Am=o(Zw,"__call__"),Zw.forEach(n),Dm=o(Gi," special method."),Gi.forEach(n),Im=d(On),w(Jt.$$.fragment,On),Om=d(On),Rl=a(On,"P",{});var Kw=i(Rl);Sm=o(Kw,"Examples:"),Kw.forEach(n),Nm=d(On),w(Cs.$$.fragment,On),On.forEach(n),Ou.forEach(n),Wc=d(s),ht=a(s,"H2",{class:!0});var Su=i(ht);eo=a(Su,"A",{id:!0,class:!0,href:!0});var Xw=i(eo);Hl=a(Xw,"SPAN",{});var Jw=i(Hl);w(js.$$.fragment,Jw),Jw.forEach(n),Xw.forEach(n),Bm=d(Su),Vl=a(Su,"SPAN",{});var eb=i(Vl);Wm=o(eb,"FunnelForMaskedLM"),eb.forEach(n),Su.forEach(n),Qc=d(s),Re=a(s,"DIV",{class:!0});var En=i(Re);w(xs.$$.fragment,En),Qm=d(En),Ls=a(En,"P",{});var Nu=i(Ls);Rm=o(Nu,"Funnel Transformer Model with a "),Yl=a(Nu,"CODE",{});var nb=i(Yl);Hm=o(nb,"language modeling"),nb.forEach(n),Vm=o(Nu," head on top."),Nu.forEach(n),Ym=d(En),As=a(En,"P",{});var Bu=i(As);Um=o(Bu,"The Funnel Transformer model was proposed in "),Ds=a(Bu,"A",{href:!0,rel:!0});var tb=i(Ds);Gm=o(tb,`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),tb.forEach(n),Zm=o(Bu," by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),Bu.forEach(n),Km=d(En),Is=a(En,"P",{});var Wu=i(Is);Xm=o(Wu,"This model inherits from "),mi=a(Wu,"A",{href:!0});var ob=i(mi);Jm=o(ob,"PreTrainedModel"),ob.forEach(n),eg=o(Wu,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Wu.forEach(n),ng=d(En),Os=a(En,"P",{});var Qu=i(Os);tg=o(Qu,"This model is also a PyTorch "),Ss=a(Qu,"A",{href:!0,rel:!0});var sb=i(Ss);og=o(sb,"torch.nn.Module"),sb.forEach(n),sg=o(Qu,` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),Qu.forEach(n),rg=d(En),en=a(En,"DIV",{class:!0});var Sn=i(en);w(Ns.$$.fragment,Sn),ag=d(Sn),ft=a(Sn,"P",{});var Zi=i(ft);ig=o(Zi,"The "),gi=a(Zi,"A",{href:!0});var rb=i(gi);lg=o(rb,"FunnelForMaskedLM"),rb.forEach(n),dg=o(Zi," forward method, overrides the "),Ul=a(Zi,"CODE",{});var ab=i(Ul);cg=o(ab,"__call__"),ab.forEach(n),ug=o(Zi," special method."),Zi.forEach(n),pg=d(Sn),w(no.$$.fragment,Sn),hg=d(Sn),Gl=a(Sn,"P",{});var ib=i(Gl);fg=o(ib,"Example:"),ib.forEach(n),mg=d(Sn),w(Bs.$$.fragment,Sn),Sn.forEach(n),En.forEach(n),Rc=d(s),mt=a(s,"H2",{class:!0});var Ru=i(mt);to=a(Ru,"A",{id:!0,class:!0,href:!0});var lb=i(to);Zl=a(lb,"SPAN",{});var db=i(Zl);w(Ws.$$.fragment,db),db.forEach(n),lb.forEach(n),gg=d(Ru),Kl=a(Ru,"SPAN",{});var cb=i(Kl);_g=o(cb,"FunnelForSequenceClassification"),cb.forEach(n),Ru.forEach(n),Hc=d(s),He=a(s,"DIV",{class:!0});var Mn=i(He);w(Qs.$$.fragment,Mn),Tg=d(Mn),Xl=a(Mn,"P",{});var ub=i(Xl);Fg=o(ub,`Funnel Transformer Model with a sequence classification/regression head on top (two linear layer on top of the
first timestep of the last hidden state) e.g. for GLUE tasks.`),ub.forEach(n),vg=d(Mn),Rs=a(Mn,"P",{});var Hu=i(Rs);kg=o(Hu,"The Funnel Transformer model was proposed in "),Hs=a(Hu,"A",{href:!0,rel:!0});var pb=i(Hs);wg=o(pb,`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),pb.forEach(n),bg=o(Hu," by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),Hu.forEach(n),yg=d(Mn),Vs=a(Mn,"P",{});var Vu=i(Vs);$g=o(Vu,"This model inherits from "),_i=a(Vu,"A",{href:!0});var hb=i(_i);Eg=o(hb,"PreTrainedModel"),hb.forEach(n),Mg=o(Vu,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Vu.forEach(n),zg=d(Mn),Ys=a(Mn,"P",{});var Yu=i(Ys);qg=o(Yu,"This model is also a PyTorch "),Us=a(Yu,"A",{href:!0,rel:!0});var fb=i(Us);Pg=o(fb,"torch.nn.Module"),fb.forEach(n),Cg=o(Yu,` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),Yu.forEach(n),jg=d(Mn),Be=a(Mn,"DIV",{class:!0});var hn=i(Be);w(Gs.$$.fragment,hn),xg=d(hn),gt=a(hn,"P",{});var Ki=i(gt);Lg=o(Ki,"The "),Ti=a(Ki,"A",{href:!0});var mb=i(Ti);Ag=o(mb,"FunnelForSequenceClassification"),mb.forEach(n),Dg=o(Ki," forward method, overrides the "),Jl=a(Ki,"CODE",{});var gb=i(Jl);Ig=o(gb,"__call__"),gb.forEach(n),Og=o(Ki," special method."),Ki.forEach(n),Sg=d(hn),w(oo.$$.fragment,hn),Ng=d(hn),ed=a(hn,"P",{});var _b=i(ed);Bg=o(_b,"Example of single-label classification:"),_b.forEach(n),Wg=d(hn),w(Zs.$$.fragment,hn),Qg=d(hn),nd=a(hn,"P",{});var Tb=i(nd);Rg=o(Tb,"Example of multi-label classification:"),Tb.forEach(n),Hg=d(hn),w(Ks.$$.fragment,hn),hn.forEach(n),Mn.forEach(n),Vc=d(s),_t=a(s,"H2",{class:!0});var Uu=i(_t);so=a(Uu,"A",{id:!0,class:!0,href:!0});var Fb=i(so);td=a(Fb,"SPAN",{});var vb=i(td);w(Xs.$$.fragment,vb),vb.forEach(n),Fb.forEach(n),Vg=d(Uu),od=a(Uu,"SPAN",{});var kb=i(od);Yg=o(kb,"FunnelForMultipleChoice"),kb.forEach(n),Uu.forEach(n),Yc=d(s),Ve=a(s,"DIV",{class:!0});var zn=i(Ve);w(Js.$$.fragment,zn),Ug=d(zn),sd=a(zn,"P",{});var wb=i(sd);Gg=o(wb,`Funnel Transformer Model with a multiple choice classification head on top (two linear layer on top of the first
timestep of the last hidden state, and a softmax) e.g. for RocStories/SWAG tasks.`),wb.forEach(n),Zg=d(zn),er=a(zn,"P",{});var Gu=i(er);Kg=o(Gu,"The Funnel Transformer model was proposed in "),nr=a(Gu,"A",{href:!0,rel:!0});var bb=i(nr);Xg=o(bb,`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),bb.forEach(n),Jg=o(Gu," by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),Gu.forEach(n),e_=d(zn),tr=a(zn,"P",{});var Zu=i(tr);n_=o(Zu,"This model inherits from "),Fi=a(Zu,"A",{href:!0});var yb=i(Fi);t_=o(yb,"PreTrainedModel"),yb.forEach(n),o_=o(Zu,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Zu.forEach(n),s_=d(zn),or=a(zn,"P",{});var Ku=i(or);r_=o(Ku,"This model is also a PyTorch "),sr=a(Ku,"A",{href:!0,rel:!0});var $b=i(sr);a_=o($b,"torch.nn.Module"),$b.forEach(n),i_=o(Ku,` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),Ku.forEach(n),l_=d(zn),nn=a(zn,"DIV",{class:!0});var Nn=i(nn);w(rr.$$.fragment,Nn),d_=d(Nn),Tt=a(Nn,"P",{});var Xi=i(Tt);c_=o(Xi,"The "),vi=a(Xi,"A",{href:!0});var Eb=i(vi);u_=o(Eb,"FunnelForMultipleChoice"),Eb.forEach(n),p_=o(Xi," forward method, overrides the "),rd=a(Xi,"CODE",{});var Mb=i(rd);h_=o(Mb,"__call__"),Mb.forEach(n),f_=o(Xi," special method."),Xi.forEach(n),m_=d(Nn),w(ro.$$.fragment,Nn),g_=d(Nn),ad=a(Nn,"P",{});var zb=i(ad);__=o(zb,"Example:"),zb.forEach(n),T_=d(Nn),w(ar.$$.fragment,Nn),Nn.forEach(n),zn.forEach(n),Uc=d(s),Ft=a(s,"H2",{class:!0});var Xu=i(Ft);ao=a(Xu,"A",{id:!0,class:!0,href:!0});var qb=i(ao);id=a(qb,"SPAN",{});var Pb=i(id);w(ir.$$.fragment,Pb),Pb.forEach(n),qb.forEach(n),F_=d(Xu),ld=a(Xu,"SPAN",{});var Cb=i(ld);v_=o(Cb,"FunnelForTokenClassification"),Cb.forEach(n),Xu.forEach(n),Gc=d(s),Ye=a(s,"DIV",{class:!0});var qn=i(Ye);w(lr.$$.fragment,qn),k_=d(qn),dd=a(qn,"P",{});var jb=i(dd);w_=o(jb,`Funnel Transformer Model with a token classification head on top (a linear layer on top of the hidden-states
output) e.g. for Named-Entity-Recognition (NER) tasks.`),jb.forEach(n),b_=d(qn),dr=a(qn,"P",{});var Ju=i(dr);y_=o(Ju,"The Funnel Transformer model was proposed in "),cr=a(Ju,"A",{href:!0,rel:!0});var xb=i(cr);$_=o(xb,`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),xb.forEach(n),E_=o(Ju," by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),Ju.forEach(n),M_=d(qn),ur=a(qn,"P",{});var ep=i(ur);z_=o(ep,"This model inherits from "),ki=a(ep,"A",{href:!0});var Lb=i(ki);q_=o(Lb,"PreTrainedModel"),Lb.forEach(n),P_=o(ep,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),ep.forEach(n),C_=d(qn),pr=a(qn,"P",{});var np=i(pr);j_=o(np,"This model is also a PyTorch "),hr=a(np,"A",{href:!0,rel:!0});var Ab=i(hr);x_=o(Ab,"torch.nn.Module"),Ab.forEach(n),L_=o(np,` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),np.forEach(n),A_=d(qn),tn=a(qn,"DIV",{class:!0});var Bn=i(tn);w(fr.$$.fragment,Bn),D_=d(Bn),vt=a(Bn,"P",{});var Ji=i(vt);I_=o(Ji,"The "),wi=a(Ji,"A",{href:!0});var Db=i(wi);O_=o(Db,"FunnelForTokenClassification"),Db.forEach(n),S_=o(Ji," forward method, overrides the "),cd=a(Ji,"CODE",{});var Ib=i(cd);N_=o(Ib,"__call__"),Ib.forEach(n),B_=o(Ji," special method."),Ji.forEach(n),W_=d(Bn),w(io.$$.fragment,Bn),Q_=d(Bn),ud=a(Bn,"P",{});var Ob=i(ud);R_=o(Ob,"Example:"),Ob.forEach(n),H_=d(Bn),w(mr.$$.fragment,Bn),Bn.forEach(n),qn.forEach(n),Zc=d(s),kt=a(s,"H2",{class:!0});var tp=i(kt);lo=a(tp,"A",{id:!0,class:!0,href:!0});var Sb=i(lo);pd=a(Sb,"SPAN",{});var Nb=i(pd);w(gr.$$.fragment,Nb),Nb.forEach(n),Sb.forEach(n),V_=d(tp),hd=a(tp,"SPAN",{});var Bb=i(hd);Y_=o(Bb,"FunnelForQuestionAnswering"),Bb.forEach(n),tp.forEach(n),Kc=d(s),Ue=a(s,"DIV",{class:!0});var Pn=i(Ue);w(_r.$$.fragment,Pn),U_=d(Pn),wt=a(Pn,"P",{});var el=i(wt);G_=o(el,`Funnel Transformer Model with a span classification head on top for extractive question-answering tasks like SQuAD
(a linear layer on top of the hidden-states output to compute `),fd=a(el,"CODE",{});var Wb=i(fd);Z_=o(Wb,"span start logits"),Wb.forEach(n),K_=o(el," and "),md=a(el,"CODE",{});var Qb=i(md);X_=o(Qb,"span end logits"),Qb.forEach(n),J_=o(el,")."),el.forEach(n),eT=d(Pn),Tr=a(Pn,"P",{});var op=i(Tr);nT=o(op,"The Funnel Transformer model was proposed in "),Fr=a(op,"A",{href:!0,rel:!0});var Rb=i(Fr);tT=o(Rb,`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),Rb.forEach(n),oT=o(op," by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),op.forEach(n),sT=d(Pn),vr=a(Pn,"P",{});var sp=i(vr);rT=o(sp,"This model inherits from "),bi=a(sp,"A",{href:!0});var Hb=i(bi);aT=o(Hb,"PreTrainedModel"),Hb.forEach(n),iT=o(sp,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),sp.forEach(n),lT=d(Pn),kr=a(Pn,"P",{});var rp=i(kr);dT=o(rp,"This model is also a PyTorch "),wr=a(rp,"A",{href:!0,rel:!0});var Vb=i(wr);cT=o(Vb,"torch.nn.Module"),Vb.forEach(n),uT=o(rp,` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),rp.forEach(n),pT=d(Pn),on=a(Pn,"DIV",{class:!0});var Wn=i(on);w(br.$$.fragment,Wn),hT=d(Wn),bt=a(Wn,"P",{});var nl=i(bt);fT=o(nl,"The "),yi=a(nl,"A",{href:!0});var Yb=i(yi);mT=o(Yb,"FunnelForQuestionAnswering"),Yb.forEach(n),gT=o(nl," forward method, overrides the "),gd=a(nl,"CODE",{});var Ub=i(gd);_T=o(Ub,"__call__"),Ub.forEach(n),TT=o(nl," special method."),nl.forEach(n),FT=d(Wn),w(co.$$.fragment,Wn),vT=d(Wn),_d=a(Wn,"P",{});var Gb=i(_d);kT=o(Gb,"Example:"),Gb.forEach(n),wT=d(Wn),w(yr.$$.fragment,Wn),Wn.forEach(n),Pn.forEach(n),Xc=d(s),yt=a(s,"H2",{class:!0});var ap=i(yt);uo=a(ap,"A",{id:!0,class:!0,href:!0});var Zb=i(uo);Td=a(Zb,"SPAN",{});var Kb=i(Td);w($r.$$.fragment,Kb),Kb.forEach(n),Zb.forEach(n),bT=d(ap),Fd=a(ap,"SPAN",{});var Xb=i(Fd);yT=o(Xb,"TFFunnelBaseModel"),Xb.forEach(n),ap.forEach(n),Jc=d(s),je=a(s,"DIV",{class:!0});var fn=i(je);w(Er.$$.fragment,fn),$T=d(fn),vd=a(fn,"P",{});var Jb=i(vd);ET=o(Jb,`The base Funnel Transformer Model transformer outputting raw hidden-states without upsampling head (also called
decoder) or any task-specific head on top.`),Jb.forEach(n),MT=d(fn),Mr=a(fn,"P",{});var ip=i(Mr);zT=o(ip,"The Funnel Transformer model was proposed in "),zr=a(ip,"A",{href:!0,rel:!0});var e0=i(zr);qT=o(e0,`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),e0.forEach(n),PT=o(ip," by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),ip.forEach(n),CT=d(fn),qr=a(fn,"P",{});var lp=i(qr);jT=o(lp,"This model inherits from "),$i=a(lp,"A",{href:!0});var n0=i($i);xT=o(n0,"TFPreTrainedModel"),n0.forEach(n),LT=o(lp,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),lp.forEach(n),AT=d(fn),Pr=a(fn,"P",{});var dp=i(Pr);DT=o(dp,"This model is also a "),Cr=a(dp,"A",{href:!0,rel:!0});var t0=i(Cr);IT=o(t0,"tf.keras.Model"),t0.forEach(n),OT=o(dp,` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),dp.forEach(n),ST=d(fn),w(po.$$.fragment,fn),NT=d(fn),sn=a(fn,"DIV",{class:!0});var Qn=i(sn);w(jr.$$.fragment,Qn),BT=d(Qn),$t=a(Qn,"P",{});var tl=i($t);WT=o(tl,"The "),Ei=a(tl,"A",{href:!0});var o0=i(Ei);QT=o(o0,"TFFunnelBaseModel"),o0.forEach(n),RT=o(tl," forward method, overrides the "),kd=a(tl,"CODE",{});var s0=i(kd);HT=o(s0,"__call__"),s0.forEach(n),VT=o(tl," special method."),tl.forEach(n),YT=d(Qn),w(ho.$$.fragment,Qn),UT=d(Qn),wd=a(Qn,"P",{});var r0=i(wd);GT=o(r0,"Example:"),r0.forEach(n),ZT=d(Qn),w(xr.$$.fragment,Qn),Qn.forEach(n),fn.forEach(n),eu=d(s),Et=a(s,"H2",{class:!0});var cp=i(Et);fo=a(cp,"A",{id:!0,class:!0,href:!0});var a0=i(fo);bd=a(a0,"SPAN",{});var i0=i(bd);w(Lr.$$.fragment,i0),i0.forEach(n),a0.forEach(n),KT=d(cp),yd=a(cp,"SPAN",{});var l0=i(yd);XT=o(l0,"TFFunnelModel"),l0.forEach(n),cp.forEach(n),nu=d(s),xe=a(s,"DIV",{class:!0});var mn=i(xe);w(Ar.$$.fragment,mn),JT=d(mn),$d=a(mn,"P",{});var d0=i($d);eF=o(d0,"The bare Funnel Transformer Model transformer outputting raw hidden-states without any specific head on top."),d0.forEach(n),nF=d(mn),Dr=a(mn,"P",{});var up=i(Dr);tF=o(up,"The Funnel Transformer model was proposed in "),Ir=a(up,"A",{href:!0,rel:!0});var c0=i(Ir);oF=o(c0,`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),c0.forEach(n),sF=o(up," by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),up.forEach(n),rF=d(mn),Or=a(mn,"P",{});var pp=i(Or);aF=o(pp,"This model inherits from "),Mi=a(pp,"A",{href:!0});var u0=i(Mi);iF=o(u0,"TFPreTrainedModel"),u0.forEach(n),lF=o(pp,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),pp.forEach(n),dF=d(mn),Sr=a(mn,"P",{});var hp=i(Sr);cF=o(hp,"This model is also a "),Nr=a(hp,"A",{href:!0,rel:!0});var p0=i(Nr);uF=o(p0,"tf.keras.Model"),p0.forEach(n),pF=o(hp,` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),hp.forEach(n),hF=d(mn),w(mo.$$.fragment,mn),fF=d(mn),rn=a(mn,"DIV",{class:!0});var Rn=i(rn);w(Br.$$.fragment,Rn),mF=d(Rn),Mt=a(Rn,"P",{});var ol=i(Mt);gF=o(ol,"The "),zi=a(ol,"A",{href:!0});var h0=i(zi);_F=o(h0,"TFFunnelModel"),h0.forEach(n),TF=o(ol," forward method, overrides the "),Ed=a(ol,"CODE",{});var f0=i(Ed);FF=o(f0,"__call__"),f0.forEach(n),vF=o(ol," special method."),ol.forEach(n),kF=d(Rn),w(go.$$.fragment,Rn),wF=d(Rn),Md=a(Rn,"P",{});var m0=i(Md);bF=o(m0,"Example:"),m0.forEach(n),yF=d(Rn),w(Wr.$$.fragment,Rn),Rn.forEach(n),mn.forEach(n),tu=d(s),zt=a(s,"H2",{class:!0});var fp=i(zt);_o=a(fp,"A",{id:!0,class:!0,href:!0});var g0=i(_o);zd=a(g0,"SPAN",{});var _0=i(zd);w(Qr.$$.fragment,_0),_0.forEach(n),g0.forEach(n),$F=d(fp),qd=a(fp,"SPAN",{});var T0=i(qd);EF=o(T0,"TFFunnelModelForPreTraining"),T0.forEach(n),fp.forEach(n),ou=d(s),Le=a(s,"DIV",{class:!0});var gn=i(Le);w(Rr.$$.fragment,gn),MF=d(gn),Pd=a(gn,"P",{});var F0=i(Pd);zF=o(F0,"Funnel model with a binary classification head on top as used during pretraining for identifying generated tokens."),F0.forEach(n),qF=d(gn),Hr=a(gn,"P",{});var mp=i(Hr);PF=o(mp,"The Funnel Transformer model was proposed in "),Vr=a(mp,"A",{href:!0,rel:!0});var v0=i(Vr);CF=o(v0,`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),v0.forEach(n),jF=o(mp," by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),mp.forEach(n),xF=d(gn),Yr=a(gn,"P",{});var gp=i(Yr);LF=o(gp,"This model inherits from "),qi=a(gp,"A",{href:!0});var k0=i(qi);AF=o(k0,"TFPreTrainedModel"),k0.forEach(n),DF=o(gp,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),gp.forEach(n),IF=d(gn),Ur=a(gn,"P",{});var _p=i(Ur);OF=o(_p,"This model is also a "),Gr=a(_p,"A",{href:!0,rel:!0});var w0=i(Gr);SF=o(w0,"tf.keras.Model"),w0.forEach(n),NF=o(_p,` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),_p.forEach(n),BF=d(gn),w(To.$$.fragment,gn),WF=d(gn),an=a(gn,"DIV",{class:!0});var Hn=i(an);w(Zr.$$.fragment,Hn),QF=d(Hn),qt=a(Hn,"P",{});var sl=i(qt);RF=o(sl,"The "),Pi=a(sl,"A",{href:!0});var b0=i(Pi);HF=o(b0,"TFFunnelForPreTraining"),b0.forEach(n),VF=o(sl," forward method, overrides the "),Cd=a(sl,"CODE",{});var y0=i(Cd);YF=o(y0,"__call__"),y0.forEach(n),UF=o(sl," special method."),sl.forEach(n),GF=d(Hn),w(Fo.$$.fragment,Hn),ZF=d(Hn),jd=a(Hn,"P",{});var $0=i(jd);KF=o($0,"Examples:"),$0.forEach(n),XF=d(Hn),w(Kr.$$.fragment,Hn),Hn.forEach(n),gn.forEach(n),su=d(s),Pt=a(s,"H2",{class:!0});var Tp=i(Pt);vo=a(Tp,"A",{id:!0,class:!0,href:!0});var E0=i(vo);xd=a(E0,"SPAN",{});var M0=i(xd);w(Xr.$$.fragment,M0),M0.forEach(n),E0.forEach(n),JF=d(Tp),Ld=a(Tp,"SPAN",{});var z0=i(Ld);ev=o(z0,"TFFunnelForMaskedLM"),z0.forEach(n),Tp.forEach(n),ru=d(s),Ae=a(s,"DIV",{class:!0});var _n=i(Ae);w(Jr.$$.fragment,_n),nv=d(_n),ea=a(_n,"P",{});var Fp=i(ea);tv=o(Fp,"Funnel Model with a "),Ad=a(Fp,"CODE",{});var q0=i(Ad);ov=o(q0,"language modeling"),q0.forEach(n),sv=o(Fp," head on top."),Fp.forEach(n),rv=d(_n),na=a(_n,"P",{});var vp=i(na);av=o(vp,"The Funnel Transformer model was proposed in "),ta=a(vp,"A",{href:!0,rel:!0});var P0=i(ta);iv=o(P0,`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),P0.forEach(n),lv=o(vp," by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),vp.forEach(n),dv=d(_n),oa=a(_n,"P",{});var kp=i(oa);cv=o(kp,"This model inherits from "),Ci=a(kp,"A",{href:!0});var C0=i(Ci);uv=o(C0,"TFPreTrainedModel"),C0.forEach(n),pv=o(kp,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),kp.forEach(n),hv=d(_n),sa=a(_n,"P",{});var wp=i(sa);fv=o(wp,"This model is also a "),ra=a(wp,"A",{href:!0,rel:!0});var j0=i(ra);mv=o(j0,"tf.keras.Model"),j0.forEach(n),gv=o(wp,` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),wp.forEach(n),_v=d(_n),w(ko.$$.fragment,_n),Tv=d(_n),ln=a(_n,"DIV",{class:!0});var Vn=i(ln);w(aa.$$.fragment,Vn),Fv=d(Vn),Ct=a(Vn,"P",{});var rl=i(Ct);vv=o(rl,"The "),ji=a(rl,"A",{href:!0});var x0=i(ji);kv=o(x0,"TFFunnelForMaskedLM"),x0.forEach(n),wv=o(rl," forward method, overrides the "),Dd=a(rl,"CODE",{});var L0=i(Dd);bv=o(L0,"__call__"),L0.forEach(n),yv=o(rl," special method."),rl.forEach(n),$v=d(Vn),w(wo.$$.fragment,Vn),Ev=d(Vn),Id=a(Vn,"P",{});var A0=i(Id);Mv=o(A0,"Example:"),A0.forEach(n),zv=d(Vn),w(ia.$$.fragment,Vn),Vn.forEach(n),_n.forEach(n),au=d(s),jt=a(s,"H2",{class:!0});var bp=i(jt);bo=a(bp,"A",{id:!0,class:!0,href:!0});var D0=i(bo);Od=a(D0,"SPAN",{});var I0=i(Od);w(la.$$.fragment,I0),I0.forEach(n),D0.forEach(n),qv=d(bp),Sd=a(bp,"SPAN",{});var O0=i(Sd);Pv=o(O0,"TFFunnelForSequenceClassification"),O0.forEach(n),bp.forEach(n),iu=d(s),De=a(s,"DIV",{class:!0});var Tn=i(De);w(da.$$.fragment,Tn),Cv=d(Tn),Nd=a(Tn,"P",{});var S0=i(Nd);jv=o(S0,`Funnel Model transformer with a sequence classification/regression head on top (a linear layer on top of the pooled
output) e.g. for GLUE tasks.`),S0.forEach(n),xv=d(Tn),ca=a(Tn,"P",{});var yp=i(ca);Lv=o(yp,"The Funnel Transformer model was proposed in "),ua=a(yp,"A",{href:!0,rel:!0});var N0=i(ua);Av=o(N0,`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),N0.forEach(n),Dv=o(yp," by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),yp.forEach(n),Iv=d(Tn),pa=a(Tn,"P",{});var $p=i(pa);Ov=o($p,"This model inherits from "),xi=a($p,"A",{href:!0});var B0=i(xi);Sv=o(B0,"TFPreTrainedModel"),B0.forEach(n),Nv=o($p,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),$p.forEach(n),Bv=d(Tn),ha=a(Tn,"P",{});var Ep=i(ha);Wv=o(Ep,"This model is also a "),fa=a(Ep,"A",{href:!0,rel:!0});var W0=i(fa);Qv=o(W0,"tf.keras.Model"),W0.forEach(n),Rv=o(Ep,` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),Ep.forEach(n),Hv=d(Tn),w(yo.$$.fragment,Tn),Vv=d(Tn),dn=a(Tn,"DIV",{class:!0});var Yn=i(dn);w(ma.$$.fragment,Yn),Yv=d(Yn),xt=a(Yn,"P",{});var al=i(xt);Uv=o(al,"The "),Li=a(al,"A",{href:!0});var Q0=i(Li);Gv=o(Q0,"TFFunnelForSequenceClassification"),Q0.forEach(n),Zv=o(al," forward method, overrides the "),Bd=a(al,"CODE",{});var R0=i(Bd);Kv=o(R0,"__call__"),R0.forEach(n),Xv=o(al," special method."),al.forEach(n),Jv=d(Yn),w($o.$$.fragment,Yn),ek=d(Yn),Wd=a(Yn,"P",{});var H0=i(Wd);nk=o(H0,"Example:"),H0.forEach(n),tk=d(Yn),w(ga.$$.fragment,Yn),Yn.forEach(n),Tn.forEach(n),lu=d(s),Lt=a(s,"H2",{class:!0});var Mp=i(Lt);Eo=a(Mp,"A",{id:!0,class:!0,href:!0});var V0=i(Eo);Qd=a(V0,"SPAN",{});var Y0=i(Qd);w(_a.$$.fragment,Y0),Y0.forEach(n),V0.forEach(n),ok=d(Mp),Rd=a(Mp,"SPAN",{});var U0=i(Rd);sk=o(U0,"TFFunnelForMultipleChoice"),U0.forEach(n),Mp.forEach(n),du=d(s),Ie=a(s,"DIV",{class:!0});var Fn=i(Ie);w(Ta.$$.fragment,Fn),rk=d(Fn),Hd=a(Fn,"P",{});var G0=i(Hd);ak=o(G0,`Funnel Model with a multiple choice classification head on top (a linear layer on top of the pooled output and a
softmax) e.g. for RocStories/SWAG tasks.`),G0.forEach(n),ik=d(Fn),Fa=a(Fn,"P",{});var zp=i(Fa);lk=o(zp,"The Funnel Transformer model was proposed in "),va=a(zp,"A",{href:!0,rel:!0});var Z0=i(va);dk=o(Z0,`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),Z0.forEach(n),ck=o(zp," by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),zp.forEach(n),uk=d(Fn),ka=a(Fn,"P",{});var qp=i(ka);pk=o(qp,"This model inherits from "),Ai=a(qp,"A",{href:!0});var K0=i(Ai);hk=o(K0,"TFPreTrainedModel"),K0.forEach(n),fk=o(qp,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),qp.forEach(n),mk=d(Fn),wa=a(Fn,"P",{});var Pp=i(wa);gk=o(Pp,"This model is also a "),ba=a(Pp,"A",{href:!0,rel:!0});var X0=i(ba);_k=o(X0,"tf.keras.Model"),X0.forEach(n),Tk=o(Pp,` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),Pp.forEach(n),Fk=d(Fn),w(Mo.$$.fragment,Fn),vk=d(Fn),cn=a(Fn,"DIV",{class:!0});var Un=i(cn);w(ya.$$.fragment,Un),kk=d(Un),At=a(Un,"P",{});var il=i(At);wk=o(il,"The "),Di=a(il,"A",{href:!0});var J0=i(Di);bk=o(J0,"TFFunnelForMultipleChoice"),J0.forEach(n),yk=o(il," forward method, overrides the "),Vd=a(il,"CODE",{});var ey=i(Vd);$k=o(ey,"__call__"),ey.forEach(n),Ek=o(il," special method."),il.forEach(n),Mk=d(Un),w(zo.$$.fragment,Un),zk=d(Un),Yd=a(Un,"P",{});var ny=i(Yd);qk=o(ny,"Example:"),ny.forEach(n),Pk=d(Un),w($a.$$.fragment,Un),Un.forEach(n),Fn.forEach(n),cu=d(s),Dt=a(s,"H2",{class:!0});var Cp=i(Dt);qo=a(Cp,"A",{id:!0,class:!0,href:!0});var ty=i(qo);Ud=a(ty,"SPAN",{});var oy=i(Ud);w(Ea.$$.fragment,oy),oy.forEach(n),ty.forEach(n),Ck=d(Cp),Gd=a(Cp,"SPAN",{});var sy=i(Gd);jk=o(sy,"TFFunnelForTokenClassification"),sy.forEach(n),Cp.forEach(n),uu=d(s),Oe=a(s,"DIV",{class:!0});var vn=i(Oe);w(Ma.$$.fragment,vn),xk=d(vn),Zd=a(vn,"P",{});var ry=i(Zd);Lk=o(ry,`Funnel Model with a token classification head on top (a linear layer on top of the hidden-states output) e.g. for
Named-Entity-Recognition (NER) tasks.`),ry.forEach(n),Ak=d(vn),za=a(vn,"P",{});var jp=i(za);Dk=o(jp,"The Funnel Transformer model was proposed in "),qa=a(jp,"A",{href:!0,rel:!0});var ay=i(qa);Ik=o(ay,`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),ay.forEach(n),Ok=o(jp," by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),jp.forEach(n),Sk=d(vn),Pa=a(vn,"P",{});var xp=i(Pa);Nk=o(xp,"This model inherits from "),Ii=a(xp,"A",{href:!0});var iy=i(Ii);Bk=o(iy,"TFPreTrainedModel"),iy.forEach(n),Wk=o(xp,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),xp.forEach(n),Qk=d(vn),Ca=a(vn,"P",{});var Lp=i(Ca);Rk=o(Lp,"This model is also a "),ja=a(Lp,"A",{href:!0,rel:!0});var ly=i(ja);Hk=o(ly,"tf.keras.Model"),ly.forEach(n),Vk=o(Lp,` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),Lp.forEach(n),Yk=d(vn),w(Po.$$.fragment,vn),Uk=d(vn),un=a(vn,"DIV",{class:!0});var Gn=i(un);w(xa.$$.fragment,Gn),Gk=d(Gn),It=a(Gn,"P",{});var ll=i(It);Zk=o(ll,"The "),Oi=a(ll,"A",{href:!0});var dy=i(Oi);Kk=o(dy,"TFFunnelForTokenClassification"),dy.forEach(n),Xk=o(ll," forward method, overrides the "),Kd=a(ll,"CODE",{});var cy=i(Kd);Jk=o(cy,"__call__"),cy.forEach(n),e1=o(ll," special method."),ll.forEach(n),n1=d(Gn),w(Co.$$.fragment,Gn),t1=d(Gn),Xd=a(Gn,"P",{});var uy=i(Xd);o1=o(uy,"Example:"),uy.forEach(n),s1=d(Gn),w(La.$$.fragment,Gn),Gn.forEach(n),vn.forEach(n),pu=d(s),Ot=a(s,"H2",{class:!0});var Ap=i(Ot);jo=a(Ap,"A",{id:!0,class:!0,href:!0});var py=i(jo);Jd=a(py,"SPAN",{});var hy=i(Jd);w(Aa.$$.fragment,hy),hy.forEach(n),py.forEach(n),r1=d(Ap),ec=a(Ap,"SPAN",{});var fy=i(ec);a1=o(fy,"TFFunnelForQuestionAnswering"),fy.forEach(n),Ap.forEach(n),hu=d(s),Se=a(s,"DIV",{class:!0});var kn=i(Se);w(Da.$$.fragment,kn),i1=d(kn),St=a(kn,"P",{});var dl=i(St);l1=o(dl,`Funnel Model with a span classification head on top for extractive question-answering tasks like SQuAD (a linear
layers on top of the hidden-states output to compute `),nc=a(dl,"CODE",{});var my=i(nc);d1=o(my,"span start logits"),my.forEach(n),c1=o(dl," and "),tc=a(dl,"CODE",{});var gy=i(tc);u1=o(gy,"span end logits"),gy.forEach(n),p1=o(dl,")."),dl.forEach(n),h1=d(kn),Ia=a(kn,"P",{});var Dp=i(Ia);f1=o(Dp,"The Funnel Transformer model was proposed in "),Oa=a(Dp,"A",{href:!0,rel:!0});var _y=i(Oa);m1=o(_y,`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),_y.forEach(n),g1=o(Dp," by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),Dp.forEach(n),_1=d(kn),Sa=a(kn,"P",{});var Ip=i(Sa);T1=o(Ip,"This model inherits from "),Si=a(Ip,"A",{href:!0});var Ty=i(Si);F1=o(Ty,"TFPreTrainedModel"),Ty.forEach(n),v1=o(Ip,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Ip.forEach(n),k1=d(kn),Na=a(kn,"P",{});var Op=i(Na);w1=o(Op,"This model is also a "),Ba=a(Op,"A",{href:!0,rel:!0});var Fy=i(Ba);b1=o(Fy,"tf.keras.Model"),Fy.forEach(n),y1=o(Op,` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),Op.forEach(n),$1=d(kn),w(xo.$$.fragment,kn),E1=d(kn),pn=a(kn,"DIV",{class:!0});var Zn=i(pn);w(Wa.$$.fragment,Zn),M1=d(Zn),Nt=a(Zn,"P",{});var cl=i(Nt);z1=o(cl,"The "),Ni=a(cl,"A",{href:!0});var vy=i(Ni);q1=o(vy,"TFFunnelForQuestionAnswering"),vy.forEach(n),P1=o(cl," forward method, overrides the "),oc=a(cl,"CODE",{});var ky=i(oc);C1=o(ky,"__call__"),ky.forEach(n),j1=o(cl," special method."),cl.forEach(n),x1=d(Zn),w(Lo.$$.fragment,Zn),L1=d(Zn),sc=a(Zn,"P",{});var wy=i(sc);A1=o(wy,"Example:"),wy.forEach(n),D1=d(Zn),w(Qa.$$.fragment,Zn),Zn.forEach(n),kn.forEach(n),this.h()},h(){c(p,"name","hf:doc:metadata"),c(p,"content",JSON.stringify(Xy)),c(g,"id","funnel-transformer"),c(g,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(g,"href","#funnel-transformer"),c(m,"class","relative group"),c(J,"id","overview"),c(J,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(J,"href","#overview"),c(q,"class","relative group"),c(te,"href","https://arxiv.org/abs/2006.03236"),c(te,"rel","nofollow"),c(re,"href","/docs/transformers/pr_16073/en/model_doc/funnel#transformers.FunnelModel"),c(u,"href","/docs/transformers/pr_16073/en/model_doc/funnel#transformers.FunnelForPreTraining"),c(Te,"href","/docs/transformers/pr_16073/en/model_doc/funnel#transformers.FunnelForMaskedLM"),c(Fe,"href","/docs/transformers/pr_16073/en/model_doc/funnel#transformers.FunnelForTokenClassification"),c(ve,"href","/docs/transformers/pr_16073/en/model_doc/funnel#transformers.FunnelBaseModel"),c(ke,"href","/docs/transformers/pr_16073/en/model_doc/funnel#transformers.FunnelForSequenceClassification"),c(Ua,"href","/docs/transformers/pr_16073/en/model_doc/funnel#transformers.FunnelForMultipleChoice"),c(Oo,"href","https://huggingface.co/sgugger"),c(Oo,"rel","nofollow"),c(So,"href","https://github.com/laiguokun/Funnel-Transformer"),c(So,"rel","nofollow"),c(Bt,"id","transformers.FunnelConfig"),c(Bt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Bt,"href","#transformers.FunnelConfig"),c(Kn,"class","relative group"),c(Ga,"href","/docs/transformers/pr_16073/en/model_doc/funnel#transformers.FunnelModel"),c(Za,"href","/docs/transformers/pr_16073/en/model_doc/bert#transformers.TFBertModel"),c(Wo,"href","https://huggingface.co/funnel-transformer/small"),c(Wo,"rel","nofollow"),c(Ka,"href","/docs/transformers/pr_16073/en/main_classes/configuration#transformers.PretrainedConfig"),c(Xa,"href","/docs/transformers/pr_16073/en/main_classes/configuration#transformers.PretrainedConfig"),c(Cn,"class","docstring"),c(Wt,"id","transformers.FunnelTokenizer"),c(Wt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Wt,"href","#transformers.FunnelTokenizer"),c(Jn,"class","relative group"),c(Ja,"href","/docs/transformers/pr_16073/en/model_doc/funnel#transformers.FunnelTokenizer"),c(ei,"href","/docs/transformers/pr_16073/en/model_doc/bert#transformers.BertTokenizer"),c(ni,"href","/docs/transformers/pr_16073/en/model_doc/bert#transformers.BertTokenizer"),c(Ln,"class","docstring"),c(Rt,"class","docstring"),c(wn,"class","docstring"),c(si,"class","docstring"),c(Pe,"class","docstring"),c(Ht,"id","transformers.FunnelTokenizerFast"),c(Ht,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Ht,"href","#transformers.FunnelTokenizerFast"),c(nt,"class","relative group"),c(ri,"href","/docs/transformers/pr_16073/en/model_doc/funnel#transformers.FunnelTokenizerFast"),c(ai,"href","/docs/transformers/pr_16073/en/model_doc/bert#transformers.BertTokenizerFast"),c(ii,"href","/docs/transformers/pr_16073/en/model_doc/bert#transformers.BertTokenizerFast"),c(bn,"class","docstring"),c(Ze,"class","docstring"),c(Yt,"id","transformers.models.funnel.modeling_funnel.FunnelForPreTrainingOutput"),c(Yt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Yt,"href","#transformers.models.funnel.modeling_funnel.FunnelForPreTrainingOutput"),c(ot,"class","relative group"),c(li,"href","/docs/transformers/pr_16073/en/model_doc/funnel#transformers.FunnelForPreTraining"),c(st,"class","docstring"),c(di,"href","/docs/transformers/pr_16073/en/model_doc/funnel#transformers.FunnelForPreTraining"),c(rt,"class","docstring"),c(Ut,"id","transformers.FunnelBaseModel"),c(Ut,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Ut,"href","#transformers.FunnelBaseModel"),c(at,"class","relative group"),c(hs,"href","https://arxiv.org/abs/2006.03236"),c(hs,"rel","nofollow"),c(ci,"href","/docs/transformers/pr_16073/en/main_classes/model#transformers.PreTrainedModel"),c(gs,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(gs,"rel","nofollow"),c(ui,"href","/docs/transformers/pr_16073/en/model_doc/funnel#transformers.FunnelBaseModel"),c(Ke,"class","docstring"),c(We,"class","docstring"),c(Zt,"id","transformers.FunnelModel"),c(Zt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Zt,"href","#transformers.FunnelModel"),c(lt,"class","relative group"),c(ws,"href","https://arxiv.org/abs/2006.03236"),c(ws,"rel","nofollow"),c(pi,"href","/docs/transformers/pr_16073/en/main_classes/model#transformers.PreTrainedModel"),c($s,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c($s,"rel","nofollow"),c(hi,"href","/docs/transformers/pr_16073/en/model_doc/funnel#transformers.FunnelModel"),c(Xe,"class","docstring"),c(Qe,"class","docstring"),c(Xt,"id","transformers.FunnelForPreTraining"),c(Xt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Xt,"href","#transformers.FunnelForPreTraining"),c(ct,"class","relative group"),c(fi,"href","/docs/transformers/pr_16073/en/model_doc/funnel#transformers.FunnelForPreTraining"),c(Je,"class","docstring"),c(ut,"class","docstring"),c(eo,"id","transformers.FunnelForMaskedLM"),c(eo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(eo,"href","#transformers.FunnelForMaskedLM"),c(ht,"class","relative group"),c(Ds,"href","https://arxiv.org/abs/2006.03236"),c(Ds,"rel","nofollow"),c(mi,"href","/docs/transformers/pr_16073/en/main_classes/model#transformers.PreTrainedModel"),c(Ss,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(Ss,"rel","nofollow"),c(gi,"href","/docs/transformers/pr_16073/en/model_doc/funnel#transformers.FunnelForMaskedLM"),c(en,"class","docstring"),c(Re,"class","docstring"),c(to,"id","transformers.FunnelForSequenceClassification"),c(to,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(to,"href","#transformers.FunnelForSequenceClassification"),c(mt,"class","relative group"),c(Hs,"href","https://arxiv.org/abs/2006.03236"),c(Hs,"rel","nofollow"),c(_i,"href","/docs/transformers/pr_16073/en/main_classes/model#transformers.PreTrainedModel"),c(Us,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(Us,"rel","nofollow"),c(Ti,"href","/docs/transformers/pr_16073/en/model_doc/funnel#transformers.FunnelForSequenceClassification"),c(Be,"class","docstring"),c(He,"class","docstring"),c(so,"id","transformers.FunnelForMultipleChoice"),c(so,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(so,"href","#transformers.FunnelForMultipleChoice"),c(_t,"class","relative group"),c(nr,"href","https://arxiv.org/abs/2006.03236"),c(nr,"rel","nofollow"),c(Fi,"href","/docs/transformers/pr_16073/en/main_classes/model#transformers.PreTrainedModel"),c(sr,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(sr,"rel","nofollow"),c(vi,"href","/docs/transformers/pr_16073/en/model_doc/funnel#transformers.FunnelForMultipleChoice"),c(nn,"class","docstring"),c(Ve,"class","docstring"),c(ao,"id","transformers.FunnelForTokenClassification"),c(ao,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(ao,"href","#transformers.FunnelForTokenClassification"),c(Ft,"class","relative group"),c(cr,"href","https://arxiv.org/abs/2006.03236"),c(cr,"rel","nofollow"),c(ki,"href","/docs/transformers/pr_16073/en/main_classes/model#transformers.PreTrainedModel"),c(hr,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(hr,"rel","nofollow"),c(wi,"href","/docs/transformers/pr_16073/en/model_doc/funnel#transformers.FunnelForTokenClassification"),c(tn,"class","docstring"),c(Ye,"class","docstring"),c(lo,"id","transformers.FunnelForQuestionAnswering"),c(lo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(lo,"href","#transformers.FunnelForQuestionAnswering"),c(kt,"class","relative group"),c(Fr,"href","https://arxiv.org/abs/2006.03236"),c(Fr,"rel","nofollow"),c(bi,"href","/docs/transformers/pr_16073/en/main_classes/model#transformers.PreTrainedModel"),c(wr,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(wr,"rel","nofollow"),c(yi,"href","/docs/transformers/pr_16073/en/model_doc/funnel#transformers.FunnelForQuestionAnswering"),c(on,"class","docstring"),c(Ue,"class","docstring"),c(uo,"id","transformers.TFFunnelBaseModel"),c(uo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(uo,"href","#transformers.TFFunnelBaseModel"),c(yt,"class","relative group"),c(zr,"href","https://arxiv.org/abs/2006.03236"),c(zr,"rel","nofollow"),c($i,"href","/docs/transformers/pr_16073/en/main_classes/model#transformers.TFPreTrainedModel"),c(Cr,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),c(Cr,"rel","nofollow"),c(Ei,"href","/docs/transformers/pr_16073/en/model_doc/funnel#transformers.TFFunnelBaseModel"),c(sn,"class","docstring"),c(je,"class","docstring"),c(fo,"id","transformers.TFFunnelModel"),c(fo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(fo,"href","#transformers.TFFunnelModel"),c(Et,"class","relative group"),c(Ir,"href","https://arxiv.org/abs/2006.03236"),c(Ir,"rel","nofollow"),c(Mi,"href","/docs/transformers/pr_16073/en/main_classes/model#transformers.TFPreTrainedModel"),c(Nr,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),c(Nr,"rel","nofollow"),c(zi,"href","/docs/transformers/pr_16073/en/model_doc/funnel#transformers.TFFunnelModel"),c(rn,"class","docstring"),c(xe,"class","docstring"),c(_o,"id","transformers.TFFunnelForPreTraining"),c(_o,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(_o,"href","#transformers.TFFunnelForPreTraining"),c(zt,"class","relative group"),c(Vr,"href","https://arxiv.org/abs/2006.03236"),c(Vr,"rel","nofollow"),c(qi,"href","/docs/transformers/pr_16073/en/main_classes/model#transformers.TFPreTrainedModel"),c(Gr,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),c(Gr,"rel","nofollow"),c(Pi,"href","/docs/transformers/pr_16073/en/model_doc/funnel#transformers.TFFunnelForPreTraining"),c(an,"class","docstring"),c(Le,"class","docstring"),c(vo,"id","transformers.TFFunnelForMaskedLM"),c(vo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(vo,"href","#transformers.TFFunnelForMaskedLM"),c(Pt,"class","relative group"),c(ta,"href","https://arxiv.org/abs/2006.03236"),c(ta,"rel","nofollow"),c(Ci,"href","/docs/transformers/pr_16073/en/main_classes/model#transformers.TFPreTrainedModel"),c(ra,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),c(ra,"rel","nofollow"),c(ji,"href","/docs/transformers/pr_16073/en/model_doc/funnel#transformers.TFFunnelForMaskedLM"),c(ln,"class","docstring"),c(Ae,"class","docstring"),c(bo,"id","transformers.TFFunnelForSequenceClassification"),c(bo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(bo,"href","#transformers.TFFunnelForSequenceClassification"),c(jt,"class","relative group"),c(ua,"href","https://arxiv.org/abs/2006.03236"),c(ua,"rel","nofollow"),c(xi,"href","/docs/transformers/pr_16073/en/main_classes/model#transformers.TFPreTrainedModel"),c(fa,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),c(fa,"rel","nofollow"),c(Li,"href","/docs/transformers/pr_16073/en/model_doc/funnel#transformers.TFFunnelForSequenceClassification"),c(dn,"class","docstring"),c(De,"class","docstring"),c(Eo,"id","transformers.TFFunnelForMultipleChoice"),c(Eo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Eo,"href","#transformers.TFFunnelForMultipleChoice"),c(Lt,"class","relative group"),c(va,"href","https://arxiv.org/abs/2006.03236"),c(va,"rel","nofollow"),c(Ai,"href","/docs/transformers/pr_16073/en/main_classes/model#transformers.TFPreTrainedModel"),c(ba,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),c(ba,"rel","nofollow"),c(Di,"href","/docs/transformers/pr_16073/en/model_doc/funnel#transformers.TFFunnelForMultipleChoice"),c(cn,"class","docstring"),c(Ie,"class","docstring"),c(qo,"id","transformers.TFFunnelForTokenClassification"),c(qo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(qo,"href","#transformers.TFFunnelForTokenClassification"),c(Dt,"class","relative group"),c(qa,"href","https://arxiv.org/abs/2006.03236"),c(qa,"rel","nofollow"),c(Ii,"href","/docs/transformers/pr_16073/en/main_classes/model#transformers.TFPreTrainedModel"),c(ja,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),c(ja,"rel","nofollow"),c(Oi,"href","/docs/transformers/pr_16073/en/model_doc/funnel#transformers.TFFunnelForTokenClassification"),c(un,"class","docstring"),c(Oe,"class","docstring"),c(jo,"id","transformers.TFFunnelForQuestionAnswering"),c(jo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(jo,"href","#transformers.TFFunnelForQuestionAnswering"),c(Ot,"class","relative group"),c(Oa,"href","https://arxiv.org/abs/2006.03236"),c(Oa,"rel","nofollow"),c(Si,"href","/docs/transformers/pr_16073/en/main_classes/model#transformers.TFPreTrainedModel"),c(Ba,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),c(Ba,"rel","nofollow"),c(Ni,"href","/docs/transformers/pr_16073/en/model_doc/funnel#transformers.TFFunnelForQuestionAnswering"),c(pn,"class","docstring"),c(Se,"class","docstring")},m(s,f){e(document.head,p),h(s,M,f),h(s,m,f),e(m,g),e(g,F),b(T,F,null),e(m,_),e(m,z),e(z,ce),h(s,G,f),h(s,q,f),e(q,J),e(J,I),b(ne,I,null),e(q,ue),e(q,O),e(O,pe),h(s,ie,f),h(s,U,f),e(U,L),e(U,te),e(te,Z),e(U,P),h(s,j,f),h(s,oe,f),e(oe,Q),h(s,le,f),h(s,se,f),e(se,S),e(S,he),h(s,de,f),h(s,C,f),e(C,fe),h(s,B,f),h(s,ee,f),e(ee,ae),e(ae,R),e(ee,me),e(ee,N),e(N,A),e(N,re),e(re,H),e(N,ge),e(N,u),e(u,v),e(N,K),e(N,Te),e(Te,we),e(N,D),e(N,Fe),e(Fe,be),e(N,ye),e(N,x),e(x,V),e(N,$e),e(N,ve),e(ve,Y),e(N,Ee),e(N,ke),e(ke,_e),e(N,Me),e(N,Ua),e(Ua,Sp),e(N,Np),h(s,Ec,f),h(s,xn,f),e(xn,Bp),e(xn,Oo),e(Oo,Wp),e(xn,Qp),e(xn,So),e(So,Rp),e(xn,Hp),h(s,Mc,f),h(s,Kn,f),e(Kn,Bt),e(Bt,ul),b(No,ul,null),e(Kn,Vp),e(Kn,pl),e(pl,Yp),h(s,zc,f),h(s,Cn,f),b(Bo,Cn,null),e(Cn,Up),e(Cn,jn),e(jn,Gp),e(jn,Ga),e(Ga,Zp),e(jn,Kp),e(jn,Za),e(Za,Xp),e(jn,Jp),e(jn,Wo),e(Wo,eh),e(jn,nh),e(Cn,th),e(Cn,Xn),e(Xn,oh),e(Xn,Ka),e(Ka,sh),e(Xn,rh),e(Xn,Xa),e(Xa,ah),e(Xn,ih),h(s,qc,f),h(s,Jn,f),e(Jn,Wt),e(Wt,hl),b(Qo,hl,null),e(Jn,lh),e(Jn,fl),e(fl,dh),h(s,Pc,f),h(s,Pe,f),b(Ro,Pe,null),e(Pe,ch),e(Pe,ml),e(ml,uh),e(Pe,ph),e(Pe,Qt),e(Qt,Ja),e(Ja,hh),e(Qt,fh),e(Qt,ei),e(ei,mh),e(Qt,gh),e(Pe,_h),e(Pe,Ho),e(Ho,Th),e(Ho,ni),e(ni,Fh),e(Ho,vh),e(Pe,kh),e(Pe,Ln),b(Vo,Ln,null),e(Ln,wh),e(Ln,gl),e(gl,bh),e(Ln,yh),e(Ln,Yo),e(Yo,ti),e(ti,$h),e(ti,_l),e(_l,Eh),e(Yo,Mh),e(Yo,oi),e(oi,zh),e(oi,Tl),e(Tl,qh),e(Pe,Ph),e(Pe,Rt),b(Uo,Rt,null),e(Rt,Ch),e(Rt,Go),e(Go,jh),e(Go,Fl),e(Fl,xh),e(Go,Lh),e(Pe,Ah),e(Pe,wn),b(Zo,wn,null),e(wn,Dh),e(wn,vl),e(vl,Ih),e(wn,Oh),b(Ko,wn,null),e(wn,Sh),e(wn,et),e(et,Nh),e(et,kl),e(kl,Bh),e(et,Wh),e(et,wl),e(wl,Qh),e(et,Rh),e(Pe,Hh),e(Pe,si),b(Xo,si,null),h(s,Cc,f),h(s,nt,f),e(nt,Ht),e(Ht,bl),b(Jo,bl,null),e(nt,Vh),e(nt,yl),e(yl,Yh),h(s,jc,f),h(s,Ze,f),b(es,Ze,null),e(Ze,Uh),e(Ze,ns),e(ns,Gh),e(ns,$l),e($l,Zh),e(ns,Kh),e(Ze,Xh),e(Ze,Vt),e(Vt,ri),e(ri,Jh),e(Vt,ef),e(Vt,ai),e(ai,nf),e(Vt,tf),e(Ze,of),e(Ze,ts),e(ts,sf),e(ts,ii),e(ii,rf),e(ts,af),e(Ze,lf),e(Ze,bn),b(os,bn,null),e(bn,df),e(bn,El),e(El,cf),e(bn,uf),b(ss,bn,null),e(bn,pf),e(bn,tt),e(tt,hf),e(tt,Ml),e(Ml,ff),e(tt,mf),e(tt,zl),e(zl,gf),e(tt,_f),h(s,xc,f),h(s,ot,f),e(ot,Yt),e(Yt,ql),b(rs,ql,null),e(ot,Tf),e(ot,Pl),e(Pl,Ff),h(s,Lc,f),h(s,st,f),b(as,st,null),e(st,vf),e(st,is),e(is,kf),e(is,li),e(li,wf),e(is,bf),h(s,Ac,f),h(s,rt,f),b(ls,rt,null),e(rt,yf),e(rt,ds),e(ds,$f),e(ds,di),e(di,Ef),e(ds,Mf),h(s,Dc,f),h(s,at,f),e(at,Ut),e(Ut,Cl),b(cs,Cl,null),e(at,zf),e(at,jl),e(jl,qf),h(s,Ic,f),h(s,We,f),b(us,We,null),e(We,Pf),e(We,xl),e(xl,Cf),e(We,jf),e(We,ps),e(ps,xf),e(ps,hs),e(hs,Lf),e(ps,Af),e(We,Df),e(We,fs),e(fs,If),e(fs,ci),e(ci,Of),e(fs,Sf),e(We,Nf),e(We,ms),e(ms,Bf),e(ms,gs),e(gs,Wf),e(ms,Qf),e(We,Rf),e(We,Ke),b(_s,Ke,null),e(Ke,Hf),e(Ke,it),e(it,Vf),e(it,ui),e(ui,Yf),e(it,Uf),e(it,Ll),e(Ll,Gf),e(it,Zf),e(Ke,Kf),b(Gt,Ke,null),e(Ke,Xf),e(Ke,Al),e(Al,Jf),e(Ke,em),b(Ts,Ke,null),h(s,Oc,f),h(s,lt,f),e(lt,Zt),e(Zt,Dl),b(Fs,Dl,null),e(lt,nm),e(lt,Il),e(Il,tm),h(s,Sc,f),h(s,Qe,f),b(vs,Qe,null),e(Qe,om),e(Qe,Ol),e(Ol,sm),e(Qe,rm),e(Qe,ks),e(ks,am),e(ks,ws),e(ws,im),e(ks,lm),e(Qe,dm),e(Qe,bs),e(bs,cm),e(bs,pi),e(pi,um),e(bs,pm),e(Qe,hm),e(Qe,ys),e(ys,fm),e(ys,$s),e($s,mm),e(ys,gm),e(Qe,_m),e(Qe,Xe),b(Es,Xe,null),e(Xe,Tm),e(Xe,dt),e(dt,Fm),e(dt,hi),e(hi,vm),e(dt,km),e(dt,Sl),e(Sl,wm),e(dt,bm),e(Xe,ym),b(Kt,Xe,null),e(Xe,$m),e(Xe,Nl),e(Nl,Em),e(Xe,Mm),b(Ms,Xe,null),h(s,Nc,f),h(s,ct,f),e(ct,Xt),e(Xt,Bl),b(zs,Bl,null),e(ct,zm),e(ct,Wl),e(Wl,qm),h(s,Bc,f),h(s,ut,f),b(qs,ut,null),e(ut,Pm),e(ut,Je),b(Ps,Je,null),e(Je,Cm),e(Je,pt),e(pt,jm),e(pt,fi),e(fi,xm),e(pt,Lm),e(pt,Ql),e(Ql,Am),e(pt,Dm),e(Je,Im),b(Jt,Je,null),e(Je,Om),e(Je,Rl),e(Rl,Sm),e(Je,Nm),b(Cs,Je,null),h(s,Wc,f),h(s,ht,f),e(ht,eo),e(eo,Hl),b(js,Hl,null),e(ht,Bm),e(ht,Vl),e(Vl,Wm),h(s,Qc,f),h(s,Re,f),b(xs,Re,null),e(Re,Qm),e(Re,Ls),e(Ls,Rm),e(Ls,Yl),e(Yl,Hm),e(Ls,Vm),e(Re,Ym),e(Re,As),e(As,Um),e(As,Ds),e(Ds,Gm),e(As,Zm),e(Re,Km),e(Re,Is),e(Is,Xm),e(Is,mi),e(mi,Jm),e(Is,eg),e(Re,ng),e(Re,Os),e(Os,tg),e(Os,Ss),e(Ss,og),e(Os,sg),e(Re,rg),e(Re,en),b(Ns,en,null),e(en,ag),e(en,ft),e(ft,ig),e(ft,gi),e(gi,lg),e(ft,dg),e(ft,Ul),e(Ul,cg),e(ft,ug),e(en,pg),b(no,en,null),e(en,hg),e(en,Gl),e(Gl,fg),e(en,mg),b(Bs,en,null),h(s,Rc,f),h(s,mt,f),e(mt,to),e(to,Zl),b(Ws,Zl,null),e(mt,gg),e(mt,Kl),e(Kl,_g),h(s,Hc,f),h(s,He,f),b(Qs,He,null),e(He,Tg),e(He,Xl),e(Xl,Fg),e(He,vg),e(He,Rs),e(Rs,kg),e(Rs,Hs),e(Hs,wg),e(Rs,bg),e(He,yg),e(He,Vs),e(Vs,$g),e(Vs,_i),e(_i,Eg),e(Vs,Mg),e(He,zg),e(He,Ys),e(Ys,qg),e(Ys,Us),e(Us,Pg),e(Ys,Cg),e(He,jg),e(He,Be),b(Gs,Be,null),e(Be,xg),e(Be,gt),e(gt,Lg),e(gt,Ti),e(Ti,Ag),e(gt,Dg),e(gt,Jl),e(Jl,Ig),e(gt,Og),e(Be,Sg),b(oo,Be,null),e(Be,Ng),e(Be,ed),e(ed,Bg),e(Be,Wg),b(Zs,Be,null),e(Be,Qg),e(Be,nd),e(nd,Rg),e(Be,Hg),b(Ks,Be,null),h(s,Vc,f),h(s,_t,f),e(_t,so),e(so,td),b(Xs,td,null),e(_t,Vg),e(_t,od),e(od,Yg),h(s,Yc,f),h(s,Ve,f),b(Js,Ve,null),e(Ve,Ug),e(Ve,sd),e(sd,Gg),e(Ve,Zg),e(Ve,er),e(er,Kg),e(er,nr),e(nr,Xg),e(er,Jg),e(Ve,e_),e(Ve,tr),e(tr,n_),e(tr,Fi),e(Fi,t_),e(tr,o_),e(Ve,s_),e(Ve,or),e(or,r_),e(or,sr),e(sr,a_),e(or,i_),e(Ve,l_),e(Ve,nn),b(rr,nn,null),e(nn,d_),e(nn,Tt),e(Tt,c_),e(Tt,vi),e(vi,u_),e(Tt,p_),e(Tt,rd),e(rd,h_),e(Tt,f_),e(nn,m_),b(ro,nn,null),e(nn,g_),e(nn,ad),e(ad,__),e(nn,T_),b(ar,nn,null),h(s,Uc,f),h(s,Ft,f),e(Ft,ao),e(ao,id),b(ir,id,null),e(Ft,F_),e(Ft,ld),e(ld,v_),h(s,Gc,f),h(s,Ye,f),b(lr,Ye,null),e(Ye,k_),e(Ye,dd),e(dd,w_),e(Ye,b_),e(Ye,dr),e(dr,y_),e(dr,cr),e(cr,$_),e(dr,E_),e(Ye,M_),e(Ye,ur),e(ur,z_),e(ur,ki),e(ki,q_),e(ur,P_),e(Ye,C_),e(Ye,pr),e(pr,j_),e(pr,hr),e(hr,x_),e(pr,L_),e(Ye,A_),e(Ye,tn),b(fr,tn,null),e(tn,D_),e(tn,vt),e(vt,I_),e(vt,wi),e(wi,O_),e(vt,S_),e(vt,cd),e(cd,N_),e(vt,B_),e(tn,W_),b(io,tn,null),e(tn,Q_),e(tn,ud),e(ud,R_),e(tn,H_),b(mr,tn,null),h(s,Zc,f),h(s,kt,f),e(kt,lo),e(lo,pd),b(gr,pd,null),e(kt,V_),e(kt,hd),e(hd,Y_),h(s,Kc,f),h(s,Ue,f),b(_r,Ue,null),e(Ue,U_),e(Ue,wt),e(wt,G_),e(wt,fd),e(fd,Z_),e(wt,K_),e(wt,md),e(md,X_),e(wt,J_),e(Ue,eT),e(Ue,Tr),e(Tr,nT),e(Tr,Fr),e(Fr,tT),e(Tr,oT),e(Ue,sT),e(Ue,vr),e(vr,rT),e(vr,bi),e(bi,aT),e(vr,iT),e(Ue,lT),e(Ue,kr),e(kr,dT),e(kr,wr),e(wr,cT),e(kr,uT),e(Ue,pT),e(Ue,on),b(br,on,null),e(on,hT),e(on,bt),e(bt,fT),e(bt,yi),e(yi,mT),e(bt,gT),e(bt,gd),e(gd,_T),e(bt,TT),e(on,FT),b(co,on,null),e(on,vT),e(on,_d),e(_d,kT),e(on,wT),b(yr,on,null),h(s,Xc,f),h(s,yt,f),e(yt,uo),e(uo,Td),b($r,Td,null),e(yt,bT),e(yt,Fd),e(Fd,yT),h(s,Jc,f),h(s,je,f),b(Er,je,null),e(je,$T),e(je,vd),e(vd,ET),e(je,MT),e(je,Mr),e(Mr,zT),e(Mr,zr),e(zr,qT),e(Mr,PT),e(je,CT),e(je,qr),e(qr,jT),e(qr,$i),e($i,xT),e(qr,LT),e(je,AT),e(je,Pr),e(Pr,DT),e(Pr,Cr),e(Cr,IT),e(Pr,OT),e(je,ST),b(po,je,null),e(je,NT),e(je,sn),b(jr,sn,null),e(sn,BT),e(sn,$t),e($t,WT),e($t,Ei),e(Ei,QT),e($t,RT),e($t,kd),e(kd,HT),e($t,VT),e(sn,YT),b(ho,sn,null),e(sn,UT),e(sn,wd),e(wd,GT),e(sn,ZT),b(xr,sn,null),h(s,eu,f),h(s,Et,f),e(Et,fo),e(fo,bd),b(Lr,bd,null),e(Et,KT),e(Et,yd),e(yd,XT),h(s,nu,f),h(s,xe,f),b(Ar,xe,null),e(xe,JT),e(xe,$d),e($d,eF),e(xe,nF),e(xe,Dr),e(Dr,tF),e(Dr,Ir),e(Ir,oF),e(Dr,sF),e(xe,rF),e(xe,Or),e(Or,aF),e(Or,Mi),e(Mi,iF),e(Or,lF),e(xe,dF),e(xe,Sr),e(Sr,cF),e(Sr,Nr),e(Nr,uF),e(Sr,pF),e(xe,hF),b(mo,xe,null),e(xe,fF),e(xe,rn),b(Br,rn,null),e(rn,mF),e(rn,Mt),e(Mt,gF),e(Mt,zi),e(zi,_F),e(Mt,TF),e(Mt,Ed),e(Ed,FF),e(Mt,vF),e(rn,kF),b(go,rn,null),e(rn,wF),e(rn,Md),e(Md,bF),e(rn,yF),b(Wr,rn,null),h(s,tu,f),h(s,zt,f),e(zt,_o),e(_o,zd),b(Qr,zd,null),e(zt,$F),e(zt,qd),e(qd,EF),h(s,ou,f),h(s,Le,f),b(Rr,Le,null),e(Le,MF),e(Le,Pd),e(Pd,zF),e(Le,qF),e(Le,Hr),e(Hr,PF),e(Hr,Vr),e(Vr,CF),e(Hr,jF),e(Le,xF),e(Le,Yr),e(Yr,LF),e(Yr,qi),e(qi,AF),e(Yr,DF),e(Le,IF),e(Le,Ur),e(Ur,OF),e(Ur,Gr),e(Gr,SF),e(Ur,NF),e(Le,BF),b(To,Le,null),e(Le,WF),e(Le,an),b(Zr,an,null),e(an,QF),e(an,qt),e(qt,RF),e(qt,Pi),e(Pi,HF),e(qt,VF),e(qt,Cd),e(Cd,YF),e(qt,UF),e(an,GF),b(Fo,an,null),e(an,ZF),e(an,jd),e(jd,KF),e(an,XF),b(Kr,an,null),h(s,su,f),h(s,Pt,f),e(Pt,vo),e(vo,xd),b(Xr,xd,null),e(Pt,JF),e(Pt,Ld),e(Ld,ev),h(s,ru,f),h(s,Ae,f),b(Jr,Ae,null),e(Ae,nv),e(Ae,ea),e(ea,tv),e(ea,Ad),e(Ad,ov),e(ea,sv),e(Ae,rv),e(Ae,na),e(na,av),e(na,ta),e(ta,iv),e(na,lv),e(Ae,dv),e(Ae,oa),e(oa,cv),e(oa,Ci),e(Ci,uv),e(oa,pv),e(Ae,hv),e(Ae,sa),e(sa,fv),e(sa,ra),e(ra,mv),e(sa,gv),e(Ae,_v),b(ko,Ae,null),e(Ae,Tv),e(Ae,ln),b(aa,ln,null),e(ln,Fv),e(ln,Ct),e(Ct,vv),e(Ct,ji),e(ji,kv),e(Ct,wv),e(Ct,Dd),e(Dd,bv),e(Ct,yv),e(ln,$v),b(wo,ln,null),e(ln,Ev),e(ln,Id),e(Id,Mv),e(ln,zv),b(ia,ln,null),h(s,au,f),h(s,jt,f),e(jt,bo),e(bo,Od),b(la,Od,null),e(jt,qv),e(jt,Sd),e(Sd,Pv),h(s,iu,f),h(s,De,f),b(da,De,null),e(De,Cv),e(De,Nd),e(Nd,jv),e(De,xv),e(De,ca),e(ca,Lv),e(ca,ua),e(ua,Av),e(ca,Dv),e(De,Iv),e(De,pa),e(pa,Ov),e(pa,xi),e(xi,Sv),e(pa,Nv),e(De,Bv),e(De,ha),e(ha,Wv),e(ha,fa),e(fa,Qv),e(ha,Rv),e(De,Hv),b(yo,De,null),e(De,Vv),e(De,dn),b(ma,dn,null),e(dn,Yv),e(dn,xt),e(xt,Uv),e(xt,Li),e(Li,Gv),e(xt,Zv),e(xt,Bd),e(Bd,Kv),e(xt,Xv),e(dn,Jv),b($o,dn,null),e(dn,ek),e(dn,Wd),e(Wd,nk),e(dn,tk),b(ga,dn,null),h(s,lu,f),h(s,Lt,f),e(Lt,Eo),e(Eo,Qd),b(_a,Qd,null),e(Lt,ok),e(Lt,Rd),e(Rd,sk),h(s,du,f),h(s,Ie,f),b(Ta,Ie,null),e(Ie,rk),e(Ie,Hd),e(Hd,ak),e(Ie,ik),e(Ie,Fa),e(Fa,lk),e(Fa,va),e(va,dk),e(Fa,ck),e(Ie,uk),e(Ie,ka),e(ka,pk),e(ka,Ai),e(Ai,hk),e(ka,fk),e(Ie,mk),e(Ie,wa),e(wa,gk),e(wa,ba),e(ba,_k),e(wa,Tk),e(Ie,Fk),b(Mo,Ie,null),e(Ie,vk),e(Ie,cn),b(ya,cn,null),e(cn,kk),e(cn,At),e(At,wk),e(At,Di),e(Di,bk),e(At,yk),e(At,Vd),e(Vd,$k),e(At,Ek),e(cn,Mk),b(zo,cn,null),e(cn,zk),e(cn,Yd),e(Yd,qk),e(cn,Pk),b($a,cn,null),h(s,cu,f),h(s,Dt,f),e(Dt,qo),e(qo,Ud),b(Ea,Ud,null),e(Dt,Ck),e(Dt,Gd),e(Gd,jk),h(s,uu,f),h(s,Oe,f),b(Ma,Oe,null),e(Oe,xk),e(Oe,Zd),e(Zd,Lk),e(Oe,Ak),e(Oe,za),e(za,Dk),e(za,qa),e(qa,Ik),e(za,Ok),e(Oe,Sk),e(Oe,Pa),e(Pa,Nk),e(Pa,Ii),e(Ii,Bk),e(Pa,Wk),e(Oe,Qk),e(Oe,Ca),e(Ca,Rk),e(Ca,ja),e(ja,Hk),e(Ca,Vk),e(Oe,Yk),b(Po,Oe,null),e(Oe,Uk),e(Oe,un),b(xa,un,null),e(un,Gk),e(un,It),e(It,Zk),e(It,Oi),e(Oi,Kk),e(It,Xk),e(It,Kd),e(Kd,Jk),e(It,e1),e(un,n1),b(Co,un,null),e(un,t1),e(un,Xd),e(Xd,o1),e(un,s1),b(La,un,null),h(s,pu,f),h(s,Ot,f),e(Ot,jo),e(jo,Jd),b(Aa,Jd,null),e(Ot,r1),e(Ot,ec),e(ec,a1),h(s,hu,f),h(s,Se,f),b(Da,Se,null),e(Se,i1),e(Se,St),e(St,l1),e(St,nc),e(nc,d1),e(St,c1),e(St,tc),e(tc,u1),e(St,p1),e(Se,h1),e(Se,Ia),e(Ia,f1),e(Ia,Oa),e(Oa,m1),e(Ia,g1),e(Se,_1),e(Se,Sa),e(Sa,T1),e(Sa,Si),e(Si,F1),e(Sa,v1),e(Se,k1),e(Se,Na),e(Na,w1),e(Na,Ba),e(Ba,b1),e(Na,y1),e(Se,$1),b(xo,Se,null),e(Se,E1),e(Se,pn),b(Wa,pn,null),e(pn,M1),e(pn,Nt),e(Nt,z1),e(Nt,Ni),e(Ni,q1),e(Nt,P1),e(Nt,oc),e(oc,C1),e(Nt,j1),e(pn,x1),b(Lo,pn,null),e(pn,L1),e(pn,sc),e(sc,A1),e(pn,D1),b(Qa,pn,null),fu=!0},p(s,[f]){const Ra={};f&2&&(Ra.$$scope={dirty:f,ctx:s}),Gt.$set(Ra);const rc={};f&2&&(rc.$$scope={dirty:f,ctx:s}),Kt.$set(rc);const ac={};f&2&&(ac.$$scope={dirty:f,ctx:s}),Jt.$set(ac);const ic={};f&2&&(ic.$$scope={dirty:f,ctx:s}),no.$set(ic);const Ha={};f&2&&(Ha.$$scope={dirty:f,ctx:s}),oo.$set(Ha);const lc={};f&2&&(lc.$$scope={dirty:f,ctx:s}),ro.$set(lc);const dc={};f&2&&(dc.$$scope={dirty:f,ctx:s}),io.$set(dc);const cc={};f&2&&(cc.$$scope={dirty:f,ctx:s}),co.$set(cc);const Va={};f&2&&(Va.$$scope={dirty:f,ctx:s}),po.$set(Va);const uc={};f&2&&(uc.$$scope={dirty:f,ctx:s}),ho.$set(uc);const pc={};f&2&&(pc.$$scope={dirty:f,ctx:s}),mo.$set(pc);const hc={};f&2&&(hc.$$scope={dirty:f,ctx:s}),go.$set(hc);const fc={};f&2&&(fc.$$scope={dirty:f,ctx:s}),To.$set(fc);const mc={};f&2&&(mc.$$scope={dirty:f,ctx:s}),Fo.$set(mc);const Ya={};f&2&&(Ya.$$scope={dirty:f,ctx:s}),ko.$set(Ya);const gc={};f&2&&(gc.$$scope={dirty:f,ctx:s}),wo.$set(gc);const Ce={};f&2&&(Ce.$$scope={dirty:f,ctx:s}),yo.$set(Ce);const _c={};f&2&&(_c.$$scope={dirty:f,ctx:s}),$o.$set(_c);const Tc={};f&2&&(Tc.$$scope={dirty:f,ctx:s}),Mo.$set(Tc);const Fc={};f&2&&(Fc.$$scope={dirty:f,ctx:s}),zo.$set(Fc);const vc={};f&2&&(vc.$$scope={dirty:f,ctx:s}),Po.$set(vc);const kc={};f&2&&(kc.$$scope={dirty:f,ctx:s}),Co.$set(kc);const wc={};f&2&&(wc.$$scope={dirty:f,ctx:s}),xo.$set(wc);const bc={};f&2&&(bc.$$scope={dirty:f,ctx:s}),Lo.$set(bc)},i(s){fu||(y(T.$$.fragment,s),y(ne.$$.fragment,s),y(No.$$.fragment,s),y(Bo.$$.fragment,s),y(Qo.$$.fragment,s),y(Ro.$$.fragment,s),y(Vo.$$.fragment,s),y(Uo.$$.fragment,s),y(Zo.$$.fragment,s),y(Ko.$$.fragment,s),y(Xo.$$.fragment,s),y(Jo.$$.fragment,s),y(es.$$.fragment,s),y(os.$$.fragment,s),y(ss.$$.fragment,s),y(rs.$$.fragment,s),y(as.$$.fragment,s),y(ls.$$.fragment,s),y(cs.$$.fragment,s),y(us.$$.fragment,s),y(_s.$$.fragment,s),y(Gt.$$.fragment,s),y(Ts.$$.fragment,s),y(Fs.$$.fragment,s),y(vs.$$.fragment,s),y(Es.$$.fragment,s),y(Kt.$$.fragment,s),y(Ms.$$.fragment,s),y(zs.$$.fragment,s),y(qs.$$.fragment,s),y(Ps.$$.fragment,s),y(Jt.$$.fragment,s),y(Cs.$$.fragment,s),y(js.$$.fragment,s),y(xs.$$.fragment,s),y(Ns.$$.fragment,s),y(no.$$.fragment,s),y(Bs.$$.fragment,s),y(Ws.$$.fragment,s),y(Qs.$$.fragment,s),y(Gs.$$.fragment,s),y(oo.$$.fragment,s),y(Zs.$$.fragment,s),y(Ks.$$.fragment,s),y(Xs.$$.fragment,s),y(Js.$$.fragment,s),y(rr.$$.fragment,s),y(ro.$$.fragment,s),y(ar.$$.fragment,s),y(ir.$$.fragment,s),y(lr.$$.fragment,s),y(fr.$$.fragment,s),y(io.$$.fragment,s),y(mr.$$.fragment,s),y(gr.$$.fragment,s),y(_r.$$.fragment,s),y(br.$$.fragment,s),y(co.$$.fragment,s),y(yr.$$.fragment,s),y($r.$$.fragment,s),y(Er.$$.fragment,s),y(po.$$.fragment,s),y(jr.$$.fragment,s),y(ho.$$.fragment,s),y(xr.$$.fragment,s),y(Lr.$$.fragment,s),y(Ar.$$.fragment,s),y(mo.$$.fragment,s),y(Br.$$.fragment,s),y(go.$$.fragment,s),y(Wr.$$.fragment,s),y(Qr.$$.fragment,s),y(Rr.$$.fragment,s),y(To.$$.fragment,s),y(Zr.$$.fragment,s),y(Fo.$$.fragment,s),y(Kr.$$.fragment,s),y(Xr.$$.fragment,s),y(Jr.$$.fragment,s),y(ko.$$.fragment,s),y(aa.$$.fragment,s),y(wo.$$.fragment,s),y(ia.$$.fragment,s),y(la.$$.fragment,s),y(da.$$.fragment,s),y(yo.$$.fragment,s),y(ma.$$.fragment,s),y($o.$$.fragment,s),y(ga.$$.fragment,s),y(_a.$$.fragment,s),y(Ta.$$.fragment,s),y(Mo.$$.fragment,s),y(ya.$$.fragment,s),y(zo.$$.fragment,s),y($a.$$.fragment,s),y(Ea.$$.fragment,s),y(Ma.$$.fragment,s),y(Po.$$.fragment,s),y(xa.$$.fragment,s),y(Co.$$.fragment,s),y(La.$$.fragment,s),y(Aa.$$.fragment,s),y(Da.$$.fragment,s),y(xo.$$.fragment,s),y(Wa.$$.fragment,s),y(Lo.$$.fragment,s),y(Qa.$$.fragment,s),fu=!0)},o(s){$(T.$$.fragment,s),$(ne.$$.fragment,s),$(No.$$.fragment,s),$(Bo.$$.fragment,s),$(Qo.$$.fragment,s),$(Ro.$$.fragment,s),$(Vo.$$.fragment,s),$(Uo.$$.fragment,s),$(Zo.$$.fragment,s),$(Ko.$$.fragment,s),$(Xo.$$.fragment,s),$(Jo.$$.fragment,s),$(es.$$.fragment,s),$(os.$$.fragment,s),$(ss.$$.fragment,s),$(rs.$$.fragment,s),$(as.$$.fragment,s),$(ls.$$.fragment,s),$(cs.$$.fragment,s),$(us.$$.fragment,s),$(_s.$$.fragment,s),$(Gt.$$.fragment,s),$(Ts.$$.fragment,s),$(Fs.$$.fragment,s),$(vs.$$.fragment,s),$(Es.$$.fragment,s),$(Kt.$$.fragment,s),$(Ms.$$.fragment,s),$(zs.$$.fragment,s),$(qs.$$.fragment,s),$(Ps.$$.fragment,s),$(Jt.$$.fragment,s),$(Cs.$$.fragment,s),$(js.$$.fragment,s),$(xs.$$.fragment,s),$(Ns.$$.fragment,s),$(no.$$.fragment,s),$(Bs.$$.fragment,s),$(Ws.$$.fragment,s),$(Qs.$$.fragment,s),$(Gs.$$.fragment,s),$(oo.$$.fragment,s),$(Zs.$$.fragment,s),$(Ks.$$.fragment,s),$(Xs.$$.fragment,s),$(Js.$$.fragment,s),$(rr.$$.fragment,s),$(ro.$$.fragment,s),$(ar.$$.fragment,s),$(ir.$$.fragment,s),$(lr.$$.fragment,s),$(fr.$$.fragment,s),$(io.$$.fragment,s),$(mr.$$.fragment,s),$(gr.$$.fragment,s),$(_r.$$.fragment,s),$(br.$$.fragment,s),$(co.$$.fragment,s),$(yr.$$.fragment,s),$($r.$$.fragment,s),$(Er.$$.fragment,s),$(po.$$.fragment,s),$(jr.$$.fragment,s),$(ho.$$.fragment,s),$(xr.$$.fragment,s),$(Lr.$$.fragment,s),$(Ar.$$.fragment,s),$(mo.$$.fragment,s),$(Br.$$.fragment,s),$(go.$$.fragment,s),$(Wr.$$.fragment,s),$(Qr.$$.fragment,s),$(Rr.$$.fragment,s),$(To.$$.fragment,s),$(Zr.$$.fragment,s),$(Fo.$$.fragment,s),$(Kr.$$.fragment,s),$(Xr.$$.fragment,s),$(Jr.$$.fragment,s),$(ko.$$.fragment,s),$(aa.$$.fragment,s),$(wo.$$.fragment,s),$(ia.$$.fragment,s),$(la.$$.fragment,s),$(da.$$.fragment,s),$(yo.$$.fragment,s),$(ma.$$.fragment,s),$($o.$$.fragment,s),$(ga.$$.fragment,s),$(_a.$$.fragment,s),$(Ta.$$.fragment,s),$(Mo.$$.fragment,s),$(ya.$$.fragment,s),$(zo.$$.fragment,s),$($a.$$.fragment,s),$(Ea.$$.fragment,s),$(Ma.$$.fragment,s),$(Po.$$.fragment,s),$(xa.$$.fragment,s),$(Co.$$.fragment,s),$(La.$$.fragment,s),$(Aa.$$.fragment,s),$(Da.$$.fragment,s),$(xo.$$.fragment,s),$(Wa.$$.fragment,s),$(Lo.$$.fragment,s),$(Qa.$$.fragment,s),fu=!1},d(s){n(p),s&&n(M),s&&n(m),E(T),s&&n(G),s&&n(q),E(ne),s&&n(ie),s&&n(U),s&&n(j),s&&n(oe),s&&n(le),s&&n(se),s&&n(de),s&&n(C),s&&n(B),s&&n(ee),s&&n(Ec),s&&n(xn),s&&n(Mc),s&&n(Kn),E(No),s&&n(zc),s&&n(Cn),E(Bo),s&&n(qc),s&&n(Jn),E(Qo),s&&n(Pc),s&&n(Pe),E(Ro),E(Vo),E(Uo),E(Zo),E(Ko),E(Xo),s&&n(Cc),s&&n(nt),E(Jo),s&&n(jc),s&&n(Ze),E(es),E(os),E(ss),s&&n(xc),s&&n(ot),E(rs),s&&n(Lc),s&&n(st),E(as),s&&n(Ac),s&&n(rt),E(ls),s&&n(Dc),s&&n(at),E(cs),s&&n(Ic),s&&n(We),E(us),E(_s),E(Gt),E(Ts),s&&n(Oc),s&&n(lt),E(Fs),s&&n(Sc),s&&n(Qe),E(vs),E(Es),E(Kt),E(Ms),s&&n(Nc),s&&n(ct),E(zs),s&&n(Bc),s&&n(ut),E(qs),E(Ps),E(Jt),E(Cs),s&&n(Wc),s&&n(ht),E(js),s&&n(Qc),s&&n(Re),E(xs),E(Ns),E(no),E(Bs),s&&n(Rc),s&&n(mt),E(Ws),s&&n(Hc),s&&n(He),E(Qs),E(Gs),E(oo),E(Zs),E(Ks),s&&n(Vc),s&&n(_t),E(Xs),s&&n(Yc),s&&n(Ve),E(Js),E(rr),E(ro),E(ar),s&&n(Uc),s&&n(Ft),E(ir),s&&n(Gc),s&&n(Ye),E(lr),E(fr),E(io),E(mr),s&&n(Zc),s&&n(kt),E(gr),s&&n(Kc),s&&n(Ue),E(_r),E(br),E(co),E(yr),s&&n(Xc),s&&n(yt),E($r),s&&n(Jc),s&&n(je),E(Er),E(po),E(jr),E(ho),E(xr),s&&n(eu),s&&n(Et),E(Lr),s&&n(nu),s&&n(xe),E(Ar),E(mo),E(Br),E(go),E(Wr),s&&n(tu),s&&n(zt),E(Qr),s&&n(ou),s&&n(Le),E(Rr),E(To),E(Zr),E(Fo),E(Kr),s&&n(su),s&&n(Pt),E(Xr),s&&n(ru),s&&n(Ae),E(Jr),E(ko),E(aa),E(wo),E(ia),s&&n(au),s&&n(jt),E(la),s&&n(iu),s&&n(De),E(da),E(yo),E(ma),E($o),E(ga),s&&n(lu),s&&n(Lt),E(_a),s&&n(du),s&&n(Ie),E(Ta),E(Mo),E(ya),E(zo),E($a),s&&n(cu),s&&n(Dt),E(Ea),s&&n(uu),s&&n(Oe),E(Ma),E(Po),E(xa),E(Co),E(La),s&&n(pu),s&&n(Ot),E(Aa),s&&n(hu),s&&n(Se),E(Da),E(xo),E(Wa),E(Lo),E(Qa)}}}const Xy={local:"funnel-transformer",sections:[{local:"overview",title:"Overview"},{local:"transformers.FunnelConfig",title:"FunnelConfig"},{local:"transformers.FunnelTokenizer",title:"FunnelTokenizer"},{local:"transformers.FunnelTokenizerFast",title:"FunnelTokenizerFast"},{local:"transformers.models.funnel.modeling_funnel.FunnelForPreTrainingOutput",title:"Funnel specific outputs"},{local:"transformers.FunnelBaseModel",title:"FunnelBaseModel"},{local:"transformers.FunnelModel",title:"FunnelModel"},{local:"transformers.FunnelForPreTraining",title:"FunnelModelForPreTraining"},{local:"transformers.FunnelForMaskedLM",title:"FunnelForMaskedLM"},{local:"transformers.FunnelForSequenceClassification",title:"FunnelForSequenceClassification"},{local:"transformers.FunnelForMultipleChoice",title:"FunnelForMultipleChoice"},{local:"transformers.FunnelForTokenClassification",title:"FunnelForTokenClassification"},{local:"transformers.FunnelForQuestionAnswering",title:"FunnelForQuestionAnswering"},{local:"transformers.TFFunnelBaseModel",title:"TFFunnelBaseModel"},{local:"transformers.TFFunnelModel",title:"TFFunnelModel"},{local:"transformers.TFFunnelForPreTraining",title:"TFFunnelModelForPreTraining"},{local:"transformers.TFFunnelForMaskedLM",title:"TFFunnelForMaskedLM"},{local:"transformers.TFFunnelForSequenceClassification",title:"TFFunnelForSequenceClassification"},{local:"transformers.TFFunnelForMultipleChoice",title:"TFFunnelForMultipleChoice"},{local:"transformers.TFFunnelForTokenClassification",title:"TFFunnelForTokenClassification"},{local:"transformers.TFFunnelForQuestionAnswering",title:"TFFunnelForQuestionAnswering"}],title:"Funnel Transformer"};function Jy(W,p,M){let{fw:m}=p;return W.$$set=g=>{"fw"in g&&M(0,m=g.fw)},[m]}class a$ extends by{constructor(p){super();yy(this,p,Jy,Ky,$y,{fw:0})}}export{a$ as default,Xy as metadata};
