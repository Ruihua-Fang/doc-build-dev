import{D as Yp,S as Ge,i as Be,s as Je,O as I,P as N,a as n,d as r,b as d,g as $,F as s,L as le,t as i,h as p,e as u,w as x,k as g,c as f,x as S,m as v,y as M,Q as oi,q as j,o as q,B as P,n as ya,p as Aa,Y as Zp,v as Jp,Z as oo,X as Qp,V as Xp,H as Ds,I as Is,J as Ns,K as Fs,M as ec}from"../chunks/vendor-c570b7f7.js";import{T as ja}from"../chunks/Tip-4965f0b6.js";import{Y as Hp}from"../chunks/Youtube-ea859fc9.js";import{I as bt,C as J}from"../chunks/CodeBlock-8a2530c2.js";import{D as tc}from"../chunks/DocNotebookDropdown-e51be72e.js";var Ne=(m=>(m.OPEN="OPEN",m.CLOSED="CLOSED",m.HASHASHLINK="HASHASHLINK",m))(Ne||{});const ri={};function ac(m){return ri[m]||(ri[m]=Yp("OPEN")),ri[m]}function sc(m){let e,l,a,o,c,_;return{c(){e=I("svg"),l=I("defs"),a=I("clipPath"),o=I("rect"),c=I("g"),_=I("path"),this.h()},l(b){e=N(b,"svg",{class:!0,xmlns:!0,"xmlns:xlink":!0,"aria-hidden":!0,focusable:!0,role:!0,width:!0,height:!0,preserveAspectRatio:!0,viewBox:!0});var E=n(e);l=N(E,"defs",{});var A=n(l);a=N(A,"clipPath",{id:!0});var y=n(a);o=N(y,"rect",{x:!0,y:!0,width:!0,height:!0,fill:!0}),n(o).forEach(r),y.forEach(r),A.forEach(r),c=N(E,"g",{"clip-path":!0});var T=n(c);_=N(T,"path",{d:!0,fill:!0}),n(_).forEach(r),T.forEach(r),E.forEach(r),this.h()},h(){d(o,"x","3.05"),d(o,"y","0.5"),d(o,"width","25.73"),d(o,"height","31"),d(o,"fill","none"),d(a,"id","a"),d(_,"d","M24.94,9.51a12.81,12.81,0,0,1,0,18.16,12.68,12.68,0,0,1-18,0,12.81,12.81,0,0,1,0-18.16l9-9V5l-.84.83-6,6a9.58,9.58,0,1,0,13.55,0ZM20.44,9a1.68,1.68,0,1,1,1.67-1.67A1.68,1.68,0,0,1,20.44,9Z"),d(_,"fill","#ee4c2c"),d(c,"clip-path","url(#a)"),d(e,"class",m[0]),d(e,"xmlns","http://www.w3.org/2000/svg"),d(e,"xmlns:xlink","http://www.w3.org/1999/xlink"),d(e,"aria-hidden","true"),d(e,"focusable","false"),d(e,"role","img"),d(e,"width","1em"),d(e,"height","1em"),d(e,"preserveAspectRatio","xMidYMid meet"),d(e,"viewBox","0 0 32 32")},m(b,E){$(b,e,E),s(e,l),s(l,a),s(a,o),s(e,c),s(c,_)},p(b,[E]){E&1&&d(e,"class",b[0])},i:le,o:le,d(b){b&&r(e)}}}function rc(m,e,l){let{classNames:a=""}=e;return m.$$set=o=>{"classNames"in o&&l(0,a=o.classNames)},[a]}class oc extends Ge{constructor(e){super();Be(this,e,rc,sc,Je,{classNames:0})}}function lc(m){let e,l,a,o;return{c(){e=I("svg"),l=I("path"),a=I("path"),o=I("path"),this.h()},l(c){e=N(c,"svg",{class:!0,xmlns:!0,"xmlns:xlink":!0,"aria-hidden":!0,focusable:!0,role:!0,width:!0,height:!0,preserveAspectRatio:!0,viewBox:!0});var _=n(e);l=N(_,"path",{d:!0,fill:!0}),n(l).forEach(r),a=N(_,"path",{d:!0,fill:!0}),n(a).forEach(r),o=N(_,"path",{d:!0,fill:!0}),n(o).forEach(r),_.forEach(r),this.h()},h(){d(l,"d","M145.726 42.065v42.07l72.861 42.07v-42.07l-72.86-42.07zM0 84.135v42.07l36.43 21.03V105.17L0 84.135zm109.291 21.035l-36.43 21.034v126.2l36.43 21.035v-84.135l36.435 21.035v-42.07l-36.435-21.034V105.17z"),d(l,"fill","#E55B2D"),d(a,"d","M145.726 42.065L36.43 105.17v42.065l72.861-42.065v42.065l36.435-21.03v-84.14zM255.022 63.1l-36.435 21.035v42.07l36.435-21.035V63.1zm-72.865 84.135l-36.43 21.035v42.07l36.43-21.036v-42.07zm-36.43 63.104l-36.436-21.035v84.135l36.435-21.035V210.34z"),d(a,"fill","#ED8E24"),d(o,"d","M145.726 0L0 84.135l36.43 21.035l109.296-63.105l72.861 42.07L255.022 63.1L145.726 0zm0 126.204l-36.435 21.03l36.435 21.036l36.43-21.035l-36.43-21.03z"),d(o,"fill","#F8BF3C"),d(e,"class",m[0]),d(e,"xmlns","http://www.w3.org/2000/svg"),d(e,"xmlns:xlink","http://www.w3.org/1999/xlink"),d(e,"aria-hidden","true"),d(e,"focusable","false"),d(e,"role","img"),d(e,"width","0.94em"),d(e,"height","1em"),d(e,"preserveAspectRatio","xMidYMid meet"),d(e,"viewBox","0 0 256 274")},m(c,_){$(c,e,_),s(e,l),s(e,a),s(e,o)},p(c,[_]){_&1&&d(e,"class",c[0])},i:le,o:le,d(c){c&&r(e)}}}function nc(m,e,l){let{classNames:a=""}=e;return m.$$set=o=>{"classNames"in o&&l(0,a=o.classNames)},[a]}class ic extends Ge{constructor(e){super();Be(this,e,nc,lc,Je,{classNames:0})}}function pc(m){let e,l,a,o,c,_,b,E,A,y,T,k,C,R,w,F,D,H,K,ae,L,G,Y,U,B,oe,Z,de,re,me,pe,X,Q,ce,O,W,se,z,V,ue,he;return{c(){e=I("svg"),l=I("style"),a=i(`.J {
			stroke: #dce0df;
		}
		.K {
			stroke-linejoin: round;
		}
	`),o=I("g"),c=I("path"),_=I("path"),b=I("path"),E=I("path"),A=I("path"),y=I("path"),T=I("path"),k=I("path"),C=I("g"),R=I("path"),w=I("path"),F=I("path"),D=I("g"),H=I("path"),K=I("path"),ae=I("path"),L=I("g"),G=I("path"),Y=I("path"),U=I("g"),B=I("path"),oe=I("path"),Z=I("path"),de=I("path"),re=I("path"),me=I("path"),pe=I("path"),X=I("path"),Q=I("g"),ce=I("path"),O=I("path"),W=I("path"),se=I("path"),z=I("g"),V=I("path"),ue=I("path"),he=I("path"),this.h()},l(ne){e=N(ne,"svg",{class:!0,xmlns:!0,"xmlns:xlink":!0,"aria-hidden":!0,focusable:!0,role:!0,width:!0,height:!0,preserveAspectRatio:!0,viewBox:!0});var te=n(e);l=N(te,"style",{});var ve=n(l);a=p(ve,`.J {
			stroke: #dce0df;
		}
		.K {
			stroke-linejoin: round;
		}
	`),ve.forEach(r),o=N(te,"g",{fill:!0,class:!0});var ie=n(o);c=N(ie,"path",{d:!0}),n(c).forEach(r),_=N(ie,"path",{d:!0}),n(_).forEach(r),b=N(ie,"path",{d:!0}),n(b).forEach(r),E=N(ie,"path",{d:!0}),n(E).forEach(r),A=N(ie,"path",{d:!0}),n(A).forEach(r),y=N(ie,"path",{d:!0}),n(y).forEach(r),T=N(ie,"path",{d:!0}),n(T).forEach(r),k=N(ie,"path",{d:!0}),n(k).forEach(r),ie.forEach(r),C=N(te,"g",{fill:!0,class:!0});var $e=n(C);R=N($e,"path",{d:!0}),n(R).forEach(r),w=N($e,"path",{d:!0}),n(w).forEach(r),F=N($e,"path",{d:!0}),n(F).forEach(r),$e.forEach(r),D=N(te,"g",{fill:!0,class:!0});var Ae=n(D);H=N(Ae,"path",{d:!0}),n(H).forEach(r),K=N(Ae,"path",{d:!0}),n(K).forEach(r),Ae.forEach(r),ae=N(te,"path",{d:!0,fill:!0,class:!0}),n(ae).forEach(r),L=N(te,"g",{fill:!0,class:!0});var qe=n(L);G=N(qe,"path",{d:!0}),n(G).forEach(r),Y=N(qe,"path",{d:!0}),n(Y).forEach(r),qe.forEach(r),U=N(te,"g",{fill:!0,class:!0});var we=n(U);B=N(we,"path",{d:!0}),n(B).forEach(r),oe=N(we,"path",{d:!0}),n(oe).forEach(r),Z=N(we,"path",{d:!0}),n(Z).forEach(r),de=N(we,"path",{d:!0}),n(de).forEach(r),re=N(we,"path",{d:!0}),n(re).forEach(r),me=N(we,"path",{d:!0}),n(me).forEach(r),pe=N(we,"path",{d:!0}),n(pe).forEach(r),we.forEach(r),X=N(te,"path",{d:!0,fill:!0,class:!0}),n(X).forEach(r),Q=N(te,"g",{fill:!0,class:!0});var ee=n(Q);ce=N(ee,"path",{d:!0}),n(ce).forEach(r),O=N(ee,"path",{d:!0}),n(O).forEach(r),W=N(ee,"path",{d:!0}),n(W).forEach(r),se=N(ee,"path",{d:!0}),n(se).forEach(r),ee.forEach(r),z=N(te,"g",{fill:!0,class:!0});var Ce=n(z);V=N(Ce,"path",{d:!0}),n(V).forEach(r),ue=N(Ce,"path",{d:!0}),n(ue).forEach(r),he=N(Ce,"path",{d:!0}),n(he).forEach(r),Ce.forEach(r),te.forEach(r),this.h()},h(){d(c,"d","M50.5 130.4l-25 43.31h50l25-43.31h-50z"),d(_,"d","M.5 217.01l25-43.3h50l-25 43.3H.5z"),d(b,"d","M125.5 173.71h-50l-25 43.3h50l25-43.3z"),d(E,"d","M175.5 173.71h-50l-25 43.3h50l25-43.3z"),d(A,"d","M150.5 130.4l-25 43.31h50l25-43.31h-50z"),d(y,"d","M175.5 87.1l-25 43.3h50l25-43.3h-50z"),d(T,"d","M200.5 43.8l-25 43.3h50l25-43.3h-50z"),d(k,"d","M225.5.5l-25 43.3h50l25-43.3h-50z"),d(o,"fill","#5e97f6"),d(o,"class","J K"),d(R,"d","M.5 217.01l25 43.3h50l-25-43.3H.5z"),d(w,"d","M125.5 260.31h-50l-25-43.3h50l25 43.3z"),d(F,"d","M175.5 260.31h-50l-25-43.3h50l25 43.3z"),d(C,"fill","#2a56c6"),d(C,"class","J K"),d(H,"d","M200.5 217.01l-25-43.3-25 43.3 25 43.3 25-43.3zm50-86.61l-25-43.3-25 43.3h50z"),d(K,"d","M250.5 43.8l-25 43.3 25 43.3 25-43.3-25-43.3z"),d(D,"fill","#00796b"),d(D,"class","J K"),d(ae,"d","M125.5 173.71l-25-43.31-25 43.31h50z"),d(ae,"fill","#3367d6"),d(ae,"class","J K"),d(G,"d","M250.5 130.4h-50l-25 43.31h50l25-43.31z"),d(Y,"d","M300.5 130.4h-50l-25 43.31h50l25-43.31z"),d(L,"fill","#26a69a"),d(L,"class","J K"),d(B,"d","M350.5 43.8L325.5.5l-25 43.3 25 43.3 25-43.3z"),d(oe,"d","M375.5 87.1l-25-43.3-25 43.3 25 43.3 25-43.3z"),d(Z,"d","M400.5 130.4l-25-43.3-25 43.3 25 43.31 25-43.31z"),d(de,"d","M425.5 173.71l-25-43.31-25 43.31 25 43.3 25-43.3z"),d(re,"d","M450.5 217.01l-25-43.3-25 43.3 25 43.3 25-43.3zM425.5.5l-25 43.3 25 43.3 25-43.3-25-43.3z"),d(me,"d","M375.5 87.1l25-43.3 25 43.3-25 43.3-25-43.3zm-25 43.3l-25 43.31 25 43.3 25-43.3-25-43.31z"),d(pe,"d","M325.5 260.31l-25-43.3 25-43.3 25 43.3-25 43.3z"),d(U,"fill","#9c27b0"),d(U,"class","J K"),d(X,"d","M275.5 260.31l-25-43.3h50l25 43.3h-50z"),d(X,"fill","#6a1b9a"),d(X,"class","J K"),d(ce,"d","M225.5 173.71h-50l25 43.3h50l-25-43.3z"),d(O,"d","M275.5 173.71h-50l25 43.3 25-43.3zm0-86.61l25 43.3h50l-25-43.3h-50z"),d(W,"d","M300.5 43.8h-50l25 43.3h50l-25-43.3zm125 216.51l-25-43.3h-50l25 43.3h50z"),d(se,"d","M375.5 173.71l-25 43.3h50l-25-43.3z"),d(Q,"fill","#00695c"),d(Q,"class","J K"),d(V,"d","M325.5.5h-50l-25 43.3h50l25-43.3zm0 173.21h-50l-25 43.3h50l25-43.3z"),d(ue,"d","M350.5 130.4h-50l-25 43.31h50l25-43.31zM425.5.5h-50l-25 43.3h50l25-43.3z"),d(he,"d","M375.5 87.1l-25-43.3h50l-25 43.3z"),d(z,"fill","#ea80fc"),d(z,"class","J K"),d(e,"class",m[0]),d(e,"xmlns","http://www.w3.org/2000/svg"),d(e,"xmlns:xlink","http://www.w3.org/1999/xlink"),d(e,"aria-hidden","true"),d(e,"focusable","false"),d(e,"role","img"),d(e,"width","1.73em"),d(e,"height","1em"),d(e,"preserveAspectRatio","xMidYMid meet"),d(e,"viewBox","0 0 451 260.81")},m(ne,te){$(ne,e,te),s(e,l),s(l,a),s(e,o),s(o,c),s(o,_),s(o,b),s(o,E),s(o,A),s(o,y),s(o,T),s(o,k),s(e,C),s(C,R),s(C,w),s(C,F),s(e,D),s(D,H),s(D,K),s(e,ae),s(e,L),s(L,G),s(L,Y),s(e,U),s(U,B),s(U,oe),s(U,Z),s(U,de),s(U,re),s(U,me),s(U,pe),s(e,X),s(e,Q),s(Q,ce),s(Q,O),s(Q,W),s(Q,se),s(e,z),s(z,V),s(z,ue),s(z,he)},p(ne,[te]){te&1&&d(e,"class",ne[0])},i:le,o:le,d(ne){ne&&r(e)}}}function cc(m,e,l){let{classNames:a=""}=e;return m.$$set=o=>{"classNames"in o&&l(0,a=o.classNames)},[a]}class uc extends Ge{constructor(e){super();Be(this,e,cc,pc,Je,{classNames:0})}}function fc(m){let e,l;return{c(){e=I("svg"),l=I("path"),this.h()},l(a){e=N(a,"svg",{class:!0,width:!0,height:!0,viewBox:!0,fill:!0,xmlns:!0});var o=n(e);l=N(o,"path",{d:!0,fill:!0}),n(l).forEach(r),o.forEach(r),this.h()},h(){d(l,"d","M0 4.50001C0.390979 2.37042 2.25728 0.756592 4.5 0.756592C6.74272 0.756592 8.60861 2.37042 9 4.50001C8.60902 6.62959 6.74272 8.24342 4.5 8.24342C2.25728 8.24342 0.391395 6.62959 0 4.50001ZM4.5 6.57968C5.05156 6.57968 5.58054 6.36057 5.97055 5.97056C6.36057 5.58054 6.57967 5.05157 6.57967 4.50001C6.57967 3.94844 6.36057 3.41947 5.97055 3.02945C5.58054 2.63944 5.05156 2.42033 4.5 2.42033C3.94844 2.42033 3.41946 2.63944 3.02945 3.02945C2.63943 3.41947 2.42033 3.94844 2.42033 4.50001C2.42033 5.05157 2.63943 5.58054 3.02945 5.97056C3.41946 6.36057 3.94844 6.57968 4.5 6.57968ZM4.5 5.74781C4.16906 5.74781 3.85168 5.61635 3.61767 5.38234C3.38366 5.14833 3.2522 4.83094 3.2522 4.50001C3.2522 4.16907 3.38366 3.85168 3.61767 3.61767C3.85168 3.38367 4.16906 3.2522 4.5 3.2522C4.83094 3.2522 5.14832 3.38367 5.38233 3.61767C5.61634 3.85168 5.7478 4.16907 5.7478 4.50001C5.7478 4.83094 5.61634 5.14833 5.38233 5.38234C5.14832 5.61635 4.83094 5.74781 4.5 5.74781Z"),d(l,"fill","currentColor"),d(e,"class",m[0]),d(e,"width",m[1]),d(e,"height",m[1]),d(e,"viewBox","0 0 9 9"),d(e,"fill","currentColor"),d(e,"xmlns","http://www.w3.org/2000/svg")},m(a,o){$(a,e,o),s(e,l)},p(a,[o]){o&1&&d(e,"class",a[0]),o&2&&d(e,"width",a[1]),o&2&&d(e,"height",a[1])},i:le,o:le,d(a){a&&r(e)}}}function dc(m,e,l){let{classNames:a=""}=e,{size:o="1em"}=e;return m.$$set=c=>{"classNames"in c&&l(0,a=c.classNames),"size"in c&&l(1,o=c.size)},[a,o]}class mc extends Ge{constructor(e){super();Be(this,e,dc,fc,Je,{classNames:0,size:1})}}function hc(m){let e,l;return{c(){e=I("svg"),l=I("path"),this.h()},l(a){e=N(a,"svg",{class:!0,width:!0,height:!0,viewBox:!0,fill:!0,xmlns:!0});var o=n(e);l=N(o,"path",{d:!0,fill:!0}),n(l).forEach(r),o.forEach(r),this.h()},h(){d(l,"d","M1.39125 1.9725L0.0883333 0.669997L0.677917 0.0804138L8.9275 8.33041L8.33792 8.91958L6.95875 7.54041C6.22592 8.00523 5.37572 8.25138 4.50792 8.25C2.26125 8.25 0.392083 6.63333 0 4.5C0.179179 3.52946 0.667345 2.64287 1.39167 1.9725H1.39125ZM5.65667 6.23833L5.04667 5.62833C4.81335 5.73996 4.55116 5.77647 4.29622 5.73282C4.04129 5.68918 3.80617 5.56752 3.62328 5.38463C3.44039 5.20175 3.31874 4.96663 3.27509 4.71169C3.23144 4.45676 3.26795 4.19456 3.37958 3.96125L2.76958 3.35125C2.50447 3.75187 2.38595 4.2318 2.4341 4.70978C2.48225 5.18777 2.6941 5.63442 3.0338 5.97411C3.37349 6.31381 3.82015 6.52567 4.29813 6.57382C4.77611 6.62197 5.25605 6.50345 5.65667 6.23833ZM2.83042 1.06666C3.35 0.862497 3.91625 0.749997 4.50792 0.749997C6.75458 0.749997 8.62375 2.36666 9.01583 4.5C8.88816 5.19404 8.60119 5.84899 8.1775 6.41333L6.56917 4.805C6.61694 4.48317 6.58868 4.15463 6.48664 3.84569C6.3846 3.53675 6.21162 3.256 5.98156 3.02594C5.7515 2.79588 5.47075 2.6229 5.16181 2.52086C4.85287 2.41882 4.52433 2.39056 4.2025 2.43833L2.83042 1.06708V1.06666Z"),d(l,"fill","currentColor"),d(e,"class",m[0]),d(e,"width",m[1]),d(e,"height",m[1]),d(e,"viewBox","0 0 10 9"),d(e,"fill","currentColor"),d(e,"xmlns","http://www.w3.org/2000/svg")},m(a,o){$(a,e,o),s(e,l)},p(a,[o]){o&1&&d(e,"class",a[0]),o&2&&d(e,"width",a[1]),o&2&&d(e,"height",a[1])},i:le,o:le,d(a){a&&r(e)}}}function $c(m,e,l){let{classNames:a=""}=e,{size:o="1em"}=e;return m.$$set=c=>{"classNames"in c&&l(0,a=c.classNames),"size"in c&&l(1,o=c.size)},[a,o]}class _c extends Ge{constructor(e){super();Be(this,e,$c,hc,Je,{classNames:0,size:1})}}const{window:gc}=Qp;function Up(m){let e,l,a,o,c,_,b,E,A,y;return l=new _c({props:{size:"0.9em"}}),{c(){e=u("div"),x(l.$$.fragment),a=g(),o=u("span"),c=i("Hide "),_=i(m[3]),b=i(" content"),this.h()},l(T){e=f(T,"DIV",{class:!0});var k=n(e);S(l.$$.fragment,k),a=v(k),o=f(k,"SPAN",{});var C=n(o);c=p(C,"Hide "),_=p(C,m[3]),b=p(C," content"),C.forEach(r),k.forEach(r),this.h()},h(){d(e,"class","cursor-pointer flex items-center justify-center space-x-1 text-sm px-2 bg-white dark:bg-gray-950 hover:underline leading-none")},m(T,k){$(T,e,k),M(l,e,null),s(e,a),s(e,o),s(o,c),s(o,_),s(o,b),E=!0,A||(y=oi(e,"click",m[5]),A=!0)},p:le,i(T){E||(j(l.$$.fragment,T),E=!0)},o(T){q(l.$$.fragment,T),E=!1},d(T){T&&r(e),P(l),A=!1,y()}}}function vc(m){let e,l;const a=m[10].default,o=Ds(a,m,m[9],null);return{c(){e=u("div"),o&&o.c(),this.h()},l(c){e=f(c,"DIV",{class:!0});var _=n(e);o&&o.l(_),_.forEach(r),this.h()},h(){d(e,"class","framework-content")},m(c,_){$(c,e,_),o&&o.m(e,null),l=!0},p(c,_){o&&o.p&&(!l||_&512)&&Is(o,a,c,c[9],l?Fs(a,c[9],_,null):Ns(c[9]),null)},i(c){l||(j(o,c),l=!0)},o(c){q(o,c),l=!1},d(c){c&&r(e),o&&o.d(c)}}}function Ec(m){let e,l,a,o,c,_,b,E,A,y;return l=new mc({props:{size:"0.9em"}}),{c(){e=u("div"),x(l.$$.fragment),a=g(),o=u("span"),c=i("Show "),_=i(m[3]),b=i(" content"),this.h()},l(T){e=f(T,"DIV",{class:!0});var k=n(e);S(l.$$.fragment,k),a=v(k),o=f(k,"SPAN",{});var C=n(o);c=p(C,"Show "),_=p(C,m[3]),b=p(C," content"),C.forEach(r),k.forEach(r),this.h()},h(){d(e,"class","cursor-pointer mt-[-12.5px] flex items-center justify-center space-x-1 py-4 text-sm hover:underline leading-none")},m(T,k){$(T,e,k),M(l,e,null),s(e,a),s(e,o),s(o,c),s(o,_),s(o,b),E=!0,A||(y=oi(e,"click",m[5]),A=!0)},p:le,i(T){E||(j(l.$$.fragment,T),E=!0)},o(T){q(l.$$.fragment,T),E=!1},d(T){T&&r(e),P(l),A=!1,y()}}}function bc(m){let e,l,a,o,c,_,b,E,A,y,T,k,C,R;var w=m[2];function F(L){return{}}w&&(o=new w(F()));let D=!m[1]&&Up(m);const H=[Ec,vc],K=[];function ae(L,G){return L[1]?0:1}return y=ae(m),T=K[y]=H[y](m),{c(){e=u("div"),l=u("div"),a=u("div"),o&&x(o.$$.fragment),c=g(),_=u("span"),b=i(m[3]),E=g(),D&&D.c(),A=g(),T.c(),this.h()},l(L){e=f(L,"DIV",{class:!0});var G=n(e);l=f(G,"DIV",{class:!0});var Y=n(l);a=f(Y,"DIV",{class:!0});var U=n(a);o&&S(o.$$.fragment,U),c=v(U),_=f(U,"SPAN",{});var B=n(_);b=p(B,m[3]),B.forEach(r),U.forEach(r),E=v(Y),D&&D.l(Y),Y.forEach(r),A=v(G),T.l(G),G.forEach(r),this.h()},h(){d(a,"class","flex px-1 items-center space-x-1 bg-white dark:bg-gray-950"),d(l,"class","flex h-[22px] mt-[-12.5px] justify-between leading-none"),d(e,"class","border border-gray-200 rounded-xl px-4 relative")},m(L,G){$(L,e,G),s(e,l),s(l,a),o&&M(o,a,null),s(a,c),s(a,_),s(_,b),s(l,E),D&&D.m(l,null),s(e,A),K[y].m(e,null),m[11](e),k=!0,C||(R=oi(gc,"hashchange",m[6]),C=!0)},p(L,[G]){if(w!==(w=L[2])){if(o){ya();const U=o;q(U.$$.fragment,1,0,()=>{P(U,1)}),Aa()}w?(o=new w(F()),x(o.$$.fragment),j(o.$$.fragment,1),M(o,a,c)):o=null}L[1]?D&&(ya(),q(D,1,1,()=>{D=null}),Aa()):D?(D.p(L,G),G&2&&j(D,1)):(D=Up(L),D.c(),j(D,1),D.m(l,null));let Y=y;y=ae(L),y===Y?K[y].p(L,G):(ya(),q(K[Y],1,1,()=>{K[Y]=null}),Aa(),T=K[y],T?T.p(L,G):(T=K[y]=H[y](L),T.c()),j(T,1),T.m(e,null))},i(L){k||(o&&j(o.$$.fragment,L),j(D),j(T),k=!0)},o(L){o&&q(o.$$.fragment,L),q(D),q(T),k=!1},d(L){L&&r(e),o&&P(o),D&&D.d(),K[y].d(),m[11](null),C=!1,R()}}}function wc(m,e,l){let a,o,{$$slots:c={},$$scope:_}=e,{framework:b}=e,E,A=new Set;const y={pytorch:{Icon:oc,label:"Pytorch"},tensorflow:{Icon:ic,label:"TensorFlow"},jax:{Icon:uc,label:"JAX"}},{Icon:T,label:k}=y[b],C=`hf_doc_framework_${b}_is_hidden`,R=ac(b);Zp(m,R,H=>l(8,o=H));function w(){oo(R,o=o!==Ne.CLOSED?Ne.CLOSED:Ne.OPEN,o),localStorage.setItem(C,o)}function F(){const H=window.location.hash.slice(1);A.has(H)&&(oo(R,o=Ne.HASHASHLINK,o),localStorage.setItem(C,o))}Jp(()=>{const H=window.location.hash.slice(1),K="header-link",ae=E.querySelectorAll(`.${K}`);A=new Set([...ae].map(G=>G.id));const L=localStorage.getItem(C);A.has(H)?oo(R,o=Ne.HASHASHLINK,o):L===Ne.CLOSED&&o!==Ne.HASHASHLINK&&oo(R,o=Ne.CLOSED,o)});function D(H){Xp[H?"unshift":"push"](()=>{E=H,l(0,E)})}return m.$$set=H=>{"framework"in H&&l(7,b=H.framework),"$$scope"in H&&l(9,_=H.$$scope)},m.$$.update=()=>{m.$$.dirty&256&&l(1,a=o===Ne.CLOSED)},[E,a,T,k,R,w,F,b,o,_,c,D]}class li extends Ge{constructor(e){super();Be(this,e,wc,bc,Je,{framework:7})}}const kc=m=>({}),Rp=m=>({}),jc=m=>({}),Wp=m=>({}),yc=m=>({}),Vp=m=>({});function Kp(m){let e,l;return e=new li({props:{framework:"pytorch",$$slots:{default:[Ac]},$$scope:{ctx:m}}}),{c(){x(e.$$.fragment)},l(a){S(e.$$.fragment,a)},m(a,o){M(e,a,o),l=!0},p(a,o){const c={};o&16&&(c.$$scope={dirty:o,ctx:a}),e.$set(c)},i(a){l||(j(e.$$.fragment,a),l=!0)},o(a){q(e.$$.fragment,a),l=!1},d(a){P(e,a)}}}function Ac(m){let e;const l=m[3].pytorch,a=Ds(l,m,m[4],Vp);return{c(){a&&a.c()},l(o){a&&a.l(o)},m(o,c){a&&a.m(o,c),e=!0},p(o,c){a&&a.p&&(!e||c&16)&&Is(a,l,o,o[4],e?Fs(l,o[4],c,yc):Ns(o[4]),Vp)},i(o){e||(j(a,o),e=!0)},o(o){q(a,o),e=!1},d(o){a&&a.d(o)}}}function Gp(m){let e,l;return e=new li({props:{framework:"tensorflow",$$slots:{default:[qc]},$$scope:{ctx:m}}}),{c(){x(e.$$.fragment)},l(a){S(e.$$.fragment,a)},m(a,o){M(e,a,o),l=!0},p(a,o){const c={};o&16&&(c.$$scope={dirty:o,ctx:a}),e.$set(c)},i(a){l||(j(e.$$.fragment,a),l=!0)},o(a){q(e.$$.fragment,a),l=!1},d(a){P(e,a)}}}function qc(m){let e;const l=m[3].tensorflow,a=Ds(l,m,m[4],Wp);return{c(){a&&a.c()},l(o){a&&a.l(o)},m(o,c){a&&a.m(o,c),e=!0},p(o,c){a&&a.p&&(!e||c&16)&&Is(a,l,o,o[4],e?Fs(l,o[4],c,jc):Ns(o[4]),Wp)},i(o){e||(j(a,o),e=!0)},o(o){q(a,o),e=!1},d(o){a&&a.d(o)}}}function Bp(m){let e,l;return e=new li({props:{framework:"jax",$$slots:{default:[Cc]},$$scope:{ctx:m}}}),{c(){x(e.$$.fragment)},l(a){S(e.$$.fragment,a)},m(a,o){M(e,a,o),l=!0},p(a,o){const c={};o&16&&(c.$$scope={dirty:o,ctx:a}),e.$set(c)},i(a){l||(j(e.$$.fragment,a),l=!0)},o(a){q(e.$$.fragment,a),l=!1},d(a){P(e,a)}}}function Cc(m){let e;const l=m[3].jax,a=Ds(l,m,m[4],Rp);return{c(){a&&a.c()},l(o){a&&a.l(o)},m(o,c){a&&a.m(o,c),e=!0},p(o,c){a&&a.p&&(!e||c&16)&&Is(a,l,o,o[4],e?Fs(l,o[4],c,kc):Ns(o[4]),Rp)},i(o){e||(j(a,o),e=!0)},o(o){q(a,o),e=!1},d(o){a&&a.d(o)}}}function Tc(m){let e,l,a,o,c=m[0]&&Kp(m),_=m[1]&&Gp(m),b=m[2]&&Bp(m);return{c(){e=u("div"),c&&c.c(),l=g(),_&&_.c(),a=g(),b&&b.c(),this.h()},l(E){e=f(E,"DIV",{class:!0});var A=n(e);c&&c.l(A),l=v(A),_&&_.l(A),a=v(A),b&&b.l(A),A.forEach(r),this.h()},h(){d(e,"class","space-y-10 py-6 2xl:py-8 2xl:-mx-4")},m(E,A){$(E,e,A),c&&c.m(e,null),s(e,l),_&&_.m(e,null),s(e,a),b&&b.m(e,null),o=!0},p(E,[A]){E[0]?c?(c.p(E,A),A&1&&j(c,1)):(c=Kp(E),c.c(),j(c,1),c.m(e,l)):c&&(ya(),q(c,1,1,()=>{c=null}),Aa()),E[1]?_?(_.p(E,A),A&2&&j(_,1)):(_=Gp(E),_.c(),j(_,1),_.m(e,a)):_&&(ya(),q(_,1,1,()=>{_=null}),Aa()),E[2]?b?(b.p(E,A),A&4&&j(b,1)):(b=Bp(E),b.c(),j(b,1),b.m(e,null)):b&&(ya(),q(b,1,1,()=>{b=null}),Aa())},i(E){o||(j(c),j(_),j(b),o=!0)},o(E){q(c),q(_),q(b),o=!1},d(E){E&&r(e),c&&c.d(),_&&_.d(),b&&b.d()}}}function zc(m,e,l){let{$$slots:a={},$$scope:o}=e,{pytorch:c=!1}=e,{tensorflow:_=!1}=e,{jax:b=!1}=e;return m.$$set=E=>{"pytorch"in E&&l(0,c=E.pytorch),"tensorflow"in E&&l(1,_=E.tensorflow),"jax"in E&&l(2,b=E.jax),"$$scope"in E&&l(4,o=E.$$scope)},[c,_,b,a,o]}class ka extends Ge{constructor(e){super();Be(this,e,zc,Tc,Je,{pytorch:0,tensorflow:1,jax:2})}}function xc(m){let e;const l=m[1].default,a=Ds(l,m,m[0],null);return{c(){a&&a.c()},l(o){a&&a.l(o)},m(o,c){a&&a.m(o,c),e=!0},p(o,[c]){a&&a.p&&(!e||c&1)&&Is(a,l,o,o[0],e?Fs(l,o[0],c,null):Ns(o[0]),null)},i(o){e||(j(a,o),e=!0)},o(o){q(a,o),e=!1},d(o){a&&a.d(o)}}}function Mc(m,e,l){let{$$slots:a={},$$scope:o}=e;return m.$$set=c=>{"$$scope"in c&&l(0,o=c.$$scope)},[o,a]}class ye extends Ge{constructor(e){super();Be(this,e,Mc,xc,Je,{})}}function Pc(m){let e,l;return{c(){e=u("p"),l=i(`Todos los ejemplos de c\xF3digo presentados en la documentaci\xF3n tienen un bot\xF3n arriba a la izquierda para elegir entre Pytorch y TensorFlow.
Si no fuese as\xED, se espera que el c\xF3digo funcione para ambos backends sin ning\xFAn cambio.`)},l(a){e=f(a,"P",{});var o=n(e);l=p(o,`Todos los ejemplos de c\xF3digo presentados en la documentaci\xF3n tienen un bot\xF3n arriba a la izquierda para elegir entre Pytorch y TensorFlow.
Si no fuese as\xED, se espera que el c\xF3digo funcione para ambos backends sin ning\xFAn cambio.`),o.forEach(r)},m(a,o){$(a,e,o),s(e,l)},d(a){a&&r(e)}}}function Sc(m){let e,l,a,o,c,_,b,E;return{c(){e=u("p"),l=i("Para m\xE1s detalles acerca del "),a=u("code"),o=i("pipeline()"),c=i("y tareas asociadas, consulta la documentaci\xF3n "),_=u("a"),b=i("aqu\xED"),E=i("."),this.h()},l(A){e=f(A,"P",{});var y=n(e);l=p(y,"Para m\xE1s detalles acerca del "),a=f(y,"CODE",{});var T=n(a);o=p(T,"pipeline()"),T.forEach(r),c=p(y,"y tareas asociadas, consulta la documentaci\xF3n "),_=f(y,"A",{href:!0});var k=n(_);b=p(k,"aqu\xED"),k.forEach(r),E=p(y,"."),y.forEach(r),this.h()},h(){d(_,"href","./main_classes/pipelines")},m(A,y){$(A,e,y),s(e,l),s(e,a),s(a,o),s(e,c),s(e,_),s(_,b),s(e,E)},d(A){A&&r(e)}}}function Oc(m){let e,l;return e=new J({props:{code:"pip install torch",highlighted:"pip install torch"}}),{c(){x(e.$$.fragment)},l(a){S(e.$$.fragment,a)},m(a,o){M(e,a,o),l=!0},p:le,i(a){l||(j(e.$$.fragment,a),l=!0)},o(a){q(e.$$.fragment,a),l=!1},d(a){P(e,a)}}}function Dc(m){let e,l;return e=new ye({props:{$$slots:{default:[Oc]},$$scope:{ctx:m}}}),{c(){x(e.$$.fragment)},l(a){S(e.$$.fragment,a)},m(a,o){M(e,a,o),l=!0},p(a,o){const c={};o&2&&(c.$$scope={dirty:o,ctx:a}),e.$set(c)},i(a){l||(j(e.$$.fragment,a),l=!0)},o(a){q(e.$$.fragment,a),l=!1},d(a){P(e,a)}}}function Ic(m){let e,l;return e=new J({props:{code:"pip install tensorflow",highlighted:"pip install tensorflow"}}),{c(){x(e.$$.fragment)},l(a){S(e.$$.fragment,a)},m(a,o){M(e,a,o),l=!0},p:le,i(a){l||(j(e.$$.fragment,a),l=!0)},o(a){q(e.$$.fragment,a),l=!1},d(a){P(e,a)}}}function Nc(m){let e,l;return e=new ye({props:{$$slots:{default:[Ic]},$$scope:{ctx:m}}}),{c(){x(e.$$.fragment)},l(a){S(e.$$.fragment,a)},m(a,o){M(e,a,o),l=!0},p(a,o){const c={};o&2&&(c.$$scope={dirty:o,ctx:a}),e.$set(c)},i(a){l||(j(e.$$.fragment,a),l=!0)},o(a){q(e.$$.fragment,a),l=!1},d(a){P(e,a)}}}function Fc(m){let e,l,a,o,c,_,b,E,A,y,T;return y=new J({props:{code:`from transformers import AutoTokenizer, AutoModelForSequenceClassification

model = AutoModelForSequenceClassification.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, AutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSequenceClassification.from_pretrained(model_name)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(model_name)`}}),{c(){e=u("p"),l=i("Usa "),a=u("code"),o=i("AutoModelForSequenceClassification"),c=i("y [\u2018AutoTokenizer\u2019] para cargar un modelo preentrenado y un tokenizador asociado (m\xE1s en un "),_=u("code"),b=i("AutoClass"),E=i(" debajo):"),A=g(),x(y.$$.fragment)},l(k){e=f(k,"P",{});var C=n(e);l=p(C,"Usa "),a=f(C,"CODE",{});var R=n(a);o=p(R,"AutoModelForSequenceClassification"),R.forEach(r),c=p(C,"y [\u2018AutoTokenizer\u2019] para cargar un modelo preentrenado y un tokenizador asociado (m\xE1s en un "),_=f(C,"CODE",{});var w=n(_);b=p(w,"AutoClass"),w.forEach(r),E=p(C," debajo):"),C.forEach(r),A=v(k),S(y.$$.fragment,k)},m(k,C){$(k,e,C),s(e,l),s(e,a),s(a,o),s(e,c),s(e,_),s(_,b),s(e,E),$(k,A,C),M(y,k,C),T=!0},p:le,i(k){T||(j(y.$$.fragment,k),T=!0)},o(k){q(y.$$.fragment,k),T=!1},d(k){k&&r(e),k&&r(A),P(y,k)}}}function Lc(m){let e,l;return e=new ye({props:{$$slots:{default:[Fc]},$$scope:{ctx:m}}}),{c(){x(e.$$.fragment)},l(a){S(e.$$.fragment,a)},m(a,o){M(e,a,o),l=!0},p(a,o){const c={};o&2&&(c.$$scope={dirty:o,ctx:a}),e.$set(c)},i(a){l||(j(e.$$.fragment,a),l=!0)},o(a){q(e.$$.fragment,a),l=!1},d(a){P(e,a)}}}function Hc(m){let e,l,a,o,c,_,b,E,A,y,T;return y=new J({props:{code:`from transformers import AutoTokenizer, TFAutoModelForSequenceClassification

model = TFAutoModelForSequenceClassification.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, TFAutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSequenceClassification.from_pretrained(model_name)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(model_name)`}}),{c(){e=u("p"),l=i("Usa "),a=u("code"),o=i("TFAutoModelForSequenceClassification"),c=i("y [\u2018AutoTokenizer\u2019] para cargar un modelo preentrenado y un tokenizador asociado (m\xE1s en un "),_=u("code"),b=i("TFAutoClass"),E=i(" debajo):"),A=g(),x(y.$$.fragment)},l(k){e=f(k,"P",{});var C=n(e);l=p(C,"Usa "),a=f(C,"CODE",{});var R=n(a);o=p(R,"TFAutoModelForSequenceClassification"),R.forEach(r),c=p(C,"y [\u2018AutoTokenizer\u2019] para cargar un modelo preentrenado y un tokenizador asociado (m\xE1s en un "),_=f(C,"CODE",{});var w=n(_);b=p(w,"TFAutoClass"),w.forEach(r),E=p(C," debajo):"),C.forEach(r),A=v(k),S(y.$$.fragment,k)},m(k,C){$(k,e,C),s(e,l),s(e,a),s(a,o),s(e,c),s(e,_),s(_,b),s(e,E),$(k,A,C),M(y,k,C),T=!0},p:le,i(k){T||(j(y.$$.fragment,k),T=!0)},o(k){q(y.$$.fragment,k),T=!1},d(k){k&&r(e),k&&r(A),P(y,k)}}}function Uc(m){let e,l;return e=new ye({props:{$$slots:{default:[Hc]},$$scope:{ctx:m}}}),{c(){x(e.$$.fragment)},l(a){S(e.$$.fragment,a)},m(a,o){M(e,a,o),l=!0},p(a,o){const c={};o&2&&(c.$$scope={dirty:o,ctx:a}),e.$set(c)},i(a){l||(j(e.$$.fragment,a),l=!0)},o(a){q(e.$$.fragment,a),l=!1},d(a){P(e,a)}}}function Rc(m){let e,l;return e=new J({props:{code:`pt_batch = tokenizer(
    ["We are very happy to show you the \u{1F917} Transformers library.", "We hope you don't hate it."],
    padding=True,
    truncation=True,
    max_length=512,
    return_tensors="pt",
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>pt_batch = tokenizer(
<span class="hljs-meta">... </span>    [<span class="hljs-string">&quot;We are very happy to show you the \u{1F917} Transformers library.&quot;</span>, <span class="hljs-string">&quot;We hope you don&#x27;t hate it.&quot;</span>],
<span class="hljs-meta">... </span>    padding=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    truncation=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    max_length=<span class="hljs-number">512</span>,
<span class="hljs-meta">... </span>    return_tensors=<span class="hljs-string">&quot;pt&quot;</span>,
<span class="hljs-meta">... </span>)`}}),{c(){x(e.$$.fragment)},l(a){S(e.$$.fragment,a)},m(a,o){M(e,a,o),l=!0},p:le,i(a){l||(j(e.$$.fragment,a),l=!0)},o(a){q(e.$$.fragment,a),l=!1},d(a){P(e,a)}}}function Wc(m){let e,l;return e=new ye({props:{$$slots:{default:[Rc]},$$scope:{ctx:m}}}),{c(){x(e.$$.fragment)},l(a){S(e.$$.fragment,a)},m(a,o){M(e,a,o),l=!0},p(a,o){const c={};o&2&&(c.$$scope={dirty:o,ctx:a}),e.$set(c)},i(a){l||(j(e.$$.fragment,a),l=!0)},o(a){q(e.$$.fragment,a),l=!1},d(a){P(e,a)}}}function Vc(m){let e,l;return e=new J({props:{code:`tf_batch = tokenizer(
    ["We are very happy to show you the \u{1F917} Transformers library.", "We hope you don't hate it."],
    padding=True,
    truncation=True,
    max_length=512,
    return_tensors="tf",
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>tf_batch = tokenizer(
<span class="hljs-meta">... </span>    [<span class="hljs-string">&quot;We are very happy to show you the \u{1F917} Transformers library.&quot;</span>, <span class="hljs-string">&quot;We hope you don&#x27;t hate it.&quot;</span>],
<span class="hljs-meta">... </span>    padding=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    truncation=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    max_length=<span class="hljs-number">512</span>,
<span class="hljs-meta">... </span>    return_tensors=<span class="hljs-string">&quot;tf&quot;</span>,
<span class="hljs-meta">... </span>)`}}),{c(){x(e.$$.fragment)},l(a){S(e.$$.fragment,a)},m(a,o){M(e,a,o),l=!0},p:le,i(a){l||(j(e.$$.fragment,a),l=!0)},o(a){q(e.$$.fragment,a),l=!1},d(a){P(e,a)}}}function Kc(m){let e,l;return e=new ye({props:{$$slots:{default:[Vc]},$$scope:{ctx:m}}}),{c(){x(e.$$.fragment)},l(a){S(e.$$.fragment,a)},m(a,o){M(e,a,o),l=!0},p(a,o){const c={};o&2&&(c.$$scope={dirty:o,ctx:a}),e.$set(c)},i(a){l||(j(e.$$.fragment,a),l=!0)},o(a){q(e.$$.fragment,a),l=!1},d(a){P(e,a)}}}function Gc(m){let e,l,a,o,c,_,b,E;return{c(){e=u("p"),l=i("Ve el "),a=u("a"),o=i("task summary"),c=i(" para revisar qu\xE9 clase del "),_=u("code"),b=i("AutoModel"),E=i("deber\xEDas usar para cada tarea."),this.h()},l(A){e=f(A,"P",{});var y=n(e);l=p(y,"Ve el "),a=f(y,"A",{href:!0});var T=n(a);o=p(T,"task summary"),T.forEach(r),c=p(y," para revisar qu\xE9 clase del "),_=f(y,"CODE",{});var k=n(_);b=p(k,"AutoModel"),k.forEach(r),E=p(y,"deber\xEDas usar para cada tarea."),y.forEach(r),this.h()},h(){d(a,"href","./task_summary")},m(A,y){$(A,e,y),s(e,l),s(e,a),s(a,o),s(e,c),s(e,_),s(_,b),s(e,E)},d(A){A&&r(e)}}}function Bc(m){let e,l,a,o,c,_,b,E,A,y,T,k,C,R,w,F,D,H,K,ae,L,G,Y,U,B,oe,Z,de,re,me,pe,X,Q,ce,O,W,se;return w=new J({props:{code:`from transformers import AutoModelForSequenceClassification

model_name = "nlptown/bert-base-multilingual-uncased-sentiment"
pt_model = AutoModelForSequenceClassification.from_pretrained(model_name)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>model_name = <span class="hljs-string">&quot;nlptown/bert-base-multilingual-uncased-sentiment&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>pt_model = AutoModelForSequenceClassification.from_pretrained(model_name)`}}),D=new ja({props:{$$slots:{default:[Gc]},$$scope:{ctx:m}}}),B=new J({props:{code:"pt_outputs = pt_model(**pt_batch)",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>pt_outputs = pt_model(**pt_batch)'}}),W=new J({props:{code:`from torch import nn

pt_predictions = nn.functional.softmax(pt_outputs.logits, dim=-1)
print(pt_predictions)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn

<span class="hljs-meta">&gt;&gt;&gt; </span>pt_predictions = nn.functional.softmax(pt_outputs.logits, dim=-<span class="hljs-number">1</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(pt_predictions)
tensor([[<span class="hljs-number">0.0021</span>, <span class="hljs-number">0.0018</span>, <span class="hljs-number">0.0115</span>, <span class="hljs-number">0.2121</span>, <span class="hljs-number">0.7725</span>],
        [<span class="hljs-number">0.2084</span>, <span class="hljs-number">0.1826</span>, <span class="hljs-number">0.1969</span>, <span class="hljs-number">0.1755</span>, <span class="hljs-number">0.2365</span>]], grad_fn=&lt;SoftmaxBackward0&gt;)`}}),{c(){e=u("p"),l=i("\u{1F917} Transformers provee una forma simple y unificada de cargar tus instancias preentrenadas. Esto significa que puedes cargar un "),a=u("code"),o=i("AutoModel"),c=i("como cargar\xEDas un "),_=u("code"),b=i("AutoTokenizer"),E=i(" La \xFAnica diferencia es seleccionar el "),A=u("code"),y=i("AutoModel"),T=i("correcto para la tarea. Ya que est\xE1s clasificando texto, o secuencias, carga "),k=u("code"),C=i("AutoModelForSequenceClassification"),R=g(),x(w.$$.fragment),F=g(),x(D.$$.fragment),H=g(),K=u("p"),ae=i("Ahora puedes pasar tu lote (batch) preprocesado de inputs directamente al modelo. Solo tienes que desempacar el diccionario a\xF1adiendo "),L=u("code"),G=i("**"),Y=i(":"),U=g(),x(B.$$.fragment),oe=g(),Z=u("p"),de=i("El modelo producir\xE1 las activaciones finales en el atributo "),re=u("code"),me=i("logits"),pe=i(". Aplica la funci\xF3n softmax a "),X=u("code"),Q=i("logits"),ce=i(" para obtener las probabilidades:"),O=g(),x(W.$$.fragment)},l(z){e=f(z,"P",{});var V=n(e);l=p(V,"\u{1F917} Transformers provee una forma simple y unificada de cargar tus instancias preentrenadas. Esto significa que puedes cargar un "),a=f(V,"CODE",{});var ue=n(a);o=p(ue,"AutoModel"),ue.forEach(r),c=p(V,"como cargar\xEDas un "),_=f(V,"CODE",{});var he=n(_);b=p(he,"AutoTokenizer"),he.forEach(r),E=p(V," La \xFAnica diferencia es seleccionar el "),A=f(V,"CODE",{});var ne=n(A);y=p(ne,"AutoModel"),ne.forEach(r),T=p(V,"correcto para la tarea. Ya que est\xE1s clasificando texto, o secuencias, carga "),k=f(V,"CODE",{});var te=n(k);C=p(te,"AutoModelForSequenceClassification"),te.forEach(r),V.forEach(r),R=v(z),S(w.$$.fragment,z),F=v(z),S(D.$$.fragment,z),H=v(z),K=f(z,"P",{});var ve=n(K);ae=p(ve,"Ahora puedes pasar tu lote (batch) preprocesado de inputs directamente al modelo. Solo tienes que desempacar el diccionario a\xF1adiendo "),L=f(ve,"CODE",{});var ie=n(L);G=p(ie,"**"),ie.forEach(r),Y=p(ve,":"),ve.forEach(r),U=v(z),S(B.$$.fragment,z),oe=v(z),Z=f(z,"P",{});var $e=n(Z);de=p($e,"El modelo producir\xE1 las activaciones finales en el atributo "),re=f($e,"CODE",{});var Ae=n(re);me=p(Ae,"logits"),Ae.forEach(r),pe=p($e,". Aplica la funci\xF3n softmax a "),X=f($e,"CODE",{});var qe=n(X);Q=p(qe,"logits"),qe.forEach(r),ce=p($e," para obtener las probabilidades:"),$e.forEach(r),O=v(z),S(W.$$.fragment,z)},m(z,V){$(z,e,V),s(e,l),s(e,a),s(a,o),s(e,c),s(e,_),s(_,b),s(e,E),s(e,A),s(A,y),s(e,T),s(e,k),s(k,C),$(z,R,V),M(w,z,V),$(z,F,V),M(D,z,V),$(z,H,V),$(z,K,V),s(K,ae),s(K,L),s(L,G),s(K,Y),$(z,U,V),M(B,z,V),$(z,oe,V),$(z,Z,V),s(Z,de),s(Z,re),s(re,me),s(Z,pe),s(Z,X),s(X,Q),s(Z,ce),$(z,O,V),M(W,z,V),se=!0},p(z,V){const ue={};V&2&&(ue.$$scope={dirty:V,ctx:z}),D.$set(ue)},i(z){se||(j(w.$$.fragment,z),j(D.$$.fragment,z),j(B.$$.fragment,z),j(W.$$.fragment,z),se=!0)},o(z){q(w.$$.fragment,z),q(D.$$.fragment,z),q(B.$$.fragment,z),q(W.$$.fragment,z),se=!1},d(z){z&&r(e),z&&r(R),P(w,z),z&&r(F),P(D,z),z&&r(H),z&&r(K),z&&r(U),P(B,z),z&&r(oe),z&&r(Z),z&&r(O),P(W,z)}}}function Jc(m){let e,l;return e=new ye({props:{$$slots:{default:[Bc]},$$scope:{ctx:m}}}),{c(){x(e.$$.fragment)},l(a){S(e.$$.fragment,a)},m(a,o){M(e,a,o),l=!0},p(a,o){const c={};o&2&&(c.$$scope={dirty:o,ctx:a}),e.$set(c)},i(a){l||(j(e.$$.fragment,a),l=!0)},o(a){q(e.$$.fragment,a),l=!1},d(a){P(e,a)}}}function Yc(m){let e;return{c(){e=i("Ve el [task summary](./task_summary) para revisar qu\xE9 clase del `AutoModel`  deber\xEDas usar para cada tarea.")},l(l){e=p(l,"Ve el [task summary](./task_summary) para revisar qu\xE9 clase del `AutoModel`  deber\xEDas usar para cada tarea.")},m(l,a){$(l,e,a)},d(l){l&&r(e)}}}function Zc(m){let e,l,a,o,c,_,b,E,A,y,T,k,C,R,w,F,D,H,K,ae,L,G,Y,U,B,oe,Z,de,re,me,pe,X,Q,ce;return w=new J({props:{code:`from transformers import TFAutoModelForSequenceClassification

model_name = "nlptown/bert-base-multilingual-uncased-sentiment"
tf_model = TFAutoModelForSequenceClassification.from_pretrained(model_name)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFAutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>model_name = <span class="hljs-string">&quot;nlptown/bert-base-multilingual-uncased-sentiment&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tf_model = TFAutoModelForSequenceClassification.from_pretrained(model_name)`}}),D=new ja({props:{$$slots:{default:[Yc]},$$scope:{ctx:m}}}),G=new J({props:{code:"tf_outputs = tf_model(tf_batch)",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>tf_outputs = tf_model(tf_batch)'}}),Q=new J({props:{code:`import tensorflow as tf

tf_predictions = tf.nn.softmax(tf_outputs.logits, axis=-1)
print(tf.math.round(tf_predictions * 10**4) / 10**4)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-meta">&gt;&gt;&gt; </span>tf_predictions = tf.nn.softmax(tf_outputs.logits, axis=-<span class="hljs-number">1</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(tf.math.<span class="hljs-built_in">round</span>(tf_predictions * <span class="hljs-number">10</span>**<span class="hljs-number">4</span>) / <span class="hljs-number">10</span>**<span class="hljs-number">4</span>)
tf.Tensor(
[[<span class="hljs-number">0.0021</span> <span class="hljs-number">0.0018</span> <span class="hljs-number">0.0116</span> <span class="hljs-number">0.2121</span> <span class="hljs-number">0.7725</span>]
 [<span class="hljs-number">0.2084</span> <span class="hljs-number">0.1826</span> <span class="hljs-number">0.1969</span> <span class="hljs-number">0.1755</span>  <span class="hljs-number">0.2365</span>]], shape=(<span class="hljs-number">2</span>, <span class="hljs-number">5</span>), dtype=float32)`}}),{c(){e=u("p"),l=i("\u{1F917} Transformers provee una forma simple y unificada de cargar tus instancias preentrenadas. Esto significa que puedes cargar un "),a=u("code"),o=i("TFAutoModel"),c=i("como cargar\xEDas un "),_=u("code"),b=i("AutoTokenizer"),E=i(" La \xFAnica diferencia es seleccionar el "),A=u("code"),y=i("TFAutoModel"),T=i("correcto para la tarea. Ya que est\xE1s clasificando texto, o secuencias, carga "),k=u("code"),C=i("TFAutoModelForSequenceClassification"),R=g(),x(w.$$.fragment),F=g(),x(D.$$.fragment),H=g(),K=u("p"),ae=i("Ahora puedes pasar tu lote preprocesado de inputs directamente al modelo pasando las llaves del diccionario directamente a los tensores:"),L=g(),x(G.$$.fragment),Y=g(),U=u("p"),B=i("El modelo producir\xE1 las activaciones finales en el atributo "),oe=u("code"),Z=i("logits"),de=i(". Aplica la funci\xF3n softmax a "),re=u("code"),me=i("logits"),pe=i(" para obtener las probabilidades:"),X=g(),x(Q.$$.fragment)},l(O){e=f(O,"P",{});var W=n(e);l=p(W,"\u{1F917} Transformers provee una forma simple y unificada de cargar tus instancias preentrenadas. Esto significa que puedes cargar un "),a=f(W,"CODE",{});var se=n(a);o=p(se,"TFAutoModel"),se.forEach(r),c=p(W,"como cargar\xEDas un "),_=f(W,"CODE",{});var z=n(_);b=p(z,"AutoTokenizer"),z.forEach(r),E=p(W," La \xFAnica diferencia es seleccionar el "),A=f(W,"CODE",{});var V=n(A);y=p(V,"TFAutoModel"),V.forEach(r),T=p(W,"correcto para la tarea. Ya que est\xE1s clasificando texto, o secuencias, carga "),k=f(W,"CODE",{});var ue=n(k);C=p(ue,"TFAutoModelForSequenceClassification"),ue.forEach(r),W.forEach(r),R=v(O),S(w.$$.fragment,O),F=v(O),S(D.$$.fragment,O),H=v(O),K=f(O,"P",{});var he=n(K);ae=p(he,"Ahora puedes pasar tu lote preprocesado de inputs directamente al modelo pasando las llaves del diccionario directamente a los tensores:"),he.forEach(r),L=v(O),S(G.$$.fragment,O),Y=v(O),U=f(O,"P",{});var ne=n(U);B=p(ne,"El modelo producir\xE1 las activaciones finales en el atributo "),oe=f(ne,"CODE",{});var te=n(oe);Z=p(te,"logits"),te.forEach(r),de=p(ne,". Aplica la funci\xF3n softmax a "),re=f(ne,"CODE",{});var ve=n(re);me=p(ve,"logits"),ve.forEach(r),pe=p(ne," para obtener las probabilidades:"),ne.forEach(r),X=v(O),S(Q.$$.fragment,O)},m(O,W){$(O,e,W),s(e,l),s(e,a),s(a,o),s(e,c),s(e,_),s(_,b),s(e,E),s(e,A),s(A,y),s(e,T),s(e,k),s(k,C),$(O,R,W),M(w,O,W),$(O,F,W),M(D,O,W),$(O,H,W),$(O,K,W),s(K,ae),$(O,L,W),M(G,O,W),$(O,Y,W),$(O,U,W),s(U,B),s(U,oe),s(oe,Z),s(U,de),s(U,re),s(re,me),s(U,pe),$(O,X,W),M(Q,O,W),ce=!0},p(O,W){const se={};W&2&&(se.$$scope={dirty:W,ctx:O}),D.$set(se)},i(O){ce||(j(w.$$.fragment,O),j(D.$$.fragment,O),j(G.$$.fragment,O),j(Q.$$.fragment,O),ce=!0)},o(O){q(w.$$.fragment,O),q(D.$$.fragment,O),q(G.$$.fragment,O),q(Q.$$.fragment,O),ce=!1},d(O){O&&r(e),O&&r(R),P(w,O),O&&r(F),P(D,O),O&&r(H),O&&r(K),O&&r(L),P(G,O),O&&r(Y),O&&r(U),O&&r(X),P(Q,O)}}}function Qc(m){let e,l;return e=new ye({props:{$$slots:{default:[Zc]},$$scope:{ctx:m}}}),{c(){x(e.$$.fragment)},l(a){S(e.$$.fragment,a)},m(a,o){M(e,a,o),l=!0},p(a,o){const c={};o&2&&(c.$$scope={dirty:o,ctx:a}),e.$set(c)},i(a){l||(j(e.$$.fragment,a),l=!0)},o(a){q(e.$$.fragment,a),l=!1},d(a){P(e,a)}}}function Xc(m){let e,l,a,o,c;return{c(){e=u("p"),l=i("Todos los modelos de \u{1F917} Transformers (PyTorch o TensorFlow) producir\xE1n los tensores "),a=u("em"),o=i("antes"),c=i(` de la funci\xF3n de activaci\xF3n
final (como softmax) porque la funci\xF3n de activaci\xF3n final es com\xFAnmente fusionada con la p\xE9rdida.`)},l(_){e=f(_,"P",{});var b=n(e);l=p(b,"Todos los modelos de \u{1F917} Transformers (PyTorch o TensorFlow) producir\xE1n los tensores "),a=f(b,"EM",{});var E=n(a);o=p(E,"antes"),E.forEach(r),c=p(b,` de la funci\xF3n de activaci\xF3n
final (como softmax) porque la funci\xF3n de activaci\xF3n final es com\xFAnmente fusionada con la p\xE9rdida.`),b.forEach(r)},m(_,b){$(_,e,b),s(e,l),s(e,a),s(a,o),s(e,c)},d(_){_&&r(e)}}}function eu(m){let e,l,a,o,c;return{c(){e=u("p"),l=i(`Los outputs del modelo de \u{1F917} Transformers son dataclasses especiales por lo que sus atributos pueden ser completados en un IDE.
Los outputs del modelo tambi\xE9n se comportan como tuplas o diccionarios (e.g., puedes indexar con un entero, un slice o una cadena) en cuyo caso los atributos que son `),a=u("code"),o=i("None"),c=i(" son ignorados.")},l(_){e=f(_,"P",{});var b=n(e);l=p(b,`Los outputs del modelo de \u{1F917} Transformers son dataclasses especiales por lo que sus atributos pueden ser completados en un IDE.
Los outputs del modelo tambi\xE9n se comportan como tuplas o diccionarios (e.g., puedes indexar con un entero, un slice o una cadena) en cuyo caso los atributos que son `),a=f(b,"CODE",{});var E=n(a);o=p(E,"None"),E.forEach(r),c=p(b," son ignorados."),b.forEach(r)},m(_,b){$(_,e,b),s(e,l),s(e,a),s(a,o),s(e,c)},d(_){_&&r(e)}}}function tu(m){let e,l,a,o,c,_,b,E,A,y,T,k,C,R;return _=new J({props:{code:`pt_save_directory = "./pt_save_pretrained"
tokenizer.save_pretrained(pt_save_directory)
pt_model.save_pretrained(pt_save_directory)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>pt_save_directory = <span class="hljs-string">&quot;./pt_save_pretrained&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.save_pretrained(pt_save_directory)
<span class="hljs-meta">&gt;&gt;&gt; </span>pt_model.save_pretrained(pt_save_directory)`}}),C=new J({props:{code:'pt_model = AutoModelForSequenceClassification.from_pretrained("./pt_save_pretrained")',highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>pt_model = AutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;./pt_save_pretrained&quot;</span>)'}}),{c(){e=u("p"),l=i("Una vez que tu modelo est\xE9 fine-tuned puedes guardarlo con tu tokenizador usando "),a=u("code"),o=i("PreTrainedModel.save_pretrained()"),c=g(),x(_.$$.fragment),b=g(),E=u("p"),A=i("Cuando quieras usar el modelo otra vez c\xE1rgalo con "),y=u("code"),T=i("PreTrainedModel.from_pretrained()"),k=g(),x(C.$$.fragment)},l(w){e=f(w,"P",{});var F=n(e);l=p(F,"Una vez que tu modelo est\xE9 fine-tuned puedes guardarlo con tu tokenizador usando "),a=f(F,"CODE",{});var D=n(a);o=p(D,"PreTrainedModel.save_pretrained()"),D.forEach(r),F.forEach(r),c=v(w),S(_.$$.fragment,w),b=v(w),E=f(w,"P",{});var H=n(E);A=p(H,"Cuando quieras usar el modelo otra vez c\xE1rgalo con "),y=f(H,"CODE",{});var K=n(y);T=p(K,"PreTrainedModel.from_pretrained()"),K.forEach(r),H.forEach(r),k=v(w),S(C.$$.fragment,w)},m(w,F){$(w,e,F),s(e,l),s(e,a),s(a,o),$(w,c,F),M(_,w,F),$(w,b,F),$(w,E,F),s(E,A),s(E,y),s(y,T),$(w,k,F),M(C,w,F),R=!0},p:le,i(w){R||(j(_.$$.fragment,w),j(C.$$.fragment,w),R=!0)},o(w){q(_.$$.fragment,w),q(C.$$.fragment,w),R=!1},d(w){w&&r(e),w&&r(c),P(_,w),w&&r(b),w&&r(E),w&&r(k),P(C,w)}}}function au(m){let e,l;return e=new ye({props:{$$slots:{default:[tu]},$$scope:{ctx:m}}}),{c(){x(e.$$.fragment)},l(a){S(e.$$.fragment,a)},m(a,o){M(e,a,o),l=!0},p(a,o){const c={};o&2&&(c.$$scope={dirty:o,ctx:a}),e.$set(c)},i(a){l||(j(e.$$.fragment,a),l=!0)},o(a){q(e.$$.fragment,a),l=!1},d(a){P(e,a)}}}function su(m){let e,l,a,o,c,_,b,E,A,y,T,k,C,R;return _=new J({props:{code:`tf_save_directory = "./tf_save_pretrained"
tokenizer.save_pretrained(tf_save_directory)
tf_model.save_pretrained(tf_save_directory)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>tf_save_directory = <span class="hljs-string">&quot;./tf_save_pretrained&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.save_pretrained(tf_save_directory)
<span class="hljs-meta">&gt;&gt;&gt; </span>tf_model.save_pretrained(tf_save_directory)`}}),C=new J({props:{code:'tf_model = TFAutoModelForSequenceClassification.from_pretrained("./tf_save_pretrained")',highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>tf_model = TFAutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;./tf_save_pretrained&quot;</span>)'}}),{c(){e=u("p"),l=i("Una vez que tu modelo est\xE9 fine-tuned puedes guardarlo con tu tokenizador usando "),a=u("code"),o=i("TFPreTrainedModel.save_pretrained()"),c=g(),x(_.$$.fragment),b=g(),E=u("p"),A=i("Cuando quieras usar el modelo otra vez c\xE1rgalo con "),y=u("code"),T=i("TFPreTrainedModel.from_pretrained()"),k=g(),x(C.$$.fragment)},l(w){e=f(w,"P",{});var F=n(e);l=p(F,"Una vez que tu modelo est\xE9 fine-tuned puedes guardarlo con tu tokenizador usando "),a=f(F,"CODE",{});var D=n(a);o=p(D,"TFPreTrainedModel.save_pretrained()"),D.forEach(r),F.forEach(r),c=v(w),S(_.$$.fragment,w),b=v(w),E=f(w,"P",{});var H=n(E);A=p(H,"Cuando quieras usar el modelo otra vez c\xE1rgalo con "),y=f(H,"CODE",{});var K=n(y);T=p(K,"TFPreTrainedModel.from_pretrained()"),K.forEach(r),H.forEach(r),k=v(w),S(C.$$.fragment,w)},m(w,F){$(w,e,F),s(e,l),s(e,a),s(a,o),$(w,c,F),M(_,w,F),$(w,b,F),$(w,E,F),s(E,A),s(E,y),s(y,T),$(w,k,F),M(C,w,F),R=!0},p:le,i(w){R||(j(_.$$.fragment,w),j(C.$$.fragment,w),R=!0)},o(w){q(_.$$.fragment,w),q(C.$$.fragment,w),R=!1},d(w){w&&r(e),w&&r(c),P(_,w),w&&r(b),w&&r(E),w&&r(k),P(C,w)}}}function ru(m){let e,l;return e=new ye({props:{$$slots:{default:[su]},$$scope:{ctx:m}}}),{c(){x(e.$$.fragment)},l(a){S(e.$$.fragment,a)},m(a,o){M(e,a,o),l=!0},p(a,o){const c={};o&2&&(c.$$scope={dirty:o,ctx:a}),e.$set(c)},i(a){l||(j(e.$$.fragment,a),l=!0)},o(a){q(e.$$.fragment,a),l=!1},d(a){P(e,a)}}}function ou(m){let e,l;return e=new J({props:{code:`from transformers import AutoModel

tokenizer = AutoTokenizer.from_pretrained(tf_save_directory)
pt_model = AutoModelForSequenceClassification.from_pretrained(tf_save_directory, from_tf=True)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModel

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(tf_save_directory)
<span class="hljs-meta">&gt;&gt;&gt; </span>pt_model = AutoModelForSequenceClassification.from_pretrained(tf_save_directory, from_tf=<span class="hljs-literal">True</span>)`}}),{c(){x(e.$$.fragment)},l(a){S(e.$$.fragment,a)},m(a,o){M(e,a,o),l=!0},p:le,i(a){l||(j(e.$$.fragment,a),l=!0)},o(a){q(e.$$.fragment,a),l=!1},d(a){P(e,a)}}}function lu(m){let e,l;return e=new ye({props:{$$slots:{default:[ou]},$$scope:{ctx:m}}}),{c(){x(e.$$.fragment)},l(a){S(e.$$.fragment,a)},m(a,o){M(e,a,o),l=!0},p(a,o){const c={};o&2&&(c.$$scope={dirty:o,ctx:a}),e.$set(c)},i(a){l||(j(e.$$.fragment,a),l=!0)},o(a){q(e.$$.fragment,a),l=!1},d(a){P(e,a)}}}function nu(m){let e,l;return e=new J({props:{code:`from transformers import TFAutoModel

tokenizer = AutoTokenizer.from_pretrained(pt_save_directory)
tf_model = TFAutoModelForSequenceClassification.from_pretrained(pt_save_directory, from_pt=True)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFAutoModel

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(pt_save_directory)
<span class="hljs-meta">&gt;&gt;&gt; </span>tf_model = TFAutoModelForSequenceClassification.from_pretrained(pt_save_directory, from_pt=<span class="hljs-literal">True</span>)`}}),{c(){x(e.$$.fragment)},l(a){S(e.$$.fragment,a)},m(a,o){M(e,a,o),l=!0},p:le,i(a){l||(j(e.$$.fragment,a),l=!0)},o(a){q(e.$$.fragment,a),l=!1},d(a){P(e,a)}}}function iu(m){let e,l;return e=new ye({props:{$$slots:{default:[nu]},$$scope:{ctx:m}}}),{c(){x(e.$$.fragment)},l(a){S(e.$$.fragment,a)},m(a,o){M(e,a,o),l=!0},p(a,o){const c={};o&2&&(c.$$scope={dirty:o,ctx:a}),e.$set(c)},i(a){l||(j(e.$$.fragment,a),l=!0)},o(a){q(e.$$.fragment,a),l=!1},d(a){P(e,a)}}}function pu(m){let e,l,a,o,c,_,b,E,A,y,T,k,C,R,w,F,D,H,K,ae,L,G,Y,U,B,oe,Z,de,re,me,pe,X,Q,ce,O,W,se,z,V,ue,he,ne,te,ve,ie,$e,Ae,qe,we,ee,Ce,lo,no,qa,io,po,Ca,co,uo,Ta,fo,mo,za,ho,$o,xa,_o,go,Ma,vo,Eo,Pa,bo,Ls,wt,Sa,wo,ko,Hs,Te,Oa,jo,yo,Da,Ao,qo,Ia,Co,Us,kt,Na,To,zo,Rs,Ye,Fa,xo,Mo,La,Po,Ws,Ze,Vs,Fe,Qe,Ha,jt,So,Ua,Oo,Ks,Xe,Do,Ra,Io,No,Gs,ea,Fo,Bs,et,Js,tt,Lo,Wa,Ho,Uo,Ys,yt,Zs,ze,Ro,At,Wo,Vo,Va,Ko,Go,Qs,qt,Xs,at,Bo,Ka,Jo,Yo,er,Ct,tr,xe,Zo,Ga,Qo,Xo,Tt,el,tl,ar,zt,sr,je,al,Ba,sl,rl,Ja,ol,ll,Ya,nl,il,rr,xt,or,Me,pl,Mt,cl,ul,Pt,fl,dl,lr,St,nr,ta,ml,ir,Ot,pr,st,hl,aa,$l,_l,cr,Le,rt,Za,Dt,gl,Qa,vl,ur,Ee,El,Xa,bl,wl,It,kl,jl,es,yl,Al,Nt,ql,Cl,fr,Ft,dr,ot,mr,Pe,Tl,ts,zl,xl,as,Ml,Pl,hr,Lt,$r,Se,Sl,sa,Ol,Dl,ra,Il,Nl,_r,He,lt,ss,Ht,Fl,rs,Ll,gr,Ut,vr,_e,Hl,os,Ul,Rl,ls,Wl,Vl,ns,Kl,Gl,oa,Bl,Jl,is,Yl,Zl,ps,Ql,Er,Ue,Xl,cs,en,tn,us,an,br,Re,nt,fs,Rt,sn,ds,rn,wr,Oe,on,ms,ln,nn,la,pn,cn,kr,Wt,un,hs,fn,jr,Vt,yr,it,dn,$s,mn,hn,Ar,na,$n,qr,Kt,Cr,ia,_n,Tr,pt,pa,ca,gn,vn,En,ua,fa,bn,wn,zr,ct,kn,_s,jn,yn,xr,ut,Mr,ft,An,da,qn,Cn,Pr,We,dt,gs,Gt,Tn,vs,zn,Sr,mt,Or,ht,Dr,fe,xn,Bt,Es,Mn,Pn,Jt,bs,Sn,On,ws,Dn,In,ks,Nn,Fn,Yt,Ln,Hn,ma,Un,Rn,Ir,$t,Nr,Ve,_t,js,Zt,Wn,ys,Vn,Fr,gt,Lr,De,Kn,As,Gn,Bn,qs,Jn,Yn,Hr,vt,Ur;return _=new bt({}),T=new tc({props:{classNames:"absolute z-10 right-0 top-0",options:[{label:"Mixed",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/es/quicktour.ipynb"},{label:"PyTorch",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/es/pytorch/quicktour.ipynb"},{label:"TensorFlow",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/es/tensorflow/quicktour.ipynb"},{label:"Mixed",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/transformers_doc/es/quicktour.ipynb"},{label:"PyTorch",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/transformers_doc/es/pytorch/quicktour.ipynb"},{label:"TensorFlow",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/transformers_doc/es/tensorflow/quicktour.ipynb"}]}}),G=new ja({props:{$$slots:{default:[Pc]},$$scope:{ctx:m}}}),Z=new bt({}),se=new Hp({props:{id:"tiZFewofSLM"}}),Ze=new ja({props:{$$slots:{default:[Sc]},$$scope:{ctx:m}}}),jt=new bt({}),et=new ka({props:{pytorch:!0,tensorflow:!0,jax:!1,$$slots:{tensorflow:[Nc],pytorch:[Dc]},$$scope:{ctx:m}}}),yt=new J({props:{code:`from transformers import pipeline

classifier = pipeline("sentiment-analysis")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline

<span class="hljs-meta">&gt;&gt;&gt; </span>classifier = pipeline(<span class="hljs-string">&quot;sentiment-analysis&quot;</span>)`}}),qt=new J({props:{code:'classifier("We are very happy to show you the \u{1F917} Transformers library.")',highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>classifier(<span class="hljs-string">&quot;We are very happy to show you the \u{1F917} Transformers library.&quot;</span>)
[{<span class="hljs-string">&#x27;label&#x27;</span>: <span class="hljs-string">&#x27;POSITIVE&#x27;</span>, <span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.9998</span>}]`}}),Ct=new J({props:{code:`results = classifier(["We are very happy to show you the \u{1F917} Transformers library.", "We hope you don't hate it."])
for result in results:
    print(f"label: {result['label']}, with score: {round(result['score'], 4)}")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>results = classifier([<span class="hljs-string">&quot;We are very happy to show you the \u{1F917} Transformers library.&quot;</span>, <span class="hljs-string">&quot;We hope you don&#x27;t hate it.&quot;</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">for</span> result <span class="hljs-keyword">in</span> results:
<span class="hljs-meta">... </span>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;label: <span class="hljs-subst">{result[<span class="hljs-string">&#x27;label&#x27;</span>]}</span>, with score: <span class="hljs-subst">{<span class="hljs-built_in">round</span>(result[<span class="hljs-string">&#x27;score&#x27;</span>], <span class="hljs-number">4</span>)}</span>&quot;</span>)
label: POSITIVE, <span class="hljs-keyword">with</span> score: <span class="hljs-number">0.9998</span>
label: NEGATIVE, <span class="hljs-keyword">with</span> score: <span class="hljs-number">0.5309</span>`}}),zt=new J({props:{code:"pip install datasets",highlighted:"pip install datasets"}}),xt=new J({props:{code:`import torch
from transformers import pipeline

speech_recognizer = pipeline("automatic-speech-recognition", model="facebook/wav2vec2-base-960h", device=0)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline

<span class="hljs-meta">&gt;&gt;&gt; </span>speech_recognizer = pipeline(<span class="hljs-string">&quot;automatic-speech-recognition&quot;</span>, model=<span class="hljs-string">&quot;facebook/wav2vec2-base-960h&quot;</span>, device=<span class="hljs-number">0</span>)`}}),St=new J({props:{code:`import datasets

dataset = datasets.load_dataset("PolyAI/minds14", name="en-US", split="train")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> datasets

<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = datasets.load_dataset(<span class="hljs-string">&quot;PolyAI/minds14&quot;</span>, name=<span class="hljs-string">&quot;en-US&quot;</span>, split=<span class="hljs-string">&quot;train&quot;</span>)`}}),Ot=new J({props:{code:`files = dataset["path"]
speech_recognizer(files[:4])`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>files = dataset[<span class="hljs-string">&quot;path&quot;</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>speech_recognizer(files[:<span class="hljs-number">4</span>])
[{<span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;I WOULD LIKE TO SET UP A JOINT ACCOUNT WITH MY PARTNER HOW DO I PROCEED WITH DOING THAT&#x27;</span>}, 
 {<span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&quot;FONDERING HOW I&#x27;D SET UP A JOIN TO HELL T WITH MY WIFE AND WHERE THE AP MIGHT BE&quot;</span>}, 
 {<span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&quot;I I&#x27;D LIKE TOY SET UP A JOINT ACCOUNT WITH MY PARTNER I&#x27;M NOT SEEING THE OPTION TO DO IT ON THE APSO I CALLED IN TO GET SOME HELP CAN I JUST DO IT OVER THE PHONE WITH YOU AND GIVE YOU THE INFORMATION OR SHOULD I DO IT IN THE AP AN I&#x27;M MISSING SOMETHING UQUETTE HAD PREFERRED TO JUST DO IT OVER THE PHONE OF POSSIBLE THINGS&quot;</span>},`}}),Dt=new bt({}),Ft=new J({props:{code:'model_name = "nlptown/bert-base-multilingual-uncased-sentiment"',highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>model_name = <span class="hljs-string">&quot;nlptown/bert-base-multilingual-uncased-sentiment&quot;</span>'}}),ot=new ka({props:{pytorch:!0,tensorflow:!0,jax:!1,$$slots:{tensorflow:[Uc],pytorch:[Lc]},$$scope:{ctx:m}}}),Lt=new J({props:{code:`classifier = pipeline("sentiment-analysis", model=model, tokenizer=tokenizer)
classifier("Nous sommes tr\xE8s heureux de vous pr\xE9senter la biblioth\xE8que \u{1F917} Transformers.")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>classifier = pipeline(<span class="hljs-string">&quot;sentiment-analysis&quot;</span>, model=model, tokenizer=tokenizer)
<span class="hljs-meta">&gt;&gt;&gt; </span>classifier(<span class="hljs-string">&quot;Nous sommes tr\xE8s heureux de vous pr\xE9senter la biblioth\xE8que \u{1F917} Transformers.&quot;</span>)
[{<span class="hljs-string">&#x27;label&#x27;</span>: <span class="hljs-string">&#x27;5 stars&#x27;</span>, <span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.7273</span>}]`}}),Ht=new bt({}),Ut=new Hp({props:{id:"AhChOFRegn4"}}),Rt=new bt({}),Vt=new J({props:{code:`from transformers import AutoTokenizer

model_name = "nlptown/bert-base-multilingual-uncased-sentiment"
tokenizer = AutoTokenizer.from_pretrained(model_name)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>model_name = <span class="hljs-string">&quot;nlptown/bert-base-multilingual-uncased-sentiment&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(model_name)`}}),Kt=new J({props:{code:`encoding = tokenizer("We are very happy to show you the \u{1F917} Transformers library.")
print(encoding)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>encoding = tokenizer(<span class="hljs-string">&quot;We are very happy to show you the \u{1F917} Transformers library.&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(encoding)
{<span class="hljs-string">&#x27;input_ids&#x27;</span>: [<span class="hljs-number">101</span>, <span class="hljs-number">11312</span>, <span class="hljs-number">10320</span>, <span class="hljs-number">12495</span>, <span class="hljs-number">19308</span>, <span class="hljs-number">10114</span>, <span class="hljs-number">11391</span>, <span class="hljs-number">10855</span>, <span class="hljs-number">10103</span>, <span class="hljs-number">100</span>, <span class="hljs-number">58263</span>, <span class="hljs-number">13299</span>, <span class="hljs-number">119</span>, <span class="hljs-number">102</span>],
 <span class="hljs-string">&#x27;token_type_ids&#x27;</span>: [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
 <span class="hljs-string">&#x27;attention_mask&#x27;</span>: [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]}`}}),ut=new ka({props:{pytorch:!0,tensorflow:!0,jax:!1,$$slots:{tensorflow:[Kc],pytorch:[Wc]},$$scope:{ctx:m}}}),Gt=new bt({}),mt=new ka({props:{pytorch:!0,tensorflow:!0,jax:!1,$$slots:{tensorflow:[Qc],pytorch:[Jc]},$$scope:{ctx:m}}}),ht=new ja({props:{$$slots:{default:[Xc]},$$scope:{ctx:m}}}),$t=new ja({props:{$$slots:{default:[eu]},$$scope:{ctx:m}}}),Zt=new bt({}),gt=new ka({props:{pytorch:!0,tensorflow:!0,jax:!1,$$slots:{tensorflow:[ru],pytorch:[au]},$$scope:{ctx:m}}}),vt=new ka({props:{pytorch:!0,tensorflow:!0,jax:!1,$$slots:{tensorflow:[iu],pytorch:[lu]},$$scope:{ctx:m}}}),{c(){e=u("meta"),l=g(),a=u("h1"),o=u("a"),c=u("span"),x(_.$$.fragment),b=g(),E=u("span"),A=i("Quick tour"),y=g(),x(T.$$.fragment),k=g(),C=u("p"),R=i("\xA1Entra en marcha con los \u{1F917} Transformers! Comienza usando "),w=u("code"),F=i("pipeline()"),D=i("para una inferencia veloz, carga un modelo preentrenado y un tokenizador con una "),H=u("a"),K=i("AutoClass"),ae=i(" para resolver tu tarea de texto, visi\xF3n o audio."),L=g(),x(G.$$.fragment),Y=g(),U=u("h2"),B=u("a"),oe=u("span"),x(Z.$$.fragment),de=g(),re=u("span"),me=i("Pipeline"),pe=g(),X=u("p"),Q=u("code"),ce=i("pipeline()"),O=i("es la forma m\xE1s f\xE1cil de usar un modelo preentrenado para una tarea dada."),W=g(),x(se.$$.fragment),z=g(),V=u("p"),ue=i("El "),he=u("code"),ne=i("pipeline()"),te=i("soporta muchas tareas comunes listas para usar:"),ve=g(),ie=u("p"),$e=u("strong"),Ae=i("Texto"),qe=i(":"),we=g(),ee=u("ul"),Ce=u("li"),lo=i("An\xE1lisis de Sentimientos: clasifica la polaridad de un texto dado."),no=g(),qa=u("li"),io=i("Generaci\xF3n de texto (solo en ingl\xE9s): genera texto a partir de un input dado."),po=g(),Ca=u("li"),co=i("Name entity recognition (NER): etiqueta cada palabra con la entidad que representa (persona, fecha, ubicaci\xF3n, etc.)."),uo=g(),Ta=u("li"),fo=i("Responder preguntas: extrae la respuesta del contexto dado un contexto y una pregunta."),mo=g(),za=u("li"),ho=i("Fill-mask: rellena el espacio faltante dado un texto con palabras enmascaradas."),$o=g(),xa=u("li"),_o=i("Summarization: genera un resumen de una secuencia larga de texto o un documento."),go=g(),Ma=u("li"),vo=i("Traducci\xF3n: traduce un texto a otro idioma."),Eo=g(),Pa=u("li"),bo=i("Extracci\xF3n de caracter\xEDsticas: crea una representaci\xF3n tensorial del texto."),Ls=g(),wt=u("p"),Sa=u("strong"),wo=i("Imagen"),ko=i(":"),Hs=g(),Te=u("ul"),Oa=u("li"),jo=i("Clasificaci\xF3n de im\xE1genes: clasifica una imagen."),yo=g(),Da=u("li"),Ao=i("Segmentaci\xF3n de im\xE1genes: clasifica cada pixel de una imagen."),qo=g(),Ia=u("li"),Co=i("Detecci\xF3n de objetos: detecta objetos dentro de una imagen."),Us=g(),kt=u("p"),Na=u("strong"),To=i("Audio"),zo=i(":"),Rs=g(),Ye=u("ul"),Fa=u("li"),xo=i("Clasificaci\xF3n de audios: asigna una etiqueta a un segmento de audio."),Mo=g(),La=u("li"),Po=i("Automatic speech recognition (ASR): transcribe datos de audio a un texto."),Ws=g(),x(Ze.$$.fragment),Vs=g(),Fe=u("h3"),Qe=u("a"),Ha=u("span"),x(jt.$$.fragment),So=g(),Ua=u("span"),Oo=i("Uso del Pipeline"),Ks=g(),Xe=u("p"),Do=i("En el siguiente ejemplo, usar\xE1s el "),Ra=u("code"),Io=i("pipeline()"),No=i("para an\xE1lisis de sentimiento."),Gs=g(),ea=u("p"),Fo=i("Instala las siguientes dependencias si a\xFAn no lo has hecho:"),Bs=g(),x(et.$$.fragment),Js=g(),tt=u("p"),Lo=i("Importa "),Wa=u("code"),Ho=i("pipeline()"),Uo=i("y especifica la tarea que deseas completar:"),Ys=g(),x(yt.$$.fragment),Zs=g(),ze=u("p"),Ro=i("El pipeline descarga y almacena en cach\xE9 un "),At=u("a"),Wo=i("modelo preentrenado"),Vo=i(" por defecto y tokeniza para an\xE1lisis de sentimiento. Ahora puedes usar "),Va=u("code"),Ko=i("classifier"),Go=i(" en tu texto objetivo:"),Qs=g(),x(qt.$$.fragment),Xs=g(),at=u("p"),Bo=i("Para m\xE1s de un enunciado entrega una lista de frases al "),Ka=u("code"),Jo=i("pipeline()"),Yo=i("que devolver\xE1 una lista de diccionarios:"),er=g(),x(Ct.$$.fragment),tr=g(),xe=u("p"),Zo=i("El "),Ga=u("code"),Qo=i("pipeline()"),Xo=i("tambi\xE9n puede iterar sobre un dataset entero. Comienza instalando la biblioteca "),Tt=u("a"),el=i("\u{1F917} Datasets"),tl=i(":"),ar=g(),x(zt.$$.fragment),sr=g(),je=u("p"),al=i("Crea un "),Ba=u("code"),sl=i("pipeline()"),rl=i("con la tarea que deseas resolver y el modelo que quieres usar. Coloca el par\xE1metro "),Ja=u("code"),ol=i("device"),ll=i(" a "),Ya=u("code"),nl=i("0"),il=i(" para poner los tensores en un dispositivo CUDA:"),rr=g(),x(xt.$$.fragment),or=g(),Me=u("p"),pl=i("A continuaci\xF3n, carga el dataset (ve \u{1F917} Datasets "),Mt=u("a"),cl=i("Quick Start"),ul=i(" para m\xE1s detalles) sobre el que quisieras iterar. Por ejemplo, vamos a cargar el dataset "),Pt=u("a"),fl=i("MInDS-14"),dl=i(":"),lr=g(),x(St.$$.fragment),nr=g(),ta=u("p"),ml=i("Puedes pasar un pipeline para un dataset:"),ir=g(),x(Ot.$$.fragment),pr=g(),st=u("p"),hl=i("Para un dataset m\xE1s grande, donde los inputs son de mayor tama\xF1o (como en habla/audio o visi\xF3n), querr\xE1s pasar un generador en lugar de una lista que carga todos los inputs en memoria. Ve la "),aa=u("a"),$l=i("documentaci\xF3n del pipeline"),_l=i(" para m\xE1s informaci\xF3n."),cr=g(),Le=u("h3"),rt=u("a"),Za=u("span"),x(Dt.$$.fragment),gl=g(),Qa=u("span"),vl=i("Use otro modelo y otro tokenizador en el pipeline"),ur=g(),Ee=u("p"),El=i("El "),Xa=u("code"),bl=i("pipeline()"),wl=i("puede adaptarse a cualquier modelo del "),It=u("a"),kl=i("Model Hub"),jl=i(" haciendo m\xE1s f\xE1cil adaptar el "),es=u("code"),yl=i("pipeline()"),Al=i("para otros casos de uso. Por ejemplo, si quisieras un modelo capaz de manejar texto en franc\xE9s, usa los tags en el Model Hub para filtrar entre los modelos apropiados. El resultado mejor filtrado devuelve un "),Nt=u("a"),ql=i("modelo BERT"),Cl=i(" multilingual fine-tuned para el an\xE1lisis de sentimiento. Genial, \xA1vamos a usar este modelo!"),fr=g(),x(Ft.$$.fragment),dr=g(),x(ot.$$.fragment),mr=g(),Pe=u("p"),Tl=i("Despu\xE9s puedes especificar el modelo y el tokenizador en el "),ts=u("code"),zl=i("pipeline()"),xl=i(" y aplicar el "),as=u("code"),Ml=i("classifier"),Pl=i(" en tu texto objetivo:"),hr=g(),x(Lt.$$.fragment),$r=g(),Se=u("p"),Sl=i("Si no pudieras encontrar el modelo para tu caso respectivo de uso necesitar\xE1s ajustar un modelo preentrenado a tus datos. Mira nuestro "),sa=u("a"),Ol=i("tutorial de fine-tuning"),Dl=i(" para aprender c\xF3mo. Finalmente, despu\xE9s de que has ajustado tu modelo preentrenado, \xA1por favor considera compartirlo (ve el tutorial "),ra=u("a"),Il=i("aqu\xED"),Nl=i(") con la comunidad en el Model Hub para democratizar el NLP! \u{1F917}"),_r=g(),He=u("h2"),lt=u("a"),ss=u("span"),x(Ht.$$.fragment),Fl=g(),rs=u("span"),Ll=i("AutoClass"),gr=g(),x(Ut.$$.fragment),vr=g(),_e=u("p"),Hl=i("Debajo del cap\xF3, las clases "),os=u("code"),Ul=i("AutoModelForSequenceClassification"),Rl=i("y "),ls=u("code"),Wl=i("AutoTokenizer"),Vl=i("trabajan juntas para dar poder al "),ns=u("code"),Kl=i("pipeline()"),Gl=i(" Una "),oa=u("a"),Bl=i("AutoClass"),Jl=i(" es un atajo que autom\xE1ticamente recupera la arquitectura de un modelo preentrenado con su nombre o el path. S\xF3lo necesitar\xE1s seleccionar el "),is=u("code"),Yl=i("AutoClass"),Zl=i(" apropiado para tu tarea y tu tokenizador asociado con "),ps=u("code"),Ql=i("AutoTokenizer"),Er=g(),Ue=u("p"),Xl=i("Regresemos a nuestro ejemplo y veamos c\xF3mo puedes usar el "),cs=u("code"),en=i("AutoClass"),tn=i(" para reproducir los resultados del "),us=u("code"),an=i("pipeline()"),br=g(),Re=u("h3"),nt=u("a"),fs=u("span"),x(Rt.$$.fragment),sn=g(),ds=u("span"),rn=i("AutoTokenizer"),wr=g(),Oe=u("p"),on=i("Un tokenizador es responsable de procesar el texto a un formato que sea entendible para el modelo. Primero, el tokenizador separar\xE1 el texto en palabras llamadas "),ms=u("em"),ln=i("tokens"),nn=i(". Hay m\xFAltiples reglas que gobiernan el proceso de tokenizaci\xF3n incluyendo el c\xF3mo separar una palabra y en qu\xE9 nivel (aprende m\xE1s sobre tokenizaci\xF3n "),la=u("a"),pn=i("aqu\xED"),cn=i("). Lo m\xE1s importante es recordar que necesitar\xE1s instanciar el tokenizador con el mismo nombre del modelo para asegurar que est\xE1s usando las mismas reglas de tokenizaci\xF3n con las que el modelo fue preentrenado."),kr=g(),Wt=u("p"),un=i("Carga un tokenizador con "),hs=u("code"),fn=i("AutoTokenizer"),jr=g(),x(Vt.$$.fragment),yr=g(),it=u("p"),dn=i("Despu\xE9s, el tokenizador convierte los tokens a n\xFAmeros para construir un tensor que servir\xE1 como input para el modelo. Esto es conocido como el "),$s=u("em"),mn=i("vocabulario"),hn=i(" del modelo."),Ar=g(),na=u("p"),$n=i("Pasa tu texto al tokenizador:"),qr=g(),x(Kt.$$.fragment),Cr=g(),ia=u("p"),_n=i("El tokenizador devolver\xE1 un diccionario conteniendo:"),Tr=g(),pt=u("ul"),pa=u("li"),ca=u("a"),gn=i("input_ids"),vn=i(": representaciones num\xE9ricas de los tokens."),En=g(),ua=u("li"),fa=u("a"),bn=i("atttention_mask"),wn=i(": indica cu\xE1les tokens deben ser atendidos."),zr=g(),ct=u("p"),kn=i("Como con el "),_s=u("code"),jn=i("pipeline()"),yn=i(" el tokenizador aceptar\xE1 una lista de inputs. Adem\xE1s, el tokenizador tambi\xE9n puede rellenar (pad, en ingl\xE9s) y truncar el texto para devolver un lote (batch, en ingl\xE9s) de longitud uniforme:"),xr=g(),x(ut.$$.fragment),Mr=g(),ft=u("p"),An=i("Lee el tutorial de "),da=u("a"),qn=i("preprocessing"),Cn=i(" para m\xE1s detalles acerca de la tokenizaci\xF3n."),Pr=g(),We=u("h3"),dt=u("a"),gs=u("span"),x(Gt.$$.fragment),Tn=g(),vs=u("span"),zn=i("AutoModel"),Sr=g(),x(mt.$$.fragment),Or=g(),x(ht.$$.fragment),Dr=g(),fe=u("p"),xn=i("Los modelos son "),Bt=u("a"),Es=u("code"),Mn=i("torch.nn.Module"),Pn=i(" o "),Jt=u("a"),bs=u("code"),Sn=i("tf.keras.Model"),On=i(" est\xE1ndares as\xED que podr\xE1s usarlos en tu training loop usual. Sin embargo, para facilitar las cosas, \u{1F917} Transformers provee una clase "),ws=u("code"),Dn=i("Trainer"),In=i("para PyTorch que a\xF1ade funcionalidades para entrenamiento distribuido, precici\xF3n mixta, y m\xE1s. Para TensorFlow, puedes usar el m\xE9todo "),ks=u("code"),Nn=i("fit"),Fn=i(" desde "),Yt=u("a"),Ln=i("Keras"),Hn=i(". Consulta el "),ma=u("a"),Un=i("tutorial de entrenamiento"),Rn=i(" para m\xE1s detalles."),Ir=g(),x($t.$$.fragment),Nr=g(),Ve=u("h3"),_t=u("a"),js=u("span"),x(Zt.$$.fragment),Wn=g(),ys=u("span"),Vn=i("Guarda un modelo"),Fr=g(),x(gt.$$.fragment),Lr=g(),De=u("p"),Kn=i("Una caracter\xEDstica particularmente cool de \u{1F917} Transformers es la habilidad de guardar el modelo y cargarlo como un modelo de PyTorch o TensorFlow. El par\xE1metro "),As=u("code"),Gn=i("from_pt"),Bn=i(" o "),qs=u("code"),Jn=i("from_tf"),Yn=i(" puede convertir el modelo de un framework al otro:"),Hr=g(),x(vt.$$.fragment),this.h()},l(t){const h=ec('[data-svelte="svelte-1phssyn"]',document.head);e=f(h,"META",{name:!0,content:!0}),h.forEach(r),l=v(t),a=f(t,"H1",{class:!0});var Qt=n(a);o=f(Qt,"A",{id:!0,class:!0,href:!0});var Cs=n(o);c=f(Cs,"SPAN",{});var Ts=n(c);S(_.$$.fragment,Ts),Ts.forEach(r),Cs.forEach(r),b=v(Qt),E=f(Qt,"SPAN",{});var zs=n(E);A=p(zs,"Quick tour"),zs.forEach(r),Qt.forEach(r),y=v(t),S(T.$$.fragment,t),k=v(t),C=f(t,"P",{});var Ke=n(C);R=p(Ke,"\xA1Entra en marcha con los \u{1F917} Transformers! Comienza usando "),w=f(Ke,"CODE",{});var xs=n(w);F=p(xs,"pipeline()"),xs.forEach(r),D=p(Ke,"para una inferencia veloz, carga un modelo preentrenado y un tokenizador con una "),H=f(Ke,"A",{href:!0});var Ms=n(H);K=p(Ms,"AutoClass"),Ms.forEach(r),ae=p(Ke," para resolver tu tarea de texto, visi\xF3n o audio."),Ke.forEach(r),L=v(t),S(G.$$.fragment,t),Y=v(t),U=f(t,"H2",{class:!0});var Xt=n(U);B=f(Xt,"A",{id:!0,class:!0,href:!0});var Ps=n(B);oe=f(Ps,"SPAN",{});var Ss=n(oe);S(Z.$$.fragment,Ss),Ss.forEach(r),Ps.forEach(r),de=v(Xt),re=f(Xt,"SPAN",{});var ni=n(re);me=p(ni,"Pipeline"),ni.forEach(r),Xt.forEach(r),pe=v(t),X=f(t,"P",{});var Zn=n(X);Q=f(Zn,"CODE",{});var ii=n(Q);ce=p(ii,"pipeline()"),ii.forEach(r),O=p(Zn,"es la forma m\xE1s f\xE1cil de usar un modelo preentrenado para una tarea dada."),Zn.forEach(r),W=v(t),S(se.$$.fragment,t),z=v(t),V=f(t,"P",{});var Rr=n(V);ue=p(Rr,"El "),he=f(Rr,"CODE",{});var pi=n(he);ne=p(pi,"pipeline()"),pi.forEach(r),te=p(Rr,"soporta muchas tareas comunes listas para usar:"),Rr.forEach(r),ve=v(t),ie=f(t,"P",{});var Qn=n(ie);$e=f(Qn,"STRONG",{});var ci=n($e);Ae=p(ci,"Texto"),ci.forEach(r),qe=p(Qn,":"),Qn.forEach(r),we=v(t),ee=f(t,"UL",{});var ge=n(ee);Ce=f(ge,"LI",{});var ui=n(Ce);lo=p(ui,"An\xE1lisis de Sentimientos: clasifica la polaridad de un texto dado."),ui.forEach(r),no=v(ge),qa=f(ge,"LI",{});var fi=n(qa);io=p(fi,"Generaci\xF3n de texto (solo en ingl\xE9s): genera texto a partir de un input dado."),fi.forEach(r),po=v(ge),Ca=f(ge,"LI",{});var di=n(Ca);co=p(di,"Name entity recognition (NER): etiqueta cada palabra con la entidad que representa (persona, fecha, ubicaci\xF3n, etc.)."),di.forEach(r),uo=v(ge),Ta=f(ge,"LI",{});var mi=n(Ta);fo=p(mi,"Responder preguntas: extrae la respuesta del contexto dado un contexto y una pregunta."),mi.forEach(r),mo=v(ge),za=f(ge,"LI",{});var hi=n(za);ho=p(hi,"Fill-mask: rellena el espacio faltante dado un texto con palabras enmascaradas."),hi.forEach(r),$o=v(ge),xa=f(ge,"LI",{});var $i=n(xa);_o=p($i,"Summarization: genera un resumen de una secuencia larga de texto o un documento."),$i.forEach(r),go=v(ge),Ma=f(ge,"LI",{});var _i=n(Ma);vo=p(_i,"Traducci\xF3n: traduce un texto a otro idioma."),_i.forEach(r),Eo=v(ge),Pa=f(ge,"LI",{});var gi=n(Pa);bo=p(gi,"Extracci\xF3n de caracter\xEDsticas: crea una representaci\xF3n tensorial del texto."),gi.forEach(r),ge.forEach(r),Ls=v(t),wt=f(t,"P",{});var Xn=n(wt);Sa=f(Xn,"STRONG",{});var vi=n(Sa);wo=p(vi,"Imagen"),vi.forEach(r),ko=p(Xn,":"),Xn.forEach(r),Hs=v(t),Te=f(t,"UL",{});var ha=n(Te);Oa=f(ha,"LI",{});var Ei=n(Oa);jo=p(Ei,"Clasificaci\xF3n de im\xE1genes: clasifica una imagen."),Ei.forEach(r),yo=v(ha),Da=f(ha,"LI",{});var bi=n(Da);Ao=p(bi,"Segmentaci\xF3n de im\xE1genes: clasifica cada pixel de una imagen."),bi.forEach(r),qo=v(ha),Ia=f(ha,"LI",{});var wi=n(Ia);Co=p(wi,"Detecci\xF3n de objetos: detecta objetos dentro de una imagen."),wi.forEach(r),ha.forEach(r),Us=v(t),kt=f(t,"P",{});var ei=n(kt);Na=f(ei,"STRONG",{});var ki=n(Na);To=p(ki,"Audio"),ki.forEach(r),zo=p(ei,":"),ei.forEach(r),Rs=v(t),Ye=f(t,"UL",{});var Wr=n(Ye);Fa=f(Wr,"LI",{});var ji=n(Fa);xo=p(ji,"Clasificaci\xF3n de audios: asigna una etiqueta a un segmento de audio."),ji.forEach(r),Mo=v(Wr),La=f(Wr,"LI",{});var yi=n(La);Po=p(yi,"Automatic speech recognition (ASR): transcribe datos de audio a un texto."),yi.forEach(r),Wr.forEach(r),Ws=v(t),S(Ze.$$.fragment,t),Vs=v(t),Fe=f(t,"H3",{class:!0});var Vr=n(Fe);Qe=f(Vr,"A",{id:!0,class:!0,href:!0});var Ai=n(Qe);Ha=f(Ai,"SPAN",{});var qi=n(Ha);S(jt.$$.fragment,qi),qi.forEach(r),Ai.forEach(r),So=v(Vr),Ua=f(Vr,"SPAN",{});var Ci=n(Ua);Oo=p(Ci,"Uso del Pipeline"),Ci.forEach(r),Vr.forEach(r),Ks=v(t),Xe=f(t,"P",{});var Kr=n(Xe);Do=p(Kr,"En el siguiente ejemplo, usar\xE1s el "),Ra=f(Kr,"CODE",{});var Ti=n(Ra);Io=p(Ti,"pipeline()"),Ti.forEach(r),No=p(Kr,"para an\xE1lisis de sentimiento."),Kr.forEach(r),Gs=v(t),ea=f(t,"P",{});var zi=n(ea);Fo=p(zi,"Instala las siguientes dependencias si a\xFAn no lo has hecho:"),zi.forEach(r),Bs=v(t),S(et.$$.fragment,t),Js=v(t),tt=f(t,"P",{});var Gr=n(tt);Lo=p(Gr,"Importa "),Wa=f(Gr,"CODE",{});var xi=n(Wa);Ho=p(xi,"pipeline()"),xi.forEach(r),Uo=p(Gr,"y especifica la tarea que deseas completar:"),Gr.forEach(r),Ys=v(t),S(yt.$$.fragment,t),Zs=v(t),ze=f(t,"P",{});var $a=n(ze);Ro=p($a,"El pipeline descarga y almacena en cach\xE9 un "),At=f($a,"A",{href:!0,rel:!0});var Mi=n(At);Wo=p(Mi,"modelo preentrenado"),Mi.forEach(r),Vo=p($a," por defecto y tokeniza para an\xE1lisis de sentimiento. Ahora puedes usar "),Va=f($a,"CODE",{});var Pi=n(Va);Ko=p(Pi,"classifier"),Pi.forEach(r),Go=p($a," en tu texto objetivo:"),$a.forEach(r),Qs=v(t),S(qt.$$.fragment,t),Xs=v(t),at=f(t,"P",{});var Br=n(at);Bo=p(Br,"Para m\xE1s de un enunciado entrega una lista de frases al "),Ka=f(Br,"CODE",{});var Si=n(Ka);Jo=p(Si,"pipeline()"),Si.forEach(r),Yo=p(Br,"que devolver\xE1 una lista de diccionarios:"),Br.forEach(r),er=v(t),S(Ct.$$.fragment,t),tr=v(t),xe=f(t,"P",{});var _a=n(xe);Zo=p(_a,"El "),Ga=f(_a,"CODE",{});var Oi=n(Ga);Qo=p(Oi,"pipeline()"),Oi.forEach(r),Xo=p(_a,"tambi\xE9n puede iterar sobre un dataset entero. Comienza instalando la biblioteca "),Tt=f(_a,"A",{href:!0,rel:!0});var Di=n(Tt);el=p(Di,"\u{1F917} Datasets"),Di.forEach(r),tl=p(_a,":"),_a.forEach(r),ar=v(t),S(zt.$$.fragment,t),sr=v(t),je=f(t,"P",{});var Et=n(je);al=p(Et,"Crea un "),Ba=f(Et,"CODE",{});var Ii=n(Ba);sl=p(Ii,"pipeline()"),Ii.forEach(r),rl=p(Et,"con la tarea que deseas resolver y el modelo que quieres usar. Coloca el par\xE1metro "),Ja=f(Et,"CODE",{});var Ni=n(Ja);ol=p(Ni,"device"),Ni.forEach(r),ll=p(Et," a "),Ya=f(Et,"CODE",{});var Fi=n(Ya);nl=p(Fi,"0"),Fi.forEach(r),il=p(Et," para poner los tensores en un dispositivo CUDA:"),Et.forEach(r),rr=v(t),S(xt.$$.fragment,t),or=v(t),Me=f(t,"P",{});var ga=n(Me);pl=p(ga,"A continuaci\xF3n, carga el dataset (ve \u{1F917} Datasets "),Mt=f(ga,"A",{href:!0,rel:!0});var Li=n(Mt);cl=p(Li,"Quick Start"),Li.forEach(r),ul=p(ga," para m\xE1s detalles) sobre el que quisieras iterar. Por ejemplo, vamos a cargar el dataset "),Pt=f(ga,"A",{href:!0,rel:!0});var Hi=n(Pt);fl=p(Hi,"MInDS-14"),Hi.forEach(r),dl=p(ga,":"),ga.forEach(r),lr=v(t),S(St.$$.fragment,t),nr=v(t),ta=f(t,"P",{});var Ui=n(ta);ml=p(Ui,"Puedes pasar un pipeline para un dataset:"),Ui.forEach(r),ir=v(t),S(Ot.$$.fragment,t),pr=v(t),st=f(t,"P",{});var Jr=n(st);hl=p(Jr,"Para un dataset m\xE1s grande, donde los inputs son de mayor tama\xF1o (como en habla/audio o visi\xF3n), querr\xE1s pasar un generador en lugar de una lista que carga todos los inputs en memoria. Ve la "),aa=f(Jr,"A",{href:!0});var Ri=n(aa);$l=p(Ri,"documentaci\xF3n del pipeline"),Ri.forEach(r),_l=p(Jr," para m\xE1s informaci\xF3n."),Jr.forEach(r),cr=v(t),Le=f(t,"H3",{class:!0});var Yr=n(Le);rt=f(Yr,"A",{id:!0,class:!0,href:!0});var Wi=n(rt);Za=f(Wi,"SPAN",{});var Vi=n(Za);S(Dt.$$.fragment,Vi),Vi.forEach(r),Wi.forEach(r),gl=v(Yr),Qa=f(Yr,"SPAN",{});var Ki=n(Qa);vl=p(Ki,"Use otro modelo y otro tokenizador en el pipeline"),Ki.forEach(r),Yr.forEach(r),ur=v(t),Ee=f(t,"P",{});var Ie=n(Ee);El=p(Ie,"El "),Xa=f(Ie,"CODE",{});var Gi=n(Xa);bl=p(Gi,"pipeline()"),Gi.forEach(r),wl=p(Ie,"puede adaptarse a cualquier modelo del "),It=f(Ie,"A",{href:!0,rel:!0});var Bi=n(It);kl=p(Bi,"Model Hub"),Bi.forEach(r),jl=p(Ie," haciendo m\xE1s f\xE1cil adaptar el "),es=f(Ie,"CODE",{});var Ji=n(es);yl=p(Ji,"pipeline()"),Ji.forEach(r),Al=p(Ie,"para otros casos de uso. Por ejemplo, si quisieras un modelo capaz de manejar texto en franc\xE9s, usa los tags en el Model Hub para filtrar entre los modelos apropiados. El resultado mejor filtrado devuelve un "),Nt=f(Ie,"A",{href:!0,rel:!0});var Yi=n(Nt);ql=p(Yi,"modelo BERT"),Yi.forEach(r),Cl=p(Ie," multilingual fine-tuned para el an\xE1lisis de sentimiento. Genial, \xA1vamos a usar este modelo!"),Ie.forEach(r),fr=v(t),S(Ft.$$.fragment,t),dr=v(t),S(ot.$$.fragment,t),mr=v(t),Pe=f(t,"P",{});var va=n(Pe);Tl=p(va,"Despu\xE9s puedes especificar el modelo y el tokenizador en el "),ts=f(va,"CODE",{});var Zi=n(ts);zl=p(Zi,"pipeline()"),Zi.forEach(r),xl=p(va," y aplicar el "),as=f(va,"CODE",{});var Qi=n(as);Ml=p(Qi,"classifier"),Qi.forEach(r),Pl=p(va," en tu texto objetivo:"),va.forEach(r),hr=v(t),S(Lt.$$.fragment,t),$r=v(t),Se=f(t,"P",{});var Ea=n(Se);Sl=p(Ea,"Si no pudieras encontrar el modelo para tu caso respectivo de uso necesitar\xE1s ajustar un modelo preentrenado a tus datos. Mira nuestro "),sa=f(Ea,"A",{href:!0});var Xi=n(sa);Ol=p(Xi,"tutorial de fine-tuning"),Xi.forEach(r),Dl=p(Ea," para aprender c\xF3mo. Finalmente, despu\xE9s de que has ajustado tu modelo preentrenado, \xA1por favor considera compartirlo (ve el tutorial "),ra=f(Ea,"A",{href:!0});var ep=n(ra);Il=p(ep,"aqu\xED"),ep.forEach(r),Nl=p(Ea,") con la comunidad en el Model Hub para democratizar el NLP! \u{1F917}"),Ea.forEach(r),_r=v(t),He=f(t,"H2",{class:!0});var Zr=n(He);lt=f(Zr,"A",{id:!0,class:!0,href:!0});var tp=n(lt);ss=f(tp,"SPAN",{});var ap=n(ss);S(Ht.$$.fragment,ap),ap.forEach(r),tp.forEach(r),Fl=v(Zr),rs=f(Zr,"SPAN",{});var sp=n(rs);Ll=p(sp,"AutoClass"),sp.forEach(r),Zr.forEach(r),gr=v(t),S(Ut.$$.fragment,t),vr=v(t),_e=f(t,"P",{});var ke=n(_e);Hl=p(ke,"Debajo del cap\xF3, las clases "),os=f(ke,"CODE",{});var rp=n(os);Ul=p(rp,"AutoModelForSequenceClassification"),rp.forEach(r),Rl=p(ke,"y "),ls=f(ke,"CODE",{});var op=n(ls);Wl=p(op,"AutoTokenizer"),op.forEach(r),Vl=p(ke,"trabajan juntas para dar poder al "),ns=f(ke,"CODE",{});var lp=n(ns);Kl=p(lp,"pipeline()"),lp.forEach(r),Gl=p(ke," Una "),oa=f(ke,"A",{href:!0});var np=n(oa);Bl=p(np,"AutoClass"),np.forEach(r),Jl=p(ke," es un atajo que autom\xE1ticamente recupera la arquitectura de un modelo preentrenado con su nombre o el path. S\xF3lo necesitar\xE1s seleccionar el "),is=f(ke,"CODE",{});var ip=n(is);Yl=p(ip,"AutoClass"),ip.forEach(r),Zl=p(ke," apropiado para tu tarea y tu tokenizador asociado con "),ps=f(ke,"CODE",{});var pp=n(ps);Ql=p(pp,"AutoTokenizer"),pp.forEach(r),ke.forEach(r),Er=v(t),Ue=f(t,"P",{});var Os=n(Ue);Xl=p(Os,"Regresemos a nuestro ejemplo y veamos c\xF3mo puedes usar el "),cs=f(Os,"CODE",{});var cp=n(cs);en=p(cp,"AutoClass"),cp.forEach(r),tn=p(Os," para reproducir los resultados del "),us=f(Os,"CODE",{});var up=n(us);an=p(up,"pipeline()"),up.forEach(r),Os.forEach(r),br=v(t),Re=f(t,"H3",{class:!0});var Qr=n(Re);nt=f(Qr,"A",{id:!0,class:!0,href:!0});var fp=n(nt);fs=f(fp,"SPAN",{});var dp=n(fs);S(Rt.$$.fragment,dp),dp.forEach(r),fp.forEach(r),sn=v(Qr),ds=f(Qr,"SPAN",{});var mp=n(ds);rn=p(mp,"AutoTokenizer"),mp.forEach(r),Qr.forEach(r),wr=v(t),Oe=f(t,"P",{});var ba=n(Oe);on=p(ba,"Un tokenizador es responsable de procesar el texto a un formato que sea entendible para el modelo. Primero, el tokenizador separar\xE1 el texto en palabras llamadas "),ms=f(ba,"EM",{});var hp=n(ms);ln=p(hp,"tokens"),hp.forEach(r),nn=p(ba,". Hay m\xFAltiples reglas que gobiernan el proceso de tokenizaci\xF3n incluyendo el c\xF3mo separar una palabra y en qu\xE9 nivel (aprende m\xE1s sobre tokenizaci\xF3n "),la=f(ba,"A",{href:!0});var $p=n(la);pn=p($p,"aqu\xED"),$p.forEach(r),cn=p(ba,"). Lo m\xE1s importante es recordar que necesitar\xE1s instanciar el tokenizador con el mismo nombre del modelo para asegurar que est\xE1s usando las mismas reglas de tokenizaci\xF3n con las que el modelo fue preentrenado."),ba.forEach(r),kr=v(t),Wt=f(t,"P",{});var ti=n(Wt);un=p(ti,"Carga un tokenizador con "),hs=f(ti,"CODE",{});var _p=n(hs);fn=p(_p,"AutoTokenizer"),_p.forEach(r),ti.forEach(r),jr=v(t),S(Vt.$$.fragment,t),yr=v(t),it=f(t,"P",{});var Xr=n(it);dn=p(Xr,"Despu\xE9s, el tokenizador convierte los tokens a n\xFAmeros para construir un tensor que servir\xE1 como input para el modelo. Esto es conocido como el "),$s=f(Xr,"EM",{});var gp=n($s);mn=p(gp,"vocabulario"),gp.forEach(r),hn=p(Xr," del modelo."),Xr.forEach(r),Ar=v(t),na=f(t,"P",{});var vp=n(na);$n=p(vp,"Pasa tu texto al tokenizador:"),vp.forEach(r),qr=v(t),S(Kt.$$.fragment,t),Cr=v(t),ia=f(t,"P",{});var Ep=n(ia);_n=p(Ep,"El tokenizador devolver\xE1 un diccionario conteniendo:"),Ep.forEach(r),Tr=v(t),pt=f(t,"UL",{});var eo=n(pt);pa=f(eo,"LI",{});var ai=n(pa);ca=f(ai,"A",{href:!0});var bp=n(ca);gn=p(bp,"input_ids"),bp.forEach(r),vn=p(ai,": representaciones num\xE9ricas de los tokens."),ai.forEach(r),En=v(eo),ua=f(eo,"LI",{});var si=n(ua);fa=f(si,"A",{href:!0});var wp=n(fa);bn=p(wp,"atttention_mask"),wp.forEach(r),wn=p(si,": indica cu\xE1les tokens deben ser atendidos."),si.forEach(r),eo.forEach(r),zr=v(t),ct=f(t,"P",{});var to=n(ct);kn=p(to,"Como con el "),_s=f(to,"CODE",{});var kp=n(_s);jn=p(kp,"pipeline()"),kp.forEach(r),yn=p(to," el tokenizador aceptar\xE1 una lista de inputs. Adem\xE1s, el tokenizador tambi\xE9n puede rellenar (pad, en ingl\xE9s) y truncar el texto para devolver un lote (batch, en ingl\xE9s) de longitud uniforme:"),to.forEach(r),xr=v(t),S(ut.$$.fragment,t),Mr=v(t),ft=f(t,"P",{});var ao=n(ft);An=p(ao,"Lee el tutorial de "),da=f(ao,"A",{href:!0});var jp=n(da);qn=p(jp,"preprocessing"),jp.forEach(r),Cn=p(ao," para m\xE1s detalles acerca de la tokenizaci\xF3n."),ao.forEach(r),Pr=v(t),We=f(t,"H3",{class:!0});var so=n(We);dt=f(so,"A",{id:!0,class:!0,href:!0});var yp=n(dt);gs=f(yp,"SPAN",{});var Ap=n(gs);S(Gt.$$.fragment,Ap),Ap.forEach(r),yp.forEach(r),Tn=v(so),vs=f(so,"SPAN",{});var qp=n(vs);zn=p(qp,"AutoModel"),qp.forEach(r),so.forEach(r),Sr=v(t),S(mt.$$.fragment,t),Or=v(t),S(ht.$$.fragment,t),Dr=v(t),fe=f(t,"P",{});var be=n(fe);xn=p(be,"Los modelos son "),Bt=f(be,"A",{href:!0,rel:!0});var Cp=n(Bt);Es=f(Cp,"CODE",{});var Tp=n(Es);Mn=p(Tp,"torch.nn.Module"),Tp.forEach(r),Cp.forEach(r),Pn=p(be," o "),Jt=f(be,"A",{href:!0,rel:!0});var zp=n(Jt);bs=f(zp,"CODE",{});var xp=n(bs);Sn=p(xp,"tf.keras.Model"),xp.forEach(r),zp.forEach(r),On=p(be," est\xE1ndares as\xED que podr\xE1s usarlos en tu training loop usual. Sin embargo, para facilitar las cosas, \u{1F917} Transformers provee una clase "),ws=f(be,"CODE",{});var Mp=n(ws);Dn=p(Mp,"Trainer"),Mp.forEach(r),In=p(be,"para PyTorch que a\xF1ade funcionalidades para entrenamiento distribuido, precici\xF3n mixta, y m\xE1s. Para TensorFlow, puedes usar el m\xE9todo "),ks=f(be,"CODE",{});var Pp=n(ks);Nn=p(Pp,"fit"),Pp.forEach(r),Fn=p(be," desde "),Yt=f(be,"A",{href:!0,rel:!0});var Sp=n(Yt);Ln=p(Sp,"Keras"),Sp.forEach(r),Hn=p(be,". Consulta el "),ma=f(be,"A",{href:!0});var Op=n(ma);Un=p(Op,"tutorial de entrenamiento"),Op.forEach(r),Rn=p(be," para m\xE1s detalles."),be.forEach(r),Ir=v(t),S($t.$$.fragment,t),Nr=v(t),Ve=f(t,"H3",{class:!0});var ro=n(Ve);_t=f(ro,"A",{id:!0,class:!0,href:!0});var Dp=n(_t);js=f(Dp,"SPAN",{});var Ip=n(js);S(Zt.$$.fragment,Ip),Ip.forEach(r),Dp.forEach(r),Wn=v(ro),ys=f(ro,"SPAN",{});var Np=n(ys);Vn=p(Np,"Guarda un modelo"),Np.forEach(r),ro.forEach(r),Fr=v(t),S(gt.$$.fragment,t),Lr=v(t),De=f(t,"P",{});var wa=n(De);Kn=p(wa,"Una caracter\xEDstica particularmente cool de \u{1F917} Transformers es la habilidad de guardar el modelo y cargarlo como un modelo de PyTorch o TensorFlow. El par\xE1metro "),As=f(wa,"CODE",{});var Fp=n(As);Gn=p(Fp,"from_pt"),Fp.forEach(r),Bn=p(wa," o "),qs=f(wa,"CODE",{});var Lp=n(qs);Jn=p(Lp,"from_tf"),Lp.forEach(r),Yn=p(wa," puede convertir el modelo de un framework al otro:"),wa.forEach(r),Hr=v(t),S(vt.$$.fragment,t),this.h()},h(){d(e,"name","hf:doc:metadata"),d(e,"content",JSON.stringify(cu)),d(o,"id","quick-tour"),d(o,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(o,"href","#quick-tour"),d(a,"class","relative group"),d(H,"href","./model_doc/auto"),d(B,"id","pipeline"),d(B,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(B,"href","#pipeline"),d(U,"class","relative group"),d(Qe,"id","uso-del-pipeline"),d(Qe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Qe,"href","#uso-del-pipeline"),d(Fe,"class","relative group"),d(At,"href","https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english"),d(At,"rel","nofollow"),d(Tt,"href","https://huggingface.co/docs/datasets/"),d(Tt,"rel","nofollow"),d(Mt,"href","https://huggingface.co/docs/datasets/quickstart.html"),d(Mt,"rel","nofollow"),d(Pt,"href","https://huggingface.co/datasets/PolyAI/minds14"),d(Pt,"rel","nofollow"),d(aa,"href","./main_classes/pipelines"),d(rt,"id","use-otro-modelo-y-otro-tokenizador-en-el-pipeline"),d(rt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(rt,"href","#use-otro-modelo-y-otro-tokenizador-en-el-pipeline"),d(Le,"class","relative group"),d(It,"href","https://huggingface.co/models"),d(It,"rel","nofollow"),d(Nt,"href","https://huggingface.co/nlptown/bert-base-multilingual-uncased-sentiment"),d(Nt,"rel","nofollow"),d(sa,"href","./training"),d(ra,"href","./model_sharing"),d(lt,"id","autoclass"),d(lt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(lt,"href","#autoclass"),d(He,"class","relative group"),d(oa,"href","./model_doc/auto"),d(nt,"id","autotokenizer"),d(nt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(nt,"href","#autotokenizer"),d(Re,"class","relative group"),d(la,"href","./tokenizer_summary"),d(ca,"href","./glossary#input-ids"),d(fa,"href",".glossary#attention-mask"),d(da,"href","./preprocessing"),d(dt,"id","automodel"),d(dt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(dt,"href","#automodel"),d(We,"class","relative group"),d(Bt,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),d(Bt,"rel","nofollow"),d(Jt,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),d(Jt,"rel","nofollow"),d(Yt,"href","https://keras.io/"),d(Yt,"rel","nofollow"),d(ma,"href","./training"),d(_t,"id","guarda-un-modelo"),d(_t,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(_t,"href","#guarda-un-modelo"),d(Ve,"class","relative group")},m(t,h){s(document.head,e),$(t,l,h),$(t,a,h),s(a,o),s(o,c),M(_,c,null),s(a,b),s(a,E),s(E,A),$(t,y,h),M(T,t,h),$(t,k,h),$(t,C,h),s(C,R),s(C,w),s(w,F),s(C,D),s(C,H),s(H,K),s(C,ae),$(t,L,h),M(G,t,h),$(t,Y,h),$(t,U,h),s(U,B),s(B,oe),M(Z,oe,null),s(U,de),s(U,re),s(re,me),$(t,pe,h),$(t,X,h),s(X,Q),s(Q,ce),s(X,O),$(t,W,h),M(se,t,h),$(t,z,h),$(t,V,h),s(V,ue),s(V,he),s(he,ne),s(V,te),$(t,ve,h),$(t,ie,h),s(ie,$e),s($e,Ae),s(ie,qe),$(t,we,h),$(t,ee,h),s(ee,Ce),s(Ce,lo),s(ee,no),s(ee,qa),s(qa,io),s(ee,po),s(ee,Ca),s(Ca,co),s(ee,uo),s(ee,Ta),s(Ta,fo),s(ee,mo),s(ee,za),s(za,ho),s(ee,$o),s(ee,xa),s(xa,_o),s(ee,go),s(ee,Ma),s(Ma,vo),s(ee,Eo),s(ee,Pa),s(Pa,bo),$(t,Ls,h),$(t,wt,h),s(wt,Sa),s(Sa,wo),s(wt,ko),$(t,Hs,h),$(t,Te,h),s(Te,Oa),s(Oa,jo),s(Te,yo),s(Te,Da),s(Da,Ao),s(Te,qo),s(Te,Ia),s(Ia,Co),$(t,Us,h),$(t,kt,h),s(kt,Na),s(Na,To),s(kt,zo),$(t,Rs,h),$(t,Ye,h),s(Ye,Fa),s(Fa,xo),s(Ye,Mo),s(Ye,La),s(La,Po),$(t,Ws,h),M(Ze,t,h),$(t,Vs,h),$(t,Fe,h),s(Fe,Qe),s(Qe,Ha),M(jt,Ha,null),s(Fe,So),s(Fe,Ua),s(Ua,Oo),$(t,Ks,h),$(t,Xe,h),s(Xe,Do),s(Xe,Ra),s(Ra,Io),s(Xe,No),$(t,Gs,h),$(t,ea,h),s(ea,Fo),$(t,Bs,h),M(et,t,h),$(t,Js,h),$(t,tt,h),s(tt,Lo),s(tt,Wa),s(Wa,Ho),s(tt,Uo),$(t,Ys,h),M(yt,t,h),$(t,Zs,h),$(t,ze,h),s(ze,Ro),s(ze,At),s(At,Wo),s(ze,Vo),s(ze,Va),s(Va,Ko),s(ze,Go),$(t,Qs,h),M(qt,t,h),$(t,Xs,h),$(t,at,h),s(at,Bo),s(at,Ka),s(Ka,Jo),s(at,Yo),$(t,er,h),M(Ct,t,h),$(t,tr,h),$(t,xe,h),s(xe,Zo),s(xe,Ga),s(Ga,Qo),s(xe,Xo),s(xe,Tt),s(Tt,el),s(xe,tl),$(t,ar,h),M(zt,t,h),$(t,sr,h),$(t,je,h),s(je,al),s(je,Ba),s(Ba,sl),s(je,rl),s(je,Ja),s(Ja,ol),s(je,ll),s(je,Ya),s(Ya,nl),s(je,il),$(t,rr,h),M(xt,t,h),$(t,or,h),$(t,Me,h),s(Me,pl),s(Me,Mt),s(Mt,cl),s(Me,ul),s(Me,Pt),s(Pt,fl),s(Me,dl),$(t,lr,h),M(St,t,h),$(t,nr,h),$(t,ta,h),s(ta,ml),$(t,ir,h),M(Ot,t,h),$(t,pr,h),$(t,st,h),s(st,hl),s(st,aa),s(aa,$l),s(st,_l),$(t,cr,h),$(t,Le,h),s(Le,rt),s(rt,Za),M(Dt,Za,null),s(Le,gl),s(Le,Qa),s(Qa,vl),$(t,ur,h),$(t,Ee,h),s(Ee,El),s(Ee,Xa),s(Xa,bl),s(Ee,wl),s(Ee,It),s(It,kl),s(Ee,jl),s(Ee,es),s(es,yl),s(Ee,Al),s(Ee,Nt),s(Nt,ql),s(Ee,Cl),$(t,fr,h),M(Ft,t,h),$(t,dr,h),M(ot,t,h),$(t,mr,h),$(t,Pe,h),s(Pe,Tl),s(Pe,ts),s(ts,zl),s(Pe,xl),s(Pe,as),s(as,Ml),s(Pe,Pl),$(t,hr,h),M(Lt,t,h),$(t,$r,h),$(t,Se,h),s(Se,Sl),s(Se,sa),s(sa,Ol),s(Se,Dl),s(Se,ra),s(ra,Il),s(Se,Nl),$(t,_r,h),$(t,He,h),s(He,lt),s(lt,ss),M(Ht,ss,null),s(He,Fl),s(He,rs),s(rs,Ll),$(t,gr,h),M(Ut,t,h),$(t,vr,h),$(t,_e,h),s(_e,Hl),s(_e,os),s(os,Ul),s(_e,Rl),s(_e,ls),s(ls,Wl),s(_e,Vl),s(_e,ns),s(ns,Kl),s(_e,Gl),s(_e,oa),s(oa,Bl),s(_e,Jl),s(_e,is),s(is,Yl),s(_e,Zl),s(_e,ps),s(ps,Ql),$(t,Er,h),$(t,Ue,h),s(Ue,Xl),s(Ue,cs),s(cs,en),s(Ue,tn),s(Ue,us),s(us,an),$(t,br,h),$(t,Re,h),s(Re,nt),s(nt,fs),M(Rt,fs,null),s(Re,sn),s(Re,ds),s(ds,rn),$(t,wr,h),$(t,Oe,h),s(Oe,on),s(Oe,ms),s(ms,ln),s(Oe,nn),s(Oe,la),s(la,pn),s(Oe,cn),$(t,kr,h),$(t,Wt,h),s(Wt,un),s(Wt,hs),s(hs,fn),$(t,jr,h),M(Vt,t,h),$(t,yr,h),$(t,it,h),s(it,dn),s(it,$s),s($s,mn),s(it,hn),$(t,Ar,h),$(t,na,h),s(na,$n),$(t,qr,h),M(Kt,t,h),$(t,Cr,h),$(t,ia,h),s(ia,_n),$(t,Tr,h),$(t,pt,h),s(pt,pa),s(pa,ca),s(ca,gn),s(pa,vn),s(pt,En),s(pt,ua),s(ua,fa),s(fa,bn),s(ua,wn),$(t,zr,h),$(t,ct,h),s(ct,kn),s(ct,_s),s(_s,jn),s(ct,yn),$(t,xr,h),M(ut,t,h),$(t,Mr,h),$(t,ft,h),s(ft,An),s(ft,da),s(da,qn),s(ft,Cn),$(t,Pr,h),$(t,We,h),s(We,dt),s(dt,gs),M(Gt,gs,null),s(We,Tn),s(We,vs),s(vs,zn),$(t,Sr,h),M(mt,t,h),$(t,Or,h),M(ht,t,h),$(t,Dr,h),$(t,fe,h),s(fe,xn),s(fe,Bt),s(Bt,Es),s(Es,Mn),s(fe,Pn),s(fe,Jt),s(Jt,bs),s(bs,Sn),s(fe,On),s(fe,ws),s(ws,Dn),s(fe,In),s(fe,ks),s(ks,Nn),s(fe,Fn),s(fe,Yt),s(Yt,Ln),s(fe,Hn),s(fe,ma),s(ma,Un),s(fe,Rn),$(t,Ir,h),M($t,t,h),$(t,Nr,h),$(t,Ve,h),s(Ve,_t),s(_t,js),M(Zt,js,null),s(Ve,Wn),s(Ve,ys),s(ys,Vn),$(t,Fr,h),M(gt,t,h),$(t,Lr,h),$(t,De,h),s(De,Kn),s(De,As),s(As,Gn),s(De,Bn),s(De,qs),s(qs,Jn),s(De,Yn),$(t,Hr,h),M(vt,t,h),Ur=!0},p(t,[h]){const Qt={};h&2&&(Qt.$$scope={dirty:h,ctx:t}),G.$set(Qt);const Cs={};h&2&&(Cs.$$scope={dirty:h,ctx:t}),Ze.$set(Cs);const Ts={};h&2&&(Ts.$$scope={dirty:h,ctx:t}),et.$set(Ts);const zs={};h&2&&(zs.$$scope={dirty:h,ctx:t}),ot.$set(zs);const Ke={};h&2&&(Ke.$$scope={dirty:h,ctx:t}),ut.$set(Ke);const xs={};h&2&&(xs.$$scope={dirty:h,ctx:t}),mt.$set(xs);const Ms={};h&2&&(Ms.$$scope={dirty:h,ctx:t}),ht.$set(Ms);const Xt={};h&2&&(Xt.$$scope={dirty:h,ctx:t}),$t.$set(Xt);const Ps={};h&2&&(Ps.$$scope={dirty:h,ctx:t}),gt.$set(Ps);const Ss={};h&2&&(Ss.$$scope={dirty:h,ctx:t}),vt.$set(Ss)},i(t){Ur||(j(_.$$.fragment,t),j(T.$$.fragment,t),j(G.$$.fragment,t),j(Z.$$.fragment,t),j(se.$$.fragment,t),j(Ze.$$.fragment,t),j(jt.$$.fragment,t),j(et.$$.fragment,t),j(yt.$$.fragment,t),j(qt.$$.fragment,t),j(Ct.$$.fragment,t),j(zt.$$.fragment,t),j(xt.$$.fragment,t),j(St.$$.fragment,t),j(Ot.$$.fragment,t),j(Dt.$$.fragment,t),j(Ft.$$.fragment,t),j(ot.$$.fragment,t),j(Lt.$$.fragment,t),j(Ht.$$.fragment,t),j(Ut.$$.fragment,t),j(Rt.$$.fragment,t),j(Vt.$$.fragment,t),j(Kt.$$.fragment,t),j(ut.$$.fragment,t),j(Gt.$$.fragment,t),j(mt.$$.fragment,t),j(ht.$$.fragment,t),j($t.$$.fragment,t),j(Zt.$$.fragment,t),j(gt.$$.fragment,t),j(vt.$$.fragment,t),Ur=!0)},o(t){q(_.$$.fragment,t),q(T.$$.fragment,t),q(G.$$.fragment,t),q(Z.$$.fragment,t),q(se.$$.fragment,t),q(Ze.$$.fragment,t),q(jt.$$.fragment,t),q(et.$$.fragment,t),q(yt.$$.fragment,t),q(qt.$$.fragment,t),q(Ct.$$.fragment,t),q(zt.$$.fragment,t),q(xt.$$.fragment,t),q(St.$$.fragment,t),q(Ot.$$.fragment,t),q(Dt.$$.fragment,t),q(Ft.$$.fragment,t),q(ot.$$.fragment,t),q(Lt.$$.fragment,t),q(Ht.$$.fragment,t),q(Ut.$$.fragment,t),q(Rt.$$.fragment,t),q(Vt.$$.fragment,t),q(Kt.$$.fragment,t),q(ut.$$.fragment,t),q(Gt.$$.fragment,t),q(mt.$$.fragment,t),q(ht.$$.fragment,t),q($t.$$.fragment,t),q(Zt.$$.fragment,t),q(gt.$$.fragment,t),q(vt.$$.fragment,t),Ur=!1},d(t){r(e),t&&r(l),t&&r(a),P(_),t&&r(y),P(T,t),t&&r(k),t&&r(C),t&&r(L),P(G,t),t&&r(Y),t&&r(U),P(Z),t&&r(pe),t&&r(X),t&&r(W),P(se,t),t&&r(z),t&&r(V),t&&r(ve),t&&r(ie),t&&r(we),t&&r(ee),t&&r(Ls),t&&r(wt),t&&r(Hs),t&&r(Te),t&&r(Us),t&&r(kt),t&&r(Rs),t&&r(Ye),t&&r(Ws),P(Ze,t),t&&r(Vs),t&&r(Fe),P(jt),t&&r(Ks),t&&r(Xe),t&&r(Gs),t&&r(ea),t&&r(Bs),P(et,t),t&&r(Js),t&&r(tt),t&&r(Ys),P(yt,t),t&&r(Zs),t&&r(ze),t&&r(Qs),P(qt,t),t&&r(Xs),t&&r(at),t&&r(er),P(Ct,t),t&&r(tr),t&&r(xe),t&&r(ar),P(zt,t),t&&r(sr),t&&r(je),t&&r(rr),P(xt,t),t&&r(or),t&&r(Me),t&&r(lr),P(St,t),t&&r(nr),t&&r(ta),t&&r(ir),P(Ot,t),t&&r(pr),t&&r(st),t&&r(cr),t&&r(Le),P(Dt),t&&r(ur),t&&r(Ee),t&&r(fr),P(Ft,t),t&&r(dr),P(ot,t),t&&r(mr),t&&r(Pe),t&&r(hr),P(Lt,t),t&&r($r),t&&r(Se),t&&r(_r),t&&r(He),P(Ht),t&&r(gr),P(Ut,t),t&&r(vr),t&&r(_e),t&&r(Er),t&&r(Ue),t&&r(br),t&&r(Re),P(Rt),t&&r(wr),t&&r(Oe),t&&r(kr),t&&r(Wt),t&&r(jr),P(Vt,t),t&&r(yr),t&&r(it),t&&r(Ar),t&&r(na),t&&r(qr),P(Kt,t),t&&r(Cr),t&&r(ia),t&&r(Tr),t&&r(pt),t&&r(zr),t&&r(ct),t&&r(xr),P(ut,t),t&&r(Mr),t&&r(ft),t&&r(Pr),t&&r(We),P(Gt),t&&r(Sr),P(mt,t),t&&r(Or),P(ht,t),t&&r(Dr),t&&r(fe),t&&r(Ir),P($t,t),t&&r(Nr),t&&r(Ve),P(Zt),t&&r(Fr),P(gt,t),t&&r(Lr),t&&r(De),t&&r(Hr),P(vt,t)}}}const cu={local:"quick-tour",sections:[{local:"pipeline",sections:[{local:"uso-del-pipeline",title:"Uso del Pipeline"},{local:"use-otro-modelo-y-otro-tokenizador-en-el-pipeline",title:"Use otro modelo y otro tokenizador en el pipeline"}],title:"Pipeline"},{local:"autoclass",sections:[{local:"autotokenizer",title:"AutoTokenizer"},{local:"automodel",title:"AutoModel"},{local:"guarda-un-modelo",title:"Guarda un modelo"}],title:"AutoClass"}],title:"Quick tour"};function uu(m){return Jp(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class _u extends Ge{constructor(e){super();Be(this,e,uu,pu,Je,{})}}export{_u as default,cu as metadata};
