import{S as Xn,i as Jn,s as Kn,e as r,k as l,w as u,t as n,M as Qn,c as o,d as a,m as i,a as s,x as m,h as c,b as d,F as t,g as h,y as g,q as v,o as _,B as b,v as Yn}from"../chunks/vendor-19e06bd2.js";import{T as Ls}from"../chunks/Tip-f0fa2d82.js";import{D}from"../chunks/Docstring-395e5a9c.js";import{I as U}from"../chunks/IconCopyLink-3c713d38.js";function Zn(ce){let f,S,$,y,x;return{c(){f=r("p"),S=n("This does not support "),$=r("code"),y=n("BatchSampler"),x=n(" with varying batch size yet.")},l(w){f=o(w,"P",{});var E=s(f);S=c(E,"This does not support "),$=o(E,"CODE",{});var L=s($);y=c(L,"BatchSampler"),L.forEach(a),x=c(E," with varying batch size yet."),E.forEach(a)},m(w,E){h(w,f,E),t(f,S),t(f,$),t($,y),t(f,x)},d(w){w&&a(f)}}}function ec(ce){let f,S,$,y,x;return{c(){f=r("p"),S=n("This does not support "),$=r("code"),y=n("BatchSampler"),x=n(" with varying batch size yet.")},l(w){f=o(w,"P",{});var E=s(f);S=c(E,"This does not support "),$=o(E,"CODE",{});var L=s($);y=c(L,"BatchSampler"),L.forEach(a),x=c(E," with varying batch size yet."),E.forEach(a)},m(w,E){h(w,f,E),t(f,S),t(f,$),t($,y),t(f,x)},d(w){w&&a(f)}}}function tc(ce){let f,S;return{c(){f=r("p"),S=n("Make sure all processes will reach this instruction otherwise one of your processes will hang forever.")},l($){f=o($,"P",{});var y=s(f);S=c(y,"Make sure all processes will reach this instruction otherwise one of your processes will hang forever."),y.forEach(a)},m($,y){h($,f,y),t(f,S)},d($){$&&a(f)}}}function ac(ce){let f,S,$,y,x,w,E,L,vr,$a,q,le,bt,Pe,_r,$t,br,ya,V,Ae,$r,yt,yr,wa,F,ie,wt,Ie,wr,Et,Er,Ea,de,Dr,Dt,Sr,Tr,Da,k,ze,xr,Le,kr,St,Pr,Ar,Ir,R,zr,Tt,Lr,Nr,xt,Or,Cr,Gr,pe,Sa,W,he,kt,Ne,Ur,Pt,Br,Ta,H,Oe,qr,Ce,Vr,At,Fr,Rr,xa,M,fe,It,Ge,Wr,zt,Hr,ka,N,Ue,Mr,O,jr,Lt,Xr,Jr,Nt,Kr,Qr,Ot,Yr,Zr,eo,ue,Pa,j,me,Ct,Be,to,Gt,ao,Aa,X,qe,ro,T,oo,Ut,so,no,Bt,co,lo,qt,io,po,Vt,ho,fo,Ft,uo,mo,Ia,J,ge,Rt,Ve,go,Wt,vo,za,K,ve,Ht,Fe,_o,Mt,bo,La,Q,Re,$o,Y,yo,We,wo,Eo,jt,Do,So,Na,Z,_e,Xt,He,To,Jt,xo,Oa,P,Me,ko,Kt,Po,Ao,Qt,Io,zo,A,it,Yt,Lo,No,Oo,dt,Zt,Co,Go,Uo,pt,ea,Bo,qo,Vo,ht,ta,Fo,Ro,Wo,ft,aa,Ho,Mo,Ca,ee,be,ra,je,jo,oa,Xo,Ga,I,Xe,Jo,sa,Ko,Qo,$e,Je,Yo,te,Zo,na,es,ts,ca,as,rs,os,ye,Ke,ss,Qe,ns,la,cs,ls,Ua,ae,we,ia,Ye,is,da,ds,Ba,re,Ze,ps,pa,hs,qa,oe,et,fs,ha,us,Va,se,tt,ms,fa,gs,Fa,ne,at,vs,C,_s,ua,bs,$s,ma,ys,ws,ga,Es,Ds,Ra,rt,ot,Wa,st,nt,Ha,G,ct,Ss,va,Ts,xs,Ee,Ma;return w=new U({}),Pe=new U({}),Ae=new D({props:{name:"class accelerate.optimizer.AcceleratedOptimizer",anchor:"accelerate.optimizer.AcceleratedOptimizer",parameters:[{name:"optimizer",val:""},{name:"device_placement",val:" = True"},{name:"scaler",val:" = None"}],source:"https://github.com/huggingface/accelerate/blob/pr_293/src/accelerate/optimizer.py#L39",parametersDescription:[{anchor:"accelerate.optimizer.AcceleratedOptimizer.optimizer",description:`<strong>optimizer</strong> (<code>torch.optim.optimizer.Optimizer</code>) &#x2014;
The optimizer to wrap.`,name:"optimizer"},{anchor:"accelerate.optimizer.AcceleratedOptimizer.device_placement",description:`<strong>device_placement</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not the optimizer should handle device placement. If so, it will place the state dictionary of
<code>optimizer</code> on the right device.`,name:"device_placement"},{anchor:"accelerate.optimizer.AcceleratedOptimizer.scaler",description:`<strong>scaler</strong> (<code>torch.cuda.amp.grad_scaler.GradScaler</code>, <em>optional</em>) &#x2014;
The scaler to use in the step function if training with mixed precision.`,name:"scaler"}]}}),Ie=new U({}),ze=new D({props:{name:"accelerate.data_loader.prepare_data_loader",anchor:"accelerate.data_loader.prepare_data_loader",parameters:[{name:"dataloader",val:": DataLoader"},{name:"device",val:": typing.Optional[torch.device] = None"},{name:"num_processes",val:": typing.Optional[int] = None"},{name:"process_index",val:": typing.Optional[int] = None"},{name:"split_batches",val:": bool = False"},{name:"put_on_device",val:": bool = False"},{name:"rng_types",val:": typing.Union[typing.List[typing.Union[str, accelerate.utils.RNGType]], NoneType] = None"},{name:"dispatch_batches",val:": typing.Optional[bool] = None"}],source:"https://github.com/huggingface/accelerate/blob/pr_293/src/accelerate/data_loader.py#L417",parametersDescription:[{anchor:"accelerate.data_loader.prepare_data_loader.dataloader",description:`<strong>dataloader</strong> (<code>torch.utils.data.dataloader.DataLoader</code>) &#x2014;
The data loader to split across several devices.`,name:"dataloader"},{anchor:"accelerate.data_loader.prepare_data_loader.device",description:`<strong>device</strong> (<code>torch.device</code>) &#x2014;
The target device for the returned <code>DataLoader</code>.`,name:"device"},{anchor:"accelerate.data_loader.prepare_data_loader.num_processes",description:`<strong>num_processes</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The number of processes running concurrently. Will default to the value given by
<a href="/docs/accelerate/pr_293/en/internal#accelerate.state.AcceleratorState">AcceleratorState</a>.`,name:"num_processes"},{anchor:"accelerate.data_loader.prepare_data_loader.process_index",description:`<strong>process_index</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The index of the current process. Will default to the value given by <a href="/docs/accelerate/pr_293/en/internal#accelerate.state.AcceleratorState">AcceleratorState</a>.`,name:"process_index"},{anchor:"accelerate.data_loader.prepare_data_loader.split_batches",description:`<strong>split_batches</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether the resulting <code>DataLoader</code> should split the batches of the original data loader across devices or
yield full batches (in which case it will yield batches starting at the <code>process_index</code>-th and advancing of
<code>num_processes</code> batches at each iteration).</p>
<p>Another way to see this is that the observed batch size will be the same as the initial <code>dataloader</code> if
this option is set to <code>True</code>, the batch size of the initial <code>dataloader</code> multiplied by <code>num_processes</code>
otherwise.</p>
<p>Setting this option to <code>True</code> requires that the batch size of the <code>dataloader</code> is a round multiple of
<code>batch_size</code>.`,name:"split_batches"},{anchor:"accelerate.data_loader.prepare_data_loader.put_on_device",description:`<strong>put_on_device</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to put the batches on <code>device</code> (only works if the batches are nested list, tuples or
dictionaries of tensors).`,name:"put_on_device"},{anchor:"accelerate.data_loader.prepare_data_loader.rng_types",description:`<strong>rng_types</strong> (list of <code>str</code> or <code>RNGType</code> &#x2014;
The list of random number generators to synchronize at the beginning of each iteration. Should be one or
several of:</p>
<ul>
<li><code>&quot;torch&quot;</code>: the base torch random number generator</li>
<li><code>&quot;cuda&quot;</code>: the CUDA random number generator (GPU only)</li>
<li><code>&quot;xla&quot;</code>: the XLA random number generator (TPU only)</li>
<li><code>&quot;generator&quot;</code>: the <code>torch.Generator</code> of the sampler (or batch sampler if there is no sampler in your
dataloader) or of the iterable dataset (if it exists) if the underlying dataset is of that type.</li>
</ul>`,name:"rng_types"},{anchor:"accelerate.data_loader.prepare_data_loader.dispatch_batches",description:`<strong>dispatch_batches</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
If set to <code>True</code>, the datalaoder prepared is only iterated through on the main process and then the batches
are split and broadcast to each process. Will default to <code>True</code> when the underlying dataset is an
<code>IterableDataset</code>, <code>False</code> otherwise.`,name:"dispatch_batches"}],returnDescription:`
<p>A new data loader that will yield the portion of the batches</p>
`,returnType:`
<p><code>torch.utils.data.dataloader.DataLoader</code></p>
`}}),pe=new Ls({props:{warning:!0,$$slots:{default:[Zn]},$$scope:{ctx:ce}}}),Ne=new U({}),Oe=new D({props:{name:"class accelerate.data_loader.DataLoaderShard",anchor:"accelerate.data_loader.DataLoaderShard",parameters:[{name:"*args",val:""},{name:"**kwds",val:""}],source:"https://github.com/huggingface/accelerate/blob/pr_293/src/accelerate/data_loader.py#L270",parametersDescription:[{anchor:"accelerate.data_loader.DataLoaderShard.dataset",description:`<strong>dataset</strong> (<code>torch.utils.data.dataset.Dataset</code>) &#x2014;
The dataset to use to build this datalaoder.`,name:"dataset"},{anchor:"accelerate.data_loader.DataLoaderShard.device",description:`<strong>device</strong> (<code>torch.device</code>, <em>optional</em>) &#x2014;
If passed, the device to put all batches on.`,name:"device"},{anchor:"accelerate.data_loader.DataLoaderShard.rng_types",description:`<strong>rng_types</strong> (list of <code>str</code> or <code>RNGType</code> &#x2014;
The list of random number generators to synchronize at the beginning of each iteration. Should be one or
several of:</p>
<ul>
<li><code>&quot;torch&quot;</code>: the base torch random number generator</li>
<li><code>&quot;cuda&quot;</code>: the CUDA random number generator (GPU only)</li>
<li><code>&quot;xla&quot;</code>: the XLA random number generator (TPU only)</li>
<li><code>&quot;generator&quot;</code>: an optional <code>torch.Generator</code></li>
</ul>`,name:"rng_types"},{anchor:"accelerate.data_loader.DataLoaderShard.generator",description:`<strong>generator</strong> (<code>torch.Generator</code>, <em>optional</em>) &#x2014;
A random number generator to keep synchronized across processes.
kwargs &#x2014;
All other keyword arguments to pass to the regular <code>DataLoader</code> initialization.`,name:"generator"}]}}),Ge=new U({}),Ue=new D({props:{name:"class accelerate.data_loader.BatchSamplerShard",anchor:"accelerate.data_loader.BatchSamplerShard",parameters:[{name:"*args",val:""},{name:"**kwds",val:""}],source:"https://github.com/huggingface/accelerate/blob/pr_293/src/accelerate/data_loader.py#L68",parametersDescription:[{anchor:"accelerate.data_loader.BatchSamplerShard.batch_sampler",description:`<strong>batch_sampler</strong> (<code>torch.utils.data.sampler.BatchSampler</code>) &#x2014;
The batch sampler to split in several shards.`,name:"batch_sampler"},{anchor:"accelerate.data_loader.BatchSamplerShard.num_processes",description:`<strong>num_processes</strong> (<code>int</code>, <em>optional</em>, defaults to 1) &#x2014;
The number of processes running concurrently.`,name:"num_processes"},{anchor:"accelerate.data_loader.BatchSamplerShard.process_index",description:`<strong>process_index</strong> (<code>int</code>, <em>optional</em>, defaults to 0) &#x2014;
The index of the current process.`,name:"process_index"},{anchor:"accelerate.data_loader.BatchSamplerShard.split_batches",description:`<strong>split_batches</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether the shards should be created by splitting a batch to give a piece of it on each process, or by
yielding different full batches on each process.</p>
<p>On two processes with a sampler of <code>[[0, 1, 2, 3], [4, 5, 6, 7]]</code>, this will result in:</p>
<ul>
<li>the sampler on process 0 to yield <code>[0, 1, 2, 3]</code> and the sampler on process 1 to yield <code>[4, 5, 6, 7]</code> if
this argument is set to <code>False</code>.</li>
<li>the sampler on process 0 to yield <code>[0, 1]</code> then <code>[4, 5]</code> and the sampler on process 1 to yield <code>[2, 3]</code>
then <code>[6, 7]</code> if this argument is set to <code>True</code>.</li>
</ul>`,name:"split_batches"}]}}),ue=new Ls({props:{warning:!0,$$slots:{default:[ec]},$$scope:{ctx:ce}}}),Be=new U({}),qe=new D({props:{name:"class accelerate.data_loader.IterableDatasetShard",anchor:"accelerate.data_loader.IterableDatasetShard",parameters:[{name:"*args",val:""},{name:"**kwds",val:""}],source:"https://github.com/huggingface/accelerate/blob/pr_293/src/accelerate/data_loader.py#L189",parametersDescription:[{anchor:"accelerate.data_loader.IterableDatasetShard.dataset",description:`<strong>dataset</strong> (<code>torch.utils.data.dataset.IterableDataset</code>) &#x2014;
The batch sampler to split in several shards.`,name:"dataset"},{anchor:"accelerate.data_loader.IterableDatasetShard.batch_size",description:`<strong>batch_size</strong> (<code>int</code>, <em>optional</em>, defaults to 1) &#x2014;
The size of the batches per shard (if <code>split_batches=False</code>) or the size of the batches (if
<code>split_batches=True</code>).`,name:"batch_size"},{anchor:"accelerate.data_loader.IterableDatasetShard.drop_last",description:`<strong>drop_last</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to drop the last incomplete batch or complete the last batches by using the samples from the
beginning.`,name:"drop_last"},{anchor:"accelerate.data_loader.IterableDatasetShard.num_processes",description:`<strong>num_processes</strong> (<code>int</code>, <em>optional</em>, defaults to 1) &#x2014;
The number of processes running concurrently.`,name:"num_processes"},{anchor:"accelerate.data_loader.IterableDatasetShard.process_index",description:`<strong>process_index</strong> (<code>int</code>, <em>optional</em>, defaults to 0) &#x2014;
The index of the current process.`,name:"process_index"},{anchor:"accelerate.data_loader.IterableDatasetShard.split_batches",description:`<strong>split_batches</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether the shards should be created by splitting a batch to give a piece of it on each process, or by
yielding different full batches on each process.</p>
<p>On two processes with an iterable dataset yielding of <code>[0, 1, 2, 3, 4, 5, 6, 7]</code>, this will result in:</p>
<ul>
<li>the shard on process 0 to yield <code>[0, 1, 2, 3]</code> and the shard on process 1 to yield <code>[4, 5, 6, 7]</code> if this
argument is set to <code>False</code>.</li>
<li>the shard on process 0 to yield <code>[0, 1, 4, 5]</code> and the sampler on process 1 to yield <code>[2, 3, 6, 7]</code> if
this argument is set to <code>True</code>.</li>
</ul>`,name:"split_batches"}]}}),Ve=new U({}),Fe=new U({}),Re=new D({props:{name:"class accelerate.state.AcceleratorState",anchor:"accelerate.state.AcceleratorState",parameters:[{name:"mixed_precision",val:": str = None"},{name:"cpu",val:": bool = False"},{name:"deepspeed_plugin",val:" = None"},{name:"_from_accelerator",val:": bool = False"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/accelerate/blob/pr_293/src/accelerate/state.py#L128",parametersDescription:[{anchor:"accelerate.state.AcceleratorState.-",description:"<strong>-</strong> <strong>device</strong> (<code>torch.device</code>) &#x2014; The device to use. &#x2014;",name:"-"},{anchor:"accelerate.state.AcceleratorState.-",description:`<strong>-</strong> <strong>distributed_type</strong> (<code>~accelerate.state.DistributedType</code>) &#x2014; The type of distributed environment currently &#x2014;
in use.`,name:"-"},{anchor:"accelerate.state.AcceleratorState.-",description:"<strong>-</strong> <strong>num_processes</strong> (<code>int</code>) &#x2014; The number of processes currently launched in parallel. &#x2014;",name:"-"},{anchor:"accelerate.state.AcceleratorState.-",description:"<strong>-</strong> <strong>process_index</strong> (<code>int</code>) &#x2014; The index of the current process. &#x2014;",name:"-"},{anchor:"accelerate.state.AcceleratorState.-",description:"<strong>-</strong> <strong>local_process_index</strong> (<code>int</code>) &#x2014; The index of the current process on the current server. &#x2014;",name:"-"},{anchor:"accelerate.state.AcceleratorState.-",description:`<strong>-</strong> <strong>mixed_precision</strong> (<code>str</code>) &#x2014; Whether or not the current script will use mixed precision. If you are using &#x2014;
mixed precision, define if you want to use FP16 or BF16 (bfloat16) as the floating point.`,name:"-"}]}}),He=new U({}),Me=new D({props:{name:"class accelerate.DistributedType",anchor:"accelerate.DistributedType",parameters:[{name:"value",val:""},{name:"names",val:" = None"},{name:"module",val:" = None"},{name:"qualname",val:" = None"},{name:"type",val:" = None"},{name:"start",val:" = 1"}],source:"https://github.com/huggingface/accelerate/blob/pr_293/src/accelerate/state.py#L74"}}),je=new U({}),Xe=new D({props:{name:"class accelerate.tracking.GeneralTracker",anchor:"accelerate.tracking.GeneralTracker",parameters:[],source:"https://github.com/huggingface/accelerate/blob/pr_293/src/accelerate/tracking.py#L51"}}),Je=new D({props:{name:"log",anchor:"accelerate.tracking.GeneralTracker.log",parameters:[{name:"values",val:": dict"},{name:"step",val:": typing.Optional[int]"}],source:"https://github.com/huggingface/accelerate/blob/pr_293/src/accelerate/tracking.py#L69",parametersDescription:[{anchor:"accelerate.tracking.GeneralTracker.log.values",description:`<strong>values</strong> (Dictionary <code>str</code> to <code>str</code>, <code>float</code>, or <code>int</code>) &#x2014;
Values to be logged as key-value pairs. The values need to have type <code>str</code>, <code>float</code>, or <code>int</code>.`,name:"values"},{anchor:"accelerate.tracking.GeneralTracker.log.step",description:`<strong>step</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The run step. If included, the log will be affiliated with this step.`,name:"step"}]}}),Ke=new D({props:{name:"store_init_configuration",anchor:"accelerate.tracking.GeneralTracker.store_init_configuration",parameters:[{name:"values",val:": dict"}],source:"https://github.com/huggingface/accelerate/blob/pr_293/src/accelerate/tracking.py#L56",parametersDescription:[{anchor:"accelerate.tracking.GeneralTracker.store_init_configuration.values",description:`<strong>values</strong> (Dictionary <code>str</code> to <code>bool</code>, <code>str</code>, <code>float</code> or <code>int</code>) &#x2014;
Values to be stored as initial hyperparameters as key-value pairs. The values need to have type <code>bool</code>,
<code>str</code>, <code>float</code>, <code>int</code>, or <code>None</code>.`,name:"values"}]}}),Ye=new U({}),Ze=new D({props:{name:"accelerate.utils.extract_model_from_parallel",anchor:"accelerate.utils.extract_model_from_parallel",parameters:[{name:"model",val:""}],source:"https://github.com/huggingface/accelerate/blob/pr_293/src/accelerate/utils.py#L350",parametersDescription:[{anchor:"accelerate.utils.extract_model_from_parallel.model",description:"<strong>model</strong> (<code>torch.nn.Module</code>) &#x2014; The model to extract.",name:"model"}],returnDescription:`
<p>The extracted model.</p>
`,returnType:`
<p><code>torch.nn.Module</code></p>
`}}),et=new D({props:{name:"accelerate.utils.gather",anchor:"accelerate.utils.gather",parameters:[{name:"tensor",val:""}],source:"https://github.com/huggingface/accelerate/blob/pr_293/src/accelerate/utils.py#L395",parametersDescription:[{anchor:"accelerate.utils.gather.tensor",description:`<strong>tensor</strong> (nested list/tuple/dictionary of <code>torch.Tensor</code>) &#x2014;
The data to gather.`,name:"tensor"}],returnDescription:`
<p>The same data structure as <code>tensor</code> with all tensors sent to the proper device.</p>
`}}),tt=new D({props:{name:"accelerate.utils.send_to_device",anchor:"accelerate.utils.send_to_device",parameters:[{name:"tensor",val:""},{name:"device",val:""}],source:"https://github.com/huggingface/accelerate/blob/pr_293/src/accelerate/utils.py#L245",parametersDescription:[{anchor:"accelerate.utils.send_to_device.tensor",description:`<strong>tensor</strong> (nested list/tuple/dictionary of <code>torch.Tensor</code>) &#x2014;
The data to send to a given device.`,name:"tensor"},{anchor:"accelerate.utils.send_to_device.device",description:`<strong>device</strong> (<code>torch.device</code>) &#x2014;
The device to send the data to.`,name:"device"}],returnDescription:`
<p>The same data structure as <code>tensor</code> with all tensors sent to the proper device.</p>
`}}),at=new D({props:{name:"accelerate.utils.set_seed",anchor:"accelerate.utils.set_seed",parameters:[{name:"seed",val:": int"},{name:"device_specific",val:": bool = False"}],source:"https://github.com/huggingface/accelerate/blob/pr_293/src/accelerate/utils.py#L115",parametersDescription:[{anchor:"accelerate.utils.set_seed.seed",description:"<strong>seed</strong> (<code>int</code>) &#x2014; The seed to set.",name:"seed"},{anchor:"accelerate.utils.set_seed.device_specific",description:`<strong>device_specific</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to differ the seed on each device slightly with <code>self.process_index</code>.`,name:"device_specific"}]}}),ot=new D({props:{name:"accelerate.utils.synchronize_rng_state",anchor:"accelerate.utils.synchronize_rng_state",parameters:[{name:"rng_type",val:": typing.Optional[accelerate.utils.RNGType] = None"},{name:"generator",val:": typing.Optional[torch._C.Generator] = None"}],source:"https://github.com/huggingface/accelerate/blob/pr_293/src/accelerate/utils.py#L135"}}),nt=new D({props:{name:"accelerate.synchronize_rng_states",anchor:"accelerate.synchronize_rng_states",parameters:[{name:"rng_types",val:": typing.List[typing.Union[str, accelerate.utils.RNGType]]"},{name:"generator",val:": typing.Optional[torch._C.Generator] = None"}],source:"https://github.com/huggingface/accelerate/blob/pr_293/src/accelerate/utils.py#L170"}}),ct=new D({props:{name:"accelerate.utils.wait_for_everyone",anchor:"accelerate.utils.wait_for_everyone",parameters:[],source:"https://github.com/huggingface/accelerate/blob/pr_293/src/accelerate/utils.py#L619"}}),Ee=new Ls({props:{warning:!0,$$slots:{default:[tc]},$$scope:{ctx:ce}}}),{c(){f=r("meta"),S=l(),$=r("h1"),y=r("a"),x=r("span"),u(w.$$.fragment),E=l(),L=r("span"),vr=n("Internals"),$a=l(),q=r("h2"),le=r("a"),bt=r("span"),u(Pe.$$.fragment),_r=l(),$t=r("span"),br=n("Optimizer"),ya=l(),V=r("div"),u(Ae.$$.fragment),$r=l(),yt=r("p"),yr=n("Internal wrapper around a torch optimizer."),wa=l(),F=r("h2"),ie=r("a"),wt=r("span"),u(Ie.$$.fragment),wr=l(),Et=r("span"),Er=n("DataLoader"),Ea=l(),de=r("p"),Dr=n("The main work on your PyTorch "),Dt=r("code"),Sr=n("DataLoader"),Tr=n(" is done by the following function:"),Da=l(),k=r("div"),u(ze.$$.fragment),xr=l(),Le=r("p"),kr=n("Wraps a PyTorch "),St=r("code"),Pr=n("DataLoader"),Ar=n(" to generate batches for one of the processes only."),Ir=l(),R=r("p"),zr=n("Depending on the value of the "),Tt=r("code"),Lr=n("drop_last"),Nr=n(" attribute of the "),xt=r("code"),Or=n("dataloader"),Cr=n(` passed, it will either stop the iteration
at the first batch that would be too small / not present on all processes or loop with indices from the beginning.`),Gr=l(),u(pe.$$.fragment),Sa=l(),W=r("h3"),he=r("a"),kt=r("span"),u(Ne.$$.fragment),Ur=l(),Pt=r("span"),Br=n("BatchSamplerShard"),Ta=l(),H=r("div"),u(Oe.$$.fragment),qr=l(),Ce=r("p"),Vr=n("Subclass of a PyTorch "),At=r("code"),Fr=n("DataLoader"),Rr=n(" that will deal with device placement and current distributed setup."),xa=l(),M=r("h3"),fe=r("a"),It=r("span"),u(Ge.$$.fragment),Wr=l(),zt=r("span"),Hr=n("BatchSamplerShard"),ka=l(),N=r("div"),u(Ue.$$.fragment),Mr=l(),O=r("p"),jr=n("Wraps a PyTorch "),Lt=r("code"),Xr=n("BatchSampler"),Jr=n(` to generate batches for one of the processes only. Instances of this class will
always yield a number of batches that is a round multiple of `),Nt=r("code"),Kr=n("num_processes"),Qr=n(` and that all have the same size.
Depending on the value of the `),Ot=r("code"),Yr=n("drop_last"),Zr=n(` attribute of the batch sampler passed, it will either stop the iteration
at the first batch that would be too small / not present on all processes or loop with indices from the beginning.`),eo=l(),u(ue.$$.fragment),Pa=l(),j=r("h3"),me=r("a"),Ct=r("span"),u(Be.$$.fragment),to=l(),Gt=r("span"),ao=n("IterableDatasetShard"),Aa=l(),X=r("div"),u(qe.$$.fragment),ro=l(),T=r("p"),oo=n("Wraps a PyTorch "),Ut=r("code"),so=n("IterableDataset"),no=n(` to generate samples for one of the processes only. Instances of this class will
always yield a number of samples that is a round multiple of the actual batch size (depending of the value of
`),Bt=r("code"),co=n("split_batches"),lo=n(", this is either "),qt=r("code"),io=n("batch_size"),po=n(" or "),Vt=r("code"),ho=n("batch_size x num_processes"),fo=n(`). Depending on the value of the
`),Ft=r("code"),uo=n("drop_last"),mo=n(` attribute of the batch sampler passed, it will either stop the iteration at the first batch that would
be too small or loop with indices from the beginning.`),Ia=l(),J=r("h2"),ge=r("a"),Rt=r("span"),u(Ve.$$.fragment),go=l(),Wt=r("span"),vo=n("Distributed Config"),za=l(),K=r("h3"),ve=r("a"),Ht=r("span"),u(Fe.$$.fragment),_o=l(),Mt=r("span"),bo=n("AcceleratorState"),La=l(),Q=r("div"),u(Re.$$.fragment),$o=l(),Y=r("p"),yo=n("This is a variation of a "),We=r("a"),wo=n("singleton class"),Eo=n(` in the sense that all
instance of `),jt=r("code"),Do=n("AcceleratorState"),So=n(" share the same state, which is initialized on the first instantiation."),Na=l(),Z=r("h3"),_e=r("a"),Xt=r("span"),u(He.$$.fragment),To=l(),Jt=r("span"),xo=n("DistributedType"),Oa=l(),P=r("div"),u(Me.$$.fragment),ko=l(),Kt=r("p"),Po=n("Represents a type of distributed environment."),Ao=l(),Qt=r("p"),Io=n("Values:"),zo=l(),A=r("ul"),it=r("li"),Yt=r("strong"),Lo=n("NO"),No=n(" \u2014 Not a distributed environment, just a single process."),Oo=l(),dt=r("li"),Zt=r("strong"),Co=n("MULTI_CPU"),Go=n(" \u2014 Distributed on multiple CPU nodes."),Uo=l(),pt=r("li"),ea=r("strong"),Bo=n("MULTI_GPU"),qo=n(" \u2014 Distributed on multiple GPUs."),Vo=l(),ht=r("li"),ta=r("strong"),Fo=n("DEEPSPEED"),Ro=n(" \u2014 Using DeepSpeed."),Wo=l(),ft=r("li"),aa=r("strong"),Ho=n("TPU"),Mo=n(" \u2014 Distributed on TPUs."),Ca=l(),ee=r("h2"),be=r("a"),ra=r("span"),u(je.$$.fragment),jo=l(),oa=r("span"),Xo=n("Tracking"),Ga=l(),I=r("div"),u(Xe.$$.fragment),Jo=l(),sa=r("p"),Ko=n("A base Tracker class to be used for all logging integration implementations."),Qo=l(),$e=r("div"),u(Je.$$.fragment),Yo=l(),te=r("p"),Zo=n("Logs "),na=r("code"),es=n("values"),ts=n(" to the current run. Base "),ca=r("code"),as=n("log"),rs=n(" implementations of a tracking API should go in here, along with\nspecial behavior for the `step parameter."),os=l(),ye=r("div"),u(Ke.$$.fragment),ss=l(),Qe=r("p"),ns=n("Logs "),la=r("code"),cs=n("values"),ls=n(` as hyperparameters for the run. Implementations should use the experiment configuration
functionality of a tracking API.`),Ua=l(),ae=r("h2"),we=r("a"),ia=r("span"),u(Ye.$$.fragment),is=l(),da=r("span"),ds=n("Utilities"),Ba=l(),re=r("div"),u(Ze.$$.fragment),ps=l(),pa=r("p"),hs=n("Extract a model from its distributed containers."),qa=l(),oe=r("div"),u(et.$$.fragment),fs=l(),ha=r("p"),us=n("Recursively gather tensor in a nested list/tuple/dictionary of tensors from all devices."),Va=l(),se=r("div"),u(tt.$$.fragment),ms=l(),fa=r("p"),gs=n("Recursively sends the elements in a nested list/tuple/dictionary of tensors to a given device."),Fa=l(),ne=r("div"),u(at.$$.fragment),vs=l(),C=r("p"),_s=n("Helper function for reproducible behavior to set the seed in "),ua=r("code"),bs=n("random"),$s=n(", "),ma=r("code"),ys=n("numpy"),ws=n(", "),ga=r("code"),Es=n("torch"),Ds=n("."),Ra=l(),rt=r("div"),u(ot.$$.fragment),Wa=l(),st=r("div"),u(nt.$$.fragment),Ha=l(),G=r("div"),u(ct.$$.fragment),Ss=l(),va=r("p"),Ts=n("Introduces a blocking point in the script, making sure all processes have reached this point before continuing."),xs=l(),u(Ee.$$.fragment),this.h()},l(e){const p=Qn('[data-svelte="svelte-1phssyn"]',document.head);f=o(p,"META",{name:!0,content:!0}),p.forEach(a),S=i(e),$=o(e,"H1",{class:!0});var lt=s($);y=o(lt,"A",{id:!0,class:!0,href:!0});var _a=s(y);x=o(_a,"SPAN",{});var ba=s(x);m(w.$$.fragment,ba),ba.forEach(a),_a.forEach(a),E=i(lt),L=o(lt,"SPAN",{});var Ns=s(L);vr=c(Ns,"Internals"),Ns.forEach(a),lt.forEach(a),$a=i(e),q=o(e,"H2",{class:!0});var ja=s(q);le=o(ja,"A",{id:!0,class:!0,href:!0});var Os=s(le);bt=o(Os,"SPAN",{});var Cs=s(bt);m(Pe.$$.fragment,Cs),Cs.forEach(a),Os.forEach(a),_r=i(ja),$t=o(ja,"SPAN",{});var Gs=s($t);br=c(Gs,"Optimizer"),Gs.forEach(a),ja.forEach(a),ya=i(e),V=o(e,"DIV",{class:!0});var Xa=s(V);m(Ae.$$.fragment,Xa),$r=i(Xa),yt=o(Xa,"P",{});var Us=s(yt);yr=c(Us,"Internal wrapper around a torch optimizer."),Us.forEach(a),Xa.forEach(a),wa=i(e),F=o(e,"H2",{class:!0});var Ja=s(F);ie=o(Ja,"A",{id:!0,class:!0,href:!0});var Bs=s(ie);wt=o(Bs,"SPAN",{});var qs=s(wt);m(Ie.$$.fragment,qs),qs.forEach(a),Bs.forEach(a),wr=i(Ja),Et=o(Ja,"SPAN",{});var Vs=s(Et);Er=c(Vs,"DataLoader"),Vs.forEach(a),Ja.forEach(a),Ea=i(e),de=o(e,"P",{});var Ka=s(de);Dr=c(Ka,"The main work on your PyTorch "),Dt=o(Ka,"CODE",{});var Fs=s(Dt);Sr=c(Fs,"DataLoader"),Fs.forEach(a),Tr=c(Ka," is done by the following function:"),Ka.forEach(a),Da=i(e),k=o(e,"DIV",{class:!0});var De=s(k);m(ze.$$.fragment,De),xr=i(De),Le=o(De,"P",{});var Qa=s(Le);kr=c(Qa,"Wraps a PyTorch "),St=o(Qa,"CODE",{});var Rs=s(St);Pr=c(Rs,"DataLoader"),Rs.forEach(a),Ar=c(Qa," to generate batches for one of the processes only."),Qa.forEach(a),Ir=i(De),R=o(De,"P",{});var ut=s(R);zr=c(ut,"Depending on the value of the "),Tt=o(ut,"CODE",{});var Ws=s(Tt);Lr=c(Ws,"drop_last"),Ws.forEach(a),Nr=c(ut," attribute of the "),xt=o(ut,"CODE",{});var Hs=s(xt);Or=c(Hs,"dataloader"),Hs.forEach(a),Cr=c(ut,` passed, it will either stop the iteration
at the first batch that would be too small / not present on all processes or loop with indices from the beginning.`),ut.forEach(a),Gr=i(De),m(pe.$$.fragment,De),De.forEach(a),Sa=i(e),W=o(e,"H3",{class:!0});var Ya=s(W);he=o(Ya,"A",{id:!0,class:!0,href:!0});var Ms=s(he);kt=o(Ms,"SPAN",{});var js=s(kt);m(Ne.$$.fragment,js),js.forEach(a),Ms.forEach(a),Ur=i(Ya),Pt=o(Ya,"SPAN",{});var Xs=s(Pt);Br=c(Xs,"BatchSamplerShard"),Xs.forEach(a),Ya.forEach(a),Ta=i(e),H=o(e,"DIV",{class:!0});var Za=s(H);m(Oe.$$.fragment,Za),qr=i(Za),Ce=o(Za,"P",{});var er=s(Ce);Vr=c(er,"Subclass of a PyTorch "),At=o(er,"CODE",{});var Js=s(At);Fr=c(Js,"DataLoader"),Js.forEach(a),Rr=c(er," that will deal with device placement and current distributed setup."),er.forEach(a),Za.forEach(a),xa=i(e),M=o(e,"H3",{class:!0});var tr=s(M);fe=o(tr,"A",{id:!0,class:!0,href:!0});var Ks=s(fe);It=o(Ks,"SPAN",{});var Qs=s(It);m(Ge.$$.fragment,Qs),Qs.forEach(a),Ks.forEach(a),Wr=i(tr),zt=o(tr,"SPAN",{});var Ys=s(zt);Hr=c(Ys,"BatchSamplerShard"),Ys.forEach(a),tr.forEach(a),ka=i(e),N=o(e,"DIV",{class:!0});var mt=s(N);m(Ue.$$.fragment,mt),Mr=i(mt),O=o(mt,"P",{});var Se=s(O);jr=c(Se,"Wraps a PyTorch "),Lt=o(Se,"CODE",{});var Zs=s(Lt);Xr=c(Zs,"BatchSampler"),Zs.forEach(a),Jr=c(Se,` to generate batches for one of the processes only. Instances of this class will
always yield a number of batches that is a round multiple of `),Nt=o(Se,"CODE",{});var en=s(Nt);Kr=c(en,"num_processes"),en.forEach(a),Qr=c(Se,` and that all have the same size.
Depending on the value of the `),Ot=o(Se,"CODE",{});var tn=s(Ot);Yr=c(tn,"drop_last"),tn.forEach(a),Zr=c(Se,` attribute of the batch sampler passed, it will either stop the iteration
at the first batch that would be too small / not present on all processes or loop with indices from the beginning.`),Se.forEach(a),eo=i(mt),m(ue.$$.fragment,mt),mt.forEach(a),Pa=i(e),j=o(e,"H3",{class:!0});var ar=s(j);me=o(ar,"A",{id:!0,class:!0,href:!0});var an=s(me);Ct=o(an,"SPAN",{});var rn=s(Ct);m(Be.$$.fragment,rn),rn.forEach(a),an.forEach(a),to=i(ar),Gt=o(ar,"SPAN",{});var on=s(Gt);ao=c(on,"IterableDatasetShard"),on.forEach(a),ar.forEach(a),Aa=i(e),X=o(e,"DIV",{class:!0});var rr=s(X);m(qe.$$.fragment,rr),ro=i(rr),T=o(rr,"P",{});var z=s(T);oo=c(z,"Wraps a PyTorch "),Ut=o(z,"CODE",{});var sn=s(Ut);so=c(sn,"IterableDataset"),sn.forEach(a),no=c(z,` to generate samples for one of the processes only. Instances of this class will
always yield a number of samples that is a round multiple of the actual batch size (depending of the value of
`),Bt=o(z,"CODE",{});var nn=s(Bt);co=c(nn,"split_batches"),nn.forEach(a),lo=c(z,", this is either "),qt=o(z,"CODE",{});var cn=s(qt);io=c(cn,"batch_size"),cn.forEach(a),po=c(z," or "),Vt=o(z,"CODE",{});var ln=s(Vt);ho=c(ln,"batch_size x num_processes"),ln.forEach(a),fo=c(z,`). Depending on the value of the
`),Ft=o(z,"CODE",{});var dn=s(Ft);uo=c(dn,"drop_last"),dn.forEach(a),mo=c(z,` attribute of the batch sampler passed, it will either stop the iteration at the first batch that would
be too small or loop with indices from the beginning.`),z.forEach(a),rr.forEach(a),Ia=i(e),J=o(e,"H2",{class:!0});var or=s(J);ge=o(or,"A",{id:!0,class:!0,href:!0});var pn=s(ge);Rt=o(pn,"SPAN",{});var hn=s(Rt);m(Ve.$$.fragment,hn),hn.forEach(a),pn.forEach(a),go=i(or),Wt=o(or,"SPAN",{});var fn=s(Wt);vo=c(fn,"Distributed Config"),fn.forEach(a),or.forEach(a),za=i(e),K=o(e,"H3",{class:!0});var sr=s(K);ve=o(sr,"A",{id:!0,class:!0,href:!0});var un=s(ve);Ht=o(un,"SPAN",{});var mn=s(Ht);m(Fe.$$.fragment,mn),mn.forEach(a),un.forEach(a),_o=i(sr),Mt=o(sr,"SPAN",{});var gn=s(Mt);bo=c(gn,"AcceleratorState"),gn.forEach(a),sr.forEach(a),La=i(e),Q=o(e,"DIV",{class:!0});var nr=s(Q);m(Re.$$.fragment,nr),$o=i(nr),Y=o(nr,"P",{});var gt=s(Y);yo=c(gt,"This is a variation of a "),We=o(gt,"A",{href:!0,rel:!0});var vn=s(We);wo=c(vn,"singleton class"),vn.forEach(a),Eo=c(gt,` in the sense that all
instance of `),jt=o(gt,"CODE",{});var _n=s(jt);Do=c(_n,"AcceleratorState"),_n.forEach(a),So=c(gt," share the same state, which is initialized on the first instantiation."),gt.forEach(a),nr.forEach(a),Na=i(e),Z=o(e,"H3",{class:!0});var cr=s(Z);_e=o(cr,"A",{id:!0,class:!0,href:!0});var bn=s(_e);Xt=o(bn,"SPAN",{});var $n=s(Xt);m(He.$$.fragment,$n),$n.forEach(a),bn.forEach(a),To=i(cr),Jt=o(cr,"SPAN",{});var yn=s(Jt);xo=c(yn,"DistributedType"),yn.forEach(a),cr.forEach(a),Oa=i(e),P=o(e,"DIV",{class:!0});var Te=s(P);m(Me.$$.fragment,Te),ko=i(Te),Kt=o(Te,"P",{});var wn=s(Kt);Po=c(wn,"Represents a type of distributed environment."),wn.forEach(a),Ao=i(Te),Qt=o(Te,"P",{});var En=s(Qt);Io=c(En,"Values:"),En.forEach(a),zo=i(Te),A=o(Te,"UL",{});var B=s(A);it=o(B,"LI",{});var ks=s(it);Yt=o(ks,"STRONG",{});var Dn=s(Yt);Lo=c(Dn,"NO"),Dn.forEach(a),No=c(ks," \u2014 Not a distributed environment, just a single process."),ks.forEach(a),Oo=i(B),dt=o(B,"LI",{});var Ps=s(dt);Zt=o(Ps,"STRONG",{});var Sn=s(Zt);Co=c(Sn,"MULTI_CPU"),Sn.forEach(a),Go=c(Ps," \u2014 Distributed on multiple CPU nodes."),Ps.forEach(a),Uo=i(B),pt=o(B,"LI",{});var As=s(pt);ea=o(As,"STRONG",{});var Tn=s(ea);Bo=c(Tn,"MULTI_GPU"),Tn.forEach(a),qo=c(As," \u2014 Distributed on multiple GPUs."),As.forEach(a),Vo=i(B),ht=o(B,"LI",{});var Is=s(ht);ta=o(Is,"STRONG",{});var xn=s(ta);Fo=c(xn,"DEEPSPEED"),xn.forEach(a),Ro=c(Is," \u2014 Using DeepSpeed."),Is.forEach(a),Wo=i(B),ft=o(B,"LI",{});var zs=s(ft);aa=o(zs,"STRONG",{});var kn=s(aa);Ho=c(kn,"TPU"),kn.forEach(a),Mo=c(zs," \u2014 Distributed on TPUs."),zs.forEach(a),B.forEach(a),Te.forEach(a),Ca=i(e),ee=o(e,"H2",{class:!0});var lr=s(ee);be=o(lr,"A",{id:!0,class:!0,href:!0});var Pn=s(be);ra=o(Pn,"SPAN",{});var An=s(ra);m(je.$$.fragment,An),An.forEach(a),Pn.forEach(a),jo=i(lr),oa=o(lr,"SPAN",{});var In=s(oa);Xo=c(In,"Tracking"),In.forEach(a),lr.forEach(a),Ga=i(e),I=o(e,"DIV",{class:!0});var xe=s(I);m(Xe.$$.fragment,xe),Jo=i(xe),sa=o(xe,"P",{});var zn=s(sa);Ko=c(zn,"A base Tracker class to be used for all logging integration implementations."),zn.forEach(a),Qo=i(xe),$e=o(xe,"DIV",{class:!0});var ir=s($e);m(Je.$$.fragment,ir),Yo=i(ir),te=o(ir,"P",{});var vt=s(te);Zo=c(vt,"Logs "),na=o(vt,"CODE",{});var Ln=s(na);es=c(Ln,"values"),Ln.forEach(a),ts=c(vt," to the current run. Base "),ca=o(vt,"CODE",{});var Nn=s(ca);as=c(Nn,"log"),Nn.forEach(a),rs=c(vt," implementations of a tracking API should go in here, along with\nspecial behavior for the `step parameter."),vt.forEach(a),ir.forEach(a),os=i(xe),ye=o(xe,"DIV",{class:!0});var dr=s(ye);m(Ke.$$.fragment,dr),ss=i(dr),Qe=o(dr,"P",{});var pr=s(Qe);ns=c(pr,"Logs "),la=o(pr,"CODE",{});var On=s(la);cs=c(On,"values"),On.forEach(a),ls=c(pr,` as hyperparameters for the run. Implementations should use the experiment configuration
functionality of a tracking API.`),pr.forEach(a),dr.forEach(a),xe.forEach(a),Ua=i(e),ae=o(e,"H2",{class:!0});var hr=s(ae);we=o(hr,"A",{id:!0,class:!0,href:!0});var Cn=s(we);ia=o(Cn,"SPAN",{});var Gn=s(ia);m(Ye.$$.fragment,Gn),Gn.forEach(a),Cn.forEach(a),is=i(hr),da=o(hr,"SPAN",{});var Un=s(da);ds=c(Un,"Utilities"),Un.forEach(a),hr.forEach(a),Ba=i(e),re=o(e,"DIV",{class:!0});var fr=s(re);m(Ze.$$.fragment,fr),ps=i(fr),pa=o(fr,"P",{});var Bn=s(pa);hs=c(Bn,"Extract a model from its distributed containers."),Bn.forEach(a),fr.forEach(a),qa=i(e),oe=o(e,"DIV",{class:!0});var ur=s(oe);m(et.$$.fragment,ur),fs=i(ur),ha=o(ur,"P",{});var qn=s(ha);us=c(qn,"Recursively gather tensor in a nested list/tuple/dictionary of tensors from all devices."),qn.forEach(a),ur.forEach(a),Va=i(e),se=o(e,"DIV",{class:!0});var mr=s(se);m(tt.$$.fragment,mr),ms=i(mr),fa=o(mr,"P",{});var Vn=s(fa);gs=c(Vn,"Recursively sends the elements in a nested list/tuple/dictionary of tensors to a given device."),Vn.forEach(a),mr.forEach(a),Fa=i(e),ne=o(e,"DIV",{class:!0});var gr=s(ne);m(at.$$.fragment,gr),vs=i(gr),C=o(gr,"P",{});var ke=s(C);_s=c(ke,"Helper function for reproducible behavior to set the seed in "),ua=o(ke,"CODE",{});var Fn=s(ua);bs=c(Fn,"random"),Fn.forEach(a),$s=c(ke,", "),ma=o(ke,"CODE",{});var Rn=s(ma);ys=c(Rn,"numpy"),Rn.forEach(a),ws=c(ke,", "),ga=o(ke,"CODE",{});var Wn=s(ga);Es=c(Wn,"torch"),Wn.forEach(a),Ds=c(ke,"."),ke.forEach(a),gr.forEach(a),Ra=i(e),rt=o(e,"DIV",{class:!0});var Hn=s(rt);m(ot.$$.fragment,Hn),Hn.forEach(a),Wa=i(e),st=o(e,"DIV",{class:!0});var Mn=s(st);m(nt.$$.fragment,Mn),Mn.forEach(a),Ha=i(e),G=o(e,"DIV",{class:!0});var _t=s(G);m(ct.$$.fragment,_t),Ss=i(_t),va=o(_t,"P",{});var jn=s(va);Ts=c(jn,"Introduces a blocking point in the script, making sure all processes have reached this point before continuing."),jn.forEach(a),xs=i(_t),m(Ee.$$.fragment,_t),_t.forEach(a),this.h()},h(){d(f,"name","hf:doc:metadata"),d(f,"content",JSON.stringify(rc)),d(y,"id","internals"),d(y,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(y,"href","#internals"),d($,"class","relative group"),d(le,"id","accelerate.optimizer.AcceleratedOptimizer"),d(le,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(le,"href","#accelerate.optimizer.AcceleratedOptimizer"),d(q,"class","relative group"),d(V,"class","docstring"),d(ie,"id","accelerate.data_loader.prepare_data_loader"),d(ie,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(ie,"href","#accelerate.data_loader.prepare_data_loader"),d(F,"class","relative group"),d(k,"class","docstring"),d(he,"id","accelerate.data_loader.DataLoaderShard"),d(he,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(he,"href","#accelerate.data_loader.DataLoaderShard"),d(W,"class","relative group"),d(H,"class","docstring"),d(fe,"id","accelerate.data_loader.BatchSamplerShard"),d(fe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(fe,"href","#accelerate.data_loader.BatchSamplerShard"),d(M,"class","relative group"),d(N,"class","docstring"),d(me,"id","accelerate.data_loader.IterableDatasetShard"),d(me,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(me,"href","#accelerate.data_loader.IterableDatasetShard"),d(j,"class","relative group"),d(X,"class","docstring"),d(ge,"id","distributed-config"),d(ge,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(ge,"href","#distributed-config"),d(J,"class","relative group"),d(ve,"id","accelerate.state.AcceleratorState"),d(ve,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(ve,"href","#accelerate.state.AcceleratorState"),d(K,"class","relative group"),d(We,"href","https://en.wikipedia.org/wiki/Singleton_pattern"),d(We,"rel","nofollow"),d(Q,"class","docstring"),d(_e,"id","accelerate.DistributedType"),d(_e,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(_e,"href","#accelerate.DistributedType"),d(Z,"class","relative group"),d(P,"class","docstring"),d(be,"id","accelerate.tracking.GeneralTracker"),d(be,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(be,"href","#accelerate.tracking.GeneralTracker"),d(ee,"class","relative group"),d($e,"class","docstring"),d(ye,"class","docstring"),d(I,"class","docstring"),d(we,"id","accelerate.utils.extract_model_from_parallel"),d(we,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(we,"href","#accelerate.utils.extract_model_from_parallel"),d(ae,"class","relative group"),d(re,"class","docstring"),d(oe,"class","docstring"),d(se,"class","docstring"),d(ne,"class","docstring"),d(rt,"class","docstring"),d(st,"class","docstring"),d(G,"class","docstring")},m(e,p){t(document.head,f),h(e,S,p),h(e,$,p),t($,y),t(y,x),g(w,x,null),t($,E),t($,L),t(L,vr),h(e,$a,p),h(e,q,p),t(q,le),t(le,bt),g(Pe,bt,null),t(q,_r),t(q,$t),t($t,br),h(e,ya,p),h(e,V,p),g(Ae,V,null),t(V,$r),t(V,yt),t(yt,yr),h(e,wa,p),h(e,F,p),t(F,ie),t(ie,wt),g(Ie,wt,null),t(F,wr),t(F,Et),t(Et,Er),h(e,Ea,p),h(e,de,p),t(de,Dr),t(de,Dt),t(Dt,Sr),t(de,Tr),h(e,Da,p),h(e,k,p),g(ze,k,null),t(k,xr),t(k,Le),t(Le,kr),t(Le,St),t(St,Pr),t(Le,Ar),t(k,Ir),t(k,R),t(R,zr),t(R,Tt),t(Tt,Lr),t(R,Nr),t(R,xt),t(xt,Or),t(R,Cr),t(k,Gr),g(pe,k,null),h(e,Sa,p),h(e,W,p),t(W,he),t(he,kt),g(Ne,kt,null),t(W,Ur),t(W,Pt),t(Pt,Br),h(e,Ta,p),h(e,H,p),g(Oe,H,null),t(H,qr),t(H,Ce),t(Ce,Vr),t(Ce,At),t(At,Fr),t(Ce,Rr),h(e,xa,p),h(e,M,p),t(M,fe),t(fe,It),g(Ge,It,null),t(M,Wr),t(M,zt),t(zt,Hr),h(e,ka,p),h(e,N,p),g(Ue,N,null),t(N,Mr),t(N,O),t(O,jr),t(O,Lt),t(Lt,Xr),t(O,Jr),t(O,Nt),t(Nt,Kr),t(O,Qr),t(O,Ot),t(Ot,Yr),t(O,Zr),t(N,eo),g(ue,N,null),h(e,Pa,p),h(e,j,p),t(j,me),t(me,Ct),g(Be,Ct,null),t(j,to),t(j,Gt),t(Gt,ao),h(e,Aa,p),h(e,X,p),g(qe,X,null),t(X,ro),t(X,T),t(T,oo),t(T,Ut),t(Ut,so),t(T,no),t(T,Bt),t(Bt,co),t(T,lo),t(T,qt),t(qt,io),t(T,po),t(T,Vt),t(Vt,ho),t(T,fo),t(T,Ft),t(Ft,uo),t(T,mo),h(e,Ia,p),h(e,J,p),t(J,ge),t(ge,Rt),g(Ve,Rt,null),t(J,go),t(J,Wt),t(Wt,vo),h(e,za,p),h(e,K,p),t(K,ve),t(ve,Ht),g(Fe,Ht,null),t(K,_o),t(K,Mt),t(Mt,bo),h(e,La,p),h(e,Q,p),g(Re,Q,null),t(Q,$o),t(Q,Y),t(Y,yo),t(Y,We),t(We,wo),t(Y,Eo),t(Y,jt),t(jt,Do),t(Y,So),h(e,Na,p),h(e,Z,p),t(Z,_e),t(_e,Xt),g(He,Xt,null),t(Z,To),t(Z,Jt),t(Jt,xo),h(e,Oa,p),h(e,P,p),g(Me,P,null),t(P,ko),t(P,Kt),t(Kt,Po),t(P,Ao),t(P,Qt),t(Qt,Io),t(P,zo),t(P,A),t(A,it),t(it,Yt),t(Yt,Lo),t(it,No),t(A,Oo),t(A,dt),t(dt,Zt),t(Zt,Co),t(dt,Go),t(A,Uo),t(A,pt),t(pt,ea),t(ea,Bo),t(pt,qo),t(A,Vo),t(A,ht),t(ht,ta),t(ta,Fo),t(ht,Ro),t(A,Wo),t(A,ft),t(ft,aa),t(aa,Ho),t(ft,Mo),h(e,Ca,p),h(e,ee,p),t(ee,be),t(be,ra),g(je,ra,null),t(ee,jo),t(ee,oa),t(oa,Xo),h(e,Ga,p),h(e,I,p),g(Xe,I,null),t(I,Jo),t(I,sa),t(sa,Ko),t(I,Qo),t(I,$e),g(Je,$e,null),t($e,Yo),t($e,te),t(te,Zo),t(te,na),t(na,es),t(te,ts),t(te,ca),t(ca,as),t(te,rs),t(I,os),t(I,ye),g(Ke,ye,null),t(ye,ss),t(ye,Qe),t(Qe,ns),t(Qe,la),t(la,cs),t(Qe,ls),h(e,Ua,p),h(e,ae,p),t(ae,we),t(we,ia),g(Ye,ia,null),t(ae,is),t(ae,da),t(da,ds),h(e,Ba,p),h(e,re,p),g(Ze,re,null),t(re,ps),t(re,pa),t(pa,hs),h(e,qa,p),h(e,oe,p),g(et,oe,null),t(oe,fs),t(oe,ha),t(ha,us),h(e,Va,p),h(e,se,p),g(tt,se,null),t(se,ms),t(se,fa),t(fa,gs),h(e,Fa,p),h(e,ne,p),g(at,ne,null),t(ne,vs),t(ne,C),t(C,_s),t(C,ua),t(ua,bs),t(C,$s),t(C,ma),t(ma,ys),t(C,ws),t(C,ga),t(ga,Es),t(C,Ds),h(e,Ra,p),h(e,rt,p),g(ot,rt,null),h(e,Wa,p),h(e,st,p),g(nt,st,null),h(e,Ha,p),h(e,G,p),g(ct,G,null),t(G,Ss),t(G,va),t(va,Ts),t(G,xs),g(Ee,G,null),Ma=!0},p(e,[p]){const lt={};p&2&&(lt.$$scope={dirty:p,ctx:e}),pe.$set(lt);const _a={};p&2&&(_a.$$scope={dirty:p,ctx:e}),ue.$set(_a);const ba={};p&2&&(ba.$$scope={dirty:p,ctx:e}),Ee.$set(ba)},i(e){Ma||(v(w.$$.fragment,e),v(Pe.$$.fragment,e),v(Ae.$$.fragment,e),v(Ie.$$.fragment,e),v(ze.$$.fragment,e),v(pe.$$.fragment,e),v(Ne.$$.fragment,e),v(Oe.$$.fragment,e),v(Ge.$$.fragment,e),v(Ue.$$.fragment,e),v(ue.$$.fragment,e),v(Be.$$.fragment,e),v(qe.$$.fragment,e),v(Ve.$$.fragment,e),v(Fe.$$.fragment,e),v(Re.$$.fragment,e),v(He.$$.fragment,e),v(Me.$$.fragment,e),v(je.$$.fragment,e),v(Xe.$$.fragment,e),v(Je.$$.fragment,e),v(Ke.$$.fragment,e),v(Ye.$$.fragment,e),v(Ze.$$.fragment,e),v(et.$$.fragment,e),v(tt.$$.fragment,e),v(at.$$.fragment,e),v(ot.$$.fragment,e),v(nt.$$.fragment,e),v(ct.$$.fragment,e),v(Ee.$$.fragment,e),Ma=!0)},o(e){_(w.$$.fragment,e),_(Pe.$$.fragment,e),_(Ae.$$.fragment,e),_(Ie.$$.fragment,e),_(ze.$$.fragment,e),_(pe.$$.fragment,e),_(Ne.$$.fragment,e),_(Oe.$$.fragment,e),_(Ge.$$.fragment,e),_(Ue.$$.fragment,e),_(ue.$$.fragment,e),_(Be.$$.fragment,e),_(qe.$$.fragment,e),_(Ve.$$.fragment,e),_(Fe.$$.fragment,e),_(Re.$$.fragment,e),_(He.$$.fragment,e),_(Me.$$.fragment,e),_(je.$$.fragment,e),_(Xe.$$.fragment,e),_(Je.$$.fragment,e),_(Ke.$$.fragment,e),_(Ye.$$.fragment,e),_(Ze.$$.fragment,e),_(et.$$.fragment,e),_(tt.$$.fragment,e),_(at.$$.fragment,e),_(ot.$$.fragment,e),_(nt.$$.fragment,e),_(ct.$$.fragment,e),_(Ee.$$.fragment,e),Ma=!1},d(e){a(f),e&&a(S),e&&a($),b(w),e&&a($a),e&&a(q),b(Pe),e&&a(ya),e&&a(V),b(Ae),e&&a(wa),e&&a(F),b(Ie),e&&a(Ea),e&&a(de),e&&a(Da),e&&a(k),b(ze),b(pe),e&&a(Sa),e&&a(W),b(Ne),e&&a(Ta),e&&a(H),b(Oe),e&&a(xa),e&&a(M),b(Ge),e&&a(ka),e&&a(N),b(Ue),b(ue),e&&a(Pa),e&&a(j),b(Be),e&&a(Aa),e&&a(X),b(qe),e&&a(Ia),e&&a(J),b(Ve),e&&a(za),e&&a(K),b(Fe),e&&a(La),e&&a(Q),b(Re),e&&a(Na),e&&a(Z),b(He),e&&a(Oa),e&&a(P),b(Me),e&&a(Ca),e&&a(ee),b(je),e&&a(Ga),e&&a(I),b(Xe),b(Je),b(Ke),e&&a(Ua),e&&a(ae),b(Ye),e&&a(Ba),e&&a(re),b(Ze),e&&a(qa),e&&a(oe),b(et),e&&a(Va),e&&a(se),b(tt),e&&a(Fa),e&&a(ne),b(at),e&&a(Ra),e&&a(rt),b(ot),e&&a(Wa),e&&a(st),b(nt),e&&a(Ha),e&&a(G),b(ct),b(Ee)}}}const rc={local:"internals",sections:[{local:"accelerate.optimizer.AcceleratedOptimizer",title:"Optimizer"},{local:"accelerate.data_loader.prepare_data_loader",sections:[{local:"accelerate.data_loader.DataLoaderShard",title:"BatchSamplerShard"},{local:"accelerate.data_loader.BatchSamplerShard",title:"BatchSamplerShard"},{local:"accelerate.data_loader.IterableDatasetShard",title:"IterableDatasetShard"}],title:"DataLoader"},{local:"distributed-config",sections:[{local:"accelerate.state.AcceleratorState",title:"AcceleratorState"},{local:"accelerate.DistributedType",title:"DistributedType"}],title:"Distributed Config"},{local:"accelerate.tracking.GeneralTracker",title:"Tracking"},{local:"accelerate.utils.extract_model_from_parallel",title:"Utilities"}],title:"Internals"};function oc(ce){return Yn(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class ic extends Xn{constructor(f){super();Jn(this,f,oc,ac,Kn,{})}}export{ic as default,rc as metadata};
