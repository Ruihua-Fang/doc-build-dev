import{S as oc,i as sc,s as nc,e as r,k as l,w as u,t as n,M as cc,c as o,d as a,m as i,a as s,x as m,h as c,b as d,F as t,g as p,y as g,q as v,o as _,B as b,v as lc}from"../chunks/vendor-19e06bd2.js";import{T as Fs}from"../chunks/Tip-f0fa2d82.js";import{D as E}from"../chunks/Docstring-395e5a9c.js";import{C as ic}from"../chunks/CodeBlock-9dd1fdfb.js";import{I as U}from"../chunks/IconCopyLink-3c713d38.js";function dc(ie){let f,S,$,y,k;return{c(){f=r("p"),S=n("This does not support "),$=r("code"),y=n("BatchSampler"),k=n(" with varying batch size yet.")},l(w){f=o(w,"P",{});var D=s(f);S=c(D,"This does not support "),$=o(D,"CODE",{});var L=s($);y=c(L,"BatchSampler"),L.forEach(a),k=c(D," with varying batch size yet."),D.forEach(a)},m(w,D){p(w,f,D),t(f,S),t(f,$),t($,y),t(f,k)},d(w){w&&a(f)}}}function hc(ie){let f,S,$,y,k;return{c(){f=r("p"),S=n("This does not support "),$=r("code"),y=n("BatchSampler"),k=n(" with varying batch size yet.")},l(w){f=o(w,"P",{});var D=s(f);S=c(D,"This does not support "),$=o(D,"CODE",{});var L=s($);y=c(L,"BatchSampler"),L.forEach(a),k=c(D," with varying batch size yet."),D.forEach(a)},m(w,D){p(w,f,D),t(f,S),t(f,$),t($,y),t(f,k)},d(w){w&&a(f)}}}function pc(ie){let f,S;return{c(){f=r("p"),S=n("Make sure all processes will reach this instruction otherwise one of your processes will hang forever.")},l($){f=o($,"P",{});var y=s(f);S=c(y,"Make sure all processes will reach this instruction otherwise one of your processes will hang forever."),y.forEach(a)},m($,y){p($,f,y),t(f,S)},d($){$&&a(f)}}}function fc(ie){let f,S,$,y,k,w,D,L,wr,Sa,F,de,Et,Ae,Er,Dt,Dr,Ta,R,Ie,Sr,St,Tr,xa,W,he,Tt,ze,xr,xt,kr,ka,pe,Pr,kt,Ar,Ir,Pa,P,Le,zr,Ne,Lr,Pt,Nr,Or,Cr,H,Gr,At,Ur,Br,It,Vr,qr,Fr,fe,Aa,M,ue,zt,Oe,Rr,Lt,Wr,Ia,j,Ce,Hr,Ge,Mr,Nt,jr,Xr,za,X,me,Ot,Ue,Jr,Ct,Kr,La,N,Be,Qr,O,Yr,Gt,Zr,eo,Ut,to,ao,Bt,ro,oo,so,ge,Na,J,ve,Vt,Ve,no,qt,co,Oa,K,qe,lo,T,io,Ft,ho,po,Rt,fo,uo,Wt,mo,go,Ht,vo,_o,Mt,bo,$o,Ca,Q,_e,jt,Fe,yo,Xt,wo,Ga,Y,be,Jt,Re,Eo,Kt,Do,Ua,Z,We,So,ee,To,He,xo,ko,Qt,Po,Ao,Ba,te,$e,Yt,Me,Io,Zt,zo,Va,A,je,Lo,ea,No,Oo,ta,Co,Go,I,pt,aa,Uo,Bo,Vo,ft,ra,qo,Fo,Ro,ut,oa,Wo,Ho,Mo,mt,sa,jo,Xo,Jo,gt,na,Ko,Qo,qa,ae,ye,ca,Xe,Yo,la,Zo,Fa,x,Je,es,ia,ts,as,B,Ke,rs,da,os,ss,Qe,ns,we,Ye,cs,re,ls,ha,is,ds,pa,hs,ps,fs,Ee,Ze,us,et,ms,fa,gs,vs,Ra,oe,De,ua,tt,_s,ma,bs,Wa,se,at,$s,ga,ys,Ha,ne,rt,ws,va,Es,Ma,ce,ot,Ds,_a,Ss,ja,le,st,Ts,C,xs,ba,ks,Ps,$a,As,Is,ya,zs,Ls,Xa,nt,ct,Ja,lt,it,Ka,G,dt,Ns,wa,Os,Cs,Se,Qa;return w=new U({}),Ae=new U({}),Ie=new E({props:{name:"class accelerate.optimizer.AcceleratedOptimizer",anchor:"accelerate.optimizer.AcceleratedOptimizer",parameters:[{name:"optimizer",val:""},{name:"device_placement",val:" = True"},{name:"scaler",val:" = None"}],source:"https://github.com/huggingface/accelerate/blob/pr_293/src/accelerate/optimizer.py#L39",parametersDescription:[{anchor:"accelerate.optimizer.AcceleratedOptimizer.optimizer",description:`<strong>optimizer</strong> (<code>torch.optim.optimizer.Optimizer</code>) &#x2014;
The optimizer to wrap.`,name:"optimizer"},{anchor:"accelerate.optimizer.AcceleratedOptimizer.device_placement",description:`<strong>device_placement</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not the optimizer should handle device placement. If so, it will place the state dictionary of
<code>optimizer</code> on the right device.`,name:"device_placement"},{anchor:"accelerate.optimizer.AcceleratedOptimizer.scaler",description:`<strong>scaler</strong> (<code>torch.cuda.amp.grad_scaler.GradScaler</code>, <em>optional</em>) &#x2014;
The scaler to use in the step function if training with mixed precision.`,name:"scaler"}]}}),ze=new U({}),Le=new E({props:{name:"accelerate.data_loader.prepare_data_loader",anchor:"accelerate.data_loader.prepare_data_loader",parameters:[{name:"dataloader",val:": DataLoader"},{name:"device",val:": typing.Optional[torch.device] = None"},{name:"num_processes",val:": typing.Optional[int] = None"},{name:"process_index",val:": typing.Optional[int] = None"},{name:"split_batches",val:": bool = False"},{name:"put_on_device",val:": bool = False"},{name:"rng_types",val:": typing.Union[typing.List[typing.Union[str, accelerate.utils.RNGType]], NoneType] = None"},{name:"dispatch_batches",val:": typing.Optional[bool] = None"}],source:"https://github.com/huggingface/accelerate/blob/pr_293/src/accelerate/data_loader.py#L417",parametersDescription:[{anchor:"accelerate.data_loader.prepare_data_loader.dataloader",description:`<strong>dataloader</strong> (<code>torch.utils.data.dataloader.DataLoader</code>) &#x2014;
The data loader to split across several devices.`,name:"dataloader"},{anchor:"accelerate.data_loader.prepare_data_loader.device",description:`<strong>device</strong> (<code>torch.device</code>) &#x2014;
The target device for the returned <code>DataLoader</code>.`,name:"device"},{anchor:"accelerate.data_loader.prepare_data_loader.num_processes",description:`<strong>num_processes</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The number of processes running concurrently. Will default to the value given by
<a href="/docs/accelerate/pr_293/en/internal#accelerate.state.AcceleratorState">AcceleratorState</a>.`,name:"num_processes"},{anchor:"accelerate.data_loader.prepare_data_loader.process_index",description:`<strong>process_index</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The index of the current process. Will default to the value given by <a href="/docs/accelerate/pr_293/en/internal#accelerate.state.AcceleratorState">AcceleratorState</a>.`,name:"process_index"},{anchor:"accelerate.data_loader.prepare_data_loader.split_batches",description:`<strong>split_batches</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether the resulting <code>DataLoader</code> should split the batches of the original data loader across devices or
yield full batches (in which case it will yield batches starting at the <code>process_index</code>-th and advancing of
<code>num_processes</code> batches at each iteration).</p>
<p>Another way to see this is that the observed batch size will be the same as the initial <code>dataloader</code> if
this option is set to <code>True</code>, the batch size of the initial <code>dataloader</code> multiplied by <code>num_processes</code>
otherwise.</p>
<p>Setting this option to <code>True</code> requires that the batch size of the <code>dataloader</code> is a round multiple of
<code>batch_size</code>.`,name:"split_batches"},{anchor:"accelerate.data_loader.prepare_data_loader.put_on_device",description:`<strong>put_on_device</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to put the batches on <code>device</code> (only works if the batches are nested list, tuples or
dictionaries of tensors).`,name:"put_on_device"},{anchor:"accelerate.data_loader.prepare_data_loader.rng_types",description:`<strong>rng_types</strong> (list of <code>str</code> or <code>RNGType</code> &#x2014;
The list of random number generators to synchronize at the beginning of each iteration. Should be one or
several of:</p>
<ul>
<li><code>&quot;torch&quot;</code>: the base torch random number generator</li>
<li><code>&quot;cuda&quot;</code>: the CUDA random number generator (GPU only)</li>
<li><code>&quot;xla&quot;</code>: the XLA random number generator (TPU only)</li>
<li><code>&quot;generator&quot;</code>: the <code>torch.Generator</code> of the sampler (or batch sampler if there is no sampler in your
dataloader) or of the iterable dataset (if it exists) if the underlying dataset is of that type.</li>
</ul>`,name:"rng_types"},{anchor:"accelerate.data_loader.prepare_data_loader.dispatch_batches",description:`<strong>dispatch_batches</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
If set to <code>True</code>, the datalaoder prepared is only iterated through on the main process and then the batches
are split and broadcast to each process. Will default to <code>True</code> when the underlying dataset is an
<code>IterableDataset</code>, <code>False</code> otherwise.`,name:"dispatch_batches"}],returnDescription:`
<p>A new data loader that will yield the portion of the batches</p>
`,returnType:`
<p><code>torch.utils.data.dataloader.DataLoader</code></p>
`}}),fe=new Fs({props:{warning:!0,$$slots:{default:[dc]},$$scope:{ctx:ie}}}),Oe=new U({}),Ce=new E({props:{name:"class accelerate.data_loader.DataLoaderShard",anchor:"accelerate.data_loader.DataLoaderShard",parameters:[{name:"*args",val:""},{name:"**kwds",val:""}],source:"https://github.com/huggingface/accelerate/blob/pr_293/src/accelerate/data_loader.py#L270",parametersDescription:[{anchor:"accelerate.data_loader.DataLoaderShard.dataset",description:`<strong>dataset</strong> (<code>torch.utils.data.dataset.Dataset</code>) &#x2014;
The dataset to use to build this datalaoder.`,name:"dataset"},{anchor:"accelerate.data_loader.DataLoaderShard.device",description:`<strong>device</strong> (<code>torch.device</code>, <em>optional</em>) &#x2014;
If passed, the device to put all batches on.`,name:"device"},{anchor:"accelerate.data_loader.DataLoaderShard.rng_types",description:`<strong>rng_types</strong> (list of <code>str</code> or <code>RNGType</code> &#x2014;
The list of random number generators to synchronize at the beginning of each iteration. Should be one or
several of:</p>
<ul>
<li><code>&quot;torch&quot;</code>: the base torch random number generator</li>
<li><code>&quot;cuda&quot;</code>: the CUDA random number generator (GPU only)</li>
<li><code>&quot;xla&quot;</code>: the XLA random number generator (TPU only)</li>
<li><code>&quot;generator&quot;</code>: an optional <code>torch.Generator</code></li>
</ul>`,name:"rng_types"},{anchor:"accelerate.data_loader.DataLoaderShard.generator",description:`<strong>generator</strong> (<code>torch.Generator</code>, <em>optional</em>) &#x2014;
A random number generator to keep synchronized across processes.
kwargs &#x2014;
All other keyword arguments to pass to the regular <code>DataLoader</code> initialization.`,name:"generator"}]}}),Ue=new U({}),Be=new E({props:{name:"class accelerate.data_loader.BatchSamplerShard",anchor:"accelerate.data_loader.BatchSamplerShard",parameters:[{name:"*args",val:""},{name:"**kwds",val:""}],source:"https://github.com/huggingface/accelerate/blob/pr_293/src/accelerate/data_loader.py#L68",parametersDescription:[{anchor:"accelerate.data_loader.BatchSamplerShard.batch_sampler",description:`<strong>batch_sampler</strong> (<code>torch.utils.data.sampler.BatchSampler</code>) &#x2014;
The batch sampler to split in several shards.`,name:"batch_sampler"},{anchor:"accelerate.data_loader.BatchSamplerShard.num_processes",description:`<strong>num_processes</strong> (<code>int</code>, <em>optional</em>, defaults to 1) &#x2014;
The number of processes running concurrently.`,name:"num_processes"},{anchor:"accelerate.data_loader.BatchSamplerShard.process_index",description:`<strong>process_index</strong> (<code>int</code>, <em>optional</em>, defaults to 0) &#x2014;
The index of the current process.`,name:"process_index"},{anchor:"accelerate.data_loader.BatchSamplerShard.split_batches",description:`<strong>split_batches</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether the shards should be created by splitting a batch to give a piece of it on each process, or by
yielding different full batches on each process.</p>
<p>On two processes with a sampler of <code>[[0, 1, 2, 3], [4, 5, 6, 7]]</code>, this will result in:</p>
<ul>
<li>the sampler on process 0 to yield <code>[0, 1, 2, 3]</code> and the sampler on process 1 to yield <code>[4, 5, 6, 7]</code> if
this argument is set to <code>False</code>.</li>
<li>the sampler on process 0 to yield <code>[0, 1]</code> then <code>[4, 5]</code> and the sampler on process 1 to yield <code>[2, 3]</code>
then <code>[6, 7]</code> if this argument is set to <code>True</code>.</li>
</ul>`,name:"split_batches"}]}}),ge=new Fs({props:{warning:!0,$$slots:{default:[hc]},$$scope:{ctx:ie}}}),Ve=new U({}),qe=new E({props:{name:"class accelerate.data_loader.IterableDatasetShard",anchor:"accelerate.data_loader.IterableDatasetShard",parameters:[{name:"*args",val:""},{name:"**kwds",val:""}],source:"https://github.com/huggingface/accelerate/blob/pr_293/src/accelerate/data_loader.py#L189",parametersDescription:[{anchor:"accelerate.data_loader.IterableDatasetShard.dataset",description:`<strong>dataset</strong> (<code>torch.utils.data.dataset.IterableDataset</code>) &#x2014;
The batch sampler to split in several shards.`,name:"dataset"},{anchor:"accelerate.data_loader.IterableDatasetShard.batch_size",description:`<strong>batch_size</strong> (<code>int</code>, <em>optional</em>, defaults to 1) &#x2014;
The size of the batches per shard (if <code>split_batches=False</code>) or the size of the batches (if
<code>split_batches=True</code>).`,name:"batch_size"},{anchor:"accelerate.data_loader.IterableDatasetShard.drop_last",description:`<strong>drop_last</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to drop the last incomplete batch or complete the last batches by using the samples from the
beginning.`,name:"drop_last"},{anchor:"accelerate.data_loader.IterableDatasetShard.num_processes",description:`<strong>num_processes</strong> (<code>int</code>, <em>optional</em>, defaults to 1) &#x2014;
The number of processes running concurrently.`,name:"num_processes"},{anchor:"accelerate.data_loader.IterableDatasetShard.process_index",description:`<strong>process_index</strong> (<code>int</code>, <em>optional</em>, defaults to 0) &#x2014;
The index of the current process.`,name:"process_index"},{anchor:"accelerate.data_loader.IterableDatasetShard.split_batches",description:`<strong>split_batches</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether the shards should be created by splitting a batch to give a piece of it on each process, or by
yielding different full batches on each process.</p>
<p>On two processes with an iterable dataset yielding of <code>[0, 1, 2, 3, 4, 5, 6, 7]</code>, this will result in:</p>
<ul>
<li>the shard on process 0 to yield <code>[0, 1, 2, 3]</code> and the shard on process 1 to yield <code>[4, 5, 6, 7]</code> if this
argument is set to <code>False</code>.</li>
<li>the shard on process 0 to yield <code>[0, 1, 4, 5]</code> and the sampler on process 1 to yield <code>[2, 3, 6, 7]</code> if
this argument is set to <code>True</code>.</li>
</ul>`,name:"split_batches"}]}}),Fe=new U({}),Re=new U({}),We=new E({props:{name:"class accelerate.state.AcceleratorState",anchor:"accelerate.state.AcceleratorState",parameters:[{name:"mixed_precision",val:": str = None"},{name:"cpu",val:": bool = False"},{name:"deepspeed_plugin",val:" = None"},{name:"_from_accelerator",val:": bool = False"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/accelerate/blob/pr_293/src/accelerate/state.py#L128",parametersDescription:[{anchor:"accelerate.state.AcceleratorState.-",description:"<strong>-</strong> <strong>device</strong> (<code>torch.device</code>) &#x2014; The device to use. &#x2014;",name:"-"},{anchor:"accelerate.state.AcceleratorState.-",description:`<strong>-</strong> <strong>distributed_type</strong> (<code>~accelerate.state.DistributedType</code>) &#x2014; The type of distributed environment currently &#x2014;
in use.`,name:"-"},{anchor:"accelerate.state.AcceleratorState.-",description:"<strong>-</strong> <strong>num_processes</strong> (<code>int</code>) &#x2014; The number of processes currently launched in parallel. &#x2014;",name:"-"},{anchor:"accelerate.state.AcceleratorState.-",description:"<strong>-</strong> <strong>process_index</strong> (<code>int</code>) &#x2014; The index of the current process. &#x2014;",name:"-"},{anchor:"accelerate.state.AcceleratorState.-",description:"<strong>-</strong> <strong>local_process_index</strong> (<code>int</code>) &#x2014; The index of the current process on the current server. &#x2014;",name:"-"},{anchor:"accelerate.state.AcceleratorState.-",description:`<strong>-</strong> <strong>mixed_precision</strong> (<code>str</code>) &#x2014; Whether or not the current script will use mixed precision. If you are using &#x2014;
mixed precision, define if you want to use FP16 or BF16 (bfloat16) as the floating point.`,name:"-"}]}}),Me=new U({}),je=new E({props:{name:"class accelerate.DistributedType",anchor:"accelerate.DistributedType",parameters:[{name:"value",val:""},{name:"names",val:" = None"},{name:"module",val:" = None"},{name:"qualname",val:" = None"},{name:"type",val:" = None"},{name:"start",val:" = 1"}],source:"https://github.com/huggingface/accelerate/blob/pr_293/src/accelerate/state.py#L74"}}),Xe=new U({}),Je=new E({props:{name:"class accelerate.tracking.GeneralTracker",anchor:"accelerate.tracking.GeneralTracker",parameters:[],source:"https://github.com/huggingface/accelerate/blob/pr_293/src/accelerate/tracking.py#L51"}}),Ke=new E({props:{name:"finish",anchor:"accelerate.tracking.GeneralTracker.finish",parameters:[],source:"https://github.com/huggingface/accelerate/blob/pr_293/src/accelerate/tracking.py#L83"}}),Qe=new ic({props:{code:"super().finish()",highlighted:'<span class="hljs-built_in">super</span>().finish()'}}),Ye=new E({props:{name:"log",anchor:"accelerate.tracking.GeneralTracker.log",parameters:[{name:"values",val:": dict"},{name:"step",val:": typing.Optional[int]"}],source:"https://github.com/huggingface/accelerate/blob/pr_293/src/accelerate/tracking.py#L69",parametersDescription:[{anchor:"accelerate.tracking.GeneralTracker.log.values",description:`<strong>values</strong> (Dictionary <code>str</code> to <code>str</code>, <code>float</code>, or <code>int</code>) &#x2014;
Values to be logged as key-value pairs. The values need to have type <code>str</code>, <code>float</code>, or <code>int</code>.`,name:"values"},{anchor:"accelerate.tracking.GeneralTracker.log.step",description:`<strong>step</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The run step. If included, the log will be affiliated with this step.`,name:"step"}]}}),Ze=new E({props:{name:"store_init_configuration",anchor:"accelerate.tracking.GeneralTracker.store_init_configuration",parameters:[{name:"values",val:": dict"}],source:"https://github.com/huggingface/accelerate/blob/pr_293/src/accelerate/tracking.py#L56",parametersDescription:[{anchor:"accelerate.tracking.GeneralTracker.store_init_configuration.values",description:`<strong>values</strong> (Dictionary <code>str</code> to <code>bool</code>, <code>str</code>, <code>float</code> or <code>int</code>) &#x2014;
Values to be stored as initial hyperparameters as key-value pairs. The values need to have type <code>bool</code>,
<code>str</code>, <code>float</code>, <code>int</code>, or <code>None</code>.`,name:"values"}]}}),tt=new U({}),at=new E({props:{name:"accelerate.utils.extract_model_from_parallel",anchor:"accelerate.utils.extract_model_from_parallel",parameters:[{name:"model",val:""}],source:"https://github.com/huggingface/accelerate/blob/pr_293/src/accelerate/utils.py#L350",parametersDescription:[{anchor:"accelerate.utils.extract_model_from_parallel.model",description:"<strong>model</strong> (<code>torch.nn.Module</code>) &#x2014; The model to extract.",name:"model"}],returnDescription:`
<p>The extracted model.</p>
`,returnType:`
<p><code>torch.nn.Module</code></p>
`}}),rt=new E({props:{name:"accelerate.utils.gather",anchor:"accelerate.utils.gather",parameters:[{name:"tensor",val:""}],source:"https://github.com/huggingface/accelerate/blob/pr_293/src/accelerate/utils.py#L395",parametersDescription:[{anchor:"accelerate.utils.gather.tensor",description:`<strong>tensor</strong> (nested list/tuple/dictionary of <code>torch.Tensor</code>) &#x2014;
The data to gather.`,name:"tensor"}],returnDescription:`
<p>The same data structure as <code>tensor</code> with all tensors sent to the proper device.</p>
`}}),ot=new E({props:{name:"accelerate.utils.send_to_device",anchor:"accelerate.utils.send_to_device",parameters:[{name:"tensor",val:""},{name:"device",val:""}],source:"https://github.com/huggingface/accelerate/blob/pr_293/src/accelerate/utils.py#L245",parametersDescription:[{anchor:"accelerate.utils.send_to_device.tensor",description:`<strong>tensor</strong> (nested list/tuple/dictionary of <code>torch.Tensor</code>) &#x2014;
The data to send to a given device.`,name:"tensor"},{anchor:"accelerate.utils.send_to_device.device",description:`<strong>device</strong> (<code>torch.device</code>) &#x2014;
The device to send the data to.`,name:"device"}],returnDescription:`
<p>The same data structure as <code>tensor</code> with all tensors sent to the proper device.</p>
`}}),st=new E({props:{name:"accelerate.utils.set_seed",anchor:"accelerate.utils.set_seed",parameters:[{name:"seed",val:": int"},{name:"device_specific",val:": bool = False"}],source:"https://github.com/huggingface/accelerate/blob/pr_293/src/accelerate/utils.py#L115",parametersDescription:[{anchor:"accelerate.utils.set_seed.seed",description:"<strong>seed</strong> (<code>int</code>) &#x2014; The seed to set.",name:"seed"},{anchor:"accelerate.utils.set_seed.device_specific",description:`<strong>device_specific</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to differ the seed on each device slightly with <code>self.process_index</code>.`,name:"device_specific"}]}}),ct=new E({props:{name:"accelerate.utils.synchronize_rng_state",anchor:"accelerate.utils.synchronize_rng_state",parameters:[{name:"rng_type",val:": typing.Optional[accelerate.utils.RNGType] = None"},{name:"generator",val:": typing.Optional[torch._C.Generator] = None"}],source:"https://github.com/huggingface/accelerate/blob/pr_293/src/accelerate/utils.py#L135"}}),it=new E({props:{name:"accelerate.synchronize_rng_states",anchor:"accelerate.synchronize_rng_states",parameters:[{name:"rng_types",val:": typing.List[typing.Union[str, accelerate.utils.RNGType]]"},{name:"generator",val:": typing.Optional[torch._C.Generator] = None"}],source:"https://github.com/huggingface/accelerate/blob/pr_293/src/accelerate/utils.py#L170"}}),dt=new E({props:{name:"accelerate.utils.wait_for_everyone",anchor:"accelerate.utils.wait_for_everyone",parameters:[],source:"https://github.com/huggingface/accelerate/blob/pr_293/src/accelerate/utils.py#L619"}}),Se=new Fs({props:{warning:!0,$$slots:{default:[pc]},$$scope:{ctx:ie}}}),{c(){f=r("meta"),S=l(),$=r("h1"),y=r("a"),k=r("span"),u(w.$$.fragment),D=l(),L=r("span"),wr=n("Internals"),Sa=l(),F=r("h2"),de=r("a"),Et=r("span"),u(Ae.$$.fragment),Er=l(),Dt=r("span"),Dr=n("Optimizer"),Ta=l(),R=r("div"),u(Ie.$$.fragment),Sr=l(),St=r("p"),Tr=n("Internal wrapper around a torch optimizer."),xa=l(),W=r("h2"),he=r("a"),Tt=r("span"),u(ze.$$.fragment),xr=l(),xt=r("span"),kr=n("DataLoader"),ka=l(),pe=r("p"),Pr=n("The main work on your PyTorch "),kt=r("code"),Ar=n("DataLoader"),Ir=n(" is done by the following function:"),Pa=l(),P=r("div"),u(Le.$$.fragment),zr=l(),Ne=r("p"),Lr=n("Wraps a PyTorch "),Pt=r("code"),Nr=n("DataLoader"),Or=n(" to generate batches for one of the processes only."),Cr=l(),H=r("p"),Gr=n("Depending on the value of the "),At=r("code"),Ur=n("drop_last"),Br=n(" attribute of the "),It=r("code"),Vr=n("dataloader"),qr=n(` passed, it will either stop the iteration
at the first batch that would be too small / not present on all processes or loop with indices from the beginning.`),Fr=l(),u(fe.$$.fragment),Aa=l(),M=r("h3"),ue=r("a"),zt=r("span"),u(Oe.$$.fragment),Rr=l(),Lt=r("span"),Wr=n("BatchSamplerShard"),Ia=l(),j=r("div"),u(Ce.$$.fragment),Hr=l(),Ge=r("p"),Mr=n("Subclass of a PyTorch "),Nt=r("code"),jr=n("DataLoader"),Xr=n(" that will deal with device placement and current distributed setup."),za=l(),X=r("h3"),me=r("a"),Ot=r("span"),u(Ue.$$.fragment),Jr=l(),Ct=r("span"),Kr=n("BatchSamplerShard"),La=l(),N=r("div"),u(Be.$$.fragment),Qr=l(),O=r("p"),Yr=n("Wraps a PyTorch "),Gt=r("code"),Zr=n("BatchSampler"),eo=n(` to generate batches for one of the processes only. Instances of this class will
always yield a number of batches that is a round multiple of `),Ut=r("code"),to=n("num_processes"),ao=n(` and that all have the same size.
Depending on the value of the `),Bt=r("code"),ro=n("drop_last"),oo=n(` attribute of the batch sampler passed, it will either stop the iteration
at the first batch that would be too small / not present on all processes or loop with indices from the beginning.`),so=l(),u(ge.$$.fragment),Na=l(),J=r("h3"),ve=r("a"),Vt=r("span"),u(Ve.$$.fragment),no=l(),qt=r("span"),co=n("IterableDatasetShard"),Oa=l(),K=r("div"),u(qe.$$.fragment),lo=l(),T=r("p"),io=n("Wraps a PyTorch "),Ft=r("code"),ho=n("IterableDataset"),po=n(` to generate samples for one of the processes only. Instances of this class will
always yield a number of samples that is a round multiple of the actual batch size (depending of the value of
`),Rt=r("code"),fo=n("split_batches"),uo=n(", this is either "),Wt=r("code"),mo=n("batch_size"),go=n(" or "),Ht=r("code"),vo=n("batch_size x num_processes"),_o=n(`). Depending on the value of the
`),Mt=r("code"),bo=n("drop_last"),$o=n(` attribute of the batch sampler passed, it will either stop the iteration at the first batch that would
be too small or loop with indices from the beginning.`),Ca=l(),Q=r("h2"),_e=r("a"),jt=r("span"),u(Fe.$$.fragment),yo=l(),Xt=r("span"),wo=n("Distributed Config"),Ga=l(),Y=r("h3"),be=r("a"),Jt=r("span"),u(Re.$$.fragment),Eo=l(),Kt=r("span"),Do=n("AcceleratorState"),Ua=l(),Z=r("div"),u(We.$$.fragment),So=l(),ee=r("p"),To=n("This is a variation of a "),He=r("a"),xo=n("singleton class"),ko=n(` in the sense that all
instance of `),Qt=r("code"),Po=n("AcceleratorState"),Ao=n(" share the same state, which is initialized on the first instantiation."),Ba=l(),te=r("h3"),$e=r("a"),Yt=r("span"),u(Me.$$.fragment),Io=l(),Zt=r("span"),zo=n("DistributedType"),Va=l(),A=r("div"),u(je.$$.fragment),Lo=l(),ea=r("p"),No=n("Represents a type of distributed environment."),Oo=l(),ta=r("p"),Co=n("Values:"),Go=l(),I=r("ul"),pt=r("li"),aa=r("strong"),Uo=n("NO"),Bo=n(" \u2014 Not a distributed environment, just a single process."),Vo=l(),ft=r("li"),ra=r("strong"),qo=n("MULTI_CPU"),Fo=n(" \u2014 Distributed on multiple CPU nodes."),Ro=l(),ut=r("li"),oa=r("strong"),Wo=n("MULTI_GPU"),Ho=n(" \u2014 Distributed on multiple GPUs."),Mo=l(),mt=r("li"),sa=r("strong"),jo=n("DEEPSPEED"),Xo=n(" \u2014 Using DeepSpeed."),Jo=l(),gt=r("li"),na=r("strong"),Ko=n("TPU"),Qo=n(" \u2014 Distributed on TPUs."),qa=l(),ae=r("h2"),ye=r("a"),ca=r("span"),u(Xe.$$.fragment),Yo=l(),la=r("span"),Zo=n("Tracking"),Fa=l(),x=r("div"),u(Je.$$.fragment),es=l(),ia=r("p"),ts=n("A base Tracker class to be used for all logging integration implementations."),as=l(),B=r("div"),u(Ke.$$.fragment),rs=l(),da=r("p"),os=n("Should run any finalizing functions within the tracking API. If the API should not have one, just return:"),ss=l(),u(Qe.$$.fragment),ns=l(),we=r("div"),u(Ye.$$.fragment),cs=l(),re=r("p"),ls=n("Logs "),ha=r("code"),is=n("values"),ds=n(" to the current run. Base "),pa=r("code"),hs=n("log"),ps=n(" implementations of a tracking API should go in here, along with\nspecial behavior for the `step parameter."),fs=l(),Ee=r("div"),u(Ze.$$.fragment),us=l(),et=r("p"),ms=n("Logs "),fa=r("code"),gs=n("values"),vs=n(` as hyperparameters for the run. Implementations should use the experiment configuration
functionality of a tracking API.`),Ra=l(),oe=r("h2"),De=r("a"),ua=r("span"),u(tt.$$.fragment),_s=l(),ma=r("span"),bs=n("Utilities"),Wa=l(),se=r("div"),u(at.$$.fragment),$s=l(),ga=r("p"),ys=n("Extract a model from its distributed containers."),Ha=l(),ne=r("div"),u(rt.$$.fragment),ws=l(),va=r("p"),Es=n("Recursively gather tensor in a nested list/tuple/dictionary of tensors from all devices."),Ma=l(),ce=r("div"),u(ot.$$.fragment),Ds=l(),_a=r("p"),Ss=n("Recursively sends the elements in a nested list/tuple/dictionary of tensors to a given device."),ja=l(),le=r("div"),u(st.$$.fragment),Ts=l(),C=r("p"),xs=n("Helper function for reproducible behavior to set the seed in "),ba=r("code"),ks=n("random"),Ps=n(", "),$a=r("code"),As=n("numpy"),Is=n(", "),ya=r("code"),zs=n("torch"),Ls=n("."),Xa=l(),nt=r("div"),u(ct.$$.fragment),Ja=l(),lt=r("div"),u(it.$$.fragment),Ka=l(),G=r("div"),u(dt.$$.fragment),Ns=l(),wa=r("p"),Os=n("Introduces a blocking point in the script, making sure all processes have reached this point before continuing."),Cs=l(),u(Se.$$.fragment),this.h()},l(e){const h=cc('[data-svelte="svelte-1phssyn"]',document.head);f=o(h,"META",{name:!0,content:!0}),h.forEach(a),S=i(e),$=o(e,"H1",{class:!0});var ht=s($);y=o(ht,"A",{id:!0,class:!0,href:!0});var Ea=s(y);k=o(Ea,"SPAN",{});var Da=s(k);m(w.$$.fragment,Da),Da.forEach(a),Ea.forEach(a),D=i(ht),L=o(ht,"SPAN",{});var Rs=s(L);wr=c(Rs,"Internals"),Rs.forEach(a),ht.forEach(a),Sa=i(e),F=o(e,"H2",{class:!0});var Ya=s(F);de=o(Ya,"A",{id:!0,class:!0,href:!0});var Ws=s(de);Et=o(Ws,"SPAN",{});var Hs=s(Et);m(Ae.$$.fragment,Hs),Hs.forEach(a),Ws.forEach(a),Er=i(Ya),Dt=o(Ya,"SPAN",{});var Ms=s(Dt);Dr=c(Ms,"Optimizer"),Ms.forEach(a),Ya.forEach(a),Ta=i(e),R=o(e,"DIV",{class:!0});var Za=s(R);m(Ie.$$.fragment,Za),Sr=i(Za),St=o(Za,"P",{});var js=s(St);Tr=c(js,"Internal wrapper around a torch optimizer."),js.forEach(a),Za.forEach(a),xa=i(e),W=o(e,"H2",{class:!0});var er=s(W);he=o(er,"A",{id:!0,class:!0,href:!0});var Xs=s(he);Tt=o(Xs,"SPAN",{});var Js=s(Tt);m(ze.$$.fragment,Js),Js.forEach(a),Xs.forEach(a),xr=i(er),xt=o(er,"SPAN",{});var Ks=s(xt);kr=c(Ks,"DataLoader"),Ks.forEach(a),er.forEach(a),ka=i(e),pe=o(e,"P",{});var tr=s(pe);Pr=c(tr,"The main work on your PyTorch "),kt=o(tr,"CODE",{});var Qs=s(kt);Ar=c(Qs,"DataLoader"),Qs.forEach(a),Ir=c(tr," is done by the following function:"),tr.forEach(a),Pa=i(e),P=o(e,"DIV",{class:!0});var Te=s(P);m(Le.$$.fragment,Te),zr=i(Te),Ne=o(Te,"P",{});var ar=s(Ne);Lr=c(ar,"Wraps a PyTorch "),Pt=o(ar,"CODE",{});var Ys=s(Pt);Nr=c(Ys,"DataLoader"),Ys.forEach(a),Or=c(ar," to generate batches for one of the processes only."),ar.forEach(a),Cr=i(Te),H=o(Te,"P",{});var vt=s(H);Gr=c(vt,"Depending on the value of the "),At=o(vt,"CODE",{});var Zs=s(At);Ur=c(Zs,"drop_last"),Zs.forEach(a),Br=c(vt," attribute of the "),It=o(vt,"CODE",{});var en=s(It);Vr=c(en,"dataloader"),en.forEach(a),qr=c(vt,` passed, it will either stop the iteration
at the first batch that would be too small / not present on all processes or loop with indices from the beginning.`),vt.forEach(a),Fr=i(Te),m(fe.$$.fragment,Te),Te.forEach(a),Aa=i(e),M=o(e,"H3",{class:!0});var rr=s(M);ue=o(rr,"A",{id:!0,class:!0,href:!0});var tn=s(ue);zt=o(tn,"SPAN",{});var an=s(zt);m(Oe.$$.fragment,an),an.forEach(a),tn.forEach(a),Rr=i(rr),Lt=o(rr,"SPAN",{});var rn=s(Lt);Wr=c(rn,"BatchSamplerShard"),rn.forEach(a),rr.forEach(a),Ia=i(e),j=o(e,"DIV",{class:!0});var or=s(j);m(Ce.$$.fragment,or),Hr=i(or),Ge=o(or,"P",{});var sr=s(Ge);Mr=c(sr,"Subclass of a PyTorch "),Nt=o(sr,"CODE",{});var on=s(Nt);jr=c(on,"DataLoader"),on.forEach(a),Xr=c(sr," that will deal with device placement and current distributed setup."),sr.forEach(a),or.forEach(a),za=i(e),X=o(e,"H3",{class:!0});var nr=s(X);me=o(nr,"A",{id:!0,class:!0,href:!0});var sn=s(me);Ot=o(sn,"SPAN",{});var nn=s(Ot);m(Ue.$$.fragment,nn),nn.forEach(a),sn.forEach(a),Jr=i(nr),Ct=o(nr,"SPAN",{});var cn=s(Ct);Kr=c(cn,"BatchSamplerShard"),cn.forEach(a),nr.forEach(a),La=i(e),N=o(e,"DIV",{class:!0});var _t=s(N);m(Be.$$.fragment,_t),Qr=i(_t),O=o(_t,"P",{});var xe=s(O);Yr=c(xe,"Wraps a PyTorch "),Gt=o(xe,"CODE",{});var ln=s(Gt);Zr=c(ln,"BatchSampler"),ln.forEach(a),eo=c(xe,` to generate batches for one of the processes only. Instances of this class will
always yield a number of batches that is a round multiple of `),Ut=o(xe,"CODE",{});var dn=s(Ut);to=c(dn,"num_processes"),dn.forEach(a),ao=c(xe,` and that all have the same size.
Depending on the value of the `),Bt=o(xe,"CODE",{});var hn=s(Bt);ro=c(hn,"drop_last"),hn.forEach(a),oo=c(xe,` attribute of the batch sampler passed, it will either stop the iteration
at the first batch that would be too small / not present on all processes or loop with indices from the beginning.`),xe.forEach(a),so=i(_t),m(ge.$$.fragment,_t),_t.forEach(a),Na=i(e),J=o(e,"H3",{class:!0});var cr=s(J);ve=o(cr,"A",{id:!0,class:!0,href:!0});var pn=s(ve);Vt=o(pn,"SPAN",{});var fn=s(Vt);m(Ve.$$.fragment,fn),fn.forEach(a),pn.forEach(a),no=i(cr),qt=o(cr,"SPAN",{});var un=s(qt);co=c(un,"IterableDatasetShard"),un.forEach(a),cr.forEach(a),Oa=i(e),K=o(e,"DIV",{class:!0});var lr=s(K);m(qe.$$.fragment,lr),lo=i(lr),T=o(lr,"P",{});var z=s(T);io=c(z,"Wraps a PyTorch "),Ft=o(z,"CODE",{});var mn=s(Ft);ho=c(mn,"IterableDataset"),mn.forEach(a),po=c(z,` to generate samples for one of the processes only. Instances of this class will
always yield a number of samples that is a round multiple of the actual batch size (depending of the value of
`),Rt=o(z,"CODE",{});var gn=s(Rt);fo=c(gn,"split_batches"),gn.forEach(a),uo=c(z,", this is either "),Wt=o(z,"CODE",{});var vn=s(Wt);mo=c(vn,"batch_size"),vn.forEach(a),go=c(z," or "),Ht=o(z,"CODE",{});var _n=s(Ht);vo=c(_n,"batch_size x num_processes"),_n.forEach(a),_o=c(z,`). Depending on the value of the
`),Mt=o(z,"CODE",{});var bn=s(Mt);bo=c(bn,"drop_last"),bn.forEach(a),$o=c(z,` attribute of the batch sampler passed, it will either stop the iteration at the first batch that would
be too small or loop with indices from the beginning.`),z.forEach(a),lr.forEach(a),Ca=i(e),Q=o(e,"H2",{class:!0});var ir=s(Q);_e=o(ir,"A",{id:!0,class:!0,href:!0});var $n=s(_e);jt=o($n,"SPAN",{});var yn=s(jt);m(Fe.$$.fragment,yn),yn.forEach(a),$n.forEach(a),yo=i(ir),Xt=o(ir,"SPAN",{});var wn=s(Xt);wo=c(wn,"Distributed Config"),wn.forEach(a),ir.forEach(a),Ga=i(e),Y=o(e,"H3",{class:!0});var dr=s(Y);be=o(dr,"A",{id:!0,class:!0,href:!0});var En=s(be);Jt=o(En,"SPAN",{});var Dn=s(Jt);m(Re.$$.fragment,Dn),Dn.forEach(a),En.forEach(a),Eo=i(dr),Kt=o(dr,"SPAN",{});var Sn=s(Kt);Do=c(Sn,"AcceleratorState"),Sn.forEach(a),dr.forEach(a),Ua=i(e),Z=o(e,"DIV",{class:!0});var hr=s(Z);m(We.$$.fragment,hr),So=i(hr),ee=o(hr,"P",{});var bt=s(ee);To=c(bt,"This is a variation of a "),He=o(bt,"A",{href:!0,rel:!0});var Tn=s(He);xo=c(Tn,"singleton class"),Tn.forEach(a),ko=c(bt,` in the sense that all
instance of `),Qt=o(bt,"CODE",{});var xn=s(Qt);Po=c(xn,"AcceleratorState"),xn.forEach(a),Ao=c(bt," share the same state, which is initialized on the first instantiation."),bt.forEach(a),hr.forEach(a),Ba=i(e),te=o(e,"H3",{class:!0});var pr=s(te);$e=o(pr,"A",{id:!0,class:!0,href:!0});var kn=s($e);Yt=o(kn,"SPAN",{});var Pn=s(Yt);m(Me.$$.fragment,Pn),Pn.forEach(a),kn.forEach(a),Io=i(pr),Zt=o(pr,"SPAN",{});var An=s(Zt);zo=c(An,"DistributedType"),An.forEach(a),pr.forEach(a),Va=i(e),A=o(e,"DIV",{class:!0});var ke=s(A);m(je.$$.fragment,ke),Lo=i(ke),ea=o(ke,"P",{});var In=s(ea);No=c(In,"Represents a type of distributed environment."),In.forEach(a),Oo=i(ke),ta=o(ke,"P",{});var zn=s(ta);Co=c(zn,"Values:"),zn.forEach(a),Go=i(ke),I=o(ke,"UL",{});var V=s(I);pt=o(V,"LI",{});var Gs=s(pt);aa=o(Gs,"STRONG",{});var Ln=s(aa);Uo=c(Ln,"NO"),Ln.forEach(a),Bo=c(Gs," \u2014 Not a distributed environment, just a single process."),Gs.forEach(a),Vo=i(V),ft=o(V,"LI",{});var Us=s(ft);ra=o(Us,"STRONG",{});var Nn=s(ra);qo=c(Nn,"MULTI_CPU"),Nn.forEach(a),Fo=c(Us," \u2014 Distributed on multiple CPU nodes."),Us.forEach(a),Ro=i(V),ut=o(V,"LI",{});var Bs=s(ut);oa=o(Bs,"STRONG",{});var On=s(oa);Wo=c(On,"MULTI_GPU"),On.forEach(a),Ho=c(Bs," \u2014 Distributed on multiple GPUs."),Bs.forEach(a),Mo=i(V),mt=o(V,"LI",{});var Vs=s(mt);sa=o(Vs,"STRONG",{});var Cn=s(sa);jo=c(Cn,"DEEPSPEED"),Cn.forEach(a),Xo=c(Vs," \u2014 Using DeepSpeed."),Vs.forEach(a),Jo=i(V),gt=o(V,"LI",{});var qs=s(gt);na=o(qs,"STRONG",{});var Gn=s(na);Ko=c(Gn,"TPU"),Gn.forEach(a),Qo=c(qs," \u2014 Distributed on TPUs."),qs.forEach(a),V.forEach(a),ke.forEach(a),qa=i(e),ae=o(e,"H2",{class:!0});var fr=s(ae);ye=o(fr,"A",{id:!0,class:!0,href:!0});var Un=s(ye);ca=o(Un,"SPAN",{});var Bn=s(ca);m(Xe.$$.fragment,Bn),Bn.forEach(a),Un.forEach(a),Yo=i(fr),la=o(fr,"SPAN",{});var Vn=s(la);Zo=c(Vn,"Tracking"),Vn.forEach(a),fr.forEach(a),Fa=i(e),x=o(e,"DIV",{class:!0});var q=s(x);m(Je.$$.fragment,q),es=i(q),ia=o(q,"P",{});var qn=s(ia);ts=c(qn,"A base Tracker class to be used for all logging integration implementations."),qn.forEach(a),as=i(q),B=o(q,"DIV",{class:!0});var $t=s(B);m(Ke.$$.fragment,$t),rs=i($t),da=o($t,"P",{});var Fn=s(da);os=c(Fn,"Should run any finalizing functions within the tracking API. If the API should not have one, just return:"),Fn.forEach(a),ss=i($t),m(Qe.$$.fragment,$t),$t.forEach(a),ns=i(q),we=o(q,"DIV",{class:!0});var ur=s(we);m(Ye.$$.fragment,ur),cs=i(ur),re=o(ur,"P",{});var yt=s(re);ls=c(yt,"Logs "),ha=o(yt,"CODE",{});var Rn=s(ha);is=c(Rn,"values"),Rn.forEach(a),ds=c(yt," to the current run. Base "),pa=o(yt,"CODE",{});var Wn=s(pa);hs=c(Wn,"log"),Wn.forEach(a),ps=c(yt," implementations of a tracking API should go in here, along with\nspecial behavior for the `step parameter."),yt.forEach(a),ur.forEach(a),fs=i(q),Ee=o(q,"DIV",{class:!0});var mr=s(Ee);m(Ze.$$.fragment,mr),us=i(mr),et=o(mr,"P",{});var gr=s(et);ms=c(gr,"Logs "),fa=o(gr,"CODE",{});var Hn=s(fa);gs=c(Hn,"values"),Hn.forEach(a),vs=c(gr,` as hyperparameters for the run. Implementations should use the experiment configuration
functionality of a tracking API.`),gr.forEach(a),mr.forEach(a),q.forEach(a),Ra=i(e),oe=o(e,"H2",{class:!0});var vr=s(oe);De=o(vr,"A",{id:!0,class:!0,href:!0});var Mn=s(De);ua=o(Mn,"SPAN",{});var jn=s(ua);m(tt.$$.fragment,jn),jn.forEach(a),Mn.forEach(a),_s=i(vr),ma=o(vr,"SPAN",{});var Xn=s(ma);bs=c(Xn,"Utilities"),Xn.forEach(a),vr.forEach(a),Wa=i(e),se=o(e,"DIV",{class:!0});var _r=s(se);m(at.$$.fragment,_r),$s=i(_r),ga=o(_r,"P",{});var Jn=s(ga);ys=c(Jn,"Extract a model from its distributed containers."),Jn.forEach(a),_r.forEach(a),Ha=i(e),ne=o(e,"DIV",{class:!0});var br=s(ne);m(rt.$$.fragment,br),ws=i(br),va=o(br,"P",{});var Kn=s(va);Es=c(Kn,"Recursively gather tensor in a nested list/tuple/dictionary of tensors from all devices."),Kn.forEach(a),br.forEach(a),Ma=i(e),ce=o(e,"DIV",{class:!0});var $r=s(ce);m(ot.$$.fragment,$r),Ds=i($r),_a=o($r,"P",{});var Qn=s(_a);Ss=c(Qn,"Recursively sends the elements in a nested list/tuple/dictionary of tensors to a given device."),Qn.forEach(a),$r.forEach(a),ja=i(e),le=o(e,"DIV",{class:!0});var yr=s(le);m(st.$$.fragment,yr),Ts=i(yr),C=o(yr,"P",{});var Pe=s(C);xs=c(Pe,"Helper function for reproducible behavior to set the seed in "),ba=o(Pe,"CODE",{});var Yn=s(ba);ks=c(Yn,"random"),Yn.forEach(a),Ps=c(Pe,", "),$a=o(Pe,"CODE",{});var Zn=s($a);As=c(Zn,"numpy"),Zn.forEach(a),Is=c(Pe,", "),ya=o(Pe,"CODE",{});var ec=s(ya);zs=c(ec,"torch"),ec.forEach(a),Ls=c(Pe,"."),Pe.forEach(a),yr.forEach(a),Xa=i(e),nt=o(e,"DIV",{class:!0});var tc=s(nt);m(ct.$$.fragment,tc),tc.forEach(a),Ja=i(e),lt=o(e,"DIV",{class:!0});var ac=s(lt);m(it.$$.fragment,ac),ac.forEach(a),Ka=i(e),G=o(e,"DIV",{class:!0});var wt=s(G);m(dt.$$.fragment,wt),Ns=i(wt),wa=o(wt,"P",{});var rc=s(wa);Os=c(rc,"Introduces a blocking point in the script, making sure all processes have reached this point before continuing."),rc.forEach(a),Cs=i(wt),m(Se.$$.fragment,wt),wt.forEach(a),this.h()},h(){d(f,"name","hf:doc:metadata"),d(f,"content",JSON.stringify(uc)),d(y,"id","internals"),d(y,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(y,"href","#internals"),d($,"class","relative group"),d(de,"id","accelerate.optimizer.AcceleratedOptimizer"),d(de,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(de,"href","#accelerate.optimizer.AcceleratedOptimizer"),d(F,"class","relative group"),d(R,"class","docstring"),d(he,"id","accelerate.data_loader.prepare_data_loader"),d(he,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(he,"href","#accelerate.data_loader.prepare_data_loader"),d(W,"class","relative group"),d(P,"class","docstring"),d(ue,"id","accelerate.data_loader.DataLoaderShard"),d(ue,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(ue,"href","#accelerate.data_loader.DataLoaderShard"),d(M,"class","relative group"),d(j,"class","docstring"),d(me,"id","accelerate.data_loader.BatchSamplerShard"),d(me,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(me,"href","#accelerate.data_loader.BatchSamplerShard"),d(X,"class","relative group"),d(N,"class","docstring"),d(ve,"id","accelerate.data_loader.IterableDatasetShard"),d(ve,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(ve,"href","#accelerate.data_loader.IterableDatasetShard"),d(J,"class","relative group"),d(K,"class","docstring"),d(_e,"id","distributed-config"),d(_e,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(_e,"href","#distributed-config"),d(Q,"class","relative group"),d(be,"id","accelerate.state.AcceleratorState"),d(be,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(be,"href","#accelerate.state.AcceleratorState"),d(Y,"class","relative group"),d(He,"href","https://en.wikipedia.org/wiki/Singleton_pattern"),d(He,"rel","nofollow"),d(Z,"class","docstring"),d($e,"id","accelerate.DistributedType"),d($e,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d($e,"href","#accelerate.DistributedType"),d(te,"class","relative group"),d(A,"class","docstring"),d(ye,"id","accelerate.tracking.GeneralTracker"),d(ye,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(ye,"href","#accelerate.tracking.GeneralTracker"),d(ae,"class","relative group"),d(B,"class","docstring"),d(we,"class","docstring"),d(Ee,"class","docstring"),d(x,"class","docstring"),d(De,"id","accelerate.utils.extract_model_from_parallel"),d(De,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(De,"href","#accelerate.utils.extract_model_from_parallel"),d(oe,"class","relative group"),d(se,"class","docstring"),d(ne,"class","docstring"),d(ce,"class","docstring"),d(le,"class","docstring"),d(nt,"class","docstring"),d(lt,"class","docstring"),d(G,"class","docstring")},m(e,h){t(document.head,f),p(e,S,h),p(e,$,h),t($,y),t(y,k),g(w,k,null),t($,D),t($,L),t(L,wr),p(e,Sa,h),p(e,F,h),t(F,de),t(de,Et),g(Ae,Et,null),t(F,Er),t(F,Dt),t(Dt,Dr),p(e,Ta,h),p(e,R,h),g(Ie,R,null),t(R,Sr),t(R,St),t(St,Tr),p(e,xa,h),p(e,W,h),t(W,he),t(he,Tt),g(ze,Tt,null),t(W,xr),t(W,xt),t(xt,kr),p(e,ka,h),p(e,pe,h),t(pe,Pr),t(pe,kt),t(kt,Ar),t(pe,Ir),p(e,Pa,h),p(e,P,h),g(Le,P,null),t(P,zr),t(P,Ne),t(Ne,Lr),t(Ne,Pt),t(Pt,Nr),t(Ne,Or),t(P,Cr),t(P,H),t(H,Gr),t(H,At),t(At,Ur),t(H,Br),t(H,It),t(It,Vr),t(H,qr),t(P,Fr),g(fe,P,null),p(e,Aa,h),p(e,M,h),t(M,ue),t(ue,zt),g(Oe,zt,null),t(M,Rr),t(M,Lt),t(Lt,Wr),p(e,Ia,h),p(e,j,h),g(Ce,j,null),t(j,Hr),t(j,Ge),t(Ge,Mr),t(Ge,Nt),t(Nt,jr),t(Ge,Xr),p(e,za,h),p(e,X,h),t(X,me),t(me,Ot),g(Ue,Ot,null),t(X,Jr),t(X,Ct),t(Ct,Kr),p(e,La,h),p(e,N,h),g(Be,N,null),t(N,Qr),t(N,O),t(O,Yr),t(O,Gt),t(Gt,Zr),t(O,eo),t(O,Ut),t(Ut,to),t(O,ao),t(O,Bt),t(Bt,ro),t(O,oo),t(N,so),g(ge,N,null),p(e,Na,h),p(e,J,h),t(J,ve),t(ve,Vt),g(Ve,Vt,null),t(J,no),t(J,qt),t(qt,co),p(e,Oa,h),p(e,K,h),g(qe,K,null),t(K,lo),t(K,T),t(T,io),t(T,Ft),t(Ft,ho),t(T,po),t(T,Rt),t(Rt,fo),t(T,uo),t(T,Wt),t(Wt,mo),t(T,go),t(T,Ht),t(Ht,vo),t(T,_o),t(T,Mt),t(Mt,bo),t(T,$o),p(e,Ca,h),p(e,Q,h),t(Q,_e),t(_e,jt),g(Fe,jt,null),t(Q,yo),t(Q,Xt),t(Xt,wo),p(e,Ga,h),p(e,Y,h),t(Y,be),t(be,Jt),g(Re,Jt,null),t(Y,Eo),t(Y,Kt),t(Kt,Do),p(e,Ua,h),p(e,Z,h),g(We,Z,null),t(Z,So),t(Z,ee),t(ee,To),t(ee,He),t(He,xo),t(ee,ko),t(ee,Qt),t(Qt,Po),t(ee,Ao),p(e,Ba,h),p(e,te,h),t(te,$e),t($e,Yt),g(Me,Yt,null),t(te,Io),t(te,Zt),t(Zt,zo),p(e,Va,h),p(e,A,h),g(je,A,null),t(A,Lo),t(A,ea),t(ea,No),t(A,Oo),t(A,ta),t(ta,Co),t(A,Go),t(A,I),t(I,pt),t(pt,aa),t(aa,Uo),t(pt,Bo),t(I,Vo),t(I,ft),t(ft,ra),t(ra,qo),t(ft,Fo),t(I,Ro),t(I,ut),t(ut,oa),t(oa,Wo),t(ut,Ho),t(I,Mo),t(I,mt),t(mt,sa),t(sa,jo),t(mt,Xo),t(I,Jo),t(I,gt),t(gt,na),t(na,Ko),t(gt,Qo),p(e,qa,h),p(e,ae,h),t(ae,ye),t(ye,ca),g(Xe,ca,null),t(ae,Yo),t(ae,la),t(la,Zo),p(e,Fa,h),p(e,x,h),g(Je,x,null),t(x,es),t(x,ia),t(ia,ts),t(x,as),t(x,B),g(Ke,B,null),t(B,rs),t(B,da),t(da,os),t(B,ss),g(Qe,B,null),t(x,ns),t(x,we),g(Ye,we,null),t(we,cs),t(we,re),t(re,ls),t(re,ha),t(ha,is),t(re,ds),t(re,pa),t(pa,hs),t(re,ps),t(x,fs),t(x,Ee),g(Ze,Ee,null),t(Ee,us),t(Ee,et),t(et,ms),t(et,fa),t(fa,gs),t(et,vs),p(e,Ra,h),p(e,oe,h),t(oe,De),t(De,ua),g(tt,ua,null),t(oe,_s),t(oe,ma),t(ma,bs),p(e,Wa,h),p(e,se,h),g(at,se,null),t(se,$s),t(se,ga),t(ga,ys),p(e,Ha,h),p(e,ne,h),g(rt,ne,null),t(ne,ws),t(ne,va),t(va,Es),p(e,Ma,h),p(e,ce,h),g(ot,ce,null),t(ce,Ds),t(ce,_a),t(_a,Ss),p(e,ja,h),p(e,le,h),g(st,le,null),t(le,Ts),t(le,C),t(C,xs),t(C,ba),t(ba,ks),t(C,Ps),t(C,$a),t($a,As),t(C,Is),t(C,ya),t(ya,zs),t(C,Ls),p(e,Xa,h),p(e,nt,h),g(ct,nt,null),p(e,Ja,h),p(e,lt,h),g(it,lt,null),p(e,Ka,h),p(e,G,h),g(dt,G,null),t(G,Ns),t(G,wa),t(wa,Os),t(G,Cs),g(Se,G,null),Qa=!0},p(e,[h]){const ht={};h&2&&(ht.$$scope={dirty:h,ctx:e}),fe.$set(ht);const Ea={};h&2&&(Ea.$$scope={dirty:h,ctx:e}),ge.$set(Ea);const Da={};h&2&&(Da.$$scope={dirty:h,ctx:e}),Se.$set(Da)},i(e){Qa||(v(w.$$.fragment,e),v(Ae.$$.fragment,e),v(Ie.$$.fragment,e),v(ze.$$.fragment,e),v(Le.$$.fragment,e),v(fe.$$.fragment,e),v(Oe.$$.fragment,e),v(Ce.$$.fragment,e),v(Ue.$$.fragment,e),v(Be.$$.fragment,e),v(ge.$$.fragment,e),v(Ve.$$.fragment,e),v(qe.$$.fragment,e),v(Fe.$$.fragment,e),v(Re.$$.fragment,e),v(We.$$.fragment,e),v(Me.$$.fragment,e),v(je.$$.fragment,e),v(Xe.$$.fragment,e),v(Je.$$.fragment,e),v(Ke.$$.fragment,e),v(Qe.$$.fragment,e),v(Ye.$$.fragment,e),v(Ze.$$.fragment,e),v(tt.$$.fragment,e),v(at.$$.fragment,e),v(rt.$$.fragment,e),v(ot.$$.fragment,e),v(st.$$.fragment,e),v(ct.$$.fragment,e),v(it.$$.fragment,e),v(dt.$$.fragment,e),v(Se.$$.fragment,e),Qa=!0)},o(e){_(w.$$.fragment,e),_(Ae.$$.fragment,e),_(Ie.$$.fragment,e),_(ze.$$.fragment,e),_(Le.$$.fragment,e),_(fe.$$.fragment,e),_(Oe.$$.fragment,e),_(Ce.$$.fragment,e),_(Ue.$$.fragment,e),_(Be.$$.fragment,e),_(ge.$$.fragment,e),_(Ve.$$.fragment,e),_(qe.$$.fragment,e),_(Fe.$$.fragment,e),_(Re.$$.fragment,e),_(We.$$.fragment,e),_(Me.$$.fragment,e),_(je.$$.fragment,e),_(Xe.$$.fragment,e),_(Je.$$.fragment,e),_(Ke.$$.fragment,e),_(Qe.$$.fragment,e),_(Ye.$$.fragment,e),_(Ze.$$.fragment,e),_(tt.$$.fragment,e),_(at.$$.fragment,e),_(rt.$$.fragment,e),_(ot.$$.fragment,e),_(st.$$.fragment,e),_(ct.$$.fragment,e),_(it.$$.fragment,e),_(dt.$$.fragment,e),_(Se.$$.fragment,e),Qa=!1},d(e){a(f),e&&a(S),e&&a($),b(w),e&&a(Sa),e&&a(F),b(Ae),e&&a(Ta),e&&a(R),b(Ie),e&&a(xa),e&&a(W),b(ze),e&&a(ka),e&&a(pe),e&&a(Pa),e&&a(P),b(Le),b(fe),e&&a(Aa),e&&a(M),b(Oe),e&&a(Ia),e&&a(j),b(Ce),e&&a(za),e&&a(X),b(Ue),e&&a(La),e&&a(N),b(Be),b(ge),e&&a(Na),e&&a(J),b(Ve),e&&a(Oa),e&&a(K),b(qe),e&&a(Ca),e&&a(Q),b(Fe),e&&a(Ga),e&&a(Y),b(Re),e&&a(Ua),e&&a(Z),b(We),e&&a(Ba),e&&a(te),b(Me),e&&a(Va),e&&a(A),b(je),e&&a(qa),e&&a(ae),b(Xe),e&&a(Fa),e&&a(x),b(Je),b(Ke),b(Qe),b(Ye),b(Ze),e&&a(Ra),e&&a(oe),b(tt),e&&a(Wa),e&&a(se),b(at),e&&a(Ha),e&&a(ne),b(rt),e&&a(Ma),e&&a(ce),b(ot),e&&a(ja),e&&a(le),b(st),e&&a(Xa),e&&a(nt),b(ct),e&&a(Ja),e&&a(lt),b(it),e&&a(Ka),e&&a(G),b(dt),b(Se)}}}const uc={local:"internals",sections:[{local:"accelerate.optimizer.AcceleratedOptimizer",title:"Optimizer"},{local:"accelerate.data_loader.prepare_data_loader",sections:[{local:"accelerate.data_loader.DataLoaderShard",title:"BatchSamplerShard"},{local:"accelerate.data_loader.BatchSamplerShard",title:"BatchSamplerShard"},{local:"accelerate.data_loader.IterableDatasetShard",title:"IterableDatasetShard"}],title:"DataLoader"},{local:"distributed-config",sections:[{local:"accelerate.state.AcceleratorState",title:"AcceleratorState"},{local:"accelerate.DistributedType",title:"DistributedType"}],title:"Distributed Config"},{local:"accelerate.tracking.GeneralTracker",title:"Tracking"},{local:"accelerate.utils.extract_model_from_parallel",title:"Utilities"}],title:"Internals"};function mc(ie){return lc(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class yc extends oc{constructor(f){super();sc(this,f,mc,fc,nc,{})}}export{yc as default,uc as metadata};
