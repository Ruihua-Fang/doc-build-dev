import{S as ut,i as _t,s as gt,e as r,k as d,w as _,t as n,M as yt,c as l,d as a,m as c,a as o,x as g,h as s,b as m,F as t,g as p,y,L as wt,q as w,o as v,B as b,v as vt}from"../chunks/vendor-4918fc3c.js";import{D as bt}from"../chunks/Docstring-02f0cd65.js";import{C as Y}from"../chunks/CodeBlock-99419108.js";import{I as Ta}from"../chunks/IconCopyLink-21d338b1.js";function Pt(Na){let P,Pe,$,F,Z,L,Je,ee,Ve,$e,A,Xe,O,Ye,Ze,De,D,k,ae,T,ea,te,aa,Se,G,ta,Ee,N,Fe,K,ra,Ae,H,ke,Q,la,ze,I,xe,W,Ce,z,oa,re,ia,na,Le,q,Oe,f,sa,le,pa,da,oe,ca,ma,ie,fa,ha,ne,ua,_a,j,ga,ya,Te,S,M,wa,se,va,Ne,E,x,pe,U,ba,de,Pa,He,J,C,$a,ce,Da,Sa,me,fe,Ea,Ie,V,Fa,We,R,qe,u,B,he,Aa,ka,ue,_e,za,xa,ge,ye,Ca,La,we,ve,Oa,je;return L=new Ta({}),T=new Ta({}),N=new Y({props:{code:"accelerate config",highlighted:"accelerate config"}}),H=new Y({props:{code:"accelerate launch my_script.py --args_to_my_script",highlighted:"accelerate launch my_script.py --args_to_my_script"}}),I=new Y({props:{code:`compute_environment: LOCAL_MACHINE
deepspeed_config: {}
distributed_type: FSDP
fsdp_config:
  min_num_params: 2000
  offload_params: false
  sharding_strategy: 1
machine_rank: 0
main_process_ip: null
main_process_port: null
main_training_function: main
mixed_precision: 'no'
num_machines: 1
num_processes: 2
use_cpu: false`,highlighted:`compute_environment: LOCAL_MACHINE
deepspeed_config: {}
distributed_type: FSDP
fsdp_config:
  min_num_params: 2000
  offload_params: <span class="hljs-literal">false</span>
  sharding_strategy: 1
machine_rank: 0
main_process_ip: null
main_process_port: null
main_training_function: main
mixed_precision: <span class="hljs-string">&#x27;no&#x27;</span>
num_machines: 1
num_processes: 2
use_cpu: <span class="hljs-literal">false</span>`}}),W=new Y({props:{code:"accelerate launch examples/nlp_example.py",highlighted:"accelerate launch examples/nlp_example.py"}}),q=new Y({props:{code:"`Sharding Strategy`: [1] FULL_SHARD, [2] SHARD_GRAD_OP\n`Min Num Params`: FSDP\\'s minimum number of parameters for Default Auto Wrapping.\n`Offload Params`: Decides Whether to offload parameters and gradients to CPU.",highlighted:'`Sharding Strategy`: [1] FULL_SHARD, [2] SHARD_GRAD_OP\n`Min Num Params`: FSDP\\<span class="hljs-string">&#x27;s minimum number of parameters for Default Auto Wrapping.\n`Offload Params`: Decides Whether to offload parameters and gradients to CPU.</span>'}}),M=new bt({props:{name:"class accelerate.utils.FullyShardedDataParallelPlugin",anchor:"accelerate.utils.FullyShardedDataParallelPlugin",parameters:[{name:"sharding_strategy",val:": typing.Any = None"},{name:"backward_prefetch",val:": typing.Any = None"},{name:"auto_wrap_policy",val:": typing.Any = None"},{name:"cpu_offload",val:": typing.Optional[typing.Callable] = None"},{name:"min_num_params",val:": int = None"},{name:"ignored_modules",val:": typing.Optional[typing.Iterable[torch.nn.modules.module.Module]] = None"}],source:"https://github.com/huggingface/accelerate/blob/pr_321/src/accelerate/utils.py#L744"}}),U=new Ta({}),R=new Y({props:{code:`model = AutoModelForSequenceClassification.from_pretrained("bert-base-cased", return_dict=True)
+ model = accelerator.prepare(model)

optimizer = torch.optim.AdamW(params=model.parameters(), lr=lr)

- model, optimizer, train_dataloader, eval_dataloader, lr_scheduler = accelerator.prepare(model,
-        optimizer, train_dataloader, eval_dataloader, lr_scheduler
-    )

+ optimizer, train_dataloader, eval_dataloader, lr_scheduler = accelerator.prepare(
+         optimizer, train_dataloader, eval_dataloader, lr_scheduler
+        )
`,highlighted:`model = AutoModelForSequenceClassification.from_pretrained(&quot;bert-base-cased&quot;, return_dict=True)
<span class="hljs-addition">+ model = accelerator.prepare(model)</span>

optimizer = torch.optim.AdamW(params=model.parameters(), lr=lr)

<span class="hljs-deletion">- model, optimizer, train_dataloader, eval_dataloader, lr_scheduler = accelerator.prepare(model,</span>
<span class="hljs-deletion">-        optimizer, train_dataloader, eval_dataloader, lr_scheduler</span>
<span class="hljs-deletion">-    )</span>

<span class="hljs-addition">+ optimizer, train_dataloader, eval_dataloader, lr_scheduler = accelerator.prepare(</span>
<span class="hljs-addition">+         optimizer, train_dataloader, eval_dataloader, lr_scheduler</span>
<span class="hljs-addition">+        )</span>
`}}),{c(){P=r("meta"),Pe=d(),$=r("h1"),F=r("a"),Z=r("span"),_(L.$$.fragment),Je=d(),ee=r("span"),Ve=n("Fully Sharded Data Parallel"),$e=d(),A=r("p"),Xe=n(`To accelerate training huge models on larger batch sizes, we can use a fully sharded data parallel model.
This type of data parallel paradigm enables fitting more data and larger models by sharding the optimizer states, gradients and parameters.
To read more about it and the benefits, check out the `),O=r("a"),Ye=n("Fully Sharded Data Parallel blog"),Ze=n(`.
We have integrated the latest PyTorch\u2019s Fully Sharded Data Parallel (FSDP) training feature.
All you need to do is enable it through the config.`),De=d(),D=r("h2"),k=r("a"),ae=r("span"),_(T.$$.fragment),ea=d(),te=r("span"),aa=n("How it works out the box"),Se=d(),G=r("p"),ta=n("On your machine(s) just run:"),Ee=d(),_(N.$$.fragment),Fe=d(),K=r("p"),ra=n(`and answer the questions asked. This will generate a config file that will be used automatically to properly set the
default options when doing`),Ae=d(),_(H.$$.fragment),ke=d(),Q=r("p"),la=n("For instance, here is how you would run the NLP example (from the root of the repo) with FSDP enabled:"),ze=d(),_(I.$$.fragment),xe=d(),_(W.$$.fragment),Ce=d(),z=r("p"),oa=n("Currently, "),re=r("code"),ia=n("Accelerate"),na=n(" supports following config through the CLI:"),Le=d(),_(q.$$.fragment),Oe=d(),f=r("p"),sa=n("For more control, users can leverage the "),le=r("code"),pa=n("FullyShardedDataParallelPlugin"),da=n(" wherein they can specify "),oe=r("code"),ca=n("auto_wrap_policy"),ma=n(", "),ie=r("code"),fa=n("backward_prefetch"),ha=n(" and "),ne=r("code"),ua=n("ignored_modules"),_a=n(`.
After creating an instance of this class, users can pass it to the Accelerator class instantiation.
For more information on these options, please refer to the PyTorch `),j=r("a"),ga=n("FullyShardedDataParallel"),ya=n(" code."),Te=d(),S=r("div"),_(M.$$.fragment),wa=d(),se=r("p"),va=n("This plugin is used to enable fully sharded data parallelism."),Ne=d(),E=r("h2"),x=r("a"),pe=r("span"),_(U.$$.fragment),ba=d(),de=r("span"),Pa=n("Few caveats to be aware of:"),He=d(),J=r("ul"),C=r("li"),$a=n(`PyTorch FSDP auto wraps sub-modules, flattens the parameters and shards the parameters in place.
Due to this, any optimizer created before model wrapping gets broken and occupies more memory.
Hence, it is highly recommended and efficient to prepare model before creating optimizer.
`),ce=r("code"),Da=n("Accelerate"),Sa=n(" will automatically wrap the model and create an optimizer for you in case of single model with a warning message."),me=r("blockquote"),fe=r("p"),Ea=n("FSDP Warning: When using FSDP, it is efficient and recommended to call prepare for the model before creating the optimizer"),Ie=d(),V=r("p"),Fa=n("However, below is the recommended way to prepare model and optimizer while using FSDP:"),We=d(),_(R.$$.fragment),qe=d(),u=r("ul"),B=r("li"),he=r("p"),Aa=n(`In case of a single model, if you have created optimizer with multiple optimizer groups and called prepare with them together,
then the optimizer groups will be lost and the following warning is displayed:`),ka=d(),ue=r("blockquote"),_e=r("p"),za=n(`FSDP Warning: When using FSDP, several parameter groups will be conflated into
a single one due to nested module wrapping and parameter flattening.`),xa=d(),ge=r("li"),ye=r("p"),Ca=n("In case of multiple models, it is necessary to prepare the models before creating optimizers else it will throw an error."),La=d(),we=r("li"),ve=r("p"),Oa=n("Mixed precision is currently not supported with FSDP."),this.h()},l(e){const i=yt('[data-svelte="svelte-1phssyn"]',document.head);P=l(i,"META",{name:!0,content:!0}),i.forEach(a),Pe=c(e),$=l(e,"H1",{class:!0});var Me=o($);F=l(Me,"A",{id:!0,class:!0,href:!0});var Ha=o(F);Z=l(Ha,"SPAN",{});var Ia=o(Z);g(L.$$.fragment,Ia),Ia.forEach(a),Ha.forEach(a),Je=c(Me),ee=l(Me,"SPAN",{});var Wa=o(ee);Ve=s(Wa,"Fully Sharded Data Parallel"),Wa.forEach(a),Me.forEach(a),$e=c(e),A=l(e,"P",{});var Ue=o(A);Xe=s(Ue,`To accelerate training huge models on larger batch sizes, we can use a fully sharded data parallel model.
This type of data parallel paradigm enables fitting more data and larger models by sharding the optimizer states, gradients and parameters.
To read more about it and the benefits, check out the `),O=l(Ue,"A",{href:!0,rel:!0});var qa=o(O);Ye=s(qa,"Fully Sharded Data Parallel blog"),qa.forEach(a),Ze=s(Ue,`.
We have integrated the latest PyTorch\u2019s Fully Sharded Data Parallel (FSDP) training feature.
All you need to do is enable it through the config.`),Ue.forEach(a),De=c(e),D=l(e,"H2",{class:!0});var Re=o(D);k=l(Re,"A",{id:!0,class:!0,href:!0});var ja=o(k);ae=l(ja,"SPAN",{});var Ma=o(ae);g(T.$$.fragment,Ma),Ma.forEach(a),ja.forEach(a),ea=c(Re),te=l(Re,"SPAN",{});var Ua=o(te);aa=s(Ua,"How it works out the box"),Ua.forEach(a),Re.forEach(a),Se=c(e),G=l(e,"P",{});var Ra=o(G);ta=s(Ra,"On your machine(s) just run:"),Ra.forEach(a),Ee=c(e),g(N.$$.fragment,e),Fe=c(e),K=l(e,"P",{});var Ba=o(K);ra=s(Ba,`and answer the questions asked. This will generate a config file that will be used automatically to properly set the
default options when doing`),Ba.forEach(a),Ae=c(e),g(H.$$.fragment,e),ke=c(e),Q=l(e,"P",{});var Ga=o(Q);la=s(Ga,"For instance, here is how you would run the NLP example (from the root of the repo) with FSDP enabled:"),Ga.forEach(a),ze=c(e),g(I.$$.fragment,e),xe=c(e),g(W.$$.fragment,e),Ce=c(e),z=l(e,"P",{});var Be=o(z);oa=s(Be,"Currently, "),re=l(Be,"CODE",{});var Ka=o(re);ia=s(Ka,"Accelerate"),Ka.forEach(a),na=s(Be," supports following config through the CLI:"),Be.forEach(a),Le=c(e),g(q.$$.fragment,e),Oe=c(e),f=l(e,"P",{});var h=o(f);sa=s(h,"For more control, users can leverage the "),le=l(h,"CODE",{});var Qa=o(le);pa=s(Qa,"FullyShardedDataParallelPlugin"),Qa.forEach(a),da=s(h," wherein they can specify "),oe=l(h,"CODE",{});var Ja=o(oe);ca=s(Ja,"auto_wrap_policy"),Ja.forEach(a),ma=s(h,", "),ie=l(h,"CODE",{});var Va=o(ie);fa=s(Va,"backward_prefetch"),Va.forEach(a),ha=s(h," and "),ne=l(h,"CODE",{});var Xa=o(ne);ua=s(Xa,"ignored_modules"),Xa.forEach(a),_a=s(h,`.
After creating an instance of this class, users can pass it to the Accelerator class instantiation.
For more information on these options, please refer to the PyTorch `),j=l(h,"A",{href:!0,rel:!0});var Ya=o(j);ga=s(Ya,"FullyShardedDataParallel"),Ya.forEach(a),ya=s(h," code."),h.forEach(a),Te=c(e),S=l(e,"DIV",{class:!0});var Ge=o(S);g(M.$$.fragment,Ge),wa=c(Ge),se=l(Ge,"P",{});var Za=o(se);va=s(Za,"This plugin is used to enable fully sharded data parallelism."),Za.forEach(a),Ge.forEach(a),Ne=c(e),E=l(e,"H2",{class:!0});var Ke=o(E);x=l(Ke,"A",{id:!0,class:!0,href:!0});var et=o(x);pe=l(et,"SPAN",{});var at=o(pe);g(U.$$.fragment,at),at.forEach(a),et.forEach(a),ba=c(Ke),de=l(Ke,"SPAN",{});var tt=o(de);Pa=s(tt,"Few caveats to be aware of:"),tt.forEach(a),Ke.forEach(a),He=c(e),J=l(e,"UL",{});var rt=o(J);C=l(rt,"LI",{});var be=o(C);$a=s(be,`PyTorch FSDP auto wraps sub-modules, flattens the parameters and shards the parameters in place.
Due to this, any optimizer created before model wrapping gets broken and occupies more memory.
Hence, it is highly recommended and efficient to prepare model before creating optimizer.
`),ce=l(be,"CODE",{});var lt=o(ce);Da=s(lt,"Accelerate"),lt.forEach(a),Sa=s(be," will automatically wrap the model and create an optimizer for you in case of single model with a warning message."),me=l(be,"BLOCKQUOTE",{});var ot=o(me);fe=l(ot,"P",{});var it=o(fe);Ea=s(it,"FSDP Warning: When using FSDP, it is efficient and recommended to call prepare for the model before creating the optimizer"),it.forEach(a),ot.forEach(a),be.forEach(a),rt.forEach(a),Ie=c(e),V=l(e,"P",{});var nt=o(V);Fa=s(nt,"However, below is the recommended way to prepare model and optimizer while using FSDP:"),nt.forEach(a),We=c(e),g(R.$$.fragment,e),qe=c(e),u=l(e,"UL",{});var X=o(u);B=l(X,"LI",{});var Qe=o(B);he=l(Qe,"P",{});var st=o(he);Aa=s(st,`In case of a single model, if you have created optimizer with multiple optimizer groups and called prepare with them together,
then the optimizer groups will be lost and the following warning is displayed:`),st.forEach(a),ka=c(Qe),ue=l(Qe,"BLOCKQUOTE",{});var pt=o(ue);_e=l(pt,"P",{});var dt=o(_e);za=s(dt,`FSDP Warning: When using FSDP, several parameter groups will be conflated into
a single one due to nested module wrapping and parameter flattening.`),dt.forEach(a),pt.forEach(a),Qe.forEach(a),xa=c(X),ge=l(X,"LI",{});var ct=o(ge);ye=l(ct,"P",{});var mt=o(ye);Ca=s(mt,"In case of multiple models, it is necessary to prepare the models before creating optimizers else it will throw an error."),mt.forEach(a),ct.forEach(a),La=c(X),we=l(X,"LI",{});var ft=o(we);ve=l(ft,"P",{});var ht=o(ve);Oa=s(ht,"Mixed precision is currently not supported with FSDP."),ht.forEach(a),ft.forEach(a),X.forEach(a),this.h()},h(){m(P,"name","hf:doc:metadata"),m(P,"content",JSON.stringify($t)),m(F,"id","fully-sharded-data-parallel"),m(F,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(F,"href","#fully-sharded-data-parallel"),m($,"class","relative group"),m(O,"href","https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/"),m(O,"rel","nofollow"),m(k,"id","accelerate.utils.FullyShardedDataParallelPlugin"),m(k,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(k,"href","#accelerate.utils.FullyShardedDataParallelPlugin"),m(D,"class","relative group"),m(j,"href","https://github.com/pytorch/pytorch/blob/0df2e863fbd5993a7b9e652910792bd21a516ff3/torch/distributed/fsdp/fully_sharded_data_parallel.py#L236"),m(j,"rel","nofollow"),m(S,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),m(x,"id","few-caveats-to-be-aware-of"),m(x,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(x,"href","#few-caveats-to-be-aware-of"),m(E,"class","relative group")},m(e,i){t(document.head,P),p(e,Pe,i),p(e,$,i),t($,F),t(F,Z),y(L,Z,null),t($,Je),t($,ee),t(ee,Ve),p(e,$e,i),p(e,A,i),t(A,Xe),t(A,O),t(O,Ye),t(A,Ze),p(e,De,i),p(e,D,i),t(D,k),t(k,ae),y(T,ae,null),t(D,ea),t(D,te),t(te,aa),p(e,Se,i),p(e,G,i),t(G,ta),p(e,Ee,i),y(N,e,i),p(e,Fe,i),p(e,K,i),t(K,ra),p(e,Ae,i),y(H,e,i),p(e,ke,i),p(e,Q,i),t(Q,la),p(e,ze,i),y(I,e,i),p(e,xe,i),y(W,e,i),p(e,Ce,i),p(e,z,i),t(z,oa),t(z,re),t(re,ia),t(z,na),p(e,Le,i),y(q,e,i),p(e,Oe,i),p(e,f,i),t(f,sa),t(f,le),t(le,pa),t(f,da),t(f,oe),t(oe,ca),t(f,ma),t(f,ie),t(ie,fa),t(f,ha),t(f,ne),t(ne,ua),t(f,_a),t(f,j),t(j,ga),t(f,ya),p(e,Te,i),p(e,S,i),y(M,S,null),t(S,wa),t(S,se),t(se,va),p(e,Ne,i),p(e,E,i),t(E,x),t(x,pe),y(U,pe,null),t(E,ba),t(E,de),t(de,Pa),p(e,He,i),p(e,J,i),t(J,C),t(C,$a),t(C,ce),t(ce,Da),t(C,Sa),t(C,me),t(me,fe),t(fe,Ea),p(e,Ie,i),p(e,V,i),t(V,Fa),p(e,We,i),y(R,e,i),p(e,qe,i),p(e,u,i),t(u,B),t(B,he),t(he,Aa),t(B,ka),t(B,ue),t(ue,_e),t(_e,za),t(u,xa),t(u,ge),t(ge,ye),t(ye,Ca),t(u,La),t(u,we),t(we,ve),t(ve,Oa),je=!0},p:wt,i(e){je||(w(L.$$.fragment,e),w(T.$$.fragment,e),w(N.$$.fragment,e),w(H.$$.fragment,e),w(I.$$.fragment,e),w(W.$$.fragment,e),w(q.$$.fragment,e),w(M.$$.fragment,e),w(U.$$.fragment,e),w(R.$$.fragment,e),je=!0)},o(e){v(L.$$.fragment,e),v(T.$$.fragment,e),v(N.$$.fragment,e),v(H.$$.fragment,e),v(I.$$.fragment,e),v(W.$$.fragment,e),v(q.$$.fragment,e),v(M.$$.fragment,e),v(U.$$.fragment,e),v(R.$$.fragment,e),je=!1},d(e){a(P),e&&a(Pe),e&&a($),b(L),e&&a($e),e&&a(A),e&&a(De),e&&a(D),b(T),e&&a(Se),e&&a(G),e&&a(Ee),b(N,e),e&&a(Fe),e&&a(K),e&&a(Ae),b(H,e),e&&a(ke),e&&a(Q),e&&a(ze),b(I,e),e&&a(xe),b(W,e),e&&a(Ce),e&&a(z),e&&a(Le),b(q,e),e&&a(Oe),e&&a(f),e&&a(Te),e&&a(S),b(M),e&&a(Ne),e&&a(E),b(U),e&&a(He),e&&a(J),e&&a(Ie),e&&a(V),e&&a(We),b(R,e),e&&a(qe),e&&a(u)}}}const $t={local:"fully-sharded-data-parallel",sections:[{local:"accelerate.utils.FullyShardedDataParallelPlugin",title:"How it works out the box"},{local:"few-caveats-to-be-aware-of",title:"Few caveats to be aware of:"}],title:"Fully Sharded Data Parallel"};function Dt(Na){return vt(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class kt extends ut{constructor(P){super();_t(this,P,Dt,Pt,gt,{})}}export{kt as default,$t as metadata};
