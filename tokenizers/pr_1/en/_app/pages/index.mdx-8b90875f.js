import{S as Me,i as Ne,s as Ce,e as o,k as u,w as xe,t as h,M as De,c as s,d as a,m as d,a as r,x as Pe,h as c,b as i,F as t,g as p,y as Le,L as Re,q as Se,o as Ae,B as Ie,v as Ue}from"../chunks/vendor-0d3f0756.js";import{I as Fe}from"../chunks/IconCopyLink-9193371d.js";function qe(fe){let m,D,v,y,T,w,O,x,j,R,$,K,U,k,z,Q,V,E,W,X,q,_,g,P,b,Y,L,Z,B,n,S,ee,te,A,ae,oe,I,se,re,F,ne,ie,M,le,he,N,ce,G;return w=new Fe({}),b=new Fe({}),{c(){m=o("meta"),D=u(),v=o("h1"),y=o("a"),T=o("span"),xe(w.$$.fragment),O=u(),x=o("span"),j=h("Tokenizers"),R=u(),$=o("p"),K=h(`Fast State-of-the-art tokenizers, optimized for both research and
production`),U=u(),k=o("p"),z=o("a"),Q=h("\u{1F917} Tokenizers"),V=h(` provides an
implementation of today\u2019s most used tokenizers, with a focus on
performance and versatility. These tokenizers are also used in `),E=o("a"),W=h("\u{1F917} Transformers"),X=h("."),q=u(),_=o("h1"),g=o("a"),P=o("span"),xe(b.$$.fragment),Y=u(),L=o("span"),Z=h("Main features:"),B=u(),n=o("ul"),S=o("li"),ee=h("Train new vocabularies and tokenize, using today\u2019s most used tokenizers."),te=u(),A=o("li"),ae=h("Extremely fast (both training and tokenization), thanks to the Rust implementation. Takes less than 20 seconds to tokenize a GB of text on a server\u2019s CPU."),oe=u(),I=o("li"),se=h("Easy to use, but also extremely versatile."),re=u(),F=o("li"),ne=h("Designed for both research and production."),ie=u(),M=o("li"),le=h("Full alignment tracking. Even with destructive normalization, it\u2019s always possible to get the part of the original sentence that corresponds to any token."),he=u(),N=o("li"),ce=h("Does all the pre-processing: Truncation, Padding, add the special tokens your model needs."),this.h()},l(e){const l=De('[data-svelte="svelte-1phssyn"]',document.head);m=s(l,"META",{name:!0,content:!0}),l.forEach(a),D=d(e),v=s(e,"H1",{class:!0});var H=r(v);y=s(H,"A",{id:!0,class:!0,href:!0});var ue=r(y);T=s(ue,"SPAN",{});var de=r(T);Pe(w.$$.fragment,de),de.forEach(a),ue.forEach(a),O=d(H),x=s(H,"SPAN",{});var pe=r(x);j=c(pe,"Tokenizers"),pe.forEach(a),H.forEach(a),R=d(e),$=s(e,"P",{});var me=r($);K=c(me,`Fast State-of-the-art tokenizers, optimized for both research and
production`),me.forEach(a),U=d(e),k=s(e,"P",{});var C=r(k);z=s(C,"A",{href:!0,rel:!0});var ve=r(z);Q=c(ve,"\u{1F917} Tokenizers"),ve.forEach(a),V=c(C,` provides an
implementation of today\u2019s most used tokenizers, with a focus on
performance and versatility. These tokenizers are also used in `),E=s(C,"A",{href:!0,rel:!0});var ke=r(E);W=c(ke,"\u{1F917} Transformers"),ke.forEach(a),X=c(C,"."),C.forEach(a),q=d(e),_=s(e,"H1",{class:!0});var J=r(_);g=s(J,"A",{id:!0,class:!0,href:!0});var _e=r(g);P=s(_e,"SPAN",{});var ye=r(P);Pe(b.$$.fragment,ye),ye.forEach(a),_e.forEach(a),Y=d(J),L=s(J,"SPAN",{});var ge=r(L);Z=c(ge,"Main features:"),ge.forEach(a),J.forEach(a),B=d(e),n=s(e,"UL",{});var f=r(n);S=s(f,"LI",{});var we=r(S);ee=c(we,"Train new vocabularies and tokenize, using today\u2019s most used tokenizers."),we.forEach(a),te=d(f),A=s(f,"LI",{});var ze=r(A);ae=c(ze,"Extremely fast (both training and tokenization), thanks to the Rust implementation. Takes less than 20 seconds to tokenize a GB of text on a server\u2019s CPU."),ze.forEach(a),oe=d(f),I=s(f,"LI",{});var Ee=r(I);se=c(Ee,"Easy to use, but also extremely versatile."),Ee.forEach(a),re=d(f),F=s(f,"LI",{});var be=r(F);ne=c(be,"Designed for both research and production."),be.forEach(a),ie=d(f),M=s(f,"LI",{});var $e=r(M);le=c($e,"Full alignment tracking. Even with destructive normalization, it\u2019s always possible to get the part of the original sentence that corresponds to any token."),$e.forEach(a),he=d(f),N=s(f,"LI",{});var Te=r(N);ce=c(Te,"Does all the pre-processing: Truncation, Padding, add the special tokens your model needs."),Te.forEach(a),f.forEach(a),this.h()},h(){i(m,"name","hf:doc:metadata"),i(m,"content",JSON.stringify(Be)),i(y,"id","tokenizers"),i(y,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(y,"href","#tokenizers"),i(v,"class","relative group"),i(z,"href","https://github.com/huggingface/tokenizers"),i(z,"rel","nofollow"),i(E,"href","https://github.com/huggingface/transformers"),i(E,"rel","nofollow"),i(g,"id","main-features:"),i(g,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(g,"href","#main-features:"),i(_,"class","relative group")},m(e,l){t(document.head,m),p(e,D,l),p(e,v,l),t(v,y),t(y,T),Le(w,T,null),t(v,O),t(v,x),t(x,j),p(e,R,l),p(e,$,l),t($,K),p(e,U,l),p(e,k,l),t(k,z),t(z,Q),t(k,V),t(k,E),t(E,W),t(k,X),p(e,q,l),p(e,_,l),t(_,g),t(g,P),Le(b,P,null),t(_,Y),t(_,L),t(L,Z),p(e,B,l),p(e,n,l),t(n,S),t(S,ee),t(n,te),t(n,A),t(A,ae),t(n,oe),t(n,I),t(I,se),t(n,re),t(n,F),t(F,ne),t(n,ie),t(n,M),t(M,le),t(n,he),t(n,N),t(N,ce),G=!0},p:Re,i(e){G||(Se(w.$$.fragment,e),Se(b.$$.fragment,e),G=!0)},o(e){Ae(w.$$.fragment,e),Ae(b.$$.fragment,e),G=!1},d(e){a(m),e&&a(D),e&&a(v),Ie(w),e&&a(R),e&&a($),e&&a(U),e&&a(k),e&&a(q),e&&a(_),Ie(b),e&&a(B),e&&a(n)}}}const Be={local:"tokenizers",title:"Tokenizers"};function Ge(fe){return Ue(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Oe extends Me{constructor(m){super();Ne(this,m,Ge,qe,Ce,{})}}export{Oe as default,Be as metadata};
