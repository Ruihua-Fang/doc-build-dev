import{S as ii,i as ui,s as pi,H as ci,e as _,c as k,a as w,d as r,b as P,g as q,I as fi,J as $i,K as hi,q as p,o as c,k as j,w as f,t as d,M as mi,m as v,x as $,h as g,F as a,y as h,B as m,v as di,L as y}from"../chunks/vendor-0d3f0756.js";import{I as Ee}from"../chunks/IconCopyLink-9193371d.js";import{C as z}from"../chunks/CodeBlock-7b0cb15c.js";import{T as H,M as E}from"../chunks/TokenizersLanguageContent-ca787841.js";function gi(l){let t,n;const e=l[3].default,s=ci(e,l,l[2],null);return{c(){t=_("div"),s&&s.c(),this.h()},l(i){t=k(i,"DIV",{class:!0});var T=w(t);s&&s.l(T),T.forEach(r),this.h()},h(){P(t,"class","course-tip "+(l[0]==="orange"?"course-tip-orange":"")+" bg-gradient-to-br dark:bg-gradient-to-r before:border-"+l[0]+"-500 dark:before:border-"+l[0]+"-800 from-"+l[0]+"-50 dark:from-gray-900 to-white dark:to-gray-950 border border-"+l[0]+"-50 text-"+l[0]+"-700 dark:text-gray-400")},m(i,T){q(i,t,T),s&&s.m(t,null),n=!0},p(i,[T]){s&&s.p&&(!n||T&4)&&fi(s,e,i,i[2],n?hi(e,i[2],T,null):$i(i[2]),null)},i(i){n||(p(s,i),n=!0)},o(i){c(s,i),n=!1},d(i){i&&r(t),s&&s.d(i)}}}function _i(l,t,n){let{$$slots:e={},$$scope:s}=t,{warning:i=!1}=t;const T=i?"orange":"green";return l.$$set=B=>{"warning"in B&&n(1,i=B.warning),"$$scope"in B&&n(2,s=B.$$scope)},[T,i,s,e]}class ki extends ii{constructor(t){super();ui(this,t,_i,gi,pi,{warning:1})}}function wi(l){let t,n;return t=new z({props:{code:`from tokenizers import Tokenizer
from tokenizers.models import BPE
tokenizer = Tokenizer(BPE(unk_token="[UNK]"))`,highlighted:`<span class="hljs-keyword">from</span> tokenizers <span class="hljs-keyword">import</span> Tokenizer
<span class="hljs-keyword">from</span> tokenizers.models <span class="hljs-keyword">import</span> BPE
tokenizer = Tokenizer(BPE(unk_token=<span class="hljs-string">&quot;[UNK]&quot;</span>))`}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p:y,i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function qi(l){let t,n;return t=new E({props:{$$slots:{default:[wi]},$$scope:{ctx:l}}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p(e,s){const i={};s&2&&(i.$$scope={dirty:s,ctx:e}),t.$set(i)},i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function ji(l){let t,n;return t=new z({props:{code:`use tokenizers::models::bpe::BPE;
let mut tokenizer: TokenizerImpl<
    BPE,
    NormalizerWrapper,
    PreTokenizerWrapper,
    PostProcessorWrapper,
    DecoderWrapper,
> = TokenizerImpl::new(
    BPE::builder()
        .unk_token("[UNK]".to_string())
        .build()
        .unwrap(),
);`,highlighted:`<span class="hljs-keyword">use</span> tokenizers::models::bpe::BPE;
<span class="hljs-keyword">let</span> <span class="hljs-keyword">mut </span><span class="hljs-variable">tokenizer</span>: TokenizerImpl&lt;
    BPE,
    NormalizerWrapper,
    PreTokenizerWrapper,
    PostProcessorWrapper,
    DecoderWrapper,
&gt; = TokenizerImpl::<span class="hljs-title function_ invoke__">new</span>(
    BPE::<span class="hljs-title function_ invoke__">builder</span>()
        .<span class="hljs-title function_ invoke__">unk_token</span>(<span class="hljs-string">&quot;[UNK]&quot;</span>.<span class="hljs-title function_ invoke__">to_string</span>())
        .<span class="hljs-title function_ invoke__">build</span>()
        .<span class="hljs-title function_ invoke__">unwrap</span>(),
);`}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p:y,i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function vi(l){let t,n;return t=new E({props:{$$slots:{default:[ji]},$$scope:{ctx:l}}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p(e,s){const i={};s&2&&(i.$$scope={dirty:s,ctx:e}),t.$set(i)},i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function bi(l){let t,n;return t=new z({props:{code:`let { Tokenizer } = require("tokenizers/bindings/tokenizer");
let { BPE } = require("tokenizers/bindings/models");
let tokenizer = new Tokenizer(BPE.init({}, [], { unkToken: "[UNK]" }));`,highlighted:`<span class="hljs-keyword">let</span> { <span class="hljs-title class_">Tokenizer</span> } = <span class="hljs-built_in">require</span>(<span class="hljs-string">&quot;tokenizers/bindings/tokenizer&quot;</span>);
<span class="hljs-keyword">let</span> { <span class="hljs-variable constant_">BPE</span> } = <span class="hljs-built_in">require</span>(<span class="hljs-string">&quot;tokenizers/bindings/models&quot;</span>);
<span class="hljs-keyword">let</span> tokenizer = <span class="hljs-keyword">new</span> <span class="hljs-title class_">Tokenizer</span>(<span class="hljs-variable constant_">BPE</span>.<span class="hljs-title function_">init</span>({}, [], { <span class="hljs-attr">unkToken</span>: <span class="hljs-string">&quot;[UNK]&quot;</span> }));`}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p:y,i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function zi(l){let t,n;return t=new E({props:{$$slots:{default:[bi]},$$scope:{ctx:l}}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p(e,s){const i={};s&2&&(i.$$scope={dirty:s,ctx:e}),t.$set(i)},i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function yi(l){let t,n;return t=new z({props:{code:`from tokenizers.trainers import BpeTrainer
trainer = BpeTrainer(special_tokens=["[UNK]", "[CLS]", "[SEP]", "[PAD]", "[MASK]"])`,highlighted:`<span class="hljs-keyword">from</span> tokenizers.trainers <span class="hljs-keyword">import</span> BpeTrainer
trainer = BpeTrainer(special_tokens=[<span class="hljs-string">&quot;[UNK]&quot;</span>, <span class="hljs-string">&quot;[CLS]&quot;</span>, <span class="hljs-string">&quot;[SEP]&quot;</span>, <span class="hljs-string">&quot;[PAD]&quot;</span>, <span class="hljs-string">&quot;[MASK]&quot;</span>])`}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p:y,i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Ei(l){let t,n;return t=new E({props:{$$slots:{default:[yi]},$$scope:{ctx:l}}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p(e,s){const i={};s&2&&(i.$$scope={dirty:s,ctx:e}),t.$set(i)},i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Pi(l){let t,n;return t=new z({props:{code:`use tokenizers::models::bpe::BpeTrainer;
let mut trainer = BpeTrainer::builder()
    .special_tokens(vec![
        AddedToken::from("[UNK]", true),
        AddedToken::from("[CLS]", true),
        AddedToken::from("[SEP]", true),
        AddedToken::from("[PAD]", true),
        AddedToken::from("[MASK]", true),
    ])
    .build();`,highlighted:`<span class="hljs-keyword">use</span> tokenizers::models::bpe::BpeTrainer;
<span class="hljs-keyword">let</span> <span class="hljs-keyword">mut </span><span class="hljs-variable">trainer</span> = BpeTrainer::<span class="hljs-title function_ invoke__">builder</span>()
    .<span class="hljs-title function_ invoke__">special_tokens</span>(<span class="hljs-built_in">vec!</span>[
        AddedToken::<span class="hljs-title function_ invoke__">from</span>(<span class="hljs-string">&quot;[UNK]&quot;</span>, <span class="hljs-literal">true</span>),
        AddedToken::<span class="hljs-title function_ invoke__">from</span>(<span class="hljs-string">&quot;[CLS]&quot;</span>, <span class="hljs-literal">true</span>),
        AddedToken::<span class="hljs-title function_ invoke__">from</span>(<span class="hljs-string">&quot;[SEP]&quot;</span>, <span class="hljs-literal">true</span>),
        AddedToken::<span class="hljs-title function_ invoke__">from</span>(<span class="hljs-string">&quot;[PAD]&quot;</span>, <span class="hljs-literal">true</span>),
        AddedToken::<span class="hljs-title function_ invoke__">from</span>(<span class="hljs-string">&quot;[MASK]&quot;</span>, <span class="hljs-literal">true</span>),
    ])
    .<span class="hljs-title function_ invoke__">build</span>();`}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p:y,i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Ti(l){let t,n;return t=new E({props:{$$slots:{default:[Pi]},$$scope:{ctx:l}}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p(e,s){const i={};s&2&&(i.$$scope={dirty:s,ctx:e}),t.$set(i)},i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Si(l){let t,n;return t=new z({props:{code:`let { bpeTrainer } = require("tokenizers/bindings/trainers");
let trainer = bpeTrainer({
    specialTokens: ["[UNK]", "[CLS]", "[SEP]", "[PAD]", "[MASK]"]
});`,highlighted:`<span class="hljs-keyword">let</span> { bpeTrainer } = <span class="hljs-built_in">require</span>(<span class="hljs-string">&quot;tokenizers/bindings/trainers&quot;</span>);
<span class="hljs-keyword">let</span> trainer = <span class="hljs-title function_">bpeTrainer</span>({
    <span class="hljs-attr">specialTokens</span>: [<span class="hljs-string">&quot;[UNK]&quot;</span>, <span class="hljs-string">&quot;[CLS]&quot;</span>, <span class="hljs-string">&quot;[SEP]&quot;</span>, <span class="hljs-string">&quot;[PAD]&quot;</span>, <span class="hljs-string">&quot;[MASK]&quot;</span>]
});`}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p:y,i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Hi(l){let t,n;return t=new E({props:{$$slots:{default:[Si]},$$scope:{ctx:l}}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p(e,s){const i={};s&2&&(i.$$scope={dirty:s,ctx:e}),t.$set(i)},i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function xi(l){let t,n,e,s,i,T,B,K;return{c(){t=_("p"),n=d("The order in which you write the special tokens list matters: here "),e=_("code"),s=d('"[UNK]"'),i=d(` will get the ID 0,
`),T=_("code"),B=d('"[CLS]"'),K=d(" will get the ID 1 and so forth.")},l(C){t=k(C,"P",{});var D=w(t);n=g(D,"The order in which you write the special tokens list matters: here "),e=k(D,"CODE",{});var O=w(e);s=g(O,'"[UNK]"'),O.forEach(r),i=g(D,` will get the ID 0,
`),T=k(D,"CODE",{});var re=w(T);B=g(re,'"[CLS]"'),re.forEach(r),K=g(D," will get the ID 1 and so forth."),D.forEach(r)},m(C,D){q(C,t,D),a(t,n),a(t,e),a(e,s),a(t,i),a(t,T),a(T,B),a(t,K)},d(C){C&&r(t)}}}function Ci(l){let t,n;return t=new z({props:{code:`from tokenizers.pre_tokenizers import Whitespace
tokenizer.pre_tokenizer = Whitespace()`,highlighted:`<span class="hljs-keyword">from</span> tokenizers.pre_tokenizers <span class="hljs-keyword">import</span> Whitespace
tokenizer.pre_tokenizer = Whitespace()`}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p:y,i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Ai(l){let t,n;return t=new E({props:{$$slots:{default:[Ci]},$$scope:{ctx:l}}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p(e,s){const i={};s&2&&(i.$$scope={dirty:s,ctx:e}),t.$set(i)},i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Di(l){let t,n;return t=new z({props:{code:`use tokenizers::pre_tokenizers::whitespace::Whitespace;
tokenizer.with_pre_tokenizer(Whitespace::default());`,highlighted:`<span class="hljs-keyword">use</span> tokenizers::pre_tokenizers::whitespace::Whitespace;
tokenizer.<span class="hljs-title function_ invoke__">with_pre_tokenizer</span>(Whitespace::<span class="hljs-title function_ invoke__">default</span>());`}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p:y,i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Li(l){let t,n;return t=new E({props:{$$slots:{default:[Di]},$$scope:{ctx:l}}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p(e,s){const i={};s&2&&(i.$$scope={dirty:s,ctx:e}),t.$set(i)},i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Bi(l){let t,n;return t=new z({props:{code:`let { whitespacePreTokenizer } = require("tokenizers/bindings/pre-tokenizers");
tokenizer.setPreTokenizer(whitespacePreTokenizer());`,highlighted:`<span class="hljs-keyword">let</span> { whitespacePreTokenizer } = <span class="hljs-built_in">require</span>(<span class="hljs-string">&quot;tokenizers/bindings/pre-tokenizers&quot;</span>);
tokenizer.<span class="hljs-title function_">setPreTokenizer</span>(<span class="hljs-title function_">whitespacePreTokenizer</span>());`}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p:y,i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Ni(l){let t,n;return t=new E({props:{$$slots:{default:[Bi]},$$scope:{ctx:l}}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p(e,s){const i={};s&2&&(i.$$scope={dirty:s,ctx:e}),t.$set(i)},i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Ii(l){let t,n;return t=new z({props:{code:`files = [f"data/wikitext-103-raw/wiki.{split}.raw" for split in ["test", "train", "valid"]]
tokenizer.train(files, trainer)`,highlighted:`files = [<span class="hljs-string">f&quot;data/wikitext-103-raw/wiki.<span class="hljs-subst">{split}</span>.raw&quot;</span> <span class="hljs-keyword">for</span> split <span class="hljs-keyword">in</span> [<span class="hljs-string">&quot;test&quot;</span>, <span class="hljs-string">&quot;train&quot;</span>, <span class="hljs-string">&quot;valid&quot;</span>]]
tokenizer.train(files, trainer)`}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p:y,i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Oi(l){let t,n;return t=new E({props:{$$slots:{default:[Ii]},$$scope:{ctx:l}}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p(e,s){const i={};s&2&&(i.$$scope={dirty:s,ctx:e}),t.$set(i)},i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Ui(l){let t,n;return t=new z({props:{code:`let files = vec![
    "data/wikitext-103-raw/wiki.train.raw".into(),
    "data/wikitext-103-raw/wiki.test.raw".into(),
    "data/wikitext-103-raw/wiki.valid.raw".into(),
];
tokenizer.train_from_files(&mut trainer, files)?;`,highlighted:`<span class="hljs-keyword">let</span> <span class="hljs-variable">files</span> = <span class="hljs-built_in">vec!</span>[
    <span class="hljs-string">&quot;data/wikitext-103-raw/wiki.train.raw&quot;</span>.<span class="hljs-title function_ invoke__">into</span>(),
    <span class="hljs-string">&quot;data/wikitext-103-raw/wiki.test.raw&quot;</span>.<span class="hljs-title function_ invoke__">into</span>(),
    <span class="hljs-string">&quot;data/wikitext-103-raw/wiki.valid.raw&quot;</span>.<span class="hljs-title function_ invoke__">into</span>(),
];
tokenizer.<span class="hljs-title function_ invoke__">train_from_files</span>(&amp;<span class="hljs-keyword">mut</span> trainer, files)?;`}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p:y,i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Ki(l){let t,n;return t=new E({props:{$$slots:{default:[Ui]},$$scope:{ctx:l}}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p(e,s){const i={};s&2&&(i.$$scope={dirty:s,ctx:e}),t.$set(i)},i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Wi(l){let t,n;return t=new z({props:{code:'let files = ["test", "train", "valid"].map(split => `data/wikitext-103-raw/wiki.${split}.raw`);\ntokenizer.train(files, trainer);',highlighted:'<span class="hljs-keyword">let</span> files = [<span class="hljs-string">&quot;test&quot;</span>, <span class="hljs-string">&quot;train&quot;</span>, <span class="hljs-string">&quot;valid&quot;</span>].<span class="hljs-title function_">map</span>(<span class="hljs-function"><span class="hljs-params">split</span> =&gt;</span> <span class="hljs-string">`data/wikitext-103-raw/wiki.<span class="hljs-subst">${split}</span>.raw`</span>);\ntokenizer.<span class="hljs-title function_">train</span>(files, trainer);'}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p:y,i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Fi(l){let t,n;return t=new E({props:{$$slots:{default:[Wi]},$$scope:{ctx:l}}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p(e,s){const i={};s&2&&(i.$$scope={dirty:s,ctx:e}),t.$set(i)},i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Mi(l){let t,n;return t=new z({props:{code:'tokenizer.save("data/tokenizer-wiki.json")',highlighted:'tokenizer.save(<span class="hljs-string">&quot;data/tokenizer-wiki.json&quot;</span>)'}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p:y,i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Yi(l){let t,n;return t=new E({props:{$$slots:{default:[Mi]},$$scope:{ctx:l}}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p(e,s){const i={};s&2&&(i.$$scope={dirty:s,ctx:e}),t.$set(i)},i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Ri(l){let t,n;return t=new z({props:{code:'tokenizer.save("data/tokenizer-wiki.json", false)?;',highlighted:'tokenizer.<span class="hljs-title function_ invoke__">save</span>(<span class="hljs-string">&quot;data/tokenizer-wiki.json&quot;</span>, <span class="hljs-literal">false</span>)?;'}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p:y,i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Qi(l){let t,n;return t=new E({props:{$$slots:{default:[Ri]},$$scope:{ctx:l}}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p(e,s){const i={};s&2&&(i.$$scope={dirty:s,ctx:e}),t.$set(i)},i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Ji(l){let t,n;return t=new z({props:{code:'tokenizer.save("data/tokenizer-wiki.json");',highlighted:'tokenizer.<span class="hljs-title function_">save</span>(<span class="hljs-string">&quot;data/tokenizer-wiki.json&quot;</span>);'}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p:y,i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Vi(l){let t,n;return t=new E({props:{$$slots:{default:[Ji]},$$scope:{ctx:l}}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p(e,s){const i={};s&2&&(i.$$scope={dirty:s,ctx:e}),t.$set(i)},i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Gi(l){let t,n;return t=new z({props:{code:'tokenizer = Tokenizer.from_file("data/tokenizer-wiki.json")',highlighted:'tokenizer = Tokenizer.from_file(<span class="hljs-string">&quot;data/tokenizer-wiki.json&quot;</span>)'}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p:y,i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Xi(l){let t,n;return t=new E({props:{$$slots:{default:[Gi]},$$scope:{ctx:l}}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p(e,s){const i={};s&2&&(i.$$scope={dirty:s,ctx:e}),t.$set(i)},i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Zi(l){let t,n;return t=new z({props:{code:'let mut tokenizer = Tokenizer::from_file("data/tokenizer-wiki.json")?;',highlighted:'<span class="hljs-keyword">let</span> <span class="hljs-keyword">mut </span><span class="hljs-variable">tokenizer</span> = Tokenizer::<span class="hljs-title function_ invoke__">from_file</span>(<span class="hljs-string">&quot;data/tokenizer-wiki.json&quot;</span>)?;'}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p:y,i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function eu(l){let t,n;return t=new E({props:{$$slots:{default:[Zi]},$$scope:{ctx:l}}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p(e,s){const i={};s&2&&(i.$$scope={dirty:s,ctx:e}),t.$set(i)},i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function tu(l){let t,n;return t=new z({props:{code:'let tokenizer = Tokenizer.fromFile("data/tokenizer-wiki.json");',highlighted:'<span class="hljs-keyword">let</span> tokenizer = <span class="hljs-title class_">Tokenizer</span>.<span class="hljs-title function_">fromFile</span>(<span class="hljs-string">&quot;data/tokenizer-wiki.json&quot;</span>);'}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p:y,i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function nu(l){let t,n;return t=new E({props:{$$slots:{default:[tu]},$$scope:{ctx:l}}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p(e,s){const i={};s&2&&(i.$$scope={dirty:s,ctx:e}),t.$set(i)},i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function su(l){let t,n;return t=new z({props:{code:`output = tokenizer.encode("Hello, y'all! How are you \u{1F601} ?")`,highlighted:'output = tokenizer.encode(<span class="hljs-string">&quot;Hello, y&#x27;all! How are you \u{1F601} ?&quot;</span>)'}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p:y,i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function ou(l){let t,n;return t=new E({props:{$$slots:{default:[su]},$$scope:{ctx:l}}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p(e,s){const i={};s&2&&(i.$$scope={dirty:s,ctx:e}),t.$set(i)},i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function ru(l){let t,n;return t=new z({props:{code:`let output = tokenizer.encode("Hello, y'all! How are you \u{1F601} ?", true)?;`,highlighted:'<span class="hljs-keyword">let</span> <span class="hljs-variable">output</span> = tokenizer.<span class="hljs-title function_ invoke__">encode</span>(<span class="hljs-string">&quot;Hello, y&#x27;all! How are you \u{1F601} ?&quot;</span>, <span class="hljs-literal">true</span>)?;'}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p:y,i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function au(l){let t,n;return t=new E({props:{$$slots:{default:[ru]},$$scope:{ctx:l}}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p(e,s){const i={};s&2&&(i.$$scope={dirty:s,ctx:e}),t.$set(i)},i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function lu(l){let t,n;return t=new z({props:{code:`let { promisify } = require('util');
let encode = promisify(tokenizer.encode.bind(tokenizer));
var output = await encode("Hello, y'all! How are you \u{1F601} ?");`,highlighted:`<span class="hljs-keyword">let</span> { promisify } = <span class="hljs-built_in">require</span>(<span class="hljs-string">&#x27;util&#x27;</span>);
<span class="hljs-keyword">let</span> encode = <span class="hljs-title function_">promisify</span>(tokenizer.<span class="hljs-property">encode</span>.<span class="hljs-title function_">bind</span>(tokenizer));
<span class="hljs-keyword">var</span> output = <span class="hljs-keyword">await</span> <span class="hljs-title function_">encode</span>(<span class="hljs-string">&quot;Hello, y&#x27;all! How are you \u{1F601} ?&quot;</span>);`}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p:y,i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function iu(l){let t,n;return t=new E({props:{$$slots:{default:[lu]},$$scope:{ctx:l}}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p(e,s){const i={};s&2&&(i.$$scope={dirty:s,ctx:e}),t.$set(i)},i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function uu(l){let t,n;return t=new z({props:{code:`print(output.tokens)
# ["Hello", ",", "y", "'", "all", "!", "How", "are", "you", "[UNK]", "?"]`,highlighted:`<span class="hljs-built_in">print</span>(output.tokens)
<span class="hljs-comment"># [&quot;Hello&quot;, &quot;,&quot;, &quot;y&quot;, &quot;&#x27;&quot;, &quot;all&quot;, &quot;!&quot;, &quot;How&quot;, &quot;are&quot;, &quot;you&quot;, &quot;[UNK]&quot;, &quot;?&quot;]</span>`}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p:y,i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function pu(l){let t,n;return t=new E({props:{$$slots:{default:[uu]},$$scope:{ctx:l}}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p(e,s){const i={};s&2&&(i.$$scope={dirty:s,ctx:e}),t.$set(i)},i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function cu(l){let t,n;return t=new z({props:{code:`println!("{:?}", output.get_tokens());
// ["Hello", ",", "y", "'", "all", "!", "How", "are", "you", "[UNK]", "?",]`,highlighted:`<span class="hljs-built_in">println!</span>(<span class="hljs-string">&quot;{:?}&quot;</span>, output.<span class="hljs-title function_ invoke__">get_tokens</span>());
<span class="hljs-comment">// [&quot;Hello&quot;, &quot;,&quot;, &quot;y&quot;, &quot;&#x27;&quot;, &quot;all&quot;, &quot;!&quot;, &quot;How&quot;, &quot;are&quot;, &quot;you&quot;, &quot;[UNK]&quot;, &quot;?&quot;,]</span>`}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p:y,i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function fu(l){let t,n;return t=new E({props:{$$slots:{default:[cu]},$$scope:{ctx:l}}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p(e,s){const i={};s&2&&(i.$$scope={dirty:s,ctx:e}),t.$set(i)},i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function $u(l){let t,n;return t=new z({props:{code:`console.log(output.getTokens());
// ["Hello", ",", "y", "'", "all", "!", "How", "are", "you", "[UNK]", "?"]`,highlighted:`<span class="hljs-variable language_">console</span>.<span class="hljs-title function_">log</span>(output.<span class="hljs-title function_">getTokens</span>());
<span class="hljs-comment">// [&quot;Hello&quot;, &quot;,&quot;, &quot;y&quot;, &quot;&#x27;&quot;, &quot;all&quot;, &quot;!&quot;, &quot;How&quot;, &quot;are&quot;, &quot;you&quot;, &quot;[UNK]&quot;, &quot;?&quot;]</span>`}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p:y,i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function hu(l){let t,n;return t=new E({props:{$$slots:{default:[$u]},$$scope:{ctx:l}}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p(e,s){const i={};s&2&&(i.$$scope={dirty:s,ctx:e}),t.$set(i)},i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function mu(l){let t,n;return t=new z({props:{code:`print(output.ids)
# [27253, 16, 93, 11, 5097, 5, 7961, 5112, 6218, 0, 35]`,highlighted:`<span class="hljs-built_in">print</span>(output.ids)
<span class="hljs-comment"># [27253, 16, 93, 11, 5097, 5, 7961, 5112, 6218, 0, 35]</span>`}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p:y,i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function du(l){let t,n;return t=new E({props:{$$slots:{default:[mu]},$$scope:{ctx:l}}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p(e,s){const i={};s&2&&(i.$$scope={dirty:s,ctx:e}),t.$set(i)},i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function gu(l){let t,n;return t=new z({props:{code:`println!("{:?}", output.get_ids());
// [27253, 16, 93, 11, 5097, 5, 7961, 5112, 6218, 0, 35]`,highlighted:`<span class="hljs-built_in">println!</span>(<span class="hljs-string">&quot;{:?}&quot;</span>, output.<span class="hljs-title function_ invoke__">get_ids</span>());
<span class="hljs-comment">// [27253, 16, 93, 11, 5097, 5, 7961, 5112, 6218, 0, 35]</span>`}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p:y,i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function _u(l){let t,n;return t=new E({props:{$$slots:{default:[gu]},$$scope:{ctx:l}}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p(e,s){const i={};s&2&&(i.$$scope={dirty:s,ctx:e}),t.$set(i)},i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function ku(l){let t,n;return t=new z({props:{code:`console.log(output.getIds());
// [27253, 16, 93, 11, 5097, 5, 7961, 5112, 6218, 0, 35]`,highlighted:`<span class="hljs-variable language_">console</span>.<span class="hljs-title function_">log</span>(output.<span class="hljs-title function_">getIds</span>());
<span class="hljs-comment">// [27253, 16, 93, 11, 5097, 5, 7961, 5112, 6218, 0, 35]</span>`}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p:y,i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function wu(l){let t,n;return t=new E({props:{$$slots:{default:[ku]},$$scope:{ctx:l}}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p(e,s){const i={};s&2&&(i.$$scope={dirty:s,ctx:e}),t.$set(i)},i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function qu(l){let t,n;return t=new z({props:{code:`print(output.offsets[9])
# (26, 27)`,highlighted:`<span class="hljs-built_in">print</span>(output.offsets[<span class="hljs-number">9</span>])
<span class="hljs-comment"># (26, 27)</span>`}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p:y,i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function ju(l){let t,n;return t=new E({props:{$$slots:{default:[qu]},$$scope:{ctx:l}}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p(e,s){const i={};s&2&&(i.$$scope={dirty:s,ctx:e}),t.$set(i)},i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function vu(l){let t,n;return t=new z({props:{code:`println!("{:?}", output.get_offsets()[9]);
// (26, 30)`,highlighted:`<span class="hljs-built_in">println!</span>(<span class="hljs-string">&quot;{:?}&quot;</span>, output.<span class="hljs-title function_ invoke__">get_offsets</span>()[<span class="hljs-number">9</span>]);
<span class="hljs-comment">// (26, 30)</span>`}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p:y,i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function bu(l){let t,n;return t=new E({props:{$$slots:{default:[vu]},$$scope:{ctx:l}}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p(e,s){const i={};s&2&&(i.$$scope={dirty:s,ctx:e}),t.$set(i)},i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function zu(l){let t,n;return t=new z({props:{code:`let offsets = output.getOffsets();
console.log(offsets[9]);
// (26, 27)`,highlighted:`<span class="hljs-keyword">let</span> offsets = output.<span class="hljs-title function_">getOffsets</span>();
<span class="hljs-variable language_">console</span>.<span class="hljs-title function_">log</span>(offsets[<span class="hljs-number">9</span>]);
<span class="hljs-comment">// (26, 27)</span>`}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p:y,i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function yu(l){let t,n;return t=new E({props:{$$slots:{default:[zu]},$$scope:{ctx:l}}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p(e,s){const i={};s&2&&(i.$$scope={dirty:s,ctx:e}),t.$set(i)},i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Eu(l){let t,n;return t=new z({props:{code:`sentence = "Hello, y'all! How are you \u{1F601} ?"
sentence[26:27]
# "\u{1F601}"`,highlighted:`sentence = <span class="hljs-string">&quot;Hello, y&#x27;all! How are you \u{1F601} ?&quot;</span>
sentence[<span class="hljs-number">26</span>:<span class="hljs-number">27</span>]
<span class="hljs-comment"># &quot;\u{1F601}&quot;</span>`}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p:y,i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Pu(l){let t,n;return t=new E({props:{$$slots:{default:[Eu]},$$scope:{ctx:l}}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p(e,s){const i={};s&2&&(i.$$scope={dirty:s,ctx:e}),t.$set(i)},i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Tu(l){let t,n;return t=new z({props:{code:`let sentence = "Hello, y'all! How are you \u{1F601} ?";
println!("{}", &sentence[26..30]);
// "\u{1F601}"`,highlighted:`<span class="hljs-keyword">let</span> <span class="hljs-variable">sentence</span> = <span class="hljs-string">&quot;Hello, y&#x27;all! How are you \u{1F601} ?&quot;</span>;
<span class="hljs-built_in">println!</span>(<span class="hljs-string">&quot;{}&quot;</span>, &amp;sentence[<span class="hljs-number">26</span>..<span class="hljs-number">30</span>]);
<span class="hljs-comment">// &quot;\u{1F601}&quot;</span>`}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p:y,i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Su(l){let t,n;return t=new E({props:{$$slots:{default:[Tu]},$$scope:{ctx:l}}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p(e,s){const i={};s&2&&(i.$$scope={dirty:s,ctx:e}),t.$set(i)},i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Hu(l){let t,n;return t=new z({props:{code:`let { slice } = require("tokenizers/bindings/utils");
let sentence = "Hello, y'all! How are you \u{1F601} ?"
let [start, end] = offsets[9];
console.log(slice(sentence, start, end));
// "\u{1F601}"`,highlighted:`<span class="hljs-keyword">let</span> { slice } = <span class="hljs-built_in">require</span>(<span class="hljs-string">&quot;tokenizers/bindings/utils&quot;</span>);
<span class="hljs-keyword">let</span> sentence = <span class="hljs-string">&quot;Hello, y&#x27;all! How are you \u{1F601} ?&quot;</span>
<span class="hljs-keyword">let</span> [start, end] = offsets[<span class="hljs-number">9</span>];
<span class="hljs-variable language_">console</span>.<span class="hljs-title function_">log</span>(<span class="hljs-title function_">slice</span>(sentence, start, end));
<span class="hljs-comment">// &quot;\u{1F601}&quot;</span>`}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p:y,i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function xu(l){let t,n;return t=new E({props:{$$slots:{default:[Hu]},$$scope:{ctx:l}}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p(e,s){const i={};s&2&&(i.$$scope={dirty:s,ctx:e}),t.$set(i)},i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Cu(l){let t,n;return t=new z({props:{code:`tokenizer.token_to_id("[SEP]")
# 2`,highlighted:`tokenizer.token_to_id(<span class="hljs-string">&quot;[SEP]&quot;</span>)
<span class="hljs-comment"># 2</span>`}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p:y,i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Au(l){let t,n;return t=new E({props:{$$slots:{default:[Cu]},$$scope:{ctx:l}}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p(e,s){const i={};s&2&&(i.$$scope={dirty:s,ctx:e}),t.$set(i)},i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Du(l){let t,n;return t=new z({props:{code:`println!("{}", tokenizer.token_to_id("[SEP]").unwrap());
// 2`,highlighted:`<span class="hljs-built_in">println!</span>(<span class="hljs-string">&quot;{}&quot;</span>, tokenizer.<span class="hljs-title function_ invoke__">token_to_id</span>(<span class="hljs-string">&quot;[SEP]&quot;</span>).<span class="hljs-title function_ invoke__">unwrap</span>());
<span class="hljs-comment">// 2</span>`}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p:y,i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Lu(l){let t,n;return t=new E({props:{$$slots:{default:[Du]},$$scope:{ctx:l}}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p(e,s){const i={};s&2&&(i.$$scope={dirty:s,ctx:e}),t.$set(i)},i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Bu(l){let t,n;return t=new z({props:{code:`console.log(tokenizer.tokenToId("[SEP]"));
// 2`,highlighted:`<span class="hljs-variable language_">console</span>.<span class="hljs-title function_">log</span>(tokenizer.<span class="hljs-title function_">tokenToId</span>(<span class="hljs-string">&quot;[SEP]&quot;</span>));
<span class="hljs-comment">// 2</span>`}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p:y,i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Nu(l){let t,n;return t=new E({props:{$$slots:{default:[Bu]},$$scope:{ctx:l}}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p(e,s){const i={};s&2&&(i.$$scope={dirty:s,ctx:e}),t.$set(i)},i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Iu(l){let t,n;return t=new z({props:{code:`from tokenizers.processors import TemplateProcessing
tokenizer.post_processor = TemplateProcessing(
    single="[CLS] $A [SEP]",
    pair="[CLS] $A [SEP] $B:1 [SEP]:1",
    special_tokens=[
        ("[CLS]", tokenizer.token_to_id("[CLS]")),
        ("[SEP]", tokenizer.token_to_id("[SEP]")),
    ],
)`,highlighted:`<span class="hljs-keyword">from</span> tokenizers.processors <span class="hljs-keyword">import</span> TemplateProcessing
tokenizer.post_processor = TemplateProcessing(
    single=<span class="hljs-string">&quot;[CLS] $A [SEP]&quot;</span>,
    pair=<span class="hljs-string">&quot;[CLS] $A [SEP] $B:1 [SEP]:1&quot;</span>,
    special_tokens=[
        (<span class="hljs-string">&quot;[CLS]&quot;</span>, tokenizer.token_to_id(<span class="hljs-string">&quot;[CLS]&quot;</span>)),
        (<span class="hljs-string">&quot;[SEP]&quot;</span>, tokenizer.token_to_id(<span class="hljs-string">&quot;[SEP]&quot;</span>)),
    ],
)`}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p:y,i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Ou(l){let t,n;return t=new E({props:{$$slots:{default:[Iu]},$$scope:{ctx:l}}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p(e,s){const i={};s&2&&(i.$$scope={dirty:s,ctx:e}),t.$set(i)},i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Uu(l){let t,n;return t=new z({props:{code:`use tokenizers::processors::template::TemplateProcessing;
let special_tokens = vec![
    ("[CLS]", tokenizer.token_to_id("[CLS]").unwrap()),
    ("[SEP]", tokenizer.token_to_id("[SEP]").unwrap()),
];
tokenizer.with_post_processor(
    TemplateProcessing::builder()
        .try_single("[CLS] $A [SEP]")
        .unwrap()
        .try_pair("[CLS] $A [SEP] $B:1 [SEP]:1")
        .unwrap()
        .special_tokens(special_tokens)
        .build()?,
);`,highlighted:`<span class="hljs-keyword">use</span> tokenizers::processors::template::TemplateProcessing;
<span class="hljs-keyword">let</span> <span class="hljs-variable">special_tokens</span> = <span class="hljs-built_in">vec!</span>[
    (<span class="hljs-string">&quot;[CLS]&quot;</span>, tokenizer.<span class="hljs-title function_ invoke__">token_to_id</span>(<span class="hljs-string">&quot;[CLS]&quot;</span>).<span class="hljs-title function_ invoke__">unwrap</span>()),
    (<span class="hljs-string">&quot;[SEP]&quot;</span>, tokenizer.<span class="hljs-title function_ invoke__">token_to_id</span>(<span class="hljs-string">&quot;[SEP]&quot;</span>).<span class="hljs-title function_ invoke__">unwrap</span>()),
];
tokenizer.<span class="hljs-title function_ invoke__">with_post_processor</span>(
    TemplateProcessing::<span class="hljs-title function_ invoke__">builder</span>()
        .<span class="hljs-title function_ invoke__">try_single</span>(<span class="hljs-string">&quot;[CLS] $A [SEP]&quot;</span>)
        .<span class="hljs-title function_ invoke__">unwrap</span>()
        .<span class="hljs-title function_ invoke__">try_pair</span>(<span class="hljs-string">&quot;[CLS] $A [SEP] $B:1 [SEP]:1&quot;</span>)
        .<span class="hljs-title function_ invoke__">unwrap</span>()
        .<span class="hljs-title function_ invoke__">special_tokens</span>(special_tokens)
        .<span class="hljs-title function_ invoke__">build</span>()?,
);`}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p:y,i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Ku(l){let t,n;return t=new E({props:{$$slots:{default:[Uu]},$$scope:{ctx:l}}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p(e,s){const i={};s&2&&(i.$$scope={dirty:s,ctx:e}),t.$set(i)},i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Wu(l){let t,n;return t=new z({props:{code:`let { templateProcessing } = require("tokenizers/bindings/post-processors");
tokenizer.setPostProcessor(templateProcessing(
    "[CLS] $A [SEP]",
    "[CLS] $A [SEP] $B:1 [SEP]:1",
    [
        ["[CLS]", tokenizer.tokenToId("[CLS]")],
        ["[SEP]", tokenizer.tokenToId("[SEP]")],
    ],
));`,highlighted:`<span class="hljs-keyword">let</span> { templateProcessing } = <span class="hljs-built_in">require</span>(<span class="hljs-string">&quot;tokenizers/bindings/post-processors&quot;</span>);
tokenizer.<span class="hljs-title function_">setPostProcessor</span>(<span class="hljs-title function_">templateProcessing</span>(
    <span class="hljs-string">&quot;[CLS] $A [SEP]&quot;</span>,
    <span class="hljs-string">&quot;[CLS] $A [SEP] $B:1 [SEP]:1&quot;</span>,
    [
        [<span class="hljs-string">&quot;[CLS]&quot;</span>, tokenizer.<span class="hljs-title function_">tokenToId</span>(<span class="hljs-string">&quot;[CLS]&quot;</span>)],
        [<span class="hljs-string">&quot;[SEP]&quot;</span>, tokenizer.<span class="hljs-title function_">tokenToId</span>(<span class="hljs-string">&quot;[SEP]&quot;</span>)],
    ],
));`}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p:y,i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Fu(l){let t,n;return t=new E({props:{$$slots:{default:[Wu]},$$scope:{ctx:l}}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p(e,s){const i={};s&2&&(i.$$scope={dirty:s,ctx:e}),t.$set(i)},i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Mu(l){let t,n;return t=new z({props:{code:`output = tokenizer.encode("Hello, y'all! How are you \u{1F601} ?")
print(output.tokens)
# ["[CLS]", "Hello", ",", "y", "'", "all", "!", "How", "are", "you", "[UNK]", "?", "[SEP]"]`,highlighted:`output = tokenizer.encode(<span class="hljs-string">&quot;Hello, y&#x27;all! How are you \u{1F601} ?&quot;</span>)
<span class="hljs-built_in">print</span>(output.tokens)
<span class="hljs-comment"># [&quot;[CLS]&quot;, &quot;Hello&quot;, &quot;,&quot;, &quot;y&quot;, &quot;&#x27;&quot;, &quot;all&quot;, &quot;!&quot;, &quot;How&quot;, &quot;are&quot;, &quot;you&quot;, &quot;[UNK]&quot;, &quot;?&quot;, &quot;[SEP]&quot;]</span>`}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p:y,i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Yu(l){let t,n;return t=new E({props:{$$slots:{default:[Mu]},$$scope:{ctx:l}}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p(e,s){const i={};s&2&&(i.$$scope={dirty:s,ctx:e}),t.$set(i)},i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Ru(l){let t,n;return t=new z({props:{code:`let output = tokenizer.encode("Hello, y'all! How are you \u{1F601} ?", true)?;
println!("{:?}", output.get_tokens());
// ["[CLS]", "Hello", ",", "y", "'", "all", "!", "How", "are", "you", "[UNK]", "?", "[SEP]"]`,highlighted:`<span class="hljs-keyword">let</span> <span class="hljs-variable">output</span> = tokenizer.<span class="hljs-title function_ invoke__">encode</span>(<span class="hljs-string">&quot;Hello, y&#x27;all! How are you \u{1F601} ?&quot;</span>, <span class="hljs-literal">true</span>)?;
<span class="hljs-built_in">println!</span>(<span class="hljs-string">&quot;{:?}&quot;</span>, output.<span class="hljs-title function_ invoke__">get_tokens</span>());
<span class="hljs-comment">// [&quot;[CLS]&quot;, &quot;Hello&quot;, &quot;,&quot;, &quot;y&quot;, &quot;&#x27;&quot;, &quot;all&quot;, &quot;!&quot;, &quot;How&quot;, &quot;are&quot;, &quot;you&quot;, &quot;[UNK]&quot;, &quot;?&quot;, &quot;[SEP]&quot;]</span>`}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p:y,i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Qu(l){let t,n;return t=new E({props:{$$slots:{default:[Ru]},$$scope:{ctx:l}}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p(e,s){const i={};s&2&&(i.$$scope={dirty:s,ctx:e}),t.$set(i)},i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Ju(l){let t,n;return t=new z({props:{code:`var output = await encode("Hello, y'all! How are you \u{1F601} ?");
console.log(output.getTokens());
// ["[CLS]", "Hello", ",", "y", "'", "all", "!", "How", "are", "you", "[UNK]", "?", "[SEP]"]`,highlighted:`<span class="hljs-keyword">var</span> output = <span class="hljs-keyword">await</span> <span class="hljs-title function_">encode</span>(<span class="hljs-string">&quot;Hello, y&#x27;all! How are you \u{1F601} ?&quot;</span>);
<span class="hljs-variable language_">console</span>.<span class="hljs-title function_">log</span>(output.<span class="hljs-title function_">getTokens</span>());
<span class="hljs-comment">// [&quot;[CLS]&quot;, &quot;Hello&quot;, &quot;,&quot;, &quot;y&quot;, &quot;&#x27;&quot;, &quot;all&quot;, &quot;!&quot;, &quot;How&quot;, &quot;are&quot;, &quot;you&quot;, &quot;[UNK]&quot;, &quot;?&quot;, &quot;[SEP]&quot;]</span>`}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p:y,i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Vu(l){let t,n;return t=new E({props:{$$slots:{default:[Ju]},$$scope:{ctx:l}}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p(e,s){const i={};s&2&&(i.$$scope={dirty:s,ctx:e}),t.$set(i)},i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Gu(l){let t,n;return t=new z({props:{code:`output = tokenizer.encode("Hello, y'all!", "How are you \u{1F601} ?")
print(output.tokens)
# ["[CLS]", "Hello", ",", "y", "'", "all", "!", "[SEP]", "How", "are", "you", "[UNK]", "?", "[SEP]"]`,highlighted:`output = tokenizer.encode(<span class="hljs-string">&quot;Hello, y&#x27;all!&quot;</span>, <span class="hljs-string">&quot;How are you \u{1F601} ?&quot;</span>)
<span class="hljs-built_in">print</span>(output.tokens)
<span class="hljs-comment"># [&quot;[CLS]&quot;, &quot;Hello&quot;, &quot;,&quot;, &quot;y&quot;, &quot;&#x27;&quot;, &quot;all&quot;, &quot;!&quot;, &quot;[SEP]&quot;, &quot;How&quot;, &quot;are&quot;, &quot;you&quot;, &quot;[UNK]&quot;, &quot;?&quot;, &quot;[SEP]&quot;]</span>`}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p:y,i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Xu(l){let t,n;return t=new E({props:{$$slots:{default:[Gu]},$$scope:{ctx:l}}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p(e,s){const i={};s&2&&(i.$$scope={dirty:s,ctx:e}),t.$set(i)},i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Zu(l){let t,n;return t=new z({props:{code:`let output = tokenizer.encode(("Hello, y'all!", "How are you \u{1F601} ?"), true)?;
println!("{:?}", output.get_tokens());
// ["[CLS]", "Hello", ",", "y", "'", "all", "!", "[SEP]", "How", "are", "you", "[UNK]", "?", "[SEP]"]`,highlighted:`<span class="hljs-keyword">let</span> <span class="hljs-variable">output</span> = tokenizer.<span class="hljs-title function_ invoke__">encode</span>((<span class="hljs-string">&quot;Hello, y&#x27;all!&quot;</span>, <span class="hljs-string">&quot;How are you \u{1F601} ?&quot;</span>), <span class="hljs-literal">true</span>)?;
<span class="hljs-built_in">println!</span>(<span class="hljs-string">&quot;{:?}&quot;</span>, output.<span class="hljs-title function_ invoke__">get_tokens</span>());
<span class="hljs-comment">// [&quot;[CLS]&quot;, &quot;Hello&quot;, &quot;,&quot;, &quot;y&quot;, &quot;&#x27;&quot;, &quot;all&quot;, &quot;!&quot;, &quot;[SEP]&quot;, &quot;How&quot;, &quot;are&quot;, &quot;you&quot;, &quot;[UNK]&quot;, &quot;?&quot;, &quot;[SEP]&quot;]</span>`}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p:y,i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function ep(l){let t,n;return t=new E({props:{$$slots:{default:[Zu]},$$scope:{ctx:l}}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p(e,s){const i={};s&2&&(i.$$scope={dirty:s,ctx:e}),t.$set(i)},i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function tp(l){let t,n;return t=new z({props:{code:`var output = await encode("Hello, y'all!", "How are you \u{1F601} ?");
console.log(output.getTokens());
// ["[CLS]", "Hello", ",", "y", "'", "all", "!", "[SEP]", "How", "are", "you", "[UNK]", "?", "[SEP]"]`,highlighted:`<span class="hljs-keyword">var</span> output = <span class="hljs-keyword">await</span> <span class="hljs-title function_">encode</span>(<span class="hljs-string">&quot;Hello, y&#x27;all!&quot;</span>, <span class="hljs-string">&quot;How are you \u{1F601} ?&quot;</span>);
<span class="hljs-variable language_">console</span>.<span class="hljs-title function_">log</span>(output.<span class="hljs-title function_">getTokens</span>());
<span class="hljs-comment">// [&quot;[CLS]&quot;, &quot;Hello&quot;, &quot;,&quot;, &quot;y&quot;, &quot;&#x27;&quot;, &quot;all&quot;, &quot;!&quot;, &quot;[SEP]&quot;, &quot;How&quot;, &quot;are&quot;, &quot;you&quot;, &quot;[UNK]&quot;, &quot;?&quot;, &quot;[SEP]&quot;]</span>`}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p:y,i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function np(l){let t,n;return t=new E({props:{$$slots:{default:[tp]},$$scope:{ctx:l}}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p(e,s){const i={};s&2&&(i.$$scope={dirty:s,ctx:e}),t.$set(i)},i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function sp(l){let t,n;return t=new z({props:{code:`print(output.type_ids)
# [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1]`,highlighted:`<span class="hljs-built_in">print</span>(output.type_ids)
<span class="hljs-comment"># [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1]</span>`}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p:y,i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function op(l){let t,n;return t=new E({props:{$$slots:{default:[sp]},$$scope:{ctx:l}}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p(e,s){const i={};s&2&&(i.$$scope={dirty:s,ctx:e}),t.$set(i)},i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function rp(l){let t,n;return t=new z({props:{code:`println!("{:?}", output.get_type_ids());
// [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1]`,highlighted:`<span class="hljs-built_in">println!</span>(<span class="hljs-string">&quot;{:?}&quot;</span>, output.<span class="hljs-title function_ invoke__">get_type_ids</span>());
<span class="hljs-comment">// [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1]</span>`}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p:y,i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function ap(l){let t,n;return t=new E({props:{$$slots:{default:[rp]},$$scope:{ctx:l}}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p(e,s){const i={};s&2&&(i.$$scope={dirty:s,ctx:e}),t.$set(i)},i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function lp(l){let t,n;return t=new z({props:{code:`console.log(output.getTypeIds());
// [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1]`,highlighted:`<span class="hljs-variable language_">console</span>.<span class="hljs-title function_">log</span>(output.<span class="hljs-title function_">getTypeIds</span>());
<span class="hljs-comment">// [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1]</span>`}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p:y,i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function ip(l){let t,n;return t=new E({props:{$$slots:{default:[lp]},$$scope:{ctx:l}}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p(e,s){const i={};s&2&&(i.$$scope={dirty:s,ctx:e}),t.$set(i)},i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function up(l){let t,n;return t=new z({props:{code:`output = tokenizer.encode_batch(["Hello, y'all!", "How are you \u{1F601} ?"])`,highlighted:'output = tokenizer.encode_batch([<span class="hljs-string">&quot;Hello, y&#x27;all!&quot;</span>, <span class="hljs-string">&quot;How are you \u{1F601} ?&quot;</span>])'}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p:y,i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function pp(l){let t,n;return t=new E({props:{$$slots:{default:[up]},$$scope:{ctx:l}}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p(e,s){const i={};s&2&&(i.$$scope={dirty:s,ctx:e}),t.$set(i)},i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function cp(l){let t,n;return t=new z({props:{code:`let output = tokenizer.encode_batch(vec!["Hello, y'all!", "How are you \u{1F601} ?"], true)?;`,highlighted:'<span class="hljs-keyword">let</span> <span class="hljs-variable">output</span> = tokenizer.<span class="hljs-title function_ invoke__">encode_batch</span>(<span class="hljs-built_in">vec!</span>[<span class="hljs-string">&quot;Hello, y&#x27;all!&quot;</span>, <span class="hljs-string">&quot;How are you \u{1F601} ?&quot;</span>], <span class="hljs-literal">true</span>)?;'}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p:y,i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function fp(l){let t,n;return t=new E({props:{$$slots:{default:[cp]},$$scope:{ctx:l}}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p(e,s){const i={};s&2&&(i.$$scope={dirty:s,ctx:e}),t.$set(i)},i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function $p(l){let t,n;return t=new z({props:{code:`let encodeBatch = promisify(tokenizer.encodeBatch.bind(tokenizer));
var output = await encodeBatch(["Hello, y'all!", "How are you \u{1F601} ?"]);`,highlighted:`<span class="hljs-keyword">let</span> encodeBatch = <span class="hljs-title function_">promisify</span>(tokenizer.<span class="hljs-property">encodeBatch</span>.<span class="hljs-title function_">bind</span>(tokenizer));
<span class="hljs-keyword">var</span> output = <span class="hljs-keyword">await</span> <span class="hljs-title function_">encodeBatch</span>([<span class="hljs-string">&quot;Hello, y&#x27;all!&quot;</span>, <span class="hljs-string">&quot;How are you \u{1F601} ?&quot;</span>]);`}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p:y,i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function hp(l){let t,n;return t=new E({props:{$$slots:{default:[$p]},$$scope:{ctx:l}}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p(e,s){const i={};s&2&&(i.$$scope={dirty:s,ctx:e}),t.$set(i)},i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function mp(l){let t,n;return t=new z({props:{code:`output = tokenizer.encode_batch(
    [["Hello, y'all!", "How are you \u{1F601} ?"], ["Hello to you too!", "I'm fine, thank you!"]]
)`,highlighted:`output = tokenizer.encode_batch(
    [[<span class="hljs-string">&quot;Hello, y&#x27;all!&quot;</span>, <span class="hljs-string">&quot;How are you \u{1F601} ?&quot;</span>], [<span class="hljs-string">&quot;Hello to you too!&quot;</span>, <span class="hljs-string">&quot;I&#x27;m fine, thank you!&quot;</span>]]
)`}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p:y,i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function dp(l){let t,n;return t=new E({props:{$$slots:{default:[mp]},$$scope:{ctx:l}}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p(e,s){const i={};s&2&&(i.$$scope={dirty:s,ctx:e}),t.$set(i)},i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function gp(l){let t,n;return t=new z({props:{code:`let output = tokenizer.encode_batch(
    vec![
        ("Hello, y'all!", "How are you \u{1F601} ?"),
        ("Hello to you too!", "I'm fine, thank you!"),
    ],
    true,
)?;`,highlighted:`<span class="hljs-keyword">let</span> <span class="hljs-variable">output</span> = tokenizer.<span class="hljs-title function_ invoke__">encode_batch</span>(
    <span class="hljs-built_in">vec!</span>[
        (<span class="hljs-string">&quot;Hello, y&#x27;all!&quot;</span>, <span class="hljs-string">&quot;How are you \u{1F601} ?&quot;</span>),
        (<span class="hljs-string">&quot;Hello to you too!&quot;</span>, <span class="hljs-string">&quot;I&#x27;m fine, thank you!&quot;</span>),
    ],
    <span class="hljs-literal">true</span>,
)?;`}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p:y,i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function _p(l){let t,n;return t=new E({props:{$$slots:{default:[gp]},$$scope:{ctx:l}}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p(e,s){const i={};s&2&&(i.$$scope={dirty:s,ctx:e}),t.$set(i)},i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function kp(l){let t,n;return t=new z({props:{code:`var output = await encodeBatch(
    [["Hello, y'all!", "How are you \u{1F601} ?"], ["Hello to you too!", "I'm fine, thank you!"]]
);`,highlighted:`<span class="hljs-keyword">var</span> output = <span class="hljs-keyword">await</span> <span class="hljs-title function_">encodeBatch</span>(
    [[<span class="hljs-string">&quot;Hello, y&#x27;all!&quot;</span>, <span class="hljs-string">&quot;How are you \u{1F601} ?&quot;</span>], [<span class="hljs-string">&quot;Hello to you too!&quot;</span>, <span class="hljs-string">&quot;I&#x27;m fine, thank you!&quot;</span>]]
);`}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p:y,i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function wp(l){let t,n;return t=new E({props:{$$slots:{default:[kp]},$$scope:{ctx:l}}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p(e,s){const i={};s&2&&(i.$$scope={dirty:s,ctx:e}),t.$set(i)},i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function qp(l){let t,n;return t=new z({props:{code:'tokenizer.enable_padding(pad_id=3, pad_token="[PAD]")',highlighted:'tokenizer.enable_padding(pad_id=<span class="hljs-number">3</span>, pad_token=<span class="hljs-string">&quot;[PAD]&quot;</span>)'}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p:y,i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function jp(l){let t,n;return t=new E({props:{$$slots:{default:[qp]},$$scope:{ctx:l}}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p(e,s){const i={};s&2&&(i.$$scope={dirty:s,ctx:e}),t.$set(i)},i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function vp(l){let t,n;return t=new z({props:{code:`use tokenizers::PaddingParams;
tokenizer.with_padding(Some(PaddingParams {
    pad_id: 3,
    pad_token: "[PAD]".to_string(),
    ..PaddingParams::default()
}));`,highlighted:`<span class="hljs-keyword">use</span> tokenizers::PaddingParams;
tokenizer.<span class="hljs-title function_ invoke__">with_padding</span>(<span class="hljs-title function_ invoke__">Some</span>(PaddingParams {
    pad_id: <span class="hljs-number">3</span>,
    pad_token: <span class="hljs-string">&quot;[PAD]&quot;</span>.<span class="hljs-title function_ invoke__">to_string</span>(),
    ..PaddingParams::<span class="hljs-title function_ invoke__">default</span>()
}));`}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p:y,i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function bp(l){let t,n;return t=new E({props:{$$slots:{default:[vp]},$$scope:{ctx:l}}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p(e,s){const i={};s&2&&(i.$$scope={dirty:s,ctx:e}),t.$set(i)},i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function zp(l){let t,n;return t=new z({props:{code:'tokenizer.setPadding({ padId: 3, padToken: "[PAD]" });',highlighted:'tokenizer.<span class="hljs-title function_">setPadding</span>({ <span class="hljs-attr">padId</span>: <span class="hljs-number">3</span>, <span class="hljs-attr">padToken</span>: <span class="hljs-string">&quot;[PAD]&quot;</span> });'}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p:y,i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function yp(l){let t,n;return t=new E({props:{$$slots:{default:[zp]},$$scope:{ctx:l}}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p(e,s){const i={};s&2&&(i.$$scope={dirty:s,ctx:e}),t.$set(i)},i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Ep(l){let t,n;return t=new z({props:{code:`output = tokenizer.encode_batch(["Hello, y'all!", "How are you \u{1F601} ?"])
print(output[1].tokens)
# ["[CLS]", "How", "are", "you", "[UNK]", "?", "[SEP]", "[PAD]"]`,highlighted:`output = tokenizer.encode_batch([<span class="hljs-string">&quot;Hello, y&#x27;all!&quot;</span>, <span class="hljs-string">&quot;How are you \u{1F601} ?&quot;</span>])
<span class="hljs-built_in">print</span>(output[<span class="hljs-number">1</span>].tokens)
<span class="hljs-comment"># [&quot;[CLS]&quot;, &quot;How&quot;, &quot;are&quot;, &quot;you&quot;, &quot;[UNK]&quot;, &quot;?&quot;, &quot;[SEP]&quot;, &quot;[PAD]&quot;]</span>`}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p:y,i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Pp(l){let t,n;return t=new E({props:{$$slots:{default:[Ep]},$$scope:{ctx:l}}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p(e,s){const i={};s&2&&(i.$$scope={dirty:s,ctx:e}),t.$set(i)},i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Tp(l){let t,n;return t=new z({props:{code:`let output = tokenizer.encode_batch(vec!["Hello, y'all!", "How are you \u{1F601} ?"], true)?;
println!("{:?}", output[1].get_tokens());
// ["[CLS]", "How", "are", "you", "[UNK]", "?", "[SEP]", "[PAD]"]`,highlighted:`<span class="hljs-keyword">let</span> <span class="hljs-variable">output</span> = tokenizer.<span class="hljs-title function_ invoke__">encode_batch</span>(<span class="hljs-built_in">vec!</span>[<span class="hljs-string">&quot;Hello, y&#x27;all!&quot;</span>, <span class="hljs-string">&quot;How are you \u{1F601} ?&quot;</span>], <span class="hljs-literal">true</span>)?;
<span class="hljs-built_in">println!</span>(<span class="hljs-string">&quot;{:?}&quot;</span>, output[<span class="hljs-number">1</span>].<span class="hljs-title function_ invoke__">get_tokens</span>());
<span class="hljs-comment">// [&quot;[CLS]&quot;, &quot;How&quot;, &quot;are&quot;, &quot;you&quot;, &quot;[UNK]&quot;, &quot;?&quot;, &quot;[SEP]&quot;, &quot;[PAD]&quot;]</span>`}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p:y,i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Sp(l){let t,n;return t=new E({props:{$$slots:{default:[Tp]},$$scope:{ctx:l}}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p(e,s){const i={};s&2&&(i.$$scope={dirty:s,ctx:e}),t.$set(i)},i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Hp(l){let t,n;return t=new z({props:{code:`var output = await encodeBatch(["Hello, y'all!", "How are you \u{1F601} ?"]);
console.log(output[1].getTokens());
// ["[CLS]", "How", "are", "you", "[UNK]", "?", "[SEP]", "[PAD]"]`,highlighted:`<span class="hljs-keyword">var</span> output = <span class="hljs-keyword">await</span> <span class="hljs-title function_">encodeBatch</span>([<span class="hljs-string">&quot;Hello, y&#x27;all!&quot;</span>, <span class="hljs-string">&quot;How are you \u{1F601} ?&quot;</span>]);
<span class="hljs-variable language_">console</span>.<span class="hljs-title function_">log</span>(output[<span class="hljs-number">1</span>].<span class="hljs-title function_">getTokens</span>());
<span class="hljs-comment">// [&quot;[CLS]&quot;, &quot;How&quot;, &quot;are&quot;, &quot;you&quot;, &quot;[UNK]&quot;, &quot;?&quot;, &quot;[SEP]&quot;, &quot;[PAD]&quot;]</span>`}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p:y,i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function xp(l){let t,n;return t=new E({props:{$$slots:{default:[Hp]},$$scope:{ctx:l}}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p(e,s){const i={};s&2&&(i.$$scope={dirty:s,ctx:e}),t.$set(i)},i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Cp(l){let t,n;return t=new z({props:{code:`print(output[1].attention_mask)
# [1, 1, 1, 1, 1, 1, 1, 0]`,highlighted:`<span class="hljs-built_in">print</span>(output[<span class="hljs-number">1</span>].attention_mask)
<span class="hljs-comment"># [1, 1, 1, 1, 1, 1, 1, 0]</span>`}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p:y,i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Ap(l){let t,n;return t=new E({props:{$$slots:{default:[Cp]},$$scope:{ctx:l}}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p(e,s){const i={};s&2&&(i.$$scope={dirty:s,ctx:e}),t.$set(i)},i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Dp(l){let t,n;return t=new z({props:{code:`println!("{:?}", output[1].get_attention_mask());
// [1, 1, 1, 1, 1, 1, 1, 0]`,highlighted:`<span class="hljs-built_in">println!</span>(<span class="hljs-string">&quot;{:?}&quot;</span>, output[<span class="hljs-number">1</span>].<span class="hljs-title function_ invoke__">get_attention_mask</span>());
<span class="hljs-comment">// [1, 1, 1, 1, 1, 1, 1, 0]</span>`}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p:y,i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Lp(l){let t,n;return t=new E({props:{$$slots:{default:[Dp]},$$scope:{ctx:l}}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p(e,s){const i={};s&2&&(i.$$scope={dirty:s,ctx:e}),t.$set(i)},i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Bp(l){let t,n;return t=new z({props:{code:`console.log(output[1].getAttentionMask());
// [1, 1, 1, 1, 1, 1, 1, 0]`,highlighted:`<span class="hljs-variable language_">console</span>.<span class="hljs-title function_">log</span>(output[<span class="hljs-number">1</span>].<span class="hljs-title function_">getAttentionMask</span>());
<span class="hljs-comment">// [1, 1, 1, 1, 1, 1, 1, 0]</span>`}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p:y,i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Np(l){let t,n;return t=new E({props:{$$slots:{default:[Bp]},$$scope:{ctx:l}}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p(e,s){const i={};s&2&&(i.$$scope={dirty:s,ctx:e}),t.$set(i)},i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Ip(l){let t,n,e,s,i,T,B,K,C,D,O,re,Pe,W,N,ae,L,Y,ee,te,Te,F,jt,R,le,vt,ge,U,_e,A,Q,ne,se,bt,ie,oe,ke;return s=new Ee({}),N=new z({props:{code:`from tokenizers import Tokenizer

tokenizer = Tokenizer.from_pretrained("bert-base-uncased")`,highlighted:`<span class="hljs-keyword">from</span> tokenizers <span class="hljs-keyword">import</span> Tokenizer

tokenizer = Tokenizer.from_pretrained(<span class="hljs-string">&quot;bert-base-uncased&quot;</span>)`}}),te=new Ee({}),U=new z({props:{code:`from tokenizers import BertWordPieceTokenizer

tokenizer = BertWordPieceTokenizer("bert-base-uncased-vocab.txt", lowercase=True)`,highlighted:`<span class="hljs-keyword">from</span> tokenizers <span class="hljs-keyword">import</span> BertWordPieceTokenizer

tokenizer = BertWordPieceTokenizer(<span class="hljs-string">&quot;bert-base-uncased-vocab.txt&quot;</span>, lowercase=<span class="hljs-literal">True</span>)`}}),oe=new z({props:{code:"wget https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt",highlighted:"wget https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt"}}),{c(){t=_("h3"),n=_("a"),e=_("span"),f(s.$$.fragment),i=j(),T=_("span"),B=d("Using a pretrained tokenizer"),K=j(),C=_("p"),D=d(`You can load any tokenizer from the Hugging Face Hub as long as a
`),O=_("code"),re=d("tokenizer.json"),Pe=d(" file is available in the repository."),W=j(),f(N.$$.fragment),ae=j(),L=_("h3"),Y=_("a"),ee=_("span"),f(te.$$.fragment),Te=j(),F=_("span"),jt=d("Importing a pretrained tokenizer from legacy vocabulary files"),R=j(),le=_("p"),vt=d(`You can also import a pretrained tokenizer directly in, as long as you
have its vocabulary file. For instance, here is how to import the
classic pretrained BERT tokenizer:`),ge=j(),f(U.$$.fragment),_e=j(),A=_("p"),Q=d("as long as you have downloaded the file "),ne=_("code"),se=d("bert-base-uncased-vocab.txt"),bt=d(" with"),ie=j(),f(oe.$$.fragment),this.h()},l(b){t=k(b,"H3",{class:!0});var S=w(t);n=k(S,"A",{id:!0,class:!0,href:!0});var ue=w(n);e=k(ue,"SPAN",{});var Dt=w(e);$(s.$$.fragment,Dt),Dt.forEach(r),ue.forEach(r),i=v(S),T=k(S,"SPAN",{});var Lt=w(T);B=g(Lt,"Using a pretrained tokenizer"),Lt.forEach(r),S.forEach(r),K=v(b),C=k(b,"P",{});var we=w(C);D=g(we,`You can load any tokenizer from the Hugging Face Hub as long as a
`),O=k(we,"CODE",{});var M=w(O);re=g(M,"tokenizer.json"),M.forEach(r),Pe=g(we," file is available in the repository."),we.forEach(r),W=v(b),$(N.$$.fragment,b),ae=v(b),L=k(b,"H3",{class:!0});var pe=w(L);Y=k(pe,"A",{id:!0,class:!0,href:!0});var Bt=w(Y);ee=k(Bt,"SPAN",{});var Nt=w(ee);$(te.$$.fragment,Nt),Nt.forEach(r),Bt.forEach(r),Te=v(pe),F=k(pe,"SPAN",{});var Se=w(F);jt=g(Se,"Importing a pretrained tokenizer from legacy vocabulary files"),Se.forEach(r),pe.forEach(r),R=v(b),le=k(b,"P",{});var It=w(le);vt=g(It,`You can also import a pretrained tokenizer directly in, as long as you
have its vocabulary file. For instance, here is how to import the
classic pretrained BERT tokenizer:`),It.forEach(r),ge=v(b),$(U.$$.fragment,b),_e=v(b),A=k(b,"P",{});var He=w(A);Q=g(He,"as long as you have downloaded the file "),ne=k(He,"CODE",{});var xe=w(ne);se=g(xe,"bert-base-uncased-vocab.txt"),xe.forEach(r),bt=g(He," with"),He.forEach(r),ie=v(b),$(oe.$$.fragment,b),this.h()},h(){P(n,"id","using-a-pretrained-tokenizer"),P(n,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),P(n,"href","#using-a-pretrained-tokenizer"),P(t,"class","relative group"),P(Y,"id","importing-a-pretrained-tokenizer-from-legacy-vocabulary-files"),P(Y,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),P(Y,"href","#importing-a-pretrained-tokenizer-from-legacy-vocabulary-files"),P(L,"class","relative group")},m(b,S){q(b,t,S),a(t,n),a(n,e),h(s,e,null),a(t,i),a(t,T),a(T,B),q(b,K,S),q(b,C,S),a(C,D),a(C,O),a(O,re),a(C,Pe),q(b,W,S),h(N,b,S),q(b,ae,S),q(b,L,S),a(L,Y),a(Y,ee),h(te,ee,null),a(L,Te),a(L,F),a(F,jt),q(b,R,S),q(b,le,S),a(le,vt),q(b,ge,S),h(U,b,S),q(b,_e,S),q(b,A,S),a(A,Q),a(A,ne),a(ne,se),a(A,bt),q(b,ie,S),h(oe,b,S),ke=!0},p:y,i(b){ke||(p(s.$$.fragment,b),p(N.$$.fragment,b),p(te.$$.fragment,b),p(U.$$.fragment,b),p(oe.$$.fragment,b),ke=!0)},o(b){c(s.$$.fragment,b),c(N.$$.fragment,b),c(te.$$.fragment,b),c(U.$$.fragment,b),c(oe.$$.fragment,b),ke=!1},d(b){b&&r(t),m(s),b&&r(K),b&&r(C),b&&r(W),m(N,b),b&&r(ae),b&&r(L),m(te),b&&r(R),b&&r(le),b&&r(ge),m(U,b),b&&r(_e),b&&r(A),b&&r(ie),m(oe,b)}}}function Op(l){let t,n;return t=new E({props:{$$slots:{default:[Ip]},$$scope:{ctx:l}}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p(e,s){const i={};s&2&&(i.$$scope={dirty:s,ctx:e}),t.$set(i)},i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Up(l){let t,n,e,s,i,T,B,K,C,D,O,re,Pe,W,N,ae,L,Y,ee,te,Te,F,jt,R,le,vt,ge,U,_e,A,Q,ne,se,bt,ie,oe,ke,b,S,ue,Dt,Lt,we,M,pe,Bt,Nt,Se,It,He,xe,xo,$s,ce,Co,Vt,Ao,Do,Gt,Lo,Bo,hs,Ce,ms,zt,No,Xt,Io,ds,Ae,gs,J,Oo,Zt,Uo,Ko,en,Wo,Fo,tn,Mo,Yo,_s,De,ks,Le,Ro,nn,Qo,Jo,ws,Be,qs,Ne,Vo,sn,Go,Xo,js,Ie,vs,Oe,Zo,on,er,tr,bs,Ue,zs,fe,nr,rn,sr,or,an,rr,ar,ys,Ke,Es,qe,We,ln,yt,lr,un,ir,Ps,Fe,ur,pn,pr,cr,Ts,Me,Ss,$e,fr,cn,$r,hr,fn,mr,dr,Hs,he,gr,$n,_r,kr,hn,wr,qr,xs,Ye,Cs,Re,jr,mn,vr,br,As,Qe,Ds,V,zr,dn,yr,Er,gn,Pr,Tr,_n,Sr,Hr,Ls,Je,Bs,Ot,xr,Ns,Ve,Is,je,Ge,kn,Et,Cr,wn,Ar,Os,G,Dr,qn,Lr,Br,jn,Nr,Ir,vn,Or,Ur,Us,X,Kr,bn,Wr,Fr,zn,Mr,Yr,yn,Rr,Qr,Ks,Xe,Ws,Ut,Jr,Fs,Ze,Ms,me,Vr,En,Gr,Xr,Pn,Zr,ea,Ys,x,ta,Tn,na,sa,Sn,oa,ra,Hn,aa,la,xn,ia,ua,Cn,pa,ca,An,fa,$a,Dn,ha,ma,Rs,Kt,da,Qs,Wt,ga,Js,et,Vs,tt,_a,Ln,ka,wa,Gs,nt,Xs,Ft,qa,Zs,st,eo,ot,ja,Bn,va,ba,to,ve,rt,Nn,Pt,za,In,ya,no,at,Ea,On,Pa,Ta,so,lt,oo,it,Sa,Un,Ha,xa,ro,ut,Ca,Kn,Aa,Da,ao,pt,lo,Z,La,Wn,Ba,Na,Fn,Ia,Oa,Mn,Ua,Ka,io,ct,uo,de,Wa,Yn,Fa,Ma,Rn,Ya,Ra,po,ft,co,$t,Qa,Qn,Ja,Va,fo,ht,$o,be,mt,Jn,Tt,Ga,Vn,Xa,ho,dt,mo;return T=new Ee({}),L=new Ee({}),U=new z({props:{code:`wget https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-103-raw-v1.zip
unzip wikitext-103-raw-v1.zip`,highlighted:`wget https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-103-raw-v1.zip
unzip wikitext-103-raw-v1.zip`}}),se=new Ee({}),Ce=new H({props:{python:!0,rust:!0,node:!0,$$slots:{node:[zi],rust:[vi],python:[qi]},$$scope:{ctx:l}}}),Ae=new H({props:{python:!0,rust:!0,node:!0,$$slots:{node:[Hi],rust:[Ti],python:[Ei]},$$scope:{ctx:l}}}),De=new ki({props:{$$slots:{default:[xi]},$$scope:{ctx:l}}}),Be=new H({props:{python:!0,rust:!0,node:!0,$$slots:{node:[Ni],rust:[Li],python:[Ai]},$$scope:{ctx:l}}}),Ie=new H({props:{python:!0,rust:!0,node:!0,$$slots:{node:[Fi],rust:[Ki],python:[Oi]},$$scope:{ctx:l}}}),Ue=new H({props:{python:!0,rust:!0,node:!0,$$slots:{node:[Vi],rust:[Qi],python:[Yi]},$$scope:{ctx:l}}}),Ke=new H({props:{python:!0,rust:!0,node:!0,$$slots:{node:[nu],rust:[eu],python:[Xi]},$$scope:{ctx:l}}}),yt=new Ee({}),Me=new H({props:{python:!0,rust:!0,node:!0,$$slots:{node:[iu],rust:[au],python:[ou]},$$scope:{ctx:l}}}),Ye=new H({props:{python:!0,rust:!0,node:!0,$$slots:{node:[hu],rust:[fu],python:[pu]},$$scope:{ctx:l}}}),Qe=new H({props:{python:!0,rust:!0,node:!0,$$slots:{node:[wu],rust:[_u],python:[du]},$$scope:{ctx:l}}}),Je=new H({props:{python:!0,rust:!0,node:!0,$$slots:{node:[yu],rust:[bu],python:[ju]},$$scope:{ctx:l}}}),Ve=new H({props:{python:!0,rust:!0,node:!0,$$slots:{node:[xu],rust:[Su],python:[Pu]},$$scope:{ctx:l}}}),Et=new Ee({}),Xe=new H({props:{python:!0,rust:!0,node:!0,$$slots:{node:[Nu],rust:[Lu],python:[Au]},$$scope:{ctx:l}}}),Ze=new H({props:{python:!0,rust:!0,node:!0,$$slots:{node:[Fu],rust:[Ku],python:[Ou]},$$scope:{ctx:l}}}),et=new H({props:{python:!0,rust:!0,node:!0,$$slots:{node:[Vu],rust:[Qu],python:[Yu]},$$scope:{ctx:l}}}),nt=new H({props:{python:!0,rust:!0,node:!0,$$slots:{node:[np],rust:[ep],python:[Xu]},$$scope:{ctx:l}}}),st=new H({props:{python:!0,rust:!0,node:!0,$$slots:{node:[ip],rust:[ap],python:[op]},$$scope:{ctx:l}}}),Pt=new Ee({}),lt=new H({props:{python:!0,rust:!0,node:!0,$$slots:{node:[hp],rust:[fp],python:[pp]},$$scope:{ctx:l}}}),pt=new H({props:{python:!0,rust:!0,node:!0,$$slots:{node:[wp],rust:[_p],python:[dp]},$$scope:{ctx:l}}}),ct=new H({props:{python:!0,rust:!0,node:!0,$$slots:{node:[yp],rust:[bp],python:[jp]},$$scope:{ctx:l}}}),ft=new H({props:{python:!0,rust:!0,node:!0,$$slots:{node:[xp],rust:[Sp],python:[Pp]},$$scope:{ctx:l}}}),ht=new H({props:{python:!0,rust:!0,node:!0,$$slots:{node:[Np],rust:[Lp],python:[Ap]},$$scope:{ctx:l}}}),Tt=new Ee({}),dt=new H({props:{python:!0,rust:!0,node:!0,$$slots:{python:[Op]},$$scope:{ctx:l}}}),{c(){t=_("meta"),n=j(),e=_("h1"),s=_("a"),i=_("span"),f(T.$$.fragment),B=j(),K=_("span"),C=d("Quicktour"),D=j(),O=_("p"),re=d(`Let\u2019s have a quick look at the \u{1F917} Tokenizers library features. The
library provides an implementation of today\u2019s most used tokenizers that
is both easy to use and blazing fast.`),Pe=j(),W=_("h2"),N=_("a"),ae=_("span"),f(L.$$.fragment),Y=j(),ee=_("span"),te=d("Build a tokenizer from scratch"),Te=j(),F=_("p"),jt=d(`To illustrate how fast the \u{1F917} Tokenizers library is, let\u2019s train a new
tokenizer on `),R=_("a"),le=d("wikitext-103"),vt=d(`
(516M of text) in just a few seconds. First things first, you will need
to download this dataset and unzip it with:`),ge=j(),f(U.$$.fragment),_e=j(),A=_("h3"),Q=_("a"),ne=_("span"),f(se.$$.fragment),bt=j(),ie=_("span"),oe=d("Training the tokenizer"),ke=j(),b=_("p"),S=d(`In this tour, we will build and train a Byte-Pair Encoding (BPE)
tokenizer. For more information about the different type of tokenizers,
check out this `),ue=_("a"),Dt=d("guide"),Lt=d(` in
the \u{1F917} Transformers documentation. Here, training the tokenizer means it
will learn merge rules by:`),we=j(),M=_("ul"),pe=_("li"),Bt=d(`Start with all the characters present in the training corpus as
tokens.`),Nt=j(),Se=_("li"),It=d("Identify the most common pair of tokens and merge it into one token."),He=j(),xe=_("li"),xo=d(`Repeat until the vocabulary (e.g., the number of tokens) has reached
the size we want.`),$s=j(),ce=_("p"),Co=d("The main API of the library is the "),Vt=_("code"),Ao=d("class"),Do=j(),Gt=_("code"),Lo=d("Tokenizer"),Bo=d(`, here is how
we instantiate one with a BPE model:`),hs=j(),f(Ce.$$.fragment),ms=j(),zt=_("p"),No=d(`To train our tokenizer on the wikitext files, we will need to
instantiate a [trainer]{.title-ref}, in this case a
`),Xt=_("code"),Io=d("BpeTrainer"),ds=j(),f(Ae.$$.fragment),gs=j(),J=_("p"),Oo=d("We can set the training arguments like "),Zt=_("code"),Uo=d("vocab_size"),Ko=d(" or "),en=_("code"),Wo=d("min_frequency"),Fo=d(` (here
left at their default values of 30,000 and 0) but the most important
part is to give the `),tn=_("code"),Mo=d("special_tokens"),Yo=d(` we
plan to use later on (they are not used at all during training) so that
they get inserted in the vocabulary.`),_s=j(),f(De.$$.fragment),ks=j(),Le=_("p"),Ro=d(`We could train our tokenizer right now, but it wouldn\u2019t be optimal.
Without a pre-tokenizer that will split our inputs into words, we might
get tokens that overlap several words: for instance we could get an
`),nn=_("code"),Qo=d('"it is"'),Jo=d(` token since those two words
often appear next to each other. Using a pre-tokenizer will ensure no
token is bigger than a word returned by the pre-tokenizer. Here we want
to train a subword BPE tokenizer, and we will use the easiest
pre-tokenizer possible by splitting on whitespace.`),ws=j(),f(Be.$$.fragment),qs=j(),Ne=_("p"),Vo=d("Now, we can just call the "),sn=_("code"),Go=d("Tokenizer.train"),Xo=d(" method with any list of files we want to use:"),js=j(),f(Ie.$$.fragment),vs=j(),Oe=_("p"),Zo=d(`This should only take a few seconds to train our tokenizer on the full
wikitext dataset! To save the tokenizer in one file that contains all
its configuration and vocabulary, just use the
`),on=_("code"),er=d("Tokenizer.save"),tr=d(" method:"),bs=j(),f(Ue.$$.fragment),zs=j(),fe=_("p"),nr=d(`and you can reload your tokenizer from that file with the
`),rn=_("code"),sr=d("Tokenizer.from_file"),or=j(),an=_("code"),rr=d("classmethod"),ar=d(":"),ys=j(),f(Ke.$$.fragment),Es=j(),qe=_("h3"),We=_("a"),ln=_("span"),f(yt.$$.fragment),lr=j(),un=_("span"),ir=d("Using the tokenizer"),Ps=j(),Fe=_("p"),ur=d(`Now that we have trained a tokenizer, we can use it on any text we want
with the `),pn=_("code"),pr=d("Tokenizer.encode"),cr=d(" method:"),Ts=j(),f(Me.$$.fragment),Ss=j(),$e=_("p"),fr=d(`This applied the full pipeline of the tokenizer on the text, returning
an `),cn=_("code"),$r=d("Encoding"),hr=d(` object. To learn more
about this pipeline, and how to apply (or customize) parts of it, check out `),fn=_("code"),mr=d("this page <pipeline>"),dr=d("."),Hs=j(),he=_("p"),gr=d("This "),$n=_("code"),_r=d("Encoding"),kr=d(` object then has all the
attributes you need for your deep learning model (or other). The
`),hn=_("code"),wr=d("tokens"),qr=d(` attribute contains the
segmentation of your text in tokens:`),xs=j(),f(Ye.$$.fragment),Cs=j(),Re=_("p"),jr=d("Similarly, the "),mn=_("code"),vr=d("ids"),br=d(` attribute will
contain the index of each of those tokens in the tokenizer\u2019s
vocabulary:`),As=j(),f(Qe.$$.fragment),Ds=j(),V=_("p"),zr=d(`An important feature of the \u{1F917} Tokenizers library is that it comes with
full alignment tracking, meaning you can always get the part of your
original sentence that corresponds to a given token. Those are stored in
the `),dn=_("code"),yr=d("offsets"),Er=d(` attribute of our
`),gn=_("code"),Pr=d("Encoding"),Tr=d(` object. For instance, let\u2019s
assume we would want to find back what caused the
`),_n=_("code"),Sr=d('"[UNK]"'),Hr=d(` token to appear, which is the
token at index 9 in the list, we can just ask for the offset at the
index:`),Ls=j(),f(Je.$$.fragment),Bs=j(),Ot=_("p"),xr=d(`and those are the indices that correspond to the emoji in the original
sentence:`),Ns=j(),f(Ve.$$.fragment),Is=j(),je=_("h3"),Ge=_("a"),kn=_("span"),f(Et.$$.fragment),Cr=j(),wn=_("span"),Ar=d("Post-processing"),Os=j(),G=_("p"),Dr=d(`We might want our tokenizer to automatically add special tokens, like
`),qn=_("code"),Lr=d('"[CLS]"'),Br=d(" or "),jn=_("code"),Nr=d('"[SEP]"'),Ir=d(`. To do this, we use a post-processor.
`),vn=_("code"),Or=d("TemplateProcessing"),Ur=d(` is the most
commonly used, you just have to specify a template for the processing of
single sentences and pairs of sentences, along with the special tokens
and their IDs.`),Us=j(),X=_("p"),Kr=d("When we built our tokenizer, we set "),bn=_("code"),Wr=d('"[CLS]"'),Fr=d(" and "),zn=_("code"),Mr=d('"[SEP]"'),Yr=d(` in positions 1
and 2 of our list of special tokens, so this should be their IDs. To
double-check, we can use the `),yn=_("code"),Rr=d("Tokenizer.token_to_id"),Qr=d(" method:"),Ks=j(),f(Xe.$$.fragment),Ws=j(),Ut=_("p"),Jr=d(`Here is how we can set the post-processing to give us the traditional
BERT inputs:`),Fs=j(),f(Ze.$$.fragment),Ms=j(),me=_("p"),Vr=d(`Let\u2019s go over this snippet of code in more details. First we specify
the template for single sentences: those should have the form
`),En=_("code"),Gr=d('"[CLS] $A [SEP]"'),Xr=d(` where
`),Pn=_("code"),Zr=d("$A"),ea=d(" represents our sentence."),Ys=j(),x=_("p"),ta=d(`Then, we specify the template for sentence pairs, which should have the
form `),Tn=_("code"),na=d('"[CLS] $A [SEP] $B [SEP]"'),sa=d(` where
`),Sn=_("code"),oa=d("$A"),ra=d(` represents the first sentence and
`),Hn=_("code"),aa=d("$B"),la=d(` the second one. The
`),xn=_("code"),ia=d(":1"),ua=d(" added in the template represent the "),Cn=_("code"),pa=d("type IDs"),ca=d(` we want for each part of our input: it defaults
to 0 for everything (which is why we don\u2019t have
`),An=_("code"),fa=d("$A:0"),$a=d(`) and here we set it to 1 for the
tokens of the second sentence and the last `),Dn=_("code"),ha=d('"[SEP]"'),ma=d(" token."),Rs=j(),Kt=_("p"),da=d(`Lastly, we specify the special tokens we used and their IDs in our
tokenizer\u2019s vocabulary.`),Qs=j(),Wt=_("p"),ga=d(`To check out this worked properly, let\u2019s try to encode the same
sentence as before:`),Js=j(),f(et.$$.fragment),Vs=j(),tt=_("p"),_a=d(`To check the results on a pair of sentences, we just pass the two
sentences to `),Ln=_("code"),ka=d("Tokenizer.encode"),wa=d(":"),Gs=j(),f(nt.$$.fragment),Xs=j(),Ft=_("p"),qa=d("You can then check the type IDs attributed to each token is correct with"),Zs=j(),f(st.$$.fragment),eo=j(),ot=_("p"),ja=d("If you save your tokenizer with "),Bn=_("code"),va=d("Tokenizer.save"),ba=d(", the post-processor will be saved along."),to=j(),ve=_("h3"),rt=_("a"),Nn=_("span"),f(Pt.$$.fragment),za=j(),In=_("span"),ya=d("Encoding multiple sentences in a batch"),no=j(),at=_("p"),Ea=d(`To get the full speed of the \u{1F917} Tokenizers library, it\u2019s best to
process your texts by batches by using the
`),On=_("code"),Pa=d("Tokenizer.encode_batch"),Ta=d(" method:"),so=j(),f(lt.$$.fragment),oo=j(),it=_("p"),Sa=d("The output is then a list of "),Un=_("code"),Ha=d("Encoding"),xa=d(`
objects like the ones we saw before. You can process together as many
texts as you like, as long as it fits in memory.`),ro=j(),ut=_("p"),Ca=d(`To process a batch of sentences pairs, pass two lists to the
`),Kn=_("code"),Aa=d("Tokenizer.encode_batch"),Da=d(` method: the
list of sentences A and the list of sentences B:`),ao=j(),f(pt.$$.fragment),lo=j(),Z=_("p"),La=d(`When encoding multiple sentences, you can automatically pad the outputs
to the longest sentence present by using
`),Wn=_("code"),Ba=d("Tokenizer.enable_padding"),Na=d(`, with the
`),Fn=_("code"),Ia=d("pad_token"),Oa=d(` and its ID (which we can
double-check the id for the padding token with
`),Mn=_("code"),Ua=d("Tokenizer.token_to_id"),Ka=d(" like before):"),io=j(),f(ct.$$.fragment),uo=j(),de=_("p"),Wa=d("We can set the "),Yn=_("code"),Fa=d("direction"),Ma=d(` of the padding
(defaults to the right) or a given `),Rn=_("code"),Ya=d("length"),Ra=d(` if we want to pad every sample to that specific number (here
we leave it unset to pad to the size of the longest text).`),po=j(),f(ft.$$.fragment),co=j(),$t=_("p"),Qa=d("In this case, the "),Qn=_("code"),Ja=d("attention mask"),Va=d(` generated by the
tokenizer takes the padding into account:`),fo=j(),f(ht.$$.fragment),$o=j(),be=_("h2"),mt=_("a"),Jn=_("span"),f(Tt.$$.fragment),Ga=j(),Vn=_("span"),Xa=d("Pretrained"),ho=j(),f(dt.$$.fragment),this.h()},l(o){const u=mi('[data-svelte="svelte-1phssyn"]',document.head);t=k(u,"META",{name:!0,content:!0}),u.forEach(r),n=v(o),e=k(o,"H1",{class:!0});var St=w(e);s=k(St,"A",{id:!0,class:!0,href:!0});var Gn=w(s);i=k(Gn,"SPAN",{});var Xn=w(i);$(T.$$.fragment,Xn),Xn.forEach(r),Gn.forEach(r),B=v(St),K=k(St,"SPAN",{});var Zn=w(K);C=g(Zn,"Quicktour"),Zn.forEach(r),St.forEach(r),D=v(o),O=k(o,"P",{});var es=w(O);re=g(es,`Let\u2019s have a quick look at the \u{1F917} Tokenizers library features. The
library provides an implementation of today\u2019s most used tokenizers that
is both easy to use and blazing fast.`),es.forEach(r),Pe=v(o),W=k(o,"H2",{class:!0});var Ht=w(W);N=k(Ht,"A",{id:!0,class:!0,href:!0});var ts=w(N);ae=k(ts,"SPAN",{});var ns=w(ae);$(L.$$.fragment,ns),ns.forEach(r),ts.forEach(r),Y=v(Ht),ee=k(Ht,"SPAN",{});var ss=w(ee);te=g(ss,"Build a tokenizer from scratch"),ss.forEach(r),Ht.forEach(r),Te=v(o),F=k(o,"P",{});var xt=w(F);jt=g(xt,`To illustrate how fast the \u{1F917} Tokenizers library is, let\u2019s train a new
tokenizer on `),R=k(xt,"A",{href:!0,rel:!0});var os=w(R);le=g(os,"wikitext-103"),os.forEach(r),vt=g(xt,`
(516M of text) in just a few seconds. First things first, you will need
to download this dataset and unzip it with:`),xt.forEach(r),ge=v(o),$(U.$$.fragment,o),_e=v(o),A=k(o,"H3",{class:!0});var Ct=w(A);Q=k(Ct,"A",{id:!0,class:!0,href:!0});var rs=w(Q);ne=k(rs,"SPAN",{});var as=w(ne);$(se.$$.fragment,as),as.forEach(r),rs.forEach(r),bt=v(Ct),ie=k(Ct,"SPAN",{});var ls=w(ie);oe=g(ls,"Training the tokenizer"),ls.forEach(r),Ct.forEach(r),ke=v(o),b=k(o,"P",{});var At=w(b);S=g(At,`In this tour, we will build and train a Byte-Pair Encoding (BPE)
tokenizer. For more information about the different type of tokenizers,
check out this `),ue=k(At,"A",{href:!0,rel:!0});var is=w(ue);Dt=g(is,"guide"),is.forEach(r),Lt=g(At,` in
the \u{1F917} Transformers documentation. Here, training the tokenizer means it
will learn merge rules by:`),At.forEach(r),we=v(o),M=k(o,"UL",{});var ze=w(M);pe=k(ze,"LI",{});var us=w(pe);Bt=g(us,`Start with all the characters present in the training corpus as
tokens.`),us.forEach(r),Nt=v(ze),Se=k(ze,"LI",{});var ps=w(Se);It=g(ps,"Identify the most common pair of tokens and merge it into one token."),ps.forEach(r),He=v(ze),xe=k(ze,"LI",{});var cs=w(xe);xo=g(cs,`Repeat until the vocabulary (e.g., the number of tokens) has reached
the size we want.`),cs.forEach(r),ze.forEach(r),$s=v(o),ce=k(o,"P",{});var ye=w(ce);Co=g(ye,"The main API of the library is the "),Vt=k(ye,"CODE",{});var fs=w(Vt);Ao=g(fs,"class"),fs.forEach(r),Do=v(ye),Gt=k(ye,"CODE",{});var el=w(Gt);Lo=g(el,"Tokenizer"),el.forEach(r),Bo=g(ye,`, here is how
we instantiate one with a BPE model:`),ye.forEach(r),hs=v(o),$(Ce.$$.fragment,o),ms=v(o),zt=k(o,"P",{});var Za=w(zt);No=g(Za,`To train our tokenizer on the wikitext files, we will need to
instantiate a [trainer]{.title-ref}, in this case a
`),Xt=k(Za,"CODE",{});var tl=w(Xt);Io=g(tl,"BpeTrainer"),tl.forEach(r),Za.forEach(r),ds=v(o),$(Ae.$$.fragment,o),gs=v(o),J=k(o,"P",{});var gt=w(J);Oo=g(gt,"We can set the training arguments like "),Zt=k(gt,"CODE",{});var nl=w(Zt);Uo=g(nl,"vocab_size"),nl.forEach(r),Ko=g(gt," or "),en=k(gt,"CODE",{});var sl=w(en);Wo=g(sl,"min_frequency"),sl.forEach(r),Fo=g(gt,` (here
left at their default values of 30,000 and 0) but the most important
part is to give the `),tn=k(gt,"CODE",{});var ol=w(tn);Mo=g(ol,"special_tokens"),ol.forEach(r),Yo=g(gt,` we
plan to use later on (they are not used at all during training) so that
they get inserted in the vocabulary.`),gt.forEach(r),_s=v(o),$(De.$$.fragment,o),ks=v(o),Le=k(o,"P",{});var go=w(Le);Ro=g(go,`We could train our tokenizer right now, but it wouldn\u2019t be optimal.
Without a pre-tokenizer that will split our inputs into words, we might
get tokens that overlap several words: for instance we could get an
`),nn=k(go,"CODE",{});var rl=w(nn);Qo=g(rl,'"it is"'),rl.forEach(r),Jo=g(go,` token since those two words
often appear next to each other. Using a pre-tokenizer will ensure no
token is bigger than a word returned by the pre-tokenizer. Here we want
to train a subword BPE tokenizer, and we will use the easiest
pre-tokenizer possible by splitting on whitespace.`),go.forEach(r),ws=v(o),$(Be.$$.fragment,o),qs=v(o),Ne=k(o,"P",{});var _o=w(Ne);Vo=g(_o,"Now, we can just call the "),sn=k(_o,"CODE",{});var al=w(sn);Go=g(al,"Tokenizer.train"),al.forEach(r),Xo=g(_o," method with any list of files we want to use:"),_o.forEach(r),js=v(o),$(Ie.$$.fragment,o),vs=v(o),Oe=k(o,"P",{});var ko=w(Oe);Zo=g(ko,`This should only take a few seconds to train our tokenizer on the full
wikitext dataset! To save the tokenizer in one file that contains all
its configuration and vocabulary, just use the
`),on=k(ko,"CODE",{});var ll=w(on);er=g(ll,"Tokenizer.save"),ll.forEach(r),tr=g(ko," method:"),ko.forEach(r),bs=v(o),$(Ue.$$.fragment,o),zs=v(o),fe=k(o,"P",{});var Mt=w(fe);nr=g(Mt,`and you can reload your tokenizer from that file with the
`),rn=k(Mt,"CODE",{});var il=w(rn);sr=g(il,"Tokenizer.from_file"),il.forEach(r),or=v(Mt),an=k(Mt,"CODE",{});var ul=w(an);rr=g(ul,"classmethod"),ul.forEach(r),ar=g(Mt,":"),Mt.forEach(r),ys=v(o),$(Ke.$$.fragment,o),Es=v(o),qe=k(o,"H3",{class:!0});var wo=w(qe);We=k(wo,"A",{id:!0,class:!0,href:!0});var pl=w(We);ln=k(pl,"SPAN",{});var cl=w(ln);$(yt.$$.fragment,cl),cl.forEach(r),pl.forEach(r),lr=v(wo),un=k(wo,"SPAN",{});var fl=w(un);ir=g(fl,"Using the tokenizer"),fl.forEach(r),wo.forEach(r),Ps=v(o),Fe=k(o,"P",{});var qo=w(Fe);ur=g(qo,`Now that we have trained a tokenizer, we can use it on any text we want
with the `),pn=k(qo,"CODE",{});var $l=w(pn);pr=g($l,"Tokenizer.encode"),$l.forEach(r),cr=g(qo," method:"),qo.forEach(r),Ts=v(o),$(Me.$$.fragment,o),Ss=v(o),$e=k(o,"P",{});var Yt=w($e);fr=g(Yt,`This applied the full pipeline of the tokenizer on the text, returning
an `),cn=k(Yt,"CODE",{});var hl=w(cn);$r=g(hl,"Encoding"),hl.forEach(r),hr=g(Yt,` object. To learn more
about this pipeline, and how to apply (or customize) parts of it, check out `),fn=k(Yt,"CODE",{});var ml=w(fn);mr=g(ml,"this page <pipeline>"),ml.forEach(r),dr=g(Yt,"."),Yt.forEach(r),Hs=v(o),he=k(o,"P",{});var Rt=w(he);gr=g(Rt,"This "),$n=k(Rt,"CODE",{});var dl=w($n);_r=g(dl,"Encoding"),dl.forEach(r),kr=g(Rt,` object then has all the
attributes you need for your deep learning model (or other). The
`),hn=k(Rt,"CODE",{});var gl=w(hn);wr=g(gl,"tokens"),gl.forEach(r),qr=g(Rt,` attribute contains the
segmentation of your text in tokens:`),Rt.forEach(r),xs=v(o),$(Ye.$$.fragment,o),Cs=v(o),Re=k(o,"P",{});var jo=w(Re);jr=g(jo,"Similarly, the "),mn=k(jo,"CODE",{});var _l=w(mn);vr=g(_l,"ids"),_l.forEach(r),br=g(jo,` attribute will
contain the index of each of those tokens in the tokenizer\u2019s
vocabulary:`),jo.forEach(r),As=v(o),$(Qe.$$.fragment,o),Ds=v(o),V=k(o,"P",{});var _t=w(V);zr=g(_t,`An important feature of the \u{1F917} Tokenizers library is that it comes with
full alignment tracking, meaning you can always get the part of your
original sentence that corresponds to a given token. Those are stored in
the `),dn=k(_t,"CODE",{});var kl=w(dn);yr=g(kl,"offsets"),kl.forEach(r),Er=g(_t,` attribute of our
`),gn=k(_t,"CODE",{});var wl=w(gn);Pr=g(wl,"Encoding"),wl.forEach(r),Tr=g(_t,` object. For instance, let\u2019s
assume we would want to find back what caused the
`),_n=k(_t,"CODE",{});var ql=w(_n);Sr=g(ql,'"[UNK]"'),ql.forEach(r),Hr=g(_t,` token to appear, which is the
token at index 9 in the list, we can just ask for the offset at the
index:`),_t.forEach(r),Ls=v(o),$(Je.$$.fragment,o),Bs=v(o),Ot=k(o,"P",{});var jl=w(Ot);xr=g(jl,`and those are the indices that correspond to the emoji in the original
sentence:`),jl.forEach(r),Ns=v(o),$(Ve.$$.fragment,o),Is=v(o),je=k(o,"H3",{class:!0});var vo=w(je);Ge=k(vo,"A",{id:!0,class:!0,href:!0});var vl=w(Ge);kn=k(vl,"SPAN",{});var bl=w(kn);$(Et.$$.fragment,bl),bl.forEach(r),vl.forEach(r),Cr=v(vo),wn=k(vo,"SPAN",{});var zl=w(wn);Ar=g(zl,"Post-processing"),zl.forEach(r),vo.forEach(r),Os=v(o),G=k(o,"P",{});var kt=w(G);Dr=g(kt,`We might want our tokenizer to automatically add special tokens, like
`),qn=k(kt,"CODE",{});var yl=w(qn);Lr=g(yl,'"[CLS]"'),yl.forEach(r),Br=g(kt," or "),jn=k(kt,"CODE",{});var El=w(jn);Nr=g(El,'"[SEP]"'),El.forEach(r),Ir=g(kt,`. To do this, we use a post-processor.
`),vn=k(kt,"CODE",{});var Pl=w(vn);Or=g(Pl,"TemplateProcessing"),Pl.forEach(r),Ur=g(kt,` is the most
commonly used, you just have to specify a template for the processing of
single sentences and pairs of sentences, along with the special tokens
and their IDs.`),kt.forEach(r),Us=v(o),X=k(o,"P",{});var wt=w(X);Kr=g(wt,"When we built our tokenizer, we set "),bn=k(wt,"CODE",{});var Tl=w(bn);Wr=g(Tl,'"[CLS]"'),Tl.forEach(r),Fr=g(wt," and "),zn=k(wt,"CODE",{});var Sl=w(zn);Mr=g(Sl,'"[SEP]"'),Sl.forEach(r),Yr=g(wt,` in positions 1
and 2 of our list of special tokens, so this should be their IDs. To
double-check, we can use the `),yn=k(wt,"CODE",{});var Hl=w(yn);Rr=g(Hl,"Tokenizer.token_to_id"),Hl.forEach(r),Qr=g(wt," method:"),wt.forEach(r),Ks=v(o),$(Xe.$$.fragment,o),Ws=v(o),Ut=k(o,"P",{});var xl=w(Ut);Jr=g(xl,`Here is how we can set the post-processing to give us the traditional
BERT inputs:`),xl.forEach(r),Fs=v(o),$(Ze.$$.fragment,o),Ms=v(o),me=k(o,"P",{});var Qt=w(me);Vr=g(Qt,`Let\u2019s go over this snippet of code in more details. First we specify
the template for single sentences: those should have the form
`),En=k(Qt,"CODE",{});var Cl=w(En);Gr=g(Cl,'"[CLS] $A [SEP]"'),Cl.forEach(r),Xr=g(Qt,` where
`),Pn=k(Qt,"CODE",{});var Al=w(Pn);Zr=g(Al,"$A"),Al.forEach(r),ea=g(Qt," represents our sentence."),Qt.forEach(r),Ys=v(o),x=k(o,"P",{});var I=w(x);ta=g(I,`Then, we specify the template for sentence pairs, which should have the
form `),Tn=k(I,"CODE",{});var Dl=w(Tn);na=g(Dl,'"[CLS] $A [SEP] $B [SEP]"'),Dl.forEach(r),sa=g(I,` where
`),Sn=k(I,"CODE",{});var Ll=w(Sn);oa=g(Ll,"$A"),Ll.forEach(r),ra=g(I,` represents the first sentence and
`),Hn=k(I,"CODE",{});var Bl=w(Hn);aa=g(Bl,"$B"),Bl.forEach(r),la=g(I,` the second one. The
`),xn=k(I,"CODE",{});var Nl=w(xn);ia=g(Nl,":1"),Nl.forEach(r),ua=g(I," added in the template represent the "),Cn=k(I,"CODE",{});var Il=w(Cn);pa=g(Il,"type IDs"),Il.forEach(r),ca=g(I,` we want for each part of our input: it defaults
to 0 for everything (which is why we don\u2019t have
`),An=k(I,"CODE",{});var Ol=w(An);fa=g(Ol,"$A:0"),Ol.forEach(r),$a=g(I,`) and here we set it to 1 for the
tokens of the second sentence and the last `),Dn=k(I,"CODE",{});var Ul=w(Dn);ha=g(Ul,'"[SEP]"'),Ul.forEach(r),ma=g(I," token."),I.forEach(r),Rs=v(o),Kt=k(o,"P",{});var Kl=w(Kt);da=g(Kl,`Lastly, we specify the special tokens we used and their IDs in our
tokenizer\u2019s vocabulary.`),Kl.forEach(r),Qs=v(o),Wt=k(o,"P",{});var Wl=w(Wt);ga=g(Wl,`To check out this worked properly, let\u2019s try to encode the same
sentence as before:`),Wl.forEach(r),Js=v(o),$(et.$$.fragment,o),Vs=v(o),tt=k(o,"P",{});var bo=w(tt);_a=g(bo,`To check the results on a pair of sentences, we just pass the two
sentences to `),Ln=k(bo,"CODE",{});var Fl=w(Ln);ka=g(Fl,"Tokenizer.encode"),Fl.forEach(r),wa=g(bo,":"),bo.forEach(r),Gs=v(o),$(nt.$$.fragment,o),Xs=v(o),Ft=k(o,"P",{});var Ml=w(Ft);qa=g(Ml,"You can then check the type IDs attributed to each token is correct with"),Ml.forEach(r),Zs=v(o),$(st.$$.fragment,o),eo=v(o),ot=k(o,"P",{});var zo=w(ot);ja=g(zo,"If you save your tokenizer with "),Bn=k(zo,"CODE",{});var Yl=w(Bn);va=g(Yl,"Tokenizer.save"),Yl.forEach(r),ba=g(zo,", the post-processor will be saved along."),zo.forEach(r),to=v(o),ve=k(o,"H3",{class:!0});var yo=w(ve);rt=k(yo,"A",{id:!0,class:!0,href:!0});var Rl=w(rt);Nn=k(Rl,"SPAN",{});var Ql=w(Nn);$(Pt.$$.fragment,Ql),Ql.forEach(r),Rl.forEach(r),za=v(yo),In=k(yo,"SPAN",{});var Jl=w(In);ya=g(Jl,"Encoding multiple sentences in a batch"),Jl.forEach(r),yo.forEach(r),no=v(o),at=k(o,"P",{});var Eo=w(at);Ea=g(Eo,`To get the full speed of the \u{1F917} Tokenizers library, it\u2019s best to
process your texts by batches by using the
`),On=k(Eo,"CODE",{});var Vl=w(On);Pa=g(Vl,"Tokenizer.encode_batch"),Vl.forEach(r),Ta=g(Eo," method:"),Eo.forEach(r),so=v(o),$(lt.$$.fragment,o),oo=v(o),it=k(o,"P",{});var Po=w(it);Sa=g(Po,"The output is then a list of "),Un=k(Po,"CODE",{});var Gl=w(Un);Ha=g(Gl,"Encoding"),Gl.forEach(r),xa=g(Po,`
objects like the ones we saw before. You can process together as many
texts as you like, as long as it fits in memory.`),Po.forEach(r),ro=v(o),ut=k(o,"P",{});var To=w(ut);Ca=g(To,`To process a batch of sentences pairs, pass two lists to the
`),Kn=k(To,"CODE",{});var Xl=w(Kn);Aa=g(Xl,"Tokenizer.encode_batch"),Xl.forEach(r),Da=g(To,` method: the
list of sentences A and the list of sentences B:`),To.forEach(r),ao=v(o),$(pt.$$.fragment,o),lo=v(o),Z=k(o,"P",{});var qt=w(Z);La=g(qt,`When encoding multiple sentences, you can automatically pad the outputs
to the longest sentence present by using
`),Wn=k(qt,"CODE",{});var Zl=w(Wn);Ba=g(Zl,"Tokenizer.enable_padding"),Zl.forEach(r),Na=g(qt,`, with the
`),Fn=k(qt,"CODE",{});var ei=w(Fn);Ia=g(ei,"pad_token"),ei.forEach(r),Oa=g(qt,` and its ID (which we can
double-check the id for the padding token with
`),Mn=k(qt,"CODE",{});var ti=w(Mn);Ua=g(ti,"Tokenizer.token_to_id"),ti.forEach(r),Ka=g(qt," like before):"),qt.forEach(r),io=v(o),$(ct.$$.fragment,o),uo=v(o),de=k(o,"P",{});var Jt=w(de);Wa=g(Jt,"We can set the "),Yn=k(Jt,"CODE",{});var ni=w(Yn);Fa=g(ni,"direction"),ni.forEach(r),Ma=g(Jt,` of the padding
(defaults to the right) or a given `),Rn=k(Jt,"CODE",{});var si=w(Rn);Ya=g(si,"length"),si.forEach(r),Ra=g(Jt,` if we want to pad every sample to that specific number (here
we leave it unset to pad to the size of the longest text).`),Jt.forEach(r),po=v(o),$(ft.$$.fragment,o),co=v(o),$t=k(o,"P",{});var So=w($t);Qa=g(So,"In this case, the "),Qn=k(So,"CODE",{});var oi=w(Qn);Ja=g(oi,"attention mask"),oi.forEach(r),Va=g(So,` generated by the
tokenizer takes the padding into account:`),So.forEach(r),fo=v(o),$(ht.$$.fragment,o),$o=v(o),be=k(o,"H2",{class:!0});var Ho=w(be);mt=k(Ho,"A",{id:!0,class:!0,href:!0});var ri=w(mt);Jn=k(ri,"SPAN",{});var ai=w(Jn);$(Tt.$$.fragment,ai),ai.forEach(r),ri.forEach(r),Ga=v(Ho),Vn=k(Ho,"SPAN",{});var li=w(Vn);Xa=g(li,"Pretrained"),li.forEach(r),Ho.forEach(r),ho=v(o),$(dt.$$.fragment,o),this.h()},h(){P(t,"name","hf:doc:metadata"),P(t,"content",JSON.stringify(Kp)),P(s,"id","quicktour"),P(s,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),P(s,"href","#quicktour"),P(e,"class","relative group"),P(N,"id","build-a-tokenizer-from-scratch"),P(N,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),P(N,"href","#build-a-tokenizer-from-scratch"),P(W,"class","relative group"),P(R,"href","https://blog.einstein.ai/the-wikitext-long-term-dependency-language-modeling-dataset/"),P(R,"rel","nofollow"),P(Q,"id","training-the-tokenizer"),P(Q,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),P(Q,"href","#training-the-tokenizer"),P(A,"class","relative group"),P(ue,"href","https://huggingface.co/transformers/tokenizer_summary.html"),P(ue,"rel","nofollow"),P(We,"id","using-the-tokenizer"),P(We,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),P(We,"href","#using-the-tokenizer"),P(qe,"class","relative group"),P(Ge,"id","postprocessing"),P(Ge,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),P(Ge,"href","#postprocessing"),P(je,"class","relative group"),P(rt,"id","encoding-multiple-sentences-in-a-batch"),P(rt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),P(rt,"href","#encoding-multiple-sentences-in-a-batch"),P(ve,"class","relative group"),P(mt,"id","pretrained"),P(mt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),P(mt,"href","#pretrained"),P(be,"class","relative group")},m(o,u){a(document.head,t),q(o,n,u),q(o,e,u),a(e,s),a(s,i),h(T,i,null),a(e,B),a(e,K),a(K,C),q(o,D,u),q(o,O,u),a(O,re),q(o,Pe,u),q(o,W,u),a(W,N),a(N,ae),h(L,ae,null),a(W,Y),a(W,ee),a(ee,te),q(o,Te,u),q(o,F,u),a(F,jt),a(F,R),a(R,le),a(F,vt),q(o,ge,u),h(U,o,u),q(o,_e,u),q(o,A,u),a(A,Q),a(Q,ne),h(se,ne,null),a(A,bt),a(A,ie),a(ie,oe),q(o,ke,u),q(o,b,u),a(b,S),a(b,ue),a(ue,Dt),a(b,Lt),q(o,we,u),q(o,M,u),a(M,pe),a(pe,Bt),a(M,Nt),a(M,Se),a(Se,It),a(M,He),a(M,xe),a(xe,xo),q(o,$s,u),q(o,ce,u),a(ce,Co),a(ce,Vt),a(Vt,Ao),a(ce,Do),a(ce,Gt),a(Gt,Lo),a(ce,Bo),q(o,hs,u),h(Ce,o,u),q(o,ms,u),q(o,zt,u),a(zt,No),a(zt,Xt),a(Xt,Io),q(o,ds,u),h(Ae,o,u),q(o,gs,u),q(o,J,u),a(J,Oo),a(J,Zt),a(Zt,Uo),a(J,Ko),a(J,en),a(en,Wo),a(J,Fo),a(J,tn),a(tn,Mo),a(J,Yo),q(o,_s,u),h(De,o,u),q(o,ks,u),q(o,Le,u),a(Le,Ro),a(Le,nn),a(nn,Qo),a(Le,Jo),q(o,ws,u),h(Be,o,u),q(o,qs,u),q(o,Ne,u),a(Ne,Vo),a(Ne,sn),a(sn,Go),a(Ne,Xo),q(o,js,u),h(Ie,o,u),q(o,vs,u),q(o,Oe,u),a(Oe,Zo),a(Oe,on),a(on,er),a(Oe,tr),q(o,bs,u),h(Ue,o,u),q(o,zs,u),q(o,fe,u),a(fe,nr),a(fe,rn),a(rn,sr),a(fe,or),a(fe,an),a(an,rr),a(fe,ar),q(o,ys,u),h(Ke,o,u),q(o,Es,u),q(o,qe,u),a(qe,We),a(We,ln),h(yt,ln,null),a(qe,lr),a(qe,un),a(un,ir),q(o,Ps,u),q(o,Fe,u),a(Fe,ur),a(Fe,pn),a(pn,pr),a(Fe,cr),q(o,Ts,u),h(Me,o,u),q(o,Ss,u),q(o,$e,u),a($e,fr),a($e,cn),a(cn,$r),a($e,hr),a($e,fn),a(fn,mr),a($e,dr),q(o,Hs,u),q(o,he,u),a(he,gr),a(he,$n),a($n,_r),a(he,kr),a(he,hn),a(hn,wr),a(he,qr),q(o,xs,u),h(Ye,o,u),q(o,Cs,u),q(o,Re,u),a(Re,jr),a(Re,mn),a(mn,vr),a(Re,br),q(o,As,u),h(Qe,o,u),q(o,Ds,u),q(o,V,u),a(V,zr),a(V,dn),a(dn,yr),a(V,Er),a(V,gn),a(gn,Pr),a(V,Tr),a(V,_n),a(_n,Sr),a(V,Hr),q(o,Ls,u),h(Je,o,u),q(o,Bs,u),q(o,Ot,u),a(Ot,xr),q(o,Ns,u),h(Ve,o,u),q(o,Is,u),q(o,je,u),a(je,Ge),a(Ge,kn),h(Et,kn,null),a(je,Cr),a(je,wn),a(wn,Ar),q(o,Os,u),q(o,G,u),a(G,Dr),a(G,qn),a(qn,Lr),a(G,Br),a(G,jn),a(jn,Nr),a(G,Ir),a(G,vn),a(vn,Or),a(G,Ur),q(o,Us,u),q(o,X,u),a(X,Kr),a(X,bn),a(bn,Wr),a(X,Fr),a(X,zn),a(zn,Mr),a(X,Yr),a(X,yn),a(yn,Rr),a(X,Qr),q(o,Ks,u),h(Xe,o,u),q(o,Ws,u),q(o,Ut,u),a(Ut,Jr),q(o,Fs,u),h(Ze,o,u),q(o,Ms,u),q(o,me,u),a(me,Vr),a(me,En),a(En,Gr),a(me,Xr),a(me,Pn),a(Pn,Zr),a(me,ea),q(o,Ys,u),q(o,x,u),a(x,ta),a(x,Tn),a(Tn,na),a(x,sa),a(x,Sn),a(Sn,oa),a(x,ra),a(x,Hn),a(Hn,aa),a(x,la),a(x,xn),a(xn,ia),a(x,ua),a(x,Cn),a(Cn,pa),a(x,ca),a(x,An),a(An,fa),a(x,$a),a(x,Dn),a(Dn,ha),a(x,ma),q(o,Rs,u),q(o,Kt,u),a(Kt,da),q(o,Qs,u),q(o,Wt,u),a(Wt,ga),q(o,Js,u),h(et,o,u),q(o,Vs,u),q(o,tt,u),a(tt,_a),a(tt,Ln),a(Ln,ka),a(tt,wa),q(o,Gs,u),h(nt,o,u),q(o,Xs,u),q(o,Ft,u),a(Ft,qa),q(o,Zs,u),h(st,o,u),q(o,eo,u),q(o,ot,u),a(ot,ja),a(ot,Bn),a(Bn,va),a(ot,ba),q(o,to,u),q(o,ve,u),a(ve,rt),a(rt,Nn),h(Pt,Nn,null),a(ve,za),a(ve,In),a(In,ya),q(o,no,u),q(o,at,u),a(at,Ea),a(at,On),a(On,Pa),a(at,Ta),q(o,so,u),h(lt,o,u),q(o,oo,u),q(o,it,u),a(it,Sa),a(it,Un),a(Un,Ha),a(it,xa),q(o,ro,u),q(o,ut,u),a(ut,Ca),a(ut,Kn),a(Kn,Aa),a(ut,Da),q(o,ao,u),h(pt,o,u),q(o,lo,u),q(o,Z,u),a(Z,La),a(Z,Wn),a(Wn,Ba),a(Z,Na),a(Z,Fn),a(Fn,Ia),a(Z,Oa),a(Z,Mn),a(Mn,Ua),a(Z,Ka),q(o,io,u),h(ct,o,u),q(o,uo,u),q(o,de,u),a(de,Wa),a(de,Yn),a(Yn,Fa),a(de,Ma),a(de,Rn),a(Rn,Ya),a(de,Ra),q(o,po,u),h(ft,o,u),q(o,co,u),q(o,$t,u),a($t,Qa),a($t,Qn),a(Qn,Ja),a($t,Va),q(o,fo,u),h(ht,o,u),q(o,$o,u),q(o,be,u),a(be,mt),a(mt,Jn),h(Tt,Jn,null),a(be,Ga),a(be,Vn),a(Vn,Xa),q(o,ho,u),h(dt,o,u),mo=!0},p(o,[u]){const St={};u&2&&(St.$$scope={dirty:u,ctx:o}),Ce.$set(St);const Gn={};u&2&&(Gn.$$scope={dirty:u,ctx:o}),Ae.$set(Gn);const Xn={};u&2&&(Xn.$$scope={dirty:u,ctx:o}),De.$set(Xn);const Zn={};u&2&&(Zn.$$scope={dirty:u,ctx:o}),Be.$set(Zn);const es={};u&2&&(es.$$scope={dirty:u,ctx:o}),Ie.$set(es);const Ht={};u&2&&(Ht.$$scope={dirty:u,ctx:o}),Ue.$set(Ht);const ts={};u&2&&(ts.$$scope={dirty:u,ctx:o}),Ke.$set(ts);const ns={};u&2&&(ns.$$scope={dirty:u,ctx:o}),Me.$set(ns);const ss={};u&2&&(ss.$$scope={dirty:u,ctx:o}),Ye.$set(ss);const xt={};u&2&&(xt.$$scope={dirty:u,ctx:o}),Qe.$set(xt);const os={};u&2&&(os.$$scope={dirty:u,ctx:o}),Je.$set(os);const Ct={};u&2&&(Ct.$$scope={dirty:u,ctx:o}),Ve.$set(Ct);const rs={};u&2&&(rs.$$scope={dirty:u,ctx:o}),Xe.$set(rs);const as={};u&2&&(as.$$scope={dirty:u,ctx:o}),Ze.$set(as);const ls={};u&2&&(ls.$$scope={dirty:u,ctx:o}),et.$set(ls);const At={};u&2&&(At.$$scope={dirty:u,ctx:o}),nt.$set(At);const is={};u&2&&(is.$$scope={dirty:u,ctx:o}),st.$set(is);const ze={};u&2&&(ze.$$scope={dirty:u,ctx:o}),lt.$set(ze);const us={};u&2&&(us.$$scope={dirty:u,ctx:o}),pt.$set(us);const ps={};u&2&&(ps.$$scope={dirty:u,ctx:o}),ct.$set(ps);const cs={};u&2&&(cs.$$scope={dirty:u,ctx:o}),ft.$set(cs);const ye={};u&2&&(ye.$$scope={dirty:u,ctx:o}),ht.$set(ye);const fs={};u&2&&(fs.$$scope={dirty:u,ctx:o}),dt.$set(fs)},i(o){mo||(p(T.$$.fragment,o),p(L.$$.fragment,o),p(U.$$.fragment,o),p(se.$$.fragment,o),p(Ce.$$.fragment,o),p(Ae.$$.fragment,o),p(De.$$.fragment,o),p(Be.$$.fragment,o),p(Ie.$$.fragment,o),p(Ue.$$.fragment,o),p(Ke.$$.fragment,o),p(yt.$$.fragment,o),p(Me.$$.fragment,o),p(Ye.$$.fragment,o),p(Qe.$$.fragment,o),p(Je.$$.fragment,o),p(Ve.$$.fragment,o),p(Et.$$.fragment,o),p(Xe.$$.fragment,o),p(Ze.$$.fragment,o),p(et.$$.fragment,o),p(nt.$$.fragment,o),p(st.$$.fragment,o),p(Pt.$$.fragment,o),p(lt.$$.fragment,o),p(pt.$$.fragment,o),p(ct.$$.fragment,o),p(ft.$$.fragment,o),p(ht.$$.fragment,o),p(Tt.$$.fragment,o),p(dt.$$.fragment,o),mo=!0)},o(o){c(T.$$.fragment,o),c(L.$$.fragment,o),c(U.$$.fragment,o),c(se.$$.fragment,o),c(Ce.$$.fragment,o),c(Ae.$$.fragment,o),c(De.$$.fragment,o),c(Be.$$.fragment,o),c(Ie.$$.fragment,o),c(Ue.$$.fragment,o),c(Ke.$$.fragment,o),c(yt.$$.fragment,o),c(Me.$$.fragment,o),c(Ye.$$.fragment,o),c(Qe.$$.fragment,o),c(Je.$$.fragment,o),c(Ve.$$.fragment,o),c(Et.$$.fragment,o),c(Xe.$$.fragment,o),c(Ze.$$.fragment,o),c(et.$$.fragment,o),c(nt.$$.fragment,o),c(st.$$.fragment,o),c(Pt.$$.fragment,o),c(lt.$$.fragment,o),c(pt.$$.fragment,o),c(ct.$$.fragment,o),c(ft.$$.fragment,o),c(ht.$$.fragment,o),c(Tt.$$.fragment,o),c(dt.$$.fragment,o),mo=!1},d(o){r(t),o&&r(n),o&&r(e),m(T),o&&r(D),o&&r(O),o&&r(Pe),o&&r(W),m(L),o&&r(Te),o&&r(F),o&&r(ge),m(U,o),o&&r(_e),o&&r(A),m(se),o&&r(ke),o&&r(b),o&&r(we),o&&r(M),o&&r($s),o&&r(ce),o&&r(hs),m(Ce,o),o&&r(ms),o&&r(zt),o&&r(ds),m(Ae,o),o&&r(gs),o&&r(J),o&&r(_s),m(De,o),o&&r(ks),o&&r(Le),o&&r(ws),m(Be,o),o&&r(qs),o&&r(Ne),o&&r(js),m(Ie,o),o&&r(vs),o&&r(Oe),o&&r(bs),m(Ue,o),o&&r(zs),o&&r(fe),o&&r(ys),m(Ke,o),o&&r(Es),o&&r(qe),m(yt),o&&r(Ps),o&&r(Fe),o&&r(Ts),m(Me,o),o&&r(Ss),o&&r($e),o&&r(Hs),o&&r(he),o&&r(xs),m(Ye,o),o&&r(Cs),o&&r(Re),o&&r(As),m(Qe,o),o&&r(Ds),o&&r(V),o&&r(Ls),m(Je,o),o&&r(Bs),o&&r(Ot),o&&r(Ns),m(Ve,o),o&&r(Is),o&&r(je),m(Et),o&&r(Os),o&&r(G),o&&r(Us),o&&r(X),o&&r(Ks),m(Xe,o),o&&r(Ws),o&&r(Ut),o&&r(Fs),m(Ze,o),o&&r(Ms),o&&r(me),o&&r(Ys),o&&r(x),o&&r(Rs),o&&r(Kt),o&&r(Qs),o&&r(Wt),o&&r(Js),m(et,o),o&&r(Vs),o&&r(tt),o&&r(Gs),m(nt,o),o&&r(Xs),o&&r(Ft),o&&r(Zs),m(st,o),o&&r(eo),o&&r(ot),o&&r(to),o&&r(ve),m(Pt),o&&r(no),o&&r(at),o&&r(so),m(lt,o),o&&r(oo),o&&r(it),o&&r(ro),o&&r(ut),o&&r(ao),m(pt,o),o&&r(lo),o&&r(Z),o&&r(io),m(ct,o),o&&r(uo),o&&r(de),o&&r(po),m(ft,o),o&&r(co),o&&r($t),o&&r(fo),m(ht,o),o&&r($o),o&&r(be),m(Tt),o&&r(ho),m(dt,o)}}}const Kp={local:"quicktour",sections:[{local:"build-a-tokenizer-from-scratch",sections:[{local:"training-the-tokenizer",title:"Training the tokenizer"},{local:"using-the-tokenizer",title:"Using the tokenizer"},{local:"postprocessing",title:"Post-processing"},{local:"encoding-multiple-sentences-in-a-batch",title:"Encoding multiple sentences in a batch"}],title:"Build a tokenizer from scratch"},{local:"pretrained",sections:[{local:"using-a-pretrained-tokenizer",title:"Using a pretrained tokenizer"},{local:"importing-a-pretrained-tokenizer-from-legacy-vocabulary-files",title:"Importing a pretrained tokenizer from legacy vocabulary files"}],title:"Pretrained"}],title:"Quicktour"};function Wp(l){return di(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Qp extends ii{constructor(t){super();ui(this,t,Wp,Up,pi,{})}}export{Qp as default,Kp as metadata};
