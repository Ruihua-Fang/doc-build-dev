import{S as bn,i as Un,s as gn,e as s,k as E,w as W,t as i,M as Cn,c as l,d as t,m as v,a,x as F,h as u,b as _,F as e,g as h,y as B,q as J,o as j,B as G,v as On}from"../../chunks/vendor-0d3f0756.js";import{I as nt}from"../../chunks/IconCopyLink-9193371d.js";import{T as Dn,M as en}from"../../chunks/TokenizersLanguageContent-ca787841.js";function Rn(L){let o,f,r,p,$,k,T,P,S,z,m,A,d,I,w,y,te,K,ne,rt,be,b,re,ot,oe,st,lt,se,at,Q,le,it,ae,ut,pt,V,ct,ie,ft,dt,Ue,U,ht,ke,_t,Et,ge,q,g,me,X,vt,Te,ze,$t,Ce,ue,kt,Oe,C,pe,mt,ce,Tt,zt,fe,It,Y,de,wt,he,Lt,Pt,Z,At,_e,qt,xt,De,O,St,Ie,yt,bt,Re,x,D,we,ee,Ut,Le,Pe,gt,Ne,Ee,Ct,Me,R,N,Ot,Ae,Dt,Rt,ve,Nt,Mt,M,Ht,qe,Wt,Ft,$e,Bt,He,H,Jt,xe,jt,Gt,We;return d=new nt({}),X=new nt({}),ee=new nt({}),{c(){o=s("p"),f=i("These types represent all the different kinds of input that a "),r=s("a"),p=i("Tokenizer"),$=i(` accepts
when using `),k=s("code"),T=i("encode_batch()"),P=i("."),S=E(),z=s("h2"),m=s("a"),A=s("span"),W(d.$$.fragment),I=E(),w=s("span"),y=s("code"),te=i("tokenizers.TextEncodeInput"),K=E(),ne=s("p"),rt=i("Represents a textual input for encoding. Can be either:"),be=E(),b=s("ul"),re=s("li"),ot=i("A single sequence: "),oe=s("a"),st=i("TextInputSequence"),lt=E(),se=s("li"),at=i("A pair of sequences:"),Q=s("ul"),le=s("li"),it=i("A Tuple of "),ae=s("a"),ut=i("TextInputSequence"),pt=E(),V=s("li"),ct=i("Or a List of "),ie=s("a"),ft=i("TextInputSequence"),dt=i(" of size 2"),Ue=E(),U=s("p"),ht=i("alias of "),ke=s("code"),_t=i("Union[str, Tuple[str, str], List[str]]"),Et=i("."),ge=E(),q=s("h2"),g=s("a"),me=s("span"),W(X.$$.fragment),vt=E(),Te=s("span"),ze=s("code"),$t=i("tokenizers.PreTokenizedEncodeInput"),Ce=E(),ue=s("p"),kt=i("Represents a pre-tokenized input for encoding. Can be either:"),Oe=E(),C=s("ul"),pe=s("li"),mt=i("A single sequence: "),ce=s("a"),Tt=i("PreTokenizedInputSequence"),zt=E(),fe=s("li"),It=i("A pair of sequences:"),Y=s("ul"),de=s("li"),wt=i("A Tuple of "),he=s("a"),Lt=i("PreTokenizedInputSequence"),Pt=E(),Z=s("li"),At=i("Or a List of "),_e=s("a"),qt=i("PreTokenizedInputSequence"),xt=i(" of size 2"),De=E(),O=s("p"),St=i("alias of "),Ie=s("code"),yt=i("Union[List[str], Tuple[str], Tuple[Union[List[str], Tuple[str]], Union[List[str], Tuple[str]]], List[Union[List[str], Tuple[str]]]]"),bt=i("."),Re=E(),x=s("h2"),D=s("a"),we=s("span"),W(ee.$$.fragment),Ut=E(),Le=s("span"),Pe=s("code"),gt=i("tokenizers.EncodeInput"),Ne=E(),Ee=s("p"),Ct=i("Represents all the possible types of input for encoding. Can be:"),Me=E(),R=s("ul"),N=s("li"),Ot=i("When "),Ae=s("code"),Dt=i("is_pretokenized=False"),Rt=i(": "),ve=s("a"),Nt=i("TextEncodeInput"),Mt=E(),M=s("li"),Ht=i("When "),qe=s("code"),Wt=i("is_pretokenized=True"),Ft=i(": "),$e=s("a"),Bt=i("PreTokenizedEncodeInput"),He=E(),H=s("p"),Jt=i("alias of "),xe=s("code"),jt=i("Union[str, Tuple[str, str], List[str], Tuple[str], Tuple[Union[List[str], Tuple[str]], Union[List[str], Tuple[str]]], List[Union[List[str], Tuple[str]]]]"),Gt=i("."),this.h()},l(n){o=l(n,"P",{});var c=a(o);f=u(c,"These types represent all the different kinds of input that a "),r=l(c,"A",{href:!0});var tn=a(r);p=u(tn,"Tokenizer"),tn.forEach(t),$=u(c,` accepts
when using `),k=l(c,"CODE",{});var nn=a(k);T=u(nn,"encode_batch()"),nn.forEach(t),P=u(c,"."),c.forEach(t),S=v(n),z=l(n,"H2",{class:!0});var Fe=a(z);m=l(Fe,"A",{id:!0,class:!0,href:!0});var rn=a(m);A=l(rn,"SPAN",{});var on=a(A);F(d.$$.fragment,on),on.forEach(t),rn.forEach(t),I=v(Fe),w=l(Fe,"SPAN",{});var sn=a(w);y=l(sn,"CODE",{});var ln=a(y);te=u(ln,"tokenizers.TextEncodeInput"),ln.forEach(t),sn.forEach(t),Fe.forEach(t),K=v(n),ne=l(n,"P",{});var an=a(ne);rt=u(an,"Represents a textual input for encoding. Can be either:"),an.forEach(t),be=v(n),b=l(n,"UL",{});var Be=a(b);re=l(Be,"LI",{});var Kt=a(re);ot=u(Kt,"A single sequence: "),oe=l(Kt,"A",{href:!0});var un=a(oe);st=u(un,"TextInputSequence"),un.forEach(t),Kt.forEach(t),lt=v(Be),se=l(Be,"LI",{});var Qt=a(se);at=u(Qt,"A pair of sequences:"),Q=l(Qt,"UL",{});var Je=a(Q);le=l(Je,"LI",{});var Vt=a(le);it=u(Vt,"A Tuple of "),ae=l(Vt,"A",{href:!0});var pn=a(ae);ut=u(pn,"TextInputSequence"),pn.forEach(t),Vt.forEach(t),pt=v(Je),V=l(Je,"LI",{});var je=a(V);ct=u(je,"Or a List of "),ie=l(je,"A",{href:!0});var cn=a(ie);ft=u(cn,"TextInputSequence"),cn.forEach(t),dt=u(je," of size 2"),je.forEach(t),Je.forEach(t),Qt.forEach(t),Be.forEach(t),Ue=v(n),U=l(n,"P",{});var Ge=a(U);ht=u(Ge,"alias of "),ke=l(Ge,"CODE",{});var fn=a(ke);_t=u(fn,"Union[str, Tuple[str, str], List[str]]"),fn.forEach(t),Et=u(Ge,"."),Ge.forEach(t),ge=v(n),q=l(n,"H2",{class:!0});var Ke=a(q);g=l(Ke,"A",{id:!0,class:!0,href:!0});var dn=a(g);me=l(dn,"SPAN",{});var hn=a(me);F(X.$$.fragment,hn),hn.forEach(t),dn.forEach(t),vt=v(Ke),Te=l(Ke,"SPAN",{});var _n=a(Te);ze=l(_n,"CODE",{});var En=a(ze);$t=u(En,"tokenizers.PreTokenizedEncodeInput"),En.forEach(t),_n.forEach(t),Ke.forEach(t),Ce=v(n),ue=l(n,"P",{});var vn=a(ue);kt=u(vn,"Represents a pre-tokenized input for encoding. Can be either:"),vn.forEach(t),Oe=v(n),C=l(n,"UL",{});var Qe=a(C);pe=l(Qe,"LI",{});var Xt=a(pe);mt=u(Xt,"A single sequence: "),ce=l(Xt,"A",{href:!0});var $n=a(ce);Tt=u($n,"PreTokenizedInputSequence"),$n.forEach(t),Xt.forEach(t),zt=v(Qe),fe=l(Qe,"LI",{});var Yt=a(fe);It=u(Yt,"A pair of sequences:"),Y=l(Yt,"UL",{});var Ve=a(Y);de=l(Ve,"LI",{});var Zt=a(de);wt=u(Zt,"A Tuple of "),he=l(Zt,"A",{href:!0});var kn=a(he);Lt=u(kn,"PreTokenizedInputSequence"),kn.forEach(t),Zt.forEach(t),Pt=v(Ve),Z=l(Ve,"LI",{});var Xe=a(Z);At=u(Xe,"Or a List of "),_e=l(Xe,"A",{href:!0});var mn=a(_e);qt=u(mn,"PreTokenizedInputSequence"),mn.forEach(t),xt=u(Xe," of size 2"),Xe.forEach(t),Ve.forEach(t),Yt.forEach(t),Qe.forEach(t),De=v(n),O=l(n,"P",{});var Ye=a(O);St=u(Ye,"alias of "),Ie=l(Ye,"CODE",{});var Tn=a(Ie);yt=u(Tn,"Union[List[str], Tuple[str], Tuple[Union[List[str], Tuple[str]], Union[List[str], Tuple[str]]], List[Union[List[str], Tuple[str]]]]"),Tn.forEach(t),bt=u(Ye,"."),Ye.forEach(t),Re=v(n),x=l(n,"H2",{class:!0});var Ze=a(x);D=l(Ze,"A",{id:!0,class:!0,href:!0});var zn=a(D);we=l(zn,"SPAN",{});var In=a(we);F(ee.$$.fragment,In),In.forEach(t),zn.forEach(t),Ut=v(Ze),Le=l(Ze,"SPAN",{});var wn=a(Le);Pe=l(wn,"CODE",{});var Ln=a(Pe);gt=u(Ln,"tokenizers.EncodeInput"),Ln.forEach(t),wn.forEach(t),Ze.forEach(t),Ne=v(n),Ee=l(n,"P",{});var Pn=a(Ee);Ct=u(Pn,"Represents all the possible types of input for encoding. Can be:"),Pn.forEach(t),Me=v(n),R=l(n,"UL",{});var et=a(R);N=l(et,"LI",{});var Se=a(N);Ot=u(Se,"When "),Ae=l(Se,"CODE",{});var An=a(Ae);Dt=u(An,"is_pretokenized=False"),An.forEach(t),Rt=u(Se,": "),ve=l(Se,"A",{href:!0});var qn=a(ve);Nt=u(qn,"TextEncodeInput"),qn.forEach(t),Se.forEach(t),Mt=v(et),M=l(et,"LI",{});var ye=a(M);Ht=u(ye,"When "),qe=l(ye,"CODE",{});var xn=a(qe);Wt=u(xn,"is_pretokenized=True"),xn.forEach(t),Ft=u(ye,": "),$e=l(ye,"A",{href:!0});var Sn=a($e);Bt=u(Sn,"PreTokenizedEncodeInput"),Sn.forEach(t),ye.forEach(t),et.forEach(t),He=v(n),H=l(n,"P",{});var tt=a(H);Jt=u(tt,"alias of "),xe=l(tt,"CODE",{});var yn=a(xe);jt=u(yn,"Union[str, Tuple[str, str], List[str], Tuple[str], Tuple[Union[List[str], Tuple[str]], Union[List[str], Tuple[str]]], List[Union[List[str], Tuple[str]]]]"),yn.forEach(t),Gt=u(tt,"."),tt.forEach(t),this.h()},h(){_(r,"href","/docs/tokenizers/pr_1/en/api/tokenizer#tokenizers.Tokenizer"),_(m,"id","tokenizers.TextEncodeInput"),_(m,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),_(m,"href","#tokenizers.TextEncodeInput"),_(z,"class","relative group"),_(oe,"href","/docs/tokenizers/api/input-sequences#tokenizers.TextInputSequence"),_(ae,"href","/docs/tokenizers/api/input-sequences#tokenizers.TextInputSequence"),_(ie,"href","/docs/tokenizers/api/input-sequences#tokenizers.TextInputSequence"),_(g,"id","tokenizers.PreTokenizedEncodeInput"),_(g,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),_(g,"href","#tokenizers.PreTokenizedEncodeInput"),_(q,"class","relative group"),_(ce,"href","/docs/tokenizers/api/input-sequences#tokenizers.PreTokenizedInputSequence"),_(he,"href","/docs/tokenizers/api/input-sequences#tokenizers.PreTokenizedInputSequence"),_(_e,"href","/docs/tokenizers/api/input-sequences#tokenizers.PreTokenizedInputSequence"),_(D,"id","tokenizers.EncodeInput"),_(D,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),_(D,"href","#tokenizers.EncodeInput"),_(x,"class","relative group"),_(ve,"href","#tokenizers.TextEncodeInput"),_($e,"href","#tokenizers.PreTokenizedEncodeInput")},m(n,c){h(n,o,c),e(o,f),e(o,r),e(r,p),e(o,$),e(o,k),e(k,T),e(o,P),h(n,S,c),h(n,z,c),e(z,m),e(m,A),B(d,A,null),e(z,I),e(z,w),e(w,y),e(y,te),h(n,K,c),h(n,ne,c),e(ne,rt),h(n,be,c),h(n,b,c),e(b,re),e(re,ot),e(re,oe),e(oe,st),e(b,lt),e(b,se),e(se,at),e(se,Q),e(Q,le),e(le,it),e(le,ae),e(ae,ut),e(Q,pt),e(Q,V),e(V,ct),e(V,ie),e(ie,ft),e(V,dt),h(n,Ue,c),h(n,U,c),e(U,ht),e(U,ke),e(ke,_t),e(U,Et),h(n,ge,c),h(n,q,c),e(q,g),e(g,me),B(X,me,null),e(q,vt),e(q,Te),e(Te,ze),e(ze,$t),h(n,Ce,c),h(n,ue,c),e(ue,kt),h(n,Oe,c),h(n,C,c),e(C,pe),e(pe,mt),e(pe,ce),e(ce,Tt),e(C,zt),e(C,fe),e(fe,It),e(fe,Y),e(Y,de),e(de,wt),e(de,he),e(he,Lt),e(Y,Pt),e(Y,Z),e(Z,At),e(Z,_e),e(_e,qt),e(Z,xt),h(n,De,c),h(n,O,c),e(O,St),e(O,Ie),e(Ie,yt),e(O,bt),h(n,Re,c),h(n,x,c),e(x,D),e(D,we),B(ee,we,null),e(x,Ut),e(x,Le),e(Le,Pe),e(Pe,gt),h(n,Ne,c),h(n,Ee,c),e(Ee,Ct),h(n,Me,c),h(n,R,c),e(R,N),e(N,Ot),e(N,Ae),e(Ae,Dt),e(N,Rt),e(N,ve),e(ve,Nt),e(R,Mt),e(R,M),e(M,Ht),e(M,qe),e(qe,Wt),e(M,Ft),e(M,$e),e($e,Bt),h(n,He,c),h(n,H,c),e(H,Jt),e(H,xe),e(xe,jt),e(H,Gt),We=!0},i(n){We||(J(d.$$.fragment,n),J(X.$$.fragment,n),J(ee.$$.fragment,n),We=!0)},o(n){j(d.$$.fragment,n),j(X.$$.fragment,n),j(ee.$$.fragment,n),We=!1},d(n){n&&t(o),n&&t(S),n&&t(z),G(d),n&&t(K),n&&t(ne),n&&t(be),n&&t(b),n&&t(Ue),n&&t(U),n&&t(ge),n&&t(q),G(X),n&&t(Ce),n&&t(ue),n&&t(Oe),n&&t(C),n&&t(De),n&&t(O),n&&t(Re),n&&t(x),G(ee),n&&t(Ne),n&&t(Ee),n&&t(Me),n&&t(R),n&&t(He),n&&t(H)}}}function Nn(L){let o,f;return o=new en({props:{$$slots:{default:[Rn]},$$scope:{ctx:L}}}),{c(){W(o.$$.fragment)},l(r){F(o.$$.fragment,r)},m(r,p){B(o,r,p),f=!0},p(r,p){const $={};p&2&&($.$$scope={dirty:p,ctx:r}),o.$set($)},i(r){f||(J(o.$$.fragment,r),f=!0)},o(r){j(o.$$.fragment,r),f=!1},d(r){G(o,r)}}}function Mn(L){let o,f,r,p,$;return{c(){o=s("p"),f=i("The Rust API Reference is available directly on the "),r=s("a"),p=i("Docs.rs"),$=i(" website."),this.h()},l(k){o=l(k,"P",{});var T=a(o);f=u(T,"The Rust API Reference is available directly on the "),r=l(T,"A",{href:!0,rel:!0});var P=a(r);p=u(P,"Docs.rs"),P.forEach(t),$=u(T," website."),T.forEach(t),this.h()},h(){_(r,"href","https://docs.rs/tokenizers/latest/tokenizers/"),_(r,"rel","nofollow")},m(k,T){h(k,o,T),e(o,f),e(o,r),e(r,p),e(o,$)},d(k){k&&t(o)}}}function Hn(L){let o,f;return o=new en({props:{$$slots:{default:[Mn]},$$scope:{ctx:L}}}),{c(){W(o.$$.fragment)},l(r){F(o.$$.fragment,r)},m(r,p){B(o,r,p),f=!0},p(r,p){const $={};p&2&&($.$$scope={dirty:p,ctx:r}),o.$set($)},i(r){f||(J(o.$$.fragment,r),f=!0)},o(r){j(o.$$.fragment,r),f=!1},d(r){G(o,r)}}}function Wn(L){let o,f;return{c(){o=s("p"),f=i("The node API has not been documented yet.")},l(r){o=l(r,"P",{});var p=a(o);f=u(p,"The node API has not been documented yet."),p.forEach(t)},m(r,p){h(r,o,p),e(o,f)},d(r){r&&t(o)}}}function Fn(L){let o,f;return o=new en({props:{$$slots:{default:[Wn]},$$scope:{ctx:L}}}),{c(){W(o.$$.fragment)},l(r){F(o.$$.fragment,r)},m(r,p){B(o,r,p),f=!0},p(r,p){const $={};p&2&&($.$$scope={dirty:p,ctx:r}),o.$set($)},i(r){f||(J(o.$$.fragment,r),f=!0)},o(r){j(o.$$.fragment,r),f=!1},d(r){G(o,r)}}}function Bn(L){let o,f,r,p,$,k,T,P,S,z,m,A;return k=new nt({}),m=new Dn({props:{python:!0,rust:!0,node:!0,$$slots:{node:[Fn],rust:[Hn],python:[Nn]},$$scope:{ctx:L}}}),{c(){o=s("meta"),f=E(),r=s("h1"),p=s("a"),$=s("span"),W(k.$$.fragment),T=E(),P=s("span"),S=i("Encode Inputs"),z=E(),W(m.$$.fragment),this.h()},l(d){const I=Cn('[data-svelte="svelte-1phssyn"]',document.head);o=l(I,"META",{name:!0,content:!0}),I.forEach(t),f=v(d),r=l(d,"H1",{class:!0});var w=a(r);p=l(w,"A",{id:!0,class:!0,href:!0});var y=a(p);$=l(y,"SPAN",{});var te=a($);F(k.$$.fragment,te),te.forEach(t),y.forEach(t),T=v(w),P=l(w,"SPAN",{});var K=a(P);S=u(K,"Encode Inputs"),K.forEach(t),w.forEach(t),z=v(d),F(m.$$.fragment,d),this.h()},h(){_(o,"name","hf:doc:metadata"),_(o,"content",JSON.stringify(Jn)),_(p,"id","encode-inputs"),_(p,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),_(p,"href","#encode-inputs"),_(r,"class","relative group")},m(d,I){e(document.head,o),h(d,f,I),h(d,r,I),e(r,p),e(p,$),B(k,$,null),e(r,T),e(r,P),e(P,S),h(d,z,I),B(m,d,I),A=!0},p(d,[I]){const w={};I&2&&(w.$$scope={dirty:I,ctx:d}),m.$set(w)},i(d){A||(J(k.$$.fragment,d),J(m.$$.fragment,d),A=!0)},o(d){j(k.$$.fragment,d),j(m.$$.fragment,d),A=!1},d(d){t(o),d&&t(f),d&&t(r),G(k),d&&t(z),G(m,d)}}}const Jn={local:"encode-inputs",sections:[{local:"tokenizers.TextEncodeInput",title:"`tokenizers.TextEncodeInput`"},{local:"tokenizers.PreTokenizedEncodeInput",title:"`tokenizers.PreTokenizedEncodeInput`"},{local:"tokenizers.EncodeInput",title:"`tokenizers.EncodeInput`"}],title:"Encode Inputs"};function jn(L){return On(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Vn extends bn{constructor(o){super();Un(this,o,jn,Bn,gn,{})}}export{Vn as default,Jn as metadata};
