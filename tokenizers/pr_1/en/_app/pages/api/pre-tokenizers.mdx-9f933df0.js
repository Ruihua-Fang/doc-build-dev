import{S as ps,i as cs,s as ds,e as o,k as c,w as v,t as l,M as hs,c as a,d as r,m as d,a as i,x as _,h as p,b as g,F as e,g as k,y as z,q as w,o as T,B as P,v as fs,L as ms}from"../../chunks/vendor-0d3f0756.js";import{D as C}from"../../chunks/Docstring-f752f2c3.js";import{C as ls}from"../../chunks/CodeBlock-7b0cb15c.js";import{I as us}from"../../chunks/IconCopyLink-9193371d.js";import{T as ks,M as Ir}from"../../chunks/TokenizersLanguageContent-ca787841.js";function gs(q){let s,f,n,h,$,b,x,B,X,y,D,Y,u,I,V,Z,ze,we,M,te,_t,Ie,zt,wt,qe,Tt,tt,U,re,Pt,Te,yt,Be,bt,rt,E,se,Et,Ae,xt,Dt,Ve,St,Ct,ne,It,Le,qt,Bt,oe,st,L,ae,At,je,Vt,Lt,Me,jt,nt,S,ie,Mt,Oe,Ot,Wt,We,Ft,Rt,O,le,Ht,pe,Ut,Fe,Nt,Kt,Jt,W,Gt,Re,Qt,Xt,He,Yt,Zt,Ue,er,tr,F,ce,rr,Ne,sr,nr,R,or,Pe,ar,ir,Ke,lr,pr,Je,cr,ot,N,de,dr,Ge,hr,at,K,he,fr,Qe,mr,it,j,fe,ur,Xe,kr,gr,Ye,$r,lt,J,me,vr,ue,_r,ke,zr,wr,pt,G,ge,Tr,ye,Pr,Ze,yr,ct,Q,$e,br,be,Er,et,xr,dt;return f=new C({props:{name:"class tokenizers.pre_tokenizers.BertPreTokenizer",anchor:"tokenizers.pre_tokenizers.BertPreTokenizer",parameters:[]}}),D=new C({props:{name:"class tokenizers.pre_tokenizers.ByteLevel",anchor:"tokenizers.pre_tokenizers.ByteLevel",parameters:[{name:"add_prefix_space",val:" = True"}],parametersDescription:[{anchor:"tokenizers.pre_tokenizers.ByteLevel.add_prefix_space",description:`<strong>add_prefix_space</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to add a space to the first word if there isn&#x2019;t already one. This
lets us treat <em>hello</em> exactly like <em>say hello</em>.`,name:"add_prefix_space"}]}}),te=new C({props:{name:"alphabet",anchor:"tokenizers.pre_tokenizers.ByteLevel.alphabet",parameters:[],returnDescription:`
<p>A list of characters that compose the alphabet</p>
`,returnType:`
<p><code>List[str]</code></p>
`}}),re=new C({props:{name:"class tokenizers.pre_tokenizers.CharDelimiterSplit",anchor:"tokenizers.pre_tokenizers.CharDelimiterSplit",parameters:""}}),se=new C({props:{name:"class tokenizers.pre_tokenizers.Digits",anchor:"tokenizers.pre_tokenizers.Digits",parameters:[{name:"individual_digits",val:" = False"}],parametersDescription:[{anchor:"tokenizers.pre_tokenizers.Digits.individual_digits",description:"<strong>individual_digits</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;",name:"individual_digits"}]}}),ne=new ls({props:{code:'"Call 123 please" -> "Call ", "1", "2", "3", " please"',highlighted:'<span class="hljs-string">&quot;Call 123 please&quot;</span> -&gt; <span class="hljs-string">&quot;Call &quot;</span>, <span class="hljs-string">&quot;1&quot;</span>, <span class="hljs-string">&quot;2&quot;</span>, <span class="hljs-string">&quot;3&quot;</span>, <span class="hljs-string">&quot; please&quot;</span>'}}),oe=new ls({props:{code:'"Call 123 please" -> "Call ", "123", " please"',highlighted:'<span class="hljs-string">&quot;Call 123 please&quot;</span> -&gt; <span class="hljs-string">&quot;Call &quot;</span>, <span class="hljs-string">&quot;123&quot;</span>, <span class="hljs-string">&quot; please&quot;</span>'}}),ae=new C({props:{name:"class tokenizers.pre_tokenizers.Metaspace",anchor:"tokenizers.pre_tokenizers.Metaspace",parameters:[{name:"replacement",val:" = '_'"},{name:"add_prefix_space",val:" = True"}],parametersDescription:[{anchor:"tokenizers.pre_tokenizers.Metaspace.replacement",description:`<strong>replacement</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&#x2581;</code>) &#x2014;
The replacement character. Must be exactly one character. By default we
use the <em>&#x2581;</em> (U+2581) meta symbol (Same as in SentencePiece).`,name:"replacement"},{anchor:"tokenizers.pre_tokenizers.Metaspace.add_prefix_space",description:`<strong>add_prefix_space</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to add a space to the first word if there isn&#x2019;t already one. This
lets us treat <em>hello</em> exactly like <em>say hello</em>.`,name:"add_prefix_space"}]}}),ie=new C({props:{name:"class tokenizers.pre_tokenizers.PreTokenizer",anchor:"tokenizers.pre_tokenizers.PreTokenizer",parameters:[]}}),le=new C({props:{name:"pre_tokenize",anchor:"tokenizers.pre_tokenizers.PreTokenizer.pre_tokenize",parameters:[{name:"pretok",val:""}],parametersDescription:[{anchor:"tokenizers.pre_tokenizers.PreTokenizer.pre_tokenize.pretok",description:"<strong>pretok</strong> (<code>PreTokenizedString) -- The pre-tokenized string on which to apply this :class:</code>~tokenizers.pre_tokenizers.PreTokenizer`",name:"pretok"}]}}),ce=new C({props:{name:"pre_tokenize_str",anchor:"tokenizers.pre_tokenizers.PreTokenizer.pre_tokenize_str",parameters:[{name:"sequence",val:""}],parametersDescription:[{anchor:"tokenizers.pre_tokenizers.PreTokenizer.pre_tokenize_str.sequence",description:`<strong>sequence</strong> (<code>str</code>) &#x2014;
A string to pre-tokeize`,name:"sequence"}],returnDescription:`
<p>A list of tuple with the pre-tokenized parts and their offsets</p>
`,returnType:`
<p><code>List[Tuple[str, Offsets]]</code></p>
`}}),de=new C({props:{name:"class tokenizers.pre_tokenizers.Punctuation",anchor:"tokenizers.pre_tokenizers.Punctuation",parameters:[{name:"behavior",val:" = 'isolated'"}],parametersDescription:[{anchor:"tokenizers.pre_tokenizers.Punctuation.behavior",description:`<strong>behavior</strong> (<code>SplitDelimiterBehavior</code>) &#x2014;
The behavior to use when splitting.
Choices: &#x201C;removed&#x201D;, &#x201C;isolated&#x201D; (default), &#x201C;merged_with_previous&#x201D;, &#x201C;merged_with_next&#x201D;,
&#x201C;contiguous&#x201D;`,name:"behavior"}]}}),he=new C({props:{name:"class tokenizers.pre_tokenizers.Sequence",anchor:"tokenizers.pre_tokenizers.Sequence",parameters:[{name:"pretokenizers",val:""}]}}),fe=new C({props:{name:"class tokenizers.pre_tokenizers.Split",anchor:"tokenizers.pre_tokenizers.Split",parameters:[{name:"pattern",val:""},{name:"behavior",val:""},{name:"invert",val:" = False"}],parametersDescription:[{anchor:"tokenizers.pre_tokenizers.Split.pattern",description:`<strong>pattern</strong> (<code>str</code> or <code>Regex</code>) &#x2014;
A pattern used to split the string. Usually a string or a Regex`,name:"pattern"},{anchor:"tokenizers.pre_tokenizers.Split.behavior",description:`<strong>behavior</strong> (<code>SplitDelimiterBehavior</code>) &#x2014;
The behavior to use when splitting.
Choices: &#x201C;removed&#x201D;, &#x201C;isolated&#x201D;, &#x201C;merged_with_previous&#x201D;, &#x201C;merged_with_next&#x201D;,
&#x201C;contiguous&#x201D;`,name:"behavior"},{anchor:"tokenizers.pre_tokenizers.Split.invert",description:`<strong>invert</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to invert the pattern.`,name:"invert"}]}}),me=new C({props:{name:"class tokenizers.pre_tokenizers.UnicodeScripts",anchor:"tokenizers.pre_tokenizers.UnicodeScripts",parameters:[]}}),ge=new C({props:{name:"class tokenizers.pre_tokenizers.Whitespace",anchor:"tokenizers.pre_tokenizers.Whitespace",parameters:[]}}),$e=new C({props:{name:"class tokenizers.pre_tokenizers.WhitespaceSplit",anchor:"tokenizers.pre_tokenizers.WhitespaceSplit",parameters:[]}}),{c(){s=o("div"),v(f.$$.fragment),n=c(),h=o("p"),$=l("BertPreTokenizer"),b=c(),x=o("p"),B=l(`This pre-tokenizer splits tokens on spaces, and also on punctuation.
Each occurence of a punctuation character will be treated separately.`),X=c(),y=o("div"),v(D.$$.fragment),Y=c(),u=o("p"),I=l("ByteLevel PreTokenizer"),V=c(),Z=o("p"),ze=l(`This pre-tokenizer takes care of replacing all bytes of the given string
with a corresponding representation, as well as splitting into words.`),we=c(),M=o("div"),v(te.$$.fragment),_t=c(),Ie=o("p"),zt=l("Returns the alphabet used by this PreTokenizer."),wt=c(),qe=o("p"),Tt=l(`Since the ByteLevel works as its name suggests, at the byte level, it
encodes each byte value to a unique visible character. This means that there is a
total of 256 different characters composing this alphabet.`),tt=c(),U=o("div"),v(re.$$.fragment),Pt=c(),Te=o("p"),yt=l("This pre-tokenizer simply splits on the provided char. Works like "),Be=o("code"),bt=l(".split(delimiter)"),rt=c(),E=o("div"),v(se.$$.fragment),Et=c(),Ae=o("p"),xt=l("This pre-tokenizer simply splits using the digits in separate tokens"),Dt=c(),Ve=o("p"),St=l("If set to True, digits will each be separated as follows:"),Ct=c(),v(ne.$$.fragment),It=c(),Le=o("p"),qt=l("If set to False, digits will grouped as follows:"),Bt=c(),v(oe.$$.fragment),st=c(),L=o("div"),v(ae.$$.fragment),At=c(),je=o("p"),Vt=l("Metaspace pre-tokenizer"),Lt=c(),Me=o("p"),jt=l(`This pre-tokenizer replaces any whitespace by the provided replacement character.
It then tries to split on these spaces.`),nt=c(),S=o("div"),v(ie.$$.fragment),Mt=c(),Oe=o("p"),Ot=l("Base class for all pre-tokenizers"),Wt=c(),We=o("p"),Ft=l(`This class is not supposed to be instantiated directly. Instead, any implementation of a
PreTokenizer will return an instance of this class when instantiated.`),Rt=c(),O=o("div"),v(le.$$.fragment),Ht=c(),pe=o("p"),Ut=l("Pre-tokenize a "),Fe=o("code"),Nt=l("PyPreTokenizedString"),Kt=l(" in-place"),Jt=c(),W=o("p"),Gt=l("This method allows to modify a "),Re=o("code"),Qt=l("PreTokenizedString"),Xt=l(` to
keep track of the pre-tokenization, and leverage the capabilities of the
`),He=o("code"),Yt=l("PreTokenizedString"),Zt=l(`. If you just want to see the result of
the pre-tokenization of a raw string, you can use
`),Ue=o("code"),er=l("pre_tokenize_str()"),tr=c(),F=o("div"),v(ce.$$.fragment),rr=c(),Ne=o("p"),sr=l("Pre tokenize the given string"),nr=c(),R=o("p"),or=l(`This method provides a way to visualize the effect of a
`),Pe=o("a"),ar=l("PreTokenizer"),ir=l(` but it does not keep track of the
alignment, nor does it provide all the capabilities of the
`),Ke=o("code"),lr=l("PreTokenizedString"),pr=l(`. If you need some of these, you can use
`),Je=o("code"),cr=l("pre_tokenize()"),ot=c(),N=o("div"),v(de.$$.fragment),dr=c(),Ge=o("p"),hr=l("This pre-tokenizer simply splits on punctuation as individual characters."),at=c(),K=o("div"),v(he.$$.fragment),fr=c(),Qe=o("p"),mr=l("This pre-tokenizer composes other pre_tokenizers and applies them in sequence"),it=c(),j=o("div"),v(fe.$$.fragment),ur=c(),Xe=o("p"),kr=l("Split PreTokenizer"),gr=c(),Ye=o("p"),$r=l(`This versatile pre-tokenizer splits using the provided pattern and
according to the provided behavior. The pattern can be inverted by
making use of the invert flag.`),lt=c(),J=o("div"),v(me.$$.fragment),vr=c(),ue=o("p"),_r=l(`This pre-tokenizer splits on characters that belong to different language family
It roughly follows `),ke=o("a"),zr=l("https://github.com/google/sentencepiece/blob/master/data/Scripts.txt"),wr=l(`
Actually Hiragana and Katakana are fused with Han, and 0x30FC is Han too.
This mimicks SentencePiece Unigram implementation.`),pt=c(),G=o("div"),v(ge.$$.fragment),Tr=c(),ye=o("p"),Pr=l("This pre-tokenizer simply splits using the following regex: "),Ze=o("code"),yr=l("\\w+|[^\\w\\s]+"),ct=c(),Q=o("div"),v($e.$$.fragment),br=c(),be=o("p"),Er=l("This pre-tokenizer simply splits on the whitespace. Works like "),et=o("code"),xr=l(".split()"),this.h()},l(t){s=a(t,"DIV",{class:!0});var m=i(s);_(f.$$.fragment,m),n=d(m),h=a(m,"P",{});var qr=i(h);$=p(qr,"BertPreTokenizer"),qr.forEach(r),b=d(m),x=a(m,"P",{});var Br=i(x);B=p(Br,`This pre-tokenizer splits tokens on spaces, and also on punctuation.
Each occurence of a punctuation character will be treated separately.`),Br.forEach(r),m.forEach(r),X=d(t),y=a(t,"DIV",{class:!0});var ee=i(y);_(D.$$.fragment,ee),Y=d(ee),u=a(ee,"P",{});var Ar=i(u);I=p(Ar,"ByteLevel PreTokenizer"),Ar.forEach(r),V=d(ee),Z=a(ee,"P",{});var Vr=i(Z);ze=p(Vr,`This pre-tokenizer takes care of replacing all bytes of the given string
with a corresponding representation, as well as splitting into words.`),Vr.forEach(r),we=d(ee),M=a(ee,"DIV",{class:!0});var Ee=i(M);_(te.$$.fragment,Ee),_t=d(Ee),Ie=a(Ee,"P",{});var Lr=i(Ie);zt=p(Lr,"Returns the alphabet used by this PreTokenizer."),Lr.forEach(r),wt=d(Ee),qe=a(Ee,"P",{});var jr=i(qe);Tt=p(jr,`Since the ByteLevel works as its name suggests, at the byte level, it
encodes each byte value to a unique visible character. This means that there is a
total of 256 different characters composing this alphabet.`),jr.forEach(r),Ee.forEach(r),ee.forEach(r),tt=d(t),U=a(t,"DIV",{class:!0});var ht=i(U);_(re.$$.fragment,ht),Pt=d(ht),Te=a(ht,"P",{});var Dr=i(Te);yt=p(Dr,"This pre-tokenizer simply splits on the provided char. Works like "),Be=a(Dr,"CODE",{});var Mr=i(Be);bt=p(Mr,".split(delimiter)"),Mr.forEach(r),Dr.forEach(r),ht.forEach(r),rt=d(t),E=a(t,"DIV",{class:!0});var A=i(E);_(se.$$.fragment,A),Et=d(A),Ae=a(A,"P",{});var Or=i(Ae);xt=p(Or,"This pre-tokenizer simply splits using the digits in separate tokens"),Or.forEach(r),Dt=d(A),Ve=a(A,"P",{});var Wr=i(Ve);St=p(Wr,"If set to True, digits will each be separated as follows:"),Wr.forEach(r),Ct=d(A),_(ne.$$.fragment,A),It=d(A),Le=a(A,"P",{});var Fr=i(Le);qt=p(Fr,"If set to False, digits will grouped as follows:"),Fr.forEach(r),Bt=d(A),_(oe.$$.fragment,A),A.forEach(r),st=d(t),L=a(t,"DIV",{class:!0});var xe=i(L);_(ae.$$.fragment,xe),At=d(xe),je=a(xe,"P",{});var Rr=i(je);Vt=p(Rr,"Metaspace pre-tokenizer"),Rr.forEach(r),Lt=d(xe),Me=a(xe,"P",{});var Hr=i(Me);jt=p(Hr,`This pre-tokenizer replaces any whitespace by the provided replacement character.
It then tries to split on these spaces.`),Hr.forEach(r),xe.forEach(r),nt=d(t),S=a(t,"DIV",{class:!0});var H=i(S);_(ie.$$.fragment,H),Mt=d(H),Oe=a(H,"P",{});var Ur=i(Oe);Ot=p(Ur,"Base class for all pre-tokenizers"),Ur.forEach(r),Wt=d(H),We=a(H,"P",{});var Nr=i(We);Ft=p(Nr,`This class is not supposed to be instantiated directly. Instead, any implementation of a
PreTokenizer will return an instance of this class when instantiated.`),Nr.forEach(r),Rt=d(H),O=a(H,"DIV",{class:!0});var De=i(O);_(le.$$.fragment,De),Ht=d(De),pe=a(De,"P",{});var ft=i(pe);Ut=p(ft,"Pre-tokenize a "),Fe=a(ft,"CODE",{});var Kr=i(Fe);Nt=p(Kr,"PyPreTokenizedString"),Kr.forEach(r),Kt=p(ft," in-place"),ft.forEach(r),Jt=d(De),W=a(De,"P",{});var ve=i(W);Gt=p(ve,"This method allows to modify a "),Re=a(ve,"CODE",{});var Jr=i(Re);Qt=p(Jr,"PreTokenizedString"),Jr.forEach(r),Xt=p(ve,` to
keep track of the pre-tokenization, and leverage the capabilities of the
`),He=a(ve,"CODE",{});var Gr=i(He);Yt=p(Gr,"PreTokenizedString"),Gr.forEach(r),Zt=p(ve,`. If you just want to see the result of
the pre-tokenization of a raw string, you can use
`),Ue=a(ve,"CODE",{});var Qr=i(Ue);er=p(Qr,"pre_tokenize_str()"),Qr.forEach(r),ve.forEach(r),De.forEach(r),tr=d(H),F=a(H,"DIV",{class:!0});var Se=i(F);_(ce.$$.fragment,Se),rr=d(Se),Ne=a(Se,"P",{});var Xr=i(Ne);sr=p(Xr,"Pre tokenize the given string"),Xr.forEach(r),nr=d(Se),R=a(Se,"P",{});var _e=i(R);or=p(_e,`This method provides a way to visualize the effect of a
`),Pe=a(_e,"A",{href:!0});var Yr=i(Pe);ar=p(Yr,"PreTokenizer"),Yr.forEach(r),ir=p(_e,` but it does not keep track of the
alignment, nor does it provide all the capabilities of the
`),Ke=a(_e,"CODE",{});var Zr=i(Ke);lr=p(Zr,"PreTokenizedString"),Zr.forEach(r),pr=p(_e,`. If you need some of these, you can use
`),Je=a(_e,"CODE",{});var es=i(Je);cr=p(es,"pre_tokenize()"),es.forEach(r),_e.forEach(r),Se.forEach(r),H.forEach(r),ot=d(t),N=a(t,"DIV",{class:!0});var mt=i(N);_(de.$$.fragment,mt),dr=d(mt),Ge=a(mt,"P",{});var ts=i(Ge);hr=p(ts,"This pre-tokenizer simply splits on punctuation as individual characters."),ts.forEach(r),mt.forEach(r),at=d(t),K=a(t,"DIV",{class:!0});var ut=i(K);_(he.$$.fragment,ut),fr=d(ut),Qe=a(ut,"P",{});var rs=i(Qe);mr=p(rs,"This pre-tokenizer composes other pre_tokenizers and applies them in sequence"),rs.forEach(r),ut.forEach(r),it=d(t),j=a(t,"DIV",{class:!0});var Ce=i(j);_(fe.$$.fragment,Ce),ur=d(Ce),Xe=a(Ce,"P",{});var ss=i(Xe);kr=p(ss,"Split PreTokenizer"),ss.forEach(r),gr=d(Ce),Ye=a(Ce,"P",{});var ns=i(Ye);$r=p(ns,`This versatile pre-tokenizer splits using the provided pattern and
according to the provided behavior. The pattern can be inverted by
making use of the invert flag.`),ns.forEach(r),Ce.forEach(r),lt=d(t),J=a(t,"DIV",{class:!0});var kt=i(J);_(me.$$.fragment,kt),vr=d(kt),ue=a(kt,"P",{});var gt=i(ue);_r=p(gt,`This pre-tokenizer splits on characters that belong to different language family
It roughly follows `),ke=a(gt,"A",{href:!0,rel:!0});var os=i(ke);zr=p(os,"https://github.com/google/sentencepiece/blob/master/data/Scripts.txt"),os.forEach(r),wr=p(gt,`
Actually Hiragana and Katakana are fused with Han, and 0x30FC is Han too.
This mimicks SentencePiece Unigram implementation.`),gt.forEach(r),kt.forEach(r),pt=d(t),G=a(t,"DIV",{class:!0});var $t=i(G);_(ge.$$.fragment,$t),Tr=d($t),ye=a($t,"P",{});var Sr=i(ye);Pr=p(Sr,"This pre-tokenizer simply splits using the following regex: "),Ze=a(Sr,"CODE",{});var as=i(Ze);yr=p(as,"\\w+|[^\\w\\s]+"),as.forEach(r),Sr.forEach(r),$t.forEach(r),ct=d(t),Q=a(t,"DIV",{class:!0});var vt=i(Q);_($e.$$.fragment,vt),br=d(vt),be=a(vt,"P",{});var Cr=i(be);Er=p(Cr,"This pre-tokenizer simply splits on the whitespace. Works like "),et=a(Cr,"CODE",{});var is=i(et);xr=p(is,".split()"),is.forEach(r),Cr.forEach(r),vt.forEach(r),this.h()},h(){g(s,"class","docstring"),g(M,"class","docstring"),g(y,"class","docstring"),g(U,"class","docstring"),g(E,"class","docstring"),g(L,"class","docstring"),g(O,"class","docstring"),g(Pe,"href","/docs/tokenizers/pr_1/en/api/pre-tokenizers#tokenizers.pre_tokenizers.PreTokenizer"),g(F,"class","docstring"),g(S,"class","docstring"),g(N,"class","docstring"),g(K,"class","docstring"),g(j,"class","docstring"),g(ke,"href","https://github.com/google/sentencepiece/blob/master/data/Scripts.txt"),g(ke,"rel","nofollow"),g(J,"class","docstring"),g(G,"class","docstring"),g(Q,"class","docstring")},m(t,m){k(t,s,m),z(f,s,null),e(s,n),e(s,h),e(h,$),e(s,b),e(s,x),e(x,B),k(t,X,m),k(t,y,m),z(D,y,null),e(y,Y),e(y,u),e(u,I),e(y,V),e(y,Z),e(Z,ze),e(y,we),e(y,M),z(te,M,null),e(M,_t),e(M,Ie),e(Ie,zt),e(M,wt),e(M,qe),e(qe,Tt),k(t,tt,m),k(t,U,m),z(re,U,null),e(U,Pt),e(U,Te),e(Te,yt),e(Te,Be),e(Be,bt),k(t,rt,m),k(t,E,m),z(se,E,null),e(E,Et),e(E,Ae),e(Ae,xt),e(E,Dt),e(E,Ve),e(Ve,St),e(E,Ct),z(ne,E,null),e(E,It),e(E,Le),e(Le,qt),e(E,Bt),z(oe,E,null),k(t,st,m),k(t,L,m),z(ae,L,null),e(L,At),e(L,je),e(je,Vt),e(L,Lt),e(L,Me),e(Me,jt),k(t,nt,m),k(t,S,m),z(ie,S,null),e(S,Mt),e(S,Oe),e(Oe,Ot),e(S,Wt),e(S,We),e(We,Ft),e(S,Rt),e(S,O),z(le,O,null),e(O,Ht),e(O,pe),e(pe,Ut),e(pe,Fe),e(Fe,Nt),e(pe,Kt),e(O,Jt),e(O,W),e(W,Gt),e(W,Re),e(Re,Qt),e(W,Xt),e(W,He),e(He,Yt),e(W,Zt),e(W,Ue),e(Ue,er),e(S,tr),e(S,F),z(ce,F,null),e(F,rr),e(F,Ne),e(Ne,sr),e(F,nr),e(F,R),e(R,or),e(R,Pe),e(Pe,ar),e(R,ir),e(R,Ke),e(Ke,lr),e(R,pr),e(R,Je),e(Je,cr),k(t,ot,m),k(t,N,m),z(de,N,null),e(N,dr),e(N,Ge),e(Ge,hr),k(t,at,m),k(t,K,m),z(he,K,null),e(K,fr),e(K,Qe),e(Qe,mr),k(t,it,m),k(t,j,m),z(fe,j,null),e(j,ur),e(j,Xe),e(Xe,kr),e(j,gr),e(j,Ye),e(Ye,$r),k(t,lt,m),k(t,J,m),z(me,J,null),e(J,vr),e(J,ue),e(ue,_r),e(ue,ke),e(ke,zr),e(ue,wr),k(t,pt,m),k(t,G,m),z(ge,G,null),e(G,Tr),e(G,ye),e(ye,Pr),e(ye,Ze),e(Ze,yr),k(t,ct,m),k(t,Q,m),z($e,Q,null),e(Q,br),e(Q,be),e(be,Er),e(be,et),e(et,xr),dt=!0},p:ms,i(t){dt||(w(f.$$.fragment,t),w(D.$$.fragment,t),w(te.$$.fragment,t),w(re.$$.fragment,t),w(se.$$.fragment,t),w(ne.$$.fragment,t),w(oe.$$.fragment,t),w(ae.$$.fragment,t),w(ie.$$.fragment,t),w(le.$$.fragment,t),w(ce.$$.fragment,t),w(de.$$.fragment,t),w(he.$$.fragment,t),w(fe.$$.fragment,t),w(me.$$.fragment,t),w(ge.$$.fragment,t),w($e.$$.fragment,t),dt=!0)},o(t){T(f.$$.fragment,t),T(D.$$.fragment,t),T(te.$$.fragment,t),T(re.$$.fragment,t),T(se.$$.fragment,t),T(ne.$$.fragment,t),T(oe.$$.fragment,t),T(ae.$$.fragment,t),T(ie.$$.fragment,t),T(le.$$.fragment,t),T(ce.$$.fragment,t),T(de.$$.fragment,t),T(he.$$.fragment,t),T(fe.$$.fragment,t),T(me.$$.fragment,t),T(ge.$$.fragment,t),T($e.$$.fragment,t),dt=!1},d(t){t&&r(s),P(f),t&&r(X),t&&r(y),P(D),P(te),t&&r(tt),t&&r(U),P(re),t&&r(rt),t&&r(E),P(se),P(ne),P(oe),t&&r(st),t&&r(L),P(ae),t&&r(nt),t&&r(S),P(ie),P(le),P(ce),t&&r(ot),t&&r(N),P(de),t&&r(at),t&&r(K),P(he),t&&r(it),t&&r(j),P(fe),t&&r(lt),t&&r(J),P(me),t&&r(pt),t&&r(G),P(ge),t&&r(ct),t&&r(Q),P($e)}}}function $s(q){let s,f;return s=new Ir({props:{$$slots:{default:[gs]},$$scope:{ctx:q}}}),{c(){v(s.$$.fragment)},l(n){_(s.$$.fragment,n)},m(n,h){z(s,n,h),f=!0},p(n,h){const $={};h&2&&($.$$scope={dirty:h,ctx:n}),s.$set($)},i(n){f||(w(s.$$.fragment,n),f=!0)},o(n){T(s.$$.fragment,n),f=!1},d(n){P(s,n)}}}function vs(q){let s,f,n,h,$;return{c(){s=o("p"),f=l("The Rust API Reference is available directly on the "),n=o("a"),h=l("Docs.rs"),$=l(" website."),this.h()},l(b){s=a(b,"P",{});var x=i(s);f=p(x,"The Rust API Reference is available directly on the "),n=a(x,"A",{href:!0,rel:!0});var B=i(n);h=p(B,"Docs.rs"),B.forEach(r),$=p(x," website."),x.forEach(r),this.h()},h(){g(n,"href","https://docs.rs/tokenizers/latest/tokenizers/"),g(n,"rel","nofollow")},m(b,x){k(b,s,x),e(s,f),e(s,n),e(n,h),e(s,$)},d(b){b&&r(s)}}}function _s(q){let s,f;return s=new Ir({props:{$$slots:{default:[vs]},$$scope:{ctx:q}}}),{c(){v(s.$$.fragment)},l(n){_(s.$$.fragment,n)},m(n,h){z(s,n,h),f=!0},p(n,h){const $={};h&2&&($.$$scope={dirty:h,ctx:n}),s.$set($)},i(n){f||(w(s.$$.fragment,n),f=!0)},o(n){T(s.$$.fragment,n),f=!1},d(n){P(s,n)}}}function zs(q){let s,f;return{c(){s=o("p"),f=l("The node API has not been documented yet.")},l(n){s=a(n,"P",{});var h=i(s);f=p(h,"The node API has not been documented yet."),h.forEach(r)},m(n,h){k(n,s,h),e(s,f)},d(n){n&&r(s)}}}function ws(q){let s,f;return s=new Ir({props:{$$slots:{default:[zs]},$$scope:{ctx:q}}}),{c(){v(s.$$.fragment)},l(n){_(s.$$.fragment,n)},m(n,h){z(s,n,h),f=!0},p(n,h){const $={};h&2&&($.$$scope={dirty:h,ctx:n}),s.$set($)},i(n){f||(w(s.$$.fragment,n),f=!0)},o(n){T(s.$$.fragment,n),f=!1},d(n){P(s,n)}}}function Ts(q){let s,f,n,h,$,b,x,B,X,y,D,Y;return b=new us({}),D=new ks({props:{python:!0,rust:!0,node:!0,$$slots:{node:[ws],rust:[_s],python:[$s]},$$scope:{ctx:q}}}),{c(){s=o("meta"),f=c(),n=o("h1"),h=o("a"),$=o("span"),v(b.$$.fragment),x=c(),B=o("span"),X=l("Pre-tokenizers"),y=c(),v(D.$$.fragment),this.h()},l(u){const I=hs('[data-svelte="svelte-1phssyn"]',document.head);s=a(I,"META",{name:!0,content:!0}),I.forEach(r),f=d(u),n=a(u,"H1",{class:!0});var V=i(n);h=a(V,"A",{id:!0,class:!0,href:!0});var Z=i(h);$=a(Z,"SPAN",{});var ze=i($);_(b.$$.fragment,ze),ze.forEach(r),Z.forEach(r),x=d(V),B=a(V,"SPAN",{});var we=i(B);X=p(we,"Pre-tokenizers"),we.forEach(r),V.forEach(r),y=d(u),_(D.$$.fragment,u),this.h()},h(){g(s,"name","hf:doc:metadata"),g(s,"content",JSON.stringify(Ps)),g(h,"id","tokenizers.pre_tokenizers.BertPreTokenizer"),g(h,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),g(h,"href","#tokenizers.pre_tokenizers.BertPreTokenizer"),g(n,"class","relative group")},m(u,I){e(document.head,s),k(u,f,I),k(u,n,I),e(n,h),e(h,$),z(b,$,null),e(n,x),e(n,B),e(B,X),k(u,y,I),z(D,u,I),Y=!0},p(u,[I]){const V={};I&2&&(V.$$scope={dirty:I,ctx:u}),D.$set(V)},i(u){Y||(w(b.$$.fragment,u),w(D.$$.fragment,u),Y=!0)},o(u){T(b.$$.fragment,u),T(D.$$.fragment,u),Y=!1},d(u){r(s),u&&r(f),u&&r(n),P(b),u&&r(y),P(D,u)}}}const Ps={local:"tokenizers.pre_tokenizers.BertPreTokenizer",title:"Pre-tokenizers"};function ys(q){return fs(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Cs extends ps{constructor(s){super();cs(this,s,ys,Ts,ds,{})}}export{Cs as default,Ps as metadata};
