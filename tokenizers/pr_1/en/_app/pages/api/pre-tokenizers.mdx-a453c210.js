import{S as Jo,i as Go,s as Qo,e as s,k as a,w as v,t as h,M as Xo,c as n,d as r,m as l,a as o,x as z,h as d,b as i,F as t,g as k,y as $,q as _,o as g,B as w,v as Yo,L as Zo}from"../../chunks/vendor-0d3f0756.js";import{D as x}from"../../chunks/Docstring-cebdcc97.js";import{C as Ko}from"../../chunks/CodeBlock-7b0cb15c.js";import{I}from"../../chunks/IconCopyLink-9193371d.js";import{T as ei,M as Bn}from"../../chunks/TokenizersLanguageContent-ca787841.js";function ti(W){let c,m,p,u,y,P,D,C,S,N,L,j,b,A,q,st,ye,M,de,kt,Pe,Jr,ut,Gr,ar,B,Ee,Qr,mt,Xr,Yr,vt,Zr,es,O,Se,ts,zt,rs,ss,$t,ns,lr,Q,fe,_t,Te,os,gt,is,pr,X,xe,as,nt,ls,wt,ps,cr,Y,ke,bt,De,cs,yt,hs,hr,E,Ae,ds,Pt,fs,ks,Et,us,ms,Ce,vs,St,zs,$s,qe,dr,Z,ue,Tt,Be,_s,xt,gs,fr,U,Ie,ws,Dt,bs,ys,At,Ps,kr,ee,me,Ct,We,Es,qt,Ss,ur,T,Ne,Ts,Bt,xs,Ds,It,As,Cs,F,Le,qs,Me,Bs,Wt,Is,Ws,Ns,R,Ls,Nt,Ms,Hs,Lt,Us,Vs,Mt,js,Os,K,He,Fs,Ht,Rs,Ks,J,Js,ot,Gs,Qs,Ut,Xs,Ys,Vt,Zs,mr,te,ve,jt,Ue,en,Ot,tn,vr,re,Ve,rn,Ft,sn,zr,se,ze,Rt,je,nn,Kt,on,$r,ne,Oe,an,Jt,ln,_r,oe,$e,Gt,Fe,pn,Qt,cn,gr,V,Re,hn,Xt,dn,fn,Yt,kn,wr,ie,_e,Zt,Ke,un,er,mn,br,ae,Je,vn,Ge,zn,Qe,$n,_n,yr,le,ge,tr,Xe,gn,rr,wn,Pr,pe,Ye,bn,it,yn,sr,Pn,Er,ce,we,nr,Ze,En,or,Sn,Sr,he,et,Tn,at,xn,ir,Dn,Tr;return u=new I({}),N=new x({props:{name:"class tokenizers.pre_tokenizers.BertPreTokenizer",anchor:"tokenizers.pre_tokenizers.BertPreTokenizer",parameters:[]}}),Pe=new I({}),Ee=new x({props:{name:"class tokenizers.pre_tokenizers.ByteLevel",anchor:"tokenizers.pre_tokenizers.ByteLevel",parameters:[{name:"add_prefix_space",val:" = True"}],parametersDescription:[{anchor:"tokenizers.pre_tokenizers.ByteLevel.add_prefix_space",description:`<strong>add_prefix_space</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to add a space to the first word if there isn&#x2019;t already one. This
lets us treat <em>hello</em> exactly like <em>say hello</em>.`,name:"add_prefix_space"}]}}),Se=new x({props:{name:"alphabet",anchor:"tokenizers.pre_tokenizers.ByteLevel.alphabet",parameters:[],returnDescription:`
<p>A list of characters that compose the alphabet</p>
`,returnType:`
<p><code>List[str]</code></p>
`}}),Te=new I({}),xe=new x({props:{name:"class tokenizers.pre_tokenizers.CharDelimiterSplit",anchor:"tokenizers.pre_tokenizers.CharDelimiterSplit",parameters:""}}),De=new I({}),Ae=new x({props:{name:"class tokenizers.pre_tokenizers.Digits",anchor:"tokenizers.pre_tokenizers.Digits",parameters:[{name:"individual_digits",val:" = False"}],parametersDescription:[{anchor:"tokenizers.pre_tokenizers.Digits.individual_digits",description:"<strong>individual_digits</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;",name:"individual_digits"}]}}),Ce=new Ko({props:{code:'"Call 123 please" -> "Call ", "1", "2", "3", " please"',highlighted:'<span class="hljs-string">&quot;Call 123 please&quot;</span> -&gt; <span class="hljs-string">&quot;Call &quot;</span>, <span class="hljs-string">&quot;1&quot;</span>, <span class="hljs-string">&quot;2&quot;</span>, <span class="hljs-string">&quot;3&quot;</span>, <span class="hljs-string">&quot; please&quot;</span>'}}),qe=new Ko({props:{code:'"Call 123 please" -> "Call ", "123", " please"',highlighted:'<span class="hljs-string">&quot;Call 123 please&quot;</span> -&gt; <span class="hljs-string">&quot;Call &quot;</span>, <span class="hljs-string">&quot;123&quot;</span>, <span class="hljs-string">&quot; please&quot;</span>'}}),Be=new I({}),Ie=new x({props:{name:"class tokenizers.pre_tokenizers.Metaspace",anchor:"tokenizers.pre_tokenizers.Metaspace",parameters:[{name:"replacement",val:" = '_'"},{name:"add_prefix_space",val:" = True"}],parametersDescription:[{anchor:"tokenizers.pre_tokenizers.Metaspace.replacement",description:`<strong>replacement</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&#x2581;</code>) &#x2014;
The replacement character. Must be exactly one character. By default we
use the <em>&#x2581;</em> (U+2581) meta symbol (Same as in SentencePiece).`,name:"replacement"},{anchor:"tokenizers.pre_tokenizers.Metaspace.add_prefix_space",description:`<strong>add_prefix_space</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to add a space to the first word if there isn&#x2019;t already one. This
lets us treat <em>hello</em> exactly like <em>say hello</em>.`,name:"add_prefix_space"}]}}),We=new I({}),Ne=new x({props:{name:"class tokenizers.pre_tokenizers.PreTokenizer",anchor:"tokenizers.pre_tokenizers.PreTokenizer",parameters:[]}}),Le=new x({props:{name:"pre_tokenize",anchor:"tokenizers.pre_tokenizers.PreTokenizer.pre_tokenize",parameters:[{name:"pretok",val:""}],parametersDescription:[{anchor:"tokenizers.pre_tokenizers.PreTokenizer.pre_tokenize.pretok",description:"<strong>pretok</strong> (<code>PreTokenizedString) -- The pre-tokenized string on which to apply this :class:</code>~tokenizers.pre_tokenizers.PreTokenizer`",name:"pretok"}]}}),He=new x({props:{name:"pre_tokenize_str",anchor:"tokenizers.pre_tokenizers.PreTokenizer.pre_tokenize_str",parameters:[{name:"sequence",val:""}],parametersDescription:[{anchor:"tokenizers.pre_tokenizers.PreTokenizer.pre_tokenize_str.sequence",description:`<strong>sequence</strong> (<code>str</code>) &#x2014;
A string to pre-tokeize`,name:"sequence"}],returnDescription:`
<p>A list of tuple with the pre-tokenized parts and their offsets</p>
`,returnType:`
<p><code>List[Tuple[str, Offsets]]</code></p>
`}}),Ue=new I({}),Ve=new x({props:{name:"class tokenizers.pre_tokenizers.Punctuation",anchor:"tokenizers.pre_tokenizers.Punctuation",parameters:[{name:"behavior",val:" = 'isolated'"}],parametersDescription:[{anchor:"tokenizers.pre_tokenizers.Punctuation.behavior",description:`<strong>behavior</strong> (<code>SplitDelimiterBehavior</code>) &#x2014;
The behavior to use when splitting.
Choices: &#x201C;removed&#x201D;, &#x201C;isolated&#x201D; (default), &#x201C;merged_with_previous&#x201D;, &#x201C;merged_with_next&#x201D;,
&#x201C;contiguous&#x201D;`,name:"behavior"}]}}),je=new I({}),Oe=new x({props:{name:"class tokenizers.pre_tokenizers.Sequence",anchor:"tokenizers.pre_tokenizers.Sequence",parameters:[{name:"pretokenizers",val:""}]}}),Fe=new I({}),Re=new x({props:{name:"class tokenizers.pre_tokenizers.Split",anchor:"tokenizers.pre_tokenizers.Split",parameters:[{name:"pattern",val:""},{name:"behavior",val:""},{name:"invert",val:" = False"}],parametersDescription:[{anchor:"tokenizers.pre_tokenizers.Split.pattern",description:`<strong>pattern</strong> (<code>str</code> or <code>Regex</code>) &#x2014;
A pattern used to split the string. Usually a string or a Regex`,name:"pattern"},{anchor:"tokenizers.pre_tokenizers.Split.behavior",description:`<strong>behavior</strong> (<code>SplitDelimiterBehavior</code>) &#x2014;
The behavior to use when splitting.
Choices: &#x201C;removed&#x201D;, &#x201C;isolated&#x201D;, &#x201C;merged_with_previous&#x201D;, &#x201C;merged_with_next&#x201D;,
&#x201C;contiguous&#x201D;`,name:"behavior"},{anchor:"tokenizers.pre_tokenizers.Split.invert",description:`<strong>invert</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to invert the pattern.`,name:"invert"}]}}),Ke=new I({}),Je=new x({props:{name:"class tokenizers.pre_tokenizers.UnicodeScripts",anchor:"tokenizers.pre_tokenizers.UnicodeScripts",parameters:[]}}),Xe=new I({}),Ye=new x({props:{name:"class tokenizers.pre_tokenizers.Whitespace",anchor:"tokenizers.pre_tokenizers.Whitespace",parameters:[]}}),Ze=new I({}),et=new x({props:{name:"class tokenizers.pre_tokenizers.WhitespaceSplit",anchor:"tokenizers.pre_tokenizers.WhitespaceSplit",parameters:[]}}),{c(){c=s("h2"),m=s("a"),p=s("span"),v(u.$$.fragment),y=a(),P=s("span"),D=h("BertPreTokenizer"),C=a(),S=s("div"),v(N.$$.fragment),L=a(),j=s("p"),b=h("BertPreTokenizer"),A=a(),q=s("p"),st=h(`This pre-tokenizer splits tokens on spaces, and also on punctuation.
Each occurence of a punctuation character will be treated separately.`),ye=a(),M=s("h2"),de=s("a"),kt=s("span"),v(Pe.$$.fragment),Jr=a(),ut=s("span"),Gr=h("ByteLevel"),ar=a(),B=s("div"),v(Ee.$$.fragment),Qr=a(),mt=s("p"),Xr=h("ByteLevel PreTokenizer"),Yr=a(),vt=s("p"),Zr=h(`This pre-tokenizer takes care of replacing all bytes of the given string
with a corresponding representation, as well as splitting into words.`),es=a(),O=s("div"),v(Se.$$.fragment),ts=a(),zt=s("p"),rs=h("Returns the alphabet used by this PreTokenizer."),ss=a(),$t=s("p"),ns=h(`Since the ByteLevel works as its name suggests, at the byte level, it
encodes each byte value to a unique visible character. This means that there is a
total of 256 different characters composing this alphabet.`),lr=a(),Q=s("h2"),fe=s("a"),_t=s("span"),v(Te.$$.fragment),os=a(),gt=s("span"),is=h("CharDelimiterSplit"),pr=a(),X=s("div"),v(xe.$$.fragment),as=a(),nt=s("p"),ls=h("This pre-tokenizer simply splits on the provided char. Works like "),wt=s("code"),ps=h(".split(delimiter)"),cr=a(),Y=s("h2"),ke=s("a"),bt=s("span"),v(De.$$.fragment),cs=a(),yt=s("span"),hs=h("Digits"),hr=a(),E=s("div"),v(Ae.$$.fragment),ds=a(),Pt=s("p"),fs=h("This pre-tokenizer simply splits using the digits in separate tokens"),ks=a(),Et=s("p"),us=h("If set to True, digits will each be separated as follows:"),ms=a(),v(Ce.$$.fragment),vs=a(),St=s("p"),zs=h("If set to False, digits will grouped as follows:"),$s=a(),v(qe.$$.fragment),dr=a(),Z=s("h2"),ue=s("a"),Tt=s("span"),v(Be.$$.fragment),_s=a(),xt=s("span"),gs=h("Metaspace"),fr=a(),U=s("div"),v(Ie.$$.fragment),ws=a(),Dt=s("p"),bs=h("Metaspace pre-tokenizer"),ys=a(),At=s("p"),Ps=h(`This pre-tokenizer replaces any whitespace by the provided replacement character.
It then tries to split on these spaces.`),kr=a(),ee=s("h2"),me=s("a"),Ct=s("span"),v(We.$$.fragment),Es=a(),qt=s("span"),Ss=h("PreTokenizer"),ur=a(),T=s("div"),v(Ne.$$.fragment),Ts=a(),Bt=s("p"),xs=h("Base class for all pre-tokenizers"),Ds=a(),It=s("p"),As=h(`This class is not supposed to be instantiated directly. Instead, any implementation of a
PreTokenizer will return an instance of this class when instantiated.`),Cs=a(),F=s("div"),v(Le.$$.fragment),qs=a(),Me=s("p"),Bs=h("Pre-tokenize a "),Wt=s("code"),Is=h("PyPreTokenizedString"),Ws=h(" in-place"),Ns=a(),R=s("p"),Ls=h("This method allows to modify a "),Nt=s("code"),Ms=h("PreTokenizedString"),Hs=h(` to
keep track of the pre-tokenization, and leverage the capabilities of the
`),Lt=s("code"),Us=h("PreTokenizedString"),Vs=h(`. If you just want to see the result of
the pre-tokenization of a raw string, you can use
`),Mt=s("code"),js=h("pre_tokenize_str()"),Os=a(),K=s("div"),v(He.$$.fragment),Fs=a(),Ht=s("p"),Rs=h("Pre tokenize the given string"),Ks=a(),J=s("p"),Js=h(`This method provides a way to visualize the effect of a
`),ot=s("a"),Gs=h("PreTokenizer"),Qs=h(` but it does not keep track of the
alignment, nor does it provide all the capabilities of the
`),Ut=s("code"),Xs=h("PreTokenizedString"),Ys=h(`. If you need some of these, you can use
`),Vt=s("code"),Zs=h("pre_tokenize()"),mr=a(),te=s("h2"),ve=s("a"),jt=s("span"),v(Ue.$$.fragment),en=a(),Ot=s("span"),tn=h("Punctuation"),vr=a(),re=s("div"),v(Ve.$$.fragment),rn=a(),Ft=s("p"),sn=h("This pre-tokenizer simply splits on punctuation as individual characters."),zr=a(),se=s("h2"),ze=s("a"),Rt=s("span"),v(je.$$.fragment),nn=a(),Kt=s("span"),on=h("Sequence"),$r=a(),ne=s("div"),v(Oe.$$.fragment),an=a(),Jt=s("p"),ln=h("This pre-tokenizer composes other pre_tokenizers and applies them in sequence"),_r=a(),oe=s("h2"),$e=s("a"),Gt=s("span"),v(Fe.$$.fragment),pn=a(),Qt=s("span"),cn=h("Split"),gr=a(),V=s("div"),v(Re.$$.fragment),hn=a(),Xt=s("p"),dn=h("Split PreTokenizer"),fn=a(),Yt=s("p"),kn=h(`This versatile pre-tokenizer splits using the provided pattern and
according to the provided behavior. The pattern can be inverted by
making use of the invert flag.`),wr=a(),ie=s("h2"),_e=s("a"),Zt=s("span"),v(Ke.$$.fragment),un=a(),er=s("span"),mn=h("UnicodeScripts"),br=a(),ae=s("div"),v(Je.$$.fragment),vn=a(),Ge=s("p"),zn=h(`This pre-tokenizer splits on characters that belong to different language family
It roughly follows `),Qe=s("a"),$n=h("https://github.com/google/sentencepiece/blob/master/data/Scripts.txt"),_n=h(`
Actually Hiragana and Katakana are fused with Han, and 0x30FC is Han too.
This mimicks SentencePiece Unigram implementation.`),yr=a(),le=s("h2"),ge=s("a"),tr=s("span"),v(Xe.$$.fragment),gn=a(),rr=s("span"),wn=h("Whitespace"),Pr=a(),pe=s("div"),v(Ye.$$.fragment),bn=a(),it=s("p"),yn=h("This pre-tokenizer simply splits using the following regex: "),sr=s("code"),Pn=h("\\w+|[^\\w\\s]+"),Er=a(),ce=s("h2"),we=s("a"),nr=s("span"),v(Ze.$$.fragment),En=a(),or=s("span"),Sn=h("WhitespaceSplit"),Sr=a(),he=s("div"),v(et.$$.fragment),Tn=a(),at=s("p"),xn=h("This pre-tokenizer simply splits on the whitespace. Works like "),ir=s("code"),Dn=h(".split()"),this.h()},l(e){c=n(e,"H2",{class:!0});var f=o(c);m=n(f,"A",{id:!0,class:!0,href:!0});var In=o(m);p=n(In,"SPAN",{});var Wn=o(p);z(u.$$.fragment,Wn),Wn.forEach(r),In.forEach(r),y=l(f),P=n(f,"SPAN",{});var Nn=o(P);D=d(Nn,"BertPreTokenizer"),Nn.forEach(r),f.forEach(r),C=l(e),S=n(e,"DIV",{class:!0});var lt=o(S);z(N.$$.fragment,lt),L=l(lt),j=n(lt,"P",{});var Ln=o(j);b=d(Ln,"BertPreTokenizer"),Ln.forEach(r),A=l(lt),q=n(lt,"P",{});var Mn=o(q);st=d(Mn,`This pre-tokenizer splits tokens on spaces, and also on punctuation.
Each occurence of a punctuation character will be treated separately.`),Mn.forEach(r),lt.forEach(r),ye=l(e),M=n(e,"H2",{class:!0});var xr=o(M);de=n(xr,"A",{id:!0,class:!0,href:!0});var Hn=o(de);kt=n(Hn,"SPAN",{});var Un=o(kt);z(Pe.$$.fragment,Un),Un.forEach(r),Hn.forEach(r),Jr=l(xr),ut=n(xr,"SPAN",{});var Vn=o(ut);Gr=d(Vn,"ByteLevel"),Vn.forEach(r),xr.forEach(r),ar=l(e),B=n(e,"DIV",{class:!0});var be=o(B);z(Ee.$$.fragment,be),Qr=l(be),mt=n(be,"P",{});var jn=o(mt);Xr=d(jn,"ByteLevel PreTokenizer"),jn.forEach(r),Yr=l(be),vt=n(be,"P",{});var On=o(vt);Zr=d(On,`This pre-tokenizer takes care of replacing all bytes of the given string
with a corresponding representation, as well as splitting into words.`),On.forEach(r),es=l(be),O=n(be,"DIV",{class:!0});var pt=o(O);z(Se.$$.fragment,pt),ts=l(pt),zt=n(pt,"P",{});var Fn=o(zt);rs=d(Fn,"Returns the alphabet used by this PreTokenizer."),Fn.forEach(r),ss=l(pt),$t=n(pt,"P",{});var Rn=o($t);ns=d(Rn,`Since the ByteLevel works as its name suggests, at the byte level, it
encodes each byte value to a unique visible character. This means that there is a
total of 256 different characters composing this alphabet.`),Rn.forEach(r),pt.forEach(r),be.forEach(r),lr=l(e),Q=n(e,"H2",{class:!0});var Dr=o(Q);fe=n(Dr,"A",{id:!0,class:!0,href:!0});var Kn=o(fe);_t=n(Kn,"SPAN",{});var Jn=o(_t);z(Te.$$.fragment,Jn),Jn.forEach(r),Kn.forEach(r),os=l(Dr),gt=n(Dr,"SPAN",{});var Gn=o(gt);is=d(Gn,"CharDelimiterSplit"),Gn.forEach(r),Dr.forEach(r),pr=l(e),X=n(e,"DIV",{class:!0});var Ar=o(X);z(xe.$$.fragment,Ar),as=l(Ar),nt=n(Ar,"P",{});var An=o(nt);ls=d(An,"This pre-tokenizer simply splits on the provided char. Works like "),wt=n(An,"CODE",{});var Qn=o(wt);ps=d(Qn,".split(delimiter)"),Qn.forEach(r),An.forEach(r),Ar.forEach(r),cr=l(e),Y=n(e,"H2",{class:!0});var Cr=o(Y);ke=n(Cr,"A",{id:!0,class:!0,href:!0});var Xn=o(ke);bt=n(Xn,"SPAN",{});var Yn=o(bt);z(De.$$.fragment,Yn),Yn.forEach(r),Xn.forEach(r),cs=l(Cr),yt=n(Cr,"SPAN",{});var Zn=o(yt);hs=d(Zn,"Digits"),Zn.forEach(r),Cr.forEach(r),hr=l(e),E=n(e,"DIV",{class:!0});var H=o(E);z(Ae.$$.fragment,H),ds=l(H),Pt=n(H,"P",{});var eo=o(Pt);fs=d(eo,"This pre-tokenizer simply splits using the digits in separate tokens"),eo.forEach(r),ks=l(H),Et=n(H,"P",{});var to=o(Et);us=d(to,"If set to True, digits will each be separated as follows:"),to.forEach(r),ms=l(H),z(Ce.$$.fragment,H),vs=l(H),St=n(H,"P",{});var ro=o(St);zs=d(ro,"If set to False, digits will grouped as follows:"),ro.forEach(r),$s=l(H),z(qe.$$.fragment,H),H.forEach(r),dr=l(e),Z=n(e,"H2",{class:!0});var qr=o(Z);ue=n(qr,"A",{id:!0,class:!0,href:!0});var so=o(ue);Tt=n(so,"SPAN",{});var no=o(Tt);z(Be.$$.fragment,no),no.forEach(r),so.forEach(r),_s=l(qr),xt=n(qr,"SPAN",{});var oo=o(xt);gs=d(oo,"Metaspace"),oo.forEach(r),qr.forEach(r),fr=l(e),U=n(e,"DIV",{class:!0});var ct=o(U);z(Ie.$$.fragment,ct),ws=l(ct),Dt=n(ct,"P",{});var io=o(Dt);bs=d(io,"Metaspace pre-tokenizer"),io.forEach(r),ys=l(ct),At=n(ct,"P",{});var ao=o(At);Ps=d(ao,`This pre-tokenizer replaces any whitespace by the provided replacement character.
It then tries to split on these spaces.`),ao.forEach(r),ct.forEach(r),kr=l(e),ee=n(e,"H2",{class:!0});var Br=o(ee);me=n(Br,"A",{id:!0,class:!0,href:!0});var lo=o(me);Ct=n(lo,"SPAN",{});var po=o(Ct);z(We.$$.fragment,po),po.forEach(r),lo.forEach(r),Es=l(Br),qt=n(Br,"SPAN",{});var co=o(qt);Ss=d(co,"PreTokenizer"),co.forEach(r),Br.forEach(r),ur=l(e),T=n(e,"DIV",{class:!0});var G=o(T);z(Ne.$$.fragment,G),Ts=l(G),Bt=n(G,"P",{});var ho=o(Bt);xs=d(ho,"Base class for all pre-tokenizers"),ho.forEach(r),Ds=l(G),It=n(G,"P",{});var fo=o(It);As=d(fo,`This class is not supposed to be instantiated directly. Instead, any implementation of a
PreTokenizer will return an instance of this class when instantiated.`),fo.forEach(r),Cs=l(G),F=n(G,"DIV",{class:!0});var ht=o(F);z(Le.$$.fragment,ht),qs=l(ht),Me=n(ht,"P",{});var Ir=o(Me);Bs=d(Ir,"Pre-tokenize a "),Wt=n(Ir,"CODE",{});var ko=o(Wt);Is=d(ko,"PyPreTokenizedString"),ko.forEach(r),Ws=d(Ir," in-place"),Ir.forEach(r),Ns=l(ht),R=n(ht,"P",{});var tt=o(R);Ls=d(tt,"This method allows to modify a "),Nt=n(tt,"CODE",{});var uo=o(Nt);Ms=d(uo,"PreTokenizedString"),uo.forEach(r),Hs=d(tt,` to
keep track of the pre-tokenization, and leverage the capabilities of the
`),Lt=n(tt,"CODE",{});var mo=o(Lt);Us=d(mo,"PreTokenizedString"),mo.forEach(r),Vs=d(tt,`. If you just want to see the result of
the pre-tokenization of a raw string, you can use
`),Mt=n(tt,"CODE",{});var vo=o(Mt);js=d(vo,"pre_tokenize_str()"),vo.forEach(r),tt.forEach(r),ht.forEach(r),Os=l(G),K=n(G,"DIV",{class:!0});var dt=o(K);z(He.$$.fragment,dt),Fs=l(dt),Ht=n(dt,"P",{});var zo=o(Ht);Rs=d(zo,"Pre tokenize the given string"),zo.forEach(r),Ks=l(dt),J=n(dt,"P",{});var rt=o(J);Js=d(rt,`This method provides a way to visualize the effect of a
`),ot=n(rt,"A",{href:!0});var $o=o(ot);Gs=d($o,"PreTokenizer"),$o.forEach(r),Qs=d(rt,` but it does not keep track of the
alignment, nor does it provide all the capabilities of the
`),Ut=n(rt,"CODE",{});var _o=o(Ut);Xs=d(_o,"PreTokenizedString"),_o.forEach(r),Ys=d(rt,`. If you need some of these, you can use
`),Vt=n(rt,"CODE",{});var go=o(Vt);Zs=d(go,"pre_tokenize()"),go.forEach(r),rt.forEach(r),dt.forEach(r),G.forEach(r),mr=l(e),te=n(e,"H2",{class:!0});var Wr=o(te);ve=n(Wr,"A",{id:!0,class:!0,href:!0});var wo=o(ve);jt=n(wo,"SPAN",{});var bo=o(jt);z(Ue.$$.fragment,bo),bo.forEach(r),wo.forEach(r),en=l(Wr),Ot=n(Wr,"SPAN",{});var yo=o(Ot);tn=d(yo,"Punctuation"),yo.forEach(r),Wr.forEach(r),vr=l(e),re=n(e,"DIV",{class:!0});var Nr=o(re);z(Ve.$$.fragment,Nr),rn=l(Nr),Ft=n(Nr,"P",{});var Po=o(Ft);sn=d(Po,"This pre-tokenizer simply splits on punctuation as individual characters."),Po.forEach(r),Nr.forEach(r),zr=l(e),se=n(e,"H2",{class:!0});var Lr=o(se);ze=n(Lr,"A",{id:!0,class:!0,href:!0});var Eo=o(ze);Rt=n(Eo,"SPAN",{});var So=o(Rt);z(je.$$.fragment,So),So.forEach(r),Eo.forEach(r),nn=l(Lr),Kt=n(Lr,"SPAN",{});var To=o(Kt);on=d(To,"Sequence"),To.forEach(r),Lr.forEach(r),$r=l(e),ne=n(e,"DIV",{class:!0});var Mr=o(ne);z(Oe.$$.fragment,Mr),an=l(Mr),Jt=n(Mr,"P",{});var xo=o(Jt);ln=d(xo,"This pre-tokenizer composes other pre_tokenizers and applies them in sequence"),xo.forEach(r),Mr.forEach(r),_r=l(e),oe=n(e,"H2",{class:!0});var Hr=o(oe);$e=n(Hr,"A",{id:!0,class:!0,href:!0});var Do=o($e);Gt=n(Do,"SPAN",{});var Ao=o(Gt);z(Fe.$$.fragment,Ao),Ao.forEach(r),Do.forEach(r),pn=l(Hr),Qt=n(Hr,"SPAN",{});var Co=o(Qt);cn=d(Co,"Split"),Co.forEach(r),Hr.forEach(r),gr=l(e),V=n(e,"DIV",{class:!0});var ft=o(V);z(Re.$$.fragment,ft),hn=l(ft),Xt=n(ft,"P",{});var qo=o(Xt);dn=d(qo,"Split PreTokenizer"),qo.forEach(r),fn=l(ft),Yt=n(ft,"P",{});var Bo=o(Yt);kn=d(Bo,`This versatile pre-tokenizer splits using the provided pattern and
according to the provided behavior. The pattern can be inverted by
making use of the invert flag.`),Bo.forEach(r),ft.forEach(r),wr=l(e),ie=n(e,"H2",{class:!0});var Ur=o(ie);_e=n(Ur,"A",{id:!0,class:!0,href:!0});var Io=o(_e);Zt=n(Io,"SPAN",{});var Wo=o(Zt);z(Ke.$$.fragment,Wo),Wo.forEach(r),Io.forEach(r),un=l(Ur),er=n(Ur,"SPAN",{});var No=o(er);mn=d(No,"UnicodeScripts"),No.forEach(r),Ur.forEach(r),br=l(e),ae=n(e,"DIV",{class:!0});var Vr=o(ae);z(Je.$$.fragment,Vr),vn=l(Vr),Ge=n(Vr,"P",{});var jr=o(Ge);zn=d(jr,`This pre-tokenizer splits on characters that belong to different language family
It roughly follows `),Qe=n(jr,"A",{href:!0,rel:!0});var Lo=o(Qe);$n=d(Lo,"https://github.com/google/sentencepiece/blob/master/data/Scripts.txt"),Lo.forEach(r),_n=d(jr,`
Actually Hiragana and Katakana are fused with Han, and 0x30FC is Han too.
This mimicks SentencePiece Unigram implementation.`),jr.forEach(r),Vr.forEach(r),yr=l(e),le=n(e,"H2",{class:!0});var Or=o(le);ge=n(Or,"A",{id:!0,class:!0,href:!0});var Mo=o(ge);tr=n(Mo,"SPAN",{});var Ho=o(tr);z(Xe.$$.fragment,Ho),Ho.forEach(r),Mo.forEach(r),gn=l(Or),rr=n(Or,"SPAN",{});var Uo=o(rr);wn=d(Uo,"Whitespace"),Uo.forEach(r),Or.forEach(r),Pr=l(e),pe=n(e,"DIV",{class:!0});var Fr=o(pe);z(Ye.$$.fragment,Fr),bn=l(Fr),it=n(Fr,"P",{});var Cn=o(it);yn=d(Cn,"This pre-tokenizer simply splits using the following regex: "),sr=n(Cn,"CODE",{});var Vo=o(sr);Pn=d(Vo,"\\w+|[^\\w\\s]+"),Vo.forEach(r),Cn.forEach(r),Fr.forEach(r),Er=l(e),ce=n(e,"H2",{class:!0});var Rr=o(ce);we=n(Rr,"A",{id:!0,class:!0,href:!0});var jo=o(we);nr=n(jo,"SPAN",{});var Oo=o(nr);z(Ze.$$.fragment,Oo),Oo.forEach(r),jo.forEach(r),En=l(Rr),or=n(Rr,"SPAN",{});var Fo=o(or);Sn=d(Fo,"WhitespaceSplit"),Fo.forEach(r),Rr.forEach(r),Sr=l(e),he=n(e,"DIV",{class:!0});var Kr=o(he);z(et.$$.fragment,Kr),Tn=l(Kr),at=n(Kr,"P",{});var qn=o(at);xn=d(qn,"This pre-tokenizer simply splits on the whitespace. Works like "),ir=n(qn,"CODE",{});var Ro=o(ir);Dn=d(Ro,".split()"),Ro.forEach(r),qn.forEach(r),Kr.forEach(r),this.h()},h(){i(m,"id","tokenizers.pre_tokenizers.BertPreTokenizer]][[tokenizers.pre_tokenizers.BertPreTokenizer"),i(m,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(m,"href","#tokenizers.pre_tokenizers.BertPreTokenizer]][[tokenizers.pre_tokenizers.BertPreTokenizer"),i(c,"class","relative group"),i(S,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),i(de,"id","tokenizers.pre_tokenizers.ByteLevel]][[tokenizers.pre_tokenizers.ByteLevel"),i(de,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(de,"href","#tokenizers.pre_tokenizers.ByteLevel]][[tokenizers.pre_tokenizers.ByteLevel"),i(M,"class","relative group"),i(O,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),i(B,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),i(fe,"id","tokenizers.pre_tokenizers.CharDelimiterSplit]][[tokenizers.pre_tokenizers.CharDelimiterSplit"),i(fe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(fe,"href","#tokenizers.pre_tokenizers.CharDelimiterSplit]][[tokenizers.pre_tokenizers.CharDelimiterSplit"),i(Q,"class","relative group"),i(X,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),i(ke,"id","tokenizers.pre_tokenizers.Digits]][[tokenizers.pre_tokenizers.Digits"),i(ke,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(ke,"href","#tokenizers.pre_tokenizers.Digits]][[tokenizers.pre_tokenizers.Digits"),i(Y,"class","relative group"),i(E,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),i(ue,"id","tokenizers.pre_tokenizers.Metaspace]][[tokenizers.pre_tokenizers.Metaspace"),i(ue,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(ue,"href","#tokenizers.pre_tokenizers.Metaspace]][[tokenizers.pre_tokenizers.Metaspace"),i(Z,"class","relative group"),i(U,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),i(me,"id","tokenizers.pre_tokenizers.PreTokenizer]][[tokenizers.pre_tokenizers.PreTokenizer"),i(me,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(me,"href","#tokenizers.pre_tokenizers.PreTokenizer]][[tokenizers.pre_tokenizers.PreTokenizer"),i(ee,"class","relative group"),i(F,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),i(ot,"href","/docs/tokenizers/pr_1/en/api/pre-tokenizers#tokenizers.pre_tokenizers.PreTokenizer"),i(K,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),i(T,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),i(ve,"id","tokenizers.pre_tokenizers.Punctuation]][[tokenizers.pre_tokenizers.Punctuation"),i(ve,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(ve,"href","#tokenizers.pre_tokenizers.Punctuation]][[tokenizers.pre_tokenizers.Punctuation"),i(te,"class","relative group"),i(re,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),i(ze,"id","tokenizers.pre_tokenizers.Sequence]][[tokenizers.pre_tokenizers.Sequence"),i(ze,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(ze,"href","#tokenizers.pre_tokenizers.Sequence]][[tokenizers.pre_tokenizers.Sequence"),i(se,"class","relative group"),i(ne,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),i($e,"id","tokenizers.pre_tokenizers.Split]][[tokenizers.pre_tokenizers.Split"),i($e,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i($e,"href","#tokenizers.pre_tokenizers.Split]][[tokenizers.pre_tokenizers.Split"),i(oe,"class","relative group"),i(V,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),i(_e,"id","tokenizers.pre_tokenizers.UnicodeScripts]][[tokenizers.pre_tokenizers.UnicodeScripts"),i(_e,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(_e,"href","#tokenizers.pre_tokenizers.UnicodeScripts]][[tokenizers.pre_tokenizers.UnicodeScripts"),i(ie,"class","relative group"),i(Qe,"href","https://github.com/google/sentencepiece/blob/master/data/Scripts.txt"),i(Qe,"rel","nofollow"),i(ae,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),i(ge,"id","tokenizers.pre_tokenizers.Whitespace]][[tokenizers.pre_tokenizers.Whitespace"),i(ge,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(ge,"href","#tokenizers.pre_tokenizers.Whitespace]][[tokenizers.pre_tokenizers.Whitespace"),i(le,"class","relative group"),i(pe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),i(we,"id","tokenizers.pre_tokenizers.WhitespaceSplit]][[tokenizers.pre_tokenizers.WhitespaceSplit"),i(we,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(we,"href","#tokenizers.pre_tokenizers.WhitespaceSplit]][[tokenizers.pre_tokenizers.WhitespaceSplit"),i(ce,"class","relative group"),i(he,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(e,f){k(e,c,f),t(c,m),t(m,p),$(u,p,null),t(c,y),t(c,P),t(P,D),k(e,C,f),k(e,S,f),$(N,S,null),t(S,L),t(S,j),t(j,b),t(S,A),t(S,q),t(q,st),k(e,ye,f),k(e,M,f),t(M,de),t(de,kt),$(Pe,kt,null),t(M,Jr),t(M,ut),t(ut,Gr),k(e,ar,f),k(e,B,f),$(Ee,B,null),t(B,Qr),t(B,mt),t(mt,Xr),t(B,Yr),t(B,vt),t(vt,Zr),t(B,es),t(B,O),$(Se,O,null),t(O,ts),t(O,zt),t(zt,rs),t(O,ss),t(O,$t),t($t,ns),k(e,lr,f),k(e,Q,f),t(Q,fe),t(fe,_t),$(Te,_t,null),t(Q,os),t(Q,gt),t(gt,is),k(e,pr,f),k(e,X,f),$(xe,X,null),t(X,as),t(X,nt),t(nt,ls),t(nt,wt),t(wt,ps),k(e,cr,f),k(e,Y,f),t(Y,ke),t(ke,bt),$(De,bt,null),t(Y,cs),t(Y,yt),t(yt,hs),k(e,hr,f),k(e,E,f),$(Ae,E,null),t(E,ds),t(E,Pt),t(Pt,fs),t(E,ks),t(E,Et),t(Et,us),t(E,ms),$(Ce,E,null),t(E,vs),t(E,St),t(St,zs),t(E,$s),$(qe,E,null),k(e,dr,f),k(e,Z,f),t(Z,ue),t(ue,Tt),$(Be,Tt,null),t(Z,_s),t(Z,xt),t(xt,gs),k(e,fr,f),k(e,U,f),$(Ie,U,null),t(U,ws),t(U,Dt),t(Dt,bs),t(U,ys),t(U,At),t(At,Ps),k(e,kr,f),k(e,ee,f),t(ee,me),t(me,Ct),$(We,Ct,null),t(ee,Es),t(ee,qt),t(qt,Ss),k(e,ur,f),k(e,T,f),$(Ne,T,null),t(T,Ts),t(T,Bt),t(Bt,xs),t(T,Ds),t(T,It),t(It,As),t(T,Cs),t(T,F),$(Le,F,null),t(F,qs),t(F,Me),t(Me,Bs),t(Me,Wt),t(Wt,Is),t(Me,Ws),t(F,Ns),t(F,R),t(R,Ls),t(R,Nt),t(Nt,Ms),t(R,Hs),t(R,Lt),t(Lt,Us),t(R,Vs),t(R,Mt),t(Mt,js),t(T,Os),t(T,K),$(He,K,null),t(K,Fs),t(K,Ht),t(Ht,Rs),t(K,Ks),t(K,J),t(J,Js),t(J,ot),t(ot,Gs),t(J,Qs),t(J,Ut),t(Ut,Xs),t(J,Ys),t(J,Vt),t(Vt,Zs),k(e,mr,f),k(e,te,f),t(te,ve),t(ve,jt),$(Ue,jt,null),t(te,en),t(te,Ot),t(Ot,tn),k(e,vr,f),k(e,re,f),$(Ve,re,null),t(re,rn),t(re,Ft),t(Ft,sn),k(e,zr,f),k(e,se,f),t(se,ze),t(ze,Rt),$(je,Rt,null),t(se,nn),t(se,Kt),t(Kt,on),k(e,$r,f),k(e,ne,f),$(Oe,ne,null),t(ne,an),t(ne,Jt),t(Jt,ln),k(e,_r,f),k(e,oe,f),t(oe,$e),t($e,Gt),$(Fe,Gt,null),t(oe,pn),t(oe,Qt),t(Qt,cn),k(e,gr,f),k(e,V,f),$(Re,V,null),t(V,hn),t(V,Xt),t(Xt,dn),t(V,fn),t(V,Yt),t(Yt,kn),k(e,wr,f),k(e,ie,f),t(ie,_e),t(_e,Zt),$(Ke,Zt,null),t(ie,un),t(ie,er),t(er,mn),k(e,br,f),k(e,ae,f),$(Je,ae,null),t(ae,vn),t(ae,Ge),t(Ge,zn),t(Ge,Qe),t(Qe,$n),t(Ge,_n),k(e,yr,f),k(e,le,f),t(le,ge),t(ge,tr),$(Xe,tr,null),t(le,gn),t(le,rr),t(rr,wn),k(e,Pr,f),k(e,pe,f),$(Ye,pe,null),t(pe,bn),t(pe,it),t(it,yn),t(it,sr),t(sr,Pn),k(e,Er,f),k(e,ce,f),t(ce,we),t(we,nr),$(Ze,nr,null),t(ce,En),t(ce,or),t(or,Sn),k(e,Sr,f),k(e,he,f),$(et,he,null),t(he,Tn),t(he,at),t(at,xn),t(at,ir),t(ir,Dn),Tr=!0},p:Zo,i(e){Tr||(_(u.$$.fragment,e),_(N.$$.fragment,e),_(Pe.$$.fragment,e),_(Ee.$$.fragment,e),_(Se.$$.fragment,e),_(Te.$$.fragment,e),_(xe.$$.fragment,e),_(De.$$.fragment,e),_(Ae.$$.fragment,e),_(Ce.$$.fragment,e),_(qe.$$.fragment,e),_(Be.$$.fragment,e),_(Ie.$$.fragment,e),_(We.$$.fragment,e),_(Ne.$$.fragment,e),_(Le.$$.fragment,e),_(He.$$.fragment,e),_(Ue.$$.fragment,e),_(Ve.$$.fragment,e),_(je.$$.fragment,e),_(Oe.$$.fragment,e),_(Fe.$$.fragment,e),_(Re.$$.fragment,e),_(Ke.$$.fragment,e),_(Je.$$.fragment,e),_(Xe.$$.fragment,e),_(Ye.$$.fragment,e),_(Ze.$$.fragment,e),_(et.$$.fragment,e),Tr=!0)},o(e){g(u.$$.fragment,e),g(N.$$.fragment,e),g(Pe.$$.fragment,e),g(Ee.$$.fragment,e),g(Se.$$.fragment,e),g(Te.$$.fragment,e),g(xe.$$.fragment,e),g(De.$$.fragment,e),g(Ae.$$.fragment,e),g(Ce.$$.fragment,e),g(qe.$$.fragment,e),g(Be.$$.fragment,e),g(Ie.$$.fragment,e),g(We.$$.fragment,e),g(Ne.$$.fragment,e),g(Le.$$.fragment,e),g(He.$$.fragment,e),g(Ue.$$.fragment,e),g(Ve.$$.fragment,e),g(je.$$.fragment,e),g(Oe.$$.fragment,e),g(Fe.$$.fragment,e),g(Re.$$.fragment,e),g(Ke.$$.fragment,e),g(Je.$$.fragment,e),g(Xe.$$.fragment,e),g(Ye.$$.fragment,e),g(Ze.$$.fragment,e),g(et.$$.fragment,e),Tr=!1},d(e){e&&r(c),w(u),e&&r(C),e&&r(S),w(N),e&&r(ye),e&&r(M),w(Pe),e&&r(ar),e&&r(B),w(Ee),w(Se),e&&r(lr),e&&r(Q),w(Te),e&&r(pr),e&&r(X),w(xe),e&&r(cr),e&&r(Y),w(De),e&&r(hr),e&&r(E),w(Ae),w(Ce),w(qe),e&&r(dr),e&&r(Z),w(Be),e&&r(fr),e&&r(U),w(Ie),e&&r(kr),e&&r(ee),w(We),e&&r(ur),e&&r(T),w(Ne),w(Le),w(He),e&&r(mr),e&&r(te),w(Ue),e&&r(vr),e&&r(re),w(Ve),e&&r(zr),e&&r(se),w(je),e&&r($r),e&&r(ne),w(Oe),e&&r(_r),e&&r(oe),w(Fe),e&&r(gr),e&&r(V),w(Re),e&&r(wr),e&&r(ie),w(Ke),e&&r(br),e&&r(ae),w(Je),e&&r(yr),e&&r(le),w(Xe),e&&r(Pr),e&&r(pe),w(Ye),e&&r(Er),e&&r(ce),w(Ze),e&&r(Sr),e&&r(he),w(et)}}}function ri(W){let c,m;return c=new Bn({props:{$$slots:{default:[ti]},$$scope:{ctx:W}}}),{c(){v(c.$$.fragment)},l(p){z(c.$$.fragment,p)},m(p,u){$(c,p,u),m=!0},p(p,u){const y={};u&2&&(y.$$scope={dirty:u,ctx:p}),c.$set(y)},i(p){m||(_(c.$$.fragment,p),m=!0)},o(p){g(c.$$.fragment,p),m=!1},d(p){w(c,p)}}}function si(W){let c,m,p,u,y;return{c(){c=s("p"),m=h("The Rust API Reference is available directly on the "),p=s("a"),u=h("Docs.rs"),y=h(" website."),this.h()},l(P){c=n(P,"P",{});var D=o(c);m=d(D,"The Rust API Reference is available directly on the "),p=n(D,"A",{href:!0,rel:!0});var C=o(p);u=d(C,"Docs.rs"),C.forEach(r),y=d(D," website."),D.forEach(r),this.h()},h(){i(p,"href","https://docs.rs/tokenizers/latest/tokenizers/"),i(p,"rel","nofollow")},m(P,D){k(P,c,D),t(c,m),t(c,p),t(p,u),t(c,y)},d(P){P&&r(c)}}}function ni(W){let c,m;return c=new Bn({props:{$$slots:{default:[si]},$$scope:{ctx:W}}}),{c(){v(c.$$.fragment)},l(p){z(c.$$.fragment,p)},m(p,u){$(c,p,u),m=!0},p(p,u){const y={};u&2&&(y.$$scope={dirty:u,ctx:p}),c.$set(y)},i(p){m||(_(c.$$.fragment,p),m=!0)},o(p){g(c.$$.fragment,p),m=!1},d(p){w(c,p)}}}function oi(W){let c,m;return{c(){c=s("p"),m=h("The node API has not been documented yet.")},l(p){c=n(p,"P",{});var u=o(c);m=d(u,"The node API has not been documented yet."),u.forEach(r)},m(p,u){k(p,c,u),t(c,m)},d(p){p&&r(c)}}}function ii(W){let c,m;return c=new Bn({props:{$$slots:{default:[oi]},$$scope:{ctx:W}}}),{c(){v(c.$$.fragment)},l(p){z(c.$$.fragment,p)},m(p,u){$(c,p,u),m=!0},p(p,u){const y={};u&2&&(y.$$scope={dirty:u,ctx:p}),c.$set(y)},i(p){m||(_(c.$$.fragment,p),m=!0)},o(p){g(c.$$.fragment,p),m=!1},d(p){w(c,p)}}}function ai(W){let c,m,p,u,y,P,D,C,S,N,L,j;return P=new I({}),L=new ei({props:{python:!0,rust:!0,node:!0,$$slots:{node:[ii],rust:[ni],python:[ri]},$$scope:{ctx:W}}}),{c(){c=s("meta"),m=a(),p=s("h1"),u=s("a"),y=s("span"),v(P.$$.fragment),D=a(),C=s("span"),S=h("Pre-tokenizers"),N=a(),v(L.$$.fragment),this.h()},l(b){const A=Xo('[data-svelte="svelte-1phssyn"]',document.head);c=n(A,"META",{name:!0,content:!0}),A.forEach(r),m=l(b),p=n(b,"H1",{class:!0});var q=o(p);u=n(q,"A",{id:!0,class:!0,href:!0});var st=o(u);y=n(st,"SPAN",{});var ye=o(y);z(P.$$.fragment,ye),ye.forEach(r),st.forEach(r),D=l(q),C=n(q,"SPAN",{});var M=o(C);S=d(M,"Pre-tokenizers"),M.forEach(r),q.forEach(r),N=l(b),z(L.$$.fragment,b),this.h()},h(){i(c,"name","hf:doc:metadata"),i(c,"content",JSON.stringify(li)),i(u,"id","pretokenizers"),i(u,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(u,"href","#pretokenizers"),i(p,"class","relative group")},m(b,A){t(document.head,c),k(b,m,A),k(b,p,A),t(p,u),t(u,y),$(P,y,null),t(p,D),t(p,C),t(C,S),k(b,N,A),$(L,b,A),j=!0},p(b,[A]){const q={};A&2&&(q.$$scope={dirty:A,ctx:b}),L.$set(q)},i(b){j||(_(P.$$.fragment,b),_(L.$$.fragment,b),j=!0)},o(b){g(P.$$.fragment,b),g(L.$$.fragment,b),j=!1},d(b){r(c),b&&r(m),b&&r(p),w(P),b&&r(N),w(L,b)}}}const li={local:"pretokenizers",sections:[{local:"tokenizers.pre_tokenizers.BertPreTokenizer]][[tokenizers.pre_tokenizers.BertPreTokenizer",title:"BertPreTokenizer"},{local:"tokenizers.pre_tokenizers.ByteLevel]][[tokenizers.pre_tokenizers.ByteLevel",title:"ByteLevel"},{local:"tokenizers.pre_tokenizers.CharDelimiterSplit]][[tokenizers.pre_tokenizers.CharDelimiterSplit",title:"CharDelimiterSplit"},{local:"tokenizers.pre_tokenizers.Digits]][[tokenizers.pre_tokenizers.Digits",title:"Digits"},{local:"tokenizers.pre_tokenizers.Metaspace]][[tokenizers.pre_tokenizers.Metaspace",title:"Metaspace"},{local:"tokenizers.pre_tokenizers.PreTokenizer]][[tokenizers.pre_tokenizers.PreTokenizer",title:"PreTokenizer"},{local:"tokenizers.pre_tokenizers.Punctuation]][[tokenizers.pre_tokenizers.Punctuation",title:"Punctuation"},{local:"tokenizers.pre_tokenizers.Sequence]][[tokenizers.pre_tokenizers.Sequence",title:"Sequence"},{local:"tokenizers.pre_tokenizers.Split]][[tokenizers.pre_tokenizers.Split",title:"Split"},{local:"tokenizers.pre_tokenizers.UnicodeScripts]][[tokenizers.pre_tokenizers.UnicodeScripts",title:"UnicodeScripts"},{local:"tokenizers.pre_tokenizers.Whitespace]][[tokenizers.pre_tokenizers.Whitespace",title:"Whitespace"},{local:"tokenizers.pre_tokenizers.WhitespaceSplit]][[tokenizers.pre_tokenizers.WhitespaceSplit",title:"WhitespaceSplit"}],title:"Pre-tokenizers"};function pi(W){return Yo(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class ui extends Jo{constructor(c){super();Go(this,c,pi,ai,Qo,{})}}export{ui as default,li as metadata};
