import{S as dt,i as $t,s as zt,e as o,k as m,w as v,t as d,M as ut,c as s,d as t,m as c,a as i,x as _,h as $,b as h,F as r,g as z,y as N,q as w,o as k,B as E,v as ht,L as gt}from"../../chunks/vendor-0d3f0756.js";import{D as b}from"../../chunks/Docstring-f752f2c3.js";import{I as vt}from"../../chunks/IconCopyLink-9193371d.js";import{T as _t,M as Wr}from"../../chunks/TokenizersLanguageContent-ca787841.js";function Nt(x){let n,f,a,l,g,y,D,A,G,T,P,Q,u,S,B,F,q,pe,ue,ar,Fe,R,Z,or,he,sr,qe,U,ee,ir,ge,lr,Ce,K,re,mr,ve,cr,Ve,M,te,fr,_e,pr,Le,I,ne,dr,Ne,$r,zr,we,ur,hr,C,ae,gr,oe,vr,ke,_r,Nr,wr,X,kr,Ee,Er,yr,ye,Dr,Pr,V,se,Ir,De,br,Tr,Y,Sr,de,xr,Ar,Pe,Br,Re,O,ie,Fr,Ie,qr,Ue,W,le,Cr,be,Vr,Ke,j,me,Lr,Te,Rr,Me,H,ce,Ur,Se,Kr,Oe,J,fe,Mr,xe,Or,We;return f=new b({props:{name:"class tokenizers.normalizers.BertNormalizer",anchor:"tokenizers.normalizers.BertNormalizer",parameters:[{name:"clean_text",val:" = True"},{name:"handle_chinese_chars",val:" = True"},{name:"strip_accents",val:" = None"},{name:"lowercase",val:" = True"}],parametersDescription:[{anchor:"tokenizers.normalizers.BertNormalizer.clean_text",description:`<strong>clean_text</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to clean the text, by removing any control characters
and replacing all whitespaces by the classic one.`,name:"clean_text"},{anchor:"tokenizers.normalizers.BertNormalizer.handle_chinese_chars",description:`<strong>handle_chinese_chars</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to handle chinese chars by putting spaces around them.`,name:"handle_chinese_chars"},{anchor:"tokenizers.normalizers.BertNormalizer.strip_accents",description:`<strong>strip_accents</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether to strip all accents. If this option is not specified (ie == None),
then it will be determined by the value for <em>lowercase</em> (as in the original Bert).`,name:"strip_accents"},{anchor:"tokenizers.normalizers.BertNormalizer.lowercase",description:`<strong>lowercase</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to lowercase.`,name:"lowercase"}]}}),P=new b({props:{name:"class tokenizers.normalizers.Lowercase",anchor:"tokenizers.normalizers.Lowercase",parameters:[]}}),q=new b({props:{name:"class tokenizers.normalizers.NFC",anchor:"tokenizers.normalizers.NFC",parameters:[]}}),Z=new b({props:{name:"class tokenizers.normalizers.NFD",anchor:"tokenizers.normalizers.NFD",parameters:[]}}),ee=new b({props:{name:"class tokenizers.normalizers.NFKC",anchor:"tokenizers.normalizers.NFKC",parameters:[]}}),re=new b({props:{name:"class tokenizers.normalizers.NFKD",anchor:"tokenizers.normalizers.NFKD",parameters:[]}}),te=new b({props:{name:"class tokenizers.normalizers.Nmt",anchor:"tokenizers.normalizers.Nmt",parameters:[]}}),ne=new b({props:{name:"class tokenizers.normalizers.Normalizer",anchor:"tokenizers.normalizers.Normalizer",parameters:[]}}),ae=new b({props:{name:"normalize",anchor:"tokenizers.normalizers.Normalizer.normalize",parameters:[{name:"normalized",val:""}],parametersDescription:[{anchor:"tokenizers.normalizers.Normalizer.normalize.normalized",description:`<strong>normalized</strong> (<code>NormalizedString</code>) &#x2014;
The normalized string on which to apply this
<a href="/docs/tokenizers/pr_1/en/api/normalizers#tokenizers.normalizers.Normalizer">Normalizer</a>`,name:"normalized"}]}}),se=new b({props:{name:"normalize_str",anchor:"tokenizers.normalizers.Normalizer.normalize_str",parameters:[{name:"sequence",val:""}],parametersDescription:[{anchor:"tokenizers.normalizers.Normalizer.normalize_str.sequence",description:`<strong>sequence</strong> (<code>str</code>) &#x2014;
A string to normalize`,name:"sequence"}],returnDescription:`
<p>A string after normalization</p>
`,returnType:`
<p><code>str</code></p>
`}}),ie=new b({props:{name:"class tokenizers.normalizers.Precompiled",anchor:"tokenizers.normalizers.Precompiled",parameters:[{name:"precompiled_charsmap",val:""}]}}),le=new b({props:{name:"class tokenizers.normalizers.Replace",anchor:"tokenizers.normalizers.Replace",parameters:[{name:"pattern",val:""},{name:"content",val:""}]}}),me=new b({props:{name:"class tokenizers.normalizers.Sequence",anchor:"tokenizers.normalizers.Sequence",parameters:"",parametersDescription:[{anchor:"tokenizers.normalizers.Sequence.normalizers",description:`<strong>normalizers</strong> (<code>List[Normalizer]</code>) &#x2014;
A list of Normalizer to be run as a sequence`,name:"normalizers"}]}}),ce=new b({props:{name:"class tokenizers.normalizers.Strip",anchor:"tokenizers.normalizers.Strip",parameters:[{name:"left",val:" = True"},{name:"right",val:" = True"}]}}),fe=new b({props:{name:"class tokenizers.normalizers.StripAccents",anchor:"tokenizers.normalizers.StripAccents",parameters:[]}}),{c(){n=o("div"),v(f.$$.fragment),a=m(),l=o("p"),g=d("BertNormalizer"),y=m(),D=o("p"),A=d(`Takes care of normalizing raw text before giving it to a Bert model.
This includes cleaning the text, handling accents, chinese chars and lowercasing`),G=m(),T=o("div"),v(P.$$.fragment),Q=m(),u=o("p"),S=d("Lowercase Normalizer"),B=m(),F=o("div"),v(q.$$.fragment),pe=m(),ue=o("p"),ar=d("NFC Unicode Normalizer"),Fe=m(),R=o("div"),v(Z.$$.fragment),or=m(),he=o("p"),sr=d("NFD Unicode Normalizer"),qe=m(),U=o("div"),v(ee.$$.fragment),ir=m(),ge=o("p"),lr=d("NFKC Unicode Normalizer"),Ce=m(),K=o("div"),v(re.$$.fragment),mr=m(),ve=o("p"),cr=d("NFKD Unicode Normalizer"),Ve=m(),M=o("div"),v(te.$$.fragment),fr=m(),_e=o("p"),pr=d("Nmt normalizer"),Le=m(),I=o("div"),v(ne.$$.fragment),dr=m(),Ne=o("p"),$r=d("Base class for all normalizers"),zr=m(),we=o("p"),ur=d(`This class is not supposed to be instantiated directly. Instead, any implementation of a
Normalizer will return an instance of this class when instantiated.`),hr=m(),C=o("div"),v(ae.$$.fragment),gr=m(),oe=o("p"),vr=d("Normalize a "),ke=o("code"),_r=d("NormalizedString"),Nr=d(" in-place"),wr=m(),X=o("p"),kr=d("This method allows to modify a "),Ee=o("code"),Er=d("NormalizedString"),yr=d(` to
keep track of the alignment information. If you just want to see the result
of the normalization on a raw string, you can use
`),ye=o("code"),Dr=d("normalize_str()"),Pr=m(),V=o("div"),v(se.$$.fragment),Ir=m(),De=o("p"),br=d("Normalize the given string"),Tr=m(),Y=o("p"),Sr=d(`This method provides a way to visualize the effect of a
`),de=o("a"),xr=d("Normalizer"),Ar=d(` but it does not keep track of the alignment
information. If you need to get/convert offsets, you can use
`),Pe=o("code"),Br=d("normalize()"),Re=m(),O=o("div"),v(ie.$$.fragment),Fr=m(),Ie=o("p"),qr=d(`Precompiled normalizer
Don\u2019t use manually it is used for compatiblity for SentencePiece.`),Ue=m(),W=o("div"),v(le.$$.fragment),Cr=m(),be=o("p"),Vr=d("Replace normalizer"),Ke=m(),j=o("div"),v(me.$$.fragment),Lr=m(),Te=o("p"),Rr=d(`Allows concatenating multiple other Normalizer as a Sequence.
All the normalizers run in sequence in the given order`),Me=m(),H=o("div"),v(ce.$$.fragment),Ur=m(),Se=o("p"),Kr=d("Strip normalizer"),Oe=m(),J=o("div"),v(fe.$$.fragment),Mr=m(),xe=o("p"),Or=d("StripAccents normalizer"),this.h()},l(e){n=s(e,"DIV",{class:!0});var p=i(n);_(f.$$.fragment,p),a=c(p),l=s(p,"P",{});var jr=i(l);g=$(jr,"BertNormalizer"),jr.forEach(t),y=c(p),D=s(p,"P",{});var Hr=i(D);A=$(Hr,`Takes care of normalizing raw text before giving it to a Bert model.
This includes cleaning the text, handling accents, chinese chars and lowercasing`),Hr.forEach(t),p.forEach(t),G=c(e),T=s(e,"DIV",{class:!0});var je=i(T);_(P.$$.fragment,je),Q=c(je),u=s(je,"P",{});var Jr=i(u);S=$(Jr,"Lowercase Normalizer"),Jr.forEach(t),je.forEach(t),B=c(e),F=s(e,"DIV",{class:!0});var He=i(F);_(q.$$.fragment,He),pe=c(He),ue=s(He,"P",{});var Gr=i(ue);ar=$(Gr,"NFC Unicode Normalizer"),Gr.forEach(t),He.forEach(t),Fe=c(e),R=s(e,"DIV",{class:!0});var Je=i(R);_(Z.$$.fragment,Je),or=c(Je),he=s(Je,"P",{});var Qr=i(he);sr=$(Qr,"NFD Unicode Normalizer"),Qr.forEach(t),Je.forEach(t),qe=c(e),U=s(e,"DIV",{class:!0});var Ge=i(U);_(ee.$$.fragment,Ge),ir=c(Ge),ge=s(Ge,"P",{});var Xr=i(ge);lr=$(Xr,"NFKC Unicode Normalizer"),Xr.forEach(t),Ge.forEach(t),Ce=c(e),K=s(e,"DIV",{class:!0});var Qe=i(K);_(re.$$.fragment,Qe),mr=c(Qe),ve=s(Qe,"P",{});var Yr=i(ve);cr=$(Yr,"NFKD Unicode Normalizer"),Yr.forEach(t),Qe.forEach(t),Ve=c(e),M=s(e,"DIV",{class:!0});var Xe=i(M);_(te.$$.fragment,Xe),fr=c(Xe),_e=s(Xe,"P",{});var Zr=i(_e);pr=$(Zr,"Nmt normalizer"),Zr.forEach(t),Xe.forEach(t),Le=c(e),I=s(e,"DIV",{class:!0});var L=i(I);_(ne.$$.fragment,L),dr=c(L),Ne=s(L,"P",{});var et=i(Ne);$r=$(et,"Base class for all normalizers"),et.forEach(t),zr=c(L),we=s(L,"P",{});var rt=i(we);ur=$(rt,`This class is not supposed to be instantiated directly. Instead, any implementation of a
Normalizer will return an instance of this class when instantiated.`),rt.forEach(t),hr=c(L),C=s(L,"DIV",{class:!0});var $e=i(C);_(ae.$$.fragment,$e),gr=c($e),oe=s($e,"P",{});var Ye=i(oe);vr=$(Ye,"Normalize a "),ke=s(Ye,"CODE",{});var tt=i(ke);_r=$(tt,"NormalizedString"),tt.forEach(t),Nr=$(Ye," in-place"),Ye.forEach(t),wr=c($e),X=s($e,"P",{});var Ae=i(X);kr=$(Ae,"This method allows to modify a "),Ee=s(Ae,"CODE",{});var nt=i(Ee);Er=$(nt,"NormalizedString"),nt.forEach(t),yr=$(Ae,` to
keep track of the alignment information. If you just want to see the result
of the normalization on a raw string, you can use
`),ye=s(Ae,"CODE",{});var at=i(ye);Dr=$(at,"normalize_str()"),at.forEach(t),Ae.forEach(t),$e.forEach(t),Pr=c(L),V=s(L,"DIV",{class:!0});var ze=i(V);_(se.$$.fragment,ze),Ir=c(ze),De=s(ze,"P",{});var ot=i(De);br=$(ot,"Normalize the given string"),ot.forEach(t),Tr=c(ze),Y=s(ze,"P",{});var Be=i(Y);Sr=$(Be,`This method provides a way to visualize the effect of a
`),de=s(Be,"A",{href:!0});var st=i(de);xr=$(st,"Normalizer"),st.forEach(t),Ar=$(Be,` but it does not keep track of the alignment
information. If you need to get/convert offsets, you can use
`),Pe=s(Be,"CODE",{});var it=i(Pe);Br=$(it,"normalize()"),it.forEach(t),Be.forEach(t),ze.forEach(t),L.forEach(t),Re=c(e),O=s(e,"DIV",{class:!0});var Ze=i(O);_(ie.$$.fragment,Ze),Fr=c(Ze),Ie=s(Ze,"P",{});var lt=i(Ie);qr=$(lt,`Precompiled normalizer
Don\u2019t use manually it is used for compatiblity for SentencePiece.`),lt.forEach(t),Ze.forEach(t),Ue=c(e),W=s(e,"DIV",{class:!0});var er=i(W);_(le.$$.fragment,er),Cr=c(er),be=s(er,"P",{});var mt=i(be);Vr=$(mt,"Replace normalizer"),mt.forEach(t),er.forEach(t),Ke=c(e),j=s(e,"DIV",{class:!0});var rr=i(j);_(me.$$.fragment,rr),Lr=c(rr),Te=s(rr,"P",{});var ct=i(Te);Rr=$(ct,`Allows concatenating multiple other Normalizer as a Sequence.
All the normalizers run in sequence in the given order`),ct.forEach(t),rr.forEach(t),Me=c(e),H=s(e,"DIV",{class:!0});var tr=i(H);_(ce.$$.fragment,tr),Ur=c(tr),Se=s(tr,"P",{});var ft=i(Se);Kr=$(ft,"Strip normalizer"),ft.forEach(t),tr.forEach(t),Oe=c(e),J=s(e,"DIV",{class:!0});var nr=i(J);_(fe.$$.fragment,nr),Mr=c(nr),xe=s(nr,"P",{});var pt=i(xe);Or=$(pt,"StripAccents normalizer"),pt.forEach(t),nr.forEach(t),this.h()},h(){h(n,"class","docstring"),h(T,"class","docstring"),h(F,"class","docstring"),h(R,"class","docstring"),h(U,"class","docstring"),h(K,"class","docstring"),h(M,"class","docstring"),h(C,"class","docstring"),h(de,"href","/docs/tokenizers/pr_1/en/api/normalizers#tokenizers.normalizers.Normalizer"),h(V,"class","docstring"),h(I,"class","docstring"),h(O,"class","docstring"),h(W,"class","docstring"),h(j,"class","docstring"),h(H,"class","docstring"),h(J,"class","docstring")},m(e,p){z(e,n,p),N(f,n,null),r(n,a),r(n,l),r(l,g),r(n,y),r(n,D),r(D,A),z(e,G,p),z(e,T,p),N(P,T,null),r(T,Q),r(T,u),r(u,S),z(e,B,p),z(e,F,p),N(q,F,null),r(F,pe),r(F,ue),r(ue,ar),z(e,Fe,p),z(e,R,p),N(Z,R,null),r(R,or),r(R,he),r(he,sr),z(e,qe,p),z(e,U,p),N(ee,U,null),r(U,ir),r(U,ge),r(ge,lr),z(e,Ce,p),z(e,K,p),N(re,K,null),r(K,mr),r(K,ve),r(ve,cr),z(e,Ve,p),z(e,M,p),N(te,M,null),r(M,fr),r(M,_e),r(_e,pr),z(e,Le,p),z(e,I,p),N(ne,I,null),r(I,dr),r(I,Ne),r(Ne,$r),r(I,zr),r(I,we),r(we,ur),r(I,hr),r(I,C),N(ae,C,null),r(C,gr),r(C,oe),r(oe,vr),r(oe,ke),r(ke,_r),r(oe,Nr),r(C,wr),r(C,X),r(X,kr),r(X,Ee),r(Ee,Er),r(X,yr),r(X,ye),r(ye,Dr),r(I,Pr),r(I,V),N(se,V,null),r(V,Ir),r(V,De),r(De,br),r(V,Tr),r(V,Y),r(Y,Sr),r(Y,de),r(de,xr),r(Y,Ar),r(Y,Pe),r(Pe,Br),z(e,Re,p),z(e,O,p),N(ie,O,null),r(O,Fr),r(O,Ie),r(Ie,qr),z(e,Ue,p),z(e,W,p),N(le,W,null),r(W,Cr),r(W,be),r(be,Vr),z(e,Ke,p),z(e,j,p),N(me,j,null),r(j,Lr),r(j,Te),r(Te,Rr),z(e,Me,p),z(e,H,p),N(ce,H,null),r(H,Ur),r(H,Se),r(Se,Kr),z(e,Oe,p),z(e,J,p),N(fe,J,null),r(J,Mr),r(J,xe),r(xe,Or),We=!0},p:gt,i(e){We||(w(f.$$.fragment,e),w(P.$$.fragment,e),w(q.$$.fragment,e),w(Z.$$.fragment,e),w(ee.$$.fragment,e),w(re.$$.fragment,e),w(te.$$.fragment,e),w(ne.$$.fragment,e),w(ae.$$.fragment,e),w(se.$$.fragment,e),w(ie.$$.fragment,e),w(le.$$.fragment,e),w(me.$$.fragment,e),w(ce.$$.fragment,e),w(fe.$$.fragment,e),We=!0)},o(e){k(f.$$.fragment,e),k(P.$$.fragment,e),k(q.$$.fragment,e),k(Z.$$.fragment,e),k(ee.$$.fragment,e),k(re.$$.fragment,e),k(te.$$.fragment,e),k(ne.$$.fragment,e),k(ae.$$.fragment,e),k(se.$$.fragment,e),k(ie.$$.fragment,e),k(le.$$.fragment,e),k(me.$$.fragment,e),k(ce.$$.fragment,e),k(fe.$$.fragment,e),We=!1},d(e){e&&t(n),E(f),e&&t(G),e&&t(T),E(P),e&&t(B),e&&t(F),E(q),e&&t(Fe),e&&t(R),E(Z),e&&t(qe),e&&t(U),E(ee),e&&t(Ce),e&&t(K),E(re),e&&t(Ve),e&&t(M),E(te),e&&t(Le),e&&t(I),E(ne),E(ae),E(se),e&&t(Re),e&&t(O),E(ie),e&&t(Ue),e&&t(W),E(le),e&&t(Ke),e&&t(j),E(me),e&&t(Me),e&&t(H),E(ce),e&&t(Oe),e&&t(J),E(fe)}}}function wt(x){let n,f;return n=new Wr({props:{$$slots:{default:[Nt]},$$scope:{ctx:x}}}),{c(){v(n.$$.fragment)},l(a){_(n.$$.fragment,a)},m(a,l){N(n,a,l),f=!0},p(a,l){const g={};l&2&&(g.$$scope={dirty:l,ctx:a}),n.$set(g)},i(a){f||(w(n.$$.fragment,a),f=!0)},o(a){k(n.$$.fragment,a),f=!1},d(a){E(n,a)}}}function kt(x){let n,f,a,l,g;return{c(){n=o("p"),f=d("The Rust API Reference is available directly on the "),a=o("a"),l=d("Docs.rs"),g=d(" website."),this.h()},l(y){n=s(y,"P",{});var D=i(n);f=$(D,"The Rust API Reference is available directly on the "),a=s(D,"A",{href:!0,rel:!0});var A=i(a);l=$(A,"Docs.rs"),A.forEach(t),g=$(D," website."),D.forEach(t),this.h()},h(){h(a,"href","https://docs.rs/tokenizers/latest/tokenizers/"),h(a,"rel","nofollow")},m(y,D){z(y,n,D),r(n,f),r(n,a),r(a,l),r(n,g)},d(y){y&&t(n)}}}function Et(x){let n,f;return n=new Wr({props:{$$slots:{default:[kt]},$$scope:{ctx:x}}}),{c(){v(n.$$.fragment)},l(a){_(n.$$.fragment,a)},m(a,l){N(n,a,l),f=!0},p(a,l){const g={};l&2&&(g.$$scope={dirty:l,ctx:a}),n.$set(g)},i(a){f||(w(n.$$.fragment,a),f=!0)},o(a){k(n.$$.fragment,a),f=!1},d(a){E(n,a)}}}function yt(x){let n,f;return{c(){n=o("p"),f=d("The node API has not been documented yet.")},l(a){n=s(a,"P",{});var l=i(n);f=$(l,"The node API has not been documented yet."),l.forEach(t)},m(a,l){z(a,n,l),r(n,f)},d(a){a&&t(n)}}}function Dt(x){let n,f;return n=new Wr({props:{$$slots:{default:[yt]},$$scope:{ctx:x}}}),{c(){v(n.$$.fragment)},l(a){_(n.$$.fragment,a)},m(a,l){N(n,a,l),f=!0},p(a,l){const g={};l&2&&(g.$$scope={dirty:l,ctx:a}),n.$set(g)},i(a){f||(w(n.$$.fragment,a),f=!0)},o(a){k(n.$$.fragment,a),f=!1},d(a){E(n,a)}}}function Pt(x){let n,f,a,l,g,y,D,A,G,T,P,Q;return y=new vt({}),P=new _t({props:{python:!0,rust:!0,node:!0,$$slots:{node:[Dt],rust:[Et],python:[wt]},$$scope:{ctx:x}}}),{c(){n=o("meta"),f=m(),a=o("h1"),l=o("a"),g=o("span"),v(y.$$.fragment),D=m(),A=o("span"),G=d("Normalizers"),T=m(),v(P.$$.fragment),this.h()},l(u){const S=ut('[data-svelte="svelte-1phssyn"]',document.head);n=s(S,"META",{name:!0,content:!0}),S.forEach(t),f=c(u),a=s(u,"H1",{class:!0});var B=i(a);l=s(B,"A",{id:!0,class:!0,href:!0});var F=i(l);g=s(F,"SPAN",{});var q=i(g);_(y.$$.fragment,q),q.forEach(t),F.forEach(t),D=c(B),A=s(B,"SPAN",{});var pe=i(A);G=$(pe,"Normalizers"),pe.forEach(t),B.forEach(t),T=c(u),_(P.$$.fragment,u),this.h()},h(){h(n,"name","hf:doc:metadata"),h(n,"content",JSON.stringify(It)),h(l,"id","tokenizers.normalizers.BertNormalizer"),h(l,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(l,"href","#tokenizers.normalizers.BertNormalizer"),h(a,"class","relative group")},m(u,S){r(document.head,n),z(u,f,S),z(u,a,S),r(a,l),r(l,g),N(y,g,null),r(a,D),r(a,A),r(A,G),z(u,T,S),N(P,u,S),Q=!0},p(u,[S]){const B={};S&2&&(B.$$scope={dirty:S,ctx:u}),P.$set(B)},i(u){Q||(w(y.$$.fragment,u),w(P.$$.fragment,u),Q=!0)},o(u){k(y.$$.fragment,u),k(P.$$.fragment,u),Q=!1},d(u){t(n),u&&t(f),u&&t(a),E(y),u&&t(T),E(P,u)}}}const It={local:"tokenizers.normalizers.BertNormalizer",title:"Normalizers"};function bt(x){return ht(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Bt extends dt{constructor(n){super();$t(this,n,bt,Pt,zt,{})}}export{Bt as default,It as metadata};
