import{S as O,i as U,s as V,e as h,k as M,w as E,t as A,M as Y,c as f,d as a,m as R,a as p,x as I,h as z,b as m,F as d,g as b,y as P,q as D,o as F,B as q,v as G,L as Q}from"../../chunks/vendor-0d3f0756.js";import{D as W}from"../../chunks/Docstring-f752f2c3.js";import{I as J}from"../../chunks/IconCopyLink-9193371d.js";import{T as X,M as C}from"../../chunks/TokenizersLanguageContent-ca787841.js";function Z(g){let t,n,e,o,s,l,u,$,k,w,_,v,r,c,T,S,x;return o=new J({}),w=new W({props:{name:"class tokenizers.AddedToken",anchor:"tokenizers.AddedToken",parameters:[{name:"content",val:""},{name:"single_word",val:" = False"},{name:"lstrip",val:" = False"},{name:"rstrip",val:" = False"},{name:"normalized",val:" = True"}],parametersDescription:[{anchor:"tokenizers.AddedToken.content",description:"<strong>content</strong> (<code>str</code>) &#x2014; The content of the token",name:"content"},{anchor:"tokenizers.AddedToken.single_word",description:`<strong>single_word</strong> (<code>bool</code>, defaults to <code>False</code>) &#x2014;
Defines whether this token should only match single words. If <code>True</code>, this
token will never match inside of a word. For example the token <code>ing</code> would match
on <code>tokenizing</code> if this option is <code>False</code>, but not if it is <code>True</code>.
The notion of &#x201D;<em>inside of a word</em>&#x201D; is defined by the word boundaries pattern in
regular expressions (ie. the token should start and end with word boundaries).`,name:"single_word"},{anchor:"tokenizers.AddedToken.lstrip",description:`<strong>lstrip</strong> (<code>bool</code>, defaults to <code>False</code>) &#x2014;
Defines whether this token should strip all potential whitespaces on its left side.
If <code>True</code>, this token will greedily match any whitespace on its left. For
example if we try to match the token <code>[MASK]</code> with <code>lstrip=True</code>, in the text
<code>&quot;I saw a [MASK]&quot;</code>, we would match on <code>&quot; [MASK]&quot;</code>. (Note the space on the left).`,name:"lstrip"},{anchor:"tokenizers.AddedToken.rstrip",description:`<strong>rstrip</strong> (<code>bool</code>, defaults to <code>False</code>) &#x2014;
Defines whether this token should strip all potential whitespaces on its right
side. If <code>True</code>, this token will greedily match any whitespace on its right.
It works just like <code>lstrip</code> but on the right.`,name:"rstrip"},{anchor:"tokenizers.AddedToken.normalized",description:`<strong>normalized</strong> (<code>bool</code>, defaults to <code>True</code> with  &#x2014;meth:<em>~tokenizers.Tokenizer.add_tokens</em> and <code>False</code> with <code>add_special_tokens()</code>):
Defines whether this token should match against the normalized version of the input
text. For example, with the added token <code>&quot;yesterday&quot;</code>, and a normalizer in charge of
lowercasing the text, the token could be extract from the input <code>&quot;I saw a lion Yesterday&quot;</code>.`,name:"normalized"}]}}),{c(){t=h("h2"),n=h("a"),e=h("span"),E(o.$$.fragment),s=M(),l=h("span"),u=A("AddedToken"),$=M(),k=h("div"),E(w.$$.fragment),_=M(),v=h("p"),r=A("Represents a token that can be be added to a "),c=h("a"),T=A("Tokenizer"),S=A(`.
It can have special options that defines the way it should behave.`),this.h()},l(i){t=f(i,"H2",{class:!0});var y=p(t);n=f(y,"A",{id:!0,class:!0,href:!0});var K=p(n);e=f(K,"SPAN",{});var H=p(e);I(o.$$.fragment,H),H.forEach(a),K.forEach(a),s=R(y),l=f(y,"SPAN",{});var j=p(l);u=z(j,"AddedToken"),j.forEach(a),y.forEach(a),$=R(i),k=f(i,"DIV",{class:!0});var N=p(k);I(w.$$.fragment,N),_=R(N),v=f(N,"P",{});var L=p(v);r=z(L,"Represents a token that can be be added to a "),c=f(L,"A",{href:!0});var B=p(c);T=z(B,"Tokenizer"),B.forEach(a),S=z(L,`.
It can have special options that defines the way it should behave.`),L.forEach(a),N.forEach(a),this.h()},h(){m(n,"id","tokenizers.AddedToken]][[tokenizers.AddedToken"),m(n,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(n,"href","#tokenizers.AddedToken]][[tokenizers.AddedToken"),m(t,"class","relative group"),m(c,"href","/docs/tokenizers/pr_1/en/api/tokenizer#tokenizers.Tokenizer"),m(k,"class","docstring")},m(i,y){b(i,t,y),d(t,n),d(n,e),P(o,e,null),d(t,s),d(t,l),d(l,u),b(i,$,y),b(i,k,y),P(w,k,null),d(k,_),d(k,v),d(v,r),d(v,c),d(c,T),d(v,S),x=!0},p:Q,i(i){x||(D(o.$$.fragment,i),D(w.$$.fragment,i),x=!0)},o(i){F(o.$$.fragment,i),F(w.$$.fragment,i),x=!1},d(i){i&&a(t),q(o),i&&a($),i&&a(k),q(w)}}}function ee(g){let t,n;return t=new C({props:{$$slots:{default:[Z]},$$scope:{ctx:g}}}),{c(){E(t.$$.fragment)},l(e){I(t.$$.fragment,e)},m(e,o){P(t,e,o),n=!0},p(e,o){const s={};o&2&&(s.$$scope={dirty:o,ctx:e}),t.$set(s)},i(e){n||(D(t.$$.fragment,e),n=!0)},o(e){F(t.$$.fragment,e),n=!1},d(e){q(t,e)}}}function te(g){let t,n,e,o,s;return{c(){t=h("p"),n=A("The Rust API Reference is available directly on the "),e=h("a"),o=A("Docs.rs"),s=A(" website."),this.h()},l(l){t=f(l,"P",{});var u=p(t);n=z(u,"The Rust API Reference is available directly on the "),e=f(u,"A",{href:!0,rel:!0});var $=p(e);o=z($,"Docs.rs"),$.forEach(a),s=z(u," website."),u.forEach(a),this.h()},h(){m(e,"href","https://docs.rs/tokenizers/latest/tokenizers/"),m(e,"rel","nofollow")},m(l,u){b(l,t,u),d(t,n),d(t,e),d(e,o),d(t,s)},d(l){l&&a(t)}}}function oe(g){let t,n;return t=new C({props:{$$slots:{default:[te]},$$scope:{ctx:g}}}),{c(){E(t.$$.fragment)},l(e){I(t.$$.fragment,e)},m(e,o){P(t,e,o),n=!0},p(e,o){const s={};o&2&&(s.$$scope={dirty:o,ctx:e}),t.$set(s)},i(e){n||(D(t.$$.fragment,e),n=!0)},o(e){F(t.$$.fragment,e),n=!1},d(e){q(t,e)}}}function ne(g){let t,n;return{c(){t=h("p"),n=A("The node API has not been documented yet.")},l(e){t=f(e,"P",{});var o=p(t);n=z(o,"The node API has not been documented yet."),o.forEach(a)},m(e,o){b(e,t,o),d(t,n)},d(e){e&&a(t)}}}function re(g){let t,n;return t=new C({props:{$$slots:{default:[ne]},$$scope:{ctx:g}}}),{c(){E(t.$$.fragment)},l(e){I(t.$$.fragment,e)},m(e,o){P(t,e,o),n=!0},p(e,o){const s={};o&2&&(s.$$scope={dirty:o,ctx:e}),t.$set(s)},i(e){n||(D(t.$$.fragment,e),n=!0)},o(e){F(t.$$.fragment,e),n=!1},d(e){q(t,e)}}}function ae(g){let t,n,e,o,s,l,u,$,k,w,_,v;return l=new J({}),_=new X({props:{python:!0,rust:!0,node:!0,$$slots:{node:[re],rust:[oe],python:[ee]},$$scope:{ctx:g}}}),{c(){t=h("meta"),n=M(),e=h("h1"),o=h("a"),s=h("span"),E(l.$$.fragment),u=M(),$=h("span"),k=A("Added Tokens"),w=M(),E(_.$$.fragment),this.h()},l(r){const c=Y('[data-svelte="svelte-1phssyn"]',document.head);t=f(c,"META",{name:!0,content:!0}),c.forEach(a),n=R(r),e=f(r,"H1",{class:!0});var T=p(e);o=f(T,"A",{id:!0,class:!0,href:!0});var S=p(o);s=f(S,"SPAN",{});var x=p(s);I(l.$$.fragment,x),x.forEach(a),S.forEach(a),u=R(T),$=f(T,"SPAN",{});var i=p($);k=z(i,"Added Tokens"),i.forEach(a),T.forEach(a),w=R(r),I(_.$$.fragment,r),this.h()},h(){m(t,"name","hf:doc:metadata"),m(t,"content",JSON.stringify(se)),m(o,"id","added-tokens"),m(o,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(o,"href","#added-tokens"),m(e,"class","relative group")},m(r,c){d(document.head,t),b(r,n,c),b(r,e,c),d(e,o),d(o,s),P(l,s,null),d(e,u),d(e,$),d($,k),b(r,w,c),P(_,r,c),v=!0},p(r,[c]){const T={};c&2&&(T.$$scope={dirty:c,ctx:r}),_.$set(T)},i(r){v||(D(l.$$.fragment,r),D(_.$$.fragment,r),v=!0)},o(r){F(l.$$.fragment,r),F(_.$$.fragment,r),v=!1},d(r){a(t),r&&a(n),r&&a(e),q(l),r&&a(w),q(_,r)}}}const se={local:"added-tokens",sections:[{local:"tokenizers.AddedToken]][[tokenizers.AddedToken",title:"AddedToken"}],title:"Added Tokens"};function de(g){return G(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class fe extends O{constructor(t){super();U(this,t,de,ae,V,{})}}export{fe as default,se as metadata};
