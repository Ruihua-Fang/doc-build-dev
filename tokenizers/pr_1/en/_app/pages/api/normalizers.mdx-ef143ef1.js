import{S as $r,i as dr,s as ur,e as i,k as $,w as _,t as z,M as hr,c as l,d as n,m as d,a as m,x as w,h as v,b as h,F as a,g as p,y as N,q as k,o as E,B as P,v as zr,L as vr}from"../../chunks/vendor-0d3f0756.js";import{D as F}from"../../chunks/Docstring-f752f2c3.js";import{I as gr}from"../../chunks/IconCopyLink-9193371d.js";import{T as _r,M as Ze}from"../../chunks/TokenizersLanguageContent-84b27ba3.js";function wr(S){let r,o,t,s,u,g,D,I,H,x,b,J,f,y,T,A,B,te,ne,Fe,$e,V,O,Be,ae,Ve,de,q,j,qe,se,Ce,ue,C,G,Le,oe,Re,he,L,Q,Ue,ie,Ke,ze,R,X,Me,le,We,ve,U,Y,He,me,Je,ge,K,Z,Oe,ce,je,_e,M,ee,Ge,fe,Qe,we,W,re,Xe,pe,Ye,Ne;return o=new F({props:{name:"class tokenizers.normalizers.BertNormalizer",anchor:"tokenizers.normalizers.BertNormalizer",parameters:[{name:"clean_text",val:" = True"},{name:"handle_chinese_chars",val:" = True"},{name:"strip_accents",val:" = None"},{name:"lowercase",val:" = True"}],parametersDescription:[{anchor:"tokenizers.normalizers.BertNormalizer.clean_text",description:`<strong>clean_text</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to clean the text, by removing any control characters
and replacing all whitespaces by the classic one.`,name:"clean_text"},{anchor:"tokenizers.normalizers.BertNormalizer.handle_chinese_chars",description:`<strong>handle_chinese_chars</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to handle chinese chars by putting spaces around them.`,name:"handle_chinese_chars"},{anchor:"tokenizers.normalizers.BertNormalizer.strip_accents",description:`<strong>strip_accents</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether to strip all accents. If this option is not specified (ie == None),
then it will be determined by the value for <em>lowercase</em> (as in the original Bert).`,name:"strip_accents"},{anchor:"tokenizers.normalizers.BertNormalizer.lowercase",description:`<strong>lowercase</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to lowercase.`,name:"lowercase"}]}}),b=new F({props:{name:"class tokenizers.normalizers.Lowercase",anchor:"tokenizers.normalizers.Lowercase",parameters:[]}}),B=new F({props:{name:"class tokenizers.normalizers.NFC",anchor:"tokenizers.normalizers.NFC",parameters:[]}}),O=new F({props:{name:"class tokenizers.normalizers.NFD",anchor:"tokenizers.normalizers.NFD",parameters:[]}}),j=new F({props:{name:"class tokenizers.normalizers.NFKC",anchor:"tokenizers.normalizers.NFKC",parameters:[]}}),G=new F({props:{name:"class tokenizers.normalizers.NFKD",anchor:"tokenizers.normalizers.NFKD",parameters:[]}}),Q=new F({props:{name:"class tokenizers.normalizers.Nmt",anchor:"tokenizers.normalizers.Nmt",parameters:[]}}),X=new F({props:{name:"class tokenizers.normalizers.Precompiled",anchor:"tokenizers.normalizers.Precompiled",parameters:[{name:"precompiled_charsmap",val:""}]}}),Y=new F({props:{name:"class tokenizers.normalizers.Replace",anchor:"tokenizers.normalizers.Replace",parameters:[{name:"pattern",val:""},{name:"content",val:""}]}}),Z=new F({props:{name:"class tokenizers.normalizers.Sequence",anchor:"tokenizers.normalizers.Sequence",parameters:"",parametersDescription:[{anchor:"tokenizers.normalizers.Sequence.normalizers",description:`<strong>normalizers</strong> (<code>List[Normalizer]</code>) &#x2014;
A list of Normalizer to be run as a sequence`,name:"normalizers"}]}}),ee=new F({props:{name:"class tokenizers.normalizers.Strip",anchor:"tokenizers.normalizers.Strip",parameters:[{name:"left",val:" = True"},{name:"right",val:" = True"}]}}),re=new F({props:{name:"class tokenizers.normalizers.StripAccents",anchor:"tokenizers.normalizers.StripAccents",parameters:[]}}),{c(){r=i("div"),_(o.$$.fragment),t=$(),s=i("p"),u=z("BertNormalizer"),g=$(),D=i("p"),I=z(`Takes care of normalizing raw text before giving it to a Bert model.
This includes cleaning the text, handling accents, chinese chars and lowercasing`),H=$(),x=i("div"),_(b.$$.fragment),J=$(),f=i("p"),y=z("Lowercase Normalizer"),T=$(),A=i("div"),_(B.$$.fragment),te=$(),ne=i("p"),Fe=z("NFC Unicode Normalizer"),$e=$(),V=i("div"),_(O.$$.fragment),Be=$(),ae=i("p"),Ve=z("NFD Unicode Normalizer"),de=$(),q=i("div"),_(j.$$.fragment),qe=$(),se=i("p"),Ce=z("NFKC Unicode Normalizer"),ue=$(),C=i("div"),_(G.$$.fragment),Le=$(),oe=i("p"),Re=z("NFKD Unicode Normalizer"),he=$(),L=i("div"),_(Q.$$.fragment),Ue=$(),ie=i("p"),Ke=z("Nmt normalizer"),ze=$(),R=i("div"),_(X.$$.fragment),Me=$(),le=i("p"),We=z(`Precompiled normalizer
Don\u2019t use manually it is used for compatiblity for SentencePiece.`),ve=$(),U=i("div"),_(Y.$$.fragment),He=$(),me=i("p"),Je=z("Replace normalizer"),ge=$(),K=i("div"),_(Z.$$.fragment),Oe=$(),ce=i("p"),je=z(`Allows concatenating multiple other Normalizer as a Sequence.
All the normalizers run in sequence in the given order`),_e=$(),M=i("div"),_(ee.$$.fragment),Ge=$(),fe=i("p"),Qe=z("Strip normalizer"),we=$(),W=i("div"),_(re.$$.fragment),Xe=$(),pe=i("p"),Ye=z("StripAccents normalizer"),this.h()},l(e){r=l(e,"DIV",{class:!0});var c=m(r);w(o.$$.fragment,c),t=d(c),s=l(c,"P",{});var er=m(s);u=v(er,"BertNormalizer"),er.forEach(n),g=d(c),D=l(c,"P",{});var rr=m(D);I=v(rr,`Takes care of normalizing raw text before giving it to a Bert model.
This includes cleaning the text, handling accents, chinese chars and lowercasing`),rr.forEach(n),c.forEach(n),H=d(e),x=l(e,"DIV",{class:!0});var ke=m(x);w(b.$$.fragment,ke),J=d(ke),f=l(ke,"P",{});var tr=m(f);y=v(tr,"Lowercase Normalizer"),tr.forEach(n),ke.forEach(n),T=d(e),A=l(e,"DIV",{class:!0});var Ee=m(A);w(B.$$.fragment,Ee),te=d(Ee),ne=l(Ee,"P",{});var nr=m(ne);Fe=v(nr,"NFC Unicode Normalizer"),nr.forEach(n),Ee.forEach(n),$e=d(e),V=l(e,"DIV",{class:!0});var Pe=m(V);w(O.$$.fragment,Pe),Be=d(Pe),ae=l(Pe,"P",{});var ar=m(ae);Ve=v(ar,"NFD Unicode Normalizer"),ar.forEach(n),Pe.forEach(n),de=d(e),q=l(e,"DIV",{class:!0});var De=m(q);w(j.$$.fragment,De),qe=d(De),se=l(De,"P",{});var sr=m(se);Ce=v(sr,"NFKC Unicode Normalizer"),sr.forEach(n),De.forEach(n),ue=d(e),C=l(e,"DIV",{class:!0});var be=m(C);w(G.$$.fragment,be),Le=d(be),oe=l(be,"P",{});var or=m(oe);Re=v(or,"NFKD Unicode Normalizer"),or.forEach(n),be.forEach(n),he=d(e),L=l(e,"DIV",{class:!0});var xe=m(L);w(Q.$$.fragment,xe),Ue=d(xe),ie=l(xe,"P",{});var ir=m(ie);Ke=v(ir,"Nmt normalizer"),ir.forEach(n),xe.forEach(n),ze=d(e),R=l(e,"DIV",{class:!0});var ye=m(R);w(X.$$.fragment,ye),Me=d(ye),le=l(ye,"P",{});var lr=m(le);We=v(lr,`Precompiled normalizer
Don\u2019t use manually it is used for compatiblity for SentencePiece.`),lr.forEach(n),ye.forEach(n),ve=d(e),U=l(e,"DIV",{class:!0});var Se=m(U);w(Y.$$.fragment,Se),He=d(Se),me=l(Se,"P",{});var mr=m(me);Je=v(mr,"Replace normalizer"),mr.forEach(n),Se.forEach(n),ge=d(e),K=l(e,"DIV",{class:!0});var Ie=m(K);w(Z.$$.fragment,Ie),Oe=d(Ie),ce=l(Ie,"P",{});var cr=m(ce);je=v(cr,`Allows concatenating multiple other Normalizer as a Sequence.
All the normalizers run in sequence in the given order`),cr.forEach(n),Ie.forEach(n),_e=d(e),M=l(e,"DIV",{class:!0});var Te=m(M);w(ee.$$.fragment,Te),Ge=d(Te),fe=l(Te,"P",{});var fr=m(fe);Qe=v(fr,"Strip normalizer"),fr.forEach(n),Te.forEach(n),we=d(e),W=l(e,"DIV",{class:!0});var Ae=m(W);w(re.$$.fragment,Ae),Xe=d(Ae),pe=l(Ae,"P",{});var pr=m(pe);Ye=v(pr,"StripAccents normalizer"),pr.forEach(n),Ae.forEach(n),this.h()},h(){h(r,"class","docstring"),h(x,"class","docstring"),h(A,"class","docstring"),h(V,"class","docstring"),h(q,"class","docstring"),h(C,"class","docstring"),h(L,"class","docstring"),h(R,"class","docstring"),h(U,"class","docstring"),h(K,"class","docstring"),h(M,"class","docstring"),h(W,"class","docstring")},m(e,c){p(e,r,c),N(o,r,null),a(r,t),a(r,s),a(s,u),a(r,g),a(r,D),a(D,I),p(e,H,c),p(e,x,c),N(b,x,null),a(x,J),a(x,f),a(f,y),p(e,T,c),p(e,A,c),N(B,A,null),a(A,te),a(A,ne),a(ne,Fe),p(e,$e,c),p(e,V,c),N(O,V,null),a(V,Be),a(V,ae),a(ae,Ve),p(e,de,c),p(e,q,c),N(j,q,null),a(q,qe),a(q,se),a(se,Ce),p(e,ue,c),p(e,C,c),N(G,C,null),a(C,Le),a(C,oe),a(oe,Re),p(e,he,c),p(e,L,c),N(Q,L,null),a(L,Ue),a(L,ie),a(ie,Ke),p(e,ze,c),p(e,R,c),N(X,R,null),a(R,Me),a(R,le),a(le,We),p(e,ve,c),p(e,U,c),N(Y,U,null),a(U,He),a(U,me),a(me,Je),p(e,ge,c),p(e,K,c),N(Z,K,null),a(K,Oe),a(K,ce),a(ce,je),p(e,_e,c),p(e,M,c),N(ee,M,null),a(M,Ge),a(M,fe),a(fe,Qe),p(e,we,c),p(e,W,c),N(re,W,null),a(W,Xe),a(W,pe),a(pe,Ye),Ne=!0},p:vr,i(e){Ne||(k(o.$$.fragment,e),k(b.$$.fragment,e),k(B.$$.fragment,e),k(O.$$.fragment,e),k(j.$$.fragment,e),k(G.$$.fragment,e),k(Q.$$.fragment,e),k(X.$$.fragment,e),k(Y.$$.fragment,e),k(Z.$$.fragment,e),k(ee.$$.fragment,e),k(re.$$.fragment,e),Ne=!0)},o(e){E(o.$$.fragment,e),E(b.$$.fragment,e),E(B.$$.fragment,e),E(O.$$.fragment,e),E(j.$$.fragment,e),E(G.$$.fragment,e),E(Q.$$.fragment,e),E(X.$$.fragment,e),E(Y.$$.fragment,e),E(Z.$$.fragment,e),E(ee.$$.fragment,e),E(re.$$.fragment,e),Ne=!1},d(e){e&&n(r),P(o),e&&n(H),e&&n(x),P(b),e&&n(T),e&&n(A),P(B),e&&n($e),e&&n(V),P(O),e&&n(de),e&&n(q),P(j),e&&n(ue),e&&n(C),P(G),e&&n(he),e&&n(L),P(Q),e&&n(ze),e&&n(R),P(X),e&&n(ve),e&&n(U),P(Y),e&&n(ge),e&&n(K),P(Z),e&&n(_e),e&&n(M),P(ee),e&&n(we),e&&n(W),P(re)}}}function Nr(S){let r,o;return r=new Ze({props:{$$slots:{default:[wr]},$$scope:{ctx:S}}}),{c(){_(r.$$.fragment)},l(t){w(r.$$.fragment,t)},m(t,s){N(r,t,s),o=!0},p(t,s){const u={};s&2&&(u.$$scope={dirty:s,ctx:t}),r.$set(u)},i(t){o||(k(r.$$.fragment,t),o=!0)},o(t){E(r.$$.fragment,t),o=!1},d(t){P(r,t)}}}function kr(S){let r,o,t,s,u;return{c(){r=i("p"),o=z("The Rust API Reference is available directly on the "),t=i("a"),s=z("Docs.rs"),u=z(" website."),this.h()},l(g){r=l(g,"P",{});var D=m(r);o=v(D,"The Rust API Reference is available directly on the "),t=l(D,"A",{href:!0,rel:!0});var I=m(t);s=v(I,"Docs.rs"),I.forEach(n),u=v(D," website."),D.forEach(n),this.h()},h(){h(t,"href","https://docs.rs/tokenizers/latest/tokenizers/"),h(t,"rel","nofollow")},m(g,D){p(g,r,D),a(r,o),a(r,t),a(t,s),a(r,u)},d(g){g&&n(r)}}}function Er(S){let r,o;return r=new Ze({props:{$$slots:{default:[kr]},$$scope:{ctx:S}}}),{c(){_(r.$$.fragment)},l(t){w(r.$$.fragment,t)},m(t,s){N(r,t,s),o=!0},p(t,s){const u={};s&2&&(u.$$scope={dirty:s,ctx:t}),r.$set(u)},i(t){o||(k(r.$$.fragment,t),o=!0)},o(t){E(r.$$.fragment,t),o=!1},d(t){P(r,t)}}}function Pr(S){let r,o;return{c(){r=i("p"),o=z("The node API has not been documented yet.")},l(t){r=l(t,"P",{});var s=m(r);o=v(s,"The node API has not been documented yet."),s.forEach(n)},m(t,s){p(t,r,s),a(r,o)},d(t){t&&n(r)}}}function Dr(S){let r,o;return r=new Ze({props:{$$slots:{default:[Pr]},$$scope:{ctx:S}}}),{c(){_(r.$$.fragment)},l(t){w(r.$$.fragment,t)},m(t,s){N(r,t,s),o=!0},p(t,s){const u={};s&2&&(u.$$scope={dirty:s,ctx:t}),r.$set(u)},i(t){o||(k(r.$$.fragment,t),o=!0)},o(t){E(r.$$.fragment,t),o=!1},d(t){P(r,t)}}}function br(S){let r,o,t,s,u,g,D,I,H,x,b,J;return g=new gr({}),b=new _r({props:{python:!0,rust:!0,node:!0,$$slots:{node:[Dr],rust:[Er],python:[Nr]},$$scope:{ctx:S}}}),{c(){r=i("meta"),o=$(),t=i("h1"),s=i("a"),u=i("span"),_(g.$$.fragment),D=$(),I=i("span"),H=z("Normalizers"),x=$(),_(b.$$.fragment),this.h()},l(f){const y=hr('[data-svelte="svelte-1phssyn"]',document.head);r=l(y,"META",{name:!0,content:!0}),y.forEach(n),o=d(f),t=l(f,"H1",{class:!0});var T=m(t);s=l(T,"A",{id:!0,class:!0,href:!0});var A=m(s);u=l(A,"SPAN",{});var B=m(u);w(g.$$.fragment,B),B.forEach(n),A.forEach(n),D=d(T),I=l(T,"SPAN",{});var te=m(I);H=v(te,"Normalizers"),te.forEach(n),T.forEach(n),x=d(f),w(b.$$.fragment,f),this.h()},h(){h(r,"name","hf:doc:metadata"),h(r,"content",JSON.stringify(xr)),h(s,"id","tokenizers.normalizers.BertNormalizer"),h(s,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(s,"href","#tokenizers.normalizers.BertNormalizer"),h(t,"class","relative group")},m(f,y){a(document.head,r),p(f,o,y),p(f,t,y),a(t,s),a(s,u),N(g,u,null),a(t,D),a(t,I),a(I,H),p(f,x,y),N(b,f,y),J=!0},p(f,[y]){const T={};y&2&&(T.$$scope={dirty:y,ctx:f}),b.$set(T)},i(f){J||(k(g.$$.fragment,f),k(b.$$.fragment,f),J=!0)},o(f){E(g.$$.fragment,f),E(b.$$.fragment,f),J=!1},d(f){n(r),f&&n(o),f&&n(t),P(g),f&&n(x),P(b,f)}}}const xr={local:"tokenizers.normalizers.BertNormalizer",title:"Normalizers"};function yr(S){return zr(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Fr extends $r{constructor(r){super();dr(this,r,yr,br,ur,{})}}export{Fr as default,xr as metadata};
