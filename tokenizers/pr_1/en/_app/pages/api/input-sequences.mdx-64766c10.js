import{S as jt,i as Kt,s as Qt,e as s,k as m,w as F,t as i,M as Vt,c as a,d as t,m as k,a as l,x as G,h as c,b as v,F as e,g as d,y as B,q as J,o as j,B as K,v as Xt}from"../../chunks/vendor-0d3f0756.js";import{I as Ue}from"../../chunks/IconCopyLink-9193371d.js";import{T as Yt,M as kt}from"../../chunks/TokenizersLanguageContent-ca787841.js";function Zt(S){let o,f,r,u,$,_,z,I,b,w,E,P,p,q,T,g,Y,Q,x,Me,re,He,We,we,y,C,oe,V,Fe,se,ae,Ge,qe,Z,Be,Te,D,L,Je,le,je,Ke,ie,Qe,Ve,O,Xe,ce,Ye,Ze,ue,et,Se,N,tt,fe,nt,rt,Ie,A,R,pe,X,ot,he,de,st,Pe,ee,at,ye,U,M,lt,$e,it,ct,te,ut,ft,H,pt,ve,ht,dt,ne,$t,Ae,W,vt,_e,_t,mt,be;return p=new Ue({}),V=new Ue({}),X=new Ue({}),{c(){o=s("p"),f=i(`These types represent all the different kinds of sequence that can be used as input of a Tokenizer.
Globally, any sequence can be either a string or a list of strings, according to the operating
mode of the tokenizer: `),r=s("code"),u=i("raw text"),$=i(" vs "),_=s("code"),z=i("pre-tokenized"),I=i("."),b=m(),w=s("h2"),E=s("a"),P=s("span"),F(p.$$.fragment),q=m(),T=s("span"),g=s("code"),Y=i("tokenizers.TextInputSequence"),Q=m(),x=s("p"),Me=i("A "),re=s("code"),He=i("str"),We=i(" that represents an input sequence"),we=m(),y=s("h2"),C=s("a"),oe=s("span"),F(V.$$.fragment),Fe=m(),se=s("span"),ae=s("code"),Ge=i("tokenizers.PreTokenizedInputSequence"),qe=m(),Z=s("p"),Be=i("A pre-tokenized input sequence. Can be one of:"),Te=m(),D=s("ul"),L=s("li"),Je=i("A "),le=s("code"),je=i("List"),Ke=i(" of "),ie=s("code"),Qe=i("str"),Ve=m(),O=s("li"),Xe=i("A "),ce=s("code"),Ye=i("Tuple"),Ze=i(" of "),ue=s("code"),et=i("str"),Se=m(),N=s("p"),tt=i("alias of "),fe=s("code"),nt=i("Union[List[str], Tuple[str]]"),rt=i("."),Ie=m(),A=s("h2"),R=s("a"),pe=s("span"),F(X.$$.fragment),ot=m(),he=s("span"),de=s("code"),st=i("tokenizers.InputSequence"),Pe=m(),ee=s("p"),at=i("Represents all the possible types of input sequences for encoding. Can be:"),ye=m(),U=s("ul"),M=s("li"),lt=i("When "),$e=s("code"),it=i("is_pretokenized=False"),ct=i(": "),te=s("a"),ut=i("TextInputSequence"),ft=m(),H=s("li"),pt=i("When "),ve=s("code"),ht=i("is_pretokenized=True"),dt=i(": "),ne=s("a"),$t=i("PreTokenizedInputSequence"),Ae=m(),W=s("p"),vt=i("alias of "),_e=s("code"),_t=i("Union[str, List[str], Tuple[str]]"),mt=i("."),this.h()},l(n){o=a(n,"P",{});var h=l(o);f=c(h,`These types represent all the different kinds of sequence that can be used as input of a Tokenizer.
Globally, any sequence can be either a string or a list of strings, according to the operating
mode of the tokenizer: `),r=a(h,"CODE",{});var Et=l(r);u=c(Et,"raw text"),Et.forEach(t),$=c(h," vs "),_=a(h,"CODE",{});var zt=l(_);z=c(zt,"pre-tokenized"),zt.forEach(t),I=c(h,"."),h.forEach(t),b=k(n),w=a(n,"H2",{class:!0});var ge=l(w);E=a(ge,"A",{id:!0,class:!0,href:!0});var wt=l(E);P=a(wt,"SPAN",{});var qt=l(P);G(p.$$.fragment,qt),qt.forEach(t),wt.forEach(t),q=k(ge),T=a(ge,"SPAN",{});var Tt=l(T);g=a(Tt,"CODE",{});var St=l(g);Y=c(St,"tokenizers.TextInputSequence"),St.forEach(t),Tt.forEach(t),ge.forEach(t),Q=k(n),x=a(n,"P",{});var xe=l(x);Me=c(xe,"A "),re=a(xe,"CODE",{});var It=l(re);He=c(It,"str"),It.forEach(t),We=c(xe," that represents an input sequence"),xe.forEach(t),we=k(n),y=a(n,"H2",{class:!0});var Ce=l(y);C=a(Ce,"A",{id:!0,class:!0,href:!0});var Pt=l(C);oe=a(Pt,"SPAN",{});var yt=l(oe);G(V.$$.fragment,yt),yt.forEach(t),Pt.forEach(t),Fe=k(Ce),se=a(Ce,"SPAN",{});var At=l(se);ae=a(At,"CODE",{});var bt=l(ae);Ge=c(bt,"tokenizers.PreTokenizedInputSequence"),bt.forEach(t),At.forEach(t),Ce.forEach(t),qe=k(n),Z=a(n,"P",{});var gt=l(Z);Be=c(gt,"A pre-tokenized input sequence. Can be one of:"),gt.forEach(t),Te=k(n),D=a(n,"UL",{});var De=l(D);L=a(De,"LI",{});var me=l(L);Je=c(me,"A "),le=a(me,"CODE",{});var xt=l(le);je=c(xt,"List"),xt.forEach(t),Ke=c(me," of "),ie=a(me,"CODE",{});var Ct=l(ie);Qe=c(Ct,"str"),Ct.forEach(t),me.forEach(t),Ve=k(De),O=a(De,"LI",{});var ke=l(O);Xe=c(ke,"A "),ce=a(ke,"CODE",{});var Dt=l(ce);Ye=c(Dt,"Tuple"),Dt.forEach(t),Ze=c(ke," of "),ue=a(ke,"CODE",{});var Lt=l(ue);et=c(Lt,"str"),Lt.forEach(t),ke.forEach(t),De.forEach(t),Se=k(n),N=a(n,"P",{});var Le=l(N);tt=c(Le,"alias of "),fe=a(Le,"CODE",{});var Ot=l(fe);nt=c(Ot,"Union[List[str], Tuple[str]]"),Ot.forEach(t),rt=c(Le,"."),Le.forEach(t),Ie=k(n),A=a(n,"H2",{class:!0});var Oe=l(A);R=a(Oe,"A",{id:!0,class:!0,href:!0});var Nt=l(R);pe=a(Nt,"SPAN",{});var Rt=l(pe);G(X.$$.fragment,Rt),Rt.forEach(t),Nt.forEach(t),ot=k(Oe),he=a(Oe,"SPAN",{});var Ut=l(he);de=a(Ut,"CODE",{});var Mt=l(de);st=c(Mt,"tokenizers.InputSequence"),Mt.forEach(t),Ut.forEach(t),Oe.forEach(t),Pe=k(n),ee=a(n,"P",{});var Ht=l(ee);at=c(Ht,"Represents all the possible types of input sequences for encoding. Can be:"),Ht.forEach(t),ye=k(n),U=a(n,"UL",{});var Ne=l(U);M=a(Ne,"LI",{});var Ee=l(M);lt=c(Ee,"When "),$e=a(Ee,"CODE",{});var Wt=l($e);it=c(Wt,"is_pretokenized=False"),Wt.forEach(t),ct=c(Ee,": "),te=a(Ee,"A",{href:!0});var Ft=l(te);ut=c(Ft,"TextInputSequence"),Ft.forEach(t),Ee.forEach(t),ft=k(Ne),H=a(Ne,"LI",{});var ze=l(H);pt=c(ze,"When "),ve=a(ze,"CODE",{});var Gt=l(ve);ht=c(Gt,"is_pretokenized=True"),Gt.forEach(t),dt=c(ze,": "),ne=a(ze,"A",{href:!0});var Bt=l(ne);$t=c(Bt,"PreTokenizedInputSequence"),Bt.forEach(t),ze.forEach(t),Ne.forEach(t),Ae=k(n),W=a(n,"P",{});var Re=l(W);vt=c(Re,"alias of "),_e=a(Re,"CODE",{});var Jt=l(_e);_t=c(Jt,"Union[str, List[str], Tuple[str]]"),Jt.forEach(t),mt=c(Re,"."),Re.forEach(t),this.h()},h(){v(E,"id","tokenizers.TextInputSequence"),v(E,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),v(E,"href","#tokenizers.TextInputSequence"),v(w,"class","relative group"),v(C,"id","tokenizers.PreTokenizedInputSequence"),v(C,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),v(C,"href","#tokenizers.PreTokenizedInputSequence"),v(y,"class","relative group"),v(R,"id","tokenizersinputsequence"),v(R,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),v(R,"href","#tokenizersinputsequence"),v(A,"class","relative group"),v(te,"href","#tokenizers.TextInputSequence"),v(ne,"href","#tokenizers.PreTokenizedInputSequence")},m(n,h){d(n,o,h),e(o,f),e(o,r),e(r,u),e(o,$),e(o,_),e(_,z),e(o,I),d(n,b,h),d(n,w,h),e(w,E),e(E,P),B(p,P,null),e(w,q),e(w,T),e(T,g),e(g,Y),d(n,Q,h),d(n,x,h),e(x,Me),e(x,re),e(re,He),e(x,We),d(n,we,h),d(n,y,h),e(y,C),e(C,oe),B(V,oe,null),e(y,Fe),e(y,se),e(se,ae),e(ae,Ge),d(n,qe,h),d(n,Z,h),e(Z,Be),d(n,Te,h),d(n,D,h),e(D,L),e(L,Je),e(L,le),e(le,je),e(L,Ke),e(L,ie),e(ie,Qe),e(D,Ve),e(D,O),e(O,Xe),e(O,ce),e(ce,Ye),e(O,Ze),e(O,ue),e(ue,et),d(n,Se,h),d(n,N,h),e(N,tt),e(N,fe),e(fe,nt),e(N,rt),d(n,Ie,h),d(n,A,h),e(A,R),e(R,pe),B(X,pe,null),e(A,ot),e(A,he),e(he,de),e(de,st),d(n,Pe,h),d(n,ee,h),e(ee,at),d(n,ye,h),d(n,U,h),e(U,M),e(M,lt),e(M,$e),e($e,it),e(M,ct),e(M,te),e(te,ut),e(U,ft),e(U,H),e(H,pt),e(H,ve),e(ve,ht),e(H,dt),e(H,ne),e(ne,$t),d(n,Ae,h),d(n,W,h),e(W,vt),e(W,_e),e(_e,_t),e(W,mt),be=!0},i(n){be||(J(p.$$.fragment,n),J(V.$$.fragment,n),J(X.$$.fragment,n),be=!0)},o(n){j(p.$$.fragment,n),j(V.$$.fragment,n),j(X.$$.fragment,n),be=!1},d(n){n&&t(o),n&&t(b),n&&t(w),K(p),n&&t(Q),n&&t(x),n&&t(we),n&&t(y),K(V),n&&t(qe),n&&t(Z),n&&t(Te),n&&t(D),n&&t(Se),n&&t(N),n&&t(Ie),n&&t(A),K(X),n&&t(Pe),n&&t(ee),n&&t(ye),n&&t(U),n&&t(Ae),n&&t(W)}}}function en(S){let o,f;return o=new kt({props:{$$slots:{default:[Zt]},$$scope:{ctx:S}}}),{c(){F(o.$$.fragment)},l(r){G(o.$$.fragment,r)},m(r,u){B(o,r,u),f=!0},p(r,u){const $={};u&2&&($.$$scope={dirty:u,ctx:r}),o.$set($)},i(r){f||(J(o.$$.fragment,r),f=!0)},o(r){j(o.$$.fragment,r),f=!1},d(r){K(o,r)}}}function tn(S){let o,f,r,u,$;return{c(){o=s("p"),f=i("The Rust API Reference is available directly on the "),r=s("a"),u=i("Docs.rs"),$=i(" website."),this.h()},l(_){o=a(_,"P",{});var z=l(o);f=c(z,"The Rust API Reference is available directly on the "),r=a(z,"A",{href:!0,rel:!0});var I=l(r);u=c(I,"Docs.rs"),I.forEach(t),$=c(z," website."),z.forEach(t),this.h()},h(){v(r,"href","https://docs.rs/tokenizers/latest/tokenizers/"),v(r,"rel","nofollow")},m(_,z){d(_,o,z),e(o,f),e(o,r),e(r,u),e(o,$)},d(_){_&&t(o)}}}function nn(S){let o,f;return o=new kt({props:{$$slots:{default:[tn]},$$scope:{ctx:S}}}),{c(){F(o.$$.fragment)},l(r){G(o.$$.fragment,r)},m(r,u){B(o,r,u),f=!0},p(r,u){const $={};u&2&&($.$$scope={dirty:u,ctx:r}),o.$set($)},i(r){f||(J(o.$$.fragment,r),f=!0)},o(r){j(o.$$.fragment,r),f=!1},d(r){K(o,r)}}}function rn(S){let o,f;return{c(){o=s("p"),f=i("The node API has not been documented yet.")},l(r){o=a(r,"P",{});var u=l(o);f=c(u,"The node API has not been documented yet."),u.forEach(t)},m(r,u){d(r,o,u),e(o,f)},d(r){r&&t(o)}}}function on(S){let o,f;return o=new kt({props:{$$slots:{default:[rn]},$$scope:{ctx:S}}}),{c(){F(o.$$.fragment)},l(r){G(o.$$.fragment,r)},m(r,u){B(o,r,u),f=!0},p(r,u){const $={};u&2&&($.$$scope={dirty:u,ctx:r}),o.$set($)},i(r){f||(J(o.$$.fragment,r),f=!0)},o(r){j(o.$$.fragment,r),f=!1},d(r){K(o,r)}}}function sn(S){let o,f,r,u,$,_,z,I,b,w,E,P;return _=new Ue({}),E=new Yt({props:{python:!0,rust:!0,node:!0,$$slots:{node:[on],rust:[nn],python:[en]},$$scope:{ctx:S}}}),{c(){o=s("meta"),f=m(),r=s("h1"),u=s("a"),$=s("span"),F(_.$$.fragment),z=m(),I=s("span"),b=i("Input Sequences"),w=m(),F(E.$$.fragment),this.h()},l(p){const q=Vt('[data-svelte="svelte-1phssyn"]',document.head);o=a(q,"META",{name:!0,content:!0}),q.forEach(t),f=k(p),r=a(p,"H1",{class:!0});var T=l(r);u=a(T,"A",{id:!0,class:!0,href:!0});var g=l(u);$=a(g,"SPAN",{});var Y=l($);G(_.$$.fragment,Y),Y.forEach(t),g.forEach(t),z=k(T),I=a(T,"SPAN",{});var Q=l(I);b=c(Q,"Input Sequences"),Q.forEach(t),T.forEach(t),w=k(p),G(E.$$.fragment,p),this.h()},h(){v(o,"name","hf:doc:metadata"),v(o,"content",JSON.stringify(an)),v(u,"id","input-sequences"),v(u,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),v(u,"href","#input-sequences"),v(r,"class","relative group")},m(p,q){e(document.head,o),d(p,f,q),d(p,r,q),e(r,u),e(u,$),B(_,$,null),e(r,z),e(r,I),e(I,b),d(p,w,q),B(E,p,q),P=!0},p(p,[q]){const T={};q&2&&(T.$$scope={dirty:q,ctx:p}),E.$set(T)},i(p){P||(J(_.$$.fragment,p),J(E.$$.fragment,p),P=!0)},o(p){j(_.$$.fragment,p),j(E.$$.fragment,p),P=!1},d(p){t(o),p&&t(f),p&&t(r),K(_),p&&t(w),K(E,p)}}}const an={local:"input-sequences",sections:[{local:"tokenizers.TextInputSequence",title:"`tokenizers.TextInputSequence`"},{local:"tokenizers.PreTokenizedInputSequence",title:"`tokenizers.PreTokenizedInputSequence`"},{local:"tokenizersinputsequence",title:"`tokenizers.InputSequence`"}],title:"Input Sequences"};function ln(S){return Xt(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class pn extends jt{constructor(o){super();Kt(this,o,ln,sn,Qt,{})}}export{pn as default,an as metadata};
