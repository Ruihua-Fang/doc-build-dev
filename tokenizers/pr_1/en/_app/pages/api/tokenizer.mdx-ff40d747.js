import{S as Ha,i as Ma,s as Wa,e as n,k as i,w as k,t as a,M as Ra,c as r,d as t,m as d,a as o,x as _,h as s,b as m,F as e,g as fe,y as z,q as v,o as $,B as b,v as Ja,L as Ga}from"../../chunks/vendor-0d3f0756.js";import{D as y}from"../../chunks/Docstring-6b5b4b86.js";import{C as Oa}from"../../chunks/CodeBlock-7b0cb15c.js";import{I as Fa}from"../../chunks/IconCopyLink-9193371d.js";import{T as Ba,M as Go}from"../../chunks/TokenizersLanguageContent-ca787841.js";function Ua(I){let p,f,l,u,w,E,q,D,h,P,j,A,T,x,O,Qe,R,Xe,$n,bn,L,ke,Tn,gt,yn,wn,mt,En,xn,ut,qn,An,H,_e,Dn,ft,In,Pn,kt,jn,Ln,M,ze,Vn,_t,Nn,Sn,zt,Cn,On,G,ve,Fn,vt,Hn,Mn,B,$e,Wn,$t,Rn,Jn,U,be,Gn,bt,Bn,Un,V,Te,Yn,Tt,Kn,Qn,yt,Xn,Zn,ye,er,N,we,tr,wt,nr,rr,Et,or,ar,Ee,sr,Y,xe,ir,qe,dr,Ze,cr,lr,pr,K,Ae,hr,De,gr,et,mr,ur,fr,Q,Ie,kr,Pe,_r,tt,zr,vr,$r,X,je,br,Le,Tr,nt,yr,wr,Er,Z,Ve,xr,xt,qr,Ar,ee,Ne,Dr,qt,Ir,Pr,te,Se,jr,At,Lr,Vr,ne,Ce,Nr,Dt,Sr,Cr,re,Oe,Or,It,Fr,Hr,oe,Fe,Mr,Pt,Wr,Rr,S,He,Jr,jt,Gr,Br,Lt,Ur,Yr,J,Me,Kr,Vt,Qr,Xr,Zr,rt,eo,Nt,to,no,We,ro,St,oo,ao,so,ae,Re,io,Je,co,ot,lo,po,ho,se,Ge,go,Be,mo,at,uo,fo,ko,ie,Ue,_o,Ct,zo,vo,W,Ye,$o,Ot,bo,To,st,yo,Ft,wo,Eo,C,Ke,xo,Ht,qo,Ao,Mt,Do,Io,F,it,Po,Wt,jo,Lo,de,Vo,Rt,No,So,Jt,Co,Oo,Gt,Fo,Ho,Bt,Mo,Yt;return u=new Fa({}),P=new y({props:{name:"class tokenizers.Tokenizer",anchor:"tokenizers.Tokenizer",parameters:[{name:"model",val:""}],parametersDescription:[{anchor:"tokenizers.Tokenizer.model",description:`<strong>model</strong> (<a href="/docs/tokenizers/pr_1/en/api/models#tokenizers.models.Model">Model</a>) &#x2014;
The core algorithm that this <code>Tokenizer</code> should be using.`,name:"model"}]}}),ke=new y({props:{name:"add_special_tokens",anchor:"tokenizers.Tokenizer.add_special_tokens",parameters:[{name:"tokens",val:""}],parametersDescription:[{anchor:"tokenizers.Tokenizer.add_special_tokens.tokens",description:`<strong>tokens</strong> (A <code>List</code> of <a href="/docs/tokenizers/pr_1/en/api/added-tokens#tokenizers.AddedToken">AddedToken</a> or <code>str</code>) &#x2014;
The list of special tokens we want to add to the vocabulary. Each token can either
be a string or an instance of <a href="/docs/tokenizers/pr_1/en/api/added-tokens#tokenizers.AddedToken">AddedToken</a> for more
customization.`,name:"tokens"}],returnDescription:`
<p>The number of tokens that were created in the vocabulary</p>
`,returnType:`
<p><code>int</code></p>
`}}),_e=new y({props:{name:"add_tokens",anchor:"tokenizers.Tokenizer.add_tokens",parameters:[{name:"tokens",val:""}],parametersDescription:[{anchor:"tokenizers.Tokenizer.add_tokens.tokens",description:`<strong>tokens</strong> (A <code>List</code> of <a href="/docs/tokenizers/pr_1/en/api/added-tokens#tokenizers.AddedToken">AddedToken</a> or <code>str</code>) &#x2014;
The list of tokens we want to add to the vocabulary. Each token can be either a
string or an instance of <a href="/docs/tokenizers/pr_1/en/api/added-tokens#tokenizers.AddedToken">AddedToken</a> for more customization.`,name:"tokens"}],returnDescription:`
<p>The number of tokens that were created in the vocabulary</p>
`,returnType:`
<p><code>int</code></p>
`}}),ze=new y({props:{name:"decode",anchor:"tokenizers.Tokenizer.decode",parameters:[{name:"ids",val:""},{name:"skip_special_tokens",val:" = True"}],parametersDescription:[{anchor:"tokenizers.Tokenizer.decode.ids",description:`<strong>ids</strong> (A <code>List/Tuple</code> of <code>int</code>) &#x2014;
The list of ids that we want to decode`,name:"ids"},{anchor:"tokenizers.Tokenizer.decode.skip_special_tokens",description:`<strong>skip_special_tokens</strong> (<code>bool</code>, defaults to <code>True</code>) &#x2014;
Whether the special tokens should be removed from the decoded string`,name:"skip_special_tokens"}],returnDescription:`
<p>The decoded string</p>
`,returnType:`
<p><code>str</code></p>
`}}),ve=new y({props:{name:"decode_batch",anchor:"tokenizers.Tokenizer.decode_batch",parameters:[{name:"sequences",val:""},{name:"skip_special_tokens",val:" = True"}],parametersDescription:[{anchor:"tokenizers.Tokenizer.decode_batch.sequences",description:`<strong>sequences</strong> (<code>List</code> of <code>List[int]</code>) &#x2014;
The batch of sequences we want to decode`,name:"sequences"},{anchor:"tokenizers.Tokenizer.decode_batch.skip_special_tokens",description:`<strong>skip_special_tokens</strong> (<code>bool</code>, defaults to <code>True</code>) &#x2014;
Whether the special tokens should be removed from the decoded strings`,name:"skip_special_tokens"}],returnDescription:`
<p>A list of decoded strings</p>
`,returnType:`
<p><code>List[str]</code></p>
`}}),$e=new y({props:{name:"enable_padding",anchor:"tokenizers.Tokenizer.enable_padding",parameters:[{name:"direction",val:" = 'right'"},{name:"pad_id",val:" = 0"},{name:"pad_type_id",val:" = 0"},{name:"pad_token",val:" = '[PAD]'"},{name:"length",val:" = None"},{name:"pad_to_multiple_of",val:" = None"}],parametersDescription:[{anchor:"tokenizers.Tokenizer.enable_padding.direction",description:`<strong>direction</strong> (<code>str</code>, <em>optional</em>, defaults to <code>right</code>) &#x2014;
The direction in which to pad. Can be either <code>right</code> or <code>left</code>`,name:"direction"},{anchor:"tokenizers.Tokenizer.enable_padding.pad_to_multiple_of",description:`<strong>pad_to_multiple_of</strong> (<code>int</code>, <em>optional</em>) &#x2014;
If specified, the padding length should always snap to the next multiple of the
given value. For example if we were going to pad witha length of 250 but
<code>pad_to_multiple_of=8</code> then we will pad to 256.`,name:"pad_to_multiple_of"},{anchor:"tokenizers.Tokenizer.enable_padding.pad_id",description:`<strong>pad_id</strong> (<code>int</code>, defaults to 0) &#x2014;
The id to be used when padding`,name:"pad_id"},{anchor:"tokenizers.Tokenizer.enable_padding.pad_type_id",description:`<strong>pad_type_id</strong> (<code>int</code>, defaults to 0) &#x2014;
The type id to be used when padding`,name:"pad_type_id"},{anchor:"tokenizers.Tokenizer.enable_padding.pad_token",description:`<strong>pad_token</strong> (<code>str</code>, defaults to <code>[PAD]</code>) &#x2014;
The pad token to be used when padding`,name:"pad_token"},{anchor:"tokenizers.Tokenizer.enable_padding.length",description:`<strong>length</strong> (<code>int</code>, <em>optional</em>) &#x2014;
If specified, the length at which to pad. If not specified we pad using the size of
the longest sequence in a batch.`,name:"length"}]}}),be=new y({props:{name:"enable_truncation",anchor:"tokenizers.Tokenizer.enable_truncation",parameters:[{name:"max_length",val:""},{name:"stride",val:" = 0"},{name:"strategy",val:" = 'longest_first'"},{name:"direction",val:" = 'right'"}],parametersDescription:[{anchor:"tokenizers.Tokenizer.enable_truncation.max_length",description:`<strong>max_length</strong> (<code>int</code>) &#x2014;
The max length at which to truncate`,name:"max_length"},{anchor:"tokenizers.Tokenizer.enable_truncation.stride",description:`<strong>stride</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The length of the previous first sequence to be included in the overflowing
sequence`,name:"stride"},{anchor:"tokenizers.Tokenizer.enable_truncation.strategy",description:`<strong>strategy</strong> (<code>str</code>, <em>optional</em>, defaults to <code>longest_first</code>) &#x2014;
The strategy used to truncation. Can be one of <code>longest_first</code>, <code>only_first</code> or
<code>only_second</code>.`,name:"strategy"},{anchor:"tokenizers.Tokenizer.enable_truncation.direction",description:`<strong>direction</strong> (<code>str</code>, defaults to <code>right</code>) &#x2014;
Truncate direction`,name:"direction"}]}}),Te=new y({props:{name:"encode",anchor:"tokenizers.Tokenizer.encode",parameters:[{name:"sequence",val:""},{name:"pair",val:" = None"},{name:"is_pretokenized",val:" = False"},{name:"add_special_tokens",val:" = True"}],parametersDescription:[{anchor:"tokenizers.Tokenizer.encode.sequence",description:`<strong>sequence</strong> (<code>~tokenizers.InputSequence</code>) &#x2014;
The main input sequence we want to encode. This sequence can be either raw
text or pre-tokenized, according to the <code>is_pretokenized</code> argument:</p>
<ul>
<li>If <code>is_pretokenized=False</code>: <code>TextInputSequence</code></li>
<li>If <code>is_pretokenized=True</code>: <code>PreTokenizedInputSequence()</code></li>
</ul>`,name:"sequence"},{anchor:"tokenizers.Tokenizer.encode.pair",description:`<strong>pair</strong> (<code>~tokenizers.InputSequence</code>, <em>optional</em>) &#x2014;
An optional input sequence. The expected format is the same that for <code>sequence</code>.`,name:"pair"},{anchor:"tokenizers.Tokenizer.encode.is_pretokenized",description:`<strong>is_pretokenized</strong> (<code>bool</code>, defaults to <code>False</code>) &#x2014;
Whether the input is already pre-tokenized`,name:"is_pretokenized"},{anchor:"tokenizers.Tokenizer.encode.add_special_tokens",description:`<strong>add_special_tokens</strong> (<code>bool</code>, defaults to <code>True</code>) &#x2014;
Whether to add the special tokens`,name:"add_special_tokens"}],returnDescription:`
<p>The encoded result</p>
`,returnType:`
<p><a href="/docs/tokenizers/pr_1/en/api/encoding#tokenizers.Encoding">Encoding</a></p>
`}}),ye=new Oa({props:{code:`encode("A single sequence")*
encode("A sequence", "And its pair")*
encode([ "A", "pre", "tokenized", "sequence" ], is_pretokenized=True)\`
encode(
[ "A", "pre", "tokenized", "sequence" ], [ "And", "its", "pair" ],
is_pretokenized=True
)`,highlighted:`encode(<span class="hljs-string">&quot;A single sequence&quot;</span>)*
encode(<span class="hljs-string">&quot;A sequence&quot;</span>, <span class="hljs-string">&quot;And its pair&quot;</span>)*
encode([ <span class="hljs-string">&quot;A&quot;</span>, <span class="hljs-string">&quot;pre&quot;</span>, <span class="hljs-string">&quot;tokenized&quot;</span>, <span class="hljs-string">&quot;sequence&quot;</span> ], is_pretokenized=<span class="hljs-literal">True</span>)\`
encode(
[ <span class="hljs-string">&quot;A&quot;</span>, <span class="hljs-string">&quot;pre&quot;</span>, <span class="hljs-string">&quot;tokenized&quot;</span>, <span class="hljs-string">&quot;sequence&quot;</span> ], [ <span class="hljs-string">&quot;And&quot;</span>, <span class="hljs-string">&quot;its&quot;</span>, <span class="hljs-string">&quot;pair&quot;</span> ],
is_pretokenized=<span class="hljs-literal">True</span>
)`}}),we=new y({props:{name:"encode_batch",anchor:"tokenizers.Tokenizer.encode_batch",parameters:[{name:"input",val:""},{name:"is_pretokenized",val:" = False"},{name:"add_special_tokens",val:" = True"}],parametersDescription:[{anchor:"tokenizers.Tokenizer.encode_batch.input",description:`<strong>input</strong> (A <code>List</code>/\`<code>Tuple</code> of <code>~tokenizers.EncodeInput</code>) &#x2014;
A list of single sequences or pair sequences to encode. Each sequence
can be either raw text or pre-tokenized, according to the <code>is_pretokenized</code>
argument:</p>
<ul>
<li>If <code>is_pretokenized=False</code>: <code>TextEncodeInput()</code></li>
<li>If <code>is_pretokenized=True</code>: <code>PreTokenizedEncodeInput()</code></li>
</ul>`,name:"input"},{anchor:"tokenizers.Tokenizer.encode_batch.is_pretokenized",description:`<strong>is_pretokenized</strong> (<code>bool</code>, defaults to <code>False</code>) &#x2014;
Whether the input is already pre-tokenized`,name:"is_pretokenized"},{anchor:"tokenizers.Tokenizer.encode_batch.add_special_tokens",description:`<strong>add_special_tokens</strong> (<code>bool</code>, defaults to <code>True</code>) &#x2014;
Whether to add the special tokens`,name:"add_special_tokens"}],returnDescription:`
<p>The encoded batch</p>
`,returnType:`
<p>A <code>List</code> of [\`~tokenizers.Encoding\u201C]</p>
`}}),Ee=new Oa({props:{code:`encode_batch([
"A single sequence",
("A tuple with a sequence", "And its pair"),
[ "A", "pre", "tokenized", "sequence" ],
([ "A", "pre", "tokenized", "sequence" ], "And its pair")
])`,highlighted:`encode_batch([
<span class="hljs-string">&quot;A single sequence&quot;</span>,
(<span class="hljs-string">&quot;A tuple with a sequence&quot;</span>, <span class="hljs-string">&quot;And its pair&quot;</span>),
[ <span class="hljs-string">&quot;A&quot;</span>, <span class="hljs-string">&quot;pre&quot;</span>, <span class="hljs-string">&quot;tokenized&quot;</span>, <span class="hljs-string">&quot;sequence&quot;</span> ],
([ <span class="hljs-string">&quot;A&quot;</span>, <span class="hljs-string">&quot;pre&quot;</span>, <span class="hljs-string">&quot;tokenized&quot;</span>, <span class="hljs-string">&quot;sequence&quot;</span> ], <span class="hljs-string">&quot;And its pair&quot;</span>)
])`}}),xe=new y({props:{name:"from_buffer",anchor:"tokenizers.Tokenizer.from_buffer",parameters:[{name:"buffer",val:""}],parametersDescription:[{anchor:"tokenizers.Tokenizer.from_buffer.buffer",description:`<strong>buffer</strong> (<code>bytes</code>) &#x2014;
A buffer containing a previously serialized <a href="/docs/tokenizers/pr_1/en/api/tokenizer#tokenizers.Tokenizer">Tokenizer</a>`,name:"buffer"}],returnDescription:`
<p>The new tokenizer</p>
`,returnType:`
<p><a href="/docs/tokenizers/pr_1/en/api/tokenizer#tokenizers.Tokenizer">Tokenizer</a></p>
`}}),Ae=new y({props:{name:"from_file",anchor:"tokenizers.Tokenizer.from_file",parameters:[{name:"path",val:""}],parametersDescription:[{anchor:"tokenizers.Tokenizer.from_file.path",description:`<strong>path</strong> (<code>str</code>) &#x2014;
A path to a local JSON file representing a previously serialized
<a href="/docs/tokenizers/pr_1/en/api/tokenizer#tokenizers.Tokenizer">Tokenizer</a>`,name:"path"}],returnDescription:`
<p>The new tokenizer</p>
`,returnType:`
<p><a href="/docs/tokenizers/pr_1/en/api/tokenizer#tokenizers.Tokenizer">Tokenizer</a></p>
`}}),Ie=new y({props:{name:"from_pretrained",anchor:"tokenizers.Tokenizer.from_pretrained",parameters:[{name:"identifier",val:""},{name:"revision",val:" = 'main'"},{name:"auth_token",val:" = None"}],parametersDescription:[{anchor:"tokenizers.Tokenizer.from_pretrained.identifier",description:`<strong>identifier</strong> (<code>str</code>) &#x2014;
The identifier of a Model on the Hugging Face Hub, that contains
a tokenizer.json file`,name:"identifier"},{anchor:"tokenizers.Tokenizer.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, defaults to <em>main</em>) &#x2014;
A branch or commit id`,name:"revision"},{anchor:"tokenizers.Tokenizer.from_pretrained.auth_token",description:`<strong>auth_token</strong> (<code>str</code>, <em>optional</em>, defaults to <em>None</em>) &#x2014;
An optional auth token used to access private repositories on the
Hugging Face Hub`,name:"auth_token"}],returnDescription:`
<p>The new tokenizer</p>
`,returnType:`
<p><a href="/docs/tokenizers/pr_1/en/api/tokenizer#tokenizers.Tokenizer">Tokenizer</a></p>
`}}),je=new y({props:{name:"from_str",anchor:"tokenizers.Tokenizer.from_str",parameters:[{name:"json",val:""}],parametersDescription:[{anchor:"tokenizers.Tokenizer.from_str.json",description:`<strong>json</strong> (<code>str</code>) &#x2014;
A valid JSON string representing a previously serialized
<a href="/docs/tokenizers/pr_1/en/api/tokenizer#tokenizers.Tokenizer">Tokenizer</a>`,name:"json"}],returnDescription:`
<p>The new tokenizer</p>
`,returnType:`
<p><a href="/docs/tokenizers/pr_1/en/api/tokenizer#tokenizers.Tokenizer">Tokenizer</a></p>
`}}),Ve=new y({props:{name:"get_vocab",anchor:"tokenizers.Tokenizer.get_vocab",parameters:[{name:"with_added_tokens",val:" = True"}],parametersDescription:[{anchor:"tokenizers.Tokenizer.get_vocab.with_added_tokens",description:`<strong>with_added_tokens</strong> (<code>bool</code>, defaults to <code>True</code>) &#x2014;
Whether to include the added tokens`,name:"with_added_tokens"}],returnDescription:`
<p>The vocabulary</p>
`,returnType:`
<p><code>Dict[str, int]</code></p>
`}}),Ne=new y({props:{name:"get_vocab_size",anchor:"tokenizers.Tokenizer.get_vocab_size",parameters:[{name:"with_added_tokens",val:" = True"}],parametersDescription:[{anchor:"tokenizers.Tokenizer.get_vocab_size.with_added_tokens",description:`<strong>with_added_tokens</strong> (<code>bool</code>, defaults to <code>True</code>) &#x2014;
Whether to include the added tokens`,name:"with_added_tokens"}],returnDescription:`
<p>The size of the vocabulary</p>
`,returnType:`
<p><code>int</code></p>
`}}),Se=new y({props:{name:"id_to_token",anchor:"tokenizers.Tokenizer.id_to_token",parameters:[{name:"id",val:""}],parametersDescription:[{anchor:"tokenizers.Tokenizer.id_to_token.id",description:`<strong>id</strong> (<code>int</code>) &#x2014;
The id to convert`,name:"id"}],returnDescription:`
<p>An optional token, <code>None</code> if out of vocabulary</p>
`,returnType:`
<p><code>Optional[str]</code></p>
`}}),Ce=new y({props:{name:"no_padding",anchor:"tokenizers.Tokenizer.no_padding",parameters:[]}}),Oe=new y({props:{name:"no_truncation",anchor:"tokenizers.Tokenizer.no_truncation",parameters:[]}}),Fe=new y({props:{name:"num_special_tokens_to_add",anchor:"tokenizers.Tokenizer.num_special_tokens_to_add",parameters:[{name:"is_pair",val:""}]}}),He=new y({props:{name:"post_process",anchor:"tokenizers.Tokenizer.post_process",parameters:[{name:"encoding",val:""},{name:"pair",val:" = None"},{name:"add_special_tokens",val:" = True"}],parametersDescription:[{anchor:"tokenizers.Tokenizer.post_process.encoding",description:`<strong>encoding</strong> (<a href="/docs/tokenizers/pr_1/en/api/encoding#tokenizers.Encoding">Encoding</a>) &#x2014;
The <a href="/docs/tokenizers/pr_1/en/api/encoding#tokenizers.Encoding">Encoding</a> corresponding to the main sequence.`,name:"encoding"},{anchor:"tokenizers.Tokenizer.post_process.pair",description:`<strong>pair</strong> (<a href="/docs/tokenizers/pr_1/en/api/encoding#tokenizers.Encoding">Encoding</a>, <em>optional</em>) &#x2014;
An optional <a href="/docs/tokenizers/pr_1/en/api/encoding#tokenizers.Encoding">Encoding</a> corresponding to the pair sequence.`,name:"pair"},{anchor:"tokenizers.Tokenizer.post_process.add_special_tokens",description:`<strong>add_special_tokens</strong> (<code>bool</code>) &#x2014;
Whether to add the special tokens`,name:"add_special_tokens"}],returnDescription:`
<p>The final post-processed encoding</p>
`,returnType:`
<p><a href="/docs/tokenizers/pr_1/en/api/encoding#tokenizers.Encoding">Encoding</a></p>
`}}),Re=new y({props:{name:"save",anchor:"tokenizers.Tokenizer.save",parameters:[{name:"pretty",val:" = True"}],parametersDescription:[{anchor:"tokenizers.Tokenizer.save.path",description:`<strong>path</strong> (<code>str</code>) &#x2014;
A path to a file in which to save the serialized tokenizer.`,name:"path"},{anchor:"tokenizers.Tokenizer.save.pretty",description:`<strong>pretty</strong> (<code>bool</code>, defaults to <code>True</code>) &#x2014;
Whether the JSON file should be pretty formatted.`,name:"pretty"}]}}),Ge=new y({props:{name:"to_str",anchor:"tokenizers.Tokenizer.to_str",parameters:[{name:"pretty",val:" = False"}],parametersDescription:[{anchor:"tokenizers.Tokenizer.to_str.pretty",description:`<strong>pretty</strong> (<code>bool</code>, defaults to <code>False</code>) &#x2014;
Whether the JSON string should be pretty formatted.`,name:"pretty"}],returnDescription:`
<p>A string representing the serialized Tokenizer</p>
`,returnType:`
<p><code>str</code></p>
`}}),Ue=new y({props:{name:"token_to_id",anchor:"tokenizers.Tokenizer.token_to_id",parameters:[{name:"token",val:""}],parametersDescription:[{anchor:"tokenizers.Tokenizer.token_to_id.token",description:`<strong>token</strong> (<code>str</code>) &#x2014;
The token to convert`,name:"token"}],returnDescription:`
<p>An optional id, <code>None</code> if out of vocabulary</p>
`,returnType:`
<p><code>Optional[int]</code></p>
`}}),Ye=new y({props:{name:"train",anchor:"tokenizers.Tokenizer.train",parameters:[{name:"files",val:""},{name:"trainer",val:" = None"}],parametersDescription:[{anchor:"tokenizers.Tokenizer.train.files",description:`<strong>files</strong> (<code>List[str]</code>) &#x2014;
A list of path to the files that we should use for training`,name:"files"},{anchor:"tokenizers.Tokenizer.train.trainer",description:`<strong>trainer</strong> (<code>~tokenizers.trainers.Trainer</code>, <em>optional</em>) &#x2014;
An optional trainer that should be used to train our Model`,name:"trainer"}]}}),Ke=new y({props:{name:"train_from_iterator",anchor:"tokenizers.Tokenizer.train_from_iterator",parameters:[{name:"iterator",val:""},{name:"trainer",val:" = None"},{name:"length",val:" = None"}],parametersDescription:[{anchor:"tokenizers.Tokenizer.train_from_iterator.iterator",description:`<strong>iterator</strong> (<code>Iterator</code>) &#x2014;
Any iterator over strings or list of strings`,name:"iterator"},{anchor:"tokenizers.Tokenizer.train_from_iterator.trainer",description:`<strong>trainer</strong> (<code>~tokenizers.trainers.Trainer</code>, <em>optional</em>) &#x2014;
An optional trainer that should be used to train our Model`,name:"trainer"},{anchor:"tokenizers.Tokenizer.train_from_iterator.length",description:`<strong>length</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The total number of sequences in the iterator. This is used to
provide meaningful progress tracking`,name:"length"}]}}),{c(){p=n("h2"),f=n("a"),l=n("span"),k(u.$$.fragment),w=i(),E=n("span"),q=a("Tokenizer"),D=i(),h=n("div"),k(P.$$.fragment),j=i(),A=n("p"),T=a("A "),x=n("code"),O=a("Tokenizer"),Qe=a(` works as a pipeline. It processes some raw text as input
and outputs an `),R=n("a"),Xe=a("Encoding"),$n=a("."),bn=i(),L=n("div"),k(ke.$$.fragment),Tn=i(),gt=n("p"),yn=a("Add the given special tokens to the Tokenizer."),wn=i(),mt=n("p"),En=a(`If these tokens are already part of the vocabulary, it just let the Tokenizer know about
them. If they don\u2019t exist, the Tokenizer creates them, giving them a new id.`),xn=i(),ut=n("p"),qn=a(`These special tokens will never be processed by the model (ie won\u2019t be split into
multiple tokens), and they can be removed from the output when decoding.`),An=i(),H=n("div"),k(_e.$$.fragment),Dn=i(),ft=n("p"),In=a("Add the given tokens to the vocabulary"),Pn=i(),kt=n("p"),jn=a(`The given tokens are added only if they don\u2019t already exist in the vocabulary.
Each token then gets a new attributed id.`),Ln=i(),M=n("div"),k(ze.$$.fragment),Vn=i(),_t=n("p"),Nn=a("Decode the given list of ids back to a string"),Sn=i(),zt=n("p"),Cn=a("This is used to decode anything coming back from a Language Model"),On=i(),G=n("div"),k(ve.$$.fragment),Fn=i(),vt=n("p"),Hn=a("Decode a batch of ids back to their corresponding string"),Mn=i(),B=n("div"),k($e.$$.fragment),Wn=i(),$t=n("p"),Rn=a("Enable the padding"),Jn=i(),U=n("div"),k(be.$$.fragment),Gn=i(),bt=n("p"),Bn=a("Enable truncation"),Un=i(),V=n("div"),k(Te.$$.fragment),Yn=i(),Tt=n("p"),Kn=a(`Encode the given sequence and pair. This method can process raw text sequences
as well as already pre-tokenized sequences.`),Qn=i(),yt=n("p"),Xn=a(`Example:
Here are some examples of the inputs that are accepted:`),Zn=i(),k(ye.$$.fragment),er=i(),N=n("div"),k(we.$$.fragment),tr=i(),wt=n("p"),nr=a(`Encode the given batch of inputs. This method accept both raw text sequences
as well as already pre-tokenized sequences.`),rr=i(),Et=n("p"),or=a(`Example:
Here are some examples of the inputs that are accepted:`),ar=i(),k(Ee.$$.fragment),sr=i(),Y=n("div"),k(xe.$$.fragment),ir=i(),qe=n("p"),dr=a("Instantiate a new "),Ze=n("a"),cr=a("Tokenizer"),lr=a(" from the given buffer."),pr=i(),K=n("div"),k(Ae.$$.fragment),hr=i(),De=n("p"),gr=a("Instantiate a new "),et=n("a"),mr=a("Tokenizer"),ur=a(" from the file at the given path."),fr=i(),Q=n("div"),k(Ie.$$.fragment),kr=i(),Pe=n("p"),_r=a("Instantiate a new "),tt=n("a"),zr=a("Tokenizer"),vr=a(` from an existing file on the
Hugging Face Hub.`),$r=i(),X=n("div"),k(je.$$.fragment),br=i(),Le=n("p"),Tr=a("Instantiate a new "),nt=n("a"),yr=a("Tokenizer"),wr=a(" from the given JSON string."),Er=i(),Z=n("div"),k(Ve.$$.fragment),xr=i(),xt=n("p"),qr=a("Get the underlying vocabulary"),Ar=i(),ee=n("div"),k(Ne.$$.fragment),Dr=i(),qt=n("p"),Ir=a("Get the size of the underlying vocabulary"),Pr=i(),te=n("div"),k(Se.$$.fragment),jr=i(),At=n("p"),Lr=a("Convert the given id to its corresponding token if it exists"),Vr=i(),ne=n("div"),k(Ce.$$.fragment),Nr=i(),Dt=n("p"),Sr=a("Disable padding"),Cr=i(),re=n("div"),k(Oe.$$.fragment),Or=i(),It=n("p"),Fr=a("Disable truncation"),Hr=i(),oe=n("div"),k(Fe.$$.fragment),Mr=i(),Pt=n("p"),Wr=a(`Return the number of special tokens that would be added for single/pair sentences.
:param is_pair: Boolean indicating if the input would be a single sentence or a pair
:return:`),Rr=i(),S=n("div"),k(He.$$.fragment),Jr=i(),jt=n("p"),Gr=a("Apply all the post-processing steps to the given encodings."),Br=i(),Lt=n("p"),Ur=a("The various steps are:"),Yr=i(),J=n("ol"),Me=n("li"),Kr=a(`Truncate according to the set truncation params (provided with
`),Vt=n("code"),Qr=a("enable_truncation()"),Xr=a(")"),Zr=i(),rt=n("li"),eo=a("Apply the "),Nt=n("code"),to=a("PostProcessor"),no=i(),We=n("li"),ro=a(`Pad according to the set padding params (provided with
`),St=n("code"),oo=a("enable_padding()"),ao=a(")"),so=i(),ae=n("div"),k(Re.$$.fragment),io=i(),Je=n("p"),co=a("Save the "),ot=n("a"),lo=a("Tokenizer"),po=a(" to the file at the given path."),ho=i(),se=n("div"),k(Ge.$$.fragment),go=i(),Be=n("p"),mo=a("Gets a serialized string representing this "),at=n("a"),uo=a("Tokenizer"),fo=a("."),ko=i(),ie=n("div"),k(Ue.$$.fragment),_o=i(),Ct=n("p"),zo=a("Convert the given token to its corresponding id if it exists"),vo=i(),W=n("div"),k(Ye.$$.fragment),$o=i(),Ot=n("p"),bo=a("Train the Tokenizer using the given files."),To=i(),st=n("p"),yo=a(`Reads the files line by line, while keeping all the whitespace, even new lines.
If you want to train from data store in-memory, you can check
`),Ft=n("code"),wo=a("train_from_iterator()"),Eo=i(),C=n("div"),k(Ke.$$.fragment),xo=i(),Ht=n("p"),qo=a("Train the Tokenizer using the provided iterator."),Ao=i(),Mt=n("p"),Do=a("You can provide anything that is a Python Iterator"),Io=i(),F=n("ul"),it=n("li"),Po=a("A list of sequences "),Wt=n("code"),jo=a("List[str]"),Lo=i(),de=n("li"),Vo=a("A generator that yields "),Rt=n("code"),No=a("str"),So=a(" or "),Jt=n("code"),Co=a("List[str]"),Oo=i(),Gt=n("li"),Fo=a("A Numpy array of strings"),Ho=i(),Bt=n("li"),Mo=a("\u2026"),this.h()},l(c){p=r(c,"H2",{class:!0});var ce=o(p);f=r(ce,"A",{id:!0,class:!0,href:!0});var Bo=o(f);l=r(Bo,"SPAN",{});var Uo=o(l);_(u.$$.fragment,Uo),Uo.forEach(t),Bo.forEach(t),w=d(ce),E=r(ce,"SPAN",{});var Yo=o(E);q=s(Yo,"Tokenizer"),Yo.forEach(t),ce.forEach(t),D=d(c),h=r(c,"DIV",{class:!0});var g=o(h);_(P.$$.fragment,g),j=d(g),A=r(g,"P",{});var dt=o(A);T=s(dt,"A "),x=r(dt,"CODE",{});var Ko=o(x);O=s(Ko,"Tokenizer"),Ko.forEach(t),Qe=s(dt,` works as a pipeline. It processes some raw text as input
and outputs an `),R=r(dt,"A",{href:!0});var Qo=o(R);Xe=s(Qo,"Encoding"),Qo.forEach(t),$n=s(dt,"."),dt.forEach(t),bn=d(g),L=r(g,"DIV",{class:!0});var le=o(L);_(ke.$$.fragment,le),Tn=d(le),gt=r(le,"P",{});var Xo=o(gt);yn=s(Xo,"Add the given special tokens to the Tokenizer."),Xo.forEach(t),wn=d(le),mt=r(le,"P",{});var Zo=o(mt);En=s(Zo,`If these tokens are already part of the vocabulary, it just let the Tokenizer know about
them. If they don\u2019t exist, the Tokenizer creates them, giving them a new id.`),Zo.forEach(t),xn=d(le),ut=r(le,"P",{});var ea=o(ut);qn=s(ea,`These special tokens will never be processed by the model (ie won\u2019t be split into
multiple tokens), and they can be removed from the output when decoding.`),ea.forEach(t),le.forEach(t),An=d(g),H=r(g,"DIV",{class:!0});var ct=o(H);_(_e.$$.fragment,ct),Dn=d(ct),ft=r(ct,"P",{});var ta=o(ft);In=s(ta,"Add the given tokens to the vocabulary"),ta.forEach(t),Pn=d(ct),kt=r(ct,"P",{});var na=o(kt);jn=s(na,`The given tokens are added only if they don\u2019t already exist in the vocabulary.
Each token then gets a new attributed id.`),na.forEach(t),ct.forEach(t),Ln=d(g),M=r(g,"DIV",{class:!0});var lt=o(M);_(ze.$$.fragment,lt),Vn=d(lt),_t=r(lt,"P",{});var ra=o(_t);Nn=s(ra,"Decode the given list of ids back to a string"),ra.forEach(t),Sn=d(lt),zt=r(lt,"P",{});var oa=o(zt);Cn=s(oa,"This is used to decode anything coming back from a Language Model"),oa.forEach(t),lt.forEach(t),On=d(g),G=r(g,"DIV",{class:!0});var Kt=o(G);_(ve.$$.fragment,Kt),Fn=d(Kt),vt=r(Kt,"P",{});var aa=o(vt);Hn=s(aa,"Decode a batch of ids back to their corresponding string"),aa.forEach(t),Kt.forEach(t),Mn=d(g),B=r(g,"DIV",{class:!0});var Qt=o(B);_($e.$$.fragment,Qt),Wn=d(Qt),$t=r(Qt,"P",{});var sa=o($t);Rn=s(sa,"Enable the padding"),sa.forEach(t),Qt.forEach(t),Jn=d(g),U=r(g,"DIV",{class:!0});var Xt=o(U);_(be.$$.fragment,Xt),Gn=d(Xt),bt=r(Xt,"P",{});var ia=o(bt);Bn=s(ia,"Enable truncation"),ia.forEach(t),Xt.forEach(t),Un=d(g),V=r(g,"DIV",{class:!0});var pe=o(V);_(Te.$$.fragment,pe),Yn=d(pe),Tt=r(pe,"P",{});var da=o(Tt);Kn=s(da,`Encode the given sequence and pair. This method can process raw text sequences
as well as already pre-tokenized sequences.`),da.forEach(t),Qn=d(pe),yt=r(pe,"P",{});var ca=o(yt);Xn=s(ca,`Example:
Here are some examples of the inputs that are accepted:`),ca.forEach(t),Zn=d(pe),_(ye.$$.fragment,pe),pe.forEach(t),er=d(g),N=r(g,"DIV",{class:!0});var he=o(N);_(we.$$.fragment,he),tr=d(he),wt=r(he,"P",{});var la=o(wt);nr=s(la,`Encode the given batch of inputs. This method accept both raw text sequences
as well as already pre-tokenized sequences.`),la.forEach(t),rr=d(he),Et=r(he,"P",{});var pa=o(Et);or=s(pa,`Example:
Here are some examples of the inputs that are accepted:`),pa.forEach(t),ar=d(he),_(Ee.$$.fragment,he),he.forEach(t),sr=d(g),Y=r(g,"DIV",{class:!0});var Zt=o(Y);_(xe.$$.fragment,Zt),ir=d(Zt),qe=r(Zt,"P",{});var en=o(qe);dr=s(en,"Instantiate a new "),Ze=r(en,"A",{href:!0});var ha=o(Ze);cr=s(ha,"Tokenizer"),ha.forEach(t),lr=s(en," from the given buffer."),en.forEach(t),Zt.forEach(t),pr=d(g),K=r(g,"DIV",{class:!0});var tn=o(K);_(Ae.$$.fragment,tn),hr=d(tn),De=r(tn,"P",{});var nn=o(De);gr=s(nn,"Instantiate a new "),et=r(nn,"A",{href:!0});var ga=o(et);mr=s(ga,"Tokenizer"),ga.forEach(t),ur=s(nn," from the file at the given path."),nn.forEach(t),tn.forEach(t),fr=d(g),Q=r(g,"DIV",{class:!0});var rn=o(Q);_(Ie.$$.fragment,rn),kr=d(rn),Pe=r(rn,"P",{});var on=o(Pe);_r=s(on,"Instantiate a new "),tt=r(on,"A",{href:!0});var ma=o(tt);zr=s(ma,"Tokenizer"),ma.forEach(t),vr=s(on,` from an existing file on the
Hugging Face Hub.`),on.forEach(t),rn.forEach(t),$r=d(g),X=r(g,"DIV",{class:!0});var an=o(X);_(je.$$.fragment,an),br=d(an),Le=r(an,"P",{});var sn=o(Le);Tr=s(sn,"Instantiate a new "),nt=r(sn,"A",{href:!0});var ua=o(nt);yr=s(ua,"Tokenizer"),ua.forEach(t),wr=s(sn," from the given JSON string."),sn.forEach(t),an.forEach(t),Er=d(g),Z=r(g,"DIV",{class:!0});var dn=o(Z);_(Ve.$$.fragment,dn),xr=d(dn),xt=r(dn,"P",{});var fa=o(xt);qr=s(fa,"Get the underlying vocabulary"),fa.forEach(t),dn.forEach(t),Ar=d(g),ee=r(g,"DIV",{class:!0});var cn=o(ee);_(Ne.$$.fragment,cn),Dr=d(cn),qt=r(cn,"P",{});var ka=o(qt);Ir=s(ka,"Get the size of the underlying vocabulary"),ka.forEach(t),cn.forEach(t),Pr=d(g),te=r(g,"DIV",{class:!0});var ln=o(te);_(Se.$$.fragment,ln),jr=d(ln),At=r(ln,"P",{});var _a=o(At);Lr=s(_a,"Convert the given id to its corresponding token if it exists"),_a.forEach(t),ln.forEach(t),Vr=d(g),ne=r(g,"DIV",{class:!0});var pn=o(ne);_(Ce.$$.fragment,pn),Nr=d(pn),Dt=r(pn,"P",{});var za=o(Dt);Sr=s(za,"Disable padding"),za.forEach(t),pn.forEach(t),Cr=d(g),re=r(g,"DIV",{class:!0});var hn=o(re);_(Oe.$$.fragment,hn),Or=d(hn),It=r(hn,"P",{});var va=o(It);Fr=s(va,"Disable truncation"),va.forEach(t),hn.forEach(t),Hr=d(g),oe=r(g,"DIV",{class:!0});var gn=o(oe);_(Fe.$$.fragment,gn),Mr=d(gn),Pt=r(gn,"P",{});var $a=o(Pt);Wr=s($a,`Return the number of special tokens that would be added for single/pair sentences.
:param is_pair: Boolean indicating if the input would be a single sentence or a pair
:return:`),$a.forEach(t),gn.forEach(t),Rr=d(g),S=r(g,"DIV",{class:!0});var ge=o(S);_(He.$$.fragment,ge),Jr=d(ge),jt=r(ge,"P",{});var ba=o(jt);Gr=s(ba,"Apply all the post-processing steps to the given encodings."),ba.forEach(t),Br=d(ge),Lt=r(ge,"P",{});var Ta=o(Lt);Ur=s(Ta,"The various steps are:"),Ta.forEach(t),Yr=d(ge),J=r(ge,"OL",{});var pt=o(J);Me=r(pt,"LI",{});var mn=o(Me);Kr=s(mn,`Truncate according to the set truncation params (provided with
`),Vt=r(mn,"CODE",{});var ya=o(Vt);Qr=s(ya,"enable_truncation()"),ya.forEach(t),Xr=s(mn,")"),mn.forEach(t),Zr=d(pt),rt=r(pt,"LI",{});var Wo=o(rt);eo=s(Wo,"Apply the "),Nt=r(Wo,"CODE",{});var wa=o(Nt);to=s(wa,"PostProcessor"),wa.forEach(t),Wo.forEach(t),no=d(pt),We=r(pt,"LI",{});var un=o(We);ro=s(un,`Pad according to the set padding params (provided with
`),St=r(un,"CODE",{});var Ea=o(St);oo=s(Ea,"enable_padding()"),Ea.forEach(t),ao=s(un,")"),un.forEach(t),pt.forEach(t),ge.forEach(t),so=d(g),ae=r(g,"DIV",{class:!0});var fn=o(ae);_(Re.$$.fragment,fn),io=d(fn),Je=r(fn,"P",{});var kn=o(Je);co=s(kn,"Save the "),ot=r(kn,"A",{href:!0});var xa=o(ot);lo=s(xa,"Tokenizer"),xa.forEach(t),po=s(kn," to the file at the given path."),kn.forEach(t),fn.forEach(t),ho=d(g),se=r(g,"DIV",{class:!0});var _n=o(se);_(Ge.$$.fragment,_n),go=d(_n),Be=r(_n,"P",{});var zn=o(Be);mo=s(zn,"Gets a serialized string representing this "),at=r(zn,"A",{href:!0});var qa=o(at);uo=s(qa,"Tokenizer"),qa.forEach(t),fo=s(zn,"."),zn.forEach(t),_n.forEach(t),ko=d(g),ie=r(g,"DIV",{class:!0});var vn=o(ie);_(Ue.$$.fragment,vn),_o=d(vn),Ct=r(vn,"P",{});var Aa=o(Ct);zo=s(Aa,"Convert the given token to its corresponding id if it exists"),Aa.forEach(t),vn.forEach(t),vo=d(g),W=r(g,"DIV",{class:!0});var ht=o(W);_(Ye.$$.fragment,ht),$o=d(ht),Ot=r(ht,"P",{});var Da=o(Ot);bo=s(Da,"Train the Tokenizer using the given files."),Da.forEach(t),To=d(ht),st=r(ht,"P",{});var Ro=o(st);yo=s(Ro,`Reads the files line by line, while keeping all the whitespace, even new lines.
If you want to train from data store in-memory, you can check
`),Ft=r(Ro,"CODE",{});var Ia=o(Ft);wo=s(Ia,"train_from_iterator()"),Ia.forEach(t),Ro.forEach(t),ht.forEach(t),Eo=d(g),C=r(g,"DIV",{class:!0});var me=o(C);_(Ke.$$.fragment,me),xo=d(me),Ht=r(me,"P",{});var Pa=o(Ht);qo=s(Pa,"Train the Tokenizer using the provided iterator."),Pa.forEach(t),Ao=d(me),Mt=r(me,"P",{});var ja=o(Mt);Do=s(ja,"You can provide anything that is a Python Iterator"),ja.forEach(t),Io=d(me),F=r(me,"UL",{});var ue=o(F);it=r(ue,"LI",{});var Jo=o(it);Po=s(Jo,"A list of sequences "),Wt=r(Jo,"CODE",{});var La=o(Wt);jo=s(La,"List[str]"),La.forEach(t),Jo.forEach(t),Lo=d(ue),de=r(ue,"LI",{});var Ut=o(de);Vo=s(Ut,"A generator that yields "),Rt=r(Ut,"CODE",{});var Va=o(Rt);No=s(Va,"str"),Va.forEach(t),So=s(Ut," or "),Jt=r(Ut,"CODE",{});var Na=o(Jt);Co=s(Na,"List[str]"),Na.forEach(t),Ut.forEach(t),Oo=d(ue),Gt=r(ue,"LI",{});var Sa=o(Gt);Fo=s(Sa,"A Numpy array of strings"),Sa.forEach(t),Ho=d(ue),Bt=r(ue,"LI",{});var Ca=o(Bt);Mo=s(Ca,"\u2026"),Ca.forEach(t),ue.forEach(t),me.forEach(t),g.forEach(t),this.h()},h(){m(f,"id","tokenizers.Tokenizer]][[tokenizers.Tokenizer"),m(f,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(f,"href","#tokenizers.Tokenizer]][[tokenizers.Tokenizer"),m(p,"class","relative group"),m(R,"href","/docs/tokenizers/pr_1/en/api/encoding#tokenizers.Encoding"),m(L,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),m(H,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),m(M,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),m(G,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),m(B,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),m(U,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),m(V,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),m(N,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),m(Ze,"href","/docs/tokenizers/pr_1/en/api/tokenizer#tokenizers.Tokenizer"),m(Y,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),m(et,"href","/docs/tokenizers/pr_1/en/api/tokenizer#tokenizers.Tokenizer"),m(K,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),m(tt,"href","/docs/tokenizers/pr_1/en/api/tokenizer#tokenizers.Tokenizer"),m(Q,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),m(nt,"href","/docs/tokenizers/pr_1/en/api/tokenizer#tokenizers.Tokenizer"),m(X,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),m(Z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),m(ee,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),m(te,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),m(ne,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),m(re,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),m(oe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),m(S,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),m(ot,"href","/docs/tokenizers/pr_1/en/api/tokenizer#tokenizers.Tokenizer"),m(ae,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),m(at,"href","/docs/tokenizers/pr_1/en/api/tokenizer#tokenizers.Tokenizer"),m(se,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),m(ie,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),m(W,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),m(C,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),m(h,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(c,ce){fe(c,p,ce),e(p,f),e(f,l),z(u,l,null),e(p,w),e(p,E),e(E,q),fe(c,D,ce),fe(c,h,ce),z(P,h,null),e(h,j),e(h,A),e(A,T),e(A,x),e(x,O),e(A,Qe),e(A,R),e(R,Xe),e(A,$n),e(h,bn),e(h,L),z(ke,L,null),e(L,Tn),e(L,gt),e(gt,yn),e(L,wn),e(L,mt),e(mt,En),e(L,xn),e(L,ut),e(ut,qn),e(h,An),e(h,H),z(_e,H,null),e(H,Dn),e(H,ft),e(ft,In),e(H,Pn),e(H,kt),e(kt,jn),e(h,Ln),e(h,M),z(ze,M,null),e(M,Vn),e(M,_t),e(_t,Nn),e(M,Sn),e(M,zt),e(zt,Cn),e(h,On),e(h,G),z(ve,G,null),e(G,Fn),e(G,vt),e(vt,Hn),e(h,Mn),e(h,B),z($e,B,null),e(B,Wn),e(B,$t),e($t,Rn),e(h,Jn),e(h,U),z(be,U,null),e(U,Gn),e(U,bt),e(bt,Bn),e(h,Un),e(h,V),z(Te,V,null),e(V,Yn),e(V,Tt),e(Tt,Kn),e(V,Qn),e(V,yt),e(yt,Xn),e(V,Zn),z(ye,V,null),e(h,er),e(h,N),z(we,N,null),e(N,tr),e(N,wt),e(wt,nr),e(N,rr),e(N,Et),e(Et,or),e(N,ar),z(Ee,N,null),e(h,sr),e(h,Y),z(xe,Y,null),e(Y,ir),e(Y,qe),e(qe,dr),e(qe,Ze),e(Ze,cr),e(qe,lr),e(h,pr),e(h,K),z(Ae,K,null),e(K,hr),e(K,De),e(De,gr),e(De,et),e(et,mr),e(De,ur),e(h,fr),e(h,Q),z(Ie,Q,null),e(Q,kr),e(Q,Pe),e(Pe,_r),e(Pe,tt),e(tt,zr),e(Pe,vr),e(h,$r),e(h,X),z(je,X,null),e(X,br),e(X,Le),e(Le,Tr),e(Le,nt),e(nt,yr),e(Le,wr),e(h,Er),e(h,Z),z(Ve,Z,null),e(Z,xr),e(Z,xt),e(xt,qr),e(h,Ar),e(h,ee),z(Ne,ee,null),e(ee,Dr),e(ee,qt),e(qt,Ir),e(h,Pr),e(h,te),z(Se,te,null),e(te,jr),e(te,At),e(At,Lr),e(h,Vr),e(h,ne),z(Ce,ne,null),e(ne,Nr),e(ne,Dt),e(Dt,Sr),e(h,Cr),e(h,re),z(Oe,re,null),e(re,Or),e(re,It),e(It,Fr),e(h,Hr),e(h,oe),z(Fe,oe,null),e(oe,Mr),e(oe,Pt),e(Pt,Wr),e(h,Rr),e(h,S),z(He,S,null),e(S,Jr),e(S,jt),e(jt,Gr),e(S,Br),e(S,Lt),e(Lt,Ur),e(S,Yr),e(S,J),e(J,Me),e(Me,Kr),e(Me,Vt),e(Vt,Qr),e(Me,Xr),e(J,Zr),e(J,rt),e(rt,eo),e(rt,Nt),e(Nt,to),e(J,no),e(J,We),e(We,ro),e(We,St),e(St,oo),e(We,ao),e(h,so),e(h,ae),z(Re,ae,null),e(ae,io),e(ae,Je),e(Je,co),e(Je,ot),e(ot,lo),e(Je,po),e(h,ho),e(h,se),z(Ge,se,null),e(se,go),e(se,Be),e(Be,mo),e(Be,at),e(at,uo),e(Be,fo),e(h,ko),e(h,ie),z(Ue,ie,null),e(ie,_o),e(ie,Ct),e(Ct,zo),e(h,vo),e(h,W),z(Ye,W,null),e(W,$o),e(W,Ot),e(Ot,bo),e(W,To),e(W,st),e(st,yo),e(st,Ft),e(Ft,wo),e(h,Eo),e(h,C),z(Ke,C,null),e(C,xo),e(C,Ht),e(Ht,qo),e(C,Ao),e(C,Mt),e(Mt,Do),e(C,Io),e(C,F),e(F,it),e(it,Po),e(it,Wt),e(Wt,jo),e(F,Lo),e(F,de),e(de,Vo),e(de,Rt),e(Rt,No),e(de,So),e(de,Jt),e(Jt,Co),e(F,Oo),e(F,Gt),e(Gt,Fo),e(F,Ho),e(F,Bt),e(Bt,Mo),Yt=!0},p:Ga,i(c){Yt||(v(u.$$.fragment,c),v(P.$$.fragment,c),v(ke.$$.fragment,c),v(_e.$$.fragment,c),v(ze.$$.fragment,c),v(ve.$$.fragment,c),v($e.$$.fragment,c),v(be.$$.fragment,c),v(Te.$$.fragment,c),v(ye.$$.fragment,c),v(we.$$.fragment,c),v(Ee.$$.fragment,c),v(xe.$$.fragment,c),v(Ae.$$.fragment,c),v(Ie.$$.fragment,c),v(je.$$.fragment,c),v(Ve.$$.fragment,c),v(Ne.$$.fragment,c),v(Se.$$.fragment,c),v(Ce.$$.fragment,c),v(Oe.$$.fragment,c),v(Fe.$$.fragment,c),v(He.$$.fragment,c),v(Re.$$.fragment,c),v(Ge.$$.fragment,c),v(Ue.$$.fragment,c),v(Ye.$$.fragment,c),v(Ke.$$.fragment,c),Yt=!0)},o(c){$(u.$$.fragment,c),$(P.$$.fragment,c),$(ke.$$.fragment,c),$(_e.$$.fragment,c),$(ze.$$.fragment,c),$(ve.$$.fragment,c),$($e.$$.fragment,c),$(be.$$.fragment,c),$(Te.$$.fragment,c),$(ye.$$.fragment,c),$(we.$$.fragment,c),$(Ee.$$.fragment,c),$(xe.$$.fragment,c),$(Ae.$$.fragment,c),$(Ie.$$.fragment,c),$(je.$$.fragment,c),$(Ve.$$.fragment,c),$(Ne.$$.fragment,c),$(Se.$$.fragment,c),$(Ce.$$.fragment,c),$(Oe.$$.fragment,c),$(Fe.$$.fragment,c),$(He.$$.fragment,c),$(Re.$$.fragment,c),$(Ge.$$.fragment,c),$(Ue.$$.fragment,c),$(Ye.$$.fragment,c),$(Ke.$$.fragment,c),Yt=!1},d(c){c&&t(p),b(u),c&&t(D),c&&t(h),b(P),b(ke),b(_e),b(ze),b(ve),b($e),b(be),b(Te),b(ye),b(we),b(Ee),b(xe),b(Ae),b(Ie),b(je),b(Ve),b(Ne),b(Se),b(Ce),b(Oe),b(Fe),b(He),b(Re),b(Ge),b(Ue),b(Ye),b(Ke)}}}function Ya(I){let p,f;return p=new Go({props:{$$slots:{default:[Ua]},$$scope:{ctx:I}}}),{c(){k(p.$$.fragment)},l(l){_(p.$$.fragment,l)},m(l,u){z(p,l,u),f=!0},p(l,u){const w={};u&2&&(w.$$scope={dirty:u,ctx:l}),p.$set(w)},i(l){f||(v(p.$$.fragment,l),f=!0)},o(l){$(p.$$.fragment,l),f=!1},d(l){b(p,l)}}}function Ka(I){let p,f,l,u,w;return{c(){p=n("p"),f=a("The Rust API Reference is available directly on the "),l=n("a"),u=a("Docs.rs"),w=a(" website."),this.h()},l(E){p=r(E,"P",{});var q=o(p);f=s(q,"The Rust API Reference is available directly on the "),l=r(q,"A",{href:!0,rel:!0});var D=o(l);u=s(D,"Docs.rs"),D.forEach(t),w=s(q," website."),q.forEach(t),this.h()},h(){m(l,"href","https://docs.rs/tokenizers/latest/tokenizers/"),m(l,"rel","nofollow")},m(E,q){fe(E,p,q),e(p,f),e(p,l),e(l,u),e(p,w)},d(E){E&&t(p)}}}function Qa(I){let p,f;return p=new Go({props:{$$slots:{default:[Ka]},$$scope:{ctx:I}}}),{c(){k(p.$$.fragment)},l(l){_(p.$$.fragment,l)},m(l,u){z(p,l,u),f=!0},p(l,u){const w={};u&2&&(w.$$scope={dirty:u,ctx:l}),p.$set(w)},i(l){f||(v(p.$$.fragment,l),f=!0)},o(l){$(p.$$.fragment,l),f=!1},d(l){b(p,l)}}}function Xa(I){let p,f;return{c(){p=n("p"),f=a("The node API has not been documented yet.")},l(l){p=r(l,"P",{});var u=o(p);f=s(u,"The node API has not been documented yet."),u.forEach(t)},m(l,u){fe(l,p,u),e(p,f)},d(l){l&&t(p)}}}function Za(I){let p,f;return p=new Go({props:{$$slots:{default:[Xa]},$$scope:{ctx:I}}}),{c(){k(p.$$.fragment)},l(l){_(p.$$.fragment,l)},m(l,u){z(p,l,u),f=!0},p(l,u){const w={};u&2&&(w.$$scope={dirty:u,ctx:l}),p.$set(w)},i(l){f||(v(p.$$.fragment,l),f=!0)},o(l){$(p.$$.fragment,l),f=!1},d(l){b(p,l)}}}function es(I){let p,f,l,u,w,E,q,D,h,P,j,A;return E=new Fa({}),j=new Ba({props:{python:!0,rust:!0,node:!0,$$slots:{node:[Za],rust:[Qa],python:[Ya]},$$scope:{ctx:I}}}),{c(){p=n("meta"),f=i(),l=n("h1"),u=n("a"),w=n("span"),k(E.$$.fragment),q=i(),D=n("span"),h=a("Tokenizer"),P=i(),k(j.$$.fragment),this.h()},l(T){const x=Ra('[data-svelte="svelte-1phssyn"]',document.head);p=r(x,"META",{name:!0,content:!0}),x.forEach(t),f=d(T),l=r(T,"H1",{class:!0});var O=o(l);u=r(O,"A",{id:!0,class:!0,href:!0});var Qe=o(u);w=r(Qe,"SPAN",{});var R=o(w);_(E.$$.fragment,R),R.forEach(t),Qe.forEach(t),q=d(O),D=r(O,"SPAN",{});var Xe=o(D);h=s(Xe,"Tokenizer"),Xe.forEach(t),O.forEach(t),P=d(T),_(j.$$.fragment,T),this.h()},h(){m(p,"name","hf:doc:metadata"),m(p,"content",JSON.stringify(ts)),m(u,"id","tokenizer"),m(u,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(u,"href","#tokenizer"),m(l,"class","relative group")},m(T,x){e(document.head,p),fe(T,f,x),fe(T,l,x),e(l,u),e(u,w),z(E,w,null),e(l,q),e(l,D),e(D,h),fe(T,P,x),z(j,T,x),A=!0},p(T,[x]){const O={};x&2&&(O.$$scope={dirty:x,ctx:T}),j.$set(O)},i(T){A||(v(E.$$.fragment,T),v(j.$$.fragment,T),A=!0)},o(T){$(E.$$.fragment,T),$(j.$$.fragment,T),A=!1},d(T){t(p),T&&t(f),T&&t(l),b(E),T&&t(P),b(j,T)}}}const ts={local:"tokenizer",sections:[{local:"tokenizers.Tokenizer]][[tokenizers.Tokenizer",title:"Tokenizer"}],title:"Tokenizer"};function ns(I){return Ja(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class ds extends Ha{constructor(p){super();Ma(this,p,ns,es,Wa,{})}}export{ds as default,ts as metadata};
