import{S as me,i as $e,s as ke,e as l,k as y,w as E,t as k,M as ge,c,d as a,m as x,a as h,x as D,h as g,b as m,F as n,g as G,y as I,q as P,o as S,B as F,v as we,L as ve}from"../../chunks/vendor-0d3f0756.js";import{D as se}from"../../chunks/Docstring-2d4c5ec2.js";import{I as ue}from"../../chunks/IconCopyLink-9193371d.js";import{T as _e,M as de}from"../../chunks/TokenizersLanguageContent-ca787841.js";function Te(v){let t,r,e,o,d,p,$,w,f,_,T,A,s,u,b,K,O,z,N,W,V,X,H,Z,ee,q,C,te,L,oe,j,ne,re,B;return o=new ue({}),_=new se({props:{name:"class tokenizers.AddedToken",anchor:"tokenizers.AddedToken",parameters:[{name:"content",val:""},{name:"single_word",val:" = False"},{name:"lstrip",val:" = False"},{name:"rstrip",val:" = False"},{name:"normalized",val:" = True"}],parametersDescription:[{anchor:"tokenizers.AddedToken.content",description:"<strong>content</strong> (<code>str</code>) &#x2014; The content of the token",name:"content"},{anchor:"tokenizers.AddedToken.single_word",description:`<strong>single_word</strong> (<code>bool</code>, defaults to <code>False</code>) &#x2014;
Defines whether this token should only match single words. If <code>True</code>, this
token will never match inside of a word. For example the token <code>ing</code> would match
on <code>tokenizing</code> if this option is <code>False</code>, but not if it is <code>True</code>.
The notion of &#x201D;<em>inside of a word</em>&#x201D; is defined by the word boundaries pattern in
regular expressions (ie. the token should start and end with word boundaries).`,name:"single_word"},{anchor:"tokenizers.AddedToken.lstrip",description:`<strong>lstrip</strong> (<code>bool</code>, defaults to <code>False</code>) &#x2014;
Defines whether this token should strip all potential whitespaces on its left side.
If <code>True</code>, this token will greedily match any whitespace on its left. For
example if we try to match the token <code>[MASK]</code> with <code>lstrip=True</code>, in the text
<code>&quot;I saw a [MASK]&quot;</code>, we would match on <code>&quot; [MASK]&quot;</code>. (Note the space on the left).`,name:"lstrip"},{anchor:"tokenizers.AddedToken.rstrip",description:`<strong>rstrip</strong> (<code>bool</code>, defaults to <code>False</code>) &#x2014;
Defines whether this token should strip all potential whitespaces on its right
side. If <code>True</code>, this token will greedily match any whitespace on its right.
It works just like <code>lstrip</code> but on the right.`,name:"rstrip"},{anchor:"tokenizers.AddedToken.normalized",description:`<strong>normalized</strong> (<code>bool</code>, defaults to <code>True</code> with  &#x2014;meth:<em>~tokenizers.Tokenizer.add_tokens</em> and <code>False</code> with <code>add_special_tokens()</code>):
Defines whether this token should match against the normalized version of the input
text. For example, with the added token <code>&quot;yesterday&quot;</code>, and a normalizer in charge of
lowercasing the text, the token could be extract from the input <code>&quot;I saw a lion Yesterday&quot;</code>.`,name:"normalized"}]}}),N=new se({props:{name:"content",anchor:"tokenizers.AddedToken.content",parameters:[],isGetSetDescriptor:!0}}),C=new se({props:{name:"lstrip",anchor:"tokenizers.AddedToken.lstrip",parameters:[],isGetSetDescriptor:!0}}),{c(){t=l("h2"),r=l("a"),e=l("span"),E(o.$$.fragment),d=y(),p=l("span"),$=k("AddedToken"),w=y(),f=l("div"),E(_.$$.fragment),T=y(),A=l("p"),s=k("Represents a token that can be be added to a "),u=l("a"),b=k("Tokenizer"),K=k(`.
It can have special options that defines the way it should behave.`),O=y(),z=l("div"),E(N.$$.fragment),W=y(),V=l("p"),X=k("Get the content of this "),H=l("code"),Z=k("AddedToken"),ee=y(),q=l("div"),E(C.$$.fragment),te=y(),L=l("p"),oe=k("Get the value of the "),j=l("code"),ne=k("lstrip"),re=k(" option"),this.h()},l(i){t=c(i,"H2",{class:!0});var M=h(t);r=c(M,"A",{id:!0,class:!0,href:!0});var ie=h(r);e=c(ie,"SPAN",{});var le=h(e);D(o.$$.fragment,le),le.forEach(a),ie.forEach(a),d=x(M),p=c(M,"SPAN",{});var ce=h(p);$=g(ce,"AddedToken"),ce.forEach(a),M.forEach(a),w=x(i),f=c(i,"DIV",{class:!0});var R=h(f);D(_.$$.fragment,R),T=x(R),A=c(R,"P",{});var J=h(A);s=g(J,"Represents a token that can be be added to a "),u=c(J,"A",{href:!0});var he=h(u);b=g(he,"Tokenizer"),he.forEach(a),K=g(J,`.
It can have special options that defines the way it should behave.`),J.forEach(a),O=x(R),z=c(R,"DIV",{class:!0});var U=h(z);D(N.$$.fragment,U),W=x(U),V=c(U,"P",{});var ae=h(V);X=g(ae,"Get the content of this "),H=c(ae,"CODE",{});var pe=h(H);Z=g(pe,"AddedToken"),pe.forEach(a),ae.forEach(a),U.forEach(a),ee=x(R),q=c(R,"DIV",{class:!0});var Y=h(q);D(C.$$.fragment,Y),te=x(Y),L=c(Y,"P",{});var Q=h(L);oe=g(Q,"Get the value of the "),j=c(Q,"CODE",{});var fe=h(j);ne=g(fe,"lstrip"),fe.forEach(a),re=g(Q," option"),Q.forEach(a),Y.forEach(a),R.forEach(a),this.h()},h(){m(r,"id","tokenizers.AddedToken]][[tokenizers.AddedToken"),m(r,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(r,"href","#tokenizers.AddedToken]][[tokenizers.AddedToken"),m(t,"class","relative group"),m(u,"href","/docs/tokenizers/pr_1/en/api/tokenizer#tokenizers.Tokenizer"),m(z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),m(q,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),m(f,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(i,M){G(i,t,M),n(t,r),n(r,e),I(o,e,null),n(t,d),n(t,p),n(p,$),G(i,w,M),G(i,f,M),I(_,f,null),n(f,T),n(f,A),n(A,s),n(A,u),n(u,b),n(A,K),n(f,O),n(f,z),I(N,z,null),n(z,W),n(z,V),n(V,X),n(V,H),n(H,Z),n(f,ee),n(f,q),I(C,q,null),n(q,te),n(q,L),n(L,oe),n(L,j),n(j,ne),n(L,re),B=!0},p:ve,i(i){B||(P(o.$$.fragment,i),P(_.$$.fragment,i),P(N.$$.fragment,i),P(C.$$.fragment,i),B=!0)},o(i){S(o.$$.fragment,i),S(_.$$.fragment,i),S(N.$$.fragment,i),S(C.$$.fragment,i),B=!1},d(i){i&&a(t),F(o),i&&a(w),i&&a(f),F(_),F(N),F(C)}}}function Ae(v){let t,r;return t=new de({props:{$$slots:{default:[Te]},$$scope:{ctx:v}}}),{c(){E(t.$$.fragment)},l(e){D(t.$$.fragment,e)},m(e,o){I(t,e,o),r=!0},p(e,o){const d={};o&2&&(d.$$scope={dirty:o,ctx:e}),t.$set(d)},i(e){r||(P(t.$$.fragment,e),r=!0)},o(e){S(t.$$.fragment,e),r=!1},d(e){F(t,e)}}}function be(v){let t,r,e,o,d;return{c(){t=l("p"),r=k("The Rust API Reference is available directly on the "),e=l("a"),o=k("Docs.rs"),d=k(" website."),this.h()},l(p){t=c(p,"P",{});var $=h(t);r=g($,"The Rust API Reference is available directly on the "),e=c($,"A",{href:!0,rel:!0});var w=h(e);o=g(w,"Docs.rs"),w.forEach(a),d=g($," website."),$.forEach(a),this.h()},h(){m(e,"href","https://docs.rs/tokenizers/latest/tokenizers/"),m(e,"rel","nofollow")},m(p,$){G(p,t,$),n(t,r),n(t,e),n(e,o),n(t,d)},d(p){p&&a(t)}}}function ze(v){let t,r;return t=new de({props:{$$slots:{default:[be]},$$scope:{ctx:v}}}),{c(){E(t.$$.fragment)},l(e){D(t.$$.fragment,e)},m(e,o){I(t,e,o),r=!0},p(e,o){const d={};o&2&&(d.$$scope={dirty:o,ctx:e}),t.$set(d)},i(e){r||(P(t.$$.fragment,e),r=!0)},o(e){S(t.$$.fragment,e),r=!1},d(e){F(t,e)}}}function ye(v){let t,r;return{c(){t=l("p"),r=k("The node API has not been documented yet.")},l(e){t=c(e,"P",{});var o=h(t);r=g(o,"The node API has not been documented yet."),o.forEach(a)},m(e,o){G(e,t,o),n(t,r)},d(e){e&&a(t)}}}function xe(v){let t,r;return t=new de({props:{$$slots:{default:[ye]},$$scope:{ctx:v}}}),{c(){E(t.$$.fragment)},l(e){D(t.$$.fragment,e)},m(e,o){I(t,e,o),r=!0},p(e,o){const d={};o&2&&(d.$$scope={dirty:o,ctx:e}),t.$set(d)},i(e){r||(P(t.$$.fragment,e),r=!0)},o(e){S(t.$$.fragment,e),r=!1},d(e){F(t,e)}}}function Ee(v){let t,r,e,o,d,p,$,w,f,_,T,A;return p=new ue({}),T=new _e({props:{python:!0,rust:!0,node:!0,$$slots:{node:[xe],rust:[ze],python:[Ae]},$$scope:{ctx:v}}}),{c(){t=l("meta"),r=y(),e=l("h1"),o=l("a"),d=l("span"),E(p.$$.fragment),$=y(),w=l("span"),f=k("Added Tokens"),_=y(),E(T.$$.fragment),this.h()},l(s){const u=ge('[data-svelte="svelte-1phssyn"]',document.head);t=c(u,"META",{name:!0,content:!0}),u.forEach(a),r=x(s),e=c(s,"H1",{class:!0});var b=h(e);o=c(b,"A",{id:!0,class:!0,href:!0});var K=h(o);d=c(K,"SPAN",{});var O=h(d);D(p.$$.fragment,O),O.forEach(a),K.forEach(a),$=x(b),w=c(b,"SPAN",{});var z=h(w);f=g(z,"Added Tokens"),z.forEach(a),b.forEach(a),_=x(s),D(T.$$.fragment,s),this.h()},h(){m(t,"name","hf:doc:metadata"),m(t,"content",JSON.stringify(De)),m(o,"id","added-tokens"),m(o,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(o,"href","#added-tokens"),m(e,"class","relative group")},m(s,u){n(document.head,t),G(s,r,u),G(s,e,u),n(e,o),n(o,d),I(p,d,null),n(e,$),n(e,w),n(w,f),G(s,_,u),I(T,s,u),A=!0},p(s,[u]){const b={};u&2&&(b.$$scope={dirty:u,ctx:s}),T.$set(b)},i(s){A||(P(p.$$.fragment,s),P(T.$$.fragment,s),A=!0)},o(s){S(p.$$.fragment,s),S(T.$$.fragment,s),A=!1},d(s){a(t),s&&a(r),s&&a(e),F(p),s&&a(_),F(T,s)}}}const De={local:"added-tokens",sections:[{local:"tokenizers.AddedToken]][[tokenizers.AddedToken",title:"AddedToken"}],title:"Added Tokens"};function Ie(v){return we(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Me extends me{constructor(t){super();$e(this,t,Ie,Ee,ke,{})}}export{Me as default,De as metadata};
