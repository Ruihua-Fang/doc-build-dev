import{S as ve,i as ke,s as we,e as l,k as v,w as T,t as w,M as ze,c as p,d as n,m as k,a as f,x as E,h as z,b as h,F as s,g as x,y as b,q as C,o as B,B as M,v as xe,L as De}from"../../chunks/vendor-0d3f0756.js";import{D as K}from"../../chunks/Docstring-f752f2c3.js";import{I as Pe}from"../../chunks/IconCopyLink-9193371d.js";import{T as ye,M as pe}from"../../chunks/TokenizersLanguageContent-ca787841.js";function Te(D){let t,a,e,o,d,m,i,u,R,y,P,S,c,$,_,U,F,I,oe,ne,Q,A,V,ae,O,se,X,L,q,ce,j,de,Y,W,N,ie,G,le,Z;return a=new K({props:{name:"class tokenizers.decoders.BPEDecoder",anchor:"tokenizers.decoders.BPEDecoder",parameters:[{name:"suffix",val:" = '</w>'"}],parametersDescription:[{anchor:"tokenizers.decoders.BPEDecoder.suffix",description:`<strong>suffix</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&lt;/w&gt;</code>) &#x2014;
The suffix that was used to caracterize an end-of-word. This suffix will
be replaced by whitespaces during the decoding`,name:"suffix"}]}}),u=new K({props:{name:"class tokenizers.decoders.ByteLevel",anchor:"tokenizers.decoders.ByteLevel",parameters:[]}}),V=new K({props:{name:"class tokenizers.decoders.CTC",anchor:"tokenizers.decoders.CTC",parameters:[{name:"pad_token",val:" = '<pad>'"},{name:"word_delimiter_token",val:" = '|'"},{name:"cleanup",val:" = True"}],parametersDescription:[{anchor:"tokenizers.decoders.CTC.pad_token",description:`<strong>pad_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&lt;pad&gt;</code>) &#x2014;
The pad token used by CTC to delimit a new token.`,name:"pad_token"},{anchor:"tokenizers.decoders.CTC.word_delimiter_token",description:`<strong>word_delimiter_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>|</code>) &#x2014;
The word delimiter token. It will be replaced by a <space></space>`,name:"word_delimiter_token"},{anchor:"tokenizers.decoders.CTC.cleanup",description:`<strong>cleanup</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to cleanup some tokenization artifacts.
Mainly spaces before punctuation, and some abbreviated english forms.`,name:"cleanup"}]}}),q=new K({props:{name:"class tokenizers.decoders.Metaspace",anchor:"tokenizers.decoders.Metaspace",parameters:"",parametersDescription:[{anchor:"tokenizers.decoders.Metaspace.replacement",description:`<strong>replacement</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&#x2581;</code>) &#x2014;
The replacement character. Must be exactly one character. By default we
use the <em>&#x2581;</em> (U+2581) meta symbol (Same as in SentencePiece).`,name:"replacement"},{anchor:"tokenizers.decoders.Metaspace.add_prefix_space",description:`<strong>add_prefix_space</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to add a space to the first word if there isn&#x2019;t already one. This
lets us treat <em>hello</em> exactly like <em>say hello</em>.`,name:"add_prefix_space"}]}}),N=new K({props:{name:"class tokenizers.decoders.WordPiece",anchor:"tokenizers.decoders.WordPiece",parameters:[{name:"prefix",val:" = '##'"},{name:"cleanup",val:" = True"}],parametersDescription:[{anchor:"tokenizers.decoders.WordPiece.prefix",description:`<strong>prefix</strong> (<code>str</code>, <em>optional</em>, defaults to <code>##</code>) &#x2014;
The prefix to use for subwords that are not a beginning-of-word`,name:"prefix"},{anchor:"tokenizers.decoders.WordPiece.cleanup",description:`<strong>cleanup</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to cleanup some tokenization artifacts. Mainly spaces before punctuation,
and some abbreviated english forms.`,name:"cleanup"}]}}),{c(){t=l("div"),T(a.$$.fragment),e=v(),o=l("p"),d=w("BPEDecoder Decoder"),m=v(),i=l("div"),T(u.$$.fragment),R=v(),y=l("p"),P=w("ByteLevel Decoder"),S=v(),c=l("p"),$=w("This decoder is to be used in tandem with the "),_=l("a"),U=w("ByteLevel"),F=v(),I=l("a"),oe=w("PreTokenizer"),ne=w("."),Q=v(),A=l("div"),T(V.$$.fragment),ae=v(),O=l("p"),se=w("CTC Decoder"),X=v(),L=l("div"),T(q.$$.fragment),ce=v(),j=l("p"),de=w("Metaspace Decoder"),Y=v(),W=l("div"),T(N.$$.fragment),ie=v(),G=l("p"),le=w("WordPiece Decoder"),this.h()},l(r){t=p(r,"DIV",{class:!0});var g=f(t);E(a.$$.fragment,g),e=k(g),o=p(g,"P",{});var fe=f(o);d=z(fe,"BPEDecoder Decoder"),fe.forEach(n),g.forEach(n),m=k(r),i=p(r,"DIV",{class:!0});var H=f(i);E(u.$$.fragment,H),R=k(H),y=p(H,"P",{});var me=f(y);P=z(me,"ByteLevel Decoder"),me.forEach(n),S=k(H),c=p(H,"P",{});var J=f(c);$=z(J,"This decoder is to be used in tandem with the "),_=p(J,"A",{href:!0});var ue=f(_);U=z(ue,"ByteLevel"),ue.forEach(n),F=k(J),I=p(J,"A",{href:!0});var he=f(I);oe=z(he,"PreTokenizer"),he.forEach(n),ne=z(J,"."),J.forEach(n),H.forEach(n),Q=k(r),A=p(r,"DIV",{class:!0});var ee=f(A);E(V.$$.fragment,ee),ae=k(ee),O=p(ee,"P",{});var $e=f(O);se=z($e,"CTC Decoder"),$e.forEach(n),ee.forEach(n),X=k(r),L=p(r,"DIV",{class:!0});var te=f(L);E(q.$$.fragment,te),ce=k(te),j=p(te,"P",{});var _e=f(j);de=z(_e,"Metaspace Decoder"),_e.forEach(n),te.forEach(n),Y=k(r),W=p(r,"DIV",{class:!0});var re=f(W);E(N.$$.fragment,re),ie=k(re),G=p(re,"P",{});var ge=f(G);le=z(ge,"WordPiece Decoder"),ge.forEach(n),re.forEach(n),this.h()},h(){h(t,"class","docstring"),h(_,"href","/docs/tokenizers/pr_1/en/api/pre-tokenizers#tokenizers.pre_tokenizers.ByteLevel"),h(I,"href","/docs/tokenizers/pr_1/en/api/pre-tokenizers#tokenizers.pre_tokenizers.PreTokenizer"),h(i,"class","docstring"),h(A,"class","docstring"),h(L,"class","docstring"),h(W,"class","docstring")},m(r,g){x(r,t,g),b(a,t,null),s(t,e),s(t,o),s(o,d),x(r,m,g),x(r,i,g),b(u,i,null),s(i,R),s(i,y),s(y,P),s(i,S),s(i,c),s(c,$),s(c,_),s(_,U),s(c,F),s(c,I),s(I,oe),s(c,ne),x(r,Q,g),x(r,A,g),b(V,A,null),s(A,ae),s(A,O),s(O,se),x(r,X,g),x(r,L,g),b(q,L,null),s(L,ce),s(L,j),s(j,de),x(r,Y,g),x(r,W,g),b(N,W,null),s(W,ie),s(W,G),s(G,le),Z=!0},p:De,i(r){Z||(C(a.$$.fragment,r),C(u.$$.fragment,r),C(V.$$.fragment,r),C(q.$$.fragment,r),C(N.$$.fragment,r),Z=!0)},o(r){B(a.$$.fragment,r),B(u.$$.fragment,r),B(V.$$.fragment,r),B(q.$$.fragment,r),B(N.$$.fragment,r),Z=!1},d(r){r&&n(t),M(a),r&&n(m),r&&n(i),M(u),r&&n(Q),r&&n(A),M(V),r&&n(X),r&&n(L),M(q),r&&n(Y),r&&n(W),M(N)}}}function Ee(D){let t,a;return t=new pe({props:{$$slots:{default:[Te]},$$scope:{ctx:D}}}),{c(){T(t.$$.fragment)},l(e){E(t.$$.fragment,e)},m(e,o){b(t,e,o),a=!0},p(e,o){const d={};o&2&&(d.$$scope={dirty:o,ctx:e}),t.$set(d)},i(e){a||(C(t.$$.fragment,e),a=!0)},o(e){B(t.$$.fragment,e),a=!1},d(e){M(t,e)}}}function be(D){let t,a,e,o,d;return{c(){t=l("p"),a=w("The Rust API Reference is available directly on the "),e=l("a"),o=w("Docs.rs"),d=w(" website."),this.h()},l(m){t=p(m,"P",{});var i=f(t);a=z(i,"The Rust API Reference is available directly on the "),e=p(i,"A",{href:!0,rel:!0});var u=f(e);o=z(u,"Docs.rs"),u.forEach(n),d=z(i," website."),i.forEach(n),this.h()},h(){h(e,"href","https://docs.rs/tokenizers/latest/tokenizers/"),h(e,"rel","nofollow")},m(m,i){x(m,t,i),s(t,a),s(t,e),s(e,o),s(t,d)},d(m){m&&n(t)}}}function Ce(D){let t,a;return t=new pe({props:{$$slots:{default:[be]},$$scope:{ctx:D}}}),{c(){T(t.$$.fragment)},l(e){E(t.$$.fragment,e)},m(e,o){b(t,e,o),a=!0},p(e,o){const d={};o&2&&(d.$$scope={dirty:o,ctx:e}),t.$set(d)},i(e){a||(C(t.$$.fragment,e),a=!0)},o(e){B(t.$$.fragment,e),a=!1},d(e){M(t,e)}}}function Be(D){let t,a;return{c(){t=l("p"),a=w("The node API has not been documented yet.")},l(e){t=p(e,"P",{});var o=f(t);a=z(o,"The node API has not been documented yet."),o.forEach(n)},m(e,o){x(e,t,o),s(t,a)},d(e){e&&n(t)}}}function Me(D){let t,a;return t=new pe({props:{$$slots:{default:[Be]},$$scope:{ctx:D}}}),{c(){T(t.$$.fragment)},l(e){E(t.$$.fragment,e)},m(e,o){b(t,e,o),a=!0},p(e,o){const d={};o&2&&(d.$$scope={dirty:o,ctx:e}),t.$set(d)},i(e){a||(C(t.$$.fragment,e),a=!0)},o(e){B(t.$$.fragment,e),a=!1},d(e){M(t,e)}}}function Ie(D){let t,a,e,o,d,m,i,u,R,y,P,S;return m=new Pe({}),P=new ye({props:{python:!0,rust:!0,node:!0,$$slots:{node:[Me],rust:[Ce],python:[Ee]},$$scope:{ctx:D}}}),{c(){t=l("meta"),a=v(),e=l("h1"),o=l("a"),d=l("span"),T(m.$$.fragment),i=v(),u=l("span"),R=w("Decoders"),y=v(),T(P.$$.fragment),this.h()},l(c){const $=ze('[data-svelte="svelte-1phssyn"]',document.head);t=p($,"META",{name:!0,content:!0}),$.forEach(n),a=k(c),e=p(c,"H1",{class:!0});var _=f(e);o=p(_,"A",{id:!0,class:!0,href:!0});var U=f(o);d=p(U,"SPAN",{});var F=f(d);E(m.$$.fragment,F),F.forEach(n),U.forEach(n),i=k(_),u=p(_,"SPAN",{});var I=f(u);R=z(I,"Decoders"),I.forEach(n),_.forEach(n),y=k(c),E(P.$$.fragment,c),this.h()},h(){h(t,"name","hf:doc:metadata"),h(t,"content",JSON.stringify(Ae)),h(o,"id","tokenizers.decoders.BPEDecoder"),h(o,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(o,"href","#tokenizers.decoders.BPEDecoder"),h(e,"class","relative group")},m(c,$){s(document.head,t),x(c,a,$),x(c,e,$),s(e,o),s(o,d),b(m,d,null),s(e,i),s(e,u),s(u,R),x(c,y,$),b(P,c,$),S=!0},p(c,[$]){const _={};$&2&&(_.$$scope={dirty:$,ctx:c}),P.$set(_)},i(c){S||(C(m.$$.fragment,c),C(P.$$.fragment,c),S=!0)},o(c){B(m.$$.fragment,c),B(P.$$.fragment,c),S=!1},d(c){n(t),c&&n(a),c&&n(e),M(m),c&&n(y),M(P,c)}}}const Ae={local:"tokenizers.decoders.BPEDecoder",title:"Decoders"};function Le(D){return xe(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class qe extends ve{constructor(t){super();ke(this,t,Le,Ie,we,{})}}export{qe as default,Ae as metadata};
