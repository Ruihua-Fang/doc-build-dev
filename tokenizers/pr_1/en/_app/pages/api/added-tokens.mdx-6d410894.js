import{S as L,i as N,s as C,e as p,k as F,w as y,t as v,M as K,c as m,d as i,m as P,a as g,x as b,h as T,b as k,F as d,g as A,y as x,q as I,o as D,B as E,v as j,L as B}from"../../chunks/vendor-0d3f0756.js";import{D as H}from"../../chunks/Docstring-f752f2c3.js";import{I as J}from"../../chunks/IconCopyLink-9193371d.js";import{T as O,M as q}from"../../chunks/TokenizersLanguageContent-ca787841.js";function U(u){let t,n,e,o,a,s,l,f,_;return n=new H({props:{name:"class tokenizers.AddedToken",anchor:"tokenizers.AddedToken",parameters:[{name:"content",val:""},{name:"single_word",val:" = False"},{name:"lstrip",val:" = False"},{name:"rstrip",val:" = False"},{name:"normalized",val:" = True"}],parametersDescription:[{anchor:"tokenizers.AddedToken.content",description:"<strong>content</strong> (<code>str</code>) &#x2014; The content of the token",name:"content"},{anchor:"tokenizers.AddedToken.single_word",description:`<strong>single_word</strong> (<code>bool</code>, defaults to <code>False</code>) &#x2014;
Defines whether this token should only match single words. If <code>True</code>, this
token will never match inside of a word. For example the token <code>ing</code> would match
on <code>tokenizing</code> if this option is <code>False</code>, but not if it is <code>True</code>.
The notion of &#x201D;<em>inside of a word</em>&#x201D; is defined by the word boundaries pattern in
regular expressions (ie. the token should start and end with word boundaries).`,name:"single_word"},{anchor:"tokenizers.AddedToken.lstrip",description:`<strong>lstrip</strong> (<code>bool</code>, defaults to <code>False</code>) &#x2014;
Defines whether this token should strip all potential whitespaces on its left side.
If <code>True</code>, this token will greedily match any whitespace on its left. For
example if we try to match the token <code>[MASK]</code> with <code>lstrip=True</code>, in the text
<code>&quot;I saw a [MASK]&quot;</code>, we would match on <code>&quot; [MASK]&quot;</code>. (Note the space on the left).`,name:"lstrip"},{anchor:"tokenizers.AddedToken.rstrip",description:`<strong>rstrip</strong> (<code>bool</code>, defaults to <code>False</code>) &#x2014;
Defines whether this token should strip all potential whitespaces on its right
side. If <code>True</code>, this token will greedily match any whitespace on its right.
It works just like <code>lstrip</code> but on the right.`,name:"rstrip"},{anchor:"tokenizers.AddedToken.normalized",description:`<strong>normalized</strong> (<code>bool</code>, defaults to <code>True</code> with  &#x2014;meth:<em>~tokenizers.Tokenizer.add_tokens</em> and <code>False</code> with <code>add_special_tokens()</code>):
Defines whether this token should match against the normalized version of the input
text. For example, with the added token <code>&quot;yesterday&quot;</code>, and a normalizer in charge of
lowercasing the text, the token could be extract from the input <code>&quot;I saw a lion Yesterday&quot;</code>.`,name:"normalized"}]}}),{c(){t=p("div"),y(n.$$.fragment),e=F(),o=p("p"),a=v("Represents a token that can be be added to a "),s=p("a"),l=v("Tokenizer"),f=v(`.
It can have special options that defines the way it should behave.`),this.h()},l(h){t=m(h,"DIV",{class:!0});var c=g(t);b(n.$$.fragment,c),e=P(c),o=m(c,"P",{});var w=g(o);a=T(w,"Represents a token that can be be added to a "),s=m(w,"A",{href:!0});var r=g(s);l=T(r,"Tokenizer"),r.forEach(i),f=T(w,`.
It can have special options that defines the way it should behave.`),w.forEach(i),c.forEach(i),this.h()},h(){k(s,"href","/docs/tokenizers/pr_1/en/api/tokenizer#tokenizers.Tokenizer"),k(t,"class","docstring")},m(h,c){A(h,t,c),x(n,t,null),d(t,e),d(t,o),d(o,a),d(o,s),d(s,l),d(o,f),_=!0},p:B,i(h){_||(I(n.$$.fragment,h),_=!0)},o(h){D(n.$$.fragment,h),_=!1},d(h){h&&i(t),E(n)}}}function V(u){let t,n;return t=new q({props:{$$slots:{default:[U]},$$scope:{ctx:u}}}),{c(){y(t.$$.fragment)},l(e){b(t.$$.fragment,e)},m(e,o){x(t,e,o),n=!0},p(e,o){const a={};o&2&&(a.$$scope={dirty:o,ctx:e}),t.$set(a)},i(e){n||(I(t.$$.fragment,e),n=!0)},o(e){D(t.$$.fragment,e),n=!1},d(e){E(t,e)}}}function Y(u){let t,n,e,o,a;return{c(){t=p("p"),n=v("The Rust API Reference is available directly on the "),e=p("a"),o=v("Docs.rs"),a=v(" website."),this.h()},l(s){t=m(s,"P",{});var l=g(t);n=T(l,"The Rust API Reference is available directly on the "),e=m(l,"A",{href:!0,rel:!0});var f=g(e);o=T(f,"Docs.rs"),f.forEach(i),a=T(l," website."),l.forEach(i),this.h()},h(){k(e,"href","https://docs.rs/tokenizers/latest/tokenizers/"),k(e,"rel","nofollow")},m(s,l){A(s,t,l),d(t,n),d(t,e),d(e,o),d(t,a)},d(s){s&&i(t)}}}function G(u){let t,n;return t=new q({props:{$$slots:{default:[Y]},$$scope:{ctx:u}}}),{c(){y(t.$$.fragment)},l(e){b(t.$$.fragment,e)},m(e,o){x(t,e,o),n=!0},p(e,o){const a={};o&2&&(a.$$scope={dirty:o,ctx:e}),t.$set(a)},i(e){n||(I(t.$$.fragment,e),n=!0)},o(e){D(t.$$.fragment,e),n=!1},d(e){E(t,e)}}}function Q(u){let t,n;return{c(){t=p("p"),n=v("The node API has not been documented yet.")},l(e){t=m(e,"P",{});var o=g(t);n=T(o,"The node API has not been documented yet."),o.forEach(i)},m(e,o){A(e,t,o),d(t,n)},d(e){e&&i(t)}}}function W(u){let t,n;return t=new q({props:{$$slots:{default:[Q]},$$scope:{ctx:u}}}),{c(){y(t.$$.fragment)},l(e){b(t.$$.fragment,e)},m(e,o){x(t,e,o),n=!0},p(e,o){const a={};o&2&&(a.$$scope={dirty:o,ctx:e}),t.$set(a)},i(e){n||(I(t.$$.fragment,e),n=!0)},o(e){D(t.$$.fragment,e),n=!1},d(e){E(t,e)}}}function X(u){let t,n,e,o,a,s,l,f,_,h,c,w;return s=new J({}),c=new O({props:{python:!0,rust:!0,node:!0,$$slots:{node:[W],rust:[G],python:[V]},$$scope:{ctx:u}}}),{c(){t=p("meta"),n=F(),e=p("h1"),o=p("a"),a=p("span"),y(s.$$.fragment),l=F(),f=p("span"),_=v("Added Tokens"),h=F(),y(c.$$.fragment),this.h()},l(r){const $=K('[data-svelte="svelte-1phssyn"]',document.head);t=m($,"META",{name:!0,content:!0}),$.forEach(i),n=P(r),e=m(r,"H1",{class:!0});var z=g(e);o=m(z,"A",{id:!0,class:!0,href:!0});var S=g(o);a=m(S,"SPAN",{});var M=g(a);b(s.$$.fragment,M),M.forEach(i),S.forEach(i),l=P(z),f=m(z,"SPAN",{});var R=g(f);_=T(R,"Added Tokens"),R.forEach(i),z.forEach(i),h=P(r),b(c.$$.fragment,r),this.h()},h(){k(t,"name","hf:doc:metadata"),k(t,"content",JSON.stringify(Z)),k(o,"id","tokenizers.AddedToken"),k(o,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),k(o,"href","#tokenizers.AddedToken"),k(e,"class","relative group")},m(r,$){d(document.head,t),A(r,n,$),A(r,e,$),d(e,o),d(o,a),x(s,a,null),d(e,l),d(e,f),d(f,_),A(r,h,$),x(c,r,$),w=!0},p(r,[$]){const z={};$&2&&(z.$$scope={dirty:$,ctx:r}),c.$set(z)},i(r){w||(I(s.$$.fragment,r),I(c.$$.fragment,r),w=!0)},o(r){D(s.$$.fragment,r),D(c.$$.fragment,r),w=!1},d(r){i(t),r&&i(n),r&&i(e),E(s),r&&i(h),E(c,r)}}}const Z={local:"tokenizers.AddedToken",title:"Added Tokens"};function ee(u){return j(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class ae extends L{constructor(t){super();N(this,t,ee,X,C,{})}}export{ae as default,Z as metadata};
