import{S as xa,i as Aa,s as Da,e as o,k as d,w as k,t as s,M as Ia,c as r,d as n,m as c,a,x as _,h as i,b as u,F as e,g as ct,y as z,q as v,o as $,B as T,v as Pa,L as ja}from"../../chunks/vendor-0d3f0756.js";import{D as b}from"../../chunks/Docstring-f752f2c3.js";import{C as qa}from"../../chunks/CodeBlock-7b0cb15c.js";import{I as La}from"../../chunks/IconCopyLink-9193371d.js";import{T as Va,M as Cr}from"../../chunks/TokenizersLanguageContent-8807aac4.js";function Na(x){let t,f,p,g,w,E,q,A,C,M,D,W,m,y,V,R,Be,Ue,pt,kn,_n,lt,zn,vn,O,ue,$n,ht,Tn,bn,gt,wn,En,S,fe,yn,ut,qn,xn,ft,An,Dn,J,me,In,mt,Pn,jn,G,ke,Ln,kt,Vn,Nn,B,_e,Cn,_t,On,Sn,I,ze,Fn,zt,Hn,Mn,vt,Wn,Rn,ve,Jn,P,$e,Gn,$t,Bn,Un,Tt,Yn,Kn,Te,Qn,U,be,Xn,we,Zn,Ye,eo,to,no,Y,Ee,oo,ye,ro,Ke,ao,so,io,K,qe,co,xe,po,Qe,lo,ho,go,Q,Ae,uo,De,fo,Xe,mo,ko,_o,X,Ie,zo,bt,vo,$o,Z,Pe,To,wt,bo,wo,ee,je,Eo,Et,yo,qo,te,Le,xo,yt,Ao,Do,ne,Ve,Io,qt,Po,jo,oe,Ne,Lo,xt,Vo,No,j,Ce,Co,At,Oo,So,Dt,Fo,Ho,H,Oe,Mo,It,Wo,Ro,Jo,Ze,Go,Pt,Bo,Uo,Se,Yo,jt,Ko,Qo,Xo,re,Fe,Zo,He,er,et,tr,nr,or,ae,Me,rr,We,ar,tt,sr,ir,dr,se,Re,cr,Lt,pr,lr,F,Je,hr,Vt,gr,ur,nt,fr,Nt,mr,kr,L,Ge,_r,Ct,zr,vr,Ot,$r,Tr,N,ot,br,St,wr,Er,ie,yr,Ft,qr,xr,Ht,Ar,Dr,Mt,Ir,Pr,Wt,jr,Jt;return f=new b({props:{name:"class tokenizers.Tokenizer",anchor:"tokenizers.Tokenizer",parameters:[{name:"model",val:""}],parametersDescription:[{anchor:"tokenizers.Tokenizer.model",description:`<strong>model</strong> (<a href="/docs/tokenizers/pr_1/en/api/models#tokenizers.models.Model">Model</a>) &#x2014;
The core algorithm that this <code>Tokenizer</code> should be using.`,name:"model"}]}}),y=new b({props:{name:"add_special_tokens",anchor:"tokenizers.Tokenizer.add_special_tokens",parameters:[{name:"tokens",val:""}],parametersDescription:[{anchor:"tokenizers.Tokenizer.add_special_tokens.tokens",description:`<strong>tokens</strong> (A <code>List</code> of <a href="/docs/tokenizers/pr_1/en/api/added-tokens#tokenizers.AddedToken">AddedToken</a> or <code>str</code>) &#x2014;
The list of special tokens we want to add to the vocabulary. Each token can either
be a string or an instance of <a href="/docs/tokenizers/pr_1/en/api/added-tokens#tokenizers.AddedToken">AddedToken</a> for more
customization.`,name:"tokens"}],returnDescription:`
<p>The number of tokens that were created in the vocabulary</p>
`,returnType:`
<p><code>int</code></p>
`}}),ue=new b({props:{name:"add_tokens",anchor:"tokenizers.Tokenizer.add_tokens",parameters:[{name:"tokens",val:""}],parametersDescription:[{anchor:"tokenizers.Tokenizer.add_tokens.tokens",description:`<strong>tokens</strong> (A <code>List</code> of <a href="/docs/tokenizers/pr_1/en/api/added-tokens#tokenizers.AddedToken">AddedToken</a> or <code>str</code>) &#x2014;
The list of tokens we want to add to the vocabulary. Each token can be either a
string or an instance of <a href="/docs/tokenizers/pr_1/en/api/added-tokens#tokenizers.AddedToken">AddedToken</a> for more customization.`,name:"tokens"}],returnDescription:`
<p>The number of tokens that were created in the vocabulary</p>
`,returnType:`
<p><code>int</code></p>
`}}),fe=new b({props:{name:"decode",anchor:"tokenizers.Tokenizer.decode",parameters:[{name:"ids",val:""},{name:"skip_special_tokens",val:" = True"}],parametersDescription:[{anchor:"tokenizers.Tokenizer.decode.ids",description:`<strong>ids</strong> (A <code>List/Tuple</code> of <code>int</code>) &#x2014;
The list of ids that we want to decode`,name:"ids"},{anchor:"tokenizers.Tokenizer.decode.skip_special_tokens",description:`<strong>skip_special_tokens</strong> (<code>bool</code>, defaults to <code>True</code>) &#x2014;
Whether the special tokens should be removed from the decoded string`,name:"skip_special_tokens"}],returnDescription:`
<p>The decoded string</p>
`,returnType:`
<p><code>str</code></p>
`}}),me=new b({props:{name:"decode_batch",anchor:"tokenizers.Tokenizer.decode_batch",parameters:[{name:"sequences",val:""},{name:"skip_special_tokens",val:" = True"}],parametersDescription:[{anchor:"tokenizers.Tokenizer.decode_batch.sequences",description:`<strong>sequences</strong> (<code>List</code> of <code>List[int]</code>) &#x2014;
The batch of sequences we want to decode`,name:"sequences"},{anchor:"tokenizers.Tokenizer.decode_batch.skip_special_tokens",description:`<strong>skip_special_tokens</strong> (<code>bool</code>, defaults to <code>True</code>) &#x2014;
Whether the special tokens should be removed from the decoded strings`,name:"skip_special_tokens"}],returnDescription:`
<p>A list of decoded strings</p>
`,returnType:`
<p><code>List[str]</code></p>
`}}),ke=new b({props:{name:"enable_padding",anchor:"tokenizers.Tokenizer.enable_padding",parameters:[{name:"direction",val:" = 'right'"},{name:"pad_id",val:" = 0"},{name:"pad_type_id",val:" = 0"},{name:"pad_token",val:" = '[PAD]'"},{name:"length",val:" = None"},{name:"pad_to_multiple_of",val:" = None"}],parametersDescription:[{anchor:"tokenizers.Tokenizer.enable_padding.direction",description:`<strong>direction</strong> (<code>str</code>, <em>optional</em>, defaults to <code>right</code>) &#x2014;
The direction in which to pad. Can be either <code>right</code> or <code>left</code>`,name:"direction"},{anchor:"tokenizers.Tokenizer.enable_padding.pad_to_multiple_of",description:`<strong>pad_to_multiple_of</strong> (<code>int</code>, <em>optional</em>) &#x2014;
If specified, the padding length should always snap to the next multiple of the
given value. For example if we were going to pad witha length of 250 but
<code>pad_to_multiple_of=8</code> then we will pad to 256.`,name:"pad_to_multiple_of"},{anchor:"tokenizers.Tokenizer.enable_padding.pad_id",description:`<strong>pad_id</strong> (<code>int</code>, defaults to 0) &#x2014;
The id to be used when padding`,name:"pad_id"},{anchor:"tokenizers.Tokenizer.enable_padding.pad_type_id",description:`<strong>pad_type_id</strong> (<code>int</code>, defaults to 0) &#x2014;
The type id to be used when padding`,name:"pad_type_id"},{anchor:"tokenizers.Tokenizer.enable_padding.pad_token",description:`<strong>pad_token</strong> (<code>str</code>, defaults to <code>[PAD]</code>) &#x2014;
The pad token to be used when padding`,name:"pad_token"},{anchor:"tokenizers.Tokenizer.enable_padding.length",description:`<strong>length</strong> (<code>int</code>, <em>optional</em>) &#x2014;
If specified, the length at which to pad. If not specified we pad using the size of
the longest sequence in a batch.`,name:"length"}]}}),_e=new b({props:{name:"enable_truncation",anchor:"tokenizers.Tokenizer.enable_truncation",parameters:[{name:"max_length",val:""},{name:"stride",val:" = 0"},{name:"strategy",val:" = 'longest_first'"},{name:"direction",val:" = 'right'"}],parametersDescription:[{anchor:"tokenizers.Tokenizer.enable_truncation.max_length",description:`<strong>max_length</strong> (<code>int</code>) &#x2014;
The max length at which to truncate`,name:"max_length"},{anchor:"tokenizers.Tokenizer.enable_truncation.stride",description:`<strong>stride</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The length of the previous first sequence to be included in the overflowing
sequence`,name:"stride"},{anchor:"tokenizers.Tokenizer.enable_truncation.strategy",description:`<strong>strategy</strong> (<code>str</code>, <em>optional</em>, defaults to <code>longest_first</code>) &#x2014;
The strategy used to truncation. Can be one of <code>longest_first</code>, <code>only_first</code> or
<code>only_second</code>.`,name:"strategy"},{anchor:"tokenizers.Tokenizer.enable_truncation.direction",description:`<strong>direction</strong> (<code>str</code>, defaults to <code>right</code>) &#x2014;
Truncate direction`,name:"direction"}]}}),ze=new b({props:{name:"encode",anchor:"tokenizers.Tokenizer.encode",parameters:[{name:"sequence",val:""},{name:"pair",val:" = None"},{name:"is_pretokenized",val:" = False"},{name:"add_special_tokens",val:" = True"}],parametersDescription:[{anchor:"tokenizers.Tokenizer.encode.sequence",description:`<strong>sequence</strong> (<code>~tokenizers.InputSequence</code>) &#x2014;
The main input sequence we want to encode. This sequence can be either raw
text or pre-tokenized, according to the <code>is_pretokenized</code> argument:</p>
<ul>
<li>If <code>is_pretokenized=False</code>: <code>TextInputSequence</code></li>
<li>If <code>is_pretokenized=True</code>: <code>PreTokenizedInputSequence()</code></li>
</ul>`,name:"sequence"},{anchor:"tokenizers.Tokenizer.encode.pair",description:`<strong>pair</strong> (<code>~tokenizers.InputSequence</code>, <em>optional</em>) &#x2014;
An optional input sequence. The expected format is the same that for <code>sequence</code>.`,name:"pair"},{anchor:"tokenizers.Tokenizer.encode.is_pretokenized",description:`<strong>is_pretokenized</strong> (<code>bool</code>, defaults to <code>False</code>) &#x2014;
Whether the input is already pre-tokenized`,name:"is_pretokenized"},{anchor:"tokenizers.Tokenizer.encode.add_special_tokens",description:`<strong>add_special_tokens</strong> (<code>bool</code>, defaults to <code>True</code>) &#x2014;
Whether to add the special tokens`,name:"add_special_tokens"}],returnDescription:`
<p>The encoded result</p>
`,returnType:`
<p><a href="/docs/tokenizers/pr_1/en/api/encoding#tokenizers.Encoding">Encoding</a></p>
`}}),ve=new qa({props:{code:`encode("A single sequence")*
encode("A sequence", "And its pair")*
encode([ "A", "pre", "tokenized", "sequence" ], is_pretokenized=True)\`
encode(
[ "A", "pre", "tokenized", "sequence" ], [ "And", "its", "pair" ],
is_pretokenized=True
)`,highlighted:`encode(<span class="hljs-string">&quot;A single sequence&quot;</span>)*
encode(<span class="hljs-string">&quot;A sequence&quot;</span>, <span class="hljs-string">&quot;And its pair&quot;</span>)*
encode([ <span class="hljs-string">&quot;A&quot;</span>, <span class="hljs-string">&quot;pre&quot;</span>, <span class="hljs-string">&quot;tokenized&quot;</span>, <span class="hljs-string">&quot;sequence&quot;</span> ], is_pretokenized=<span class="hljs-literal">True</span>)\`
encode(
[ <span class="hljs-string">&quot;A&quot;</span>, <span class="hljs-string">&quot;pre&quot;</span>, <span class="hljs-string">&quot;tokenized&quot;</span>, <span class="hljs-string">&quot;sequence&quot;</span> ], [ <span class="hljs-string">&quot;And&quot;</span>, <span class="hljs-string">&quot;its&quot;</span>, <span class="hljs-string">&quot;pair&quot;</span> ],
is_pretokenized=<span class="hljs-literal">True</span>
)`}}),$e=new b({props:{name:"encode_batch",anchor:"tokenizers.Tokenizer.encode_batch",parameters:[{name:"input",val:""},{name:"is_pretokenized",val:" = False"},{name:"add_special_tokens",val:" = True"}],parametersDescription:[{anchor:"tokenizers.Tokenizer.encode_batch.input",description:`<strong>input</strong> (A <code>List</code>/\`<code>Tuple</code> of <code>~tokenizers.EncodeInput</code>) &#x2014;
A list of single sequences or pair sequences to encode. Each sequence
can be either raw text or pre-tokenized, according to the <code>is_pretokenized</code>
argument:</p>
<ul>
<li>If <code>is_pretokenized=False</code>: <code>TextEncodeInput()</code></li>
<li>If <code>is_pretokenized=True</code>: <code>PreTokenizedEncodeInput()</code></li>
</ul>`,name:"input"},{anchor:"tokenizers.Tokenizer.encode_batch.is_pretokenized",description:`<strong>is_pretokenized</strong> (<code>bool</code>, defaults to <code>False</code>) &#x2014;
Whether the input is already pre-tokenized`,name:"is_pretokenized"},{anchor:"tokenizers.Tokenizer.encode_batch.add_special_tokens",description:`<strong>add_special_tokens</strong> (<code>bool</code>, defaults to <code>True</code>) &#x2014;
Whether to add the special tokens`,name:"add_special_tokens"}],returnDescription:`
<p>The encoded batch</p>
`,returnType:`
<p>A <code>List</code> of [\`~tokenizers.Encoding\u201C]</p>
`}}),Te=new qa({props:{code:`encode_batch([
"A single sequence",
("A tuple with a sequence", "And its pair"),
[ "A", "pre", "tokenized", "sequence" ],
([ "A", "pre", "tokenized", "sequence" ], "And its pair")
])`,highlighted:`encode_batch([
<span class="hljs-string">&quot;A single sequence&quot;</span>,
(<span class="hljs-string">&quot;A tuple with a sequence&quot;</span>, <span class="hljs-string">&quot;And its pair&quot;</span>),
[ <span class="hljs-string">&quot;A&quot;</span>, <span class="hljs-string">&quot;pre&quot;</span>, <span class="hljs-string">&quot;tokenized&quot;</span>, <span class="hljs-string">&quot;sequence&quot;</span> ],
([ <span class="hljs-string">&quot;A&quot;</span>, <span class="hljs-string">&quot;pre&quot;</span>, <span class="hljs-string">&quot;tokenized&quot;</span>, <span class="hljs-string">&quot;sequence&quot;</span> ], <span class="hljs-string">&quot;And its pair&quot;</span>)
])`}}),be=new b({props:{name:"from_buffer",anchor:"tokenizers.Tokenizer.from_buffer",parameters:[{name:"buffer",val:""}],parametersDescription:[{anchor:"tokenizers.Tokenizer.from_buffer.buffer",description:`<strong>buffer</strong> (<code>bytes</code>) &#x2014;
A buffer containing a previously serialized <a href="/docs/tokenizers/pr_1/en/api/tokenizer#tokenizers.Tokenizer">Tokenizer</a>`,name:"buffer"}],returnDescription:`
<p>The new tokenizer</p>
`,returnType:`
<p><a href="/docs/tokenizers/pr_1/en/api/tokenizer#tokenizers.Tokenizer">Tokenizer</a></p>
`}}),Ee=new b({props:{name:"from_file",anchor:"tokenizers.Tokenizer.from_file",parameters:[{name:"path",val:""}],parametersDescription:[{anchor:"tokenizers.Tokenizer.from_file.path",description:`<strong>path</strong> (<code>str</code>) &#x2014;
A path to a local JSON file representing a previously serialized
<a href="/docs/tokenizers/pr_1/en/api/tokenizer#tokenizers.Tokenizer">Tokenizer</a>`,name:"path"}],returnDescription:`
<p>The new tokenizer</p>
`,returnType:`
<p><a href="/docs/tokenizers/pr_1/en/api/tokenizer#tokenizers.Tokenizer">Tokenizer</a></p>
`}}),qe=new b({props:{name:"from_pretrained",anchor:"tokenizers.Tokenizer.from_pretrained",parameters:[{name:"identifier",val:""},{name:"revision",val:" = 'main'"},{name:"auth_token",val:" = None"}],parametersDescription:[{anchor:"tokenizers.Tokenizer.from_pretrained.identifier",description:`<strong>identifier</strong> (<code>str</code>) &#x2014;
The identifier of a Model on the Hugging Face Hub, that contains
a tokenizer.json file`,name:"identifier"},{anchor:"tokenizers.Tokenizer.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, defaults to <em>main</em>) &#x2014;
A branch or commit id`,name:"revision"},{anchor:"tokenizers.Tokenizer.from_pretrained.auth_token",description:`<strong>auth_token</strong> (<code>str</code>, <em>optional</em>, defaults to <em>None</em>) &#x2014;
An optional auth token used to access private repositories on the
Hugging Face Hub`,name:"auth_token"}],returnDescription:`
<p>The new tokenizer</p>
`,returnType:`
<p><a href="/docs/tokenizers/pr_1/en/api/tokenizer#tokenizers.Tokenizer">Tokenizer</a></p>
`}}),Ae=new b({props:{name:"from_str",anchor:"tokenizers.Tokenizer.from_str",parameters:[{name:"json",val:""}],parametersDescription:[{anchor:"tokenizers.Tokenizer.from_str.json",description:`<strong>json</strong> (<code>str</code>) &#x2014;
A valid JSON string representing a previously serialized
<a href="/docs/tokenizers/pr_1/en/api/tokenizer#tokenizers.Tokenizer">Tokenizer</a>`,name:"json"}],returnDescription:`
<p>The new tokenizer</p>
`,returnType:`
<p><a href="/docs/tokenizers/pr_1/en/api/tokenizer#tokenizers.Tokenizer">Tokenizer</a></p>
`}}),Ie=new b({props:{name:"get_vocab",anchor:"tokenizers.Tokenizer.get_vocab",parameters:[{name:"with_added_tokens",val:" = True"}],parametersDescription:[{anchor:"tokenizers.Tokenizer.get_vocab.with_added_tokens",description:`<strong>with_added_tokens</strong> (<code>bool</code>, defaults to <code>True</code>) &#x2014;
Whether to include the added tokens`,name:"with_added_tokens"}],returnDescription:`
<p>The vocabulary</p>
`,returnType:`
<p><code>Dict[str, int]</code></p>
`}}),Pe=new b({props:{name:"get_vocab_size",anchor:"tokenizers.Tokenizer.get_vocab_size",parameters:[{name:"with_added_tokens",val:" = True"}],parametersDescription:[{anchor:"tokenizers.Tokenizer.get_vocab_size.with_added_tokens",description:`<strong>with_added_tokens</strong> (<code>bool</code>, defaults to <code>True</code>) &#x2014;
Whether to include the added tokens`,name:"with_added_tokens"}],returnDescription:`
<p>The size of the vocabulary</p>
`,returnType:`
<p><code>int</code></p>
`}}),je=new b({props:{name:"id_to_token",anchor:"tokenizers.Tokenizer.id_to_token",parameters:[{name:"id",val:""}],parametersDescription:[{anchor:"tokenizers.Tokenizer.id_to_token.id",description:`<strong>id</strong> (<code>int</code>) &#x2014;
The id to convert`,name:"id"}],returnDescription:`
<p>An optional token, <code>None</code> if out of vocabulary</p>
`,returnType:`
<p><code>Optional[str]</code></p>
`}}),Le=new b({props:{name:"no_padding",anchor:"tokenizers.Tokenizer.no_padding",parameters:[]}}),Ve=new b({props:{name:"no_truncation",anchor:"tokenizers.Tokenizer.no_truncation",parameters:[]}}),Ne=new b({props:{name:"num_special_tokens_to_add",anchor:"tokenizers.Tokenizer.num_special_tokens_to_add",parameters:[{name:"is_pair",val:""}]}}),Ce=new b({props:{name:"post_process",anchor:"tokenizers.Tokenizer.post_process",parameters:[{name:"encoding",val:""},{name:"pair",val:" = None"},{name:"add_special_tokens",val:" = True"}],parametersDescription:[{anchor:"tokenizers.Tokenizer.post_process.encoding",description:`<strong>encoding</strong> (<a href="/docs/tokenizers/pr_1/en/api/encoding#tokenizers.Encoding">Encoding</a>) &#x2014;
The <a href="/docs/tokenizers/pr_1/en/api/encoding#tokenizers.Encoding">Encoding</a> corresponding to the main sequence.`,name:"encoding"},{anchor:"tokenizers.Tokenizer.post_process.pair",description:`<strong>pair</strong> (<a href="/docs/tokenizers/pr_1/en/api/encoding#tokenizers.Encoding">Encoding</a>, <em>optional</em>) &#x2014;
An optional <a href="/docs/tokenizers/pr_1/en/api/encoding#tokenizers.Encoding">Encoding</a> corresponding to the pair sequence.`,name:"pair"},{anchor:"tokenizers.Tokenizer.post_process.add_special_tokens",description:`<strong>add_special_tokens</strong> (<code>bool</code>) &#x2014;
Whether to add the special tokens`,name:"add_special_tokens"}],returnDescription:`
<p>The final post-processed encoding</p>
`,returnType:`
<p><a href="/docs/tokenizers/pr_1/en/api/encoding#tokenizers.Encoding">Encoding</a></p>
`}}),Fe=new b({props:{name:"save",anchor:"tokenizers.Tokenizer.save",parameters:[{name:"pretty",val:" = True"}],parametersDescription:[{anchor:"tokenizers.Tokenizer.save.path",description:`<strong>path</strong> (<code>str</code>) &#x2014;
A path to a file in which to save the serialized tokenizer.`,name:"path"},{anchor:"tokenizers.Tokenizer.save.pretty",description:`<strong>pretty</strong> (<code>bool</code>, defaults to <code>True</code>) &#x2014;
Whether the JSON file should be pretty formatted.`,name:"pretty"}]}}),Me=new b({props:{name:"to_str",anchor:"tokenizers.Tokenizer.to_str",parameters:[{name:"pretty",val:" = False"}],parametersDescription:[{anchor:"tokenizers.Tokenizer.to_str.pretty",description:`<strong>pretty</strong> (<code>bool</code>, defaults to <code>False</code>) &#x2014;
Whether the JSON string should be pretty formatted.`,name:"pretty"}],returnDescription:`
<p>A string representing the serialized Tokenizer</p>
`,returnType:`
<p><code>str</code></p>
`}}),Re=new b({props:{name:"token_to_id",anchor:"tokenizers.Tokenizer.token_to_id",parameters:[{name:"token",val:""}],parametersDescription:[{anchor:"tokenizers.Tokenizer.token_to_id.token",description:`<strong>token</strong> (<code>str</code>) &#x2014;
The token to convert`,name:"token"}],returnDescription:`
<p>An optional id, <code>None</code> if out of vocabulary</p>
`,returnType:`
<p><code>Optional[int]</code></p>
`}}),Je=new b({props:{name:"train",anchor:"tokenizers.Tokenizer.train",parameters:[{name:"files",val:""},{name:"trainer",val:" = None"}],parametersDescription:[{anchor:"tokenizers.Tokenizer.train.files",description:`<strong>files</strong> (<code>List[str]</code>) &#x2014;
A list of path to the files that we should use for training`,name:"files"},{anchor:"tokenizers.Tokenizer.train.trainer",description:`<strong>trainer</strong> (<code>~tokenizers.trainers.Trainer</code>, <em>optional</em>) &#x2014;
An optional trainer that should be used to train our Model`,name:"trainer"}]}}),Ge=new b({props:{name:"train_from_iterator",anchor:"tokenizers.Tokenizer.train_from_iterator",parameters:[{name:"iterator",val:""},{name:"trainer",val:" = None"},{name:"length",val:" = None"}],parametersDescription:[{anchor:"tokenizers.Tokenizer.train_from_iterator.iterator",description:`<strong>iterator</strong> (<code>Iterator</code>) &#x2014;
Any iterator over strings or list of strings`,name:"iterator"},{anchor:"tokenizers.Tokenizer.train_from_iterator.trainer",description:`<strong>trainer</strong> (<code>~tokenizers.trainers.Trainer</code>, <em>optional</em>) &#x2014;
An optional trainer that should be used to train our Model`,name:"trainer"},{anchor:"tokenizers.Tokenizer.train_from_iterator.length",description:`<strong>length</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The total number of sequences in the iterator. This is used to
provide meaningful progress tracking`,name:"length"}]}}),{c(){t=o("div"),k(f.$$.fragment),p=d(),g=o("p"),w=s("A "),E=o("code"),q=s("Tokenizer"),A=s(` works as a pipeline. It processes some raw text as input
and outputs an `),C=o("a"),M=s("Encoding"),D=s("."),W=d(),m=o("div"),k(y.$$.fragment),V=d(),R=o("p"),Be=s("Add the given special tokens to the Tokenizer."),Ue=d(),pt=o("p"),kn=s(`If these tokens are already part of the vocabulary, it just let the Tokenizer know about
them. If they don\u2019t exist, the Tokenizer creates them, giving them a new id.`),_n=d(),lt=o("p"),zn=s(`These special tokens will never be processed by the model (ie won\u2019t be split into
multiple tokens), and they can be removed from the output when decoding.`),vn=d(),O=o("div"),k(ue.$$.fragment),$n=d(),ht=o("p"),Tn=s("Add the given tokens to the vocabulary"),bn=d(),gt=o("p"),wn=s(`The given tokens are added only if they don\u2019t already exist in the vocabulary.
Each token then gets a new attributed id.`),En=d(),S=o("div"),k(fe.$$.fragment),yn=d(),ut=o("p"),qn=s("Decode the given list of ids back to a string"),xn=d(),ft=o("p"),An=s("This is used to decode anything coming back from a Language Model"),Dn=d(),J=o("div"),k(me.$$.fragment),In=d(),mt=o("p"),Pn=s("Decode a batch of ids back to their corresponding string"),jn=d(),G=o("div"),k(ke.$$.fragment),Ln=d(),kt=o("p"),Vn=s("Enable the padding"),Nn=d(),B=o("div"),k(_e.$$.fragment),Cn=d(),_t=o("p"),On=s("Enable truncation"),Sn=d(),I=o("div"),k(ze.$$.fragment),Fn=d(),zt=o("p"),Hn=s(`Encode the given sequence and pair. This method can process raw text sequences
as well as already pre-tokenized sequences.`),Mn=d(),vt=o("p"),Wn=s(`Example:
Here are some examples of the inputs that are accepted:`),Rn=d(),k(ve.$$.fragment),Jn=d(),P=o("div"),k($e.$$.fragment),Gn=d(),$t=o("p"),Bn=s(`Encode the given batch of inputs. This method accept both raw text sequences
as well as already pre-tokenized sequences.`),Un=d(),Tt=o("p"),Yn=s(`Example:
Here are some examples of the inputs that are accepted:`),Kn=d(),k(Te.$$.fragment),Qn=d(),U=o("div"),k(be.$$.fragment),Xn=d(),we=o("p"),Zn=s("Instantiate a new "),Ye=o("a"),eo=s("Tokenizer"),to=s(" from the given buffer."),no=d(),Y=o("div"),k(Ee.$$.fragment),oo=d(),ye=o("p"),ro=s("Instantiate a new "),Ke=o("a"),ao=s("Tokenizer"),so=s(" from the file at the given path."),io=d(),K=o("div"),k(qe.$$.fragment),co=d(),xe=o("p"),po=s("Instantiate a new "),Qe=o("a"),lo=s("Tokenizer"),ho=s(` from an existing file on the
Hugging Face Hub.`),go=d(),Q=o("div"),k(Ae.$$.fragment),uo=d(),De=o("p"),fo=s("Instantiate a new "),Xe=o("a"),mo=s("Tokenizer"),ko=s(" from the given JSON string."),_o=d(),X=o("div"),k(Ie.$$.fragment),zo=d(),bt=o("p"),vo=s("Get the underlying vocabulary"),$o=d(),Z=o("div"),k(Pe.$$.fragment),To=d(),wt=o("p"),bo=s("Get the size of the underlying vocabulary"),wo=d(),ee=o("div"),k(je.$$.fragment),Eo=d(),Et=o("p"),yo=s("Convert the given id to its corresponding token if it exists"),qo=d(),te=o("div"),k(Le.$$.fragment),xo=d(),yt=o("p"),Ao=s("Disable padding"),Do=d(),ne=o("div"),k(Ve.$$.fragment),Io=d(),qt=o("p"),Po=s("Disable truncation"),jo=d(),oe=o("div"),k(Ne.$$.fragment),Lo=d(),xt=o("p"),Vo=s(`Return the number of special tokens that would be added for single/pair sentences.
:param is_pair: Boolean indicating if the input would be a single sentence or a pair
:return:`),No=d(),j=o("div"),k(Ce.$$.fragment),Co=d(),At=o("p"),Oo=s("Apply all the post-processing steps to the given encodings."),So=d(),Dt=o("p"),Fo=s("The various steps are:"),Ho=d(),H=o("ol"),Oe=o("li"),Mo=s(`Truncate according to the set truncation params (provided with
`),It=o("code"),Wo=s("enable_truncation()"),Ro=s(")"),Jo=d(),Ze=o("li"),Go=s("Apply the "),Pt=o("code"),Bo=s("PostProcessor"),Uo=d(),Se=o("li"),Yo=s(`Pad according to the set padding params (provided with
`),jt=o("code"),Ko=s("enable_padding()"),Qo=s(")"),Xo=d(),re=o("div"),k(Fe.$$.fragment),Zo=d(),He=o("p"),er=s("Save the "),et=o("a"),tr=s("Tokenizer"),nr=s(" to the file at the given path."),or=d(),ae=o("div"),k(Me.$$.fragment),rr=d(),We=o("p"),ar=s("Gets a serialized string representing this "),tt=o("a"),sr=s("Tokenizer"),ir=s("."),dr=d(),se=o("div"),k(Re.$$.fragment),cr=d(),Lt=o("p"),pr=s("Convert the given token to its corresponding id if it exists"),lr=d(),F=o("div"),k(Je.$$.fragment),hr=d(),Vt=o("p"),gr=s("Train the Tokenizer using the given files."),ur=d(),nt=o("p"),fr=s(`Reads the files line by line, while keeping all the whitespace, even new lines.
If you want to train from data store in-memory, you can check
`),Nt=o("code"),mr=s("train_from_iterator()"),kr=d(),L=o("div"),k(Ge.$$.fragment),_r=d(),Ct=o("p"),zr=s("Train the Tokenizer using the provided iterator."),vr=d(),Ot=o("p"),$r=s("You can provide anything that is a Python Iterator"),Tr=d(),N=o("ul"),ot=o("li"),br=s("A list of sequences "),St=o("code"),wr=s("List[str]"),Er=d(),ie=o("li"),yr=s("A generator that yields "),Ft=o("code"),qr=s("str"),xr=s(" or "),Ht=o("code"),Ar=s("List[str]"),Dr=d(),Mt=o("li"),Ir=s("A Numpy array of strings"),Pr=d(),Wt=o("li"),jr=s("\u2026"),this.h()},l(l){t=r(l,"DIV",{class:!0});var h=a(t);_(f.$$.fragment,h),p=c(h),g=r(h,"P",{});var rt=a(g);w=i(rt,"A "),E=r(rt,"CODE",{});var Or=a(E);q=i(Or,"Tokenizer"),Or.forEach(n),A=i(rt,` works as a pipeline. It processes some raw text as input
and outputs an `),C=r(rt,"A",{href:!0});var Sr=a(C);M=i(Sr,"Encoding"),Sr.forEach(n),D=i(rt,"."),rt.forEach(n),W=c(h),m=r(h,"DIV",{class:!0});var de=a(m);_(y.$$.fragment,de),V=c(de),R=r(de,"P",{});var Fr=a(R);Be=i(Fr,"Add the given special tokens to the Tokenizer."),Fr.forEach(n),Ue=c(de),pt=r(de,"P",{});var Hr=a(pt);kn=i(Hr,`If these tokens are already part of the vocabulary, it just let the Tokenizer know about
them. If they don\u2019t exist, the Tokenizer creates them, giving them a new id.`),Hr.forEach(n),_n=c(de),lt=r(de,"P",{});var Mr=a(lt);zn=i(Mr,`These special tokens will never be processed by the model (ie won\u2019t be split into
multiple tokens), and they can be removed from the output when decoding.`),Mr.forEach(n),de.forEach(n),vn=c(h),O=r(h,"DIV",{class:!0});var at=a(O);_(ue.$$.fragment,at),$n=c(at),ht=r(at,"P",{});var Wr=a(ht);Tn=i(Wr,"Add the given tokens to the vocabulary"),Wr.forEach(n),bn=c(at),gt=r(at,"P",{});var Rr=a(gt);wn=i(Rr,`The given tokens are added only if they don\u2019t already exist in the vocabulary.
Each token then gets a new attributed id.`),Rr.forEach(n),at.forEach(n),En=c(h),S=r(h,"DIV",{class:!0});var st=a(S);_(fe.$$.fragment,st),yn=c(st),ut=r(st,"P",{});var Jr=a(ut);qn=i(Jr,"Decode the given list of ids back to a string"),Jr.forEach(n),xn=c(st),ft=r(st,"P",{});var Gr=a(ft);An=i(Gr,"This is used to decode anything coming back from a Language Model"),Gr.forEach(n),st.forEach(n),Dn=c(h),J=r(h,"DIV",{class:!0});var Gt=a(J);_(me.$$.fragment,Gt),In=c(Gt),mt=r(Gt,"P",{});var Br=a(mt);Pn=i(Br,"Decode a batch of ids back to their corresponding string"),Br.forEach(n),Gt.forEach(n),jn=c(h),G=r(h,"DIV",{class:!0});var Bt=a(G);_(ke.$$.fragment,Bt),Ln=c(Bt),kt=r(Bt,"P",{});var Ur=a(kt);Vn=i(Ur,"Enable the padding"),Ur.forEach(n),Bt.forEach(n),Nn=c(h),B=r(h,"DIV",{class:!0});var Ut=a(B);_(_e.$$.fragment,Ut),Cn=c(Ut),_t=r(Ut,"P",{});var Yr=a(_t);On=i(Yr,"Enable truncation"),Yr.forEach(n),Ut.forEach(n),Sn=c(h),I=r(h,"DIV",{class:!0});var ce=a(I);_(ze.$$.fragment,ce),Fn=c(ce),zt=r(ce,"P",{});var Kr=a(zt);Hn=i(Kr,`Encode the given sequence and pair. This method can process raw text sequences
as well as already pre-tokenized sequences.`),Kr.forEach(n),Mn=c(ce),vt=r(ce,"P",{});var Qr=a(vt);Wn=i(Qr,`Example:
Here are some examples of the inputs that are accepted:`),Qr.forEach(n),Rn=c(ce),_(ve.$$.fragment,ce),ce.forEach(n),Jn=c(h),P=r(h,"DIV",{class:!0});var pe=a(P);_($e.$$.fragment,pe),Gn=c(pe),$t=r(pe,"P",{});var Xr=a($t);Bn=i(Xr,`Encode the given batch of inputs. This method accept both raw text sequences
as well as already pre-tokenized sequences.`),Xr.forEach(n),Un=c(pe),Tt=r(pe,"P",{});var Zr=a(Tt);Yn=i(Zr,`Example:
Here are some examples of the inputs that are accepted:`),Zr.forEach(n),Kn=c(pe),_(Te.$$.fragment,pe),pe.forEach(n),Qn=c(h),U=r(h,"DIV",{class:!0});var Yt=a(U);_(be.$$.fragment,Yt),Xn=c(Yt),we=r(Yt,"P",{});var Kt=a(we);Zn=i(Kt,"Instantiate a new "),Ye=r(Kt,"A",{href:!0});var ea=a(Ye);eo=i(ea,"Tokenizer"),ea.forEach(n),to=i(Kt," from the given buffer."),Kt.forEach(n),Yt.forEach(n),no=c(h),Y=r(h,"DIV",{class:!0});var Qt=a(Y);_(Ee.$$.fragment,Qt),oo=c(Qt),ye=r(Qt,"P",{});var Xt=a(ye);ro=i(Xt,"Instantiate a new "),Ke=r(Xt,"A",{href:!0});var ta=a(Ke);ao=i(ta,"Tokenizer"),ta.forEach(n),so=i(Xt," from the file at the given path."),Xt.forEach(n),Qt.forEach(n),io=c(h),K=r(h,"DIV",{class:!0});var Zt=a(K);_(qe.$$.fragment,Zt),co=c(Zt),xe=r(Zt,"P",{});var en=a(xe);po=i(en,"Instantiate a new "),Qe=r(en,"A",{href:!0});var na=a(Qe);lo=i(na,"Tokenizer"),na.forEach(n),ho=i(en,` from an existing file on the
Hugging Face Hub.`),en.forEach(n),Zt.forEach(n),go=c(h),Q=r(h,"DIV",{class:!0});var tn=a(Q);_(Ae.$$.fragment,tn),uo=c(tn),De=r(tn,"P",{});var nn=a(De);fo=i(nn,"Instantiate a new "),Xe=r(nn,"A",{href:!0});var oa=a(Xe);mo=i(oa,"Tokenizer"),oa.forEach(n),ko=i(nn," from the given JSON string."),nn.forEach(n),tn.forEach(n),_o=c(h),X=r(h,"DIV",{class:!0});var on=a(X);_(Ie.$$.fragment,on),zo=c(on),bt=r(on,"P",{});var ra=a(bt);vo=i(ra,"Get the underlying vocabulary"),ra.forEach(n),on.forEach(n),$o=c(h),Z=r(h,"DIV",{class:!0});var rn=a(Z);_(Pe.$$.fragment,rn),To=c(rn),wt=r(rn,"P",{});var aa=a(wt);bo=i(aa,"Get the size of the underlying vocabulary"),aa.forEach(n),rn.forEach(n),wo=c(h),ee=r(h,"DIV",{class:!0});var an=a(ee);_(je.$$.fragment,an),Eo=c(an),Et=r(an,"P",{});var sa=a(Et);yo=i(sa,"Convert the given id to its corresponding token if it exists"),sa.forEach(n),an.forEach(n),qo=c(h),te=r(h,"DIV",{class:!0});var sn=a(te);_(Le.$$.fragment,sn),xo=c(sn),yt=r(sn,"P",{});var ia=a(yt);Ao=i(ia,"Disable padding"),ia.forEach(n),sn.forEach(n),Do=c(h),ne=r(h,"DIV",{class:!0});var dn=a(ne);_(Ve.$$.fragment,dn),Io=c(dn),qt=r(dn,"P",{});var da=a(qt);Po=i(da,"Disable truncation"),da.forEach(n),dn.forEach(n),jo=c(h),oe=r(h,"DIV",{class:!0});var cn=a(oe);_(Ne.$$.fragment,cn),Lo=c(cn),xt=r(cn,"P",{});var ca=a(xt);Vo=i(ca,`Return the number of special tokens that would be added for single/pair sentences.
:param is_pair: Boolean indicating if the input would be a single sentence or a pair
:return:`),ca.forEach(n),cn.forEach(n),No=c(h),j=r(h,"DIV",{class:!0});var le=a(j);_(Ce.$$.fragment,le),Co=c(le),At=r(le,"P",{});var pa=a(At);Oo=i(pa,"Apply all the post-processing steps to the given encodings."),pa.forEach(n),So=c(le),Dt=r(le,"P",{});var la=a(Dt);Fo=i(la,"The various steps are:"),la.forEach(n),Ho=c(le),H=r(le,"OL",{});var it=a(H);Oe=r(it,"LI",{});var pn=a(Oe);Mo=i(pn,`Truncate according to the set truncation params (provided with
`),It=r(pn,"CODE",{});var ha=a(It);Wo=i(ha,"enable_truncation()"),ha.forEach(n),Ro=i(pn,")"),pn.forEach(n),Jo=c(it),Ze=r(it,"LI",{});var Lr=a(Ze);Go=i(Lr,"Apply the "),Pt=r(Lr,"CODE",{});var ga=a(Pt);Bo=i(ga,"PostProcessor"),ga.forEach(n),Lr.forEach(n),Uo=c(it),Se=r(it,"LI",{});var ln=a(Se);Yo=i(ln,`Pad according to the set padding params (provided with
`),jt=r(ln,"CODE",{});var ua=a(jt);Ko=i(ua,"enable_padding()"),ua.forEach(n),Qo=i(ln,")"),ln.forEach(n),it.forEach(n),le.forEach(n),Xo=c(h),re=r(h,"DIV",{class:!0});var hn=a(re);_(Fe.$$.fragment,hn),Zo=c(hn),He=r(hn,"P",{});var gn=a(He);er=i(gn,"Save the "),et=r(gn,"A",{href:!0});var fa=a(et);tr=i(fa,"Tokenizer"),fa.forEach(n),nr=i(gn," to the file at the given path."),gn.forEach(n),hn.forEach(n),or=c(h),ae=r(h,"DIV",{class:!0});var un=a(ae);_(Me.$$.fragment,un),rr=c(un),We=r(un,"P",{});var fn=a(We);ar=i(fn,"Gets a serialized string representing this "),tt=r(fn,"A",{href:!0});var ma=a(tt);sr=i(ma,"Tokenizer"),ma.forEach(n),ir=i(fn,"."),fn.forEach(n),un.forEach(n),dr=c(h),se=r(h,"DIV",{class:!0});var mn=a(se);_(Re.$$.fragment,mn),cr=c(mn),Lt=r(mn,"P",{});var ka=a(Lt);pr=i(ka,"Convert the given token to its corresponding id if it exists"),ka.forEach(n),mn.forEach(n),lr=c(h),F=r(h,"DIV",{class:!0});var dt=a(F);_(Je.$$.fragment,dt),hr=c(dt),Vt=r(dt,"P",{});var _a=a(Vt);gr=i(_a,"Train the Tokenizer using the given files."),_a.forEach(n),ur=c(dt),nt=r(dt,"P",{});var Vr=a(nt);fr=i(Vr,`Reads the files line by line, while keeping all the whitespace, even new lines.
If you want to train from data store in-memory, you can check
`),Nt=r(Vr,"CODE",{});var za=a(Nt);mr=i(za,"train_from_iterator()"),za.forEach(n),Vr.forEach(n),dt.forEach(n),kr=c(h),L=r(h,"DIV",{class:!0});var he=a(L);_(Ge.$$.fragment,he),_r=c(he),Ct=r(he,"P",{});var va=a(Ct);zr=i(va,"Train the Tokenizer using the provided iterator."),va.forEach(n),vr=c(he),Ot=r(he,"P",{});var $a=a(Ot);$r=i($a,"You can provide anything that is a Python Iterator"),$a.forEach(n),Tr=c(he),N=r(he,"UL",{});var ge=a(N);ot=r(ge,"LI",{});var Nr=a(ot);br=i(Nr,"A list of sequences "),St=r(Nr,"CODE",{});var Ta=a(St);wr=i(Ta,"List[str]"),Ta.forEach(n),Nr.forEach(n),Er=c(ge),ie=r(ge,"LI",{});var Rt=a(ie);yr=i(Rt,"A generator that yields "),Ft=r(Rt,"CODE",{});var ba=a(Ft);qr=i(ba,"str"),ba.forEach(n),xr=i(Rt," or "),Ht=r(Rt,"CODE",{});var wa=a(Ht);Ar=i(wa,"List[str]"),wa.forEach(n),Rt.forEach(n),Dr=c(ge),Mt=r(ge,"LI",{});var Ea=a(Mt);Ir=i(Ea,"A Numpy array of strings"),Ea.forEach(n),Pr=c(ge),Wt=r(ge,"LI",{});var ya=a(Wt);jr=i(ya,"\u2026"),ya.forEach(n),ge.forEach(n),he.forEach(n),h.forEach(n),this.h()},h(){u(C,"href","/docs/tokenizers/pr_1/en/api/encoding#tokenizers.Encoding"),u(m,"class","docstring"),u(O,"class","docstring"),u(S,"class","docstring"),u(J,"class","docstring"),u(G,"class","docstring"),u(B,"class","docstring"),u(I,"class","docstring"),u(P,"class","docstring"),u(Ye,"href","/docs/tokenizers/pr_1/en/api/tokenizer#tokenizers.Tokenizer"),u(U,"class","docstring"),u(Ke,"href","/docs/tokenizers/pr_1/en/api/tokenizer#tokenizers.Tokenizer"),u(Y,"class","docstring"),u(Qe,"href","/docs/tokenizers/pr_1/en/api/tokenizer#tokenizers.Tokenizer"),u(K,"class","docstring"),u(Xe,"href","/docs/tokenizers/pr_1/en/api/tokenizer#tokenizers.Tokenizer"),u(Q,"class","docstring"),u(X,"class","docstring"),u(Z,"class","docstring"),u(ee,"class","docstring"),u(te,"class","docstring"),u(ne,"class","docstring"),u(oe,"class","docstring"),u(j,"class","docstring"),u(et,"href","/docs/tokenizers/pr_1/en/api/tokenizer#tokenizers.Tokenizer"),u(re,"class","docstring"),u(tt,"href","/docs/tokenizers/pr_1/en/api/tokenizer#tokenizers.Tokenizer"),u(ae,"class","docstring"),u(se,"class","docstring"),u(F,"class","docstring"),u(L,"class","docstring"),u(t,"class","docstring")},m(l,h){ct(l,t,h),z(f,t,null),e(t,p),e(t,g),e(g,w),e(g,E),e(E,q),e(g,A),e(g,C),e(C,M),e(g,D),e(t,W),e(t,m),z(y,m,null),e(m,V),e(m,R),e(R,Be),e(m,Ue),e(m,pt),e(pt,kn),e(m,_n),e(m,lt),e(lt,zn),e(t,vn),e(t,O),z(ue,O,null),e(O,$n),e(O,ht),e(ht,Tn),e(O,bn),e(O,gt),e(gt,wn),e(t,En),e(t,S),z(fe,S,null),e(S,yn),e(S,ut),e(ut,qn),e(S,xn),e(S,ft),e(ft,An),e(t,Dn),e(t,J),z(me,J,null),e(J,In),e(J,mt),e(mt,Pn),e(t,jn),e(t,G),z(ke,G,null),e(G,Ln),e(G,kt),e(kt,Vn),e(t,Nn),e(t,B),z(_e,B,null),e(B,Cn),e(B,_t),e(_t,On),e(t,Sn),e(t,I),z(ze,I,null),e(I,Fn),e(I,zt),e(zt,Hn),e(I,Mn),e(I,vt),e(vt,Wn),e(I,Rn),z(ve,I,null),e(t,Jn),e(t,P),z($e,P,null),e(P,Gn),e(P,$t),e($t,Bn),e(P,Un),e(P,Tt),e(Tt,Yn),e(P,Kn),z(Te,P,null),e(t,Qn),e(t,U),z(be,U,null),e(U,Xn),e(U,we),e(we,Zn),e(we,Ye),e(Ye,eo),e(we,to),e(t,no),e(t,Y),z(Ee,Y,null),e(Y,oo),e(Y,ye),e(ye,ro),e(ye,Ke),e(Ke,ao),e(ye,so),e(t,io),e(t,K),z(qe,K,null),e(K,co),e(K,xe),e(xe,po),e(xe,Qe),e(Qe,lo),e(xe,ho),e(t,go),e(t,Q),z(Ae,Q,null),e(Q,uo),e(Q,De),e(De,fo),e(De,Xe),e(Xe,mo),e(De,ko),e(t,_o),e(t,X),z(Ie,X,null),e(X,zo),e(X,bt),e(bt,vo),e(t,$o),e(t,Z),z(Pe,Z,null),e(Z,To),e(Z,wt),e(wt,bo),e(t,wo),e(t,ee),z(je,ee,null),e(ee,Eo),e(ee,Et),e(Et,yo),e(t,qo),e(t,te),z(Le,te,null),e(te,xo),e(te,yt),e(yt,Ao),e(t,Do),e(t,ne),z(Ve,ne,null),e(ne,Io),e(ne,qt),e(qt,Po),e(t,jo),e(t,oe),z(Ne,oe,null),e(oe,Lo),e(oe,xt),e(xt,Vo),e(t,No),e(t,j),z(Ce,j,null),e(j,Co),e(j,At),e(At,Oo),e(j,So),e(j,Dt),e(Dt,Fo),e(j,Ho),e(j,H),e(H,Oe),e(Oe,Mo),e(Oe,It),e(It,Wo),e(Oe,Ro),e(H,Jo),e(H,Ze),e(Ze,Go),e(Ze,Pt),e(Pt,Bo),e(H,Uo),e(H,Se),e(Se,Yo),e(Se,jt),e(jt,Ko),e(Se,Qo),e(t,Xo),e(t,re),z(Fe,re,null),e(re,Zo),e(re,He),e(He,er),e(He,et),e(et,tr),e(He,nr),e(t,or),e(t,ae),z(Me,ae,null),e(ae,rr),e(ae,We),e(We,ar),e(We,tt),e(tt,sr),e(We,ir),e(t,dr),e(t,se),z(Re,se,null),e(se,cr),e(se,Lt),e(Lt,pr),e(t,lr),e(t,F),z(Je,F,null),e(F,hr),e(F,Vt),e(Vt,gr),e(F,ur),e(F,nt),e(nt,fr),e(nt,Nt),e(Nt,mr),e(t,kr),e(t,L),z(Ge,L,null),e(L,_r),e(L,Ct),e(Ct,zr),e(L,vr),e(L,Ot),e(Ot,$r),e(L,Tr),e(L,N),e(N,ot),e(ot,br),e(ot,St),e(St,wr),e(N,Er),e(N,ie),e(ie,yr),e(ie,Ft),e(Ft,qr),e(ie,xr),e(ie,Ht),e(Ht,Ar),e(N,Dr),e(N,Mt),e(Mt,Ir),e(N,Pr),e(N,Wt),e(Wt,jr),Jt=!0},p:ja,i(l){Jt||(v(f.$$.fragment,l),v(y.$$.fragment,l),v(ue.$$.fragment,l),v(fe.$$.fragment,l),v(me.$$.fragment,l),v(ke.$$.fragment,l),v(_e.$$.fragment,l),v(ze.$$.fragment,l),v(ve.$$.fragment,l),v($e.$$.fragment,l),v(Te.$$.fragment,l),v(be.$$.fragment,l),v(Ee.$$.fragment,l),v(qe.$$.fragment,l),v(Ae.$$.fragment,l),v(Ie.$$.fragment,l),v(Pe.$$.fragment,l),v(je.$$.fragment,l),v(Le.$$.fragment,l),v(Ve.$$.fragment,l),v(Ne.$$.fragment,l),v(Ce.$$.fragment,l),v(Fe.$$.fragment,l),v(Me.$$.fragment,l),v(Re.$$.fragment,l),v(Je.$$.fragment,l),v(Ge.$$.fragment,l),Jt=!0)},o(l){$(f.$$.fragment,l),$(y.$$.fragment,l),$(ue.$$.fragment,l),$(fe.$$.fragment,l),$(me.$$.fragment,l),$(ke.$$.fragment,l),$(_e.$$.fragment,l),$(ze.$$.fragment,l),$(ve.$$.fragment,l),$($e.$$.fragment,l),$(Te.$$.fragment,l),$(be.$$.fragment,l),$(Ee.$$.fragment,l),$(qe.$$.fragment,l),$(Ae.$$.fragment,l),$(Ie.$$.fragment,l),$(Pe.$$.fragment,l),$(je.$$.fragment,l),$(Le.$$.fragment,l),$(Ve.$$.fragment,l),$(Ne.$$.fragment,l),$(Ce.$$.fragment,l),$(Fe.$$.fragment,l),$(Me.$$.fragment,l),$(Re.$$.fragment,l),$(Je.$$.fragment,l),$(Ge.$$.fragment,l),Jt=!1},d(l){l&&n(t),T(f),T(y),T(ue),T(fe),T(me),T(ke),T(_e),T(ze),T(ve),T($e),T(Te),T(be),T(Ee),T(qe),T(Ae),T(Ie),T(Pe),T(je),T(Le),T(Ve),T(Ne),T(Ce),T(Fe),T(Me),T(Re),T(Je),T(Ge)}}}function Ca(x){let t,f;return t=new Cr({props:{$$slots:{default:[Na]},$$scope:{ctx:x}}}),{c(){k(t.$$.fragment)},l(p){_(t.$$.fragment,p)},m(p,g){z(t,p,g),f=!0},p(p,g){const w={};g&2&&(w.$$scope={dirty:g,ctx:p}),t.$set(w)},i(p){f||(v(t.$$.fragment,p),f=!0)},o(p){$(t.$$.fragment,p),f=!1},d(p){T(t,p)}}}function Oa(x){let t,f,p,g,w;return{c(){t=o("p"),f=s("The Rust API Reference is available directly on the "),p=o("a"),g=s("Docs.rs"),w=s(" website."),this.h()},l(E){t=r(E,"P",{});var q=a(t);f=i(q,"The Rust API Reference is available directly on the "),p=r(q,"A",{href:!0,rel:!0});var A=a(p);g=i(A,"Docs.rs"),A.forEach(n),w=i(q," website."),q.forEach(n),this.h()},h(){u(p,"href","https://docs.rs/tokenizers/latest/tokenizers/"),u(p,"rel","nofollow")},m(E,q){ct(E,t,q),e(t,f),e(t,p),e(p,g),e(t,w)},d(E){E&&n(t)}}}function Sa(x){let t,f;return t=new Cr({props:{$$slots:{default:[Oa]},$$scope:{ctx:x}}}),{c(){k(t.$$.fragment)},l(p){_(t.$$.fragment,p)},m(p,g){z(t,p,g),f=!0},p(p,g){const w={};g&2&&(w.$$scope={dirty:g,ctx:p}),t.$set(w)},i(p){f||(v(t.$$.fragment,p),f=!0)},o(p){$(t.$$.fragment,p),f=!1},d(p){T(t,p)}}}function Fa(x){let t,f;return{c(){t=o("p"),f=s("The node API has not been documented yet.")},l(p){t=r(p,"P",{});var g=a(t);f=i(g,"The node API has not been documented yet."),g.forEach(n)},m(p,g){ct(p,t,g),e(t,f)},d(p){p&&n(t)}}}function Ha(x){let t,f;return t=new Cr({props:{$$slots:{default:[Fa]},$$scope:{ctx:x}}}),{c(){k(t.$$.fragment)},l(p){_(t.$$.fragment,p)},m(p,g){z(t,p,g),f=!0},p(p,g){const w={};g&2&&(w.$$scope={dirty:g,ctx:p}),t.$set(w)},i(p){f||(v(t.$$.fragment,p),f=!0)},o(p){$(t.$$.fragment,p),f=!1},d(p){T(t,p)}}}function Ma(x){let t,f,p,g,w,E,q,A,C,M,D,W;return E=new La({}),D=new Va({props:{python:!0,rust:!0,node:!0,$$slots:{node:[Ha],rust:[Sa],python:[Ca]},$$scope:{ctx:x}}}),{c(){t=o("meta"),f=d(),p=o("h1"),g=o("a"),w=o("span"),k(E.$$.fragment),q=d(),A=o("span"),C=s("Tokenizer"),M=d(),k(D.$$.fragment),this.h()},l(m){const y=Ia('[data-svelte="svelte-1phssyn"]',document.head);t=r(y,"META",{name:!0,content:!0}),y.forEach(n),f=c(m),p=r(m,"H1",{class:!0});var V=a(p);g=r(V,"A",{id:!0,class:!0,href:!0});var R=a(g);w=r(R,"SPAN",{});var Be=a(w);_(E.$$.fragment,Be),Be.forEach(n),R.forEach(n),q=c(V),A=r(V,"SPAN",{});var Ue=a(A);C=i(Ue,"Tokenizer"),Ue.forEach(n),V.forEach(n),M=c(m),_(D.$$.fragment,m),this.h()},h(){u(t,"name","hf:doc:metadata"),u(t,"content",JSON.stringify(Wa)),u(g,"id","tokenizers.Tokenizer"),u(g,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(g,"href","#tokenizers.Tokenizer"),u(p,"class","relative group")},m(m,y){e(document.head,t),ct(m,f,y),ct(m,p,y),e(p,g),e(g,w),z(E,w,null),e(p,q),e(p,A),e(A,C),ct(m,M,y),z(D,m,y),W=!0},p(m,[y]){const V={};y&2&&(V.$$scope={dirty:y,ctx:m}),D.$set(V)},i(m){W||(v(E.$$.fragment,m),v(D.$$.fragment,m),W=!0)},o(m){$(E.$$.fragment,m),$(D.$$.fragment,m),W=!1},d(m){n(t),m&&n(f),m&&n(p),T(E),m&&n(M),T(D,m)}}}const Wa={local:"tokenizers.Tokenizer",title:"Tokenizer"};function Ra(x){return Pa(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Ka extends xa{constructor(t){super();Aa(this,t,Ra,Ma,Da,{})}}export{Ka as default,Wa as metadata};
