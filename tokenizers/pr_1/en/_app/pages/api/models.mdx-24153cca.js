import{S as ir,i as dr,s as lr,e as o,k as c,w as g,t as i,M as cr,c as n,d as t,m,a as r,x as _,h as d,b as v,F as e,g as L,y as $,q as k,o as b,B as E,v as mr,L as pr}from"../../chunks/vendor-0d3f0756.js";import{D}from"../../chunks/Docstring-f752f2c3.js";import{C as bn}from"../../chunks/CodeBlock-7b0cb15c.js";import{I as fr}from"../../chunks/IconCopyLink-9193371d.js";import{T as hr,M as En}from"../../chunks/TokenizersLanguageContent-ca787841.js";function vr(C){let s,f,l,p,y,x,u,T,ie,R,V,oe,h,I,O,N,De,M,It,je,Bt,At,Ge,Lt,Mt,We,qt,Ct,S,de,Vt,Z,Ot,Ue,Rt,Nt,Fe,St,jt,Gt,He,Ut,kt,P,le,Ft,Je,Ht,Jt,Ke,Kt,Qt,Qe,Xt,Yt,j,ce,Zt,Ie,eo,Xe,to,oo,ee,no,Ye,ro,ao,Be,so,io,lo,ne,me,co,Ze,mo,po,G,pe,fo,et,ho,vo,tt,uo,go,re,fe,_o,ot,$o,ko,ae,he,bo,nt,Eo,bt,te,ve,Po,rt,yo,Et,W,ue,wo,at,zo,xo,st,To,Do,B,ge,Wo,it,Io,Bo,dt,Ao,Lo,_e,Mo,U,qo,lt,Co,Vo,ct,Oo,Ro,Ae,No,So,F,$e,jo,Le,Go,mt,Uo,Fo,pt,Ho,Pt,q,ke,Jo,ft,Ko,Qo,A,be,Xo,ht,Yo,Zo,vt,en,tn,Ee,on,H,nn,ut,rn,an,gt,sn,dn,Me,ln,cn,J,Pe,mn,ye,pn,_t,fn,hn,vn,we,un,$t,gn,_n,yt;return f=new D({props:{name:"class tokenizers.models.BPE",anchor:"tokenizers.models.BPE",parameters:[{name:"vocab",val:" = None"},{name:"merges",val:" = None"},{name:"cache_capacity",val:" = None"},{name:"dropout",val:" = None"},{name:"unk_token",val:" = None"},{name:"continuing_subword_prefix",val:" = None"},{name:"end_of_word_suffix",val:" = None"},{name:"fuse_unk",val:" = None"}],parametersDescription:[{anchor:"tokenizers.models.BPE.vocab",description:`<strong>vocab</strong> (<code>Dict[str, int]</code>, <em>optional</em>) &#x2014;
A dictionnary of string keys and their ids <code>{&quot;am&quot;: 0,...}</code>`,name:"vocab"},{anchor:"tokenizers.models.BPE.merges",description:`<strong>merges</strong> (<code>List[Tuple[str, str]]</code>, <em>optional</em>) &#x2014;
A list of pairs of tokens (<code>Tuple[str, str]</code>) <code>[(&quot;a&quot;, &quot;b&quot;),...]</code>`,name:"merges"},{anchor:"tokenizers.models.BPE.cache_capacity",description:`<strong>cache_capacity</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The number of words that the BPE cache can contain. The cache allows
to speed-up the process by keeping the result of the merge operations
for a number of words.`,name:"cache_capacity"},{anchor:"tokenizers.models.BPE.dropout",description:`<strong>dropout</strong> (<code>float</code>, <em>optional</em>) &#x2014;
A float between 0 and 1 that represents the BPE dropout to use.`,name:"dropout"},{anchor:"tokenizers.models.BPE.unk_token",description:`<strong>unk_token</strong> (<code>str</code>, <em>optional</em>) &#x2014;
The unknown token to be used by the model.`,name:"unk_token"},{anchor:"tokenizers.models.BPE.continuing_subword_prefix",description:`<strong>continuing_subword_prefix</strong> (<code>str</code>, <em>optional</em>) &#x2014;
The prefix to attach to subword units that don&#x2019;t represent a beginning of word.`,name:"continuing_subword_prefix"},{anchor:"tokenizers.models.BPE.end_of_word_suffix",description:`<strong>end_of_word_suffix</strong> (<code>str</code>, <em>optional</em>) &#x2014;
The suffix to attach to subword units that represent an end of word.`,name:"end_of_word_suffix"},{anchor:"tokenizers.models.BPE.fuse_unk",description:`<strong>fuse_unk</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether to fuse any subsequent unknown tokens into a single one`,name:"fuse_unk"}]}}),T=new D({props:{name:"from_file",anchor:"tokenizers.models.BPE.from_file",parameters:[{name:"vocab",val:""},{name:"merge",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"tokenizers.models.BPE.from_file.vocab",description:`<strong>vocab</strong> (<code>str</code>) &#x2014;
The path to a <code>vocab.json</code> file`,name:"vocab"},{anchor:"tokenizers.models.BPE.from_file.merges",description:`<strong>merges</strong> (<code>str</code>) &#x2014;
The path to a <code>merges.txt</code> file`,name:"merges"}],returnDescription:`
<p>An instance of BPE loaded from these files</p>
`,returnType:`
<p><a href="/docs/tokenizers/pr_1/en/api/models#tokenizers.models.BPE">BPE</a></p>
`}}),N=new bn({props:{code:`vocab, merges = BPE.read_file(vocab_filename, merges_filename)
bpe = BPE(vocab, merges)`,highlighted:`vocab, merges = BPE.read_file(vocab_filename, merges_filename)
bpe = BPE(vocab, merges)`}}),de=new D({props:{name:"read_file",anchor:"tokenizers.models.BPE.read_file",parameters:[{name:"vocab",val:""},{name:"merges",val:""}],parametersDescription:[{anchor:"tokenizers.models.BPE.read_file.vocab",description:`<strong>vocab</strong> (<code>str</code>) &#x2014;
The path to a <code>vocab.json</code> file`,name:"vocab"},{anchor:"tokenizers.models.BPE.read_file.merges",description:`<strong>merges</strong> (<code>str</code>) &#x2014;
The path to a <code>merges.txt</code> file`,name:"merges"}],returnDescription:`
<p>The vocabulary and merges loaded into memory</p>
`,returnType:`
<p>A <code>Tuple</code> with the vocab and the merges</p>
`}}),le=new D({props:{name:"class tokenizers.models.Model",anchor:"tokenizers.models.Model",parameters:""}}),ce=new D({props:{name:"get_trainer",anchor:"tokenizers.models.Model.get_trainer",parameters:"",returnDescription:`
<p>The Trainer used to train this model</p>
`,returnType:`
<p><code>Trainer</code></p>
`}}),me=new D({props:{name:"id_to_token",anchor:"tokenizers.models.Model.id_to_token",parameters:[{name:"id",val:""}],parametersDescription:[{anchor:"tokenizers.models.Model.id_to_token.id",description:`<strong>id</strong> (<code>int</code>) &#x2014;
An ID to convert to a token`,name:"id"}],returnDescription:`
<p>The token associated to the ID</p>
`,returnType:`
<p><code>str</code></p>
`}}),pe=new D({props:{name:"save",anchor:"tokenizers.models.Model.save",parameters:[{name:"folder",val:""},{name:"prefix",val:""}],parametersDescription:[{anchor:"tokenizers.models.Model.save.folder",description:`<strong>folder</strong> (<code>str</code>) &#x2014;
The path to the target folder in which to save the various files`,name:"folder"},{anchor:"tokenizers.models.Model.save.prefix",description:`<strong>prefix</strong> (<code>str</code>, <em>optional</em>) &#x2014;
An optional prefix, used to prefix each file name`,name:"prefix"}],returnDescription:`
<p>The list of saved files</p>
`,returnType:`
<p><code>List[str]</code></p>
`}}),fe=new D({props:{name:"token_to_id",anchor:"tokenizers.models.Model.token_to_id",parameters:[{name:"tokens",val:""}],parametersDescription:[{anchor:"tokenizers.models.Model.token_to_id.token",description:`<strong>token</strong> (<code>str</code>) &#x2014;
A token to convert to an ID`,name:"token"}],returnDescription:`
<p>The ID associated to the token</p>
`,returnType:`
<p><code>int</code></p>
`}}),he=new D({props:{name:"tokenize",anchor:"tokenizers.models.Model.tokenize",parameters:[{name:"sequence",val:""}],parametersDescription:[{anchor:"tokenizers.models.Model.tokenize.sequence",description:`<strong>sequence</strong> (<code>str</code>) &#x2014;
A sequence to tokenize`,name:"sequence"}],returnDescription:`
<p>The generated tokens</p>
`,returnType:`
<p>A <code>List</code> of <code>Token</code></p>
`}}),ve=new D({props:{name:"class tokenizers.models.Unigram",anchor:"tokenizers.models.Unigram",parameters:[{name:"vocab",val:""}],parametersDescription:[{anchor:"tokenizers.models.Unigram.vocab",description:`<strong>vocab</strong> (<code>List[Tuple[str, float]]</code>, <em>optional</em>) &#x2014;
A list of vocabulary items and their relative score [(&#x201C;am&#x201D;, -0.2442),&#x2026;]`,name:"vocab"}]}}),ue=new D({props:{name:"class tokenizers.models.WordLevel",anchor:"tokenizers.models.WordLevel",parameters:[{name:"vocab",val:""},{name:"unk_token",val:""}],parametersDescription:[{anchor:"tokenizers.models.WordLevel.vocab",description:`<strong>vocab</strong> (<code>str</code>, <em>optional</em>) &#x2014;
A dictionnary of string keys and their ids <code>{&quot;am&quot;: 0,...}</code>`,name:"vocab"},{anchor:"tokenizers.models.WordLevel.unk_token",description:`<strong>unk_token</strong> (<code>str</code>, <em>optional</em>) &#x2014;
The unknown token to be used by the model.`,name:"unk_token"}]}}),ge=new D({props:{name:"from_file",anchor:"tokenizers.models.WordLevel.from_file",parameters:[{name:"vocab",val:""},{name:"unk_token",val:""}],parametersDescription:[{anchor:"tokenizers.models.WordLevel.from_file.vocab",description:`<strong>vocab</strong> (<code>str</code>) &#x2014;
The path to a <code>vocab.json</code> file`,name:"vocab"}],returnDescription:`
<p>An instance of WordLevel loaded from file</p>
`,returnType:`
<p><a
  href="/docs/tokenizers/pr_1/en/api/models#tokenizers.models.WordLevel"
>WordLevel</a></p>
`}}),_e=new bn({props:{code:`vocab = WordLevel.read_file(vocab_filename)
wordlevel = WordLevel(vocab)`,highlighted:`vocab = WordLevel.read_file(vocab_filename)
wordlevel = WordLevel(vocab)`}}),$e=new D({props:{name:"read_file",anchor:"tokenizers.models.WordLevel.read_file",parameters:[{name:"vocab",val:""}],parametersDescription:[{anchor:"tokenizers.models.WordLevel.read_file.vocab",description:`<strong>vocab</strong> (<code>str</code>) &#x2014;
The path to a <code>vocab.json</code> file`,name:"vocab"}],returnDescription:`
<p>The vocabulary as a <code>dict</code></p>
`,returnType:`
<p><code>Dict[str, int]</code></p>
`}}),ke=new D({props:{name:"class tokenizers.models.WordPiece",anchor:"tokenizers.models.WordPiece",parameters:[{name:"vocab",val:""},{name:"unk_token",val:""},{name:"max_input_chars_per_word",val:""}],parametersDescription:[{anchor:"tokenizers.models.WordPiece.vocab",description:`<strong>vocab</strong> (<code>Dict[str, int]</code>, <em>optional</em>) &#x2014;
A dictionnary of string keys and their ids <code>{&quot;am&quot;: 0,...}</code>`,name:"vocab"},{anchor:"tokenizers.models.WordPiece.unk_token",description:`<strong>unk_token</strong> (<code>str</code>, <em>optional</em>) &#x2014;
The unknown token to be used by the model.`,name:"unk_token"},{anchor:"tokenizers.models.WordPiece.max_input_chars_per_word",description:`<strong>max_input_chars_per_word</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The maximum number of characters to authorize in a single word.`,name:"max_input_chars_per_word"}]}}),be=new D({props:{name:"from_file",anchor:"tokenizers.models.WordPiece.from_file",parameters:[{name:"vocab",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"tokenizers.models.WordPiece.from_file.vocab",description:`<strong>vocab</strong> (<code>str</code>) &#x2014;
The path to a <code>vocab.txt</code> file`,name:"vocab"}],returnDescription:`
<p>An instance of WordPiece loaded from file</p>
`,returnType:`
<p><a
  href="/docs/tokenizers/pr_1/en/api/models#tokenizers.models.WordPiece"
>WordPiece</a></p>
`}}),Ee=new bn({props:{code:`vocab = WordPiece.read_file(vocab_filename)
wordpiece = WordPiece(vocab)`,highlighted:`vocab = WordPiece.read_file(vocab_filename)
wordpiece = WordPiece(vocab)`}}),Pe=new D({props:{name:"read_file",anchor:"tokenizers.models.WordPiece.read_file",parameters:[{name:"vocab",val:""}],parametersDescription:[{anchor:"tokenizers.models.WordPiece.read_file.vocab",description:`<strong>vocab</strong> (<code>str</code>) &#x2014;
The path to a <code>vocab.txt</code> file`,name:"vocab"}],returnDescription:`
<p>The vocabulary as a <code>dict</code></p>
`,returnType:`
<p><code>Dict[str, int]</code></p>
`}}),{c(){s=o("div"),g(f.$$.fragment),l=c(),p=o("p"),y=i("An implementation of the BPE (Byte-Pair Encoding) algorithm"),x=c(),u=o("div"),g(T.$$.fragment),ie=c(),R=o("p"),V=i("Instantiate a BPE model from the given files."),oe=c(),h=o("p"),I=i("This method is roughly equivalent to doing:"),O=c(),g(N.$$.fragment),De=c(),M=o("p"),It=i("If you don\u2019t need to keep the "),je=o("code"),Bt=i("vocab, merges"),At=i(` values lying around,
this method is more optimized than manually calling
`),Ge=o("code"),Lt=i("read_file()"),Mt=i(" to initialize a "),We=o("a"),qt=i("BPE"),Ct=c(),S=o("div"),g(de.$$.fragment),Vt=c(),Z=o("p"),Ot=i("Read a "),Ue=o("code"),Rt=i("vocab.json"),Nt=i(" and a "),Fe=o("code"),St=i("merges.txt"),jt=i(" files"),Gt=c(),He=o("p"),Ut=i(`This method provides a way to read and parse the content of these files,
returning the relevant data structures. If you want to instantiate some BPE models
from memory, this method gives you the expected input from the standard files.`),kt=c(),P=o("div"),g(le.$$.fragment),Ft=c(),Je=o("p"),Ht=i("Base class for all models"),Jt=c(),Ke=o("p"),Kt=i(`The model represents the actual tokenization algorithm. This is the part that
will contain and manage the learned vocabulary.`),Qt=c(),Qe=o("p"),Xt=i("This class cannot be constructed directly. Please use one of the concrete models."),Yt=c(),j=o("div"),g(ce.$$.fragment),Zt=c(),Ie=o("p"),eo=i("Get the associated "),Xe=o("code"),to=i("Trainer"),oo=c(),ee=o("p"),no=i("Retrieve the "),Ye=o("code"),ro=i("Trainer"),ao=i(` associated to this
`),Be=o("a"),so=i("Model"),io=i("."),lo=c(),ne=o("div"),g(me.$$.fragment),co=c(),Ze=o("p"),mo=i("Get the token associated to an ID"),po=c(),G=o("div"),g(pe.$$.fragment),fo=c(),et=o("p"),ho=i("Save the current model"),vo=c(),tt=o("p"),uo=i(`Save the current model in the given folder, using the given prefix for the various
files that will get created.
Any file with the same name that already exists in this folder will be overwritten.`),go=c(),re=o("div"),g(fe.$$.fragment),_o=c(),ot=o("p"),$o=i("Get the ID associated to a token"),ko=c(),ae=o("div"),g(he.$$.fragment),bo=c(),nt=o("p"),Eo=i("Tokenize a sequence"),bt=c(),te=o("div"),g(ve.$$.fragment),Po=c(),rt=o("p"),yo=i("An implementation of the Unigram algorithm"),Et=c(),W=o("div"),g(ue.$$.fragment),wo=c(),at=o("p"),zo=i("An implementation of the WordLevel algorithm"),xo=c(),st=o("p"),To=i("Most simple tokenizer model based on mapping tokens to their corresponding id."),Do=c(),B=o("div"),g(ge.$$.fragment),Wo=c(),it=o("p"),Io=i("Instantiate a WordLevel model from the given file"),Bo=c(),dt=o("p"),Ao=i("This method is roughly equivalent to doing:"),Lo=c(),g(_e.$$.fragment),Mo=c(),U=o("p"),qo=i("If you don\u2019t need to keep the "),lt=o("code"),Co=i("vocab"),Vo=i(` values lying around, this method is
more optimized than manually calling `),ct=o("code"),Oo=i("read_file()"),Ro=i(` to
initialize a `),Ae=o("a"),No=i("WordLevel"),So=c(),F=o("div"),g($e.$$.fragment),jo=c(),Le=o("p"),Go=i("Read a "),mt=o("code"),Uo=i("vocab.json"),Fo=c(),pt=o("p"),Ho=i(`This method provides a way to read and parse the content of a vocabulary file,
returning the relevant data structures. If you want to instantiate some WordLevel models
from memory, this method gives you the expected input from the standard files.`),Pt=c(),q=o("div"),g(ke.$$.fragment),Jo=c(),ft=o("p"),Ko=i("An implementation of the WordPiece algorithm"),Qo=c(),A=o("div"),g(be.$$.fragment),Xo=c(),ht=o("p"),Yo=i("Instantiate a WordPiece model from the given file"),Zo=c(),vt=o("p"),en=i("This method is roughly equivalent to doing:"),tn=c(),g(Ee.$$.fragment),on=c(),H=o("p"),nn=i("If you don\u2019t need to keep the "),ut=o("code"),rn=i("vocab"),an=i(` values lying around, this method is
more optimized than manually calling `),gt=o("code"),sn=i("read_file()"),dn=i(` to
initialize a `),Me=o("a"),ln=i("WordPiece"),cn=c(),J=o("div"),g(Pe.$$.fragment),mn=c(),ye=o("p"),pn=i("Read a "),_t=o("code"),fn=i("vocab.txt"),hn=i(" file"),vn=c(),we=o("p"),un=i("This method provides a way to read and parse the content of a standard "),$t=o("em"),gn=i("vocab.txt"),_n=i(`
file as used by the WordPiece Model, returning the relevant data structures. If you
want to instantiate some WordPiece models from memory, this method gives you the
expected input from the standard files.`),this.h()},l(a){s=n(a,"DIV",{class:!0});var w=r(s);_(f.$$.fragment,w),l=m(w),p=n(w,"P",{});var Pn=r(p);y=d(Pn,"An implementation of the BPE (Byte-Pair Encoding) algorithm"),Pn.forEach(t),x=m(w),u=n(w,"DIV",{class:!0});var K=r(u);_(T.$$.fragment,K),ie=m(K),R=n(K,"P",{});var yn=r(R);V=d(yn,"Instantiate a BPE model from the given files."),yn.forEach(t),oe=m(K),h=n(K,"P",{});var wn=r(h);I=d(wn,"This method is roughly equivalent to doing:"),wn.forEach(t),O=m(K),_(N.$$.fragment,K),De=m(K),M=n(K,"P",{});var ze=r(M);It=d(ze,"If you don\u2019t need to keep the "),je=n(ze,"CODE",{});var zn=r(je);Bt=d(zn,"vocab, merges"),zn.forEach(t),At=d(ze,` values lying around,
this method is more optimized than manually calling
`),Ge=n(ze,"CODE",{});var xn=r(Ge);Lt=d(xn,"read_file()"),xn.forEach(t),Mt=d(ze," to initialize a "),We=n(ze,"A",{href:!0});var Tn=r(We);qt=d(Tn,"BPE"),Tn.forEach(t),ze.forEach(t),K.forEach(t),Ct=m(w),S=n(w,"DIV",{class:!0});var qe=r(S);_(de.$$.fragment,qe),Vt=m(qe),Z=n(qe,"P",{});var Ce=r(Z);Ot=d(Ce,"Read a "),Ue=n(Ce,"CODE",{});var Dn=r(Ue);Rt=d(Dn,"vocab.json"),Dn.forEach(t),Nt=d(Ce," and a "),Fe=n(Ce,"CODE",{});var Wn=r(Fe);St=d(Wn,"merges.txt"),Wn.forEach(t),jt=d(Ce," files"),Ce.forEach(t),Gt=m(qe),He=n(qe,"P",{});var In=r(He);Ut=d(In,`This method provides a way to read and parse the content of these files,
returning the relevant data structures. If you want to instantiate some BPE models
from memory, this method gives you the expected input from the standard files.`),In.forEach(t),qe.forEach(t),w.forEach(t),kt=m(a),P=n(a,"DIV",{class:!0});var z=r(P);_(le.$$.fragment,z),Ft=m(z),Je=n(z,"P",{});var Bn=r(Je);Ht=d(Bn,"Base class for all models"),Bn.forEach(t),Jt=m(z),Ke=n(z,"P",{});var An=r(Ke);Kt=d(An,`The model represents the actual tokenization algorithm. This is the part that
will contain and manage the learned vocabulary.`),An.forEach(t),Qt=m(z),Qe=n(z,"P",{});var Ln=r(Qe);Xt=d(Ln,"This class cannot be constructed directly. Please use one of the concrete models."),Ln.forEach(t),Yt=m(z),j=n(z,"DIV",{class:!0});var Ve=r(j);_(ce.$$.fragment,Ve),Zt=m(Ve),Ie=n(Ve,"P",{});var $n=r(Ie);eo=d($n,"Get the associated "),Xe=n($n,"CODE",{});var Mn=r(Xe);to=d(Mn,"Trainer"),Mn.forEach(t),$n.forEach(t),oo=m(Ve),ee=n(Ve,"P",{});var Oe=r(ee);no=d(Oe,"Retrieve the "),Ye=n(Oe,"CODE",{});var qn=r(Ye);ro=d(qn,"Trainer"),qn.forEach(t),ao=d(Oe,` associated to this
`),Be=n(Oe,"A",{href:!0});var Cn=r(Be);so=d(Cn,"Model"),Cn.forEach(t),io=d(Oe,"."),Oe.forEach(t),Ve.forEach(t),lo=m(z),ne=n(z,"DIV",{class:!0});var wt=r(ne);_(me.$$.fragment,wt),co=m(wt),Ze=n(wt,"P",{});var Vn=r(Ze);mo=d(Vn,"Get the token associated to an ID"),Vn.forEach(t),wt.forEach(t),po=m(z),G=n(z,"DIV",{class:!0});var Re=r(G);_(pe.$$.fragment,Re),fo=m(Re),et=n(Re,"P",{});var On=r(et);ho=d(On,"Save the current model"),On.forEach(t),vo=m(Re),tt=n(Re,"P",{});var Rn=r(tt);uo=d(Rn,`Save the current model in the given folder, using the given prefix for the various
files that will get created.
Any file with the same name that already exists in this folder will be overwritten.`),Rn.forEach(t),Re.forEach(t),go=m(z),re=n(z,"DIV",{class:!0});var zt=r(re);_(fe.$$.fragment,zt),_o=m(zt),ot=n(zt,"P",{});var Nn=r(ot);$o=d(Nn,"Get the ID associated to a token"),Nn.forEach(t),zt.forEach(t),ko=m(z),ae=n(z,"DIV",{class:!0});var xt=r(ae);_(he.$$.fragment,xt),bo=m(xt),nt=n(xt,"P",{});var Sn=r(nt);Eo=d(Sn,"Tokenize a sequence"),Sn.forEach(t),xt.forEach(t),z.forEach(t),bt=m(a),te=n(a,"DIV",{class:!0});var Tt=r(te);_(ve.$$.fragment,Tt),Po=m(Tt),rt=n(Tt,"P",{});var jn=r(rt);yo=d(jn,"An implementation of the Unigram algorithm"),jn.forEach(t),Tt.forEach(t),Et=m(a),W=n(a,"DIV",{class:!0});var Q=r(W);_(ue.$$.fragment,Q),wo=m(Q),at=n(Q,"P",{});var Gn=r(at);zo=d(Gn,"An implementation of the WordLevel algorithm"),Gn.forEach(t),xo=m(Q),st=n(Q,"P",{});var Un=r(st);To=d(Un,"Most simple tokenizer model based on mapping tokens to their corresponding id."),Un.forEach(t),Do=m(Q),B=n(Q,"DIV",{class:!0});var X=r(B);_(ge.$$.fragment,X),Wo=m(X),it=n(X,"P",{});var Fn=r(it);Io=d(Fn,"Instantiate a WordLevel model from the given file"),Fn.forEach(t),Bo=m(X),dt=n(X,"P",{});var Hn=r(dt);Ao=d(Hn,"This method is roughly equivalent to doing:"),Hn.forEach(t),Lo=m(X),_(_e.$$.fragment,X),Mo=m(X),U=n(X,"P",{});var xe=r(U);qo=d(xe,"If you don\u2019t need to keep the "),lt=n(xe,"CODE",{});var Jn=r(lt);Co=d(Jn,"vocab"),Jn.forEach(t),Vo=d(xe,` values lying around, this method is
more optimized than manually calling `),ct=n(xe,"CODE",{});var Kn=r(ct);Oo=d(Kn,"read_file()"),Kn.forEach(t),Ro=d(xe,` to
initialize a `),Ae=n(xe,"A",{href:!0});var Qn=r(Ae);No=d(Qn,"WordLevel"),Qn.forEach(t),xe.forEach(t),X.forEach(t),So=m(Q),F=n(Q,"DIV",{class:!0});var Ne=r(F);_($e.$$.fragment,Ne),jo=m(Ne),Le=n(Ne,"P",{});var kn=r(Le);Go=d(kn,"Read a "),mt=n(kn,"CODE",{});var Xn=r(mt);Uo=d(Xn,"vocab.json"),Xn.forEach(t),kn.forEach(t),Fo=m(Ne),pt=n(Ne,"P",{});var Yn=r(pt);Ho=d(Yn,`This method provides a way to read and parse the content of a vocabulary file,
returning the relevant data structures. If you want to instantiate some WordLevel models
from memory, this method gives you the expected input from the standard files.`),Yn.forEach(t),Ne.forEach(t),Q.forEach(t),Pt=m(a),q=n(a,"DIV",{class:!0});var se=r(q);_(ke.$$.fragment,se),Jo=m(se),ft=n(se,"P",{});var Zn=r(ft);Ko=d(Zn,"An implementation of the WordPiece algorithm"),Zn.forEach(t),Qo=m(se),A=n(se,"DIV",{class:!0});var Y=r(A);_(be.$$.fragment,Y),Xo=m(Y),ht=n(Y,"P",{});var er=r(ht);Yo=d(er,"Instantiate a WordPiece model from the given file"),er.forEach(t),Zo=m(Y),vt=n(Y,"P",{});var tr=r(vt);en=d(tr,"This method is roughly equivalent to doing:"),tr.forEach(t),tn=m(Y),_(Ee.$$.fragment,Y),on=m(Y),H=n(Y,"P",{});var Te=r(H);nn=d(Te,"If you don\u2019t need to keep the "),ut=n(Te,"CODE",{});var or=r(ut);rn=d(or,"vocab"),or.forEach(t),an=d(Te,` values lying around, this method is
more optimized than manually calling `),gt=n(Te,"CODE",{});var nr=r(gt);sn=d(nr,"read_file()"),nr.forEach(t),dn=d(Te,` to
initialize a `),Me=n(Te,"A",{href:!0});var rr=r(Me);ln=d(rr,"WordPiece"),rr.forEach(t),Te.forEach(t),Y.forEach(t),cn=m(se),J=n(se,"DIV",{class:!0});var Se=r(J);_(Pe.$$.fragment,Se),mn=m(Se),ye=n(Se,"P",{});var Dt=r(ye);pn=d(Dt,"Read a "),_t=n(Dt,"CODE",{});var ar=r(_t);fn=d(ar,"vocab.txt"),ar.forEach(t),hn=d(Dt," file"),Dt.forEach(t),vn=m(Se),we=n(Se,"P",{});var Wt=r(we);un=d(Wt,"This method provides a way to read and parse the content of a standard "),$t=n(Wt,"EM",{});var sr=r($t);gn=d(sr,"vocab.txt"),sr.forEach(t),_n=d(Wt,`
file as used by the WordPiece Model, returning the relevant data structures. If you
want to instantiate some WordPiece models from memory, this method gives you the
expected input from the standard files.`),Wt.forEach(t),Se.forEach(t),se.forEach(t),this.h()},h(){v(We,"href","/docs/tokenizers/pr_1/en/api/models#tokenizers.models.BPE"),v(u,"class","docstring"),v(S,"class","docstring"),v(s,"class","docstring"),v(Be,"href","/docs/tokenizers/pr_1/en/api/models#tokenizers.models.Model"),v(j,"class","docstring"),v(ne,"class","docstring"),v(G,"class","docstring"),v(re,"class","docstring"),v(ae,"class","docstring"),v(P,"class","docstring"),v(te,"class","docstring"),v(Ae,"href","/docs/tokenizers/pr_1/en/api/models#tokenizers.models.WordLevel"),v(B,"class","docstring"),v(F,"class","docstring"),v(W,"class","docstring"),v(Me,"href","/docs/tokenizers/pr_1/en/api/models#tokenizers.models.WordPiece"),v(A,"class","docstring"),v(J,"class","docstring"),v(q,"class","docstring")},m(a,w){L(a,s,w),$(f,s,null),e(s,l),e(s,p),e(p,y),e(s,x),e(s,u),$(T,u,null),e(u,ie),e(u,R),e(R,V),e(u,oe),e(u,h),e(h,I),e(u,O),$(N,u,null),e(u,De),e(u,M),e(M,It),e(M,je),e(je,Bt),e(M,At),e(M,Ge),e(Ge,Lt),e(M,Mt),e(M,We),e(We,qt),e(s,Ct),e(s,S),$(de,S,null),e(S,Vt),e(S,Z),e(Z,Ot),e(Z,Ue),e(Ue,Rt),e(Z,Nt),e(Z,Fe),e(Fe,St),e(Z,jt),e(S,Gt),e(S,He),e(He,Ut),L(a,kt,w),L(a,P,w),$(le,P,null),e(P,Ft),e(P,Je),e(Je,Ht),e(P,Jt),e(P,Ke),e(Ke,Kt),e(P,Qt),e(P,Qe),e(Qe,Xt),e(P,Yt),e(P,j),$(ce,j,null),e(j,Zt),e(j,Ie),e(Ie,eo),e(Ie,Xe),e(Xe,to),e(j,oo),e(j,ee),e(ee,no),e(ee,Ye),e(Ye,ro),e(ee,ao),e(ee,Be),e(Be,so),e(ee,io),e(P,lo),e(P,ne),$(me,ne,null),e(ne,co),e(ne,Ze),e(Ze,mo),e(P,po),e(P,G),$(pe,G,null),e(G,fo),e(G,et),e(et,ho),e(G,vo),e(G,tt),e(tt,uo),e(P,go),e(P,re),$(fe,re,null),e(re,_o),e(re,ot),e(ot,$o),e(P,ko),e(P,ae),$(he,ae,null),e(ae,bo),e(ae,nt),e(nt,Eo),L(a,bt,w),L(a,te,w),$(ve,te,null),e(te,Po),e(te,rt),e(rt,yo),L(a,Et,w),L(a,W,w),$(ue,W,null),e(W,wo),e(W,at),e(at,zo),e(W,xo),e(W,st),e(st,To),e(W,Do),e(W,B),$(ge,B,null),e(B,Wo),e(B,it),e(it,Io),e(B,Bo),e(B,dt),e(dt,Ao),e(B,Lo),$(_e,B,null),e(B,Mo),e(B,U),e(U,qo),e(U,lt),e(lt,Co),e(U,Vo),e(U,ct),e(ct,Oo),e(U,Ro),e(U,Ae),e(Ae,No),e(W,So),e(W,F),$($e,F,null),e(F,jo),e(F,Le),e(Le,Go),e(Le,mt),e(mt,Uo),e(F,Fo),e(F,pt),e(pt,Ho),L(a,Pt,w),L(a,q,w),$(ke,q,null),e(q,Jo),e(q,ft),e(ft,Ko),e(q,Qo),e(q,A),$(be,A,null),e(A,Xo),e(A,ht),e(ht,Yo),e(A,Zo),e(A,vt),e(vt,en),e(A,tn),$(Ee,A,null),e(A,on),e(A,H),e(H,nn),e(H,ut),e(ut,rn),e(H,an),e(H,gt),e(gt,sn),e(H,dn),e(H,Me),e(Me,ln),e(q,cn),e(q,J),$(Pe,J,null),e(J,mn),e(J,ye),e(ye,pn),e(ye,_t),e(_t,fn),e(ye,hn),e(J,vn),e(J,we),e(we,un),e(we,$t),e($t,gn),e(we,_n),yt=!0},p:pr,i(a){yt||(k(f.$$.fragment,a),k(T.$$.fragment,a),k(N.$$.fragment,a),k(de.$$.fragment,a),k(le.$$.fragment,a),k(ce.$$.fragment,a),k(me.$$.fragment,a),k(pe.$$.fragment,a),k(fe.$$.fragment,a),k(he.$$.fragment,a),k(ve.$$.fragment,a),k(ue.$$.fragment,a),k(ge.$$.fragment,a),k(_e.$$.fragment,a),k($e.$$.fragment,a),k(ke.$$.fragment,a),k(be.$$.fragment,a),k(Ee.$$.fragment,a),k(Pe.$$.fragment,a),yt=!0)},o(a){b(f.$$.fragment,a),b(T.$$.fragment,a),b(N.$$.fragment,a),b(de.$$.fragment,a),b(le.$$.fragment,a),b(ce.$$.fragment,a),b(me.$$.fragment,a),b(pe.$$.fragment,a),b(fe.$$.fragment,a),b(he.$$.fragment,a),b(ve.$$.fragment,a),b(ue.$$.fragment,a),b(ge.$$.fragment,a),b(_e.$$.fragment,a),b($e.$$.fragment,a),b(ke.$$.fragment,a),b(be.$$.fragment,a),b(Ee.$$.fragment,a),b(Pe.$$.fragment,a),yt=!1},d(a){a&&t(s),E(f),E(T),E(N),E(de),a&&t(kt),a&&t(P),E(le),E(ce),E(me),E(pe),E(fe),E(he),a&&t(bt),a&&t(te),E(ve),a&&t(Et),a&&t(W),E(ue),E(ge),E(_e),E($e),a&&t(Pt),a&&t(q),E(ke),E(be),E(Ee),E(Pe)}}}function ur(C){let s,f;return s=new En({props:{$$slots:{default:[vr]},$$scope:{ctx:C}}}),{c(){g(s.$$.fragment)},l(l){_(s.$$.fragment,l)},m(l,p){$(s,l,p),f=!0},p(l,p){const y={};p&2&&(y.$$scope={dirty:p,ctx:l}),s.$set(y)},i(l){f||(k(s.$$.fragment,l),f=!0)},o(l){b(s.$$.fragment,l),f=!1},d(l){E(s,l)}}}function gr(C){let s,f,l,p,y;return{c(){s=o("p"),f=i("The Rust API Reference is available directly on the "),l=o("a"),p=i("Docs.rs"),y=i(" website."),this.h()},l(x){s=n(x,"P",{});var u=r(s);f=d(u,"The Rust API Reference is available directly on the "),l=n(u,"A",{href:!0,rel:!0});var T=r(l);p=d(T,"Docs.rs"),T.forEach(t),y=d(u," website."),u.forEach(t),this.h()},h(){v(l,"href","https://docs.rs/tokenizers/latest/tokenizers/"),v(l,"rel","nofollow")},m(x,u){L(x,s,u),e(s,f),e(s,l),e(l,p),e(s,y)},d(x){x&&t(s)}}}function _r(C){let s,f;return s=new En({props:{$$slots:{default:[gr]},$$scope:{ctx:C}}}),{c(){g(s.$$.fragment)},l(l){_(s.$$.fragment,l)},m(l,p){$(s,l,p),f=!0},p(l,p){const y={};p&2&&(y.$$scope={dirty:p,ctx:l}),s.$set(y)},i(l){f||(k(s.$$.fragment,l),f=!0)},o(l){b(s.$$.fragment,l),f=!1},d(l){E(s,l)}}}function $r(C){let s,f;return{c(){s=o("p"),f=i("The node API has not been documented yet.")},l(l){s=n(l,"P",{});var p=r(s);f=d(p,"The node API has not been documented yet."),p.forEach(t)},m(l,p){L(l,s,p),e(s,f)},d(l){l&&t(s)}}}function kr(C){let s,f;return s=new En({props:{$$slots:{default:[$r]},$$scope:{ctx:C}}}),{c(){g(s.$$.fragment)},l(l){_(s.$$.fragment,l)},m(l,p){$(s,l,p),f=!0},p(l,p){const y={};p&2&&(y.$$scope={dirty:p,ctx:l}),s.$set(y)},i(l){f||(k(s.$$.fragment,l),f=!0)},o(l){b(s.$$.fragment,l),f=!1},d(l){E(s,l)}}}function br(C){let s,f,l,p,y,x,u,T,ie,R,V,oe;return x=new fr({}),V=new hr({props:{python:!0,rust:!0,node:!0,$$slots:{node:[kr],rust:[_r],python:[ur]},$$scope:{ctx:C}}}),{c(){s=o("meta"),f=c(),l=o("h1"),p=o("a"),y=o("span"),g(x.$$.fragment),u=c(),T=o("span"),ie=i("Models"),R=c(),g(V.$$.fragment),this.h()},l(h){const I=cr('[data-svelte="svelte-1phssyn"]',document.head);s=n(I,"META",{name:!0,content:!0}),I.forEach(t),f=m(h),l=n(h,"H1",{class:!0});var O=r(l);p=n(O,"A",{id:!0,class:!0,href:!0});var N=r(p);y=n(N,"SPAN",{});var De=r(y);_(x.$$.fragment,De),De.forEach(t),N.forEach(t),u=m(O),T=n(O,"SPAN",{});var M=r(T);ie=d(M,"Models"),M.forEach(t),O.forEach(t),R=m(h),_(V.$$.fragment,h),this.h()},h(){v(s,"name","hf:doc:metadata"),v(s,"content",JSON.stringify(Er)),v(p,"id","tokenizers.models.BPE"),v(p,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),v(p,"href","#tokenizers.models.BPE"),v(l,"class","relative group")},m(h,I){e(document.head,s),L(h,f,I),L(h,l,I),e(l,p),e(p,y),$(x,y,null),e(l,u),e(l,T),e(T,ie),L(h,R,I),$(V,h,I),oe=!0},p(h,[I]){const O={};I&2&&(O.$$scope={dirty:I,ctx:h}),V.$set(O)},i(h){oe||(k(x.$$.fragment,h),k(V.$$.fragment,h),oe=!0)},o(h){b(x.$$.fragment,h),b(V.$$.fragment,h),oe=!1},d(h){t(s),h&&t(f),h&&t(l),E(x),h&&t(R),E(V,h)}}}const Er={local:"tokenizers.models.BPE",title:"Models"};function Pr(C){return mr(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Dr extends ir{constructor(s){super();dr(this,s,Pr,br,lr,{})}}export{Dr as default,Er as metadata};
