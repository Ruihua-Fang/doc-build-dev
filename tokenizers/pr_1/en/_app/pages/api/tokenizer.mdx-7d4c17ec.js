import{S as Ha,i as Ma,s as Wa,e as n,k as i,w as k,t as a,M as Ra,c as o,d as t,m as d,a as r,x as _,h as s,b as u,F as e,g as me,y as z,q as v,o as $,B as T,v as Ja,L as Ga}from"../../chunks/vendor-0d3f0756.js";import{D as w}from"../../chunks/Docstring-f752f2c3.js";import{C as Oa}from"../../chunks/CodeBlock-7b0cb15c.js";import{I as Fa}from"../../chunks/IconCopyLink-9193371d.js";import{T as Ba,M as Gr}from"../../chunks/TokenizersLanguageContent-ca787841.js";function Ua(I){let l,m,p,f,E,y,x,D,h,P,j,A,b,q,O,Qe,R,Xe,$n,Tn,L,ke,bn,gt,wn,En,ut,yn,qn,ft,xn,An,H,_e,Dn,mt,In,Pn,kt,jn,Ln,M,ze,Vn,_t,Nn,Sn,zt,Cn,On,G,ve,Fn,vt,Hn,Mn,B,$e,Wn,$t,Rn,Jn,U,Te,Gn,Tt,Bn,Un,V,be,Yn,bt,Kn,Qn,wt,Xn,Zn,we,eo,N,Ee,to,Et,no,oo,yt,ro,ao,ye,so,Y,qe,io,xe,co,Ze,po,lo,ho,K,Ae,go,De,uo,et,fo,mo,ko,Q,Ie,_o,Pe,zo,tt,vo,$o,To,X,je,bo,Le,wo,nt,Eo,yo,qo,Z,Ve,xo,qt,Ao,Do,ee,Ne,Io,xt,Po,jo,te,Se,Lo,At,Vo,No,ne,Ce,So,Dt,Co,Oo,oe,Oe,Fo,It,Ho,Mo,re,Fe,Wo,Pt,Ro,Jo,S,He,Go,jt,Bo,Uo,Lt,Yo,Ko,J,Me,Qo,Vt,Xo,Zo,er,ot,tr,Nt,nr,or,We,rr,St,ar,sr,ir,ae,Re,dr,Je,cr,rt,pr,lr,hr,se,Ge,gr,Be,ur,at,fr,mr,kr,ie,Ue,_r,Ct,zr,vr,W,Ye,$r,Ot,Tr,br,st,wr,Ft,Er,yr,C,Ke,qr,Ht,xr,Ar,Mt,Dr,Ir,F,it,Pr,Wt,jr,Lr,de,Vr,Rt,Nr,Sr,Jt,Cr,Or,Gt,Fr,Hr,Bt,Mr,Yt;return f=new Fa({}),P=new w({props:{name:"class tokenizers.Tokenizer",anchor:"tokenizers.Tokenizer",parameters:[{name:"model",val:""}],parametersDescription:[{anchor:"tokenizers.Tokenizer.model",description:`<strong>model</strong> (<a href="/docs/tokenizers/pr_1/en/api/models#tokenizers.models.Model">Model</a>) &#x2014;
The core algorithm that this <code>Tokenizer</code> should be using.`,name:"model"}]}}),ke=new w({props:{name:"add_special_tokens",anchor:"tokenizers.Tokenizer.add_special_tokens",parameters:[{name:"tokens",val:""}],parametersDescription:[{anchor:"tokenizers.Tokenizer.add_special_tokens.tokens",description:`<strong>tokens</strong> (A <code>List</code> of <a href="/docs/tokenizers/pr_1/en/api/added-tokens#tokenizers.AddedToken">AddedToken</a> or <code>str</code>) &#x2014;
The list of special tokens we want to add to the vocabulary. Each token can either
be a string or an instance of <a href="/docs/tokenizers/pr_1/en/api/added-tokens#tokenizers.AddedToken">AddedToken</a> for more
customization.`,name:"tokens"}],returnDescription:`
<p>The number of tokens that were created in the vocabulary</p>
`,returnType:`
<p><code>int</code></p>
`}}),_e=new w({props:{name:"add_tokens",anchor:"tokenizers.Tokenizer.add_tokens",parameters:[{name:"tokens",val:""}],parametersDescription:[{anchor:"tokenizers.Tokenizer.add_tokens.tokens",description:`<strong>tokens</strong> (A <code>List</code> of <a href="/docs/tokenizers/pr_1/en/api/added-tokens#tokenizers.AddedToken">AddedToken</a> or <code>str</code>) &#x2014;
The list of tokens we want to add to the vocabulary. Each token can be either a
string or an instance of <a href="/docs/tokenizers/pr_1/en/api/added-tokens#tokenizers.AddedToken">AddedToken</a> for more customization.`,name:"tokens"}],returnDescription:`
<p>The number of tokens that were created in the vocabulary</p>
`,returnType:`
<p><code>int</code></p>
`}}),ze=new w({props:{name:"decode",anchor:"tokenizers.Tokenizer.decode",parameters:[{name:"ids",val:""},{name:"skip_special_tokens",val:" = True"}],parametersDescription:[{anchor:"tokenizers.Tokenizer.decode.ids",description:`<strong>ids</strong> (A <code>List/Tuple</code> of <code>int</code>) &#x2014;
The list of ids that we want to decode`,name:"ids"},{anchor:"tokenizers.Tokenizer.decode.skip_special_tokens",description:`<strong>skip_special_tokens</strong> (<code>bool</code>, defaults to <code>True</code>) &#x2014;
Whether the special tokens should be removed from the decoded string`,name:"skip_special_tokens"}],returnDescription:`
<p>The decoded string</p>
`,returnType:`
<p><code>str</code></p>
`}}),ve=new w({props:{name:"decode_batch",anchor:"tokenizers.Tokenizer.decode_batch",parameters:[{name:"sequences",val:""},{name:"skip_special_tokens",val:" = True"}],parametersDescription:[{anchor:"tokenizers.Tokenizer.decode_batch.sequences",description:`<strong>sequences</strong> (<code>List</code> of <code>List[int]</code>) &#x2014;
The batch of sequences we want to decode`,name:"sequences"},{anchor:"tokenizers.Tokenizer.decode_batch.skip_special_tokens",description:`<strong>skip_special_tokens</strong> (<code>bool</code>, defaults to <code>True</code>) &#x2014;
Whether the special tokens should be removed from the decoded strings`,name:"skip_special_tokens"}],returnDescription:`
<p>A list of decoded strings</p>
`,returnType:`
<p><code>List[str]</code></p>
`}}),$e=new w({props:{name:"enable_padding",anchor:"tokenizers.Tokenizer.enable_padding",parameters:[{name:"direction",val:" = 'right'"},{name:"pad_id",val:" = 0"},{name:"pad_type_id",val:" = 0"},{name:"pad_token",val:" = '[PAD]'"},{name:"length",val:" = None"},{name:"pad_to_multiple_of",val:" = None"}],parametersDescription:[{anchor:"tokenizers.Tokenizer.enable_padding.direction",description:`<strong>direction</strong> (<code>str</code>, <em>optional</em>, defaults to <code>right</code>) &#x2014;
The direction in which to pad. Can be either <code>right</code> or <code>left</code>`,name:"direction"},{anchor:"tokenizers.Tokenizer.enable_padding.pad_to_multiple_of",description:`<strong>pad_to_multiple_of</strong> (<code>int</code>, <em>optional</em>) &#x2014;
If specified, the padding length should always snap to the next multiple of the
given value. For example if we were going to pad witha length of 250 but
<code>pad_to_multiple_of=8</code> then we will pad to 256.`,name:"pad_to_multiple_of"},{anchor:"tokenizers.Tokenizer.enable_padding.pad_id",description:`<strong>pad_id</strong> (<code>int</code>, defaults to 0) &#x2014;
The id to be used when padding`,name:"pad_id"},{anchor:"tokenizers.Tokenizer.enable_padding.pad_type_id",description:`<strong>pad_type_id</strong> (<code>int</code>, defaults to 0) &#x2014;
The type id to be used when padding`,name:"pad_type_id"},{anchor:"tokenizers.Tokenizer.enable_padding.pad_token",description:`<strong>pad_token</strong> (<code>str</code>, defaults to <code>[PAD]</code>) &#x2014;
The pad token to be used when padding`,name:"pad_token"},{anchor:"tokenizers.Tokenizer.enable_padding.length",description:`<strong>length</strong> (<code>int</code>, <em>optional</em>) &#x2014;
If specified, the length at which to pad. If not specified we pad using the size of
the longest sequence in a batch.`,name:"length"}]}}),Te=new w({props:{name:"enable_truncation",anchor:"tokenizers.Tokenizer.enable_truncation",parameters:[{name:"max_length",val:""},{name:"stride",val:" = 0"},{name:"strategy",val:" = 'longest_first'"},{name:"direction",val:" = 'right'"}],parametersDescription:[{anchor:"tokenizers.Tokenizer.enable_truncation.max_length",description:`<strong>max_length</strong> (<code>int</code>) &#x2014;
The max length at which to truncate`,name:"max_length"},{anchor:"tokenizers.Tokenizer.enable_truncation.stride",description:`<strong>stride</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The length of the previous first sequence to be included in the overflowing
sequence`,name:"stride"},{anchor:"tokenizers.Tokenizer.enable_truncation.strategy",description:`<strong>strategy</strong> (<code>str</code>, <em>optional</em>, defaults to <code>longest_first</code>) &#x2014;
The strategy used to truncation. Can be one of <code>longest_first</code>, <code>only_first</code> or
<code>only_second</code>.`,name:"strategy"},{anchor:"tokenizers.Tokenizer.enable_truncation.direction",description:`<strong>direction</strong> (<code>str</code>, defaults to <code>right</code>) &#x2014;
Truncate direction`,name:"direction"}]}}),be=new w({props:{name:"encode",anchor:"tokenizers.Tokenizer.encode",parameters:[{name:"sequence",val:""},{name:"pair",val:" = None"},{name:"is_pretokenized",val:" = False"},{name:"add_special_tokens",val:" = True"}],parametersDescription:[{anchor:"tokenizers.Tokenizer.encode.sequence",description:`<strong>sequence</strong> (<code>~tokenizers.InputSequence</code>) &#x2014;
The main input sequence we want to encode. This sequence can be either raw
text or pre-tokenized, according to the <code>is_pretokenized</code> argument:</p>
<ul>
<li>If <code>is_pretokenized=False</code>: <code>TextInputSequence</code></li>
<li>If <code>is_pretokenized=True</code>: <code>PreTokenizedInputSequence()</code></li>
</ul>`,name:"sequence"},{anchor:"tokenizers.Tokenizer.encode.pair",description:`<strong>pair</strong> (<code>~tokenizers.InputSequence</code>, <em>optional</em>) &#x2014;
An optional input sequence. The expected format is the same that for <code>sequence</code>.`,name:"pair"},{anchor:"tokenizers.Tokenizer.encode.is_pretokenized",description:`<strong>is_pretokenized</strong> (<code>bool</code>, defaults to <code>False</code>) &#x2014;
Whether the input is already pre-tokenized`,name:"is_pretokenized"},{anchor:"tokenizers.Tokenizer.encode.add_special_tokens",description:`<strong>add_special_tokens</strong> (<code>bool</code>, defaults to <code>True</code>) &#x2014;
Whether to add the special tokens`,name:"add_special_tokens"}],returnDescription:`
<p>The encoded result</p>
`,returnType:`
<p><a href="/docs/tokenizers/pr_1/en/api/encoding#tokenizers.Encoding">Encoding</a></p>
`}}),we=new Oa({props:{code:`encode("A single sequence")*
encode("A sequence", "And its pair")*
encode([ "A", "pre", "tokenized", "sequence" ], is_pretokenized=True)\`
encode(
[ "A", "pre", "tokenized", "sequence" ], [ "And", "its", "pair" ],
is_pretokenized=True
)`,highlighted:`encode(<span class="hljs-string">&quot;A single sequence&quot;</span>)*
encode(<span class="hljs-string">&quot;A sequence&quot;</span>, <span class="hljs-string">&quot;And its pair&quot;</span>)*
encode([ <span class="hljs-string">&quot;A&quot;</span>, <span class="hljs-string">&quot;pre&quot;</span>, <span class="hljs-string">&quot;tokenized&quot;</span>, <span class="hljs-string">&quot;sequence&quot;</span> ], is_pretokenized=<span class="hljs-literal">True</span>)\`
encode(
[ <span class="hljs-string">&quot;A&quot;</span>, <span class="hljs-string">&quot;pre&quot;</span>, <span class="hljs-string">&quot;tokenized&quot;</span>, <span class="hljs-string">&quot;sequence&quot;</span> ], [ <span class="hljs-string">&quot;And&quot;</span>, <span class="hljs-string">&quot;its&quot;</span>, <span class="hljs-string">&quot;pair&quot;</span> ],
is_pretokenized=<span class="hljs-literal">True</span>
)`}}),Ee=new w({props:{name:"encode_batch",anchor:"tokenizers.Tokenizer.encode_batch",parameters:[{name:"input",val:""},{name:"is_pretokenized",val:" = False"},{name:"add_special_tokens",val:" = True"}],parametersDescription:[{anchor:"tokenizers.Tokenizer.encode_batch.input",description:`<strong>input</strong> (A <code>List</code>/\`<code>Tuple</code> of <code>~tokenizers.EncodeInput</code>) &#x2014;
A list of single sequences or pair sequences to encode. Each sequence
can be either raw text or pre-tokenized, according to the <code>is_pretokenized</code>
argument:</p>
<ul>
<li>If <code>is_pretokenized=False</code>: <code>TextEncodeInput()</code></li>
<li>If <code>is_pretokenized=True</code>: <code>PreTokenizedEncodeInput()</code></li>
</ul>`,name:"input"},{anchor:"tokenizers.Tokenizer.encode_batch.is_pretokenized",description:`<strong>is_pretokenized</strong> (<code>bool</code>, defaults to <code>False</code>) &#x2014;
Whether the input is already pre-tokenized`,name:"is_pretokenized"},{anchor:"tokenizers.Tokenizer.encode_batch.add_special_tokens",description:`<strong>add_special_tokens</strong> (<code>bool</code>, defaults to <code>True</code>) &#x2014;
Whether to add the special tokens`,name:"add_special_tokens"}],returnDescription:`
<p>The encoded batch</p>
`,returnType:`
<p>A <code>List</code> of [\`~tokenizers.Encoding\u201C]</p>
`}}),ye=new Oa({props:{code:`encode_batch([
"A single sequence",
("A tuple with a sequence", "And its pair"),
[ "A", "pre", "tokenized", "sequence" ],
([ "A", "pre", "tokenized", "sequence" ], "And its pair")
])`,highlighted:`encode_batch([
<span class="hljs-string">&quot;A single sequence&quot;</span>,
(<span class="hljs-string">&quot;A tuple with a sequence&quot;</span>, <span class="hljs-string">&quot;And its pair&quot;</span>),
[ <span class="hljs-string">&quot;A&quot;</span>, <span class="hljs-string">&quot;pre&quot;</span>, <span class="hljs-string">&quot;tokenized&quot;</span>, <span class="hljs-string">&quot;sequence&quot;</span> ],
([ <span class="hljs-string">&quot;A&quot;</span>, <span class="hljs-string">&quot;pre&quot;</span>, <span class="hljs-string">&quot;tokenized&quot;</span>, <span class="hljs-string">&quot;sequence&quot;</span> ], <span class="hljs-string">&quot;And its pair&quot;</span>)
])`}}),qe=new w({props:{name:"from_buffer",anchor:"tokenizers.Tokenizer.from_buffer",parameters:[{name:"buffer",val:""}],parametersDescription:[{anchor:"tokenizers.Tokenizer.from_buffer.buffer",description:`<strong>buffer</strong> (<code>bytes</code>) &#x2014;
A buffer containing a previously serialized <a href="/docs/tokenizers/pr_1/en/api/tokenizer#tokenizers.Tokenizer">Tokenizer</a>`,name:"buffer"}],returnDescription:`
<p>The new tokenizer</p>
`,returnType:`
<p><a href="/docs/tokenizers/pr_1/en/api/tokenizer#tokenizers.Tokenizer">Tokenizer</a></p>
`}}),Ae=new w({props:{name:"from_file",anchor:"tokenizers.Tokenizer.from_file",parameters:[{name:"path",val:""}],parametersDescription:[{anchor:"tokenizers.Tokenizer.from_file.path",description:`<strong>path</strong> (<code>str</code>) &#x2014;
A path to a local JSON file representing a previously serialized
<a href="/docs/tokenizers/pr_1/en/api/tokenizer#tokenizers.Tokenizer">Tokenizer</a>`,name:"path"}],returnDescription:`
<p>The new tokenizer</p>
`,returnType:`
<p><a href="/docs/tokenizers/pr_1/en/api/tokenizer#tokenizers.Tokenizer">Tokenizer</a></p>
`}}),Ie=new w({props:{name:"from_pretrained",anchor:"tokenizers.Tokenizer.from_pretrained",parameters:[{name:"identifier",val:""},{name:"revision",val:" = 'main'"},{name:"auth_token",val:" = None"}],parametersDescription:[{anchor:"tokenizers.Tokenizer.from_pretrained.identifier",description:`<strong>identifier</strong> (<code>str</code>) &#x2014;
The identifier of a Model on the Hugging Face Hub, that contains
a tokenizer.json file`,name:"identifier"},{anchor:"tokenizers.Tokenizer.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, defaults to <em>main</em>) &#x2014;
A branch or commit id`,name:"revision"},{anchor:"tokenizers.Tokenizer.from_pretrained.auth_token",description:`<strong>auth_token</strong> (<code>str</code>, <em>optional</em>, defaults to <em>None</em>) &#x2014;
An optional auth token used to access private repositories on the
Hugging Face Hub`,name:"auth_token"}],returnDescription:`
<p>The new tokenizer</p>
`,returnType:`
<p><a href="/docs/tokenizers/pr_1/en/api/tokenizer#tokenizers.Tokenizer">Tokenizer</a></p>
`}}),je=new w({props:{name:"from_str",anchor:"tokenizers.Tokenizer.from_str",parameters:[{name:"json",val:""}],parametersDescription:[{anchor:"tokenizers.Tokenizer.from_str.json",description:`<strong>json</strong> (<code>str</code>) &#x2014;
A valid JSON string representing a previously serialized
<a href="/docs/tokenizers/pr_1/en/api/tokenizer#tokenizers.Tokenizer">Tokenizer</a>`,name:"json"}],returnDescription:`
<p>The new tokenizer</p>
`,returnType:`
<p><a href="/docs/tokenizers/pr_1/en/api/tokenizer#tokenizers.Tokenizer">Tokenizer</a></p>
`}}),Ve=new w({props:{name:"get_vocab",anchor:"tokenizers.Tokenizer.get_vocab",parameters:[{name:"with_added_tokens",val:" = True"}],parametersDescription:[{anchor:"tokenizers.Tokenizer.get_vocab.with_added_tokens",description:`<strong>with_added_tokens</strong> (<code>bool</code>, defaults to <code>True</code>) &#x2014;
Whether to include the added tokens`,name:"with_added_tokens"}],returnDescription:`
<p>The vocabulary</p>
`,returnType:`
<p><code>Dict[str, int]</code></p>
`}}),Ne=new w({props:{name:"get_vocab_size",anchor:"tokenizers.Tokenizer.get_vocab_size",parameters:[{name:"with_added_tokens",val:" = True"}],parametersDescription:[{anchor:"tokenizers.Tokenizer.get_vocab_size.with_added_tokens",description:`<strong>with_added_tokens</strong> (<code>bool</code>, defaults to <code>True</code>) &#x2014;
Whether to include the added tokens`,name:"with_added_tokens"}],returnDescription:`
<p>The size of the vocabulary</p>
`,returnType:`
<p><code>int</code></p>
`}}),Se=new w({props:{name:"id_to_token",anchor:"tokenizers.Tokenizer.id_to_token",parameters:[{name:"id",val:""}],parametersDescription:[{anchor:"tokenizers.Tokenizer.id_to_token.id",description:`<strong>id</strong> (<code>int</code>) &#x2014;
The id to convert`,name:"id"}],returnDescription:`
<p>An optional token, <code>None</code> if out of vocabulary</p>
`,returnType:`
<p><code>Optional[str]</code></p>
`}}),Ce=new w({props:{name:"no_padding",anchor:"tokenizers.Tokenizer.no_padding",parameters:[]}}),Oe=new w({props:{name:"no_truncation",anchor:"tokenizers.Tokenizer.no_truncation",parameters:[]}}),Fe=new w({props:{name:"num_special_tokens_to_add",anchor:"tokenizers.Tokenizer.num_special_tokens_to_add",parameters:[{name:"is_pair",val:""}]}}),He=new w({props:{name:"post_process",anchor:"tokenizers.Tokenizer.post_process",parameters:[{name:"encoding",val:""},{name:"pair",val:" = None"},{name:"add_special_tokens",val:" = True"}],parametersDescription:[{anchor:"tokenizers.Tokenizer.post_process.encoding",description:`<strong>encoding</strong> (<a href="/docs/tokenizers/pr_1/en/api/encoding#tokenizers.Encoding">Encoding</a>) &#x2014;
The <a href="/docs/tokenizers/pr_1/en/api/encoding#tokenizers.Encoding">Encoding</a> corresponding to the main sequence.`,name:"encoding"},{anchor:"tokenizers.Tokenizer.post_process.pair",description:`<strong>pair</strong> (<a href="/docs/tokenizers/pr_1/en/api/encoding#tokenizers.Encoding">Encoding</a>, <em>optional</em>) &#x2014;
An optional <a href="/docs/tokenizers/pr_1/en/api/encoding#tokenizers.Encoding">Encoding</a> corresponding to the pair sequence.`,name:"pair"},{anchor:"tokenizers.Tokenizer.post_process.add_special_tokens",description:`<strong>add_special_tokens</strong> (<code>bool</code>) &#x2014;
Whether to add the special tokens`,name:"add_special_tokens"}],returnDescription:`
<p>The final post-processed encoding</p>
`,returnType:`
<p><a href="/docs/tokenizers/pr_1/en/api/encoding#tokenizers.Encoding">Encoding</a></p>
`}}),Re=new w({props:{name:"save",anchor:"tokenizers.Tokenizer.save",parameters:[{name:"pretty",val:" = True"}],parametersDescription:[{anchor:"tokenizers.Tokenizer.save.path",description:`<strong>path</strong> (<code>str</code>) &#x2014;
A path to a file in which to save the serialized tokenizer.`,name:"path"},{anchor:"tokenizers.Tokenizer.save.pretty",description:`<strong>pretty</strong> (<code>bool</code>, defaults to <code>True</code>) &#x2014;
Whether the JSON file should be pretty formatted.`,name:"pretty"}]}}),Ge=new w({props:{name:"to_str",anchor:"tokenizers.Tokenizer.to_str",parameters:[{name:"pretty",val:" = False"}],parametersDescription:[{anchor:"tokenizers.Tokenizer.to_str.pretty",description:`<strong>pretty</strong> (<code>bool</code>, defaults to <code>False</code>) &#x2014;
Whether the JSON string should be pretty formatted.`,name:"pretty"}],returnDescription:`
<p>A string representing the serialized Tokenizer</p>
`,returnType:`
<p><code>str</code></p>
`}}),Ue=new w({props:{name:"token_to_id",anchor:"tokenizers.Tokenizer.token_to_id",parameters:[{name:"token",val:""}],parametersDescription:[{anchor:"tokenizers.Tokenizer.token_to_id.token",description:`<strong>token</strong> (<code>str</code>) &#x2014;
The token to convert`,name:"token"}],returnDescription:`
<p>An optional id, <code>None</code> if out of vocabulary</p>
`,returnType:`
<p><code>Optional[int]</code></p>
`}}),Ye=new w({props:{name:"train",anchor:"tokenizers.Tokenizer.train",parameters:[{name:"files",val:""},{name:"trainer",val:" = None"}],parametersDescription:[{anchor:"tokenizers.Tokenizer.train.files",description:`<strong>files</strong> (<code>List[str]</code>) &#x2014;
A list of path to the files that we should use for training`,name:"files"},{anchor:"tokenizers.Tokenizer.train.trainer",description:`<strong>trainer</strong> (<code>~tokenizers.trainers.Trainer</code>, <em>optional</em>) &#x2014;
An optional trainer that should be used to train our Model`,name:"trainer"}]}}),Ke=new w({props:{name:"train_from_iterator",anchor:"tokenizers.Tokenizer.train_from_iterator",parameters:[{name:"iterator",val:""},{name:"trainer",val:" = None"},{name:"length",val:" = None"}],parametersDescription:[{anchor:"tokenizers.Tokenizer.train_from_iterator.iterator",description:`<strong>iterator</strong> (<code>Iterator</code>) &#x2014;
Any iterator over strings or list of strings`,name:"iterator"},{anchor:"tokenizers.Tokenizer.train_from_iterator.trainer",description:`<strong>trainer</strong> (<code>~tokenizers.trainers.Trainer</code>, <em>optional</em>) &#x2014;
An optional trainer that should be used to train our Model`,name:"trainer"},{anchor:"tokenizers.Tokenizer.train_from_iterator.length",description:`<strong>length</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The total number of sequences in the iterator. This is used to
provide meaningful progress tracking`,name:"length"}]}}),{c(){l=n("h2"),m=n("a"),p=n("span"),k(f.$$.fragment),E=i(),y=n("span"),x=a("Tokenizer"),D=i(),h=n("div"),k(P.$$.fragment),j=i(),A=n("p"),b=a("A "),q=n("code"),O=a("Tokenizer"),Qe=a(` works as a pipeline. It processes some raw text as input
and outputs an `),R=n("a"),Xe=a("Encoding"),$n=a("."),Tn=i(),L=n("div"),k(ke.$$.fragment),bn=i(),gt=n("p"),wn=a("Add the given special tokens to the Tokenizer."),En=i(),ut=n("p"),yn=a(`If these tokens are already part of the vocabulary, it just let the Tokenizer know about
them. If they don\u2019t exist, the Tokenizer creates them, giving them a new id.`),qn=i(),ft=n("p"),xn=a(`These special tokens will never be processed by the model (ie won\u2019t be split into
multiple tokens), and they can be removed from the output when decoding.`),An=i(),H=n("div"),k(_e.$$.fragment),Dn=i(),mt=n("p"),In=a("Add the given tokens to the vocabulary"),Pn=i(),kt=n("p"),jn=a(`The given tokens are added only if they don\u2019t already exist in the vocabulary.
Each token then gets a new attributed id.`),Ln=i(),M=n("div"),k(ze.$$.fragment),Vn=i(),_t=n("p"),Nn=a("Decode the given list of ids back to a string"),Sn=i(),zt=n("p"),Cn=a("This is used to decode anything coming back from a Language Model"),On=i(),G=n("div"),k(ve.$$.fragment),Fn=i(),vt=n("p"),Hn=a("Decode a batch of ids back to their corresponding string"),Mn=i(),B=n("div"),k($e.$$.fragment),Wn=i(),$t=n("p"),Rn=a("Enable the padding"),Jn=i(),U=n("div"),k(Te.$$.fragment),Gn=i(),Tt=n("p"),Bn=a("Enable truncation"),Un=i(),V=n("div"),k(be.$$.fragment),Yn=i(),bt=n("p"),Kn=a(`Encode the given sequence and pair. This method can process raw text sequences
as well as already pre-tokenized sequences.`),Qn=i(),wt=n("p"),Xn=a(`Example:
Here are some examples of the inputs that are accepted:`),Zn=i(),k(we.$$.fragment),eo=i(),N=n("div"),k(Ee.$$.fragment),to=i(),Et=n("p"),no=a(`Encode the given batch of inputs. This method accept both raw text sequences
as well as already pre-tokenized sequences.`),oo=i(),yt=n("p"),ro=a(`Example:
Here are some examples of the inputs that are accepted:`),ao=i(),k(ye.$$.fragment),so=i(),Y=n("div"),k(qe.$$.fragment),io=i(),xe=n("p"),co=a("Instantiate a new "),Ze=n("a"),po=a("Tokenizer"),lo=a(" from the given buffer."),ho=i(),K=n("div"),k(Ae.$$.fragment),go=i(),De=n("p"),uo=a("Instantiate a new "),et=n("a"),fo=a("Tokenizer"),mo=a(" from the file at the given path."),ko=i(),Q=n("div"),k(Ie.$$.fragment),_o=i(),Pe=n("p"),zo=a("Instantiate a new "),tt=n("a"),vo=a("Tokenizer"),$o=a(` from an existing file on the
Hugging Face Hub.`),To=i(),X=n("div"),k(je.$$.fragment),bo=i(),Le=n("p"),wo=a("Instantiate a new "),nt=n("a"),Eo=a("Tokenizer"),yo=a(" from the given JSON string."),qo=i(),Z=n("div"),k(Ve.$$.fragment),xo=i(),qt=n("p"),Ao=a("Get the underlying vocabulary"),Do=i(),ee=n("div"),k(Ne.$$.fragment),Io=i(),xt=n("p"),Po=a("Get the size of the underlying vocabulary"),jo=i(),te=n("div"),k(Se.$$.fragment),Lo=i(),At=n("p"),Vo=a("Convert the given id to its corresponding token if it exists"),No=i(),ne=n("div"),k(Ce.$$.fragment),So=i(),Dt=n("p"),Co=a("Disable padding"),Oo=i(),oe=n("div"),k(Oe.$$.fragment),Fo=i(),It=n("p"),Ho=a("Disable truncation"),Mo=i(),re=n("div"),k(Fe.$$.fragment),Wo=i(),Pt=n("p"),Ro=a(`Return the number of special tokens that would be added for single/pair sentences.
:param is_pair: Boolean indicating if the input would be a single sentence or a pair
:return:`),Jo=i(),S=n("div"),k(He.$$.fragment),Go=i(),jt=n("p"),Bo=a("Apply all the post-processing steps to the given encodings."),Uo=i(),Lt=n("p"),Yo=a("The various steps are:"),Ko=i(),J=n("ol"),Me=n("li"),Qo=a(`Truncate according to the set truncation params (provided with
`),Vt=n("code"),Xo=a("enable_truncation()"),Zo=a(")"),er=i(),ot=n("li"),tr=a("Apply the "),Nt=n("code"),nr=a("PostProcessor"),or=i(),We=n("li"),rr=a(`Pad according to the set padding params (provided with
`),St=n("code"),ar=a("enable_padding()"),sr=a(")"),ir=i(),ae=n("div"),k(Re.$$.fragment),dr=i(),Je=n("p"),cr=a("Save the "),rt=n("a"),pr=a("Tokenizer"),lr=a(" to the file at the given path."),hr=i(),se=n("div"),k(Ge.$$.fragment),gr=i(),Be=n("p"),ur=a("Gets a serialized string representing this "),at=n("a"),fr=a("Tokenizer"),mr=a("."),kr=i(),ie=n("div"),k(Ue.$$.fragment),_r=i(),Ct=n("p"),zr=a("Convert the given token to its corresponding id if it exists"),vr=i(),W=n("div"),k(Ye.$$.fragment),$r=i(),Ot=n("p"),Tr=a("Train the Tokenizer using the given files."),br=i(),st=n("p"),wr=a(`Reads the files line by line, while keeping all the whitespace, even new lines.
If you want to train from data store in-memory, you can check
`),Ft=n("code"),Er=a("train_from_iterator()"),yr=i(),C=n("div"),k(Ke.$$.fragment),qr=i(),Ht=n("p"),xr=a("Train the Tokenizer using the provided iterator."),Ar=i(),Mt=n("p"),Dr=a("You can provide anything that is a Python Iterator"),Ir=i(),F=n("ul"),it=n("li"),Pr=a("A list of sequences "),Wt=n("code"),jr=a("List[str]"),Lr=i(),de=n("li"),Vr=a("A generator that yields "),Rt=n("code"),Nr=a("str"),Sr=a(" or "),Jt=n("code"),Cr=a("List[str]"),Or=i(),Gt=n("li"),Fr=a("A Numpy array of strings"),Hr=i(),Bt=n("li"),Mr=a("\u2026"),this.h()},l(c){l=o(c,"H2",{class:!0});var ce=r(l);m=o(ce,"A",{id:!0,class:!0,href:!0});var Br=r(m);p=o(Br,"SPAN",{});var Ur=r(p);_(f.$$.fragment,Ur),Ur.forEach(t),Br.forEach(t),E=d(ce),y=o(ce,"SPAN",{});var Yr=r(y);x=s(Yr,"Tokenizer"),Yr.forEach(t),ce.forEach(t),D=d(c),h=o(c,"DIV",{class:!0});var g=r(h);_(P.$$.fragment,g),j=d(g),A=o(g,"P",{});var dt=r(A);b=s(dt,"A "),q=o(dt,"CODE",{});var Kr=r(q);O=s(Kr,"Tokenizer"),Kr.forEach(t),Qe=s(dt,` works as a pipeline. It processes some raw text as input
and outputs an `),R=o(dt,"A",{href:!0});var Qr=r(R);Xe=s(Qr,"Encoding"),Qr.forEach(t),$n=s(dt,"."),dt.forEach(t),Tn=d(g),L=o(g,"DIV",{class:!0});var pe=r(L);_(ke.$$.fragment,pe),bn=d(pe),gt=o(pe,"P",{});var Xr=r(gt);wn=s(Xr,"Add the given special tokens to the Tokenizer."),Xr.forEach(t),En=d(pe),ut=o(pe,"P",{});var Zr=r(ut);yn=s(Zr,`If these tokens are already part of the vocabulary, it just let the Tokenizer know about
them. If they don\u2019t exist, the Tokenizer creates them, giving them a new id.`),Zr.forEach(t),qn=d(pe),ft=o(pe,"P",{});var ea=r(ft);xn=s(ea,`These special tokens will never be processed by the model (ie won\u2019t be split into
multiple tokens), and they can be removed from the output when decoding.`),ea.forEach(t),pe.forEach(t),An=d(g),H=o(g,"DIV",{class:!0});var ct=r(H);_(_e.$$.fragment,ct),Dn=d(ct),mt=o(ct,"P",{});var ta=r(mt);In=s(ta,"Add the given tokens to the vocabulary"),ta.forEach(t),Pn=d(ct),kt=o(ct,"P",{});var na=r(kt);jn=s(na,`The given tokens are added only if they don\u2019t already exist in the vocabulary.
Each token then gets a new attributed id.`),na.forEach(t),ct.forEach(t),Ln=d(g),M=o(g,"DIV",{class:!0});var pt=r(M);_(ze.$$.fragment,pt),Vn=d(pt),_t=o(pt,"P",{});var oa=r(_t);Nn=s(oa,"Decode the given list of ids back to a string"),oa.forEach(t),Sn=d(pt),zt=o(pt,"P",{});var ra=r(zt);Cn=s(ra,"This is used to decode anything coming back from a Language Model"),ra.forEach(t),pt.forEach(t),On=d(g),G=o(g,"DIV",{class:!0});var Kt=r(G);_(ve.$$.fragment,Kt),Fn=d(Kt),vt=o(Kt,"P",{});var aa=r(vt);Hn=s(aa,"Decode a batch of ids back to their corresponding string"),aa.forEach(t),Kt.forEach(t),Mn=d(g),B=o(g,"DIV",{class:!0});var Qt=r(B);_($e.$$.fragment,Qt),Wn=d(Qt),$t=o(Qt,"P",{});var sa=r($t);Rn=s(sa,"Enable the padding"),sa.forEach(t),Qt.forEach(t),Jn=d(g),U=o(g,"DIV",{class:!0});var Xt=r(U);_(Te.$$.fragment,Xt),Gn=d(Xt),Tt=o(Xt,"P",{});var ia=r(Tt);Bn=s(ia,"Enable truncation"),ia.forEach(t),Xt.forEach(t),Un=d(g),V=o(g,"DIV",{class:!0});var le=r(V);_(be.$$.fragment,le),Yn=d(le),bt=o(le,"P",{});var da=r(bt);Kn=s(da,`Encode the given sequence and pair. This method can process raw text sequences
as well as already pre-tokenized sequences.`),da.forEach(t),Qn=d(le),wt=o(le,"P",{});var ca=r(wt);Xn=s(ca,`Example:
Here are some examples of the inputs that are accepted:`),ca.forEach(t),Zn=d(le),_(we.$$.fragment,le),le.forEach(t),eo=d(g),N=o(g,"DIV",{class:!0});var he=r(N);_(Ee.$$.fragment,he),to=d(he),Et=o(he,"P",{});var pa=r(Et);no=s(pa,`Encode the given batch of inputs. This method accept both raw text sequences
as well as already pre-tokenized sequences.`),pa.forEach(t),oo=d(he),yt=o(he,"P",{});var la=r(yt);ro=s(la,`Example:
Here are some examples of the inputs that are accepted:`),la.forEach(t),ao=d(he),_(ye.$$.fragment,he),he.forEach(t),so=d(g),Y=o(g,"DIV",{class:!0});var Zt=r(Y);_(qe.$$.fragment,Zt),io=d(Zt),xe=o(Zt,"P",{});var en=r(xe);co=s(en,"Instantiate a new "),Ze=o(en,"A",{href:!0});var ha=r(Ze);po=s(ha,"Tokenizer"),ha.forEach(t),lo=s(en," from the given buffer."),en.forEach(t),Zt.forEach(t),ho=d(g),K=o(g,"DIV",{class:!0});var tn=r(K);_(Ae.$$.fragment,tn),go=d(tn),De=o(tn,"P",{});var nn=r(De);uo=s(nn,"Instantiate a new "),et=o(nn,"A",{href:!0});var ga=r(et);fo=s(ga,"Tokenizer"),ga.forEach(t),mo=s(nn," from the file at the given path."),nn.forEach(t),tn.forEach(t),ko=d(g),Q=o(g,"DIV",{class:!0});var on=r(Q);_(Ie.$$.fragment,on),_o=d(on),Pe=o(on,"P",{});var rn=r(Pe);zo=s(rn,"Instantiate a new "),tt=o(rn,"A",{href:!0});var ua=r(tt);vo=s(ua,"Tokenizer"),ua.forEach(t),$o=s(rn,` from an existing file on the
Hugging Face Hub.`),rn.forEach(t),on.forEach(t),To=d(g),X=o(g,"DIV",{class:!0});var an=r(X);_(je.$$.fragment,an),bo=d(an),Le=o(an,"P",{});var sn=r(Le);wo=s(sn,"Instantiate a new "),nt=o(sn,"A",{href:!0});var fa=r(nt);Eo=s(fa,"Tokenizer"),fa.forEach(t),yo=s(sn," from the given JSON string."),sn.forEach(t),an.forEach(t),qo=d(g),Z=o(g,"DIV",{class:!0});var dn=r(Z);_(Ve.$$.fragment,dn),xo=d(dn),qt=o(dn,"P",{});var ma=r(qt);Ao=s(ma,"Get the underlying vocabulary"),ma.forEach(t),dn.forEach(t),Do=d(g),ee=o(g,"DIV",{class:!0});var cn=r(ee);_(Ne.$$.fragment,cn),Io=d(cn),xt=o(cn,"P",{});var ka=r(xt);Po=s(ka,"Get the size of the underlying vocabulary"),ka.forEach(t),cn.forEach(t),jo=d(g),te=o(g,"DIV",{class:!0});var pn=r(te);_(Se.$$.fragment,pn),Lo=d(pn),At=o(pn,"P",{});var _a=r(At);Vo=s(_a,"Convert the given id to its corresponding token if it exists"),_a.forEach(t),pn.forEach(t),No=d(g),ne=o(g,"DIV",{class:!0});var ln=r(ne);_(Ce.$$.fragment,ln),So=d(ln),Dt=o(ln,"P",{});var za=r(Dt);Co=s(za,"Disable padding"),za.forEach(t),ln.forEach(t),Oo=d(g),oe=o(g,"DIV",{class:!0});var hn=r(oe);_(Oe.$$.fragment,hn),Fo=d(hn),It=o(hn,"P",{});var va=r(It);Ho=s(va,"Disable truncation"),va.forEach(t),hn.forEach(t),Mo=d(g),re=o(g,"DIV",{class:!0});var gn=r(re);_(Fe.$$.fragment,gn),Wo=d(gn),Pt=o(gn,"P",{});var $a=r(Pt);Ro=s($a,`Return the number of special tokens that would be added for single/pair sentences.
:param is_pair: Boolean indicating if the input would be a single sentence or a pair
:return:`),$a.forEach(t),gn.forEach(t),Jo=d(g),S=o(g,"DIV",{class:!0});var ge=r(S);_(He.$$.fragment,ge),Go=d(ge),jt=o(ge,"P",{});var Ta=r(jt);Bo=s(Ta,"Apply all the post-processing steps to the given encodings."),Ta.forEach(t),Uo=d(ge),Lt=o(ge,"P",{});var ba=r(Lt);Yo=s(ba,"The various steps are:"),ba.forEach(t),Ko=d(ge),J=o(ge,"OL",{});var lt=r(J);Me=o(lt,"LI",{});var un=r(Me);Qo=s(un,`Truncate according to the set truncation params (provided with
`),Vt=o(un,"CODE",{});var wa=r(Vt);Xo=s(wa,"enable_truncation()"),wa.forEach(t),Zo=s(un,")"),un.forEach(t),er=d(lt),ot=o(lt,"LI",{});var Wr=r(ot);tr=s(Wr,"Apply the "),Nt=o(Wr,"CODE",{});var Ea=r(Nt);nr=s(Ea,"PostProcessor"),Ea.forEach(t),Wr.forEach(t),or=d(lt),We=o(lt,"LI",{});var fn=r(We);rr=s(fn,`Pad according to the set padding params (provided with
`),St=o(fn,"CODE",{});var ya=r(St);ar=s(ya,"enable_padding()"),ya.forEach(t),sr=s(fn,")"),fn.forEach(t),lt.forEach(t),ge.forEach(t),ir=d(g),ae=o(g,"DIV",{class:!0});var mn=r(ae);_(Re.$$.fragment,mn),dr=d(mn),Je=o(mn,"P",{});var kn=r(Je);cr=s(kn,"Save the "),rt=o(kn,"A",{href:!0});var qa=r(rt);pr=s(qa,"Tokenizer"),qa.forEach(t),lr=s(kn," to the file at the given path."),kn.forEach(t),mn.forEach(t),hr=d(g),se=o(g,"DIV",{class:!0});var _n=r(se);_(Ge.$$.fragment,_n),gr=d(_n),Be=o(_n,"P",{});var zn=r(Be);ur=s(zn,"Gets a serialized string representing this "),at=o(zn,"A",{href:!0});var xa=r(at);fr=s(xa,"Tokenizer"),xa.forEach(t),mr=s(zn,"."),zn.forEach(t),_n.forEach(t),kr=d(g),ie=o(g,"DIV",{class:!0});var vn=r(ie);_(Ue.$$.fragment,vn),_r=d(vn),Ct=o(vn,"P",{});var Aa=r(Ct);zr=s(Aa,"Convert the given token to its corresponding id if it exists"),Aa.forEach(t),vn.forEach(t),vr=d(g),W=o(g,"DIV",{class:!0});var ht=r(W);_(Ye.$$.fragment,ht),$r=d(ht),Ot=o(ht,"P",{});var Da=r(Ot);Tr=s(Da,"Train the Tokenizer using the given files."),Da.forEach(t),br=d(ht),st=o(ht,"P",{});var Rr=r(st);wr=s(Rr,`Reads the files line by line, while keeping all the whitespace, even new lines.
If you want to train from data store in-memory, you can check
`),Ft=o(Rr,"CODE",{});var Ia=r(Ft);Er=s(Ia,"train_from_iterator()"),Ia.forEach(t),Rr.forEach(t),ht.forEach(t),yr=d(g),C=o(g,"DIV",{class:!0});var ue=r(C);_(Ke.$$.fragment,ue),qr=d(ue),Ht=o(ue,"P",{});var Pa=r(Ht);xr=s(Pa,"Train the Tokenizer using the provided iterator."),Pa.forEach(t),Ar=d(ue),Mt=o(ue,"P",{});var ja=r(Mt);Dr=s(ja,"You can provide anything that is a Python Iterator"),ja.forEach(t),Ir=d(ue),F=o(ue,"UL",{});var fe=r(F);it=o(fe,"LI",{});var Jr=r(it);Pr=s(Jr,"A list of sequences "),Wt=o(Jr,"CODE",{});var La=r(Wt);jr=s(La,"List[str]"),La.forEach(t),Jr.forEach(t),Lr=d(fe),de=o(fe,"LI",{});var Ut=r(de);Vr=s(Ut,"A generator that yields "),Rt=o(Ut,"CODE",{});var Va=r(Rt);Nr=s(Va,"str"),Va.forEach(t),Sr=s(Ut," or "),Jt=o(Ut,"CODE",{});var Na=r(Jt);Cr=s(Na,"List[str]"),Na.forEach(t),Ut.forEach(t),Or=d(fe),Gt=o(fe,"LI",{});var Sa=r(Gt);Fr=s(Sa,"A Numpy array of strings"),Sa.forEach(t),Hr=d(fe),Bt=o(fe,"LI",{});var Ca=r(Bt);Mr=s(Ca,"\u2026"),Ca.forEach(t),fe.forEach(t),ue.forEach(t),g.forEach(t),this.h()},h(){u(m,"id","tokenizers.Tokenizer]][[tokenizers.Tokenizer"),u(m,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(m,"href","#tokenizers.Tokenizer]][[tokenizers.Tokenizer"),u(l,"class","relative group"),u(R,"href","/docs/tokenizers/pr_1/en/api/encoding#tokenizers.Encoding"),u(L,"class","docstring"),u(H,"class","docstring"),u(M,"class","docstring"),u(G,"class","docstring"),u(B,"class","docstring"),u(U,"class","docstring"),u(V,"class","docstring"),u(N,"class","docstring"),u(Ze,"href","/docs/tokenizers/pr_1/en/api/tokenizer#tokenizers.Tokenizer"),u(Y,"class","docstring"),u(et,"href","/docs/tokenizers/pr_1/en/api/tokenizer#tokenizers.Tokenizer"),u(K,"class","docstring"),u(tt,"href","/docs/tokenizers/pr_1/en/api/tokenizer#tokenizers.Tokenizer"),u(Q,"class","docstring"),u(nt,"href","/docs/tokenizers/pr_1/en/api/tokenizer#tokenizers.Tokenizer"),u(X,"class","docstring"),u(Z,"class","docstring"),u(ee,"class","docstring"),u(te,"class","docstring"),u(ne,"class","docstring"),u(oe,"class","docstring"),u(re,"class","docstring"),u(S,"class","docstring"),u(rt,"href","/docs/tokenizers/pr_1/en/api/tokenizer#tokenizers.Tokenizer"),u(ae,"class","docstring"),u(at,"href","/docs/tokenizers/pr_1/en/api/tokenizer#tokenizers.Tokenizer"),u(se,"class","docstring"),u(ie,"class","docstring"),u(W,"class","docstring"),u(C,"class","docstring"),u(h,"class","docstring")},m(c,ce){me(c,l,ce),e(l,m),e(m,p),z(f,p,null),e(l,E),e(l,y),e(y,x),me(c,D,ce),me(c,h,ce),z(P,h,null),e(h,j),e(h,A),e(A,b),e(A,q),e(q,O),e(A,Qe),e(A,R),e(R,Xe),e(A,$n),e(h,Tn),e(h,L),z(ke,L,null),e(L,bn),e(L,gt),e(gt,wn),e(L,En),e(L,ut),e(ut,yn),e(L,qn),e(L,ft),e(ft,xn),e(h,An),e(h,H),z(_e,H,null),e(H,Dn),e(H,mt),e(mt,In),e(H,Pn),e(H,kt),e(kt,jn),e(h,Ln),e(h,M),z(ze,M,null),e(M,Vn),e(M,_t),e(_t,Nn),e(M,Sn),e(M,zt),e(zt,Cn),e(h,On),e(h,G),z(ve,G,null),e(G,Fn),e(G,vt),e(vt,Hn),e(h,Mn),e(h,B),z($e,B,null),e(B,Wn),e(B,$t),e($t,Rn),e(h,Jn),e(h,U),z(Te,U,null),e(U,Gn),e(U,Tt),e(Tt,Bn),e(h,Un),e(h,V),z(be,V,null),e(V,Yn),e(V,bt),e(bt,Kn),e(V,Qn),e(V,wt),e(wt,Xn),e(V,Zn),z(we,V,null),e(h,eo),e(h,N),z(Ee,N,null),e(N,to),e(N,Et),e(Et,no),e(N,oo),e(N,yt),e(yt,ro),e(N,ao),z(ye,N,null),e(h,so),e(h,Y),z(qe,Y,null),e(Y,io),e(Y,xe),e(xe,co),e(xe,Ze),e(Ze,po),e(xe,lo),e(h,ho),e(h,K),z(Ae,K,null),e(K,go),e(K,De),e(De,uo),e(De,et),e(et,fo),e(De,mo),e(h,ko),e(h,Q),z(Ie,Q,null),e(Q,_o),e(Q,Pe),e(Pe,zo),e(Pe,tt),e(tt,vo),e(Pe,$o),e(h,To),e(h,X),z(je,X,null),e(X,bo),e(X,Le),e(Le,wo),e(Le,nt),e(nt,Eo),e(Le,yo),e(h,qo),e(h,Z),z(Ve,Z,null),e(Z,xo),e(Z,qt),e(qt,Ao),e(h,Do),e(h,ee),z(Ne,ee,null),e(ee,Io),e(ee,xt),e(xt,Po),e(h,jo),e(h,te),z(Se,te,null),e(te,Lo),e(te,At),e(At,Vo),e(h,No),e(h,ne),z(Ce,ne,null),e(ne,So),e(ne,Dt),e(Dt,Co),e(h,Oo),e(h,oe),z(Oe,oe,null),e(oe,Fo),e(oe,It),e(It,Ho),e(h,Mo),e(h,re),z(Fe,re,null),e(re,Wo),e(re,Pt),e(Pt,Ro),e(h,Jo),e(h,S),z(He,S,null),e(S,Go),e(S,jt),e(jt,Bo),e(S,Uo),e(S,Lt),e(Lt,Yo),e(S,Ko),e(S,J),e(J,Me),e(Me,Qo),e(Me,Vt),e(Vt,Xo),e(Me,Zo),e(J,er),e(J,ot),e(ot,tr),e(ot,Nt),e(Nt,nr),e(J,or),e(J,We),e(We,rr),e(We,St),e(St,ar),e(We,sr),e(h,ir),e(h,ae),z(Re,ae,null),e(ae,dr),e(ae,Je),e(Je,cr),e(Je,rt),e(rt,pr),e(Je,lr),e(h,hr),e(h,se),z(Ge,se,null),e(se,gr),e(se,Be),e(Be,ur),e(Be,at),e(at,fr),e(Be,mr),e(h,kr),e(h,ie),z(Ue,ie,null),e(ie,_r),e(ie,Ct),e(Ct,zr),e(h,vr),e(h,W),z(Ye,W,null),e(W,$r),e(W,Ot),e(Ot,Tr),e(W,br),e(W,st),e(st,wr),e(st,Ft),e(Ft,Er),e(h,yr),e(h,C),z(Ke,C,null),e(C,qr),e(C,Ht),e(Ht,xr),e(C,Ar),e(C,Mt),e(Mt,Dr),e(C,Ir),e(C,F),e(F,it),e(it,Pr),e(it,Wt),e(Wt,jr),e(F,Lr),e(F,de),e(de,Vr),e(de,Rt),e(Rt,Nr),e(de,Sr),e(de,Jt),e(Jt,Cr),e(F,Or),e(F,Gt),e(Gt,Fr),e(F,Hr),e(F,Bt),e(Bt,Mr),Yt=!0},p:Ga,i(c){Yt||(v(f.$$.fragment,c),v(P.$$.fragment,c),v(ke.$$.fragment,c),v(_e.$$.fragment,c),v(ze.$$.fragment,c),v(ve.$$.fragment,c),v($e.$$.fragment,c),v(Te.$$.fragment,c),v(be.$$.fragment,c),v(we.$$.fragment,c),v(Ee.$$.fragment,c),v(ye.$$.fragment,c),v(qe.$$.fragment,c),v(Ae.$$.fragment,c),v(Ie.$$.fragment,c),v(je.$$.fragment,c),v(Ve.$$.fragment,c),v(Ne.$$.fragment,c),v(Se.$$.fragment,c),v(Ce.$$.fragment,c),v(Oe.$$.fragment,c),v(Fe.$$.fragment,c),v(He.$$.fragment,c),v(Re.$$.fragment,c),v(Ge.$$.fragment,c),v(Ue.$$.fragment,c),v(Ye.$$.fragment,c),v(Ke.$$.fragment,c),Yt=!0)},o(c){$(f.$$.fragment,c),$(P.$$.fragment,c),$(ke.$$.fragment,c),$(_e.$$.fragment,c),$(ze.$$.fragment,c),$(ve.$$.fragment,c),$($e.$$.fragment,c),$(Te.$$.fragment,c),$(be.$$.fragment,c),$(we.$$.fragment,c),$(Ee.$$.fragment,c),$(ye.$$.fragment,c),$(qe.$$.fragment,c),$(Ae.$$.fragment,c),$(Ie.$$.fragment,c),$(je.$$.fragment,c),$(Ve.$$.fragment,c),$(Ne.$$.fragment,c),$(Se.$$.fragment,c),$(Ce.$$.fragment,c),$(Oe.$$.fragment,c),$(Fe.$$.fragment,c),$(He.$$.fragment,c),$(Re.$$.fragment,c),$(Ge.$$.fragment,c),$(Ue.$$.fragment,c),$(Ye.$$.fragment,c),$(Ke.$$.fragment,c),Yt=!1},d(c){c&&t(l),T(f),c&&t(D),c&&t(h),T(P),T(ke),T(_e),T(ze),T(ve),T($e),T(Te),T(be),T(we),T(Ee),T(ye),T(qe),T(Ae),T(Ie),T(je),T(Ve),T(Ne),T(Se),T(Ce),T(Oe),T(Fe),T(He),T(Re),T(Ge),T(Ue),T(Ye),T(Ke)}}}function Ya(I){let l,m;return l=new Gr({props:{$$slots:{default:[Ua]},$$scope:{ctx:I}}}),{c(){k(l.$$.fragment)},l(p){_(l.$$.fragment,p)},m(p,f){z(l,p,f),m=!0},p(p,f){const E={};f&2&&(E.$$scope={dirty:f,ctx:p}),l.$set(E)},i(p){m||(v(l.$$.fragment,p),m=!0)},o(p){$(l.$$.fragment,p),m=!1},d(p){T(l,p)}}}function Ka(I){let l,m,p,f,E;return{c(){l=n("p"),m=a("The Rust API Reference is available directly on the "),p=n("a"),f=a("Docs.rs"),E=a(" website."),this.h()},l(y){l=o(y,"P",{});var x=r(l);m=s(x,"The Rust API Reference is available directly on the "),p=o(x,"A",{href:!0,rel:!0});var D=r(p);f=s(D,"Docs.rs"),D.forEach(t),E=s(x," website."),x.forEach(t),this.h()},h(){u(p,"href","https://docs.rs/tokenizers/latest/tokenizers/"),u(p,"rel","nofollow")},m(y,x){me(y,l,x),e(l,m),e(l,p),e(p,f),e(l,E)},d(y){y&&t(l)}}}function Qa(I){let l,m;return l=new Gr({props:{$$slots:{default:[Ka]},$$scope:{ctx:I}}}),{c(){k(l.$$.fragment)},l(p){_(l.$$.fragment,p)},m(p,f){z(l,p,f),m=!0},p(p,f){const E={};f&2&&(E.$$scope={dirty:f,ctx:p}),l.$set(E)},i(p){m||(v(l.$$.fragment,p),m=!0)},o(p){$(l.$$.fragment,p),m=!1},d(p){T(l,p)}}}function Xa(I){let l,m;return{c(){l=n("p"),m=a("The node API has not been documented yet.")},l(p){l=o(p,"P",{});var f=r(l);m=s(f,"The node API has not been documented yet."),f.forEach(t)},m(p,f){me(p,l,f),e(l,m)},d(p){p&&t(l)}}}function Za(I){let l,m;return l=new Gr({props:{$$slots:{default:[Xa]},$$scope:{ctx:I}}}),{c(){k(l.$$.fragment)},l(p){_(l.$$.fragment,p)},m(p,f){z(l,p,f),m=!0},p(p,f){const E={};f&2&&(E.$$scope={dirty:f,ctx:p}),l.$set(E)},i(p){m||(v(l.$$.fragment,p),m=!0)},o(p){$(l.$$.fragment,p),m=!1},d(p){T(l,p)}}}function es(I){let l,m,p,f,E,y,x,D,h,P,j,A;return y=new Fa({}),j=new Ba({props:{python:!0,rust:!0,node:!0,$$slots:{node:[Za],rust:[Qa],python:[Ya]},$$scope:{ctx:I}}}),{c(){l=n("meta"),m=i(),p=n("h1"),f=n("a"),E=n("span"),k(y.$$.fragment),x=i(),D=n("span"),h=a("Tokenizer"),P=i(),k(j.$$.fragment),this.h()},l(b){const q=Ra('[data-svelte="svelte-1phssyn"]',document.head);l=o(q,"META",{name:!0,content:!0}),q.forEach(t),m=d(b),p=o(b,"H1",{class:!0});var O=r(p);f=o(O,"A",{id:!0,class:!0,href:!0});var Qe=r(f);E=o(Qe,"SPAN",{});var R=r(E);_(y.$$.fragment,R),R.forEach(t),Qe.forEach(t),x=d(O),D=o(O,"SPAN",{});var Xe=r(D);h=s(Xe,"Tokenizer"),Xe.forEach(t),O.forEach(t),P=d(b),_(j.$$.fragment,b),this.h()},h(){u(l,"name","hf:doc:metadata"),u(l,"content",JSON.stringify(ts)),u(f,"id","tokenizer"),u(f,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(f,"href","#tokenizer"),u(p,"class","relative group")},m(b,q){e(document.head,l),me(b,m,q),me(b,p,q),e(p,f),e(f,E),z(y,E,null),e(p,x),e(p,D),e(D,h),me(b,P,q),z(j,b,q),A=!0},p(b,[q]){const O={};q&2&&(O.$$scope={dirty:q,ctx:b}),j.$set(O)},i(b){A||(v(y.$$.fragment,b),v(j.$$.fragment,b),A=!0)},o(b){$(y.$$.fragment,b),$(j.$$.fragment,b),A=!1},d(b){t(l),b&&t(m),b&&t(p),T(y),b&&t(P),T(j,b)}}}const ts={local:"tokenizer",sections:[{local:"tokenizers.Tokenizer]][[tokenizers.Tokenizer",title:"Tokenizer"}],title:"Tokenizer"};function ns(I){return Ja(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class ds extends Ha{constructor(l){super();Ma(this,l,ns,es,Wa,{})}}export{ds as default,ts as metadata};
