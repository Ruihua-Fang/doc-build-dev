import{S as ve,i as ke,s as we,e as l,k as g,w as E,t as k,M as xe,c as p,d as n,m as v,a as f,x as T,h as w,b as D,F as s,g as x,y as b,q as C,o as B,B as M,v as De,L as ze}from"../../chunks/vendor-0d3f0756.js";import{D as K}from"../../chunks/Docstring-f752f2c3.js";import{I as Pe}from"../../chunks/IconCopyLink-9193371d.js";import{T as ye,M as pe}from"../../chunks/TokenizersLanguageContent-ca787841.js";function Ee(z){let t,a,e,o,d,m,i,u,R,y,P,W,c,h,$,O,U,S,oe,ne,Q,I,V,ae,J,se,X,L,q,ce,j,de,Y,A,N,ie,G,le,Z;return a=new K({props:{name:"class tokenizers.decoders.BPEDecoder",anchor:"tokenizers.decoders.BPEDecoder",parameters:[{name:"suffix",val:" = '</w>'"}],parametersDescription:[{anchor:"tokenizers.decoders.BPEDecoder.suffix",description:`<strong>suffix</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&lt;/w&gt;</code>) &#x2014;
The suffix that was used to caracterize an end-of-word. This suffix will
be replaced by whitespaces during the decoding`,name:"suffix"}]}}),u=new K({props:{name:"class tokenizers.decoders.ByteLevel",anchor:"tokenizers.decoders.ByteLevel",parameters:[]}}),V=new K({props:{name:"class tokenizers.decoders.CTC",anchor:"tokenizers.decoders.CTC",parameters:[{name:"pad_token",val:" = '<pad>'"},{name:"word_delimiter_token",val:" = '|'"},{name:"cleanup",val:" = True"}],parametersDescription:[{anchor:"tokenizers.decoders.CTC.pad_token",description:`<strong>pad_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&lt;pad&gt;</code>) &#x2014;
The pad token used by CTC to delimit a new token.`,name:"pad_token"},{anchor:"tokenizers.decoders.CTC.word_delimiter_token",description:`<strong>word_delimiter_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>|</code>) &#x2014;
The word delimiter token. It will be replaced by a <space></space>`,name:"word_delimiter_token"},{anchor:"tokenizers.decoders.CTC.cleanup",description:`<strong>cleanup</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to cleanup some tokenization artifacts.
Mainly spaces before punctuation, and some abbreviated english forms.`,name:"cleanup"}]}}),q=new K({props:{name:"class tokenizers.decoders.Metaspace",anchor:"tokenizers.decoders.Metaspace",parameters:"",parametersDescription:[{anchor:"tokenizers.decoders.Metaspace.replacement",description:`<strong>replacement</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&#x2581;</code>) &#x2014;
The replacement character. Must be exactly one character. By default we
use the <em>&#x2581;</em> (U+2581) meta symbol (Same as in SentencePiece).`,name:"replacement"},{anchor:"tokenizers.decoders.Metaspace.add_prefix_space",description:`<strong>add_prefix_space</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to add a space to the first word if there isn&#x2019;t already one. This
lets us treat <em>hello</em> exactly like <em>say hello</em>.`,name:"add_prefix_space"}]}}),N=new K({props:{name:"class tokenizers.decoders.WordPiece",anchor:"tokenizers.decoders.WordPiece",parameters:[{name:"prefix",val:" = '##'"},{name:"cleanup",val:" = True"}],parametersDescription:[{anchor:"tokenizers.decoders.WordPiece.prefix",description:`<strong>prefix</strong> (<code>str</code>, <em>optional</em>, defaults to <code>##</code>) &#x2014;
The prefix to use for subwords that are not a beginning-of-word`,name:"prefix"},{anchor:"tokenizers.decoders.WordPiece.cleanup",description:`<strong>cleanup</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to cleanup some tokenization artifacts. Mainly spaces before punctuation,
and some abbreviated english forms.`,name:"cleanup"}]}}),{c(){t=l("div"),E(a.$$.fragment),e=g(),o=l("p"),d=k("BPEDecoder Decoder"),m=g(),i=l("div"),E(u.$$.fragment),R=g(),y=l("p"),P=k("ByteLevel Decoder"),W=g(),c=l("p"),h=k("This decoder is to be used in tandem with the "),$=l("a"),O=k("ByteLevel"),U=g(),S=l("code"),oe=k("PreTokenizer"),ne=k("."),Q=g(),I=l("div"),E(V.$$.fragment),ae=g(),J=l("p"),se=k("CTC Decoder"),X=g(),L=l("div"),E(q.$$.fragment),ce=g(),j=l("p"),de=k("Metaspace Decoder"),Y=g(),A=l("div"),E(N.$$.fragment),ie=g(),G=l("p"),le=k("WordPiece Decoder"),this.h()},l(r){t=p(r,"DIV",{class:!0});var _=f(t);T(a.$$.fragment,_),e=v(_),o=p(_,"P",{});var fe=f(o);d=w(fe,"BPEDecoder Decoder"),fe.forEach(n),_.forEach(n),m=v(r),i=p(r,"DIV",{class:!0});var F=f(i);T(u.$$.fragment,F),R=v(F),y=p(F,"P",{});var me=f(y);P=w(me,"ByteLevel Decoder"),me.forEach(n),W=v(F),c=p(F,"P",{});var H=f(c);h=w(H,"This decoder is to be used in tandem with the "),$=p(H,"A",{href:!0});var ue=f($);O=w(ue,"ByteLevel"),ue.forEach(n),U=v(H),S=p(H,"CODE",{});var he=f(S);oe=w(he,"PreTokenizer"),he.forEach(n),ne=w(H,"."),H.forEach(n),F.forEach(n),Q=v(r),I=p(r,"DIV",{class:!0});var ee=f(I);T(V.$$.fragment,ee),ae=v(ee),J=p(ee,"P",{});var $e=f(J);se=w($e,"CTC Decoder"),$e.forEach(n),ee.forEach(n),X=v(r),L=p(r,"DIV",{class:!0});var te=f(L);T(q.$$.fragment,te),ce=v(te),j=p(te,"P",{});var _e=f(j);de=w(_e,"Metaspace Decoder"),_e.forEach(n),te.forEach(n),Y=v(r),A=p(r,"DIV",{class:!0});var re=f(A);T(N.$$.fragment,re),ie=v(re),G=p(re,"P",{});var ge=f(G);le=w(ge,"WordPiece Decoder"),ge.forEach(n),re.forEach(n),this.h()},h(){D(t,"class","docstring"),D($,"href","/docs/tokenizers/pr_1/en/api/pre-tokenizers#tokenizers.pre_tokenizers.ByteLevel"),D(i,"class","docstring"),D(I,"class","docstring"),D(L,"class","docstring"),D(A,"class","docstring")},m(r,_){x(r,t,_),b(a,t,null),s(t,e),s(t,o),s(o,d),x(r,m,_),x(r,i,_),b(u,i,null),s(i,R),s(i,y),s(y,P),s(i,W),s(i,c),s(c,h),s(c,$),s($,O),s(c,U),s(c,S),s(S,oe),s(c,ne),x(r,Q,_),x(r,I,_),b(V,I,null),s(I,ae),s(I,J),s(J,se),x(r,X,_),x(r,L,_),b(q,L,null),s(L,ce),s(L,j),s(j,de),x(r,Y,_),x(r,A,_),b(N,A,null),s(A,ie),s(A,G),s(G,le),Z=!0},p:ze,i(r){Z||(C(a.$$.fragment,r),C(u.$$.fragment,r),C(V.$$.fragment,r),C(q.$$.fragment,r),C(N.$$.fragment,r),Z=!0)},o(r){B(a.$$.fragment,r),B(u.$$.fragment,r),B(V.$$.fragment,r),B(q.$$.fragment,r),B(N.$$.fragment,r),Z=!1},d(r){r&&n(t),M(a),r&&n(m),r&&n(i),M(u),r&&n(Q),r&&n(I),M(V),r&&n(X),r&&n(L),M(q),r&&n(Y),r&&n(A),M(N)}}}function Te(z){let t,a;return t=new pe({props:{$$slots:{default:[Ee]},$$scope:{ctx:z}}}),{c(){E(t.$$.fragment)},l(e){T(t.$$.fragment,e)},m(e,o){b(t,e,o),a=!0},p(e,o){const d={};o&2&&(d.$$scope={dirty:o,ctx:e}),t.$set(d)},i(e){a||(C(t.$$.fragment,e),a=!0)},o(e){B(t.$$.fragment,e),a=!1},d(e){M(t,e)}}}function be(z){let t,a,e,o,d;return{c(){t=l("p"),a=k("The Rust API Reference is available directly on the "),e=l("a"),o=k("Docs.rs"),d=k(" website."),this.h()},l(m){t=p(m,"P",{});var i=f(t);a=w(i,"The Rust API Reference is available directly on the "),e=p(i,"A",{href:!0,rel:!0});var u=f(e);o=w(u,"Docs.rs"),u.forEach(n),d=w(i," website."),i.forEach(n),this.h()},h(){D(e,"href","https://docs.rs/tokenizers/latest/tokenizers/"),D(e,"rel","nofollow")},m(m,i){x(m,t,i),s(t,a),s(t,e),s(e,o),s(t,d)},d(m){m&&n(t)}}}function Ce(z){let t,a;return t=new pe({props:{$$slots:{default:[be]},$$scope:{ctx:z}}}),{c(){E(t.$$.fragment)},l(e){T(t.$$.fragment,e)},m(e,o){b(t,e,o),a=!0},p(e,o){const d={};o&2&&(d.$$scope={dirty:o,ctx:e}),t.$set(d)},i(e){a||(C(t.$$.fragment,e),a=!0)},o(e){B(t.$$.fragment,e),a=!1},d(e){M(t,e)}}}function Be(z){let t,a;return{c(){t=l("p"),a=k("The node API has not been documented yet.")},l(e){t=p(e,"P",{});var o=f(t);a=w(o,"The node API has not been documented yet."),o.forEach(n)},m(e,o){x(e,t,o),s(t,a)},d(e){e&&n(t)}}}function Me(z){let t,a;return t=new pe({props:{$$slots:{default:[Be]},$$scope:{ctx:z}}}),{c(){E(t.$$.fragment)},l(e){T(t.$$.fragment,e)},m(e,o){b(t,e,o),a=!0},p(e,o){const d={};o&2&&(d.$$scope={dirty:o,ctx:e}),t.$set(d)},i(e){a||(C(t.$$.fragment,e),a=!0)},o(e){B(t.$$.fragment,e),a=!1},d(e){M(t,e)}}}function Ie(z){let t,a,e,o,d,m,i,u,R,y,P,W;return m=new Pe({}),P=new ye({props:{python:!0,rust:!0,node:!0,$$slots:{node:[Me],rust:[Ce],python:[Te]},$$scope:{ctx:z}}}),{c(){t=l("meta"),a=g(),e=l("h1"),o=l("a"),d=l("span"),E(m.$$.fragment),i=g(),u=l("span"),R=k("Decoders"),y=g(),E(P.$$.fragment),this.h()},l(c){const h=xe('[data-svelte="svelte-1phssyn"]',document.head);t=p(h,"META",{name:!0,content:!0}),h.forEach(n),a=v(c),e=p(c,"H1",{class:!0});var $=f(e);o=p($,"A",{id:!0,class:!0,href:!0});var O=f(o);d=p(O,"SPAN",{});var U=f(d);T(m.$$.fragment,U),U.forEach(n),O.forEach(n),i=v($),u=p($,"SPAN",{});var S=f(u);R=w(S,"Decoders"),S.forEach(n),$.forEach(n),y=v(c),T(P.$$.fragment,c),this.h()},h(){D(t,"name","hf:doc:metadata"),D(t,"content",JSON.stringify(Le)),D(o,"id","tokenizers.decoders.BPEDecoder"),D(o,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),D(o,"href","#tokenizers.decoders.BPEDecoder"),D(e,"class","relative group")},m(c,h){s(document.head,t),x(c,a,h),x(c,e,h),s(e,o),s(o,d),b(m,d,null),s(e,i),s(e,u),s(u,R),x(c,y,h),b(P,c,h),W=!0},p(c,[h]){const $={};h&2&&($.$$scope={dirty:h,ctx:c}),P.$set($)},i(c){W||(C(m.$$.fragment,c),C(P.$$.fragment,c),W=!0)},o(c){B(m.$$.fragment,c),B(P.$$.fragment,c),W=!1},d(c){n(t),c&&n(a),c&&n(e),M(m),c&&n(y),M(P,c)}}}const Le={local:"tokenizers.decoders.BPEDecoder",title:"Decoders"};function Ae(z){return De(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class qe extends ve{constructor(t){super();ke(this,t,Ae,Ie,we,{})}}export{qe as default,Le as metadata};
