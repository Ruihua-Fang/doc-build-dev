import{S as At,i as Gt,s as St,e as r,k as g,w as k,t as i,M as Vt,c as s,d as o,m as u,a,x as E,h as d,b as f,F as e,g as K,y as x,q as w,o as q,B as b,v as Ct,L as Mt}from"../../chunks/vendor-0d3f0756.js";import{D as G}from"../../chunks/Docstring-6b5b4b86.js";import{I as It}from"../../chunks/IconCopyLink-9193371d.js";import{T as Lt,M as dt}from"../../chunks/TokenizersLanguageContent-ca787841.js";function Nt(P){let t,l,n,c,$,v,T,D,p,I,A,y,m,z,S,ge,O,ue,Ze,en,H,Q,nn,ye,tn,on,B,X,rn,De,sn,an,F,Y,dn,fe,cn,me,hn,ln,J,Z,pn,ee,gn,_e,un,fn,mn,V,ne,_n,Pe,$n,vn,te,kn,$e,En,xn,wn,C,oe,qn,Ie,bn,zn,re,Tn,Ae,yn,Dn,Pn,M,se,In,Ge,An,Gn,R,Sn,Se,Vn,Cn,Ve,Mn,Ln,Nn,L,ae,On,Ce,Rn,Hn,ie,Bn,Me,Fn,Jn,Un,N,de,Wn,ce,jn,ve,Kn,Qn,Xn,he,Yn,ke,Zn,et,nt,U,le,tt,Le,ot,rt,W,pe,st,Ne,at,Oe;return c=new It({}),I=new G({props:{name:"class tokenizers.Encoding",anchor:"tokenizers.Encoding",parameters:""}}),Q=new G({props:{name:"char_to_token",anchor:"tokenizers.Encoding.char_to_token",parameters:[{name:"char_pos",val:""},{name:"sequence_index",val:" = 0"}],parametersDescription:[{anchor:"tokenizers.Encoding.char_to_token.char_pos",description:`<strong>char_pos</strong> (<code>int</code>) &#x2014;
The position of a char in the input string`,name:"char_pos"},{anchor:"tokenizers.Encoding.char_to_token.sequence_index",description:`<strong>sequence_index</strong> (<code>int</code>, defaults to <code>0</code>) &#x2014;
The index of the sequence that contains the target char`,name:"sequence_index"}],returnDescription:`
<p>The index of the token that contains this char in the encoded sequence</p>
`,returnType:`
<p><code>int</code></p>
`}}),X=new G({props:{name:"char_to_word",anchor:"tokenizers.Encoding.char_to_word",parameters:[{name:"char_pos",val:""},{name:"sequence_index",val:" = 0"}],parametersDescription:[{anchor:"tokenizers.Encoding.char_to_word.char_pos",description:`<strong>char_pos</strong> (<code>int</code>) &#x2014;
The position of a char in the input string`,name:"char_pos"},{anchor:"tokenizers.Encoding.char_to_word.sequence_index",description:`<strong>sequence_index</strong> (<code>int</code>, defaults to <code>0</code>) &#x2014;
The index of the sequence that contains the target char`,name:"sequence_index"}],returnDescription:`
<p>The index of the word that contains this char in the input sequence</p>
`,returnType:`
<p><code>int</code></p>
`}}),Y=new G({props:{name:"merge",anchor:"tokenizers.Encoding.merge",parameters:[{name:"encodings",val:""},{name:"growing_offsets",val:" = True"}],parametersDescription:[{anchor:"tokenizers.Encoding.merge.encodings",description:`<strong>encodings</strong> (A <code>List</code> of <a href="/docs/tokenizers/pr_1/en/api/encoding#tokenizers.Encoding">Encoding</a>) &#x2014;
The list of encodings that should be merged in one`,name:"encodings"},{anchor:"tokenizers.Encoding.merge.growing_offsets",description:`<strong>growing_offsets</strong> (<code>bool</code>, defaults to <code>True</code>) &#x2014;
Whether the offsets should accumulate while merging`,name:"growing_offsets"}],returnDescription:`
<p>The resulting Encoding</p>
`,returnType:`
<p><a href="/docs/tokenizers/pr_1/en/api/encoding#tokenizers.Encoding">Encoding</a></p>
`}}),Z=new G({props:{name:"pad",anchor:"tokenizers.Encoding.pad",parameters:[{name:"length",val:""},{name:"direction",val:" = 'right'"},{name:"pad_id",val:" = 0"},{name:"pad_type_id",val:" = 0"},{name:"pad_token",val:" = '[PAD]'"}],parametersDescription:[{anchor:"tokenizers.Encoding.pad.length",description:`<strong>length</strong> (<code>int</code>) &#x2014;
The desired length</p>
<p>direction &#x2014; (<code>str</code>, defaults to <code>right</code>):
The expected padding direction. Can be either <code>right</code> or <code>left</code>`,name:"length"},{anchor:"tokenizers.Encoding.pad.pad_id",description:`<strong>pad_id</strong> (<code>int</code>, defaults to <code>0</code>) &#x2014;
The ID corresponding to the padding token`,name:"pad_id"},{anchor:"tokenizers.Encoding.pad.pad_type_id",description:`<strong>pad_type_id</strong> (<code>int</code>, defaults to <code>0</code>) &#x2014;
The type ID corresponding to the padding token`,name:"pad_type_id"},{anchor:"tokenizers.Encoding.pad.pad_token",description:`<strong>pad_token</strong> (<code>str</code>, defaults to <em>[PAD]</em>) &#x2014;
The pad token to use`,name:"pad_token"}]}}),ne=new G({props:{name:"set_sequence_id",anchor:"tokenizers.Encoding.set_sequence_id",parameters:[{name:"sequence_id",val:""}]}}),oe=new G({props:{name:"token_to_chars",anchor:"tokenizers.Encoding.token_to_chars",parameters:[{name:"token_index",val:""}],parametersDescription:[{anchor:"tokenizers.Encoding.token_to_chars.token_index",description:`<strong>token_index</strong> (<code>int</code>) &#x2014;
The index of a token in the encoded sequence.`,name:"token_index"}],returnDescription:`
<p>The token offsets <code>(first, last + 1)</code></p>
`,returnType:`
<p><code>Tuple[int, int]</code></p>
`}}),se=new G({props:{name:"token_to_sequence",anchor:"tokenizers.Encoding.token_to_sequence",parameters:[{name:"token_index",val:""}],parametersDescription:[{anchor:"tokenizers.Encoding.token_to_sequence.token_index",description:`<strong>token_index</strong> (<code>int</code>) &#x2014;
The index of a token in the encoded sequence.`,name:"token_index"}],returnDescription:`
<p>The sequence id of the given token</p>
`,returnType:`
<p><code>int</code></p>
`}}),ae=new G({props:{name:"token_to_word",anchor:"tokenizers.Encoding.token_to_word",parameters:[{name:"token_index",val:""}],parametersDescription:[{anchor:"tokenizers.Encoding.token_to_word.token_index",description:`<strong>token_index</strong> (<code>int</code>) &#x2014;
The index of a token in the encoded sequence.`,name:"token_index"}],returnDescription:`
<p>The index of the word in the relevant input sequence.</p>
`,returnType:`
<p><code>int</code></p>
`}}),de=new G({props:{name:"truncate",anchor:"tokenizers.Encoding.truncate",parameters:[{name:"max_length",val:""},{name:"stride",val:" = 0"},{name:"direction",val:" = 'right'"}],parametersDescription:[{anchor:"tokenizers.Encoding.truncate.max_length",description:`<strong>max_length</strong> (<code>int</code>) &#x2014;
The desired length`,name:"max_length"},{anchor:"tokenizers.Encoding.truncate.stride",description:`<strong>stride</strong> (<code>int</code>, defaults to <code>0</code>) &#x2014;
The length of previous content to be included in each overflowing piece`,name:"stride"},{anchor:"tokenizers.Encoding.truncate.direction",description:`<strong>direction</strong> (<code>str</code>, defaults to <code>right</code>) &#x2014;
Truncate direction`,name:"direction"}]}}),le=new G({props:{name:"word_to_chars",anchor:"tokenizers.Encoding.word_to_chars",parameters:[{name:"word_index",val:""},{name:"sequence_index",val:" = 0"}],parametersDescription:[{anchor:"tokenizers.Encoding.word_to_chars.word_index",description:`<strong>word_index</strong> (<code>int</code>) &#x2014;
The index of a word in one of the input sequences.`,name:"word_index"},{anchor:"tokenizers.Encoding.word_to_chars.sequence_index",description:`<strong>sequence_index</strong> (<code>int</code>, defaults to <code>0</code>) &#x2014;
The index of the sequence that contains the target word`,name:"sequence_index"}],returnDescription:`
<p>The range of characters (span) <code>(first, last + 1)</code></p>
`,returnType:`
<p><code>Tuple[int, int]</code></p>
`}}),pe=new G({props:{name:"word_to_tokens",anchor:"tokenizers.Encoding.word_to_tokens",parameters:[{name:"word_index",val:""},{name:"sequence_index",val:" = 0"}],parametersDescription:[{anchor:"tokenizers.Encoding.word_to_tokens.word_index",description:`<strong>word_index</strong> (<code>int</code>) &#x2014;
The index of a word in one of the input sequences.`,name:"word_index"},{anchor:"tokenizers.Encoding.word_to_tokens.sequence_index",description:`<strong>sequence_index</strong> (<code>int</code>, defaults to <code>0</code>) &#x2014;
The index of the sequence that contains the target word`,name:"sequence_index"}],returnDescription:`
<p>The range of tokens: <code>(first, last + 1)</code></p>
`,returnType:`
<p><code>Tuple[int, int]</code></p>
`}}),{c(){t=r("h2"),l=r("a"),n=r("span"),k(c.$$.fragment),$=g(),v=r("span"),T=i("Encoding"),D=g(),p=r("div"),k(I.$$.fragment),A=g(),y=r("p"),m=i("The "),z=r("a"),S=i("Encoding"),ge=i(" represents the output of a "),O=r("a"),ue=i("Tokenizer"),Ze=i("."),en=g(),H=r("div"),k(Q.$$.fragment),nn=g(),ye=r("p"),tn=i("Get the token that contains the char at the given position in the input sequence."),on=g(),B=r("div"),k(X.$$.fragment),rn=g(),De=r("p"),sn=i("Get the word that contains the char at the given position in the input sequence."),an=g(),F=r("div"),k(Y.$$.fragment),dn=g(),fe=r("p"),cn=i("Merge the list of encodings into one final "),me=r("a"),hn=i("Encoding"),ln=g(),J=r("div"),k(Z.$$.fragment),pn=g(),ee=r("p"),gn=i("Pad the "),_e=r("a"),un=i("Encoding"),fn=i(" at the given length"),mn=g(),V=r("div"),k(ne.$$.fragment),_n=g(),Pe=r("p"),$n=i("Set the given sequence index"),vn=g(),te=r("p"),kn=i(`Set the given sequence index for the whole range of tokens contained in this
`),$e=r("a"),En=i("Encoding"),xn=i("."),wn=g(),C=r("div"),k(oe.$$.fragment),qn=g(),Ie=r("p"),bn=i("Get the offsets of the token at the given index."),zn=g(),re=r("p"),Tn=i(`The returned offsets are related to the input sequence that contains the
token.  In order to determine in which input sequence it belongs, you
must call `),Ae=r("code"),yn=i("token_to_sequence()"),Dn=i("."),Pn=g(),M=r("div"),k(se.$$.fragment),In=g(),Ge=r("p"),An=i("Get the index of the sequence represented by the given token."),Gn=g(),R=r("p"),Sn=i("In the general use case, this method returns "),Se=r("code"),Vn=i("0"),Cn=i(` for a single sequence or
the first sequence of a pair, and `),Ve=r("code"),Mn=i("1"),Ln=i(" for the second sequence of a pair"),Nn=g(),L=r("div"),k(ae.$$.fragment),On=g(),Ce=r("p"),Rn=i("Get the index of the word that contains the token in one of the input sequences."),Hn=g(),ie=r("p"),Bn=i(`The returned word index is related to the input sequence that contains
the token.  In order to determine in which input sequence it belongs, you
must call `),Me=r("code"),Fn=i("token_to_sequence()"),Jn=i("."),Un=g(),N=r("div"),k(de.$$.fragment),Wn=g(),ce=r("p"),jn=i("Truncate the "),ve=r("a"),Kn=i("Encoding"),Qn=i(" at the given length"),Xn=g(),he=r("p"),Yn=i("If this "),ke=r("a"),Zn=i("Encoding"),et=i(` represents multiple sequences, when truncating
this information is lost. It will be considered as representing a single sequence.`),nt=g(),U=r("div"),k(le.$$.fragment),tt=g(),Le=r("p"),ot=i("Get the offsets of the word at the given index in one of the input sequences."),rt=g(),W=r("div"),k(pe.$$.fragment),st=g(),Ne=r("p"),at=i(`Get the encoded tokens corresponding to the word at the given index
in one of the input sequences.`),this.h()},l(h){t=s(h,"H2",{class:!0});var j=a(t);l=s(j,"A",{id:!0,class:!0,href:!0});var ct=a(l);n=s(ct,"SPAN",{});var ht=a(n);E(c.$$.fragment,ht),ht.forEach(o),ct.forEach(o),$=u(j),v=s(j,"SPAN",{});var lt=a(v);T=d(lt,"Encoding"),lt.forEach(o),j.forEach(o),D=u(h),p=s(h,"DIV",{class:!0});var _=a(p);E(I.$$.fragment,_),A=u(_),y=s(_,"P",{});var Ee=a(y);m=d(Ee,"The "),z=s(Ee,"A",{href:!0});var pt=a(z);S=d(pt,"Encoding"),pt.forEach(o),ge=d(Ee," represents the output of a "),O=s(Ee,"A",{href:!0});var gt=a(O);ue=d(gt,"Tokenizer"),gt.forEach(o),Ze=d(Ee,"."),Ee.forEach(o),en=u(_),H=s(_,"DIV",{class:!0});var Re=a(H);E(Q.$$.fragment,Re),nn=u(Re),ye=s(Re,"P",{});var ut=a(ye);tn=d(ut,"Get the token that contains the char at the given position in the input sequence."),ut.forEach(o),Re.forEach(o),on=u(_),B=s(_,"DIV",{class:!0});var He=a(B);E(X.$$.fragment,He),rn=u(He),De=s(He,"P",{});var ft=a(De);sn=d(ft,"Get the word that contains the char at the given position in the input sequence."),ft.forEach(o),He.forEach(o),an=u(_),F=s(_,"DIV",{class:!0});var Be=a(F);E(Y.$$.fragment,Be),dn=u(Be),fe=s(Be,"P",{});var it=a(fe);cn=d(it,"Merge the list of encodings into one final "),me=s(it,"A",{href:!0});var mt=a(me);hn=d(mt,"Encoding"),mt.forEach(o),it.forEach(o),Be.forEach(o),ln=u(_),J=s(_,"DIV",{class:!0});var Fe=a(J);E(Z.$$.fragment,Fe),pn=u(Fe),ee=s(Fe,"P",{});var Je=a(ee);gn=d(Je,"Pad the "),_e=s(Je,"A",{href:!0});var _t=a(_e);un=d(_t,"Encoding"),_t.forEach(o),fn=d(Je," at the given length"),Je.forEach(o),Fe.forEach(o),mn=u(_),V=s(_,"DIV",{class:!0});var xe=a(V);E(ne.$$.fragment,xe),_n=u(xe),Pe=s(xe,"P",{});var $t=a(Pe);$n=d($t,"Set the given sequence index"),$t.forEach(o),vn=u(xe),te=s(xe,"P",{});var Ue=a(te);kn=d(Ue,`Set the given sequence index for the whole range of tokens contained in this
`),$e=s(Ue,"A",{href:!0});var vt=a($e);En=d(vt,"Encoding"),vt.forEach(o),xn=d(Ue,"."),Ue.forEach(o),xe.forEach(o),wn=u(_),C=s(_,"DIV",{class:!0});var we=a(C);E(oe.$$.fragment,we),qn=u(we),Ie=s(we,"P",{});var kt=a(Ie);bn=d(kt,"Get the offsets of the token at the given index."),kt.forEach(o),zn=u(we),re=s(we,"P",{});var We=a(re);Tn=d(We,`The returned offsets are related to the input sequence that contains the
token.  In order to determine in which input sequence it belongs, you
must call `),Ae=s(We,"CODE",{});var Et=a(Ae);yn=d(Et,"token_to_sequence()"),Et.forEach(o),Dn=d(We,"."),We.forEach(o),we.forEach(o),Pn=u(_),M=s(_,"DIV",{class:!0});var qe=a(M);E(se.$$.fragment,qe),In=u(qe),Ge=s(qe,"P",{});var xt=a(Ge);An=d(xt,"Get the index of the sequence represented by the given token."),xt.forEach(o),Gn=u(qe),R=s(qe,"P",{});var be=a(R);Sn=d(be,"In the general use case, this method returns "),Se=s(be,"CODE",{});var wt=a(Se);Vn=d(wt,"0"),wt.forEach(o),Cn=d(be,` for a single sequence or
the first sequence of a pair, and `),Ve=s(be,"CODE",{});var qt=a(Ve);Mn=d(qt,"1"),qt.forEach(o),Ln=d(be," for the second sequence of a pair"),be.forEach(o),qe.forEach(o),Nn=u(_),L=s(_,"DIV",{class:!0});var ze=a(L);E(ae.$$.fragment,ze),On=u(ze),Ce=s(ze,"P",{});var bt=a(Ce);Rn=d(bt,"Get the index of the word that contains the token in one of the input sequences."),bt.forEach(o),Hn=u(ze),ie=s(ze,"P",{});var je=a(ie);Bn=d(je,`The returned word index is related to the input sequence that contains
the token.  In order to determine in which input sequence it belongs, you
must call `),Me=s(je,"CODE",{});var zt=a(Me);Fn=d(zt,"token_to_sequence()"),zt.forEach(o),Jn=d(je,"."),je.forEach(o),ze.forEach(o),Un=u(_),N=s(_,"DIV",{class:!0});var Te=a(N);E(de.$$.fragment,Te),Wn=u(Te),ce=s(Te,"P",{});var Ke=a(ce);jn=d(Ke,"Truncate the "),ve=s(Ke,"A",{href:!0});var Tt=a(ve);Kn=d(Tt,"Encoding"),Tt.forEach(o),Qn=d(Ke," at the given length"),Ke.forEach(o),Xn=u(Te),he=s(Te,"P",{});var Qe=a(he);Yn=d(Qe,"If this "),ke=s(Qe,"A",{href:!0});var yt=a(ke);Zn=d(yt,"Encoding"),yt.forEach(o),et=d(Qe,` represents multiple sequences, when truncating
this information is lost. It will be considered as representing a single sequence.`),Qe.forEach(o),Te.forEach(o),nt=u(_),U=s(_,"DIV",{class:!0});var Xe=a(U);E(le.$$.fragment,Xe),tt=u(Xe),Le=s(Xe,"P",{});var Dt=a(Le);ot=d(Dt,"Get the offsets of the word at the given index in one of the input sequences."),Dt.forEach(o),Xe.forEach(o),rt=u(_),W=s(_,"DIV",{class:!0});var Ye=a(W);E(pe.$$.fragment,Ye),st=u(Ye),Ne=s(Ye,"P",{});var Pt=a(Ne);at=d(Pt,`Get the encoded tokens corresponding to the word at the given index
in one of the input sequences.`),Pt.forEach(o),Ye.forEach(o),_.forEach(o),this.h()},h(){f(l,"id","tokenizers.Encoding]][[tokenizers.Encoding"),f(l,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(l,"href","#tokenizers.Encoding]][[tokenizers.Encoding"),f(t,"class","relative group"),f(z,"href","/docs/tokenizers/pr_1/en/api/encoding#tokenizers.Encoding"),f(O,"href","/docs/tokenizers/pr_1/en/api/tokenizer#tokenizers.Tokenizer"),f(H,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),f(B,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),f(me,"href","/docs/tokenizers/pr_1/en/api/encoding#tokenizers.Encoding"),f(F,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),f(_e,"href","/docs/tokenizers/pr_1/en/api/encoding#tokenizers.Encoding"),f(J,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),f($e,"href","/docs/tokenizers/pr_1/en/api/encoding#tokenizers.Encoding"),f(V,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),f(C,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),f(M,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),f(L,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),f(ve,"href","/docs/tokenizers/pr_1/en/api/encoding#tokenizers.Encoding"),f(ke,"href","/docs/tokenizers/pr_1/en/api/encoding#tokenizers.Encoding"),f(N,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),f(U,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),f(W,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),f(p,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(h,j){K(h,t,j),e(t,l),e(l,n),x(c,n,null),e(t,$),e(t,v),e(v,T),K(h,D,j),K(h,p,j),x(I,p,null),e(p,A),e(p,y),e(y,m),e(y,z),e(z,S),e(y,ge),e(y,O),e(O,ue),e(y,Ze),e(p,en),e(p,H),x(Q,H,null),e(H,nn),e(H,ye),e(ye,tn),e(p,on),e(p,B),x(X,B,null),e(B,rn),e(B,De),e(De,sn),e(p,an),e(p,F),x(Y,F,null),e(F,dn),e(F,fe),e(fe,cn),e(fe,me),e(me,hn),e(p,ln),e(p,J),x(Z,J,null),e(J,pn),e(J,ee),e(ee,gn),e(ee,_e),e(_e,un),e(ee,fn),e(p,mn),e(p,V),x(ne,V,null),e(V,_n),e(V,Pe),e(Pe,$n),e(V,vn),e(V,te),e(te,kn),e(te,$e),e($e,En),e(te,xn),e(p,wn),e(p,C),x(oe,C,null),e(C,qn),e(C,Ie),e(Ie,bn),e(C,zn),e(C,re),e(re,Tn),e(re,Ae),e(Ae,yn),e(re,Dn),e(p,Pn),e(p,M),x(se,M,null),e(M,In),e(M,Ge),e(Ge,An),e(M,Gn),e(M,R),e(R,Sn),e(R,Se),e(Se,Vn),e(R,Cn),e(R,Ve),e(Ve,Mn),e(R,Ln),e(p,Nn),e(p,L),x(ae,L,null),e(L,On),e(L,Ce),e(Ce,Rn),e(L,Hn),e(L,ie),e(ie,Bn),e(ie,Me),e(Me,Fn),e(ie,Jn),e(p,Un),e(p,N),x(de,N,null),e(N,Wn),e(N,ce),e(ce,jn),e(ce,ve),e(ve,Kn),e(ce,Qn),e(N,Xn),e(N,he),e(he,Yn),e(he,ke),e(ke,Zn),e(he,et),e(p,nt),e(p,U),x(le,U,null),e(U,tt),e(U,Le),e(Le,ot),e(p,rt),e(p,W),x(pe,W,null),e(W,st),e(W,Ne),e(Ne,at),Oe=!0},p:Mt,i(h){Oe||(w(c.$$.fragment,h),w(I.$$.fragment,h),w(Q.$$.fragment,h),w(X.$$.fragment,h),w(Y.$$.fragment,h),w(Z.$$.fragment,h),w(ne.$$.fragment,h),w(oe.$$.fragment,h),w(se.$$.fragment,h),w(ae.$$.fragment,h),w(de.$$.fragment,h),w(le.$$.fragment,h),w(pe.$$.fragment,h),Oe=!0)},o(h){q(c.$$.fragment,h),q(I.$$.fragment,h),q(Q.$$.fragment,h),q(X.$$.fragment,h),q(Y.$$.fragment,h),q(Z.$$.fragment,h),q(ne.$$.fragment,h),q(oe.$$.fragment,h),q(se.$$.fragment,h),q(ae.$$.fragment,h),q(de.$$.fragment,h),q(le.$$.fragment,h),q(pe.$$.fragment,h),Oe=!1},d(h){h&&o(t),b(c),h&&o(D),h&&o(p),b(I),b(Q),b(X),b(Y),b(Z),b(ne),b(oe),b(se),b(ae),b(de),b(le),b(pe)}}}function Ot(P){let t,l;return t=new dt({props:{$$slots:{default:[Nt]},$$scope:{ctx:P}}}),{c(){k(t.$$.fragment)},l(n){E(t.$$.fragment,n)},m(n,c){x(t,n,c),l=!0},p(n,c){const $={};c&2&&($.$$scope={dirty:c,ctx:n}),t.$set($)},i(n){l||(w(t.$$.fragment,n),l=!0)},o(n){q(t.$$.fragment,n),l=!1},d(n){b(t,n)}}}function Rt(P){let t,l,n,c,$;return{c(){t=r("p"),l=i("The Rust API Reference is available directly on the "),n=r("a"),c=i("Docs.rs"),$=i(" website."),this.h()},l(v){t=s(v,"P",{});var T=a(t);l=d(T,"The Rust API Reference is available directly on the "),n=s(T,"A",{href:!0,rel:!0});var D=a(n);c=d(D,"Docs.rs"),D.forEach(o),$=d(T," website."),T.forEach(o),this.h()},h(){f(n,"href","https://docs.rs/tokenizers/latest/tokenizers/"),f(n,"rel","nofollow")},m(v,T){K(v,t,T),e(t,l),e(t,n),e(n,c),e(t,$)},d(v){v&&o(t)}}}function Ht(P){let t,l;return t=new dt({props:{$$slots:{default:[Rt]},$$scope:{ctx:P}}}),{c(){k(t.$$.fragment)},l(n){E(t.$$.fragment,n)},m(n,c){x(t,n,c),l=!0},p(n,c){const $={};c&2&&($.$$scope={dirty:c,ctx:n}),t.$set($)},i(n){l||(w(t.$$.fragment,n),l=!0)},o(n){q(t.$$.fragment,n),l=!1},d(n){b(t,n)}}}function Bt(P){let t,l;return{c(){t=r("p"),l=i("The node API has not been documented yet.")},l(n){t=s(n,"P",{});var c=a(t);l=d(c,"The node API has not been documented yet."),c.forEach(o)},m(n,c){K(n,t,c),e(t,l)},d(n){n&&o(t)}}}function Ft(P){let t,l;return t=new dt({props:{$$slots:{default:[Bt]},$$scope:{ctx:P}}}),{c(){k(t.$$.fragment)},l(n){E(t.$$.fragment,n)},m(n,c){x(t,n,c),l=!0},p(n,c){const $={};c&2&&($.$$scope={dirty:c,ctx:n}),t.$set($)},i(n){l||(w(t.$$.fragment,n),l=!0)},o(n){q(t.$$.fragment,n),l=!1},d(n){b(t,n)}}}function Jt(P){let t,l,n,c,$,v,T,D,p,I,A,y;return v=new It({}),A=new Lt({props:{python:!0,rust:!0,node:!0,$$slots:{node:[Ft],rust:[Ht],python:[Ot]},$$scope:{ctx:P}}}),{c(){t=r("meta"),l=g(),n=r("h1"),c=r("a"),$=r("span"),k(v.$$.fragment),T=g(),D=r("span"),p=i("Encoding"),I=g(),k(A.$$.fragment),this.h()},l(m){const z=Vt('[data-svelte="svelte-1phssyn"]',document.head);t=s(z,"META",{name:!0,content:!0}),z.forEach(o),l=u(m),n=s(m,"H1",{class:!0});var S=a(n);c=s(S,"A",{id:!0,class:!0,href:!0});var ge=a(c);$=s(ge,"SPAN",{});var O=a($);E(v.$$.fragment,O),O.forEach(o),ge.forEach(o),T=u(S),D=s(S,"SPAN",{});var ue=a(D);p=d(ue,"Encoding"),ue.forEach(o),S.forEach(o),I=u(m),E(A.$$.fragment,m),this.h()},h(){f(t,"name","hf:doc:metadata"),f(t,"content",JSON.stringify(Ut)),f(c,"id","encoding"),f(c,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(c,"href","#encoding"),f(n,"class","relative group")},m(m,z){e(document.head,t),K(m,l,z),K(m,n,z),e(n,c),e(c,$),x(v,$,null),e(n,T),e(n,D),e(D,p),K(m,I,z),x(A,m,z),y=!0},p(m,[z]){const S={};z&2&&(S.$$scope={dirty:z,ctx:m}),A.$set(S)},i(m){y||(w(v.$$.fragment,m),w(A.$$.fragment,m),y=!0)},o(m){q(v.$$.fragment,m),q(A.$$.fragment,m),y=!1},d(m){o(t),m&&o(l),m&&o(n),b(v),m&&o(I),b(A,m)}}}const Ut={local:"encoding",sections:[{local:"tokenizers.Encoding]][[tokenizers.Encoding",title:"Encoding"}],title:"Encoding"};function Wt(P){return Ct(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Yt extends At{constructor(t){super();Gt(this,t,Wt,Jt,St,{})}}export{Yt as default,Ut as metadata};
