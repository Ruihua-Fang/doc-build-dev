import{S as At,i as Gt,s as St,e as r,k as g,w as k,t as a,M as Vt,c as s,d as o,m as f,a as i,x as E,h as c,b as u,F as e,g as K,y as x,q as w,o as q,B as z,v as Ct,L as Mt}from"../../chunks/vendor-0d3f0756.js";import{D as G}from"../../chunks/Docstring-f752f2c3.js";import{I as bt}from"../../chunks/IconCopyLink-9193371d.js";import{T as Lt,M as ct}from"../../chunks/TokenizersLanguageContent-ca787841.js";function Nt(I){let t,p,n,d,$,v,D,P,l,b,A,y,_,T,S,ge,O,fe,Ze,en,H,Q,nn,ye,tn,on,B,X,rn,Pe,sn,an,F,Y,cn,ue,dn,_e,hn,pn,J,Z,ln,ee,gn,me,fn,un,_n,V,ne,mn,Ie,$n,vn,te,kn,$e,En,xn,wn,C,oe,qn,be,zn,Tn,re,Dn,Ae,yn,Pn,In,M,se,bn,Ge,An,Gn,R,Sn,Se,Vn,Cn,Ve,Mn,Ln,Nn,L,ie,On,Ce,Rn,Hn,ae,Bn,Me,Fn,Jn,Un,N,ce,Wn,de,jn,ve,Kn,Qn,Xn,he,Yn,ke,Zn,et,nt,U,pe,tt,Le,ot,rt,W,le,st,Ne,it,Oe;return d=new bt({}),b=new G({props:{name:"class tokenizers.Encoding",anchor:"tokenizers.Encoding",parameters:""}}),Q=new G({props:{name:"char_to_token",anchor:"tokenizers.Encoding.char_to_token",parameters:[{name:"char_pos",val:""},{name:"sequence_index",val:" = 0"}],parametersDescription:[{anchor:"tokenizers.Encoding.char_to_token.char_pos",description:`<strong>char_pos</strong> (<code>int</code>) &#x2014;
The position of a char in the input string`,name:"char_pos"},{anchor:"tokenizers.Encoding.char_to_token.sequence_index",description:`<strong>sequence_index</strong> (<code>int</code>, defaults to <code>0</code>) &#x2014;
The index of the sequence that contains the target char`,name:"sequence_index"}],returnDescription:`
<p>The index of the token that contains this char in the encoded sequence</p>
`,returnType:`
<p><code>int</code></p>
`}}),X=new G({props:{name:"char_to_word",anchor:"tokenizers.Encoding.char_to_word",parameters:[{name:"char_pos",val:""},{name:"sequence_index",val:" = 0"}],parametersDescription:[{anchor:"tokenizers.Encoding.char_to_word.char_pos",description:`<strong>char_pos</strong> (<code>int</code>) &#x2014;
The position of a char in the input string`,name:"char_pos"},{anchor:"tokenizers.Encoding.char_to_word.sequence_index",description:`<strong>sequence_index</strong> (<code>int</code>, defaults to <code>0</code>) &#x2014;
The index of the sequence that contains the target char`,name:"sequence_index"}],returnDescription:`
<p>The index of the word that contains this char in the input sequence</p>
`,returnType:`
<p><code>int</code></p>
`}}),Y=new G({props:{name:"merge",anchor:"tokenizers.Encoding.merge",parameters:[{name:"encodings",val:""},{name:"growing_offsets",val:" = True"}],parametersDescription:[{anchor:"tokenizers.Encoding.merge.encodings",description:`<strong>encodings</strong> (A <code>List</code> of <a href="/docs/tokenizers/pr_1/en/api/encoding#tokenizers.Encoding">Encoding</a>) &#x2014;
The list of encodings that should be merged in one`,name:"encodings"},{anchor:"tokenizers.Encoding.merge.growing_offsets",description:`<strong>growing_offsets</strong> (<code>bool</code>, defaults to <code>True</code>) &#x2014;
Whether the offsets should accumulate while merging`,name:"growing_offsets"}],returnDescription:`
<p>The resulting Encoding</p>
`,returnType:`
<p><a href="/docs/tokenizers/pr_1/en/api/encoding#tokenizers.Encoding">Encoding</a></p>
`}}),Z=new G({props:{name:"pad",anchor:"tokenizers.Encoding.pad",parameters:[{name:"length",val:""},{name:"direction",val:" = 'right'"},{name:"pad_id",val:" = 0"},{name:"pad_type_id",val:" = 0"},{name:"pad_token",val:" = '[PAD]'"}],parametersDescription:[{anchor:"tokenizers.Encoding.pad.length",description:`<strong>length</strong> (<code>int</code>) &#x2014;
The desired length</p>
<p>direction &#x2014; (<code>str</code>, defaults to <code>right</code>):
The expected padding direction. Can be either <code>right</code> or <code>left</code>`,name:"length"},{anchor:"tokenizers.Encoding.pad.pad_id",description:`<strong>pad_id</strong> (<code>int</code>, defaults to <code>0</code>) &#x2014;
The ID corresponding to the padding token`,name:"pad_id"},{anchor:"tokenizers.Encoding.pad.pad_type_id",description:`<strong>pad_type_id</strong> (<code>int</code>, defaults to <code>0</code>) &#x2014;
The type ID corresponding to the padding token`,name:"pad_type_id"},{anchor:"tokenizers.Encoding.pad.pad_token",description:`<strong>pad_token</strong> (<code>str</code>, defaults to <em>[PAD]</em>) &#x2014;
The pad token to use`,name:"pad_token"}]}}),ne=new G({props:{name:"set_sequence_id",anchor:"tokenizers.Encoding.set_sequence_id",parameters:[{name:"sequence_id",val:""}]}}),oe=new G({props:{name:"token_to_chars",anchor:"tokenizers.Encoding.token_to_chars",parameters:[{name:"token_index",val:""}],parametersDescription:[{anchor:"tokenizers.Encoding.token_to_chars.token_index",description:`<strong>token_index</strong> (<code>int</code>) &#x2014;
The index of a token in the encoded sequence.`,name:"token_index"}],returnDescription:`
<p>The token offsets <code>(first, last + 1)</code></p>
`,returnType:`
<p><code>Tuple[int, int]</code></p>
`}}),se=new G({props:{name:"token_to_sequence",anchor:"tokenizers.Encoding.token_to_sequence",parameters:[{name:"token_index",val:""}],parametersDescription:[{anchor:"tokenizers.Encoding.token_to_sequence.token_index",description:`<strong>token_index</strong> (<code>int</code>) &#x2014;
The index of a token in the encoded sequence.`,name:"token_index"}],returnDescription:`
<p>The sequence id of the given token</p>
`,returnType:`
<p><code>int</code></p>
`}}),ie=new G({props:{name:"token_to_word",anchor:"tokenizers.Encoding.token_to_word",parameters:[{name:"token_index",val:""}],parametersDescription:[{anchor:"tokenizers.Encoding.token_to_word.token_index",description:`<strong>token_index</strong> (<code>int</code>) &#x2014;
The index of a token in the encoded sequence.`,name:"token_index"}],returnDescription:`
<p>The index of the word in the relevant input sequence.</p>
`,returnType:`
<p><code>int</code></p>
`}}),ce=new G({props:{name:"truncate",anchor:"tokenizers.Encoding.truncate",parameters:[{name:"max_length",val:""},{name:"stride",val:" = 0"},{name:"direction",val:" = 'right'"}],parametersDescription:[{anchor:"tokenizers.Encoding.truncate.max_length",description:`<strong>max_length</strong> (<code>int</code>) &#x2014;
The desired length`,name:"max_length"},{anchor:"tokenizers.Encoding.truncate.stride",description:`<strong>stride</strong> (<code>int</code>, defaults to <code>0</code>) &#x2014;
The length of previous content to be included in each overflowing piece`,name:"stride"},{anchor:"tokenizers.Encoding.truncate.direction",description:`<strong>direction</strong> (<code>str</code>, defaults to <code>right</code>) &#x2014;
Truncate direction`,name:"direction"}]}}),pe=new G({props:{name:"word_to_chars",anchor:"tokenizers.Encoding.word_to_chars",parameters:[{name:"word_index",val:""},{name:"sequence_index",val:" = 0"}],parametersDescription:[{anchor:"tokenizers.Encoding.word_to_chars.word_index",description:`<strong>word_index</strong> (<code>int</code>) &#x2014;
The index of a word in one of the input sequences.`,name:"word_index"},{anchor:"tokenizers.Encoding.word_to_chars.sequence_index",description:`<strong>sequence_index</strong> (<code>int</code>, defaults to <code>0</code>) &#x2014;
The index of the sequence that contains the target word`,name:"sequence_index"}],returnDescription:`
<p>The range of characters (span) <code>(first, last + 1)</code></p>
`,returnType:`
<p><code>Tuple[int, int]</code></p>
`}}),le=new G({props:{name:"word_to_tokens",anchor:"tokenizers.Encoding.word_to_tokens",parameters:[{name:"word_index",val:""},{name:"sequence_index",val:" = 0"}],parametersDescription:[{anchor:"tokenizers.Encoding.word_to_tokens.word_index",description:`<strong>word_index</strong> (<code>int</code>) &#x2014;
The index of a word in one of the input sequences.`,name:"word_index"},{anchor:"tokenizers.Encoding.word_to_tokens.sequence_index",description:`<strong>sequence_index</strong> (<code>int</code>, defaults to <code>0</code>) &#x2014;
The index of the sequence that contains the target word`,name:"sequence_index"}],returnDescription:`
<p>The range of tokens: <code>(first, last + 1)</code></p>
`,returnType:`
<p><code>Tuple[int, int]</code></p>
`}}),{c(){t=r("h2"),p=r("a"),n=r("span"),k(d.$$.fragment),$=g(),v=r("span"),D=a("Encoding"),P=g(),l=r("div"),k(b.$$.fragment),A=g(),y=r("p"),_=a("The "),T=r("a"),S=a("Encoding"),ge=a(" represents the output of a "),O=r("a"),fe=a("Tokenizer"),Ze=a("."),en=g(),H=r("div"),k(Q.$$.fragment),nn=g(),ye=r("p"),tn=a("Get the token that contains the char at the given position in the input sequence."),on=g(),B=r("div"),k(X.$$.fragment),rn=g(),Pe=r("p"),sn=a("Get the word that contains the char at the given position in the input sequence."),an=g(),F=r("div"),k(Y.$$.fragment),cn=g(),ue=r("p"),dn=a("Merge the list of encodings into one final "),_e=r("a"),hn=a("Encoding"),pn=g(),J=r("div"),k(Z.$$.fragment),ln=g(),ee=r("p"),gn=a("Pad the "),me=r("a"),fn=a("Encoding"),un=a(" at the given length"),_n=g(),V=r("div"),k(ne.$$.fragment),mn=g(),Ie=r("p"),$n=a("Set the given sequence index"),vn=g(),te=r("p"),kn=a(`Set the given sequence index for the whole range of tokens contained in this
`),$e=r("a"),En=a("Encoding"),xn=a("."),wn=g(),C=r("div"),k(oe.$$.fragment),qn=g(),be=r("p"),zn=a("Get the offsets of the token at the given index."),Tn=g(),re=r("p"),Dn=a(`The returned offsets are related to the input sequence that contains the
token.  In order to determine in which input sequence it belongs, you
must call `),Ae=r("code"),yn=a("token_to_sequence()"),Pn=a("."),In=g(),M=r("div"),k(se.$$.fragment),bn=g(),Ge=r("p"),An=a("Get the index of the sequence represented by the given token."),Gn=g(),R=r("p"),Sn=a("In the general use case, this method returns "),Se=r("code"),Vn=a("0"),Cn=a(` for a single sequence or
the first sequence of a pair, and `),Ve=r("code"),Mn=a("1"),Ln=a(" for the second sequence of a pair"),Nn=g(),L=r("div"),k(ie.$$.fragment),On=g(),Ce=r("p"),Rn=a("Get the index of the word that contains the token in one of the input sequences."),Hn=g(),ae=r("p"),Bn=a(`The returned word index is related to the input sequence that contains
the token.  In order to determine in which input sequence it belongs, you
must call `),Me=r("code"),Fn=a("token_to_sequence()"),Jn=a("."),Un=g(),N=r("div"),k(ce.$$.fragment),Wn=g(),de=r("p"),jn=a("Truncate the "),ve=r("a"),Kn=a("Encoding"),Qn=a(" at the given length"),Xn=g(),he=r("p"),Yn=a("If this "),ke=r("a"),Zn=a("Encoding"),et=a(` represents multiple sequences, when truncating
this information is lost. It will be considered as representing a single sequence.`),nt=g(),U=r("div"),k(pe.$$.fragment),tt=g(),Le=r("p"),ot=a("Get the offsets of the word at the given index in one of the input sequences."),rt=g(),W=r("div"),k(le.$$.fragment),st=g(),Ne=r("p"),it=a(`Get the encoded tokens corresponding to the word at the given index
in one of the input sequences.`),this.h()},l(h){t=s(h,"H2",{class:!0});var j=i(t);p=s(j,"A",{id:!0,class:!0,href:!0});var dt=i(p);n=s(dt,"SPAN",{});var ht=i(n);E(d.$$.fragment,ht),ht.forEach(o),dt.forEach(o),$=f(j),v=s(j,"SPAN",{});var pt=i(v);D=c(pt,"Encoding"),pt.forEach(o),j.forEach(o),P=f(h),l=s(h,"DIV",{class:!0});var m=i(l);E(b.$$.fragment,m),A=f(m),y=s(m,"P",{});var Ee=i(y);_=c(Ee,"The "),T=s(Ee,"A",{href:!0});var lt=i(T);S=c(lt,"Encoding"),lt.forEach(o),ge=c(Ee," represents the output of a "),O=s(Ee,"A",{href:!0});var gt=i(O);fe=c(gt,"Tokenizer"),gt.forEach(o),Ze=c(Ee,"."),Ee.forEach(o),en=f(m),H=s(m,"DIV",{class:!0});var Re=i(H);E(Q.$$.fragment,Re),nn=f(Re),ye=s(Re,"P",{});var ft=i(ye);tn=c(ft,"Get the token that contains the char at the given position in the input sequence."),ft.forEach(o),Re.forEach(o),on=f(m),B=s(m,"DIV",{class:!0});var He=i(B);E(X.$$.fragment,He),rn=f(He),Pe=s(He,"P",{});var ut=i(Pe);sn=c(ut,"Get the word that contains the char at the given position in the input sequence."),ut.forEach(o),He.forEach(o),an=f(m),F=s(m,"DIV",{class:!0});var Be=i(F);E(Y.$$.fragment,Be),cn=f(Be),ue=s(Be,"P",{});var at=i(ue);dn=c(at,"Merge the list of encodings into one final "),_e=s(at,"A",{href:!0});var _t=i(_e);hn=c(_t,"Encoding"),_t.forEach(o),at.forEach(o),Be.forEach(o),pn=f(m),J=s(m,"DIV",{class:!0});var Fe=i(J);E(Z.$$.fragment,Fe),ln=f(Fe),ee=s(Fe,"P",{});var Je=i(ee);gn=c(Je,"Pad the "),me=s(Je,"A",{href:!0});var mt=i(me);fn=c(mt,"Encoding"),mt.forEach(o),un=c(Je," at the given length"),Je.forEach(o),Fe.forEach(o),_n=f(m),V=s(m,"DIV",{class:!0});var xe=i(V);E(ne.$$.fragment,xe),mn=f(xe),Ie=s(xe,"P",{});var $t=i(Ie);$n=c($t,"Set the given sequence index"),$t.forEach(o),vn=f(xe),te=s(xe,"P",{});var Ue=i(te);kn=c(Ue,`Set the given sequence index for the whole range of tokens contained in this
`),$e=s(Ue,"A",{href:!0});var vt=i($e);En=c(vt,"Encoding"),vt.forEach(o),xn=c(Ue,"."),Ue.forEach(o),xe.forEach(o),wn=f(m),C=s(m,"DIV",{class:!0});var we=i(C);E(oe.$$.fragment,we),qn=f(we),be=s(we,"P",{});var kt=i(be);zn=c(kt,"Get the offsets of the token at the given index."),kt.forEach(o),Tn=f(we),re=s(we,"P",{});var We=i(re);Dn=c(We,`The returned offsets are related to the input sequence that contains the
token.  In order to determine in which input sequence it belongs, you
must call `),Ae=s(We,"CODE",{});var Et=i(Ae);yn=c(Et,"token_to_sequence()"),Et.forEach(o),Pn=c(We,"."),We.forEach(o),we.forEach(o),In=f(m),M=s(m,"DIV",{class:!0});var qe=i(M);E(se.$$.fragment,qe),bn=f(qe),Ge=s(qe,"P",{});var xt=i(Ge);An=c(xt,"Get the index of the sequence represented by the given token."),xt.forEach(o),Gn=f(qe),R=s(qe,"P",{});var ze=i(R);Sn=c(ze,"In the general use case, this method returns "),Se=s(ze,"CODE",{});var wt=i(Se);Vn=c(wt,"0"),wt.forEach(o),Cn=c(ze,` for a single sequence or
the first sequence of a pair, and `),Ve=s(ze,"CODE",{});var qt=i(Ve);Mn=c(qt,"1"),qt.forEach(o),Ln=c(ze," for the second sequence of a pair"),ze.forEach(o),qe.forEach(o),Nn=f(m),L=s(m,"DIV",{class:!0});var Te=i(L);E(ie.$$.fragment,Te),On=f(Te),Ce=s(Te,"P",{});var zt=i(Ce);Rn=c(zt,"Get the index of the word that contains the token in one of the input sequences."),zt.forEach(o),Hn=f(Te),ae=s(Te,"P",{});var je=i(ae);Bn=c(je,`The returned word index is related to the input sequence that contains
the token.  In order to determine in which input sequence it belongs, you
must call `),Me=s(je,"CODE",{});var Tt=i(Me);Fn=c(Tt,"token_to_sequence()"),Tt.forEach(o),Jn=c(je,"."),je.forEach(o),Te.forEach(o),Un=f(m),N=s(m,"DIV",{class:!0});var De=i(N);E(ce.$$.fragment,De),Wn=f(De),de=s(De,"P",{});var Ke=i(de);jn=c(Ke,"Truncate the "),ve=s(Ke,"A",{href:!0});var Dt=i(ve);Kn=c(Dt,"Encoding"),Dt.forEach(o),Qn=c(Ke," at the given length"),Ke.forEach(o),Xn=f(De),he=s(De,"P",{});var Qe=i(he);Yn=c(Qe,"If this "),ke=s(Qe,"A",{href:!0});var yt=i(ke);Zn=c(yt,"Encoding"),yt.forEach(o),et=c(Qe,` represents multiple sequences, when truncating
this information is lost. It will be considered as representing a single sequence.`),Qe.forEach(o),De.forEach(o),nt=f(m),U=s(m,"DIV",{class:!0});var Xe=i(U);E(pe.$$.fragment,Xe),tt=f(Xe),Le=s(Xe,"P",{});var Pt=i(Le);ot=c(Pt,"Get the offsets of the word at the given index in one of the input sequences."),Pt.forEach(o),Xe.forEach(o),rt=f(m),W=s(m,"DIV",{class:!0});var Ye=i(W);E(le.$$.fragment,Ye),st=f(Ye),Ne=s(Ye,"P",{});var It=i(Ne);it=c(It,`Get the encoded tokens corresponding to the word at the given index
in one of the input sequences.`),It.forEach(o),Ye.forEach(o),m.forEach(o),this.h()},h(){u(p,"id","tokenizers.Encoding]][[tokenizers.Encoding"),u(p,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(p,"href","#tokenizers.Encoding]][[tokenizers.Encoding"),u(t,"class","relative group"),u(T,"href","/docs/tokenizers/pr_1/en/api/encoding#tokenizers.Encoding"),u(O,"href","/docs/tokenizers/pr_1/en/api/tokenizer#tokenizers.Tokenizer"),u(H,"class","docstring"),u(B,"class","docstring"),u(_e,"href","/docs/tokenizers/pr_1/en/api/encoding#tokenizers.Encoding"),u(F,"class","docstring"),u(me,"href","/docs/tokenizers/pr_1/en/api/encoding#tokenizers.Encoding"),u(J,"class","docstring"),u($e,"href","/docs/tokenizers/pr_1/en/api/encoding#tokenizers.Encoding"),u(V,"class","docstring"),u(C,"class","docstring"),u(M,"class","docstring"),u(L,"class","docstring"),u(ve,"href","/docs/tokenizers/pr_1/en/api/encoding#tokenizers.Encoding"),u(ke,"href","/docs/tokenizers/pr_1/en/api/encoding#tokenizers.Encoding"),u(N,"class","docstring"),u(U,"class","docstring"),u(W,"class","docstring"),u(l,"class","docstring")},m(h,j){K(h,t,j),e(t,p),e(p,n),x(d,n,null),e(t,$),e(t,v),e(v,D),K(h,P,j),K(h,l,j),x(b,l,null),e(l,A),e(l,y),e(y,_),e(y,T),e(T,S),e(y,ge),e(y,O),e(O,fe),e(y,Ze),e(l,en),e(l,H),x(Q,H,null),e(H,nn),e(H,ye),e(ye,tn),e(l,on),e(l,B),x(X,B,null),e(B,rn),e(B,Pe),e(Pe,sn),e(l,an),e(l,F),x(Y,F,null),e(F,cn),e(F,ue),e(ue,dn),e(ue,_e),e(_e,hn),e(l,pn),e(l,J),x(Z,J,null),e(J,ln),e(J,ee),e(ee,gn),e(ee,me),e(me,fn),e(ee,un),e(l,_n),e(l,V),x(ne,V,null),e(V,mn),e(V,Ie),e(Ie,$n),e(V,vn),e(V,te),e(te,kn),e(te,$e),e($e,En),e(te,xn),e(l,wn),e(l,C),x(oe,C,null),e(C,qn),e(C,be),e(be,zn),e(C,Tn),e(C,re),e(re,Dn),e(re,Ae),e(Ae,yn),e(re,Pn),e(l,In),e(l,M),x(se,M,null),e(M,bn),e(M,Ge),e(Ge,An),e(M,Gn),e(M,R),e(R,Sn),e(R,Se),e(Se,Vn),e(R,Cn),e(R,Ve),e(Ve,Mn),e(R,Ln),e(l,Nn),e(l,L),x(ie,L,null),e(L,On),e(L,Ce),e(Ce,Rn),e(L,Hn),e(L,ae),e(ae,Bn),e(ae,Me),e(Me,Fn),e(ae,Jn),e(l,Un),e(l,N),x(ce,N,null),e(N,Wn),e(N,de),e(de,jn),e(de,ve),e(ve,Kn),e(de,Qn),e(N,Xn),e(N,he),e(he,Yn),e(he,ke),e(ke,Zn),e(he,et),e(l,nt),e(l,U),x(pe,U,null),e(U,tt),e(U,Le),e(Le,ot),e(l,rt),e(l,W),x(le,W,null),e(W,st),e(W,Ne),e(Ne,it),Oe=!0},p:Mt,i(h){Oe||(w(d.$$.fragment,h),w(b.$$.fragment,h),w(Q.$$.fragment,h),w(X.$$.fragment,h),w(Y.$$.fragment,h),w(Z.$$.fragment,h),w(ne.$$.fragment,h),w(oe.$$.fragment,h),w(se.$$.fragment,h),w(ie.$$.fragment,h),w(ce.$$.fragment,h),w(pe.$$.fragment,h),w(le.$$.fragment,h),Oe=!0)},o(h){q(d.$$.fragment,h),q(b.$$.fragment,h),q(Q.$$.fragment,h),q(X.$$.fragment,h),q(Y.$$.fragment,h),q(Z.$$.fragment,h),q(ne.$$.fragment,h),q(oe.$$.fragment,h),q(se.$$.fragment,h),q(ie.$$.fragment,h),q(ce.$$.fragment,h),q(pe.$$.fragment,h),q(le.$$.fragment,h),Oe=!1},d(h){h&&o(t),z(d),h&&o(P),h&&o(l),z(b),z(Q),z(X),z(Y),z(Z),z(ne),z(oe),z(se),z(ie),z(ce),z(pe),z(le)}}}function Ot(I){let t,p;return t=new ct({props:{$$slots:{default:[Nt]},$$scope:{ctx:I}}}),{c(){k(t.$$.fragment)},l(n){E(t.$$.fragment,n)},m(n,d){x(t,n,d),p=!0},p(n,d){const $={};d&2&&($.$$scope={dirty:d,ctx:n}),t.$set($)},i(n){p||(w(t.$$.fragment,n),p=!0)},o(n){q(t.$$.fragment,n),p=!1},d(n){z(t,n)}}}function Rt(I){let t,p,n,d,$;return{c(){t=r("p"),p=a("The Rust API Reference is available directly on the "),n=r("a"),d=a("Docs.rs"),$=a(" website."),this.h()},l(v){t=s(v,"P",{});var D=i(t);p=c(D,"The Rust API Reference is available directly on the "),n=s(D,"A",{href:!0,rel:!0});var P=i(n);d=c(P,"Docs.rs"),P.forEach(o),$=c(D," website."),D.forEach(o),this.h()},h(){u(n,"href","https://docs.rs/tokenizers/latest/tokenizers/"),u(n,"rel","nofollow")},m(v,D){K(v,t,D),e(t,p),e(t,n),e(n,d),e(t,$)},d(v){v&&o(t)}}}function Ht(I){let t,p;return t=new ct({props:{$$slots:{default:[Rt]},$$scope:{ctx:I}}}),{c(){k(t.$$.fragment)},l(n){E(t.$$.fragment,n)},m(n,d){x(t,n,d),p=!0},p(n,d){const $={};d&2&&($.$$scope={dirty:d,ctx:n}),t.$set($)},i(n){p||(w(t.$$.fragment,n),p=!0)},o(n){q(t.$$.fragment,n),p=!1},d(n){z(t,n)}}}function Bt(I){let t,p;return{c(){t=r("p"),p=a("The node API has not been documented yet.")},l(n){t=s(n,"P",{});var d=i(t);p=c(d,"The node API has not been documented yet."),d.forEach(o)},m(n,d){K(n,t,d),e(t,p)},d(n){n&&o(t)}}}function Ft(I){let t,p;return t=new ct({props:{$$slots:{default:[Bt]},$$scope:{ctx:I}}}),{c(){k(t.$$.fragment)},l(n){E(t.$$.fragment,n)},m(n,d){x(t,n,d),p=!0},p(n,d){const $={};d&2&&($.$$scope={dirty:d,ctx:n}),t.$set($)},i(n){p||(w(t.$$.fragment,n),p=!0)},o(n){q(t.$$.fragment,n),p=!1},d(n){z(t,n)}}}function Jt(I){let t,p,n,d,$,v,D,P,l,b,A,y;return v=new bt({}),A=new Lt({props:{python:!0,rust:!0,node:!0,$$slots:{node:[Ft],rust:[Ht],python:[Ot]},$$scope:{ctx:I}}}),{c(){t=r("meta"),p=g(),n=r("h1"),d=r("a"),$=r("span"),k(v.$$.fragment),D=g(),P=r("span"),l=a("Encoding"),b=g(),k(A.$$.fragment),this.h()},l(_){const T=Vt('[data-svelte="svelte-1phssyn"]',document.head);t=s(T,"META",{name:!0,content:!0}),T.forEach(o),p=f(_),n=s(_,"H1",{class:!0});var S=i(n);d=s(S,"A",{id:!0,class:!0,href:!0});var ge=i(d);$=s(ge,"SPAN",{});var O=i($);E(v.$$.fragment,O),O.forEach(o),ge.forEach(o),D=f(S),P=s(S,"SPAN",{});var fe=i(P);l=c(fe,"Encoding"),fe.forEach(o),S.forEach(o),b=f(_),E(A.$$.fragment,_),this.h()},h(){u(t,"name","hf:doc:metadata"),u(t,"content",JSON.stringify(Ut)),u(d,"id","encoding"),u(d,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(d,"href","#encoding"),u(n,"class","relative group")},m(_,T){e(document.head,t),K(_,p,T),K(_,n,T),e(n,d),e(d,$),x(v,$,null),e(n,D),e(n,P),e(P,l),K(_,b,T),x(A,_,T),y=!0},p(_,[T]){const S={};T&2&&(S.$$scope={dirty:T,ctx:_}),A.$set(S)},i(_){y||(w(v.$$.fragment,_),w(A.$$.fragment,_),y=!0)},o(_){q(v.$$.fragment,_),q(A.$$.fragment,_),y=!1},d(_){o(t),_&&o(p),_&&o(n),z(v),_&&o(b),z(A,_)}}}const Ut={local:"encoding",sections:[{local:"tokenizers.Encoding]][[tokenizers.Encoding",title:"Encoding"}],title:"Encoding"};function Wt(I){return Ct(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Yt extends At{constructor(t){super();Gt(this,t,Wt,Jt,St,{})}}export{Yt as default,Ut as metadata};
