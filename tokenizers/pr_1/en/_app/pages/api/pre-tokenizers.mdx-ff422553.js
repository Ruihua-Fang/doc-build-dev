import{S as Ki,i as Ji,s as Gi,e as s,k as a,w as v,t as h,M as Qi,c as n,d as r,m as l,a as i,x as z,h as f,b as o,F as t,g as k,y as $,q as _,o as g,B as w,v as Xi,L as Yi}from"../../chunks/vendor-0d3f0756.js";import{D as x}from"../../chunks/Docstring-f752f2c3.js";import{C as Ri}from"../../chunks/CodeBlock-7b0cb15c.js";import{I}from"../../chunks/IconCopyLink-9193371d.js";import{T as Zi,M as Bn}from"../../chunks/TokenizersLanguageContent-ca787841.js";function eo(W){let c,m,p,u,y,E,D,C,T,N,L,j,P,A,q,st,ye,M,fe,kt,Ee,Jr,ut,Gr,ar,B,Se,Qr,mt,Xr,Yr,vt,Zr,es,O,Te,ts,zt,rs,ss,$t,ns,lr,Q,de,_t,be,is,gt,os,pr,X,xe,as,nt,ls,wt,ps,cr,Y,ke,Pt,De,cs,yt,hs,hr,S,Ae,fs,Et,ds,ks,St,us,ms,Ce,vs,Tt,zs,$s,qe,fr,Z,ue,bt,Be,_s,xt,gs,dr,U,Ie,ws,Dt,Ps,ys,At,Es,kr,ee,me,Ct,We,Ss,qt,Ts,ur,b,Ne,bs,Bt,xs,Ds,It,As,Cs,F,Le,qs,Me,Bs,Wt,Is,Ws,Ns,R,Ls,Nt,Ms,Hs,Lt,Us,Vs,Mt,js,Os,K,He,Fs,Ht,Rs,Ks,J,Js,it,Gs,Qs,Ut,Xs,Ys,Vt,Zs,mr,te,ve,jt,Ue,en,Ot,tn,vr,re,Ve,rn,Ft,sn,zr,se,ze,Rt,je,nn,Kt,on,$r,ne,Oe,an,Jt,ln,_r,ie,$e,Gt,Fe,pn,Qt,cn,gr,V,Re,hn,Xt,fn,dn,Yt,kn,wr,oe,_e,Zt,Ke,un,er,mn,Pr,ae,Je,vn,Ge,zn,Qe,$n,_n,yr,le,ge,tr,Xe,gn,rr,wn,Er,pe,Ye,Pn,ot,yn,sr,En,Sr,ce,we,nr,Ze,Sn,ir,Tn,Tr,he,et,bn,at,xn,or,Dn,br;return u=new I({}),N=new x({props:{name:"class tokenizers.pre_tokenizers.BertPreTokenizer",anchor:"tokenizers.pre_tokenizers.BertPreTokenizer",parameters:[]}}),Ee=new I({}),Se=new x({props:{name:"class tokenizers.pre_tokenizers.ByteLevel",anchor:"tokenizers.pre_tokenizers.ByteLevel",parameters:[{name:"add_prefix_space",val:" = True"}],parametersDescription:[{anchor:"tokenizers.pre_tokenizers.ByteLevel.add_prefix_space",description:`<strong>add_prefix_space</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to add a space to the first word if there isn&#x2019;t already one. This
lets us treat <em>hello</em> exactly like <em>say hello</em>.`,name:"add_prefix_space"}]}}),Te=new x({props:{name:"alphabet",anchor:"tokenizers.pre_tokenizers.ByteLevel.alphabet",parameters:[],returnDescription:`
<p>A list of characters that compose the alphabet</p>
`,returnType:`
<p><code>List[str]</code></p>
`}}),be=new I({}),xe=new x({props:{name:"class tokenizers.pre_tokenizers.CharDelimiterSplit",anchor:"tokenizers.pre_tokenizers.CharDelimiterSplit",parameters:""}}),De=new I({}),Ae=new x({props:{name:"class tokenizers.pre_tokenizers.Digits",anchor:"tokenizers.pre_tokenizers.Digits",parameters:[{name:"individual_digits",val:" = False"}],parametersDescription:[{anchor:"tokenizers.pre_tokenizers.Digits.individual_digits",description:"<strong>individual_digits</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;",name:"individual_digits"}]}}),Ce=new Ri({props:{code:'"Call 123 please" -> "Call ", "1", "2", "3", " please"',highlighted:'<span class="hljs-string">&quot;Call 123 please&quot;</span> -&gt; <span class="hljs-string">&quot;Call &quot;</span>, <span class="hljs-string">&quot;1&quot;</span>, <span class="hljs-string">&quot;2&quot;</span>, <span class="hljs-string">&quot;3&quot;</span>, <span class="hljs-string">&quot; please&quot;</span>'}}),qe=new Ri({props:{code:'"Call 123 please" -> "Call ", "123", " please"',highlighted:'<span class="hljs-string">&quot;Call 123 please&quot;</span> -&gt; <span class="hljs-string">&quot;Call &quot;</span>, <span class="hljs-string">&quot;123&quot;</span>, <span class="hljs-string">&quot; please&quot;</span>'}}),Be=new I({}),Ie=new x({props:{name:"class tokenizers.pre_tokenizers.Metaspace",anchor:"tokenizers.pre_tokenizers.Metaspace",parameters:[{name:"replacement",val:" = '_'"},{name:"add_prefix_space",val:" = True"}],parametersDescription:[{anchor:"tokenizers.pre_tokenizers.Metaspace.replacement",description:`<strong>replacement</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&#x2581;</code>) &#x2014;
The replacement character. Must be exactly one character. By default we
use the <em>&#x2581;</em> (U+2581) meta symbol (Same as in SentencePiece).`,name:"replacement"},{anchor:"tokenizers.pre_tokenizers.Metaspace.add_prefix_space",description:`<strong>add_prefix_space</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to add a space to the first word if there isn&#x2019;t already one. This
lets us treat <em>hello</em> exactly like <em>say hello</em>.`,name:"add_prefix_space"}]}}),We=new I({}),Ne=new x({props:{name:"class tokenizers.pre_tokenizers.PreTokenizer",anchor:"tokenizers.pre_tokenizers.PreTokenizer",parameters:[]}}),Le=new x({props:{name:"pre_tokenize",anchor:"tokenizers.pre_tokenizers.PreTokenizer.pre_tokenize",parameters:[{name:"pretok",val:""}],parametersDescription:[{anchor:"tokenizers.pre_tokenizers.PreTokenizer.pre_tokenize.pretok",description:"<strong>pretok</strong> (<code>PreTokenizedString) -- The pre-tokenized string on which to apply this :class:</code>~tokenizers.pre_tokenizers.PreTokenizer`",name:"pretok"}]}}),He=new x({props:{name:"pre_tokenize_str",anchor:"tokenizers.pre_tokenizers.PreTokenizer.pre_tokenize_str",parameters:[{name:"sequence",val:""}],parametersDescription:[{anchor:"tokenizers.pre_tokenizers.PreTokenizer.pre_tokenize_str.sequence",description:`<strong>sequence</strong> (<code>str</code>) &#x2014;
A string to pre-tokeize`,name:"sequence"}],returnDescription:`
<p>A list of tuple with the pre-tokenized parts and their offsets</p>
`,returnType:`
<p><code>List[Tuple[str, Offsets]]</code></p>
`}}),Ue=new I({}),Ve=new x({props:{name:"class tokenizers.pre_tokenizers.Punctuation",anchor:"tokenizers.pre_tokenizers.Punctuation",parameters:[{name:"behavior",val:" = 'isolated'"}],parametersDescription:[{anchor:"tokenizers.pre_tokenizers.Punctuation.behavior",description:`<strong>behavior</strong> (<code>SplitDelimiterBehavior</code>) &#x2014;
The behavior to use when splitting.
Choices: &#x201C;removed&#x201D;, &#x201C;isolated&#x201D; (default), &#x201C;merged_with_previous&#x201D;, &#x201C;merged_with_next&#x201D;,
&#x201C;contiguous&#x201D;`,name:"behavior"}]}}),je=new I({}),Oe=new x({props:{name:"class tokenizers.pre_tokenizers.Sequence",anchor:"tokenizers.pre_tokenizers.Sequence",parameters:[{name:"pretokenizers",val:""}]}}),Fe=new I({}),Re=new x({props:{name:"class tokenizers.pre_tokenizers.Split",anchor:"tokenizers.pre_tokenizers.Split",parameters:[{name:"pattern",val:""},{name:"behavior",val:""},{name:"invert",val:" = False"}],parametersDescription:[{anchor:"tokenizers.pre_tokenizers.Split.pattern",description:`<strong>pattern</strong> (<code>str</code> or <code>Regex</code>) &#x2014;
A pattern used to split the string. Usually a string or a Regex`,name:"pattern"},{anchor:"tokenizers.pre_tokenizers.Split.behavior",description:`<strong>behavior</strong> (<code>SplitDelimiterBehavior</code>) &#x2014;
The behavior to use when splitting.
Choices: &#x201C;removed&#x201D;, &#x201C;isolated&#x201D;, &#x201C;merged_with_previous&#x201D;, &#x201C;merged_with_next&#x201D;,
&#x201C;contiguous&#x201D;`,name:"behavior"},{anchor:"tokenizers.pre_tokenizers.Split.invert",description:`<strong>invert</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to invert the pattern.`,name:"invert"}]}}),Ke=new I({}),Je=new x({props:{name:"class tokenizers.pre_tokenizers.UnicodeScripts",anchor:"tokenizers.pre_tokenizers.UnicodeScripts",parameters:[]}}),Xe=new I({}),Ye=new x({props:{name:"class tokenizers.pre_tokenizers.Whitespace",anchor:"tokenizers.pre_tokenizers.Whitespace",parameters:[]}}),Ze=new I({}),et=new x({props:{name:"class tokenizers.pre_tokenizers.WhitespaceSplit",anchor:"tokenizers.pre_tokenizers.WhitespaceSplit",parameters:[]}}),{c(){c=s("h2"),m=s("a"),p=s("span"),v(u.$$.fragment),y=a(),E=s("span"),D=h("BertPreTokenizer"),C=a(),T=s("div"),v(N.$$.fragment),L=a(),j=s("p"),P=h("BertPreTokenizer"),A=a(),q=s("p"),st=h(`This pre-tokenizer splits tokens on spaces, and also on punctuation.
Each occurence of a punctuation character will be treated separately.`),ye=a(),M=s("h2"),fe=s("a"),kt=s("span"),v(Ee.$$.fragment),Jr=a(),ut=s("span"),Gr=h("ByteLevel"),ar=a(),B=s("div"),v(Se.$$.fragment),Qr=a(),mt=s("p"),Xr=h("ByteLevel PreTokenizer"),Yr=a(),vt=s("p"),Zr=h(`This pre-tokenizer takes care of replacing all bytes of the given string
with a corresponding representation, as well as splitting into words.`),es=a(),O=s("div"),v(Te.$$.fragment),ts=a(),zt=s("p"),rs=h("Returns the alphabet used by this PreTokenizer."),ss=a(),$t=s("p"),ns=h(`Since the ByteLevel works as its name suggests, at the byte level, it
encodes each byte value to a unique visible character. This means that there is a
total of 256 different characters composing this alphabet.`),lr=a(),Q=s("h2"),de=s("a"),_t=s("span"),v(be.$$.fragment),is=a(),gt=s("span"),os=h("CharDelimiterSplit"),pr=a(),X=s("div"),v(xe.$$.fragment),as=a(),nt=s("p"),ls=h("This pre-tokenizer simply splits on the provided char. Works like "),wt=s("code"),ps=h(".split(delimiter)"),cr=a(),Y=s("h2"),ke=s("a"),Pt=s("span"),v(De.$$.fragment),cs=a(),yt=s("span"),hs=h("Digits"),hr=a(),S=s("div"),v(Ae.$$.fragment),fs=a(),Et=s("p"),ds=h("This pre-tokenizer simply splits using the digits in separate tokens"),ks=a(),St=s("p"),us=h("If set to True, digits will each be separated as follows:"),ms=a(),v(Ce.$$.fragment),vs=a(),Tt=s("p"),zs=h("If set to False, digits will grouped as follows:"),$s=a(),v(qe.$$.fragment),fr=a(),Z=s("h2"),ue=s("a"),bt=s("span"),v(Be.$$.fragment),_s=a(),xt=s("span"),gs=h("Metaspace"),dr=a(),U=s("div"),v(Ie.$$.fragment),ws=a(),Dt=s("p"),Ps=h("Metaspace pre-tokenizer"),ys=a(),At=s("p"),Es=h(`This pre-tokenizer replaces any whitespace by the provided replacement character.
It then tries to split on these spaces.`),kr=a(),ee=s("h2"),me=s("a"),Ct=s("span"),v(We.$$.fragment),Ss=a(),qt=s("span"),Ts=h("PreTokenizer"),ur=a(),b=s("div"),v(Ne.$$.fragment),bs=a(),Bt=s("p"),xs=h("Base class for all pre-tokenizers"),Ds=a(),It=s("p"),As=h(`This class is not supposed to be instantiated directly. Instead, any implementation of a
PreTokenizer will return an instance of this class when instantiated.`),Cs=a(),F=s("div"),v(Le.$$.fragment),qs=a(),Me=s("p"),Bs=h("Pre-tokenize a "),Wt=s("code"),Is=h("PyPreTokenizedString"),Ws=h(" in-place"),Ns=a(),R=s("p"),Ls=h("This method allows to modify a "),Nt=s("code"),Ms=h("PreTokenizedString"),Hs=h(` to
keep track of the pre-tokenization, and leverage the capabilities of the
`),Lt=s("code"),Us=h("PreTokenizedString"),Vs=h(`. If you just want to see the result of
the pre-tokenization of a raw string, you can use
`),Mt=s("code"),js=h("pre_tokenize_str()"),Os=a(),K=s("div"),v(He.$$.fragment),Fs=a(),Ht=s("p"),Rs=h("Pre tokenize the given string"),Ks=a(),J=s("p"),Js=h(`This method provides a way to visualize the effect of a
`),it=s("a"),Gs=h("PreTokenizer"),Qs=h(` but it does not keep track of the
alignment, nor does it provide all the capabilities of the
`),Ut=s("code"),Xs=h("PreTokenizedString"),Ys=h(`. If you need some of these, you can use
`),Vt=s("code"),Zs=h("pre_tokenize()"),mr=a(),te=s("h2"),ve=s("a"),jt=s("span"),v(Ue.$$.fragment),en=a(),Ot=s("span"),tn=h("Punctuation"),vr=a(),re=s("div"),v(Ve.$$.fragment),rn=a(),Ft=s("p"),sn=h("This pre-tokenizer simply splits on punctuation as individual characters."),zr=a(),se=s("h2"),ze=s("a"),Rt=s("span"),v(je.$$.fragment),nn=a(),Kt=s("span"),on=h("Sequence"),$r=a(),ne=s("div"),v(Oe.$$.fragment),an=a(),Jt=s("p"),ln=h("This pre-tokenizer composes other pre_tokenizers and applies them in sequence"),_r=a(),ie=s("h2"),$e=s("a"),Gt=s("span"),v(Fe.$$.fragment),pn=a(),Qt=s("span"),cn=h("Split"),gr=a(),V=s("div"),v(Re.$$.fragment),hn=a(),Xt=s("p"),fn=h("Split PreTokenizer"),dn=a(),Yt=s("p"),kn=h(`This versatile pre-tokenizer splits using the provided pattern and
according to the provided behavior. The pattern can be inverted by
making use of the invert flag.`),wr=a(),oe=s("h2"),_e=s("a"),Zt=s("span"),v(Ke.$$.fragment),un=a(),er=s("span"),mn=h("UnicodeScripts"),Pr=a(),ae=s("div"),v(Je.$$.fragment),vn=a(),Ge=s("p"),zn=h(`This pre-tokenizer splits on characters that belong to different language family
It roughly follows `),Qe=s("a"),$n=h("https://github.com/google/sentencepiece/blob/master/data/Scripts.txt"),_n=h(`
Actually Hiragana and Katakana are fused with Han, and 0x30FC is Han too.
This mimicks SentencePiece Unigram implementation.`),yr=a(),le=s("h2"),ge=s("a"),tr=s("span"),v(Xe.$$.fragment),gn=a(),rr=s("span"),wn=h("Whitespace"),Er=a(),pe=s("div"),v(Ye.$$.fragment),Pn=a(),ot=s("p"),yn=h("This pre-tokenizer simply splits using the following regex: "),sr=s("code"),En=h("\\w+|[^\\w\\s]+"),Sr=a(),ce=s("h2"),we=s("a"),nr=s("span"),v(Ze.$$.fragment),Sn=a(),ir=s("span"),Tn=h("WhitespaceSplit"),Tr=a(),he=s("div"),v(et.$$.fragment),bn=a(),at=s("p"),xn=h("This pre-tokenizer simply splits on the whitespace. Works like "),or=s("code"),Dn=h(".split()"),this.h()},l(e){c=n(e,"H2",{class:!0});var d=i(c);m=n(d,"A",{id:!0,class:!0,href:!0});var In=i(m);p=n(In,"SPAN",{});var Wn=i(p);z(u.$$.fragment,Wn),Wn.forEach(r),In.forEach(r),y=l(d),E=n(d,"SPAN",{});var Nn=i(E);D=f(Nn,"BertPreTokenizer"),Nn.forEach(r),d.forEach(r),C=l(e),T=n(e,"DIV",{class:!0});var lt=i(T);z(N.$$.fragment,lt),L=l(lt),j=n(lt,"P",{});var Ln=i(j);P=f(Ln,"BertPreTokenizer"),Ln.forEach(r),A=l(lt),q=n(lt,"P",{});var Mn=i(q);st=f(Mn,`This pre-tokenizer splits tokens on spaces, and also on punctuation.
Each occurence of a punctuation character will be treated separately.`),Mn.forEach(r),lt.forEach(r),ye=l(e),M=n(e,"H2",{class:!0});var xr=i(M);fe=n(xr,"A",{id:!0,class:!0,href:!0});var Hn=i(fe);kt=n(Hn,"SPAN",{});var Un=i(kt);z(Ee.$$.fragment,Un),Un.forEach(r),Hn.forEach(r),Jr=l(xr),ut=n(xr,"SPAN",{});var Vn=i(ut);Gr=f(Vn,"ByteLevel"),Vn.forEach(r),xr.forEach(r),ar=l(e),B=n(e,"DIV",{class:!0});var Pe=i(B);z(Se.$$.fragment,Pe),Qr=l(Pe),mt=n(Pe,"P",{});var jn=i(mt);Xr=f(jn,"ByteLevel PreTokenizer"),jn.forEach(r),Yr=l(Pe),vt=n(Pe,"P",{});var On=i(vt);Zr=f(On,`This pre-tokenizer takes care of replacing all bytes of the given string
with a corresponding representation, as well as splitting into words.`),On.forEach(r),es=l(Pe),O=n(Pe,"DIV",{class:!0});var pt=i(O);z(Te.$$.fragment,pt),ts=l(pt),zt=n(pt,"P",{});var Fn=i(zt);rs=f(Fn,"Returns the alphabet used by this PreTokenizer."),Fn.forEach(r),ss=l(pt),$t=n(pt,"P",{});var Rn=i($t);ns=f(Rn,`Since the ByteLevel works as its name suggests, at the byte level, it
encodes each byte value to a unique visible character. This means that there is a
total of 256 different characters composing this alphabet.`),Rn.forEach(r),pt.forEach(r),Pe.forEach(r),lr=l(e),Q=n(e,"H2",{class:!0});var Dr=i(Q);de=n(Dr,"A",{id:!0,class:!0,href:!0});var Kn=i(de);_t=n(Kn,"SPAN",{});var Jn=i(_t);z(be.$$.fragment,Jn),Jn.forEach(r),Kn.forEach(r),is=l(Dr),gt=n(Dr,"SPAN",{});var Gn=i(gt);os=f(Gn,"CharDelimiterSplit"),Gn.forEach(r),Dr.forEach(r),pr=l(e),X=n(e,"DIV",{class:!0});var Ar=i(X);z(xe.$$.fragment,Ar),as=l(Ar),nt=n(Ar,"P",{});var An=i(nt);ls=f(An,"This pre-tokenizer simply splits on the provided char. Works like "),wt=n(An,"CODE",{});var Qn=i(wt);ps=f(Qn,".split(delimiter)"),Qn.forEach(r),An.forEach(r),Ar.forEach(r),cr=l(e),Y=n(e,"H2",{class:!0});var Cr=i(Y);ke=n(Cr,"A",{id:!0,class:!0,href:!0});var Xn=i(ke);Pt=n(Xn,"SPAN",{});var Yn=i(Pt);z(De.$$.fragment,Yn),Yn.forEach(r),Xn.forEach(r),cs=l(Cr),yt=n(Cr,"SPAN",{});var Zn=i(yt);hs=f(Zn,"Digits"),Zn.forEach(r),Cr.forEach(r),hr=l(e),S=n(e,"DIV",{class:!0});var H=i(S);z(Ae.$$.fragment,H),fs=l(H),Et=n(H,"P",{});var ei=i(Et);ds=f(ei,"This pre-tokenizer simply splits using the digits in separate tokens"),ei.forEach(r),ks=l(H),St=n(H,"P",{});var ti=i(St);us=f(ti,"If set to True, digits will each be separated as follows:"),ti.forEach(r),ms=l(H),z(Ce.$$.fragment,H),vs=l(H),Tt=n(H,"P",{});var ri=i(Tt);zs=f(ri,"If set to False, digits will grouped as follows:"),ri.forEach(r),$s=l(H),z(qe.$$.fragment,H),H.forEach(r),fr=l(e),Z=n(e,"H2",{class:!0});var qr=i(Z);ue=n(qr,"A",{id:!0,class:!0,href:!0});var si=i(ue);bt=n(si,"SPAN",{});var ni=i(bt);z(Be.$$.fragment,ni),ni.forEach(r),si.forEach(r),_s=l(qr),xt=n(qr,"SPAN",{});var ii=i(xt);gs=f(ii,"Metaspace"),ii.forEach(r),qr.forEach(r),dr=l(e),U=n(e,"DIV",{class:!0});var ct=i(U);z(Ie.$$.fragment,ct),ws=l(ct),Dt=n(ct,"P",{});var oi=i(Dt);Ps=f(oi,"Metaspace pre-tokenizer"),oi.forEach(r),ys=l(ct),At=n(ct,"P",{});var ai=i(At);Es=f(ai,`This pre-tokenizer replaces any whitespace by the provided replacement character.
It then tries to split on these spaces.`),ai.forEach(r),ct.forEach(r),kr=l(e),ee=n(e,"H2",{class:!0});var Br=i(ee);me=n(Br,"A",{id:!0,class:!0,href:!0});var li=i(me);Ct=n(li,"SPAN",{});var pi=i(Ct);z(We.$$.fragment,pi),pi.forEach(r),li.forEach(r),Ss=l(Br),qt=n(Br,"SPAN",{});var ci=i(qt);Ts=f(ci,"PreTokenizer"),ci.forEach(r),Br.forEach(r),ur=l(e),b=n(e,"DIV",{class:!0});var G=i(b);z(Ne.$$.fragment,G),bs=l(G),Bt=n(G,"P",{});var hi=i(Bt);xs=f(hi,"Base class for all pre-tokenizers"),hi.forEach(r),Ds=l(G),It=n(G,"P",{});var fi=i(It);As=f(fi,`This class is not supposed to be instantiated directly. Instead, any implementation of a
PreTokenizer will return an instance of this class when instantiated.`),fi.forEach(r),Cs=l(G),F=n(G,"DIV",{class:!0});var ht=i(F);z(Le.$$.fragment,ht),qs=l(ht),Me=n(ht,"P",{});var Ir=i(Me);Bs=f(Ir,"Pre-tokenize a "),Wt=n(Ir,"CODE",{});var di=i(Wt);Is=f(di,"PyPreTokenizedString"),di.forEach(r),Ws=f(Ir," in-place"),Ir.forEach(r),Ns=l(ht),R=n(ht,"P",{});var tt=i(R);Ls=f(tt,"This method allows to modify a "),Nt=n(tt,"CODE",{});var ki=i(Nt);Ms=f(ki,"PreTokenizedString"),ki.forEach(r),Hs=f(tt,` to
keep track of the pre-tokenization, and leverage the capabilities of the
`),Lt=n(tt,"CODE",{});var ui=i(Lt);Us=f(ui,"PreTokenizedString"),ui.forEach(r),Vs=f(tt,`. If you just want to see the result of
the pre-tokenization of a raw string, you can use
`),Mt=n(tt,"CODE",{});var mi=i(Mt);js=f(mi,"pre_tokenize_str()"),mi.forEach(r),tt.forEach(r),ht.forEach(r),Os=l(G),K=n(G,"DIV",{class:!0});var ft=i(K);z(He.$$.fragment,ft),Fs=l(ft),Ht=n(ft,"P",{});var vi=i(Ht);Rs=f(vi,"Pre tokenize the given string"),vi.forEach(r),Ks=l(ft),J=n(ft,"P",{});var rt=i(J);Js=f(rt,`This method provides a way to visualize the effect of a
`),it=n(rt,"A",{href:!0});var zi=i(it);Gs=f(zi,"PreTokenizer"),zi.forEach(r),Qs=f(rt,` but it does not keep track of the
alignment, nor does it provide all the capabilities of the
`),Ut=n(rt,"CODE",{});var $i=i(Ut);Xs=f($i,"PreTokenizedString"),$i.forEach(r),Ys=f(rt,`. If you need some of these, you can use
`),Vt=n(rt,"CODE",{});var _i=i(Vt);Zs=f(_i,"pre_tokenize()"),_i.forEach(r),rt.forEach(r),ft.forEach(r),G.forEach(r),mr=l(e),te=n(e,"H2",{class:!0});var Wr=i(te);ve=n(Wr,"A",{id:!0,class:!0,href:!0});var gi=i(ve);jt=n(gi,"SPAN",{});var wi=i(jt);z(Ue.$$.fragment,wi),wi.forEach(r),gi.forEach(r),en=l(Wr),Ot=n(Wr,"SPAN",{});var Pi=i(Ot);tn=f(Pi,"Punctuation"),Pi.forEach(r),Wr.forEach(r),vr=l(e),re=n(e,"DIV",{class:!0});var Nr=i(re);z(Ve.$$.fragment,Nr),rn=l(Nr),Ft=n(Nr,"P",{});var yi=i(Ft);sn=f(yi,"This pre-tokenizer simply splits on punctuation as individual characters."),yi.forEach(r),Nr.forEach(r),zr=l(e),se=n(e,"H2",{class:!0});var Lr=i(se);ze=n(Lr,"A",{id:!0,class:!0,href:!0});var Ei=i(ze);Rt=n(Ei,"SPAN",{});var Si=i(Rt);z(je.$$.fragment,Si),Si.forEach(r),Ei.forEach(r),nn=l(Lr),Kt=n(Lr,"SPAN",{});var Ti=i(Kt);on=f(Ti,"Sequence"),Ti.forEach(r),Lr.forEach(r),$r=l(e),ne=n(e,"DIV",{class:!0});var Mr=i(ne);z(Oe.$$.fragment,Mr),an=l(Mr),Jt=n(Mr,"P",{});var bi=i(Jt);ln=f(bi,"This pre-tokenizer composes other pre_tokenizers and applies them in sequence"),bi.forEach(r),Mr.forEach(r),_r=l(e),ie=n(e,"H2",{class:!0});var Hr=i(ie);$e=n(Hr,"A",{id:!0,class:!0,href:!0});var xi=i($e);Gt=n(xi,"SPAN",{});var Di=i(Gt);z(Fe.$$.fragment,Di),Di.forEach(r),xi.forEach(r),pn=l(Hr),Qt=n(Hr,"SPAN",{});var Ai=i(Qt);cn=f(Ai,"Split"),Ai.forEach(r),Hr.forEach(r),gr=l(e),V=n(e,"DIV",{class:!0});var dt=i(V);z(Re.$$.fragment,dt),hn=l(dt),Xt=n(dt,"P",{});var Ci=i(Xt);fn=f(Ci,"Split PreTokenizer"),Ci.forEach(r),dn=l(dt),Yt=n(dt,"P",{});var qi=i(Yt);kn=f(qi,`This versatile pre-tokenizer splits using the provided pattern and
according to the provided behavior. The pattern can be inverted by
making use of the invert flag.`),qi.forEach(r),dt.forEach(r),wr=l(e),oe=n(e,"H2",{class:!0});var Ur=i(oe);_e=n(Ur,"A",{id:!0,class:!0,href:!0});var Bi=i(_e);Zt=n(Bi,"SPAN",{});var Ii=i(Zt);z(Ke.$$.fragment,Ii),Ii.forEach(r),Bi.forEach(r),un=l(Ur),er=n(Ur,"SPAN",{});var Wi=i(er);mn=f(Wi,"UnicodeScripts"),Wi.forEach(r),Ur.forEach(r),Pr=l(e),ae=n(e,"DIV",{class:!0});var Vr=i(ae);z(Je.$$.fragment,Vr),vn=l(Vr),Ge=n(Vr,"P",{});var jr=i(Ge);zn=f(jr,`This pre-tokenizer splits on characters that belong to different language family
It roughly follows `),Qe=n(jr,"A",{href:!0,rel:!0});var Ni=i(Qe);$n=f(Ni,"https://github.com/google/sentencepiece/blob/master/data/Scripts.txt"),Ni.forEach(r),_n=f(jr,`
Actually Hiragana and Katakana are fused with Han, and 0x30FC is Han too.
This mimicks SentencePiece Unigram implementation.`),jr.forEach(r),Vr.forEach(r),yr=l(e),le=n(e,"H2",{class:!0});var Or=i(le);ge=n(Or,"A",{id:!0,class:!0,href:!0});var Li=i(ge);tr=n(Li,"SPAN",{});var Mi=i(tr);z(Xe.$$.fragment,Mi),Mi.forEach(r),Li.forEach(r),gn=l(Or),rr=n(Or,"SPAN",{});var Hi=i(rr);wn=f(Hi,"Whitespace"),Hi.forEach(r),Or.forEach(r),Er=l(e),pe=n(e,"DIV",{class:!0});var Fr=i(pe);z(Ye.$$.fragment,Fr),Pn=l(Fr),ot=n(Fr,"P",{});var Cn=i(ot);yn=f(Cn,"This pre-tokenizer simply splits using the following regex: "),sr=n(Cn,"CODE",{});var Ui=i(sr);En=f(Ui,"\\w+|[^\\w\\s]+"),Ui.forEach(r),Cn.forEach(r),Fr.forEach(r),Sr=l(e),ce=n(e,"H2",{class:!0});var Rr=i(ce);we=n(Rr,"A",{id:!0,class:!0,href:!0});var Vi=i(we);nr=n(Vi,"SPAN",{});var ji=i(nr);z(Ze.$$.fragment,ji),ji.forEach(r),Vi.forEach(r),Sn=l(Rr),ir=n(Rr,"SPAN",{});var Oi=i(ir);Tn=f(Oi,"WhitespaceSplit"),Oi.forEach(r),Rr.forEach(r),Tr=l(e),he=n(e,"DIV",{class:!0});var Kr=i(he);z(et.$$.fragment,Kr),bn=l(Kr),at=n(Kr,"P",{});var qn=i(at);xn=f(qn,"This pre-tokenizer simply splits on the whitespace. Works like "),or=n(qn,"CODE",{});var Fi=i(or);Dn=f(Fi,".split()"),Fi.forEach(r),qn.forEach(r),Kr.forEach(r),this.h()},h(){o(m,"id","tokenizers.pre_tokenizers.BertPreTokenizer]][[tokenizers.pre_tokenizers.BertPreTokenizer"),o(m,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),o(m,"href","#tokenizers.pre_tokenizers.BertPreTokenizer]][[tokenizers.pre_tokenizers.BertPreTokenizer"),o(c,"class","relative group"),o(T,"class","docstring"),o(fe,"id","tokenizers.pre_tokenizers.ByteLevel]][[tokenizers.pre_tokenizers.ByteLevel"),o(fe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),o(fe,"href","#tokenizers.pre_tokenizers.ByteLevel]][[tokenizers.pre_tokenizers.ByteLevel"),o(M,"class","relative group"),o(O,"class","docstring"),o(B,"class","docstring"),o(de,"id","tokenizers.pre_tokenizers.CharDelimiterSplit]][[tokenizers.pre_tokenizers.CharDelimiterSplit"),o(de,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),o(de,"href","#tokenizers.pre_tokenizers.CharDelimiterSplit]][[tokenizers.pre_tokenizers.CharDelimiterSplit"),o(Q,"class","relative group"),o(X,"class","docstring"),o(ke,"id","tokenizers.pre_tokenizers.Digits]][[tokenizers.pre_tokenizers.Digits"),o(ke,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),o(ke,"href","#tokenizers.pre_tokenizers.Digits]][[tokenizers.pre_tokenizers.Digits"),o(Y,"class","relative group"),o(S,"class","docstring"),o(ue,"id","tokenizers.pre_tokenizers.Metaspace]][[tokenizers.pre_tokenizers.Metaspace"),o(ue,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),o(ue,"href","#tokenizers.pre_tokenizers.Metaspace]][[tokenizers.pre_tokenizers.Metaspace"),o(Z,"class","relative group"),o(U,"class","docstring"),o(me,"id","tokenizers.pre_tokenizers.PreTokenizer]][[tokenizers.pre_tokenizers.PreTokenizer"),o(me,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),o(me,"href","#tokenizers.pre_tokenizers.PreTokenizer]][[tokenizers.pre_tokenizers.PreTokenizer"),o(ee,"class","relative group"),o(F,"class","docstring"),o(it,"href","/docs/tokenizers/pr_1/en/api/pre-tokenizers#tokenizers.pre_tokenizers.PreTokenizer"),o(K,"class","docstring"),o(b,"class","docstring"),o(ve,"id","tokenizers.pre_tokenizers.Punctuation]][[tokenizers.pre_tokenizers.Punctuation"),o(ve,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),o(ve,"href","#tokenizers.pre_tokenizers.Punctuation]][[tokenizers.pre_tokenizers.Punctuation"),o(te,"class","relative group"),o(re,"class","docstring"),o(ze,"id","tokenizers.pre_tokenizers.Sequence]][[tokenizers.pre_tokenizers.Sequence"),o(ze,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),o(ze,"href","#tokenizers.pre_tokenizers.Sequence]][[tokenizers.pre_tokenizers.Sequence"),o(se,"class","relative group"),o(ne,"class","docstring"),o($e,"id","tokenizers.pre_tokenizers.Split]][[tokenizers.pre_tokenizers.Split"),o($e,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),o($e,"href","#tokenizers.pre_tokenizers.Split]][[tokenizers.pre_tokenizers.Split"),o(ie,"class","relative group"),o(V,"class","docstring"),o(_e,"id","tokenizers.pre_tokenizers.UnicodeScripts]][[tokenizers.pre_tokenizers.UnicodeScripts"),o(_e,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),o(_e,"href","#tokenizers.pre_tokenizers.UnicodeScripts]][[tokenizers.pre_tokenizers.UnicodeScripts"),o(oe,"class","relative group"),o(Qe,"href","https://github.com/google/sentencepiece/blob/master/data/Scripts.txt"),o(Qe,"rel","nofollow"),o(ae,"class","docstring"),o(ge,"id","tokenizers.pre_tokenizers.Whitespace]][[tokenizers.pre_tokenizers.Whitespace"),o(ge,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),o(ge,"href","#tokenizers.pre_tokenizers.Whitespace]][[tokenizers.pre_tokenizers.Whitespace"),o(le,"class","relative group"),o(pe,"class","docstring"),o(we,"id","tokenizers.pre_tokenizers.WhitespaceSplit]][[tokenizers.pre_tokenizers.WhitespaceSplit"),o(we,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),o(we,"href","#tokenizers.pre_tokenizers.WhitespaceSplit]][[tokenizers.pre_tokenizers.WhitespaceSplit"),o(ce,"class","relative group"),o(he,"class","docstring")},m(e,d){k(e,c,d),t(c,m),t(m,p),$(u,p,null),t(c,y),t(c,E),t(E,D),k(e,C,d),k(e,T,d),$(N,T,null),t(T,L),t(T,j),t(j,P),t(T,A),t(T,q),t(q,st),k(e,ye,d),k(e,M,d),t(M,fe),t(fe,kt),$(Ee,kt,null),t(M,Jr),t(M,ut),t(ut,Gr),k(e,ar,d),k(e,B,d),$(Se,B,null),t(B,Qr),t(B,mt),t(mt,Xr),t(B,Yr),t(B,vt),t(vt,Zr),t(B,es),t(B,O),$(Te,O,null),t(O,ts),t(O,zt),t(zt,rs),t(O,ss),t(O,$t),t($t,ns),k(e,lr,d),k(e,Q,d),t(Q,de),t(de,_t),$(be,_t,null),t(Q,is),t(Q,gt),t(gt,os),k(e,pr,d),k(e,X,d),$(xe,X,null),t(X,as),t(X,nt),t(nt,ls),t(nt,wt),t(wt,ps),k(e,cr,d),k(e,Y,d),t(Y,ke),t(ke,Pt),$(De,Pt,null),t(Y,cs),t(Y,yt),t(yt,hs),k(e,hr,d),k(e,S,d),$(Ae,S,null),t(S,fs),t(S,Et),t(Et,ds),t(S,ks),t(S,St),t(St,us),t(S,ms),$(Ce,S,null),t(S,vs),t(S,Tt),t(Tt,zs),t(S,$s),$(qe,S,null),k(e,fr,d),k(e,Z,d),t(Z,ue),t(ue,bt),$(Be,bt,null),t(Z,_s),t(Z,xt),t(xt,gs),k(e,dr,d),k(e,U,d),$(Ie,U,null),t(U,ws),t(U,Dt),t(Dt,Ps),t(U,ys),t(U,At),t(At,Es),k(e,kr,d),k(e,ee,d),t(ee,me),t(me,Ct),$(We,Ct,null),t(ee,Ss),t(ee,qt),t(qt,Ts),k(e,ur,d),k(e,b,d),$(Ne,b,null),t(b,bs),t(b,Bt),t(Bt,xs),t(b,Ds),t(b,It),t(It,As),t(b,Cs),t(b,F),$(Le,F,null),t(F,qs),t(F,Me),t(Me,Bs),t(Me,Wt),t(Wt,Is),t(Me,Ws),t(F,Ns),t(F,R),t(R,Ls),t(R,Nt),t(Nt,Ms),t(R,Hs),t(R,Lt),t(Lt,Us),t(R,Vs),t(R,Mt),t(Mt,js),t(b,Os),t(b,K),$(He,K,null),t(K,Fs),t(K,Ht),t(Ht,Rs),t(K,Ks),t(K,J),t(J,Js),t(J,it),t(it,Gs),t(J,Qs),t(J,Ut),t(Ut,Xs),t(J,Ys),t(J,Vt),t(Vt,Zs),k(e,mr,d),k(e,te,d),t(te,ve),t(ve,jt),$(Ue,jt,null),t(te,en),t(te,Ot),t(Ot,tn),k(e,vr,d),k(e,re,d),$(Ve,re,null),t(re,rn),t(re,Ft),t(Ft,sn),k(e,zr,d),k(e,se,d),t(se,ze),t(ze,Rt),$(je,Rt,null),t(se,nn),t(se,Kt),t(Kt,on),k(e,$r,d),k(e,ne,d),$(Oe,ne,null),t(ne,an),t(ne,Jt),t(Jt,ln),k(e,_r,d),k(e,ie,d),t(ie,$e),t($e,Gt),$(Fe,Gt,null),t(ie,pn),t(ie,Qt),t(Qt,cn),k(e,gr,d),k(e,V,d),$(Re,V,null),t(V,hn),t(V,Xt),t(Xt,fn),t(V,dn),t(V,Yt),t(Yt,kn),k(e,wr,d),k(e,oe,d),t(oe,_e),t(_e,Zt),$(Ke,Zt,null),t(oe,un),t(oe,er),t(er,mn),k(e,Pr,d),k(e,ae,d),$(Je,ae,null),t(ae,vn),t(ae,Ge),t(Ge,zn),t(Ge,Qe),t(Qe,$n),t(Ge,_n),k(e,yr,d),k(e,le,d),t(le,ge),t(ge,tr),$(Xe,tr,null),t(le,gn),t(le,rr),t(rr,wn),k(e,Er,d),k(e,pe,d),$(Ye,pe,null),t(pe,Pn),t(pe,ot),t(ot,yn),t(ot,sr),t(sr,En),k(e,Sr,d),k(e,ce,d),t(ce,we),t(we,nr),$(Ze,nr,null),t(ce,Sn),t(ce,ir),t(ir,Tn),k(e,Tr,d),k(e,he,d),$(et,he,null),t(he,bn),t(he,at),t(at,xn),t(at,or),t(or,Dn),br=!0},p:Yi,i(e){br||(_(u.$$.fragment,e),_(N.$$.fragment,e),_(Ee.$$.fragment,e),_(Se.$$.fragment,e),_(Te.$$.fragment,e),_(be.$$.fragment,e),_(xe.$$.fragment,e),_(De.$$.fragment,e),_(Ae.$$.fragment,e),_(Ce.$$.fragment,e),_(qe.$$.fragment,e),_(Be.$$.fragment,e),_(Ie.$$.fragment,e),_(We.$$.fragment,e),_(Ne.$$.fragment,e),_(Le.$$.fragment,e),_(He.$$.fragment,e),_(Ue.$$.fragment,e),_(Ve.$$.fragment,e),_(je.$$.fragment,e),_(Oe.$$.fragment,e),_(Fe.$$.fragment,e),_(Re.$$.fragment,e),_(Ke.$$.fragment,e),_(Je.$$.fragment,e),_(Xe.$$.fragment,e),_(Ye.$$.fragment,e),_(Ze.$$.fragment,e),_(et.$$.fragment,e),br=!0)},o(e){g(u.$$.fragment,e),g(N.$$.fragment,e),g(Ee.$$.fragment,e),g(Se.$$.fragment,e),g(Te.$$.fragment,e),g(be.$$.fragment,e),g(xe.$$.fragment,e),g(De.$$.fragment,e),g(Ae.$$.fragment,e),g(Ce.$$.fragment,e),g(qe.$$.fragment,e),g(Be.$$.fragment,e),g(Ie.$$.fragment,e),g(We.$$.fragment,e),g(Ne.$$.fragment,e),g(Le.$$.fragment,e),g(He.$$.fragment,e),g(Ue.$$.fragment,e),g(Ve.$$.fragment,e),g(je.$$.fragment,e),g(Oe.$$.fragment,e),g(Fe.$$.fragment,e),g(Re.$$.fragment,e),g(Ke.$$.fragment,e),g(Je.$$.fragment,e),g(Xe.$$.fragment,e),g(Ye.$$.fragment,e),g(Ze.$$.fragment,e),g(et.$$.fragment,e),br=!1},d(e){e&&r(c),w(u),e&&r(C),e&&r(T),w(N),e&&r(ye),e&&r(M),w(Ee),e&&r(ar),e&&r(B),w(Se),w(Te),e&&r(lr),e&&r(Q),w(be),e&&r(pr),e&&r(X),w(xe),e&&r(cr),e&&r(Y),w(De),e&&r(hr),e&&r(S),w(Ae),w(Ce),w(qe),e&&r(fr),e&&r(Z),w(Be),e&&r(dr),e&&r(U),w(Ie),e&&r(kr),e&&r(ee),w(We),e&&r(ur),e&&r(b),w(Ne),w(Le),w(He),e&&r(mr),e&&r(te),w(Ue),e&&r(vr),e&&r(re),w(Ve),e&&r(zr),e&&r(se),w(je),e&&r($r),e&&r(ne),w(Oe),e&&r(_r),e&&r(ie),w(Fe),e&&r(gr),e&&r(V),w(Re),e&&r(wr),e&&r(oe),w(Ke),e&&r(Pr),e&&r(ae),w(Je),e&&r(yr),e&&r(le),w(Xe),e&&r(Er),e&&r(pe),w(Ye),e&&r(Sr),e&&r(ce),w(Ze),e&&r(Tr),e&&r(he),w(et)}}}function to(W){let c,m;return c=new Bn({props:{$$slots:{default:[eo]},$$scope:{ctx:W}}}),{c(){v(c.$$.fragment)},l(p){z(c.$$.fragment,p)},m(p,u){$(c,p,u),m=!0},p(p,u){const y={};u&2&&(y.$$scope={dirty:u,ctx:p}),c.$set(y)},i(p){m||(_(c.$$.fragment,p),m=!0)},o(p){g(c.$$.fragment,p),m=!1},d(p){w(c,p)}}}function ro(W){let c,m,p,u,y;return{c(){c=s("p"),m=h("The Rust API Reference is available directly on the "),p=s("a"),u=h("Docs.rs"),y=h(" website."),this.h()},l(E){c=n(E,"P",{});var D=i(c);m=f(D,"The Rust API Reference is available directly on the "),p=n(D,"A",{href:!0,rel:!0});var C=i(p);u=f(C,"Docs.rs"),C.forEach(r),y=f(D," website."),D.forEach(r),this.h()},h(){o(p,"href","https://docs.rs/tokenizers/latest/tokenizers/"),o(p,"rel","nofollow")},m(E,D){k(E,c,D),t(c,m),t(c,p),t(p,u),t(c,y)},d(E){E&&r(c)}}}function so(W){let c,m;return c=new Bn({props:{$$slots:{default:[ro]},$$scope:{ctx:W}}}),{c(){v(c.$$.fragment)},l(p){z(c.$$.fragment,p)},m(p,u){$(c,p,u),m=!0},p(p,u){const y={};u&2&&(y.$$scope={dirty:u,ctx:p}),c.$set(y)},i(p){m||(_(c.$$.fragment,p),m=!0)},o(p){g(c.$$.fragment,p),m=!1},d(p){w(c,p)}}}function no(W){let c,m;return{c(){c=s("p"),m=h("The node API has not been documented yet.")},l(p){c=n(p,"P",{});var u=i(c);m=f(u,"The node API has not been documented yet."),u.forEach(r)},m(p,u){k(p,c,u),t(c,m)},d(p){p&&r(c)}}}function io(W){let c,m;return c=new Bn({props:{$$slots:{default:[no]},$$scope:{ctx:W}}}),{c(){v(c.$$.fragment)},l(p){z(c.$$.fragment,p)},m(p,u){$(c,p,u),m=!0},p(p,u){const y={};u&2&&(y.$$scope={dirty:u,ctx:p}),c.$set(y)},i(p){m||(_(c.$$.fragment,p),m=!0)},o(p){g(c.$$.fragment,p),m=!1},d(p){w(c,p)}}}function oo(W){let c,m,p,u,y,E,D,C,T,N,L,j;return E=new I({}),L=new Zi({props:{python:!0,rust:!0,node:!0,$$slots:{node:[io],rust:[so],python:[to]},$$scope:{ctx:W}}}),{c(){c=s("meta"),m=a(),p=s("h1"),u=s("a"),y=s("span"),v(E.$$.fragment),D=a(),C=s("span"),T=h("Pre-tokenizers"),N=a(),v(L.$$.fragment),this.h()},l(P){const A=Qi('[data-svelte="svelte-1phssyn"]',document.head);c=n(A,"META",{name:!0,content:!0}),A.forEach(r),m=l(P),p=n(P,"H1",{class:!0});var q=i(p);u=n(q,"A",{id:!0,class:!0,href:!0});var st=i(u);y=n(st,"SPAN",{});var ye=i(y);z(E.$$.fragment,ye),ye.forEach(r),st.forEach(r),D=l(q),C=n(q,"SPAN",{});var M=i(C);T=f(M,"Pre-tokenizers"),M.forEach(r),q.forEach(r),N=l(P),z(L.$$.fragment,P),this.h()},h(){o(c,"name","hf:doc:metadata"),o(c,"content",JSON.stringify(ao)),o(u,"id","pretokenizers"),o(u,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),o(u,"href","#pretokenizers"),o(p,"class","relative group")},m(P,A){t(document.head,c),k(P,m,A),k(P,p,A),t(p,u),t(u,y),$(E,y,null),t(p,D),t(p,C),t(C,T),k(P,N,A),$(L,P,A),j=!0},p(P,[A]){const q={};A&2&&(q.$$scope={dirty:A,ctx:P}),L.$set(q)},i(P){j||(_(E.$$.fragment,P),_(L.$$.fragment,P),j=!0)},o(P){g(E.$$.fragment,P),g(L.$$.fragment,P),j=!1},d(P){r(c),P&&r(m),P&&r(p),w(E),P&&r(N),w(L,P)}}}const ao={local:"pretokenizers",sections:[{local:"tokenizers.pre_tokenizers.BertPreTokenizer]][[tokenizers.pre_tokenizers.BertPreTokenizer",title:"BertPreTokenizer"},{local:"tokenizers.pre_tokenizers.ByteLevel]][[tokenizers.pre_tokenizers.ByteLevel",title:"ByteLevel"},{local:"tokenizers.pre_tokenizers.CharDelimiterSplit]][[tokenizers.pre_tokenizers.CharDelimiterSplit",title:"CharDelimiterSplit"},{local:"tokenizers.pre_tokenizers.Digits]][[tokenizers.pre_tokenizers.Digits",title:"Digits"},{local:"tokenizers.pre_tokenizers.Metaspace]][[tokenizers.pre_tokenizers.Metaspace",title:"Metaspace"},{local:"tokenizers.pre_tokenizers.PreTokenizer]][[tokenizers.pre_tokenizers.PreTokenizer",title:"PreTokenizer"},{local:"tokenizers.pre_tokenizers.Punctuation]][[tokenizers.pre_tokenizers.Punctuation",title:"Punctuation"},{local:"tokenizers.pre_tokenizers.Sequence]][[tokenizers.pre_tokenizers.Sequence",title:"Sequence"},{local:"tokenizers.pre_tokenizers.Split]][[tokenizers.pre_tokenizers.Split",title:"Split"},{local:"tokenizers.pre_tokenizers.UnicodeScripts]][[tokenizers.pre_tokenizers.UnicodeScripts",title:"UnicodeScripts"},{local:"tokenizers.pre_tokenizers.Whitespace]][[tokenizers.pre_tokenizers.Whitespace",title:"Whitespace"},{local:"tokenizers.pre_tokenizers.WhitespaceSplit]][[tokenizers.pre_tokenizers.WhitespaceSplit",title:"WhitespaceSplit"}],title:"Pre-tokenizers"};function lo(W){return Xi(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class uo extends Ki{constructor(c){super();Ji(this,c,lo,oo,Gi,{})}}export{uo as default,ao as metadata};
