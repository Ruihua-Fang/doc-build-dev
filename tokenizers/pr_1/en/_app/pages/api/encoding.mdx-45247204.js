import{S as vt,i as kt,s as Et,e as r,k as g,w as k,t as a,M as xt,c as s,d as o,m as f,a as i,x as E,h as c,b as _,F as e,g as qe,y as x,q as w,o as q,B as z,v as wt,L as qt}from"../../chunks/vendor-0d3f0756.js";import{D as I}from"../../chunks/Docstring-f752f2c3.js";import{I as zt}from"../../chunks/IconCopyLink-9193371d.js";import{T as Tt,M as Zn}from"../../chunks/TokenizersLanguageContent-ca787841.js";function Dt(D){let n,h,t,d,m,$,T,y,A,O,P,R,p,v,b,N,de,he,B,W,Ke,ze,Qe,Xe,F,j,Ye,pe,Ze,le,en,nn,H,K,tn,Q,on,ge,rn,sn,an,G,X,cn,Te,dn,hn,Y,pn,fe,ln,gn,fn,V,Z,un,De,_n,mn,ee,$n,ye,vn,kn,En,S,ne,xn,Pe,wn,qn,L,zn,Ie,Tn,Dn,be,yn,Pn,In,C,te,bn,Ae,An,Gn,oe,Vn,Ge,Sn,Cn,Mn,M,re,Ln,se,On,ue,Rn,Nn,Bn,ie,Fn,_e,Hn,Jn,Un,J,ae,Wn,Ve,jn,Kn,U,ce,Qn,Se,Xn,Ce;return h=new I({props:{name:"class tokenizers.Encoding",anchor:"tokenizers.Encoding",parameters:""}}),v=new I({props:{name:"char_to_token",anchor:"tokenizers.Encoding.char_to_token",parameters:[{name:"char_pos",val:""},{name:"sequence_index",val:" = 0"}],parametersDescription:[{anchor:"tokenizers.Encoding.char_to_token.char_pos",description:`<strong>char_pos</strong> (<code>int</code>) &#x2014;
The position of a char in the input string`,name:"char_pos"},{anchor:"tokenizers.Encoding.char_to_token.sequence_index",description:`<strong>sequence_index</strong> (<code>int</code>, defaults to <code>0</code>) &#x2014;
The index of the sequence that contains the target char`,name:"sequence_index"}],returnDescription:`
<p>The index of the token that contains this char in the encoded sequence</p>
`,returnType:`
<p><code>int</code></p>
`}}),W=new I({props:{name:"char_to_word",anchor:"tokenizers.Encoding.char_to_word",parameters:[{name:"char_pos",val:""},{name:"sequence_index",val:" = 0"}],parametersDescription:[{anchor:"tokenizers.Encoding.char_to_word.char_pos",description:`<strong>char_pos</strong> (<code>int</code>) &#x2014;
The position of a char in the input string`,name:"char_pos"},{anchor:"tokenizers.Encoding.char_to_word.sequence_index",description:`<strong>sequence_index</strong> (<code>int</code>, defaults to <code>0</code>) &#x2014;
The index of the sequence that contains the target char`,name:"sequence_index"}],returnDescription:`
<p>The index of the word that contains this char in the input sequence</p>
`,returnType:`
<p><code>int</code></p>
`}}),j=new I({props:{name:"merge",anchor:"tokenizers.Encoding.merge",parameters:[{name:"encodings",val:""},{name:"growing_offsets",val:" = True"}],parametersDescription:[{anchor:"tokenizers.Encoding.merge.encodings",description:`<strong>encodings</strong> (A <code>List</code> of <a href="/docs/tokenizers/pr_1/en/api/encoding#tokenizers.Encoding">Encoding</a>) &#x2014;
The list of encodings that should be merged in one`,name:"encodings"},{anchor:"tokenizers.Encoding.merge.growing_offsets",description:`<strong>growing_offsets</strong> (<code>bool</code>, defaults to <code>True</code>) &#x2014;
Whether the offsets should accumulate while merging`,name:"growing_offsets"}],returnDescription:`
<p>The resulting Encoding</p>
`,returnType:`
<p><a href="/docs/tokenizers/pr_1/en/api/encoding#tokenizers.Encoding">Encoding</a></p>
`}}),K=new I({props:{name:"pad",anchor:"tokenizers.Encoding.pad",parameters:[{name:"length",val:""},{name:"direction",val:" = 'right'"},{name:"pad_id",val:" = 0"},{name:"pad_type_id",val:" = 0"},{name:"pad_token",val:" = '[PAD]'"}],parametersDescription:[{anchor:"tokenizers.Encoding.pad.length",description:`<strong>length</strong> (<code>int</code>) &#x2014;
The desired length</p>
<p>direction &#x2014; (<code>str</code>, defaults to <code>right</code>):
The expected padding direction. Can be either <code>right</code> or <code>left</code>`,name:"length"},{anchor:"tokenizers.Encoding.pad.pad_id",description:`<strong>pad_id</strong> (<code>int</code>, defaults to <code>0</code>) &#x2014;
The ID corresponding to the padding token`,name:"pad_id"},{anchor:"tokenizers.Encoding.pad.pad_type_id",description:`<strong>pad_type_id</strong> (<code>int</code>, defaults to <code>0</code>) &#x2014;
The type ID corresponding to the padding token`,name:"pad_type_id"},{anchor:"tokenizers.Encoding.pad.pad_token",description:`<strong>pad_token</strong> (<code>str</code>, defaults to <em>[PAD]</em>) &#x2014;
The pad token to use`,name:"pad_token"}]}}),X=new I({props:{name:"set_sequence_id",anchor:"tokenizers.Encoding.set_sequence_id",parameters:[{name:"sequence_id",val:""}]}}),Z=new I({props:{name:"token_to_chars",anchor:"tokenizers.Encoding.token_to_chars",parameters:[{name:"token_index",val:""}],parametersDescription:[{anchor:"tokenizers.Encoding.token_to_chars.token_index",description:`<strong>token_index</strong> (<code>int</code>) &#x2014;
The index of a token in the encoded sequence.`,name:"token_index"}],returnDescription:`
<p>The token offsets <code>(first, last + 1)</code></p>
`,returnType:`
<p><code>Tuple[int, int]</code></p>
`}}),ne=new I({props:{name:"token_to_sequence",anchor:"tokenizers.Encoding.token_to_sequence",parameters:[{name:"token_index",val:""}],parametersDescription:[{anchor:"tokenizers.Encoding.token_to_sequence.token_index",description:`<strong>token_index</strong> (<code>int</code>) &#x2014;
The index of a token in the encoded sequence.`,name:"token_index"}],returnDescription:`
<p>The sequence id of the given token</p>
`,returnType:`
<p><code>int</code></p>
`}}),te=new I({props:{name:"token_to_word",anchor:"tokenizers.Encoding.token_to_word",parameters:[{name:"token_index",val:""}],parametersDescription:[{anchor:"tokenizers.Encoding.token_to_word.token_index",description:`<strong>token_index</strong> (<code>int</code>) &#x2014;
The index of a token in the encoded sequence.`,name:"token_index"}],returnDescription:`
<p>The index of the word in the relevant input sequence.</p>
`,returnType:`
<p><code>int</code></p>
`}}),re=new I({props:{name:"truncate",anchor:"tokenizers.Encoding.truncate",parameters:[{name:"max_length",val:""},{name:"stride",val:" = 0"},{name:"direction",val:" = 'right'"}],parametersDescription:[{anchor:"tokenizers.Encoding.truncate.max_length",description:`<strong>max_length</strong> (<code>int</code>) &#x2014;
The desired length`,name:"max_length"},{anchor:"tokenizers.Encoding.truncate.stride",description:`<strong>stride</strong> (<code>int</code>, defaults to <code>0</code>) &#x2014;
The length of previous content to be included in each overflowing piece`,name:"stride"},{anchor:"tokenizers.Encoding.truncate.direction",description:`<strong>direction</strong> (<code>str</code>, defaults to <code>right</code>) &#x2014;
Truncate direction`,name:"direction"}]}}),ae=new I({props:{name:"word_to_chars",anchor:"tokenizers.Encoding.word_to_chars",parameters:[{name:"word_index",val:""},{name:"sequence_index",val:" = 0"}],parametersDescription:[{anchor:"tokenizers.Encoding.word_to_chars.word_index",description:`<strong>word_index</strong> (<code>int</code>) &#x2014;
The index of a word in one of the input sequences.`,name:"word_index"},{anchor:"tokenizers.Encoding.word_to_chars.sequence_index",description:`<strong>sequence_index</strong> (<code>int</code>, defaults to <code>0</code>) &#x2014;
The index of the sequence that contains the target word`,name:"sequence_index"}],returnDescription:`
<p>The range of characters (span) <code>(first, last + 1)</code></p>
`,returnType:`
<p><code>Tuple[int, int]</code></p>
`}}),ce=new I({props:{name:"word_to_tokens",anchor:"tokenizers.Encoding.word_to_tokens",parameters:[{name:"word_index",val:""},{name:"sequence_index",val:" = 0"}],parametersDescription:[{anchor:"tokenizers.Encoding.word_to_tokens.word_index",description:`<strong>word_index</strong> (<code>int</code>) &#x2014;
The index of a word in one of the input sequences.`,name:"word_index"},{anchor:"tokenizers.Encoding.word_to_tokens.sequence_index",description:`<strong>sequence_index</strong> (<code>int</code>, defaults to <code>0</code>) &#x2014;
The index of the sequence that contains the target word`,name:"sequence_index"}],returnDescription:`
<p>The range of tokens: <code>(first, last + 1)</code></p>
`,returnType:`
<p><code>Tuple[int, int]</code></p>
`}}),{c(){n=r("div"),k(h.$$.fragment),t=g(),d=r("p"),m=a("The "),$=r("a"),T=a("Encoding"),y=a(" represents the output of a "),A=r("a"),O=a("Tokenizer"),P=a("."),R=g(),p=r("div"),k(v.$$.fragment),b=g(),N=r("p"),de=a("Get the token that contains the char at the given position in the input sequence."),he=g(),B=r("div"),k(W.$$.fragment),Ke=g(),ze=r("p"),Qe=a("Get the word that contains the char at the given position in the input sequence."),Xe=g(),F=r("div"),k(j.$$.fragment),Ye=g(),pe=r("p"),Ze=a("Merge the list of encodings into one final "),le=r("a"),en=a("Encoding"),nn=g(),H=r("div"),k(K.$$.fragment),tn=g(),Q=r("p"),on=a("Pad the "),ge=r("a"),rn=a("Encoding"),sn=a(" at the given length"),an=g(),G=r("div"),k(X.$$.fragment),cn=g(),Te=r("p"),dn=a("Set the given sequence index"),hn=g(),Y=r("p"),pn=a(`Set the given sequence index for the whole range of tokens contained in this
`),fe=r("a"),ln=a("Encoding"),gn=a("."),fn=g(),V=r("div"),k(Z.$$.fragment),un=g(),De=r("p"),_n=a("Get the offsets of the token at the given index."),mn=g(),ee=r("p"),$n=a(`The returned offsets are related to the input sequence that contains the
token.  In order to determine in which input sequence it belongs, you
must call `),ye=r("code"),vn=a("token_to_sequence()"),kn=a("."),En=g(),S=r("div"),k(ne.$$.fragment),xn=g(),Pe=r("p"),wn=a("Get the index of the sequence represented by the given token."),qn=g(),L=r("p"),zn=a("In the general use case, this method returns "),Ie=r("code"),Tn=a("0"),Dn=a(` for a single sequence or
the first sequence of a pair, and `),be=r("code"),yn=a("1"),Pn=a(" for the second sequence of a pair"),In=g(),C=r("div"),k(te.$$.fragment),bn=g(),Ae=r("p"),An=a("Get the index of the word that contains the token in one of the input sequences."),Gn=g(),oe=r("p"),Vn=a(`The returned word index is related to the input sequence that contains
the token.  In order to determine in which input sequence it belongs, you
must call `),Ge=r("code"),Sn=a("token_to_sequence()"),Cn=a("."),Mn=g(),M=r("div"),k(re.$$.fragment),Ln=g(),se=r("p"),On=a("Truncate the "),ue=r("a"),Rn=a("Encoding"),Nn=a(" at the given length"),Bn=g(),ie=r("p"),Fn=a("If this "),_e=r("a"),Hn=a("Encoding"),Jn=a(` represents multiple sequences, when truncating
this information is lost. It will be considered as representing a single sequence.`),Un=g(),J=r("div"),k(ae.$$.fragment),Wn=g(),Ve=r("p"),jn=a("Get the offsets of the word at the given index in one of the input sequences."),Kn=g(),U=r("div"),k(ce.$$.fragment),Qn=g(),Se=r("p"),Xn=a(`Get the encoded tokens corresponding to the word at the given index
in one of the input sequences.`),this.h()},l(l){n=s(l,"DIV",{class:!0});var u=i(n);E(h.$$.fragment,u),t=f(u),d=s(u,"P",{});var me=i(d);m=c(me,"The "),$=s(me,"A",{href:!0});var et=i($);T=c(et,"Encoding"),et.forEach(o),y=c(me," represents the output of a "),A=s(me,"A",{href:!0});var nt=i(A);O=c(nt,"Tokenizer"),nt.forEach(o),P=c(me,"."),me.forEach(o),R=f(u),p=s(u,"DIV",{class:!0});var Me=i(p);E(v.$$.fragment,Me),b=f(Me),N=s(Me,"P",{});var tt=i(N);de=c(tt,"Get the token that contains the char at the given position in the input sequence."),tt.forEach(o),Me.forEach(o),he=f(u),B=s(u,"DIV",{class:!0});var Le=i(B);E(W.$$.fragment,Le),Ke=f(Le),ze=s(Le,"P",{});var ot=i(ze);Qe=c(ot,"Get the word that contains the char at the given position in the input sequence."),ot.forEach(o),Le.forEach(o),Xe=f(u),F=s(u,"DIV",{class:!0});var Oe=i(F);E(j.$$.fragment,Oe),Ye=f(Oe),pe=s(Oe,"P",{});var Yn=i(pe);Ze=c(Yn,"Merge the list of encodings into one final "),le=s(Yn,"A",{href:!0});var rt=i(le);en=c(rt,"Encoding"),rt.forEach(o),Yn.forEach(o),Oe.forEach(o),nn=f(u),H=s(u,"DIV",{class:!0});var Re=i(H);E(K.$$.fragment,Re),tn=f(Re),Q=s(Re,"P",{});var Ne=i(Q);on=c(Ne,"Pad the "),ge=s(Ne,"A",{href:!0});var st=i(ge);rn=c(st,"Encoding"),st.forEach(o),sn=c(Ne," at the given length"),Ne.forEach(o),Re.forEach(o),an=f(u),G=s(u,"DIV",{class:!0});var $e=i(G);E(X.$$.fragment,$e),cn=f($e),Te=s($e,"P",{});var it=i(Te);dn=c(it,"Set the given sequence index"),it.forEach(o),hn=f($e),Y=s($e,"P",{});var Be=i(Y);pn=c(Be,`Set the given sequence index for the whole range of tokens contained in this
`),fe=s(Be,"A",{href:!0});var at=i(fe);ln=c(at,"Encoding"),at.forEach(o),gn=c(Be,"."),Be.forEach(o),$e.forEach(o),fn=f(u),V=s(u,"DIV",{class:!0});var ve=i(V);E(Z.$$.fragment,ve),un=f(ve),De=s(ve,"P",{});var ct=i(De);_n=c(ct,"Get the offsets of the token at the given index."),ct.forEach(o),mn=f(ve),ee=s(ve,"P",{});var Fe=i(ee);$n=c(Fe,`The returned offsets are related to the input sequence that contains the
token.  In order to determine in which input sequence it belongs, you
must call `),ye=s(Fe,"CODE",{});var dt=i(ye);vn=c(dt,"token_to_sequence()"),dt.forEach(o),kn=c(Fe,"."),Fe.forEach(o),ve.forEach(o),En=f(u),S=s(u,"DIV",{class:!0});var ke=i(S);E(ne.$$.fragment,ke),xn=f(ke),Pe=s(ke,"P",{});var ht=i(Pe);wn=c(ht,"Get the index of the sequence represented by the given token."),ht.forEach(o),qn=f(ke),L=s(ke,"P",{});var Ee=i(L);zn=c(Ee,"In the general use case, this method returns "),Ie=s(Ee,"CODE",{});var pt=i(Ie);Tn=c(pt,"0"),pt.forEach(o),Dn=c(Ee,` for a single sequence or
the first sequence of a pair, and `),be=s(Ee,"CODE",{});var lt=i(be);yn=c(lt,"1"),lt.forEach(o),Pn=c(Ee," for the second sequence of a pair"),Ee.forEach(o),ke.forEach(o),In=f(u),C=s(u,"DIV",{class:!0});var xe=i(C);E(te.$$.fragment,xe),bn=f(xe),Ae=s(xe,"P",{});var gt=i(Ae);An=c(gt,"Get the index of the word that contains the token in one of the input sequences."),gt.forEach(o),Gn=f(xe),oe=s(xe,"P",{});var He=i(oe);Vn=c(He,`The returned word index is related to the input sequence that contains
the token.  In order to determine in which input sequence it belongs, you
must call `),Ge=s(He,"CODE",{});var ft=i(Ge);Sn=c(ft,"token_to_sequence()"),ft.forEach(o),Cn=c(He,"."),He.forEach(o),xe.forEach(o),Mn=f(u),M=s(u,"DIV",{class:!0});var we=i(M);E(re.$$.fragment,we),Ln=f(we),se=s(we,"P",{});var Je=i(se);On=c(Je,"Truncate the "),ue=s(Je,"A",{href:!0});var ut=i(ue);Rn=c(ut,"Encoding"),ut.forEach(o),Nn=c(Je," at the given length"),Je.forEach(o),Bn=f(we),ie=s(we,"P",{});var Ue=i(ie);Fn=c(Ue,"If this "),_e=s(Ue,"A",{href:!0});var _t=i(_e);Hn=c(_t,"Encoding"),_t.forEach(o),Jn=c(Ue,` represents multiple sequences, when truncating
this information is lost. It will be considered as representing a single sequence.`),Ue.forEach(o),we.forEach(o),Un=f(u),J=s(u,"DIV",{class:!0});var We=i(J);E(ae.$$.fragment,We),Wn=f(We),Ve=s(We,"P",{});var mt=i(Ve);jn=c(mt,"Get the offsets of the word at the given index in one of the input sequences."),mt.forEach(o),We.forEach(o),Kn=f(u),U=s(u,"DIV",{class:!0});var je=i(U);E(ce.$$.fragment,je),Qn=f(je),Se=s(je,"P",{});var $t=i(Se);Xn=c($t,`Get the encoded tokens corresponding to the word at the given index
in one of the input sequences.`),$t.forEach(o),je.forEach(o),u.forEach(o),this.h()},h(){_($,"href","/docs/tokenizers/pr_1/en/api/encoding#tokenizers.Encoding"),_(A,"href","/docs/tokenizers/pr_1/en/api/tokenizer#tokenizers.Tokenizer"),_(p,"class","docstring"),_(B,"class","docstring"),_(le,"href","/docs/tokenizers/pr_1/en/api/encoding#tokenizers.Encoding"),_(F,"class","docstring"),_(ge,"href","/docs/tokenizers/pr_1/en/api/encoding#tokenizers.Encoding"),_(H,"class","docstring"),_(fe,"href","/docs/tokenizers/pr_1/en/api/encoding#tokenizers.Encoding"),_(G,"class","docstring"),_(V,"class","docstring"),_(S,"class","docstring"),_(C,"class","docstring"),_(ue,"href","/docs/tokenizers/pr_1/en/api/encoding#tokenizers.Encoding"),_(_e,"href","/docs/tokenizers/pr_1/en/api/encoding#tokenizers.Encoding"),_(M,"class","docstring"),_(J,"class","docstring"),_(U,"class","docstring"),_(n,"class","docstring")},m(l,u){qe(l,n,u),x(h,n,null),e(n,t),e(n,d),e(d,m),e(d,$),e($,T),e(d,y),e(d,A),e(A,O),e(d,P),e(n,R),e(n,p),x(v,p,null),e(p,b),e(p,N),e(N,de),e(n,he),e(n,B),x(W,B,null),e(B,Ke),e(B,ze),e(ze,Qe),e(n,Xe),e(n,F),x(j,F,null),e(F,Ye),e(F,pe),e(pe,Ze),e(pe,le),e(le,en),e(n,nn),e(n,H),x(K,H,null),e(H,tn),e(H,Q),e(Q,on),e(Q,ge),e(ge,rn),e(Q,sn),e(n,an),e(n,G),x(X,G,null),e(G,cn),e(G,Te),e(Te,dn),e(G,hn),e(G,Y),e(Y,pn),e(Y,fe),e(fe,ln),e(Y,gn),e(n,fn),e(n,V),x(Z,V,null),e(V,un),e(V,De),e(De,_n),e(V,mn),e(V,ee),e(ee,$n),e(ee,ye),e(ye,vn),e(ee,kn),e(n,En),e(n,S),x(ne,S,null),e(S,xn),e(S,Pe),e(Pe,wn),e(S,qn),e(S,L),e(L,zn),e(L,Ie),e(Ie,Tn),e(L,Dn),e(L,be),e(be,yn),e(L,Pn),e(n,In),e(n,C),x(te,C,null),e(C,bn),e(C,Ae),e(Ae,An),e(C,Gn),e(C,oe),e(oe,Vn),e(oe,Ge),e(Ge,Sn),e(oe,Cn),e(n,Mn),e(n,M),x(re,M,null),e(M,Ln),e(M,se),e(se,On),e(se,ue),e(ue,Rn),e(se,Nn),e(M,Bn),e(M,ie),e(ie,Fn),e(ie,_e),e(_e,Hn),e(ie,Jn),e(n,Un),e(n,J),x(ae,J,null),e(J,Wn),e(J,Ve),e(Ve,jn),e(n,Kn),e(n,U),x(ce,U,null),e(U,Qn),e(U,Se),e(Se,Xn),Ce=!0},p:qt,i(l){Ce||(w(h.$$.fragment,l),w(v.$$.fragment,l),w(W.$$.fragment,l),w(j.$$.fragment,l),w(K.$$.fragment,l),w(X.$$.fragment,l),w(Z.$$.fragment,l),w(ne.$$.fragment,l),w(te.$$.fragment,l),w(re.$$.fragment,l),w(ae.$$.fragment,l),w(ce.$$.fragment,l),Ce=!0)},o(l){q(h.$$.fragment,l),q(v.$$.fragment,l),q(W.$$.fragment,l),q(j.$$.fragment,l),q(K.$$.fragment,l),q(X.$$.fragment,l),q(Z.$$.fragment,l),q(ne.$$.fragment,l),q(te.$$.fragment,l),q(re.$$.fragment,l),q(ae.$$.fragment,l),q(ce.$$.fragment,l),Ce=!1},d(l){l&&o(n),z(h),z(v),z(W),z(j),z(K),z(X),z(Z),z(ne),z(te),z(re),z(ae),z(ce)}}}function yt(D){let n,h;return n=new Zn({props:{$$slots:{default:[Dt]},$$scope:{ctx:D}}}),{c(){k(n.$$.fragment)},l(t){E(n.$$.fragment,t)},m(t,d){x(n,t,d),h=!0},p(t,d){const m={};d&2&&(m.$$scope={dirty:d,ctx:t}),n.$set(m)},i(t){h||(w(n.$$.fragment,t),h=!0)},o(t){q(n.$$.fragment,t),h=!1},d(t){z(n,t)}}}function Pt(D){let n,h,t,d,m;return{c(){n=r("p"),h=a("The Rust API Reference is available directly on the "),t=r("a"),d=a("Docs.rs"),m=a(" website."),this.h()},l($){n=s($,"P",{});var T=i(n);h=c(T,"The Rust API Reference is available directly on the "),t=s(T,"A",{href:!0,rel:!0});var y=i(t);d=c(y,"Docs.rs"),y.forEach(o),m=c(T," website."),T.forEach(o),this.h()},h(){_(t,"href","https://docs.rs/tokenizers/latest/tokenizers/"),_(t,"rel","nofollow")},m($,T){qe($,n,T),e(n,h),e(n,t),e(t,d),e(n,m)},d($){$&&o(n)}}}function It(D){let n,h;return n=new Zn({props:{$$slots:{default:[Pt]},$$scope:{ctx:D}}}),{c(){k(n.$$.fragment)},l(t){E(n.$$.fragment,t)},m(t,d){x(n,t,d),h=!0},p(t,d){const m={};d&2&&(m.$$scope={dirty:d,ctx:t}),n.$set(m)},i(t){h||(w(n.$$.fragment,t),h=!0)},o(t){q(n.$$.fragment,t),h=!1},d(t){z(n,t)}}}function bt(D){let n,h;return{c(){n=r("p"),h=a("The node API has not been documented yet.")},l(t){n=s(t,"P",{});var d=i(n);h=c(d,"The node API has not been documented yet."),d.forEach(o)},m(t,d){qe(t,n,d),e(n,h)},d(t){t&&o(n)}}}function At(D){let n,h;return n=new Zn({props:{$$slots:{default:[bt]},$$scope:{ctx:D}}}),{c(){k(n.$$.fragment)},l(t){E(n.$$.fragment,t)},m(t,d){x(n,t,d),h=!0},p(t,d){const m={};d&2&&(m.$$scope={dirty:d,ctx:t}),n.$set(m)},i(t){h||(w(n.$$.fragment,t),h=!0)},o(t){q(n.$$.fragment,t),h=!1},d(t){z(n,t)}}}function Gt(D){let n,h,t,d,m,$,T,y,A,O,P,R;return $=new zt({}),P=new Tt({props:{python:!0,rust:!0,node:!0,$$slots:{node:[At],rust:[It],python:[yt]},$$scope:{ctx:D}}}),{c(){n=r("meta"),h=g(),t=r("h1"),d=r("a"),m=r("span"),k($.$$.fragment),T=g(),y=r("span"),A=a("Encoding"),O=g(),k(P.$$.fragment),this.h()},l(p){const v=xt('[data-svelte="svelte-1phssyn"]',document.head);n=s(v,"META",{name:!0,content:!0}),v.forEach(o),h=f(p),t=s(p,"H1",{class:!0});var b=i(t);d=s(b,"A",{id:!0,class:!0,href:!0});var N=i(d);m=s(N,"SPAN",{});var de=i(m);E($.$$.fragment,de),de.forEach(o),N.forEach(o),T=f(b),y=s(b,"SPAN",{});var he=i(y);A=c(he,"Encoding"),he.forEach(o),b.forEach(o),O=f(p),E(P.$$.fragment,p),this.h()},h(){_(n,"name","hf:doc:metadata"),_(n,"content",JSON.stringify(Vt)),_(d,"id","tokenizers.Encoding"),_(d,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),_(d,"href","#tokenizers.Encoding"),_(t,"class","relative group")},m(p,v){e(document.head,n),qe(p,h,v),qe(p,t,v),e(t,d),e(d,m),x($,m,null),e(t,T),e(t,y),e(y,A),qe(p,O,v),x(P,p,v),R=!0},p(p,[v]){const b={};v&2&&(b.$$scope={dirty:v,ctx:p}),P.$set(b)},i(p){R||(w($.$$.fragment,p),w(P.$$.fragment,p),R=!0)},o(p){q($.$$.fragment,p),q(P.$$.fragment,p),R=!1},d(p){o(n),p&&o(h),p&&o(t),z($),p&&o(O),z(P,p)}}}const Vt={local:"tokenizers.Encoding",title:"Encoding"};function St(D){return wt(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Rt extends vt{constructor(n){super();kt(this,n,St,Gt,Et,{})}}export{Rt as default,Vt as metadata};
