import{S as tr,i as rr,s as sr,e as a,k as c,w as k,t as h,M as nr,c as o,d as r,m as d,a as i,x as _,h as m,b as v,F as t,g,y as z,q as w,o as b,B as y,v as ar,L as or}from"../../chunks/vendor-0d3f0756.js";import{D as B}from"../../chunks/Docstring-f752f2c3.js";import{C as er}from"../../chunks/CodeBlock-7b0cb15c.js";import{I as ir}from"../../chunks/IconCopyLink-9193371d.js";import{T as lr,M as Bt}from"../../chunks/TokenizersLanguageContent-8807aac4.js";function pr(S){let s,p,n,l,$,T,E,q,O,x,D,N,f,C,A,K,pe,ce,V,G,Je,$e,Ge,Qe,ve,Xe,Se,W,Q,Ye,de,Ze,ke,et,qe,P,X,tt,_e,rt,st,ze,nt,at,Y,ot,we,it,lt,Z,Ie,L,ee,pt,be,ct,dt,ye,ht,Be,j,te,mt,xe,ft,Ae,F,re,ut,Te,gt,Le,M,se,$t,Pe,vt,kt,Ee,_t,Me,R,ne,zt,ae,wt,oe,bt,yt,Ve,H,ie,xt,he,Tt,De,Pt,We,U,le,Et,me,Dt,Ce,Ct,je;return p=new B({props:{name:"class tokenizers.pre_tokenizers.BertPreTokenizer",anchor:"tokenizers.pre_tokenizers.BertPreTokenizer",parameters:[]}}),D=new B({props:{name:"class tokenizers.pre_tokenizers.ByteLevel",anchor:"tokenizers.pre_tokenizers.ByteLevel",parameters:[{name:"add_prefix_space",val:" = True"}],parametersDescription:[{anchor:"tokenizers.pre_tokenizers.ByteLevel.add_prefix_space",description:`<strong>add_prefix_space</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to add a space to the first word if there isn&#x2019;t already one. This
lets us treat <em>hello</em> exactly like <em>say hello</em>.`,name:"add_prefix_space"}]}}),G=new B({props:{name:"alphabet",anchor:"tokenizers.pre_tokenizers.ByteLevel.alphabet",parameters:[],returnDescription:`
<p>A list of characters that compose the alphabet</p>
`,returnType:`
<p><code>List[str]</code></p>
`}}),Q=new B({props:{name:"class tokenizers.pre_tokenizers.CharDelimiterSplit",anchor:"tokenizers.pre_tokenizers.CharDelimiterSplit",parameters:""}}),X=new B({props:{name:"class tokenizers.pre_tokenizers.Digits",anchor:"tokenizers.pre_tokenizers.Digits",parameters:[{name:"individual_digits",val:" = False"}],parametersDescription:[{anchor:"tokenizers.pre_tokenizers.Digits.individual_digits",description:"<strong>individual_digits</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;",name:"individual_digits"}]}}),Y=new er({props:{code:'"Call 123 please" -> "Call ", "1", "2", "3", " please"',highlighted:'<span class="hljs-string">&quot;Call 123 please&quot;</span> -&gt; <span class="hljs-string">&quot;Call &quot;</span>, <span class="hljs-string">&quot;1&quot;</span>, <span class="hljs-string">&quot;2&quot;</span>, <span class="hljs-string">&quot;3&quot;</span>, <span class="hljs-string">&quot; please&quot;</span>'}}),Z=new er({props:{code:'"Call 123 please" -> "Call ", "123", " please"',highlighted:'<span class="hljs-string">&quot;Call 123 please&quot;</span> -&gt; <span class="hljs-string">&quot;Call &quot;</span>, <span class="hljs-string">&quot;123&quot;</span>, <span class="hljs-string">&quot; please&quot;</span>'}}),ee=new B({props:{name:"class tokenizers.pre_tokenizers.Metaspace",anchor:"tokenizers.pre_tokenizers.Metaspace",parameters:[{name:"replacement",val:" = '_'"},{name:"add_prefix_space",val:" = True"}],parametersDescription:[{anchor:"tokenizers.pre_tokenizers.Metaspace.replacement",description:`<strong>replacement</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&#x2581;</code>) &#x2014;
The replacement character. Must be exactly one character. By default we
use the <em>&#x2581;</em> (U+2581) meta symbol (Same as in SentencePiece).`,name:"replacement"},{anchor:"tokenizers.pre_tokenizers.Metaspace.add_prefix_space",description:`<strong>add_prefix_space</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to add a space to the first word if there isn&#x2019;t already one. This
lets us treat <em>hello</em> exactly like <em>say hello</em>.`,name:"add_prefix_space"}]}}),te=new B({props:{name:"class tokenizers.pre_tokenizers.Punctuation",anchor:"tokenizers.pre_tokenizers.Punctuation",parameters:[{name:"behavior",val:" = 'isolated'"}],parametersDescription:[{anchor:"tokenizers.pre_tokenizers.Punctuation.behavior",description:`<strong>behavior</strong> (<code>SplitDelimiterBehavior</code>) &#x2014;
The behavior to use when splitting.
Choices: &#x201C;removed&#x201D;, &#x201C;isolated&#x201D; (default), &#x201C;merged_with_previous&#x201D;, &#x201C;merged_with_next&#x201D;,
&#x201C;contiguous&#x201D;`,name:"behavior"}]}}),re=new B({props:{name:"class tokenizers.pre_tokenizers.Sequence",anchor:"tokenizers.pre_tokenizers.Sequence",parameters:[{name:"pretokenizers",val:""}]}}),se=new B({props:{name:"class tokenizers.pre_tokenizers.Split",anchor:"tokenizers.pre_tokenizers.Split",parameters:[{name:"pattern",val:""},{name:"behavior",val:""},{name:"invert",val:" = False"}],parametersDescription:[{anchor:"tokenizers.pre_tokenizers.Split.pattern",description:`<strong>pattern</strong> (<code>str</code> or <code>Regex</code>) &#x2014;
A pattern used to split the string. Usually a string or a Regex`,name:"pattern"},{anchor:"tokenizers.pre_tokenizers.Split.behavior",description:`<strong>behavior</strong> (<code>SplitDelimiterBehavior</code>) &#x2014;
The behavior to use when splitting.
Choices: &#x201C;removed&#x201D;, &#x201C;isolated&#x201D;, &#x201C;merged_with_previous&#x201D;, &#x201C;merged_with_next&#x201D;,
&#x201C;contiguous&#x201D;`,name:"behavior"},{anchor:"tokenizers.pre_tokenizers.Split.invert",description:`<strong>invert</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to invert the pattern.`,name:"invert"}]}}),ne=new B({props:{name:"class tokenizers.pre_tokenizers.UnicodeScripts",anchor:"tokenizers.pre_tokenizers.UnicodeScripts",parameters:[]}}),ie=new B({props:{name:"class tokenizers.pre_tokenizers.Whitespace",anchor:"tokenizers.pre_tokenizers.Whitespace",parameters:[]}}),le=new B({props:{name:"class tokenizers.pre_tokenizers.WhitespaceSplit",anchor:"tokenizers.pre_tokenizers.WhitespaceSplit",parameters:[]}}),{c(){s=a("div"),k(p.$$.fragment),n=c(),l=a("p"),$=h("BertPreTokenizer"),T=c(),E=a("p"),q=h(`This pre-tokenizer splits tokens on spaces, and also on punctuation.
Each occurence of a punctuation character will be treated separately.`),O=c(),x=a("div"),k(D.$$.fragment),N=c(),f=a("p"),C=h("ByteLevel PreTokenizer"),A=c(),K=a("p"),pe=h(`This pre-tokenizer takes care of replacing all bytes of the given string
with a corresponding representation, as well as splitting into words.`),ce=c(),V=a("div"),k(G.$$.fragment),Je=c(),$e=a("p"),Ge=h("Returns the alphabet used by this PreTokenizer."),Qe=c(),ve=a("p"),Xe=h(`Since the ByteLevel works as its name suggests, at the byte level, it
encodes each byte value to a unique visible character. This means that there is a
total of 256 different characters composing this alphabet.`),Se=c(),W=a("div"),k(Q.$$.fragment),Ye=c(),de=a("p"),Ze=h("This pre-tokenizer simply splits on the provided char. Works like "),ke=a("code"),et=h(".split(delimiter)"),qe=c(),P=a("div"),k(X.$$.fragment),tt=c(),_e=a("p"),rt=h("This pre-tokenizer simply splits using the digits in separate tokens"),st=c(),ze=a("p"),nt=h("If set to True, digits will each be separated as follows:"),at=c(),k(Y.$$.fragment),ot=c(),we=a("p"),it=h("If set to False, digits will grouped as follows:"),lt=c(),k(Z.$$.fragment),Ie=c(),L=a("div"),k(ee.$$.fragment),pt=c(),be=a("p"),ct=h("Metaspace pre-tokenizer"),dt=c(),ye=a("p"),ht=h(`This pre-tokenizer replaces any whitespace by the provided replacement character.
It then tries to split on these spaces.`),Be=c(),j=a("div"),k(te.$$.fragment),mt=c(),xe=a("p"),ft=h("This pre-tokenizer simply splits on punctuation as individual characters."),Ae=c(),F=a("div"),k(re.$$.fragment),ut=c(),Te=a("p"),gt=h("This pre-tokenizer composes other pre_tokenizers and applies them in sequence"),Le=c(),M=a("div"),k(se.$$.fragment),$t=c(),Pe=a("p"),vt=h("Split PreTokenizer"),kt=c(),Ee=a("p"),_t=h(`This versatile pre-tokenizer splits using the provided pattern and
according to the provided behavior. The pattern can be inverted by
making use of the invert flag.`),Me=c(),R=a("div"),k(ne.$$.fragment),zt=c(),ae=a("p"),wt=h(`This pre-tokenizer splits on characters that belong to different language family
It roughly follows `),oe=a("a"),bt=h("https://github.com/google/sentencepiece/blob/master/data/Scripts.txt"),yt=h(`
Actually Hiragana and Katakana are fused with Han, and 0x30FC is Han too.
This mimicks SentencePiece Unigram implementation.`),Ve=c(),H=a("div"),k(ie.$$.fragment),xt=c(),he=a("p"),Tt=h("This pre-tokenizer simply splits using the following regex: "),De=a("code"),Pt=h("\\w+|[^\\w\\s]+"),We=c(),U=a("div"),k(le.$$.fragment),Et=c(),me=a("p"),Dt=h("This pre-tokenizer simply splits on the whitespace. Works like "),Ce=a("code"),Ct=h(".split()"),this.h()},l(e){s=o(e,"DIV",{class:!0});var u=i(s);_(p.$$.fragment,u),n=d(u),l=o(u,"P",{});var At=i(l);$=m(At,"BertPreTokenizer"),At.forEach(r),T=d(u),E=o(u,"P",{});var Lt=i(E);q=m(Lt,`This pre-tokenizer splits tokens on spaces, and also on punctuation.
Each occurence of a punctuation character will be treated separately.`),Lt.forEach(r),u.forEach(r),O=d(e),x=o(e,"DIV",{class:!0});var J=i(x);_(D.$$.fragment,J),N=d(J),f=o(J,"P",{});var Mt=i(f);C=m(Mt,"ByteLevel PreTokenizer"),Mt.forEach(r),A=d(J),K=o(J,"P",{});var Vt=i(K);pe=m(Vt,`This pre-tokenizer takes care of replacing all bytes of the given string
with a corresponding representation, as well as splitting into words.`),Vt.forEach(r),ce=d(J),V=o(J,"DIV",{class:!0});var fe=i(V);_(G.$$.fragment,fe),Je=d(fe),$e=o(fe,"P",{});var Wt=i($e);Ge=m(Wt,"Returns the alphabet used by this PreTokenizer."),Wt.forEach(r),Qe=d(fe),ve=o(fe,"P",{});var jt=i(ve);Xe=m(jt,`Since the ByteLevel works as its name suggests, at the byte level, it
encodes each byte value to a unique visible character. This means that there is a
total of 256 different characters composing this alphabet.`),jt.forEach(r),fe.forEach(r),J.forEach(r),Se=d(e),W=o(e,"DIV",{class:!0});var Fe=i(W);_(Q.$$.fragment,Fe),Ye=d(Fe),de=o(Fe,"P",{});var St=i(de);Ze=m(St,"This pre-tokenizer simply splits on the provided char. Works like "),ke=o(St,"CODE",{});var Ft=i(ke);et=m(Ft,".split(delimiter)"),Ft.forEach(r),St.forEach(r),Fe.forEach(r),qe=d(e),P=o(e,"DIV",{class:!0});var I=i(P);_(X.$$.fragment,I),tt=d(I),_e=o(I,"P",{});var Rt=i(_e);rt=m(Rt,"This pre-tokenizer simply splits using the digits in separate tokens"),Rt.forEach(r),st=d(I),ze=o(I,"P",{});var Ht=i(ze);nt=m(Ht,"If set to True, digits will each be separated as follows:"),Ht.forEach(r),at=d(I),_(Y.$$.fragment,I),ot=d(I),we=o(I,"P",{});var Ut=i(we);it=m(Ut,"If set to False, digits will grouped as follows:"),Ut.forEach(r),lt=d(I),_(Z.$$.fragment,I),I.forEach(r),Ie=d(e),L=o(e,"DIV",{class:!0});var ue=i(L);_(ee.$$.fragment,ue),pt=d(ue),be=o(ue,"P",{});var Ot=i(be);ct=m(Ot,"Metaspace pre-tokenizer"),Ot.forEach(r),dt=d(ue),ye=o(ue,"P",{});var Nt=i(ye);ht=m(Nt,`This pre-tokenizer replaces any whitespace by the provided replacement character.
It then tries to split on these spaces.`),Nt.forEach(r),ue.forEach(r),Be=d(e),j=o(e,"DIV",{class:!0});var Re=i(j);_(te.$$.fragment,Re),mt=d(Re),xe=o(Re,"P",{});var Kt=i(xe);ft=m(Kt,"This pre-tokenizer simply splits on punctuation as individual characters."),Kt.forEach(r),Re.forEach(r),Ae=d(e),F=o(e,"DIV",{class:!0});var He=i(F);_(re.$$.fragment,He),ut=d(He),Te=o(He,"P",{});var Jt=i(Te);gt=m(Jt,"This pre-tokenizer composes other pre_tokenizers and applies them in sequence"),Jt.forEach(r),He.forEach(r),Le=d(e),M=o(e,"DIV",{class:!0});var ge=i(M);_(se.$$.fragment,ge),$t=d(ge),Pe=o(ge,"P",{});var Gt=i(Pe);vt=m(Gt,"Split PreTokenizer"),Gt.forEach(r),kt=d(ge),Ee=o(ge,"P",{});var Qt=i(Ee);_t=m(Qt,`This versatile pre-tokenizer splits using the provided pattern and
according to the provided behavior. The pattern can be inverted by
making use of the invert flag.`),Qt.forEach(r),ge.forEach(r),Me=d(e),R=o(e,"DIV",{class:!0});var Ue=i(R);_(ne.$$.fragment,Ue),zt=d(Ue),ae=o(Ue,"P",{});var Oe=i(ae);wt=m(Oe,`This pre-tokenizer splits on characters that belong to different language family
It roughly follows `),oe=o(Oe,"A",{href:!0,rel:!0});var Xt=i(oe);bt=m(Xt,"https://github.com/google/sentencepiece/blob/master/data/Scripts.txt"),Xt.forEach(r),yt=m(Oe,`
Actually Hiragana and Katakana are fused with Han, and 0x30FC is Han too.
This mimicks SentencePiece Unigram implementation.`),Oe.forEach(r),Ue.forEach(r),Ve=d(e),H=o(e,"DIV",{class:!0});var Ne=i(H);_(ie.$$.fragment,Ne),xt=d(Ne),he=o(Ne,"P",{});var qt=i(he);Tt=m(qt,"This pre-tokenizer simply splits using the following regex: "),De=o(qt,"CODE",{});var Yt=i(De);Pt=m(Yt,"\\w+|[^\\w\\s]+"),Yt.forEach(r),qt.forEach(r),Ne.forEach(r),We=d(e),U=o(e,"DIV",{class:!0});var Ke=i(U);_(le.$$.fragment,Ke),Et=d(Ke),me=o(Ke,"P",{});var It=i(me);Dt=m(It,"This pre-tokenizer simply splits on the whitespace. Works like "),Ce=o(It,"CODE",{});var Zt=i(Ce);Ct=m(Zt,".split()"),Zt.forEach(r),It.forEach(r),Ke.forEach(r),this.h()},h(){v(s,"class","docstring"),v(V,"class","docstring"),v(x,"class","docstring"),v(W,"class","docstring"),v(P,"class","docstring"),v(L,"class","docstring"),v(j,"class","docstring"),v(F,"class","docstring"),v(M,"class","docstring"),v(oe,"href","https://github.com/google/sentencepiece/blob/master/data/Scripts.txt"),v(oe,"rel","nofollow"),v(R,"class","docstring"),v(H,"class","docstring"),v(U,"class","docstring")},m(e,u){g(e,s,u),z(p,s,null),t(s,n),t(s,l),t(l,$),t(s,T),t(s,E),t(E,q),g(e,O,u),g(e,x,u),z(D,x,null),t(x,N),t(x,f),t(f,C),t(x,A),t(x,K),t(K,pe),t(x,ce),t(x,V),z(G,V,null),t(V,Je),t(V,$e),t($e,Ge),t(V,Qe),t(V,ve),t(ve,Xe),g(e,Se,u),g(e,W,u),z(Q,W,null),t(W,Ye),t(W,de),t(de,Ze),t(de,ke),t(ke,et),g(e,qe,u),g(e,P,u),z(X,P,null),t(P,tt),t(P,_e),t(_e,rt),t(P,st),t(P,ze),t(ze,nt),t(P,at),z(Y,P,null),t(P,ot),t(P,we),t(we,it),t(P,lt),z(Z,P,null),g(e,Ie,u),g(e,L,u),z(ee,L,null),t(L,pt),t(L,be),t(be,ct),t(L,dt),t(L,ye),t(ye,ht),g(e,Be,u),g(e,j,u),z(te,j,null),t(j,mt),t(j,xe),t(xe,ft),g(e,Ae,u),g(e,F,u),z(re,F,null),t(F,ut),t(F,Te),t(Te,gt),g(e,Le,u),g(e,M,u),z(se,M,null),t(M,$t),t(M,Pe),t(Pe,vt),t(M,kt),t(M,Ee),t(Ee,_t),g(e,Me,u),g(e,R,u),z(ne,R,null),t(R,zt),t(R,ae),t(ae,wt),t(ae,oe),t(oe,bt),t(ae,yt),g(e,Ve,u),g(e,H,u),z(ie,H,null),t(H,xt),t(H,he),t(he,Tt),t(he,De),t(De,Pt),g(e,We,u),g(e,U,u),z(le,U,null),t(U,Et),t(U,me),t(me,Dt),t(me,Ce),t(Ce,Ct),je=!0},p:or,i(e){je||(w(p.$$.fragment,e),w(D.$$.fragment,e),w(G.$$.fragment,e),w(Q.$$.fragment,e),w(X.$$.fragment,e),w(Y.$$.fragment,e),w(Z.$$.fragment,e),w(ee.$$.fragment,e),w(te.$$.fragment,e),w(re.$$.fragment,e),w(se.$$.fragment,e),w(ne.$$.fragment,e),w(ie.$$.fragment,e),w(le.$$.fragment,e),je=!0)},o(e){b(p.$$.fragment,e),b(D.$$.fragment,e),b(G.$$.fragment,e),b(Q.$$.fragment,e),b(X.$$.fragment,e),b(Y.$$.fragment,e),b(Z.$$.fragment,e),b(ee.$$.fragment,e),b(te.$$.fragment,e),b(re.$$.fragment,e),b(se.$$.fragment,e),b(ne.$$.fragment,e),b(ie.$$.fragment,e),b(le.$$.fragment,e),je=!1},d(e){e&&r(s),y(p),e&&r(O),e&&r(x),y(D),y(G),e&&r(Se),e&&r(W),y(Q),e&&r(qe),e&&r(P),y(X),y(Y),y(Z),e&&r(Ie),e&&r(L),y(ee),e&&r(Be),e&&r(j),y(te),e&&r(Ae),e&&r(F),y(re),e&&r(Le),e&&r(M),y(se),e&&r(Me),e&&r(R),y(ne),e&&r(Ve),e&&r(H),y(ie),e&&r(We),e&&r(U),y(le)}}}function cr(S){let s,p;return s=new Bt({props:{$$slots:{default:[pr]},$$scope:{ctx:S}}}),{c(){k(s.$$.fragment)},l(n){_(s.$$.fragment,n)},m(n,l){z(s,n,l),p=!0},p(n,l){const $={};l&2&&($.$$scope={dirty:l,ctx:n}),s.$set($)},i(n){p||(w(s.$$.fragment,n),p=!0)},o(n){b(s.$$.fragment,n),p=!1},d(n){y(s,n)}}}function dr(S){let s,p,n,l,$;return{c(){s=a("p"),p=h("The Rust API Reference is available directly on the "),n=a("a"),l=h("Docs.rs"),$=h(" website."),this.h()},l(T){s=o(T,"P",{});var E=i(s);p=m(E,"The Rust API Reference is available directly on the "),n=o(E,"A",{href:!0,rel:!0});var q=i(n);l=m(q,"Docs.rs"),q.forEach(r),$=m(E," website."),E.forEach(r),this.h()},h(){v(n,"href","https://docs.rs/tokenizers/latest/tokenizers/"),v(n,"rel","nofollow")},m(T,E){g(T,s,E),t(s,p),t(s,n),t(n,l),t(s,$)},d(T){T&&r(s)}}}function hr(S){let s,p;return s=new Bt({props:{$$slots:{default:[dr]},$$scope:{ctx:S}}}),{c(){k(s.$$.fragment)},l(n){_(s.$$.fragment,n)},m(n,l){z(s,n,l),p=!0},p(n,l){const $={};l&2&&($.$$scope={dirty:l,ctx:n}),s.$set($)},i(n){p||(w(s.$$.fragment,n),p=!0)},o(n){b(s.$$.fragment,n),p=!1},d(n){y(s,n)}}}function mr(S){let s,p;return{c(){s=a("p"),p=h("The node API has not been documented yet.")},l(n){s=o(n,"P",{});var l=i(s);p=m(l,"The node API has not been documented yet."),l.forEach(r)},m(n,l){g(n,s,l),t(s,p)},d(n){n&&r(s)}}}function fr(S){let s,p;return s=new Bt({props:{$$slots:{default:[mr]},$$scope:{ctx:S}}}),{c(){k(s.$$.fragment)},l(n){_(s.$$.fragment,n)},m(n,l){z(s,n,l),p=!0},p(n,l){const $={};l&2&&($.$$scope={dirty:l,ctx:n}),s.$set($)},i(n){p||(w(s.$$.fragment,n),p=!0)},o(n){b(s.$$.fragment,n),p=!1},d(n){y(s,n)}}}function ur(S){let s,p,n,l,$,T,E,q,O,x,D,N;return T=new ir({}),D=new lr({props:{python:!0,rust:!0,node:!0,$$slots:{node:[fr],rust:[hr],python:[cr]},$$scope:{ctx:S}}}),{c(){s=a("meta"),p=c(),n=a("h1"),l=a("a"),$=a("span"),k(T.$$.fragment),E=c(),q=a("span"),O=h("Pre-tokenizers"),x=c(),k(D.$$.fragment),this.h()},l(f){const C=nr('[data-svelte="svelte-1phssyn"]',document.head);s=o(C,"META",{name:!0,content:!0}),C.forEach(r),p=d(f),n=o(f,"H1",{class:!0});var A=i(n);l=o(A,"A",{id:!0,class:!0,href:!0});var K=i(l);$=o(K,"SPAN",{});var pe=i($);_(T.$$.fragment,pe),pe.forEach(r),K.forEach(r),E=d(A),q=o(A,"SPAN",{});var ce=i(q);O=m(ce,"Pre-tokenizers"),ce.forEach(r),A.forEach(r),x=d(f),_(D.$$.fragment,f),this.h()},h(){v(s,"name","hf:doc:metadata"),v(s,"content",JSON.stringify(gr)),v(l,"id","tokenizers.pre_tokenizers.BertPreTokenizer"),v(l,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),v(l,"href","#tokenizers.pre_tokenizers.BertPreTokenizer"),v(n,"class","relative group")},m(f,C){t(document.head,s),g(f,p,C),g(f,n,C),t(n,l),t(l,$),z(T,$,null),t(n,E),t(n,q),t(q,O),g(f,x,C),z(D,f,C),N=!0},p(f,[C]){const A={};C&2&&(A.$$scope={dirty:C,ctx:f}),D.$set(A)},i(f){N||(w(T.$$.fragment,f),w(D.$$.fragment,f),N=!0)},o(f){b(T.$$.fragment,f),b(D.$$.fragment,f),N=!1},d(f){r(s),f&&r(p),f&&r(n),y(T),f&&r(x),y(D,f)}}}const gr={local:"tokenizers.pre_tokenizers.BertPreTokenizer",title:"Pre-tokenizers"};function $r(S){return ar(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class br extends tr{constructor(s){super();rr(this,s,$r,ur,sr,{})}}export{br as default,gr as metadata};
