import{S as Mn,i as Un,s as Hn,e as r,k as d,w as b,t as s,M as Vn,c as n,d as t,m as p,a,x as T,h as o,b as g,F as e,g as E,y as L,q as S,o as z,B as C,v as Yn,L as Fn}from"../../chunks/vendor-0d3f0756.js";import{D as ds}from"../../chunks/Docstring-cebdcc97.js";import{C as Wn}from"../../chunks/CodeBlock-7b0cb15c.js";import{I as Rt}from"../../chunks/IconCopyLink-9193371d.js";import{T as Gn,M as Ar}from"../../chunks/TokenizersLanguageContent-ca787841.js";function Jn(A){let c,u,l,h,$,k,B,D,P,q,O,W,v,I,w,Z,ze,Ce,Me,ps,Nt,H,ee,Ue,ne,hs,He,fs,jt,N,ae,us,Ve,ms,gs,Ye,vs,Wt,V,te,Fe,ie,$s,Ge,_s,Mt,x,le,Es,Je,ks,Ps,ce,Ke,ws,ys,Qe,bs,Ts,Be,Ls,Xe,Ss,Ut,Y,se,Ze,de,zs,et,Cs,Ht,f,pe,Bs,tt,Is,Ds,j,xs,st,As,qs,ot,Os,Rs,rt,Ns,js,Ws,he,Ie,Ms,nt,Us,Hs,De,Vs,at,Ys,Fs,it,Gs,Js,fe,Ks,lt,Qs,Xs,ue,Zs,me,eo,ct,to,so,oo,F,M,ro,dt,no,ao,pt,io,lo,ht,co,po,y,ho,ft,fo,uo,ut,mo,go,mt,vo,$o,gt,_o,Eo,vt,ko,Po,wo,G,yo,$t,bo,To,_t,Lo,So,zo,ge,Co,Et,Bo,Io,Do,U,kt,xo,Ao,Pt,qo,Oo,wt,Ro,No,jo,yt,Wo,Mo,J,Uo,bt,Ho,Vo,Tt,Yo,Fo,Go,ve,$e,Jo,Lt,Ko,Qo,Xo,_e,Zo,St,er,tr,sr,Ee,or,zt,rr,nr,ar,ke,Ct,Pe,ir,Bt,lr,cr,dr,K,we,pr,It,hr,fr,ur,Q,ye,mr,Dt,gr,vr,$r,be,_r,xt,Er,kr,Pr,Te,wr,At,yr,br,Tr,X,Lr,qt,Sr,zr,Ot,Cr,Br,Vt;return h=new Rt({}),q=new ds({props:{name:"class tokenizers.processors.BertProcessing",anchor:"tokenizers.processors.BertProcessing",parameters:[{name:"sep",val:""},{name:"cls",val:""}],parametersDescription:[{anchor:"tokenizers.processors.BertProcessing.sep",description:`<strong>sep</strong> (<code>Tuple[str, int]</code>) &#x2014;
A tuple with the string representation of the SEP token, and its id`,name:"sep"},{anchor:"tokenizers.processors.BertProcessing.cls",description:`<strong>cls</strong> (<code>Tuple[str, int]</code>) &#x2014;
A tuple with the string representation of the CLS token, and its id`,name:"cls"}]}}),ne=new Rt({}),ae=new ds({props:{name:"class tokenizers.processors.ByteLevel",anchor:"tokenizers.processors.ByteLevel",parameters:[{name:"trim_offsets",val:" = True"}],parametersDescription:[{anchor:"tokenizers.processors.ByteLevel.trim_offsets",description:`<strong>trim_offsets</strong> (<code>bool</code>) &#x2014;
Whether to trim the whitespaces from the produced offsets.`,name:"trim_offsets"}]}}),ie=new Rt({}),le=new ds({props:{name:"class tokenizers.processors.RobertaProcessing",anchor:"tokenizers.processors.RobertaProcessing",parameters:[{name:"sep",val:""},{name:"cls",val:""},{name:"trim_offsets",val:" = True"},{name:"add_prefix_space",val:" = True"}],parametersDescription:[{anchor:"tokenizers.processors.RobertaProcessing.sep",description:`<strong>sep</strong> (<code>Tuple[str, int]</code>) &#x2014;
A tuple with the string representation of the SEP token, and its id`,name:"sep"},{anchor:"tokenizers.processors.RobertaProcessing.cls",description:`<strong>cls</strong> (<code>Tuple[str, int]</code>) &#x2014;
A tuple with the string representation of the CLS token, and its id`,name:"cls"},{anchor:"tokenizers.processors.RobertaProcessing.trim_offsets",description:`<strong>trim_offsets</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to trim the whitespaces from the produced offsets.`,name:"trim_offsets"},{anchor:"tokenizers.processors.RobertaProcessing.add_prefix_space",description:`<strong>add_prefix_space</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether the add_prefix_space option was enabled during pre-tokenization. This
is relevant because it defines the way the offsets are trimmed out.`,name:"add_prefix_space"}]}}),de=new Rt({}),pe=new ds({props:{name:"class tokenizers.processors.TemplateProcessing",anchor:"tokenizers.processors.TemplateProcessing",parameters:[{name:"single",val:""},{name:"pair",val:""},{name:"special_tokens",val:""}],parametersDescription:[{anchor:"tokenizers.processors.TemplateProcessing.single",description:`<strong>single</strong> (<code>Template</code>) &#x2014;
The template used for single sequences`,name:"single"},{anchor:"tokenizers.processors.TemplateProcessing.pair",description:`<strong>pair</strong> (<code>Template</code>) &#x2014;
The template used when both sequences are specified`,name:"pair"},{anchor:"tokenizers.processors.TemplateProcessing.special_tokens",description:`<strong>special_tokens</strong> (<code>Tokens</code>) &#x2014;
The list of special tokens used in each sequences`,name:"special_tokens"}]}}),fe=new Wn({props:{code:`[CLS]   ...   [SEP]   ...   [SEP]
0      0      0      1      1`,highlighted:`[CLS]   ...   [SEP]   ...   [SEP]
<span class="hljs-number">0</span>      <span class="hljs-number">0</span>      <span class="hljs-number">0</span>      <span class="hljs-number">1</span>      <span class="hljs-number">1</span>`}}),ue=new Wn({props:{code:`TemplateProcessing(
    single="[CLS] $0 [SEP]",
    pair="[CLS] $A [SEP] $B:1 [SEP]:1",
    special_tokens=[("[CLS]", 1), ("[SEP]", 0)],
)`,highlighted:`TemplateProcessing(
    single=<span class="hljs-string">&quot;[CLS] $0 [SEP]&quot;</span>,
    pair=<span class="hljs-string">&quot;[CLS] $A [SEP] $B:1 [SEP]:1&quot;</span>,
    special_tokens=[(<span class="hljs-string">&quot;[CLS]&quot;</span>, <span class="hljs-number">1</span>), (<span class="hljs-string">&quot;[SEP]&quot;</span>, <span class="hljs-number">0</span>)],
)`}}),{c(){c=r("h2"),u=r("a"),l=r("span"),b(h.$$.fragment),$=d(),k=r("span"),B=s("BertProcessing"),D=d(),P=r("div"),b(q.$$.fragment),O=d(),W=r("p"),v=s(`This post-processor takes care of adding the special tokens needed by
a Bert model:`),I=d(),w=r("ul"),Z=r("li"),ze=s("a SEP token"),Ce=d(),Me=r("li"),ps=s("a CLS token"),Nt=d(),H=r("h2"),ee=r("a"),Ue=r("span"),b(ne.$$.fragment),hs=d(),He=r("span"),fs=s("ByteLevel"),jt=d(),N=r("div"),b(ae.$$.fragment),us=d(),Ve=r("p"),ms=s("This post-processor takes care of trimming the offsets."),gs=d(),Ye=r("p"),vs=s(`By default, the ByteLevel BPE might include whitespaces in the produced tokens. If you don\u2019t
want the offsets to include these whitespaces, then this PostProcessor must be used.`),Wt=d(),V=r("h2"),te=r("a"),Fe=r("span"),b(ie.$$.fragment),$s=d(),Ge=r("span"),_s=s("RobertaProcessing"),Mt=d(),x=r("div"),b(le.$$.fragment),Es=d(),Je=r("p"),ks=s(`This post-processor takes care of adding the special tokens needed by
a Roberta model:`),Ps=d(),ce=r("ul"),Ke=r("li"),ws=s("a SEP token"),ys=d(),Qe=r("li"),bs=s("a CLS token"),Ts=d(),Be=r("p"),Ls=s(`It also takes care of trimming the offsets.
By default, the ByteLevel BPE might include whitespaces in the produced tokens. If you don\u2019t
want the offsets to include these whitespaces, then this PostProcessor should be initialized
with `),Xe=r("code"),Ss=s("trim_offsets=True"),Ut=d(),Y=r("h2"),se=r("a"),Ze=r("span"),b(de.$$.fragment),zs=d(),et=r("span"),Cs=s("TemplateProcessing"),Ht=d(),f=r("div"),b(pe.$$.fragment),Bs=d(),tt=r("p"),Is=s(`Provides a way to specify templates in order to add the special tokens to each
input sequence as relevant.`),Ds=d(),j=r("p"),xs=s("Let\u2019s take "),st=r("code"),As=s("BERT"),qs=s(` tokenizer as an example. It uses two special tokens, used to
delimitate each sequence. `),ot=r("code"),Os=s("[CLS]"),Rs=s(` is always used at the beginning of the first
sequence, and `),rt=r("code"),Ns=s("[SEP]"),js=s(` is added at the end of both the first, and the pair
sequences. The final result looks like this:`),Ws=d(),he=r("ul"),Ie=r("li"),Ms=s("Single sequence: "),nt=r("code"),Us=s("[CLS] Hello there [SEP]"),Hs=d(),De=r("li"),Vs=s("Pair sequences: "),at=r("code"),Ys=s("[CLS] My name is Anthony [SEP] What is my name? [SEP]"),Fs=d(),it=r("p"),Gs=s("With the type ids as following:"),Js=d(),b(fe.$$.fragment),Ks=d(),lt=r("p"),Qs=s("You can achieve such behavior using a TemplateProcessing:"),Xs=d(),b(ue.$$.fragment),Zs=d(),me=r("p"),eo=s("In this example, each input sequence is identified using a "),ct=r("code"),to=s("$"),so=s(` construct. This identifier
lets us specify each input sequence, and the type_id to use. When nothing is specified,
it uses the default values. Here are the different ways to specify it:`),oo=d(),F=r("ul"),M=r("li"),ro=s("Specifying the sequence, with default "),dt=r("code"),no=s("type_id == 0"),ao=s(": "),pt=r("code"),io=s("$A"),lo=s(" or "),ht=r("code"),co=s("$B"),po=d(),y=r("li"),ho=s("Specifying the "),ft=r("em"),fo=s("type_id"),uo=s(" with default "),ut=r("code"),mo=s("sequence == A"),go=s(": "),mt=r("code"),vo=s("$0"),$o=s(", "),gt=r("code"),_o=s("$1"),Eo=s(", "),vt=r("code"),ko=s("$2"),Po=s(", \u2026"),wo=d(),G=r("li"),yo=s("Specifying both: "),$t=r("code"),bo=s("$A:0"),To=s(", "),_t=r("code"),Lo=s("$B:1"),So=s(", \u2026"),zo=d(),ge=r("p"),Co=s("The same construct is used for special tokens: "),Et=r("code"),Bo=s("<identifier>(:<type_id>)?"),Io=s("."),Do=d(),U=r("p"),kt=r("strong"),xo=s("Warning"),Ao=s(`: You must ensure that you are giving the correct tokens/ids as these
will be added to the Encoding without any further check. If the given ids correspond
to something totally different in a `),Pt=r("em"),qo=s("Tokenizer"),Oo=s(" using this "),wt=r("em"),Ro=s("PostProcessor"),No=s(`, it
might lead to unexpected results.`),jo=d(),yt=r("p"),Wo=s("Types:"),Mo=d(),J=r("p"),Uo=s("Template ("),bt=r("code"),Ho=s("str"),Vo=s(" or "),Tt=r("code"),Yo=s("List"),Fo=s("):"),Go=d(),ve=r("ul"),$e=r("li"),Jo=s("If a "),Lt=r("code"),Ko=s("str"),Qo=s(" is provided, the whitespace is used as delimiter between tokens"),Xo=d(),_e=r("li"),Zo=s("If a "),St=r("code"),er=s("List[str]"),tr=s(" is provided, a list of tokens"),sr=d(),Ee=r("p"),or=s("Tokens ("),zt=r("code"),rr=s("List[Union[Tuple[int, str], Tuple[str, int], dict]]"),nr=s("):"),ar=d(),ke=r("ul"),Ct=r("li"),Pe=r("p"),ir=s("A "),Bt=r("code"),lr=s("Tuple"),cr=s(" with both a token and its associated ID, in any order"),dr=d(),K=r("li"),we=r("p"),pr=s("A "),It=r("code"),hr=s("dict"),fr=s(" with the following keys:"),ur=d(),Q=r("ul"),ye=r("li"),mr=s("\u201Cid\u201D: "),Dt=r("code"),gr=s("str"),vr=s(" => The special token id, as specified in the Template"),$r=d(),be=r("li"),_r=s("\u201Cids\u201D: "),xt=r("code"),Er=s("List[int]"),kr=s(" => The associated IDs"),Pr=d(),Te=r("li"),wr=s("\u201Ctokens\u201D: "),At=r("code"),yr=s("List[str]"),br=s(" => The associated tokens"),Tr=d(),X=r("p"),Lr=s("The given dict expects the provided "),qt=r("code"),Sr=s("ids"),zr=s(" and "),Ot=r("code"),Cr=s("tokens"),Br=s(` lists to have
the same length.`),this.h()},l(i){c=n(i,"H2",{class:!0});var _=a(c);u=n(_,"A",{id:!0,class:!0,href:!0});var qr=a(u);l=n(qr,"SPAN",{});var Or=a(l);T(h.$$.fragment,Or),Or.forEach(t),qr.forEach(t),$=p(_),k=n(_,"SPAN",{});var Rr=a(k);B=o(Rr,"BertProcessing"),Rr.forEach(t),_.forEach(t),D=p(i),P=n(i,"DIV",{class:!0});var xe=a(P);T(q.$$.fragment,xe),O=p(xe),W=n(xe,"P",{});var Nr=a(W);v=o(Nr,`This post-processor takes care of adding the special tokens needed by
a Bert model:`),Nr.forEach(t),I=p(xe),w=n(xe,"UL",{});var Yt=a(w);Z=n(Yt,"LI",{});var jr=a(Z);ze=o(jr,"a SEP token"),jr.forEach(t),Ce=p(Yt),Me=n(Yt,"LI",{});var Wr=a(Me);ps=o(Wr,"a CLS token"),Wr.forEach(t),Yt.forEach(t),xe.forEach(t),Nt=p(i),H=n(i,"H2",{class:!0});var Ft=a(H);ee=n(Ft,"A",{id:!0,class:!0,href:!0});var Mr=a(ee);Ue=n(Mr,"SPAN",{});var Ur=a(Ue);T(ne.$$.fragment,Ur),Ur.forEach(t),Mr.forEach(t),hs=p(Ft),He=n(Ft,"SPAN",{});var Hr=a(He);fs=o(Hr,"ByteLevel"),Hr.forEach(t),Ft.forEach(t),jt=p(i),N=n(i,"DIV",{class:!0});var Ae=a(N);T(ae.$$.fragment,Ae),us=p(Ae),Ve=n(Ae,"P",{});var Vr=a(Ve);ms=o(Vr,"This post-processor takes care of trimming the offsets."),Vr.forEach(t),gs=p(Ae),Ye=n(Ae,"P",{});var Yr=a(Ye);vs=o(Yr,`By default, the ByteLevel BPE might include whitespaces in the produced tokens. If you don\u2019t
want the offsets to include these whitespaces, then this PostProcessor must be used.`),Yr.forEach(t),Ae.forEach(t),Wt=p(i),V=n(i,"H2",{class:!0});var Gt=a(V);te=n(Gt,"A",{id:!0,class:!0,href:!0});var Fr=a(te);Fe=n(Fr,"SPAN",{});var Gr=a(Fe);T(ie.$$.fragment,Gr),Gr.forEach(t),Fr.forEach(t),$s=p(Gt),Ge=n(Gt,"SPAN",{});var Jr=a(Ge);_s=o(Jr,"RobertaProcessing"),Jr.forEach(t),Gt.forEach(t),Mt=p(i),x=n(i,"DIV",{class:!0});var oe=a(x);T(le.$$.fragment,oe),Es=p(oe),Je=n(oe,"P",{});var Kr=a(Je);ks=o(Kr,`This post-processor takes care of adding the special tokens needed by
a Roberta model:`),Kr.forEach(t),Ps=p(oe),ce=n(oe,"UL",{});var Jt=a(ce);Ke=n(Jt,"LI",{});var Qr=a(Ke);ws=o(Qr,"a SEP token"),Qr.forEach(t),ys=p(Jt),Qe=n(Jt,"LI",{});var Xr=a(Qe);bs=o(Xr,"a CLS token"),Xr.forEach(t),Jt.forEach(t),Ts=p(oe),Be=n(oe,"P",{});var Ir=a(Be);Ls=o(Ir,`It also takes care of trimming the offsets.
By default, the ByteLevel BPE might include whitespaces in the produced tokens. If you don\u2019t
want the offsets to include these whitespaces, then this PostProcessor should be initialized
with `),Xe=n(Ir,"CODE",{});var Zr=a(Xe);Ss=o(Zr,"trim_offsets=True"),Zr.forEach(t),Ir.forEach(t),oe.forEach(t),Ut=p(i),Y=n(i,"H2",{class:!0});var Kt=a(Y);se=n(Kt,"A",{id:!0,class:!0,href:!0});var en=a(se);Ze=n(en,"SPAN",{});var tn=a(Ze);T(de.$$.fragment,tn),tn.forEach(t),en.forEach(t),zs=p(Kt),et=n(Kt,"SPAN",{});var sn=a(et);Cs=o(sn,"TemplateProcessing"),sn.forEach(t),Kt.forEach(t),Ht=p(i),f=n(i,"DIV",{class:!0});var m=a(f);T(pe.$$.fragment,m),Bs=p(m),tt=n(m,"P",{});var on=a(tt);Is=o(on,`Provides a way to specify templates in order to add the special tokens to each
input sequence as relevant.`),on.forEach(t),Ds=p(m),j=n(m,"P",{});var re=a(j);xs=o(re,"Let\u2019s take "),st=n(re,"CODE",{});var rn=a(st);As=o(rn,"BERT"),rn.forEach(t),qs=o(re,` tokenizer as an example. It uses two special tokens, used to
delimitate each sequence. `),ot=n(re,"CODE",{});var nn=a(ot);Os=o(nn,"[CLS]"),nn.forEach(t),Rs=o(re,` is always used at the beginning of the first
sequence, and `),rt=n(re,"CODE",{});var an=a(rt);Ns=o(an,"[SEP]"),an.forEach(t),js=o(re,` is added at the end of both the first, and the pair
sequences. The final result looks like this:`),re.forEach(t),Ws=p(m),he=n(m,"UL",{});var Qt=a(he);Ie=n(Qt,"LI",{});var Dr=a(Ie);Ms=o(Dr,"Single sequence: "),nt=n(Dr,"CODE",{});var ln=a(nt);Us=o(ln,"[CLS] Hello there [SEP]"),ln.forEach(t),Dr.forEach(t),Hs=p(Qt),De=n(Qt,"LI",{});var xr=a(De);Vs=o(xr,"Pair sequences: "),at=n(xr,"CODE",{});var cn=a(at);Ys=o(cn,"[CLS] My name is Anthony [SEP] What is my name? [SEP]"),cn.forEach(t),xr.forEach(t),Qt.forEach(t),Fs=p(m),it=n(m,"P",{});var dn=a(it);Gs=o(dn,"With the type ids as following:"),dn.forEach(t),Js=p(m),T(fe.$$.fragment,m),Ks=p(m),lt=n(m,"P",{});var pn=a(lt);Qs=o(pn,"You can achieve such behavior using a TemplateProcessing:"),pn.forEach(t),Xs=p(m),T(ue.$$.fragment,m),Zs=p(m),me=n(m,"P",{});var Xt=a(me);eo=o(Xt,"In this example, each input sequence is identified using a "),ct=n(Xt,"CODE",{});var hn=a(ct);to=o(hn,"$"),hn.forEach(t),so=o(Xt,` construct. This identifier
lets us specify each input sequence, and the type_id to use. When nothing is specified,
it uses the default values. Here are the different ways to specify it:`),Xt.forEach(t),oo=p(m),F=n(m,"UL",{});var qe=a(F);M=n(qe,"LI",{});var Le=a(M);ro=o(Le,"Specifying the sequence, with default "),dt=n(Le,"CODE",{});var fn=a(dt);no=o(fn,"type_id == 0"),fn.forEach(t),ao=o(Le,": "),pt=n(Le,"CODE",{});var un=a(pt);io=o(un,"$A"),un.forEach(t),lo=o(Le," or "),ht=n(Le,"CODE",{});var mn=a(ht);co=o(mn,"$B"),mn.forEach(t),Le.forEach(t),po=p(qe),y=n(qe,"LI",{});var R=a(y);ho=o(R,"Specifying the "),ft=n(R,"EM",{});var gn=a(ft);fo=o(gn,"type_id"),gn.forEach(t),uo=o(R," with default "),ut=n(R,"CODE",{});var vn=a(ut);mo=o(vn,"sequence == A"),vn.forEach(t),go=o(R,": "),mt=n(R,"CODE",{});var $n=a(mt);vo=o($n,"$0"),$n.forEach(t),$o=o(R,", "),gt=n(R,"CODE",{});var _n=a(gt);_o=o(_n,"$1"),_n.forEach(t),Eo=o(R,", "),vt=n(R,"CODE",{});var En=a(vt);ko=o(En,"$2"),En.forEach(t),Po=o(R,", \u2026"),R.forEach(t),wo=p(qe),G=n(qe,"LI",{});var Oe=a(G);yo=o(Oe,"Specifying both: "),$t=n(Oe,"CODE",{});var kn=a($t);bo=o(kn,"$A:0"),kn.forEach(t),To=o(Oe,", "),_t=n(Oe,"CODE",{});var Pn=a(_t);Lo=o(Pn,"$B:1"),Pn.forEach(t),So=o(Oe,", \u2026"),Oe.forEach(t),qe.forEach(t),zo=p(m),ge=n(m,"P",{});var Zt=a(ge);Co=o(Zt,"The same construct is used for special tokens: "),Et=n(Zt,"CODE",{});var wn=a(Et);Bo=o(wn,"<identifier>(:<type_id>)?"),wn.forEach(t),Io=o(Zt,"."),Zt.forEach(t),Do=p(m),U=n(m,"P",{});var Se=a(U);kt=n(Se,"STRONG",{});var yn=a(kt);xo=o(yn,"Warning"),yn.forEach(t),Ao=o(Se,`: You must ensure that you are giving the correct tokens/ids as these
will be added to the Encoding without any further check. If the given ids correspond
to something totally different in a `),Pt=n(Se,"EM",{});var bn=a(Pt);qo=o(bn,"Tokenizer"),bn.forEach(t),Oo=o(Se," using this "),wt=n(Se,"EM",{});var Tn=a(wt);Ro=o(Tn,"PostProcessor"),Tn.forEach(t),No=o(Se,`, it
might lead to unexpected results.`),Se.forEach(t),jo=p(m),yt=n(m,"P",{});var Ln=a(yt);Wo=o(Ln,"Types:"),Ln.forEach(t),Mo=p(m),J=n(m,"P",{});var Re=a(J);Uo=o(Re,"Template ("),bt=n(Re,"CODE",{});var Sn=a(bt);Ho=o(Sn,"str"),Sn.forEach(t),Vo=o(Re," or "),Tt=n(Re,"CODE",{});var zn=a(Tt);Yo=o(zn,"List"),zn.forEach(t),Fo=o(Re,"):"),Re.forEach(t),Go=p(m),ve=n(m,"UL",{});var es=a(ve);$e=n(es,"LI",{});var ts=a($e);Jo=o(ts,"If a "),Lt=n(ts,"CODE",{});var Cn=a(Lt);Ko=o(Cn,"str"),Cn.forEach(t),Qo=o(ts," is provided, the whitespace is used as delimiter between tokens"),ts.forEach(t),Xo=p(es),_e=n(es,"LI",{});var ss=a(_e);Zo=o(ss,"If a "),St=n(ss,"CODE",{});var Bn=a(St);er=o(Bn,"List[str]"),Bn.forEach(t),tr=o(ss," is provided, a list of tokens"),ss.forEach(t),es.forEach(t),sr=p(m),Ee=n(m,"P",{});var os=a(Ee);or=o(os,"Tokens ("),zt=n(os,"CODE",{});var In=a(zt);rr=o(In,"List[Union[Tuple[int, str], Tuple[str, int], dict]]"),In.forEach(t),nr=o(os,"):"),os.forEach(t),ar=p(m),ke=n(m,"UL",{});var rs=a(ke);Ct=n(rs,"LI",{});var Dn=a(Ct);Pe=n(Dn,"P",{});var ns=a(Pe);ir=o(ns,"A "),Bt=n(ns,"CODE",{});var xn=a(Bt);lr=o(xn,"Tuple"),xn.forEach(t),cr=o(ns," with both a token and its associated ID, in any order"),ns.forEach(t),Dn.forEach(t),dr=p(rs),K=n(rs,"LI",{});var Ne=a(K);we=n(Ne,"P",{});var as=a(we);pr=o(as,"A "),It=n(as,"CODE",{});var An=a(It);hr=o(An,"dict"),An.forEach(t),fr=o(as," with the following keys:"),as.forEach(t),ur=p(Ne),Q=n(Ne,"UL",{});var je=a(Q);ye=n(je,"LI",{});var is=a(ye);mr=o(is,"\u201Cid\u201D: "),Dt=n(is,"CODE",{});var qn=a(Dt);gr=o(qn,"str"),qn.forEach(t),vr=o(is," => The special token id, as specified in the Template"),is.forEach(t),$r=p(je),be=n(je,"LI",{});var ls=a(be);_r=o(ls,"\u201Cids\u201D: "),xt=n(ls,"CODE",{});var On=a(xt);Er=o(On,"List[int]"),On.forEach(t),kr=o(ls," => The associated IDs"),ls.forEach(t),Pr=p(je),Te=n(je,"LI",{});var cs=a(Te);wr=o(cs,"\u201Ctokens\u201D: "),At=n(cs,"CODE",{});var Rn=a(At);yr=o(Rn,"List[str]"),Rn.forEach(t),br=o(cs," => The associated tokens"),cs.forEach(t),je.forEach(t),Tr=p(Ne),X=n(Ne,"P",{});var We=a(X);Lr=o(We,"The given dict expects the provided "),qt=n(We,"CODE",{});var Nn=a(qt);Sr=o(Nn,"ids"),Nn.forEach(t),zr=o(We," and "),Ot=n(We,"CODE",{});var jn=a(Ot);Cr=o(jn,"tokens"),jn.forEach(t),Br=o(We,` lists to have
the same length.`),We.forEach(t),Ne.forEach(t),rs.forEach(t),m.forEach(t),this.h()},h(){g(u,"id","tokenizers.processors.BertProcessing]][[tokenizers.processors.BertProcessing"),g(u,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),g(u,"href","#tokenizers.processors.BertProcessing]][[tokenizers.processors.BertProcessing"),g(c,"class","relative group"),g(P,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),g(ee,"id","tokenizers.processors.ByteLevel]][[tokenizers.processors.ByteLevel"),g(ee,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),g(ee,"href","#tokenizers.processors.ByteLevel]][[tokenizers.processors.ByteLevel"),g(H,"class","relative group"),g(N,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),g(te,"id","tokenizers.processors.RobertaProcessing]][[tokenizers.processors.RobertaProcessing"),g(te,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),g(te,"href","#tokenizers.processors.RobertaProcessing]][[tokenizers.processors.RobertaProcessing"),g(V,"class","relative group"),g(x,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),g(se,"id","tokenizers.processors.TemplateProcessing]][[tokenizers.processors.TemplateProcessing"),g(se,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),g(se,"href","#tokenizers.processors.TemplateProcessing]][[tokenizers.processors.TemplateProcessing"),g(Y,"class","relative group"),g(f,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(i,_){E(i,c,_),e(c,u),e(u,l),L(h,l,null),e(c,$),e(c,k),e(k,B),E(i,D,_),E(i,P,_),L(q,P,null),e(P,O),e(P,W),e(W,v),e(P,I),e(P,w),e(w,Z),e(Z,ze),e(w,Ce),e(w,Me),e(Me,ps),E(i,Nt,_),E(i,H,_),e(H,ee),e(ee,Ue),L(ne,Ue,null),e(H,hs),e(H,He),e(He,fs),E(i,jt,_),E(i,N,_),L(ae,N,null),e(N,us),e(N,Ve),e(Ve,ms),e(N,gs),e(N,Ye),e(Ye,vs),E(i,Wt,_),E(i,V,_),e(V,te),e(te,Fe),L(ie,Fe,null),e(V,$s),e(V,Ge),e(Ge,_s),E(i,Mt,_),E(i,x,_),L(le,x,null),e(x,Es),e(x,Je),e(Je,ks),e(x,Ps),e(x,ce),e(ce,Ke),e(Ke,ws),e(ce,ys),e(ce,Qe),e(Qe,bs),e(x,Ts),e(x,Be),e(Be,Ls),e(Be,Xe),e(Xe,Ss),E(i,Ut,_),E(i,Y,_),e(Y,se),e(se,Ze),L(de,Ze,null),e(Y,zs),e(Y,et),e(et,Cs),E(i,Ht,_),E(i,f,_),L(pe,f,null),e(f,Bs),e(f,tt),e(tt,Is),e(f,Ds),e(f,j),e(j,xs),e(j,st),e(st,As),e(j,qs),e(j,ot),e(ot,Os),e(j,Rs),e(j,rt),e(rt,Ns),e(j,js),e(f,Ws),e(f,he),e(he,Ie),e(Ie,Ms),e(Ie,nt),e(nt,Us),e(he,Hs),e(he,De),e(De,Vs),e(De,at),e(at,Ys),e(f,Fs),e(f,it),e(it,Gs),e(f,Js),L(fe,f,null),e(f,Ks),e(f,lt),e(lt,Qs),e(f,Xs),L(ue,f,null),e(f,Zs),e(f,me),e(me,eo),e(me,ct),e(ct,to),e(me,so),e(f,oo),e(f,F),e(F,M),e(M,ro),e(M,dt),e(dt,no),e(M,ao),e(M,pt),e(pt,io),e(M,lo),e(M,ht),e(ht,co),e(F,po),e(F,y),e(y,ho),e(y,ft),e(ft,fo),e(y,uo),e(y,ut),e(ut,mo),e(y,go),e(y,mt),e(mt,vo),e(y,$o),e(y,gt),e(gt,_o),e(y,Eo),e(y,vt),e(vt,ko),e(y,Po),e(F,wo),e(F,G),e(G,yo),e(G,$t),e($t,bo),e(G,To),e(G,_t),e(_t,Lo),e(G,So),e(f,zo),e(f,ge),e(ge,Co),e(ge,Et),e(Et,Bo),e(ge,Io),e(f,Do),e(f,U),e(U,kt),e(kt,xo),e(U,Ao),e(U,Pt),e(Pt,qo),e(U,Oo),e(U,wt),e(wt,Ro),e(U,No),e(f,jo),e(f,yt),e(yt,Wo),e(f,Mo),e(f,J),e(J,Uo),e(J,bt),e(bt,Ho),e(J,Vo),e(J,Tt),e(Tt,Yo),e(J,Fo),e(f,Go),e(f,ve),e(ve,$e),e($e,Jo),e($e,Lt),e(Lt,Ko),e($e,Qo),e(ve,Xo),e(ve,_e),e(_e,Zo),e(_e,St),e(St,er),e(_e,tr),e(f,sr),e(f,Ee),e(Ee,or),e(Ee,zt),e(zt,rr),e(Ee,nr),e(f,ar),e(f,ke),e(ke,Ct),e(Ct,Pe),e(Pe,ir),e(Pe,Bt),e(Bt,lr),e(Pe,cr),e(ke,dr),e(ke,K),e(K,we),e(we,pr),e(we,It),e(It,hr),e(we,fr),e(K,ur),e(K,Q),e(Q,ye),e(ye,mr),e(ye,Dt),e(Dt,gr),e(ye,vr),e(Q,$r),e(Q,be),e(be,_r),e(be,xt),e(xt,Er),e(be,kr),e(Q,Pr),e(Q,Te),e(Te,wr),e(Te,At),e(At,yr),e(Te,br),e(K,Tr),e(K,X),e(X,Lr),e(X,qt),e(qt,Sr),e(X,zr),e(X,Ot),e(Ot,Cr),e(X,Br),Vt=!0},p:Fn,i(i){Vt||(S(h.$$.fragment,i),S(q.$$.fragment,i),S(ne.$$.fragment,i),S(ae.$$.fragment,i),S(ie.$$.fragment,i),S(le.$$.fragment,i),S(de.$$.fragment,i),S(pe.$$.fragment,i),S(fe.$$.fragment,i),S(ue.$$.fragment,i),Vt=!0)},o(i){z(h.$$.fragment,i),z(q.$$.fragment,i),z(ne.$$.fragment,i),z(ae.$$.fragment,i),z(ie.$$.fragment,i),z(le.$$.fragment,i),z(de.$$.fragment,i),z(pe.$$.fragment,i),z(fe.$$.fragment,i),z(ue.$$.fragment,i),Vt=!1},d(i){i&&t(c),C(h),i&&t(D),i&&t(P),C(q),i&&t(Nt),i&&t(H),C(ne),i&&t(jt),i&&t(N),C(ae),i&&t(Wt),i&&t(V),C(ie),i&&t(Mt),i&&t(x),C(le),i&&t(Ut),i&&t(Y),C(de),i&&t(Ht),i&&t(f),C(pe),C(fe),C(ue)}}}function Kn(A){let c,u;return c=new Ar({props:{$$slots:{default:[Jn]},$$scope:{ctx:A}}}),{c(){b(c.$$.fragment)},l(l){T(c.$$.fragment,l)},m(l,h){L(c,l,h),u=!0},p(l,h){const $={};h&2&&($.$$scope={dirty:h,ctx:l}),c.$set($)},i(l){u||(S(c.$$.fragment,l),u=!0)},o(l){z(c.$$.fragment,l),u=!1},d(l){C(c,l)}}}function Qn(A){let c,u,l,h,$;return{c(){c=r("p"),u=s("The Rust API Reference is available directly on the "),l=r("a"),h=s("Docs.rs"),$=s(" website."),this.h()},l(k){c=n(k,"P",{});var B=a(c);u=o(B,"The Rust API Reference is available directly on the "),l=n(B,"A",{href:!0,rel:!0});var D=a(l);h=o(D,"Docs.rs"),D.forEach(t),$=o(B," website."),B.forEach(t),this.h()},h(){g(l,"href","https://docs.rs/tokenizers/latest/tokenizers/"),g(l,"rel","nofollow")},m(k,B){E(k,c,B),e(c,u),e(c,l),e(l,h),e(c,$)},d(k){k&&t(c)}}}function Xn(A){let c,u;return c=new Ar({props:{$$slots:{default:[Qn]},$$scope:{ctx:A}}}),{c(){b(c.$$.fragment)},l(l){T(c.$$.fragment,l)},m(l,h){L(c,l,h),u=!0},p(l,h){const $={};h&2&&($.$$scope={dirty:h,ctx:l}),c.$set($)},i(l){u||(S(c.$$.fragment,l),u=!0)},o(l){z(c.$$.fragment,l),u=!1},d(l){C(c,l)}}}function Zn(A){let c,u;return{c(){c=r("p"),u=s("The node API has not been documented yet.")},l(l){c=n(l,"P",{});var h=a(c);u=o(h,"The node API has not been documented yet."),h.forEach(t)},m(l,h){E(l,c,h),e(c,u)},d(l){l&&t(c)}}}function ea(A){let c,u;return c=new Ar({props:{$$slots:{default:[Zn]},$$scope:{ctx:A}}}),{c(){b(c.$$.fragment)},l(l){T(c.$$.fragment,l)},m(l,h){L(c,l,h),u=!0},p(l,h){const $={};h&2&&($.$$scope={dirty:h,ctx:l}),c.$set($)},i(l){u||(S(c.$$.fragment,l),u=!0)},o(l){z(c.$$.fragment,l),u=!1},d(l){C(c,l)}}}function ta(A){let c,u,l,h,$,k,B,D,P,q,O,W;return k=new Rt({}),O=new Gn({props:{python:!0,rust:!0,node:!0,$$slots:{node:[ea],rust:[Xn],python:[Kn]},$$scope:{ctx:A}}}),{c(){c=r("meta"),u=d(),l=r("h1"),h=r("a"),$=r("span"),b(k.$$.fragment),B=d(),D=r("span"),P=s("Post-processors"),q=d(),b(O.$$.fragment),this.h()},l(v){const I=Vn('[data-svelte="svelte-1phssyn"]',document.head);c=n(I,"META",{name:!0,content:!0}),I.forEach(t),u=p(v),l=n(v,"H1",{class:!0});var w=a(l);h=n(w,"A",{id:!0,class:!0,href:!0});var Z=a(h);$=n(Z,"SPAN",{});var ze=a($);T(k.$$.fragment,ze),ze.forEach(t),Z.forEach(t),B=p(w),D=n(w,"SPAN",{});var Ce=a(D);P=o(Ce,"Post-processors"),Ce.forEach(t),w.forEach(t),q=p(v),T(O.$$.fragment,v),this.h()},h(){g(c,"name","hf:doc:metadata"),g(c,"content",JSON.stringify(sa)),g(h,"id","postprocessors"),g(h,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),g(h,"href","#postprocessors"),g(l,"class","relative group")},m(v,I){e(document.head,c),E(v,u,I),E(v,l,I),e(l,h),e(h,$),L(k,$,null),e(l,B),e(l,D),e(D,P),E(v,q,I),L(O,v,I),W=!0},p(v,[I]){const w={};I&2&&(w.$$scope={dirty:I,ctx:v}),O.$set(w)},i(v){W||(S(k.$$.fragment,v),S(O.$$.fragment,v),W=!0)},o(v){z(k.$$.fragment,v),z(O.$$.fragment,v),W=!1},d(v){t(c),v&&t(u),v&&t(l),C(k),v&&t(q),C(O,v)}}}const sa={local:"postprocessors",sections:[{local:"tokenizers.processors.BertProcessing]][[tokenizers.processors.BertProcessing",title:"BertProcessing"},{local:"tokenizers.processors.ByteLevel]][[tokenizers.processors.ByteLevel",title:"ByteLevel"},{local:"tokenizers.processors.RobertaProcessing]][[tokenizers.processors.RobertaProcessing",title:"RobertaProcessing"},{local:"tokenizers.processors.TemplateProcessing]][[tokenizers.processors.TemplateProcessing",title:"TemplateProcessing"}],title:"Post-processors"};function oa(A){return Yn(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class ca extends Mn{constructor(c){super();Un(this,c,oa,ta,Hn,{})}}export{ca as default,sa as metadata};
