import{S as Jo,i as Go,s as Qo,e as s,k as i,w as k,t as h,M as Xo,c as n,d as r,m as l,a as o,x as $,h as d,b as a,F as t,g as u,y as g,q as _,o as z,B as w,v as Yo,L as Zo}from"../../chunks/vendor-0d3f0756.js";import{D as x}from"../../chunks/Docstring-f99fb2a0.js";import{C as Ko}from"../../chunks/CodeBlock-7b0cb15c.js";import{I as B}from"../../chunks/IconCopyLink-9193371d.js";import{T as ea,M as In}from"../../chunks/TokenizersLanguageContent-ca787841.js";function ta(N){let c,v,p,m,y,P,D,C,T,W,L,j,b,A,q,st,ye,H,de,ut,Pe,Jr,mt,Gr,ir,I,Ee,Qr,vt,Xr,Yr,kt,Zr,es,O,Te,ts,$t,rs,ss,gt,ns,lr,Q,fe,_t,Se,os,zt,as,pr,X,xe,is,nt,ls,wt,ps,cr,Y,ue,bt,De,cs,yt,hs,hr,E,Ae,ds,Pt,fs,us,Et,ms,vs,Ce,ks,Tt,$s,gs,qe,dr,Z,me,St,Ie,_s,xt,zs,fr,V,Be,ws,Dt,bs,ys,At,Ps,ur,ee,ve,Ct,Ne,Es,qt,Ts,mr,S,We,Ss,It,xs,Ds,Bt,As,Cs,F,Le,qs,He,Is,Nt,Bs,Ns,Ws,R,Ls,Wt,Hs,Ms,Lt,Vs,Us,Ht,js,Os,K,Me,Fs,Mt,Rs,Ks,J,Js,ot,Gs,Qs,Vt,Xs,Ys,Ut,Zs,vr,te,ke,jt,Ve,en,Ot,tn,kr,re,Ue,rn,Ft,sn,$r,se,$e,Rt,je,nn,Kt,on,gr,ne,Oe,an,Jt,ln,_r,oe,ge,Gt,Fe,pn,Qt,cn,zr,U,Re,hn,Xt,dn,fn,Yt,un,wr,ae,_e,Zt,Ke,mn,er,vn,br,ie,Je,kn,Ge,$n,Qe,gn,_n,yr,le,ze,tr,Xe,zn,rr,wn,Pr,pe,Ye,bn,at,yn,sr,Pn,Er,ce,we,nr,Ze,En,or,Tn,Tr,he,et,Sn,it,xn,ar,Dn,Sr;return m=new B({}),W=new x({props:{name:"class tokenizers.pre_tokenizers.BertPreTokenizer",anchor:"tokenizers.pre_tokenizers.BertPreTokenizer",parameters:[]}}),Pe=new B({}),Ee=new x({props:{name:"class tokenizers.pre_tokenizers.ByteLevel",anchor:"tokenizers.pre_tokenizers.ByteLevel",parameters:[{name:"add_prefix_space",val:" = True"}],parametersDescription:[{anchor:"tokenizers.pre_tokenizers.ByteLevel.add_prefix_space",description:`<strong>add_prefix_space</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to add a space to the first word if there isn&#x2019;t already one. This
lets us treat <em>hello</em> exactly like <em>say hello</em>.`,name:"add_prefix_space"}]}}),Te=new x({props:{name:"alphabet",anchor:"tokenizers.pre_tokenizers.ByteLevel.alphabet",parameters:[],returnDescription:`
<p>A list of characters that compose the alphabet</p>
`,returnType:`
<p><code>List[str]</code></p>
`}}),Se=new B({}),xe=new x({props:{name:"class tokenizers.pre_tokenizers.CharDelimiterSplit",anchor:"tokenizers.pre_tokenizers.CharDelimiterSplit",parameters:""}}),De=new B({}),Ae=new x({props:{name:"class tokenizers.pre_tokenizers.Digits",anchor:"tokenizers.pre_tokenizers.Digits",parameters:[{name:"individual_digits",val:" = False"}],parametersDescription:[{anchor:"tokenizers.pre_tokenizers.Digits.individual_digits",description:"<strong>individual_digits</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;",name:"individual_digits"}]}}),Ce=new Ko({props:{code:'"Call 123 please" -> "Call ", "1", "2", "3", " please"',highlighted:'<span class="hljs-string">&quot;Call 123 please&quot;</span> -&gt; <span class="hljs-string">&quot;Call &quot;</span>, <span class="hljs-string">&quot;1&quot;</span>, <span class="hljs-string">&quot;2&quot;</span>, <span class="hljs-string">&quot;3&quot;</span>, <span class="hljs-string">&quot; please&quot;</span>'}}),qe=new Ko({props:{code:'"Call 123 please" -> "Call ", "123", " please"',highlighted:'<span class="hljs-string">&quot;Call 123 please&quot;</span> -&gt; <span class="hljs-string">&quot;Call &quot;</span>, <span class="hljs-string">&quot;123&quot;</span>, <span class="hljs-string">&quot; please&quot;</span>'}}),Ie=new B({}),Be=new x({props:{name:"class tokenizers.pre_tokenizers.Metaspace",anchor:"tokenizers.pre_tokenizers.Metaspace",parameters:[{name:"replacement",val:" = '_'"},{name:"add_prefix_space",val:" = True"}],parametersDescription:[{anchor:"tokenizers.pre_tokenizers.Metaspace.replacement",description:`<strong>replacement</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&#x2581;</code>) &#x2014;
The replacement character. Must be exactly one character. By default we
use the <em>&#x2581;</em> (U+2581) meta symbol (Same as in SentencePiece).`,name:"replacement"},{anchor:"tokenizers.pre_tokenizers.Metaspace.add_prefix_space",description:`<strong>add_prefix_space</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to add a space to the first word if there isn&#x2019;t already one. This
lets us treat <em>hello</em> exactly like <em>say hello</em>.`,name:"add_prefix_space"}]}}),Ne=new B({}),We=new x({props:{name:"class tokenizers.pre_tokenizers.PreTokenizer",anchor:"tokenizers.pre_tokenizers.PreTokenizer",parameters:[]}}),Le=new x({props:{name:"pre_tokenize",anchor:"tokenizers.pre_tokenizers.PreTokenizer.pre_tokenize",parameters:[{name:"pretok",val:""}],parametersDescription:[{anchor:"tokenizers.pre_tokenizers.PreTokenizer.pre_tokenize.pretok",description:"<strong>pretok</strong> (<code>PreTokenizedString) -- The pre-tokenized string on which to apply this :class:</code>~tokenizers.pre_tokenizers.PreTokenizer`",name:"pretok"}]}}),Me=new x({props:{name:"pre_tokenize_str",anchor:"tokenizers.pre_tokenizers.PreTokenizer.pre_tokenize_str",parameters:[{name:"sequence",val:""}],parametersDescription:[{anchor:"tokenizers.pre_tokenizers.PreTokenizer.pre_tokenize_str.sequence",description:`<strong>sequence</strong> (<code>str</code>) &#x2014;
A string to pre-tokeize`,name:"sequence"}],returnDescription:`
<p>A list of tuple with the pre-tokenized parts and their offsets</p>
`,returnType:`
<p><code>List[Tuple[str, Offsets]]</code></p>
`}}),Ve=new B({}),Ue=new x({props:{name:"class tokenizers.pre_tokenizers.Punctuation",anchor:"tokenizers.pre_tokenizers.Punctuation",parameters:[{name:"behavior",val:" = 'isolated'"}],parametersDescription:[{anchor:"tokenizers.pre_tokenizers.Punctuation.behavior",description:`<strong>behavior</strong> (<code>SplitDelimiterBehavior</code>) &#x2014;
The behavior to use when splitting.
Choices: &#x201C;removed&#x201D;, &#x201C;isolated&#x201D; (default), &#x201C;merged_with_previous&#x201D;, &#x201C;merged_with_next&#x201D;,
&#x201C;contiguous&#x201D;`,name:"behavior"}]}}),je=new B({}),Oe=new x({props:{name:"class tokenizers.pre_tokenizers.Sequence",anchor:"tokenizers.pre_tokenizers.Sequence",parameters:[{name:"pretokenizers",val:""}]}}),Fe=new B({}),Re=new x({props:{name:"class tokenizers.pre_tokenizers.Split",anchor:"tokenizers.pre_tokenizers.Split",parameters:[{name:"pattern",val:""},{name:"behavior",val:""},{name:"invert",val:" = False"}],parametersDescription:[{anchor:"tokenizers.pre_tokenizers.Split.pattern",description:`<strong>pattern</strong> (<code>str</code> or <code>Regex</code>) &#x2014;
A pattern used to split the string. Usually a string or a Regex`,name:"pattern"},{anchor:"tokenizers.pre_tokenizers.Split.behavior",description:`<strong>behavior</strong> (<code>SplitDelimiterBehavior</code>) &#x2014;
The behavior to use when splitting.
Choices: &#x201C;removed&#x201D;, &#x201C;isolated&#x201D;, &#x201C;merged_with_previous&#x201D;, &#x201C;merged_with_next&#x201D;,
&#x201C;contiguous&#x201D;`,name:"behavior"},{anchor:"tokenizers.pre_tokenizers.Split.invert",description:`<strong>invert</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to invert the pattern.`,name:"invert"}]}}),Ke=new B({}),Je=new x({props:{name:"class tokenizers.pre_tokenizers.UnicodeScripts",anchor:"tokenizers.pre_tokenizers.UnicodeScripts",parameters:[]}}),Xe=new B({}),Ye=new x({props:{name:"class tokenizers.pre_tokenizers.Whitespace",anchor:"tokenizers.pre_tokenizers.Whitespace",parameters:[]}}),Ze=new B({}),et=new x({props:{name:"class tokenizers.pre_tokenizers.WhitespaceSplit",anchor:"tokenizers.pre_tokenizers.WhitespaceSplit",parameters:[]}}),{c(){c=s("h2"),v=s("a"),p=s("span"),k(m.$$.fragment),y=i(),P=s("span"),D=h("BertPreTokenizer"),C=i(),T=s("div"),k(W.$$.fragment),L=i(),j=s("p"),b=h("BertPreTokenizer"),A=i(),q=s("p"),st=h(`This pre-tokenizer splits tokens on spaces, and also on punctuation.
Each occurence of a punctuation character will be treated separately.`),ye=i(),H=s("h2"),de=s("a"),ut=s("span"),k(Pe.$$.fragment),Jr=i(),mt=s("span"),Gr=h("ByteLevel"),ir=i(),I=s("div"),k(Ee.$$.fragment),Qr=i(),vt=s("p"),Xr=h("ByteLevel PreTokenizer"),Yr=i(),kt=s("p"),Zr=h(`This pre-tokenizer takes care of replacing all bytes of the given string
with a corresponding representation, as well as splitting into words.`),es=i(),O=s("div"),k(Te.$$.fragment),ts=i(),$t=s("p"),rs=h("Returns the alphabet used by this PreTokenizer."),ss=i(),gt=s("p"),ns=h(`Since the ByteLevel works as its name suggests, at the byte level, it
encodes each byte value to a unique visible character. This means that there is a
total of 256 different characters composing this alphabet.`),lr=i(),Q=s("h2"),fe=s("a"),_t=s("span"),k(Se.$$.fragment),os=i(),zt=s("span"),as=h("CharDelimiterSplit"),pr=i(),X=s("div"),k(xe.$$.fragment),is=i(),nt=s("p"),ls=h("This pre-tokenizer simply splits on the provided char. Works like "),wt=s("code"),ps=h(".split(delimiter)"),cr=i(),Y=s("h2"),ue=s("a"),bt=s("span"),k(De.$$.fragment),cs=i(),yt=s("span"),hs=h("Digits"),hr=i(),E=s("div"),k(Ae.$$.fragment),ds=i(),Pt=s("p"),fs=h("This pre-tokenizer simply splits using the digits in separate tokens"),us=i(),Et=s("p"),ms=h("If set to True, digits will each be separated as follows:"),vs=i(),k(Ce.$$.fragment),ks=i(),Tt=s("p"),$s=h("If set to False, digits will grouped as follows:"),gs=i(),k(qe.$$.fragment),dr=i(),Z=s("h2"),me=s("a"),St=s("span"),k(Ie.$$.fragment),_s=i(),xt=s("span"),zs=h("Metaspace"),fr=i(),V=s("div"),k(Be.$$.fragment),ws=i(),Dt=s("p"),bs=h("Metaspace pre-tokenizer"),ys=i(),At=s("p"),Ps=h(`This pre-tokenizer replaces any whitespace by the provided replacement character.
It then tries to split on these spaces.`),ur=i(),ee=s("h2"),ve=s("a"),Ct=s("span"),k(Ne.$$.fragment),Es=i(),qt=s("span"),Ts=h("PreTokenizer"),mr=i(),S=s("div"),k(We.$$.fragment),Ss=i(),It=s("p"),xs=h("Base class for all pre-tokenizers"),Ds=i(),Bt=s("p"),As=h(`This class is not supposed to be instantiated directly. Instead, any implementation of a
PreTokenizer will return an instance of this class when instantiated.`),Cs=i(),F=s("div"),k(Le.$$.fragment),qs=i(),He=s("p"),Is=h("Pre-tokenize a "),Nt=s("code"),Bs=h("PyPreTokenizedString"),Ns=h(" in-place"),Ws=i(),R=s("p"),Ls=h("This method allows to modify a "),Wt=s("code"),Hs=h("PreTokenizedString"),Ms=h(` to
keep track of the pre-tokenization, and leverage the capabilities of the
`),Lt=s("code"),Vs=h("PreTokenizedString"),Us=h(`. If you just want to see the result of
the pre-tokenization of a raw string, you can use
`),Ht=s("code"),js=h("pre_tokenize_str()"),Os=i(),K=s("div"),k(Me.$$.fragment),Fs=i(),Mt=s("p"),Rs=h("Pre tokenize the given string"),Ks=i(),J=s("p"),Js=h(`This method provides a way to visualize the effect of a
`),ot=s("a"),Gs=h("PreTokenizer"),Qs=h(` but it does not keep track of the
alignment, nor does it provide all the capabilities of the
`),Vt=s("code"),Xs=h("PreTokenizedString"),Ys=h(`. If you need some of these, you can use
`),Ut=s("code"),Zs=h("pre_tokenize()"),vr=i(),te=s("h2"),ke=s("a"),jt=s("span"),k(Ve.$$.fragment),en=i(),Ot=s("span"),tn=h("Punctuation"),kr=i(),re=s("div"),k(Ue.$$.fragment),rn=i(),Ft=s("p"),sn=h("This pre-tokenizer simply splits on punctuation as individual characters."),$r=i(),se=s("h2"),$e=s("a"),Rt=s("span"),k(je.$$.fragment),nn=i(),Kt=s("span"),on=h("Sequence"),gr=i(),ne=s("div"),k(Oe.$$.fragment),an=i(),Jt=s("p"),ln=h("This pre-tokenizer composes other pre_tokenizers and applies them in sequence"),_r=i(),oe=s("h2"),ge=s("a"),Gt=s("span"),k(Fe.$$.fragment),pn=i(),Qt=s("span"),cn=h("Split"),zr=i(),U=s("div"),k(Re.$$.fragment),hn=i(),Xt=s("p"),dn=h("Split PreTokenizer"),fn=i(),Yt=s("p"),un=h(`This versatile pre-tokenizer splits using the provided pattern and
according to the provided behavior. The pattern can be inverted by
making use of the invert flag.`),wr=i(),ae=s("h2"),_e=s("a"),Zt=s("span"),k(Ke.$$.fragment),mn=i(),er=s("span"),vn=h("UnicodeScripts"),br=i(),ie=s("div"),k(Je.$$.fragment),kn=i(),Ge=s("p"),$n=h(`This pre-tokenizer splits on characters that belong to different language family
It roughly follows `),Qe=s("a"),gn=h("https://github.com/google/sentencepiece/blob/master/data/Scripts.txt"),_n=h(`
Actually Hiragana and Katakana are fused with Han, and 0x30FC is Han too.
This mimicks SentencePiece Unigram implementation.`),yr=i(),le=s("h2"),ze=s("a"),tr=s("span"),k(Xe.$$.fragment),zn=i(),rr=s("span"),wn=h("Whitespace"),Pr=i(),pe=s("div"),k(Ye.$$.fragment),bn=i(),at=s("p"),yn=h("This pre-tokenizer simply splits using the following regex: "),sr=s("code"),Pn=h("\\w+|[^\\w\\s]+"),Er=i(),ce=s("h2"),we=s("a"),nr=s("span"),k(Ze.$$.fragment),En=i(),or=s("span"),Tn=h("WhitespaceSplit"),Tr=i(),he=s("div"),k(et.$$.fragment),Sn=i(),it=s("p"),xn=h("This pre-tokenizer simply splits on the whitespace. Works like "),ar=s("code"),Dn=h(".split()"),this.h()},l(e){c=n(e,"H2",{class:!0});var f=o(c);v=n(f,"A",{id:!0,class:!0,href:!0});var Bn=o(v);p=n(Bn,"SPAN",{});var Nn=o(p);$(m.$$.fragment,Nn),Nn.forEach(r),Bn.forEach(r),y=l(f),P=n(f,"SPAN",{});var Wn=o(P);D=d(Wn,"BertPreTokenizer"),Wn.forEach(r),f.forEach(r),C=l(e),T=n(e,"DIV",{class:!0});var lt=o(T);$(W.$$.fragment,lt),L=l(lt),j=n(lt,"P",{});var Ln=o(j);b=d(Ln,"BertPreTokenizer"),Ln.forEach(r),A=l(lt),q=n(lt,"P",{});var Hn=o(q);st=d(Hn,`This pre-tokenizer splits tokens on spaces, and also on punctuation.
Each occurence of a punctuation character will be treated separately.`),Hn.forEach(r),lt.forEach(r),ye=l(e),H=n(e,"H2",{class:!0});var xr=o(H);de=n(xr,"A",{id:!0,class:!0,href:!0});var Mn=o(de);ut=n(Mn,"SPAN",{});var Vn=o(ut);$(Pe.$$.fragment,Vn),Vn.forEach(r),Mn.forEach(r),Jr=l(xr),mt=n(xr,"SPAN",{});var Un=o(mt);Gr=d(Un,"ByteLevel"),Un.forEach(r),xr.forEach(r),ir=l(e),I=n(e,"DIV",{class:!0});var be=o(I);$(Ee.$$.fragment,be),Qr=l(be),vt=n(be,"P",{});var jn=o(vt);Xr=d(jn,"ByteLevel PreTokenizer"),jn.forEach(r),Yr=l(be),kt=n(be,"P",{});var On=o(kt);Zr=d(On,`This pre-tokenizer takes care of replacing all bytes of the given string
with a corresponding representation, as well as splitting into words.`),On.forEach(r),es=l(be),O=n(be,"DIV",{class:!0});var pt=o(O);$(Te.$$.fragment,pt),ts=l(pt),$t=n(pt,"P",{});var Fn=o($t);rs=d(Fn,"Returns the alphabet used by this PreTokenizer."),Fn.forEach(r),ss=l(pt),gt=n(pt,"P",{});var Rn=o(gt);ns=d(Rn,`Since the ByteLevel works as its name suggests, at the byte level, it
encodes each byte value to a unique visible character. This means that there is a
total of 256 different characters composing this alphabet.`),Rn.forEach(r),pt.forEach(r),be.forEach(r),lr=l(e),Q=n(e,"H2",{class:!0});var Dr=o(Q);fe=n(Dr,"A",{id:!0,class:!0,href:!0});var Kn=o(fe);_t=n(Kn,"SPAN",{});var Jn=o(_t);$(Se.$$.fragment,Jn),Jn.forEach(r),Kn.forEach(r),os=l(Dr),zt=n(Dr,"SPAN",{});var Gn=o(zt);as=d(Gn,"CharDelimiterSplit"),Gn.forEach(r),Dr.forEach(r),pr=l(e),X=n(e,"DIV",{class:!0});var Ar=o(X);$(xe.$$.fragment,Ar),is=l(Ar),nt=n(Ar,"P",{});var An=o(nt);ls=d(An,"This pre-tokenizer simply splits on the provided char. Works like "),wt=n(An,"CODE",{});var Qn=o(wt);ps=d(Qn,".split(delimiter)"),Qn.forEach(r),An.forEach(r),Ar.forEach(r),cr=l(e),Y=n(e,"H2",{class:!0});var Cr=o(Y);ue=n(Cr,"A",{id:!0,class:!0,href:!0});var Xn=o(ue);bt=n(Xn,"SPAN",{});var Yn=o(bt);$(De.$$.fragment,Yn),Yn.forEach(r),Xn.forEach(r),cs=l(Cr),yt=n(Cr,"SPAN",{});var Zn=o(yt);hs=d(Zn,"Digits"),Zn.forEach(r),Cr.forEach(r),hr=l(e),E=n(e,"DIV",{class:!0});var M=o(E);$(Ae.$$.fragment,M),ds=l(M),Pt=n(M,"P",{});var eo=o(Pt);fs=d(eo,"This pre-tokenizer simply splits using the digits in separate tokens"),eo.forEach(r),us=l(M),Et=n(M,"P",{});var to=o(Et);ms=d(to,"If set to True, digits will each be separated as follows:"),to.forEach(r),vs=l(M),$(Ce.$$.fragment,M),ks=l(M),Tt=n(M,"P",{});var ro=o(Tt);$s=d(ro,"If set to False, digits will grouped as follows:"),ro.forEach(r),gs=l(M),$(qe.$$.fragment,M),M.forEach(r),dr=l(e),Z=n(e,"H2",{class:!0});var qr=o(Z);me=n(qr,"A",{id:!0,class:!0,href:!0});var so=o(me);St=n(so,"SPAN",{});var no=o(St);$(Ie.$$.fragment,no),no.forEach(r),so.forEach(r),_s=l(qr),xt=n(qr,"SPAN",{});var oo=o(xt);zs=d(oo,"Metaspace"),oo.forEach(r),qr.forEach(r),fr=l(e),V=n(e,"DIV",{class:!0});var ct=o(V);$(Be.$$.fragment,ct),ws=l(ct),Dt=n(ct,"P",{});var ao=o(Dt);bs=d(ao,"Metaspace pre-tokenizer"),ao.forEach(r),ys=l(ct),At=n(ct,"P",{});var io=o(At);Ps=d(io,`This pre-tokenizer replaces any whitespace by the provided replacement character.
It then tries to split on these spaces.`),io.forEach(r),ct.forEach(r),ur=l(e),ee=n(e,"H2",{class:!0});var Ir=o(ee);ve=n(Ir,"A",{id:!0,class:!0,href:!0});var lo=o(ve);Ct=n(lo,"SPAN",{});var po=o(Ct);$(Ne.$$.fragment,po),po.forEach(r),lo.forEach(r),Es=l(Ir),qt=n(Ir,"SPAN",{});var co=o(qt);Ts=d(co,"PreTokenizer"),co.forEach(r),Ir.forEach(r),mr=l(e),S=n(e,"DIV",{class:!0});var G=o(S);$(We.$$.fragment,G),Ss=l(G),It=n(G,"P",{});var ho=o(It);xs=d(ho,"Base class for all pre-tokenizers"),ho.forEach(r),Ds=l(G),Bt=n(G,"P",{});var fo=o(Bt);As=d(fo,`This class is not supposed to be instantiated directly. Instead, any implementation of a
PreTokenizer will return an instance of this class when instantiated.`),fo.forEach(r),Cs=l(G),F=n(G,"DIV",{class:!0});var ht=o(F);$(Le.$$.fragment,ht),qs=l(ht),He=n(ht,"P",{});var Br=o(He);Is=d(Br,"Pre-tokenize a "),Nt=n(Br,"CODE",{});var uo=o(Nt);Bs=d(uo,"PyPreTokenizedString"),uo.forEach(r),Ns=d(Br," in-place"),Br.forEach(r),Ws=l(ht),R=n(ht,"P",{});var tt=o(R);Ls=d(tt,"This method allows to modify a "),Wt=n(tt,"CODE",{});var mo=o(Wt);Hs=d(mo,"PreTokenizedString"),mo.forEach(r),Ms=d(tt,` to
keep track of the pre-tokenization, and leverage the capabilities of the
`),Lt=n(tt,"CODE",{});var vo=o(Lt);Vs=d(vo,"PreTokenizedString"),vo.forEach(r),Us=d(tt,`. If you just want to see the result of
the pre-tokenization of a raw string, you can use
`),Ht=n(tt,"CODE",{});var ko=o(Ht);js=d(ko,"pre_tokenize_str()"),ko.forEach(r),tt.forEach(r),ht.forEach(r),Os=l(G),K=n(G,"DIV",{class:!0});var dt=o(K);$(Me.$$.fragment,dt),Fs=l(dt),Mt=n(dt,"P",{});var $o=o(Mt);Rs=d($o,"Pre tokenize the given string"),$o.forEach(r),Ks=l(dt),J=n(dt,"P",{});var rt=o(J);Js=d(rt,`This method provides a way to visualize the effect of a
`),ot=n(rt,"A",{href:!0});var go=o(ot);Gs=d(go,"PreTokenizer"),go.forEach(r),Qs=d(rt,` but it does not keep track of the
alignment, nor does it provide all the capabilities of the
`),Vt=n(rt,"CODE",{});var _o=o(Vt);Xs=d(_o,"PreTokenizedString"),_o.forEach(r),Ys=d(rt,`. If you need some of these, you can use
`),Ut=n(rt,"CODE",{});var zo=o(Ut);Zs=d(zo,"pre_tokenize()"),zo.forEach(r),rt.forEach(r),dt.forEach(r),G.forEach(r),vr=l(e),te=n(e,"H2",{class:!0});var Nr=o(te);ke=n(Nr,"A",{id:!0,class:!0,href:!0});var wo=o(ke);jt=n(wo,"SPAN",{});var bo=o(jt);$(Ve.$$.fragment,bo),bo.forEach(r),wo.forEach(r),en=l(Nr),Ot=n(Nr,"SPAN",{});var yo=o(Ot);tn=d(yo,"Punctuation"),yo.forEach(r),Nr.forEach(r),kr=l(e),re=n(e,"DIV",{class:!0});var Wr=o(re);$(Ue.$$.fragment,Wr),rn=l(Wr),Ft=n(Wr,"P",{});var Po=o(Ft);sn=d(Po,"This pre-tokenizer simply splits on punctuation as individual characters."),Po.forEach(r),Wr.forEach(r),$r=l(e),se=n(e,"H2",{class:!0});var Lr=o(se);$e=n(Lr,"A",{id:!0,class:!0,href:!0});var Eo=o($e);Rt=n(Eo,"SPAN",{});var To=o(Rt);$(je.$$.fragment,To),To.forEach(r),Eo.forEach(r),nn=l(Lr),Kt=n(Lr,"SPAN",{});var So=o(Kt);on=d(So,"Sequence"),So.forEach(r),Lr.forEach(r),gr=l(e),ne=n(e,"DIV",{class:!0});var Hr=o(ne);$(Oe.$$.fragment,Hr),an=l(Hr),Jt=n(Hr,"P",{});var xo=o(Jt);ln=d(xo,"This pre-tokenizer composes other pre_tokenizers and applies them in sequence"),xo.forEach(r),Hr.forEach(r),_r=l(e),oe=n(e,"H2",{class:!0});var Mr=o(oe);ge=n(Mr,"A",{id:!0,class:!0,href:!0});var Do=o(ge);Gt=n(Do,"SPAN",{});var Ao=o(Gt);$(Fe.$$.fragment,Ao),Ao.forEach(r),Do.forEach(r),pn=l(Mr),Qt=n(Mr,"SPAN",{});var Co=o(Qt);cn=d(Co,"Split"),Co.forEach(r),Mr.forEach(r),zr=l(e),U=n(e,"DIV",{class:!0});var ft=o(U);$(Re.$$.fragment,ft),hn=l(ft),Xt=n(ft,"P",{});var qo=o(Xt);dn=d(qo,"Split PreTokenizer"),qo.forEach(r),fn=l(ft),Yt=n(ft,"P",{});var Io=o(Yt);un=d(Io,`This versatile pre-tokenizer splits using the provided pattern and
according to the provided behavior. The pattern can be inverted by
making use of the invert flag.`),Io.forEach(r),ft.forEach(r),wr=l(e),ae=n(e,"H2",{class:!0});var Vr=o(ae);_e=n(Vr,"A",{id:!0,class:!0,href:!0});var Bo=o(_e);Zt=n(Bo,"SPAN",{});var No=o(Zt);$(Ke.$$.fragment,No),No.forEach(r),Bo.forEach(r),mn=l(Vr),er=n(Vr,"SPAN",{});var Wo=o(er);vn=d(Wo,"UnicodeScripts"),Wo.forEach(r),Vr.forEach(r),br=l(e),ie=n(e,"DIV",{class:!0});var Ur=o(ie);$(Je.$$.fragment,Ur),kn=l(Ur),Ge=n(Ur,"P",{});var jr=o(Ge);$n=d(jr,`This pre-tokenizer splits on characters that belong to different language family
It roughly follows `),Qe=n(jr,"A",{href:!0,rel:!0});var Lo=o(Qe);gn=d(Lo,"https://github.com/google/sentencepiece/blob/master/data/Scripts.txt"),Lo.forEach(r),_n=d(jr,`
Actually Hiragana and Katakana are fused with Han, and 0x30FC is Han too.
This mimicks SentencePiece Unigram implementation.`),jr.forEach(r),Ur.forEach(r),yr=l(e),le=n(e,"H2",{class:!0});var Or=o(le);ze=n(Or,"A",{id:!0,class:!0,href:!0});var Ho=o(ze);tr=n(Ho,"SPAN",{});var Mo=o(tr);$(Xe.$$.fragment,Mo),Mo.forEach(r),Ho.forEach(r),zn=l(Or),rr=n(Or,"SPAN",{});var Vo=o(rr);wn=d(Vo,"Whitespace"),Vo.forEach(r),Or.forEach(r),Pr=l(e),pe=n(e,"DIV",{class:!0});var Fr=o(pe);$(Ye.$$.fragment,Fr),bn=l(Fr),at=n(Fr,"P",{});var Cn=o(at);yn=d(Cn,"This pre-tokenizer simply splits using the following regex: "),sr=n(Cn,"CODE",{});var Uo=o(sr);Pn=d(Uo,"\\w+|[^\\w\\s]+"),Uo.forEach(r),Cn.forEach(r),Fr.forEach(r),Er=l(e),ce=n(e,"H2",{class:!0});var Rr=o(ce);we=n(Rr,"A",{id:!0,class:!0,href:!0});var jo=o(we);nr=n(jo,"SPAN",{});var Oo=o(nr);$(Ze.$$.fragment,Oo),Oo.forEach(r),jo.forEach(r),En=l(Rr),or=n(Rr,"SPAN",{});var Fo=o(or);Tn=d(Fo,"WhitespaceSplit"),Fo.forEach(r),Rr.forEach(r),Tr=l(e),he=n(e,"DIV",{class:!0});var Kr=o(he);$(et.$$.fragment,Kr),Sn=l(Kr),it=n(Kr,"P",{});var qn=o(it);xn=d(qn,"This pre-tokenizer simply splits on the whitespace. Works like "),ar=n(qn,"CODE",{});var Ro=o(ar);Dn=d(Ro,".split()"),Ro.forEach(r),qn.forEach(r),Kr.forEach(r),this.h()},h(){a(v,"id","tokenizers.pre_tokenizers.BertPreTokenizer"),a(v,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),a(v,"href","#tokenizers.pre_tokenizers.BertPreTokenizer"),a(c,"class","relative group"),a(T,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),a(de,"id","tokenizers.pre_tokenizers.ByteLevel"),a(de,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),a(de,"href","#tokenizers.pre_tokenizers.ByteLevel"),a(H,"class","relative group"),a(O,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),a(I,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),a(fe,"id","tokenizers.pre_tokenizers.CharDelimiterSplit"),a(fe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),a(fe,"href","#tokenizers.pre_tokenizers.CharDelimiterSplit"),a(Q,"class","relative group"),a(X,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),a(ue,"id","tokenizers.pre_tokenizers.Digits"),a(ue,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),a(ue,"href","#tokenizers.pre_tokenizers.Digits"),a(Y,"class","relative group"),a(E,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),a(me,"id","tokenizers.pre_tokenizers.Metaspace"),a(me,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),a(me,"href","#tokenizers.pre_tokenizers.Metaspace"),a(Z,"class","relative group"),a(V,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),a(ve,"id","tokenizers.pre_tokenizers.PreTokenizer"),a(ve,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),a(ve,"href","#tokenizers.pre_tokenizers.PreTokenizer"),a(ee,"class","relative group"),a(F,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),a(ot,"href","/docs/tokenizers/pr_1/en/api/pre-tokenizers#tokenizers.pre_tokenizers.PreTokenizer"),a(K,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),a(S,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),a(ke,"id","tokenizers.pre_tokenizers.Punctuation"),a(ke,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),a(ke,"href","#tokenizers.pre_tokenizers.Punctuation"),a(te,"class","relative group"),a(re,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),a($e,"id","tokenizers.pre_tokenizers.Sequence"),a($e,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),a($e,"href","#tokenizers.pre_tokenizers.Sequence"),a(se,"class","relative group"),a(ne,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),a(ge,"id","tokenizers.pre_tokenizers.Split"),a(ge,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),a(ge,"href","#tokenizers.pre_tokenizers.Split"),a(oe,"class","relative group"),a(U,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),a(_e,"id","tokenizers.pre_tokenizers.UnicodeScripts"),a(_e,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),a(_e,"href","#tokenizers.pre_tokenizers.UnicodeScripts"),a(ae,"class","relative group"),a(Qe,"href","https://github.com/google/sentencepiece/blob/master/data/Scripts.txt"),a(Qe,"rel","nofollow"),a(ie,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),a(ze,"id","tokenizers.pre_tokenizers.Whitespace"),a(ze,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),a(ze,"href","#tokenizers.pre_tokenizers.Whitespace"),a(le,"class","relative group"),a(pe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),a(we,"id","tokenizers.pre_tokenizers.WhitespaceSplit"),a(we,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),a(we,"href","#tokenizers.pre_tokenizers.WhitespaceSplit"),a(ce,"class","relative group"),a(he,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(e,f){u(e,c,f),t(c,v),t(v,p),g(m,p,null),t(c,y),t(c,P),t(P,D),u(e,C,f),u(e,T,f),g(W,T,null),t(T,L),t(T,j),t(j,b),t(T,A),t(T,q),t(q,st),u(e,ye,f),u(e,H,f),t(H,de),t(de,ut),g(Pe,ut,null),t(H,Jr),t(H,mt),t(mt,Gr),u(e,ir,f),u(e,I,f),g(Ee,I,null),t(I,Qr),t(I,vt),t(vt,Xr),t(I,Yr),t(I,kt),t(kt,Zr),t(I,es),t(I,O),g(Te,O,null),t(O,ts),t(O,$t),t($t,rs),t(O,ss),t(O,gt),t(gt,ns),u(e,lr,f),u(e,Q,f),t(Q,fe),t(fe,_t),g(Se,_t,null),t(Q,os),t(Q,zt),t(zt,as),u(e,pr,f),u(e,X,f),g(xe,X,null),t(X,is),t(X,nt),t(nt,ls),t(nt,wt),t(wt,ps),u(e,cr,f),u(e,Y,f),t(Y,ue),t(ue,bt),g(De,bt,null),t(Y,cs),t(Y,yt),t(yt,hs),u(e,hr,f),u(e,E,f),g(Ae,E,null),t(E,ds),t(E,Pt),t(Pt,fs),t(E,us),t(E,Et),t(Et,ms),t(E,vs),g(Ce,E,null),t(E,ks),t(E,Tt),t(Tt,$s),t(E,gs),g(qe,E,null),u(e,dr,f),u(e,Z,f),t(Z,me),t(me,St),g(Ie,St,null),t(Z,_s),t(Z,xt),t(xt,zs),u(e,fr,f),u(e,V,f),g(Be,V,null),t(V,ws),t(V,Dt),t(Dt,bs),t(V,ys),t(V,At),t(At,Ps),u(e,ur,f),u(e,ee,f),t(ee,ve),t(ve,Ct),g(Ne,Ct,null),t(ee,Es),t(ee,qt),t(qt,Ts),u(e,mr,f),u(e,S,f),g(We,S,null),t(S,Ss),t(S,It),t(It,xs),t(S,Ds),t(S,Bt),t(Bt,As),t(S,Cs),t(S,F),g(Le,F,null),t(F,qs),t(F,He),t(He,Is),t(He,Nt),t(Nt,Bs),t(He,Ns),t(F,Ws),t(F,R),t(R,Ls),t(R,Wt),t(Wt,Hs),t(R,Ms),t(R,Lt),t(Lt,Vs),t(R,Us),t(R,Ht),t(Ht,js),t(S,Os),t(S,K),g(Me,K,null),t(K,Fs),t(K,Mt),t(Mt,Rs),t(K,Ks),t(K,J),t(J,Js),t(J,ot),t(ot,Gs),t(J,Qs),t(J,Vt),t(Vt,Xs),t(J,Ys),t(J,Ut),t(Ut,Zs),u(e,vr,f),u(e,te,f),t(te,ke),t(ke,jt),g(Ve,jt,null),t(te,en),t(te,Ot),t(Ot,tn),u(e,kr,f),u(e,re,f),g(Ue,re,null),t(re,rn),t(re,Ft),t(Ft,sn),u(e,$r,f),u(e,se,f),t(se,$e),t($e,Rt),g(je,Rt,null),t(se,nn),t(se,Kt),t(Kt,on),u(e,gr,f),u(e,ne,f),g(Oe,ne,null),t(ne,an),t(ne,Jt),t(Jt,ln),u(e,_r,f),u(e,oe,f),t(oe,ge),t(ge,Gt),g(Fe,Gt,null),t(oe,pn),t(oe,Qt),t(Qt,cn),u(e,zr,f),u(e,U,f),g(Re,U,null),t(U,hn),t(U,Xt),t(Xt,dn),t(U,fn),t(U,Yt),t(Yt,un),u(e,wr,f),u(e,ae,f),t(ae,_e),t(_e,Zt),g(Ke,Zt,null),t(ae,mn),t(ae,er),t(er,vn),u(e,br,f),u(e,ie,f),g(Je,ie,null),t(ie,kn),t(ie,Ge),t(Ge,$n),t(Ge,Qe),t(Qe,gn),t(Ge,_n),u(e,yr,f),u(e,le,f),t(le,ze),t(ze,tr),g(Xe,tr,null),t(le,zn),t(le,rr),t(rr,wn),u(e,Pr,f),u(e,pe,f),g(Ye,pe,null),t(pe,bn),t(pe,at),t(at,yn),t(at,sr),t(sr,Pn),u(e,Er,f),u(e,ce,f),t(ce,we),t(we,nr),g(Ze,nr,null),t(ce,En),t(ce,or),t(or,Tn),u(e,Tr,f),u(e,he,f),g(et,he,null),t(he,Sn),t(he,it),t(it,xn),t(it,ar),t(ar,Dn),Sr=!0},p:Zo,i(e){Sr||(_(m.$$.fragment,e),_(W.$$.fragment,e),_(Pe.$$.fragment,e),_(Ee.$$.fragment,e),_(Te.$$.fragment,e),_(Se.$$.fragment,e),_(xe.$$.fragment,e),_(De.$$.fragment,e),_(Ae.$$.fragment,e),_(Ce.$$.fragment,e),_(qe.$$.fragment,e),_(Ie.$$.fragment,e),_(Be.$$.fragment,e),_(Ne.$$.fragment,e),_(We.$$.fragment,e),_(Le.$$.fragment,e),_(Me.$$.fragment,e),_(Ve.$$.fragment,e),_(Ue.$$.fragment,e),_(je.$$.fragment,e),_(Oe.$$.fragment,e),_(Fe.$$.fragment,e),_(Re.$$.fragment,e),_(Ke.$$.fragment,e),_(Je.$$.fragment,e),_(Xe.$$.fragment,e),_(Ye.$$.fragment,e),_(Ze.$$.fragment,e),_(et.$$.fragment,e),Sr=!0)},o(e){z(m.$$.fragment,e),z(W.$$.fragment,e),z(Pe.$$.fragment,e),z(Ee.$$.fragment,e),z(Te.$$.fragment,e),z(Se.$$.fragment,e),z(xe.$$.fragment,e),z(De.$$.fragment,e),z(Ae.$$.fragment,e),z(Ce.$$.fragment,e),z(qe.$$.fragment,e),z(Ie.$$.fragment,e),z(Be.$$.fragment,e),z(Ne.$$.fragment,e),z(We.$$.fragment,e),z(Le.$$.fragment,e),z(Me.$$.fragment,e),z(Ve.$$.fragment,e),z(Ue.$$.fragment,e),z(je.$$.fragment,e),z(Oe.$$.fragment,e),z(Fe.$$.fragment,e),z(Re.$$.fragment,e),z(Ke.$$.fragment,e),z(Je.$$.fragment,e),z(Xe.$$.fragment,e),z(Ye.$$.fragment,e),z(Ze.$$.fragment,e),z(et.$$.fragment,e),Sr=!1},d(e){e&&r(c),w(m),e&&r(C),e&&r(T),w(W),e&&r(ye),e&&r(H),w(Pe),e&&r(ir),e&&r(I),w(Ee),w(Te),e&&r(lr),e&&r(Q),w(Se),e&&r(pr),e&&r(X),w(xe),e&&r(cr),e&&r(Y),w(De),e&&r(hr),e&&r(E),w(Ae),w(Ce),w(qe),e&&r(dr),e&&r(Z),w(Ie),e&&r(fr),e&&r(V),w(Be),e&&r(ur),e&&r(ee),w(Ne),e&&r(mr),e&&r(S),w(We),w(Le),w(Me),e&&r(vr),e&&r(te),w(Ve),e&&r(kr),e&&r(re),w(Ue),e&&r($r),e&&r(se),w(je),e&&r(gr),e&&r(ne),w(Oe),e&&r(_r),e&&r(oe),w(Fe),e&&r(zr),e&&r(U),w(Re),e&&r(wr),e&&r(ae),w(Ke),e&&r(br),e&&r(ie),w(Je),e&&r(yr),e&&r(le),w(Xe),e&&r(Pr),e&&r(pe),w(Ye),e&&r(Er),e&&r(ce),w(Ze),e&&r(Tr),e&&r(he),w(et)}}}function ra(N){let c,v;return c=new In({props:{$$slots:{default:[ta]},$$scope:{ctx:N}}}),{c(){k(c.$$.fragment)},l(p){$(c.$$.fragment,p)},m(p,m){g(c,p,m),v=!0},p(p,m){const y={};m&2&&(y.$$scope={dirty:m,ctx:p}),c.$set(y)},i(p){v||(_(c.$$.fragment,p),v=!0)},o(p){z(c.$$.fragment,p),v=!1},d(p){w(c,p)}}}function sa(N){let c,v,p,m,y;return{c(){c=s("p"),v=h("The Rust API Reference is available directly on the "),p=s("a"),m=h("Docs.rs"),y=h(" website."),this.h()},l(P){c=n(P,"P",{});var D=o(c);v=d(D,"The Rust API Reference is available directly on the "),p=n(D,"A",{href:!0,rel:!0});var C=o(p);m=d(C,"Docs.rs"),C.forEach(r),y=d(D," website."),D.forEach(r),this.h()},h(){a(p,"href","https://docs.rs/tokenizers/latest/tokenizers/"),a(p,"rel","nofollow")},m(P,D){u(P,c,D),t(c,v),t(c,p),t(p,m),t(c,y)},d(P){P&&r(c)}}}function na(N){let c,v;return c=new In({props:{$$slots:{default:[sa]},$$scope:{ctx:N}}}),{c(){k(c.$$.fragment)},l(p){$(c.$$.fragment,p)},m(p,m){g(c,p,m),v=!0},p(p,m){const y={};m&2&&(y.$$scope={dirty:m,ctx:p}),c.$set(y)},i(p){v||(_(c.$$.fragment,p),v=!0)},o(p){z(c.$$.fragment,p),v=!1},d(p){w(c,p)}}}function oa(N){let c,v;return{c(){c=s("p"),v=h("The node API has not been documented yet.")},l(p){c=n(p,"P",{});var m=o(c);v=d(m,"The node API has not been documented yet."),m.forEach(r)},m(p,m){u(p,c,m),t(c,v)},d(p){p&&r(c)}}}function aa(N){let c,v;return c=new In({props:{$$slots:{default:[oa]},$$scope:{ctx:N}}}),{c(){k(c.$$.fragment)},l(p){$(c.$$.fragment,p)},m(p,m){g(c,p,m),v=!0},p(p,m){const y={};m&2&&(y.$$scope={dirty:m,ctx:p}),c.$set(y)},i(p){v||(_(c.$$.fragment,p),v=!0)},o(p){z(c.$$.fragment,p),v=!1},d(p){w(c,p)}}}function ia(N){let c,v,p,m,y,P,D,C,T,W,L,j;return P=new B({}),L=new ea({props:{python:!0,rust:!0,node:!0,$$slots:{node:[aa],rust:[na],python:[ra]},$$scope:{ctx:N}}}),{c(){c=s("meta"),v=i(),p=s("h1"),m=s("a"),y=s("span"),k(P.$$.fragment),D=i(),C=s("span"),T=h("Pre-tokenizers"),W=i(),k(L.$$.fragment),this.h()},l(b){const A=Xo('[data-svelte="svelte-1phssyn"]',document.head);c=n(A,"META",{name:!0,content:!0}),A.forEach(r),v=l(b),p=n(b,"H1",{class:!0});var q=o(p);m=n(q,"A",{id:!0,class:!0,href:!0});var st=o(m);y=n(st,"SPAN",{});var ye=o(y);$(P.$$.fragment,ye),ye.forEach(r),st.forEach(r),D=l(q),C=n(q,"SPAN",{});var H=o(C);T=d(H,"Pre-tokenizers"),H.forEach(r),q.forEach(r),W=l(b),$(L.$$.fragment,b),this.h()},h(){a(c,"name","hf:doc:metadata"),a(c,"content",JSON.stringify(la)),a(m,"id","pretokenizers"),a(m,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),a(m,"href","#pretokenizers"),a(p,"class","relative group")},m(b,A){t(document.head,c),u(b,v,A),u(b,p,A),t(p,m),t(m,y),g(P,y,null),t(p,D),t(p,C),t(C,T),u(b,W,A),g(L,b,A),j=!0},p(b,[A]){const q={};A&2&&(q.$$scope={dirty:A,ctx:b}),L.$set(q)},i(b){j||(_(P.$$.fragment,b),_(L.$$.fragment,b),j=!0)},o(b){z(P.$$.fragment,b),z(L.$$.fragment,b),j=!1},d(b){r(c),b&&r(v),b&&r(p),w(P),b&&r(W),w(L,b)}}}const la={local:"pretokenizers",sections:[{local:"tokenizers.pre_tokenizers.BertPreTokenizer",title:"BertPreTokenizer"},{local:"tokenizers.pre_tokenizers.ByteLevel",title:"ByteLevel"},{local:"tokenizers.pre_tokenizers.CharDelimiterSplit",title:"CharDelimiterSplit"},{local:"tokenizers.pre_tokenizers.Digits",title:"Digits"},{local:"tokenizers.pre_tokenizers.Metaspace",title:"Metaspace"},{local:"tokenizers.pre_tokenizers.PreTokenizer",title:"PreTokenizer"},{local:"tokenizers.pre_tokenizers.Punctuation",title:"Punctuation"},{local:"tokenizers.pre_tokenizers.Sequence",title:"Sequence"},{local:"tokenizers.pre_tokenizers.Split",title:"Split"},{local:"tokenizers.pre_tokenizers.UnicodeScripts",title:"UnicodeScripts"},{local:"tokenizers.pre_tokenizers.Whitespace",title:"Whitespace"},{local:"tokenizers.pre_tokenizers.WhitespaceSplit",title:"WhitespaceSplit"}],title:"Pre-tokenizers"};function pa(N){return Yo(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class ma extends Jo{constructor(c){super();Go(this,c,pa,ia,Qo,{})}}export{ma as default,la as metadata};
