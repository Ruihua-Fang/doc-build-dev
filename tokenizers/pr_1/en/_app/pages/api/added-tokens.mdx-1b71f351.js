import{S as ne,i as re,s as ae,e as c,k as x,w as I,t as b,M as se,c as h,d as a,m as E,a as p,x as D,h as z,b as m,F as r,g as P,y as F,q as S,o as q,B as M,v as de,L as ie}from"../../chunks/vendor-0d3f0756.js";import{D as te}from"../../chunks/Docstring-2d4c5ec2.js";import{I as oe}from"../../chunks/IconCopyLink-9193371d.js";import{T as le,M as Y}from"../../chunks/TokenizersLanguageContent-ca787841.js";function ce(g){let t,n,e,o,d,i,$,k,u,w,_,v,s,f,T,N,C,A,R,j,L,B,K,J,H;return o=new oe({}),w=new te({props:{name:"class tokenizers.AddedToken",anchor:"tokenizers.AddedToken",parameters:[{name:"content",val:""},{name:"single_word",val:" = False"},{name:"lstrip",val:" = False"},{name:"rstrip",val:" = False"},{name:"normalized",val:" = True"}],parametersDescription:[{anchor:"tokenizers.AddedToken.content",description:"<strong>content</strong> (<code>str</code>) &#x2014; The content of the token",name:"content"},{anchor:"tokenizers.AddedToken.single_word",description:`<strong>single_word</strong> (<code>bool</code>, defaults to <code>False</code>) &#x2014;
Defines whether this token should only match single words. If <code>True</code>, this
token will never match inside of a word. For example the token <code>ing</code> would match
on <code>tokenizing</code> if this option is <code>False</code>, but not if it is <code>True</code>.
The notion of &#x201D;<em>inside of a word</em>&#x201D; is defined by the word boundaries pattern in
regular expressions (ie. the token should start and end with word boundaries).`,name:"single_word"},{anchor:"tokenizers.AddedToken.lstrip",description:`<strong>lstrip</strong> (<code>bool</code>, defaults to <code>False</code>) &#x2014;
Defines whether this token should strip all potential whitespaces on its left side.
If <code>True</code>, this token will greedily match any whitespace on its left. For
example if we try to match the token <code>[MASK]</code> with <code>lstrip=True</code>, in the text
<code>&quot;I saw a [MASK]&quot;</code>, we would match on <code>&quot; [MASK]&quot;</code>. (Note the space on the left).`,name:"lstrip"},{anchor:"tokenizers.AddedToken.rstrip",description:`<strong>rstrip</strong> (<code>bool</code>, defaults to <code>False</code>) &#x2014;
Defines whether this token should strip all potential whitespaces on its right
side. If <code>True</code>, this token will greedily match any whitespace on its right.
It works just like <code>lstrip</code> but on the right.`,name:"rstrip"},{anchor:"tokenizers.AddedToken.normalized",description:`<strong>normalized</strong> (<code>bool</code>, defaults to <code>True</code> with  &#x2014;meth:<em>~tokenizers.Tokenizer.add_tokens</em> and <code>False</code> with <code>add_special_tokens()</code>):
Defines whether this token should match against the normalized version of the input
text. For example, with the added token <code>&quot;yesterday&quot;</code>, and a normalizer in charge of
lowercasing the text, the token could be extract from the input <code>&quot;I saw a lion Yesterday&quot;</code>.`,name:"normalized"}]}}),R=new te({props:{name:"content",anchor:"tokenizers.AddedToken.content",parameters:[],isGetSetDescriptor:!0}}),{c(){t=c("h2"),n=c("a"),e=c("span"),I(o.$$.fragment),d=x(),i=c("span"),$=b("AddedToken"),k=x(),u=c("div"),I(w.$$.fragment),_=x(),v=c("p"),s=b("Represents a token that can be be added to a "),f=c("a"),T=b("Tokenizer"),N=b(`.
It can have special options that defines the way it should behave.`),C=x(),A=c("div"),I(R.$$.fragment),j=x(),L=c("p"),B=b("Get the content of this "),K=c("code"),J=b("AddedToken"),this.h()},l(l){t=h(l,"H2",{class:!0});var y=p(t);n=h(y,"A",{id:!0,class:!0,href:!0});var Q=p(n);e=h(Q,"SPAN",{});var W=p(e);D(o.$$.fragment,W),W.forEach(a),Q.forEach(a),d=E(y),i=h(y,"SPAN",{});var X=p(i);$=z(X,"AddedToken"),X.forEach(a),y.forEach(a),k=E(l),u=h(l,"DIV",{class:!0});var G=p(u);D(w.$$.fragment,G),_=E(G),v=h(G,"P",{});var O=p(v);s=z(O,"Represents a token that can be be added to a "),f=h(O,"A",{href:!0});var Z=p(f);T=z(Z,"Tokenizer"),Z.forEach(a),N=z(O,`.
It can have special options that defines the way it should behave.`),O.forEach(a),C=E(G),A=h(G,"DIV",{class:!0});var V=p(A);D(R.$$.fragment,V),j=E(V),L=h(V,"P",{});var U=p(L);B=z(U,"Get the content of this "),K=h(U,"CODE",{});var ee=p(K);J=z(ee,"AddedToken"),ee.forEach(a),U.forEach(a),V.forEach(a),G.forEach(a),this.h()},h(){m(n,"id","tokenizers.AddedToken]][[tokenizers.AddedToken"),m(n,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(n,"href","#tokenizers.AddedToken]][[tokenizers.AddedToken"),m(t,"class","relative group"),m(f,"href","/docs/tokenizers/pr_1/en/api/tokenizer#tokenizers.Tokenizer"),m(A,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),m(u,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(l,y){P(l,t,y),r(t,n),r(n,e),F(o,e,null),r(t,d),r(t,i),r(i,$),P(l,k,y),P(l,u,y),F(w,u,null),r(u,_),r(u,v),r(v,s),r(v,f),r(f,T),r(v,N),r(u,C),r(u,A),F(R,A,null),r(A,j),r(A,L),r(L,B),r(L,K),r(K,J),H=!0},p:ie,i(l){H||(S(o.$$.fragment,l),S(w.$$.fragment,l),S(R.$$.fragment,l),H=!0)},o(l){q(o.$$.fragment,l),q(w.$$.fragment,l),q(R.$$.fragment,l),H=!1},d(l){l&&a(t),M(o),l&&a(k),l&&a(u),M(w),M(R)}}}function he(g){let t,n;return t=new Y({props:{$$slots:{default:[ce]},$$scope:{ctx:g}}}),{c(){I(t.$$.fragment)},l(e){D(t.$$.fragment,e)},m(e,o){F(t,e,o),n=!0},p(e,o){const d={};o&2&&(d.$$scope={dirty:o,ctx:e}),t.$set(d)},i(e){n||(S(t.$$.fragment,e),n=!0)},o(e){q(t.$$.fragment,e),n=!1},d(e){M(t,e)}}}function fe(g){let t,n,e,o,d;return{c(){t=c("p"),n=b("The Rust API Reference is available directly on the "),e=c("a"),o=b("Docs.rs"),d=b(" website."),this.h()},l(i){t=h(i,"P",{});var $=p(t);n=z($,"The Rust API Reference is available directly on the "),e=h($,"A",{href:!0,rel:!0});var k=p(e);o=z(k,"Docs.rs"),k.forEach(a),d=z($," website."),$.forEach(a),this.h()},h(){m(e,"href","https://docs.rs/tokenizers/latest/tokenizers/"),m(e,"rel","nofollow")},m(i,$){P(i,t,$),r(t,n),r(t,e),r(e,o),r(t,d)},d(i){i&&a(t)}}}function pe(g){let t,n;return t=new Y({props:{$$slots:{default:[fe]},$$scope:{ctx:g}}}),{c(){I(t.$$.fragment)},l(e){D(t.$$.fragment,e)},m(e,o){F(t,e,o),n=!0},p(e,o){const d={};o&2&&(d.$$scope={dirty:o,ctx:e}),t.$set(d)},i(e){n||(S(t.$$.fragment,e),n=!0)},o(e){q(t.$$.fragment,e),n=!1},d(e){M(t,e)}}}function ue(g){let t,n;return{c(){t=c("p"),n=b("The node API has not been documented yet.")},l(e){t=h(e,"P",{});var o=p(t);n=z(o,"The node API has not been documented yet."),o.forEach(a)},m(e,o){P(e,t,o),r(t,n)},d(e){e&&a(t)}}}function me(g){let t,n;return t=new Y({props:{$$slots:{default:[ue]},$$scope:{ctx:g}}}),{c(){I(t.$$.fragment)},l(e){D(t.$$.fragment,e)},m(e,o){F(t,e,o),n=!0},p(e,o){const d={};o&2&&(d.$$scope={dirty:o,ctx:e}),t.$set(d)},i(e){n||(S(t.$$.fragment,e),n=!0)},o(e){q(t.$$.fragment,e),n=!1},d(e){M(t,e)}}}function $e(g){let t,n,e,o,d,i,$,k,u,w,_,v;return i=new oe({}),_=new le({props:{python:!0,rust:!0,node:!0,$$slots:{node:[me],rust:[pe],python:[he]},$$scope:{ctx:g}}}),{c(){t=c("meta"),n=x(),e=c("h1"),o=c("a"),d=c("span"),I(i.$$.fragment),$=x(),k=c("span"),u=b("Added Tokens"),w=x(),I(_.$$.fragment),this.h()},l(s){const f=se('[data-svelte="svelte-1phssyn"]',document.head);t=h(f,"META",{name:!0,content:!0}),f.forEach(a),n=E(s),e=h(s,"H1",{class:!0});var T=p(e);o=h(T,"A",{id:!0,class:!0,href:!0});var N=p(o);d=h(N,"SPAN",{});var C=p(d);D(i.$$.fragment,C),C.forEach(a),N.forEach(a),$=E(T),k=h(T,"SPAN",{});var A=p(k);u=z(A,"Added Tokens"),A.forEach(a),T.forEach(a),w=E(s),D(_.$$.fragment,s),this.h()},h(){m(t,"name","hf:doc:metadata"),m(t,"content",JSON.stringify(ke)),m(o,"id","added-tokens"),m(o,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(o,"href","#added-tokens"),m(e,"class","relative group")},m(s,f){r(document.head,t),P(s,n,f),P(s,e,f),r(e,o),r(o,d),F(i,d,null),r(e,$),r(e,k),r(k,u),P(s,w,f),F(_,s,f),v=!0},p(s,[f]){const T={};f&2&&(T.$$scope={dirty:f,ctx:s}),_.$set(T)},i(s){v||(S(i.$$.fragment,s),S(_.$$.fragment,s),v=!0)},o(s){q(i.$$.fragment,s),q(_.$$.fragment,s),v=!1},d(s){a(t),s&&a(n),s&&a(e),M(i),s&&a(w),M(_,s)}}}const ke={local:"added-tokens",sections:[{local:"tokenizers.AddedToken]][[tokenizers.AddedToken",title:"AddedToken"}],title:"Added Tokens"};function ge(g){return de(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Ae extends ne{constructor(t){super();re(this,t,ge,$e,ae,{})}}export{Ae as default,ke as metadata};
