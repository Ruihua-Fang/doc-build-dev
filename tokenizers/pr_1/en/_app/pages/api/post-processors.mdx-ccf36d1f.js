import{S as Fn,i as Gn,s as Jn,e as n,k as c,w as q,t as s,M as Kn,c as r,d as t,m as d,a,x as B,h as o,b as I,F as e,g as C,y as z,q as x,o as O,B as A,v as Qn,L as Xn}from"../../chunks/vendor-0d3f0756.js";import{D as Ot}from"../../chunks/Docstring-f752f2c3.js";import{C as Yn}from"../../chunks/CodeBlock-7b0cb15c.js";import{I as Zn}from"../../chunks/IconCopyLink-9193371d.js";import{T as er,M as Xo}from"../../chunks/TokenizersLanguageContent-ca787841.js";function tr(S){let i,u,l,p,_,$,E,w,Q,Y,y,F,g,v,k,ve,G,Ee,At,Ie,Rt,vt,L,X,jt,De,Wt,Mt,Z,qe,Ut,Ht,Be,Nt,Vt,$e,Yt,ze,Ft,Et,h,ee,Gt,xe,Jt,Kt,D,Qt,Oe,Xt,Zt,Ae,es,ts,Re,ss,os,ns,te,ke,rs,je,as,is,Pe,ls,We,cs,ds,Me,hs,ps,se,fs,Ue,us,ms,oe,gs,ne,_s,He,vs,Es,$s,W,R,ks,Ne,Ps,ws,Ve,ys,Ts,Ye,Ls,Ss,P,bs,Fe,Cs,Is,Ge,Ds,qs,Je,Bs,zs,Ke,xs,Os,Qe,As,Rs,js,M,Ws,Xe,Ms,Us,Ze,Hs,Ns,Vs,re,Ys,et,Fs,Gs,Js,j,tt,Ks,Qs,st,Xs,Zs,ot,eo,to,so,nt,oo,no,U,ro,rt,ao,io,at,lo,co,ho,ae,ie,po,it,fo,uo,mo,le,go,lt,_o,vo,Eo,ce,$o,ct,ko,Po,wo,de,dt,he,yo,ht,To,Lo,So,H,pe,bo,pt,Co,Io,Do,N,fe,qo,ft,Bo,zo,xo,ue,Oo,ut,Ao,Ro,jo,me,Wo,mt,Mo,Uo,Ho,V,No,gt,Vo,Yo,_t,Fo,Go,$t;return u=new Ot({props:{name:"class tokenizers.processors.BertProcessing",anchor:"tokenizers.processors.BertProcessing",parameters:[{name:"sep",val:""},{name:"cls",val:""}],parametersDescription:[{anchor:"tokenizers.processors.BertProcessing.sep",description:`<strong>sep</strong> (<code>Tuple[str, int]</code>) &#x2014;
A tuple with the string representation of the SEP token, and its id`,name:"sep"},{anchor:"tokenizers.processors.BertProcessing.cls",description:`<strong>cls</strong> (<code>Tuple[str, int]</code>) &#x2014;
A tuple with the string representation of the CLS token, and its id`,name:"cls"}]}}),k=new Ot({props:{name:"class tokenizers.processors.ByteLevel",anchor:"tokenizers.processors.ByteLevel",parameters:[{name:"trim_offsets",val:" = True"}],parametersDescription:[{anchor:"tokenizers.processors.ByteLevel.trim_offsets",description:`<strong>trim_offsets</strong> (<code>bool</code>) &#x2014;
Whether to trim the whitespaces from the produced offsets.`,name:"trim_offsets"}]}}),X=new Ot({props:{name:"class tokenizers.processors.RobertaProcessing",anchor:"tokenizers.processors.RobertaProcessing",parameters:[{name:"sep",val:""},{name:"cls",val:""},{name:"trim_offsets",val:" = True"},{name:"add_prefix_space",val:" = True"}],parametersDescription:[{anchor:"tokenizers.processors.RobertaProcessing.sep",description:`<strong>sep</strong> (<code>Tuple[str, int]</code>) &#x2014;
A tuple with the string representation of the SEP token, and its id`,name:"sep"},{anchor:"tokenizers.processors.RobertaProcessing.cls",description:`<strong>cls</strong> (<code>Tuple[str, int]</code>) &#x2014;
A tuple with the string representation of the CLS token, and its id`,name:"cls"},{anchor:"tokenizers.processors.RobertaProcessing.trim_offsets",description:`<strong>trim_offsets</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to trim the whitespaces from the produced offsets.`,name:"trim_offsets"},{anchor:"tokenizers.processors.RobertaProcessing.add_prefix_space",description:`<strong>add_prefix_space</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether the add_prefix_space option was enabled during pre-tokenization. This
is relevant because it defines the way the offsets are trimmed out.`,name:"add_prefix_space"}]}}),ee=new Ot({props:{name:"class tokenizers.processors.TemplateProcessing",anchor:"tokenizers.processors.TemplateProcessing",parameters:[{name:"single",val:""},{name:"pair",val:""},{name:"special_tokens",val:""}],parametersDescription:[{anchor:"tokenizers.processors.TemplateProcessing.single",description:`<strong>single</strong> (<code>Template</code>) &#x2014;
The template used for single sequences`,name:"single"},{anchor:"tokenizers.processors.TemplateProcessing.pair",description:`<strong>pair</strong> (<code>Template</code>) &#x2014;
The template used when both sequences are specified`,name:"pair"},{anchor:"tokenizers.processors.TemplateProcessing.special_tokens",description:`<strong>special_tokens</strong> (<code>Tokens</code>) &#x2014;
The list of special tokens used in each sequences`,name:"special_tokens"}]}}),se=new Yn({props:{code:`[CLS]   ...   [SEP]   ...   [SEP]
0      0      0      1      1`,highlighted:`[CLS]   ...   [SEP]   ...   [SEP]
<span class="hljs-number">0</span>      <span class="hljs-number">0</span>      <span class="hljs-number">0</span>      <span class="hljs-number">1</span>      <span class="hljs-number">1</span>`}}),oe=new Yn({props:{code:`TemplateProcessing(
    single="[CLS] $0 [SEP]",
    pair="[CLS] $A [SEP] $B:1 [SEP]:1",
    special_tokens=[("[CLS]", 1), ("[SEP]", 0)],
)`,highlighted:`TemplateProcessing(
    single=<span class="hljs-string">&quot;[CLS] $0 [SEP]&quot;</span>,
    pair=<span class="hljs-string">&quot;[CLS] $A [SEP] $B:1 [SEP]:1&quot;</span>,
    special_tokens=[(<span class="hljs-string">&quot;[CLS]&quot;</span>, <span class="hljs-number">1</span>), (<span class="hljs-string">&quot;[SEP]&quot;</span>, <span class="hljs-number">0</span>)],
)`}}),{c(){i=n("div"),q(u.$$.fragment),l=c(),p=n("p"),_=s(`This post-processor takes care of adding the special tokens needed by
a Bert model:`),$=c(),E=n("ul"),w=n("li"),Q=s("a SEP token"),Y=c(),y=n("li"),F=s("a CLS token"),g=c(),v=n("div"),q(k.$$.fragment),ve=c(),G=n("p"),Ee=s("This post-processor takes care of trimming the offsets."),At=c(),Ie=n("p"),Rt=s(`By default, the ByteLevel BPE might include whitespaces in the produced tokens. If you don\u2019t
want the offsets to include these whitespaces, then this PostProcessor must be used.`),vt=c(),L=n("div"),q(X.$$.fragment),jt=c(),De=n("p"),Wt=s(`This post-processor takes care of adding the special tokens needed by
a Roberta model:`),Mt=c(),Z=n("ul"),qe=n("li"),Ut=s("a SEP token"),Ht=c(),Be=n("li"),Nt=s("a CLS token"),Vt=c(),$e=n("p"),Yt=s(`It also takes care of trimming the offsets.
By default, the ByteLevel BPE might include whitespaces in the produced tokens. If you don\u2019t
want the offsets to include these whitespaces, then this PostProcessor should be initialized
with `),ze=n("code"),Ft=s("trim_offsets=True"),Et=c(),h=n("div"),q(ee.$$.fragment),Gt=c(),xe=n("p"),Jt=s(`Provides a way to specify templates in order to add the special tokens to each
input sequence as relevant.`),Kt=c(),D=n("p"),Qt=s("Let\u2019s take "),Oe=n("code"),Xt=s("BERT"),Zt=s(` tokenizer as an example. It uses two special tokens, used to
delimitate each sequence. `),Ae=n("code"),es=s("[CLS]"),ts=s(` is always used at the beginning of the first
sequence, and `),Re=n("code"),ss=s("[SEP]"),os=s(` is added at the end of both the first, and the pair
sequences. The final result looks like this:`),ns=c(),te=n("ul"),ke=n("li"),rs=s("Single sequence: "),je=n("code"),as=s("[CLS] Hello there [SEP]"),is=c(),Pe=n("li"),ls=s("Pair sequences: "),We=n("code"),cs=s("[CLS] My name is Anthony [SEP] What is my name? [SEP]"),ds=c(),Me=n("p"),hs=s("With the type ids as following:"),ps=c(),q(se.$$.fragment),fs=c(),Ue=n("p"),us=s("You can achieve such behavior using a TemplateProcessing:"),ms=c(),q(oe.$$.fragment),gs=c(),ne=n("p"),_s=s("In this example, each input sequence is identified using a "),He=n("code"),vs=s("$"),Es=s(` construct. This identifier
lets us specify each input sequence, and the type_id to use. When nothing is specified,
it uses the default values. Here are the different ways to specify it:`),$s=c(),W=n("ul"),R=n("li"),ks=s("Specifying the sequence, with default "),Ne=n("code"),Ps=s("type_id == 0"),ws=s(": "),Ve=n("code"),ys=s("$A"),Ts=s(" or "),Ye=n("code"),Ls=s("$B"),Ss=c(),P=n("li"),bs=s("Specifying the "),Fe=n("em"),Cs=s("type_id"),Is=s(" with default "),Ge=n("code"),Ds=s("sequence == A"),qs=s(": "),Je=n("code"),Bs=s("$0"),zs=s(", "),Ke=n("code"),xs=s("$1"),Os=s(", "),Qe=n("code"),As=s("$2"),Rs=s(", \u2026"),js=c(),M=n("li"),Ws=s("Specifying both: "),Xe=n("code"),Ms=s("$A:0"),Us=s(", "),Ze=n("code"),Hs=s("$B:1"),Ns=s(", \u2026"),Vs=c(),re=n("p"),Ys=s("The same construct is used for special tokens: "),et=n("code"),Fs=s("<identifier>(:<type_id>)?"),Gs=s("."),Js=c(),j=n("p"),tt=n("strong"),Ks=s("Warning"),Qs=s(`: You must ensure that you are giving the correct tokens/ids as these
will be added to the Encoding without any further check. If the given ids correspond
to something totally different in a `),st=n("em"),Xs=s("Tokenizer"),Zs=s(" using this "),ot=n("em"),eo=s("PostProcessor"),to=s(`, it
might lead to unexpected results.`),so=c(),nt=n("p"),oo=s("Types:"),no=c(),U=n("p"),ro=s("Template ("),rt=n("code"),ao=s("str"),io=s(" or "),at=n("code"),lo=s("List"),co=s("):"),ho=c(),ae=n("ul"),ie=n("li"),po=s("If a "),it=n("code"),fo=s("str"),uo=s(" is provided, the whitespace is used as delimiter between tokens"),mo=c(),le=n("li"),go=s("If a "),lt=n("code"),_o=s("List[str]"),vo=s(" is provided, a list of tokens"),Eo=c(),ce=n("p"),$o=s("Tokens ("),ct=n("code"),ko=s("List[Union[Tuple[int, str], Tuple[str, int], dict]]"),Po=s("):"),wo=c(),de=n("ul"),dt=n("li"),he=n("p"),yo=s("A "),ht=n("code"),To=s("Tuple"),Lo=s(" with both a token and its associated ID, in any order"),So=c(),H=n("li"),pe=n("p"),bo=s("A "),pt=n("code"),Co=s("dict"),Io=s(" with the following keys:"),Do=c(),N=n("ul"),fe=n("li"),qo=s("\u201Cid\u201D: "),ft=n("code"),Bo=s("str"),zo=s(" => The special token id, as specified in the Template"),xo=c(),ue=n("li"),Oo=s("\u201Cids\u201D: "),ut=n("code"),Ao=s("List[int]"),Ro=s(" => The associated IDs"),jo=c(),me=n("li"),Wo=s("\u201Ctokens\u201D: "),mt=n("code"),Mo=s("List[str]"),Uo=s(" => The associated tokens"),Ho=c(),V=n("p"),No=s("The given dict expects the provided "),gt=n("code"),Vo=s("ids"),Yo=s(" and "),_t=n("code"),Fo=s("tokens"),Go=s(` lists to have
the same length.`),this.h()},l(f){i=r(f,"DIV",{class:!0});var T=a(i);B(u.$$.fragment,T),l=d(T),p=r(T,"P",{});var Zo=a(p);_=o(Zo,`This post-processor takes care of adding the special tokens needed by
a Bert model:`),Zo.forEach(t),$=d(T),E=r(T,"UL",{});var kt=a(E);w=r(kt,"LI",{});var en=a(w);Q=o(en,"a SEP token"),en.forEach(t),Y=d(kt),y=r(kt,"LI",{});var tn=a(y);F=o(tn,"a CLS token"),tn.forEach(t),kt.forEach(t),T.forEach(t),g=d(f),v=r(f,"DIV",{class:!0});var we=a(v);B(k.$$.fragment,we),ve=d(we),G=r(we,"P",{});var sn=a(G);Ee=o(sn,"This post-processor takes care of trimming the offsets."),sn.forEach(t),At=d(we),Ie=r(we,"P",{});var on=a(Ie);Rt=o(on,`By default, the ByteLevel BPE might include whitespaces in the produced tokens. If you don\u2019t
want the offsets to include these whitespaces, then this PostProcessor must be used.`),on.forEach(t),we.forEach(t),vt=d(f),L=r(f,"DIV",{class:!0});var J=a(L);B(X.$$.fragment,J),jt=d(J),De=r(J,"P",{});var nn=a(De);Wt=o(nn,`This post-processor takes care of adding the special tokens needed by
a Roberta model:`),nn.forEach(t),Mt=d(J),Z=r(J,"UL",{});var Pt=a(Z);qe=r(Pt,"LI",{});var rn=a(qe);Ut=o(rn,"a SEP token"),rn.forEach(t),Ht=d(Pt),Be=r(Pt,"LI",{});var an=a(Be);Nt=o(an,"a CLS token"),an.forEach(t),Pt.forEach(t),Vt=d(J),$e=r(J,"P",{});var Jo=a($e);Yt=o(Jo,`It also takes care of trimming the offsets.
By default, the ByteLevel BPE might include whitespaces in the produced tokens. If you don\u2019t
want the offsets to include these whitespaces, then this PostProcessor should be initialized
with `),ze=r(Jo,"CODE",{});var ln=a(ze);Ft=o(ln,"trim_offsets=True"),ln.forEach(t),Jo.forEach(t),J.forEach(t),Et=d(f),h=r(f,"DIV",{class:!0});var m=a(h);B(ee.$$.fragment,m),Gt=d(m),xe=r(m,"P",{});var cn=a(xe);Jt=o(cn,`Provides a way to specify templates in order to add the special tokens to each
input sequence as relevant.`),cn.forEach(t),Kt=d(m),D=r(m,"P",{});var K=a(D);Qt=o(K,"Let\u2019s take "),Oe=r(K,"CODE",{});var dn=a(Oe);Xt=o(dn,"BERT"),dn.forEach(t),Zt=o(K,` tokenizer as an example. It uses two special tokens, used to
delimitate each sequence. `),Ae=r(K,"CODE",{});var hn=a(Ae);es=o(hn,"[CLS]"),hn.forEach(t),ts=o(K,` is always used at the beginning of the first
sequence, and `),Re=r(K,"CODE",{});var pn=a(Re);ss=o(pn,"[SEP]"),pn.forEach(t),os=o(K,` is added at the end of both the first, and the pair
sequences. The final result looks like this:`),K.forEach(t),ns=d(m),te=r(m,"UL",{});var wt=a(te);ke=r(wt,"LI",{});var Ko=a(ke);rs=o(Ko,"Single sequence: "),je=r(Ko,"CODE",{});var fn=a(je);as=o(fn,"[CLS] Hello there [SEP]"),fn.forEach(t),Ko.forEach(t),is=d(wt),Pe=r(wt,"LI",{});var Qo=a(Pe);ls=o(Qo,"Pair sequences: "),We=r(Qo,"CODE",{});var un=a(We);cs=o(un,"[CLS] My name is Anthony [SEP] What is my name? [SEP]"),un.forEach(t),Qo.forEach(t),wt.forEach(t),ds=d(m),Me=r(m,"P",{});var mn=a(Me);hs=o(mn,"With the type ids as following:"),mn.forEach(t),ps=d(m),B(se.$$.fragment,m),fs=d(m),Ue=r(m,"P",{});var gn=a(Ue);us=o(gn,"You can achieve such behavior using a TemplateProcessing:"),gn.forEach(t),ms=d(m),B(oe.$$.fragment,m),gs=d(m),ne=r(m,"P",{});var yt=a(ne);_s=o(yt,"In this example, each input sequence is identified using a "),He=r(yt,"CODE",{});var _n=a(He);vs=o(_n,"$"),_n.forEach(t),Es=o(yt,` construct. This identifier
lets us specify each input sequence, and the type_id to use. When nothing is specified,
it uses the default values. Here are the different ways to specify it:`),yt.forEach(t),$s=d(m),W=r(m,"UL",{});var ye=a(W);R=r(ye,"LI",{});var ge=a(R);ks=o(ge,"Specifying the sequence, with default "),Ne=r(ge,"CODE",{});var vn=a(Ne);Ps=o(vn,"type_id == 0"),vn.forEach(t),ws=o(ge,": "),Ve=r(ge,"CODE",{});var En=a(Ve);ys=o(En,"$A"),En.forEach(t),Ts=o(ge," or "),Ye=r(ge,"CODE",{});var $n=a(Ye);Ls=o($n,"$B"),$n.forEach(t),ge.forEach(t),Ss=d(ye),P=r(ye,"LI",{});var b=a(P);bs=o(b,"Specifying the "),Fe=r(b,"EM",{});var kn=a(Fe);Cs=o(kn,"type_id"),kn.forEach(t),Is=o(b," with default "),Ge=r(b,"CODE",{});var Pn=a(Ge);Ds=o(Pn,"sequence == A"),Pn.forEach(t),qs=o(b,": "),Je=r(b,"CODE",{});var wn=a(Je);Bs=o(wn,"$0"),wn.forEach(t),zs=o(b,", "),Ke=r(b,"CODE",{});var yn=a(Ke);xs=o(yn,"$1"),yn.forEach(t),Os=o(b,", "),Qe=r(b,"CODE",{});var Tn=a(Qe);As=o(Tn,"$2"),Tn.forEach(t),Rs=o(b,", \u2026"),b.forEach(t),js=d(ye),M=r(ye,"LI",{});var Te=a(M);Ws=o(Te,"Specifying both: "),Xe=r(Te,"CODE",{});var Ln=a(Xe);Ms=o(Ln,"$A:0"),Ln.forEach(t),Us=o(Te,", "),Ze=r(Te,"CODE",{});var Sn=a(Ze);Hs=o(Sn,"$B:1"),Sn.forEach(t),Ns=o(Te,", \u2026"),Te.forEach(t),ye.forEach(t),Vs=d(m),re=r(m,"P",{});var Tt=a(re);Ys=o(Tt,"The same construct is used for special tokens: "),et=r(Tt,"CODE",{});var bn=a(et);Fs=o(bn,"<identifier>(:<type_id>)?"),bn.forEach(t),Gs=o(Tt,"."),Tt.forEach(t),Js=d(m),j=r(m,"P",{});var _e=a(j);tt=r(_e,"STRONG",{});var Cn=a(tt);Ks=o(Cn,"Warning"),Cn.forEach(t),Qs=o(_e,`: You must ensure that you are giving the correct tokens/ids as these
will be added to the Encoding without any further check. If the given ids correspond
to something totally different in a `),st=r(_e,"EM",{});var In=a(st);Xs=o(In,"Tokenizer"),In.forEach(t),Zs=o(_e," using this "),ot=r(_e,"EM",{});var Dn=a(ot);eo=o(Dn,"PostProcessor"),Dn.forEach(t),to=o(_e,`, it
might lead to unexpected results.`),_e.forEach(t),so=d(m),nt=r(m,"P",{});var qn=a(nt);oo=o(qn,"Types:"),qn.forEach(t),no=d(m),U=r(m,"P",{});var Le=a(U);ro=o(Le,"Template ("),rt=r(Le,"CODE",{});var Bn=a(rt);ao=o(Bn,"str"),Bn.forEach(t),io=o(Le," or "),at=r(Le,"CODE",{});var zn=a(at);lo=o(zn,"List"),zn.forEach(t),co=o(Le,"):"),Le.forEach(t),ho=d(m),ae=r(m,"UL",{});var Lt=a(ae);ie=r(Lt,"LI",{});var St=a(ie);po=o(St,"If a "),it=r(St,"CODE",{});var xn=a(it);fo=o(xn,"str"),xn.forEach(t),uo=o(St," is provided, the whitespace is used as delimiter between tokens"),St.forEach(t),mo=d(Lt),le=r(Lt,"LI",{});var bt=a(le);go=o(bt,"If a "),lt=r(bt,"CODE",{});var On=a(lt);_o=o(On,"List[str]"),On.forEach(t),vo=o(bt," is provided, a list of tokens"),bt.forEach(t),Lt.forEach(t),Eo=d(m),ce=r(m,"P",{});var Ct=a(ce);$o=o(Ct,"Tokens ("),ct=r(Ct,"CODE",{});var An=a(ct);ko=o(An,"List[Union[Tuple[int, str], Tuple[str, int], dict]]"),An.forEach(t),Po=o(Ct,"):"),Ct.forEach(t),wo=d(m),de=r(m,"UL",{});var It=a(de);dt=r(It,"LI",{});var Rn=a(dt);he=r(Rn,"P",{});var Dt=a(he);yo=o(Dt,"A "),ht=r(Dt,"CODE",{});var jn=a(ht);To=o(jn,"Tuple"),jn.forEach(t),Lo=o(Dt," with both a token and its associated ID, in any order"),Dt.forEach(t),Rn.forEach(t),So=d(It),H=r(It,"LI",{});var Se=a(H);pe=r(Se,"P",{});var qt=a(pe);bo=o(qt,"A "),pt=r(qt,"CODE",{});var Wn=a(pt);Co=o(Wn,"dict"),Wn.forEach(t),Io=o(qt," with the following keys:"),qt.forEach(t),Do=d(Se),N=r(Se,"UL",{});var be=a(N);fe=r(be,"LI",{});var Bt=a(fe);qo=o(Bt,"\u201Cid\u201D: "),ft=r(Bt,"CODE",{});var Mn=a(ft);Bo=o(Mn,"str"),Mn.forEach(t),zo=o(Bt," => The special token id, as specified in the Template"),Bt.forEach(t),xo=d(be),ue=r(be,"LI",{});var zt=a(ue);Oo=o(zt,"\u201Cids\u201D: "),ut=r(zt,"CODE",{});var Un=a(ut);Ao=o(Un,"List[int]"),Un.forEach(t),Ro=o(zt," => The associated IDs"),zt.forEach(t),jo=d(be),me=r(be,"LI",{});var xt=a(me);Wo=o(xt,"\u201Ctokens\u201D: "),mt=r(xt,"CODE",{});var Hn=a(mt);Mo=o(Hn,"List[str]"),Hn.forEach(t),Uo=o(xt," => The associated tokens"),xt.forEach(t),be.forEach(t),Ho=d(Se),V=r(Se,"P",{});var Ce=a(V);No=o(Ce,"The given dict expects the provided "),gt=r(Ce,"CODE",{});var Nn=a(gt);Vo=o(Nn,"ids"),Nn.forEach(t),Yo=o(Ce," and "),_t=r(Ce,"CODE",{});var Vn=a(_t);Fo=o(Vn,"tokens"),Vn.forEach(t),Go=o(Ce,` lists to have
the same length.`),Ce.forEach(t),Se.forEach(t),It.forEach(t),m.forEach(t),this.h()},h(){I(i,"class","docstring"),I(v,"class","docstring"),I(L,"class","docstring"),I(h,"class","docstring")},m(f,T){C(f,i,T),z(u,i,null),e(i,l),e(i,p),e(p,_),e(i,$),e(i,E),e(E,w),e(w,Q),e(E,Y),e(E,y),e(y,F),C(f,g,T),C(f,v,T),z(k,v,null),e(v,ve),e(v,G),e(G,Ee),e(v,At),e(v,Ie),e(Ie,Rt),C(f,vt,T),C(f,L,T),z(X,L,null),e(L,jt),e(L,De),e(De,Wt),e(L,Mt),e(L,Z),e(Z,qe),e(qe,Ut),e(Z,Ht),e(Z,Be),e(Be,Nt),e(L,Vt),e(L,$e),e($e,Yt),e($e,ze),e(ze,Ft),C(f,Et,T),C(f,h,T),z(ee,h,null),e(h,Gt),e(h,xe),e(xe,Jt),e(h,Kt),e(h,D),e(D,Qt),e(D,Oe),e(Oe,Xt),e(D,Zt),e(D,Ae),e(Ae,es),e(D,ts),e(D,Re),e(Re,ss),e(D,os),e(h,ns),e(h,te),e(te,ke),e(ke,rs),e(ke,je),e(je,as),e(te,is),e(te,Pe),e(Pe,ls),e(Pe,We),e(We,cs),e(h,ds),e(h,Me),e(Me,hs),e(h,ps),z(se,h,null),e(h,fs),e(h,Ue),e(Ue,us),e(h,ms),z(oe,h,null),e(h,gs),e(h,ne),e(ne,_s),e(ne,He),e(He,vs),e(ne,Es),e(h,$s),e(h,W),e(W,R),e(R,ks),e(R,Ne),e(Ne,Ps),e(R,ws),e(R,Ve),e(Ve,ys),e(R,Ts),e(R,Ye),e(Ye,Ls),e(W,Ss),e(W,P),e(P,bs),e(P,Fe),e(Fe,Cs),e(P,Is),e(P,Ge),e(Ge,Ds),e(P,qs),e(P,Je),e(Je,Bs),e(P,zs),e(P,Ke),e(Ke,xs),e(P,Os),e(P,Qe),e(Qe,As),e(P,Rs),e(W,js),e(W,M),e(M,Ws),e(M,Xe),e(Xe,Ms),e(M,Us),e(M,Ze),e(Ze,Hs),e(M,Ns),e(h,Vs),e(h,re),e(re,Ys),e(re,et),e(et,Fs),e(re,Gs),e(h,Js),e(h,j),e(j,tt),e(tt,Ks),e(j,Qs),e(j,st),e(st,Xs),e(j,Zs),e(j,ot),e(ot,eo),e(j,to),e(h,so),e(h,nt),e(nt,oo),e(h,no),e(h,U),e(U,ro),e(U,rt),e(rt,ao),e(U,io),e(U,at),e(at,lo),e(U,co),e(h,ho),e(h,ae),e(ae,ie),e(ie,po),e(ie,it),e(it,fo),e(ie,uo),e(ae,mo),e(ae,le),e(le,go),e(le,lt),e(lt,_o),e(le,vo),e(h,Eo),e(h,ce),e(ce,$o),e(ce,ct),e(ct,ko),e(ce,Po),e(h,wo),e(h,de),e(de,dt),e(dt,he),e(he,yo),e(he,ht),e(ht,To),e(he,Lo),e(de,So),e(de,H),e(H,pe),e(pe,bo),e(pe,pt),e(pt,Co),e(pe,Io),e(H,Do),e(H,N),e(N,fe),e(fe,qo),e(fe,ft),e(ft,Bo),e(fe,zo),e(N,xo),e(N,ue),e(ue,Oo),e(ue,ut),e(ut,Ao),e(ue,Ro),e(N,jo),e(N,me),e(me,Wo),e(me,mt),e(mt,Mo),e(me,Uo),e(H,Ho),e(H,V),e(V,No),e(V,gt),e(gt,Vo),e(V,Yo),e(V,_t),e(_t,Fo),e(V,Go),$t=!0},p:Xn,i(f){$t||(x(u.$$.fragment,f),x(k.$$.fragment,f),x(X.$$.fragment,f),x(ee.$$.fragment,f),x(se.$$.fragment,f),x(oe.$$.fragment,f),$t=!0)},o(f){O(u.$$.fragment,f),O(k.$$.fragment,f),O(X.$$.fragment,f),O(ee.$$.fragment,f),O(se.$$.fragment,f),O(oe.$$.fragment,f),$t=!1},d(f){f&&t(i),A(u),f&&t(g),f&&t(v),A(k),f&&t(vt),f&&t(L),A(X),f&&t(Et),f&&t(h),A(ee),A(se),A(oe)}}}function sr(S){let i,u;return i=new Xo({props:{$$slots:{default:[tr]},$$scope:{ctx:S}}}),{c(){q(i.$$.fragment)},l(l){B(i.$$.fragment,l)},m(l,p){z(i,l,p),u=!0},p(l,p){const _={};p&2&&(_.$$scope={dirty:p,ctx:l}),i.$set(_)},i(l){u||(x(i.$$.fragment,l),u=!0)},o(l){O(i.$$.fragment,l),u=!1},d(l){A(i,l)}}}function or(S){let i,u,l,p,_;return{c(){i=n("p"),u=s("The Rust API Reference is available directly on the "),l=n("a"),p=s("Docs.rs"),_=s(" website."),this.h()},l($){i=r($,"P",{});var E=a(i);u=o(E,"The Rust API Reference is available directly on the "),l=r(E,"A",{href:!0,rel:!0});var w=a(l);p=o(w,"Docs.rs"),w.forEach(t),_=o(E," website."),E.forEach(t),this.h()},h(){I(l,"href","https://docs.rs/tokenizers/latest/tokenizers/"),I(l,"rel","nofollow")},m($,E){C($,i,E),e(i,u),e(i,l),e(l,p),e(i,_)},d($){$&&t(i)}}}function nr(S){let i,u;return i=new Xo({props:{$$slots:{default:[or]},$$scope:{ctx:S}}}),{c(){q(i.$$.fragment)},l(l){B(i.$$.fragment,l)},m(l,p){z(i,l,p),u=!0},p(l,p){const _={};p&2&&(_.$$scope={dirty:p,ctx:l}),i.$set(_)},i(l){u||(x(i.$$.fragment,l),u=!0)},o(l){O(i.$$.fragment,l),u=!1},d(l){A(i,l)}}}function rr(S){let i,u;return{c(){i=n("p"),u=s("The node API has not been documented yet.")},l(l){i=r(l,"P",{});var p=a(i);u=o(p,"The node API has not been documented yet."),p.forEach(t)},m(l,p){C(l,i,p),e(i,u)},d(l){l&&t(i)}}}function ar(S){let i,u;return i=new Xo({props:{$$slots:{default:[rr]},$$scope:{ctx:S}}}),{c(){q(i.$$.fragment)},l(l){B(i.$$.fragment,l)},m(l,p){z(i,l,p),u=!0},p(l,p){const _={};p&2&&(_.$$scope={dirty:p,ctx:l}),i.$set(_)},i(l){u||(x(i.$$.fragment,l),u=!0)},o(l){O(i.$$.fragment,l),u=!1},d(l){A(i,l)}}}function ir(S){let i,u,l,p,_,$,E,w,Q,Y,y,F;return $=new Zn({}),y=new er({props:{python:!0,rust:!0,node:!0,$$slots:{node:[ar],rust:[nr],python:[sr]},$$scope:{ctx:S}}}),{c(){i=n("meta"),u=c(),l=n("h1"),p=n("a"),_=n("span"),q($.$$.fragment),E=c(),w=n("span"),Q=s("Post-processors"),Y=c(),q(y.$$.fragment),this.h()},l(g){const v=Kn('[data-svelte="svelte-1phssyn"]',document.head);i=r(v,"META",{name:!0,content:!0}),v.forEach(t),u=d(g),l=r(g,"H1",{class:!0});var k=a(l);p=r(k,"A",{id:!0,class:!0,href:!0});var ve=a(p);_=r(ve,"SPAN",{});var G=a(_);B($.$$.fragment,G),G.forEach(t),ve.forEach(t),E=d(k),w=r(k,"SPAN",{});var Ee=a(w);Q=o(Ee,"Post-processors"),Ee.forEach(t),k.forEach(t),Y=d(g),B(y.$$.fragment,g),this.h()},h(){I(i,"name","hf:doc:metadata"),I(i,"content",JSON.stringify(lr)),I(p,"id","tokenizers.processors.BertProcessing"),I(p,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),I(p,"href","#tokenizers.processors.BertProcessing"),I(l,"class","relative group")},m(g,v){e(document.head,i),C(g,u,v),C(g,l,v),e(l,p),e(p,_),z($,_,null),e(l,E),e(l,w),e(w,Q),C(g,Y,v),z(y,g,v),F=!0},p(g,[v]){const k={};v&2&&(k.$$scope={dirty:v,ctx:g}),y.$set(k)},i(g){F||(x($.$$.fragment,g),x(y.$$.fragment,g),F=!0)},o(g){O($.$$.fragment,g),O(y.$$.fragment,g),F=!1},d(g){t(i),g&&t(u),g&&t(l),A($),g&&t(Y),A(y,g)}}}const lr={local:"tokenizers.processors.BertProcessing",title:"Post-processors"};function cr(S){return Qn(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class mr extends Fn{constructor(i){super();Gn(this,i,cr,ir,Jn,{})}}export{mr as default,lr as metadata};
