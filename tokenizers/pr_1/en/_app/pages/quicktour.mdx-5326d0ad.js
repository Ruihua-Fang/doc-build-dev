import{S as pi,i as ci,s as fi,H as $i,e as _,c as k,a as w,d as r,b as P,g as q,I as hi,J as mi,K as di,q as p,o as c,k as j,w as f,t as d,M as gi,m as v,x as $,h as g,F as a,y as h,B as m,v as _i,L as y}from"../chunks/vendor-a667fb6e.js";import{I as Te}from"../chunks/IconCopyLink-3bc33587.js";import{C as b}from"../chunks/CodeBlock-ae9d3d05.js";import{T as C,M as z}from"../chunks/TokenizersLanguageContent-c33e4dff.js";function ki(l){let t,n;const e=l[3].default,s=$i(e,l,l[2],null);return{c(){t=_("div"),s&&s.c(),this.h()},l(i){t=k(i,"DIV",{class:!0});var T=w(t);s&&s.l(T),T.forEach(r),this.h()},h(){P(t,"class","course-tip "+(l[0]==="orange"?"course-tip-orange":"")+" bg-gradient-to-br dark:bg-gradient-to-r before:border-"+l[0]+"-500 dark:before:border-"+l[0]+"-800 from-"+l[0]+"-50 dark:from-gray-900 to-white dark:to-gray-950 border border-"+l[0]+"-50 text-"+l[0]+"-700 dark:text-gray-400")},m(i,T){q(i,t,T),s&&s.m(t,null),n=!0},p(i,[T]){s&&s.p&&(!n||T&4)&&hi(s,e,i,i[2],n?di(e,i[2],T,null):mi(i[2]),null)},i(i){n||(p(s,i),n=!0)},o(i){c(s,i),n=!1},d(i){i&&r(t),s&&s.d(i)}}}function wi(l,t,n){let{$$slots:e={},$$scope:s}=t,{warning:i=!1}=t;const T=i?"orange":"green";return l.$$set=H=>{"warning"in H&&n(1,i=H.warning),"$$scope"in H&&n(2,s=H.$$scope)},[T,i,s,e]}class qi extends pi{constructor(t){super();ci(this,t,wi,ki,fi,{warning:1})}}function ji(l){let t,n,e,s,i;return{c(){t=_("p"),n=d("It can be used to instantiate a "),e=_("a"),s=d("pretrained tokenizer"),i=d(` but we
will start our quicktour by building one from scratch and see how we can train it.`),this.h()},l(T){t=k(T,"P",{});var H=w(t);n=g(H,"It can be used to instantiate a "),e=k(H,"A",{href:!0});var L=w(e);s=g(L,"pretrained tokenizer"),L.forEach(r),i=g(H,` but we
will start our quicktour by building one from scratch and see how we can train it.`),H.forEach(r),this.h()},h(){P(e,"href","#using-a-pretrained-tokenizer")},m(T,H){q(T,t,H),a(t,n),a(t,e),a(e,s),a(t,i)},d(T){T&&r(t)}}}function vi(l){let t,n;return t=new z({props:{$$slots:{default:[ji]},$$scope:{ctx:l}}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p(e,s){const i={};s&2&&(i.$$scope={dirty:s,ctx:e}),t.$set(i)},i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function bi(l){let t,n;return t=new b({props:{code:`from tokenizers import Tokenizer
from tokenizers.models import BPE
tokenizer = Tokenizer(BPE(unk_token="[UNK]"))`,highlighted:`<span class="hljs-keyword">from</span> tokenizers <span class="hljs-keyword">import</span> Tokenizer
<span class="hljs-keyword">from</span> tokenizers.models <span class="hljs-keyword">import</span> BPE
tokenizer = Tokenizer(BPE(unk_token=<span class="hljs-string">&quot;[UNK]&quot;</span>))`}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p:y,i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function zi(l){let t,n;return t=new z({props:{$$slots:{default:[bi]},$$scope:{ctx:l}}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p(e,s){const i={};s&2&&(i.$$scope={dirty:s,ctx:e}),t.$set(i)},i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function yi(l){let t,n;return t=new b({props:{code:`use tokenizers::models::bpe::BPE;
let mut tokenizer: TokenizerImpl<
    BPE,
    NormalizerWrapper,
    PreTokenizerWrapper,
    PostProcessorWrapper,
    DecoderWrapper,
> = TokenizerImpl::new(
    BPE::builder()
        .unk_token("[UNK]".to_string())
        .build()
        .unwrap(),
);`,highlighted:`<span class="hljs-keyword">use</span> tokenizers::models::bpe::BPE;
<span class="hljs-keyword">let</span> <span class="hljs-keyword">mut </span><span class="hljs-variable">tokenizer</span>: TokenizerImpl&lt;
    BPE,
    NormalizerWrapper,
    PreTokenizerWrapper,
    PostProcessorWrapper,
    DecoderWrapper,
&gt; = TokenizerImpl::<span class="hljs-title function_ invoke__">new</span>(
    BPE::<span class="hljs-title function_ invoke__">builder</span>()
        .<span class="hljs-title function_ invoke__">unk_token</span>(<span class="hljs-string">&quot;[UNK]&quot;</span>.<span class="hljs-title function_ invoke__">to_string</span>())
        .<span class="hljs-title function_ invoke__">build</span>()
        .<span class="hljs-title function_ invoke__">unwrap</span>(),
);`}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p:y,i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Ei(l){let t,n;return t=new z({props:{$$slots:{default:[yi]},$$scope:{ctx:l}}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p(e,s){const i={};s&2&&(i.$$scope={dirty:s,ctx:e}),t.$set(i)},i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Pi(l){let t,n;return t=new b({props:{code:`let { Tokenizer } = require("tokenizers/bindings/tokenizer");
let { BPE } = require("tokenizers/bindings/models");
let tokenizer = new Tokenizer(BPE.init({}, [], { unkToken: "[UNK]" }));`,highlighted:`<span class="hljs-keyword">let</span> { <span class="hljs-title class_">Tokenizer</span> } = <span class="hljs-built_in">require</span>(<span class="hljs-string">&quot;tokenizers/bindings/tokenizer&quot;</span>);
<span class="hljs-keyword">let</span> { <span class="hljs-variable constant_">BPE</span> } = <span class="hljs-built_in">require</span>(<span class="hljs-string">&quot;tokenizers/bindings/models&quot;</span>);
<span class="hljs-keyword">let</span> tokenizer = <span class="hljs-keyword">new</span> <span class="hljs-title class_">Tokenizer</span>(<span class="hljs-variable constant_">BPE</span>.<span class="hljs-title function_">init</span>({}, [], { <span class="hljs-attr">unkToken</span>: <span class="hljs-string">&quot;[UNK]&quot;</span> }));`}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p:y,i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Ti(l){let t,n;return t=new z({props:{$$slots:{default:[Pi]},$$scope:{ctx:l}}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p(e,s){const i={};s&2&&(i.$$scope={dirty:s,ctx:e}),t.$set(i)},i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Si(l){let t,n;return t=new b({props:{code:`from tokenizers.trainers import BpeTrainer
trainer = BpeTrainer(special_tokens=["[UNK]", "[CLS]", "[SEP]", "[PAD]", "[MASK]"])`,highlighted:`<span class="hljs-keyword">from</span> tokenizers.trainers <span class="hljs-keyword">import</span> BpeTrainer
trainer = BpeTrainer(special_tokens=[<span class="hljs-string">&quot;[UNK]&quot;</span>, <span class="hljs-string">&quot;[CLS]&quot;</span>, <span class="hljs-string">&quot;[SEP]&quot;</span>, <span class="hljs-string">&quot;[PAD]&quot;</span>, <span class="hljs-string">&quot;[MASK]&quot;</span>])`}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p:y,i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Hi(l){let t,n;return t=new z({props:{$$slots:{default:[Si]},$$scope:{ctx:l}}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p(e,s){const i={};s&2&&(i.$$scope={dirty:s,ctx:e}),t.$set(i)},i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Ci(l){let t,n;return t=new b({props:{code:`use tokenizers::models::bpe::BpeTrainer;
let mut trainer = BpeTrainer::builder()
    .special_tokens(vec![
        AddedToken::from("[UNK]", true),
        AddedToken::from("[CLS]", true),
        AddedToken::from("[SEP]", true),
        AddedToken::from("[PAD]", true),
        AddedToken::from("[MASK]", true),
    ])
    .build();`,highlighted:`<span class="hljs-keyword">use</span> tokenizers::models::bpe::BpeTrainer;
<span class="hljs-keyword">let</span> <span class="hljs-keyword">mut </span><span class="hljs-variable">trainer</span> = BpeTrainer::<span class="hljs-title function_ invoke__">builder</span>()
    .<span class="hljs-title function_ invoke__">special_tokens</span>(<span class="hljs-built_in">vec!</span>[
        AddedToken::<span class="hljs-title function_ invoke__">from</span>(<span class="hljs-string">&quot;[UNK]&quot;</span>, <span class="hljs-literal">true</span>),
        AddedToken::<span class="hljs-title function_ invoke__">from</span>(<span class="hljs-string">&quot;[CLS]&quot;</span>, <span class="hljs-literal">true</span>),
        AddedToken::<span class="hljs-title function_ invoke__">from</span>(<span class="hljs-string">&quot;[SEP]&quot;</span>, <span class="hljs-literal">true</span>),
        AddedToken::<span class="hljs-title function_ invoke__">from</span>(<span class="hljs-string">&quot;[PAD]&quot;</span>, <span class="hljs-literal">true</span>),
        AddedToken::<span class="hljs-title function_ invoke__">from</span>(<span class="hljs-string">&quot;[MASK]&quot;</span>, <span class="hljs-literal">true</span>),
    ])
    .<span class="hljs-title function_ invoke__">build</span>();`}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p:y,i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function xi(l){let t,n;return t=new z({props:{$$slots:{default:[Ci]},$$scope:{ctx:l}}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p(e,s){const i={};s&2&&(i.$$scope={dirty:s,ctx:e}),t.$set(i)},i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Ai(l){let t,n;return t=new b({props:{code:`let { bpeTrainer } = require("tokenizers/bindings/trainers");
let trainer = bpeTrainer({
    specialTokens: ["[UNK]", "[CLS]", "[SEP]", "[PAD]", "[MASK]"]
});`,highlighted:`<span class="hljs-keyword">let</span> { bpeTrainer } = <span class="hljs-built_in">require</span>(<span class="hljs-string">&quot;tokenizers/bindings/trainers&quot;</span>);
<span class="hljs-keyword">let</span> trainer = <span class="hljs-title function_">bpeTrainer</span>({
    <span class="hljs-attr">specialTokens</span>: [<span class="hljs-string">&quot;[UNK]&quot;</span>, <span class="hljs-string">&quot;[CLS]&quot;</span>, <span class="hljs-string">&quot;[SEP]&quot;</span>, <span class="hljs-string">&quot;[PAD]&quot;</span>, <span class="hljs-string">&quot;[MASK]&quot;</span>]
});`}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p:y,i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Di(l){let t,n;return t=new z({props:{$$slots:{default:[Ai]},$$scope:{ctx:l}}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p(e,s){const i={};s&2&&(i.$$scope={dirty:s,ctx:e}),t.$set(i)},i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Li(l){let t,n,e,s,i,T,H,L;return{c(){t=_("p"),n=d("The order in which you write the special tokens list matters: here "),e=_("code"),s=d('"[UNK]"'),i=d(` will get the ID 0,
`),T=_("code"),H=d('"[CLS]"'),L=d(" will get the ID 1 and so forth.")},l(A){t=k(A,"P",{});var B=w(t);n=g(B,"The order in which you write the special tokens list matters: here "),e=k(B,"CODE",{});var O=w(e);s=g(O,'"[UNK]"'),O.forEach(r),i=g(B,` will get the ID 0,
`),T=k(B,"CODE",{});var le=w(T);H=g(le,'"[CLS]"'),le.forEach(r),L=g(B," will get the ID 1 and so forth."),B.forEach(r)},m(A,B){q(A,t,B),a(t,n),a(t,e),a(e,s),a(t,i),a(t,T),a(T,H),a(t,L)},d(A){A&&r(t)}}}function Bi(l){let t,n;return t=new b({props:{code:`from tokenizers.pre_tokenizers import Whitespace
tokenizer.pre_tokenizer = Whitespace()`,highlighted:`<span class="hljs-keyword">from</span> tokenizers.pre_tokenizers <span class="hljs-keyword">import</span> Whitespace
tokenizer.pre_tokenizer = Whitespace()`}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p:y,i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Ni(l){let t,n;return t=new z({props:{$$slots:{default:[Bi]},$$scope:{ctx:l}}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p(e,s){const i={};s&2&&(i.$$scope={dirty:s,ctx:e}),t.$set(i)},i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Ii(l){let t,n;return t=new b({props:{code:`use tokenizers::pre_tokenizers::whitespace::Whitespace;
tokenizer.with_pre_tokenizer(Whitespace::default());`,highlighted:`<span class="hljs-keyword">use</span> tokenizers::pre_tokenizers::whitespace::Whitespace;
tokenizer.<span class="hljs-title function_ invoke__">with_pre_tokenizer</span>(Whitespace::<span class="hljs-title function_ invoke__">default</span>());`}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p:y,i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Oi(l){let t,n;return t=new z({props:{$$slots:{default:[Ii]},$$scope:{ctx:l}}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p(e,s){const i={};s&2&&(i.$$scope={dirty:s,ctx:e}),t.$set(i)},i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Ui(l){let t,n;return t=new b({props:{code:`let { whitespacePreTokenizer } = require("tokenizers/bindings/pre-tokenizers");
tokenizer.setPreTokenizer(whitespacePreTokenizer());`,highlighted:`<span class="hljs-keyword">let</span> { whitespacePreTokenizer } = <span class="hljs-built_in">require</span>(<span class="hljs-string">&quot;tokenizers/bindings/pre-tokenizers&quot;</span>);
tokenizer.<span class="hljs-title function_">setPreTokenizer</span>(<span class="hljs-title function_">whitespacePreTokenizer</span>());`}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p:y,i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Ki(l){let t,n;return t=new z({props:{$$slots:{default:[Ui]},$$scope:{ctx:l}}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p(e,s){const i={};s&2&&(i.$$scope={dirty:s,ctx:e}),t.$set(i)},i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Wi(l){let t,n;return t=new b({props:{code:`files = [f"data/wikitext-103-raw/wiki.{split}.raw" for split in ["test", "train", "valid"]]
tokenizer.train(files, trainer)`,highlighted:`files = [<span class="hljs-string">f&quot;data/wikitext-103-raw/wiki.<span class="hljs-subst">{split}</span>.raw&quot;</span> <span class="hljs-keyword">for</span> split <span class="hljs-keyword">in</span> [<span class="hljs-string">&quot;test&quot;</span>, <span class="hljs-string">&quot;train&quot;</span>, <span class="hljs-string">&quot;valid&quot;</span>]]
tokenizer.train(files, trainer)`}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p:y,i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Fi(l){let t,n;return t=new z({props:{$$slots:{default:[Wi]},$$scope:{ctx:l}}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p(e,s){const i={};s&2&&(i.$$scope={dirty:s,ctx:e}),t.$set(i)},i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Mi(l){let t,n;return t=new b({props:{code:`let files = vec![
    "data/wikitext-103-raw/wiki.train.raw".into(),
    "data/wikitext-103-raw/wiki.test.raw".into(),
    "data/wikitext-103-raw/wiki.valid.raw".into(),
];
tokenizer.train_from_files(&mut trainer, files)?;`,highlighted:`<span class="hljs-keyword">let</span> <span class="hljs-variable">files</span> = <span class="hljs-built_in">vec!</span>[
    <span class="hljs-string">&quot;data/wikitext-103-raw/wiki.train.raw&quot;</span>.<span class="hljs-title function_ invoke__">into</span>(),
    <span class="hljs-string">&quot;data/wikitext-103-raw/wiki.test.raw&quot;</span>.<span class="hljs-title function_ invoke__">into</span>(),
    <span class="hljs-string">&quot;data/wikitext-103-raw/wiki.valid.raw&quot;</span>.<span class="hljs-title function_ invoke__">into</span>(),
];
tokenizer.<span class="hljs-title function_ invoke__">train_from_files</span>(&amp;<span class="hljs-keyword">mut</span> trainer, files)?;`}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p:y,i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Yi(l){let t,n;return t=new z({props:{$$slots:{default:[Mi]},$$scope:{ctx:l}}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p(e,s){const i={};s&2&&(i.$$scope={dirty:s,ctx:e}),t.$set(i)},i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Ri(l){let t,n;return t=new b({props:{code:'let files = ["test", "train", "valid"].map(split => `data/wikitext-103-raw/wiki.${split}.raw`);\ntokenizer.train(files, trainer);',highlighted:'<span class="hljs-keyword">let</span> files = [<span class="hljs-string">&quot;test&quot;</span>, <span class="hljs-string">&quot;train&quot;</span>, <span class="hljs-string">&quot;valid&quot;</span>].<span class="hljs-title function_">map</span>(<span class="hljs-function"><span class="hljs-params">split</span> =&gt;</span> <span class="hljs-string">`data/wikitext-103-raw/wiki.<span class="hljs-subst">${split}</span>.raw`</span>);\ntokenizer.<span class="hljs-title function_">train</span>(files, trainer);'}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p:y,i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Qi(l){let t,n;return t=new z({props:{$$slots:{default:[Ri]},$$scope:{ctx:l}}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p(e,s){const i={};s&2&&(i.$$scope={dirty:s,ctx:e}),t.$set(i)},i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Ji(l){let t,n;return t=new b({props:{code:'tokenizer.save("data/tokenizer-wiki.json")',highlighted:'tokenizer.save(<span class="hljs-string">&quot;data/tokenizer-wiki.json&quot;</span>)'}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p:y,i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Vi(l){let t,n;return t=new z({props:{$$slots:{default:[Ji]},$$scope:{ctx:l}}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p(e,s){const i={};s&2&&(i.$$scope={dirty:s,ctx:e}),t.$set(i)},i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Gi(l){let t,n;return t=new b({props:{code:'tokenizer.save("data/tokenizer-wiki.json", false)?;',highlighted:'tokenizer.<span class="hljs-title function_ invoke__">save</span>(<span class="hljs-string">&quot;data/tokenizer-wiki.json&quot;</span>, <span class="hljs-literal">false</span>)?;'}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p:y,i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Xi(l){let t,n;return t=new z({props:{$$slots:{default:[Gi]},$$scope:{ctx:l}}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p(e,s){const i={};s&2&&(i.$$scope={dirty:s,ctx:e}),t.$set(i)},i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Zi(l){let t,n;return t=new b({props:{code:'tokenizer.save("data/tokenizer-wiki.json");',highlighted:'tokenizer.<span class="hljs-title function_">save</span>(<span class="hljs-string">&quot;data/tokenizer-wiki.json&quot;</span>);'}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p:y,i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function eu(l){let t,n;return t=new z({props:{$$slots:{default:[Zi]},$$scope:{ctx:l}}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p(e,s){const i={};s&2&&(i.$$scope={dirty:s,ctx:e}),t.$set(i)},i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function tu(l){let t,n;return t=new b({props:{code:'tokenizer = Tokenizer.from_file("data/tokenizer-wiki.json")',highlighted:'tokenizer = Tokenizer.from_file(<span class="hljs-string">&quot;data/tokenizer-wiki.json&quot;</span>)'}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p:y,i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function nu(l){let t,n;return t=new z({props:{$$slots:{default:[tu]},$$scope:{ctx:l}}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p(e,s){const i={};s&2&&(i.$$scope={dirty:s,ctx:e}),t.$set(i)},i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function su(l){let t,n;return t=new b({props:{code:'let mut tokenizer = Tokenizer::from_file("data/tokenizer-wiki.json")?;',highlighted:'<span class="hljs-keyword">let</span> <span class="hljs-keyword">mut </span><span class="hljs-variable">tokenizer</span> = Tokenizer::<span class="hljs-title function_ invoke__">from_file</span>(<span class="hljs-string">&quot;data/tokenizer-wiki.json&quot;</span>)?;'}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p:y,i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function ou(l){let t,n;return t=new z({props:{$$slots:{default:[su]},$$scope:{ctx:l}}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p(e,s){const i={};s&2&&(i.$$scope={dirty:s,ctx:e}),t.$set(i)},i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function ru(l){let t,n;return t=new b({props:{code:'let tokenizer = Tokenizer.fromFile("data/tokenizer-wiki.json");',highlighted:'<span class="hljs-keyword">let</span> tokenizer = <span class="hljs-title class_">Tokenizer</span>.<span class="hljs-title function_">fromFile</span>(<span class="hljs-string">&quot;data/tokenizer-wiki.json&quot;</span>);'}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p:y,i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function au(l){let t,n;return t=new z({props:{$$slots:{default:[ru]},$$scope:{ctx:l}}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p(e,s){const i={};s&2&&(i.$$scope={dirty:s,ctx:e}),t.$set(i)},i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function lu(l){let t,n;return t=new b({props:{code:`output = tokenizer.encode("Hello, y'all! How are you \u{1F601} ?")`,highlighted:'output = tokenizer.encode(<span class="hljs-string">&quot;Hello, y&#x27;all! How are you \u{1F601} ?&quot;</span>)'}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p:y,i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function iu(l){let t,n;return t=new z({props:{$$slots:{default:[lu]},$$scope:{ctx:l}}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p(e,s){const i={};s&2&&(i.$$scope={dirty:s,ctx:e}),t.$set(i)},i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function uu(l){let t,n;return t=new b({props:{code:`let output = tokenizer.encode("Hello, y'all! How are you \u{1F601} ?", true)?;`,highlighted:'<span class="hljs-keyword">let</span> <span class="hljs-variable">output</span> = tokenizer.<span class="hljs-title function_ invoke__">encode</span>(<span class="hljs-string">&quot;Hello, y&#x27;all! How are you \u{1F601} ?&quot;</span>, <span class="hljs-literal">true</span>)?;'}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p:y,i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function pu(l){let t,n;return t=new z({props:{$$slots:{default:[uu]},$$scope:{ctx:l}}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p(e,s){const i={};s&2&&(i.$$scope={dirty:s,ctx:e}),t.$set(i)},i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function cu(l){let t,n;return t=new b({props:{code:`let { promisify } = require('util');
let encode = promisify(tokenizer.encode.bind(tokenizer));
var output = await encode("Hello, y'all! How are you \u{1F601} ?");`,highlighted:`<span class="hljs-keyword">let</span> { promisify } = <span class="hljs-built_in">require</span>(<span class="hljs-string">&#x27;util&#x27;</span>);
<span class="hljs-keyword">let</span> encode = <span class="hljs-title function_">promisify</span>(tokenizer.<span class="hljs-property">encode</span>.<span class="hljs-title function_">bind</span>(tokenizer));
<span class="hljs-keyword">var</span> output = <span class="hljs-keyword">await</span> <span class="hljs-title function_">encode</span>(<span class="hljs-string">&quot;Hello, y&#x27;all! How are you \u{1F601} ?&quot;</span>);`}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p:y,i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function fu(l){let t,n;return t=new z({props:{$$slots:{default:[cu]},$$scope:{ctx:l}}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p(e,s){const i={};s&2&&(i.$$scope={dirty:s,ctx:e}),t.$set(i)},i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function $u(l){let t,n;return t=new b({props:{code:`print(output.tokens)
# ["Hello", ",", "y", "'", "all", "!", "How", "are", "you", "[UNK]", "?"]`,highlighted:`<span class="hljs-built_in">print</span>(output.tokens)
<span class="hljs-comment"># [&quot;Hello&quot;, &quot;,&quot;, &quot;y&quot;, &quot;&#x27;&quot;, &quot;all&quot;, &quot;!&quot;, &quot;How&quot;, &quot;are&quot;, &quot;you&quot;, &quot;[UNK]&quot;, &quot;?&quot;]</span>`}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p:y,i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function hu(l){let t,n;return t=new z({props:{$$slots:{default:[$u]},$$scope:{ctx:l}}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p(e,s){const i={};s&2&&(i.$$scope={dirty:s,ctx:e}),t.$set(i)},i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function mu(l){let t,n;return t=new b({props:{code:`println!("{:?}", output.get_tokens());
// ["Hello", ",", "y", "'", "all", "!", "How", "are", "you", "[UNK]", "?",]`,highlighted:`<span class="hljs-built_in">println!</span>(<span class="hljs-string">&quot;{:?}&quot;</span>, output.<span class="hljs-title function_ invoke__">get_tokens</span>());
<span class="hljs-comment">// [&quot;Hello&quot;, &quot;,&quot;, &quot;y&quot;, &quot;&#x27;&quot;, &quot;all&quot;, &quot;!&quot;, &quot;How&quot;, &quot;are&quot;, &quot;you&quot;, &quot;[UNK]&quot;, &quot;?&quot;,]</span>`}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p:y,i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function du(l){let t,n;return t=new z({props:{$$slots:{default:[mu]},$$scope:{ctx:l}}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p(e,s){const i={};s&2&&(i.$$scope={dirty:s,ctx:e}),t.$set(i)},i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function gu(l){let t,n;return t=new b({props:{code:`console.log(output.getTokens());
// ["Hello", ",", "y", "'", "all", "!", "How", "are", "you", "[UNK]", "?"]`,highlighted:`<span class="hljs-variable language_">console</span>.<span class="hljs-title function_">log</span>(output.<span class="hljs-title function_">getTokens</span>());
<span class="hljs-comment">// [&quot;Hello&quot;, &quot;,&quot;, &quot;y&quot;, &quot;&#x27;&quot;, &quot;all&quot;, &quot;!&quot;, &quot;How&quot;, &quot;are&quot;, &quot;you&quot;, &quot;[UNK]&quot;, &quot;?&quot;]</span>`}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p:y,i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function _u(l){let t,n;return t=new z({props:{$$slots:{default:[gu]},$$scope:{ctx:l}}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p(e,s){const i={};s&2&&(i.$$scope={dirty:s,ctx:e}),t.$set(i)},i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function ku(l){let t,n;return t=new b({props:{code:`print(output.ids)
# [27253, 16, 93, 11, 5097, 5, 7961, 5112, 6218, 0, 35]`,highlighted:`<span class="hljs-built_in">print</span>(output.ids)
<span class="hljs-comment"># [27253, 16, 93, 11, 5097, 5, 7961, 5112, 6218, 0, 35]</span>`}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p:y,i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function wu(l){let t,n;return t=new z({props:{$$slots:{default:[ku]},$$scope:{ctx:l}}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p(e,s){const i={};s&2&&(i.$$scope={dirty:s,ctx:e}),t.$set(i)},i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function qu(l){let t,n;return t=new b({props:{code:`println!("{:?}", output.get_ids());
// [27253, 16, 93, 11, 5097, 5, 7961, 5112, 6218, 0, 35]`,highlighted:`<span class="hljs-built_in">println!</span>(<span class="hljs-string">&quot;{:?}&quot;</span>, output.<span class="hljs-title function_ invoke__">get_ids</span>());
<span class="hljs-comment">// [27253, 16, 93, 11, 5097, 5, 7961, 5112, 6218, 0, 35]</span>`}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p:y,i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function ju(l){let t,n;return t=new z({props:{$$slots:{default:[qu]},$$scope:{ctx:l}}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p(e,s){const i={};s&2&&(i.$$scope={dirty:s,ctx:e}),t.$set(i)},i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function vu(l){let t,n;return t=new b({props:{code:`console.log(output.getIds());
// [27253, 16, 93, 11, 5097, 5, 7961, 5112, 6218, 0, 35]`,highlighted:`<span class="hljs-variable language_">console</span>.<span class="hljs-title function_">log</span>(output.<span class="hljs-title function_">getIds</span>());
<span class="hljs-comment">// [27253, 16, 93, 11, 5097, 5, 7961, 5112, 6218, 0, 35]</span>`}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p:y,i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function bu(l){let t,n;return t=new z({props:{$$slots:{default:[vu]},$$scope:{ctx:l}}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p(e,s){const i={};s&2&&(i.$$scope={dirty:s,ctx:e}),t.$set(i)},i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function zu(l){let t,n;return t=new b({props:{code:`print(output.offsets[9])
# (26, 27)`,highlighted:`<span class="hljs-built_in">print</span>(output.offsets[<span class="hljs-number">9</span>])
<span class="hljs-comment"># (26, 27)</span>`}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p:y,i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function yu(l){let t,n;return t=new z({props:{$$slots:{default:[zu]},$$scope:{ctx:l}}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p(e,s){const i={};s&2&&(i.$$scope={dirty:s,ctx:e}),t.$set(i)},i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Eu(l){let t,n;return t=new b({props:{code:`println!("{:?}", output.get_offsets()[9]);
// (26, 30)`,highlighted:`<span class="hljs-built_in">println!</span>(<span class="hljs-string">&quot;{:?}&quot;</span>, output.<span class="hljs-title function_ invoke__">get_offsets</span>()[<span class="hljs-number">9</span>]);
<span class="hljs-comment">// (26, 30)</span>`}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p:y,i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Pu(l){let t,n;return t=new z({props:{$$slots:{default:[Eu]},$$scope:{ctx:l}}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p(e,s){const i={};s&2&&(i.$$scope={dirty:s,ctx:e}),t.$set(i)},i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Tu(l){let t,n;return t=new b({props:{code:`let offsets = output.getOffsets();
console.log(offsets[9]);
// (26, 27)`,highlighted:`<span class="hljs-keyword">let</span> offsets = output.<span class="hljs-title function_">getOffsets</span>();
<span class="hljs-variable language_">console</span>.<span class="hljs-title function_">log</span>(offsets[<span class="hljs-number">9</span>]);
<span class="hljs-comment">// (26, 27)</span>`}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p:y,i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Su(l){let t,n;return t=new z({props:{$$slots:{default:[Tu]},$$scope:{ctx:l}}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p(e,s){const i={};s&2&&(i.$$scope={dirty:s,ctx:e}),t.$set(i)},i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Hu(l){let t,n;return t=new b({props:{code:`sentence = "Hello, y'all! How are you \u{1F601} ?"
sentence[26:27]
# "\u{1F601}"`,highlighted:`sentence = <span class="hljs-string">&quot;Hello, y&#x27;all! How are you \u{1F601} ?&quot;</span>
sentence[<span class="hljs-number">26</span>:<span class="hljs-number">27</span>]
<span class="hljs-comment"># &quot;\u{1F601}&quot;</span>`}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p:y,i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Cu(l){let t,n;return t=new z({props:{$$slots:{default:[Hu]},$$scope:{ctx:l}}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p(e,s){const i={};s&2&&(i.$$scope={dirty:s,ctx:e}),t.$set(i)},i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function xu(l){let t,n;return t=new b({props:{code:`let sentence = "Hello, y'all! How are you \u{1F601} ?";
println!("{}", &sentence[26..30]);
// "\u{1F601}"`,highlighted:`<span class="hljs-keyword">let</span> <span class="hljs-variable">sentence</span> = <span class="hljs-string">&quot;Hello, y&#x27;all! How are you \u{1F601} ?&quot;</span>;
<span class="hljs-built_in">println!</span>(<span class="hljs-string">&quot;{}&quot;</span>, &amp;sentence[<span class="hljs-number">26</span>..<span class="hljs-number">30</span>]);
<span class="hljs-comment">// &quot;\u{1F601}&quot;</span>`}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p:y,i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Au(l){let t,n;return t=new z({props:{$$slots:{default:[xu]},$$scope:{ctx:l}}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p(e,s){const i={};s&2&&(i.$$scope={dirty:s,ctx:e}),t.$set(i)},i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Du(l){let t,n;return t=new b({props:{code:`let { slice } = require("tokenizers/bindings/utils");
let sentence = "Hello, y'all! How are you \u{1F601} ?"
let [start, end] = offsets[9];
console.log(slice(sentence, start, end));
// "\u{1F601}"`,highlighted:`<span class="hljs-keyword">let</span> { slice } = <span class="hljs-built_in">require</span>(<span class="hljs-string">&quot;tokenizers/bindings/utils&quot;</span>);
<span class="hljs-keyword">let</span> sentence = <span class="hljs-string">&quot;Hello, y&#x27;all! How are you \u{1F601} ?&quot;</span>
<span class="hljs-keyword">let</span> [start, end] = offsets[<span class="hljs-number">9</span>];
<span class="hljs-variable language_">console</span>.<span class="hljs-title function_">log</span>(<span class="hljs-title function_">slice</span>(sentence, start, end));
<span class="hljs-comment">// &quot;\u{1F601}&quot;</span>`}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p:y,i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Lu(l){let t,n;return t=new z({props:{$$slots:{default:[Du]},$$scope:{ctx:l}}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p(e,s){const i={};s&2&&(i.$$scope={dirty:s,ctx:e}),t.$set(i)},i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Bu(l){let t,n;return t=new b({props:{code:`tokenizer.token_to_id("[SEP]")
# 2`,highlighted:`tokenizer.token_to_id(<span class="hljs-string">&quot;[SEP]&quot;</span>)
<span class="hljs-comment"># 2</span>`}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p:y,i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Nu(l){let t,n;return t=new z({props:{$$slots:{default:[Bu]},$$scope:{ctx:l}}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p(e,s){const i={};s&2&&(i.$$scope={dirty:s,ctx:e}),t.$set(i)},i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Iu(l){let t,n;return t=new b({props:{code:`println!("{}", tokenizer.token_to_id("[SEP]").unwrap());
// 2`,highlighted:`<span class="hljs-built_in">println!</span>(<span class="hljs-string">&quot;{}&quot;</span>, tokenizer.<span class="hljs-title function_ invoke__">token_to_id</span>(<span class="hljs-string">&quot;[SEP]&quot;</span>).<span class="hljs-title function_ invoke__">unwrap</span>());
<span class="hljs-comment">// 2</span>`}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p:y,i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Ou(l){let t,n;return t=new z({props:{$$slots:{default:[Iu]},$$scope:{ctx:l}}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p(e,s){const i={};s&2&&(i.$$scope={dirty:s,ctx:e}),t.$set(i)},i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Uu(l){let t,n;return t=new b({props:{code:`console.log(tokenizer.tokenToId("[SEP]"));
// 2`,highlighted:`<span class="hljs-variable language_">console</span>.<span class="hljs-title function_">log</span>(tokenizer.<span class="hljs-title function_">tokenToId</span>(<span class="hljs-string">&quot;[SEP]&quot;</span>));
<span class="hljs-comment">// 2</span>`}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p:y,i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Ku(l){let t,n;return t=new z({props:{$$slots:{default:[Uu]},$$scope:{ctx:l}}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p(e,s){const i={};s&2&&(i.$$scope={dirty:s,ctx:e}),t.$set(i)},i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Wu(l){let t,n;return t=new b({props:{code:`from tokenizers.processors import TemplateProcessing
tokenizer.post_processor = TemplateProcessing(
    single="[CLS] $A [SEP]",
    pair="[CLS] $A [SEP] $B:1 [SEP]:1",
    special_tokens=[
        ("[CLS]", tokenizer.token_to_id("[CLS]")),
        ("[SEP]", tokenizer.token_to_id("[SEP]")),
    ],
)`,highlighted:`<span class="hljs-keyword">from</span> tokenizers.processors <span class="hljs-keyword">import</span> TemplateProcessing
tokenizer.post_processor = TemplateProcessing(
    single=<span class="hljs-string">&quot;[CLS] $A [SEP]&quot;</span>,
    pair=<span class="hljs-string">&quot;[CLS] $A [SEP] $B:1 [SEP]:1&quot;</span>,
    special_tokens=[
        (<span class="hljs-string">&quot;[CLS]&quot;</span>, tokenizer.token_to_id(<span class="hljs-string">&quot;[CLS]&quot;</span>)),
        (<span class="hljs-string">&quot;[SEP]&quot;</span>, tokenizer.token_to_id(<span class="hljs-string">&quot;[SEP]&quot;</span>)),
    ],
)`}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p:y,i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Fu(l){let t,n;return t=new z({props:{$$slots:{default:[Wu]},$$scope:{ctx:l}}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p(e,s){const i={};s&2&&(i.$$scope={dirty:s,ctx:e}),t.$set(i)},i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Mu(l){let t,n;return t=new b({props:{code:`use tokenizers::processors::template::TemplateProcessing;
let special_tokens = vec![
    ("[CLS]", tokenizer.token_to_id("[CLS]").unwrap()),
    ("[SEP]", tokenizer.token_to_id("[SEP]").unwrap()),
];
tokenizer.with_post_processor(
    TemplateProcessing::builder()
        .try_single("[CLS] $A [SEP]")
        .unwrap()
        .try_pair("[CLS] $A [SEP] $B:1 [SEP]:1")
        .unwrap()
        .special_tokens(special_tokens)
        .build()?,
);`,highlighted:`<span class="hljs-keyword">use</span> tokenizers::processors::template::TemplateProcessing;
<span class="hljs-keyword">let</span> <span class="hljs-variable">special_tokens</span> = <span class="hljs-built_in">vec!</span>[
    (<span class="hljs-string">&quot;[CLS]&quot;</span>, tokenizer.<span class="hljs-title function_ invoke__">token_to_id</span>(<span class="hljs-string">&quot;[CLS]&quot;</span>).<span class="hljs-title function_ invoke__">unwrap</span>()),
    (<span class="hljs-string">&quot;[SEP]&quot;</span>, tokenizer.<span class="hljs-title function_ invoke__">token_to_id</span>(<span class="hljs-string">&quot;[SEP]&quot;</span>).<span class="hljs-title function_ invoke__">unwrap</span>()),
];
tokenizer.<span class="hljs-title function_ invoke__">with_post_processor</span>(
    TemplateProcessing::<span class="hljs-title function_ invoke__">builder</span>()
        .<span class="hljs-title function_ invoke__">try_single</span>(<span class="hljs-string">&quot;[CLS] $A [SEP]&quot;</span>)
        .<span class="hljs-title function_ invoke__">unwrap</span>()
        .<span class="hljs-title function_ invoke__">try_pair</span>(<span class="hljs-string">&quot;[CLS] $A [SEP] $B:1 [SEP]:1&quot;</span>)
        .<span class="hljs-title function_ invoke__">unwrap</span>()
        .<span class="hljs-title function_ invoke__">special_tokens</span>(special_tokens)
        .<span class="hljs-title function_ invoke__">build</span>()?,
);`}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p:y,i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Yu(l){let t,n;return t=new z({props:{$$slots:{default:[Mu]},$$scope:{ctx:l}}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p(e,s){const i={};s&2&&(i.$$scope={dirty:s,ctx:e}),t.$set(i)},i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Ru(l){let t,n;return t=new b({props:{code:`let { templateProcessing } = require("tokenizers/bindings/post-processors");
tokenizer.setPostProcessor(templateProcessing(
    "[CLS] $A [SEP]",
    "[CLS] $A [SEP] $B:1 [SEP]:1",
    [
        ["[CLS]", tokenizer.tokenToId("[CLS]")],
        ["[SEP]", tokenizer.tokenToId("[SEP]")],
    ],
));`,highlighted:`<span class="hljs-keyword">let</span> { templateProcessing } = <span class="hljs-built_in">require</span>(<span class="hljs-string">&quot;tokenizers/bindings/post-processors&quot;</span>);
tokenizer.<span class="hljs-title function_">setPostProcessor</span>(<span class="hljs-title function_">templateProcessing</span>(
    <span class="hljs-string">&quot;[CLS] $A [SEP]&quot;</span>,
    <span class="hljs-string">&quot;[CLS] $A [SEP] $B:1 [SEP]:1&quot;</span>,
    [
        [<span class="hljs-string">&quot;[CLS]&quot;</span>, tokenizer.<span class="hljs-title function_">tokenToId</span>(<span class="hljs-string">&quot;[CLS]&quot;</span>)],
        [<span class="hljs-string">&quot;[SEP]&quot;</span>, tokenizer.<span class="hljs-title function_">tokenToId</span>(<span class="hljs-string">&quot;[SEP]&quot;</span>)],
    ],
));`}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p:y,i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Qu(l){let t,n;return t=new z({props:{$$slots:{default:[Ru]},$$scope:{ctx:l}}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p(e,s){const i={};s&2&&(i.$$scope={dirty:s,ctx:e}),t.$set(i)},i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Ju(l){let t,n;return t=new b({props:{code:`output = tokenizer.encode("Hello, y'all! How are you \u{1F601} ?")
print(output.tokens)
# ["[CLS]", "Hello", ",", "y", "'", "all", "!", "How", "are", "you", "[UNK]", "?", "[SEP]"]`,highlighted:`output = tokenizer.encode(<span class="hljs-string">&quot;Hello, y&#x27;all! How are you \u{1F601} ?&quot;</span>)
<span class="hljs-built_in">print</span>(output.tokens)
<span class="hljs-comment"># [&quot;[CLS]&quot;, &quot;Hello&quot;, &quot;,&quot;, &quot;y&quot;, &quot;&#x27;&quot;, &quot;all&quot;, &quot;!&quot;, &quot;How&quot;, &quot;are&quot;, &quot;you&quot;, &quot;[UNK]&quot;, &quot;?&quot;, &quot;[SEP]&quot;]</span>`}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p:y,i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Vu(l){let t,n;return t=new z({props:{$$slots:{default:[Ju]},$$scope:{ctx:l}}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p(e,s){const i={};s&2&&(i.$$scope={dirty:s,ctx:e}),t.$set(i)},i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Gu(l){let t,n;return t=new b({props:{code:`let output = tokenizer.encode("Hello, y'all! How are you \u{1F601} ?", true)?;
println!("{:?}", output.get_tokens());
// ["[CLS]", "Hello", ",", "y", "'", "all", "!", "How", "are", "you", "[UNK]", "?", "[SEP]"]`,highlighted:`<span class="hljs-keyword">let</span> <span class="hljs-variable">output</span> = tokenizer.<span class="hljs-title function_ invoke__">encode</span>(<span class="hljs-string">&quot;Hello, y&#x27;all! How are you \u{1F601} ?&quot;</span>, <span class="hljs-literal">true</span>)?;
<span class="hljs-built_in">println!</span>(<span class="hljs-string">&quot;{:?}&quot;</span>, output.<span class="hljs-title function_ invoke__">get_tokens</span>());
<span class="hljs-comment">// [&quot;[CLS]&quot;, &quot;Hello&quot;, &quot;,&quot;, &quot;y&quot;, &quot;&#x27;&quot;, &quot;all&quot;, &quot;!&quot;, &quot;How&quot;, &quot;are&quot;, &quot;you&quot;, &quot;[UNK]&quot;, &quot;?&quot;, &quot;[SEP]&quot;]</span>`}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p:y,i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Xu(l){let t,n;return t=new z({props:{$$slots:{default:[Gu]},$$scope:{ctx:l}}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p(e,s){const i={};s&2&&(i.$$scope={dirty:s,ctx:e}),t.$set(i)},i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Zu(l){let t,n;return t=new b({props:{code:`var output = await encode("Hello, y'all! How are you \u{1F601} ?");
console.log(output.getTokens());
// ["[CLS]", "Hello", ",", "y", "'", "all", "!", "How", "are", "you", "[UNK]", "?", "[SEP]"]`,highlighted:`<span class="hljs-keyword">var</span> output = <span class="hljs-keyword">await</span> <span class="hljs-title function_">encode</span>(<span class="hljs-string">&quot;Hello, y&#x27;all! How are you \u{1F601} ?&quot;</span>);
<span class="hljs-variable language_">console</span>.<span class="hljs-title function_">log</span>(output.<span class="hljs-title function_">getTokens</span>());
<span class="hljs-comment">// [&quot;[CLS]&quot;, &quot;Hello&quot;, &quot;,&quot;, &quot;y&quot;, &quot;&#x27;&quot;, &quot;all&quot;, &quot;!&quot;, &quot;How&quot;, &quot;are&quot;, &quot;you&quot;, &quot;[UNK]&quot;, &quot;?&quot;, &quot;[SEP]&quot;]</span>`}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p:y,i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function ep(l){let t,n;return t=new z({props:{$$slots:{default:[Zu]},$$scope:{ctx:l}}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p(e,s){const i={};s&2&&(i.$$scope={dirty:s,ctx:e}),t.$set(i)},i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function tp(l){let t,n;return t=new b({props:{code:`output = tokenizer.encode("Hello, y'all!", "How are you \u{1F601} ?")
print(output.tokens)
# ["[CLS]", "Hello", ",", "y", "'", "all", "!", "[SEP]", "How", "are", "you", "[UNK]", "?", "[SEP]"]`,highlighted:`output = tokenizer.encode(<span class="hljs-string">&quot;Hello, y&#x27;all!&quot;</span>, <span class="hljs-string">&quot;How are you \u{1F601} ?&quot;</span>)
<span class="hljs-built_in">print</span>(output.tokens)
<span class="hljs-comment"># [&quot;[CLS]&quot;, &quot;Hello&quot;, &quot;,&quot;, &quot;y&quot;, &quot;&#x27;&quot;, &quot;all&quot;, &quot;!&quot;, &quot;[SEP]&quot;, &quot;How&quot;, &quot;are&quot;, &quot;you&quot;, &quot;[UNK]&quot;, &quot;?&quot;, &quot;[SEP]&quot;]</span>`}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p:y,i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function np(l){let t,n;return t=new z({props:{$$slots:{default:[tp]},$$scope:{ctx:l}}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p(e,s){const i={};s&2&&(i.$$scope={dirty:s,ctx:e}),t.$set(i)},i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function sp(l){let t,n;return t=new b({props:{code:`let output = tokenizer.encode(("Hello, y'all!", "How are you \u{1F601} ?"), true)?;
println!("{:?}", output.get_tokens());
// ["[CLS]", "Hello", ",", "y", "'", "all", "!", "[SEP]", "How", "are", "you", "[UNK]", "?", "[SEP]"]`,highlighted:`<span class="hljs-keyword">let</span> <span class="hljs-variable">output</span> = tokenizer.<span class="hljs-title function_ invoke__">encode</span>((<span class="hljs-string">&quot;Hello, y&#x27;all!&quot;</span>, <span class="hljs-string">&quot;How are you \u{1F601} ?&quot;</span>), <span class="hljs-literal">true</span>)?;
<span class="hljs-built_in">println!</span>(<span class="hljs-string">&quot;{:?}&quot;</span>, output.<span class="hljs-title function_ invoke__">get_tokens</span>());
<span class="hljs-comment">// [&quot;[CLS]&quot;, &quot;Hello&quot;, &quot;,&quot;, &quot;y&quot;, &quot;&#x27;&quot;, &quot;all&quot;, &quot;!&quot;, &quot;[SEP]&quot;, &quot;How&quot;, &quot;are&quot;, &quot;you&quot;, &quot;[UNK]&quot;, &quot;?&quot;, &quot;[SEP]&quot;]</span>`}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p:y,i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function op(l){let t,n;return t=new z({props:{$$slots:{default:[sp]},$$scope:{ctx:l}}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p(e,s){const i={};s&2&&(i.$$scope={dirty:s,ctx:e}),t.$set(i)},i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function rp(l){let t,n;return t=new b({props:{code:`var output = await encode("Hello, y'all!", "How are you \u{1F601} ?");
console.log(output.getTokens());
// ["[CLS]", "Hello", ",", "y", "'", "all", "!", "[SEP]", "How", "are", "you", "[UNK]", "?", "[SEP]"]`,highlighted:`<span class="hljs-keyword">var</span> output = <span class="hljs-keyword">await</span> <span class="hljs-title function_">encode</span>(<span class="hljs-string">&quot;Hello, y&#x27;all!&quot;</span>, <span class="hljs-string">&quot;How are you \u{1F601} ?&quot;</span>);
<span class="hljs-variable language_">console</span>.<span class="hljs-title function_">log</span>(output.<span class="hljs-title function_">getTokens</span>());
<span class="hljs-comment">// [&quot;[CLS]&quot;, &quot;Hello&quot;, &quot;,&quot;, &quot;y&quot;, &quot;&#x27;&quot;, &quot;all&quot;, &quot;!&quot;, &quot;[SEP]&quot;, &quot;How&quot;, &quot;are&quot;, &quot;you&quot;, &quot;[UNK]&quot;, &quot;?&quot;, &quot;[SEP]&quot;]</span>`}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p:y,i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function ap(l){let t,n;return t=new z({props:{$$slots:{default:[rp]},$$scope:{ctx:l}}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p(e,s){const i={};s&2&&(i.$$scope={dirty:s,ctx:e}),t.$set(i)},i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function lp(l){let t,n;return t=new b({props:{code:`print(output.type_ids)
# [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1]`,highlighted:`<span class="hljs-built_in">print</span>(output.type_ids)
<span class="hljs-comment"># [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1]</span>`}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p:y,i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function ip(l){let t,n;return t=new z({props:{$$slots:{default:[lp]},$$scope:{ctx:l}}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p(e,s){const i={};s&2&&(i.$$scope={dirty:s,ctx:e}),t.$set(i)},i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function up(l){let t,n;return t=new b({props:{code:`println!("{:?}", output.get_type_ids());
// [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1]`,highlighted:`<span class="hljs-built_in">println!</span>(<span class="hljs-string">&quot;{:?}&quot;</span>, output.<span class="hljs-title function_ invoke__">get_type_ids</span>());
<span class="hljs-comment">// [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1]</span>`}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p:y,i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function pp(l){let t,n;return t=new z({props:{$$slots:{default:[up]},$$scope:{ctx:l}}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p(e,s){const i={};s&2&&(i.$$scope={dirty:s,ctx:e}),t.$set(i)},i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function cp(l){let t,n;return t=new b({props:{code:`console.log(output.getTypeIds());
// [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1]`,highlighted:`<span class="hljs-variable language_">console</span>.<span class="hljs-title function_">log</span>(output.<span class="hljs-title function_">getTypeIds</span>());
<span class="hljs-comment">// [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1]</span>`}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p:y,i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function fp(l){let t,n;return t=new z({props:{$$slots:{default:[cp]},$$scope:{ctx:l}}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p(e,s){const i={};s&2&&(i.$$scope={dirty:s,ctx:e}),t.$set(i)},i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function $p(l){let t,n;return t=new b({props:{code:`output = tokenizer.encode_batch(["Hello, y'all!", "How are you \u{1F601} ?"])`,highlighted:'output = tokenizer.encode_batch([<span class="hljs-string">&quot;Hello, y&#x27;all!&quot;</span>, <span class="hljs-string">&quot;How are you \u{1F601} ?&quot;</span>])'}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p:y,i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function hp(l){let t,n;return t=new z({props:{$$slots:{default:[$p]},$$scope:{ctx:l}}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p(e,s){const i={};s&2&&(i.$$scope={dirty:s,ctx:e}),t.$set(i)},i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function mp(l){let t,n;return t=new b({props:{code:`let output = tokenizer.encode_batch(vec!["Hello, y'all!", "How are you \u{1F601} ?"], true)?;`,highlighted:'<span class="hljs-keyword">let</span> <span class="hljs-variable">output</span> = tokenizer.<span class="hljs-title function_ invoke__">encode_batch</span>(<span class="hljs-built_in">vec!</span>[<span class="hljs-string">&quot;Hello, y&#x27;all!&quot;</span>, <span class="hljs-string">&quot;How are you \u{1F601} ?&quot;</span>], <span class="hljs-literal">true</span>)?;'}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p:y,i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function dp(l){let t,n;return t=new z({props:{$$slots:{default:[mp]},$$scope:{ctx:l}}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p(e,s){const i={};s&2&&(i.$$scope={dirty:s,ctx:e}),t.$set(i)},i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function gp(l){let t,n;return t=new b({props:{code:`let encodeBatch = promisify(tokenizer.encodeBatch.bind(tokenizer));
var output = await encodeBatch(["Hello, y'all!", "How are you \u{1F601} ?"]);`,highlighted:`<span class="hljs-keyword">let</span> encodeBatch = <span class="hljs-title function_">promisify</span>(tokenizer.<span class="hljs-property">encodeBatch</span>.<span class="hljs-title function_">bind</span>(tokenizer));
<span class="hljs-keyword">var</span> output = <span class="hljs-keyword">await</span> <span class="hljs-title function_">encodeBatch</span>([<span class="hljs-string">&quot;Hello, y&#x27;all!&quot;</span>, <span class="hljs-string">&quot;How are you \u{1F601} ?&quot;</span>]);`}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p:y,i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function _p(l){let t,n;return t=new z({props:{$$slots:{default:[gp]},$$scope:{ctx:l}}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p(e,s){const i={};s&2&&(i.$$scope={dirty:s,ctx:e}),t.$set(i)},i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function kp(l){let t,n;return t=new b({props:{code:`output = tokenizer.encode_batch(
    [["Hello, y'all!", "How are you \u{1F601} ?"], ["Hello to you too!", "I'm fine, thank you!"]]
)`,highlighted:`output = tokenizer.encode_batch(
    [[<span class="hljs-string">&quot;Hello, y&#x27;all!&quot;</span>, <span class="hljs-string">&quot;How are you \u{1F601} ?&quot;</span>], [<span class="hljs-string">&quot;Hello to you too!&quot;</span>, <span class="hljs-string">&quot;I&#x27;m fine, thank you!&quot;</span>]]
)`}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p:y,i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function wp(l){let t,n;return t=new z({props:{$$slots:{default:[kp]},$$scope:{ctx:l}}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p(e,s){const i={};s&2&&(i.$$scope={dirty:s,ctx:e}),t.$set(i)},i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function qp(l){let t,n;return t=new b({props:{code:`let output = tokenizer.encode_batch(
    vec![
        ("Hello, y'all!", "How are you \u{1F601} ?"),
        ("Hello to you too!", "I'm fine, thank you!"),
    ],
    true,
)?;`,highlighted:`<span class="hljs-keyword">let</span> <span class="hljs-variable">output</span> = tokenizer.<span class="hljs-title function_ invoke__">encode_batch</span>(
    <span class="hljs-built_in">vec!</span>[
        (<span class="hljs-string">&quot;Hello, y&#x27;all!&quot;</span>, <span class="hljs-string">&quot;How are you \u{1F601} ?&quot;</span>),
        (<span class="hljs-string">&quot;Hello to you too!&quot;</span>, <span class="hljs-string">&quot;I&#x27;m fine, thank you!&quot;</span>),
    ],
    <span class="hljs-literal">true</span>,
)?;`}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p:y,i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function jp(l){let t,n;return t=new z({props:{$$slots:{default:[qp]},$$scope:{ctx:l}}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p(e,s){const i={};s&2&&(i.$$scope={dirty:s,ctx:e}),t.$set(i)},i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function vp(l){let t,n;return t=new b({props:{code:`var output = await encodeBatch(
    [["Hello, y'all!", "How are you \u{1F601} ?"], ["Hello to you too!", "I'm fine, thank you!"]]
);`,highlighted:`<span class="hljs-keyword">var</span> output = <span class="hljs-keyword">await</span> <span class="hljs-title function_">encodeBatch</span>(
    [[<span class="hljs-string">&quot;Hello, y&#x27;all!&quot;</span>, <span class="hljs-string">&quot;How are you \u{1F601} ?&quot;</span>], [<span class="hljs-string">&quot;Hello to you too!&quot;</span>, <span class="hljs-string">&quot;I&#x27;m fine, thank you!&quot;</span>]]
);`}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p:y,i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function bp(l){let t,n;return t=new z({props:{$$slots:{default:[vp]},$$scope:{ctx:l}}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p(e,s){const i={};s&2&&(i.$$scope={dirty:s,ctx:e}),t.$set(i)},i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function zp(l){let t,n;return t=new b({props:{code:'tokenizer.enable_padding(pad_id=3, pad_token="[PAD]")',highlighted:'tokenizer.enable_padding(pad_id=<span class="hljs-number">3</span>, pad_token=<span class="hljs-string">&quot;[PAD]&quot;</span>)'}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p:y,i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function yp(l){let t,n;return t=new z({props:{$$slots:{default:[zp]},$$scope:{ctx:l}}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p(e,s){const i={};s&2&&(i.$$scope={dirty:s,ctx:e}),t.$set(i)},i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Ep(l){let t,n;return t=new b({props:{code:`use tokenizers::PaddingParams;
tokenizer.with_padding(Some(PaddingParams {
    pad_id: 3,
    pad_token: "[PAD]".to_string(),
    ..PaddingParams::default()
}));`,highlighted:`<span class="hljs-keyword">use</span> tokenizers::PaddingParams;
tokenizer.<span class="hljs-title function_ invoke__">with_padding</span>(<span class="hljs-title function_ invoke__">Some</span>(PaddingParams {
    pad_id: <span class="hljs-number">3</span>,
    pad_token: <span class="hljs-string">&quot;[PAD]&quot;</span>.<span class="hljs-title function_ invoke__">to_string</span>(),
    ..PaddingParams::<span class="hljs-title function_ invoke__">default</span>()
}));`}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p:y,i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Pp(l){let t,n;return t=new z({props:{$$slots:{default:[Ep]},$$scope:{ctx:l}}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p(e,s){const i={};s&2&&(i.$$scope={dirty:s,ctx:e}),t.$set(i)},i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Tp(l){let t,n;return t=new b({props:{code:'tokenizer.setPadding({ padId: 3, padToken: "[PAD]" });',highlighted:'tokenizer.<span class="hljs-title function_">setPadding</span>({ <span class="hljs-attr">padId</span>: <span class="hljs-number">3</span>, <span class="hljs-attr">padToken</span>: <span class="hljs-string">&quot;[PAD]&quot;</span> });'}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p:y,i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Sp(l){let t,n;return t=new z({props:{$$slots:{default:[Tp]},$$scope:{ctx:l}}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p(e,s){const i={};s&2&&(i.$$scope={dirty:s,ctx:e}),t.$set(i)},i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Hp(l){let t,n;return t=new b({props:{code:`output = tokenizer.encode_batch(["Hello, y'all!", "How are you \u{1F601} ?"])
print(output[1].tokens)
# ["[CLS]", "How", "are", "you", "[UNK]", "?", "[SEP]", "[PAD]"]`,highlighted:`output = tokenizer.encode_batch([<span class="hljs-string">&quot;Hello, y&#x27;all!&quot;</span>, <span class="hljs-string">&quot;How are you \u{1F601} ?&quot;</span>])
<span class="hljs-built_in">print</span>(output[<span class="hljs-number">1</span>].tokens)
<span class="hljs-comment"># [&quot;[CLS]&quot;, &quot;How&quot;, &quot;are&quot;, &quot;you&quot;, &quot;[UNK]&quot;, &quot;?&quot;, &quot;[SEP]&quot;, &quot;[PAD]&quot;]</span>`}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p:y,i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Cp(l){let t,n;return t=new z({props:{$$slots:{default:[Hp]},$$scope:{ctx:l}}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p(e,s){const i={};s&2&&(i.$$scope={dirty:s,ctx:e}),t.$set(i)},i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function xp(l){let t,n;return t=new b({props:{code:`let output = tokenizer.encode_batch(vec!["Hello, y'all!", "How are you \u{1F601} ?"], true)?;
println!("{:?}", output[1].get_tokens());
// ["[CLS]", "How", "are", "you", "[UNK]", "?", "[SEP]", "[PAD]"]`,highlighted:`<span class="hljs-keyword">let</span> <span class="hljs-variable">output</span> = tokenizer.<span class="hljs-title function_ invoke__">encode_batch</span>(<span class="hljs-built_in">vec!</span>[<span class="hljs-string">&quot;Hello, y&#x27;all!&quot;</span>, <span class="hljs-string">&quot;How are you \u{1F601} ?&quot;</span>], <span class="hljs-literal">true</span>)?;
<span class="hljs-built_in">println!</span>(<span class="hljs-string">&quot;{:?}&quot;</span>, output[<span class="hljs-number">1</span>].<span class="hljs-title function_ invoke__">get_tokens</span>());
<span class="hljs-comment">// [&quot;[CLS]&quot;, &quot;How&quot;, &quot;are&quot;, &quot;you&quot;, &quot;[UNK]&quot;, &quot;?&quot;, &quot;[SEP]&quot;, &quot;[PAD]&quot;]</span>`}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p:y,i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Ap(l){let t,n;return t=new z({props:{$$slots:{default:[xp]},$$scope:{ctx:l}}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p(e,s){const i={};s&2&&(i.$$scope={dirty:s,ctx:e}),t.$set(i)},i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Dp(l){let t,n;return t=new b({props:{code:`var output = await encodeBatch(["Hello, y'all!", "How are you \u{1F601} ?"]);
console.log(output[1].getTokens());
// ["[CLS]", "How", "are", "you", "[UNK]", "?", "[SEP]", "[PAD]"]`,highlighted:`<span class="hljs-keyword">var</span> output = <span class="hljs-keyword">await</span> <span class="hljs-title function_">encodeBatch</span>([<span class="hljs-string">&quot;Hello, y&#x27;all!&quot;</span>, <span class="hljs-string">&quot;How are you \u{1F601} ?&quot;</span>]);
<span class="hljs-variable language_">console</span>.<span class="hljs-title function_">log</span>(output[<span class="hljs-number">1</span>].<span class="hljs-title function_">getTokens</span>());
<span class="hljs-comment">// [&quot;[CLS]&quot;, &quot;How&quot;, &quot;are&quot;, &quot;you&quot;, &quot;[UNK]&quot;, &quot;?&quot;, &quot;[SEP]&quot;, &quot;[PAD]&quot;]</span>`}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p:y,i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Lp(l){let t,n;return t=new z({props:{$$slots:{default:[Dp]},$$scope:{ctx:l}}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p(e,s){const i={};s&2&&(i.$$scope={dirty:s,ctx:e}),t.$set(i)},i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Bp(l){let t,n;return t=new b({props:{code:`print(output[1].attention_mask)
# [1, 1, 1, 1, 1, 1, 1, 0]`,highlighted:`<span class="hljs-built_in">print</span>(output[<span class="hljs-number">1</span>].attention_mask)
<span class="hljs-comment"># [1, 1, 1, 1, 1, 1, 1, 0]</span>`}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p:y,i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Np(l){let t,n;return t=new z({props:{$$slots:{default:[Bp]},$$scope:{ctx:l}}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p(e,s){const i={};s&2&&(i.$$scope={dirty:s,ctx:e}),t.$set(i)},i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Ip(l){let t,n;return t=new b({props:{code:`println!("{:?}", output[1].get_attention_mask());
// [1, 1, 1, 1, 1, 1, 1, 0]`,highlighted:`<span class="hljs-built_in">println!</span>(<span class="hljs-string">&quot;{:?}&quot;</span>, output[<span class="hljs-number">1</span>].<span class="hljs-title function_ invoke__">get_attention_mask</span>());
<span class="hljs-comment">// [1, 1, 1, 1, 1, 1, 1, 0]</span>`}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p:y,i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Op(l){let t,n;return t=new z({props:{$$slots:{default:[Ip]},$$scope:{ctx:l}}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p(e,s){const i={};s&2&&(i.$$scope={dirty:s,ctx:e}),t.$set(i)},i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Up(l){let t,n;return t=new b({props:{code:`console.log(output[1].getAttentionMask());
// [1, 1, 1, 1, 1, 1, 1, 0]`,highlighted:`<span class="hljs-variable language_">console</span>.<span class="hljs-title function_">log</span>(output[<span class="hljs-number">1</span>].<span class="hljs-title function_">getAttentionMask</span>());
<span class="hljs-comment">// [1, 1, 1, 1, 1, 1, 1, 0]</span>`}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p:y,i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Kp(l){let t,n;return t=new z({props:{$$slots:{default:[Up]},$$scope:{ctx:l}}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p(e,s){const i={};s&2&&(i.$$scope={dirty:s,ctx:e}),t.$set(i)},i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Wp(l){let t,n,e,s,i,T,H,L,A,B,O,le,Se,F,Q,K,D,W,M,ne,_e,ke,He,Y,ie,se,Ce,oe,we,N,xe,U,J,qe,V,re,ue;return s=new Te({}),Q=new b({props:{code:`from tokenizers import Tokenizer

tokenizer = Tokenizer.from_pretrained("bert-base-uncased")`,highlighted:`<span class="hljs-keyword">from</span> tokenizers <span class="hljs-keyword">import</span> Tokenizer

tokenizer = Tokenizer.from_pretrained(<span class="hljs-string">&quot;bert-base-uncased&quot;</span>)`}}),ne=new Te({}),oe=new b({props:{code:`from tokenizers import BertWordPieceTokenizer

tokenizer = BertWordPieceTokenizer("bert-base-uncased-vocab.txt", lowercase=True)`,highlighted:`<span class="hljs-keyword">from</span> tokenizers <span class="hljs-keyword">import</span> BertWordPieceTokenizer

tokenizer = BertWordPieceTokenizer(<span class="hljs-string">&quot;bert-base-uncased-vocab.txt&quot;</span>, lowercase=<span class="hljs-literal">True</span>)`}}),re=new b({props:{code:"wget https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt",highlighted:"wget https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt"}}),{c(){t=_("h3"),n=_("a"),e=_("span"),f(s.$$.fragment),i=j(),T=_("span"),H=d("Using a pretrained tokenizer"),L=j(),A=_("p"),B=d(`You can load any tokenizer from the Hugging Face Hub as long as a
`),O=_("code"),le=d("tokenizer.json"),Se=d(" file is available in the repository."),F=j(),f(Q.$$.fragment),K=j(),D=_("h3"),W=_("a"),M=_("span"),f(ne.$$.fragment),_e=j(),ke=_("span"),He=d("Importing a pretrained tokenizer from legacy vocabulary files"),Y=j(),ie=_("p"),se=d(`You can also import a pretrained tokenizer directly in, as long as you
have its vocabulary file. For instance, here is how to import the
classic pretrained BERT tokenizer:`),Ce=j(),f(oe.$$.fragment),we=j(),N=_("p"),xe=d("as long as you have downloaded the file "),U=_("code"),J=d("bert-base-uncased-vocab.txt"),qe=d(" with"),V=j(),f(re.$$.fragment),this.h()},l(E){t=k(E,"H3",{class:!0});var S=w(t);n=k(S,"A",{id:!0,class:!0,href:!0});var ae=w(n);e=k(ae,"SPAN",{});var Dt=w(e);$(s.$$.fragment,Dt),Dt.forEach(r),ae.forEach(r),i=v(S),T=k(S,"SPAN",{});var pe=w(T);H=g(pe,"Using a pretrained tokenizer"),pe.forEach(r),S.forEach(r),L=v(E),A=k(E,"P",{});var Ae=w(A);B=g(Ae,`You can load any tokenizer from the Hugging Face Hub as long as a
`),O=k(Ae,"CODE",{});var Lt=w(O);le=g(Lt,"tokenizer.json"),Lt.forEach(r),Se=g(Ae," file is available in the repository."),Ae.forEach(r),F=v(E),$(Q.$$.fragment,E),K=v(E),D=k(E,"H3",{class:!0});var je=w(D);W=k(je,"A",{id:!0,class:!0,href:!0});var R=w(W);M=k(R,"SPAN",{});var De=w(M);$(ne.$$.fragment,De),De.forEach(r),R.forEach(r),_e=v(je),ke=k(je,"SPAN",{});var Bt=w(ke);He=g(Bt,"Importing a pretrained tokenizer from legacy vocabulary files"),Bt.forEach(r),je.forEach(r),Y=v(E),ie=k(E,"P",{});var Nt=w(ie);se=g(Nt,`You can also import a pretrained tokenizer directly in, as long as you
have its vocabulary file. For instance, here is how to import the
classic pretrained BERT tokenizer:`),Nt.forEach(r),Ce=v(E),$(oe.$$.fragment,E),we=v(E),N=k(E,"P",{});var ce=w(N);xe=g(ce,"as long as you have downloaded the file "),U=k(ce,"CODE",{});var It=w(U);J=g(It,"bert-base-uncased-vocab.txt"),It.forEach(r),qe=g(ce," with"),ce.forEach(r),V=v(E),$(re.$$.fragment,E),this.h()},h(){P(n,"id","using-a-pretrained-tokenizer"),P(n,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),P(n,"href","#using-a-pretrained-tokenizer"),P(t,"class","relative group"),P(W,"id","importing-a-pretrained-tokenizer-from-legacy-vocabulary-files"),P(W,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),P(W,"href","#importing-a-pretrained-tokenizer-from-legacy-vocabulary-files"),P(D,"class","relative group")},m(E,S){q(E,t,S),a(t,n),a(n,e),h(s,e,null),a(t,i),a(t,T),a(T,H),q(E,L,S),q(E,A,S),a(A,B),a(A,O),a(O,le),a(A,Se),q(E,F,S),h(Q,E,S),q(E,K,S),q(E,D,S),a(D,W),a(W,M),h(ne,M,null),a(D,_e),a(D,ke),a(ke,He),q(E,Y,S),q(E,ie,S),a(ie,se),q(E,Ce,S),h(oe,E,S),q(E,we,S),q(E,N,S),a(N,xe),a(N,U),a(U,J),a(N,qe),q(E,V,S),h(re,E,S),ue=!0},p:y,i(E){ue||(p(s.$$.fragment,E),p(Q.$$.fragment,E),p(ne.$$.fragment,E),p(oe.$$.fragment,E),p(re.$$.fragment,E),ue=!0)},o(E){c(s.$$.fragment,E),c(Q.$$.fragment,E),c(ne.$$.fragment,E),c(oe.$$.fragment,E),c(re.$$.fragment,E),ue=!1},d(E){E&&r(t),m(s),E&&r(L),E&&r(A),E&&r(F),m(Q,E),E&&r(K),E&&r(D),m(ne),E&&r(Y),E&&r(ie),E&&r(Ce),m(oe,E),E&&r(we),E&&r(N),E&&r(V),m(re,E)}}}function Fp(l){let t,n;return t=new z({props:{$$slots:{default:[Wp]},$$scope:{ctx:l}}}),{c(){f(t.$$.fragment)},l(e){$(t.$$.fragment,e)},m(e,s){h(t,e,s),n=!0},p(e,s){const i={};s&2&&(i.$$scope={dirty:s,ctx:e}),t.$set(i)},i(e){n||(p(t.$$.fragment,e),n=!0)},o(e){c(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Mp(l){let t,n,e,s,i,T,H,L,A,B,O,le,Se,F,Q,K,D,W,M,ne,_e,ke,He,Y,ie,se,Ce,oe,we,N,xe,U,J,qe,V,re,ue,E,S,ae,Dt,pe,Ae,Lt,je,R,De,Bt,Nt,ce,It,Ao,Vt,Do,ms,fe,Lo,Gt,Bo,No,Xt,Io,Oo,ds,Le,gs,zt,Uo,Zt,Ko,_s,Be,ks,G,Wo,en,Fo,Mo,tn,Yo,Ro,nn,Qo,Jo,ws,Ne,qs,Ie,Vo,sn,Go,Xo,js,Oe,vs,Ue,Zo,on,er,tr,bs,Ke,zs,We,nr,rn,sr,or,ys,Fe,Es,$e,rr,an,ar,lr,ln,ir,ur,Ps,Me,Ts,ve,Ye,un,yt,pr,pn,cr,Ss,Re,fr,cn,$r,hr,Hs,Qe,Cs,he,mr,fn,dr,gr,$n,_r,kr,xs,me,wr,hn,qr,jr,mn,vr,br,As,Je,Ds,Ve,zr,dn,yr,Er,Ls,Ge,Bs,X,Pr,gn,Tr,Sr,_n,Hr,Cr,kn,xr,Ar,Ns,Xe,Is,Ot,Dr,Os,Ze,Us,be,et,wn,Et,Lr,qn,Br,Ks,Z,Nr,jn,Ir,Or,vn,Ur,Kr,bn,Wr,Fr,Ws,ee,Mr,zn,Yr,Rr,yn,Qr,Jr,En,Vr,Gr,Fs,tt,Ms,Ut,Xr,Ys,nt,Rs,de,Zr,Pn,ea,ta,Tn,na,sa,Qs,x,oa,Sn,ra,aa,Hn,la,ia,Cn,ua,pa,xn,ca,fa,An,$a,ha,Dn,ma,da,Ln,ga,_a,Js,Kt,ka,Vs,Wt,wa,Gs,st,Xs,ot,qa,Bn,ja,va,Zs,rt,eo,Ft,ba,to,at,no,lt,za,Nn,ya,Ea,so,ze,it,In,Pt,Pa,On,Ta,oo,ut,Sa,Un,Ha,Ca,ro,pt,ao,ct,xa,Kn,Aa,Da,lo,ft,La,Wn,Ba,Na,io,$t,uo,te,Ia,Fn,Oa,Ua,Mn,Ka,Wa,Yn,Fa,Ma,po,ht,co,ge,Ya,Rn,Ra,Qa,Qn,Ja,Va,fo,mt,$o,dt,Ga,Jn,Xa,Za,ho,gt,mo,ye,_t,Vn,Tt,el,Gn,tl,go,kt,_o;return T=new Te({}),F=new C({props:{python:!0,rust:!1,node:!1,$$slots:{python:[vi]},$$scope:{ctx:l}}}),M=new Te({}),N=new b({props:{code:`wget https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-103-raw-v1.zip
unzip wikitext-103-raw-v1.zip`,highlighted:`wget https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-103-raw-v1.zip
unzip wikitext-103-raw-v1.zip`}}),V=new Te({}),Le=new C({props:{python:!0,rust:!0,node:!0,$$slots:{node:[Ti],rust:[Ei],python:[zi]},$$scope:{ctx:l}}}),Be=new C({props:{python:!0,rust:!0,node:!0,$$slots:{node:[Di],rust:[xi],python:[Hi]},$$scope:{ctx:l}}}),Ne=new qi({props:{$$slots:{default:[Li]},$$scope:{ctx:l}}}),Oe=new C({props:{python:!0,rust:!0,node:!0,$$slots:{node:[Ki],rust:[Oi],python:[Ni]},$$scope:{ctx:l}}}),Ke=new C({props:{python:!0,rust:!0,node:!0,$$slots:{node:[Qi],rust:[Yi],python:[Fi]},$$scope:{ctx:l}}}),Fe=new C({props:{python:!0,rust:!0,node:!0,$$slots:{node:[eu],rust:[Xi],python:[Vi]},$$scope:{ctx:l}}}),Me=new C({props:{python:!0,rust:!0,node:!0,$$slots:{node:[au],rust:[ou],python:[nu]},$$scope:{ctx:l}}}),yt=new Te({}),Qe=new C({props:{python:!0,rust:!0,node:!0,$$slots:{node:[fu],rust:[pu],python:[iu]},$$scope:{ctx:l}}}),Je=new C({props:{python:!0,rust:!0,node:!0,$$slots:{node:[_u],rust:[du],python:[hu]},$$scope:{ctx:l}}}),Ge=new C({props:{python:!0,rust:!0,node:!0,$$slots:{node:[bu],rust:[ju],python:[wu]},$$scope:{ctx:l}}}),Xe=new C({props:{python:!0,rust:!0,node:!0,$$slots:{node:[Su],rust:[Pu],python:[yu]},$$scope:{ctx:l}}}),Ze=new C({props:{python:!0,rust:!0,node:!0,$$slots:{node:[Lu],rust:[Au],python:[Cu]},$$scope:{ctx:l}}}),Et=new Te({}),tt=new C({props:{python:!0,rust:!0,node:!0,$$slots:{node:[Ku],rust:[Ou],python:[Nu]},$$scope:{ctx:l}}}),nt=new C({props:{python:!0,rust:!0,node:!0,$$slots:{node:[Qu],rust:[Yu],python:[Fu]},$$scope:{ctx:l}}}),st=new C({props:{python:!0,rust:!0,node:!0,$$slots:{node:[ep],rust:[Xu],python:[Vu]},$$scope:{ctx:l}}}),rt=new C({props:{python:!0,rust:!0,node:!0,$$slots:{node:[ap],rust:[op],python:[np]},$$scope:{ctx:l}}}),at=new C({props:{python:!0,rust:!0,node:!0,$$slots:{node:[fp],rust:[pp],python:[ip]},$$scope:{ctx:l}}}),Pt=new Te({}),pt=new C({props:{python:!0,rust:!0,node:!0,$$slots:{node:[_p],rust:[dp],python:[hp]},$$scope:{ctx:l}}}),$t=new C({props:{python:!0,rust:!0,node:!0,$$slots:{node:[bp],rust:[jp],python:[wp]},$$scope:{ctx:l}}}),ht=new C({props:{python:!0,rust:!0,node:!0,$$slots:{node:[Sp],rust:[Pp],python:[yp]},$$scope:{ctx:l}}}),mt=new C({props:{python:!0,rust:!0,node:!0,$$slots:{node:[Lp],rust:[Ap],python:[Cp]},$$scope:{ctx:l}}}),gt=new C({props:{python:!0,rust:!0,node:!0,$$slots:{node:[Kp],rust:[Op],python:[Np]},$$scope:{ctx:l}}}),Tt=new Te({}),kt=new C({props:{python:!0,rust:!0,node:!0,$$slots:{python:[Fp]},$$scope:{ctx:l}}}),{c(){t=_("meta"),n=j(),e=_("h1"),s=_("a"),i=_("span"),f(T.$$.fragment),H=j(),L=_("span"),A=d("Quicktour"),B=j(),O=_("p"),le=d(`Let\u2019s have a quick look at the \u{1F917} Tokenizers library features. The
library provides an implementation of today\u2019s most used tokenizers that
is both easy to use and blazing fast.`),Se=j(),f(F.$$.fragment),Q=j(),K=_("h2"),D=_("a"),W=_("span"),f(M.$$.fragment),ne=j(),_e=_("span"),ke=d("Build a tokenizer from scratch"),He=j(),Y=_("p"),ie=d(`To illustrate how fast the \u{1F917} Tokenizers library is, let\u2019s train a new
tokenizer on `),se=_("a"),Ce=d("wikitext-103"),oe=d(`
(516M of text) in just a few seconds. First things first, you will need
to download this dataset and unzip it with:`),we=j(),f(N.$$.fragment),xe=j(),U=_("h3"),J=_("a"),qe=_("span"),f(V.$$.fragment),re=j(),ue=_("span"),E=d("Training the tokenizer"),S=j(),ae=_("p"),Dt=d(`In this tour, we will build and train a Byte-Pair Encoding (BPE)
tokenizer. For more information about the different type of tokenizers,
check out this `),pe=_("a"),Ae=d("guide"),Lt=d(` in
the \u{1F917} Transformers documentation. Here, training the tokenizer means it
will learn merge rules by:`),je=j(),R=_("ul"),De=_("li"),Bt=d(`Start with all the characters present in the training corpus as
tokens.`),Nt=j(),ce=_("li"),It=d("Identify the most common pair of tokens and merge it into one token."),Ao=j(),Vt=_("li"),Do=d(`Repeat until the vocabulary (e.g., the number of tokens) has reached
the size we want.`),ms=j(),fe=_("p"),Lo=d("The main API of the library is the "),Gt=_("code"),Bo=d("class"),No=j(),Xt=_("code"),Io=d("Tokenizer"),Oo=d(`, here is how
we instantiate one with a BPE model:`),ds=j(),f(Le.$$.fragment),gs=j(),zt=_("p"),Uo=d(`To train our tokenizer on the wikitext files, we will need to
instantiate a [trainer]{.title-ref}, in this case a
`),Zt=_("code"),Ko=d("BpeTrainer"),_s=j(),f(Be.$$.fragment),ks=j(),G=_("p"),Wo=d("We can set the training arguments like "),en=_("code"),Fo=d("vocab_size"),Mo=d(" or "),tn=_("code"),Yo=d("min_frequency"),Ro=d(` (here
left at their default values of 30,000 and 0) but the most important
part is to give the `),nn=_("code"),Qo=d("special_tokens"),Jo=d(` we
plan to use later on (they are not used at all during training) so that
they get inserted in the vocabulary.`),ws=j(),f(Ne.$$.fragment),qs=j(),Ie=_("p"),Vo=d(`We could train our tokenizer right now, but it wouldn\u2019t be optimal.
Without a pre-tokenizer that will split our inputs into words, we might
get tokens that overlap several words: for instance we could get an
`),sn=_("code"),Go=d('"it is"'),Xo=d(` token since those two words
often appear next to each other. Using a pre-tokenizer will ensure no
token is bigger than a word returned by the pre-tokenizer. Here we want
to train a subword BPE tokenizer, and we will use the easiest
pre-tokenizer possible by splitting on whitespace.`),js=j(),f(Oe.$$.fragment),vs=j(),Ue=_("p"),Zo=d("Now, we can just call the "),on=_("code"),er=d("Tokenizer.train"),tr=d(" method with any list of files we want to use:"),bs=j(),f(Ke.$$.fragment),zs=j(),We=_("p"),nr=d(`This should only take a few seconds to train our tokenizer on the full
wikitext dataset! To save the tokenizer in one file that contains all
its configuration and vocabulary, just use the
`),rn=_("code"),sr=d("Tokenizer.save"),or=d(" method:"),ys=j(),f(Fe.$$.fragment),Es=j(),$e=_("p"),rr=d(`and you can reload your tokenizer from that file with the
`),an=_("code"),ar=d("Tokenizer.from_file"),lr=j(),ln=_("code"),ir=d("classmethod"),ur=d(":"),Ps=j(),f(Me.$$.fragment),Ts=j(),ve=_("h3"),Ye=_("a"),un=_("span"),f(yt.$$.fragment),pr=j(),pn=_("span"),cr=d("Using the tokenizer"),Ss=j(),Re=_("p"),fr=d(`Now that we have trained a tokenizer, we can use it on any text we want
with the `),cn=_("code"),$r=d("Tokenizer.encode"),hr=d(" method:"),Hs=j(),f(Qe.$$.fragment),Cs=j(),he=_("p"),mr=d(`This applied the full pipeline of the tokenizer on the text, returning
an `),fn=_("code"),dr=d("Encoding"),gr=d(` object. To learn more
about this pipeline, and how to apply (or customize) parts of it, check out `),$n=_("code"),_r=d("this page <pipeline>"),kr=d("."),xs=j(),me=_("p"),wr=d("This "),hn=_("code"),qr=d("Encoding"),jr=d(` object then has all the
attributes you need for your deep learning model (or other). The
`),mn=_("code"),vr=d("tokens"),br=d(` attribute contains the
segmentation of your text in tokens:`),As=j(),f(Je.$$.fragment),Ds=j(),Ve=_("p"),zr=d("Similarly, the "),dn=_("code"),yr=d("ids"),Er=d(` attribute will
contain the index of each of those tokens in the tokenizer\u2019s
vocabulary:`),Ls=j(),f(Ge.$$.fragment),Bs=j(),X=_("p"),Pr=d(`An important feature of the \u{1F917} Tokenizers library is that it comes with
full alignment tracking, meaning you can always get the part of your
original sentence that corresponds to a given token. Those are stored in
the `),gn=_("code"),Tr=d("offsets"),Sr=d(` attribute of our
`),_n=_("code"),Hr=d("Encoding"),Cr=d(` object. For instance, let\u2019s
assume we would want to find back what caused the
`),kn=_("code"),xr=d('"[UNK]"'),Ar=d(` token to appear, which is the
token at index 9 in the list, we can just ask for the offset at the
index:`),Ns=j(),f(Xe.$$.fragment),Is=j(),Ot=_("p"),Dr=d(`and those are the indices that correspond to the emoji in the original
sentence:`),Os=j(),f(Ze.$$.fragment),Us=j(),be=_("h3"),et=_("a"),wn=_("span"),f(Et.$$.fragment),Lr=j(),qn=_("span"),Br=d("Post-processing"),Ks=j(),Z=_("p"),Nr=d(`We might want our tokenizer to automatically add special tokens, like
`),jn=_("code"),Ir=d('"[CLS]"'),Or=d(" or "),vn=_("code"),Ur=d('"[SEP]"'),Kr=d(`. To do this, we use a post-processor.
`),bn=_("code"),Wr=d("TemplateProcessing"),Fr=d(` is the most
commonly used, you just have to specify a template for the processing of
single sentences and pairs of sentences, along with the special tokens
and their IDs.`),Ws=j(),ee=_("p"),Mr=d("When we built our tokenizer, we set "),zn=_("code"),Yr=d('"[CLS]"'),Rr=d(" and "),yn=_("code"),Qr=d('"[SEP]"'),Jr=d(` in positions 1
and 2 of our list of special tokens, so this should be their IDs. To
double-check, we can use the `),En=_("code"),Vr=d("Tokenizer.token_to_id"),Gr=d(" method:"),Fs=j(),f(tt.$$.fragment),Ms=j(),Ut=_("p"),Xr=d(`Here is how we can set the post-processing to give us the traditional
BERT inputs:`),Ys=j(),f(nt.$$.fragment),Rs=j(),de=_("p"),Zr=d(`Let\u2019s go over this snippet of code in more details. First we specify
the template for single sentences: those should have the form
`),Pn=_("code"),ea=d('"[CLS] $A [SEP]"'),ta=d(` where
`),Tn=_("code"),na=d("$A"),sa=d(" represents our sentence."),Qs=j(),x=_("p"),oa=d(`Then, we specify the template for sentence pairs, which should have the
form `),Sn=_("code"),ra=d('"[CLS] $A [SEP] $B [SEP]"'),aa=d(` where
`),Hn=_("code"),la=d("$A"),ia=d(` represents the first sentence and
`),Cn=_("code"),ua=d("$B"),pa=d(` the second one. The
`),xn=_("code"),ca=d(":1"),fa=d(" added in the template represent the "),An=_("code"),$a=d("type IDs"),ha=d(` we want for each part of our input: it defaults
to 0 for everything (which is why we don\u2019t have
`),Dn=_("code"),ma=d("$A:0"),da=d(`) and here we set it to 1 for the
tokens of the second sentence and the last `),Ln=_("code"),ga=d('"[SEP]"'),_a=d(" token."),Js=j(),Kt=_("p"),ka=d(`Lastly, we specify the special tokens we used and their IDs in our
tokenizer\u2019s vocabulary.`),Vs=j(),Wt=_("p"),wa=d(`To check out this worked properly, let\u2019s try to encode the same
sentence as before:`),Gs=j(),f(st.$$.fragment),Xs=j(),ot=_("p"),qa=d(`To check the results on a pair of sentences, we just pass the two
sentences to `),Bn=_("code"),ja=d("Tokenizer.encode"),va=d(":"),Zs=j(),f(rt.$$.fragment),eo=j(),Ft=_("p"),ba=d("You can then check the type IDs attributed to each token is correct with"),to=j(),f(at.$$.fragment),no=j(),lt=_("p"),za=d("If you save your tokenizer with "),Nn=_("code"),ya=d("Tokenizer.save"),Ea=d(", the post-processor will be saved along."),so=j(),ze=_("h3"),it=_("a"),In=_("span"),f(Pt.$$.fragment),Pa=j(),On=_("span"),Ta=d("Encoding multiple sentences in a batch"),oo=j(),ut=_("p"),Sa=d(`To get the full speed of the \u{1F917} Tokenizers library, it\u2019s best to
process your texts by batches by using the
`),Un=_("code"),Ha=d("Tokenizer.encode_batch"),Ca=d(" method:"),ro=j(),f(pt.$$.fragment),ao=j(),ct=_("p"),xa=d("The output is then a list of "),Kn=_("code"),Aa=d("Encoding"),Da=d(`
objects like the ones we saw before. You can process together as many
texts as you like, as long as it fits in memory.`),lo=j(),ft=_("p"),La=d(`To process a batch of sentences pairs, pass two lists to the
`),Wn=_("code"),Ba=d("Tokenizer.encode_batch"),Na=d(` method: the
list of sentences A and the list of sentences B:`),io=j(),f($t.$$.fragment),uo=j(),te=_("p"),Ia=d(`When encoding multiple sentences, you can automatically pad the outputs
to the longest sentence present by using
`),Fn=_("code"),Oa=d("Tokenizer.enable_padding"),Ua=d(`, with the
`),Mn=_("code"),Ka=d("pad_token"),Wa=d(` and its ID (which we can
double-check the id for the padding token with
`),Yn=_("code"),Fa=d("Tokenizer.token_to_id"),Ma=d(" like before):"),po=j(),f(ht.$$.fragment),co=j(),ge=_("p"),Ya=d("We can set the "),Rn=_("code"),Ra=d("direction"),Qa=d(` of the padding
(defaults to the right) or a given `),Qn=_("code"),Ja=d("length"),Va=d(` if we want to pad every sample to that specific number (here
we leave it unset to pad to the size of the longest text).`),fo=j(),f(mt.$$.fragment),$o=j(),dt=_("p"),Ga=d("In this case, the "),Jn=_("code"),Xa=d("attention mask"),Za=d(` generated by the
tokenizer takes the padding into account:`),ho=j(),f(gt.$$.fragment),mo=j(),ye=_("h2"),_t=_("a"),Vn=_("span"),f(Tt.$$.fragment),el=j(),Gn=_("span"),tl=d("Pretrained"),go=j(),f(kt.$$.fragment),this.h()},l(o){const u=gi('[data-svelte="svelte-1phssyn"]',document.head);t=k(u,"META",{name:!0,content:!0}),u.forEach(r),n=v(o),e=k(o,"H1",{class:!0});var St=w(e);s=k(St,"A",{id:!0,class:!0,href:!0});var Xn=w(s);i=k(Xn,"SPAN",{});var Zn=w(i);$(T.$$.fragment,Zn),Zn.forEach(r),Xn.forEach(r),H=v(St),L=k(St,"SPAN",{});var es=w(L);A=g(es,"Quicktour"),es.forEach(r),St.forEach(r),B=v(o),O=k(o,"P",{});var ts=w(O);le=g(ts,`Let\u2019s have a quick look at the \u{1F917} Tokenizers library features. The
library provides an implementation of today\u2019s most used tokenizers that
is both easy to use and blazing fast.`),ts.forEach(r),Se=v(o),$(F.$$.fragment,o),Q=v(o),K=k(o,"H2",{class:!0});var Ht=w(K);D=k(Ht,"A",{id:!0,class:!0,href:!0});var ns=w(D);W=k(ns,"SPAN",{});var ss=w(W);$(M.$$.fragment,ss),ss.forEach(r),ns.forEach(r),ne=v(Ht),_e=k(Ht,"SPAN",{});var os=w(_e);ke=g(os,"Build a tokenizer from scratch"),os.forEach(r),Ht.forEach(r),He=v(o),Y=k(o,"P",{});var Ct=w(Y);ie=g(Ct,`To illustrate how fast the \u{1F917} Tokenizers library is, let\u2019s train a new
tokenizer on `),se=k(Ct,"A",{href:!0,rel:!0});var rs=w(se);Ce=g(rs,"wikitext-103"),rs.forEach(r),oe=g(Ct,`
(516M of text) in just a few seconds. First things first, you will need
to download this dataset and unzip it with:`),Ct.forEach(r),we=v(o),$(N.$$.fragment,o),xe=v(o),U=k(o,"H3",{class:!0});var xt=w(U);J=k(xt,"A",{id:!0,class:!0,href:!0});var as=w(J);qe=k(as,"SPAN",{});var ls=w(qe);$(V.$$.fragment,ls),ls.forEach(r),as.forEach(r),re=v(xt),ue=k(xt,"SPAN",{});var is=w(ue);E=g(is,"Training the tokenizer"),is.forEach(r),xt.forEach(r),S=v(o),ae=k(o,"P",{});var At=w(ae);Dt=g(At,`In this tour, we will build and train a Byte-Pair Encoding (BPE)
tokenizer. For more information about the different type of tokenizers,
check out this `),pe=k(At,"A",{href:!0,rel:!0});var us=w(pe);Ae=g(us,"guide"),us.forEach(r),Lt=g(At,` in
the \u{1F917} Transformers documentation. Here, training the tokenizer means it
will learn merge rules by:`),At.forEach(r),je=v(o),R=k(o,"UL",{});var Ee=w(R);De=k(Ee,"LI",{});var ps=w(De);Bt=g(ps,`Start with all the characters present in the training corpus as
tokens.`),ps.forEach(r),Nt=v(Ee),ce=k(Ee,"LI",{});var cs=w(ce);It=g(cs,"Identify the most common pair of tokens and merge it into one token."),cs.forEach(r),Ao=v(Ee),Vt=k(Ee,"LI",{});var fs=w(Vt);Do=g(fs,`Repeat until the vocabulary (e.g., the number of tokens) has reached
the size we want.`),fs.forEach(r),Ee.forEach(r),ms=v(o),fe=k(o,"P",{});var Pe=w(fe);Lo=g(Pe,"The main API of the library is the "),Gt=k(Pe,"CODE",{});var $s=w(Gt);Bo=g($s,"class"),$s.forEach(r),No=v(Pe),Xt=k(Pe,"CODE",{});var hs=w(Xt);Io=g(hs,"Tokenizer"),hs.forEach(r),Oo=g(Pe,`, here is how
we instantiate one with a BPE model:`),Pe.forEach(r),ds=v(o),$(Le.$$.fragment,o),gs=v(o),zt=k(o,"P",{});var nl=w(zt);Uo=g(nl,`To train our tokenizer on the wikitext files, we will need to
instantiate a [trainer]{.title-ref}, in this case a
`),Zt=k(nl,"CODE",{});var sl=w(Zt);Ko=g(sl,"BpeTrainer"),sl.forEach(r),nl.forEach(r),_s=v(o),$(Be.$$.fragment,o),ks=v(o),G=k(o,"P",{});var wt=w(G);Wo=g(wt,"We can set the training arguments like "),en=k(wt,"CODE",{});var ol=w(en);Fo=g(ol,"vocab_size"),ol.forEach(r),Mo=g(wt," or "),tn=k(wt,"CODE",{});var rl=w(tn);Yo=g(rl,"min_frequency"),rl.forEach(r),Ro=g(wt,` (here
left at their default values of 30,000 and 0) but the most important
part is to give the `),nn=k(wt,"CODE",{});var al=w(nn);Qo=g(al,"special_tokens"),al.forEach(r),Jo=g(wt,` we
plan to use later on (they are not used at all during training) so that
they get inserted in the vocabulary.`),wt.forEach(r),ws=v(o),$(Ne.$$.fragment,o),qs=v(o),Ie=k(o,"P",{});var ko=w(Ie);Vo=g(ko,`We could train our tokenizer right now, but it wouldn\u2019t be optimal.
Without a pre-tokenizer that will split our inputs into words, we might
get tokens that overlap several words: for instance we could get an
`),sn=k(ko,"CODE",{});var ll=w(sn);Go=g(ll,'"it is"'),ll.forEach(r),Xo=g(ko,` token since those two words
often appear next to each other. Using a pre-tokenizer will ensure no
token is bigger than a word returned by the pre-tokenizer. Here we want
to train a subword BPE tokenizer, and we will use the easiest
pre-tokenizer possible by splitting on whitespace.`),ko.forEach(r),js=v(o),$(Oe.$$.fragment,o),vs=v(o),Ue=k(o,"P",{});var wo=w(Ue);Zo=g(wo,"Now, we can just call the "),on=k(wo,"CODE",{});var il=w(on);er=g(il,"Tokenizer.train"),il.forEach(r),tr=g(wo," method with any list of files we want to use:"),wo.forEach(r),bs=v(o),$(Ke.$$.fragment,o),zs=v(o),We=k(o,"P",{});var qo=w(We);nr=g(qo,`This should only take a few seconds to train our tokenizer on the full
wikitext dataset! To save the tokenizer in one file that contains all
its configuration and vocabulary, just use the
`),rn=k(qo,"CODE",{});var ul=w(rn);sr=g(ul,"Tokenizer.save"),ul.forEach(r),or=g(qo," method:"),qo.forEach(r),ys=v(o),$(Fe.$$.fragment,o),Es=v(o),$e=k(o,"P",{});var Mt=w($e);rr=g(Mt,`and you can reload your tokenizer from that file with the
`),an=k(Mt,"CODE",{});var pl=w(an);ar=g(pl,"Tokenizer.from_file"),pl.forEach(r),lr=v(Mt),ln=k(Mt,"CODE",{});var cl=w(ln);ir=g(cl,"classmethod"),cl.forEach(r),ur=g(Mt,":"),Mt.forEach(r),Ps=v(o),$(Me.$$.fragment,o),Ts=v(o),ve=k(o,"H3",{class:!0});var jo=w(ve);Ye=k(jo,"A",{id:!0,class:!0,href:!0});var fl=w(Ye);un=k(fl,"SPAN",{});var $l=w(un);$(yt.$$.fragment,$l),$l.forEach(r),fl.forEach(r),pr=v(jo),pn=k(jo,"SPAN",{});var hl=w(pn);cr=g(hl,"Using the tokenizer"),hl.forEach(r),jo.forEach(r),Ss=v(o),Re=k(o,"P",{});var vo=w(Re);fr=g(vo,`Now that we have trained a tokenizer, we can use it on any text we want
with the `),cn=k(vo,"CODE",{});var ml=w(cn);$r=g(ml,"Tokenizer.encode"),ml.forEach(r),hr=g(vo," method:"),vo.forEach(r),Hs=v(o),$(Qe.$$.fragment,o),Cs=v(o),he=k(o,"P",{});var Yt=w(he);mr=g(Yt,`This applied the full pipeline of the tokenizer on the text, returning
an `),fn=k(Yt,"CODE",{});var dl=w(fn);dr=g(dl,"Encoding"),dl.forEach(r),gr=g(Yt,` object. To learn more
about this pipeline, and how to apply (or customize) parts of it, check out `),$n=k(Yt,"CODE",{});var gl=w($n);_r=g(gl,"this page <pipeline>"),gl.forEach(r),kr=g(Yt,"."),Yt.forEach(r),xs=v(o),me=k(o,"P",{});var Rt=w(me);wr=g(Rt,"This "),hn=k(Rt,"CODE",{});var _l=w(hn);qr=g(_l,"Encoding"),_l.forEach(r),jr=g(Rt,` object then has all the
attributes you need for your deep learning model (or other). The
`),mn=k(Rt,"CODE",{});var kl=w(mn);vr=g(kl,"tokens"),kl.forEach(r),br=g(Rt,` attribute contains the
segmentation of your text in tokens:`),Rt.forEach(r),As=v(o),$(Je.$$.fragment,o),Ds=v(o),Ve=k(o,"P",{});var bo=w(Ve);zr=g(bo,"Similarly, the "),dn=k(bo,"CODE",{});var wl=w(dn);yr=g(wl,"ids"),wl.forEach(r),Er=g(bo,` attribute will
contain the index of each of those tokens in the tokenizer\u2019s
vocabulary:`),bo.forEach(r),Ls=v(o),$(Ge.$$.fragment,o),Bs=v(o),X=k(o,"P",{});var qt=w(X);Pr=g(qt,`An important feature of the \u{1F917} Tokenizers library is that it comes with
full alignment tracking, meaning you can always get the part of your
original sentence that corresponds to a given token. Those are stored in
the `),gn=k(qt,"CODE",{});var ql=w(gn);Tr=g(ql,"offsets"),ql.forEach(r),Sr=g(qt,` attribute of our
`),_n=k(qt,"CODE",{});var jl=w(_n);Hr=g(jl,"Encoding"),jl.forEach(r),Cr=g(qt,` object. For instance, let\u2019s
assume we would want to find back what caused the
`),kn=k(qt,"CODE",{});var vl=w(kn);xr=g(vl,'"[UNK]"'),vl.forEach(r),Ar=g(qt,` token to appear, which is the
token at index 9 in the list, we can just ask for the offset at the
index:`),qt.forEach(r),Ns=v(o),$(Xe.$$.fragment,o),Is=v(o),Ot=k(o,"P",{});var bl=w(Ot);Dr=g(bl,`and those are the indices that correspond to the emoji in the original
sentence:`),bl.forEach(r),Os=v(o),$(Ze.$$.fragment,o),Us=v(o),be=k(o,"H3",{class:!0});var zo=w(be);et=k(zo,"A",{id:!0,class:!0,href:!0});var zl=w(et);wn=k(zl,"SPAN",{});var yl=w(wn);$(Et.$$.fragment,yl),yl.forEach(r),zl.forEach(r),Lr=v(zo),qn=k(zo,"SPAN",{});var El=w(qn);Br=g(El,"Post-processing"),El.forEach(r),zo.forEach(r),Ks=v(o),Z=k(o,"P",{});var jt=w(Z);Nr=g(jt,`We might want our tokenizer to automatically add special tokens, like
`),jn=k(jt,"CODE",{});var Pl=w(jn);Ir=g(Pl,'"[CLS]"'),Pl.forEach(r),Or=g(jt," or "),vn=k(jt,"CODE",{});var Tl=w(vn);Ur=g(Tl,'"[SEP]"'),Tl.forEach(r),Kr=g(jt,`. To do this, we use a post-processor.
`),bn=k(jt,"CODE",{});var Sl=w(bn);Wr=g(Sl,"TemplateProcessing"),Sl.forEach(r),Fr=g(jt,` is the most
commonly used, you just have to specify a template for the processing of
single sentences and pairs of sentences, along with the special tokens
and their IDs.`),jt.forEach(r),Ws=v(o),ee=k(o,"P",{});var vt=w(ee);Mr=g(vt,"When we built our tokenizer, we set "),zn=k(vt,"CODE",{});var Hl=w(zn);Yr=g(Hl,'"[CLS]"'),Hl.forEach(r),Rr=g(vt," and "),yn=k(vt,"CODE",{});var Cl=w(yn);Qr=g(Cl,'"[SEP]"'),Cl.forEach(r),Jr=g(vt,` in positions 1
and 2 of our list of special tokens, so this should be their IDs. To
double-check, we can use the `),En=k(vt,"CODE",{});var xl=w(En);Vr=g(xl,"Tokenizer.token_to_id"),xl.forEach(r),Gr=g(vt," method:"),vt.forEach(r),Fs=v(o),$(tt.$$.fragment,o),Ms=v(o),Ut=k(o,"P",{});var Al=w(Ut);Xr=g(Al,`Here is how we can set the post-processing to give us the traditional
BERT inputs:`),Al.forEach(r),Ys=v(o),$(nt.$$.fragment,o),Rs=v(o),de=k(o,"P",{});var Qt=w(de);Zr=g(Qt,`Let\u2019s go over this snippet of code in more details. First we specify
the template for single sentences: those should have the form
`),Pn=k(Qt,"CODE",{});var Dl=w(Pn);ea=g(Dl,'"[CLS] $A [SEP]"'),Dl.forEach(r),ta=g(Qt,` where
`),Tn=k(Qt,"CODE",{});var Ll=w(Tn);na=g(Ll,"$A"),Ll.forEach(r),sa=g(Qt," represents our sentence."),Qt.forEach(r),Qs=v(o),x=k(o,"P",{});var I=w(x);oa=g(I,`Then, we specify the template for sentence pairs, which should have the
form `),Sn=k(I,"CODE",{});var Bl=w(Sn);ra=g(Bl,'"[CLS] $A [SEP] $B [SEP]"'),Bl.forEach(r),aa=g(I,` where
`),Hn=k(I,"CODE",{});var Nl=w(Hn);la=g(Nl,"$A"),Nl.forEach(r),ia=g(I,` represents the first sentence and
`),Cn=k(I,"CODE",{});var Il=w(Cn);ua=g(Il,"$B"),Il.forEach(r),pa=g(I,` the second one. The
`),xn=k(I,"CODE",{});var Ol=w(xn);ca=g(Ol,":1"),Ol.forEach(r),fa=g(I," added in the template represent the "),An=k(I,"CODE",{});var Ul=w(An);$a=g(Ul,"type IDs"),Ul.forEach(r),ha=g(I,` we want for each part of our input: it defaults
to 0 for everything (which is why we don\u2019t have
`),Dn=k(I,"CODE",{});var Kl=w(Dn);ma=g(Kl,"$A:0"),Kl.forEach(r),da=g(I,`) and here we set it to 1 for the
tokens of the second sentence and the last `),Ln=k(I,"CODE",{});var Wl=w(Ln);ga=g(Wl,'"[SEP]"'),Wl.forEach(r),_a=g(I," token."),I.forEach(r),Js=v(o),Kt=k(o,"P",{});var Fl=w(Kt);ka=g(Fl,`Lastly, we specify the special tokens we used and their IDs in our
tokenizer\u2019s vocabulary.`),Fl.forEach(r),Vs=v(o),Wt=k(o,"P",{});var Ml=w(Wt);wa=g(Ml,`To check out this worked properly, let\u2019s try to encode the same
sentence as before:`),Ml.forEach(r),Gs=v(o),$(st.$$.fragment,o),Xs=v(o),ot=k(o,"P",{});var yo=w(ot);qa=g(yo,`To check the results on a pair of sentences, we just pass the two
sentences to `),Bn=k(yo,"CODE",{});var Yl=w(Bn);ja=g(Yl,"Tokenizer.encode"),Yl.forEach(r),va=g(yo,":"),yo.forEach(r),Zs=v(o),$(rt.$$.fragment,o),eo=v(o),Ft=k(o,"P",{});var Rl=w(Ft);ba=g(Rl,"You can then check the type IDs attributed to each token is correct with"),Rl.forEach(r),to=v(o),$(at.$$.fragment,o),no=v(o),lt=k(o,"P",{});var Eo=w(lt);za=g(Eo,"If you save your tokenizer with "),Nn=k(Eo,"CODE",{});var Ql=w(Nn);ya=g(Ql,"Tokenizer.save"),Ql.forEach(r),Ea=g(Eo,", the post-processor will be saved along."),Eo.forEach(r),so=v(o),ze=k(o,"H3",{class:!0});var Po=w(ze);it=k(Po,"A",{id:!0,class:!0,href:!0});var Jl=w(it);In=k(Jl,"SPAN",{});var Vl=w(In);$(Pt.$$.fragment,Vl),Vl.forEach(r),Jl.forEach(r),Pa=v(Po),On=k(Po,"SPAN",{});var Gl=w(On);Ta=g(Gl,"Encoding multiple sentences in a batch"),Gl.forEach(r),Po.forEach(r),oo=v(o),ut=k(o,"P",{});var To=w(ut);Sa=g(To,`To get the full speed of the \u{1F917} Tokenizers library, it\u2019s best to
process your texts by batches by using the
`),Un=k(To,"CODE",{});var Xl=w(Un);Ha=g(Xl,"Tokenizer.encode_batch"),Xl.forEach(r),Ca=g(To," method:"),To.forEach(r),ro=v(o),$(pt.$$.fragment,o),ao=v(o),ct=k(o,"P",{});var So=w(ct);xa=g(So,"The output is then a list of "),Kn=k(So,"CODE",{});var Zl=w(Kn);Aa=g(Zl,"Encoding"),Zl.forEach(r),Da=g(So,`
objects like the ones we saw before. You can process together as many
texts as you like, as long as it fits in memory.`),So.forEach(r),lo=v(o),ft=k(o,"P",{});var Ho=w(ft);La=g(Ho,`To process a batch of sentences pairs, pass two lists to the
`),Wn=k(Ho,"CODE",{});var ei=w(Wn);Ba=g(ei,"Tokenizer.encode_batch"),ei.forEach(r),Na=g(Ho,` method: the
list of sentences A and the list of sentences B:`),Ho.forEach(r),io=v(o),$($t.$$.fragment,o),uo=v(o),te=k(o,"P",{});var bt=w(te);Ia=g(bt,`When encoding multiple sentences, you can automatically pad the outputs
to the longest sentence present by using
`),Fn=k(bt,"CODE",{});var ti=w(Fn);Oa=g(ti,"Tokenizer.enable_padding"),ti.forEach(r),Ua=g(bt,`, with the
`),Mn=k(bt,"CODE",{});var ni=w(Mn);Ka=g(ni,"pad_token"),ni.forEach(r),Wa=g(bt,` and its ID (which we can
double-check the id for the padding token with
`),Yn=k(bt,"CODE",{});var si=w(Yn);Fa=g(si,"Tokenizer.token_to_id"),si.forEach(r),Ma=g(bt," like before):"),bt.forEach(r),po=v(o),$(ht.$$.fragment,o),co=v(o),ge=k(o,"P",{});var Jt=w(ge);Ya=g(Jt,"We can set the "),Rn=k(Jt,"CODE",{});var oi=w(Rn);Ra=g(oi,"direction"),oi.forEach(r),Qa=g(Jt,` of the padding
(defaults to the right) or a given `),Qn=k(Jt,"CODE",{});var ri=w(Qn);Ja=g(ri,"length"),ri.forEach(r),Va=g(Jt,` if we want to pad every sample to that specific number (here
we leave it unset to pad to the size of the longest text).`),Jt.forEach(r),fo=v(o),$(mt.$$.fragment,o),$o=v(o),dt=k(o,"P",{});var Co=w(dt);Ga=g(Co,"In this case, the "),Jn=k(Co,"CODE",{});var ai=w(Jn);Xa=g(ai,"attention mask"),ai.forEach(r),Za=g(Co,` generated by the
tokenizer takes the padding into account:`),Co.forEach(r),ho=v(o),$(gt.$$.fragment,o),mo=v(o),ye=k(o,"H2",{class:!0});var xo=w(ye);_t=k(xo,"A",{id:!0,class:!0,href:!0});var li=w(_t);Vn=k(li,"SPAN",{});var ii=w(Vn);$(Tt.$$.fragment,ii),ii.forEach(r),li.forEach(r),el=v(xo),Gn=k(xo,"SPAN",{});var ui=w(Gn);tl=g(ui,"Pretrained"),ui.forEach(r),xo.forEach(r),go=v(o),$(kt.$$.fragment,o),this.h()},h(){P(t,"name","hf:doc:metadata"),P(t,"content",JSON.stringify(Yp)),P(s,"id","quicktour"),P(s,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),P(s,"href","#quicktour"),P(e,"class","relative group"),P(D,"id","build-a-tokenizer-from-scratch"),P(D,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),P(D,"href","#build-a-tokenizer-from-scratch"),P(K,"class","relative group"),P(se,"href","https://blog.einstein.ai/the-wikitext-long-term-dependency-language-modeling-dataset/"),P(se,"rel","nofollow"),P(J,"id","training-the-tokenizer"),P(J,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),P(J,"href","#training-the-tokenizer"),P(U,"class","relative group"),P(pe,"href","https://huggingface.co/transformers/tokenizer_summary.html"),P(pe,"rel","nofollow"),P(Ye,"id","using-the-tokenizer"),P(Ye,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),P(Ye,"href","#using-the-tokenizer"),P(ve,"class","relative group"),P(et,"id","postprocessing"),P(et,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),P(et,"href","#postprocessing"),P(be,"class","relative group"),P(it,"id","encoding-multiple-sentences-in-a-batch"),P(it,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),P(it,"href","#encoding-multiple-sentences-in-a-batch"),P(ze,"class","relative group"),P(_t,"id","pretrained"),P(_t,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),P(_t,"href","#pretrained"),P(ye,"class","relative group")},m(o,u){a(document.head,t),q(o,n,u),q(o,e,u),a(e,s),a(s,i),h(T,i,null),a(e,H),a(e,L),a(L,A),q(o,B,u),q(o,O,u),a(O,le),q(o,Se,u),h(F,o,u),q(o,Q,u),q(o,K,u),a(K,D),a(D,W),h(M,W,null),a(K,ne),a(K,_e),a(_e,ke),q(o,He,u),q(o,Y,u),a(Y,ie),a(Y,se),a(se,Ce),a(Y,oe),q(o,we,u),h(N,o,u),q(o,xe,u),q(o,U,u),a(U,J),a(J,qe),h(V,qe,null),a(U,re),a(U,ue),a(ue,E),q(o,S,u),q(o,ae,u),a(ae,Dt),a(ae,pe),a(pe,Ae),a(ae,Lt),q(o,je,u),q(o,R,u),a(R,De),a(De,Bt),a(R,Nt),a(R,ce),a(ce,It),a(R,Ao),a(R,Vt),a(Vt,Do),q(o,ms,u),q(o,fe,u),a(fe,Lo),a(fe,Gt),a(Gt,Bo),a(fe,No),a(fe,Xt),a(Xt,Io),a(fe,Oo),q(o,ds,u),h(Le,o,u),q(o,gs,u),q(o,zt,u),a(zt,Uo),a(zt,Zt),a(Zt,Ko),q(o,_s,u),h(Be,o,u),q(o,ks,u),q(o,G,u),a(G,Wo),a(G,en),a(en,Fo),a(G,Mo),a(G,tn),a(tn,Yo),a(G,Ro),a(G,nn),a(nn,Qo),a(G,Jo),q(o,ws,u),h(Ne,o,u),q(o,qs,u),q(o,Ie,u),a(Ie,Vo),a(Ie,sn),a(sn,Go),a(Ie,Xo),q(o,js,u),h(Oe,o,u),q(o,vs,u),q(o,Ue,u),a(Ue,Zo),a(Ue,on),a(on,er),a(Ue,tr),q(o,bs,u),h(Ke,o,u),q(o,zs,u),q(o,We,u),a(We,nr),a(We,rn),a(rn,sr),a(We,or),q(o,ys,u),h(Fe,o,u),q(o,Es,u),q(o,$e,u),a($e,rr),a($e,an),a(an,ar),a($e,lr),a($e,ln),a(ln,ir),a($e,ur),q(o,Ps,u),h(Me,o,u),q(o,Ts,u),q(o,ve,u),a(ve,Ye),a(Ye,un),h(yt,un,null),a(ve,pr),a(ve,pn),a(pn,cr),q(o,Ss,u),q(o,Re,u),a(Re,fr),a(Re,cn),a(cn,$r),a(Re,hr),q(o,Hs,u),h(Qe,o,u),q(o,Cs,u),q(o,he,u),a(he,mr),a(he,fn),a(fn,dr),a(he,gr),a(he,$n),a($n,_r),a(he,kr),q(o,xs,u),q(o,me,u),a(me,wr),a(me,hn),a(hn,qr),a(me,jr),a(me,mn),a(mn,vr),a(me,br),q(o,As,u),h(Je,o,u),q(o,Ds,u),q(o,Ve,u),a(Ve,zr),a(Ve,dn),a(dn,yr),a(Ve,Er),q(o,Ls,u),h(Ge,o,u),q(o,Bs,u),q(o,X,u),a(X,Pr),a(X,gn),a(gn,Tr),a(X,Sr),a(X,_n),a(_n,Hr),a(X,Cr),a(X,kn),a(kn,xr),a(X,Ar),q(o,Ns,u),h(Xe,o,u),q(o,Is,u),q(o,Ot,u),a(Ot,Dr),q(o,Os,u),h(Ze,o,u),q(o,Us,u),q(o,be,u),a(be,et),a(et,wn),h(Et,wn,null),a(be,Lr),a(be,qn),a(qn,Br),q(o,Ks,u),q(o,Z,u),a(Z,Nr),a(Z,jn),a(jn,Ir),a(Z,Or),a(Z,vn),a(vn,Ur),a(Z,Kr),a(Z,bn),a(bn,Wr),a(Z,Fr),q(o,Ws,u),q(o,ee,u),a(ee,Mr),a(ee,zn),a(zn,Yr),a(ee,Rr),a(ee,yn),a(yn,Qr),a(ee,Jr),a(ee,En),a(En,Vr),a(ee,Gr),q(o,Fs,u),h(tt,o,u),q(o,Ms,u),q(o,Ut,u),a(Ut,Xr),q(o,Ys,u),h(nt,o,u),q(o,Rs,u),q(o,de,u),a(de,Zr),a(de,Pn),a(Pn,ea),a(de,ta),a(de,Tn),a(Tn,na),a(de,sa),q(o,Qs,u),q(o,x,u),a(x,oa),a(x,Sn),a(Sn,ra),a(x,aa),a(x,Hn),a(Hn,la),a(x,ia),a(x,Cn),a(Cn,ua),a(x,pa),a(x,xn),a(xn,ca),a(x,fa),a(x,An),a(An,$a),a(x,ha),a(x,Dn),a(Dn,ma),a(x,da),a(x,Ln),a(Ln,ga),a(x,_a),q(o,Js,u),q(o,Kt,u),a(Kt,ka),q(o,Vs,u),q(o,Wt,u),a(Wt,wa),q(o,Gs,u),h(st,o,u),q(o,Xs,u),q(o,ot,u),a(ot,qa),a(ot,Bn),a(Bn,ja),a(ot,va),q(o,Zs,u),h(rt,o,u),q(o,eo,u),q(o,Ft,u),a(Ft,ba),q(o,to,u),h(at,o,u),q(o,no,u),q(o,lt,u),a(lt,za),a(lt,Nn),a(Nn,ya),a(lt,Ea),q(o,so,u),q(o,ze,u),a(ze,it),a(it,In),h(Pt,In,null),a(ze,Pa),a(ze,On),a(On,Ta),q(o,oo,u),q(o,ut,u),a(ut,Sa),a(ut,Un),a(Un,Ha),a(ut,Ca),q(o,ro,u),h(pt,o,u),q(o,ao,u),q(o,ct,u),a(ct,xa),a(ct,Kn),a(Kn,Aa),a(ct,Da),q(o,lo,u),q(o,ft,u),a(ft,La),a(ft,Wn),a(Wn,Ba),a(ft,Na),q(o,io,u),h($t,o,u),q(o,uo,u),q(o,te,u),a(te,Ia),a(te,Fn),a(Fn,Oa),a(te,Ua),a(te,Mn),a(Mn,Ka),a(te,Wa),a(te,Yn),a(Yn,Fa),a(te,Ma),q(o,po,u),h(ht,o,u),q(o,co,u),q(o,ge,u),a(ge,Ya),a(ge,Rn),a(Rn,Ra),a(ge,Qa),a(ge,Qn),a(Qn,Ja),a(ge,Va),q(o,fo,u),h(mt,o,u),q(o,$o,u),q(o,dt,u),a(dt,Ga),a(dt,Jn),a(Jn,Xa),a(dt,Za),q(o,ho,u),h(gt,o,u),q(o,mo,u),q(o,ye,u),a(ye,_t),a(_t,Vn),h(Tt,Vn,null),a(ye,el),a(ye,Gn),a(Gn,tl),q(o,go,u),h(kt,o,u),_o=!0},p(o,[u]){const St={};u&2&&(St.$$scope={dirty:u,ctx:o}),F.$set(St);const Xn={};u&2&&(Xn.$$scope={dirty:u,ctx:o}),Le.$set(Xn);const Zn={};u&2&&(Zn.$$scope={dirty:u,ctx:o}),Be.$set(Zn);const es={};u&2&&(es.$$scope={dirty:u,ctx:o}),Ne.$set(es);const ts={};u&2&&(ts.$$scope={dirty:u,ctx:o}),Oe.$set(ts);const Ht={};u&2&&(Ht.$$scope={dirty:u,ctx:o}),Ke.$set(Ht);const ns={};u&2&&(ns.$$scope={dirty:u,ctx:o}),Fe.$set(ns);const ss={};u&2&&(ss.$$scope={dirty:u,ctx:o}),Me.$set(ss);const os={};u&2&&(os.$$scope={dirty:u,ctx:o}),Qe.$set(os);const Ct={};u&2&&(Ct.$$scope={dirty:u,ctx:o}),Je.$set(Ct);const rs={};u&2&&(rs.$$scope={dirty:u,ctx:o}),Ge.$set(rs);const xt={};u&2&&(xt.$$scope={dirty:u,ctx:o}),Xe.$set(xt);const as={};u&2&&(as.$$scope={dirty:u,ctx:o}),Ze.$set(as);const ls={};u&2&&(ls.$$scope={dirty:u,ctx:o}),tt.$set(ls);const is={};u&2&&(is.$$scope={dirty:u,ctx:o}),nt.$set(is);const At={};u&2&&(At.$$scope={dirty:u,ctx:o}),st.$set(At);const us={};u&2&&(us.$$scope={dirty:u,ctx:o}),rt.$set(us);const Ee={};u&2&&(Ee.$$scope={dirty:u,ctx:o}),at.$set(Ee);const ps={};u&2&&(ps.$$scope={dirty:u,ctx:o}),pt.$set(ps);const cs={};u&2&&(cs.$$scope={dirty:u,ctx:o}),$t.$set(cs);const fs={};u&2&&(fs.$$scope={dirty:u,ctx:o}),ht.$set(fs);const Pe={};u&2&&(Pe.$$scope={dirty:u,ctx:o}),mt.$set(Pe);const $s={};u&2&&($s.$$scope={dirty:u,ctx:o}),gt.$set($s);const hs={};u&2&&(hs.$$scope={dirty:u,ctx:o}),kt.$set(hs)},i(o){_o||(p(T.$$.fragment,o),p(F.$$.fragment,o),p(M.$$.fragment,o),p(N.$$.fragment,o),p(V.$$.fragment,o),p(Le.$$.fragment,o),p(Be.$$.fragment,o),p(Ne.$$.fragment,o),p(Oe.$$.fragment,o),p(Ke.$$.fragment,o),p(Fe.$$.fragment,o),p(Me.$$.fragment,o),p(yt.$$.fragment,o),p(Qe.$$.fragment,o),p(Je.$$.fragment,o),p(Ge.$$.fragment,o),p(Xe.$$.fragment,o),p(Ze.$$.fragment,o),p(Et.$$.fragment,o),p(tt.$$.fragment,o),p(nt.$$.fragment,o),p(st.$$.fragment,o),p(rt.$$.fragment,o),p(at.$$.fragment,o),p(Pt.$$.fragment,o),p(pt.$$.fragment,o),p($t.$$.fragment,o),p(ht.$$.fragment,o),p(mt.$$.fragment,o),p(gt.$$.fragment,o),p(Tt.$$.fragment,o),p(kt.$$.fragment,o),_o=!0)},o(o){c(T.$$.fragment,o),c(F.$$.fragment,o),c(M.$$.fragment,o),c(N.$$.fragment,o),c(V.$$.fragment,o),c(Le.$$.fragment,o),c(Be.$$.fragment,o),c(Ne.$$.fragment,o),c(Oe.$$.fragment,o),c(Ke.$$.fragment,o),c(Fe.$$.fragment,o),c(Me.$$.fragment,o),c(yt.$$.fragment,o),c(Qe.$$.fragment,o),c(Je.$$.fragment,o),c(Ge.$$.fragment,o),c(Xe.$$.fragment,o),c(Ze.$$.fragment,o),c(Et.$$.fragment,o),c(tt.$$.fragment,o),c(nt.$$.fragment,o),c(st.$$.fragment,o),c(rt.$$.fragment,o),c(at.$$.fragment,o),c(Pt.$$.fragment,o),c(pt.$$.fragment,o),c($t.$$.fragment,o),c(ht.$$.fragment,o),c(mt.$$.fragment,o),c(gt.$$.fragment,o),c(Tt.$$.fragment,o),c(kt.$$.fragment,o),_o=!1},d(o){r(t),o&&r(n),o&&r(e),m(T),o&&r(B),o&&r(O),o&&r(Se),m(F,o),o&&r(Q),o&&r(K),m(M),o&&r(He),o&&r(Y),o&&r(we),m(N,o),o&&r(xe),o&&r(U),m(V),o&&r(S),o&&r(ae),o&&r(je),o&&r(R),o&&r(ms),o&&r(fe),o&&r(ds),m(Le,o),o&&r(gs),o&&r(zt),o&&r(_s),m(Be,o),o&&r(ks),o&&r(G),o&&r(ws),m(Ne,o),o&&r(qs),o&&r(Ie),o&&r(js),m(Oe,o),o&&r(vs),o&&r(Ue),o&&r(bs),m(Ke,o),o&&r(zs),o&&r(We),o&&r(ys),m(Fe,o),o&&r(Es),o&&r($e),o&&r(Ps),m(Me,o),o&&r(Ts),o&&r(ve),m(yt),o&&r(Ss),o&&r(Re),o&&r(Hs),m(Qe,o),o&&r(Cs),o&&r(he),o&&r(xs),o&&r(me),o&&r(As),m(Je,o),o&&r(Ds),o&&r(Ve),o&&r(Ls),m(Ge,o),o&&r(Bs),o&&r(X),o&&r(Ns),m(Xe,o),o&&r(Is),o&&r(Ot),o&&r(Os),m(Ze,o),o&&r(Us),o&&r(be),m(Et),o&&r(Ks),o&&r(Z),o&&r(Ws),o&&r(ee),o&&r(Fs),m(tt,o),o&&r(Ms),o&&r(Ut),o&&r(Ys),m(nt,o),o&&r(Rs),o&&r(de),o&&r(Qs),o&&r(x),o&&r(Js),o&&r(Kt),o&&r(Vs),o&&r(Wt),o&&r(Gs),m(st,o),o&&r(Xs),o&&r(ot),o&&r(Zs),m(rt,o),o&&r(eo),o&&r(Ft),o&&r(to),m(at,o),o&&r(no),o&&r(lt),o&&r(so),o&&r(ze),m(Pt),o&&r(oo),o&&r(ut),o&&r(ro),m(pt,o),o&&r(ao),o&&r(ct),o&&r(lo),o&&r(ft),o&&r(io),m($t,o),o&&r(uo),o&&r(te),o&&r(po),m(ht,o),o&&r(co),o&&r(ge),o&&r(fo),m(mt,o),o&&r($o),o&&r(dt),o&&r(ho),m(gt,o),o&&r(mo),o&&r(ye),m(Tt),o&&r(go),m(kt,o)}}}const Yp={local:"quicktour",sections:[{local:"build-a-tokenizer-from-scratch",sections:[{local:"training-the-tokenizer",title:"Training the tokenizer"},{local:"using-the-tokenizer",title:"Using the tokenizer"},{local:"postprocessing",title:"Post-processing"},{local:"encoding-multiple-sentences-in-a-batch",title:"Encoding multiple sentences in a batch"}],title:"Build a tokenizer from scratch"},{local:"pretrained",sections:[{local:"using-a-pretrained-tokenizer",title:"Using a pretrained tokenizer"},{local:"importing-a-pretrained-tokenizer-from-legacy-vocabulary-files",title:"Importing a pretrained tokenizer from legacy vocabulary files"}],title:"Pretrained"}],title:"Quicktour"};function Rp(l){return _i(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Xp extends pi{constructor(t){super();ci(this,t,Rp,Mp,fi,{})}}export{Xp as default,Yp as metadata};
