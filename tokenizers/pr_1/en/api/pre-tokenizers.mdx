<script lang="ts">
import {onMount} from "svelte";
import Tip from "$lib/Tip.svelte";
import Youtube from "$lib/Youtube.svelte";
import Docstring from "$lib/Docstring.svelte";
import CodeBlock from "$lib/CodeBlock.svelte";
import CodeBlockFw from "$lib/CodeBlockFw.svelte";
import DocNotebookDropdown from "$lib/DocNotebookDropdown.svelte";
import IconCopyLink from "$lib/IconCopyLink.svelte";
import FrameworkContent from "$lib/FrameworkContent.svelte";
import Markdown from "$lib/Markdown.svelte";
import Question from "$lib/Question.svelte";
import FrameworkSwitchCourse from "$lib/FrameworkSwitchCourse.svelte";
import InferenceApi from "$lib/InferenceApi.svelte";
import TokenizersLanguageContent from "$lib/TokenizersLanguageContent.svelte";
let fw: "pt" | "tf" = "pt";
onMount(() => {
    const urlParams = new URLSearchParams(window.location.search);
    fw = urlParams.get("fw") || "pt";
});
</script>
<svelte:head>
  <meta name="hf:doc:metadata" content={JSON.stringify(metadata)} >
</svelte:head>
# Pre-tokenizers

<tokenizerslangcontent>
<python>
## BertPreTokenizer[[tokenizers.pre_tokenizers.BertPreTokenizer]][[tokenizers.pre_tokenizers.BertPreTokenizer]]

<div class="docstring">

<docstring><name>class tokenizers.pre_tokenizers.BertPreTokenizer</name><anchor>tokenizers.pre_tokenizers.BertPreTokenizer</anchor><parameters>[]</parameters></docstring>
BertPreTokenizer

This pre-tokenizer splits tokens on spaces, and also on punctuation.
Each occurence of a punctuation character will be treated separately.

</div>

## ByteLevel[[tokenizers.pre_tokenizers.ByteLevel]][[tokenizers.pre_tokenizers.ByteLevel]]

<div class="docstring">

<docstring><name>class tokenizers.pre_tokenizers.ByteLevel</name><anchor>tokenizers.pre_tokenizers.ByteLevel</anchor><parameters>[{"name": "add_prefix_space", "val": " = True"}]</parameters><paramsdesc>- **add_prefix_space** (`bool`, *optional*, defaults to `True`) --
  Whether to add a space to the first word if there isn't already one. This
  lets us treat *hello* exactly like *say hello*.</paramsdesc><paramgroups>0</paramgroups></docstring>
ByteLevel PreTokenizer

This pre-tokenizer takes care of replacing all bytes of the given string
with a corresponding representation, as well as splitting into words.





<div class="docstring">
<docstring><name>alphabet</name><anchor>tokenizers.pre_tokenizers.ByteLevel.alphabet</anchor><parameters>[]</parameters><rettype>`List[str]`</rettype><retdesc>A list of characters that compose the alphabet</retdesc></docstring>
Returns the alphabet used by this PreTokenizer.

Since the ByteLevel works as its name suggests, at the byte level, it
encodes each byte value to a unique visible character. This means that there is a
total of 256 different characters composing this alphabet.






</div></div>

## CharDelimiterSplit[[tokenizers.pre_tokenizers.CharDelimiterSplit]][[tokenizers.pre_tokenizers.CharDelimiterSplit]]

<div class="docstring">

<docstring><name>class tokenizers.pre_tokenizers.CharDelimiterSplit</name><anchor>tokenizers.pre_tokenizers.CharDelimiterSplit</anchor><parameters>""</parameters><paramsdesc>delimiter -- str:
The delimiter char that will be used to split input</paramsdesc><paramgroups>0</paramgroups></docstring>
This pre-tokenizer simply splits on the provided char. Works like `.split(delimiter)`




</div>

## Digits[[tokenizers.pre_tokenizers.Digits]][[tokenizers.pre_tokenizers.Digits]]

<div class="docstring">

<docstring><name>class tokenizers.pre_tokenizers.Digits</name><anchor>tokenizers.pre_tokenizers.Digits</anchor><parameters>[{"name": "individual_digits", "val": " = False"}]</parameters><paramsdesc>- **individual_digits** (`bool`, *optional*, defaults to `False`) --</paramsdesc><paramgroups>0</paramgroups></docstring>
This pre-tokenizer simply splits using the digits in separate tokens

If set to True, digits will each be separated as follows:

```python
"Call 123 please" -> "Call ", "1", "2", "3", " please"
```

If set to False, digits will grouped as follows:

```python
"Call 123 please" -> "Call ", "123", " please"
```




</div>

## Metaspace[[tokenizers.pre_tokenizers.Metaspace]][[tokenizers.pre_tokenizers.Metaspace]]

<div class="docstring">

<docstring><name>class tokenizers.pre_tokenizers.Metaspace</name><anchor>tokenizers.pre_tokenizers.Metaspace</anchor><parameters>[{"name": "replacement", "val": " = '_'"}, {"name": "add_prefix_space", "val": " = True"}]</parameters><paramsdesc>- **replacement** (`str`, *optional*, defaults to `▁`) --
  The replacement character. Must be exactly one character. By default we
  use the *▁* (U+2581) meta symbol (Same as in SentencePiece).

- **add_prefix_space** (`bool`, *optional*, defaults to `True`) --
  Whether to add a space to the first word if there isn't already one. This
  lets us treat *hello* exactly like *say hello*.</paramsdesc><paramgroups>0</paramgroups></docstring>
Metaspace pre-tokenizer

This pre-tokenizer replaces any whitespace by the provided replacement character.
It then tries to split on these spaces.




</div>

## PreTokenizer[[tokenizers.pre_tokenizers.PreTokenizer]][[tokenizers.pre_tokenizers.PreTokenizer]]

<div class="docstring">

<docstring><name>class tokenizers.pre_tokenizers.PreTokenizer</name><anchor>tokenizers.pre_tokenizers.PreTokenizer</anchor><parameters>[]</parameters></docstring>
Base class for all pre-tokenizers

This class is not supposed to be instantiated directly. Instead, any implementation of a
PreTokenizer will return an instance of this class when instantiated.


<div class="docstring">
<docstring><name>pre_tokenize</name><anchor>tokenizers.pre_tokenizers.PreTokenizer.pre_tokenize</anchor><parameters>[{"name": "pretok", "val": ""}]</parameters><paramsdesc>- **pretok** ([`~tokenizers.PreTokenizedString) --
  The pre-tokenized string on which to apply this
  :class:`]~tokenizers.pre_tokenizers.PreTokenizer`</paramsdesc><paramgroups>0</paramgroups></docstring>
Pre-tokenize a [`~tokenizers.PyPreTokenizedString`] in-place

This method allows to modify a [`~tokenizers.PreTokenizedString`] to
keep track of the pre-tokenization, and leverage the capabilities of the
[`~tokenizers.PreTokenizedString`]. If you just want to see the result of
the pre-tokenization of a raw string, you can use
[`~tokenizers.pre_tokenizers.PreTokenizer.pre_tokenize_str`]




</div>
<div class="docstring">
<docstring><name>pre_tokenize_str</name><anchor>tokenizers.pre_tokenizers.PreTokenizer.pre_tokenize_str</anchor><parameters>[{"name": "sequence", "val": ""}]</parameters><paramsdesc>- **sequence** (`str`) --
  A string to pre-tokeize</paramsdesc><paramgroups>0</paramgroups><rettype>`List[Tuple[str, Offsets]]`</rettype><retdesc>A list of tuple with the pre-tokenized parts and their offsets</retdesc></docstring>
Pre tokenize the given string

This method provides a way to visualize the effect of a
[`~tokenizers.pre_tokenizers.PreTokenizer`] but it does not keep track of the
alignment, nor does it provide all the capabilities of the
[`~tokenizers.PreTokenizedString`]. If you need some of these, you can use
[`~tokenizers.pre_tokenizers.PreTokenizer.pre_tokenize`]








</div></div>

## Punctuation[[tokenizers.pre_tokenizers.Punctuation]][[tokenizers.pre_tokenizers.Punctuation]]

<div class="docstring">

<docstring><name>class tokenizers.pre_tokenizers.Punctuation</name><anchor>tokenizers.pre_tokenizers.Punctuation</anchor><parameters>[{"name": "behavior", "val": " = 'isolated'"}]</parameters><paramsdesc>- **behavior** ([`~tokenizers.SplitDelimiterBehavior`]) --
  The behavior to use when splitting.
  Choices: "removed", "isolated" (default), "merged_with_previous", "merged_with_next",
  "contiguous"</paramsdesc><paramgroups>0</paramgroups></docstring>
This pre-tokenizer simply splits on punctuation as individual characters.




</div>

## Sequence[[tokenizers.pre_tokenizers.Sequence]][[tokenizers.pre_tokenizers.Sequence]]

<div class="docstring">

<docstring><name>class tokenizers.pre_tokenizers.Sequence</name><anchor>tokenizers.pre_tokenizers.Sequence</anchor><parameters>[{"name": "pretokenizers", "val": ""}]</parameters></docstring>
This pre-tokenizer composes other pre_tokenizers and applies them in sequence

</div>

## Split[[tokenizers.pre_tokenizers.Split]][[tokenizers.pre_tokenizers.Split]]

<div class="docstring">

<docstring><name>class tokenizers.pre_tokenizers.Split</name><anchor>tokenizers.pre_tokenizers.Split</anchor><parameters>[{"name": "pattern", "val": ""}, {"name": "behavior", "val": ""}, {"name": "invert", "val": " = False"}]</parameters><paramsdesc>- **pattern** (`str` or [`~tokenizers.Regex`]) --
  A pattern used to split the string. Usually a string or a Regex

- **behavior** ([`~tokenizers.SplitDelimiterBehavior`]) --
  The behavior to use when splitting.
  Choices: "removed", "isolated", "merged_with_previous", "merged_with_next",
  "contiguous"

- **invert** (`bool`, *optional*, defaults to `False`) --
  Whether to invert the pattern.</paramsdesc><paramgroups>0</paramgroups></docstring>
Split PreTokenizer

This versatile pre-tokenizer splits using the provided pattern and
according to the provided behavior. The pattern can be inverted by
making use of the invert flag.




</div>

## UnicodeScripts[[tokenizers.pre_tokenizers.UnicodeScripts]][[tokenizers.pre_tokenizers.UnicodeScripts]]

<div class="docstring">

<docstring><name>class tokenizers.pre_tokenizers.UnicodeScripts</name><anchor>tokenizers.pre_tokenizers.UnicodeScripts</anchor><parameters>[]</parameters></docstring>
This pre-tokenizer splits on characters that belong to different language family
It roughly follows https://github.com/google/sentencepiece/blob/master/data/Scripts.txt
Actually Hiragana and Katakana are fused with Han, and 0x30FC is Han too.
This mimicks SentencePiece Unigram implementation.

</div>

## Whitespace[[tokenizers.pre_tokenizers.Whitespace]][[tokenizers.pre_tokenizers.Whitespace]]

<div class="docstring">

<docstring><name>class tokenizers.pre_tokenizers.Whitespace</name><anchor>tokenizers.pre_tokenizers.Whitespace</anchor><parameters>[]</parameters></docstring>
This pre-tokenizer simply splits using the following regex: `\w+|[^\w\s]+`

</div>

## WhitespaceSplit[[tokenizers.pre_tokenizers.WhitespaceSplit]][[tokenizers.pre_tokenizers.WhitespaceSplit]]

<div class="docstring">

<docstring><name>class tokenizers.pre_tokenizers.WhitespaceSplit</name><anchor>tokenizers.pre_tokenizers.WhitespaceSplit</anchor><parameters>[]</parameters></docstring>
This pre-tokenizer simply splits on the whitespace. Works like `.split()`

</div>

</python>
<rust>
The Rust API Reference is available directly on the [Docs.rs](https://docs.rs/tokenizers/latest/tokenizers/) website.
</rust>
<node>
The node API has not been documented yet.
</node>
</tokenizerslangcontent>