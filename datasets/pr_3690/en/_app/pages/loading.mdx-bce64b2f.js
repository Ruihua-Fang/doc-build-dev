import{S as yu,i as wu,s as ju,e as l,k as d,w as u,t as r,M as bu,c as o,d as a,m as f,a as n,x as m,h as i,b as c,F as t,g as p,y as g,q as _,o as v,B as $}from"../chunks/vendor-e67aec41.js";import{T as Ee}from"../chunks/Tip-76459d1c.js";import{I as P}from"../chunks/IconCopyLink-ffd7f84e.js";import{C as b}from"../chunks/CodeBlock-e2bcf023.js";function xu(I){let h,k,y,x,E;return{c(){h=l("p"),k=r("Refer to the "),y=l("a"),x=r("upload_dataset_repo"),E=r(" guide for more instructions on how to create a dataset repository on the Hub, and how to upload your data files."),this.h()},l(w){h=o(w,"P",{});var j=n(h);k=i(j,"Refer to the "),y=o(j,"A",{href:!0});var q=n(y);x=i(q,"upload_dataset_repo"),q.forEach(a),E=i(j," guide for more instructions on how to create a dataset repository on the Hub, and how to upload your data files."),j.forEach(a),this.h()},h(){c(y,"href","#upload_dataset_repo")},m(w,j){p(w,h,j),t(h,k),t(h,y),t(y,x),t(h,E)},d(w){w&&a(h)}}}function ku(I){let h,k,y,x,E;return{c(){h=l("p"),k=r("If you don\u2019t specify which data files to use, "),y=l("code"),x=r("load_dataset"),E=r(" will return all the data files. This can take a long time if you are loading a large dataset like C4, which is approximately 13TB of data.")},l(w){h=o(w,"P",{});var j=n(h);k=i(j,"If you don\u2019t specify which data files to use, "),y=o(j,"CODE",{});var q=n(y);x=i(q,"load_dataset"),q.forEach(a),E=i(j," will return all the data files. This can take a long time if you are loading a large dataset like C4, which is approximately 13TB of data."),j.forEach(a)},m(w,j){p(w,h,j),t(h,k),t(h,y),t(y,x),t(h,E)},d(w){w&&a(h)}}}function Eu(I){let h,k,y,x,E,w,j,q,ss,js,R,as,bs,z,M,xs,A;return{c(){h=l("p"),k=r("An object data type in "),y=l("a"),x=r("pandas.Series"),E=r(" doesn\u2019t always carry enough information for Arrow to automatically infer a data type. For example, if a DataFrame is of length 0 or the Series only contains None/nan objects, the type is set to null. Avoid potential errors by constructing an explicit schema with "),w=l("a"),j=r("datasets.Features"),q=r(" using the "),ss=l("code"),js=r("from_dict"),R=r(" or "),as=l("code"),bs=r("from_pandas"),z=r(" methods. See the "),M=l("a"),xs=r("troubleshoot"),A=r(" for more details on how to explicitly specify your own features."),this.h()},l(L){h=o(L,"P",{});var S=n(h);k=i(S,"An object data type in "),y=o(S,"A",{href:!0,rel:!0});var jt=n(y);x=i(jt,"pandas.Series"),jt.forEach(a),E=i(S," doesn\u2019t always carry enough information for Arrow to automatically infer a data type. For example, if a DataFrame is of length 0 or the Series only contains None/nan objects, the type is set to null. Avoid potential errors by constructing an explicit schema with "),w=o(S,"A",{href:!0});var ks=n(w);j=i(ks,"datasets.Features"),ks.forEach(a),q=i(S," using the "),ss=o(S,"CODE",{});var bt=n(ss);js=i(bt,"from_dict"),bt.forEach(a),R=i(S," or "),as=o(S,"CODE",{});var xt=n(as);bs=i(xt,"from_pandas"),xt.forEach(a),z=i(S," methods. See the "),M=o(S,"A",{href:!0});var Es=n(M);xs=i(Es,"troubleshoot"),Es.forEach(a),A=i(S," for more details on how to explicitly specify your own features."),S.forEach(a),this.h()},h(){c(y,"href","https://pandas.pydata.org/docs/reference/api/pandas.Series.html"),c(y,"rel","nofollow"),c(w,"href","/docs/datasets/pr_3690/en/package_reference/main_classes#datasets.Features"),c(M,"href","#troubleshoot")},m(L,S){p(L,h,S),t(h,k),t(h,y),t(y,x),t(h,E),t(h,w),t(w,j),t(h,q),t(h,ss),t(ss,js),t(h,R),t(h,as),t(as,bs),t(h,z),t(h,M),t(M,xs),t(h,A)},d(L){L&&a(h)}}}function qu(I){let h,k,y,x,E;return{c(){h=l("p"),k=r("Using "),y=l("code"),x=r("pct1_dropremainder"),E=r(" rounding may truncate the last examples in a dataset if the number of examples in your dataset don\u2019t divide evenly by 100.")},l(w){h=o(w,"P",{});var j=n(h);k=i(j,"Using "),y=o(j,"CODE",{});var q=n(y);x=i(q,"pct1_dropremainder"),q.forEach(a),E=i(j," rounding may truncate the last examples in a dataset if the number of examples in your dataset don\u2019t divide evenly by 100."),j.forEach(a)},m(w,j){p(w,h,j),t(h,k),t(h,y),t(y,x),t(h,E)},d(w){w&&a(h)}}}function Pu(I){let h,k,y,x,E;return{c(){h=l("p"),k=r("See the "),y=l("a"),x=r("metric_script"),E=r(" guide for more details on how to write your own metric loading script."),this.h()},l(w){h=o(w,"P",{});var j=n(h);k=i(j,"See the "),y=o(j,"A",{href:!0});var q=n(y);x=i(q,"metric_script"),q.forEach(a),E=i(j," guide for more details on how to write your own metric loading script."),j.forEach(a),this.h()},h(){c(y,"href","#metric_script")},m(w,j){p(w,h,j),t(h,k),t(h,y),t(y,x),t(h,E)},d(w){w&&a(h)}}}function Au(I){let h,k,y,x,E;return{c(){h=l("p"),k=r("Once you\u2019ve loaded a metric for distributed usage, you can compute the metric as usual. Behind the scenes, "),y=l("a"),x=r("datasets.Metric.compute()"),E=r(" gathers all the predictions and references from the nodes, and computes the final metric."),this.h()},l(w){h=o(w,"P",{});var j=n(h);k=i(j,"Once you\u2019ve loaded a metric for distributed usage, you can compute the metric as usual. Behind the scenes, "),y=o(j,"A",{href:!0});var q=n(y);x=i(q,"datasets.Metric.compute()"),q.forEach(a),E=i(j," gathers all the predictions and references from the nodes, and computes the final metric."),j.forEach(a),this.h()},h(){c(y,"href","/docs/datasets/pr_3690/en/package_reference/main_classes#datasets.Metric.compute")},m(w,j){p(w,h,j),t(h,k),t(h,y),t(y,x),t(h,E)},d(w){w&&a(h)}}}function Su(I){let h,k,y,x,E,w,j,q,ss,js,R,as,bs,z,M,xs,A,L,S,jt,ks,bt,xt,Es,Yr,Gr,qe,Wr,Qr,Pe,Xr,so,kt,Kr,ao,Et,to,ts,qs,Ae,ca,Zr,Se,si,eo,Ps,ai,Te,ti,ei,lo,U,li,qt,oi,ni,ha,ri,ii,oo,ua,no,Pt,pi,ro,As,di,De,fi,ci,io,ma,po,Ss,fo,T,hi,Ie,ui,mi,Ne,gi,_i,Ce,vi,$i,Oe,yi,wi,He,ji,bi,co,ga,ho,Ts,uo,V,xi,Fe,ki,Ei,_a,qi,Pi,mo,va,go,Ds,Ai,Le,Si,Ti,_o,$a,vo,es,Is,Re,ya,Di,ze,Ii,$o,D,Ni,Me,Ci,Oi,Ue,Hi,Fi,Ve,Li,Ri,Be,zi,Mi,At,Ui,Vi,yo,ls,Ns,Je,wa,Bi,Ye,Ji,wo,St,Yi,jo,ja,bo,Tt,Gi,xo,ba,ko,Dt,Wi,Eo,xa,qo,It,Qi,Po,ka,Ao,Nt,Xi,So,Ea,To,os,Cs,Ge,qa,Ki,We,Zi,Do,Os,sp,Ct,ap,tp,Io,Pa,No,Ot,ep,Co,Aa,Oo,Hs,lp,Qe,op,np,Ho,Sa,Fo,Ht,rp,Lo,Ta,Ro,Ft,ip,zo,ns,Fs,Xe,Da,pp,Ke,dp,Mo,Lt,fp,Uo,Ia,Vo,Rt,cp,Bo,Na,Jo,rs,Ls,Ze,Ca,hp,sl,up,Yo,zt,mp,Go,Oa,Wo,Mt,gp,Qo,Ha,Xo,is,Rs,al,Fa,_p,tl,vp,Ko,Ut,$p,Zo,Vt,yp,sn,La,an,N,wp,el,jp,bp,ll,xp,kp,Bt,Ep,qp,ol,Pp,Ap,tn,Ra,en,Jt,Sp,ln,za,on,C,Tp,nl,Dp,Ip,rl,Np,Cp,il,Op,Hp,nn,ps,zs,pl,Ma,Fp,dl,Lp,rn,Ms,Rp,Yt,zp,Mp,pn,ds,Us,fl,Ua,Up,cl,Vp,dn,Vs,Bp,Gt,Jp,Yp,fn,Va,cn,fs,Bs,hl,Ba,Gp,ul,Wp,hn,Js,Qp,Wt,Xp,Kp,un,Ja,mn,Ys,gn,cs,Gs,ml,Ya,Zp,gl,sd,_n,Qt,ad,vn,B,td,_l,ed,ld,vl,od,nd,$n,hs,Ws,$l,Ga,rd,yl,id,yn,J,pd,Xt,dd,fd,Kt,cd,hd,wn,Y,ud,wl,md,gd,jl,_d,vd,jn,Wa,bn,Qs,$d,bl,yd,wd,xn,Qa,kn,Zt,jd,En,Xa,qn,se,bd,Pn,Ka,An,ae,xd,Sn,Za,Tn,us,Xs,xl,st,kd,kl,Ed,Dn,te,qd,In,at,Nn,Ks,Pd,El,Ad,Sd,Cn,tt,On,Zs,Hn,ee,Fn,ms,sa,ql,et,Td,Pl,Dd,Ln,le,Id,Rn,gs,aa,Al,lt,Nd,Sl,Cd,zn,O,Od,oe,Hd,Fd,Tl,Ld,Rd,Dl,zd,Md,Mn,ta,Ud,ot,Vd,Bd,Un,nt,Vn,_s,ea,Il,rt,Jd,Nl,Yd,Bn,G,Gd,ne,Wd,Qd,it,Xd,Kd,Jn,W,Zd,re,sf,af,ie,tf,ef,Yn,pt,Gn,Q,lf,Cl,of,nf,pe,rf,pf,Wn,dt,Qn,de,df,Xn,ft,Kn,vs,la,Ol,ct,ff,Hl,cf,Zn,fe,hf,sr,ht,ar,oa,tr,$s,na,Fl,ut,uf,Ll,mf,er,ce,gf,lr,mt,or,ys,ra,Rl,gt,_f,zl,vf,nr,he,$f,rr,ue,yf,ir,X,Ml,_t,wf,Ul,jf,bf,xf,Vl,ws,kf,Bl,Ef,qf,Jl,Pf,Af,Sf,Yl,vt,Tf,me,Df,If,pr,$t,dr,ia,fr,pa,Nf,Gl,Cf,Of,cr,yt,hr;return w=new P({}),ca=new P({}),ua=new b({props:{code:`from datasets import load_dataset
dataset = load_dataset('lhoestq/demo1'),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;lhoestq/demo1&#x27;</span>)`}}),ma=new b({props:{code:`dataset = load_dataset(
  "lhoestq/custom_squad",
  revision="main"  # tag name, or branch name, or commit hash
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(
<span class="hljs-meta">&gt;&gt;&gt; </span>  <span class="hljs-string">&quot;lhoestq/custom_squad&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>  revision=<span class="hljs-string">&quot;main&quot;</span>  <span class="hljs-comment"># tag name, or branch name, or commit hash</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>)`}}),Ss=new Ee({props:{$$slots:{default:[xu]},$$scope:{ctx:I}}}),ga=new b({props:{code:`data_files = {"train": "train.csv", "test": "test.csv"}
dataset = load_dataset("namespace/your_dataset_name", data_files=data_files),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>data_files = {<span class="hljs-string">&quot;train&quot;</span>: <span class="hljs-string">&quot;train.csv&quot;</span>, <span class="hljs-string">&quot;test&quot;</span>: <span class="hljs-string">&quot;test.csv&quot;</span>}
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;namespace/your_dataset_name&quot;</span>, data_files=data_files)`}}),Ts=new Ee({props:{warning:"&lcub;true}",$$slots:{default:[ku]},$$scope:{ctx:I}}}),va=new b({props:{code:`from datasets import load_dataset
c4_subset = load_dataset('allenai/c4', data_files='en/c4-train.0000*-of-01024.json.gz'),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>c4_subset = load_dataset(<span class="hljs-string">&#x27;allenai/c4&#x27;</span>, data_files=<span class="hljs-string">&#x27;en/c4-train.0000*-of-01024.json.gz&#x27;</span>)`}}),$a=new b({props:{code:`data_files = {"validation": "en/c4-validation.*.json.gz"}
c4_validation = load_dataset("allenai/c4", data_files=data_files, split="validation"),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>data_files = {<span class="hljs-string">&quot;validation&quot;</span>: <span class="hljs-string">&quot;en/c4-validation.*.json.gz&quot;</span>}
<span class="hljs-meta">&gt;&gt;&gt; </span>c4_validation = load_dataset(<span class="hljs-string">&quot;allenai/c4&quot;</span>, data_files=data_files, split=<span class="hljs-string">&quot;validation&quot;</span>)`}}),ya=new P({}),wa=new P({}),ja=new b({props:{code:`from datasets import load_dataset
dataset = load_dataset('csv', data_files='my_file.csv'),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;csv&#x27;</span>, data_files=<span class="hljs-string">&#x27;my_file.csv&#x27;</span>)`}}),ba=new b({props:{code:"dataset = load_dataset('csv', data_files=['my_file_1.csv', 'my_file_2.csv', 'my_file_3.csv']),",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;csv&#x27;</span>, data_files=[<span class="hljs-string">&#x27;my_file_1.csv&#x27;</span>, <span class="hljs-string">&#x27;my_file_2.csv&#x27;</span>, <span class="hljs-string">&#x27;my_file_3.csv&#x27;</span>])'}}),xa=new b({props:{code:"dataset = load_dataset('csv', data_files={'train': ['my_train_file_1.csv', 'my_train_file_2.csv'] 'test': 'my_test_file.csv'}),",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;csv&#x27;</span>, data_files={<span class="hljs-string">&#x27;train&#x27;</span>: [<span class="hljs-string">&#x27;my_train_file_1.csv&#x27;</span>, <span class="hljs-string">&#x27;my_train_file_2.csv&#x27;</span>] <span class="hljs-string">&#x27;test&#x27;</span>: <span class="hljs-string">&#x27;my_test_file.csv&#x27;</span>})'}}),ka=new b({props:{code:`base_url = "https://huggingface.co/datasets/lhoestq/demo1/resolve/main/data/"
dataset = load_dataset('csv', data_files={'train': base_url + 'train.csv', 'test': base_url + 'test.csv'}),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>base_url = <span class="hljs-string">&quot;https://huggingface.co/datasets/lhoestq/demo1/resolve/main/data/&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;csv&#x27;</span>, data_files={<span class="hljs-string">&#x27;train&#x27;</span>: base_url + <span class="hljs-string">&#x27;train.csv&#x27;</span>, <span class="hljs-string">&#x27;test&#x27;</span>: base_url + <span class="hljs-string">&#x27;test.csv&#x27;</span>})`}}),Ea=new b({props:{code:`url = "https://domain.org/train_data.zip"
data_files = {"train": url}
dataset = load_dataset("csv", data_files=data_files),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;https://domain.org/train_data.zip&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>data_files = {<span class="hljs-string">&quot;train&quot;</span>: url}
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;csv&quot;</span>, data_files=data_files)`}}),qa=new P({}),Pa=new b({props:{code:`from datasets import load_dataset
dataset = load_dataset('json', data_files='my_file.json'),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;json&#x27;</span>, data_files=<span class="hljs-string">&#x27;my_file.json&#x27;</span>)`}}),Aa=new b({props:{code:`{"a": 1, "b": 2.0, "c": "foo", "d": false}
{"a": 4, "b": -5.5, "c": null, "d": true},`,highlighted:`{<span class="hljs-string">&quot;a&quot;</span>: <span class="hljs-number">1</span>, <span class="hljs-string">&quot;b&quot;</span>: <span class="hljs-number">2.0</span>, <span class="hljs-string">&quot;c&quot;</span>: <span class="hljs-string">&quot;foo&quot;</span>, <span class="hljs-string">&quot;d&quot;</span>: false}
{<span class="hljs-string">&quot;a&quot;</span>: <span class="hljs-number">4</span>, <span class="hljs-string">&quot;b&quot;</span>: -<span class="hljs-number">5.5</span>, <span class="hljs-string">&quot;c&quot;</span>: null, <span class="hljs-string">&quot;d&quot;</span>: true}`}}),Sa=new b({props:{code:`{"version": "0.1.0",
    "data": [{"a": 1, "b": 2.0, "c": "foo", "d": false},
            {"a": 4, "b": -5.5, "c": null, "d": true}]
}

>>> from datasets import load_dataset
>>> dataset = load_dataset('json', data_files='my_file.json', field='data'),`,highlighted:`{<span class="hljs-string">&quot;version&quot;</span>: <span class="hljs-string">&quot;0.1.0&quot;</span>,
    <span class="hljs-string">&quot;data&quot;</span>: [{<span class="hljs-string">&quot;a&quot;</span>: <span class="hljs-number">1</span>, <span class="hljs-string">&quot;b&quot;</span>: <span class="hljs-number">2.0</span>, <span class="hljs-string">&quot;c&quot;</span>: <span class="hljs-string">&quot;foo&quot;</span>, <span class="hljs-string">&quot;d&quot;</span>: false},
            {<span class="hljs-string">&quot;a&quot;</span>: <span class="hljs-number">4</span>, <span class="hljs-string">&quot;b&quot;</span>: -<span class="hljs-number">5.5</span>, <span class="hljs-string">&quot;c&quot;</span>: null, <span class="hljs-string">&quot;d&quot;</span>: true}]
}

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;json&#x27;</span>, data_files=<span class="hljs-string">&#x27;my_file.json&#x27;</span>, field=<span class="hljs-string">&#x27;data&#x27;</span>)`}}),Ta=new b({props:{code:`base_url = "https://rajpurkar.github.io/SQuAD-explorer/dataset/"
dataset = load_dataset('json', data_files={'train': base_url + 'train-v1.1.json', 'validation': base_url + 'dev-v1.1.json'}, field="data"),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>base_url = <span class="hljs-string">&quot;https://rajpurkar.github.io/SQuAD-explorer/dataset/&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;json&#x27;</span>, data_files={<span class="hljs-string">&#x27;train&#x27;</span>: base_url + <span class="hljs-string">&#x27;train-v1.1.json&#x27;</span>, <span class="hljs-string">&#x27;validation&#x27;</span>: base_url + <span class="hljs-string">&#x27;dev-v1.1.json&#x27;</span>}, field=<span class="hljs-string">&quot;data&quot;</span>)`}}),Da=new P({}),Ia=new b({props:{code:`from datasets import load_dataset
dataset = load_dataset('text', data_files={'train': ['my_text_1.txt', 'my_text_2.txt'], 'test': 'my_test_file.txt'}),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;text&#x27;</span>, data_files={<span class="hljs-string">&#x27;train&#x27;</span>: [<span class="hljs-string">&#x27;my_text_1.txt&#x27;</span>, <span class="hljs-string">&#x27;my_text_2.txt&#x27;</span>], <span class="hljs-string">&#x27;test&#x27;</span>: <span class="hljs-string">&#x27;my_test_file.txt&#x27;</span>})`}}),Na=new b({props:{code:"dataset = load_dataset('text', data_files='https://huggingface.co/datasets/lhoestq/test/resolve/main/some_text.txt'),",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;text&#x27;</span>, data_files=<span class="hljs-string">&#x27;https://huggingface.co/datasets/lhoestq/test/resolve/main/some_text.txt&#x27;</span>)'}}),Ca=new P({}),Oa=new b({props:{code:`from datasets import load_dataset
dataset = load_dataset("parquet", data_files={'train': 'train.parquet', 'test': 'test.parquet'}),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;parquet&quot;</span>, data_files={<span class="hljs-string">&#x27;train&#x27;</span>: <span class="hljs-string">&#x27;train.parquet&#x27;</span>, <span class="hljs-string">&#x27;test&#x27;</span>: <span class="hljs-string">&#x27;test.parquet&#x27;</span>})`}}),Ha=new b({props:{code:`base_url = "https://storage.googleapis.com/huggingface-nlp/cache/datasets/wikipedia/20200501.en/1.0.0/"
data_files = {"train": base_url + "wikipedia-train.parquet"}
wiki = load_dataset("parquet", data_files=data_files, split="train"),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>base_url = <span class="hljs-string">&quot;https://storage.googleapis.com/huggingface-nlp/cache/datasets/wikipedia/20200501.en/1.0.0/&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>data_files = {<span class="hljs-string">&quot;train&quot;</span>: base_url + <span class="hljs-string">&quot;wikipedia-train.parquet&quot;</span>}
<span class="hljs-meta">&gt;&gt;&gt; </span>wiki = load_dataset(<span class="hljs-string">&quot;parquet&quot;</span>, data_files=data_files, split=<span class="hljs-string">&quot;train&quot;</span>)`}}),Fa=new P({}),La=new b({props:{code:`   data/dog/xxx.png
   data/dog/xxy.png
   data/dog/xxz.png

   data/cat/123.png
   data/cat/nsdf3.png
   data/cat/asd932_.png,`,highlighted:`   data<span class="hljs-regexp">/dog/</span>xxx.png
   data<span class="hljs-regexp">/dog/</span>xxy.png
   data<span class="hljs-regexp">/dog/</span>xxz.png

   data<span class="hljs-regexp">/cat/</span><span class="hljs-number">123</span>.png
   data<span class="hljs-regexp">/cat/</span>nsdf3.png
   data<span class="hljs-regexp">/cat/</span>asd932_.png`}}),Ra=new b({props:{code:`from datasets import load_dataset
dataset = load_dataset("imagefolder", data_dir="/path/to/data"),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;imagefolder&quot;</span>, data_dir=<span class="hljs-string">&quot;/path/to/data&quot;</span>)`}}),za=new b({props:{code:'dataset = load_dataset("imagefolder", data_files="https://download.microsoft.com/download/3/E/1/3E1C3F21-ECDB-4869-8368-6DEBA77B919F/kagglecatsanddogs_3367a.zip", split="train"),',highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;imagefolder&quot;</span>, data_files=<span class="hljs-string">&quot;https://download.microsoft.com/download/3/E/1/3E1C3F21-ECDB-4869-8368-6DEBA77B919F/kagglecatsanddogs_3367a.zip&quot;</span>, split=<span class="hljs-string">&quot;train&quot;</span>)'}}),Ma=new P({}),Ua=new P({}),Va=new b({props:{code:`from datasets import Dataset
my_dict = {"a": [1, 2, 3]}
dataset = Dataset.from_dict(my_dict),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> Dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>my_dict = {<span class="hljs-string">&quot;a&quot;</span>: [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>]}
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = Dataset.from_dict(my_dict)`}}),Ba=new P({}),Ja=new b({props:{code:`from datasets import Dataset
import pandas as pd
df = pd.DataFrame({"a": [1, 2, 3]})
dataset = Dataset.from_pandas(df),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> Dataset
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd
<span class="hljs-meta">&gt;&gt;&gt; </span>df = pd.DataFrame({<span class="hljs-string">&quot;a&quot;</span>: [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>]})
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = Dataset.from_pandas(df)`}}),Ys=new Ee({props:{warning:"&lcub;true}",$$slots:{default:[Eu]},$$scope:{ctx:I}}}),Ya=new P({}),Ga=new P({}),Wa=new b({props:{code:`train_test_ds = datasets.load_dataset('bookcorpus', split='train+test')
ri = datasets.ReadInstruction('train') + datasets.ReadInstruction('test')
train_test_ds = datasets.load_dataset('bookcorpus', split=ri),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>train_test_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=<span class="hljs-string">&#x27;train+test&#x27;</span>)
===STRINGAPI-READINSTRUCTION-SPLIT===
<span class="hljs-meta">&gt;&gt;&gt; </span>ri = datasets.ReadInstruction(<span class="hljs-string">&#x27;train&#x27;</span>) + datasets.ReadInstruction(<span class="hljs-string">&#x27;test&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>train_test_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=ri)`}}),Qa=new b({props:{code:`train_10_20_ds = datasets.load_dataset('bookcorpus', split='train[10:20]')
train_10_20_ds = datasets.load_dataset('bookcorpus', split=datasets.ReadInstruction('train', from_=10, to=20, unit='abs')),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>train_10_20_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=<span class="hljs-string">&#x27;train[10:20]&#x27;</span>)
===STRINGAPI-READINSTRUCTION-SPLIT===
<span class="hljs-meta">&gt;&gt;&gt; </span>train_10_20_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=datasets.ReadInstruction(<span class="hljs-string">&#x27;train&#x27;</span>, from_=<span class="hljs-number">10</span>, to=<span class="hljs-number">20</span>, unit=<span class="hljs-string">&#x27;abs&#x27;</span>))`}}),Xa=new b({props:{code:`train_10pct_ds = datasets.load_dataset('bookcorpus', split='train[:10%]')
train_10_20_ds = datasets.load_dataset('bookcorpus', split=datasets.ReadInstruction('train', to=10, unit='%')),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>train_10pct_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=<span class="hljs-string">&#x27;train[:10%]&#x27;</span>)
===STRINGAPI-READINSTRUCTION-SPLIT===
<span class="hljs-meta">&gt;&gt;&gt; </span>train_10_20_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=datasets.ReadInstruction(<span class="hljs-string">&#x27;train&#x27;</span>, to=<span class="hljs-number">10</span>, unit=<span class="hljs-string">&#x27;%&#x27;</span>))`}}),Ka=new b({props:{code:`train_10_80pct_ds = datasets.load_dataset('bookcorpus', split='train[:10%]+train[-80%:]')
ri = (datasets.ReadInstruction('train', to=10, unit='%') + datasets.ReadInstruction('train', from_=-80, unit='%'))
train_10_80pct_ds = datasets.load_dataset('bookcorpus', split=ri),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>train_10_80pct_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=<span class="hljs-string">&#x27;train[:10%]+train[-80%:]&#x27;</span>)
===STRINGAPI-READINSTRUCTION-SPLIT===
<span class="hljs-meta">&gt;&gt;&gt; </span>ri = (datasets.ReadInstruction(<span class="hljs-string">&#x27;train&#x27;</span>, to=<span class="hljs-number">10</span>, unit=<span class="hljs-string">&#x27;%&#x27;</span>) + datasets.ReadInstruction(<span class="hljs-string">&#x27;train&#x27;</span>, from_=-<span class="hljs-number">80</span>, unit=<span class="hljs-string">&#x27;%&#x27;</span>))
<span class="hljs-meta">&gt;&gt;&gt; </span>train_10_80pct_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=ri)`}}),Za=new b({props:{code:`# 10-fold cross-validation (see also next section on rounding behavior):
# The validation datasets are each going to be 10%:
# [0%:10%], [10%:20%], ..., [90%:100%].
# And the training datasets are each going to be the complementary 90%:
# [10%:100%] (for a corresponding validation set of [0%:10%]),
# [0%:10%] + [20%:100%] (for a validation set of [10%:20%]), ...,
# [0%:90%] (for a validation set of [90%:100%]).
vals_ds = datasets.load_dataset('bookcorpus', split=[f'train[{k}%:{k+10}%]' for k in range(0, 100, 10)])
trains_ds = datasets.load_dataset('bookcorpus', split=[f'train[:{k}%]+train[{k+10}%:]' for k in range(0, 100, 10)])
# 10-fold cross-validation (see also next section on rounding behavior):
# The validation datasets are each going to be 10%:
# [0%:10%], [10%:20%], ..., [90%:100%].
# And the training datasets are each going to be the complementary 90%:
# [10%:100%] (for a corresponding validation set of [0%:10%]),
# [0%:10%] + [20%:100%] (for a validation set of [10%:20%]), ...,
# [0%:90%] (for a validation set of [90%:100%]).
vals_ds = datasets.load_dataset('bookcorpus', [datasets.ReadInstruction('train', from_=k, to=k+10, unit='%') for k in range(0, 100, 10)])
trains_ds = datasets.load_dataset('bookcorpus', [(datasets.ReadInstruction('train', to=k, unit='%') + datasets.ReadInstruction('train', from_=k+10, unit='%')) for k in range(0, 100, 10)]),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># 10-fold cross-validation (see also next section on rounding behavior):</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># The validation datasets are each going to be 10%:</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># [0%:10%], [10%:20%], ..., [90%:100%].</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># And the training datasets are each going to be the complementary 90%:</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># [10%:100%] (for a corresponding validation set of [0%:10%]),</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># [0%:10%] + [20%:100%] (for a validation set of [10%:20%]), ...,</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># [0%:90%] (for a validation set of [90%:100%]).</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>vals_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=[<span class="hljs-string">f&#x27;train[<span class="hljs-subst">{k}</span>%:<span class="hljs-subst">{k+<span class="hljs-number">10</span>}</span>%]&#x27;</span> <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, <span class="hljs-number">100</span>, <span class="hljs-number">10</span>)])
<span class="hljs-meta">&gt;&gt;&gt; </span>trains_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=[<span class="hljs-string">f&#x27;train[:<span class="hljs-subst">{k}</span>%]+train[<span class="hljs-subst">{k+<span class="hljs-number">10</span>}</span>%:]&#x27;</span> <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, <span class="hljs-number">100</span>, <span class="hljs-number">10</span>)])
===STRINGAPI-READINSTRUCTION-SPLIT===
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># 10-fold cross-validation (see also next section on rounding behavior):</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># The validation datasets are each going to be 10%:</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># [0%:10%], [10%:20%], ..., [90%:100%].</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># And the training datasets are each going to be the complementary 90%:</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># [10%:100%] (for a corresponding validation set of [0%:10%]),</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># [0%:10%] + [20%:100%] (for a validation set of [10%:20%]), ...,</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># [0%:90%] (for a validation set of [90%:100%]).</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>vals_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, [datasets.ReadInstruction(<span class="hljs-string">&#x27;train&#x27;</span>, from_=k, to=k+<span class="hljs-number">10</span>, unit=<span class="hljs-string">&#x27;%&#x27;</span>) <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, <span class="hljs-number">100</span>, <span class="hljs-number">10</span>)])
<span class="hljs-meta">&gt;&gt;&gt; </span>trains_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, [(datasets.ReadInstruction(<span class="hljs-string">&#x27;train&#x27;</span>, to=k, unit=<span class="hljs-string">&#x27;%&#x27;</span>) + datasets.ReadInstruction(<span class="hljs-string">&#x27;train&#x27;</span>, from_=k+<span class="hljs-number">10</span>, unit=<span class="hljs-string">&#x27;%&#x27;</span>)) <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, <span class="hljs-number">100</span>, <span class="hljs-number">10</span>)])`}}),st=new P({}),at=new b({props:{code:`# Assuming *train* split contains 999 records.
# 19 records, from 500 (included) to 519 (excluded).
>>> train_50_52_ds = datasets.load_dataset('bookcorpus', split='train[50%:52%]')
# 20 records, from 519 (included) to 539 (excluded).
>>> train_52_54_ds = datasets.load_dataset('bookcorpus', split='train[52%:54%]'),`,highlighted:`<span class="hljs-comment"># Assuming *train* split contains 999 records.</span>
<span class="hljs-comment"># 19 records, from 500 (included) to 519 (excluded).</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>train_50_52_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=<span class="hljs-string">&#x27;train[50%:52%]&#x27;</span>)
<span class="hljs-comment"># 20 records, from 519 (included) to 539 (excluded).</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>train_52_54_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=<span class="hljs-string">&#x27;train[52%:54%]&#x27;</span>)`}}),tt=new b({props:{code:`# 18 records, from 450 (included) to 468 (excluded).
>>> train_50_52pct1_ds = datasets.load_dataset('bookcorpus', split=datasets.ReadInstruction( 'train', from_=50, to=52, unit='%', rounding='pct1_dropremainder'))
# 18 records, from 468 (included) to 486 (excluded).
>>> train_52_54pct1_ds = datasets.load_dataset('bookcorpus', split=datasets.ReadInstruction('train',from_=52, to=54, unit='%', rounding='pct1_dropremainder'))
# Or equivalently:
>>> train_50_52pct1_ds = datasets.load_dataset('bookcorpus', split='train[50%:52%](pct1_dropremainder)')
>>> train_52_54pct1_ds = datasets.load_dataset('bookcorpus', split='train[52%:54%](pct1_dropremainder)'),`,highlighted:`<span class="hljs-comment"># 18 records, from 450 (included) to 468 (excluded).</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>train_50_52pct1_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=datasets.ReadInstruction( <span class="hljs-string">&#x27;train&#x27;</span>, from_=<span class="hljs-number">50</span>, to=<span class="hljs-number">52</span>, unit=<span class="hljs-string">&#x27;%&#x27;</span>, rounding=<span class="hljs-string">&#x27;pct1_dropremainder&#x27;</span>))
<span class="hljs-comment"># 18 records, from 468 (included) to 486 (excluded).</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>train_52_54pct1_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=datasets.ReadInstruction(<span class="hljs-string">&#x27;train&#x27;</span>,from_=<span class="hljs-number">52</span>, to=<span class="hljs-number">54</span>, unit=<span class="hljs-string">&#x27;%&#x27;</span>, rounding=<span class="hljs-string">&#x27;pct1_dropremainder&#x27;</span>))
<span class="hljs-comment"># Or equivalently:</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>train_50_52pct1_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=<span class="hljs-string">&#x27;train[50%:52%](pct1_dropremainder)&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>train_52_54pct1_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=<span class="hljs-string">&#x27;train[52%:54%](pct1_dropremainder)&#x27;</span>)`}}),Zs=new Ee({props:{warning:"&lcub;true}",$$slots:{default:[qu]},$$scope:{ctx:I}}}),et=new P({}),lt=new P({}),nt=new b({props:{code:'dataset = load_dataset("matinf", "summarization"),',highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;matinf&quot;</span>, <span class="hljs-string">&quot;summarization&quot;</span>)
Downloading <span class="hljs-keyword">and</span> preparing dataset matinf/summarization (download: Unknown size, generated: <span class="hljs-number">246.89</span> MiB, post-processed: Unknown size, total: <span class="hljs-number">246.89</span> MiB) to /root/.cache/huggingface/datasets/matinf/summarization/<span class="hljs-number">1.0</span><span class="hljs-number">.0</span>/82eee5e71c3ceaf20d909bca36ff237452b4e4ab195d3be7ee1c78b53e6f540e...
AssertionError: The dataset matinf <span class="hljs-keyword">with</span> config summarization requires manual data. 
Please follow the manual download instructions: To use MATINF you have to download it manually. Please fill this google form (https://forms.gle/nkH4LVE4iNQeDzsc9). You will receive a download link <span class="hljs-keyword">and</span> a password once you complete the form. Please extract <span class="hljs-built_in">all</span> files <span class="hljs-keyword">in</span> one folder <span class="hljs-keyword">and</span> load the dataset <span class="hljs-keyword">with</span>: *datasets.load_dataset(<span class="hljs-string">&#x27;matinf&#x27;</span>, data_dir=<span class="hljs-string">&#x27;path/to/folder/folder_name&#x27;</span>)*. 
Manual data can be loaded <span class="hljs-keyword">with</span> \`datasets.load_dataset(matinf, data_dir=<span class="hljs-string">&#x27;&lt;path/to/manual/data&gt;&#x27;</span>) `}}),rt=new P({}),pt=new b({props:{code:`class_names = ["sadness", "joy", "love", "anger", "fear", "surprise"]
emotion_features = Features({'text': Value('string'), 'label': ClassLabel(names=class_names)}),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>class_names = [<span class="hljs-string">&quot;sadness&quot;</span>, <span class="hljs-string">&quot;joy&quot;</span>, <span class="hljs-string">&quot;love&quot;</span>, <span class="hljs-string">&quot;anger&quot;</span>, <span class="hljs-string">&quot;fear&quot;</span>, <span class="hljs-string">&quot;surprise&quot;</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>emotion_features = Features({<span class="hljs-string">&#x27;text&#x27;</span>: Value(<span class="hljs-string">&#x27;string&#x27;</span>), <span class="hljs-string">&#x27;label&#x27;</span>: ClassLabel(names=class_names)})`}}),dt=new b({props:{code:"dataset = load_dataset('csv', data_files=file_dict, delimiter=';', column_names=['text', 'label'], features=emotion_features),",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;csv&#x27;</span>, data_files=file_dict, delimiter=<span class="hljs-string">&#x27;;&#x27;</span>, column_names=[<span class="hljs-string">&#x27;text&#x27;</span>, <span class="hljs-string">&#x27;label&#x27;</span>], features=emotion_features)'}}),ft=new b({props:{code:"dataset['train'].features,",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>dataset[<span class="hljs-string">&#x27;train&#x27;</span>].features
{<span class="hljs-string">&#x27;text&#x27;</span>: Value(dtype=<span class="hljs-string">&#x27;string&#x27;</span>, <span class="hljs-built_in">id</span>=<span class="hljs-literal">None</span>),
<span class="hljs-string">&#x27;label&#x27;</span>: ClassLabel(num_classes=<span class="hljs-number">6</span>, names=[<span class="hljs-string">&#x27;sadness&#x27;</span>, <span class="hljs-string">&#x27;joy&#x27;</span>, <span class="hljs-string">&#x27;love&#x27;</span>, <span class="hljs-string">&#x27;anger&#x27;</span>, <span class="hljs-string">&#x27;fear&#x27;</span>, <span class="hljs-string">&#x27;surprise&#x27;</span>], names_file=<span class="hljs-literal">None</span>, <span class="hljs-built_in">id</span>=<span class="hljs-literal">None</span>)}`}}),ct=new P({}),ht=new b({props:{code:`from datasets import load_metric
metric = load_metric('PATH/TO/MY/METRIC/SCRIPT')

# Example of typical usage
for batch in dataset:
    inputs, references = batch
    predictions = model(inputs)
    metric.add_batch(predictions=predictions, references=references)
score = metric.compute(),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_metric
<span class="hljs-meta">&gt;&gt;&gt; </span>metric = load_metric(<span class="hljs-string">&#x27;PATH/TO/MY/METRIC/SCRIPT&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Example of typical usage</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">for</span> batch <span class="hljs-keyword">in</span> dataset:
<span class="hljs-meta">... </span>    inputs, references = batch
<span class="hljs-meta">... </span>    predictions = model(inputs)
<span class="hljs-meta">... </span>    metric.add_batch(predictions=predictions, references=references)
<span class="hljs-meta">&gt;&gt;&gt; </span>score = metric.compute()`}}),oa=new Ee({props:{$$slots:{default:[Pu]},$$scope:{ctx:I}}}),ut=new P({}),mt=new b({props:{code:`from datasets import load_metric
metric = load_metric('bleurt', name='bleurt-base-128')
metric = load_metric('bleurt', name='bleurt-base-512'),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python"><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_metric</span>
<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python">metric = load_metric(<span class="hljs-string">&#x27;bleurt&#x27;</span>, name=<span class="hljs-string">&#x27;bleurt-base-128&#x27;</span>)</span>
<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python">metric = load_metric(<span class="hljs-string">&#x27;bleurt&#x27;</span>, name=<span class="hljs-string">&#x27;bleurt-base-512&#x27;</span>)</span>`}}),gt=new P({}),$t=new b({props:{code:`from datasets import load_metric
metric = load_metric('glue', 'mrpc', num_process=num_process, process_id=rank),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_metric
<span class="hljs-meta">&gt;&gt;&gt; </span>metric = load_metric(<span class="hljs-string">&#x27;glue&#x27;</span>, <span class="hljs-string">&#x27;mrpc&#x27;</span>, num_process=num_process, process_id=rank)`}}),ia=new Ee({props:{$$slots:{default:[Au]},$$scope:{ctx:I}}}),yt=new b({props:{code:`from datasets import load_metric
metric = load_metric('glue', 'mrpc', num_process=num_process, process_id=process_id, experiment_id="My_experiment_10"),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_metric
<span class="hljs-meta">&gt;&gt;&gt; </span>metric = load_metric(<span class="hljs-string">&#x27;glue&#x27;</span>, <span class="hljs-string">&#x27;mrpc&#x27;</span>, num_process=num_process, process_id=process_id, experiment_id=<span class="hljs-string">&quot;My_experiment_10&quot;</span>)`}}),{c(){h=l("meta"),k=d(),y=l("h1"),x=l("a"),E=l("span"),u(w.$$.fragment),j=d(),q=l("span"),ss=r("Load"),js=d(),R=l("p"),as=r("You have already seen how to load a dataset from the Hugging Face Hub. But datasets are stored in a variety of places, and sometimes you won\u2019t find the one you want on the Hub. A dataset can be on disk on your local machine, in a Github repository, and in in-memory data structures like Python dictionaries and Pandas DataFrames. Wherever your dataset may be stored, \u{1F917} Datasets provides a way for you to load and use it for training."),bs=d(),z=l("p"),M=r("This guide will show you how to load a dataset from:"),xs=d(),A=l("ul"),L=l("li"),S=r("The Hub without a dataset loading script"),jt=d(),ks=l("li"),bt=r("Local files"),xt=d(),Es=l("li"),Yr=r("In-memory data"),Gr=d(),qe=l("li"),Wr=r("Offline"),Qr=d(),Pe=l("li"),Xr=r("A specific slice of a split"),so=d(),kt=l("p"),Kr=r("You will also learn how to troubleshoot common errors, and how to load specific configurations of a metric."),ao=d(),Et=l("a"),to=d(),ts=l("h2"),qs=l("a"),Ae=l("span"),u(ca.$$.fragment),Zr=d(),Se=l("span"),si=r("Hugging Face Hub"),eo=d(),Ps=l("p"),ai=r("In the tutorial, you learned how to load a dataset from the Hub. This method relies on a dataset loading script that downloads and builds the dataset. However, you can also load a dataset from any dataset repository on the Hub "),Te=l("strong"),ti=r("without"),ei=r(" a loading script!"),lo=d(),U=l("p"),li=r("First, create a dataset repository and upload your data files. Then you can use "),qt=l("a"),oi=r("datasets.load_dataset()"),ni=r(" like you learned in the tutorial. For example, load the files from this "),ha=l("a"),ri=r("demo repository"),ii=r(" by providing the repository namespace and dataset name:"),oo=d(),u(ua.$$.fragment),no=d(),Pt=l("p"),pi=r("This dataset repository contains CSV files, and this code loads all the data from the CSV files."),ro=d(),As=l("p"),di=r("Some datasets may have more than one version, based on Git tags, branches or commits. Use the "),De=l("code"),fi=r("revision"),ci=r(" flag to specify which dataset version you want to load:"),io=d(),u(ma.$$.fragment),po=d(),u(Ss.$$.fragment),fo=d(),T=l("p"),hi=r("If the dataset doesn\u2019t have a dataset loading script, then by default, all the data will be loaded in the "),Ie=l("code"),ui=r("train"),mi=r(" split. Use the "),Ne=l("code"),gi=r("data_files"),_i=r(" parameter to map data files to splits like "),Ce=l("code"),vi=r("train"),$i=r(", "),Oe=l("code"),yi=r("validation"),wi=r(" and "),He=l("code"),ji=r("test"),bi=r(":"),co=d(),u(ga.$$.fragment),ho=d(),u(Ts.$$.fragment),uo=d(),V=l("p"),xi=r("You can also load a specific subset of the files with the "),Fe=l("code"),ki=r("data_files"),Ei=r(" parameter. The example below loads files from the "),_a=l("a"),qi=r("C4 dataset"),Pi=r(":"),mo=d(),u(va.$$.fragment),go=d(),Ds=l("p"),Ai=r("Specify a custom split with the "),Le=l("code"),Si=r("split"),Ti=r(" parameter:"),_o=d(),u($a.$$.fragment),vo=d(),es=l("h2"),Is=l("a"),Re=l("span"),u(ya.$$.fragment),Di=d(),ze=l("span"),Ii=r("Local and remote files"),$o=d(),D=l("p"),Ni=r("\u{1F917} Datasets can be loaded from local files stored on your computer, and also from remote files. The datasets are most likely stored as a "),Me=l("code"),Ci=r("csv"),Oi=r(", "),Ue=l("code"),Hi=r("json"),Fi=r(", "),Ve=l("code"),Li=r("txt"),Ri=r(" or "),Be=l("code"),zi=r("parquet"),Mi=r(" file. The "),At=l("a"),Ui=r("datasets.load_dataset()"),Vi=r(" method is able to load each of these file types."),yo=d(),ls=l("h3"),Ns=l("a"),Je=l("span"),u(wa.$$.fragment),Bi=d(),Ye=l("span"),Ji=r("CSV"),wo=d(),St=l("p"),Yi=r("\u{1F917} Datasets can read a dataset made up of one or several CSV files:"),jo=d(),u(ja.$$.fragment),bo=d(),Tt=l("p"),Gi=r("If you have more than one CSV file:"),xo=d(),u(ba.$$.fragment),ko=d(),Dt=l("p"),Wi=r("You can also map the training and test splits to specific CSV files:"),Eo=d(),u(xa.$$.fragment),qo=d(),It=l("p"),Qi=r("To load remote CSV files via HTTP, you can pass the URLs:"),Po=d(),u(ka.$$.fragment),Ao=d(),Nt=l("p"),Xi=r("To load zipped CSV files:"),So=d(),u(Ea.$$.fragment),To=d(),os=l("h3"),Cs=l("a"),Ge=l("span"),u(qa.$$.fragment),Ki=d(),We=l("span"),Zi=r("JSON"),Do=d(),Os=l("p"),sp=r("JSON files are loaded directly with "),Ct=l("a"),ap=r("datasets.load_dataset()"),tp=r(" as shown below:"),Io=d(),u(Pa.$$.fragment),No=d(),Ot=l("p"),ep=r("JSON files can have diverse formats, but we think the most efficient format is to have multiple JSON objects; each line represents an individual row of data. For example:"),Co=d(),u(Aa.$$.fragment),Oo=d(),Hs=l("p"),lp=r("Another JSON format you may encounter is a nested field, in which case you will need to specify the "),Qe=l("code"),op=r("field"),np=r(" argument as shown in the following:"),Ho=d(),u(Sa.$$.fragment),Fo=d(),Ht=l("p"),rp=r("To load remote JSON files via HTTP, you can pass the URLs:"),Lo=d(),u(Ta.$$.fragment),Ro=d(),Ft=l("p"),ip=r("While these are the most common JSON formats, you will see other datasets that are formatted differently. \u{1F917} Datasets recognizes these other formats, and will fallback accordingly on the Python JSON loading methods to handle them."),zo=d(),ns=l("h3"),Fs=l("a"),Xe=l("span"),u(Da.$$.fragment),pp=d(),Ke=l("span"),dp=r("Text files"),Mo=d(),Lt=l("p"),fp=r("Text files are one of the most common file types for storing a dataset. \u{1F917} Datasets will read the text file line by line to build the dataset."),Uo=d(),u(Ia.$$.fragment),Vo=d(),Rt=l("p"),cp=r("To load remote TXT files via HTTP, you can pass the URLs:"),Bo=d(),u(Na.$$.fragment),Jo=d(),rs=l("h3"),Ls=l("a"),Ze=l("span"),u(Ca.$$.fragment),hp=d(),sl=l("span"),up=r("Parquet"),Yo=d(),zt=l("p"),mp=r("Parquet files are stored in a columnar format unlike row-based files like CSV. Large datasets may be stored in a Parquet file because it is more efficient, and faster at returning your query. Load a Parquet file as shown in the following example:"),Go=d(),u(Oa.$$.fragment),Wo=d(),Mt=l("p"),gp=r("To load remote parquet files via HTTP, you can pass the URLs:"),Qo=d(),u(Ha.$$.fragment),Xo=d(),is=l("h3"),Rs=l("a"),al=l("span"),u(Fa.$$.fragment),_p=d(),tl=l("span"),vp=r("Image folders"),Ko=d(),Ut=l("p"),$p=r("\u{1F917} Datasets can also load generic image folders."),Zo=d(),Vt=l("p"),yp=r("The folder structure should look like this:"),sn=d(),u(La.$$.fragment),an=d(),N=l("p"),wp=r("To load an "),el=l("code"),jp=r("imagefolder"),bp=r(" dataset, simply pass the root path of the image folder to the "),ll=l("code"),xp=r("data_dir"),kp=r(" kwarg of "),Bt=l("a"),Ep=r("datasets.load_dataset()"),qp=r(", which is a shorthand syntax for "),ol=l("code"),Pp=r("data_files=os.path.join(data_dir, **)"),Ap=r("."),tn=d(),u(Ra.$$.fragment),en=d(),Jt=l("p"),Sp=r("To load remote image folders via HTTP, you can pass the URLs:"),ln=d(),u(za.$$.fragment),on=d(),C=l("p"),Tp=r("The resulting dataset will include an "),nl=l("code"),Dp=r("image"),Ip=r(" feature, which is a "),rl=l("code"),Np=r("PIL.Image"),Cp=r(" loaded from the image file, and the corresponding "),il=l("code"),Op=r("label"),Hp=r(" inferred from the directory structure."),nn=d(),ps=l("h2"),zs=l("a"),pl=l("span"),u(Ma.$$.fragment),Fp=d(),dl=l("span"),Lp=r("In-memory data"),rn=d(),Ms=l("p"),Rp=r("\u{1F917} Datasets will also allow you to create a "),Yt=l("a"),zp=r("datasets.Dataset"),Mp=r(" directly from in-memory data structures like Python dictionaries and Pandas DataFrames."),pn=d(),ds=l("h3"),Us=l("a"),fl=l("span"),u(Ua.$$.fragment),Up=d(),cl=l("span"),Vp=r("Python dictionary"),dn=d(),Vs=l("p"),Bp=r("Load Python dictionaries with "),Gt=l("a"),Jp=r("datasets.Dataset.from_dict()"),Yp=r(":"),fn=d(),u(Va.$$.fragment),cn=d(),fs=l("h3"),Bs=l("a"),hl=l("span"),u(Ba.$$.fragment),Gp=d(),ul=l("span"),Wp=r("Pandas DataFrame"),hn=d(),Js=l("p"),Qp=r("Load Pandas DataFrames with "),Wt=l("a"),Xp=r("datasets.Dataset.from_pandas()"),Kp=r(":"),un=d(),u(Ja.$$.fragment),mn=d(),u(Ys.$$.fragment),gn=d(),cs=l("h2"),Gs=l("a"),ml=l("span"),u(Ya.$$.fragment),Zp=d(),gl=l("span"),sd=r("Offline"),_n=d(),Qt=l("p"),ad=r("Even if you don\u2019t have an internet connection, it is still possible to load a dataset. As long as you\u2019ve downloaded a dataset from the Hub or \u{1F917} Datasets GitHub repository before, it should be cached. This means you can reload the dataset from the cache and use it offline."),vn=d(),B=l("p"),td=r("If you know you won\u2019t have internet access, you can run \u{1F917} Datasets in full offline mode. This saves time because instead of waiting for the Dataset builder download to time out, \u{1F917} Datasets will look directly in the cache. Set the environment variable "),_l=l("code"),ed=r("HF_DATASETS_OFFLINE"),ld=r(" to "),vl=l("code"),od=r("1"),nd=r(" to enable full offline mode."),$n=d(),hs=l("h2"),Ws=l("a"),$l=l("span"),u(Ga.$$.fragment),rd=d(),yl=l("span"),id=r("Slice splits"),yn=d(),J=l("p"),pd=r("For even greater control over how to load a split, you can choose to only load specific slices of a split. There are two options for slicing a split: using strings or "),Xt=l("a"),dd=r("datasets.ReadInstruction"),fd=r(". Strings are more compact and readable for simple cases, while "),Kt=l("a"),cd=r("datasets.ReadInstruction"),hd=r(" is easier to use with variable slicing parameters."),wn=d(),Y=l("p"),ud=r("Concatenate the "),wl=l("code"),md=r("train"),gd=r(" and "),jl=l("code"),_d=r("test"),vd=r(" split by:"),jn=d(),u(Wa.$$.fragment),bn=d(),Qs=l("p"),$d=r("Select specific rows of the "),bl=l("code"),yd=r("train"),wd=r(" split:"),xn=d(),u(Qa.$$.fragment),kn=d(),Zt=l("p"),jd=r("Or select a percentage of the split with:"),En=d(),u(Xa.$$.fragment),qn=d(),se=l("p"),bd=r("You can even select a combination of percentages from each split:"),Pn=d(),u(Ka.$$.fragment),An=d(),ae=l("p"),xd=r("Finally, create cross-validated dataset splits by:"),Sn=d(),u(Za.$$.fragment),Tn=d(),us=l("h3"),Xs=l("a"),xl=l("span"),u(st.$$.fragment),kd=d(),kl=l("span"),Ed=r("Percent slicing and rounding"),Dn=d(),te=l("p"),qd=r("For datasets where the requested slice boundaries do not divide evenly by 100, the default behavior is to round the boundaries to the nearest integer. As a result, some slices may contain more examples than others as shown in the following example:"),In=d(),u(at.$$.fragment),Nn=d(),Ks=l("p"),Pd=r("If you want equal sized splits, use "),El=l("code"),Ad=r("pct1_dropremainder"),Sd=r(" rounding instead. This will treat the specified percentage boundaries as multiples of 1%."),Cn=d(),u(tt.$$.fragment),On=d(),u(Zs.$$.fragment),Hn=d(),ee=l("a"),Fn=d(),ms=l("h2"),sa=l("a"),ql=l("span"),u(et.$$.fragment),Td=d(),Pl=l("span"),Dd=r("Troubleshooting"),Ln=d(),le=l("p"),Id=r("Sometimes, you may get unexpected results when you load a dataset. In this section, you will learn how to solve two common issues you may encounter when you load a dataset: manually download a dataset, and specify features of a dataset."),Rn=d(),gs=l("h3"),aa=l("a"),Al=l("span"),u(lt.$$.fragment),Nd=d(),Sl=l("span"),Cd=r("Manual download"),zn=d(),O=l("p"),Od=r("Certain datasets require you to manually download the dataset files due to licensing incompatibility, or if the files are hidden behind a login page. This will cause "),oe=l("a"),Hd=r("datasets.load_dataset()"),Fd=r(" to throw an "),Tl=l("code"),Ld=r("AssertionError"),Rd=r(". But \u{1F917} Datasets provides detailed instructions for downloading the missing files. After you have downloaded the files, use the "),Dl=l("code"),zd=r("data_dir"),Md=r(" argument to specify the path to the files you just downloaded."),Mn=d(),ta=l("p"),Ud=r("For example, if you try to download a configuration from the "),ot=l("a"),Vd=r("MATINF"),Bd=r(" dataset:"),Un=d(),u(nt.$$.fragment),Vn=d(),_s=l("h3"),ea=l("a"),Il=l("span"),u(rt.$$.fragment),Jd=d(),Nl=l("span"),Yd=r("Specify features"),Bn=d(),G=l("p"),Gd=r("When you create a dataset from local files, the "),ne=l("a"),Wd=r("datasets.Features"),Qd=r(" are automatically inferred by "),it=l("a"),Xd=r("Apache Arrow"),Kd=r(". However, the features of the dataset may not always align with your expectations or you may want to define the features yourself."),Jn=d(),W=l("p"),Zd=r("The following example shows how you can add custom labels with "),re=l("a"),sf=r("datasets.ClassLabel"),af=r(". First, define your own labels using the "),ie=l("a"),tf=r("datasets.Features"),ef=r(" class:"),Yn=d(),u(pt.$$.fragment),Gn=d(),Q=l("p"),lf=r("Next, specify the "),Cl=l("code"),of=r("features"),nf=r(" argument in "),pe=l("a"),rf=r("datasets.load_dataset()"),pf=r(" with the features you just created:"),Wn=d(),u(dt.$$.fragment),Qn=d(),de=l("p"),df=r("Now when you look at your dataset features, you can see it uses the custom labels you defined:"),Xn=d(),u(ft.$$.fragment),Kn=d(),vs=l("h2"),la=l("a"),Ol=l("span"),u(ct.$$.fragment),ff=d(),Hl=l("span"),cf=r("Metrics"),Zn=d(),fe=l("p"),hf=r("When the metric you want to use is not supported by \u{1F917} Datasets, you can write and use your own metric script. Load your metric by providing the path to your local metric loading script:"),sr=d(),u(ht.$$.fragment),ar=d(),u(oa.$$.fragment),tr=d(),$s=l("h3"),na=l("a"),Fl=l("span"),u(ut.$$.fragment),uf=d(),Ll=l("span"),mf=r("Load configurations"),er=d(),ce=l("p"),gf=r("It is possible for a metric to have different configurations. The configurations are stored in the [\u2018datasets.Metric.config_name\u2019] attribute. When you load a metric, provide the configuration name as shown in the following:"),lr=d(),u(mt.$$.fragment),or=d(),ys=l("h3"),ra=l("a"),Rl=l("span"),u(gt.$$.fragment),_f=d(),zl=l("span"),vf=r("Distributed setup"),nr=d(),he=l("p"),$f=r("When you work in a distributed or parallel processing environment, loading and computing a metric can be tricky because these processes are executed in parallel on separate subsets of the data. \u{1F917} Datasets supports distributed usage with a few additional arguments when you load a metric."),rr=d(),ue=l("p"),yf=r("For example, imagine you are training and evaluating on eight parallel processes. Here\u2019s how you would load a metric in this distributed setting:"),ir=d(),X=l("ol"),Ml=l("li"),_t=l("p"),wf=r("Define the total number of processes with the "),Ul=l("code"),jf=r("num_process"),bf=r(" argument."),xf=d(),Vl=l("li"),ws=l("p"),kf=r("Set the process "),Bl=l("code"),Ef=r("rank"),qf=r(" as an integer between zero and "),Jl=l("code"),Pf=r("num_process - 1"),Af=r("."),Sf=d(),Yl=l("li"),vt=l("p"),Tf=r("Load your metric with "),me=l("a"),Df=r("datasets.load_metric()"),If=r(" with these arguments:"),pr=d(),u($t.$$.fragment),dr=d(),u(ia.$$.fragment),fr=d(),pa=l("p"),Nf=r("In some instances, you may be simultaneously running multiple independent distributed evaluations on the same server and files. To avoid any conflicts, it is important to provide an "),Gl=l("code"),Cf=r("experiment_id"),Of=r(" to distinguish the separate evaluations:"),cr=d(),u(yt.$$.fragment),this.h()},l(s){const e=bu('[data-svelte="svelte-1phssyn"]',document.head);h=o(e,"META",{name:!0,content:!0}),e.forEach(a),k=f(s),y=o(s,"H1",{class:!0});var wt=n(y);x=o(wt,"A",{id:!0,class:!0,href:!0});var Wl=n(x);E=o(Wl,"SPAN",{});var Ql=n(E);m(w.$$.fragment,Ql),Ql.forEach(a),Wl.forEach(a),j=f(wt),q=o(wt,"SPAN",{});var Xl=n(q);ss=i(Xl,"Load"),Xl.forEach(a),wt.forEach(a),js=f(s),R=o(s,"P",{});var Kl=n(R);as=i(Kl,"You have already seen how to load a dataset from the Hugging Face Hub. But datasets are stored in a variety of places, and sometimes you won\u2019t find the one you want on the Hub. A dataset can be on disk on your local machine, in a Github repository, and in in-memory data structures like Python dictionaries and Pandas DataFrames. Wherever your dataset may be stored, \u{1F917} Datasets provides a way for you to load and use it for training."),Kl.forEach(a),bs=f(s),z=o(s,"P",{});var Zl=n(z);M=i(Zl,"This guide will show you how to load a dataset from:"),Zl.forEach(a),xs=f(s),A=o(s,"UL",{});var K=n(A);L=o(K,"LI",{});var Hf=n(L);S=i(Hf,"The Hub without a dataset loading script"),Hf.forEach(a),jt=f(K),ks=o(K,"LI",{});var Ff=n(ks);bt=i(Ff,"Local files"),Ff.forEach(a),xt=f(K),Es=o(K,"LI",{});var Lf=n(Es);Yr=i(Lf,"In-memory data"),Lf.forEach(a),Gr=f(K),qe=o(K,"LI",{});var Rf=n(qe);Wr=i(Rf,"Offline"),Rf.forEach(a),Qr=f(K),Pe=o(K,"LI",{});var zf=n(Pe);Xr=i(zf,"A specific slice of a split"),zf.forEach(a),K.forEach(a),so=f(s),kt=o(s,"P",{});var Mf=n(kt);Kr=i(Mf,"You will also learn how to troubleshoot common errors, and how to load specific configurations of a metric."),Mf.forEach(a),ao=f(s),Et=o(s,"A",{id:!0}),n(Et).forEach(a),to=f(s),ts=o(s,"H2",{class:!0});var ur=n(ts);qs=o(ur,"A",{id:!0,class:!0,href:!0});var Uf=n(qs);Ae=o(Uf,"SPAN",{});var Vf=n(Ae);m(ca.$$.fragment,Vf),Vf.forEach(a),Uf.forEach(a),Zr=f(ur),Se=o(ur,"SPAN",{});var Bf=n(Se);si=i(Bf,"Hugging Face Hub"),Bf.forEach(a),ur.forEach(a),eo=f(s),Ps=o(s,"P",{});var mr=n(Ps);ai=i(mr,"In the tutorial, you learned how to load a dataset from the Hub. This method relies on a dataset loading script that downloads and builds the dataset. However, you can also load a dataset from any dataset repository on the Hub "),Te=o(mr,"STRONG",{});var Jf=n(Te);ti=i(Jf,"without"),Jf.forEach(a),ei=i(mr," a loading script!"),mr.forEach(a),lo=f(s),U=o(s,"P",{});var ge=n(U);li=i(ge,"First, create a dataset repository and upload your data files. Then you can use "),qt=o(ge,"A",{href:!0});var Yf=n(qt);oi=i(Yf,"datasets.load_dataset()"),Yf.forEach(a),ni=i(ge," like you learned in the tutorial. For example, load the files from this "),ha=o(ge,"A",{href:!0,rel:!0});var Gf=n(ha);ri=i(Gf,"demo repository"),Gf.forEach(a),ii=i(ge," by providing the repository namespace and dataset name:"),ge.forEach(a),oo=f(s),m(ua.$$.fragment,s),no=f(s),Pt=o(s,"P",{});var Wf=n(Pt);pi=i(Wf,"This dataset repository contains CSV files, and this code loads all the data from the CSV files."),Wf.forEach(a),ro=f(s),As=o(s,"P",{});var gr=n(As);di=i(gr,"Some datasets may have more than one version, based on Git tags, branches or commits. Use the "),De=o(gr,"CODE",{});var Qf=n(De);fi=i(Qf,"revision"),Qf.forEach(a),ci=i(gr," flag to specify which dataset version you want to load:"),gr.forEach(a),io=f(s),m(ma.$$.fragment,s),po=f(s),m(Ss.$$.fragment,s),fo=f(s),T=o(s,"P",{});var H=n(T);hi=i(H,"If the dataset doesn\u2019t have a dataset loading script, then by default, all the data will be loaded in the "),Ie=o(H,"CODE",{});var Xf=n(Ie);ui=i(Xf,"train"),Xf.forEach(a),mi=i(H," split. Use the "),Ne=o(H,"CODE",{});var Kf=n(Ne);gi=i(Kf,"data_files"),Kf.forEach(a),_i=i(H," parameter to map data files to splits like "),Ce=o(H,"CODE",{});var Zf=n(Ce);vi=i(Zf,"train"),Zf.forEach(a),$i=i(H,", "),Oe=o(H,"CODE",{});var sc=n(Oe);yi=i(sc,"validation"),sc.forEach(a),wi=i(H," and "),He=o(H,"CODE",{});var ac=n(He);ji=i(ac,"test"),ac.forEach(a),bi=i(H,":"),H.forEach(a),co=f(s),m(ga.$$.fragment,s),ho=f(s),m(Ts.$$.fragment,s),uo=f(s),V=o(s,"P",{});var _e=n(V);xi=i(_e,"You can also load a specific subset of the files with the "),Fe=o(_e,"CODE",{});var tc=n(Fe);ki=i(tc,"data_files"),tc.forEach(a),Ei=i(_e," parameter. The example below loads files from the "),_a=o(_e,"A",{href:!0,rel:!0});var ec=n(_a);qi=i(ec,"C4 dataset"),ec.forEach(a),Pi=i(_e,":"),_e.forEach(a),mo=f(s),m(va.$$.fragment,s),go=f(s),Ds=o(s,"P",{});var _r=n(Ds);Ai=i(_r,"Specify a custom split with the "),Le=o(_r,"CODE",{});var lc=n(Le);Si=i(lc,"split"),lc.forEach(a),Ti=i(_r," parameter:"),_r.forEach(a),_o=f(s),m($a.$$.fragment,s),vo=f(s),es=o(s,"H2",{class:!0});var vr=n(es);Is=o(vr,"A",{id:!0,class:!0,href:!0});var oc=n(Is);Re=o(oc,"SPAN",{});var nc=n(Re);m(ya.$$.fragment,nc),nc.forEach(a),oc.forEach(a),Di=f(vr),ze=o(vr,"SPAN",{});var rc=n(ze);Ii=i(rc,"Local and remote files"),rc.forEach(a),vr.forEach(a),$o=f(s),D=o(s,"P",{});var F=n(D);Ni=i(F,"\u{1F917} Datasets can be loaded from local files stored on your computer, and also from remote files. The datasets are most likely stored as a "),Me=o(F,"CODE",{});var ic=n(Me);Ci=i(ic,"csv"),ic.forEach(a),Oi=i(F,", "),Ue=o(F,"CODE",{});var pc=n(Ue);Hi=i(pc,"json"),pc.forEach(a),Fi=i(F,", "),Ve=o(F,"CODE",{});var dc=n(Ve);Li=i(dc,"txt"),dc.forEach(a),Ri=i(F," or "),Be=o(F,"CODE",{});var fc=n(Be);zi=i(fc,"parquet"),fc.forEach(a),Mi=i(F," file. The "),At=o(F,"A",{href:!0});var cc=n(At);Ui=i(cc,"datasets.load_dataset()"),cc.forEach(a),Vi=i(F," method is able to load each of these file types."),F.forEach(a),yo=f(s),ls=o(s,"H3",{class:!0});var $r=n(ls);Ns=o($r,"A",{id:!0,class:!0,href:!0});var hc=n(Ns);Je=o(hc,"SPAN",{});var uc=n(Je);m(wa.$$.fragment,uc),uc.forEach(a),hc.forEach(a),Bi=f($r),Ye=o($r,"SPAN",{});var mc=n(Ye);Ji=i(mc,"CSV"),mc.forEach(a),$r.forEach(a),wo=f(s),St=o(s,"P",{});var gc=n(St);Yi=i(gc,"\u{1F917} Datasets can read a dataset made up of one or several CSV files:"),gc.forEach(a),jo=f(s),m(ja.$$.fragment,s),bo=f(s),Tt=o(s,"P",{});var _c=n(Tt);Gi=i(_c,"If you have more than one CSV file:"),_c.forEach(a),xo=f(s),m(ba.$$.fragment,s),ko=f(s),Dt=o(s,"P",{});var vc=n(Dt);Wi=i(vc,"You can also map the training and test splits to specific CSV files:"),vc.forEach(a),Eo=f(s),m(xa.$$.fragment,s),qo=f(s),It=o(s,"P",{});var $c=n(It);Qi=i($c,"To load remote CSV files via HTTP, you can pass the URLs:"),$c.forEach(a),Po=f(s),m(ka.$$.fragment,s),Ao=f(s),Nt=o(s,"P",{});var yc=n(Nt);Xi=i(yc,"To load zipped CSV files:"),yc.forEach(a),So=f(s),m(Ea.$$.fragment,s),To=f(s),os=o(s,"H3",{class:!0});var yr=n(os);Cs=o(yr,"A",{id:!0,class:!0,href:!0});var wc=n(Cs);Ge=o(wc,"SPAN",{});var jc=n(Ge);m(qa.$$.fragment,jc),jc.forEach(a),wc.forEach(a),Ki=f(yr),We=o(yr,"SPAN",{});var bc=n(We);Zi=i(bc,"JSON"),bc.forEach(a),yr.forEach(a),Do=f(s),Os=o(s,"P",{});var wr=n(Os);sp=i(wr,"JSON files are loaded directly with "),Ct=o(wr,"A",{href:!0});var xc=n(Ct);ap=i(xc,"datasets.load_dataset()"),xc.forEach(a),tp=i(wr," as shown below:"),wr.forEach(a),Io=f(s),m(Pa.$$.fragment,s),No=f(s),Ot=o(s,"P",{});var kc=n(Ot);ep=i(kc,"JSON files can have diverse formats, but we think the most efficient format is to have multiple JSON objects; each line represents an individual row of data. For example:"),kc.forEach(a),Co=f(s),m(Aa.$$.fragment,s),Oo=f(s),Hs=o(s,"P",{});var jr=n(Hs);lp=i(jr,"Another JSON format you may encounter is a nested field, in which case you will need to specify the "),Qe=o(jr,"CODE",{});var Ec=n(Qe);op=i(Ec,"field"),Ec.forEach(a),np=i(jr," argument as shown in the following:"),jr.forEach(a),Ho=f(s),m(Sa.$$.fragment,s),Fo=f(s),Ht=o(s,"P",{});var qc=n(Ht);rp=i(qc,"To load remote JSON files via HTTP, you can pass the URLs:"),qc.forEach(a),Lo=f(s),m(Ta.$$.fragment,s),Ro=f(s),Ft=o(s,"P",{});var Pc=n(Ft);ip=i(Pc,"While these are the most common JSON formats, you will see other datasets that are formatted differently. \u{1F917} Datasets recognizes these other formats, and will fallback accordingly on the Python JSON loading methods to handle them."),Pc.forEach(a),zo=f(s),ns=o(s,"H3",{class:!0});var br=n(ns);Fs=o(br,"A",{id:!0,class:!0,href:!0});var Ac=n(Fs);Xe=o(Ac,"SPAN",{});var Sc=n(Xe);m(Da.$$.fragment,Sc),Sc.forEach(a),Ac.forEach(a),pp=f(br),Ke=o(br,"SPAN",{});var Tc=n(Ke);dp=i(Tc,"Text files"),Tc.forEach(a),br.forEach(a),Mo=f(s),Lt=o(s,"P",{});var Dc=n(Lt);fp=i(Dc,"Text files are one of the most common file types for storing a dataset. \u{1F917} Datasets will read the text file line by line to build the dataset."),Dc.forEach(a),Uo=f(s),m(Ia.$$.fragment,s),Vo=f(s),Rt=o(s,"P",{});var Ic=n(Rt);cp=i(Ic,"To load remote TXT files via HTTP, you can pass the URLs:"),Ic.forEach(a),Bo=f(s),m(Na.$$.fragment,s),Jo=f(s),rs=o(s,"H3",{class:!0});var xr=n(rs);Ls=o(xr,"A",{id:!0,class:!0,href:!0});var Nc=n(Ls);Ze=o(Nc,"SPAN",{});var Cc=n(Ze);m(Ca.$$.fragment,Cc),Cc.forEach(a),Nc.forEach(a),hp=f(xr),sl=o(xr,"SPAN",{});var Oc=n(sl);up=i(Oc,"Parquet"),Oc.forEach(a),xr.forEach(a),Yo=f(s),zt=o(s,"P",{});var Hc=n(zt);mp=i(Hc,"Parquet files are stored in a columnar format unlike row-based files like CSV. Large datasets may be stored in a Parquet file because it is more efficient, and faster at returning your query. Load a Parquet file as shown in the following example:"),Hc.forEach(a),Go=f(s),m(Oa.$$.fragment,s),Wo=f(s),Mt=o(s,"P",{});var Fc=n(Mt);gp=i(Fc,"To load remote parquet files via HTTP, you can pass the URLs:"),Fc.forEach(a),Qo=f(s),m(Ha.$$.fragment,s),Xo=f(s),is=o(s,"H3",{class:!0});var kr=n(is);Rs=o(kr,"A",{id:!0,class:!0,href:!0});var Lc=n(Rs);al=o(Lc,"SPAN",{});var Rc=n(al);m(Fa.$$.fragment,Rc),Rc.forEach(a),Lc.forEach(a),_p=f(kr),tl=o(kr,"SPAN",{});var zc=n(tl);vp=i(zc,"Image folders"),zc.forEach(a),kr.forEach(a),Ko=f(s),Ut=o(s,"P",{});var Mc=n(Ut);$p=i(Mc,"\u{1F917} Datasets can also load generic image folders."),Mc.forEach(a),Zo=f(s),Vt=o(s,"P",{});var Uc=n(Vt);yp=i(Uc,"The folder structure should look like this:"),Uc.forEach(a),sn=f(s),m(La.$$.fragment,s),an=f(s),N=o(s,"P",{});var Z=n(N);wp=i(Z,"To load an "),el=o(Z,"CODE",{});var Vc=n(el);jp=i(Vc,"imagefolder"),Vc.forEach(a),bp=i(Z," dataset, simply pass the root path of the image folder to the "),ll=o(Z,"CODE",{});var Bc=n(ll);xp=i(Bc,"data_dir"),Bc.forEach(a),kp=i(Z," kwarg of "),Bt=o(Z,"A",{href:!0});var Jc=n(Bt);Ep=i(Jc,"datasets.load_dataset()"),Jc.forEach(a),qp=i(Z,", which is a shorthand syntax for "),ol=o(Z,"CODE",{});var Yc=n(ol);Pp=i(Yc,"data_files=os.path.join(data_dir, **)"),Yc.forEach(a),Ap=i(Z,"."),Z.forEach(a),tn=f(s),m(Ra.$$.fragment,s),en=f(s),Jt=o(s,"P",{});var Gc=n(Jt);Sp=i(Gc,"To load remote image folders via HTTP, you can pass the URLs:"),Gc.forEach(a),ln=f(s),m(za.$$.fragment,s),on=f(s),C=o(s,"P",{});var da=n(C);Tp=i(da,"The resulting dataset will include an "),nl=o(da,"CODE",{});var Wc=n(nl);Dp=i(Wc,"image"),Wc.forEach(a),Ip=i(da," feature, which is a "),rl=o(da,"CODE",{});var Qc=n(rl);Np=i(Qc,"PIL.Image"),Qc.forEach(a),Cp=i(da," loaded from the image file, and the corresponding "),il=o(da,"CODE",{});var Xc=n(il);Op=i(Xc,"label"),Xc.forEach(a),Hp=i(da," inferred from the directory structure."),da.forEach(a),nn=f(s),ps=o(s,"H2",{class:!0});var Er=n(ps);zs=o(Er,"A",{id:!0,class:!0,href:!0});var Kc=n(zs);pl=o(Kc,"SPAN",{});var Zc=n(pl);m(Ma.$$.fragment,Zc),Zc.forEach(a),Kc.forEach(a),Fp=f(Er),dl=o(Er,"SPAN",{});var sh=n(dl);Lp=i(sh,"In-memory data"),sh.forEach(a),Er.forEach(a),rn=f(s),Ms=o(s,"P",{});var qr=n(Ms);Rp=i(qr,"\u{1F917} Datasets will also allow you to create a "),Yt=o(qr,"A",{href:!0});var ah=n(Yt);zp=i(ah,"datasets.Dataset"),ah.forEach(a),Mp=i(qr," directly from in-memory data structures like Python dictionaries and Pandas DataFrames."),qr.forEach(a),pn=f(s),ds=o(s,"H3",{class:!0});var Pr=n(ds);Us=o(Pr,"A",{id:!0,class:!0,href:!0});var th=n(Us);fl=o(th,"SPAN",{});var eh=n(fl);m(Ua.$$.fragment,eh),eh.forEach(a),th.forEach(a),Up=f(Pr),cl=o(Pr,"SPAN",{});var lh=n(cl);Vp=i(lh,"Python dictionary"),lh.forEach(a),Pr.forEach(a),dn=f(s),Vs=o(s,"P",{});var Ar=n(Vs);Bp=i(Ar,"Load Python dictionaries with "),Gt=o(Ar,"A",{href:!0});var oh=n(Gt);Jp=i(oh,"datasets.Dataset.from_dict()"),oh.forEach(a),Yp=i(Ar,":"),Ar.forEach(a),fn=f(s),m(Va.$$.fragment,s),cn=f(s),fs=o(s,"H3",{class:!0});var Sr=n(fs);Bs=o(Sr,"A",{id:!0,class:!0,href:!0});var nh=n(Bs);hl=o(nh,"SPAN",{});var rh=n(hl);m(Ba.$$.fragment,rh),rh.forEach(a),nh.forEach(a),Gp=f(Sr),ul=o(Sr,"SPAN",{});var ih=n(ul);Wp=i(ih,"Pandas DataFrame"),ih.forEach(a),Sr.forEach(a),hn=f(s),Js=o(s,"P",{});var Tr=n(Js);Qp=i(Tr,"Load Pandas DataFrames with "),Wt=o(Tr,"A",{href:!0});var ph=n(Wt);Xp=i(ph,"datasets.Dataset.from_pandas()"),ph.forEach(a),Kp=i(Tr,":"),Tr.forEach(a),un=f(s),m(Ja.$$.fragment,s),mn=f(s),m(Ys.$$.fragment,s),gn=f(s),cs=o(s,"H2",{class:!0});var Dr=n(cs);Gs=o(Dr,"A",{id:!0,class:!0,href:!0});var dh=n(Gs);ml=o(dh,"SPAN",{});var fh=n(ml);m(Ya.$$.fragment,fh),fh.forEach(a),dh.forEach(a),Zp=f(Dr),gl=o(Dr,"SPAN",{});var ch=n(gl);sd=i(ch,"Offline"),ch.forEach(a),Dr.forEach(a),_n=f(s),Qt=o(s,"P",{});var hh=n(Qt);ad=i(hh,"Even if you don\u2019t have an internet connection, it is still possible to load a dataset. As long as you\u2019ve downloaded a dataset from the Hub or \u{1F917} Datasets GitHub repository before, it should be cached. This means you can reload the dataset from the cache and use it offline."),hh.forEach(a),vn=f(s),B=o(s,"P",{});var ve=n(B);td=i(ve,"If you know you won\u2019t have internet access, you can run \u{1F917} Datasets in full offline mode. This saves time because instead of waiting for the Dataset builder download to time out, \u{1F917} Datasets will look directly in the cache. Set the environment variable "),_l=o(ve,"CODE",{});var uh=n(_l);ed=i(uh,"HF_DATASETS_OFFLINE"),uh.forEach(a),ld=i(ve," to "),vl=o(ve,"CODE",{});var mh=n(vl);od=i(mh,"1"),mh.forEach(a),nd=i(ve," to enable full offline mode."),ve.forEach(a),$n=f(s),hs=o(s,"H2",{class:!0});var Ir=n(hs);Ws=o(Ir,"A",{id:!0,class:!0,href:!0});var gh=n(Ws);$l=o(gh,"SPAN",{});var _h=n($l);m(Ga.$$.fragment,_h),_h.forEach(a),gh.forEach(a),rd=f(Ir),yl=o(Ir,"SPAN",{});var vh=n(yl);id=i(vh,"Slice splits"),vh.forEach(a),Ir.forEach(a),yn=f(s),J=o(s,"P",{});var $e=n(J);pd=i($e,"For even greater control over how to load a split, you can choose to only load specific slices of a split. There are two options for slicing a split: using strings or "),Xt=o($e,"A",{href:!0});var $h=n(Xt);dd=i($h,"datasets.ReadInstruction"),$h.forEach(a),fd=i($e,". Strings are more compact and readable for simple cases, while "),Kt=o($e,"A",{href:!0});var yh=n(Kt);cd=i(yh,"datasets.ReadInstruction"),yh.forEach(a),hd=i($e," is easier to use with variable slicing parameters."),$e.forEach(a),wn=f(s),Y=o(s,"P",{});var ye=n(Y);ud=i(ye,"Concatenate the "),wl=o(ye,"CODE",{});var wh=n(wl);md=i(wh,"train"),wh.forEach(a),gd=i(ye," and "),jl=o(ye,"CODE",{});var jh=n(jl);_d=i(jh,"test"),jh.forEach(a),vd=i(ye," split by:"),ye.forEach(a),jn=f(s),m(Wa.$$.fragment,s),bn=f(s),Qs=o(s,"P",{});var Nr=n(Qs);$d=i(Nr,"Select specific rows of the "),bl=o(Nr,"CODE",{});var bh=n(bl);yd=i(bh,"train"),bh.forEach(a),wd=i(Nr," split:"),Nr.forEach(a),xn=f(s),m(Qa.$$.fragment,s),kn=f(s),Zt=o(s,"P",{});var xh=n(Zt);jd=i(xh,"Or select a percentage of the split with:"),xh.forEach(a),En=f(s),m(Xa.$$.fragment,s),qn=f(s),se=o(s,"P",{});var kh=n(se);bd=i(kh,"You can even select a combination of percentages from each split:"),kh.forEach(a),Pn=f(s),m(Ka.$$.fragment,s),An=f(s),ae=o(s,"P",{});var Eh=n(ae);xd=i(Eh,"Finally, create cross-validated dataset splits by:"),Eh.forEach(a),Sn=f(s),m(Za.$$.fragment,s),Tn=f(s),us=o(s,"H3",{class:!0});var Cr=n(us);Xs=o(Cr,"A",{id:!0,class:!0,href:!0});var qh=n(Xs);xl=o(qh,"SPAN",{});var Ph=n(xl);m(st.$$.fragment,Ph),Ph.forEach(a),qh.forEach(a),kd=f(Cr),kl=o(Cr,"SPAN",{});var Ah=n(kl);Ed=i(Ah,"Percent slicing and rounding"),Ah.forEach(a),Cr.forEach(a),Dn=f(s),te=o(s,"P",{});var Sh=n(te);qd=i(Sh,"For datasets where the requested slice boundaries do not divide evenly by 100, the default behavior is to round the boundaries to the nearest integer. As a result, some slices may contain more examples than others as shown in the following example:"),Sh.forEach(a),In=f(s),m(at.$$.fragment,s),Nn=f(s),Ks=o(s,"P",{});var Or=n(Ks);Pd=i(Or,"If you want equal sized splits, use "),El=o(Or,"CODE",{});var Th=n(El);Ad=i(Th,"pct1_dropremainder"),Th.forEach(a),Sd=i(Or," rounding instead. This will treat the specified percentage boundaries as multiples of 1%."),Or.forEach(a),Cn=f(s),m(tt.$$.fragment,s),On=f(s),m(Zs.$$.fragment,s),Hn=f(s),ee=o(s,"A",{id:!0}),n(ee).forEach(a),Fn=f(s),ms=o(s,"H2",{class:!0});var Hr=n(ms);sa=o(Hr,"A",{id:!0,class:!0,href:!0});var Dh=n(sa);ql=o(Dh,"SPAN",{});var Ih=n(ql);m(et.$$.fragment,Ih),Ih.forEach(a),Dh.forEach(a),Td=f(Hr),Pl=o(Hr,"SPAN",{});var Nh=n(Pl);Dd=i(Nh,"Troubleshooting"),Nh.forEach(a),Hr.forEach(a),Ln=f(s),le=o(s,"P",{});var Ch=n(le);Id=i(Ch,"Sometimes, you may get unexpected results when you load a dataset. In this section, you will learn how to solve two common issues you may encounter when you load a dataset: manually download a dataset, and specify features of a dataset."),Ch.forEach(a),Rn=f(s),gs=o(s,"H3",{class:!0});var Fr=n(gs);aa=o(Fr,"A",{id:!0,class:!0,href:!0});var Oh=n(aa);Al=o(Oh,"SPAN",{});var Hh=n(Al);m(lt.$$.fragment,Hh),Hh.forEach(a),Oh.forEach(a),Nd=f(Fr),Sl=o(Fr,"SPAN",{});var Fh=n(Sl);Cd=i(Fh,"Manual download"),Fh.forEach(a),Fr.forEach(a),zn=f(s),O=o(s,"P",{});var fa=n(O);Od=i(fa,"Certain datasets require you to manually download the dataset files due to licensing incompatibility, or if the files are hidden behind a login page. This will cause "),oe=o(fa,"A",{href:!0});var Lh=n(oe);Hd=i(Lh,"datasets.load_dataset()"),Lh.forEach(a),Fd=i(fa," to throw an "),Tl=o(fa,"CODE",{});var Rh=n(Tl);Ld=i(Rh,"AssertionError"),Rh.forEach(a),Rd=i(fa,". But \u{1F917} Datasets provides detailed instructions for downloading the missing files. After you have downloaded the files, use the "),Dl=o(fa,"CODE",{});var zh=n(Dl);zd=i(zh,"data_dir"),zh.forEach(a),Md=i(fa," argument to specify the path to the files you just downloaded."),fa.forEach(a),Mn=f(s),ta=o(s,"P",{});var Lr=n(ta);Ud=i(Lr,"For example, if you try to download a configuration from the "),ot=o(Lr,"A",{href:!0,rel:!0});var Mh=n(ot);Vd=i(Mh,"MATINF"),Mh.forEach(a),Bd=i(Lr," dataset:"),Lr.forEach(a),Un=f(s),m(nt.$$.fragment,s),Vn=f(s),_s=o(s,"H3",{class:!0});var Rr=n(_s);ea=o(Rr,"A",{id:!0,class:!0,href:!0});var Uh=n(ea);Il=o(Uh,"SPAN",{});var Vh=n(Il);m(rt.$$.fragment,Vh),Vh.forEach(a),Uh.forEach(a),Jd=f(Rr),Nl=o(Rr,"SPAN",{});var Bh=n(Nl);Yd=i(Bh,"Specify features"),Bh.forEach(a),Rr.forEach(a),Bn=f(s),G=o(s,"P",{});var we=n(G);Gd=i(we,"When you create a dataset from local files, the "),ne=o(we,"A",{href:!0});var Jh=n(ne);Wd=i(Jh,"datasets.Features"),Jh.forEach(a),Qd=i(we," are automatically inferred by "),it=o(we,"A",{href:!0,rel:!0});var Yh=n(it);Xd=i(Yh,"Apache Arrow"),Yh.forEach(a),Kd=i(we,". However, the features of the dataset may not always align with your expectations or you may want to define the features yourself."),we.forEach(a),Jn=f(s),W=o(s,"P",{});var je=n(W);Zd=i(je,"The following example shows how you can add custom labels with "),re=o(je,"A",{href:!0});var Gh=n(re);sf=i(Gh,"datasets.ClassLabel"),Gh.forEach(a),af=i(je,". First, define your own labels using the "),ie=o(je,"A",{href:!0});var Wh=n(ie);tf=i(Wh,"datasets.Features"),Wh.forEach(a),ef=i(je," class:"),je.forEach(a),Yn=f(s),m(pt.$$.fragment,s),Gn=f(s),Q=o(s,"P",{});var be=n(Q);lf=i(be,"Next, specify the "),Cl=o(be,"CODE",{});var Qh=n(Cl);of=i(Qh,"features"),Qh.forEach(a),nf=i(be," argument in "),pe=o(be,"A",{href:!0});var Xh=n(pe);rf=i(Xh,"datasets.load_dataset()"),Xh.forEach(a),pf=i(be," with the features you just created:"),be.forEach(a),Wn=f(s),m(dt.$$.fragment,s),Qn=f(s),de=o(s,"P",{});var Kh=n(de);df=i(Kh,"Now when you look at your dataset features, you can see it uses the custom labels you defined:"),Kh.forEach(a),Xn=f(s),m(ft.$$.fragment,s),Kn=f(s),vs=o(s,"H2",{class:!0});var zr=n(vs);la=o(zr,"A",{id:!0,class:!0,href:!0});var Zh=n(la);Ol=o(Zh,"SPAN",{});var su=n(Ol);m(ct.$$.fragment,su),su.forEach(a),Zh.forEach(a),ff=f(zr),Hl=o(zr,"SPAN",{});var au=n(Hl);cf=i(au,"Metrics"),au.forEach(a),zr.forEach(a),Zn=f(s),fe=o(s,"P",{});var tu=n(fe);hf=i(tu,"When the metric you want to use is not supported by \u{1F917} Datasets, you can write and use your own metric script. Load your metric by providing the path to your local metric loading script:"),tu.forEach(a),sr=f(s),m(ht.$$.fragment,s),ar=f(s),m(oa.$$.fragment,s),tr=f(s),$s=o(s,"H3",{class:!0});var Mr=n($s);na=o(Mr,"A",{id:!0,class:!0,href:!0});var eu=n(na);Fl=o(eu,"SPAN",{});var lu=n(Fl);m(ut.$$.fragment,lu),lu.forEach(a),eu.forEach(a),uf=f(Mr),Ll=o(Mr,"SPAN",{});var ou=n(Ll);mf=i(ou,"Load configurations"),ou.forEach(a),Mr.forEach(a),er=f(s),ce=o(s,"P",{});var nu=n(ce);gf=i(nu,"It is possible for a metric to have different configurations. The configurations are stored in the [\u2018datasets.Metric.config_name\u2019] attribute. When you load a metric, provide the configuration name as shown in the following:"),nu.forEach(a),lr=f(s),m(mt.$$.fragment,s),or=f(s),ys=o(s,"H3",{class:!0});var Ur=n(ys);ra=o(Ur,"A",{id:!0,class:!0,href:!0});var ru=n(ra);Rl=o(ru,"SPAN",{});var iu=n(Rl);m(gt.$$.fragment,iu),iu.forEach(a),ru.forEach(a),_f=f(Ur),zl=o(Ur,"SPAN",{});var pu=n(zl);vf=i(pu,"Distributed setup"),pu.forEach(a),Ur.forEach(a),nr=f(s),he=o(s,"P",{});var du=n(he);$f=i(du,"When you work in a distributed or parallel processing environment, loading and computing a metric can be tricky because these processes are executed in parallel on separate subsets of the data. \u{1F917} Datasets supports distributed usage with a few additional arguments when you load a metric."),du.forEach(a),rr=f(s),ue=o(s,"P",{});var fu=n(ue);yf=i(fu,"For example, imagine you are training and evaluating on eight parallel processes. Here\u2019s how you would load a metric in this distributed setting:"),fu.forEach(a),ir=f(s),X=o(s,"OL",{});var xe=n(X);Ml=o(xe,"LI",{});var cu=n(Ml);_t=o(cu,"P",{});var Vr=n(_t);wf=i(Vr,"Define the total number of processes with the "),Ul=o(Vr,"CODE",{});var hu=n(Ul);jf=i(hu,"num_process"),hu.forEach(a),bf=i(Vr," argument."),Vr.forEach(a),cu.forEach(a),xf=f(xe),Vl=o(xe,"LI",{});var uu=n(Vl);ws=o(uu,"P",{});var ke=n(ws);kf=i(ke,"Set the process "),Bl=o(ke,"CODE",{});var mu=n(Bl);Ef=i(mu,"rank"),mu.forEach(a),qf=i(ke," as an integer between zero and "),Jl=o(ke,"CODE",{});var gu=n(Jl);Pf=i(gu,"num_process - 1"),gu.forEach(a),Af=i(ke,"."),ke.forEach(a),uu.forEach(a),Sf=f(xe),Yl=o(xe,"LI",{});var _u=n(Yl);vt=o(_u,"P",{});var Br=n(vt);Tf=i(Br,"Load your metric with "),me=o(Br,"A",{href:!0});var vu=n(me);Df=i(vu,"datasets.load_metric()"),vu.forEach(a),If=i(Br," with these arguments:"),Br.forEach(a),_u.forEach(a),xe.forEach(a),pr=f(s),m($t.$$.fragment,s),dr=f(s),m(ia.$$.fragment,s),fr=f(s),pa=o(s,"P",{});var Jr=n(pa);Nf=i(Jr,"In some instances, you may be simultaneously running multiple independent distributed evaluations on the same server and files. To avoid any conflicts, it is important to provide an "),Gl=o(Jr,"CODE",{});var $u=n(Gl);Cf=i($u,"experiment_id"),$u.forEach(a),Of=i(Jr," to distinguish the separate evaluations:"),Jr.forEach(a),cr=f(s),m(yt.$$.fragment,s),this.h()},h(){c(h,"name","hf:doc:metadata"),c(h,"content",JSON.stringify(Tu)),c(x,"id","load"),c(x,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(x,"href","#load"),c(y,"class","relative group"),c(Et,"id","load-from-the-hub"),c(qs,"id","hugging-face-hub"),c(qs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(qs,"href","#hugging-face-hub"),c(ts,"class","relative group"),c(qt,"href","/docs/datasets/pr_3690/en/package_reference/loading_methods#datasets.load_dataset"),c(ha,"href","https://huggingface.co/datasets/lhoestq/demo1"),c(ha,"rel","nofollow"),c(_a,"href","https://huggingface.co/datasets/allenai/c4"),c(_a,"rel","nofollow"),c(Is,"id","local-and-remote-files"),c(Is,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Is,"href","#local-and-remote-files"),c(es,"class","relative group"),c(At,"href","/docs/datasets/pr_3690/en/package_reference/loading_methods#datasets.load_dataset"),c(Ns,"id","csv"),c(Ns,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Ns,"href","#csv"),c(ls,"class","relative group"),c(Cs,"id","json"),c(Cs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Cs,"href","#json"),c(os,"class","relative group"),c(Ct,"href","/docs/datasets/pr_3690/en/package_reference/loading_methods#datasets.load_dataset"),c(Fs,"id","text-files"),c(Fs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Fs,"href","#text-files"),c(ns,"class","relative group"),c(Ls,"id","parquet"),c(Ls,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Ls,"href","#parquet"),c(rs,"class","relative group"),c(Rs,"id","image-folders"),c(Rs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Rs,"href","#image-folders"),c(is,"class","relative group"),c(Bt,"href","/docs/datasets/pr_3690/en/package_reference/loading_methods#datasets.load_dataset"),c(zs,"id","inmemory-data"),c(zs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(zs,"href","#inmemory-data"),c(ps,"class","relative group"),c(Yt,"href","/docs/datasets/pr_3690/en/package_reference/main_classes#datasets.Dataset"),c(Us,"id","python-dictionary"),c(Us,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Us,"href","#python-dictionary"),c(ds,"class","relative group"),c(Gt,"href","/docs/datasets/pr_3690/en/package_reference/main_classes#datasets.Dataset.from_dict"),c(Bs,"id","pandas-dataframe"),c(Bs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Bs,"href","#pandas-dataframe"),c(fs,"class","relative group"),c(Wt,"href","/docs/datasets/pr_3690/en/package_reference/main_classes#datasets.Dataset.from_pandas"),c(Gs,"id","offline"),c(Gs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Gs,"href","#offline"),c(cs,"class","relative group"),c(Ws,"id","slice-splits"),c(Ws,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Ws,"href","#slice-splits"),c(hs,"class","relative group"),c(Xt,"href","/docs/datasets/pr_3690/en/package_reference/builder_classes#datasets.ReadInstruction"),c(Kt,"href","/docs/datasets/pr_3690/en/package_reference/builder_classes#datasets.ReadInstruction"),c(Xs,"id","percent-slicing-and-rounding"),c(Xs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Xs,"href","#percent-slicing-and-rounding"),c(us,"class","relative group"),c(ee,"id","troubleshoot"),c(sa,"id","troubleshooting"),c(sa,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(sa,"href","#troubleshooting"),c(ms,"class","relative group"),c(aa,"id","manual-download"),c(aa,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(aa,"href","#manual-download"),c(gs,"class","relative group"),c(oe,"href","/docs/datasets/pr_3690/en/package_reference/loading_methods#datasets.load_dataset"),c(ot,"href","https://huggingface.co/datasets/matinf"),c(ot,"rel","nofollow"),c(ea,"id","specify-features"),c(ea,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(ea,"href","#specify-features"),c(_s,"class","relative group"),c(ne,"href","/docs/datasets/pr_3690/en/package_reference/main_classes#datasets.Features"),c(it,"href","https://arrow.apache.org/docs/"),c(it,"rel","nofollow"),c(re,"href","/docs/datasets/pr_3690/en/package_reference/main_classes#datasets.ClassLabel"),c(ie,"href","/docs/datasets/pr_3690/en/package_reference/main_classes#datasets.Features"),c(pe,"href","/docs/datasets/pr_3690/en/package_reference/loading_methods#datasets.load_dataset"),c(la,"id","metrics"),c(la,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(la,"href","#metrics"),c(vs,"class","relative group"),c(na,"id","load-configurations"),c(na,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(na,"href","#load-configurations"),c($s,"class","relative group"),c(ra,"id","distributed-setup"),c(ra,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(ra,"href","#distributed-setup"),c(ys,"class","relative group"),c(me,"href","/docs/datasets/pr_3690/en/package_reference/loading_methods#datasets.load_metric")},m(s,e){t(document.head,h),p(s,k,e),p(s,y,e),t(y,x),t(x,E),g(w,E,null),t(y,j),t(y,q),t(q,ss),p(s,js,e),p(s,R,e),t(R,as),p(s,bs,e),p(s,z,e),t(z,M),p(s,xs,e),p(s,A,e),t(A,L),t(L,S),t(A,jt),t(A,ks),t(ks,bt),t(A,xt),t(A,Es),t(Es,Yr),t(A,Gr),t(A,qe),t(qe,Wr),t(A,Qr),t(A,Pe),t(Pe,Xr),p(s,so,e),p(s,kt,e),t(kt,Kr),p(s,ao,e),p(s,Et,e),p(s,to,e),p(s,ts,e),t(ts,qs),t(qs,Ae),g(ca,Ae,null),t(ts,Zr),t(ts,Se),t(Se,si),p(s,eo,e),p(s,Ps,e),t(Ps,ai),t(Ps,Te),t(Te,ti),t(Ps,ei),p(s,lo,e),p(s,U,e),t(U,li),t(U,qt),t(qt,oi),t(U,ni),t(U,ha),t(ha,ri),t(U,ii),p(s,oo,e),g(ua,s,e),p(s,no,e),p(s,Pt,e),t(Pt,pi),p(s,ro,e),p(s,As,e),t(As,di),t(As,De),t(De,fi),t(As,ci),p(s,io,e),g(ma,s,e),p(s,po,e),g(Ss,s,e),p(s,fo,e),p(s,T,e),t(T,hi),t(T,Ie),t(Ie,ui),t(T,mi),t(T,Ne),t(Ne,gi),t(T,_i),t(T,Ce),t(Ce,vi),t(T,$i),t(T,Oe),t(Oe,yi),t(T,wi),t(T,He),t(He,ji),t(T,bi),p(s,co,e),g(ga,s,e),p(s,ho,e),g(Ts,s,e),p(s,uo,e),p(s,V,e),t(V,xi),t(V,Fe),t(Fe,ki),t(V,Ei),t(V,_a),t(_a,qi),t(V,Pi),p(s,mo,e),g(va,s,e),p(s,go,e),p(s,Ds,e),t(Ds,Ai),t(Ds,Le),t(Le,Si),t(Ds,Ti),p(s,_o,e),g($a,s,e),p(s,vo,e),p(s,es,e),t(es,Is),t(Is,Re),g(ya,Re,null),t(es,Di),t(es,ze),t(ze,Ii),p(s,$o,e),p(s,D,e),t(D,Ni),t(D,Me),t(Me,Ci),t(D,Oi),t(D,Ue),t(Ue,Hi),t(D,Fi),t(D,Ve),t(Ve,Li),t(D,Ri),t(D,Be),t(Be,zi),t(D,Mi),t(D,At),t(At,Ui),t(D,Vi),p(s,yo,e),p(s,ls,e),t(ls,Ns),t(Ns,Je),g(wa,Je,null),t(ls,Bi),t(ls,Ye),t(Ye,Ji),p(s,wo,e),p(s,St,e),t(St,Yi),p(s,jo,e),g(ja,s,e),p(s,bo,e),p(s,Tt,e),t(Tt,Gi),p(s,xo,e),g(ba,s,e),p(s,ko,e),p(s,Dt,e),t(Dt,Wi),p(s,Eo,e),g(xa,s,e),p(s,qo,e),p(s,It,e),t(It,Qi),p(s,Po,e),g(ka,s,e),p(s,Ao,e),p(s,Nt,e),t(Nt,Xi),p(s,So,e),g(Ea,s,e),p(s,To,e),p(s,os,e),t(os,Cs),t(Cs,Ge),g(qa,Ge,null),t(os,Ki),t(os,We),t(We,Zi),p(s,Do,e),p(s,Os,e),t(Os,sp),t(Os,Ct),t(Ct,ap),t(Os,tp),p(s,Io,e),g(Pa,s,e),p(s,No,e),p(s,Ot,e),t(Ot,ep),p(s,Co,e),g(Aa,s,e),p(s,Oo,e),p(s,Hs,e),t(Hs,lp),t(Hs,Qe),t(Qe,op),t(Hs,np),p(s,Ho,e),g(Sa,s,e),p(s,Fo,e),p(s,Ht,e),t(Ht,rp),p(s,Lo,e),g(Ta,s,e),p(s,Ro,e),p(s,Ft,e),t(Ft,ip),p(s,zo,e),p(s,ns,e),t(ns,Fs),t(Fs,Xe),g(Da,Xe,null),t(ns,pp),t(ns,Ke),t(Ke,dp),p(s,Mo,e),p(s,Lt,e),t(Lt,fp),p(s,Uo,e),g(Ia,s,e),p(s,Vo,e),p(s,Rt,e),t(Rt,cp),p(s,Bo,e),g(Na,s,e),p(s,Jo,e),p(s,rs,e),t(rs,Ls),t(Ls,Ze),g(Ca,Ze,null),t(rs,hp),t(rs,sl),t(sl,up),p(s,Yo,e),p(s,zt,e),t(zt,mp),p(s,Go,e),g(Oa,s,e),p(s,Wo,e),p(s,Mt,e),t(Mt,gp),p(s,Qo,e),g(Ha,s,e),p(s,Xo,e),p(s,is,e),t(is,Rs),t(Rs,al),g(Fa,al,null),t(is,_p),t(is,tl),t(tl,vp),p(s,Ko,e),p(s,Ut,e),t(Ut,$p),p(s,Zo,e),p(s,Vt,e),t(Vt,yp),p(s,sn,e),g(La,s,e),p(s,an,e),p(s,N,e),t(N,wp),t(N,el),t(el,jp),t(N,bp),t(N,ll),t(ll,xp),t(N,kp),t(N,Bt),t(Bt,Ep),t(N,qp),t(N,ol),t(ol,Pp),t(N,Ap),p(s,tn,e),g(Ra,s,e),p(s,en,e),p(s,Jt,e),t(Jt,Sp),p(s,ln,e),g(za,s,e),p(s,on,e),p(s,C,e),t(C,Tp),t(C,nl),t(nl,Dp),t(C,Ip),t(C,rl),t(rl,Np),t(C,Cp),t(C,il),t(il,Op),t(C,Hp),p(s,nn,e),p(s,ps,e),t(ps,zs),t(zs,pl),g(Ma,pl,null),t(ps,Fp),t(ps,dl),t(dl,Lp),p(s,rn,e),p(s,Ms,e),t(Ms,Rp),t(Ms,Yt),t(Yt,zp),t(Ms,Mp),p(s,pn,e),p(s,ds,e),t(ds,Us),t(Us,fl),g(Ua,fl,null),t(ds,Up),t(ds,cl),t(cl,Vp),p(s,dn,e),p(s,Vs,e),t(Vs,Bp),t(Vs,Gt),t(Gt,Jp),t(Vs,Yp),p(s,fn,e),g(Va,s,e),p(s,cn,e),p(s,fs,e),t(fs,Bs),t(Bs,hl),g(Ba,hl,null),t(fs,Gp),t(fs,ul),t(ul,Wp),p(s,hn,e),p(s,Js,e),t(Js,Qp),t(Js,Wt),t(Wt,Xp),t(Js,Kp),p(s,un,e),g(Ja,s,e),p(s,mn,e),g(Ys,s,e),p(s,gn,e),p(s,cs,e),t(cs,Gs),t(Gs,ml),g(Ya,ml,null),t(cs,Zp),t(cs,gl),t(gl,sd),p(s,_n,e),p(s,Qt,e),t(Qt,ad),p(s,vn,e),p(s,B,e),t(B,td),t(B,_l),t(_l,ed),t(B,ld),t(B,vl),t(vl,od),t(B,nd),p(s,$n,e),p(s,hs,e),t(hs,Ws),t(Ws,$l),g(Ga,$l,null),t(hs,rd),t(hs,yl),t(yl,id),p(s,yn,e),p(s,J,e),t(J,pd),t(J,Xt),t(Xt,dd),t(J,fd),t(J,Kt),t(Kt,cd),t(J,hd),p(s,wn,e),p(s,Y,e),t(Y,ud),t(Y,wl),t(wl,md),t(Y,gd),t(Y,jl),t(jl,_d),t(Y,vd),p(s,jn,e),g(Wa,s,e),p(s,bn,e),p(s,Qs,e),t(Qs,$d),t(Qs,bl),t(bl,yd),t(Qs,wd),p(s,xn,e),g(Qa,s,e),p(s,kn,e),p(s,Zt,e),t(Zt,jd),p(s,En,e),g(Xa,s,e),p(s,qn,e),p(s,se,e),t(se,bd),p(s,Pn,e),g(Ka,s,e),p(s,An,e),p(s,ae,e),t(ae,xd),p(s,Sn,e),g(Za,s,e),p(s,Tn,e),p(s,us,e),t(us,Xs),t(Xs,xl),g(st,xl,null),t(us,kd),t(us,kl),t(kl,Ed),p(s,Dn,e),p(s,te,e),t(te,qd),p(s,In,e),g(at,s,e),p(s,Nn,e),p(s,Ks,e),t(Ks,Pd),t(Ks,El),t(El,Ad),t(Ks,Sd),p(s,Cn,e),g(tt,s,e),p(s,On,e),g(Zs,s,e),p(s,Hn,e),p(s,ee,e),p(s,Fn,e),p(s,ms,e),t(ms,sa),t(sa,ql),g(et,ql,null),t(ms,Td),t(ms,Pl),t(Pl,Dd),p(s,Ln,e),p(s,le,e),t(le,Id),p(s,Rn,e),p(s,gs,e),t(gs,aa),t(aa,Al),g(lt,Al,null),t(gs,Nd),t(gs,Sl),t(Sl,Cd),p(s,zn,e),p(s,O,e),t(O,Od),t(O,oe),t(oe,Hd),t(O,Fd),t(O,Tl),t(Tl,Ld),t(O,Rd),t(O,Dl),t(Dl,zd),t(O,Md),p(s,Mn,e),p(s,ta,e),t(ta,Ud),t(ta,ot),t(ot,Vd),t(ta,Bd),p(s,Un,e),g(nt,s,e),p(s,Vn,e),p(s,_s,e),t(_s,ea),t(ea,Il),g(rt,Il,null),t(_s,Jd),t(_s,Nl),t(Nl,Yd),p(s,Bn,e),p(s,G,e),t(G,Gd),t(G,ne),t(ne,Wd),t(G,Qd),t(G,it),t(it,Xd),t(G,Kd),p(s,Jn,e),p(s,W,e),t(W,Zd),t(W,re),t(re,sf),t(W,af),t(W,ie),t(ie,tf),t(W,ef),p(s,Yn,e),g(pt,s,e),p(s,Gn,e),p(s,Q,e),t(Q,lf),t(Q,Cl),t(Cl,of),t(Q,nf),t(Q,pe),t(pe,rf),t(Q,pf),p(s,Wn,e),g(dt,s,e),p(s,Qn,e),p(s,de,e),t(de,df),p(s,Xn,e),g(ft,s,e),p(s,Kn,e),p(s,vs,e),t(vs,la),t(la,Ol),g(ct,Ol,null),t(vs,ff),t(vs,Hl),t(Hl,cf),p(s,Zn,e),p(s,fe,e),t(fe,hf),p(s,sr,e),g(ht,s,e),p(s,ar,e),g(oa,s,e),p(s,tr,e),p(s,$s,e),t($s,na),t(na,Fl),g(ut,Fl,null),t($s,uf),t($s,Ll),t(Ll,mf),p(s,er,e),p(s,ce,e),t(ce,gf),p(s,lr,e),g(mt,s,e),p(s,or,e),p(s,ys,e),t(ys,ra),t(ra,Rl),g(gt,Rl,null),t(ys,_f),t(ys,zl),t(zl,vf),p(s,nr,e),p(s,he,e),t(he,$f),p(s,rr,e),p(s,ue,e),t(ue,yf),p(s,ir,e),p(s,X,e),t(X,Ml),t(Ml,_t),t(_t,wf),t(_t,Ul),t(Ul,jf),t(_t,bf),t(X,xf),t(X,Vl),t(Vl,ws),t(ws,kf),t(ws,Bl),t(Bl,Ef),t(ws,qf),t(ws,Jl),t(Jl,Pf),t(ws,Af),t(X,Sf),t(X,Yl),t(Yl,vt),t(vt,Tf),t(vt,me),t(me,Df),t(vt,If),p(s,pr,e),g($t,s,e),p(s,dr,e),g(ia,s,e),p(s,fr,e),p(s,pa,e),t(pa,Nf),t(pa,Gl),t(Gl,Cf),t(pa,Of),p(s,cr,e),g(yt,s,e),hr=!0},p(s,[e]){const wt={};e&2&&(wt.$$scope={dirty:e,ctx:s}),Ss.$set(wt);const Wl={};e&2&&(Wl.$$scope={dirty:e,ctx:s}),Ts.$set(Wl);const Ql={};e&2&&(Ql.$$scope={dirty:e,ctx:s}),Ys.$set(Ql);const Xl={};e&2&&(Xl.$$scope={dirty:e,ctx:s}),Zs.$set(Xl);const Kl={};e&2&&(Kl.$$scope={dirty:e,ctx:s}),oa.$set(Kl);const Zl={};e&2&&(Zl.$$scope={dirty:e,ctx:s}),ia.$set(Zl)},i(s){hr||(_(w.$$.fragment,s),_(ca.$$.fragment,s),_(ua.$$.fragment,s),_(ma.$$.fragment,s),_(Ss.$$.fragment,s),_(ga.$$.fragment,s),_(Ts.$$.fragment,s),_(va.$$.fragment,s),_($a.$$.fragment,s),_(ya.$$.fragment,s),_(wa.$$.fragment,s),_(ja.$$.fragment,s),_(ba.$$.fragment,s),_(xa.$$.fragment,s),_(ka.$$.fragment,s),_(Ea.$$.fragment,s),_(qa.$$.fragment,s),_(Pa.$$.fragment,s),_(Aa.$$.fragment,s),_(Sa.$$.fragment,s),_(Ta.$$.fragment,s),_(Da.$$.fragment,s),_(Ia.$$.fragment,s),_(Na.$$.fragment,s),_(Ca.$$.fragment,s),_(Oa.$$.fragment,s),_(Ha.$$.fragment,s),_(Fa.$$.fragment,s),_(La.$$.fragment,s),_(Ra.$$.fragment,s),_(za.$$.fragment,s),_(Ma.$$.fragment,s),_(Ua.$$.fragment,s),_(Va.$$.fragment,s),_(Ba.$$.fragment,s),_(Ja.$$.fragment,s),_(Ys.$$.fragment,s),_(Ya.$$.fragment,s),_(Ga.$$.fragment,s),_(Wa.$$.fragment,s),_(Qa.$$.fragment,s),_(Xa.$$.fragment,s),_(Ka.$$.fragment,s),_(Za.$$.fragment,s),_(st.$$.fragment,s),_(at.$$.fragment,s),_(tt.$$.fragment,s),_(Zs.$$.fragment,s),_(et.$$.fragment,s),_(lt.$$.fragment,s),_(nt.$$.fragment,s),_(rt.$$.fragment,s),_(pt.$$.fragment,s),_(dt.$$.fragment,s),_(ft.$$.fragment,s),_(ct.$$.fragment,s),_(ht.$$.fragment,s),_(oa.$$.fragment,s),_(ut.$$.fragment,s),_(mt.$$.fragment,s),_(gt.$$.fragment,s),_($t.$$.fragment,s),_(ia.$$.fragment,s),_(yt.$$.fragment,s),hr=!0)},o(s){v(w.$$.fragment,s),v(ca.$$.fragment,s),v(ua.$$.fragment,s),v(ma.$$.fragment,s),v(Ss.$$.fragment,s),v(ga.$$.fragment,s),v(Ts.$$.fragment,s),v(va.$$.fragment,s),v($a.$$.fragment,s),v(ya.$$.fragment,s),v(wa.$$.fragment,s),v(ja.$$.fragment,s),v(ba.$$.fragment,s),v(xa.$$.fragment,s),v(ka.$$.fragment,s),v(Ea.$$.fragment,s),v(qa.$$.fragment,s),v(Pa.$$.fragment,s),v(Aa.$$.fragment,s),v(Sa.$$.fragment,s),v(Ta.$$.fragment,s),v(Da.$$.fragment,s),v(Ia.$$.fragment,s),v(Na.$$.fragment,s),v(Ca.$$.fragment,s),v(Oa.$$.fragment,s),v(Ha.$$.fragment,s),v(Fa.$$.fragment,s),v(La.$$.fragment,s),v(Ra.$$.fragment,s),v(za.$$.fragment,s),v(Ma.$$.fragment,s),v(Ua.$$.fragment,s),v(Va.$$.fragment,s),v(Ba.$$.fragment,s),v(Ja.$$.fragment,s),v(Ys.$$.fragment,s),v(Ya.$$.fragment,s),v(Ga.$$.fragment,s),v(Wa.$$.fragment,s),v(Qa.$$.fragment,s),v(Xa.$$.fragment,s),v(Ka.$$.fragment,s),v(Za.$$.fragment,s),v(st.$$.fragment,s),v(at.$$.fragment,s),v(tt.$$.fragment,s),v(Zs.$$.fragment,s),v(et.$$.fragment,s),v(lt.$$.fragment,s),v(nt.$$.fragment,s),v(rt.$$.fragment,s),v(pt.$$.fragment,s),v(dt.$$.fragment,s),v(ft.$$.fragment,s),v(ct.$$.fragment,s),v(ht.$$.fragment,s),v(oa.$$.fragment,s),v(ut.$$.fragment,s),v(mt.$$.fragment,s),v(gt.$$.fragment,s),v($t.$$.fragment,s),v(ia.$$.fragment,s),v(yt.$$.fragment,s),hr=!1},d(s){a(h),s&&a(k),s&&a(y),$(w),s&&a(js),s&&a(R),s&&a(bs),s&&a(z),s&&a(xs),s&&a(A),s&&a(so),s&&a(kt),s&&a(ao),s&&a(Et),s&&a(to),s&&a(ts),$(ca),s&&a(eo),s&&a(Ps),s&&a(lo),s&&a(U),s&&a(oo),$(ua,s),s&&a(no),s&&a(Pt),s&&a(ro),s&&a(As),s&&a(io),$(ma,s),s&&a(po),$(Ss,s),s&&a(fo),s&&a(T),s&&a(co),$(ga,s),s&&a(ho),$(Ts,s),s&&a(uo),s&&a(V),s&&a(mo),$(va,s),s&&a(go),s&&a(Ds),s&&a(_o),$($a,s),s&&a(vo),s&&a(es),$(ya),s&&a($o),s&&a(D),s&&a(yo),s&&a(ls),$(wa),s&&a(wo),s&&a(St),s&&a(jo),$(ja,s),s&&a(bo),s&&a(Tt),s&&a(xo),$(ba,s),s&&a(ko),s&&a(Dt),s&&a(Eo),$(xa,s),s&&a(qo),s&&a(It),s&&a(Po),$(ka,s),s&&a(Ao),s&&a(Nt),s&&a(So),$(Ea,s),s&&a(To),s&&a(os),$(qa),s&&a(Do),s&&a(Os),s&&a(Io),$(Pa,s),s&&a(No),s&&a(Ot),s&&a(Co),$(Aa,s),s&&a(Oo),s&&a(Hs),s&&a(Ho),$(Sa,s),s&&a(Fo),s&&a(Ht),s&&a(Lo),$(Ta,s),s&&a(Ro),s&&a(Ft),s&&a(zo),s&&a(ns),$(Da),s&&a(Mo),s&&a(Lt),s&&a(Uo),$(Ia,s),s&&a(Vo),s&&a(Rt),s&&a(Bo),$(Na,s),s&&a(Jo),s&&a(rs),$(Ca),s&&a(Yo),s&&a(zt),s&&a(Go),$(Oa,s),s&&a(Wo),s&&a(Mt),s&&a(Qo),$(Ha,s),s&&a(Xo),s&&a(is),$(Fa),s&&a(Ko),s&&a(Ut),s&&a(Zo),s&&a(Vt),s&&a(sn),$(La,s),s&&a(an),s&&a(N),s&&a(tn),$(Ra,s),s&&a(en),s&&a(Jt),s&&a(ln),$(za,s),s&&a(on),s&&a(C),s&&a(nn),s&&a(ps),$(Ma),s&&a(rn),s&&a(Ms),s&&a(pn),s&&a(ds),$(Ua),s&&a(dn),s&&a(Vs),s&&a(fn),$(Va,s),s&&a(cn),s&&a(fs),$(Ba),s&&a(hn),s&&a(Js),s&&a(un),$(Ja,s),s&&a(mn),$(Ys,s),s&&a(gn),s&&a(cs),$(Ya),s&&a(_n),s&&a(Qt),s&&a(vn),s&&a(B),s&&a($n),s&&a(hs),$(Ga),s&&a(yn),s&&a(J),s&&a(wn),s&&a(Y),s&&a(jn),$(Wa,s),s&&a(bn),s&&a(Qs),s&&a(xn),$(Qa,s),s&&a(kn),s&&a(Zt),s&&a(En),$(Xa,s),s&&a(qn),s&&a(se),s&&a(Pn),$(Ka,s),s&&a(An),s&&a(ae),s&&a(Sn),$(Za,s),s&&a(Tn),s&&a(us),$(st),s&&a(Dn),s&&a(te),s&&a(In),$(at,s),s&&a(Nn),s&&a(Ks),s&&a(Cn),$(tt,s),s&&a(On),$(Zs,s),s&&a(Hn),s&&a(ee),s&&a(Fn),s&&a(ms),$(et),s&&a(Ln),s&&a(le),s&&a(Rn),s&&a(gs),$(lt),s&&a(zn),s&&a(O),s&&a(Mn),s&&a(ta),s&&a(Un),$(nt,s),s&&a(Vn),s&&a(_s),$(rt),s&&a(Bn),s&&a(G),s&&a(Jn),s&&a(W),s&&a(Yn),$(pt,s),s&&a(Gn),s&&a(Q),s&&a(Wn),$(dt,s),s&&a(Qn),s&&a(de),s&&a(Xn),$(ft,s),s&&a(Kn),s&&a(vs),$(ct),s&&a(Zn),s&&a(fe),s&&a(sr),$(ht,s),s&&a(ar),$(oa,s),s&&a(tr),s&&a($s),$(ut),s&&a(er),s&&a(ce),s&&a(lr),$(mt,s),s&&a(or),s&&a(ys),$(gt),s&&a(nr),s&&a(he),s&&a(rr),s&&a(ue),s&&a(ir),s&&a(X),s&&a(pr),$($t,s),s&&a(dr),$(ia,s),s&&a(fr),s&&a(pa),s&&a(cr),$(yt,s)}}}const Tu={local:"load",sections:[{local:"hugging-face-hub",title:"Hugging Face Hub"},{local:"local-and-remote-files",sections:[{local:"csv",title:"CSV"},{local:"json",title:"JSON"},{local:"text-files",title:"Text files"},{local:"parquet",title:"Parquet"},{local:"image-folders",title:"Image folders"}],title:"Local and remote files"},{local:"inmemory-data",sections:[{local:"python-dictionary",title:"Python dictionary"},{local:"pandas-dataframe",title:"Pandas DataFrame"}],title:"In-memory data"},{local:"offline",title:"Offline"},{local:"slice-splits",sections:[{local:"percent-slicing-and-rounding",title:"Percent slicing and rounding"}],title:"Slice splits"},{local:"troubleshooting",sections:[{local:"manual-download",title:"Manual download"},{local:"specify-features",title:"Specify features"}],title:"Troubleshooting"},{local:"metrics",sections:[{local:"load-configurations",title:"Load configurations"},{local:"distributed-setup",title:"Distributed setup"}],title:"Metrics"}],title:"Load"};function Du(I,h,k){let{fw:y}=h;return I.$$set=x=>{"fw"in x&&k(0,y=x.fw)},[y]}class Hu extends yu{constructor(h){super();wu(this,h,Du,Su,ju,{fw:0})}}export{Hu as default,Tu as metadata};
