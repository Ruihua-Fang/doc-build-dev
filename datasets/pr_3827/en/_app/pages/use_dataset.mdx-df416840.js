import{S as on,i as pn,s as cn,e as t,k as h,w as A,t as l,M as hn,c as n,d as e,m,a as o,x as F,h as r,b as u,F as a,g as c,y as C,q as O,o as I,B as N}from"../chunks/vendor-e67aec41.js";import{T as rn}from"../chunks/Tip-76459d1c.js";import{I as ia}from"../chunks/IconCopyLink-ffd7f84e.js";import{C as ha}from"../chunks/CodeBlock-e2bcf023.js";function mn(ls){let i,y,d,b,k,j,S,x;return{c(){i=t("p"),y=l("Often times you may want to modify the structure and content of your dataset before you use it to train a model. For example, you may want to remove a column or cast it as a different type. \u{1F917} Datasets provides the necessary tools to do this, but since each dataset is so different, the processing approach will vary individually. For more detailed information about preprocessing data, take a look at our "),d=t("a"),b=l("guide"),k=l(" from the \u{1F917} Transformers library. Then come back and read our "),j=t("a"),S=l("How-to Process"),x=l(" guide to see all the different methods for processing your dataset."),this.h()},l(E){i=n(E,"P",{});var _=o(i);y=r(_,"Often times you may want to modify the structure and content of your dataset before you use it to train a model. For example, you may want to remove a column or cast it as a different type. \u{1F917} Datasets provides the necessary tools to do this, but since each dataset is so different, the processing approach will vary individually. For more detailed information about preprocessing data, take a look at our "),d=n(_,"A",{href:!0,rel:!0});var g=o(d);b=r(g,"guide"),g.forEach(e),k=r(_," from the \u{1F917} Transformers library. Then come back and read our "),j=n(_,"A",{href:!0});var L=o(j);S=r(L,"How-to Process"),L.forEach(e),x=r(_," guide to see all the different methods for processing your dataset."),_.forEach(e),this.h()},h(){u(d,"href","https://huggingface.co/transformers/preprocessing#"),u(d,"rel","nofollow"),u(j,"href","./process")},m(E,_){c(E,i,_),a(i,y),a(i,d),a(d,b),a(i,k),a(i,j),a(j,S),a(i,x)},d(E){E&&e(i)}}}function un(ls){let i,y,d,b,k,j,S,x,E,_,g,L,K;return{c(){i=t("p"),y=t("code"),d=l("datasets.Dataset.to_tf_dataset()"),b=l("is the easiest way to create a TensorFlow compatible dataset. If you don\u2019t want a "),k=t("code"),j=l("tf.data.Dataset"),S=l(" and would rather the dataset emit "),x=t("code"),E=l("tf.Tensor"),_=l(" objects, take a look at the "),g=t("a"),L=l("format"),K=l(" section instead!"),this.h()},l($){i=n($,"P",{});var w=o(i);y=n(w,"CODE",{});var T=o(y);d=r(T,"datasets.Dataset.to_tf_dataset()"),T.forEach(e),b=r(w,"is the easiest way to create a TensorFlow compatible dataset. If you don\u2019t want a "),k=n(w,"CODE",{});var B=o(k);j=r(B,"tf.data.Dataset"),B.forEach(e),S=r(w," and would rather the dataset emit "),x=n(w,"CODE",{});var W=o(x);E=r(W,"tf.Tensor"),W.forEach(e),_=r(w," objects, take a look at the "),g=n(w,"A",{href:!0});var q=o(g);L=r(q,"format"),q.forEach(e),K=r(w," section instead!"),w.forEach(e),this.h()},h(){u(g,"href","#format")},m($,w){c($,i,w),a(i,y),a(y,d),a(i,b),a(i,k),a(k,j),a(i,S),a(i,x),a(x,E),a(i,_),a(i,g),a(g,L),a(i,K)},d($){$&&e(i)}}}function dn(ls){let i,y,d,b,k,j,S,x,E,_,g,L,K,$,w,T,B,W,q,Ma,Ts,Ya,ma,js,Ra,ua,_s,Ua,da,rs,fa,H,Ka,os,Wa,Ja,ps,Ga,Qa,ba,cs,ja,J,Va,zs,Xa,Za,_a,is,ya,z,se,Ps,ae,ee,Ds,te,ne,As,le,re,ga,M,G,Fs,hs,oe,Cs,pe,wa,ys,ce,va,Y,Q,Os,ms,ie,Is,he,ka,V,me,gs,ue,de,xa,X,Ns,Z,Ss,fe,be,Bs,je,_e,ye,Ls,ws,qs,ge,we,$a,ss,ve,Hs,ke,xe,Ea,us,Ta,R,as,Ms,ds,$e,Ys,Ee,za,f,Te,Rs,ze,Pe,Us,De,Ae,Ks,Fe,Ce,Ws,Oe,Ie,Js,Ne,Se,Gs,Be,Le,Qs,qe,He,Pa,P,Vs,vs,Xs,Me,Ye,Re,Zs,ks,sa,Ue,Ke,We,aa,xs,ea,Je,Ge,Qe,ta,D,na,Ve,Xe,la,Ze,st,ra,at,et,oa,tt,nt,Da,fs,Aa,es,Fa,$s,lt,Ca;return j=new ia({}),$=new rn({props:{warning:"&lcub;true}",$$slots:{default:[mn]},$$scope:{ctx:ls}}}),q=new ia({}),rs=new ha({props:{code:"pip install transformers",highlighted:'pip <span class="hljs-keyword">install</span> transformers'}}),cs=new ha({props:{code:`from transformers import BertTokenizerFast
tokenizer = BertTokenizerFast.from_pretrained('bert-base-cased')`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> BertTokenizerFast
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = BertTokenizerFast.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>)`}}),is=new ha({props:{code:`encoded_dataset = dataset.map(lambda examples: tokenizer(examples['sentence1']), batched=True)
encoded_dataset.column_names
encoded_dataset[0]`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>encoded_dataset = dataset.<span class="hljs-built_in">map</span>(<span class="hljs-keyword">lambda</span> examples: tokenizer(examples[<span class="hljs-string">&#x27;sentence1&#x27;</span>]), batched=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>encoded_dataset.column_names
[<span class="hljs-string">&#x27;sentence1&#x27;</span>, <span class="hljs-string">&#x27;sentence2&#x27;</span>, <span class="hljs-string">&#x27;label&#x27;</span>, <span class="hljs-string">&#x27;idx&#x27;</span>, <span class="hljs-string">&#x27;input_ids&#x27;</span>, <span class="hljs-string">&#x27;token_type_ids&#x27;</span>, <span class="hljs-string">&#x27;attention_mask&#x27;</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>encoded_dataset[<span class="hljs-number">0</span>]
{<span class="hljs-string">&#x27;sentence1&#x27;</span>: <span class="hljs-string">&#x27;Amrozi accused his brother , whom he called &quot; the witness &quot; , of deliberately distorting his evidence .&#x27;</span>,
<span class="hljs-string">&#x27;sentence2&#x27;</span>: <span class="hljs-string">&#x27;Referring to him as only &quot; the witness &quot; , Amrozi accused his brother of deliberately distorting his evidence .&#x27;</span>,
<span class="hljs-string">&#x27;label&#x27;</span>: <span class="hljs-number">1</span>,
<span class="hljs-string">&#x27;idx&#x27;</span>: <span class="hljs-number">0</span>,
<span class="hljs-string">&#x27;input_ids&#x27;</span>: [  <span class="hljs-number">101</span>,  <span class="hljs-number">7277</span>,  <span class="hljs-number">2180</span>,  <span class="hljs-number">5303</span>,  <span class="hljs-number">4806</span>,  <span class="hljs-number">1117</span>,  <span class="hljs-number">1711</span>,   <span class="hljs-number">117</span>,  <span class="hljs-number">2292</span>, <span class="hljs-number">1119</span>,  <span class="hljs-number">1270</span>,   <span class="hljs-number">107</span>,  <span class="hljs-number">1103</span>,  <span class="hljs-number">7737</span>,   <span class="hljs-number">107</span>,   <span class="hljs-number">117</span>,  <span class="hljs-number">1104</span>,  <span class="hljs-number">9938</span>, <span class="hljs-number">4267</span>, <span class="hljs-number">12223</span>, <span class="hljs-number">21811</span>,  <span class="hljs-number">1117</span>,  <span class="hljs-number">2554</span>,   <span class="hljs-number">119</span>,   <span class="hljs-number">102</span>],
<span class="hljs-string">&#x27;token_type_ids&#x27;</span>: [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
<span class="hljs-string">&#x27;attention_mask&#x27;</span>: [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]
}`}}),hs=new ia({}),ms=new ia({}),us=new ha({props:{code:`import torch
from datasets import load_dataset
from transformers import AutoTokenizer
dataset = load_dataset('glue', 'mrpc', split='train')
tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')
dataset = dataset.map(lambda e: tokenizer(e['sentence1'], truncation=True, padding='max_length'), batched=True)
dataset.set_format(type='torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'label'])
dataloader = torch.utils.data.DataLoader(dataset, batch_size=32)
next(iter(dataloader))`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;glue&#x27;</span>, <span class="hljs-string">&#x27;mrpc&#x27;</span>, split=<span class="hljs-string">&#x27;train&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = dataset.<span class="hljs-built_in">map</span>(<span class="hljs-keyword">lambda</span> e: tokenizer(e[<span class="hljs-string">&#x27;sentence1&#x27;</span>], truncation=<span class="hljs-literal">True</span>, padding=<span class="hljs-string">&#x27;max_length&#x27;</span>), batched=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset.set_format(<span class="hljs-built_in">type</span>=<span class="hljs-string">&#x27;torch&#x27;</span>, columns=[<span class="hljs-string">&#x27;input_ids&#x27;</span>, <span class="hljs-string">&#x27;token_type_ids&#x27;</span>, <span class="hljs-string">&#x27;attention_mask&#x27;</span>, <span class="hljs-string">&#x27;label&#x27;</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>dataloader = torch.utils.data.DataLoader(dataset, batch_size=<span class="hljs-number">32</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">next</span>(<span class="hljs-built_in">iter</span>(dataloader))
{<span class="hljs-string">&#x27;attention_mask&#x27;</span>: tensor([[<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>,  ..., <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
                         ...,
                         [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>,  ..., <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>]]),
<span class="hljs-string">&#x27;input_ids&#x27;</span>: tensor([[  <span class="hljs-number">101</span>,  <span class="hljs-number">7277</span>,  <span class="hljs-number">2180</span>,  ...,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>],
                    ...,
                    [  <span class="hljs-number">101</span>,  <span class="hljs-number">1109</span>,  <span class="hljs-number">4173</span>,  ...,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>]]),
<span class="hljs-string">&#x27;label&#x27;</span>: tensor([<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>]),
<span class="hljs-string">&#x27;token_type_ids&#x27;</span>: tensor([[<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>,  ..., <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
                         ...,
                         [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>,  ..., <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>]])}`}}),ds=new ia({}),fs=new ha({props:{code:`import tensorflow as tf
from datasets import load_dataset
from transformers import AutoTokenizer
dataset = load_dataset('glue', 'mrpc', split='train')
tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')
dataset = dataset.map(lambda e: tokenizer(e['sentence1'], truncation=True, padding='max_length'), batched=True)
data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors="tf")
train_dataset = dataset["train"].to_tf_dataset(
  columns=['input_ids', 'token_type_ids', 'attention_mask', 'label'],
  shuffle=True,
  batch_size=16,
  collate_fn=data_collator,
)
model.fit(train_dataset)  # The output tf.data.Dataset is ready for training immediately
next(iter(train_dataset))  # You can also iterate over the dataset manually to get batches`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;glue&#x27;</span>, <span class="hljs-string">&#x27;mrpc&#x27;</span>, split=<span class="hljs-string">&#x27;train&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = dataset.<span class="hljs-built_in">map</span>(<span class="hljs-keyword">lambda</span> e: tokenizer(e[<span class="hljs-string">&#x27;sentence1&#x27;</span>], truncation=<span class="hljs-literal">True</span>, padding=<span class="hljs-string">&#x27;max_length&#x27;</span>), batched=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>train_dataset = dataset[<span class="hljs-string">&quot;train&quot;</span>].to_tf_dataset(
<span class="hljs-meta">... </span>  columns=[<span class="hljs-string">&#x27;input_ids&#x27;</span>, <span class="hljs-string">&#x27;token_type_ids&#x27;</span>, <span class="hljs-string">&#x27;attention_mask&#x27;</span>, <span class="hljs-string">&#x27;label&#x27;</span>],
<span class="hljs-meta">... </span>  shuffle=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>  batch_size=<span class="hljs-number">16</span>,
<span class="hljs-meta">... </span>  collate_fn=data_collator,
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.fit(train_dataset)  <span class="hljs-comment"># The output tf.data.Dataset is ready for training immediately</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">next</span>(<span class="hljs-built_in">iter</span>(train_dataset))  <span class="hljs-comment"># You can also iterate over the dataset manually to get batches</span>
{<span class="hljs-string">&#x27;attention_mask&#x27;</span>: &lt;tf.Tensor: shape=(<span class="hljs-number">16</span>, <span class="hljs-number">512</span>), dtype=int64, numpy=
 array([[<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, ..., <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
      ...,
      [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, ..., <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>]])&gt;,
 <span class="hljs-string">&#x27;input_ids&#x27;</span>: &lt;tf.Tensor: shape=(<span class="hljs-number">16</span>, <span class="hljs-number">512</span>), dtype=int64, numpy=
  array([[  <span class="hljs-number">101</span>, <span class="hljs-number">11336</span>, <span class="hljs-number">11154</span>, ...,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>],
      ..., 
      [  <span class="hljs-number">101</span>,   <span class="hljs-number">156</span>, <span class="hljs-number">22705</span>, ...,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>]])&gt;,
 <span class="hljs-string">&#x27;labels&#x27;</span>: &lt;tf.Tensor: shape=(<span class="hljs-number">16</span>,), dtype=int64, numpy=
  array([<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>])&gt;,
 <span class="hljs-string">&#x27;token_type_ids&#x27;</span>: &lt;tf.Tensor: shape=(<span class="hljs-number">16</span>, <span class="hljs-number">512</span>), dtype=int64, numpy=
  array([[<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, ..., <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
       ...,
      [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, ..., <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>]])&gt;
}`}}),es=new rn({props:{$$slots:{default:[un]},$$scope:{ctx:ls}}}),{c(){i=t("meta"),y=h(),d=t("h1"),b=t("a"),k=t("span"),A(j.$$.fragment),S=h(),x=t("span"),E=l("Train with \u{1F917} Datasets"),_=h(),g=t("p"),L=l("So far, you loaded a dataset from the Hugging Face Hub and learned how to access the information stored inside the dataset. Now you will tokenize and use your dataset with a framework such as PyTorch or TensorFlow. By default, all the dataset columns are returned as Python objects. But you can bridge the gap between a Python object and your machine learning framework by setting the format of a dataset. Formatting casts the columns into compatible PyTorch or TensorFlow types."),K=h(),A($.$$.fragment),w=h(),T=t("h2"),B=t("a"),W=t("span"),A(q.$$.fragment),Ma=h(),Ts=t("span"),Ya=l("Tokenize"),ma=h(),js=t("p"),Ra=l("Tokenization divides text into individual words called tokens. Tokens are converted into numbers, which is what the model receives as its input."),ua=h(),_s=t("p"),Ua=l("The first step is to install the \u{1F917} Transformers library:"),da=h(),A(rs.$$.fragment),fa=h(),H=t("p"),Ka=l("Next, import a tokenizer. It is important to use the tokenizer that is associated with the model you are using, so the text is split in the same way. In this example, load the "),os=t("a"),Wa=l("BERT tokenizer"),Ja=l(" because you are using the "),ps=t("a"),Ga=l("BERT"),Qa=l(" model:"),ba=h(),A(cs.$$.fragment),ja=h(),J=t("p"),Va=l("Now you can tokenize "),zs=t("code"),Xa=l("sentence1"),Za=l(" field of the dataset:"),_a=h(),A(is.$$.fragment),ya=h(),z=t("p"),se=l("The tokenization process creates three new columns: "),Ps=t("code"),ae=l("input_ids"),ee=l(", "),Ds=t("code"),te=l("token_type_ids"),ne=l(", and "),As=t("code"),le=l("attention_mask"),re=l(". These are the inputs to the model."),ga=h(),M=t("h2"),G=t("a"),Fs=t("span"),A(hs.$$.fragment),oe=h(),Cs=t("span"),pe=l("Use in PyTorch or TensorFlow"),wa=h(),ys=t("p"),ce=l("Next, format the dataset into compatible PyTorch or TensorFlow types."),va=h(),Y=t("h3"),Q=t("a"),Os=t("span"),A(ms.$$.fragment),ie=h(),Is=t("span"),he=l("PyTorch"),ka=h(),V=t("p"),me=l("If you are using PyTorch, set the format with "),gs=t("a"),ue=l("datasets.Dataset.set_format()"),de=l(", which accepts two main arguments:"),xa=h(),X=t("ol"),Ns=t("li"),Z=t("p"),Ss=t("code"),fe=l("type"),be=l(" defines the type of column to cast to. For example, "),Bs=t("code"),je=l("torch"),_e=l(" returns PyTorch tensors."),ye=h(),Ls=t("li"),ws=t("p"),qs=t("code"),ge=l("columns"),we=l(" specify which columns should be formatted."),$a=h(),ss=t("p"),ve=l("After you set the format, wrap the dataset with "),Hs=t("code"),ke=l("torch.utils.data.DataLoader"),xe=l(". Your dataset is now ready for use in a training loop!"),Ea=h(),A(us.$$.fragment),Ta=h(),R=t("h3"),as=t("a"),Ms=t("span"),A(ds.$$.fragment),$e=h(),Ys=t("span"),Ee=l("TensorFlow"),za=h(),f=t("p"),Te=l("If you are using TensorFlow, you can use "),Rs=t("code"),ze=l("datasets.Dataset.to_tf_dataset()"),Pe=l("to wrap the dataset with a "),Us=t("em"),De=l("tf.data.Dataset"),Ae=l(`.
`),Ks=t("em"),Fe=l("tf.data.Dataset"),Ce=l(" objects are natively understood by Keras. This means a "),Ws=t("em"),Oe=l("tf.data.Dataset"),Ie=l(" object can be iterated over to yield batches of data, and can be passed directly to methods like "),Js=t("em"),Ne=l("model.fit()"),Se=l(`.
means they can be passed directly to methods like `),Gs=t("em"),Be=l("model.fit()"),Le=l(". "),Qs=t("code"),qe=l("datasets.Dataset.to_tf_dataset()"),He=l("accepts several arguments:"),Pa=h(),P=t("ol"),Vs=t("li"),vs=t("p"),Xs=t("code"),Me=l("columns"),Ye=l(" specify which columns should be formatted (includes the inputs and labels)."),Re=h(),Zs=t("li"),ks=t("p"),sa=t("code"),Ue=l("shuffle"),Ke=l(" determines whether the dataset should be shuffled."),We=h(),aa=t("li"),xs=t("p"),ea=t("code"),Je=l("batch_size"),Ge=l(" specifies the batch size."),Qe=h(),ta=t("li"),D=t("p"),na=t("code"),Ve=l("collate_fn"),Xe=l(" specifies a data collator that will batch each processed example and apply padding. If you are using a "),la=t("code"),Ze=l("DataCollator"),st=l(", make sure you set "),ra=t("code"),at=l('return_tensors="tf"'),et=l(" when you initialize it to return "),oa=t("code"),tt=l("tf.Tensor"),nt=l(" outputs."),Da=h(),A(fs.$$.fragment),Aa=h(),A(es.$$.fragment),Fa=h(),$s=t("p"),lt=l("Your dataset is now ready for use in a training loop!"),this.h()},l(s){const p=hn('[data-svelte="svelte-1phssyn"]',document.head);i=n(p,"META",{name:!0,content:!0}),p.forEach(e),y=m(s),d=n(s,"H1",{class:!0});var bs=o(d);b=n(bs,"A",{id:!0,class:!0,href:!0});var pa=o(b);k=n(pa,"SPAN",{});var it=o(k);F(j.$$.fragment,it),it.forEach(e),pa.forEach(e),S=m(bs),x=n(bs,"SPAN",{});var ht=o(x);E=r(ht,"Train with \u{1F917} Datasets"),ht.forEach(e),bs.forEach(e),_=m(s),g=n(s,"P",{});var mt=o(g);L=r(mt,"So far, you loaded a dataset from the Hugging Face Hub and learned how to access the information stored inside the dataset. Now you will tokenize and use your dataset with a framework such as PyTorch or TensorFlow. By default, all the dataset columns are returned as Python objects. But you can bridge the gap between a Python object and your machine learning framework by setting the format of a dataset. Formatting casts the columns into compatible PyTorch or TensorFlow types."),mt.forEach(e),K=m(s),F($.$$.fragment,s),w=m(s),T=n(s,"H2",{class:!0});var Oa=o(T);B=n(Oa,"A",{id:!0,class:!0,href:!0});var ut=o(B);W=n(ut,"SPAN",{});var dt=o(W);F(q.$$.fragment,dt),dt.forEach(e),ut.forEach(e),Ma=m(Oa),Ts=n(Oa,"SPAN",{});var ft=o(Ts);Ya=r(ft,"Tokenize"),ft.forEach(e),Oa.forEach(e),ma=m(s),js=n(s,"P",{});var bt=o(js);Ra=r(bt,"Tokenization divides text into individual words called tokens. Tokens are converted into numbers, which is what the model receives as its input."),bt.forEach(e),ua=m(s),_s=n(s,"P",{});var jt=o(_s);Ua=r(jt,"The first step is to install the \u{1F917} Transformers library:"),jt.forEach(e),da=m(s),F(rs.$$.fragment,s),fa=m(s),H=n(s,"P",{});var Es=o(H);Ka=r(Es,"Next, import a tokenizer. It is important to use the tokenizer that is associated with the model you are using, so the text is split in the same way. In this example, load the "),os=n(Es,"A",{href:!0,rel:!0});var _t=o(os);Wa=r(_t,"BERT tokenizer"),_t.forEach(e),Ja=r(Es," because you are using the "),ps=n(Es,"A",{href:!0,rel:!0});var yt=o(ps);Ga=r(yt,"BERT"),yt.forEach(e),Qa=r(Es," model:"),Es.forEach(e),ba=m(s),F(cs.$$.fragment,s),ja=m(s),J=n(s,"P",{});var Ia=o(J);Va=r(Ia,"Now you can tokenize "),zs=n(Ia,"CODE",{});var gt=o(zs);Xa=r(gt,"sentence1"),gt.forEach(e),Za=r(Ia," field of the dataset:"),Ia.forEach(e),_a=m(s),F(is.$$.fragment,s),ya=m(s),z=n(s,"P",{});var ts=o(z);se=r(ts,"The tokenization process creates three new columns: "),Ps=n(ts,"CODE",{});var wt=o(Ps);ae=r(wt,"input_ids"),wt.forEach(e),ee=r(ts,", "),Ds=n(ts,"CODE",{});var vt=o(Ds);te=r(vt,"token_type_ids"),vt.forEach(e),ne=r(ts,", and "),As=n(ts,"CODE",{});var kt=o(As);le=r(kt,"attention_mask"),kt.forEach(e),re=r(ts,". These are the inputs to the model."),ts.forEach(e),ga=m(s),M=n(s,"H2",{class:!0});var Na=o(M);G=n(Na,"A",{id:!0,class:!0,href:!0});var xt=o(G);Fs=n(xt,"SPAN",{});var $t=o(Fs);F(hs.$$.fragment,$t),$t.forEach(e),xt.forEach(e),oe=m(Na),Cs=n(Na,"SPAN",{});var Et=o(Cs);pe=r(Et,"Use in PyTorch or TensorFlow"),Et.forEach(e),Na.forEach(e),wa=m(s),ys=n(s,"P",{});var Tt=o(ys);ce=r(Tt,"Next, format the dataset into compatible PyTorch or TensorFlow types."),Tt.forEach(e),va=m(s),Y=n(s,"H3",{class:!0});var Sa=o(Y);Q=n(Sa,"A",{id:!0,class:!0,href:!0});var zt=o(Q);Os=n(zt,"SPAN",{});var Pt=o(Os);F(ms.$$.fragment,Pt),Pt.forEach(e),zt.forEach(e),ie=m(Sa),Is=n(Sa,"SPAN",{});var Dt=o(Is);he=r(Dt,"PyTorch"),Dt.forEach(e),Sa.forEach(e),ka=m(s),V=n(s,"P",{});var Ba=o(V);me=r(Ba,"If you are using PyTorch, set the format with "),gs=n(Ba,"A",{href:!0});var At=o(gs);ue=r(At,"datasets.Dataset.set_format()"),At.forEach(e),de=r(Ba,", which accepts two main arguments:"),Ba.forEach(e),xa=m(s),X=n(s,"OL",{});var La=o(X);Ns=n(La,"LI",{});var Ft=o(Ns);Z=n(Ft,"P",{});var ca=o(Z);Ss=n(ca,"CODE",{});var Ct=o(Ss);fe=r(Ct,"type"),Ct.forEach(e),be=r(ca," defines the type of column to cast to. For example, "),Bs=n(ca,"CODE",{});var Ot=o(Bs);je=r(Ot,"torch"),Ot.forEach(e),_e=r(ca," returns PyTorch tensors."),ca.forEach(e),Ft.forEach(e),ye=m(La),Ls=n(La,"LI",{});var It=o(Ls);ws=n(It,"P",{});var rt=o(ws);qs=n(rt,"CODE",{});var Nt=o(qs);ge=r(Nt,"columns"),Nt.forEach(e),we=r(rt," specify which columns should be formatted."),rt.forEach(e),It.forEach(e),La.forEach(e),$a=m(s),ss=n(s,"P",{});var qa=o(ss);ve=r(qa,"After you set the format, wrap the dataset with "),Hs=n(qa,"CODE",{});var St=o(Hs);ke=r(St,"torch.utils.data.DataLoader"),St.forEach(e),xe=r(qa,". Your dataset is now ready for use in a training loop!"),qa.forEach(e),Ea=m(s),F(us.$$.fragment,s),Ta=m(s),R=n(s,"H3",{class:!0});var Ha=o(R);as=n(Ha,"A",{id:!0,class:!0,href:!0});var Bt=o(as);Ms=n(Bt,"SPAN",{});var Lt=o(Ms);F(ds.$$.fragment,Lt),Lt.forEach(e),Bt.forEach(e),$e=m(Ha),Ys=n(Ha,"SPAN",{});var qt=o(Ys);Ee=r(qt,"TensorFlow"),qt.forEach(e),Ha.forEach(e),za=m(s),f=n(s,"P",{});var v=o(f);Te=r(v,"If you are using TensorFlow, you can use "),Rs=n(v,"CODE",{});var Ht=o(Rs);ze=r(Ht,"datasets.Dataset.to_tf_dataset()"),Ht.forEach(e),Pe=r(v,"to wrap the dataset with a "),Us=n(v,"EM",{});var Mt=o(Us);De=r(Mt,"tf.data.Dataset"),Mt.forEach(e),Ae=r(v,`.
`),Ks=n(v,"EM",{});var Yt=o(Ks);Fe=r(Yt,"tf.data.Dataset"),Yt.forEach(e),Ce=r(v," objects are natively understood by Keras. This means a "),Ws=n(v,"EM",{});var Rt=o(Ws);Oe=r(Rt,"tf.data.Dataset"),Rt.forEach(e),Ie=r(v," object can be iterated over to yield batches of data, and can be passed directly to methods like "),Js=n(v,"EM",{});var Ut=o(Js);Ne=r(Ut,"model.fit()"),Ut.forEach(e),Se=r(v,`.
means they can be passed directly to methods like `),Gs=n(v,"EM",{});var Kt=o(Gs);Be=r(Kt,"model.fit()"),Kt.forEach(e),Le=r(v,". "),Qs=n(v,"CODE",{});var Wt=o(Qs);qe=r(Wt,"datasets.Dataset.to_tf_dataset()"),Wt.forEach(e),He=r(v,"accepts several arguments:"),v.forEach(e),Pa=m(s),P=n(s,"OL",{});var ns=o(P);Vs=n(ns,"LI",{});var Jt=o(Vs);vs=n(Jt,"P",{});var ot=o(vs);Xs=n(ot,"CODE",{});var Gt=o(Xs);Me=r(Gt,"columns"),Gt.forEach(e),Ye=r(ot," specify which columns should be formatted (includes the inputs and labels)."),ot.forEach(e),Jt.forEach(e),Re=m(ns),Zs=n(ns,"LI",{});var Qt=o(Zs);ks=n(Qt,"P",{});var pt=o(ks);sa=n(pt,"CODE",{});var Vt=o(sa);Ue=r(Vt,"shuffle"),Vt.forEach(e),Ke=r(pt," determines whether the dataset should be shuffled."),pt.forEach(e),Qt.forEach(e),We=m(ns),aa=n(ns,"LI",{});var Xt=o(aa);xs=n(Xt,"P",{});var ct=o(xs);ea=n(ct,"CODE",{});var Zt=o(ea);Je=r(Zt,"batch_size"),Zt.forEach(e),Ge=r(ct," specifies the batch size."),ct.forEach(e),Xt.forEach(e),Qe=m(ns),ta=n(ns,"LI",{});var sn=o(ta);D=n(sn,"P",{});var U=o(D);na=n(U,"CODE",{});var an=o(na);Ve=r(an,"collate_fn"),an.forEach(e),Xe=r(U," specifies a data collator that will batch each processed example and apply padding. If you are using a "),la=n(U,"CODE",{});var en=o(la);Ze=r(en,"DataCollator"),en.forEach(e),st=r(U,", make sure you set "),ra=n(U,"CODE",{});var tn=o(ra);at=r(tn,'return_tensors="tf"'),tn.forEach(e),et=r(U," when you initialize it to return "),oa=n(U,"CODE",{});var nn=o(oa);tt=r(nn,"tf.Tensor"),nn.forEach(e),nt=r(U," outputs."),U.forEach(e),sn.forEach(e),ns.forEach(e),Da=m(s),F(fs.$$.fragment,s),Aa=m(s),F(es.$$.fragment,s),Fa=m(s),$s=n(s,"P",{});var ln=o($s);lt=r(ln,"Your dataset is now ready for use in a training loop!"),ln.forEach(e),this.h()},h(){u(i,"name","hf:doc:metadata"),u(i,"content",JSON.stringify(fn)),u(b,"id","train-with-datasets"),u(b,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(b,"href","#train-with-datasets"),u(d,"class","relative group"),u(B,"id","tokenize"),u(B,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(B,"href","#tokenize"),u(T,"class","relative group"),u(os,"href","https://huggingface.co/transformers/model_doc/bert#berttokenizerfast"),u(os,"rel","nofollow"),u(ps,"href","https://huggingface.co/bert-base-cased"),u(ps,"rel","nofollow"),u(G,"id","use-in-pytorch-or-tensorflow"),u(G,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(G,"href","#use-in-pytorch-or-tensorflow"),u(M,"class","relative group"),u(Q,"id","pytorch"),u(Q,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(Q,"href","#pytorch"),u(Y,"class","relative group"),u(gs,"href","/docs/datasets/pr_3827/en/package_reference/main_classes#datasets.Dataset.set_format"),u(as,"id","tensorflow"),u(as,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(as,"href","#tensorflow"),u(R,"class","relative group")},m(s,p){a(document.head,i),c(s,y,p),c(s,d,p),a(d,b),a(b,k),C(j,k,null),a(d,S),a(d,x),a(x,E),c(s,_,p),c(s,g,p),a(g,L),c(s,K,p),C($,s,p),c(s,w,p),c(s,T,p),a(T,B),a(B,W),C(q,W,null),a(T,Ma),a(T,Ts),a(Ts,Ya),c(s,ma,p),c(s,js,p),a(js,Ra),c(s,ua,p),c(s,_s,p),a(_s,Ua),c(s,da,p),C(rs,s,p),c(s,fa,p),c(s,H,p),a(H,Ka),a(H,os),a(os,Wa),a(H,Ja),a(H,ps),a(ps,Ga),a(H,Qa),c(s,ba,p),C(cs,s,p),c(s,ja,p),c(s,J,p),a(J,Va),a(J,zs),a(zs,Xa),a(J,Za),c(s,_a,p),C(is,s,p),c(s,ya,p),c(s,z,p),a(z,se),a(z,Ps),a(Ps,ae),a(z,ee),a(z,Ds),a(Ds,te),a(z,ne),a(z,As),a(As,le),a(z,re),c(s,ga,p),c(s,M,p),a(M,G),a(G,Fs),C(hs,Fs,null),a(M,oe),a(M,Cs),a(Cs,pe),c(s,wa,p),c(s,ys,p),a(ys,ce),c(s,va,p),c(s,Y,p),a(Y,Q),a(Q,Os),C(ms,Os,null),a(Y,ie),a(Y,Is),a(Is,he),c(s,ka,p),c(s,V,p),a(V,me),a(V,gs),a(gs,ue),a(V,de),c(s,xa,p),c(s,X,p),a(X,Ns),a(Ns,Z),a(Z,Ss),a(Ss,fe),a(Z,be),a(Z,Bs),a(Bs,je),a(Z,_e),a(X,ye),a(X,Ls),a(Ls,ws),a(ws,qs),a(qs,ge),a(ws,we),c(s,$a,p),c(s,ss,p),a(ss,ve),a(ss,Hs),a(Hs,ke),a(ss,xe),c(s,Ea,p),C(us,s,p),c(s,Ta,p),c(s,R,p),a(R,as),a(as,Ms),C(ds,Ms,null),a(R,$e),a(R,Ys),a(Ys,Ee),c(s,za,p),c(s,f,p),a(f,Te),a(f,Rs),a(Rs,ze),a(f,Pe),a(f,Us),a(Us,De),a(f,Ae),a(f,Ks),a(Ks,Fe),a(f,Ce),a(f,Ws),a(Ws,Oe),a(f,Ie),a(f,Js),a(Js,Ne),a(f,Se),a(f,Gs),a(Gs,Be),a(f,Le),a(f,Qs),a(Qs,qe),a(f,He),c(s,Pa,p),c(s,P,p),a(P,Vs),a(Vs,vs),a(vs,Xs),a(Xs,Me),a(vs,Ye),a(P,Re),a(P,Zs),a(Zs,ks),a(ks,sa),a(sa,Ue),a(ks,Ke),a(P,We),a(P,aa),a(aa,xs),a(xs,ea),a(ea,Je),a(xs,Ge),a(P,Qe),a(P,ta),a(ta,D),a(D,na),a(na,Ve),a(D,Xe),a(D,la),a(la,Ze),a(D,st),a(D,ra),a(ra,at),a(D,et),a(D,oa),a(oa,tt),a(D,nt),c(s,Da,p),C(fs,s,p),c(s,Aa,p),C(es,s,p),c(s,Fa,p),c(s,$s,p),a($s,lt),Ca=!0},p(s,[p]){const bs={};p&2&&(bs.$$scope={dirty:p,ctx:s}),$.$set(bs);const pa={};p&2&&(pa.$$scope={dirty:p,ctx:s}),es.$set(pa)},i(s){Ca||(O(j.$$.fragment,s),O($.$$.fragment,s),O(q.$$.fragment,s),O(rs.$$.fragment,s),O(cs.$$.fragment,s),O(is.$$.fragment,s),O(hs.$$.fragment,s),O(ms.$$.fragment,s),O(us.$$.fragment,s),O(ds.$$.fragment,s),O(fs.$$.fragment,s),O(es.$$.fragment,s),Ca=!0)},o(s){I(j.$$.fragment,s),I($.$$.fragment,s),I(q.$$.fragment,s),I(rs.$$.fragment,s),I(cs.$$.fragment,s),I(is.$$.fragment,s),I(hs.$$.fragment,s),I(ms.$$.fragment,s),I(us.$$.fragment,s),I(ds.$$.fragment,s),I(fs.$$.fragment,s),I(es.$$.fragment,s),Ca=!1},d(s){e(i),s&&e(y),s&&e(d),N(j),s&&e(_),s&&e(g),s&&e(K),N($,s),s&&e(w),s&&e(T),N(q),s&&e(ma),s&&e(js),s&&e(ua),s&&e(_s),s&&e(da),N(rs,s),s&&e(fa),s&&e(H),s&&e(ba),N(cs,s),s&&e(ja),s&&e(J),s&&e(_a),N(is,s),s&&e(ya),s&&e(z),s&&e(ga),s&&e(M),N(hs),s&&e(wa),s&&e(ys),s&&e(va),s&&e(Y),N(ms),s&&e(ka),s&&e(V),s&&e(xa),s&&e(X),s&&e($a),s&&e(ss),s&&e(Ea),N(us,s),s&&e(Ta),s&&e(R),N(ds),s&&e(za),s&&e(f),s&&e(Pa),s&&e(P),s&&e(Da),N(fs,s),s&&e(Aa),N(es,s),s&&e(Fa),s&&e($s)}}}const fn={local:"train-with-datasets",sections:[{local:"tokenize",title:"Tokenize"},{local:"use-in-pytorch-or-tensorflow",sections:[{local:"pytorch",title:"PyTorch"},{local:"tensorflow",title:"TensorFlow"}],title:"Use in PyTorch or TensorFlow"}],title:"Train with \u{1F917} Datasets"};function bn(ls,i,y){let{fw:d}=i;return ls.$$set=b=>{"fw"in b&&y(0,d=b.fw)},[d]}class wn extends on{constructor(i){super();pn(this,i,bn,dn,cn,{fw:0})}}export{wn as default,fn as metadata};
