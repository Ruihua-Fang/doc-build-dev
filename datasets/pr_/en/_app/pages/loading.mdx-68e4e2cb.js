import{S as ih,i as ph,s as dh,e as l,k as d,w as u,t as i,M as fh,c as o,d as t,m as f,a as n,x as m,h as p,b as c,F as a,g as r,y as g,q as _,o as v,B as $}from"../chunks/vendor-e67aec41.js";import{T as ce}from"../chunks/Tip-76459d1c.js";import{I as P}from"../chunks/IconCopyLink-ffd7f84e.js";import{C as x}from"../chunks/CodeBlock-e2bcf023.js";function ch(I){let h,k,y,b,E;return{c(){h=l("p"),k=i("Refer to the "),y=l("a"),b=i("upload_dataset_repo"),E=i(" guide for more instructions on how to create a dataset repository on the Hub, and how to upload your data files."),this.h()},l(w){h=o(w,"P",{});var j=n(h);k=p(j,"Refer to the "),y=o(j,"A",{href:!0});var q=n(y);b=p(q,"upload_dataset_repo"),q.forEach(t),E=p(j," guide for more instructions on how to create a dataset repository on the Hub, and how to upload your data files."),j.forEach(t),this.h()},h(){c(y,"href","#upload_dataset_repo")},m(w,j){r(w,h,j),a(h,k),a(h,y),a(y,b),a(h,E)},d(w){w&&t(h)}}}function hh(I){let h,k,y,b,E;return{c(){h=l("p"),k=i("If you don\u2019t specify which data files to use, "),y=l("code"),b=i("load_dataset"),E=i(" will return all the data files. This can take a long time if you are loading a large dataset like C4, which is approximately 13TB of data.")},l(w){h=o(w,"P",{});var j=n(h);k=p(j,"If you don\u2019t specify which data files to use, "),y=o(j,"CODE",{});var q=n(y);b=p(q,"load_dataset"),q.forEach(t),E=p(j," will return all the data files. This can take a long time if you are loading a large dataset like C4, which is approximately 13TB of data."),j.forEach(t)},m(w,j){r(w,h,j),a(h,k),a(h,y),a(y,b),a(h,E)},d(w){w&&t(h)}}}function uh(I){let h,k,y,b,E,w,j,q,X,vs,F,K,$s,R,L,ys,A;return{c(){h=l("p"),k=i("An object data type in "),y=l("a"),b=i("pandas.Series"),E=i(" doesn\u2019t always carry enough information for Arrow to automatically infer a data type. For example, if a DataFrame is of length 0 or the Series only contains None/nan objects, the type is set to null. Avoid potential errors by constructing an explicit schema with "),w=l("a"),j=i("datasets.Features"),q=i(" using the "),X=l("code"),vs=i("from_dict"),F=i(" or "),K=l("code"),$s=i("from_pandas"),R=i(" methods. See the "),L=l("a"),ys=i("troubleshoot"),A=i(" for more details on how to explicitly specify your own features."),this.h()},l(H){h=o(H,"P",{});var S=n(h);k=p(S,"An object data type in "),y=o(S,"A",{href:!0,rel:!0});var ca=n(y);b=p(ca,"pandas.Series"),ca.forEach(t),E=p(S," doesn\u2019t always carry enough information for Arrow to automatically infer a data type. For example, if a DataFrame is of length 0 or the Series only contains None/nan objects, the type is set to null. Avoid potential errors by constructing an explicit schema with "),w=o(S,"A",{href:!0});var ws=n(w);j=p(ws,"datasets.Features"),ws.forEach(t),q=p(S," using the "),X=o(S,"CODE",{});var ha=n(X);vs=p(ha,"from_dict"),ha.forEach(t),F=p(S," or "),K=o(S,"CODE",{});var ua=n(K);$s=p(ua,"from_pandas"),ua.forEach(t),R=p(S," methods. See the "),L=o(S,"A",{href:!0});var js=n(L);ys=p(js,"troubleshoot"),js.forEach(t),A=p(S," for more details on how to explicitly specify your own features."),S.forEach(t),this.h()},h(){c(y,"href","https://pandas.pydata.org/docs/reference/api/pandas.Series.html"),c(y,"rel","nofollow"),c(w,"href","/docs/datasets/pr_/en/package_reference/main_classes#datasets.Features"),c(L,"href","#troubleshoot")},m(H,S){r(H,h,S),a(h,k),a(h,y),a(y,b),a(h,E),a(h,w),a(w,j),a(h,q),a(h,X),a(X,vs),a(h,F),a(h,K),a(K,$s),a(h,R),a(h,L),a(L,ys),a(h,A)},d(H){H&&t(h)}}}function mh(I){let h,k,y,b,E;return{c(){h=l("p"),k=i("Using "),y=l("code"),b=i("pct1_dropremainder"),E=i(" rounding may truncate the last examples in a dataset if the number of examples in your dataset don\u2019t divide evenly by 100.")},l(w){h=o(w,"P",{});var j=n(h);k=p(j,"Using "),y=o(j,"CODE",{});var q=n(y);b=p(q,"pct1_dropremainder"),q.forEach(t),E=p(j," rounding may truncate the last examples in a dataset if the number of examples in your dataset don\u2019t divide evenly by 100."),j.forEach(t)},m(w,j){r(w,h,j),a(h,k),a(h,y),a(y,b),a(h,E)},d(w){w&&t(h)}}}function gh(I){let h,k,y,b,E;return{c(){h=l("p"),k=i("See the "),y=l("a"),b=i("metric_script"),E=i(" guide for more details on how to write your own metric loading script."),this.h()},l(w){h=o(w,"P",{});var j=n(h);k=p(j,"See the "),y=o(j,"A",{href:!0});var q=n(y);b=p(q,"metric_script"),q.forEach(t),E=p(j," guide for more details on how to write your own metric loading script."),j.forEach(t),this.h()},h(){c(y,"href","#metric_script")},m(w,j){r(w,h,j),a(h,k),a(h,y),a(y,b),a(h,E)},d(w){w&&t(h)}}}function _h(I){let h,k,y,b,E;return{c(){h=l("p"),k=i("Once you\u2019ve loaded a metric for distributed usage, you can compute the metric as usual. Behind the scenes, "),y=l("a"),b=i("datasets.Metric.compute()"),E=i(" gathers all the predictions and references from the nodes, and computes the final metric."),this.h()},l(w){h=o(w,"P",{});var j=n(h);k=p(j,"Once you\u2019ve loaded a metric for distributed usage, you can compute the metric as usual. Behind the scenes, "),y=o(j,"A",{href:!0});var q=n(y);b=p(q,"datasets.Metric.compute()"),q.forEach(t),E=p(j," gathers all the predictions and references from the nodes, and computes the final metric."),j.forEach(t),this.h()},h(){c(y,"href","/docs/datasets/pr_/en/package_reference/main_classes#datasets.Metric.compute")},m(w,j){r(w,h,j),a(h,k),a(h,y),a(y,b),a(h,E)},d(w){w&&t(h)}}}function vh(I){let h,k,y,b,E,w,j,q,X,vs,F,K,$s,R,L,ys,A,H,S,ca,ws,ha,ua,js,mr,gr,he,_r,vr,ue,$r,Dl,ma,yr,Il,ga,Nl,Z,bs,me,nt,wr,ge,jr,Cl,xs,br,_e,xr,kr,Ol,M,Er,_a,qr,Pr,rt,Ar,Sr,Hl,it,Fl,va,Tr,Rl,ks,Dr,ve,Ir,Nr,Ll,pt,Ml,Es,Vl,T,Cr,$e,Or,Hr,ye,Fr,Rr,we,Lr,Mr,je,Vr,zr,be,Ur,Jr,zl,dt,Ul,qs,Jl,V,Yr,xe,Br,Gr,ft,Wr,Qr,Yl,ct,Bl,Ps,Xr,ke,Kr,Zr,Gl,ht,Wl,ss,As,Ee,ut,si,qe,ti,Ql,D,ai,Pe,ei,li,Ae,oi,ni,Se,ri,ii,Te,pi,di,$a,fi,ci,Xl,ts,Ss,De,mt,hi,Ie,ui,Kl,ya,mi,Zl,gt,so,wa,gi,to,_t,ao,ja,_i,eo,vt,lo,ba,vi,oo,$t,no,xa,$i,ro,yt,io,as,Ts,Ne,wt,yi,Ce,wi,po,Ds,ji,ka,bi,xi,fo,jt,co,Ea,ki,ho,bt,uo,Is,Ei,Oe,qi,Pi,mo,xt,go,qa,Ai,_o,kt,vo,Pa,Si,$o,es,Ns,He,Et,Ti,Fe,Di,yo,Aa,Ii,wo,qt,jo,Sa,Ni,bo,Pt,xo,ls,Cs,Re,At,Ci,Le,Oi,ko,Ta,Hi,Eo,St,qo,Da,Fi,Po,Tt,Ao,os,Os,Me,Dt,Ri,Ve,Li,So,Hs,Mi,Ia,Vi,zi,To,ns,Fs,ze,It,Ui,Ue,Ji,Do,Rs,Yi,Na,Bi,Gi,Io,Nt,No,rs,Ls,Je,Ct,Wi,Ye,Qi,Co,Ms,Xi,Ca,Ki,Zi,Oo,Ot,Ho,Vs,Fo,is,zs,Be,Ht,sp,Ge,tp,Ro,Oa,ap,Lo,z,ep,We,lp,op,Qe,np,rp,Mo,ps,Us,Xe,Ft,ip,Ke,pp,Vo,U,dp,Ha,fp,cp,Fa,hp,up,zo,J,mp,Ze,gp,_p,sl,vp,$p,Uo,Rt,Jo,Js,yp,tl,wp,jp,Yo,Lt,Bo,Ra,bp,Go,Mt,Wo,La,xp,Qo,Vt,Xo,Ma,kp,Ko,zt,Zo,ds,Ys,al,Ut,Ep,el,qp,sn,Va,Pp,tn,Jt,an,Bs,Ap,ll,Sp,Tp,en,Yt,ln,Gs,on,za,nn,fs,Ws,ol,Bt,Dp,nl,Ip,rn,Ua,Np,pn,cs,Qs,rl,Gt,Cp,il,Op,dn,N,Hp,Ja,Fp,Rp,pl,Lp,Mp,dl,Vp,zp,fn,Xs,Up,Wt,Jp,Yp,cn,Qt,hn,hs,Ks,fl,Xt,Bp,cl,Gp,un,Y,Wp,Ya,Qp,Xp,Kt,Kp,Zp,mn,B,sd,Ba,td,ad,Ga,ed,ld,gn,Zt,_n,G,od,hl,nd,rd,Wa,id,pd,vn,sa,$n,Qa,dd,yn,ta,wn,us,Zs,ul,aa,fd,ml,cd,jn,Xa,hd,bn,ea,xn,st,kn,ms,tt,gl,la,ud,_l,md,En,Ka,gd,qn,oa,Pn,gs,at,vl,na,_d,$l,vd,An,Za,$d,Sn,se,yd,Tn,W,yl,ra,wd,wl,jd,bd,xd,jl,_s,kd,bl,Ed,qd,xl,Pd,Ad,Sd,kl,ia,Td,te,Dd,Id,Dn,pa,In,et,Nn,lt,Nd,El,Cd,Od,Cn,da,On;return w=new P({}),nt=new P({}),it=new x({props:{code:`from datasets import load_dataset
dataset = load_dataset('lhoestq/demo1'),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;lhoestq/demo1&#x27;</span>)`}}),pt=new x({props:{code:`dataset = load_dataset(
  "lhoestq/custom_squad",
  revision="main"  # tag name, or branch name, or commit hash
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(
<span class="hljs-meta">&gt;&gt;&gt; </span>  <span class="hljs-string">&quot;lhoestq/custom_squad&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>  revision=<span class="hljs-string">&quot;main&quot;</span>  <span class="hljs-comment"># tag name, or branch name, or commit hash</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>)`}}),Es=new ce({props:{$$slots:{default:[ch]},$$scope:{ctx:I}}}),dt=new x({props:{code:`data_files = {"train": "train.csv", "test": "test.csv"}
dataset = load_dataset("namespace/your_dataset_name", data_files=data_files),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>data_files = {<span class="hljs-string">&quot;train&quot;</span>: <span class="hljs-string">&quot;train.csv&quot;</span>, <span class="hljs-string">&quot;test&quot;</span>: <span class="hljs-string">&quot;test.csv&quot;</span>}
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;namespace/your_dataset_name&quot;</span>, data_files=data_files)`}}),qs=new ce({props:{warning:"&lcub;true}",$$slots:{default:[hh]},$$scope:{ctx:I}}}),ct=new x({props:{code:`from datasets import load_dataset
c4_subset = load_dataset('allenai/c4', data_files='en/c4-train.0000*-of-01024.json.gz'),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>c4_subset = load_dataset(<span class="hljs-string">&#x27;allenai/c4&#x27;</span>, data_files=<span class="hljs-string">&#x27;en/c4-train.0000*-of-01024.json.gz&#x27;</span>)`}}),ht=new x({props:{code:`data_files = {"validation": "en/c4-validation.*.json.gz"}
c4_validation = load_dataset("allenai/c4", data_files=data_files, split="validation"),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>data_files = {<span class="hljs-string">&quot;validation&quot;</span>: <span class="hljs-string">&quot;en/c4-validation.*.json.gz&quot;</span>}
<span class="hljs-meta">&gt;&gt;&gt; </span>c4_validation = load_dataset(<span class="hljs-string">&quot;allenai/c4&quot;</span>, data_files=data_files, split=<span class="hljs-string">&quot;validation&quot;</span>)`}}),ut=new P({}),mt=new P({}),gt=new x({props:{code:`from datasets import load_dataset
dataset = load_dataset('csv', data_files='my_file.csv'),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;csv&#x27;</span>, data_files=<span class="hljs-string">&#x27;my_file.csv&#x27;</span>)`}}),_t=new x({props:{code:"dataset = load_dataset('csv', data_files=['my_file_1.csv', 'my_file_2.csv', 'my_file_3.csv']),",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;csv&#x27;</span>, data_files=[<span class="hljs-string">&#x27;my_file_1.csv&#x27;</span>, <span class="hljs-string">&#x27;my_file_2.csv&#x27;</span>, <span class="hljs-string">&#x27;my_file_3.csv&#x27;</span>])'}}),vt=new x({props:{code:"dataset = load_dataset('csv', data_files={'train': ['my_train_file_1.csv', 'my_train_file_2.csv'] 'test': 'my_test_file.csv'}),",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;csv&#x27;</span>, data_files={<span class="hljs-string">&#x27;train&#x27;</span>: [<span class="hljs-string">&#x27;my_train_file_1.csv&#x27;</span>, <span class="hljs-string">&#x27;my_train_file_2.csv&#x27;</span>] <span class="hljs-string">&#x27;test&#x27;</span>: <span class="hljs-string">&#x27;my_test_file.csv&#x27;</span>})'}}),$t=new x({props:{code:`base_url = "https://huggingface.co/datasets/lhoestq/demo1/resolve/main/data/"
dataset = load_dataset('csv', data_files={'train': base_url + 'train.csv', 'test': base_url + 'test.csv'}),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>base_url = <span class="hljs-string">&quot;https://huggingface.co/datasets/lhoestq/demo1/resolve/main/data/&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;csv&#x27;</span>, data_files={<span class="hljs-string">&#x27;train&#x27;</span>: base_url + <span class="hljs-string">&#x27;train.csv&#x27;</span>, <span class="hljs-string">&#x27;test&#x27;</span>: base_url + <span class="hljs-string">&#x27;test.csv&#x27;</span>})`}}),yt=new x({props:{code:`url = "https://domain.org/train_data.zip"
data_files = {"train": url}
dataset = load_dataset("csv", data_files=data_files),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;https://domain.org/train_data.zip&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>data_files = {<span class="hljs-string">&quot;train&quot;</span>: url}
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;csv&quot;</span>, data_files=data_files)`}}),wt=new P({}),jt=new x({props:{code:`from datasets import load_dataset
dataset = load_dataset('json', data_files='my_file.json'),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;json&#x27;</span>, data_files=<span class="hljs-string">&#x27;my_file.json&#x27;</span>)`}}),bt=new x({props:{code:`{"a": 1, "b": 2.0, "c": "foo", "d": false}
{"a": 4, "b": -5.5, "c": null, "d": true},`,highlighted:`{<span class="hljs-string">&quot;a&quot;</span>: <span class="hljs-number">1</span>, <span class="hljs-string">&quot;b&quot;</span>: <span class="hljs-number">2.0</span>, <span class="hljs-string">&quot;c&quot;</span>: <span class="hljs-string">&quot;foo&quot;</span>, <span class="hljs-string">&quot;d&quot;</span>: false}
{<span class="hljs-string">&quot;a&quot;</span>: <span class="hljs-number">4</span>, <span class="hljs-string">&quot;b&quot;</span>: -<span class="hljs-number">5.5</span>, <span class="hljs-string">&quot;c&quot;</span>: null, <span class="hljs-string">&quot;d&quot;</span>: true}`}}),xt=new x({props:{code:`{"version": "0.1.0",
    "data": [{"a": 1, "b": 2.0, "c": "foo", "d": false},
            {"a": 4, "b": -5.5, "c": null, "d": true}]
}

>>> from datasets import load_dataset
>>> dataset = load_dataset('json', data_files='my_file.json', field='data'),`,highlighted:`{<span class="hljs-string">&quot;version&quot;</span>: <span class="hljs-string">&quot;0.1.0&quot;</span>,
    <span class="hljs-string">&quot;data&quot;</span>: [{<span class="hljs-string">&quot;a&quot;</span>: <span class="hljs-number">1</span>, <span class="hljs-string">&quot;b&quot;</span>: <span class="hljs-number">2.0</span>, <span class="hljs-string">&quot;c&quot;</span>: <span class="hljs-string">&quot;foo&quot;</span>, <span class="hljs-string">&quot;d&quot;</span>: false},
            {<span class="hljs-string">&quot;a&quot;</span>: <span class="hljs-number">4</span>, <span class="hljs-string">&quot;b&quot;</span>: -<span class="hljs-number">5.5</span>, <span class="hljs-string">&quot;c&quot;</span>: null, <span class="hljs-string">&quot;d&quot;</span>: true}]
}

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;json&#x27;</span>, data_files=<span class="hljs-string">&#x27;my_file.json&#x27;</span>, field=<span class="hljs-string">&#x27;data&#x27;</span>)`}}),kt=new x({props:{code:`base_url = "https://rajpurkar.github.io/SQuAD-explorer/dataset/"
dataset = load_dataset('json', data_files={'train': base_url + 'train-v1.1.json', 'validation': base_url + 'dev-v1.1.json'}, field="data"),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>base_url = <span class="hljs-string">&quot;https://rajpurkar.github.io/SQuAD-explorer/dataset/&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;json&#x27;</span>, data_files={<span class="hljs-string">&#x27;train&#x27;</span>: base_url + <span class="hljs-string">&#x27;train-v1.1.json&#x27;</span>, <span class="hljs-string">&#x27;validation&#x27;</span>: base_url + <span class="hljs-string">&#x27;dev-v1.1.json&#x27;</span>}, field=<span class="hljs-string">&quot;data&quot;</span>)`}}),Et=new P({}),qt=new x({props:{code:`from datasets import load_dataset
dataset = load_dataset('text', data_files={'train': ['my_text_1.txt', 'my_text_2.txt'], 'test': 'my_test_file.txt'}),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;text&#x27;</span>, data_files={<span class="hljs-string">&#x27;train&#x27;</span>: [<span class="hljs-string">&#x27;my_text_1.txt&#x27;</span>, <span class="hljs-string">&#x27;my_text_2.txt&#x27;</span>], <span class="hljs-string">&#x27;test&#x27;</span>: <span class="hljs-string">&#x27;my_test_file.txt&#x27;</span>})`}}),Pt=new x({props:{code:"dataset = load_dataset('text', data_files='https://huggingface.co/datasets/lhoestq/test/resolve/main/some_text.txt'),",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;text&#x27;</span>, data_files=<span class="hljs-string">&#x27;https://huggingface.co/datasets/lhoestq/test/resolve/main/some_text.txt&#x27;</span>)'}}),At=new P({}),St=new x({props:{code:`from datasets import load_dataset
dataset = load_dataset("parquet", data_files={'train': 'train.parquet', 'test': 'test.parquet'}),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;parquet&quot;</span>, data_files={<span class="hljs-string">&#x27;train&#x27;</span>: <span class="hljs-string">&#x27;train.parquet&#x27;</span>, <span class="hljs-string">&#x27;test&#x27;</span>: <span class="hljs-string">&#x27;test.parquet&#x27;</span>})`}}),Tt=new x({props:{code:`base_url = "https://storage.googleapis.com/huggingface-nlp/cache/datasets/wikipedia/20200501.en/1.0.0/"
data_files = {"train": base_url + "wikipedia-train.parquet"}
wiki = load_dataset("parquet", data_files=data_files, split="train"),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>base_url = <span class="hljs-string">&quot;https://storage.googleapis.com/huggingface-nlp/cache/datasets/wikipedia/20200501.en/1.0.0/&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>data_files = {<span class="hljs-string">&quot;train&quot;</span>: base_url + <span class="hljs-string">&quot;wikipedia-train.parquet&quot;</span>}
<span class="hljs-meta">&gt;&gt;&gt; </span>wiki = load_dataset(<span class="hljs-string">&quot;parquet&quot;</span>, data_files=data_files, split=<span class="hljs-string">&quot;train&quot;</span>)`}}),Dt=new P({}),It=new P({}),Nt=new x({props:{code:`from datasets import Dataset
my_dict = {"a": [1, 2, 3]}
dataset = Dataset.from_dict(my_dict),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> Dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>my_dict = {<span class="hljs-string">&quot;a&quot;</span>: [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>]}
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = Dataset.from_dict(my_dict)`}}),Ct=new P({}),Ot=new x({props:{code:`from datasets import Dataset
import pandas as pd
df = pd.DataFrame({"a": [1, 2, 3]})
dataset = Dataset.from_pandas(df),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> Dataset
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd
<span class="hljs-meta">&gt;&gt;&gt; </span>df = pd.DataFrame({<span class="hljs-string">&quot;a&quot;</span>: [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>]})
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = Dataset.from_pandas(df)`}}),Vs=new ce({props:{warning:"&lcub;true}",$$slots:{default:[uh]},$$scope:{ctx:I}}}),Ht=new P({}),Ft=new P({}),Rt=new x({props:{code:`train_test_ds = datasets.load_dataset('bookcorpus', split='train+test')
ri = datasets.ReadInstruction('train') + datasets.ReadInstruction('test')
train_test_ds = datasets.load_dataset('bookcorpus', split=ri),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>train_test_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=<span class="hljs-string">&#x27;train+test&#x27;</span>)
===STRINGAPI-READINSTRUCTION-SPLIT===
<span class="hljs-meta">&gt;&gt;&gt; </span>ri = datasets.ReadInstruction(<span class="hljs-string">&#x27;train&#x27;</span>) + datasets.ReadInstruction(<span class="hljs-string">&#x27;test&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>train_test_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=ri)`}}),Lt=new x({props:{code:`train_10_20_ds = datasets.load_dataset('bookcorpus', split='train[10:20]')
train_10_20_ds = datasets.load_dataset('bookcorpus', split=datasets.ReadInstruction('train', from_=10, to=20, unit='abs')),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>train_10_20_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=<span class="hljs-string">&#x27;train[10:20]&#x27;</span>)
===STRINGAPI-READINSTRUCTION-SPLIT===
<span class="hljs-meta">&gt;&gt;&gt; </span>train_10_20_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=datasets.ReadInstruction(<span class="hljs-string">&#x27;train&#x27;</span>, from_=<span class="hljs-number">10</span>, to=<span class="hljs-number">20</span>, unit=<span class="hljs-string">&#x27;abs&#x27;</span>))`}}),Mt=new x({props:{code:`train_10pct_ds = datasets.load_dataset('bookcorpus', split='train[:10%]')
train_10_20_ds = datasets.load_dataset('bookcorpus', split=datasets.ReadInstruction('train', to=10, unit='%')),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>train_10pct_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=<span class="hljs-string">&#x27;train[:10%]&#x27;</span>)
===STRINGAPI-READINSTRUCTION-SPLIT===
<span class="hljs-meta">&gt;&gt;&gt; </span>train_10_20_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=datasets.ReadInstruction(<span class="hljs-string">&#x27;train&#x27;</span>, to=<span class="hljs-number">10</span>, unit=<span class="hljs-string">&#x27;%&#x27;</span>))`}}),Vt=new x({props:{code:`train_10_80pct_ds = datasets.load_dataset('bookcorpus', split='train[:10%]+train[-80%:]')
ri = (datasets.ReadInstruction('train', to=10, unit='%') + datasets.ReadInstruction('train', from_=-80, unit='%'))
train_10_80pct_ds = datasets.load_dataset('bookcorpus', split=ri),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>train_10_80pct_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=<span class="hljs-string">&#x27;train[:10%]+train[-80%:]&#x27;</span>)
===STRINGAPI-READINSTRUCTION-SPLIT===
<span class="hljs-meta">&gt;&gt;&gt; </span>ri = (datasets.ReadInstruction(<span class="hljs-string">&#x27;train&#x27;</span>, to=<span class="hljs-number">10</span>, unit=<span class="hljs-string">&#x27;%&#x27;</span>) + datasets.ReadInstruction(<span class="hljs-string">&#x27;train&#x27;</span>, from_=-<span class="hljs-number">80</span>, unit=<span class="hljs-string">&#x27;%&#x27;</span>))
<span class="hljs-meta">&gt;&gt;&gt; </span>train_10_80pct_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=ri)`}}),zt=new x({props:{code:`# 10-fold cross-validation (see also next section on rounding behavior):
# The validation datasets are each going to be 10%:
# [0%:10%], [10%:20%], ..., [90%:100%].
# And the training datasets are each going to be the complementary 90%:
# [10%:100%] (for a corresponding validation set of [0%:10%]),
# [0%:10%] + [20%:100%] (for a validation set of [10%:20%]), ...,
# [0%:90%] (for a validation set of [90%:100%]).
vals_ds = datasets.load_dataset('bookcorpus', split=[f'train[{k}%:{k+10}%]' for k in range(0, 100, 10)])
trains_ds = datasets.load_dataset('bookcorpus', split=[f'train[:{k}%]+train[{k+10}%:]' for k in range(0, 100, 10)])
# 10-fold cross-validation (see also next section on rounding behavior):
# The validation datasets are each going to be 10%:
# [0%:10%], [10%:20%], ..., [90%:100%].
# And the training datasets are each going to be the complementary 90%:
# [10%:100%] (for a corresponding validation set of [0%:10%]),
# [0%:10%] + [20%:100%] (for a validation set of [10%:20%]), ...,
# [0%:90%] (for a validation set of [90%:100%]).
vals_ds = datasets.load_dataset('bookcorpus', [datasets.ReadInstruction('train', from_=k, to=k+10, unit='%') for k in range(0, 100, 10)])
trains_ds = datasets.load_dataset('bookcorpus', [(datasets.ReadInstruction('train', to=k, unit='%') + datasets.ReadInstruction('train', from_=k+10, unit='%')) for k in range(0, 100, 10)]),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># 10-fold cross-validation (see also next section on rounding behavior):</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># The validation datasets are each going to be 10%:</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># [0%:10%], [10%:20%], ..., [90%:100%].</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># And the training datasets are each going to be the complementary 90%:</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># [10%:100%] (for a corresponding validation set of [0%:10%]),</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># [0%:10%] + [20%:100%] (for a validation set of [10%:20%]), ...,</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># [0%:90%] (for a validation set of [90%:100%]).</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>vals_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=[<span class="hljs-string">f&#x27;train[<span class="hljs-subst">{k}</span>%:<span class="hljs-subst">{k+<span class="hljs-number">10</span>}</span>%]&#x27;</span> <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, <span class="hljs-number">100</span>, <span class="hljs-number">10</span>)])
<span class="hljs-meta">&gt;&gt;&gt; </span>trains_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=[<span class="hljs-string">f&#x27;train[:<span class="hljs-subst">{k}</span>%]+train[<span class="hljs-subst">{k+<span class="hljs-number">10</span>}</span>%:]&#x27;</span> <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, <span class="hljs-number">100</span>, <span class="hljs-number">10</span>)])
===STRINGAPI-READINSTRUCTION-SPLIT===
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># 10-fold cross-validation (see also next section on rounding behavior):</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># The validation datasets are each going to be 10%:</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># [0%:10%], [10%:20%], ..., [90%:100%].</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># And the training datasets are each going to be the complementary 90%:</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># [10%:100%] (for a corresponding validation set of [0%:10%]),</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># [0%:10%] + [20%:100%] (for a validation set of [10%:20%]), ...,</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># [0%:90%] (for a validation set of [90%:100%]).</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>vals_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, [datasets.ReadInstruction(<span class="hljs-string">&#x27;train&#x27;</span>, from_=k, to=k+<span class="hljs-number">10</span>, unit=<span class="hljs-string">&#x27;%&#x27;</span>) <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, <span class="hljs-number">100</span>, <span class="hljs-number">10</span>)])
<span class="hljs-meta">&gt;&gt;&gt; </span>trains_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, [(datasets.ReadInstruction(<span class="hljs-string">&#x27;train&#x27;</span>, to=k, unit=<span class="hljs-string">&#x27;%&#x27;</span>) + datasets.ReadInstruction(<span class="hljs-string">&#x27;train&#x27;</span>, from_=k+<span class="hljs-number">10</span>, unit=<span class="hljs-string">&#x27;%&#x27;</span>)) <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, <span class="hljs-number">100</span>, <span class="hljs-number">10</span>)])`}}),Ut=new P({}),Jt=new x({props:{code:`# Assuming *train* split contains 999 records.
# 19 records, from 500 (included) to 519 (excluded).
>>> train_50_52_ds = datasets.load_dataset('bookcorpus', split='train[50%:52%]')
# 20 records, from 519 (included) to 539 (excluded).
>>> train_52_54_ds = datasets.load_dataset('bookcorpus', split='train[52%:54%]'),`,highlighted:`<span class="hljs-comment"># Assuming *train* split contains 999 records.</span>
<span class="hljs-comment"># 19 records, from 500 (included) to 519 (excluded).</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>train_50_52_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=<span class="hljs-string">&#x27;train[50%:52%]&#x27;</span>)
<span class="hljs-comment"># 20 records, from 519 (included) to 539 (excluded).</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>train_52_54_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=<span class="hljs-string">&#x27;train[52%:54%]&#x27;</span>)`}}),Yt=new x({props:{code:`# 18 records, from 450 (included) to 468 (excluded).
>>> train_50_52pct1_ds = datasets.load_dataset('bookcorpus', split=datasets.ReadInstruction( 'train', from_=50, to=52, unit='%', rounding='pct1_dropremainder'))
# 18 records, from 468 (included) to 486 (excluded).
>>> train_52_54pct1_ds = datasets.load_dataset('bookcorpus', split=datasets.ReadInstruction('train',from_=52, to=54, unit='%', rounding='pct1_dropremainder'))
# Or equivalently:
>>> train_50_52pct1_ds = datasets.load_dataset('bookcorpus', split='train[50%:52%](pct1_dropremainder)')
>>> train_52_54pct1_ds = datasets.load_dataset('bookcorpus', split='train[52%:54%](pct1_dropremainder)'),`,highlighted:`<span class="hljs-comment"># 18 records, from 450 (included) to 468 (excluded).</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>train_50_52pct1_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=datasets.ReadInstruction( <span class="hljs-string">&#x27;train&#x27;</span>, from_=<span class="hljs-number">50</span>, to=<span class="hljs-number">52</span>, unit=<span class="hljs-string">&#x27;%&#x27;</span>, rounding=<span class="hljs-string">&#x27;pct1_dropremainder&#x27;</span>))
<span class="hljs-comment"># 18 records, from 468 (included) to 486 (excluded).</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>train_52_54pct1_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=datasets.ReadInstruction(<span class="hljs-string">&#x27;train&#x27;</span>,from_=<span class="hljs-number">52</span>, to=<span class="hljs-number">54</span>, unit=<span class="hljs-string">&#x27;%&#x27;</span>, rounding=<span class="hljs-string">&#x27;pct1_dropremainder&#x27;</span>))
<span class="hljs-comment"># Or equivalently:</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>train_50_52pct1_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=<span class="hljs-string">&#x27;train[50%:52%](pct1_dropremainder)&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>train_52_54pct1_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=<span class="hljs-string">&#x27;train[52%:54%](pct1_dropremainder)&#x27;</span>)`}}),Gs=new ce({props:{warning:"&lcub;true}",$$slots:{default:[mh]},$$scope:{ctx:I}}}),Bt=new P({}),Gt=new P({}),Qt=new x({props:{code:'dataset = load_dataset("matinf", "summarization"),',highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;matinf&quot;</span>, <span class="hljs-string">&quot;summarization&quot;</span>)
Downloading <span class="hljs-keyword">and</span> preparing dataset matinf/summarization (download: Unknown size, generated: <span class="hljs-number">246.89</span> MiB, post-processed: Unknown size, total: <span class="hljs-number">246.89</span> MiB) to /root/.cache/huggingface/datasets/matinf/summarization/<span class="hljs-number">1.0</span><span class="hljs-number">.0</span>/82eee5e71c3ceaf20d909bca36ff237452b4e4ab195d3be7ee1c78b53e6f540e...
AssertionError: The dataset matinf <span class="hljs-keyword">with</span> config summarization requires manual data. 
Please follow the manual download instructions: To use MATINF you have to download it manually. Please fill this google form (https://forms.gle/nkH4LVE4iNQeDzsc9). You will receive a download link <span class="hljs-keyword">and</span> a password once you complete the form. Please extract <span class="hljs-built_in">all</span> files <span class="hljs-keyword">in</span> one folder <span class="hljs-keyword">and</span> load the dataset <span class="hljs-keyword">with</span>: *datasets.load_dataset(<span class="hljs-string">&#x27;matinf&#x27;</span>, data_dir=<span class="hljs-string">&#x27;path/to/folder/folder_name&#x27;</span>)*. 
Manual data can be loaded <span class="hljs-keyword">with</span> \`datasets.load_dataset(matinf, data_dir=<span class="hljs-string">&#x27;&lt;path/to/manual/data&gt;&#x27;</span>) `}}),Xt=new P({}),Zt=new x({props:{code:`class_names = ["sadness", "joy", "love", "anger", "fear", "surprise"]
emotion_features = Features({'text': Value('string'), 'label': ClassLabel(names=class_names)}),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>class_names = [<span class="hljs-string">&quot;sadness&quot;</span>, <span class="hljs-string">&quot;joy&quot;</span>, <span class="hljs-string">&quot;love&quot;</span>, <span class="hljs-string">&quot;anger&quot;</span>, <span class="hljs-string">&quot;fear&quot;</span>, <span class="hljs-string">&quot;surprise&quot;</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>emotion_features = Features({<span class="hljs-string">&#x27;text&#x27;</span>: Value(<span class="hljs-string">&#x27;string&#x27;</span>), <span class="hljs-string">&#x27;label&#x27;</span>: ClassLabel(names=class_names)})`}}),sa=new x({props:{code:"dataset = load_dataset('csv', data_files=file_dict, delimiter=';', column_names=['text', 'label'], features=emotion_features),",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;csv&#x27;</span>, data_files=file_dict, delimiter=<span class="hljs-string">&#x27;;&#x27;</span>, column_names=[<span class="hljs-string">&#x27;text&#x27;</span>, <span class="hljs-string">&#x27;label&#x27;</span>], features=emotion_features)'}}),ta=new x({props:{code:"dataset['train'].features,",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>dataset[<span class="hljs-string">&#x27;train&#x27;</span>].features
{<span class="hljs-string">&#x27;text&#x27;</span>: Value(dtype=<span class="hljs-string">&#x27;string&#x27;</span>, <span class="hljs-built_in">id</span>=<span class="hljs-literal">None</span>),
<span class="hljs-string">&#x27;label&#x27;</span>: ClassLabel(num_classes=<span class="hljs-number">6</span>, names=[<span class="hljs-string">&#x27;sadness&#x27;</span>, <span class="hljs-string">&#x27;joy&#x27;</span>, <span class="hljs-string">&#x27;love&#x27;</span>, <span class="hljs-string">&#x27;anger&#x27;</span>, <span class="hljs-string">&#x27;fear&#x27;</span>, <span class="hljs-string">&#x27;surprise&#x27;</span>], names_file=<span class="hljs-literal">None</span>, <span class="hljs-built_in">id</span>=<span class="hljs-literal">None</span>)}`}}),aa=new P({}),ea=new x({props:{code:`from datasets import load_metric
metric = load_metric('PATH/TO/MY/METRIC/SCRIPT')

# Example of typical usage
for batch in dataset:
    inputs, references = batch
    predictions = model(inputs)
    metric.add_batch(predictions=predictions, references=references)
score = metric.compute(),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_metric
<span class="hljs-meta">&gt;&gt;&gt; </span>metric = load_metric(<span class="hljs-string">&#x27;PATH/TO/MY/METRIC/SCRIPT&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Example of typical usage</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">for</span> batch <span class="hljs-keyword">in</span> dataset:
<span class="hljs-meta">... </span>    inputs, references = batch
<span class="hljs-meta">... </span>    predictions = model(inputs)
<span class="hljs-meta">... </span>    metric.add_batch(predictions=predictions, references=references)
<span class="hljs-meta">&gt;&gt;&gt; </span>score = metric.compute()`}}),st=new ce({props:{$$slots:{default:[gh]},$$scope:{ctx:I}}}),la=new P({}),oa=new x({props:{code:`from datasets import load_metric
metric = load_metric('bleurt', name='bleurt-base-128')
metric = load_metric('bleurt', name='bleurt-base-512'),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python"><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_metric</span>
<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python">metric = load_metric(<span class="hljs-string">&#x27;bleurt&#x27;</span>, name=<span class="hljs-string">&#x27;bleurt-base-128&#x27;</span>)</span>
<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python">metric = load_metric(<span class="hljs-string">&#x27;bleurt&#x27;</span>, name=<span class="hljs-string">&#x27;bleurt-base-512&#x27;</span>)</span>`}}),na=new P({}),pa=new x({props:{code:`from datasets import load_metric
metric = load_metric('glue', 'mrpc', num_process=num_process, process_id=rank),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_metric
<span class="hljs-meta">&gt;&gt;&gt; </span>metric = load_metric(<span class="hljs-string">&#x27;glue&#x27;</span>, <span class="hljs-string">&#x27;mrpc&#x27;</span>, num_process=num_process, process_id=rank)`}}),et=new ce({props:{$$slots:{default:[_h]},$$scope:{ctx:I}}}),da=new x({props:{code:`from datasets import load_metric
metric = load_metric('glue', 'mrpc', num_process=num_process, process_id=process_id, experiment_id="My_experiment_10"),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_metric
<span class="hljs-meta">&gt;&gt;&gt; </span>metric = load_metric(<span class="hljs-string">&#x27;glue&#x27;</span>, <span class="hljs-string">&#x27;mrpc&#x27;</span>, num_process=num_process, process_id=process_id, experiment_id=<span class="hljs-string">&quot;My_experiment_10&quot;</span>)`}}),{c(){h=l("meta"),k=d(),y=l("h1"),b=l("a"),E=l("span"),u(w.$$.fragment),j=d(),q=l("span"),X=i("Load"),vs=d(),F=l("p"),K=i("You have already seen how to load a dataset from the Hugging Face Hub. But datasets are stored in a variety of places, and sometimes you won\u2019t find the one you want on the Hub. A dataset can be on disk on your local machine, in a Github repository, and in in-memory data structures like Python dictionaries and Pandas DataFrames. Wherever your dataset may be stored, \u{1F917} Datasets provides a way for you to load and use it for training."),$s=d(),R=l("p"),L=i("This guide will show you how to load a dataset from:"),ys=d(),A=l("ul"),H=l("li"),S=i("The Hub without a dataset loading script"),ca=d(),ws=l("li"),ha=i("Local files"),ua=d(),js=l("li"),mr=i("In-memory data"),gr=d(),he=l("li"),_r=i("Offline"),vr=d(),ue=l("li"),$r=i("A specific slice of a split"),Dl=d(),ma=l("p"),yr=i("You will also learn how to troubleshoot common errors, and how to load specific configurations of a metric."),Il=d(),ga=l("a"),Nl=d(),Z=l("h2"),bs=l("a"),me=l("span"),u(nt.$$.fragment),wr=d(),ge=l("span"),jr=i("Hugging Face Hub"),Cl=d(),xs=l("p"),br=i("In the tutorial, you learned how to load a dataset from the Hub. This method relies on a dataset loading script that downloads and builds the dataset. However, you can also load a dataset from any dataset repository on the Hub "),_e=l("strong"),xr=i("without"),kr=i(" a loading script!"),Ol=d(),M=l("p"),Er=i("First, create a dataset repository and upload your data files. Then you can use "),_a=l("a"),qr=i("datasets.load_dataset()"),Pr=i(" like you learned in the tutorial. For example, load the files from this "),rt=l("a"),Ar=i("demo repository"),Sr=i(" by providing the repository namespace and dataset name:"),Hl=d(),u(it.$$.fragment),Fl=d(),va=l("p"),Tr=i("This dataset repository contains CSV files, and this code loads all the data from the CSV files."),Rl=d(),ks=l("p"),Dr=i("Some datasets may have more than one version, based on Git tags, branches or commits. Use the "),ve=l("code"),Ir=i("revision"),Nr=i(" flag to specify which dataset version you want to load:"),Ll=d(),u(pt.$$.fragment),Ml=d(),u(Es.$$.fragment),Vl=d(),T=l("p"),Cr=i("If the dataset doesn\u2019t have a dataset loading script, then by default, all the data will be loaded in the "),$e=l("code"),Or=i("train"),Hr=i(" split. Use the "),ye=l("code"),Fr=i("data_files"),Rr=i(" parameter to map data files to splits like "),we=l("code"),Lr=i("train"),Mr=i(", "),je=l("code"),Vr=i("validation"),zr=i(" and "),be=l("code"),Ur=i("test"),Jr=i(":"),zl=d(),u(dt.$$.fragment),Ul=d(),u(qs.$$.fragment),Jl=d(),V=l("p"),Yr=i("You can also load a specific subset of the files with the "),xe=l("code"),Br=i("data_files"),Gr=i(" parameter. The example below loads files from the "),ft=l("a"),Wr=i("C4 dataset"),Qr=i(":"),Yl=d(),u(ct.$$.fragment),Bl=d(),Ps=l("p"),Xr=i("Specify a custom split with the "),ke=l("code"),Kr=i("split"),Zr=i(" parameter:"),Gl=d(),u(ht.$$.fragment),Wl=d(),ss=l("h2"),As=l("a"),Ee=l("span"),u(ut.$$.fragment),si=d(),qe=l("span"),ti=i("Local and remote files"),Ql=d(),D=l("p"),ai=i("\u{1F917} Datasets can be loaded from local files stored on your computer, and also from remote files. The datasets are most likely stored as a "),Pe=l("code"),ei=i("csv"),li=i(", "),Ae=l("code"),oi=i("json"),ni=i(", "),Se=l("code"),ri=i("txt"),ii=i(" or "),Te=l("code"),pi=i("parquet"),di=i(" file. The "),$a=l("a"),fi=i("datasets.load_dataset()"),ci=i(" method is able to load each of these file types."),Xl=d(),ts=l("h3"),Ss=l("a"),De=l("span"),u(mt.$$.fragment),hi=d(),Ie=l("span"),ui=i("CSV"),Kl=d(),ya=l("p"),mi=i("\u{1F917} Datasets can read a dataset made up of one or several CSV files:"),Zl=d(),u(gt.$$.fragment),so=d(),wa=l("p"),gi=i("If you have more than one CSV file:"),to=d(),u(_t.$$.fragment),ao=d(),ja=l("p"),_i=i("You can also map the training and test splits to specific CSV files:"),eo=d(),u(vt.$$.fragment),lo=d(),ba=l("p"),vi=i("To load remote CSV files via HTTP, you can pass the URLs:"),oo=d(),u($t.$$.fragment),no=d(),xa=l("p"),$i=i("To load zipped CSV files:"),ro=d(),u(yt.$$.fragment),io=d(),as=l("h3"),Ts=l("a"),Ne=l("span"),u(wt.$$.fragment),yi=d(),Ce=l("span"),wi=i("JSON"),po=d(),Ds=l("p"),ji=i("JSON files are loaded directly with "),ka=l("a"),bi=i("datasets.load_dataset()"),xi=i(" as shown below:"),fo=d(),u(jt.$$.fragment),co=d(),Ea=l("p"),ki=i("JSON files can have diverse formats, but we think the most efficient format is to have multiple JSON objects; each line represents an individual row of data. For example:"),ho=d(),u(bt.$$.fragment),uo=d(),Is=l("p"),Ei=i("Another JSON format you may encounter is a nested field, in which case you will need to specify the "),Oe=l("code"),qi=i("field"),Pi=i(" argument as shown in the following:"),mo=d(),u(xt.$$.fragment),go=d(),qa=l("p"),Ai=i("To load remote JSON files via HTTP, you can pass the URLs:"),_o=d(),u(kt.$$.fragment),vo=d(),Pa=l("p"),Si=i("While these are the most common JSON formats, you will see other datasets that are formatted differently. \u{1F917} Datasets recognizes these other formats, and will fallback accordingly on the Python JSON loading methods to handle them."),$o=d(),es=l("h3"),Ns=l("a"),He=l("span"),u(Et.$$.fragment),Ti=d(),Fe=l("span"),Di=i("Text files"),yo=d(),Aa=l("p"),Ii=i("Text files are one of the most common file types for storing a dataset. \u{1F917} Datasets will read the text file line by line to build the dataset."),wo=d(),u(qt.$$.fragment),jo=d(),Sa=l("p"),Ni=i("To load remote TXT files via HTTP, you can pass the URLs:"),bo=d(),u(Pt.$$.fragment),xo=d(),ls=l("h3"),Cs=l("a"),Re=l("span"),u(At.$$.fragment),Ci=d(),Le=l("span"),Oi=i("Parquet"),ko=d(),Ta=l("p"),Hi=i("Parquet files are stored in a columnar format unlike row-based files like CSV. Large datasets may be stored in a Parquet file because it is more efficient, and faster at returning your query. Load a Parquet file as shown in the following example:"),Eo=d(),u(St.$$.fragment),qo=d(),Da=l("p"),Fi=i("To load remote parquet files via HTTP, you can pass the URLs:"),Po=d(),u(Tt.$$.fragment),Ao=d(),os=l("h2"),Os=l("a"),Me=l("span"),u(Dt.$$.fragment),Ri=d(),Ve=l("span"),Li=i("In-memory data"),So=d(),Hs=l("p"),Mi=i("\u{1F917} Datasets will also allow you to create a "),Ia=l("a"),Vi=i("datasets.Dataset"),zi=i(" directly from in-memory data structures like Python dictionaries and Pandas DataFrames."),To=d(),ns=l("h3"),Fs=l("a"),ze=l("span"),u(It.$$.fragment),Ui=d(),Ue=l("span"),Ji=i("Python dictionary"),Do=d(),Rs=l("p"),Yi=i("Load Python dictionaries with "),Na=l("a"),Bi=i("datasets.Dataset.from_dict()"),Gi=i(":"),Io=d(),u(Nt.$$.fragment),No=d(),rs=l("h3"),Ls=l("a"),Je=l("span"),u(Ct.$$.fragment),Wi=d(),Ye=l("span"),Qi=i("Pandas DataFrame"),Co=d(),Ms=l("p"),Xi=i("Load Pandas DataFrames with "),Ca=l("a"),Ki=i("datasets.Dataset.from_pandas()"),Zi=i(":"),Oo=d(),u(Ot.$$.fragment),Ho=d(),u(Vs.$$.fragment),Fo=d(),is=l("h2"),zs=l("a"),Be=l("span"),u(Ht.$$.fragment),sp=d(),Ge=l("span"),tp=i("Offline"),Ro=d(),Oa=l("p"),ap=i("Even if you don\u2019t have an internet connection, it is still possible to load a dataset. As long as you\u2019ve downloaded a dataset from the Hub or \u{1F917} Datasets GitHub repository before, it should be cached. This means you can reload the dataset from the cache and use it offline."),Lo=d(),z=l("p"),ep=i("If you know you won\u2019t have internet access, you can run \u{1F917} Datasets in full offline mode. This saves time because instead of waiting for the Dataset builder download to time out, \u{1F917} Datasets will look directly in the cache. Set the environment variable "),We=l("code"),lp=i("HF_DATASETS_OFFLINE"),op=i(" to "),Qe=l("code"),np=i("1"),rp=i(" to enable full offline mode."),Mo=d(),ps=l("h2"),Us=l("a"),Xe=l("span"),u(Ft.$$.fragment),ip=d(),Ke=l("span"),pp=i("Slice splits"),Vo=d(),U=l("p"),dp=i("For even greater control over how to load a split, you can choose to only load specific slices of a split. There are two options for slicing a split: using strings or "),Ha=l("a"),fp=i("datasets.ReadInstruction"),cp=i(". Strings are more compact and readable for simple cases, while "),Fa=l("a"),hp=i("datasets.ReadInstruction"),up=i(" is easier to use with variable slicing parameters."),zo=d(),J=l("p"),mp=i("Concatenate the "),Ze=l("code"),gp=i("train"),_p=i(" and "),sl=l("code"),vp=i("test"),$p=i(" split by:"),Uo=d(),u(Rt.$$.fragment),Jo=d(),Js=l("p"),yp=i("Select specific rows of the "),tl=l("code"),wp=i("train"),jp=i(" split:"),Yo=d(),u(Lt.$$.fragment),Bo=d(),Ra=l("p"),bp=i("Or select a percentage of the split with:"),Go=d(),u(Mt.$$.fragment),Wo=d(),La=l("p"),xp=i("You can even select a combination of percentages from each split:"),Qo=d(),u(Vt.$$.fragment),Xo=d(),Ma=l("p"),kp=i("Finally, create cross-validated dataset splits by:"),Ko=d(),u(zt.$$.fragment),Zo=d(),ds=l("h3"),Ys=l("a"),al=l("span"),u(Ut.$$.fragment),Ep=d(),el=l("span"),qp=i("Percent slicing and rounding"),sn=d(),Va=l("p"),Pp=i("For datasets where the requested slice boundaries do not divide evenly by 100, the default behavior is to round the boundaries to the nearest integer. As a result, some slices may contain more examples than others as shown in the following example:"),tn=d(),u(Jt.$$.fragment),an=d(),Bs=l("p"),Ap=i("If you want equal sized splits, use "),ll=l("code"),Sp=i("pct1_dropremainder"),Tp=i(" rounding instead. This will treat the specified percentage boundaries as multiples of 1%."),en=d(),u(Yt.$$.fragment),ln=d(),u(Gs.$$.fragment),on=d(),za=l("a"),nn=d(),fs=l("h2"),Ws=l("a"),ol=l("span"),u(Bt.$$.fragment),Dp=d(),nl=l("span"),Ip=i("Troubleshooting"),rn=d(),Ua=l("p"),Np=i("Sometimes, you may get unexpected results when you load a dataset. In this section, you will learn how to solve two common issues you may encounter when you load a dataset: manually download a dataset, and specify features of a dataset."),pn=d(),cs=l("h3"),Qs=l("a"),rl=l("span"),u(Gt.$$.fragment),Cp=d(),il=l("span"),Op=i("Manual download"),dn=d(),N=l("p"),Hp=i("Certain datasets require you to manually download the dataset files due to licensing incompatibility, or if the files are hidden behind a login page. This will cause "),Ja=l("a"),Fp=i("datasets.load_dataset()"),Rp=i(" to throw an "),pl=l("code"),Lp=i("AssertionError"),Mp=i(". But \u{1F917} Datasets provides detailed instructions for downloading the missing files. After you have downloaded the files, use the "),dl=l("code"),Vp=i("data_dir"),zp=i(" argument to specify the path to the files you just downloaded."),fn=d(),Xs=l("p"),Up=i("For example, if you try to download a configuration from the "),Wt=l("a"),Jp=i("MATINF"),Yp=i(" dataset:"),cn=d(),u(Qt.$$.fragment),hn=d(),hs=l("h3"),Ks=l("a"),fl=l("span"),u(Xt.$$.fragment),Bp=d(),cl=l("span"),Gp=i("Specify features"),un=d(),Y=l("p"),Wp=i("When you create a dataset from local files, the "),Ya=l("a"),Qp=i("datasets.Features"),Xp=i(" are automatically inferred by "),Kt=l("a"),Kp=i("Apache Arrow"),Zp=i(". However, the features of the dataset may not always align with your expectations or you may want to define the features yourself."),mn=d(),B=l("p"),sd=i("The following example shows how you can add custom labels with "),Ba=l("a"),td=i("datasets.ClassLabel"),ad=i(". First, define your own labels using the "),Ga=l("a"),ed=i("datasets.Features"),ld=i(" class:"),gn=d(),u(Zt.$$.fragment),_n=d(),G=l("p"),od=i("Next, specify the "),hl=l("code"),nd=i("features"),rd=i(" argument in "),Wa=l("a"),id=i("datasets.load_dataset()"),pd=i(" with the features you just created:"),vn=d(),u(sa.$$.fragment),$n=d(),Qa=l("p"),dd=i("Now when you look at your dataset features, you can see it uses the custom labels you defined:"),yn=d(),u(ta.$$.fragment),wn=d(),us=l("h2"),Zs=l("a"),ul=l("span"),u(aa.$$.fragment),fd=d(),ml=l("span"),cd=i("Metrics"),jn=d(),Xa=l("p"),hd=i("When the metric you want to use is not supported by \u{1F917} Datasets, you can write and use your own metric script. Load your metric by providing the path to your local metric loading script:"),bn=d(),u(ea.$$.fragment),xn=d(),u(st.$$.fragment),kn=d(),ms=l("h3"),tt=l("a"),gl=l("span"),u(la.$$.fragment),ud=d(),_l=l("span"),md=i("Load configurations"),En=d(),Ka=l("p"),gd=i("It is possible for a metric to have different configurations. The configurations are stored in the [\u2018datasets.Metric.config_name\u2019] attribute. When you load a metric, provide the configuration name as shown in the following:"),qn=d(),u(oa.$$.fragment),Pn=d(),gs=l("h3"),at=l("a"),vl=l("span"),u(na.$$.fragment),_d=d(),$l=l("span"),vd=i("Distributed setup"),An=d(),Za=l("p"),$d=i("When you work in a distributed or parallel processing environment, loading and computing a metric can be tricky because these processes are executed in parallel on separate subsets of the data. \u{1F917} Datasets supports distributed usage with a few additional arguments when you load a metric."),Sn=d(),se=l("p"),yd=i("For example, imagine you are training and evaluating on eight parallel processes. Here\u2019s how you would load a metric in this distributed setting:"),Tn=d(),W=l("ol"),yl=l("li"),ra=l("p"),wd=i("Define the total number of processes with the "),wl=l("code"),jd=i("num_process"),bd=i(" argument."),xd=d(),jl=l("li"),_s=l("p"),kd=i("Set the process "),bl=l("code"),Ed=i("rank"),qd=i(" as an integer between zero and "),xl=l("code"),Pd=i("num_process - 1"),Ad=i("."),Sd=d(),kl=l("li"),ia=l("p"),Td=i("Load your metric with "),te=l("a"),Dd=i("datasets.load_metric()"),Id=i(" with these arguments:"),Dn=d(),u(pa.$$.fragment),In=d(),u(et.$$.fragment),Nn=d(),lt=l("p"),Nd=i("In some instances, you may be simultaneously running multiple independent distributed evaluations on the same server and files. To avoid any conflicts, it is important to provide an "),El=l("code"),Cd=i("experiment_id"),Od=i(" to distinguish the separate evaluations:"),Cn=d(),u(da.$$.fragment),this.h()},l(s){const e=fh('[data-svelte="svelte-1phssyn"]',document.head);h=o(e,"META",{name:!0,content:!0}),e.forEach(t),k=f(s),y=o(s,"H1",{class:!0});var fa=n(y);b=o(fa,"A",{id:!0,class:!0,href:!0});var ql=n(b);E=o(ql,"SPAN",{});var Pl=n(E);m(w.$$.fragment,Pl),Pl.forEach(t),ql.forEach(t),j=f(fa),q=o(fa,"SPAN",{});var Al=n(q);X=p(Al,"Load"),Al.forEach(t),fa.forEach(t),vs=f(s),F=o(s,"P",{});var Sl=n(F);K=p(Sl,"You have already seen how to load a dataset from the Hugging Face Hub. But datasets are stored in a variety of places, and sometimes you won\u2019t find the one you want on the Hub. A dataset can be on disk on your local machine, in a Github repository, and in in-memory data structures like Python dictionaries and Pandas DataFrames. Wherever your dataset may be stored, \u{1F917} Datasets provides a way for you to load and use it for training."),Sl.forEach(t),$s=f(s),R=o(s,"P",{});var Tl=n(R);L=p(Tl,"This guide will show you how to load a dataset from:"),Tl.forEach(t),ys=f(s),A=o(s,"UL",{});var Q=n(A);H=o(Q,"LI",{});var Hd=n(H);S=p(Hd,"The Hub without a dataset loading script"),Hd.forEach(t),ca=f(Q),ws=o(Q,"LI",{});var Fd=n(ws);ha=p(Fd,"Local files"),Fd.forEach(t),ua=f(Q),js=o(Q,"LI",{});var Rd=n(js);mr=p(Rd,"In-memory data"),Rd.forEach(t),gr=f(Q),he=o(Q,"LI",{});var Ld=n(he);_r=p(Ld,"Offline"),Ld.forEach(t),vr=f(Q),ue=o(Q,"LI",{});var Md=n(ue);$r=p(Md,"A specific slice of a split"),Md.forEach(t),Q.forEach(t),Dl=f(s),ma=o(s,"P",{});var Vd=n(ma);yr=p(Vd,"You will also learn how to troubleshoot common errors, and how to load specific configurations of a metric."),Vd.forEach(t),Il=f(s),ga=o(s,"A",{id:!0}),n(ga).forEach(t),Nl=f(s),Z=o(s,"H2",{class:!0});var Hn=n(Z);bs=o(Hn,"A",{id:!0,class:!0,href:!0});var zd=n(bs);me=o(zd,"SPAN",{});var Ud=n(me);m(nt.$$.fragment,Ud),Ud.forEach(t),zd.forEach(t),wr=f(Hn),ge=o(Hn,"SPAN",{});var Jd=n(ge);jr=p(Jd,"Hugging Face Hub"),Jd.forEach(t),Hn.forEach(t),Cl=f(s),xs=o(s,"P",{});var Fn=n(xs);br=p(Fn,"In the tutorial, you learned how to load a dataset from the Hub. This method relies on a dataset loading script that downloads and builds the dataset. However, you can also load a dataset from any dataset repository on the Hub "),_e=o(Fn,"STRONG",{});var Yd=n(_e);xr=p(Yd,"without"),Yd.forEach(t),kr=p(Fn," a loading script!"),Fn.forEach(t),Ol=f(s),M=o(s,"P",{});var ae=n(M);Er=p(ae,"First, create a dataset repository and upload your data files. Then you can use "),_a=o(ae,"A",{href:!0});var Bd=n(_a);qr=p(Bd,"datasets.load_dataset()"),Bd.forEach(t),Pr=p(ae," like you learned in the tutorial. For example, load the files from this "),rt=o(ae,"A",{href:!0,rel:!0});var Gd=n(rt);Ar=p(Gd,"demo repository"),Gd.forEach(t),Sr=p(ae," by providing the repository namespace and dataset name:"),ae.forEach(t),Hl=f(s),m(it.$$.fragment,s),Fl=f(s),va=o(s,"P",{});var Wd=n(va);Tr=p(Wd,"This dataset repository contains CSV files, and this code loads all the data from the CSV files."),Wd.forEach(t),Rl=f(s),ks=o(s,"P",{});var Rn=n(ks);Dr=p(Rn,"Some datasets may have more than one version, based on Git tags, branches or commits. Use the "),ve=o(Rn,"CODE",{});var Qd=n(ve);Ir=p(Qd,"revision"),Qd.forEach(t),Nr=p(Rn," flag to specify which dataset version you want to load:"),Rn.forEach(t),Ll=f(s),m(pt.$$.fragment,s),Ml=f(s),m(Es.$$.fragment,s),Vl=f(s),T=o(s,"P",{});var C=n(T);Cr=p(C,"If the dataset doesn\u2019t have a dataset loading script, then by default, all the data will be loaded in the "),$e=o(C,"CODE",{});var Xd=n($e);Or=p(Xd,"train"),Xd.forEach(t),Hr=p(C," split. Use the "),ye=o(C,"CODE",{});var Kd=n(ye);Fr=p(Kd,"data_files"),Kd.forEach(t),Rr=p(C," parameter to map data files to splits like "),we=o(C,"CODE",{});var Zd=n(we);Lr=p(Zd,"train"),Zd.forEach(t),Mr=p(C,", "),je=o(C,"CODE",{});var sf=n(je);Vr=p(sf,"validation"),sf.forEach(t),zr=p(C," and "),be=o(C,"CODE",{});var tf=n(be);Ur=p(tf,"test"),tf.forEach(t),Jr=p(C,":"),C.forEach(t),zl=f(s),m(dt.$$.fragment,s),Ul=f(s),m(qs.$$.fragment,s),Jl=f(s),V=o(s,"P",{});var ee=n(V);Yr=p(ee,"You can also load a specific subset of the files with the "),xe=o(ee,"CODE",{});var af=n(xe);Br=p(af,"data_files"),af.forEach(t),Gr=p(ee," parameter. The example below loads files from the "),ft=o(ee,"A",{href:!0,rel:!0});var ef=n(ft);Wr=p(ef,"C4 dataset"),ef.forEach(t),Qr=p(ee,":"),ee.forEach(t),Yl=f(s),m(ct.$$.fragment,s),Bl=f(s),Ps=o(s,"P",{});var Ln=n(Ps);Xr=p(Ln,"Specify a custom split with the "),ke=o(Ln,"CODE",{});var lf=n(ke);Kr=p(lf,"split"),lf.forEach(t),Zr=p(Ln," parameter:"),Ln.forEach(t),Gl=f(s),m(ht.$$.fragment,s),Wl=f(s),ss=o(s,"H2",{class:!0});var Mn=n(ss);As=o(Mn,"A",{id:!0,class:!0,href:!0});var of=n(As);Ee=o(of,"SPAN",{});var nf=n(Ee);m(ut.$$.fragment,nf),nf.forEach(t),of.forEach(t),si=f(Mn),qe=o(Mn,"SPAN",{});var rf=n(qe);ti=p(rf,"Local and remote files"),rf.forEach(t),Mn.forEach(t),Ql=f(s),D=o(s,"P",{});var O=n(D);ai=p(O,"\u{1F917} Datasets can be loaded from local files stored on your computer, and also from remote files. The datasets are most likely stored as a "),Pe=o(O,"CODE",{});var pf=n(Pe);ei=p(pf,"csv"),pf.forEach(t),li=p(O,", "),Ae=o(O,"CODE",{});var df=n(Ae);oi=p(df,"json"),df.forEach(t),ni=p(O,", "),Se=o(O,"CODE",{});var ff=n(Se);ri=p(ff,"txt"),ff.forEach(t),ii=p(O," or "),Te=o(O,"CODE",{});var cf=n(Te);pi=p(cf,"parquet"),cf.forEach(t),di=p(O," file. The "),$a=o(O,"A",{href:!0});var hf=n($a);fi=p(hf,"datasets.load_dataset()"),hf.forEach(t),ci=p(O," method is able to load each of these file types."),O.forEach(t),Xl=f(s),ts=o(s,"H3",{class:!0});var Vn=n(ts);Ss=o(Vn,"A",{id:!0,class:!0,href:!0});var uf=n(Ss);De=o(uf,"SPAN",{});var mf=n(De);m(mt.$$.fragment,mf),mf.forEach(t),uf.forEach(t),hi=f(Vn),Ie=o(Vn,"SPAN",{});var gf=n(Ie);ui=p(gf,"CSV"),gf.forEach(t),Vn.forEach(t),Kl=f(s),ya=o(s,"P",{});var _f=n(ya);mi=p(_f,"\u{1F917} Datasets can read a dataset made up of one or several CSV files:"),_f.forEach(t),Zl=f(s),m(gt.$$.fragment,s),so=f(s),wa=o(s,"P",{});var vf=n(wa);gi=p(vf,"If you have more than one CSV file:"),vf.forEach(t),to=f(s),m(_t.$$.fragment,s),ao=f(s),ja=o(s,"P",{});var $f=n(ja);_i=p($f,"You can also map the training and test splits to specific CSV files:"),$f.forEach(t),eo=f(s),m(vt.$$.fragment,s),lo=f(s),ba=o(s,"P",{});var yf=n(ba);vi=p(yf,"To load remote CSV files via HTTP, you can pass the URLs:"),yf.forEach(t),oo=f(s),m($t.$$.fragment,s),no=f(s),xa=o(s,"P",{});var wf=n(xa);$i=p(wf,"To load zipped CSV files:"),wf.forEach(t),ro=f(s),m(yt.$$.fragment,s),io=f(s),as=o(s,"H3",{class:!0});var zn=n(as);Ts=o(zn,"A",{id:!0,class:!0,href:!0});var jf=n(Ts);Ne=o(jf,"SPAN",{});var bf=n(Ne);m(wt.$$.fragment,bf),bf.forEach(t),jf.forEach(t),yi=f(zn),Ce=o(zn,"SPAN",{});var xf=n(Ce);wi=p(xf,"JSON"),xf.forEach(t),zn.forEach(t),po=f(s),Ds=o(s,"P",{});var Un=n(Ds);ji=p(Un,"JSON files are loaded directly with "),ka=o(Un,"A",{href:!0});var kf=n(ka);bi=p(kf,"datasets.load_dataset()"),kf.forEach(t),xi=p(Un," as shown below:"),Un.forEach(t),fo=f(s),m(jt.$$.fragment,s),co=f(s),Ea=o(s,"P",{});var Ef=n(Ea);ki=p(Ef,"JSON files can have diverse formats, but we think the most efficient format is to have multiple JSON objects; each line represents an individual row of data. For example:"),Ef.forEach(t),ho=f(s),m(bt.$$.fragment,s),uo=f(s),Is=o(s,"P",{});var Jn=n(Is);Ei=p(Jn,"Another JSON format you may encounter is a nested field, in which case you will need to specify the "),Oe=o(Jn,"CODE",{});var qf=n(Oe);qi=p(qf,"field"),qf.forEach(t),Pi=p(Jn," argument as shown in the following:"),Jn.forEach(t),mo=f(s),m(xt.$$.fragment,s),go=f(s),qa=o(s,"P",{});var Pf=n(qa);Ai=p(Pf,"To load remote JSON files via HTTP, you can pass the URLs:"),Pf.forEach(t),_o=f(s),m(kt.$$.fragment,s),vo=f(s),Pa=o(s,"P",{});var Af=n(Pa);Si=p(Af,"While these are the most common JSON formats, you will see other datasets that are formatted differently. \u{1F917} Datasets recognizes these other formats, and will fallback accordingly on the Python JSON loading methods to handle them."),Af.forEach(t),$o=f(s),es=o(s,"H3",{class:!0});var Yn=n(es);Ns=o(Yn,"A",{id:!0,class:!0,href:!0});var Sf=n(Ns);He=o(Sf,"SPAN",{});var Tf=n(He);m(Et.$$.fragment,Tf),Tf.forEach(t),Sf.forEach(t),Ti=f(Yn),Fe=o(Yn,"SPAN",{});var Df=n(Fe);Di=p(Df,"Text files"),Df.forEach(t),Yn.forEach(t),yo=f(s),Aa=o(s,"P",{});var If=n(Aa);Ii=p(If,"Text files are one of the most common file types for storing a dataset. \u{1F917} Datasets will read the text file line by line to build the dataset."),If.forEach(t),wo=f(s),m(qt.$$.fragment,s),jo=f(s),Sa=o(s,"P",{});var Nf=n(Sa);Ni=p(Nf,"To load remote TXT files via HTTP, you can pass the URLs:"),Nf.forEach(t),bo=f(s),m(Pt.$$.fragment,s),xo=f(s),ls=o(s,"H3",{class:!0});var Bn=n(ls);Cs=o(Bn,"A",{id:!0,class:!0,href:!0});var Cf=n(Cs);Re=o(Cf,"SPAN",{});var Of=n(Re);m(At.$$.fragment,Of),Of.forEach(t),Cf.forEach(t),Ci=f(Bn),Le=o(Bn,"SPAN",{});var Hf=n(Le);Oi=p(Hf,"Parquet"),Hf.forEach(t),Bn.forEach(t),ko=f(s),Ta=o(s,"P",{});var Ff=n(Ta);Hi=p(Ff,"Parquet files are stored in a columnar format unlike row-based files like CSV. Large datasets may be stored in a Parquet file because it is more efficient, and faster at returning your query. Load a Parquet file as shown in the following example:"),Ff.forEach(t),Eo=f(s),m(St.$$.fragment,s),qo=f(s),Da=o(s,"P",{});var Rf=n(Da);Fi=p(Rf,"To load remote parquet files via HTTP, you can pass the URLs:"),Rf.forEach(t),Po=f(s),m(Tt.$$.fragment,s),Ao=f(s),os=o(s,"H2",{class:!0});var Gn=n(os);Os=o(Gn,"A",{id:!0,class:!0,href:!0});var Lf=n(Os);Me=o(Lf,"SPAN",{});var Mf=n(Me);m(Dt.$$.fragment,Mf),Mf.forEach(t),Lf.forEach(t),Ri=f(Gn),Ve=o(Gn,"SPAN",{});var Vf=n(Ve);Li=p(Vf,"In-memory data"),Vf.forEach(t),Gn.forEach(t),So=f(s),Hs=o(s,"P",{});var Wn=n(Hs);Mi=p(Wn,"\u{1F917} Datasets will also allow you to create a "),Ia=o(Wn,"A",{href:!0});var zf=n(Ia);Vi=p(zf,"datasets.Dataset"),zf.forEach(t),zi=p(Wn," directly from in-memory data structures like Python dictionaries and Pandas DataFrames."),Wn.forEach(t),To=f(s),ns=o(s,"H3",{class:!0});var Qn=n(ns);Fs=o(Qn,"A",{id:!0,class:!0,href:!0});var Uf=n(Fs);ze=o(Uf,"SPAN",{});var Jf=n(ze);m(It.$$.fragment,Jf),Jf.forEach(t),Uf.forEach(t),Ui=f(Qn),Ue=o(Qn,"SPAN",{});var Yf=n(Ue);Ji=p(Yf,"Python dictionary"),Yf.forEach(t),Qn.forEach(t),Do=f(s),Rs=o(s,"P",{});var Xn=n(Rs);Yi=p(Xn,"Load Python dictionaries with "),Na=o(Xn,"A",{href:!0});var Bf=n(Na);Bi=p(Bf,"datasets.Dataset.from_dict()"),Bf.forEach(t),Gi=p(Xn,":"),Xn.forEach(t),Io=f(s),m(Nt.$$.fragment,s),No=f(s),rs=o(s,"H3",{class:!0});var Kn=n(rs);Ls=o(Kn,"A",{id:!0,class:!0,href:!0});var Gf=n(Ls);Je=o(Gf,"SPAN",{});var Wf=n(Je);m(Ct.$$.fragment,Wf),Wf.forEach(t),Gf.forEach(t),Wi=f(Kn),Ye=o(Kn,"SPAN",{});var Qf=n(Ye);Qi=p(Qf,"Pandas DataFrame"),Qf.forEach(t),Kn.forEach(t),Co=f(s),Ms=o(s,"P",{});var Zn=n(Ms);Xi=p(Zn,"Load Pandas DataFrames with "),Ca=o(Zn,"A",{href:!0});var Xf=n(Ca);Ki=p(Xf,"datasets.Dataset.from_pandas()"),Xf.forEach(t),Zi=p(Zn,":"),Zn.forEach(t),Oo=f(s),m(Ot.$$.fragment,s),Ho=f(s),m(Vs.$$.fragment,s),Fo=f(s),is=o(s,"H2",{class:!0});var sr=n(is);zs=o(sr,"A",{id:!0,class:!0,href:!0});var Kf=n(zs);Be=o(Kf,"SPAN",{});var Zf=n(Be);m(Ht.$$.fragment,Zf),Zf.forEach(t),Kf.forEach(t),sp=f(sr),Ge=o(sr,"SPAN",{});var sc=n(Ge);tp=p(sc,"Offline"),sc.forEach(t),sr.forEach(t),Ro=f(s),Oa=o(s,"P",{});var tc=n(Oa);ap=p(tc,"Even if you don\u2019t have an internet connection, it is still possible to load a dataset. As long as you\u2019ve downloaded a dataset from the Hub or \u{1F917} Datasets GitHub repository before, it should be cached. This means you can reload the dataset from the cache and use it offline."),tc.forEach(t),Lo=f(s),z=o(s,"P",{});var le=n(z);ep=p(le,"If you know you won\u2019t have internet access, you can run \u{1F917} Datasets in full offline mode. This saves time because instead of waiting for the Dataset builder download to time out, \u{1F917} Datasets will look directly in the cache. Set the environment variable "),We=o(le,"CODE",{});var ac=n(We);lp=p(ac,"HF_DATASETS_OFFLINE"),ac.forEach(t),op=p(le," to "),Qe=o(le,"CODE",{});var ec=n(Qe);np=p(ec,"1"),ec.forEach(t),rp=p(le," to enable full offline mode."),le.forEach(t),Mo=f(s),ps=o(s,"H2",{class:!0});var tr=n(ps);Us=o(tr,"A",{id:!0,class:!0,href:!0});var lc=n(Us);Xe=o(lc,"SPAN",{});var oc=n(Xe);m(Ft.$$.fragment,oc),oc.forEach(t),lc.forEach(t),ip=f(tr),Ke=o(tr,"SPAN",{});var nc=n(Ke);pp=p(nc,"Slice splits"),nc.forEach(t),tr.forEach(t),Vo=f(s),U=o(s,"P",{});var oe=n(U);dp=p(oe,"For even greater control over how to load a split, you can choose to only load specific slices of a split. There are two options for slicing a split: using strings or "),Ha=o(oe,"A",{href:!0});var rc=n(Ha);fp=p(rc,"datasets.ReadInstruction"),rc.forEach(t),cp=p(oe,". Strings are more compact and readable for simple cases, while "),Fa=o(oe,"A",{href:!0});var ic=n(Fa);hp=p(ic,"datasets.ReadInstruction"),ic.forEach(t),up=p(oe," is easier to use with variable slicing parameters."),oe.forEach(t),zo=f(s),J=o(s,"P",{});var ne=n(J);mp=p(ne,"Concatenate the "),Ze=o(ne,"CODE",{});var pc=n(Ze);gp=p(pc,"train"),pc.forEach(t),_p=p(ne," and "),sl=o(ne,"CODE",{});var dc=n(sl);vp=p(dc,"test"),dc.forEach(t),$p=p(ne," split by:"),ne.forEach(t),Uo=f(s),m(Rt.$$.fragment,s),Jo=f(s),Js=o(s,"P",{});var ar=n(Js);yp=p(ar,"Select specific rows of the "),tl=o(ar,"CODE",{});var fc=n(tl);wp=p(fc,"train"),fc.forEach(t),jp=p(ar," split:"),ar.forEach(t),Yo=f(s),m(Lt.$$.fragment,s),Bo=f(s),Ra=o(s,"P",{});var cc=n(Ra);bp=p(cc,"Or select a percentage of the split with:"),cc.forEach(t),Go=f(s),m(Mt.$$.fragment,s),Wo=f(s),La=o(s,"P",{});var hc=n(La);xp=p(hc,"You can even select a combination of percentages from each split:"),hc.forEach(t),Qo=f(s),m(Vt.$$.fragment,s),Xo=f(s),Ma=o(s,"P",{});var uc=n(Ma);kp=p(uc,"Finally, create cross-validated dataset splits by:"),uc.forEach(t),Ko=f(s),m(zt.$$.fragment,s),Zo=f(s),ds=o(s,"H3",{class:!0});var er=n(ds);Ys=o(er,"A",{id:!0,class:!0,href:!0});var mc=n(Ys);al=o(mc,"SPAN",{});var gc=n(al);m(Ut.$$.fragment,gc),gc.forEach(t),mc.forEach(t),Ep=f(er),el=o(er,"SPAN",{});var _c=n(el);qp=p(_c,"Percent slicing and rounding"),_c.forEach(t),er.forEach(t),sn=f(s),Va=o(s,"P",{});var vc=n(Va);Pp=p(vc,"For datasets where the requested slice boundaries do not divide evenly by 100, the default behavior is to round the boundaries to the nearest integer. As a result, some slices may contain more examples than others as shown in the following example:"),vc.forEach(t),tn=f(s),m(Jt.$$.fragment,s),an=f(s),Bs=o(s,"P",{});var lr=n(Bs);Ap=p(lr,"If you want equal sized splits, use "),ll=o(lr,"CODE",{});var $c=n(ll);Sp=p($c,"pct1_dropremainder"),$c.forEach(t),Tp=p(lr," rounding instead. This will treat the specified percentage boundaries as multiples of 1%."),lr.forEach(t),en=f(s),m(Yt.$$.fragment,s),ln=f(s),m(Gs.$$.fragment,s),on=f(s),za=o(s,"A",{id:!0}),n(za).forEach(t),nn=f(s),fs=o(s,"H2",{class:!0});var or=n(fs);Ws=o(or,"A",{id:!0,class:!0,href:!0});var yc=n(Ws);ol=o(yc,"SPAN",{});var wc=n(ol);m(Bt.$$.fragment,wc),wc.forEach(t),yc.forEach(t),Dp=f(or),nl=o(or,"SPAN",{});var jc=n(nl);Ip=p(jc,"Troubleshooting"),jc.forEach(t),or.forEach(t),rn=f(s),Ua=o(s,"P",{});var bc=n(Ua);Np=p(bc,"Sometimes, you may get unexpected results when you load a dataset. In this section, you will learn how to solve two common issues you may encounter when you load a dataset: manually download a dataset, and specify features of a dataset."),bc.forEach(t),pn=f(s),cs=o(s,"H3",{class:!0});var nr=n(cs);Qs=o(nr,"A",{id:!0,class:!0,href:!0});var xc=n(Qs);rl=o(xc,"SPAN",{});var kc=n(rl);m(Gt.$$.fragment,kc),kc.forEach(t),xc.forEach(t),Cp=f(nr),il=o(nr,"SPAN",{});var Ec=n(il);Op=p(Ec,"Manual download"),Ec.forEach(t),nr.forEach(t),dn=f(s),N=o(s,"P",{});var ot=n(N);Hp=p(ot,"Certain datasets require you to manually download the dataset files due to licensing incompatibility, or if the files are hidden behind a login page. This will cause "),Ja=o(ot,"A",{href:!0});var qc=n(Ja);Fp=p(qc,"datasets.load_dataset()"),qc.forEach(t),Rp=p(ot," to throw an "),pl=o(ot,"CODE",{});var Pc=n(pl);Lp=p(Pc,"AssertionError"),Pc.forEach(t),Mp=p(ot,". But \u{1F917} Datasets provides detailed instructions for downloading the missing files. After you have downloaded the files, use the "),dl=o(ot,"CODE",{});var Ac=n(dl);Vp=p(Ac,"data_dir"),Ac.forEach(t),zp=p(ot," argument to specify the path to the files you just downloaded."),ot.forEach(t),fn=f(s),Xs=o(s,"P",{});var rr=n(Xs);Up=p(rr,"For example, if you try to download a configuration from the "),Wt=o(rr,"A",{href:!0,rel:!0});var Sc=n(Wt);Jp=p(Sc,"MATINF"),Sc.forEach(t),Yp=p(rr," dataset:"),rr.forEach(t),cn=f(s),m(Qt.$$.fragment,s),hn=f(s),hs=o(s,"H3",{class:!0});var ir=n(hs);Ks=o(ir,"A",{id:!0,class:!0,href:!0});var Tc=n(Ks);fl=o(Tc,"SPAN",{});var Dc=n(fl);m(Xt.$$.fragment,Dc),Dc.forEach(t),Tc.forEach(t),Bp=f(ir),cl=o(ir,"SPAN",{});var Ic=n(cl);Gp=p(Ic,"Specify features"),Ic.forEach(t),ir.forEach(t),un=f(s),Y=o(s,"P",{});var re=n(Y);Wp=p(re,"When you create a dataset from local files, the "),Ya=o(re,"A",{href:!0});var Nc=n(Ya);Qp=p(Nc,"datasets.Features"),Nc.forEach(t),Xp=p(re," are automatically inferred by "),Kt=o(re,"A",{href:!0,rel:!0});var Cc=n(Kt);Kp=p(Cc,"Apache Arrow"),Cc.forEach(t),Zp=p(re,". However, the features of the dataset may not always align with your expectations or you may want to define the features yourself."),re.forEach(t),mn=f(s),B=o(s,"P",{});var ie=n(B);sd=p(ie,"The following example shows how you can add custom labels with "),Ba=o(ie,"A",{href:!0});var Oc=n(Ba);td=p(Oc,"datasets.ClassLabel"),Oc.forEach(t),ad=p(ie,". First, define your own labels using the "),Ga=o(ie,"A",{href:!0});var Hc=n(Ga);ed=p(Hc,"datasets.Features"),Hc.forEach(t),ld=p(ie," class:"),ie.forEach(t),gn=f(s),m(Zt.$$.fragment,s),_n=f(s),G=o(s,"P",{});var pe=n(G);od=p(pe,"Next, specify the "),hl=o(pe,"CODE",{});var Fc=n(hl);nd=p(Fc,"features"),Fc.forEach(t),rd=p(pe," argument in "),Wa=o(pe,"A",{href:!0});var Rc=n(Wa);id=p(Rc,"datasets.load_dataset()"),Rc.forEach(t),pd=p(pe," with the features you just created:"),pe.forEach(t),vn=f(s),m(sa.$$.fragment,s),$n=f(s),Qa=o(s,"P",{});var Lc=n(Qa);dd=p(Lc,"Now when you look at your dataset features, you can see it uses the custom labels you defined:"),Lc.forEach(t),yn=f(s),m(ta.$$.fragment,s),wn=f(s),us=o(s,"H2",{class:!0});var pr=n(us);Zs=o(pr,"A",{id:!0,class:!0,href:!0});var Mc=n(Zs);ul=o(Mc,"SPAN",{});var Vc=n(ul);m(aa.$$.fragment,Vc),Vc.forEach(t),Mc.forEach(t),fd=f(pr),ml=o(pr,"SPAN",{});var zc=n(ml);cd=p(zc,"Metrics"),zc.forEach(t),pr.forEach(t),jn=f(s),Xa=o(s,"P",{});var Uc=n(Xa);hd=p(Uc,"When the metric you want to use is not supported by \u{1F917} Datasets, you can write and use your own metric script. Load your metric by providing the path to your local metric loading script:"),Uc.forEach(t),bn=f(s),m(ea.$$.fragment,s),xn=f(s),m(st.$$.fragment,s),kn=f(s),ms=o(s,"H3",{class:!0});var dr=n(ms);tt=o(dr,"A",{id:!0,class:!0,href:!0});var Jc=n(tt);gl=o(Jc,"SPAN",{});var Yc=n(gl);m(la.$$.fragment,Yc),Yc.forEach(t),Jc.forEach(t),ud=f(dr),_l=o(dr,"SPAN",{});var Bc=n(_l);md=p(Bc,"Load configurations"),Bc.forEach(t),dr.forEach(t),En=f(s),Ka=o(s,"P",{});var Gc=n(Ka);gd=p(Gc,"It is possible for a metric to have different configurations. The configurations are stored in the [\u2018datasets.Metric.config_name\u2019] attribute. When you load a metric, provide the configuration name as shown in the following:"),Gc.forEach(t),qn=f(s),m(oa.$$.fragment,s),Pn=f(s),gs=o(s,"H3",{class:!0});var fr=n(gs);at=o(fr,"A",{id:!0,class:!0,href:!0});var Wc=n(at);vl=o(Wc,"SPAN",{});var Qc=n(vl);m(na.$$.fragment,Qc),Qc.forEach(t),Wc.forEach(t),_d=f(fr),$l=o(fr,"SPAN",{});var Xc=n($l);vd=p(Xc,"Distributed setup"),Xc.forEach(t),fr.forEach(t),An=f(s),Za=o(s,"P",{});var Kc=n(Za);$d=p(Kc,"When you work in a distributed or parallel processing environment, loading and computing a metric can be tricky because these processes are executed in parallel on separate subsets of the data. \u{1F917} Datasets supports distributed usage with a few additional arguments when you load a metric."),Kc.forEach(t),Sn=f(s),se=o(s,"P",{});var Zc=n(se);yd=p(Zc,"For example, imagine you are training and evaluating on eight parallel processes. Here\u2019s how you would load a metric in this distributed setting:"),Zc.forEach(t),Tn=f(s),W=o(s,"OL",{});var de=n(W);yl=o(de,"LI",{});var sh=n(yl);ra=o(sh,"P",{});var cr=n(ra);wd=p(cr,"Define the total number of processes with the "),wl=o(cr,"CODE",{});var th=n(wl);jd=p(th,"num_process"),th.forEach(t),bd=p(cr," argument."),cr.forEach(t),sh.forEach(t),xd=f(de),jl=o(de,"LI",{});var ah=n(jl);_s=o(ah,"P",{});var fe=n(_s);kd=p(fe,"Set the process "),bl=o(fe,"CODE",{});var eh=n(bl);Ed=p(eh,"rank"),eh.forEach(t),qd=p(fe," as an integer between zero and "),xl=o(fe,"CODE",{});var lh=n(xl);Pd=p(lh,"num_process - 1"),lh.forEach(t),Ad=p(fe,"."),fe.forEach(t),ah.forEach(t),Sd=f(de),kl=o(de,"LI",{});var oh=n(kl);ia=o(oh,"P",{});var hr=n(ia);Td=p(hr,"Load your metric with "),te=o(hr,"A",{href:!0});var nh=n(te);Dd=p(nh,"datasets.load_metric()"),nh.forEach(t),Id=p(hr," with these arguments:"),hr.forEach(t),oh.forEach(t),de.forEach(t),Dn=f(s),m(pa.$$.fragment,s),In=f(s),m(et.$$.fragment,s),Nn=f(s),lt=o(s,"P",{});var ur=n(lt);Nd=p(ur,"In some instances, you may be simultaneously running multiple independent distributed evaluations on the same server and files. To avoid any conflicts, it is important to provide an "),El=o(ur,"CODE",{});var rh=n(El);Cd=p(rh,"experiment_id"),rh.forEach(t),Od=p(ur," to distinguish the separate evaluations:"),ur.forEach(t),Cn=f(s),m(da.$$.fragment,s),this.h()},h(){c(h,"name","hf:doc:metadata"),c(h,"content",JSON.stringify($h)),c(b,"id","load"),c(b,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(b,"href","#load"),c(y,"class","relative group"),c(ga,"id","load-from-the-hub"),c(bs,"id","hugging-face-hub"),c(bs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(bs,"href","#hugging-face-hub"),c(Z,"class","relative group"),c(_a,"href","/docs/datasets/pr_/en/package_reference/loading_methods#datasets.load_dataset"),c(rt,"href","https://huggingface.co/datasets/lhoestq/demo1"),c(rt,"rel","nofollow"),c(ft,"href","https://huggingface.co/datasets/allenai/c4"),c(ft,"rel","nofollow"),c(As,"id","local-and-remote-files"),c(As,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(As,"href","#local-and-remote-files"),c(ss,"class","relative group"),c($a,"href","/docs/datasets/pr_/en/package_reference/loading_methods#datasets.load_dataset"),c(Ss,"id","csv"),c(Ss,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Ss,"href","#csv"),c(ts,"class","relative group"),c(Ts,"id","json"),c(Ts,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Ts,"href","#json"),c(as,"class","relative group"),c(ka,"href","/docs/datasets/pr_/en/package_reference/loading_methods#datasets.load_dataset"),c(Ns,"id","text-files"),c(Ns,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Ns,"href","#text-files"),c(es,"class","relative group"),c(Cs,"id","parquet"),c(Cs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Cs,"href","#parquet"),c(ls,"class","relative group"),c(Os,"id","inmemory-data"),c(Os,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Os,"href","#inmemory-data"),c(os,"class","relative group"),c(Ia,"href","/docs/datasets/pr_/en/package_reference/main_classes#datasets.Dataset"),c(Fs,"id","python-dictionary"),c(Fs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Fs,"href","#python-dictionary"),c(ns,"class","relative group"),c(Na,"href","/docs/datasets/pr_/en/package_reference/main_classes#datasets.Dataset.from_dict"),c(Ls,"id","pandas-dataframe"),c(Ls,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Ls,"href","#pandas-dataframe"),c(rs,"class","relative group"),c(Ca,"href","/docs/datasets/pr_/en/package_reference/main_classes#datasets.Dataset.from_pandas"),c(zs,"id","offline"),c(zs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(zs,"href","#offline"),c(is,"class","relative group"),c(Us,"id","slice-splits"),c(Us,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Us,"href","#slice-splits"),c(ps,"class","relative group"),c(Ha,"href","/docs/datasets/pr_/en/package_reference/builder_classes#datasets.ReadInstruction"),c(Fa,"href","/docs/datasets/pr_/en/package_reference/builder_classes#datasets.ReadInstruction"),c(Ys,"id","percent-slicing-and-rounding"),c(Ys,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Ys,"href","#percent-slicing-and-rounding"),c(ds,"class","relative group"),c(za,"id","troubleshoot"),c(Ws,"id","troubleshooting"),c(Ws,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Ws,"href","#troubleshooting"),c(fs,"class","relative group"),c(Qs,"id","manual-download"),c(Qs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Qs,"href","#manual-download"),c(cs,"class","relative group"),c(Ja,"href","/docs/datasets/pr_/en/package_reference/loading_methods#datasets.load_dataset"),c(Wt,"href","https://huggingface.co/datasets/matinf"),c(Wt,"rel","nofollow"),c(Ks,"id","specify-features"),c(Ks,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Ks,"href","#specify-features"),c(hs,"class","relative group"),c(Ya,"href","/docs/datasets/pr_/en/package_reference/main_classes#datasets.Features"),c(Kt,"href","https://arrow.apache.org/docs/"),c(Kt,"rel","nofollow"),c(Ba,"href","/docs/datasets/pr_/en/package_reference/main_classes#datasets.ClassLabel"),c(Ga,"href","/docs/datasets/pr_/en/package_reference/main_classes#datasets.Features"),c(Wa,"href","/docs/datasets/pr_/en/package_reference/loading_methods#datasets.load_dataset"),c(Zs,"id","metrics"),c(Zs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Zs,"href","#metrics"),c(us,"class","relative group"),c(tt,"id","load-configurations"),c(tt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(tt,"href","#load-configurations"),c(ms,"class","relative group"),c(at,"id","distributed-setup"),c(at,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(at,"href","#distributed-setup"),c(gs,"class","relative group"),c(te,"href","/docs/datasets/pr_/en/package_reference/loading_methods#datasets.load_metric")},m(s,e){a(document.head,h),r(s,k,e),r(s,y,e),a(y,b),a(b,E),g(w,E,null),a(y,j),a(y,q),a(q,X),r(s,vs,e),r(s,F,e),a(F,K),r(s,$s,e),r(s,R,e),a(R,L),r(s,ys,e),r(s,A,e),a(A,H),a(H,S),a(A,ca),a(A,ws),a(ws,ha),a(A,ua),a(A,js),a(js,mr),a(A,gr),a(A,he),a(he,_r),a(A,vr),a(A,ue),a(ue,$r),r(s,Dl,e),r(s,ma,e),a(ma,yr),r(s,Il,e),r(s,ga,e),r(s,Nl,e),r(s,Z,e),a(Z,bs),a(bs,me),g(nt,me,null),a(Z,wr),a(Z,ge),a(ge,jr),r(s,Cl,e),r(s,xs,e),a(xs,br),a(xs,_e),a(_e,xr),a(xs,kr),r(s,Ol,e),r(s,M,e),a(M,Er),a(M,_a),a(_a,qr),a(M,Pr),a(M,rt),a(rt,Ar),a(M,Sr),r(s,Hl,e),g(it,s,e),r(s,Fl,e),r(s,va,e),a(va,Tr),r(s,Rl,e),r(s,ks,e),a(ks,Dr),a(ks,ve),a(ve,Ir),a(ks,Nr),r(s,Ll,e),g(pt,s,e),r(s,Ml,e),g(Es,s,e),r(s,Vl,e),r(s,T,e),a(T,Cr),a(T,$e),a($e,Or),a(T,Hr),a(T,ye),a(ye,Fr),a(T,Rr),a(T,we),a(we,Lr),a(T,Mr),a(T,je),a(je,Vr),a(T,zr),a(T,be),a(be,Ur),a(T,Jr),r(s,zl,e),g(dt,s,e),r(s,Ul,e),g(qs,s,e),r(s,Jl,e),r(s,V,e),a(V,Yr),a(V,xe),a(xe,Br),a(V,Gr),a(V,ft),a(ft,Wr),a(V,Qr),r(s,Yl,e),g(ct,s,e),r(s,Bl,e),r(s,Ps,e),a(Ps,Xr),a(Ps,ke),a(ke,Kr),a(Ps,Zr),r(s,Gl,e),g(ht,s,e),r(s,Wl,e),r(s,ss,e),a(ss,As),a(As,Ee),g(ut,Ee,null),a(ss,si),a(ss,qe),a(qe,ti),r(s,Ql,e),r(s,D,e),a(D,ai),a(D,Pe),a(Pe,ei),a(D,li),a(D,Ae),a(Ae,oi),a(D,ni),a(D,Se),a(Se,ri),a(D,ii),a(D,Te),a(Te,pi),a(D,di),a(D,$a),a($a,fi),a(D,ci),r(s,Xl,e),r(s,ts,e),a(ts,Ss),a(Ss,De),g(mt,De,null),a(ts,hi),a(ts,Ie),a(Ie,ui),r(s,Kl,e),r(s,ya,e),a(ya,mi),r(s,Zl,e),g(gt,s,e),r(s,so,e),r(s,wa,e),a(wa,gi),r(s,to,e),g(_t,s,e),r(s,ao,e),r(s,ja,e),a(ja,_i),r(s,eo,e),g(vt,s,e),r(s,lo,e),r(s,ba,e),a(ba,vi),r(s,oo,e),g($t,s,e),r(s,no,e),r(s,xa,e),a(xa,$i),r(s,ro,e),g(yt,s,e),r(s,io,e),r(s,as,e),a(as,Ts),a(Ts,Ne),g(wt,Ne,null),a(as,yi),a(as,Ce),a(Ce,wi),r(s,po,e),r(s,Ds,e),a(Ds,ji),a(Ds,ka),a(ka,bi),a(Ds,xi),r(s,fo,e),g(jt,s,e),r(s,co,e),r(s,Ea,e),a(Ea,ki),r(s,ho,e),g(bt,s,e),r(s,uo,e),r(s,Is,e),a(Is,Ei),a(Is,Oe),a(Oe,qi),a(Is,Pi),r(s,mo,e),g(xt,s,e),r(s,go,e),r(s,qa,e),a(qa,Ai),r(s,_o,e),g(kt,s,e),r(s,vo,e),r(s,Pa,e),a(Pa,Si),r(s,$o,e),r(s,es,e),a(es,Ns),a(Ns,He),g(Et,He,null),a(es,Ti),a(es,Fe),a(Fe,Di),r(s,yo,e),r(s,Aa,e),a(Aa,Ii),r(s,wo,e),g(qt,s,e),r(s,jo,e),r(s,Sa,e),a(Sa,Ni),r(s,bo,e),g(Pt,s,e),r(s,xo,e),r(s,ls,e),a(ls,Cs),a(Cs,Re),g(At,Re,null),a(ls,Ci),a(ls,Le),a(Le,Oi),r(s,ko,e),r(s,Ta,e),a(Ta,Hi),r(s,Eo,e),g(St,s,e),r(s,qo,e),r(s,Da,e),a(Da,Fi),r(s,Po,e),g(Tt,s,e),r(s,Ao,e),r(s,os,e),a(os,Os),a(Os,Me),g(Dt,Me,null),a(os,Ri),a(os,Ve),a(Ve,Li),r(s,So,e),r(s,Hs,e),a(Hs,Mi),a(Hs,Ia),a(Ia,Vi),a(Hs,zi),r(s,To,e),r(s,ns,e),a(ns,Fs),a(Fs,ze),g(It,ze,null),a(ns,Ui),a(ns,Ue),a(Ue,Ji),r(s,Do,e),r(s,Rs,e),a(Rs,Yi),a(Rs,Na),a(Na,Bi),a(Rs,Gi),r(s,Io,e),g(Nt,s,e),r(s,No,e),r(s,rs,e),a(rs,Ls),a(Ls,Je),g(Ct,Je,null),a(rs,Wi),a(rs,Ye),a(Ye,Qi),r(s,Co,e),r(s,Ms,e),a(Ms,Xi),a(Ms,Ca),a(Ca,Ki),a(Ms,Zi),r(s,Oo,e),g(Ot,s,e),r(s,Ho,e),g(Vs,s,e),r(s,Fo,e),r(s,is,e),a(is,zs),a(zs,Be),g(Ht,Be,null),a(is,sp),a(is,Ge),a(Ge,tp),r(s,Ro,e),r(s,Oa,e),a(Oa,ap),r(s,Lo,e),r(s,z,e),a(z,ep),a(z,We),a(We,lp),a(z,op),a(z,Qe),a(Qe,np),a(z,rp),r(s,Mo,e),r(s,ps,e),a(ps,Us),a(Us,Xe),g(Ft,Xe,null),a(ps,ip),a(ps,Ke),a(Ke,pp),r(s,Vo,e),r(s,U,e),a(U,dp),a(U,Ha),a(Ha,fp),a(U,cp),a(U,Fa),a(Fa,hp),a(U,up),r(s,zo,e),r(s,J,e),a(J,mp),a(J,Ze),a(Ze,gp),a(J,_p),a(J,sl),a(sl,vp),a(J,$p),r(s,Uo,e),g(Rt,s,e),r(s,Jo,e),r(s,Js,e),a(Js,yp),a(Js,tl),a(tl,wp),a(Js,jp),r(s,Yo,e),g(Lt,s,e),r(s,Bo,e),r(s,Ra,e),a(Ra,bp),r(s,Go,e),g(Mt,s,e),r(s,Wo,e),r(s,La,e),a(La,xp),r(s,Qo,e),g(Vt,s,e),r(s,Xo,e),r(s,Ma,e),a(Ma,kp),r(s,Ko,e),g(zt,s,e),r(s,Zo,e),r(s,ds,e),a(ds,Ys),a(Ys,al),g(Ut,al,null),a(ds,Ep),a(ds,el),a(el,qp),r(s,sn,e),r(s,Va,e),a(Va,Pp),r(s,tn,e),g(Jt,s,e),r(s,an,e),r(s,Bs,e),a(Bs,Ap),a(Bs,ll),a(ll,Sp),a(Bs,Tp),r(s,en,e),g(Yt,s,e),r(s,ln,e),g(Gs,s,e),r(s,on,e),r(s,za,e),r(s,nn,e),r(s,fs,e),a(fs,Ws),a(Ws,ol),g(Bt,ol,null),a(fs,Dp),a(fs,nl),a(nl,Ip),r(s,rn,e),r(s,Ua,e),a(Ua,Np),r(s,pn,e),r(s,cs,e),a(cs,Qs),a(Qs,rl),g(Gt,rl,null),a(cs,Cp),a(cs,il),a(il,Op),r(s,dn,e),r(s,N,e),a(N,Hp),a(N,Ja),a(Ja,Fp),a(N,Rp),a(N,pl),a(pl,Lp),a(N,Mp),a(N,dl),a(dl,Vp),a(N,zp),r(s,fn,e),r(s,Xs,e),a(Xs,Up),a(Xs,Wt),a(Wt,Jp),a(Xs,Yp),r(s,cn,e),g(Qt,s,e),r(s,hn,e),r(s,hs,e),a(hs,Ks),a(Ks,fl),g(Xt,fl,null),a(hs,Bp),a(hs,cl),a(cl,Gp),r(s,un,e),r(s,Y,e),a(Y,Wp),a(Y,Ya),a(Ya,Qp),a(Y,Xp),a(Y,Kt),a(Kt,Kp),a(Y,Zp),r(s,mn,e),r(s,B,e),a(B,sd),a(B,Ba),a(Ba,td),a(B,ad),a(B,Ga),a(Ga,ed),a(B,ld),r(s,gn,e),g(Zt,s,e),r(s,_n,e),r(s,G,e),a(G,od),a(G,hl),a(hl,nd),a(G,rd),a(G,Wa),a(Wa,id),a(G,pd),r(s,vn,e),g(sa,s,e),r(s,$n,e),r(s,Qa,e),a(Qa,dd),r(s,yn,e),g(ta,s,e),r(s,wn,e),r(s,us,e),a(us,Zs),a(Zs,ul),g(aa,ul,null),a(us,fd),a(us,ml),a(ml,cd),r(s,jn,e),r(s,Xa,e),a(Xa,hd),r(s,bn,e),g(ea,s,e),r(s,xn,e),g(st,s,e),r(s,kn,e),r(s,ms,e),a(ms,tt),a(tt,gl),g(la,gl,null),a(ms,ud),a(ms,_l),a(_l,md),r(s,En,e),r(s,Ka,e),a(Ka,gd),r(s,qn,e),g(oa,s,e),r(s,Pn,e),r(s,gs,e),a(gs,at),a(at,vl),g(na,vl,null),a(gs,_d),a(gs,$l),a($l,vd),r(s,An,e),r(s,Za,e),a(Za,$d),r(s,Sn,e),r(s,se,e),a(se,yd),r(s,Tn,e),r(s,W,e),a(W,yl),a(yl,ra),a(ra,wd),a(ra,wl),a(wl,jd),a(ra,bd),a(W,xd),a(W,jl),a(jl,_s),a(_s,kd),a(_s,bl),a(bl,Ed),a(_s,qd),a(_s,xl),a(xl,Pd),a(_s,Ad),a(W,Sd),a(W,kl),a(kl,ia),a(ia,Td),a(ia,te),a(te,Dd),a(ia,Id),r(s,Dn,e),g(pa,s,e),r(s,In,e),g(et,s,e),r(s,Nn,e),r(s,lt,e),a(lt,Nd),a(lt,El),a(El,Cd),a(lt,Od),r(s,Cn,e),g(da,s,e),On=!0},p(s,[e]){const fa={};e&2&&(fa.$$scope={dirty:e,ctx:s}),Es.$set(fa);const ql={};e&2&&(ql.$$scope={dirty:e,ctx:s}),qs.$set(ql);const Pl={};e&2&&(Pl.$$scope={dirty:e,ctx:s}),Vs.$set(Pl);const Al={};e&2&&(Al.$$scope={dirty:e,ctx:s}),Gs.$set(Al);const Sl={};e&2&&(Sl.$$scope={dirty:e,ctx:s}),st.$set(Sl);const Tl={};e&2&&(Tl.$$scope={dirty:e,ctx:s}),et.$set(Tl)},i(s){On||(_(w.$$.fragment,s),_(nt.$$.fragment,s),_(it.$$.fragment,s),_(pt.$$.fragment,s),_(Es.$$.fragment,s),_(dt.$$.fragment,s),_(qs.$$.fragment,s),_(ct.$$.fragment,s),_(ht.$$.fragment,s),_(ut.$$.fragment,s),_(mt.$$.fragment,s),_(gt.$$.fragment,s),_(_t.$$.fragment,s),_(vt.$$.fragment,s),_($t.$$.fragment,s),_(yt.$$.fragment,s),_(wt.$$.fragment,s),_(jt.$$.fragment,s),_(bt.$$.fragment,s),_(xt.$$.fragment,s),_(kt.$$.fragment,s),_(Et.$$.fragment,s),_(qt.$$.fragment,s),_(Pt.$$.fragment,s),_(At.$$.fragment,s),_(St.$$.fragment,s),_(Tt.$$.fragment,s),_(Dt.$$.fragment,s),_(It.$$.fragment,s),_(Nt.$$.fragment,s),_(Ct.$$.fragment,s),_(Ot.$$.fragment,s),_(Vs.$$.fragment,s),_(Ht.$$.fragment,s),_(Ft.$$.fragment,s),_(Rt.$$.fragment,s),_(Lt.$$.fragment,s),_(Mt.$$.fragment,s),_(Vt.$$.fragment,s),_(zt.$$.fragment,s),_(Ut.$$.fragment,s),_(Jt.$$.fragment,s),_(Yt.$$.fragment,s),_(Gs.$$.fragment,s),_(Bt.$$.fragment,s),_(Gt.$$.fragment,s),_(Qt.$$.fragment,s),_(Xt.$$.fragment,s),_(Zt.$$.fragment,s),_(sa.$$.fragment,s),_(ta.$$.fragment,s),_(aa.$$.fragment,s),_(ea.$$.fragment,s),_(st.$$.fragment,s),_(la.$$.fragment,s),_(oa.$$.fragment,s),_(na.$$.fragment,s),_(pa.$$.fragment,s),_(et.$$.fragment,s),_(da.$$.fragment,s),On=!0)},o(s){v(w.$$.fragment,s),v(nt.$$.fragment,s),v(it.$$.fragment,s),v(pt.$$.fragment,s),v(Es.$$.fragment,s),v(dt.$$.fragment,s),v(qs.$$.fragment,s),v(ct.$$.fragment,s),v(ht.$$.fragment,s),v(ut.$$.fragment,s),v(mt.$$.fragment,s),v(gt.$$.fragment,s),v(_t.$$.fragment,s),v(vt.$$.fragment,s),v($t.$$.fragment,s),v(yt.$$.fragment,s),v(wt.$$.fragment,s),v(jt.$$.fragment,s),v(bt.$$.fragment,s),v(xt.$$.fragment,s),v(kt.$$.fragment,s),v(Et.$$.fragment,s),v(qt.$$.fragment,s),v(Pt.$$.fragment,s),v(At.$$.fragment,s),v(St.$$.fragment,s),v(Tt.$$.fragment,s),v(Dt.$$.fragment,s),v(It.$$.fragment,s),v(Nt.$$.fragment,s),v(Ct.$$.fragment,s),v(Ot.$$.fragment,s),v(Vs.$$.fragment,s),v(Ht.$$.fragment,s),v(Ft.$$.fragment,s),v(Rt.$$.fragment,s),v(Lt.$$.fragment,s),v(Mt.$$.fragment,s),v(Vt.$$.fragment,s),v(zt.$$.fragment,s),v(Ut.$$.fragment,s),v(Jt.$$.fragment,s),v(Yt.$$.fragment,s),v(Gs.$$.fragment,s),v(Bt.$$.fragment,s),v(Gt.$$.fragment,s),v(Qt.$$.fragment,s),v(Xt.$$.fragment,s),v(Zt.$$.fragment,s),v(sa.$$.fragment,s),v(ta.$$.fragment,s),v(aa.$$.fragment,s),v(ea.$$.fragment,s),v(st.$$.fragment,s),v(la.$$.fragment,s),v(oa.$$.fragment,s),v(na.$$.fragment,s),v(pa.$$.fragment,s),v(et.$$.fragment,s),v(da.$$.fragment,s),On=!1},d(s){t(h),s&&t(k),s&&t(y),$(w),s&&t(vs),s&&t(F),s&&t($s),s&&t(R),s&&t(ys),s&&t(A),s&&t(Dl),s&&t(ma),s&&t(Il),s&&t(ga),s&&t(Nl),s&&t(Z),$(nt),s&&t(Cl),s&&t(xs),s&&t(Ol),s&&t(M),s&&t(Hl),$(it,s),s&&t(Fl),s&&t(va),s&&t(Rl),s&&t(ks),s&&t(Ll),$(pt,s),s&&t(Ml),$(Es,s),s&&t(Vl),s&&t(T),s&&t(zl),$(dt,s),s&&t(Ul),$(qs,s),s&&t(Jl),s&&t(V),s&&t(Yl),$(ct,s),s&&t(Bl),s&&t(Ps),s&&t(Gl),$(ht,s),s&&t(Wl),s&&t(ss),$(ut),s&&t(Ql),s&&t(D),s&&t(Xl),s&&t(ts),$(mt),s&&t(Kl),s&&t(ya),s&&t(Zl),$(gt,s),s&&t(so),s&&t(wa),s&&t(to),$(_t,s),s&&t(ao),s&&t(ja),s&&t(eo),$(vt,s),s&&t(lo),s&&t(ba),s&&t(oo),$($t,s),s&&t(no),s&&t(xa),s&&t(ro),$(yt,s),s&&t(io),s&&t(as),$(wt),s&&t(po),s&&t(Ds),s&&t(fo),$(jt,s),s&&t(co),s&&t(Ea),s&&t(ho),$(bt,s),s&&t(uo),s&&t(Is),s&&t(mo),$(xt,s),s&&t(go),s&&t(qa),s&&t(_o),$(kt,s),s&&t(vo),s&&t(Pa),s&&t($o),s&&t(es),$(Et),s&&t(yo),s&&t(Aa),s&&t(wo),$(qt,s),s&&t(jo),s&&t(Sa),s&&t(bo),$(Pt,s),s&&t(xo),s&&t(ls),$(At),s&&t(ko),s&&t(Ta),s&&t(Eo),$(St,s),s&&t(qo),s&&t(Da),s&&t(Po),$(Tt,s),s&&t(Ao),s&&t(os),$(Dt),s&&t(So),s&&t(Hs),s&&t(To),s&&t(ns),$(It),s&&t(Do),s&&t(Rs),s&&t(Io),$(Nt,s),s&&t(No),s&&t(rs),$(Ct),s&&t(Co),s&&t(Ms),s&&t(Oo),$(Ot,s),s&&t(Ho),$(Vs,s),s&&t(Fo),s&&t(is),$(Ht),s&&t(Ro),s&&t(Oa),s&&t(Lo),s&&t(z),s&&t(Mo),s&&t(ps),$(Ft),s&&t(Vo),s&&t(U),s&&t(zo),s&&t(J),s&&t(Uo),$(Rt,s),s&&t(Jo),s&&t(Js),s&&t(Yo),$(Lt,s),s&&t(Bo),s&&t(Ra),s&&t(Go),$(Mt,s),s&&t(Wo),s&&t(La),s&&t(Qo),$(Vt,s),s&&t(Xo),s&&t(Ma),s&&t(Ko),$(zt,s),s&&t(Zo),s&&t(ds),$(Ut),s&&t(sn),s&&t(Va),s&&t(tn),$(Jt,s),s&&t(an),s&&t(Bs),s&&t(en),$(Yt,s),s&&t(ln),$(Gs,s),s&&t(on),s&&t(za),s&&t(nn),s&&t(fs),$(Bt),s&&t(rn),s&&t(Ua),s&&t(pn),s&&t(cs),$(Gt),s&&t(dn),s&&t(N),s&&t(fn),s&&t(Xs),s&&t(cn),$(Qt,s),s&&t(hn),s&&t(hs),$(Xt),s&&t(un),s&&t(Y),s&&t(mn),s&&t(B),s&&t(gn),$(Zt,s),s&&t(_n),s&&t(G),s&&t(vn),$(sa,s),s&&t($n),s&&t(Qa),s&&t(yn),$(ta,s),s&&t(wn),s&&t(us),$(aa),s&&t(jn),s&&t(Xa),s&&t(bn),$(ea,s),s&&t(xn),$(st,s),s&&t(kn),s&&t(ms),$(la),s&&t(En),s&&t(Ka),s&&t(qn),$(oa,s),s&&t(Pn),s&&t(gs),$(na),s&&t(An),s&&t(Za),s&&t(Sn),s&&t(se),s&&t(Tn),s&&t(W),s&&t(Dn),$(pa,s),s&&t(In),$(et,s),s&&t(Nn),s&&t(lt),s&&t(Cn),$(da,s)}}}const $h={local:"load",sections:[{local:"hugging-face-hub",title:"Hugging Face Hub"},{local:"local-and-remote-files",sections:[{local:"csv",title:"CSV"},{local:"json",title:"JSON"},{local:"text-files",title:"Text files"},{local:"parquet",title:"Parquet"}],title:"Local and remote files"},{local:"inmemory-data",sections:[{local:"python-dictionary",title:"Python dictionary"},{local:"pandas-dataframe",title:"Pandas DataFrame"}],title:"In-memory data"},{local:"offline",title:"Offline"},{local:"slice-splits",sections:[{local:"percent-slicing-and-rounding",title:"Percent slicing and rounding"}],title:"Slice splits"},{local:"troubleshooting",sections:[{local:"manual-download",title:"Manual download"},{local:"specify-features",title:"Specify features"}],title:"Troubleshooting"},{local:"metrics",sections:[{local:"load-configurations",title:"Load configurations"},{local:"distributed-setup",title:"Distributed setup"}],title:"Metrics"}],title:"Load"};function yh(I,h,k){let{fw:y}=h;return I.$$set=b=>{"fw"in b&&k(0,y=b.fw)},[y]}class kh extends ih{constructor(h){super();ph(this,h,yh,vh,dh,{fw:0})}}export{kh as default,$h as metadata};
