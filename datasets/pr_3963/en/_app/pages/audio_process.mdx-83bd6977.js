import{S as Lr,i as Fr,s as Ir,e as l,k as d,w as h,t as s,M as Nr,c as r,d as t,m as u,a as n,x as m,h as o,b as c,N as Tr,F as a,g as p,y as _,q as g,o as v,B as w,v as Hr}from"../chunks/vendor-aa873a46.js";import{T as Vr}from"../chunks/Tip-f7f252ab.js";import{I as Ea}from"../chunks/IconCopyLink-d0ca3106.js";import{C as b}from"../chunks/CodeBlock-1f14baf3.js";function Wr(it){let f,ee,$,E,N,j,ve,H,V,ae,z,W,P,we;return{c(){f=l("p"),ee=s("torchaudio\u2019s "),$=l("code"),E=s("sox_io"),N=d(),j=l("a"),ve=s("backend"),H=s(" supports decoding "),V=l("code"),ae=s("mp3"),z=s(" files. Unfortunately, the "),W=l("code"),P=s("sox_io"),we=s(" backend is only available on Linux/macOS, and is not supported by Windows."),this.h()},l(M){f=r(M,"P",{});var y=n(f);ee=o(y,"torchaudio\u2019s "),$=r(y,"CODE",{});var k=n($);E=o(k,"sox_io"),k.forEach(t),N=u(y),j=r(y,"A",{href:!0,rel:!0});var te=n(j);ve=o(te,"backend"),te.forEach(t),H=o(y," supports decoding "),V=r(y,"CODE",{});var ea=n(V);ae=o(ea,"mp3"),ea.forEach(t),z=o(y," files. Unfortunately, the "),W=r(y,"CODE",{});var aa=n(W);P=o(aa,"sox_io"),aa.forEach(t),we=o(y," backend is only available on Linux/macOS, and is not supported by Windows."),y.forEach(t),this.h()},h(){c(j,"href","https://pytorch.org/audio/stable/backend.html#"),c(j,"rel","nofollow")},m(M,y){p(M,f,y),a(f,ee),a(f,$),a($,E),a(f,N),a(f,j),a(j,ve),a(f,H),a(f,V),a(V,ae),a(f,z),a(f,W),a(W,P),a(f,we)},d(M){M&&t(f)}}}function Mr(it){let f,ee,$,E,N,j,ve,H,V,ae,z,W,P,we,M,y,k,te,ea,aa,za,hs,ms,ye,_s,ta,gs,vs,pt,U,se,xa,be,ws,Aa,ys,ct,D,bs,sa,$s,js,oa,ks,Es,dt,$e,ut,C,zs,je,xs,As,ke,qs,Ps,ft,Ee,ht,oe,mt,le,Ds,ze,Cs,Os,_t,xe,gt,R,re,qa,Ae,Ss,Pa,Ts,vt,x,Ls,Da,Fs,Is,Ca,Ns,Hs,Oa,Vs,Ws,wt,qe,Sa,Ms,Us,yt,Pe,bt,A,Rs,Ta,Ys,Gs,La,Js,Bs,Fa,Ks,Xs,$t,Y,Ia,Qs,Zs,Na,eo,ao,jt,De,kt,O,to,Ha,so,oo,la,lo,ro,Et,Ce,zt,G,ne,Va,Oe,no,Wa,io,xt,S,po,Ma,co,uo,Ua,fo,ho,At,Se,qt,ie,mo,Ra,_o,go,Pt,Te,Dt,ra,vo,Ct,Le,Ot,T,wo,Ya,yo,bo,Ga,$o,jo,St,Fe,Tt,pe,ko,Ja,Eo,zo,Lt,Ie,Ft,L,xo,Ba,Ao,qo,Ka,Po,Do,It,J,ce,Xa,Ne,Co,Qa,Oo,Nt,F,So,He,To,Lo,na,Fo,Io,Ht,Ve,Vt,de,No,ia,Ho,Vo,Wt,We,Mt,B,pa,kl,Wo,ca,El,Ut,K,ue,Za,Me,Mo,et,Uo,Rt,q,Ro,da,Yo,Go,Ue,Jo,Bo,at,Ko,Xo,Yt,fe,tt,X,Qo,Re,Zo,el,st,al,tl,sl,Ye,Q,ol,Ge,ll,rl,ot,nl,il,pl,Z,cl,Je,dl,ul,Be,fl,hl,Gt,Ke,Jt,he,ml,lt,_l,gl,Bt,Xe,Kt,I,vl,rt,wl,yl,ua,bl,$l,Xt,Qe,Qt;return j=new Ea({}),be=new Ea({}),$e=new b({props:{code:"pip install datasets[audio]",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>pip install datasets[audio]'}}),Ee=new b({props:{code:`pip install librosa
pip install torchaudio`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>pip install librosa
<span class="hljs-meta">&gt;&gt;&gt; </span>pip install torchaudio`}}),oe=new Vr({props:{warning:!0,$$slots:{default:[Wr]},$$scope:{ctx:it}}}),xe=new b({props:{code:`from datasets import load_dataset, load_metric, Audio
common_voice = load_dataset("common_voice", "tr", split="train")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset, load_metric, Audio
<span class="hljs-meta">&gt;&gt;&gt; </span>common_voice = load_dataset(<span class="hljs-string">&quot;common_voice&quot;</span>, <span class="hljs-string">&quot;tr&quot;</span>, split=<span class="hljs-string">&quot;train&quot;</span>)`}}),Ae=new Ea({}),Pe=new b({props:{code:'common_voice[0]["audio"]',highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>common_voice[<span class="hljs-number">0</span>][<span class="hljs-string">&quot;audio&quot;</span>]
{<span class="hljs-string">&#x27;array&#x27;</span>: array([ <span class="hljs-number">0.0000000e+00</span>,  <span class="hljs-number">0.0000000e+00</span>,  <span class="hljs-number">0.0000000e+00</span>, ...,
    -<span class="hljs-number">8.8930130e-05</span>, -<span class="hljs-number">3.8027763e-05</span>, -<span class="hljs-number">2.9146671e-05</span>], dtype=float32),
<span class="hljs-string">&#x27;path&#x27;</span>: <span class="hljs-string">&#x27;/root/.cache/huggingface/datasets/downloads/extracted/05be0c29807a73c9b099873d2f5975dae6d05e9f7d577458a2466ecb9a2b0c6b/cv-corpus-6.1-2020-12-11/tr/clips/common_voice_tr_21921195.mp3&#x27;</span>,
<span class="hljs-string">&#x27;sampling_rate&#x27;</span>: <span class="hljs-number">48000</span>}`}}),De=new b({props:{code:'common_voice[0]["path"]',highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>common_voice[<span class="hljs-number">0</span>][<span class="hljs-string">&quot;path&quot;</span>]
/root/.cache/huggingface/datasets/downloads/extracted/05be0c29807a73c9b099873d2f5975dae6d05e9f7d577458a2466ecb9a2b0c6b/cv-corpus-<span class="hljs-number">6.1</span>-<span class="hljs-number">2020</span>-<span class="hljs-number">12</span>-<span class="hljs-number">11</span>/tr/clips/common_voice_tr_21921195.mp3`}}),Ce=new b({props:{code:'my_audio_dataset = my_audio_dataset.cast_column("paths_to_my_audio_files", Audio())',highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>my_audio_dataset = my_audio_dataset.cast_column(<span class="hljs-string">&quot;paths_to_my_audio_files&quot;</span>, Audio())'}}),Oe=new Ea({}),Se=new b({props:{code:`audio/1889_1447_000071.flac
audio/1889_1447_000096.flac
audio/1889_1447_000063.flac
audio/1889_1447_000097.flac
...
transcripts.txt`,highlighted:`<span class="hljs-selector-tag">audio</span>/<span class="hljs-number">1889</span>_1447_000071<span class="hljs-selector-class">.flac</span>
<span class="hljs-selector-tag">audio</span>/<span class="hljs-number">1889</span>_1447_000096<span class="hljs-selector-class">.flac</span>
<span class="hljs-selector-tag">audio</span>/<span class="hljs-number">1889</span>_1447_000063<span class="hljs-selector-class">.flac</span>
<span class="hljs-selector-tag">audio</span>/<span class="hljs-number">1889</span>_1447_000097<span class="hljs-selector-class">.flac</span>
...
transcripts<span class="hljs-selector-class">.txt</span>`}}),Te=new b({props:{code:`1889_1447_000071	stali\u015Bmy wszyscy bezradni wobec tej szalej\u0105cej furii z\u0142o\u015Bci kt\xF3ra sama siebie trawi\u0142a i po\u017Cera\u0142a z ubolewaniem patrzyli\u015Bmy na smutny przebieg tego paroksyzmu i z pewn\u0105 ulg\u0105 wr\xF3cili\u015Bmy do naszych zaj\u0119\u0107 gdy \u017Ca\u0142osny ten proces dobieg\u0142 swego naturalnego ko\u0144ca
1889_1447_000096	nawet w obecno\u015Bci matki le\u017C\u0105cej z zawi\u0105zan\u0105 g\u0142ow\u0105 na sofie nie mog\u0142y si\u0119 powstrzyma\u0107 robi\u0142y perskie oczko dawa\u0142y sobie znaki m\xF3wi\u0142y niemym kolorowym alfabetem pe\u0142nym sekretnych znacze\u0144 irytowa\u0142o mnie to szydercze porozumienie ta migotliwa zmowa poza mymi plecami
1889_1447_000063	stali\u015Bmy pod drzwiami i nas\u0142uchiwali w lamentach wichru dawa\u0142y si\u0119 s\u0142ysze\u0107 wszelkie g\u0142osy perswazje nawo\u0142ywania i gaw\u0119dy zdawa\u0142o si\u0119 nam \u017Ce s\u0142yszymy wo\u0142anie o pomoc ojca zb\u0142\u0105kanego w wichurze to znowu \u017Ce brat z teodorem gwarz\u0105 beztrosko pod drzwiami
1889_1447_000097	owego dnia by\u0142 ci\u0119\u017Cki dzie\u0144 zimowy i od rana ju\u017C sypa\u0142 si\u0119 mi\u0119kki puch zmierzchu matka mia\u0142a migren\u0119 i le\u017Ca\u0142a na sofie samotnie w salonie
...`,highlighted:`<span class="hljs-number">1889</span>_1447_000071	stali\u015Bmy wszyscy bezradni wobec tej szalej\u0105cej furii z\u0142o\u015Bci kt\xF3ra sama siebie trawi\u0142<span class="hljs-selector-tag">a</span> <span class="hljs-selector-tag">i</span> po\u017Cera\u0142<span class="hljs-selector-tag">a</span> z ubolewaniem patrzyli\u015Bmy na smutny przebieg tego paroksyzmu <span class="hljs-selector-tag">i</span> z pewn\u0105 ulg\u0105 wr\xF3cili\u015Bmy do naszych zaj\u0119\u0107 gdy \u017C<span class="hljs-selector-tag">a</span>\u0142osny ten proces dobieg\u0142 swego naturalnego ko\u0144ca
<span class="hljs-number">1889</span>_1447_000096	nawet w obecno\u015Bci matki le\u017C\u0105cej z zawi\u0105zan\u0105 g\u0142ow\u0105 na sofie nie mog\u0142y si\u0119 powstrzyma\u0107 robi\u0142y perskie oczko dawa\u0142y sobie znaki m\xF3wi\u0142y niemym kolorowym alfabetem pe\u0142nym sekretnych znacze\u0144 irytowa\u0142o mnie <span class="hljs-selector-tag">to</span> szydercze porozumienie ta migotliwa zmowa poza mymi plecami
<span class="hljs-number">1889</span>_1447_000063	stali\u015Bmy pod drzwiami <span class="hljs-selector-tag">i</span> nas\u0142uchiwali w lamentach wichru dawa\u0142y si\u0119 s\u0142ysze\u0107 wszelkie g\u0142osy perswazje nawo\u0142ywania <span class="hljs-selector-tag">i</span> gaw\u0119dy zdawa\u0142o si\u0119 nam \u017Ce s\u0142yszymy wo\u0142anie o pomoc ojca zb\u0142\u0105kanego w wichurze <span class="hljs-selector-tag">to</span> znowu \u017Ce brat z teodorem gwarz\u0105 beztrosko pod drzwiami
<span class="hljs-number">1889</span>_1447_000097	owego dnia by\u0142 ci\u0119\u017Cki dzie\u0144 zimowy <span class="hljs-selector-tag">i</span> od rana ju\u017C sypa\u0142 si\u0119 mi\u0119kki puch zmierzchu matka mia\u0142<span class="hljs-selector-tag">a</span> migren\u0119 <span class="hljs-selector-tag">i</span> le\u017C<span class="hljs-selector-tag">a</span>\u0142<span class="hljs-selector-tag">a</span> na sofie samotnie w salonie
...`}}),Le=new b({props:{code:`from datasets import load_dataset
dataset = load_dataset("audiofolder", data_dir="/path/to/audiofolder", sampling_rate=16_000)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;audiofolder&quot;</span>, data_dir=<span class="hljs-string">&quot;/path/to/audiofolder&quot;</span>, sampling_rate=<span class="hljs-number">16_000</span>)`}}),Fe=new b({props:{code:`from datasets import load_dataset
dataset = load_dataset("audiofolder", data_files="/path/to/audioarchive.zip", sampling_rate=16_000)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;audiofolder&quot;</span>, data_files=<span class="hljs-string">&quot;/path/to/audioarchive.zip&quot;</span>, sampling_rate=<span class="hljs-number">16_000</span>)`}}),Ie=new b({props:{code:'dataset = load_dataset("audiofolder", data_files="https://huggingface.co/datasets/polinaeterna/test_audiofolder/resolve/main/MLSPolishSample.zip", sampling_rate=16_000)',highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;audiofolder&quot;</span>, data_files=<span class="hljs-string">&quot;https://huggingface.co/datasets/polinaeterna/test_audiofolder/resolve/main/MLSPolishSample.zip&quot;</span>, sampling_rate=<span class="hljs-number">16_000</span>)'}}),Ne=new Ea({}),Ve=new b({props:{code:'common_voice = common_voice.cast_column("audio", Audio(sampling_rate=16_000))',highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>common_voice = common_voice.cast_column(<span class="hljs-string">&quot;audio&quot;</span>, Audio(sampling_rate=<span class="hljs-number">16_000</span>))'}}),We=new b({props:{code:'common_voice_train[0]["audio"]',highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>common_voice_train[<span class="hljs-number">0</span>][<span class="hljs-string">&quot;audio&quot;</span>]
{<span class="hljs-string">&#x27;array&#x27;</span>: array([ <span class="hljs-number">0.0000000e+00</span>,  <span class="hljs-number">0.0000000e+00</span>,  <span class="hljs-number">0.0000000e+00</span>, ...,
-<span class="hljs-number">7.4556941e-05</span>, -<span class="hljs-number">1.4621433e-05</span>, -<span class="hljs-number">5.7861507e-05</span>], dtype=float32),
<span class="hljs-string">&#x27;path&#x27;</span>: <span class="hljs-string">&#x27;/root/.cache/huggingface/datasets/downloads/extracted/05be0c29807a73c9b099873d2f5975dae6d05e9f7d577458a2466ecb9a2b0c6b/cv-corpus-6.1-2020-12-11/tr/clips/common_voice_tr_21921195.mp3&#x27;</span>,
<span class="hljs-string">&#x27;sampling_rate&#x27;</span>: <span class="hljs-number">16000</span>}`}}),Me=new Ea({}),Ke=new b({props:{code:`from transformers import AutoTokenizer, AutoFeatureExtractor, Wav2Vec2Processor
model_checkpoint = "facebook/wav2vec2-large-xlsr-53"
# after defining a vocab.json file you can instantiate a tokenizer object:
tokenizer = AutoTokenizer("./vocab.json", unk_token="[UNK]", pad_token="[PAD]", word_delimiter_token="|")
feature_extractor = AutoFeatureExtractor.from_pretrained(model_checkpoint)
processor = Wav2Vec2Processor.from_pretrained(feature_extractor=feature_extractor, tokenizer=tokenizer)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, AutoFeatureExtractor, Wav2Vec2Processor
<span class="hljs-meta">&gt;&gt;&gt; </span>model_checkpoint = <span class="hljs-string">&quot;facebook/wav2vec2-large-xlsr-53&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># after defining a vocab.json file you can instantiate a tokenizer object:</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer(<span class="hljs-string">&quot;./vocab.json&quot;</span>, unk_token=<span class="hljs-string">&quot;[UNK]&quot;</span>, pad_token=<span class="hljs-string">&quot;[PAD]&quot;</span>, word_delimiter_token=<span class="hljs-string">&quot;|&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>feature_extractor = AutoFeatureExtractor.from_pretrained(model_checkpoint)
<span class="hljs-meta">&gt;&gt;&gt; </span>processor = Wav2Vec2Processor.from_pretrained(feature_extractor=feature_extractor, tokenizer=tokenizer)`}}),Xe=new b({props:{code:`from transformers import Wav2Vec2Processor
processor = Wav2Vec2Processor.from_pretrained("facebook/wav2vec2-base-960h")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> Wav2Vec2Processor
<span class="hljs-meta">&gt;&gt;&gt; </span>processor = Wav2Vec2Processor.from_pretrained(<span class="hljs-string">&quot;facebook/wav2vec2-base-960h&quot;</span>)`}}),Qe=new b({props:{code:`def prepare_dataset(batch):
    audio = batch["audio"]
    batch["input_values"] = processor(audio["array"], sampling_rate=audio["sampling_rate"]).input_values[0]
    batch["input_length"] = len(batch["input_values"])
    with processor.as_target_processor():
        batch["labels"] = processor(batch["sentence"]).input_ids
    return batch
common_voice_train = common_voice_train.map(prepare_dataset, remove_columns=common_voice_train.column_names)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">def</span> <span class="hljs-title function_">prepare_dataset</span>(<span class="hljs-params">batch</span>):
<span class="hljs-meta">... </span>    audio = batch[<span class="hljs-string">&quot;audio&quot;</span>]
<span class="hljs-meta">... </span>    batch[<span class="hljs-string">&quot;input_values&quot;</span>] = processor(audio[<span class="hljs-string">&quot;array&quot;</span>], sampling_rate=audio[<span class="hljs-string">&quot;sampling_rate&quot;</span>]).input_values[<span class="hljs-number">0</span>]
<span class="hljs-meta">... </span>    batch[<span class="hljs-string">&quot;input_length&quot;</span>] = <span class="hljs-built_in">len</span>(batch[<span class="hljs-string">&quot;input_values&quot;</span>])
<span class="hljs-meta">... </span>    <span class="hljs-keyword">with</span> processor.as_target_processor():
<span class="hljs-meta">... </span>        batch[<span class="hljs-string">&quot;labels&quot;</span>] = processor(batch[<span class="hljs-string">&quot;sentence&quot;</span>]).input_ids
<span class="hljs-meta">... </span>    <span class="hljs-keyword">return</span> batch
<span class="hljs-meta">&gt;&gt;&gt; </span>common_voice_train = common_voice_train.<span class="hljs-built_in">map</span>(prepare_dataset, remove_columns=common_voice_train.column_names)`}}),{c(){f=l("meta"),ee=d(),$=l("h1"),E=l("a"),N=l("span"),h(j.$$.fragment),ve=d(),H=l("span"),V=s("Process audio data"),ae=d(),z=l("p"),W=s("\u{1F917} Datasets supports an "),P=l("a"),we=s("Audio"),M=s(" feature, enabling users to load and process raw audio files for training. This guide will show you how to:"),y=d(),k=l("ul"),te=l("li"),ea=s("Load your own custom audio dataset."),aa=d(),za=l("li"),hs=s("Resample audio files."),ms=d(),ye=l("li"),_s=s("Use "),ta=l("a"),gs=s("Dataset.map()"),vs=s(" with audio files."),pt=d(),U=l("h2"),se=l("a"),xa=l("span"),h(be.$$.fragment),ws=d(),Aa=l("span"),ys=s("Installation"),ct=d(),D=l("p"),bs=s("The "),sa=l("a"),$s=s("Audio"),js=s(" feature is an experimental feature and should be installed as an extra dependency in \u{1F917} Datasets. Install the "),oa=l("a"),ks=s("Audio"),Es=s(" feature with pip:"),dt=d(),h($e.$$.fragment),ut=d(),C=l("p"),zs=s("Users should also install "),je=l("a"),xs=s("torchaudio"),As=s(" and "),ke=l("a"),qs=s("librosa"),Ps=s(", two common libraries used by \u{1F917} Datasets for handling audio data."),ft=d(),h(Ee.$$.fragment),ht=d(),h(oe.$$.fragment),mt=d(),le=l("p"),Ds=s("Then you can load an audio dataset the same way you would load a text dataset. For example, load the "),ze=l("a"),Cs=s("Common Voice"),Os=s(" dataset with the Turkish configuration:"),_t=d(),h(xe.$$.fragment),gt=d(),R=l("h2"),re=l("a"),qa=l("span"),h(Ae.$$.fragment),Ss=d(),Pa=l("span"),Ts=s("Audio datasets"),vt=d(),x=l("p"),Ls=s("Audio datasets commonly have an "),Da=l("code"),Fs=s("audio"),Is=s(" and "),Ca=l("code"),Ns=s("path"),Hs=s(" or "),Oa=l("code"),Vs=s("file"),Ws=s(" column."),wt=d(),qe=l("p"),Sa=l("code"),Ms=s("audio"),Us=s(" is the actual audio file that is loaded and resampled on-the-fly upon calling it."),yt=d(),h(Pe.$$.fragment),bt=d(),A=l("p"),Rs=s("When you access an audio file, it is automatically decoded and resampled. Generally, you should query an audio file like: "),Ta=l("code"),Ys=s('common_voice[0]["audio"]'),Gs=s(". If you query an audio file with "),La=l("code"),Js=s('common_voice["audio"][0]'),Bs=s(" instead, "),Fa=l("strong"),Ks=s("all"),Xs=s(" the audio files in your dataset will be decoded and resampled. This process can take a long time if you have a large dataset."),$t=d(),Y=l("p"),Ia=l("code"),Qs=s("path"),Zs=s(" or "),Na=l("code"),eo=s("file"),ao=s(" is an absolute path to an audio file."),jt=d(),h(De.$$.fragment),kt=d(),O=l("p"),to=s("The "),Ha=l("code"),so=s("path"),oo=s(" is useful if you want to load your own audio dataset. In this case, provide a column of audio file paths to "),la=l("a"),lo=s("Dataset.cast_column()"),ro=s(":"),Et=d(),h(Ce.$$.fragment),zt=d(),G=l("h2"),ne=l("a"),Va=l("span"),h(Oe.$$.fragment),no=d(),Wa=l("span"),io=s("AudioFolder"),xt=d(),S=l("p"),po=s("You can load your audio dataset with an "),Ma=l("code"),co=s("AudioFolder"),uo=s(` dataset. Your dataset folder / archive should contain audio files and
file with transcriptions (default filename is `),Ua=l("code"),fo=s("transcripts.txt"),ho=s("), for example like this:"),At=d(),h(Se.$$.fragment),qt=d(),ie=l("p"),mo=s("Transcripts file should contain audio id and corresponding text transcription, separated by "),Ra=l("code"),_o=s('"\\t"'),go=s(" at each line:"),Pt=d(),h(Te.$$.fragment),Dt=d(),ra=l("p"),vo=s("Load a directory:"),Ct=d(),h(Le.$$.fragment),Ot=d(),T=l("p"),wo=s("Load a local "),Ya=l("code"),yo=s(".zip"),bo=s(" archive with the "),Ga=l("code"),$o=s("data_files"),jo=s(" parameter:"),St=d(),h(Fe.$$.fragment),Tt=d(),pe=l("p"),ko=s("Load remote datasets from their URLs with the "),Ja=l("code"),Eo=s("data_files"),zo=s(" parameter:"),Lt=d(),h(Ie.$$.fragment),Ft=d(),L=l("p"),xo=s("Note that "),Ba=l("code"),Ao=s("sampling_rate"),qo=s(` is a required argument.
Only `),Ka=l("code"),Po=s(".zip"),Do=s(" archives are supported now."),It=d(),J=l("h2"),ce=l("a"),Xa=l("span"),h(Ne.$$.fragment),Co=d(),Qa=l("span"),Oo=s("Resample"),Nt=d(),F=l("p"),So=s("Some models expect the audio data to have a certain sampling rate due to how the model was pretrained. For example, the "),He=l("a"),To=s("XLSR-Wav2Vec2"),Lo=s(" model expects the input to have a sampling rate of 16kHz, but an audio file from the Common Voice dataset has a sampling rate of 48kHz. You can use "),na=l("a"),Fo=s("Dataset.cast_column()"),Io=s(" to downsample the sampling rate to 16kHz:"),Ht=d(),h(Ve.$$.fragment),Vt=d(),de=l("p"),No=s("The next time you load the audio file, the "),ia=l("a"),Ho=s("Audio"),Vo=s(" feature will load and resample it to 16kHz:"),Wt=d(),h(We.$$.fragment),Mt=d(),B=l("div"),pa=l("img"),Wo=d(),ca=l("img"),Ut=d(),K=l("h2"),ue=l("a"),Za=l("span"),h(Me.$$.fragment),Mo=d(),et=l("span"),Uo=s("Map"),Rt=d(),q=l("p"),Ro=s("Just like text datasets, you can apply a preprocessing function over an entire dataset with "),da=l("a"),Yo=s("Dataset.map()"),Go=s(", which is useful for preprocessing all of your audio data at once. Start with a "),Ue=l("a"),Jo=s("speech recognition model"),Bo=s(" of your choice, and load a "),at=l("code"),Ko=s("processor"),Xo=s(" object that contains:"),Yt=d(),fe=l("ol"),tt=l("li"),X=l("p"),Qo=s("A feature extractor to convert the speech signal to the model\u2019s input format. Every speech recognition model on the \u{1F917} "),Re=l("a"),Zo=s("Hub"),el=s(" contains a predefined feature extractor that can be easily loaded with "),st=l("code"),al=s("AutoFeatureExtractor.from_pretrained(...)"),tl=s("."),sl=d(),Ye=l("li"),Q=l("p"),ol=s("A tokenizer to convert the model\u2019s output format to text. Fine-tuned speech recognition models, such as "),Ge=l("a"),ll=s("facebook/wav2vec2-base-960h"),rl=s(", contain a predefined tokenizer that can be easily loaded with "),ot=l("code"),nl=s("AutoTokenizer.from_pretrained(...)"),il=s("."),pl=d(),Z=l("p"),cl=s("For pretrained speech recognition models, such as "),Je=l("a"),dl=s("facebook/wav2vec2-large-xlsr-53"),ul=s(", a tokenizer needs to be created from the target text as explained "),Be=l("a"),fl=s("here"),hl=s(". The following example demonstrates how to load a feature extractor, tokenizer and processor for a pretrained speech recognition model:"),Gt=d(),h(Ke.$$.fragment),Jt=d(),he=l("p"),ml=s("For fine-tuned speech recognition models, you can simply load a predefined "),lt=l("code"),_l=s("processor"),gl=s(" object with:"),Bt=d(),h(Xe.$$.fragment),Kt=d(),I=l("p"),vl=s("Make sure to include the "),rt=l("code"),wl=s("audio"),yl=s(" key in your preprocessing function when you call "),ua=l("a"),bl=s("Dataset.map()"),$l=s(" so that you are actually resampling the audio data:"),Xt=d(),h(Qe.$$.fragment),this.h()},l(e){const i=Nr('[data-svelte="svelte-1phssyn"]',document.head);f=r(i,"META",{name:!0,content:!0}),i.forEach(t),ee=u(e),$=r(e,"H1",{class:!0});var Ze=n($);E=r(Ze,"A",{id:!0,class:!0,href:!0});var zl=n(E);N=r(zl,"SPAN",{});var xl=n(N);m(j.$$.fragment,xl),xl.forEach(t),zl.forEach(t),ve=u(Ze),H=r(Ze,"SPAN",{});var Al=n(H);V=o(Al,"Process audio data"),Al.forEach(t),Ze.forEach(t),ae=u(e),z=r(e,"P",{});var Zt=n(z);W=o(Zt,"\u{1F917} Datasets supports an "),P=r(Zt,"A",{href:!0});var ql=n(P);we=o(ql,"Audio"),ql.forEach(t),M=o(Zt," feature, enabling users to load and process raw audio files for training. This guide will show you how to:"),Zt.forEach(t),y=u(e),k=r(e,"UL",{});var fa=n(k);te=r(fa,"LI",{});var Pl=n(te);ea=o(Pl,"Load your own custom audio dataset."),Pl.forEach(t),aa=u(fa),za=r(fa,"LI",{});var Dl=n(za);hs=o(Dl,"Resample audio files."),Dl.forEach(t),ms=u(fa),ye=r(fa,"LI",{});var es=n(ye);_s=o(es,"Use "),ta=r(es,"A",{href:!0});var Cl=n(ta);gs=o(Cl,"Dataset.map()"),Cl.forEach(t),vs=o(es," with audio files."),es.forEach(t),fa.forEach(t),pt=u(e),U=r(e,"H2",{class:!0});var as=n(U);se=r(as,"A",{id:!0,class:!0,href:!0});var Ol=n(se);xa=r(Ol,"SPAN",{});var Sl=n(xa);m(be.$$.fragment,Sl),Sl.forEach(t),Ol.forEach(t),ws=u(as),Aa=r(as,"SPAN",{});var Tl=n(Aa);ys=o(Tl,"Installation"),Tl.forEach(t),as.forEach(t),ct=u(e),D=r(e,"P",{});var ha=n(D);bs=o(ha,"The "),sa=r(ha,"A",{href:!0});var Ll=n(sa);$s=o(Ll,"Audio"),Ll.forEach(t),js=o(ha," feature is an experimental feature and should be installed as an extra dependency in \u{1F917} Datasets. Install the "),oa=r(ha,"A",{href:!0});var Fl=n(oa);ks=o(Fl,"Audio"),Fl.forEach(t),Es=o(ha," feature with pip:"),ha.forEach(t),dt=u(e),m($e.$$.fragment,e),ut=u(e),C=r(e,"P",{});var ma=n(C);zs=o(ma,"Users should also install "),je=r(ma,"A",{href:!0,rel:!0});var Il=n(je);xs=o(Il,"torchaudio"),Il.forEach(t),As=o(ma," and "),ke=r(ma,"A",{href:!0,rel:!0});var Nl=n(ke);qs=o(Nl,"librosa"),Nl.forEach(t),Ps=o(ma,", two common libraries used by \u{1F917} Datasets for handling audio data."),ma.forEach(t),ft=u(e),m(Ee.$$.fragment,e),ht=u(e),m(oe.$$.fragment,e),mt=u(e),le=r(e,"P",{});var ts=n(le);Ds=o(ts,"Then you can load an audio dataset the same way you would load a text dataset. For example, load the "),ze=r(ts,"A",{href:!0,rel:!0});var Hl=n(ze);Cs=o(Hl,"Common Voice"),Hl.forEach(t),Os=o(ts," dataset with the Turkish configuration:"),ts.forEach(t),_t=u(e),m(xe.$$.fragment,e),gt=u(e),R=r(e,"H2",{class:!0});var ss=n(R);re=r(ss,"A",{id:!0,class:!0,href:!0});var Vl=n(re);qa=r(Vl,"SPAN",{});var Wl=n(qa);m(Ae.$$.fragment,Wl),Wl.forEach(t),Vl.forEach(t),Ss=u(ss),Pa=r(ss,"SPAN",{});var Ml=n(Pa);Ts=o(Ml,"Audio datasets"),Ml.forEach(t),ss.forEach(t),vt=u(e),x=r(e,"P",{});var me=n(x);Ls=o(me,"Audio datasets commonly have an "),Da=r(me,"CODE",{});var Ul=n(Da);Fs=o(Ul,"audio"),Ul.forEach(t),Is=o(me," and "),Ca=r(me,"CODE",{});var Rl=n(Ca);Ns=o(Rl,"path"),Rl.forEach(t),Hs=o(me," or "),Oa=r(me,"CODE",{});var Yl=n(Oa);Vs=o(Yl,"file"),Yl.forEach(t),Ws=o(me," column."),me.forEach(t),wt=u(e),qe=r(e,"P",{});var jl=n(qe);Sa=r(jl,"CODE",{});var Gl=n(Sa);Ms=o(Gl,"audio"),Gl.forEach(t),Us=o(jl," is the actual audio file that is loaded and resampled on-the-fly upon calling it."),jl.forEach(t),yt=u(e),m(Pe.$$.fragment,e),bt=u(e),A=r(e,"P",{});var _e=n(A);Rs=o(_e,"When you access an audio file, it is automatically decoded and resampled. Generally, you should query an audio file like: "),Ta=r(_e,"CODE",{});var Jl=n(Ta);Ys=o(Jl,'common_voice[0]["audio"]'),Jl.forEach(t),Gs=o(_e,". If you query an audio file with "),La=r(_e,"CODE",{});var Bl=n(La);Js=o(Bl,'common_voice["audio"][0]'),Bl.forEach(t),Bs=o(_e," instead, "),Fa=r(_e,"STRONG",{});var Kl=n(Fa);Ks=o(Kl,"all"),Kl.forEach(t),Xs=o(_e," the audio files in your dataset will be decoded and resampled. This process can take a long time if you have a large dataset."),_e.forEach(t),$t=u(e),Y=r(e,"P",{});var nt=n(Y);Ia=r(nt,"CODE",{});var Xl=n(Ia);Qs=o(Xl,"path"),Xl.forEach(t),Zs=o(nt," or "),Na=r(nt,"CODE",{});var Ql=n(Na);eo=o(Ql,"file"),Ql.forEach(t),ao=o(nt," is an absolute path to an audio file."),nt.forEach(t),jt=u(e),m(De.$$.fragment,e),kt=u(e),O=r(e,"P",{});var _a=n(O);to=o(_a,"The "),Ha=r(_a,"CODE",{});var Zl=n(Ha);so=o(Zl,"path"),Zl.forEach(t),oo=o(_a," is useful if you want to load your own audio dataset. In this case, provide a column of audio file paths to "),la=r(_a,"A",{href:!0});var er=n(la);lo=o(er,"Dataset.cast_column()"),er.forEach(t),ro=o(_a,":"),_a.forEach(t),Et=u(e),m(Ce.$$.fragment,e),zt=u(e),G=r(e,"H2",{class:!0});var os=n(G);ne=r(os,"A",{id:!0,class:!0,href:!0});var ar=n(ne);Va=r(ar,"SPAN",{});var tr=n(Va);m(Oe.$$.fragment,tr),tr.forEach(t),ar.forEach(t),no=u(os),Wa=r(os,"SPAN",{});var sr=n(Wa);io=o(sr,"AudioFolder"),sr.forEach(t),os.forEach(t),xt=u(e),S=r(e,"P",{});var ga=n(S);po=o(ga,"You can load your audio dataset with an "),Ma=r(ga,"CODE",{});var or=n(Ma);co=o(or,"AudioFolder"),or.forEach(t),uo=o(ga,` dataset. Your dataset folder / archive should contain audio files and
file with transcriptions (default filename is `),Ua=r(ga,"CODE",{});var lr=n(Ua);fo=o(lr,"transcripts.txt"),lr.forEach(t),ho=o(ga,"), for example like this:"),ga.forEach(t),At=u(e),m(Se.$$.fragment,e),qt=u(e),ie=r(e,"P",{});var ls=n(ie);mo=o(ls,"Transcripts file should contain audio id and corresponding text transcription, separated by "),Ra=r(ls,"CODE",{});var rr=n(Ra);_o=o(rr,'"\\t"'),rr.forEach(t),go=o(ls," at each line:"),ls.forEach(t),Pt=u(e),m(Te.$$.fragment,e),Dt=u(e),ra=r(e,"P",{});var nr=n(ra);vo=o(nr,"Load a directory:"),nr.forEach(t),Ct=u(e),m(Le.$$.fragment,e),Ot=u(e),T=r(e,"P",{});var va=n(T);wo=o(va,"Load a local "),Ya=r(va,"CODE",{});var ir=n(Ya);yo=o(ir,".zip"),ir.forEach(t),bo=o(va," archive with the "),Ga=r(va,"CODE",{});var pr=n(Ga);$o=o(pr,"data_files"),pr.forEach(t),jo=o(va," parameter:"),va.forEach(t),St=u(e),m(Fe.$$.fragment,e),Tt=u(e),pe=r(e,"P",{});var rs=n(pe);ko=o(rs,"Load remote datasets from their URLs with the "),Ja=r(rs,"CODE",{});var cr=n(Ja);Eo=o(cr,"data_files"),cr.forEach(t),zo=o(rs," parameter:"),rs.forEach(t),Lt=u(e),m(Ie.$$.fragment,e),Ft=u(e),L=r(e,"P",{});var wa=n(L);xo=o(wa,"Note that "),Ba=r(wa,"CODE",{});var dr=n(Ba);Ao=o(dr,"sampling_rate"),dr.forEach(t),qo=o(wa,` is a required argument.
Only `),Ka=r(wa,"CODE",{});var ur=n(Ka);Po=o(ur,".zip"),ur.forEach(t),Do=o(wa," archives are supported now."),wa.forEach(t),It=u(e),J=r(e,"H2",{class:!0});var ns=n(J);ce=r(ns,"A",{id:!0,class:!0,href:!0});var fr=n(ce);Xa=r(fr,"SPAN",{});var hr=n(Xa);m(Ne.$$.fragment,hr),hr.forEach(t),fr.forEach(t),Co=u(ns),Qa=r(ns,"SPAN",{});var mr=n(Qa);Oo=o(mr,"Resample"),mr.forEach(t),ns.forEach(t),Nt=u(e),F=r(e,"P",{});var ya=n(F);So=o(ya,"Some models expect the audio data to have a certain sampling rate due to how the model was pretrained. For example, the "),He=r(ya,"A",{href:!0,rel:!0});var _r=n(He);To=o(_r,"XLSR-Wav2Vec2"),_r.forEach(t),Lo=o(ya," model expects the input to have a sampling rate of 16kHz, but an audio file from the Common Voice dataset has a sampling rate of 48kHz. You can use "),na=r(ya,"A",{href:!0});var gr=n(na);Fo=o(gr,"Dataset.cast_column()"),gr.forEach(t),Io=o(ya," to downsample the sampling rate to 16kHz:"),ya.forEach(t),Ht=u(e),m(Ve.$$.fragment,e),Vt=u(e),de=r(e,"P",{});var is=n(de);No=o(is,"The next time you load the audio file, the "),ia=r(is,"A",{href:!0});var vr=n(ia);Ho=o(vr,"Audio"),vr.forEach(t),Vo=o(is," feature will load and resample it to 16kHz:"),is.forEach(t),Wt=u(e),m(We.$$.fragment,e),Mt=u(e),B=r(e,"DIV",{class:!0});var ps=n(B);pa=r(ps,"IMG",{class:!0,src:!0}),Wo=u(ps),ca=r(ps,"IMG",{class:!0,src:!0}),ps.forEach(t),Ut=u(e),K=r(e,"H2",{class:!0});var cs=n(K);ue=r(cs,"A",{id:!0,class:!0,href:!0});var wr=n(ue);Za=r(wr,"SPAN",{});var yr=n(Za);m(Me.$$.fragment,yr),yr.forEach(t),wr.forEach(t),Mo=u(cs),et=r(cs,"SPAN",{});var br=n(et);Uo=o(br,"Map"),br.forEach(t),cs.forEach(t),Rt=u(e),q=r(e,"P",{});var ge=n(q);Ro=o(ge,"Just like text datasets, you can apply a preprocessing function over an entire dataset with "),da=r(ge,"A",{href:!0});var $r=n(da);Yo=o($r,"Dataset.map()"),$r.forEach(t),Go=o(ge,", which is useful for preprocessing all of your audio data at once. Start with a "),Ue=r(ge,"A",{href:!0,rel:!0});var jr=n(Ue);Jo=o(jr,"speech recognition model"),jr.forEach(t),Bo=o(ge," of your choice, and load a "),at=r(ge,"CODE",{});var kr=n(at);Ko=o(kr,"processor"),kr.forEach(t),Xo=o(ge," object that contains:"),ge.forEach(t),Yt=u(e),fe=r(e,"OL",{});var ds=n(fe);tt=r(ds,"LI",{});var Er=n(tt);X=r(Er,"P",{});var ba=n(X);Qo=o(ba,"A feature extractor to convert the speech signal to the model\u2019s input format. Every speech recognition model on the \u{1F917} "),Re=r(ba,"A",{href:!0,rel:!0});var zr=n(Re);Zo=o(zr,"Hub"),zr.forEach(t),el=o(ba," contains a predefined feature extractor that can be easily loaded with "),st=r(ba,"CODE",{});var xr=n(st);al=o(xr,"AutoFeatureExtractor.from_pretrained(...)"),xr.forEach(t),tl=o(ba,"."),ba.forEach(t),Er.forEach(t),sl=u(ds),Ye=r(ds,"LI",{});var us=n(Ye);Q=r(us,"P",{});var $a=n(Q);ol=o($a,"A tokenizer to convert the model\u2019s output format to text. Fine-tuned speech recognition models, such as "),Ge=r($a,"A",{href:!0,rel:!0});var Ar=n(Ge);ll=o(Ar,"facebook/wav2vec2-base-960h"),Ar.forEach(t),rl=o($a,", contain a predefined tokenizer that can be easily loaded with "),ot=r($a,"CODE",{});var qr=n(ot);nl=o(qr,"AutoTokenizer.from_pretrained(...)"),qr.forEach(t),il=o($a,"."),$a.forEach(t),pl=u(us),Z=r(us,"P",{});var ja=n(Z);cl=o(ja,"For pretrained speech recognition models, such as "),Je=r(ja,"A",{href:!0,rel:!0});var Pr=n(Je);dl=o(Pr,"facebook/wav2vec2-large-xlsr-53"),Pr.forEach(t),ul=o(ja,", a tokenizer needs to be created from the target text as explained "),Be=r(ja,"A",{href:!0,rel:!0});var Dr=n(Be);fl=o(Dr,"here"),Dr.forEach(t),hl=o(ja,". The following example demonstrates how to load a feature extractor, tokenizer and processor for a pretrained speech recognition model:"),ja.forEach(t),us.forEach(t),ds.forEach(t),Gt=u(e),m(Ke.$$.fragment,e),Jt=u(e),he=r(e,"P",{});var fs=n(he);ml=o(fs,"For fine-tuned speech recognition models, you can simply load a predefined "),lt=r(fs,"CODE",{});var Cr=n(lt);_l=o(Cr,"processor"),Cr.forEach(t),gl=o(fs," object with:"),fs.forEach(t),Bt=u(e),m(Xe.$$.fragment,e),Kt=u(e),I=r(e,"P",{});var ka=n(I);vl=o(ka,"Make sure to include the "),rt=r(ka,"CODE",{});var Or=n(rt);wl=o(Or,"audio"),Or.forEach(t),yl=o(ka," key in your preprocessing function when you call "),ua=r(ka,"A",{href:!0});var Sr=n(ua);bl=o(Sr,"Dataset.map()"),Sr.forEach(t),$l=o(ka," so that you are actually resampling the audio data:"),ka.forEach(t),Xt=u(e),m(Qe.$$.fragment,e),this.h()},h(){c(f,"name","hf:doc:metadata"),c(f,"content",JSON.stringify(Ur)),c(E,"id","process-audio-data"),c(E,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(E,"href","#process-audio-data"),c($,"class","relative group"),c(P,"href","/docs/datasets/pr_3963/en/package_reference/main_classes#datasets.Audio"),c(ta,"href","/docs/datasets/pr_3963/en/package_reference/main_classes#datasets.Dataset.map"),c(se,"id","installation"),c(se,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(se,"href","#installation"),c(U,"class","relative group"),c(sa,"href","/docs/datasets/pr_3963/en/package_reference/main_classes#datasets.Audio"),c(oa,"href","/docs/datasets/pr_3963/en/package_reference/main_classes#datasets.Audio"),c(je,"href","https://pytorch.org/audio/stable/index.html"),c(je,"rel","nofollow"),c(ke,"href","https://librosa.org/doc/latest/index.html"),c(ke,"rel","nofollow"),c(ze,"href","https://huggingface.co/datasets/common_voice"),c(ze,"rel","nofollow"),c(re,"id","audio-datasets"),c(re,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(re,"href","#audio-datasets"),c(R,"class","relative group"),c(la,"href","/docs/datasets/pr_3963/en/package_reference/main_classes#datasets.Dataset.cast_column"),c(ne,"id","audiofolder"),c(ne,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(ne,"href","#audiofolder"),c(G,"class","relative group"),c(ce,"id","resample"),c(ce,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(ce,"href","#resample"),c(J,"class","relative group"),c(He,"href","https://huggingface.co/facebook/wav2vec2-large-xlsr-53"),c(He,"rel","nofollow"),c(na,"href","/docs/datasets/pr_3963/en/package_reference/main_classes#datasets.Dataset.cast_column"),c(ia,"href","/docs/datasets/pr_3963/en/package_reference/main_classes#datasets.Audio"),c(pa,"class","block dark:hidden"),Tr(pa.src,kl="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/datasets/resample.gif")||c(pa,"src",kl),c(ca,"class","hidden dark:block"),Tr(ca.src,El="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/datasets/resample-dark.gif")||c(ca,"src",El),c(B,"class","flex justify-center"),c(ue,"id","map"),c(ue,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(ue,"href","#map"),c(K,"class","relative group"),c(da,"href","/docs/datasets/pr_3963/en/package_reference/main_classes#datasets.Dataset.map"),c(Ue,"href","https://huggingface.co/models?pipeline_tag=automatic-speech-recognition&sort=downloads"),c(Ue,"rel","nofollow"),c(Re,"href","https://huggingface.co/models?pipeline_tag=automatic-speech-recognition&sort=downloads"),c(Re,"rel","nofollow"),c(Ge,"href","https://huggingface.co/facebook/wav2vec2-base-960h"),c(Ge,"rel","nofollow"),c(Je,"href","https://huggingface.co/facebook/wav2vec2-large-xlsr-53"),c(Je,"rel","nofollow"),c(Be,"href","https://huggingface.co/blog/fine-tune-wav2vec2-english"),c(Be,"rel","nofollow"),c(ua,"href","/docs/datasets/pr_3963/en/package_reference/main_classes#datasets.Dataset.map")},m(e,i){a(document.head,f),p(e,ee,i),p(e,$,i),a($,E),a(E,N),_(j,N,null),a($,ve),a($,H),a(H,V),p(e,ae,i),p(e,z,i),a(z,W),a(z,P),a(P,we),a(z,M),p(e,y,i),p(e,k,i),a(k,te),a(te,ea),a(k,aa),a(k,za),a(za,hs),a(k,ms),a(k,ye),a(ye,_s),a(ye,ta),a(ta,gs),a(ye,vs),p(e,pt,i),p(e,U,i),a(U,se),a(se,xa),_(be,xa,null),a(U,ws),a(U,Aa),a(Aa,ys),p(e,ct,i),p(e,D,i),a(D,bs),a(D,sa),a(sa,$s),a(D,js),a(D,oa),a(oa,ks),a(D,Es),p(e,dt,i),_($e,e,i),p(e,ut,i),p(e,C,i),a(C,zs),a(C,je),a(je,xs),a(C,As),a(C,ke),a(ke,qs),a(C,Ps),p(e,ft,i),_(Ee,e,i),p(e,ht,i),_(oe,e,i),p(e,mt,i),p(e,le,i),a(le,Ds),a(le,ze),a(ze,Cs),a(le,Os),p(e,_t,i),_(xe,e,i),p(e,gt,i),p(e,R,i),a(R,re),a(re,qa),_(Ae,qa,null),a(R,Ss),a(R,Pa),a(Pa,Ts),p(e,vt,i),p(e,x,i),a(x,Ls),a(x,Da),a(Da,Fs),a(x,Is),a(x,Ca),a(Ca,Ns),a(x,Hs),a(x,Oa),a(Oa,Vs),a(x,Ws),p(e,wt,i),p(e,qe,i),a(qe,Sa),a(Sa,Ms),a(qe,Us),p(e,yt,i),_(Pe,e,i),p(e,bt,i),p(e,A,i),a(A,Rs),a(A,Ta),a(Ta,Ys),a(A,Gs),a(A,La),a(La,Js),a(A,Bs),a(A,Fa),a(Fa,Ks),a(A,Xs),p(e,$t,i),p(e,Y,i),a(Y,Ia),a(Ia,Qs),a(Y,Zs),a(Y,Na),a(Na,eo),a(Y,ao),p(e,jt,i),_(De,e,i),p(e,kt,i),p(e,O,i),a(O,to),a(O,Ha),a(Ha,so),a(O,oo),a(O,la),a(la,lo),a(O,ro),p(e,Et,i),_(Ce,e,i),p(e,zt,i),p(e,G,i),a(G,ne),a(ne,Va),_(Oe,Va,null),a(G,no),a(G,Wa),a(Wa,io),p(e,xt,i),p(e,S,i),a(S,po),a(S,Ma),a(Ma,co),a(S,uo),a(S,Ua),a(Ua,fo),a(S,ho),p(e,At,i),_(Se,e,i),p(e,qt,i),p(e,ie,i),a(ie,mo),a(ie,Ra),a(Ra,_o),a(ie,go),p(e,Pt,i),_(Te,e,i),p(e,Dt,i),p(e,ra,i),a(ra,vo),p(e,Ct,i),_(Le,e,i),p(e,Ot,i),p(e,T,i),a(T,wo),a(T,Ya),a(Ya,yo),a(T,bo),a(T,Ga),a(Ga,$o),a(T,jo),p(e,St,i),_(Fe,e,i),p(e,Tt,i),p(e,pe,i),a(pe,ko),a(pe,Ja),a(Ja,Eo),a(pe,zo),p(e,Lt,i),_(Ie,e,i),p(e,Ft,i),p(e,L,i),a(L,xo),a(L,Ba),a(Ba,Ao),a(L,qo),a(L,Ka),a(Ka,Po),a(L,Do),p(e,It,i),p(e,J,i),a(J,ce),a(ce,Xa),_(Ne,Xa,null),a(J,Co),a(J,Qa),a(Qa,Oo),p(e,Nt,i),p(e,F,i),a(F,So),a(F,He),a(He,To),a(F,Lo),a(F,na),a(na,Fo),a(F,Io),p(e,Ht,i),_(Ve,e,i),p(e,Vt,i),p(e,de,i),a(de,No),a(de,ia),a(ia,Ho),a(de,Vo),p(e,Wt,i),_(We,e,i),p(e,Mt,i),p(e,B,i),a(B,pa),a(B,Wo),a(B,ca),p(e,Ut,i),p(e,K,i),a(K,ue),a(ue,Za),_(Me,Za,null),a(K,Mo),a(K,et),a(et,Uo),p(e,Rt,i),p(e,q,i),a(q,Ro),a(q,da),a(da,Yo),a(q,Go),a(q,Ue),a(Ue,Jo),a(q,Bo),a(q,at),a(at,Ko),a(q,Xo),p(e,Yt,i),p(e,fe,i),a(fe,tt),a(tt,X),a(X,Qo),a(X,Re),a(Re,Zo),a(X,el),a(X,st),a(st,al),a(X,tl),a(fe,sl),a(fe,Ye),a(Ye,Q),a(Q,ol),a(Q,Ge),a(Ge,ll),a(Q,rl),a(Q,ot),a(ot,nl),a(Q,il),a(Ye,pl),a(Ye,Z),a(Z,cl),a(Z,Je),a(Je,dl),a(Z,ul),a(Z,Be),a(Be,fl),a(Z,hl),p(e,Gt,i),_(Ke,e,i),p(e,Jt,i),p(e,he,i),a(he,ml),a(he,lt),a(lt,_l),a(he,gl),p(e,Bt,i),_(Xe,e,i),p(e,Kt,i),p(e,I,i),a(I,vl),a(I,rt),a(rt,wl),a(I,yl),a(I,ua),a(ua,bl),a(I,$l),p(e,Xt,i),_(Qe,e,i),Qt=!0},p(e,[i]){const Ze={};i&2&&(Ze.$$scope={dirty:i,ctx:e}),oe.$set(Ze)},i(e){Qt||(g(j.$$.fragment,e),g(be.$$.fragment,e),g($e.$$.fragment,e),g(Ee.$$.fragment,e),g(oe.$$.fragment,e),g(xe.$$.fragment,e),g(Ae.$$.fragment,e),g(Pe.$$.fragment,e),g(De.$$.fragment,e),g(Ce.$$.fragment,e),g(Oe.$$.fragment,e),g(Se.$$.fragment,e),g(Te.$$.fragment,e),g(Le.$$.fragment,e),g(Fe.$$.fragment,e),g(Ie.$$.fragment,e),g(Ne.$$.fragment,e),g(Ve.$$.fragment,e),g(We.$$.fragment,e),g(Me.$$.fragment,e),g(Ke.$$.fragment,e),g(Xe.$$.fragment,e),g(Qe.$$.fragment,e),Qt=!0)},o(e){v(j.$$.fragment,e),v(be.$$.fragment,e),v($e.$$.fragment,e),v(Ee.$$.fragment,e),v(oe.$$.fragment,e),v(xe.$$.fragment,e),v(Ae.$$.fragment,e),v(Pe.$$.fragment,e),v(De.$$.fragment,e),v(Ce.$$.fragment,e),v(Oe.$$.fragment,e),v(Se.$$.fragment,e),v(Te.$$.fragment,e),v(Le.$$.fragment,e),v(Fe.$$.fragment,e),v(Ie.$$.fragment,e),v(Ne.$$.fragment,e),v(Ve.$$.fragment,e),v(We.$$.fragment,e),v(Me.$$.fragment,e),v(Ke.$$.fragment,e),v(Xe.$$.fragment,e),v(Qe.$$.fragment,e),Qt=!1},d(e){t(f),e&&t(ee),e&&t($),w(j),e&&t(ae),e&&t(z),e&&t(y),e&&t(k),e&&t(pt),e&&t(U),w(be),e&&t(ct),e&&t(D),e&&t(dt),w($e,e),e&&t(ut),e&&t(C),e&&t(ft),w(Ee,e),e&&t(ht),w(oe,e),e&&t(mt),e&&t(le),e&&t(_t),w(xe,e),e&&t(gt),e&&t(R),w(Ae),e&&t(vt),e&&t(x),e&&t(wt),e&&t(qe),e&&t(yt),w(Pe,e),e&&t(bt),e&&t(A),e&&t($t),e&&t(Y),e&&t(jt),w(De,e),e&&t(kt),e&&t(O),e&&t(Et),w(Ce,e),e&&t(zt),e&&t(G),w(Oe),e&&t(xt),e&&t(S),e&&t(At),w(Se,e),e&&t(qt),e&&t(ie),e&&t(Pt),w(Te,e),e&&t(Dt),e&&t(ra),e&&t(Ct),w(Le,e),e&&t(Ot),e&&t(T),e&&t(St),w(Fe,e),e&&t(Tt),e&&t(pe),e&&t(Lt),w(Ie,e),e&&t(Ft),e&&t(L),e&&t(It),e&&t(J),w(Ne),e&&t(Nt),e&&t(F),e&&t(Ht),w(Ve,e),e&&t(Vt),e&&t(de),e&&t(Wt),w(We,e),e&&t(Mt),e&&t(B),e&&t(Ut),e&&t(K),w(Me),e&&t(Rt),e&&t(q),e&&t(Yt),e&&t(fe),e&&t(Gt),w(Ke,e),e&&t(Jt),e&&t(he),e&&t(Bt),w(Xe,e),e&&t(Kt),e&&t(I),e&&t(Xt),w(Qe,e)}}}const Ur={local:"process-audio-data",sections:[{local:"installation",title:"Installation"},{local:"audio-datasets",title:"Audio datasets"},{local:"audiofolder",title:"AudioFolder"},{local:"resample",title:"Resample"},{local:"map",title:"Map"}],title:"Process audio data"};function Rr(it){return Hr(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Kr extends Lr{constructor(f){super();Fr(this,f,Rr,Mr,Ir,{})}}export{Kr as default,Ur as metadata};
