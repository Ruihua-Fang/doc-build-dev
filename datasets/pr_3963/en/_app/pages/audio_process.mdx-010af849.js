import{S as Dr,i as Cr,s as Or,e as l,k as c,w as h,t as s,M as Tr,c as r,d as t,m as d,a as i,x as _,h as o,b as u,N as qr,F as a,g as p,y as g,q as v,o as w,B as $,v as Sr,L as Lr}from"../chunks/vendor-8138ceec.js";import{T as Pr}from"../chunks/Tip-12722dfc.js";import{I as ja}from"../chunks/IconCopyLink-2dd3a6ac.js";import{C as E}from"../chunks/CodeBlock-fc89709f.js";function Fr(ea){let f,C,y,z,P,b,x,q;return x=new E({props:{code:"sudo apt-get install libsndfile1",highlighted:"sudo apt-get install libsndfile1"}}),{c(){f=l("p"),C=s("On Linux, non-Python dependency on "),y=l("code"),z=s("libsndfile"),P=s(" package must be installed manually, using your distribution package manager, for example:"),b=c(),h(x.$$.fragment)},l(m){f=r(m,"P",{});var k=i(f);C=o(k,"On Linux, non-Python dependency on "),y=r(k,"CODE",{});var A=i(y);z=o(A,"libsndfile"),A.forEach(t),P=o(k," package must be installed manually, using your distribution package manager, for example:"),k.forEach(t),b=d(m),_(x.$$.fragment,m)},m(m,k){p(m,f,k),a(f,C),a(f,y),a(y,z),a(f,P),p(m,b,k),g(x,m,k),q=!0},p:Lr,i(m){q||(v(x.$$.fragment,m),q=!0)},o(m){w(x.$$.fragment,m),q=!1},d(m){m&&t(f),m&&t(b),$(x,m)}}}function Ir(ea){let f,C,y,z,P,b,x,q,m,k,A,W,L,$e;return{c(){f=l("p"),C=s("torchaudio\u2019s "),y=l("code"),z=s("sox_io"),P=c(),b=l("a"),x=s("backend"),q=s(" supports decoding "),m=l("code"),k=s("mp3"),A=s(" files. Unfortunately, the "),W=l("code"),L=s("sox_io"),$e=s(" backend is only available on Linux/macOS, and is not supported by Windows."),this.h()},l(R){f=r(R,"P",{});var j=i(f);C=o(j,"torchaudio\u2019s "),y=r(j,"CODE",{});var D=i(y);z=o(D,"sox_io"),D.forEach(t),P=d(j),b=r(j,"A",{href:!0,rel:!0});var ae=i(b);x=o(ae,"backend"),ae.forEach(t),q=o(j," supports decoding "),m=r(j,"CODE",{});var aa=i(m);k=o(aa,"mp3"),aa.forEach(t),A=o(j," files. Unfortunately, the "),W=r(j,"CODE",{});var ta=i(W);L=o(ta,"sox_io"),ta.forEach(t),$e=o(j," backend is only available on Linux/macOS, and is not supported by Windows."),j.forEach(t),this.h()},h(){u(b,"href","https://pytorch.org/audio/stable/backend.html#"),u(b,"rel","nofollow")},m(R,j){p(R,f,j),a(f,C),a(f,y),a(y,z),a(f,P),a(f,b),a(b,x),a(f,q),a(f,m),a(m,k),a(f,A),a(f,W),a(W,L),a(f,$e)},d(R){R&&t(f)}}}function Nr(ea){let f,C,y,z,P,b,x,q,m,k,A,W,L,$e,R,j,D,ae,aa,ta,Ea,hs,_s,ye,gs,sa,vs,ws,nt,U,te,za,be,$s,xa,ys,it,F,bs,oa,ks,js,la,Es,zs,pt,ke,ct,se,dt,oe,xs,je,As,qs,ut,Ee,ft,le,mt,re,Ps,ze,Ds,Cs,ht,xe,_t,Y,ne,Aa,Ae,Os,qa,Ts,gt,O,Ss,Pa,Ls,Fs,Da,Is,Ns,Ca,Hs,Vs,vt,qe,Oa,Ms,Ws,wt,Pe,$t,T,Rs,Ta,Us,Ys,Sa,Gs,Js,La,Bs,Ks,yt,G,Fa,Xs,Qs,Ia,Zs,eo,bt,De,kt,I,ao,Na,to,so,ra,oo,lo,jt,Ce,Et,J,ie,Ha,Oe,ro,Va,no,zt,N,io,Ma,po,co,Wa,uo,fo,xt,Te,At,pe,mo,Ra,ho,_o,qt,Se,Pt,na,go,Dt,Le,Ct,H,vo,Ua,wo,$o,Ya,yo,bo,Ot,Fe,Tt,ce,ko,Ga,jo,Eo,St,Ie,Lt,de,zo,Ja,xo,Ao,Ft,B,ue,Ba,Ne,qo,Ka,Po,It,V,Do,He,Co,Oo,ia,To,So,Nt,Ve,Ht,fe,Lo,pa,Fo,Io,Vt,Me,Mt,K,ca,yl,No,da,bl,Wt,X,me,Xa,We,Ho,Qa,Vo,Rt,S,Mo,ua,Wo,Ro,Re,Uo,Yo,Za,Go,Jo,Ut,he,et,Q,Bo,Ue,Ko,Xo,at,Qo,Zo,el,Ye,Z,al,Ge,tl,sl,tt,ol,ll,rl,ee,nl,Je,il,pl,Be,cl,dl,Yt,Ke,Gt,_e,ul,st,fl,ml,Jt,Xe,Bt,M,hl,ot,_l,gl,fa,vl,wl,Kt,Qe,Xt;return b=new ja({}),be=new ja({}),ke=new E({props:{code:"pip install datasets[audio]",highlighted:"pip install datasets[audio]"}}),se=new Pr({props:{warning:!0,$$slots:{default:[Fr]},$$scope:{ctx:ea}}}),Ee=new E({props:{code:"pip install torchaudio",highlighted:"pip install torchaudio"}}),le=new Pr({props:{warning:!0,$$slots:{default:[Ir]},$$scope:{ctx:ea}}}),xe=new E({props:{code:`from datasets import load_dataset, load_metric, Audio
common_voice = load_dataset("common_voice", "tr", split="train")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset, load_metric, Audio
<span class="hljs-meta">&gt;&gt;&gt; </span>common_voice = load_dataset(<span class="hljs-string">&quot;common_voice&quot;</span>, <span class="hljs-string">&quot;tr&quot;</span>, split=<span class="hljs-string">&quot;train&quot;</span>)`}}),Ae=new ja({}),Pe=new E({props:{code:'common_voice[0]["audio"]',highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>common_voice[<span class="hljs-number">0</span>][<span class="hljs-string">&quot;audio&quot;</span>]
{<span class="hljs-string">&#x27;array&#x27;</span>: array([ <span class="hljs-number">0.0000000e+00</span>,  <span class="hljs-number">0.0000000e+00</span>,  <span class="hljs-number">0.0000000e+00</span>, ...,
    -<span class="hljs-number">8.8930130e-05</span>, -<span class="hljs-number">3.8027763e-05</span>, -<span class="hljs-number">2.9146671e-05</span>], dtype=float32),
<span class="hljs-string">&#x27;path&#x27;</span>: <span class="hljs-string">&#x27;/root/.cache/huggingface/datasets/downloads/extracted/05be0c29807a73c9b099873d2f5975dae6d05e9f7d577458a2466ecb9a2b0c6b/cv-corpus-6.1-2020-12-11/tr/clips/common_voice_tr_21921195.mp3&#x27;</span>,
<span class="hljs-string">&#x27;sampling_rate&#x27;</span>: <span class="hljs-number">48000</span>}`}}),De=new E({props:{code:'common_voice[0]["path"]',highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>common_voice[<span class="hljs-number">0</span>][<span class="hljs-string">&quot;path&quot;</span>]
/root/.cache/huggingface/datasets/downloads/extracted/05be0c29807a73c9b099873d2f5975dae6d05e9f7d577458a2466ecb9a2b0c6b/cv-corpus-<span class="hljs-number">6.1</span>-<span class="hljs-number">2020</span>-<span class="hljs-number">12</span>-<span class="hljs-number">11</span>/tr/clips/common_voice_tr_21921195.mp3`}}),Ce=new E({props:{code:'my_audio_dataset = my_audio_dataset.cast_column("paths_to_my_audio_files", Audio())',highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>my_audio_dataset = my_audio_dataset.cast_column(<span class="hljs-string">&quot;paths_to_my_audio_files&quot;</span>, Audio())'}}),Oe=new ja({}),Te=new E({props:{code:`audio/1889_1447_000071.flac
audio/1889_1447_000096.flac
audio/1889_1447_000063.flac
audio/1889_1447_000097.flac
...
transcripts.txt`,highlighted:`<span class="hljs-selector-tag">audio</span>/<span class="hljs-number">1889</span>_1447_000071<span class="hljs-selector-class">.flac</span>
<span class="hljs-selector-tag">audio</span>/<span class="hljs-number">1889</span>_1447_000096<span class="hljs-selector-class">.flac</span>
<span class="hljs-selector-tag">audio</span>/<span class="hljs-number">1889</span>_1447_000063<span class="hljs-selector-class">.flac</span>
<span class="hljs-selector-tag">audio</span>/<span class="hljs-number">1889</span>_1447_000097<span class="hljs-selector-class">.flac</span>
...
transcripts<span class="hljs-selector-class">.txt</span>`}}),Se=new E({props:{code:`1889_1447_000071	stali\u015Bmy wszyscy bezradni wobec tej szalej\u0105cej furii z\u0142o\u015Bci kt\xF3ra sama siebie trawi\u0142a i po\u017Cera\u0142a z ubolewaniem patrzyli\u015Bmy na smutny przebieg tego paroksyzmu i z pewn\u0105 ulg\u0105 wr\xF3cili\u015Bmy do naszych zaj\u0119\u0107 gdy \u017Ca\u0142osny ten proces dobieg\u0142 swego naturalnego ko\u0144ca
1889_1447_000096	nawet w obecno\u015Bci matki le\u017C\u0105cej z zawi\u0105zan\u0105 g\u0142ow\u0105 na sofie nie mog\u0142y si\u0119 powstrzyma\u0107 robi\u0142y perskie oczko dawa\u0142y sobie znaki m\xF3wi\u0142y niemym kolorowym alfabetem pe\u0142nym sekretnych znacze\u0144 irytowa\u0142o mnie to szydercze porozumienie ta migotliwa zmowa poza mymi plecami
1889_1447_000063	stali\u015Bmy pod drzwiami i nas\u0142uchiwali w lamentach wichru dawa\u0142y si\u0119 s\u0142ysze\u0107 wszelkie g\u0142osy perswazje nawo\u0142ywania i gaw\u0119dy zdawa\u0142o si\u0119 nam \u017Ce s\u0142yszymy wo\u0142anie o pomoc ojca zb\u0142\u0105kanego w wichurze to znowu \u017Ce brat z teodorem gwarz\u0105 beztrosko pod drzwiami
1889_1447_000097	owego dnia by\u0142 ci\u0119\u017Cki dzie\u0144 zimowy i od rana ju\u017C sypa\u0142 si\u0119 mi\u0119kki puch zmierzchu matka mia\u0142a migren\u0119 i le\u017Ca\u0142a na sofie samotnie w salonie
...`,highlighted:`<span class="hljs-number">1889</span>_1447_000071	stali\u015Bmy wszyscy bezradni wobec tej szalej\u0105cej furii z\u0142o\u015Bci kt\xF3ra sama siebie trawi\u0142<span class="hljs-selector-tag">a</span> <span class="hljs-selector-tag">i</span> po\u017Cera\u0142<span class="hljs-selector-tag">a</span> z ubolewaniem patrzyli\u015Bmy na smutny przebieg tego paroksyzmu <span class="hljs-selector-tag">i</span> z pewn\u0105 ulg\u0105 wr\xF3cili\u015Bmy do naszych zaj\u0119\u0107 gdy \u017C<span class="hljs-selector-tag">a</span>\u0142osny ten proces dobieg\u0142 swego naturalnego ko\u0144ca
<span class="hljs-number">1889</span>_1447_000096	nawet w obecno\u015Bci matki le\u017C\u0105cej z zawi\u0105zan\u0105 g\u0142ow\u0105 na sofie nie mog\u0142y si\u0119 powstrzyma\u0107 robi\u0142y perskie oczko dawa\u0142y sobie znaki m\xF3wi\u0142y niemym kolorowym alfabetem pe\u0142nym sekretnych znacze\u0144 irytowa\u0142o mnie <span class="hljs-selector-tag">to</span> szydercze porozumienie ta migotliwa zmowa poza mymi plecami
<span class="hljs-number">1889</span>_1447_000063	stali\u015Bmy pod drzwiami <span class="hljs-selector-tag">i</span> nas\u0142uchiwali w lamentach wichru dawa\u0142y si\u0119 s\u0142ysze\u0107 wszelkie g\u0142osy perswazje nawo\u0142ywania <span class="hljs-selector-tag">i</span> gaw\u0119dy zdawa\u0142o si\u0119 nam \u017Ce s\u0142yszymy wo\u0142anie o pomoc ojca zb\u0142\u0105kanego w wichurze <span class="hljs-selector-tag">to</span> znowu \u017Ce brat z teodorem gwarz\u0105 beztrosko pod drzwiami
<span class="hljs-number">1889</span>_1447_000097	owego dnia by\u0142 ci\u0119\u017Cki dzie\u0144 zimowy <span class="hljs-selector-tag">i</span> od rana ju\u017C sypa\u0142 si\u0119 mi\u0119kki puch zmierzchu matka mia\u0142<span class="hljs-selector-tag">a</span> migren\u0119 <span class="hljs-selector-tag">i</span> le\u017C<span class="hljs-selector-tag">a</span>\u0142<span class="hljs-selector-tag">a</span> na sofie samotnie w salonie
...`}}),Le=new E({props:{code:`from datasets import load_dataset
dataset = load_dataset("audiofolder", data_dir="/path/to/audiofolder", sampling_rate=16_000)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;audiofolder&quot;</span>, data_dir=<span class="hljs-string">&quot;/path/to/audiofolder&quot;</span>, sampling_rate=<span class="hljs-number">16_000</span>)`}}),Fe=new E({props:{code:`from datasets import load_dataset
dataset = load_dataset("audiofolder", data_files="/path/to/audioarchive.zip", sampling_rate=16_000)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;audiofolder&quot;</span>, data_files=<span class="hljs-string">&quot;/path/to/audioarchive.zip&quot;</span>, sampling_rate=<span class="hljs-number">16_000</span>)`}}),Ie=new E({props:{code:'dataset = load_dataset("audiofolder", data_files="https://huggingface.co/datasets/polinaeterna/test_audiofolder/resolve/main/MLSPolishSample.zip", sampling_rate=16_000)',highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;audiofolder&quot;</span>, data_files=<span class="hljs-string">&quot;https://huggingface.co/datasets/polinaeterna/test_audiofolder/resolve/main/MLSPolishSample.zip&quot;</span>, sampling_rate=<span class="hljs-number">16_000</span>)'}}),Ne=new ja({}),Ve=new E({props:{code:'common_voice = common_voice.cast_column("audio", Audio(sampling_rate=16_000))',highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>common_voice = common_voice.cast_column(<span class="hljs-string">&quot;audio&quot;</span>, Audio(sampling_rate=<span class="hljs-number">16_000</span>))'}}),Me=new E({props:{code:'common_voice_train[0]["audio"]',highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>common_voice_train[<span class="hljs-number">0</span>][<span class="hljs-string">&quot;audio&quot;</span>]
{<span class="hljs-string">&#x27;array&#x27;</span>: array([ <span class="hljs-number">0.0000000e+00</span>,  <span class="hljs-number">0.0000000e+00</span>,  <span class="hljs-number">0.0000000e+00</span>, ...,
-<span class="hljs-number">7.4556941e-05</span>, -<span class="hljs-number">1.4621433e-05</span>, -<span class="hljs-number">5.7861507e-05</span>], dtype=float32),
<span class="hljs-string">&#x27;path&#x27;</span>: <span class="hljs-string">&#x27;/root/.cache/huggingface/datasets/downloads/extracted/05be0c29807a73c9b099873d2f5975dae6d05e9f7d577458a2466ecb9a2b0c6b/cv-corpus-6.1-2020-12-11/tr/clips/common_voice_tr_21921195.mp3&#x27;</span>,
<span class="hljs-string">&#x27;sampling_rate&#x27;</span>: <span class="hljs-number">16000</span>}`}}),We=new ja({}),Ke=new E({props:{code:`from transformers import AutoTokenizer, AutoFeatureExtractor, Wav2Vec2Processor
model_checkpoint = "facebook/wav2vec2-large-xlsr-53"
# after defining a vocab.json file you can instantiate a tokenizer object:
tokenizer = AutoTokenizer("./vocab.json", unk_token="[UNK]", pad_token="[PAD]", word_delimiter_token="|")
feature_extractor = AutoFeatureExtractor.from_pretrained(model_checkpoint)
processor = Wav2Vec2Processor.from_pretrained(feature_extractor=feature_extractor, tokenizer=tokenizer)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, AutoFeatureExtractor, Wav2Vec2Processor
<span class="hljs-meta">&gt;&gt;&gt; </span>model_checkpoint = <span class="hljs-string">&quot;facebook/wav2vec2-large-xlsr-53&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># after defining a vocab.json file you can instantiate a tokenizer object:</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer(<span class="hljs-string">&quot;./vocab.json&quot;</span>, unk_token=<span class="hljs-string">&quot;[UNK]&quot;</span>, pad_token=<span class="hljs-string">&quot;[PAD]&quot;</span>, word_delimiter_token=<span class="hljs-string">&quot;|&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>feature_extractor = AutoFeatureExtractor.from_pretrained(model_checkpoint)
<span class="hljs-meta">&gt;&gt;&gt; </span>processor = Wav2Vec2Processor.from_pretrained(feature_extractor=feature_extractor, tokenizer=tokenizer)`}}),Xe=new E({props:{code:`from transformers import Wav2Vec2Processor
processor = Wav2Vec2Processor.from_pretrained("facebook/wav2vec2-base-960h")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> Wav2Vec2Processor
<span class="hljs-meta">&gt;&gt;&gt; </span>processor = Wav2Vec2Processor.from_pretrained(<span class="hljs-string">&quot;facebook/wav2vec2-base-960h&quot;</span>)`}}),Qe=new E({props:{code:`def prepare_dataset(batch):
    audio = batch["audio"]
    batch["input_values"] = processor(audio["array"], sampling_rate=audio["sampling_rate"]).input_values[0]
    batch["input_length"] = len(batch["input_values"])
    with processor.as_target_processor():
        batch["labels"] = processor(batch["sentence"]).input_ids
    return batch
common_voice_train = common_voice_train.map(prepare_dataset, remove_columns=common_voice_train.column_names)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">def</span> <span class="hljs-title function_">prepare_dataset</span>(<span class="hljs-params">batch</span>):
<span class="hljs-meta">... </span>    audio = batch[<span class="hljs-string">&quot;audio&quot;</span>]
<span class="hljs-meta">... </span>    batch[<span class="hljs-string">&quot;input_values&quot;</span>] = processor(audio[<span class="hljs-string">&quot;array&quot;</span>], sampling_rate=audio[<span class="hljs-string">&quot;sampling_rate&quot;</span>]).input_values[<span class="hljs-number">0</span>]
<span class="hljs-meta">... </span>    batch[<span class="hljs-string">&quot;input_length&quot;</span>] = <span class="hljs-built_in">len</span>(batch[<span class="hljs-string">&quot;input_values&quot;</span>])
<span class="hljs-meta">... </span>    <span class="hljs-keyword">with</span> processor.as_target_processor():
<span class="hljs-meta">... </span>        batch[<span class="hljs-string">&quot;labels&quot;</span>] = processor(batch[<span class="hljs-string">&quot;sentence&quot;</span>]).input_ids
<span class="hljs-meta">... </span>    <span class="hljs-keyword">return</span> batch
<span class="hljs-meta">&gt;&gt;&gt; </span>common_voice_train = common_voice_train.<span class="hljs-built_in">map</span>(prepare_dataset, remove_columns=common_voice_train.column_names)`}}),{c(){f=l("meta"),C=c(),y=l("h1"),z=l("a"),P=l("span"),h(b.$$.fragment),x=c(),q=l("span"),m=s("Process audio data"),k=c(),A=l("p"),W=s("\u{1F917} Datasets supports an "),L=l("a"),$e=s("Audio"),R=s(" feature, enabling users to load and process raw audio files for training. This guide will show you how to:"),j=c(),D=l("ul"),ae=l("li"),aa=s("Load your own custom audio dataset."),ta=c(),Ea=l("li"),hs=s("Resample audio files."),_s=c(),ye=l("li"),gs=s("Use "),sa=l("a"),vs=s("Dataset.map()"),ws=s(" with audio files."),nt=c(),U=l("h2"),te=l("a"),za=l("span"),h(be.$$.fragment),$s=c(),xa=l("span"),ys=s("Installation"),it=c(),F=l("p"),bs=s("The "),oa=l("a"),ks=s("Audio"),js=s(" feature should be installed as an extra dependency in \u{1F917} Datasets. Install the "),la=l("a"),Es=s("Audio"),zs=s(" feature (and its dependencies) with pip:"),pt=c(),h(ke.$$.fragment),ct=c(),h(se.$$.fragment),dt=c(),oe=l("p"),xs=s("To support loading audio datasets containing MP3 files, users should additionally install "),je=l("a"),As=s("torchaudio"),qs=s(", so that audio data is handled with high performance."),ut=c(),h(Ee.$$.fragment),ft=c(),h(le.$$.fragment),mt=c(),re=l("p"),Ps=s("Then you can load an audio dataset the same way you would load a text dataset. For example, load the "),ze=l("a"),Ds=s("Common Voice"),Cs=s(" dataset with the Turkish configuration:"),ht=c(),h(xe.$$.fragment),_t=c(),Y=l("h2"),ne=l("a"),Aa=l("span"),h(Ae.$$.fragment),Os=c(),qa=l("span"),Ts=s("Audio datasets"),gt=c(),O=l("p"),Ss=s("Audio datasets commonly have an "),Pa=l("code"),Ls=s("audio"),Fs=s(" and "),Da=l("code"),Is=s("path"),Ns=s(" or "),Ca=l("code"),Hs=s("file"),Vs=s(" column."),vt=c(),qe=l("p"),Oa=l("code"),Ms=s("audio"),Ws=s(" is the actual audio file that is loaded and resampled on-the-fly upon calling it."),wt=c(),h(Pe.$$.fragment),$t=c(),T=l("p"),Rs=s("When you access an audio file, it is automatically decoded and resampled. Generally, you should query an audio file like: "),Ta=l("code"),Us=s('common_voice[0]["audio"]'),Ys=s(". If you query an audio file with "),Sa=l("code"),Gs=s('common_voice["audio"][0]'),Js=s(" instead, "),La=l("strong"),Bs=s("all"),Ks=s(" the audio files in your dataset will be decoded and resampled. This process can take a long time if you have a large dataset."),yt=c(),G=l("p"),Fa=l("code"),Xs=s("path"),Qs=s(" or "),Ia=l("code"),Zs=s("file"),eo=s(" is an absolute path to an audio file."),bt=c(),h(De.$$.fragment),kt=c(),I=l("p"),ao=s("The "),Na=l("code"),to=s("path"),so=s(" is useful if you want to load your own audio dataset. In this case, provide a column of audio file paths to "),ra=l("a"),oo=s("Dataset.cast_column()"),lo=s(":"),jt=c(),h(Ce.$$.fragment),Et=c(),J=l("h2"),ie=l("a"),Ha=l("span"),h(Oe.$$.fragment),ro=c(),Va=l("span"),no=s("AudioFolder"),zt=c(),N=l("p"),io=s("You can load your audio dataset with an "),Ma=l("code"),po=s("AudioFolder"),co=s(` dataset. Your dataset folder / archive should contain audio files and
file with transcriptions (default filename is `),Wa=l("code"),uo=s("transcripts.txt"),fo=s("), for example like this:"),xt=c(),h(Te.$$.fragment),At=c(),pe=l("p"),mo=s("Transcripts file should contain audio id and corresponding text transcription, separated by "),Ra=l("code"),ho=s('"\\t"'),_o=s(" at each line:"),qt=c(),h(Se.$$.fragment),Pt=c(),na=l("p"),go=s("Load a directory:"),Dt=c(),h(Le.$$.fragment),Ct=c(),H=l("p"),vo=s("Load a local "),Ua=l("code"),wo=s(".zip"),$o=s(" archive with the "),Ya=l("code"),yo=s("data_files"),bo=s(" parameter:"),Ot=c(),h(Fe.$$.fragment),Tt=c(),ce=l("p"),ko=s("Load remote datasets from their URLs with the "),Ga=l("code"),jo=s("data_files"),Eo=s(" parameter:"),St=c(),h(Ie.$$.fragment),Lt=c(),de=l("p"),zo=s("Note that "),Ja=l("code"),xo=s("sampling_rate"),Ao=s(" is a required argument."),Ft=c(),B=l("h2"),ue=l("a"),Ba=l("span"),h(Ne.$$.fragment),qo=c(),Ka=l("span"),Po=s("Resample"),It=c(),V=l("p"),Do=s("Some models expect the audio data to have a certain sampling rate due to how the model was pretrained. For example, the "),He=l("a"),Co=s("XLSR-Wav2Vec2"),Oo=s(" model expects the input to have a sampling rate of 16kHz, but an audio file from the Common Voice dataset has a sampling rate of 48kHz. You can use "),ia=l("a"),To=s("Dataset.cast_column()"),So=s(" to downsample the sampling rate to 16kHz:"),Nt=c(),h(Ve.$$.fragment),Ht=c(),fe=l("p"),Lo=s("The next time you load the audio file, the "),pa=l("a"),Fo=s("Audio"),Io=s(" feature will load and resample it to 16kHz:"),Vt=c(),h(Me.$$.fragment),Mt=c(),K=l("div"),ca=l("img"),No=c(),da=l("img"),Wt=c(),X=l("h2"),me=l("a"),Xa=l("span"),h(We.$$.fragment),Ho=c(),Qa=l("span"),Vo=s("Map"),Rt=c(),S=l("p"),Mo=s("Just like text datasets, you can apply a preprocessing function over an entire dataset with "),ua=l("a"),Wo=s("Dataset.map()"),Ro=s(", which is useful for preprocessing all of your audio data at once. Start with a "),Re=l("a"),Uo=s("speech recognition model"),Yo=s(" of your choice, and load a "),Za=l("code"),Go=s("processor"),Jo=s(" object that contains:"),Ut=c(),he=l("ol"),et=l("li"),Q=l("p"),Bo=s("A feature extractor to convert the speech signal to the model\u2019s input format. Every speech recognition model on the \u{1F917} "),Ue=l("a"),Ko=s("Hub"),Xo=s(" contains a predefined feature extractor that can be easily loaded with "),at=l("code"),Qo=s("AutoFeatureExtractor.from_pretrained(...)"),Zo=s("."),el=c(),Ye=l("li"),Z=l("p"),al=s("A tokenizer to convert the model\u2019s output format to text. Fine-tuned speech recognition models, such as "),Ge=l("a"),tl=s("facebook/wav2vec2-base-960h"),sl=s(", contain a predefined tokenizer that can be easily loaded with "),tt=l("code"),ol=s("AutoTokenizer.from_pretrained(...)"),ll=s("."),rl=c(),ee=l("p"),nl=s("For pretrained speech recognition models, such as "),Je=l("a"),il=s("facebook/wav2vec2-large-xlsr-53"),pl=s(", a tokenizer needs to be created from the target text as explained "),Be=l("a"),cl=s("here"),dl=s(". The following example demonstrates how to load a feature extractor, tokenizer and processor for a pretrained speech recognition model:"),Yt=c(),h(Ke.$$.fragment),Gt=c(),_e=l("p"),ul=s("For fine-tuned speech recognition models, you can simply load a predefined "),st=l("code"),fl=s("processor"),ml=s(" object with:"),Jt=c(),h(Xe.$$.fragment),Bt=c(),M=l("p"),hl=s("Make sure to include the "),ot=l("code"),_l=s("audio"),gl=s(" key in your preprocessing function when you call "),fa=l("a"),vl=s("Dataset.map()"),wl=s(" so that you are actually resampling the audio data:"),Kt=c(),h(Qe.$$.fragment),this.h()},l(e){const n=Tr('[data-svelte="svelte-1phssyn"]',document.head);f=r(n,"META",{name:!0,content:!0}),n.forEach(t),C=d(e),y=r(e,"H1",{class:!0});var Ze=i(y);z=r(Ze,"A",{id:!0,class:!0,href:!0});var lt=i(z);P=r(lt,"SPAN",{});var kl=i(P);_(b.$$.fragment,kl),kl.forEach(t),lt.forEach(t),x=d(Ze),q=r(Ze,"SPAN",{});var jl=i(q);m=o(jl,"Process audio data"),jl.forEach(t),Ze.forEach(t),k=d(e),A=r(e,"P",{});var Qt=i(A);W=o(Qt,"\u{1F917} Datasets supports an "),L=r(Qt,"A",{href:!0});var El=i(L);$e=o(El,"Audio"),El.forEach(t),R=o(Qt," feature, enabling users to load and process raw audio files for training. This guide will show you how to:"),Qt.forEach(t),j=d(e),D=r(e,"UL",{});var ma=i(D);ae=r(ma,"LI",{});var zl=i(ae);aa=o(zl,"Load your own custom audio dataset."),zl.forEach(t),ta=d(ma),Ea=r(ma,"LI",{});var xl=i(Ea);hs=o(xl,"Resample audio files."),xl.forEach(t),_s=d(ma),ye=r(ma,"LI",{});var Zt=i(ye);gs=o(Zt,"Use "),sa=r(Zt,"A",{href:!0});var Al=i(sa);vs=o(Al,"Dataset.map()"),Al.forEach(t),ws=o(Zt," with audio files."),Zt.forEach(t),ma.forEach(t),nt=d(e),U=r(e,"H2",{class:!0});var es=i(U);te=r(es,"A",{id:!0,class:!0,href:!0});var ql=i(te);za=r(ql,"SPAN",{});var Pl=i(za);_(be.$$.fragment,Pl),Pl.forEach(t),ql.forEach(t),$s=d(es),xa=r(es,"SPAN",{});var Dl=i(xa);ys=o(Dl,"Installation"),Dl.forEach(t),es.forEach(t),it=d(e),F=r(e,"P",{});var ha=i(F);bs=o(ha,"The "),oa=r(ha,"A",{href:!0});var Cl=i(oa);ks=o(Cl,"Audio"),Cl.forEach(t),js=o(ha," feature should be installed as an extra dependency in \u{1F917} Datasets. Install the "),la=r(ha,"A",{href:!0});var Ol=i(la);Es=o(Ol,"Audio"),Ol.forEach(t),zs=o(ha," feature (and its dependencies) with pip:"),ha.forEach(t),pt=d(e),_(ke.$$.fragment,e),ct=d(e),_(se.$$.fragment,e),dt=d(e),oe=r(e,"P",{});var as=i(oe);xs=o(as,"To support loading audio datasets containing MP3 files, users should additionally install "),je=r(as,"A",{href:!0,rel:!0});var Tl=i(je);As=o(Tl,"torchaudio"),Tl.forEach(t),qs=o(as,", so that audio data is handled with high performance."),as.forEach(t),ut=d(e),_(Ee.$$.fragment,e),ft=d(e),_(le.$$.fragment,e),mt=d(e),re=r(e,"P",{});var ts=i(re);Ps=o(ts,"Then you can load an audio dataset the same way you would load a text dataset. For example, load the "),ze=r(ts,"A",{href:!0,rel:!0});var Sl=i(ze);Ds=o(Sl,"Common Voice"),Sl.forEach(t),Cs=o(ts," dataset with the Turkish configuration:"),ts.forEach(t),ht=d(e),_(xe.$$.fragment,e),_t=d(e),Y=r(e,"H2",{class:!0});var ss=i(Y);ne=r(ss,"A",{id:!0,class:!0,href:!0});var Ll=i(ne);Aa=r(Ll,"SPAN",{});var Fl=i(Aa);_(Ae.$$.fragment,Fl),Fl.forEach(t),Ll.forEach(t),Os=d(ss),qa=r(ss,"SPAN",{});var Il=i(qa);Ts=o(Il,"Audio datasets"),Il.forEach(t),ss.forEach(t),gt=d(e),O=r(e,"P",{});var ge=i(O);Ss=o(ge,"Audio datasets commonly have an "),Pa=r(ge,"CODE",{});var Nl=i(Pa);Ls=o(Nl,"audio"),Nl.forEach(t),Fs=o(ge," and "),Da=r(ge,"CODE",{});var Hl=i(Da);Is=o(Hl,"path"),Hl.forEach(t),Ns=o(ge," or "),Ca=r(ge,"CODE",{});var Vl=i(Ca);Hs=o(Vl,"file"),Vl.forEach(t),Vs=o(ge," column."),ge.forEach(t),vt=d(e),qe=r(e,"P",{});var $l=i(qe);Oa=r($l,"CODE",{});var Ml=i(Oa);Ms=o(Ml,"audio"),Ml.forEach(t),Ws=o($l," is the actual audio file that is loaded and resampled on-the-fly upon calling it."),$l.forEach(t),wt=d(e),_(Pe.$$.fragment,e),$t=d(e),T=r(e,"P",{});var ve=i(T);Rs=o(ve,"When you access an audio file, it is automatically decoded and resampled. Generally, you should query an audio file like: "),Ta=r(ve,"CODE",{});var Wl=i(Ta);Us=o(Wl,'common_voice[0]["audio"]'),Wl.forEach(t),Ys=o(ve,". If you query an audio file with "),Sa=r(ve,"CODE",{});var Rl=i(Sa);Gs=o(Rl,'common_voice["audio"][0]'),Rl.forEach(t),Js=o(ve," instead, "),La=r(ve,"STRONG",{});var Ul=i(La);Bs=o(Ul,"all"),Ul.forEach(t),Ks=o(ve," the audio files in your dataset will be decoded and resampled. This process can take a long time if you have a large dataset."),ve.forEach(t),yt=d(e),G=r(e,"P",{});var rt=i(G);Fa=r(rt,"CODE",{});var Yl=i(Fa);Xs=o(Yl,"path"),Yl.forEach(t),Qs=o(rt," or "),Ia=r(rt,"CODE",{});var Gl=i(Ia);Zs=o(Gl,"file"),Gl.forEach(t),eo=o(rt," is an absolute path to an audio file."),rt.forEach(t),bt=d(e),_(De.$$.fragment,e),kt=d(e),I=r(e,"P",{});var _a=i(I);ao=o(_a,"The "),Na=r(_a,"CODE",{});var Jl=i(Na);to=o(Jl,"path"),Jl.forEach(t),so=o(_a," is useful if you want to load your own audio dataset. In this case, provide a column of audio file paths to "),ra=r(_a,"A",{href:!0});var Bl=i(ra);oo=o(Bl,"Dataset.cast_column()"),Bl.forEach(t),lo=o(_a,":"),_a.forEach(t),jt=d(e),_(Ce.$$.fragment,e),Et=d(e),J=r(e,"H2",{class:!0});var os=i(J);ie=r(os,"A",{id:!0,class:!0,href:!0});var Kl=i(ie);Ha=r(Kl,"SPAN",{});var Xl=i(Ha);_(Oe.$$.fragment,Xl),Xl.forEach(t),Kl.forEach(t),ro=d(os),Va=r(os,"SPAN",{});var Ql=i(Va);no=o(Ql,"AudioFolder"),Ql.forEach(t),os.forEach(t),zt=d(e),N=r(e,"P",{});var ga=i(N);io=o(ga,"You can load your audio dataset with an "),Ma=r(ga,"CODE",{});var Zl=i(Ma);po=o(Zl,"AudioFolder"),Zl.forEach(t),co=o(ga,` dataset. Your dataset folder / archive should contain audio files and
file with transcriptions (default filename is `),Wa=r(ga,"CODE",{});var er=i(Wa);uo=o(er,"transcripts.txt"),er.forEach(t),fo=o(ga,"), for example like this:"),ga.forEach(t),xt=d(e),_(Te.$$.fragment,e),At=d(e),pe=r(e,"P",{});var ls=i(pe);mo=o(ls,"Transcripts file should contain audio id and corresponding text transcription, separated by "),Ra=r(ls,"CODE",{});var ar=i(Ra);ho=o(ar,'"\\t"'),ar.forEach(t),_o=o(ls," at each line:"),ls.forEach(t),qt=d(e),_(Se.$$.fragment,e),Pt=d(e),na=r(e,"P",{});var tr=i(na);go=o(tr,"Load a directory:"),tr.forEach(t),Dt=d(e),_(Le.$$.fragment,e),Ct=d(e),H=r(e,"P",{});var va=i(H);vo=o(va,"Load a local "),Ua=r(va,"CODE",{});var sr=i(Ua);wo=o(sr,".zip"),sr.forEach(t),$o=o(va," archive with the "),Ya=r(va,"CODE",{});var or=i(Ya);yo=o(or,"data_files"),or.forEach(t),bo=o(va," parameter:"),va.forEach(t),Ot=d(e),_(Fe.$$.fragment,e),Tt=d(e),ce=r(e,"P",{});var rs=i(ce);ko=o(rs,"Load remote datasets from their URLs with the "),Ga=r(rs,"CODE",{});var lr=i(Ga);jo=o(lr,"data_files"),lr.forEach(t),Eo=o(rs," parameter:"),rs.forEach(t),St=d(e),_(Ie.$$.fragment,e),Lt=d(e),de=r(e,"P",{});var ns=i(de);zo=o(ns,"Note that "),Ja=r(ns,"CODE",{});var rr=i(Ja);xo=o(rr,"sampling_rate"),rr.forEach(t),Ao=o(ns," is a required argument."),ns.forEach(t),Ft=d(e),B=r(e,"H2",{class:!0});var is=i(B);ue=r(is,"A",{id:!0,class:!0,href:!0});var nr=i(ue);Ba=r(nr,"SPAN",{});var ir=i(Ba);_(Ne.$$.fragment,ir),ir.forEach(t),nr.forEach(t),qo=d(is),Ka=r(is,"SPAN",{});var pr=i(Ka);Po=o(pr,"Resample"),pr.forEach(t),is.forEach(t),It=d(e),V=r(e,"P",{});var wa=i(V);Do=o(wa,"Some models expect the audio data to have a certain sampling rate due to how the model was pretrained. For example, the "),He=r(wa,"A",{href:!0,rel:!0});var cr=i(He);Co=o(cr,"XLSR-Wav2Vec2"),cr.forEach(t),Oo=o(wa," model expects the input to have a sampling rate of 16kHz, but an audio file from the Common Voice dataset has a sampling rate of 48kHz. You can use "),ia=r(wa,"A",{href:!0});var dr=i(ia);To=o(dr,"Dataset.cast_column()"),dr.forEach(t),So=o(wa," to downsample the sampling rate to 16kHz:"),wa.forEach(t),Nt=d(e),_(Ve.$$.fragment,e),Ht=d(e),fe=r(e,"P",{});var ps=i(fe);Lo=o(ps,"The next time you load the audio file, the "),pa=r(ps,"A",{href:!0});var ur=i(pa);Fo=o(ur,"Audio"),ur.forEach(t),Io=o(ps," feature will load and resample it to 16kHz:"),ps.forEach(t),Vt=d(e),_(Me.$$.fragment,e),Mt=d(e),K=r(e,"DIV",{class:!0});var cs=i(K);ca=r(cs,"IMG",{class:!0,src:!0}),No=d(cs),da=r(cs,"IMG",{class:!0,src:!0}),cs.forEach(t),Wt=d(e),X=r(e,"H2",{class:!0});var ds=i(X);me=r(ds,"A",{id:!0,class:!0,href:!0});var fr=i(me);Xa=r(fr,"SPAN",{});var mr=i(Xa);_(We.$$.fragment,mr),mr.forEach(t),fr.forEach(t),Ho=d(ds),Qa=r(ds,"SPAN",{});var hr=i(Qa);Vo=o(hr,"Map"),hr.forEach(t),ds.forEach(t),Rt=d(e),S=r(e,"P",{});var we=i(S);Mo=o(we,"Just like text datasets, you can apply a preprocessing function over an entire dataset with "),ua=r(we,"A",{href:!0});var _r=i(ua);Wo=o(_r,"Dataset.map()"),_r.forEach(t),Ro=o(we,", which is useful for preprocessing all of your audio data at once. Start with a "),Re=r(we,"A",{href:!0,rel:!0});var gr=i(Re);Uo=o(gr,"speech recognition model"),gr.forEach(t),Yo=o(we," of your choice, and load a "),Za=r(we,"CODE",{});var vr=i(Za);Go=o(vr,"processor"),vr.forEach(t),Jo=o(we," object that contains:"),we.forEach(t),Ut=d(e),he=r(e,"OL",{});var us=i(he);et=r(us,"LI",{});var wr=i(et);Q=r(wr,"P",{});var $a=i(Q);Bo=o($a,"A feature extractor to convert the speech signal to the model\u2019s input format. Every speech recognition model on the \u{1F917} "),Ue=r($a,"A",{href:!0,rel:!0});var $r=i(Ue);Ko=o($r,"Hub"),$r.forEach(t),Xo=o($a," contains a predefined feature extractor that can be easily loaded with "),at=r($a,"CODE",{});var yr=i(at);Qo=o(yr,"AutoFeatureExtractor.from_pretrained(...)"),yr.forEach(t),Zo=o($a,"."),$a.forEach(t),wr.forEach(t),el=d(us),Ye=r(us,"LI",{});var fs=i(Ye);Z=r(fs,"P",{});var ya=i(Z);al=o(ya,"A tokenizer to convert the model\u2019s output format to text. Fine-tuned speech recognition models, such as "),Ge=r(ya,"A",{href:!0,rel:!0});var br=i(Ge);tl=o(br,"facebook/wav2vec2-base-960h"),br.forEach(t),sl=o(ya,", contain a predefined tokenizer that can be easily loaded with "),tt=r(ya,"CODE",{});var kr=i(tt);ol=o(kr,"AutoTokenizer.from_pretrained(...)"),kr.forEach(t),ll=o(ya,"."),ya.forEach(t),rl=d(fs),ee=r(fs,"P",{});var ba=i(ee);nl=o(ba,"For pretrained speech recognition models, such as "),Je=r(ba,"A",{href:!0,rel:!0});var jr=i(Je);il=o(jr,"facebook/wav2vec2-large-xlsr-53"),jr.forEach(t),pl=o(ba,", a tokenizer needs to be created from the target text as explained "),Be=r(ba,"A",{href:!0,rel:!0});var Er=i(Be);cl=o(Er,"here"),Er.forEach(t),dl=o(ba,". The following example demonstrates how to load a feature extractor, tokenizer and processor for a pretrained speech recognition model:"),ba.forEach(t),fs.forEach(t),us.forEach(t),Yt=d(e),_(Ke.$$.fragment,e),Gt=d(e),_e=r(e,"P",{});var ms=i(_e);ul=o(ms,"For fine-tuned speech recognition models, you can simply load a predefined "),st=r(ms,"CODE",{});var zr=i(st);fl=o(zr,"processor"),zr.forEach(t),ml=o(ms," object with:"),ms.forEach(t),Jt=d(e),_(Xe.$$.fragment,e),Bt=d(e),M=r(e,"P",{});var ka=i(M);hl=o(ka,"Make sure to include the "),ot=r(ka,"CODE",{});var xr=i(ot);_l=o(xr,"audio"),xr.forEach(t),gl=o(ka," key in your preprocessing function when you call "),fa=r(ka,"A",{href:!0});var Ar=i(fa);vl=o(Ar,"Dataset.map()"),Ar.forEach(t),wl=o(ka," so that you are actually resampling the audio data:"),ka.forEach(t),Kt=d(e),_(Qe.$$.fragment,e),this.h()},h(){u(f,"name","hf:doc:metadata"),u(f,"content",JSON.stringify(Hr)),u(z,"id","process-audio-data"),u(z,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(z,"href","#process-audio-data"),u(y,"class","relative group"),u(L,"href","/docs/datasets/pr_3963/en/package_reference/main_classes#datasets.Audio"),u(sa,"href","/docs/datasets/pr_3963/en/package_reference/main_classes#datasets.Dataset.map"),u(te,"id","installation"),u(te,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(te,"href","#installation"),u(U,"class","relative group"),u(oa,"href","/docs/datasets/pr_3963/en/package_reference/main_classes#datasets.Audio"),u(la,"href","/docs/datasets/pr_3963/en/package_reference/main_classes#datasets.Audio"),u(je,"href","https://pytorch.org/audio/stable/index.html"),u(je,"rel","nofollow"),u(ze,"href","https://huggingface.co/datasets/common_voice"),u(ze,"rel","nofollow"),u(ne,"id","audio-datasets"),u(ne,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(ne,"href","#audio-datasets"),u(Y,"class","relative group"),u(ra,"href","/docs/datasets/pr_3963/en/package_reference/main_classes#datasets.Dataset.cast_column"),u(ie,"id","audiofolder"),u(ie,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(ie,"href","#audiofolder"),u(J,"class","relative group"),u(ue,"id","resample"),u(ue,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(ue,"href","#resample"),u(B,"class","relative group"),u(He,"href","https://huggingface.co/facebook/wav2vec2-large-xlsr-53"),u(He,"rel","nofollow"),u(ia,"href","/docs/datasets/pr_3963/en/package_reference/main_classes#datasets.Dataset.cast_column"),u(pa,"href","/docs/datasets/pr_3963/en/package_reference/main_classes#datasets.Audio"),u(ca,"class","block dark:hidden"),qr(ca.src,yl="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/datasets/resample.gif")||u(ca,"src",yl),u(da,"class","hidden dark:block"),qr(da.src,bl="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/datasets/resample-dark.gif")||u(da,"src",bl),u(K,"class","flex justify-center"),u(me,"id","map"),u(me,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(me,"href","#map"),u(X,"class","relative group"),u(ua,"href","/docs/datasets/pr_3963/en/package_reference/main_classes#datasets.Dataset.map"),u(Re,"href","https://huggingface.co/models?pipeline_tag=automatic-speech-recognition&sort=downloads"),u(Re,"rel","nofollow"),u(Ue,"href","https://huggingface.co/models?pipeline_tag=automatic-speech-recognition&sort=downloads"),u(Ue,"rel","nofollow"),u(Ge,"href","https://huggingface.co/facebook/wav2vec2-base-960h"),u(Ge,"rel","nofollow"),u(Je,"href","https://huggingface.co/facebook/wav2vec2-large-xlsr-53"),u(Je,"rel","nofollow"),u(Be,"href","https://huggingface.co/blog/fine-tune-wav2vec2-english"),u(Be,"rel","nofollow"),u(fa,"href","/docs/datasets/pr_3963/en/package_reference/main_classes#datasets.Dataset.map")},m(e,n){a(document.head,f),p(e,C,n),p(e,y,n),a(y,z),a(z,P),g(b,P,null),a(y,x),a(y,q),a(q,m),p(e,k,n),p(e,A,n),a(A,W),a(A,L),a(L,$e),a(A,R),p(e,j,n),p(e,D,n),a(D,ae),a(ae,aa),a(D,ta),a(D,Ea),a(Ea,hs),a(D,_s),a(D,ye),a(ye,gs),a(ye,sa),a(sa,vs),a(ye,ws),p(e,nt,n),p(e,U,n),a(U,te),a(te,za),g(be,za,null),a(U,$s),a(U,xa),a(xa,ys),p(e,it,n),p(e,F,n),a(F,bs),a(F,oa),a(oa,ks),a(F,js),a(F,la),a(la,Es),a(F,zs),p(e,pt,n),g(ke,e,n),p(e,ct,n),g(se,e,n),p(e,dt,n),p(e,oe,n),a(oe,xs),a(oe,je),a(je,As),a(oe,qs),p(e,ut,n),g(Ee,e,n),p(e,ft,n),g(le,e,n),p(e,mt,n),p(e,re,n),a(re,Ps),a(re,ze),a(ze,Ds),a(re,Cs),p(e,ht,n),g(xe,e,n),p(e,_t,n),p(e,Y,n),a(Y,ne),a(ne,Aa),g(Ae,Aa,null),a(Y,Os),a(Y,qa),a(qa,Ts),p(e,gt,n),p(e,O,n),a(O,Ss),a(O,Pa),a(Pa,Ls),a(O,Fs),a(O,Da),a(Da,Is),a(O,Ns),a(O,Ca),a(Ca,Hs),a(O,Vs),p(e,vt,n),p(e,qe,n),a(qe,Oa),a(Oa,Ms),a(qe,Ws),p(e,wt,n),g(Pe,e,n),p(e,$t,n),p(e,T,n),a(T,Rs),a(T,Ta),a(Ta,Us),a(T,Ys),a(T,Sa),a(Sa,Gs),a(T,Js),a(T,La),a(La,Bs),a(T,Ks),p(e,yt,n),p(e,G,n),a(G,Fa),a(Fa,Xs),a(G,Qs),a(G,Ia),a(Ia,Zs),a(G,eo),p(e,bt,n),g(De,e,n),p(e,kt,n),p(e,I,n),a(I,ao),a(I,Na),a(Na,to),a(I,so),a(I,ra),a(ra,oo),a(I,lo),p(e,jt,n),g(Ce,e,n),p(e,Et,n),p(e,J,n),a(J,ie),a(ie,Ha),g(Oe,Ha,null),a(J,ro),a(J,Va),a(Va,no),p(e,zt,n),p(e,N,n),a(N,io),a(N,Ma),a(Ma,po),a(N,co),a(N,Wa),a(Wa,uo),a(N,fo),p(e,xt,n),g(Te,e,n),p(e,At,n),p(e,pe,n),a(pe,mo),a(pe,Ra),a(Ra,ho),a(pe,_o),p(e,qt,n),g(Se,e,n),p(e,Pt,n),p(e,na,n),a(na,go),p(e,Dt,n),g(Le,e,n),p(e,Ct,n),p(e,H,n),a(H,vo),a(H,Ua),a(Ua,wo),a(H,$o),a(H,Ya),a(Ya,yo),a(H,bo),p(e,Ot,n),g(Fe,e,n),p(e,Tt,n),p(e,ce,n),a(ce,ko),a(ce,Ga),a(Ga,jo),a(ce,Eo),p(e,St,n),g(Ie,e,n),p(e,Lt,n),p(e,de,n),a(de,zo),a(de,Ja),a(Ja,xo),a(de,Ao),p(e,Ft,n),p(e,B,n),a(B,ue),a(ue,Ba),g(Ne,Ba,null),a(B,qo),a(B,Ka),a(Ka,Po),p(e,It,n),p(e,V,n),a(V,Do),a(V,He),a(He,Co),a(V,Oo),a(V,ia),a(ia,To),a(V,So),p(e,Nt,n),g(Ve,e,n),p(e,Ht,n),p(e,fe,n),a(fe,Lo),a(fe,pa),a(pa,Fo),a(fe,Io),p(e,Vt,n),g(Me,e,n),p(e,Mt,n),p(e,K,n),a(K,ca),a(K,No),a(K,da),p(e,Wt,n),p(e,X,n),a(X,me),a(me,Xa),g(We,Xa,null),a(X,Ho),a(X,Qa),a(Qa,Vo),p(e,Rt,n),p(e,S,n),a(S,Mo),a(S,ua),a(ua,Wo),a(S,Ro),a(S,Re),a(Re,Uo),a(S,Yo),a(S,Za),a(Za,Go),a(S,Jo),p(e,Ut,n),p(e,he,n),a(he,et),a(et,Q),a(Q,Bo),a(Q,Ue),a(Ue,Ko),a(Q,Xo),a(Q,at),a(at,Qo),a(Q,Zo),a(he,el),a(he,Ye),a(Ye,Z),a(Z,al),a(Z,Ge),a(Ge,tl),a(Z,sl),a(Z,tt),a(tt,ol),a(Z,ll),a(Ye,rl),a(Ye,ee),a(ee,nl),a(ee,Je),a(Je,il),a(ee,pl),a(ee,Be),a(Be,cl),a(ee,dl),p(e,Yt,n),g(Ke,e,n),p(e,Gt,n),p(e,_e,n),a(_e,ul),a(_e,st),a(st,fl),a(_e,ml),p(e,Jt,n),g(Xe,e,n),p(e,Bt,n),p(e,M,n),a(M,hl),a(M,ot),a(ot,_l),a(M,gl),a(M,fa),a(fa,vl),a(M,wl),p(e,Kt,n),g(Qe,e,n),Xt=!0},p(e,[n]){const Ze={};n&2&&(Ze.$$scope={dirty:n,ctx:e}),se.$set(Ze);const lt={};n&2&&(lt.$$scope={dirty:n,ctx:e}),le.$set(lt)},i(e){Xt||(v(b.$$.fragment,e),v(be.$$.fragment,e),v(ke.$$.fragment,e),v(se.$$.fragment,e),v(Ee.$$.fragment,e),v(le.$$.fragment,e),v(xe.$$.fragment,e),v(Ae.$$.fragment,e),v(Pe.$$.fragment,e),v(De.$$.fragment,e),v(Ce.$$.fragment,e),v(Oe.$$.fragment,e),v(Te.$$.fragment,e),v(Se.$$.fragment,e),v(Le.$$.fragment,e),v(Fe.$$.fragment,e),v(Ie.$$.fragment,e),v(Ne.$$.fragment,e),v(Ve.$$.fragment,e),v(Me.$$.fragment,e),v(We.$$.fragment,e),v(Ke.$$.fragment,e),v(Xe.$$.fragment,e),v(Qe.$$.fragment,e),Xt=!0)},o(e){w(b.$$.fragment,e),w(be.$$.fragment,e),w(ke.$$.fragment,e),w(se.$$.fragment,e),w(Ee.$$.fragment,e),w(le.$$.fragment,e),w(xe.$$.fragment,e),w(Ae.$$.fragment,e),w(Pe.$$.fragment,e),w(De.$$.fragment,e),w(Ce.$$.fragment,e),w(Oe.$$.fragment,e),w(Te.$$.fragment,e),w(Se.$$.fragment,e),w(Le.$$.fragment,e),w(Fe.$$.fragment,e),w(Ie.$$.fragment,e),w(Ne.$$.fragment,e),w(Ve.$$.fragment,e),w(Me.$$.fragment,e),w(We.$$.fragment,e),w(Ke.$$.fragment,e),w(Xe.$$.fragment,e),w(Qe.$$.fragment,e),Xt=!1},d(e){t(f),e&&t(C),e&&t(y),$(b),e&&t(k),e&&t(A),e&&t(j),e&&t(D),e&&t(nt),e&&t(U),$(be),e&&t(it),e&&t(F),e&&t(pt),$(ke,e),e&&t(ct),$(se,e),e&&t(dt),e&&t(oe),e&&t(ut),$(Ee,e),e&&t(ft),$(le,e),e&&t(mt),e&&t(re),e&&t(ht),$(xe,e),e&&t(_t),e&&t(Y),$(Ae),e&&t(gt),e&&t(O),e&&t(vt),e&&t(qe),e&&t(wt),$(Pe,e),e&&t($t),e&&t(T),e&&t(yt),e&&t(G),e&&t(bt),$(De,e),e&&t(kt),e&&t(I),e&&t(jt),$(Ce,e),e&&t(Et),e&&t(J),$(Oe),e&&t(zt),e&&t(N),e&&t(xt),$(Te,e),e&&t(At),e&&t(pe),e&&t(qt),$(Se,e),e&&t(Pt),e&&t(na),e&&t(Dt),$(Le,e),e&&t(Ct),e&&t(H),e&&t(Ot),$(Fe,e),e&&t(Tt),e&&t(ce),e&&t(St),$(Ie,e),e&&t(Lt),e&&t(de),e&&t(Ft),e&&t(B),$(Ne),e&&t(It),e&&t(V),e&&t(Nt),$(Ve,e),e&&t(Ht),e&&t(fe),e&&t(Vt),$(Me,e),e&&t(Mt),e&&t(K),e&&t(Wt),e&&t(X),$(We),e&&t(Rt),e&&t(S),e&&t(Ut),e&&t(he),e&&t(Yt),$(Ke,e),e&&t(Gt),e&&t(_e),e&&t(Jt),$(Xe,e),e&&t(Bt),e&&t(M),e&&t(Kt),$(Qe,e)}}}const Hr={local:"process-audio-data",sections:[{local:"installation",title:"Installation"},{local:"audio-datasets",title:"Audio datasets"},{local:"audiofolder",title:"AudioFolder"},{local:"resample",title:"Resample"},{local:"map",title:"Map"}],title:"Process audio data"};function Vr(ea){return Sr(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Yr extends Dr{constructor(f){super();Cr(this,f,Vr,Nr,Or,{})}}export{Yr as default,Hr as metadata};
