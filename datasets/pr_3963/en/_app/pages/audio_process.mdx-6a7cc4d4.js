import{S as Ir,i as Hr,s as Vr,e as l,k as d,w as h,t as s,M as Wr,c as r,d as t,m as u,a as n,x as m,h as o,b as c,N as Nr,F as a,g as p,y as _,q as g,o as v,B as w,v as Mr}from"../chunks/vendor-aa873a46.js";import{T as Rr}from"../chunks/Tip-f7f252ab.js";import{I as za}from"../chunks/IconCopyLink-d0ca3106.js";import{C as b}from"../chunks/CodeBlock-1f14baf3.js";function Ur(pt){let f,ee,$,E,I,j,ve,H,V,ae,z,W,P,we;return{c(){f=l("p"),ee=s("torchaudio\u2019s "),$=l("code"),E=s("sox_io"),I=d(),j=l("a"),ve=s("backend"),H=s(" supports decoding "),V=l("code"),ae=s("mp3"),z=s(" files. Unfortunately, the "),W=l("code"),P=s("sox_io"),we=s(" backend is only available on Linux/macOS, and is not supported by Windows."),this.h()},l(M){f=r(M,"P",{});var y=n(f);ee=o(y,"torchaudio\u2019s "),$=r(y,"CODE",{});var k=n($);E=o(k,"sox_io"),k.forEach(t),I=u(y),j=r(y,"A",{href:!0,rel:!0});var te=n(j);ve=o(te,"backend"),te.forEach(t),H=o(y," supports decoding "),V=r(y,"CODE",{});var ea=n(V);ae=o(ea,"mp3"),ea.forEach(t),z=o(y," files. Unfortunately, the "),W=r(y,"CODE",{});var aa=n(W);P=o(aa,"sox_io"),aa.forEach(t),we=o(y," backend is only available on Linux/macOS, and is not supported by Windows."),y.forEach(t),this.h()},h(){c(j,"href","https://pytorch.org/audio/stable/backend.html#"),c(j,"rel","nofollow")},m(M,y){p(M,f,y),a(f,ee),a(f,$),a($,E),a(f,I),a(f,j),a(j,ve),a(f,H),a(f,V),a(V,ae),a(f,z),a(f,W),a(W,P),a(f,we)},d(M){M&&t(f)}}}function Gr(pt){let f,ee,$,E,I,j,ve,H,V,ae,z,W,P,we,M,y,k,te,ea,aa,xa,ms,_s,ye,gs,ta,vs,ws,ct,R,se,Aa,be,ys,qa,bs,dt,D,$s,sa,js,ks,oa,Es,zs,ut,$e,ft,C,xs,je,As,qs,ke,Ps,Ds,ht,Ee,mt,oe,_t,le,Cs,ze,Os,Ss,gt,xe,vt,U,re,Pa,Ae,Ts,Da,Ls,wt,x,Fs,Ca,Ns,Is,Oa,Hs,Vs,Sa,Ws,Ms,yt,qe,Ta,Rs,Us,bt,Pe,$t,A,Gs,La,Ys,Js,Fa,Bs,Ks,Na,Xs,Qs,jt,G,Ia,Zs,eo,Ha,ao,to,kt,De,Et,O,so,Va,oo,lo,la,ro,no,zt,Ce,xt,Y,ne,Wa,Oe,io,Ma,po,At,S,co,Ra,uo,fo,Ua,ho,mo,qt,Se,Pt,ie,_o,ra,go,Ga,vo,wo,Dt,Te,Ct,na,yo,Ot,Le,St,T,bo,Ya,$o,jo,Ja,ko,Eo,Tt,Fe,Lt,pe,zo,Ba,xo,Ao,Ft,Ne,Nt,L,qo,Ka,Po,Do,Xa,Co,Oo,It,J,ce,Qa,Ie,So,Za,To,Ht,F,Lo,He,Fo,No,ia,Io,Ho,Vt,Ve,Wt,de,Vo,pa,Wo,Mo,Mt,We,Rt,B,ca,xl,Ro,da,Al,Ut,K,ue,et,Me,Uo,at,Go,Gt,q,Yo,ua,Jo,Bo,Re,Ko,Xo,tt,Qo,Zo,Yt,fe,st,X,el,Ue,al,tl,ot,sl,ol,ll,Ge,Q,rl,Ye,nl,il,lt,pl,cl,dl,Z,ul,Je,fl,hl,Be,ml,_l,Jt,Ke,Bt,he,gl,rt,vl,wl,Kt,Xe,Xt,N,yl,nt,bl,$l,fa,jl,kl,Qt,Qe,Zt;return j=new za({}),be=new za({}),$e=new b({props:{code:"pip install datasets[audio]",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>pip install datasets[audio]'}}),Ee=new b({props:{code:`pip install librosa
pip install torchaudio`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>pip install librosa
<span class="hljs-meta">&gt;&gt;&gt; </span>pip install torchaudio`}}),oe=new Rr({props:{warning:!0,$$slots:{default:[Ur]},$$scope:{ctx:pt}}}),xe=new b({props:{code:`from datasets import load_dataset, load_metric, Audio
common_voice = load_dataset("common_voice", "tr", split="train")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset, load_metric, Audio
<span class="hljs-meta">&gt;&gt;&gt; </span>common_voice = load_dataset(<span class="hljs-string">&quot;common_voice&quot;</span>, <span class="hljs-string">&quot;tr&quot;</span>, split=<span class="hljs-string">&quot;train&quot;</span>)`}}),Ae=new za({}),Pe=new b({props:{code:'common_voice[0]["audio"]',highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>common_voice[<span class="hljs-number">0</span>][<span class="hljs-string">&quot;audio&quot;</span>]
{<span class="hljs-string">&#x27;array&#x27;</span>: array([ <span class="hljs-number">0.0000000e+00</span>,  <span class="hljs-number">0.0000000e+00</span>,  <span class="hljs-number">0.0000000e+00</span>, ...,
    -<span class="hljs-number">8.8930130e-05</span>, -<span class="hljs-number">3.8027763e-05</span>, -<span class="hljs-number">2.9146671e-05</span>], dtype=float32),
<span class="hljs-string">&#x27;path&#x27;</span>: <span class="hljs-string">&#x27;/root/.cache/huggingface/datasets/downloads/extracted/05be0c29807a73c9b099873d2f5975dae6d05e9f7d577458a2466ecb9a2b0c6b/cv-corpus-6.1-2020-12-11/tr/clips/common_voice_tr_21921195.mp3&#x27;</span>,
<span class="hljs-string">&#x27;sampling_rate&#x27;</span>: <span class="hljs-number">48000</span>}`}}),De=new b({props:{code:'common_voice[0]["path"]',highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>common_voice[<span class="hljs-number">0</span>][<span class="hljs-string">&quot;path&quot;</span>]
/root/.cache/huggingface/datasets/downloads/extracted/05be0c29807a73c9b099873d2f5975dae6d05e9f7d577458a2466ecb9a2b0c6b/cv-corpus-<span class="hljs-number">6.1</span>-<span class="hljs-number">2020</span>-<span class="hljs-number">12</span>-<span class="hljs-number">11</span>/tr/clips/common_voice_tr_21921195.mp3`}}),Ce=new b({props:{code:'my_audio_dataset = my_audio_dataset.cast_column("paths_to_my_audio_files", Audio())',highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>my_audio_dataset = my_audio_dataset.cast_column(<span class="hljs-string">&quot;paths_to_my_audio_files&quot;</span>, Audio())'}}),Oe=new za({}),Se=new b({props:{code:`audio/1889_1447_000071.flac
audio/1889_1447_000096.flac
audio/1889_1447_000063.flac
audio/1889_1447_000097.flac
...
transcripts.txt`,highlighted:`<span class="hljs-selector-tag">audio</span>/<span class="hljs-number">1889</span>_1447_000071<span class="hljs-selector-class">.flac</span>
<span class="hljs-selector-tag">audio</span>/<span class="hljs-number">1889</span>_1447_000096<span class="hljs-selector-class">.flac</span>
<span class="hljs-selector-tag">audio</span>/<span class="hljs-number">1889</span>_1447_000063<span class="hljs-selector-class">.flac</span>
<span class="hljs-selector-tag">audio</span>/<span class="hljs-number">1889</span>_1447_000097<span class="hljs-selector-class">.flac</span>
...
transcripts<span class="hljs-selector-class">.txt</span>`}}),Te=new b({props:{code:`1889_1447_000071	stali\u015Bmy wszyscy bezradni wobec tej szalej\u0105cej furii z\u0142o\u015Bci kt\xF3ra sama siebie trawi\u0142a i po\u017Cera\u0142a z ubolewaniem patrzyli\u015Bmy na smutny przebieg tego paroksyzmu i z pewn\u0105 ulg\u0105 wr\xF3cili\u015Bmy do naszych zaj\u0119\u0107 gdy \u017Ca\u0142osny ten proces dobieg\u0142 swego naturalnego ko\u0144ca
1889_1447_000096	nawet w obecno\u015Bci matki le\u017C\u0105cej z zawi\u0105zan\u0105 g\u0142ow\u0105 na sofie nie mog\u0142y si\u0119 powstrzyma\u0107 robi\u0142y perskie oczko dawa\u0142y sobie znaki m\xF3wi\u0142y niemym kolorowym alfabetem pe\u0142nym sekretnych znacze\u0144 irytowa\u0142o mnie to szydercze porozumienie ta migotliwa zmowa poza mymi plecami
1889_1447_000063	stali\u015Bmy pod drzwiami i nas\u0142uchiwali w lamentach wichru dawa\u0142y si\u0119 s\u0142ysze\u0107 wszelkie g\u0142osy perswazje nawo\u0142ywania i gaw\u0119dy zdawa\u0142o si\u0119 nam \u017Ce s\u0142yszymy wo\u0142anie o pomoc ojca zb\u0142\u0105kanego w wichurze to znowu \u017Ce brat z teodorem gwarz\u0105 beztrosko pod drzwiami
1889_1447_000097	owego dnia by\u0142 ci\u0119\u017Cki dzie\u0144 zimowy i od rana ju\u017C sypa\u0142 si\u0119 mi\u0119kki puch zmierzchu matka mia\u0142a migren\u0119 i le\u017Ca\u0142a na sofie samotnie w salonie
...`,highlighted:`<span class="hljs-number">1889</span>_1447_000071	stali\u015Bmy wszyscy bezradni wobec tej szalej\u0105cej furii z\u0142o\u015Bci kt\xF3ra sama siebie trawi\u0142<span class="hljs-selector-tag">a</span> <span class="hljs-selector-tag">i</span> po\u017Cera\u0142<span class="hljs-selector-tag">a</span> z ubolewaniem patrzyli\u015Bmy na smutny przebieg tego paroksyzmu <span class="hljs-selector-tag">i</span> z pewn\u0105 ulg\u0105 wr\xF3cili\u015Bmy do naszych zaj\u0119\u0107 gdy \u017C<span class="hljs-selector-tag">a</span>\u0142osny ten proces dobieg\u0142 swego naturalnego ko\u0144ca
<span class="hljs-number">1889</span>_1447_000096	nawet w obecno\u015Bci matki le\u017C\u0105cej z zawi\u0105zan\u0105 g\u0142ow\u0105 na sofie nie mog\u0142y si\u0119 powstrzyma\u0107 robi\u0142y perskie oczko dawa\u0142y sobie znaki m\xF3wi\u0142y niemym kolorowym alfabetem pe\u0142nym sekretnych znacze\u0144 irytowa\u0142o mnie <span class="hljs-selector-tag">to</span> szydercze porozumienie ta migotliwa zmowa poza mymi plecami
<span class="hljs-number">1889</span>_1447_000063	stali\u015Bmy pod drzwiami <span class="hljs-selector-tag">i</span> nas\u0142uchiwali w lamentach wichru dawa\u0142y si\u0119 s\u0142ysze\u0107 wszelkie g\u0142osy perswazje nawo\u0142ywania <span class="hljs-selector-tag">i</span> gaw\u0119dy zdawa\u0142o si\u0119 nam \u017Ce s\u0142yszymy wo\u0142anie o pomoc ojca zb\u0142\u0105kanego w wichurze <span class="hljs-selector-tag">to</span> znowu \u017Ce brat z teodorem gwarz\u0105 beztrosko pod drzwiami
<span class="hljs-number">1889</span>_1447_000097	owego dnia by\u0142 ci\u0119\u017Cki dzie\u0144 zimowy <span class="hljs-selector-tag">i</span> od rana ju\u017C sypa\u0142 si\u0119 mi\u0119kki puch zmierzchu matka mia\u0142<span class="hljs-selector-tag">a</span> migren\u0119 <span class="hljs-selector-tag">i</span> le\u017C<span class="hljs-selector-tag">a</span>\u0142<span class="hljs-selector-tag">a</span> na sofie samotnie w salonie
...`}}),Le=new b({props:{code:`from datasets import load_dataset
dataset = load_dataset("audiofolder", data_dir="/path/to/audiofolder", sampling_rate=16_000)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;audiofolder&quot;</span>, data_dir=<span class="hljs-string">&quot;/path/to/audiofolder&quot;</span>, sampling_rate=<span class="hljs-number">16_000</span>)`}}),Fe=new b({props:{code:`from datasets import load_dataset
dataset = load_dataset("audiofolder", data_files="/path/to/audioarchive.zip", sampling_rate=16_000)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;audiofolder&quot;</span>, data_files=<span class="hljs-string">&quot;/path/to/audioarchive.zip&quot;</span>, sampling_rate=<span class="hljs-number">16_000</span>)`}}),Ne=new b({props:{code:'dataset = load_dataset("audiofolder", data_files="https://huggingface.co/datasets/polinaeterna/test_audiofolder/resolve/main/MLSPolishSample.zip", sampling_rate=16_000)',highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;audiofolder&quot;</span>, data_files=<span class="hljs-string">&quot;https://huggingface.co/datasets/polinaeterna/test_audiofolder/resolve/main/MLSPolishSample.zip&quot;</span>, sampling_rate=<span class="hljs-number">16_000</span>)'}}),Ie=new za({}),Ve=new b({props:{code:'common_voice = common_voice.cast_column("audio", Audio(sampling_rate=16_000))',highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>common_voice = common_voice.cast_column(<span class="hljs-string">&quot;audio&quot;</span>, Audio(sampling_rate=<span class="hljs-number">16_000</span>))'}}),We=new b({props:{code:'common_voice_train[0]["audio"]',highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>common_voice_train[<span class="hljs-number">0</span>][<span class="hljs-string">&quot;audio&quot;</span>]
{<span class="hljs-string">&#x27;array&#x27;</span>: array([ <span class="hljs-number">0.0000000e+00</span>,  <span class="hljs-number">0.0000000e+00</span>,  <span class="hljs-number">0.0000000e+00</span>, ...,
-<span class="hljs-number">7.4556941e-05</span>, -<span class="hljs-number">1.4621433e-05</span>, -<span class="hljs-number">5.7861507e-05</span>], dtype=float32),
<span class="hljs-string">&#x27;path&#x27;</span>: <span class="hljs-string">&#x27;/root/.cache/huggingface/datasets/downloads/extracted/05be0c29807a73c9b099873d2f5975dae6d05e9f7d577458a2466ecb9a2b0c6b/cv-corpus-6.1-2020-12-11/tr/clips/common_voice_tr_21921195.mp3&#x27;</span>,
<span class="hljs-string">&#x27;sampling_rate&#x27;</span>: <span class="hljs-number">16000</span>}`}}),Me=new za({}),Ke=new b({props:{code:`from transformers import AutoTokenizer, AutoFeatureExtractor, Wav2Vec2Processor
model_checkpoint = "facebook/wav2vec2-large-xlsr-53"
# after defining a vocab.json file you can instantiate a tokenizer object:
tokenizer = AutoTokenizer("./vocab.json", unk_token="[UNK]", pad_token="[PAD]", word_delimiter_token="|")
feature_extractor = AutoFeatureExtractor.from_pretrained(model_checkpoint)
processor = Wav2Vec2Processor.from_pretrained(feature_extractor=feature_extractor, tokenizer=tokenizer)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, AutoFeatureExtractor, Wav2Vec2Processor
<span class="hljs-meta">&gt;&gt;&gt; </span>model_checkpoint = <span class="hljs-string">&quot;facebook/wav2vec2-large-xlsr-53&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># after defining a vocab.json file you can instantiate a tokenizer object:</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer(<span class="hljs-string">&quot;./vocab.json&quot;</span>, unk_token=<span class="hljs-string">&quot;[UNK]&quot;</span>, pad_token=<span class="hljs-string">&quot;[PAD]&quot;</span>, word_delimiter_token=<span class="hljs-string">&quot;|&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>feature_extractor = AutoFeatureExtractor.from_pretrained(model_checkpoint)
<span class="hljs-meta">&gt;&gt;&gt; </span>processor = Wav2Vec2Processor.from_pretrained(feature_extractor=feature_extractor, tokenizer=tokenizer)`}}),Xe=new b({props:{code:`from transformers import Wav2Vec2Processor
processor = Wav2Vec2Processor.from_pretrained("facebook/wav2vec2-base-960h")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> Wav2Vec2Processor
<span class="hljs-meta">&gt;&gt;&gt; </span>processor = Wav2Vec2Processor.from_pretrained(<span class="hljs-string">&quot;facebook/wav2vec2-base-960h&quot;</span>)`}}),Qe=new b({props:{code:`def prepare_dataset(batch):
    audio = batch["audio"]
    batch["input_values"] = processor(audio["array"], sampling_rate=audio["sampling_rate"]).input_values[0]
    batch["input_length"] = len(batch["input_values"])
    with processor.as_target_processor():
        batch["labels"] = processor(batch["sentence"]).input_ids
    return batch
common_voice_train = common_voice_train.map(prepare_dataset, remove_columns=common_voice_train.column_names)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">def</span> <span class="hljs-title function_">prepare_dataset</span>(<span class="hljs-params">batch</span>):
<span class="hljs-meta">... </span>    audio = batch[<span class="hljs-string">&quot;audio&quot;</span>]
<span class="hljs-meta">... </span>    batch[<span class="hljs-string">&quot;input_values&quot;</span>] = processor(audio[<span class="hljs-string">&quot;array&quot;</span>], sampling_rate=audio[<span class="hljs-string">&quot;sampling_rate&quot;</span>]).input_values[<span class="hljs-number">0</span>]
<span class="hljs-meta">... </span>    batch[<span class="hljs-string">&quot;input_length&quot;</span>] = <span class="hljs-built_in">len</span>(batch[<span class="hljs-string">&quot;input_values&quot;</span>])
<span class="hljs-meta">... </span>    <span class="hljs-keyword">with</span> processor.as_target_processor():
<span class="hljs-meta">... </span>        batch[<span class="hljs-string">&quot;labels&quot;</span>] = processor(batch[<span class="hljs-string">&quot;sentence&quot;</span>]).input_ids
<span class="hljs-meta">... </span>    <span class="hljs-keyword">return</span> batch
<span class="hljs-meta">&gt;&gt;&gt; </span>common_voice_train = common_voice_train.<span class="hljs-built_in">map</span>(prepare_dataset, remove_columns=common_voice_train.column_names)`}}),{c(){f=l("meta"),ee=d(),$=l("h1"),E=l("a"),I=l("span"),h(j.$$.fragment),ve=d(),H=l("span"),V=s("Process audio data"),ae=d(),z=l("p"),W=s("\u{1F917} Datasets supports an "),P=l("a"),we=s("Audio"),M=s(" feature, enabling users to load and process raw audio files for training. This guide will show you how to:"),y=d(),k=l("ul"),te=l("li"),ea=s("Load your own custom audio dataset."),aa=d(),xa=l("li"),ms=s("Resample audio files."),_s=d(),ye=l("li"),gs=s("Use "),ta=l("a"),vs=s("Dataset.map()"),ws=s(" with audio files."),ct=d(),R=l("h2"),se=l("a"),Aa=l("span"),h(be.$$.fragment),ys=d(),qa=l("span"),bs=s("Installation"),dt=d(),D=l("p"),$s=s("The "),sa=l("a"),js=s("Audio"),ks=s(" feature is an experimental feature and should be installed as an extra dependency in \u{1F917} Datasets. Install the "),oa=l("a"),Es=s("Audio"),zs=s(" feature with pip:"),ut=d(),h($e.$$.fragment),ft=d(),C=l("p"),xs=s("Users should also install "),je=l("a"),As=s("torchaudio"),qs=s(" and "),ke=l("a"),Ps=s("librosa"),Ds=s(", two common libraries used by \u{1F917} Datasets for handling audio data."),ht=d(),h(Ee.$$.fragment),mt=d(),h(oe.$$.fragment),_t=d(),le=l("p"),Cs=s("Then you can load an audio dataset the same way you would load a text dataset. For example, load the "),ze=l("a"),Os=s("Common Voice"),Ss=s(" dataset with the Turkish configuration:"),gt=d(),h(xe.$$.fragment),vt=d(),U=l("h2"),re=l("a"),Pa=l("span"),h(Ae.$$.fragment),Ts=d(),Da=l("span"),Ls=s("Audio datasets"),wt=d(),x=l("p"),Fs=s("Audio datasets commonly have an "),Ca=l("code"),Ns=s("audio"),Is=s(" and "),Oa=l("code"),Hs=s("path"),Vs=s(" or "),Sa=l("code"),Ws=s("file"),Ms=s(" column."),yt=d(),qe=l("p"),Ta=l("code"),Rs=s("audio"),Us=s(" is the actual audio file that is loaded and resampled on-the-fly upon calling it."),bt=d(),h(Pe.$$.fragment),$t=d(),A=l("p"),Gs=s("When you access an audio file, it is automatically decoded and resampled. Generally, you should query an audio file like: "),La=l("code"),Ys=s('common_voice[0]["audio"]'),Js=s(". If you query an audio file with "),Fa=l("code"),Bs=s('common_voice["audio"][0]'),Ks=s(" instead, "),Na=l("strong"),Xs=s("all"),Qs=s(" the audio files in your dataset will be decoded and resampled. This process can take a long time if you have a large dataset."),jt=d(),G=l("p"),Ia=l("code"),Zs=s("path"),eo=s(" or "),Ha=l("code"),ao=s("file"),to=s(" is an absolute path to an audio file."),kt=d(),h(De.$$.fragment),Et=d(),O=l("p"),so=s("The "),Va=l("code"),oo=s("path"),lo=s(" is useful if you want to load your own audio dataset. In this case, provide a column of audio file paths to "),la=l("a"),ro=s("Dataset.cast_column()"),no=s(":"),zt=d(),h(Ce.$$.fragment),xt=d(),Y=l("h2"),ne=l("a"),Wa=l("span"),h(Oe.$$.fragment),io=d(),Ma=l("span"),po=s("AudioFolder"),At=d(),S=l("p"),co=s("You can load your audio dataset with an "),Ra=l("code"),uo=s("AudioFolder"),fo=s(` dataset. Your dataset folder / archive should contain audio files and
`),Ua=l("code"),ho=s("transcripts.txt"),mo=s(" file with transcriptions, for example like this:"),qt=d(),h(Se.$$.fragment),Pt=d(),ie=l("p"),_o=s("Transcripts file should contain audio id and corresponding text transcription, "),ra=l("strong"),go=s("separated by "),Ga=l("code"),vo=s('"\\t"'),wo=s(" at each line:"),Dt=d(),h(Te.$$.fragment),Ct=d(),na=l("p"),yo=s("Load a directory:"),Ot=d(),h(Le.$$.fragment),St=d(),T=l("p"),bo=s("Load a local "),Ya=l("code"),$o=s(".zip"),jo=s(" archive with the "),Ja=l("code"),ko=s("data_files"),Eo=s(" parameter:"),Tt=d(),h(Fe.$$.fragment),Lt=d(),pe=l("p"),zo=s("Load remote datasets from their URLs with the "),Ba=l("code"),xo=s("data_files"),Ao=s(" parameter:"),Ft=d(),h(Ne.$$.fragment),Nt=d(),L=l("p"),qo=s("Note that "),Ka=l("code"),Po=s("sampling_rate"),Do=s(" is a required argument. Only "),Xa=l("code"),Co=s(".zip"),Oo=s(" archives are supported now."),It=d(),J=l("h2"),ce=l("a"),Qa=l("span"),h(Ie.$$.fragment),So=d(),Za=l("span"),To=s("Resample"),Ht=d(),F=l("p"),Lo=s("Some models expect the audio data to have a certain sampling rate due to how the model was pretrained. For example, the "),He=l("a"),Fo=s("XLSR-Wav2Vec2"),No=s(" model expects the input to have a sampling rate of 16kHz, but an audio file from the Common Voice dataset has a sampling rate of 48kHz. You can use "),ia=l("a"),Io=s("Dataset.cast_column()"),Ho=s(" to downsample the sampling rate to 16kHz:"),Vt=d(),h(Ve.$$.fragment),Wt=d(),de=l("p"),Vo=s("The next time you load the audio file, the "),pa=l("a"),Wo=s("Audio"),Mo=s(" feature will load and resample it to 16kHz:"),Mt=d(),h(We.$$.fragment),Rt=d(),B=l("div"),ca=l("img"),Ro=d(),da=l("img"),Ut=d(),K=l("h2"),ue=l("a"),et=l("span"),h(Me.$$.fragment),Uo=d(),at=l("span"),Go=s("Map"),Gt=d(),q=l("p"),Yo=s("Just like text datasets, you can apply a preprocessing function over an entire dataset with "),ua=l("a"),Jo=s("Dataset.map()"),Bo=s(", which is useful for preprocessing all of your audio data at once. Start with a "),Re=l("a"),Ko=s("speech recognition model"),Xo=s(" of your choice, and load a "),tt=l("code"),Qo=s("processor"),Zo=s(" object that contains:"),Yt=d(),fe=l("ol"),st=l("li"),X=l("p"),el=s("A feature extractor to convert the speech signal to the model\u2019s input format. Every speech recognition model on the \u{1F917} "),Ue=l("a"),al=s("Hub"),tl=s(" contains a predefined feature extractor that can be easily loaded with "),ot=l("code"),sl=s("AutoFeatureExtractor.from_pretrained(...)"),ol=s("."),ll=d(),Ge=l("li"),Q=l("p"),rl=s("A tokenizer to convert the model\u2019s output format to text. Fine-tuned speech recognition models, such as "),Ye=l("a"),nl=s("facebook/wav2vec2-base-960h"),il=s(", contain a predefined tokenizer that can be easily loaded with "),lt=l("code"),pl=s("AutoTokenizer.from_pretrained(...)"),cl=s("."),dl=d(),Z=l("p"),ul=s("For pretrained speech recognition models, such as "),Je=l("a"),fl=s("facebook/wav2vec2-large-xlsr-53"),hl=s(", a tokenizer needs to be created from the target text as explained "),Be=l("a"),ml=s("here"),_l=s(". The following example demonstrates how to load a feature extractor, tokenizer and processor for a pretrained speech recognition model:"),Jt=d(),h(Ke.$$.fragment),Bt=d(),he=l("p"),gl=s("For fine-tuned speech recognition models, you can simply load a predefined "),rt=l("code"),vl=s("processor"),wl=s(" object with:"),Kt=d(),h(Xe.$$.fragment),Xt=d(),N=l("p"),yl=s("Make sure to include the "),nt=l("code"),bl=s("audio"),$l=s(" key in your preprocessing function when you call "),fa=l("a"),jl=s("Dataset.map()"),kl=s(" so that you are actually resampling the audio data:"),Qt=d(),h(Qe.$$.fragment),this.h()},l(e){const i=Wr('[data-svelte="svelte-1phssyn"]',document.head);f=r(i,"META",{name:!0,content:!0}),i.forEach(t),ee=u(e),$=r(e,"H1",{class:!0});var Ze=n($);E=r(Ze,"A",{id:!0,class:!0,href:!0});var ql=n(E);I=r(ql,"SPAN",{});var Pl=n(I);m(j.$$.fragment,Pl),Pl.forEach(t),ql.forEach(t),ve=u(Ze),H=r(Ze,"SPAN",{});var Dl=n(H);V=o(Dl,"Process audio data"),Dl.forEach(t),Ze.forEach(t),ae=u(e),z=r(e,"P",{});var es=n(z);W=o(es,"\u{1F917} Datasets supports an "),P=r(es,"A",{href:!0});var Cl=n(P);we=o(Cl,"Audio"),Cl.forEach(t),M=o(es," feature, enabling users to load and process raw audio files for training. This guide will show you how to:"),es.forEach(t),y=u(e),k=r(e,"UL",{});var ha=n(k);te=r(ha,"LI",{});var Ol=n(te);ea=o(Ol,"Load your own custom audio dataset."),Ol.forEach(t),aa=u(ha),xa=r(ha,"LI",{});var Sl=n(xa);ms=o(Sl,"Resample audio files."),Sl.forEach(t),_s=u(ha),ye=r(ha,"LI",{});var as=n(ye);gs=o(as,"Use "),ta=r(as,"A",{href:!0});var Tl=n(ta);vs=o(Tl,"Dataset.map()"),Tl.forEach(t),ws=o(as," with audio files."),as.forEach(t),ha.forEach(t),ct=u(e),R=r(e,"H2",{class:!0});var ts=n(R);se=r(ts,"A",{id:!0,class:!0,href:!0});var Ll=n(se);Aa=r(Ll,"SPAN",{});var Fl=n(Aa);m(be.$$.fragment,Fl),Fl.forEach(t),Ll.forEach(t),ys=u(ts),qa=r(ts,"SPAN",{});var Nl=n(qa);bs=o(Nl,"Installation"),Nl.forEach(t),ts.forEach(t),dt=u(e),D=r(e,"P",{});var ma=n(D);$s=o(ma,"The "),sa=r(ma,"A",{href:!0});var Il=n(sa);js=o(Il,"Audio"),Il.forEach(t),ks=o(ma," feature is an experimental feature and should be installed as an extra dependency in \u{1F917} Datasets. Install the "),oa=r(ma,"A",{href:!0});var Hl=n(oa);Es=o(Hl,"Audio"),Hl.forEach(t),zs=o(ma," feature with pip:"),ma.forEach(t),ut=u(e),m($e.$$.fragment,e),ft=u(e),C=r(e,"P",{});var _a=n(C);xs=o(_a,"Users should also install "),je=r(_a,"A",{href:!0,rel:!0});var Vl=n(je);As=o(Vl,"torchaudio"),Vl.forEach(t),qs=o(_a," and "),ke=r(_a,"A",{href:!0,rel:!0});var Wl=n(ke);Ps=o(Wl,"librosa"),Wl.forEach(t),Ds=o(_a,", two common libraries used by \u{1F917} Datasets for handling audio data."),_a.forEach(t),ht=u(e),m(Ee.$$.fragment,e),mt=u(e),m(oe.$$.fragment,e),_t=u(e),le=r(e,"P",{});var ss=n(le);Cs=o(ss,"Then you can load an audio dataset the same way you would load a text dataset. For example, load the "),ze=r(ss,"A",{href:!0,rel:!0});var Ml=n(ze);Os=o(Ml,"Common Voice"),Ml.forEach(t),Ss=o(ss," dataset with the Turkish configuration:"),ss.forEach(t),gt=u(e),m(xe.$$.fragment,e),vt=u(e),U=r(e,"H2",{class:!0});var os=n(U);re=r(os,"A",{id:!0,class:!0,href:!0});var Rl=n(re);Pa=r(Rl,"SPAN",{});var Ul=n(Pa);m(Ae.$$.fragment,Ul),Ul.forEach(t),Rl.forEach(t),Ts=u(os),Da=r(os,"SPAN",{});var Gl=n(Da);Ls=o(Gl,"Audio datasets"),Gl.forEach(t),os.forEach(t),wt=u(e),x=r(e,"P",{});var me=n(x);Fs=o(me,"Audio datasets commonly have an "),Ca=r(me,"CODE",{});var Yl=n(Ca);Ns=o(Yl,"audio"),Yl.forEach(t),Is=o(me," and "),Oa=r(me,"CODE",{});var Jl=n(Oa);Hs=o(Jl,"path"),Jl.forEach(t),Vs=o(me," or "),Sa=r(me,"CODE",{});var Bl=n(Sa);Ws=o(Bl,"file"),Bl.forEach(t),Ms=o(me," column."),me.forEach(t),yt=u(e),qe=r(e,"P",{});var El=n(qe);Ta=r(El,"CODE",{});var Kl=n(Ta);Rs=o(Kl,"audio"),Kl.forEach(t),Us=o(El," is the actual audio file that is loaded and resampled on-the-fly upon calling it."),El.forEach(t),bt=u(e),m(Pe.$$.fragment,e),$t=u(e),A=r(e,"P",{});var _e=n(A);Gs=o(_e,"When you access an audio file, it is automatically decoded and resampled. Generally, you should query an audio file like: "),La=r(_e,"CODE",{});var Xl=n(La);Ys=o(Xl,'common_voice[0]["audio"]'),Xl.forEach(t),Js=o(_e,". If you query an audio file with "),Fa=r(_e,"CODE",{});var Ql=n(Fa);Bs=o(Ql,'common_voice["audio"][0]'),Ql.forEach(t),Ks=o(_e," instead, "),Na=r(_e,"STRONG",{});var Zl=n(Na);Xs=o(Zl,"all"),Zl.forEach(t),Qs=o(_e," the audio files in your dataset will be decoded and resampled. This process can take a long time if you have a large dataset."),_e.forEach(t),jt=u(e),G=r(e,"P",{});var it=n(G);Ia=r(it,"CODE",{});var er=n(Ia);Zs=o(er,"path"),er.forEach(t),eo=o(it," or "),Ha=r(it,"CODE",{});var ar=n(Ha);ao=o(ar,"file"),ar.forEach(t),to=o(it," is an absolute path to an audio file."),it.forEach(t),kt=u(e),m(De.$$.fragment,e),Et=u(e),O=r(e,"P",{});var ga=n(O);so=o(ga,"The "),Va=r(ga,"CODE",{});var tr=n(Va);oo=o(tr,"path"),tr.forEach(t),lo=o(ga," is useful if you want to load your own audio dataset. In this case, provide a column of audio file paths to "),la=r(ga,"A",{href:!0});var sr=n(la);ro=o(sr,"Dataset.cast_column()"),sr.forEach(t),no=o(ga,":"),ga.forEach(t),zt=u(e),m(Ce.$$.fragment,e),xt=u(e),Y=r(e,"H2",{class:!0});var ls=n(Y);ne=r(ls,"A",{id:!0,class:!0,href:!0});var or=n(ne);Wa=r(or,"SPAN",{});var lr=n(Wa);m(Oe.$$.fragment,lr),lr.forEach(t),or.forEach(t),io=u(ls),Ma=r(ls,"SPAN",{});var rr=n(Ma);po=o(rr,"AudioFolder"),rr.forEach(t),ls.forEach(t),At=u(e),S=r(e,"P",{});var va=n(S);co=o(va,"You can load your audio dataset with an "),Ra=r(va,"CODE",{});var nr=n(Ra);uo=o(nr,"AudioFolder"),nr.forEach(t),fo=o(va,` dataset. Your dataset folder / archive should contain audio files and
`),Ua=r(va,"CODE",{});var ir=n(Ua);ho=o(ir,"transcripts.txt"),ir.forEach(t),mo=o(va," file with transcriptions, for example like this:"),va.forEach(t),qt=u(e),m(Se.$$.fragment,e),Pt=u(e),ie=r(e,"P",{});var rs=n(ie);_o=o(rs,"Transcripts file should contain audio id and corresponding text transcription, "),ra=r(rs,"STRONG",{});var zl=n(ra);go=o(zl,"separated by "),Ga=r(zl,"CODE",{});var pr=n(Ga);vo=o(pr,'"\\t"'),pr.forEach(t),zl.forEach(t),wo=o(rs," at each line:"),rs.forEach(t),Dt=u(e),m(Te.$$.fragment,e),Ct=u(e),na=r(e,"P",{});var cr=n(na);yo=o(cr,"Load a directory:"),cr.forEach(t),Ot=u(e),m(Le.$$.fragment,e),St=u(e),T=r(e,"P",{});var wa=n(T);bo=o(wa,"Load a local "),Ya=r(wa,"CODE",{});var dr=n(Ya);$o=o(dr,".zip"),dr.forEach(t),jo=o(wa," archive with the "),Ja=r(wa,"CODE",{});var ur=n(Ja);ko=o(ur,"data_files"),ur.forEach(t),Eo=o(wa," parameter:"),wa.forEach(t),Tt=u(e),m(Fe.$$.fragment,e),Lt=u(e),pe=r(e,"P",{});var ns=n(pe);zo=o(ns,"Load remote datasets from their URLs with the "),Ba=r(ns,"CODE",{});var fr=n(Ba);xo=o(fr,"data_files"),fr.forEach(t),Ao=o(ns," parameter:"),ns.forEach(t),Ft=u(e),m(Ne.$$.fragment,e),Nt=u(e),L=r(e,"P",{});var ya=n(L);qo=o(ya,"Note that "),Ka=r(ya,"CODE",{});var hr=n(Ka);Po=o(hr,"sampling_rate"),hr.forEach(t),Do=o(ya," is a required argument. Only "),Xa=r(ya,"CODE",{});var mr=n(Xa);Co=o(mr,".zip"),mr.forEach(t),Oo=o(ya," archives are supported now."),ya.forEach(t),It=u(e),J=r(e,"H2",{class:!0});var is=n(J);ce=r(is,"A",{id:!0,class:!0,href:!0});var _r=n(ce);Qa=r(_r,"SPAN",{});var gr=n(Qa);m(Ie.$$.fragment,gr),gr.forEach(t),_r.forEach(t),So=u(is),Za=r(is,"SPAN",{});var vr=n(Za);To=o(vr,"Resample"),vr.forEach(t),is.forEach(t),Ht=u(e),F=r(e,"P",{});var ba=n(F);Lo=o(ba,"Some models expect the audio data to have a certain sampling rate due to how the model was pretrained. For example, the "),He=r(ba,"A",{href:!0,rel:!0});var wr=n(He);Fo=o(wr,"XLSR-Wav2Vec2"),wr.forEach(t),No=o(ba," model expects the input to have a sampling rate of 16kHz, but an audio file from the Common Voice dataset has a sampling rate of 48kHz. You can use "),ia=r(ba,"A",{href:!0});var yr=n(ia);Io=o(yr,"Dataset.cast_column()"),yr.forEach(t),Ho=o(ba," to downsample the sampling rate to 16kHz:"),ba.forEach(t),Vt=u(e),m(Ve.$$.fragment,e),Wt=u(e),de=r(e,"P",{});var ps=n(de);Vo=o(ps,"The next time you load the audio file, the "),pa=r(ps,"A",{href:!0});var br=n(pa);Wo=o(br,"Audio"),br.forEach(t),Mo=o(ps," feature will load and resample it to 16kHz:"),ps.forEach(t),Mt=u(e),m(We.$$.fragment,e),Rt=u(e),B=r(e,"DIV",{class:!0});var cs=n(B);ca=r(cs,"IMG",{class:!0,src:!0}),Ro=u(cs),da=r(cs,"IMG",{class:!0,src:!0}),cs.forEach(t),Ut=u(e),K=r(e,"H2",{class:!0});var ds=n(K);ue=r(ds,"A",{id:!0,class:!0,href:!0});var $r=n(ue);et=r($r,"SPAN",{});var jr=n(et);m(Me.$$.fragment,jr),jr.forEach(t),$r.forEach(t),Uo=u(ds),at=r(ds,"SPAN",{});var kr=n(at);Go=o(kr,"Map"),kr.forEach(t),ds.forEach(t),Gt=u(e),q=r(e,"P",{});var ge=n(q);Yo=o(ge,"Just like text datasets, you can apply a preprocessing function over an entire dataset with "),ua=r(ge,"A",{href:!0});var Er=n(ua);Jo=o(Er,"Dataset.map()"),Er.forEach(t),Bo=o(ge,", which is useful for preprocessing all of your audio data at once. Start with a "),Re=r(ge,"A",{href:!0,rel:!0});var zr=n(Re);Ko=o(zr,"speech recognition model"),zr.forEach(t),Xo=o(ge," of your choice, and load a "),tt=r(ge,"CODE",{});var xr=n(tt);Qo=o(xr,"processor"),xr.forEach(t),Zo=o(ge," object that contains:"),ge.forEach(t),Yt=u(e),fe=r(e,"OL",{});var us=n(fe);st=r(us,"LI",{});var Ar=n(st);X=r(Ar,"P",{});var $a=n(X);el=o($a,"A feature extractor to convert the speech signal to the model\u2019s input format. Every speech recognition model on the \u{1F917} "),Ue=r($a,"A",{href:!0,rel:!0});var qr=n(Ue);al=o(qr,"Hub"),qr.forEach(t),tl=o($a," contains a predefined feature extractor that can be easily loaded with "),ot=r($a,"CODE",{});var Pr=n(ot);sl=o(Pr,"AutoFeatureExtractor.from_pretrained(...)"),Pr.forEach(t),ol=o($a,"."),$a.forEach(t),Ar.forEach(t),ll=u(us),Ge=r(us,"LI",{});var fs=n(Ge);Q=r(fs,"P",{});var ja=n(Q);rl=o(ja,"A tokenizer to convert the model\u2019s output format to text. Fine-tuned speech recognition models, such as "),Ye=r(ja,"A",{href:!0,rel:!0});var Dr=n(Ye);nl=o(Dr,"facebook/wav2vec2-base-960h"),Dr.forEach(t),il=o(ja,", contain a predefined tokenizer that can be easily loaded with "),lt=r(ja,"CODE",{});var Cr=n(lt);pl=o(Cr,"AutoTokenizer.from_pretrained(...)"),Cr.forEach(t),cl=o(ja,"."),ja.forEach(t),dl=u(fs),Z=r(fs,"P",{});var ka=n(Z);ul=o(ka,"For pretrained speech recognition models, such as "),Je=r(ka,"A",{href:!0,rel:!0});var Or=n(Je);fl=o(Or,"facebook/wav2vec2-large-xlsr-53"),Or.forEach(t),hl=o(ka,", a tokenizer needs to be created from the target text as explained "),Be=r(ka,"A",{href:!0,rel:!0});var Sr=n(Be);ml=o(Sr,"here"),Sr.forEach(t),_l=o(ka,". The following example demonstrates how to load a feature extractor, tokenizer and processor for a pretrained speech recognition model:"),ka.forEach(t),fs.forEach(t),us.forEach(t),Jt=u(e),m(Ke.$$.fragment,e),Bt=u(e),he=r(e,"P",{});var hs=n(he);gl=o(hs,"For fine-tuned speech recognition models, you can simply load a predefined "),rt=r(hs,"CODE",{});var Tr=n(rt);vl=o(Tr,"processor"),Tr.forEach(t),wl=o(hs," object with:"),hs.forEach(t),Kt=u(e),m(Xe.$$.fragment,e),Xt=u(e),N=r(e,"P",{});var Ea=n(N);yl=o(Ea,"Make sure to include the "),nt=r(Ea,"CODE",{});var Lr=n(nt);bl=o(Lr,"audio"),Lr.forEach(t),$l=o(Ea," key in your preprocessing function when you call "),fa=r(Ea,"A",{href:!0});var Fr=n(fa);jl=o(Fr,"Dataset.map()"),Fr.forEach(t),kl=o(Ea," so that you are actually resampling the audio data:"),Ea.forEach(t),Qt=u(e),m(Qe.$$.fragment,e),this.h()},h(){c(f,"name","hf:doc:metadata"),c(f,"content",JSON.stringify(Yr)),c(E,"id","process-audio-data"),c(E,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(E,"href","#process-audio-data"),c($,"class","relative group"),c(P,"href","/docs/datasets/pr_3963/en/package_reference/main_classes#datasets.Audio"),c(ta,"href","/docs/datasets/pr_3963/en/package_reference/main_classes#datasets.Dataset.map"),c(se,"id","installation"),c(se,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(se,"href","#installation"),c(R,"class","relative group"),c(sa,"href","/docs/datasets/pr_3963/en/package_reference/main_classes#datasets.Audio"),c(oa,"href","/docs/datasets/pr_3963/en/package_reference/main_classes#datasets.Audio"),c(je,"href","https://pytorch.org/audio/stable/index.html"),c(je,"rel","nofollow"),c(ke,"href","https://librosa.org/doc/latest/index.html"),c(ke,"rel","nofollow"),c(ze,"href","https://huggingface.co/datasets/common_voice"),c(ze,"rel","nofollow"),c(re,"id","audio-datasets"),c(re,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(re,"href","#audio-datasets"),c(U,"class","relative group"),c(la,"href","/docs/datasets/pr_3963/en/package_reference/main_classes#datasets.Dataset.cast_column"),c(ne,"id","audiofolder"),c(ne,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(ne,"href","#audiofolder"),c(Y,"class","relative group"),c(ce,"id","resample"),c(ce,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(ce,"href","#resample"),c(J,"class","relative group"),c(He,"href","https://huggingface.co/facebook/wav2vec2-large-xlsr-53"),c(He,"rel","nofollow"),c(ia,"href","/docs/datasets/pr_3963/en/package_reference/main_classes#datasets.Dataset.cast_column"),c(pa,"href","/docs/datasets/pr_3963/en/package_reference/main_classes#datasets.Audio"),c(ca,"class","block dark:hidden"),Nr(ca.src,xl="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/datasets/resample.gif")||c(ca,"src",xl),c(da,"class","hidden dark:block"),Nr(da.src,Al="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/datasets/resample-dark.gif")||c(da,"src",Al),c(B,"class","flex justify-center"),c(ue,"id","map"),c(ue,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(ue,"href","#map"),c(K,"class","relative group"),c(ua,"href","/docs/datasets/pr_3963/en/package_reference/main_classes#datasets.Dataset.map"),c(Re,"href","https://huggingface.co/models?pipeline_tag=automatic-speech-recognition&sort=downloads"),c(Re,"rel","nofollow"),c(Ue,"href","https://huggingface.co/models?pipeline_tag=automatic-speech-recognition&sort=downloads"),c(Ue,"rel","nofollow"),c(Ye,"href","https://huggingface.co/facebook/wav2vec2-base-960h"),c(Ye,"rel","nofollow"),c(Je,"href","https://huggingface.co/facebook/wav2vec2-large-xlsr-53"),c(Je,"rel","nofollow"),c(Be,"href","https://huggingface.co/blog/fine-tune-wav2vec2-english"),c(Be,"rel","nofollow"),c(fa,"href","/docs/datasets/pr_3963/en/package_reference/main_classes#datasets.Dataset.map")},m(e,i){a(document.head,f),p(e,ee,i),p(e,$,i),a($,E),a(E,I),_(j,I,null),a($,ve),a($,H),a(H,V),p(e,ae,i),p(e,z,i),a(z,W),a(z,P),a(P,we),a(z,M),p(e,y,i),p(e,k,i),a(k,te),a(te,ea),a(k,aa),a(k,xa),a(xa,ms),a(k,_s),a(k,ye),a(ye,gs),a(ye,ta),a(ta,vs),a(ye,ws),p(e,ct,i),p(e,R,i),a(R,se),a(se,Aa),_(be,Aa,null),a(R,ys),a(R,qa),a(qa,bs),p(e,dt,i),p(e,D,i),a(D,$s),a(D,sa),a(sa,js),a(D,ks),a(D,oa),a(oa,Es),a(D,zs),p(e,ut,i),_($e,e,i),p(e,ft,i),p(e,C,i),a(C,xs),a(C,je),a(je,As),a(C,qs),a(C,ke),a(ke,Ps),a(C,Ds),p(e,ht,i),_(Ee,e,i),p(e,mt,i),_(oe,e,i),p(e,_t,i),p(e,le,i),a(le,Cs),a(le,ze),a(ze,Os),a(le,Ss),p(e,gt,i),_(xe,e,i),p(e,vt,i),p(e,U,i),a(U,re),a(re,Pa),_(Ae,Pa,null),a(U,Ts),a(U,Da),a(Da,Ls),p(e,wt,i),p(e,x,i),a(x,Fs),a(x,Ca),a(Ca,Ns),a(x,Is),a(x,Oa),a(Oa,Hs),a(x,Vs),a(x,Sa),a(Sa,Ws),a(x,Ms),p(e,yt,i),p(e,qe,i),a(qe,Ta),a(Ta,Rs),a(qe,Us),p(e,bt,i),_(Pe,e,i),p(e,$t,i),p(e,A,i),a(A,Gs),a(A,La),a(La,Ys),a(A,Js),a(A,Fa),a(Fa,Bs),a(A,Ks),a(A,Na),a(Na,Xs),a(A,Qs),p(e,jt,i),p(e,G,i),a(G,Ia),a(Ia,Zs),a(G,eo),a(G,Ha),a(Ha,ao),a(G,to),p(e,kt,i),_(De,e,i),p(e,Et,i),p(e,O,i),a(O,so),a(O,Va),a(Va,oo),a(O,lo),a(O,la),a(la,ro),a(O,no),p(e,zt,i),_(Ce,e,i),p(e,xt,i),p(e,Y,i),a(Y,ne),a(ne,Wa),_(Oe,Wa,null),a(Y,io),a(Y,Ma),a(Ma,po),p(e,At,i),p(e,S,i),a(S,co),a(S,Ra),a(Ra,uo),a(S,fo),a(S,Ua),a(Ua,ho),a(S,mo),p(e,qt,i),_(Se,e,i),p(e,Pt,i),p(e,ie,i),a(ie,_o),a(ie,ra),a(ra,go),a(ra,Ga),a(Ga,vo),a(ie,wo),p(e,Dt,i),_(Te,e,i),p(e,Ct,i),p(e,na,i),a(na,yo),p(e,Ot,i),_(Le,e,i),p(e,St,i),p(e,T,i),a(T,bo),a(T,Ya),a(Ya,$o),a(T,jo),a(T,Ja),a(Ja,ko),a(T,Eo),p(e,Tt,i),_(Fe,e,i),p(e,Lt,i),p(e,pe,i),a(pe,zo),a(pe,Ba),a(Ba,xo),a(pe,Ao),p(e,Ft,i),_(Ne,e,i),p(e,Nt,i),p(e,L,i),a(L,qo),a(L,Ka),a(Ka,Po),a(L,Do),a(L,Xa),a(Xa,Co),a(L,Oo),p(e,It,i),p(e,J,i),a(J,ce),a(ce,Qa),_(Ie,Qa,null),a(J,So),a(J,Za),a(Za,To),p(e,Ht,i),p(e,F,i),a(F,Lo),a(F,He),a(He,Fo),a(F,No),a(F,ia),a(ia,Io),a(F,Ho),p(e,Vt,i),_(Ve,e,i),p(e,Wt,i),p(e,de,i),a(de,Vo),a(de,pa),a(pa,Wo),a(de,Mo),p(e,Mt,i),_(We,e,i),p(e,Rt,i),p(e,B,i),a(B,ca),a(B,Ro),a(B,da),p(e,Ut,i),p(e,K,i),a(K,ue),a(ue,et),_(Me,et,null),a(K,Uo),a(K,at),a(at,Go),p(e,Gt,i),p(e,q,i),a(q,Yo),a(q,ua),a(ua,Jo),a(q,Bo),a(q,Re),a(Re,Ko),a(q,Xo),a(q,tt),a(tt,Qo),a(q,Zo),p(e,Yt,i),p(e,fe,i),a(fe,st),a(st,X),a(X,el),a(X,Ue),a(Ue,al),a(X,tl),a(X,ot),a(ot,sl),a(X,ol),a(fe,ll),a(fe,Ge),a(Ge,Q),a(Q,rl),a(Q,Ye),a(Ye,nl),a(Q,il),a(Q,lt),a(lt,pl),a(Q,cl),a(Ge,dl),a(Ge,Z),a(Z,ul),a(Z,Je),a(Je,fl),a(Z,hl),a(Z,Be),a(Be,ml),a(Z,_l),p(e,Jt,i),_(Ke,e,i),p(e,Bt,i),p(e,he,i),a(he,gl),a(he,rt),a(rt,vl),a(he,wl),p(e,Kt,i),_(Xe,e,i),p(e,Xt,i),p(e,N,i),a(N,yl),a(N,nt),a(nt,bl),a(N,$l),a(N,fa),a(fa,jl),a(N,kl),p(e,Qt,i),_(Qe,e,i),Zt=!0},p(e,[i]){const Ze={};i&2&&(Ze.$$scope={dirty:i,ctx:e}),oe.$set(Ze)},i(e){Zt||(g(j.$$.fragment,e),g(be.$$.fragment,e),g($e.$$.fragment,e),g(Ee.$$.fragment,e),g(oe.$$.fragment,e),g(xe.$$.fragment,e),g(Ae.$$.fragment,e),g(Pe.$$.fragment,e),g(De.$$.fragment,e),g(Ce.$$.fragment,e),g(Oe.$$.fragment,e),g(Se.$$.fragment,e),g(Te.$$.fragment,e),g(Le.$$.fragment,e),g(Fe.$$.fragment,e),g(Ne.$$.fragment,e),g(Ie.$$.fragment,e),g(Ve.$$.fragment,e),g(We.$$.fragment,e),g(Me.$$.fragment,e),g(Ke.$$.fragment,e),g(Xe.$$.fragment,e),g(Qe.$$.fragment,e),Zt=!0)},o(e){v(j.$$.fragment,e),v(be.$$.fragment,e),v($e.$$.fragment,e),v(Ee.$$.fragment,e),v(oe.$$.fragment,e),v(xe.$$.fragment,e),v(Ae.$$.fragment,e),v(Pe.$$.fragment,e),v(De.$$.fragment,e),v(Ce.$$.fragment,e),v(Oe.$$.fragment,e),v(Se.$$.fragment,e),v(Te.$$.fragment,e),v(Le.$$.fragment,e),v(Fe.$$.fragment,e),v(Ne.$$.fragment,e),v(Ie.$$.fragment,e),v(Ve.$$.fragment,e),v(We.$$.fragment,e),v(Me.$$.fragment,e),v(Ke.$$.fragment,e),v(Xe.$$.fragment,e),v(Qe.$$.fragment,e),Zt=!1},d(e){t(f),e&&t(ee),e&&t($),w(j),e&&t(ae),e&&t(z),e&&t(y),e&&t(k),e&&t(ct),e&&t(R),w(be),e&&t(dt),e&&t(D),e&&t(ut),w($e,e),e&&t(ft),e&&t(C),e&&t(ht),w(Ee,e),e&&t(mt),w(oe,e),e&&t(_t),e&&t(le),e&&t(gt),w(xe,e),e&&t(vt),e&&t(U),w(Ae),e&&t(wt),e&&t(x),e&&t(yt),e&&t(qe),e&&t(bt),w(Pe,e),e&&t($t),e&&t(A),e&&t(jt),e&&t(G),e&&t(kt),w(De,e),e&&t(Et),e&&t(O),e&&t(zt),w(Ce,e),e&&t(xt),e&&t(Y),w(Oe),e&&t(At),e&&t(S),e&&t(qt),w(Se,e),e&&t(Pt),e&&t(ie),e&&t(Dt),w(Te,e),e&&t(Ct),e&&t(na),e&&t(Ot),w(Le,e),e&&t(St),e&&t(T),e&&t(Tt),w(Fe,e),e&&t(Lt),e&&t(pe),e&&t(Ft),w(Ne,e),e&&t(Nt),e&&t(L),e&&t(It),e&&t(J),w(Ie),e&&t(Ht),e&&t(F),e&&t(Vt),w(Ve,e),e&&t(Wt),e&&t(de),e&&t(Mt),w(We,e),e&&t(Rt),e&&t(B),e&&t(Ut),e&&t(K),w(Me),e&&t(Gt),e&&t(q),e&&t(Yt),e&&t(fe),e&&t(Jt),w(Ke,e),e&&t(Bt),e&&t(he),e&&t(Kt),w(Xe,e),e&&t(Xt),e&&t(N),e&&t(Qt),w(Qe,e)}}}const Yr={local:"process-audio-data",sections:[{local:"installation",title:"Installation"},{local:"audio-datasets",title:"Audio datasets"},{local:"audiofolder",title:"AudioFolder"},{local:"resample",title:"Resample"},{local:"map",title:"Map"}],title:"Process audio data"};function Jr(pt){return Mr(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Zr extends Ir{constructor(f){super();Hr(this,f,Jr,Gr,Vr,{})}}export{Zr as default,Yr as metadata};
