import{S as Ni,i as ji,s as Ri,e as s,k as l,w as c,t as o,M as Ci,c as r,d as t,m as i,a as n,x as u,h as d,b as p,F as e,g as v,y as m,L as ki,q as f,o as g,B as h,v as Oi}from"../../chunks/vendor-8138ceec.js";import{D as b}from"../../chunks/Docstring-6fa3bd37.js";import{C as qt}from"../../chunks/CodeBlock-fc89709f.js";import{I as Ai}from"../../chunks/IconCopyLink-2dd3a6ac.js";function Li(tl){let q,us,H,re,Ht,Ee,er,Gt,tr,ms,P,ar,ut,sr,rr,mt,nr,or,fs,$,xe,dr,Wt,lr,ir,ft,Xt,pr,cr,ur,G,gt,zt,mr,fr,gr,ht,_t,hr,_r,vr,ne,vt,br,$r,Jt,wr,Er,xr,N,Kt,Dr,yr,Yt,Tr,Ir,Qt,Br,Sr,Zt,Nr,jr,Rr,oe,De,Cr,ea,kr,Or,de,ye,Ar,ta,Lr,Pr,le,Te,Vr,aa,Fr,Mr,ie,Ie,Ur,sa,qr,Hr,pe,Be,Gr,ra,Wr,gs,k,Se,Xr,na,zr,Jr,V,oa,Kr,Yr,da,Qr,Zr,la,en,tn,hs,W,Ne,an,ia,sn,_s,X,je,rn,pa,nn,vs,S,Re,on,Ce,dn,bt,ln,pn,cn,ke,un,$t,mn,fn,gn,F,Oe,hn,ca,_n,vn,z,ua,bn,$n,ma,wn,En,fa,xn,bs,E,Ae,Dn,ce,Le,yn,ga,Tn,In,j,Pe,Bn,ha,Sn,Nn,_a,jn,Rn,Ve,Cn,ue,Fe,kn,Me,On,va,An,Ln,Pn,me,Ue,Vn,ba,Fn,Mn,fe,qe,Un,$a,qn,Hn,ge,He,Gn,wa,Wn,Xn,he,Ge,zn,Ea,Jn,$s,B,We,Kn,wt,xa,Yn,Qn,Zn,Xe,eo,Da,to,ao,so,ya,ro,no,ze,Ta,J,ws,oo,Ia,lo,io,Ba,po,co,K,Y,Et,Sa,uo,mo,fo,Na,go,ho,ja,_o,vo,Q,Ra,Ca,bo,$o,ka,wo,Eo,Oa,xo,Do,Z,Aa,La,yo,To,Pa,Io,Bo,Va,So,Es,O,Je,No,Fa,jo,Ro,ee,Co,Ma,ko,Oo,Ua,Ao,Lo,xs,y,Ke,Po,xt,qa,Vo,Fo,Mo,Ha,Uo,qo,A,Dt,Ga,Ho,Go,Wo,yt,Wa,Xo,zo,Jo,Tt,Xa,Ko,Yo,Qo,It,za,Zo,ed,td,Bt,ad,Ja,sd,rd,Ye,nd,Ka,od,dd,Ds,w,Qe,ld,Ya,id,pd,Qa,cd,ud,Ze,md,Za,fd,gd,et,hd,es,_d,vd,tt,bd,ts,$d,wd,at,ys,te,st,Ed,as,xd,Ts,T,rt,Dd,ss,yd,Td,rs,Id,Bd,nt,Sd,_e,ot,Nd,ns,jd,Rd,M,dt,Cd,os,kd,Od,ds,Ad,Is,ae,lt,Ld,ls,Pd,Bs,L,it,Vd,is,Fd,Md,ve,pt,Ud,ps,qd,Ss;return Ee=new Ai({}),xe=new b({props:{name:"class datasets.DatasetBuilder",anchor:"datasets.DatasetBuilder",parameters:[{name:"cache_dir",val:": typing.Optional[str] = None"},{name:"name",val:": typing.Optional[str] = None"},{name:"hash",val:": typing.Optional[str] = None"},{name:"base_path",val:": typing.Optional[str] = None"},{name:"info",val:": typing.Optional[datasets.info.DatasetInfo] = None"},{name:"features",val:": typing.Optional[datasets.features.features.Features] = None"},{name:"use_auth_token",val:": typing.Union[str, bool, NoneType] = None"},{name:"namespace",val:": typing.Optional[str] = None"},{name:"data_files",val:": typing.Union[str, list, dict, datasets.data_files.DataFilesDict, NoneType] = None"},{name:"data_dir",val:": typing.Optional[str] = None"},{name:"**config_kwargs",val:""}],source:"https://github.com/huggingface/datasets/blob/pr_4175/src/datasets/builder.py#L177"}}),De=new b({props:{name:"as_dataset",anchor:"datasets.DatasetBuilder.as_dataset",parameters:[{name:"split",val:": typing.Optional[datasets.splits.Split] = None"},{name:"run_post_process",val:" = True"},{name:"ignore_verifications",val:" = False"},{name:"in_memory",val:" = False"}],parametersDescription:[{anchor:"datasets.DatasetBuilder.as_dataset.split",description:"<strong>split</strong> (<code>datasets.Split</code>) &#x2014; Which subset of the data to return.",name:"split"},{anchor:"datasets.DatasetBuilder.as_dataset.run_post_process",description:`<strong>run_post_process</strong> (bool, default=True) &#x2014; Whether to run post-processing dataset transforms and/or add
indexes.`,name:"run_post_process"},{anchor:"datasets.DatasetBuilder.as_dataset.ignore_verifications",description:`<strong>ignore_verifications</strong> (bool, default=False) &#x2014; Whether to ignore the verifications of the
downloaded/processed dataset information (checksums/size/splits/&#x2026;).`,name:"ignore_verifications"},{anchor:"datasets.DatasetBuilder.as_dataset.in_memory",description:"<strong>in_memory</strong> (bool, default=False) &#x2014; Whether to copy the data in-memory.",name:"in_memory"}],source:"https://github.com/huggingface/datasets/blob/pr_4175/src/datasets/builder.py#L744",returnDescription:`
<p>datasets.Dataset</p>
`}}),ye=new b({props:{name:"download_and_prepare",anchor:"datasets.DatasetBuilder.download_and_prepare",parameters:[{name:"download_config",val:": typing.Optional[datasets.utils.file_utils.DownloadConfig] = None"},{name:"download_mode",val:": typing.Optional[datasets.utils.download_manager.DownloadMode] = None"},{name:"ignore_verifications",val:": bool = False"},{name:"try_from_hf_gcs",val:": bool = True"},{name:"dl_manager",val:": typing.Optional[datasets.utils.download_manager.DownloadManager] = None"},{name:"base_path",val:": typing.Optional[str] = None"},{name:"use_auth_token",val:": typing.Union[str, bool, NoneType] = None"},{name:"**download_and_prepare_kwargs",val:""}],parametersDescription:[{anchor:"datasets.DatasetBuilder.download_and_prepare.download_config",description:'<strong>download_config</strong> (<a href="/docs/datasets/pr_4175/en/package_reference/builder_classes#datasets.DownloadConfig">DownloadConfig</a>, optional) &#x2014; specific download configuration parameters.',name:"download_config"},{anchor:"datasets.DatasetBuilder.download_and_prepare.download_mode",description:'<strong>download_mode</strong> (<a href="/docs/datasets/pr_4175/en/package_reference/builder_classes#datasets.DownloadMode">DownloadMode</a>, optional) &#x2014; select the download/generate mode - Default to <code>REUSE_DATASET_IF_EXISTS</code>',name:"download_mode"},{anchor:"datasets.DatasetBuilder.download_and_prepare.ignore_verifications",description:"<strong>ignore_verifications</strong> (<code>bool</code>) &#x2014; Ignore the verifications of the downloaded/processed dataset information (checksums/size/splits/&#x2026;)",name:"ignore_verifications"},{anchor:"datasets.DatasetBuilder.download_and_prepare.save_infos",description:"<strong>save_infos</strong> (<code>bool</code>) &#x2014; Save the dataset information (checksums/size/splits/&#x2026;)",name:"save_infos"},{anchor:"datasets.DatasetBuilder.download_and_prepare.try_from_hf_gcs",description:"<strong>try_from_hf_gcs</strong> (<code>bool</code>) &#x2014; If True, it will try to download the already prepared dataset from the Hf google cloud storage",name:"try_from_hf_gcs"},{anchor:"datasets.DatasetBuilder.download_and_prepare.dl_manager",description:'<strong>dl_manager</strong> (<a href="/docs/datasets/pr_4175/en/package_reference/builder_classes#datasets.DownloadManager">DownloadManager</a>, optional) &#x2014; specific Download Manger to use',name:"dl_manager"},{anchor:"datasets.DatasetBuilder.download_and_prepare.base_path",description:`<strong>base_path</strong> (<code>str</code>, optional) &#x2014; base path for relative paths that are used to download files. This can be a remote url.
If not specified, the value of the <em>base_path</em> attribute (<em>self.base_path</em>) will be used instead.`,name:"base_path"},{anchor:"datasets.DatasetBuilder.download_and_prepare.use_auth_token",description:`<strong>use_auth_token</strong> (<code>Union[str, bool]</code>, optional) &#x2014; Optional string or boolean to use as Bearer token for remote files on the Datasets Hub.
If True, will get token from ~/.huggingface.`,name:"use_auth_token"}],source:"https://github.com/huggingface/datasets/blob/pr_4175/src/datasets/builder.py#L487"}}),Te=new b({props:{name:"get_all_exported_dataset_infos",anchor:"datasets.DatasetBuilder.get_all_exported_dataset_infos",parameters:[],source:"https://github.com/huggingface/datasets/blob/pr_4175/src/datasets/builder.py#L320"}}),Ie=new b({props:{name:"get_exported_dataset_info",anchor:"datasets.DatasetBuilder.get_exported_dataset_info",parameters:[],source:"https://github.com/huggingface/datasets/blob/pr_4175/src/datasets/builder.py#L328"}}),Be=new b({props:{name:"get_imported_module_dir",anchor:"datasets.DatasetBuilder.get_imported_module_dir",parameters:[],source:"https://github.com/huggingface/datasets/blob/pr_4175/src/datasets/builder.py#L482"}}),Se=new b({props:{name:"class datasets.GeneratorBasedBuilder",anchor:"datasets.GeneratorBasedBuilder",parameters:[{name:"*args",val:""},{name:"writer_batch_size",val:" = None"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/datasets/blob/pr_4175/src/datasets/builder.py#L1012"}}),Ne=new b({props:{name:"class datasets.BeamBasedBuilder",anchor:"datasets.BeamBasedBuilder",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/datasets/blob/pr_4175/src/datasets/builder.py#L1172"}}),je=new b({props:{name:"class datasets.ArrowBasedBuilder",anchor:"datasets.ArrowBasedBuilder",parameters:[{name:"cache_dir",val:": typing.Optional[str] = None"},{name:"name",val:": typing.Optional[str] = None"},{name:"hash",val:": typing.Optional[str] = None"},{name:"base_path",val:": typing.Optional[str] = None"},{name:"info",val:": typing.Optional[datasets.info.DatasetInfo] = None"},{name:"features",val:": typing.Optional[datasets.features.features.Features] = None"},{name:"use_auth_token",val:": typing.Union[str, bool, NoneType] = None"},{name:"namespace",val:": typing.Optional[str] = None"},{name:"data_files",val:": typing.Union[str, list, dict, datasets.data_files.DataFilesDict, NoneType] = None"},{name:"data_dir",val:": typing.Optional[str] = None"},{name:"**config_kwargs",val:""}],source:"https://github.com/huggingface/datasets/blob/pr_4175/src/datasets/builder.py#L1110"}}),Re=new b({props:{name:"class datasets.BuilderConfig",anchor:"datasets.BuilderConfig",parameters:[{name:"name",val:": str = 'default'"},{name:"version",val:": typing.Union[str, datasets.utils.version.Version, NoneType] = '0.0.0'"},{name:"data_dir",val:": typing.Optional[str] = None"},{name:"data_files",val:": typing.Optional[datasets.data_files.DataFilesDict] = None"},{name:"description",val:": typing.Optional[str] = None"}],parametersDescription:[{anchor:"datasets.BuilderConfig.name",description:"<strong>name</strong> (<code>str</code>, default <code>&quot;default&quot;</code>) &#x2014;",name:"name"},{anchor:"datasets.BuilderConfig.version",description:'<strong>version</strong> (<a href="/docs/datasets/pr_4175/en/package_reference/builder_classes#datasets.Version">Version</a> or <code>str</code>, optional) &#x2014;',name:"version"},{anchor:"datasets.BuilderConfig.data_dir",description:"<strong>data_dir</strong> (<code>str</code>, optional) &#x2014;",name:"data_dir"},{anchor:"datasets.BuilderConfig.data_files",description:"<strong>data_files</strong> (<code>str</code> or <code>Sequence</code> or <code>Mapping</code>, optional) &#x2014; Path(s) to source data file(s).",name:"data_files"},{anchor:"datasets.BuilderConfig.description",description:"<strong>description</strong> (<code>str</code>, optional) &#x2014;",name:"description"}],source:"https://github.com/huggingface/datasets/blob/pr_4175/src/datasets/builder.py#L81"}}),Oe=new b({props:{name:"create_config_id",anchor:"datasets.BuilderConfig.create_config_id",parameters:[{name:"config_kwargs",val:": dict"},{name:"custom_features",val:": typing.Optional[datasets.features.features.Features] = None"}],source:"https://github.com/huggingface/datasets/blob/pr_4175/src/datasets/builder.py#L120"}}),Ae=new b({props:{name:"class datasets.DownloadManager",anchor:"datasets.DownloadManager",parameters:[{name:"dataset_name",val:": typing.Optional[str] = None"},{name:"data_dir",val:": typing.Optional[str] = None"},{name:"download_config",val:": typing.Optional[datasets.utils.file_utils.DownloadConfig] = None"},{name:"base_path",val:": typing.Optional[str] = None"},{name:"record_checksums",val:": bool = True"}],source:"https://github.com/huggingface/datasets/blob/pr_4175/src/datasets/utils/download_manager.py#L141"}}),Le=new b({props:{name:"download",anchor:"datasets.DownloadManager.download",parameters:[{name:"url_or_urls",val:""}],source:"https://github.com/huggingface/datasets/blob/pr_4175/src/datasets/utils/download_manager.py#L259",returnDescription:`
<p><code>str</code>, The downloaded paths matching the given input
url_or_urls.</p>
`,returnType:`
<p>downloaded_path(s)</p>
`}}),Pe=new b({props:{name:"download_and_extract",anchor:"datasets.DownloadManager.download_and_extract",parameters:[{name:"url_or_urls",val:""}],source:"https://github.com/huggingface/datasets/blob/pr_4175/src/datasets/utils/download_manager.py#L367",returnDescription:`
<p><code>str</code>, extracted paths of given URL(s).</p>
`,returnType:`
<p>extracted_path(s)</p>
`}}),Ve=new qt({props:{code:"extracted_paths = dl_manager.extract(dl_manager.download(url_or_urls))",highlighted:'<span class="hljs-attr">extracted_paths</span> = dl_manager.extract(dl_manager.download(url_or_urls))'}}),Fe=new b({props:{name:"download_custom",anchor:"datasets.DownloadManager.download_custom",parameters:[{name:"url_or_urls",val:""},{name:"custom_download",val:""}],source:"https://github.com/huggingface/datasets/blob/pr_4175/src/datasets/utils/download_manager.py#L218",returnDescription:`
<p><code>str</code>, The downloaded paths matching the given input
url_or_urls.</p>
`,returnType:`
<p>downloaded_path(s)</p>
`}}),Ue=new b({props:{name:"extract",anchor:"datasets.DownloadManager.extract",parameters:[{name:"path_or_paths",val:""},{name:"num_proc",val:" = None"}],source:"https://github.com/huggingface/datasets/blob/pr_4175/src/datasets/utils/download_manager.py#L337",returnDescription:`
<p><code>str</code>, The extracted paths matching the given input
path_or_paths.</p>
`,returnType:`
<p>extracted_path(s)</p>
`}}),qe=new b({props:{name:"iter_archive",anchor:"datasets.DownloadManager.iter_archive",parameters:[{name:"path_or_buf",val:": typing.Union[str, _io.BufferedReader]"}],parametersDescription:[{anchor:"datasets.DownloadManager.iter_archive.path_or_buf",description:"<strong>path_or_buf</strong> (<code>str</code> or <code>io.BufferedReader</code>) &#x2014; Archive path or archive binary file object.",name:"path_or_buf"}],source:"https://github.com/huggingface/datasets/blob/pr_4175/src/datasets/utils/download_manager.py#L310"}}),He=new b({props:{name:"iter_files",anchor:"datasets.DownloadManager.iter_files",parameters:[{name:"paths",val:": typing.Union[str, typing.List[str]]"}],parametersDescription:[{anchor:"datasets.DownloadManager.iter_files.paths",description:"<strong>paths</strong> (<code>str</code> or <code>list</code> of <code>str</code>) &#x2014; Root paths.",name:"paths"}],source:"https://github.com/huggingface/datasets/blob/pr_4175/src/datasets/utils/download_manager.py#L326"}}),Ge=new b({props:{name:"ship_files_with_pipeline",anchor:"datasets.DownloadManager.ship_files_with_pipeline",parameters:[{name:"downloaded_path_or_paths",val:""},{name:"pipeline",val:""}],source:"https://github.com/huggingface/datasets/blob/pr_4175/src/datasets/utils/download_manager.py#L183"}}),We=new b({props:{name:"class datasets.DownloadMode",anchor:"datasets.DownloadMode",parameters:[{name:"value",val:""},{name:"names",val:" = None"},{name:"module",val:" = None"},{name:"qualname",val:" = None"},{name:"type",val:" = None"},{name:"start",val:" = 1"}],source:"https://github.com/huggingface/datasets/blob/pr_4175/src/datasets/utils/download_manager.py#L44"}}),Je=new b({props:{name:"class datasets.SplitGenerator",anchor:"datasets.SplitGenerator",parameters:[{name:"name",val:": str"},{name:"gen_kwargs",val:": typing.Dict = <factory>"}],parametersDescription:[{anchor:"datasets.SplitGenerator.name",description:`<strong>name</strong> (str) &#x2014; Name of the Split for which the generator will
create the examples.
**gen_kwargs &#x2014; Keyword arguments to forward to the <code>DatasetBuilder._generate_examples</code> method
of the builder.`,name:"name"}],source:"https://github.com/huggingface/datasets/blob/pr_4175/src/datasets/splits.py#L549"}}),Ke=new b({props:{name:"class datasets.Split",anchor:"datasets.Split",parameters:[{name:"name",val:""}],source:"https://github.com/huggingface/datasets/blob/pr_4175/src/datasets/splits.py#L387"}}),Qe=new b({props:{name:"class datasets.NamedSplit",anchor:"datasets.NamedSplit",parameters:[{name:"name",val:""}],source:"https://github.com/huggingface/datasets/blob/pr_4175/src/datasets/splits.py#L303"}}),Ze=new qt({props:{code:`Each descriptor can be composed with other using addition or slice. Ex
split = datasets.Split.TRAIN.subsplit(datasets.percent[0:25]) + datasets.Split.TEST

The resulting split will correspond to 25% of the train split merged with
100% of the test split.`,highlighted:`Each descriptor can be composed <span class="hljs-keyword">with</span> other using addition <span class="hljs-keyword">or</span> <span class="hljs-built_in">slice</span>. Ex
split = datasets.Split.TRAIN.subsplit(datasets.percent[<span class="hljs-number">0</span>:<span class="hljs-number">25</span>]) + datasets.Split.TEST

The resulting split will correspond to <span class="hljs-number">25</span>% of the train split merged <span class="hljs-keyword">with</span>
<span class="hljs-number">100</span>% of the test split.`}}),et=new qt({props:{code:`split = (
        datasets.Split.TRAIN.subsplit(datasets.percent[:25]) +
        datasets.Split.TRAIN.subsplit(datasets.percent[75:])
)  # Error
split = datasets.Split.TEST + datasets.Split.ALL  # Error`,highlighted:`split = (
        datasets.Split.TRAIN.subsplit(datasets.percent[:<span class="hljs-number">25</span>]) +
        datasets.Split.TRAIN.subsplit(datasets.percent[<span class="hljs-number">75</span>:])
)  <span class="hljs-comment"># Error</span>
split = datasets.Split.TEST + datasets.Split.ALL  <span class="hljs-comment"># Error</span>`}}),tt=new qt({props:{code:`split = (
datasets.Split.TRAIN.subsplit(datasets.percent[:25]) +
datasets.Split.TEST.subsplit(datasets.percent[:50])
)
split = (datasets.Split.TRAIN + datasets.Split.TEST).subsplit(datasets.percent[:50])`,highlighted:`split = (
datasets.Split.TRAIN.subsplit(datasets.percent[:<span class="hljs-number">25</span>]) +
datasets.Split.TEST.subsplit(datasets.percent[:<span class="hljs-number">50</span>])
)
split = (datasets.Split.TRAIN + datasets.Split.TEST).subsplit(datasets.percent[:<span class="hljs-number">50</span>])`}}),at=new qt({props:{code:`train = datasets.Split.TRAIN
test = datasets.Split.TEST
split = train.subsplit(datasets.percent[:25]).subsplit(datasets.percent[:25])
split = (train.subsplit(datasets.percent[:25]) + test).subsplit(datasets.percent[:50])`,highlighted:`train = datasets.Split.TRAIN
test = datasets.Split.TEST
split = train.subsplit(datasets.percent[:<span class="hljs-number">25</span>]).subsplit(datasets.percent[:<span class="hljs-number">25</span>])
split = (train.subsplit(datasets.percent[:<span class="hljs-number">25</span>]) + test).subsplit(datasets.percent[:<span class="hljs-number">50</span>])`}}),st=new b({props:{name:"class datasets.NamedSplitAll",anchor:"datasets.NamedSplitAll",parameters:[],source:"https://github.com/huggingface/datasets/blob/pr_4175/src/datasets/splits.py#L372"}}),rt=new b({props:{name:"class datasets.ReadInstruction",anchor:"datasets.ReadInstruction",parameters:[{name:"split_name",val:""},{name:"rounding",val:" = None"},{name:"from_",val:" = None"},{name:"to",val:" = None"},{name:"unit",val:" = None"}],source:"https://github.com/huggingface/datasets/blob/pr_4175/src/datasets/arrow_reader.py#L456"}}),nt=new qt({props:{code:`# The following lines are equivalent:
ds = datasets.load_dataset('mnist', split='test[:33%]')
ds = datasets.load_dataset('mnist', split=datasets.ReadInstruction.from_spec('test[:33%]'))
ds = datasets.load_dataset('mnist', split=datasets.ReadInstruction('test', to=33, unit='%'))
ds = datasets.load_dataset('mnist', split=datasets.ReadInstruction(
'test', from_=0, to=33, unit='%'))

# The following lines are equivalent:
ds = datasets.load_dataset('mnist', split='test[:33%]+train[1:-1]')
ds = datasets.load_dataset('mnist', split=datasets.ReadInstruction.from_spec(
'test[:33%]+train[1:-1]'))
ds = datasets.load_dataset('mnist', split=(
datasets.ReadInstruction('test', to=33, unit='%') +
datasets.ReadInstruction('train', from_=1, to=-1, unit='abs')))

# The following lines are equivalent:
ds = datasets.load_dataset('mnist', split='test[:33%](pct1_dropremainder)')
ds = datasets.load_dataset('mnist', split=datasets.ReadInstruction.from_spec(
'test[:33%](pct1_dropremainder)'))
ds = datasets.load_dataset('mnist', split=datasets.ReadInstruction(
'test', from_=0, to=33, unit='%', rounding="pct1_dropremainder"))

# 10-fold validation:
tests = datasets.load_dataset(
'mnist',
[datasets.ReadInstruction('train', from_=k, to=k+10, unit='%')
for k in range(0, 100, 10)])
trains = datasets.load_dataset(
'mnist',
[datasets.ReadInstruction('train', to=k, unit='%') + datasets.ReadInstruction('train', from_=k+10, unit='%')
for k in range(0, 100, 10)])`,highlighted:`<span class="hljs-comment"># The following lines are equivalent:</span>
ds = datasets.load_dataset(<span class="hljs-string">&#x27;mnist&#x27;</span>, split=<span class="hljs-string">&#x27;test[:33%]&#x27;</span>)
ds = datasets.load_dataset(<span class="hljs-string">&#x27;mnist&#x27;</span>, split=datasets.ReadInstruction.from_spec(<span class="hljs-string">&#x27;test[:33%]&#x27;</span>))
ds = datasets.load_dataset(<span class="hljs-string">&#x27;mnist&#x27;</span>, split=datasets.ReadInstruction(<span class="hljs-string">&#x27;test&#x27;</span>, to=<span class="hljs-number">33</span>, unit=<span class="hljs-string">&#x27;%&#x27;</span>))
ds = datasets.load_dataset(<span class="hljs-string">&#x27;mnist&#x27;</span>, split=datasets.ReadInstruction(
<span class="hljs-string">&#x27;test&#x27;</span>, from_=<span class="hljs-number">0</span>, to=<span class="hljs-number">33</span>, unit=<span class="hljs-string">&#x27;%&#x27;</span>))

<span class="hljs-comment"># The following lines are equivalent:</span>
ds = datasets.load_dataset(<span class="hljs-string">&#x27;mnist&#x27;</span>, split=<span class="hljs-string">&#x27;test[:33%]+train[1:-1]&#x27;</span>)
ds = datasets.load_dataset(<span class="hljs-string">&#x27;mnist&#x27;</span>, split=datasets.ReadInstruction.from_spec(
<span class="hljs-string">&#x27;test[:33%]+train[1:-1]&#x27;</span>))
ds = datasets.load_dataset(<span class="hljs-string">&#x27;mnist&#x27;</span>, split=(
datasets.ReadInstruction(<span class="hljs-string">&#x27;test&#x27;</span>, to=<span class="hljs-number">33</span>, unit=<span class="hljs-string">&#x27;%&#x27;</span>) +
datasets.ReadInstruction(<span class="hljs-string">&#x27;train&#x27;</span>, from_=<span class="hljs-number">1</span>, to=-<span class="hljs-number">1</span>, unit=<span class="hljs-string">&#x27;abs&#x27;</span>)))

<span class="hljs-comment"># The following lines are equivalent:</span>
ds = datasets.load_dataset(<span class="hljs-string">&#x27;mnist&#x27;</span>, split=<span class="hljs-string">&#x27;test[:33%](pct1_dropremainder)&#x27;</span>)
ds = datasets.load_dataset(<span class="hljs-string">&#x27;mnist&#x27;</span>, split=datasets.ReadInstruction.from_spec(
<span class="hljs-string">&#x27;test[:33%](pct1_dropremainder)&#x27;</span>))
ds = datasets.load_dataset(<span class="hljs-string">&#x27;mnist&#x27;</span>, split=datasets.ReadInstruction(
<span class="hljs-string">&#x27;test&#x27;</span>, from_=<span class="hljs-number">0</span>, to=<span class="hljs-number">33</span>, unit=<span class="hljs-string">&#x27;%&#x27;</span>, rounding=<span class="hljs-string">&quot;pct1_dropremainder&quot;</span>))

<span class="hljs-comment"># 10-fold validation:</span>
tests = datasets.load_dataset(
<span class="hljs-string">&#x27;mnist&#x27;</span>,
[datasets.ReadInstruction(<span class="hljs-string">&#x27;train&#x27;</span>, from_=k, to=k+<span class="hljs-number">10</span>, unit=<span class="hljs-string">&#x27;%&#x27;</span>)
<span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, <span class="hljs-number">100</span>, <span class="hljs-number">10</span>)])
trains = datasets.load_dataset(
<span class="hljs-string">&#x27;mnist&#x27;</span>,
[datasets.ReadInstruction(<span class="hljs-string">&#x27;train&#x27;</span>, to=k, unit=<span class="hljs-string">&#x27;%&#x27;</span>) + datasets.ReadInstruction(<span class="hljs-string">&#x27;train&#x27;</span>, from_=k+<span class="hljs-number">10</span>, unit=<span class="hljs-string">&#x27;%&#x27;</span>)
<span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, <span class="hljs-number">100</span>, <span class="hljs-number">10</span>)])`}}),ot=new b({props:{name:"from_spec",anchor:"datasets.ReadInstruction.from_spec",parameters:[{name:"spec",val:""}],parametersDescription:[{anchor:"datasets.ReadInstruction.from_spec.spec",description:`<strong>spec</strong> (str) &#x2014; split(s) + optional slice(s) to read + optional rounding
if percents are used as the slicing unit. A slice can be specified,
using absolute numbers (int) or percentages (int). E.g.
<code>test</code>: test split.
<code>test + validation</code>: test split + validation split.
<code>test[10:]</code>: test split, minus its first 10 records.
<code>test[:10%]</code>: first 10% records of test split.
<code>test[:20%](pct1_dropremainder)</code>: first 10% records, rounded with
the <code>pct1_dropremainder</code> rounding.
<code>test[:-5%]+train[40%:60%]</code>: first 95% of test + middle 20% of
train.`,name:"spec"}],source:"https://github.com/huggingface/datasets/blob/pr_4175/src/datasets/arrow_reader.py#L536",returnDescription:`
<p>ReadInstruction instance.</p>
`}}),dt=new b({props:{name:"to_absolute",anchor:"datasets.ReadInstruction.to_absolute",parameters:[{name:"name2len",val:""}],source:"https://github.com/huggingface/datasets/blob/pr_4175/src/datasets/arrow_reader.py#L604",returnDescription:`
<p>list of _AbsoluteInstruction instances (corresponds to the + in spec).</p>
`}}),lt=new b({props:{name:"class datasets.DownloadConfig",anchor:"datasets.DownloadConfig",parameters:[{name:"cache_dir",val:": typing.Union[pathlib.Path, str, NoneType] = None"},{name:"force_download",val:": bool = False"},{name:"resume_download",val:": bool = False"},{name:"local_files_only",val:": bool = False"},{name:"proxies",val:": typing.Optional[typing.Dict] = None"},{name:"user_agent",val:": typing.Optional[str] = None"},{name:"extract_compressed_file",val:": bool = False"},{name:"force_extract",val:": bool = False"},{name:"delete_extracted",val:": bool = False"},{name:"use_etag",val:": bool = True"},{name:"num_proc",val:": typing.Optional[int] = None"},{name:"max_retries",val:": int = 1"},{name:"use_auth_token",val:": typing.Union[bool, str, NoneType] = None"},{name:"ignore_url_params",val:": bool = False"},{name:"download_desc",val:": typing.Optional[str] = None"}],parametersDescription:[{anchor:"datasets.DownloadConfig.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>Path</code>, optional) &#x2014; Specify a cache directory to save the file to (overwrite the
default cache dir).`,name:"cache_dir"},{anchor:"datasets.DownloadConfig.force_download",description:`<strong>force_download</strong> (<code>bool</code>, default <code>False</code>) &#x2014; If True, re-dowload the file even if it&#x2019;s already cached in
the cache dir.`,name:"force_download"},{anchor:"datasets.DownloadConfig.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, default <code>False</code>) &#x2014; If True, resume the download if incompletly recieved file is
found.`,name:"resume_download"},{anchor:"datasets.DownloadConfig.proxies",description:"<strong>proxies</strong> (<code>dict</code>, optional) &#x2014;",name:"proxies"},{anchor:"datasets.DownloadConfig.user_agent",description:`<strong>user_agent</strong> (<code>str</code>, optional) &#x2014; Optional string or dict that will be appended to the user-agent on remote
requests.`,name:"user_agent"},{anchor:"datasets.DownloadConfig.extract_compressed_file",description:`<strong>extract_compressed_file</strong> (<code>bool</code>, default <code>False</code>) &#x2014; If True and the path point to a zip or tar file,
extract the compressed file in a folder along the archive.`,name:"extract_compressed_file"},{anchor:"datasets.DownloadConfig.force_extract",description:`<strong>force_extract</strong> (<code>bool</code>, default <code>False</code>) &#x2014; If True when extract_compressed_file is True and the archive
was already extracted, re-extract the archive and override the folder where it was extracted.`,name:"force_extract"},{anchor:"datasets.DownloadConfig.delete_extracted",description:"<strong>delete_extracted</strong> (<code>bool</code>, default <code>False</code>) &#x2014; Whether to delete (or keep) the extracted files.",name:"delete_extracted"},{anchor:"datasets.DownloadConfig.use_etag",description:"<strong>use_etag</strong> (<code>bool</code>, default <code>True</code>) &#x2014; Whether to use the ETag HTTP response header to validate the cached files.",name:"use_etag"},{anchor:"datasets.DownloadConfig.num_proc",description:"<strong>num_proc</strong> (<code>int</code>, optional) &#x2014; The number of processes to launch to download the files in parallel.",name:"num_proc"},{anchor:"datasets.DownloadConfig.max_retries",description:"<strong>max_retries</strong> (<code>int</code>, default <code>1</code>) &#x2014; The number of times to retry an HTTP request if it fails.",name:"max_retries"},{anchor:"datasets.DownloadConfig.use_auth_token",description:`<strong>use_auth_token</strong> (<code>str</code> or <code>bool</code>, optional) &#x2014; Optional string or boolean to use as Bearer token
for remote files on the Datasets Hub. If True, will get token from ~/.huggingface.`,name:"use_auth_token"},{anchor:"datasets.DownloadConfig.ignore_url_params",description:`<strong>ignore_url_params</strong> (<code>bool</code>, default <code>False</code>) &#x2014; Whether to strip all query parameters and #fragments from
the download URL before using it for caching the file.`,name:"ignore_url_params"},{anchor:"datasets.DownloadConfig.download_desc",description:"<strong>download_desc</strong> (<code>str</code>, optional) &#x2014; A description to be displayed alongside with the progress bar while downloading the files.",name:"download_desc"}],source:"https://github.com/huggingface/datasets/blob/pr_4175/src/datasets/utils/file_utils.py#L153"}}),it=new b({props:{name:"class datasets.Version",anchor:"datasets.Version",parameters:[{name:"version_str",val:": str"},{name:"description",val:": typing.Optional[str] = None"},{name:"major",val:": typing.Union[str, int, NoneType] = None"},{name:"minor",val:": typing.Union[str, int, NoneType] = None"},{name:"patch",val:": typing.Union[str, int, NoneType] = None"}],parametersDescription:[{anchor:"datasets.Version.version_str",description:"<strong>version_str</strong> (<code>str</code>) &#x2014; Eg: &#x201C;1.2.3&#x201D;.",name:"version_str"},{anchor:"datasets.Version.description",description:"<strong>description</strong> (<code>str</code>) &#x2014; A description of what is new in this version.",name:"description"},{anchor:"datasets.Version.version_str",description:"<strong>version_str</strong> (<code>str</code>) &#x2014; Eg: &#x201C;1.2.3&#x201D;.",name:"version_str"},{anchor:"datasets.Version.description",description:"<strong>description</strong> (<code>str</code>) &#x2014; A description of what is new in this version.",name:"description"},{anchor:"datasets.Version.major",description:"<strong>major</strong> (<code>str</code>) &#x2014;",name:"major"},{anchor:"datasets.Version.minor",description:"<strong>minor</strong> (<code>str</code>) &#x2014;",name:"minor"},{anchor:"datasets.Version.patch",description:"<strong>patch</strong> (<code>str</code>) &#x2014;",name:"patch"}],source:"https://github.com/huggingface/datasets/blob/pr_4175/src/datasets/utils/version.py#L30"}}),pt=new b({props:{name:"match",anchor:"datasets.Version.match",parameters:[{name:"other_version",val:""}],source:"https://github.com/huggingface/datasets/blob/pr_4175/src/datasets/utils/version.py#L92"}}),{c(){q=s("meta"),us=l(),H=s("h1"),re=s("a"),Ht=s("span"),c(Ee.$$.fragment),er=l(),Gt=s("span"),tr=o("Builder classes"),ms=l(),P=s("p"),ar=o("\u{1F917} Datasets relies on two main classes during the dataset building process: "),ut=s("a"),sr=o("DatasetBuilder"),rr=o(" and "),mt=s("a"),nr=o("BuilderConfig"),or=o("."),fs=l(),$=s("div"),c(xe.$$.fragment),dr=l(),Wt=s("p"),lr=o("Abstract base class for all datasets."),ir=l(),ft=s("p"),Xt=s("em"),pr=o("DatasetBuilder"),cr=o(" has 3 key methods:"),ur=l(),G=s("ul"),gt=s("li"),zt=s("code"),mr=o("datasets.DatasetBuilder.info"),fr=o(`: Documents the dataset, including feature
names, types, and shapes, version, splits, citation, etc.`),gr=l(),ht=s("li"),_t=s("a"),hr=o("datasets.DatasetBuilder.download_and_prepare()"),_r=o(`: Downloads the source data
and writes it to disk.`),vr=l(),ne=s("li"),vt=s("a"),br=o("datasets.DatasetBuilder.as_dataset()"),$r=o(": Generates a "),Jt=s("em"),wr=o("Dataset"),Er=o("."),xr=l(),N=s("p"),Kt=s("strong"),Dr=o("Configuration"),yr=o(": Some "),Yt=s("em"),Tr=o("DatasetBuilder"),Ir=o(`s expose multiple variants of the
dataset by defining a `),Qt=s("em"),Br=o("datasets.BuilderConfig"),Sr=o(` subclass and accepting a
config object (or name) on construction. Configurable datasets expose a
pre-defined set of configurations in `),Zt=s("code"),Nr=o("datasets.DatasetBuilder.builder_configs()"),jr=o("."),Rr=l(),oe=s("div"),c(De.$$.fragment),Cr=l(),ea=s("p"),kr=o("Return a Dataset for the specified split."),Or=l(),de=s("div"),c(ye.$$.fragment),Ar=l(),ta=s("p"),Lr=o("Downloads and prepares dataset for reading."),Pr=l(),le=s("div"),c(Te.$$.fragment),Vr=l(),aa=s("p"),Fr=o("Empty dict if doesn\u2019t exist"),Mr=l(),ie=s("div"),c(Ie.$$.fragment),Ur=l(),sa=s("p"),qr=o("Empty DatasetInfo if doesn\u2019t exist"),Hr=l(),pe=s("div"),c(Be.$$.fragment),Gr=l(),ra=s("p"),Wr=o("Return the path of the module of this class or subclass."),gs=l(),k=s("div"),c(Se.$$.fragment),Xr=l(),na=s("p"),zr=o("Base class for datasets with data generation based on dict generators."),Jr=l(),V=s("p"),oa=s("code"),Kr=o("GeneratorBasedBuilder"),Yr=o(` is a convenience class that abstracts away much
of the data writing and reading of `),da=s("code"),Qr=o("DatasetBuilder"),Zr=o(`. It expects subclasses to
implement generators of feature dictionaries across the dataset splits
(`),la=s("code"),en=o("_split_generators"),tn=o("). See the method docstrings for details."),hs=l(),W=s("div"),c(Ne.$$.fragment),an=l(),ia=s("p"),sn=o("Beam based Builder."),_s=l(),X=s("div"),c(je.$$.fragment),rn=l(),pa=s("p"),nn=o("Base class for datasets with data generation based on Arrow loading functions (CSV/JSON/Parquet)."),vs=l(),S=s("div"),c(Re.$$.fragment),on=l(),Ce=s("p"),dn=o("Base class for "),bt=s("a"),ln=o("DatasetBuilder"),pn=o(" data configuration."),cn=l(),ke=s("p"),un=o(`DatasetBuilder subclasses with data configuration options should subclass
`),$t=s("a"),mn=o("BuilderConfig"),fn=o(" and add their own properties."),gn=l(),F=s("div"),c(Oe.$$.fragment),hn=l(),ca=s("p"),_n=o(`The config id is used to build the cache directory.
By default it is equal to the config name.
However the name of a config is not sufficient to have a unique identifier for the dataset being generated
since it doesn\u2019t take into account:`),vn=l(),z=s("ul"),ua=s("li"),bn=o("the config kwargs that can be used to overwrite attributes"),$n=l(),ma=s("li"),wn=o("the custom features used to write the dataset"),En=l(),fa=s("li"),xn=o(`the data_files for json/text/csv/pandas datasets
Therefore the config id is just the config name with an optional suffix based on these.`),bs=l(),E=s("div"),c(Ae.$$.fragment),Dn=l(),ce=s("div"),c(Le.$$.fragment),yn=l(),ga=s("p"),Tn=o("Download given url(s)."),In=l(),j=s("div"),c(Pe.$$.fragment),Bn=l(),ha=s("p"),Sn=o("Download and extract given url_or_urls."),Nn=l(),_a=s("p"),jn=o("Is roughly equivalent to:"),Rn=l(),c(Ve.$$.fragment),Cn=l(),ue=s("div"),c(Fe.$$.fragment),kn=l(),Me=s("p"),On=o("Download given urls(s) by calling "),va=s("code"),An=o("custom_download"),Ln=o("."),Pn=l(),me=s("div"),c(Ue.$$.fragment),Vn=l(),ba=s("p"),Fn=o("Extract given path(s)."),Mn=l(),fe=s("div"),c(qe.$$.fragment),Un=l(),$a=s("p"),qn=o("Iterate over files within an archive."),Hn=l(),ge=s("div"),c(He.$$.fragment),Gn=l(),wa=s("p"),Wn=o("Iterate over file paths."),Xn=l(),he=s("div"),c(Ge.$$.fragment),zn=l(),Ea=s("p"),Jn=o("Ship the files using Beam FileSystems to the pipeline temp dir."),$s=l(),B=s("div"),c(We.$$.fragment),Kn=l(),wt=s("p"),xa=s("code"),Yn=o("Enum"),Qn=o(" for how to treat pre-existing downloads and data."),Zn=l(),Xe=s("p"),eo=o("The default mode is "),Da=s("code"),to=o("REUSE_DATASET_IF_EXISTS"),ao=o(`, which will reuse both
raw downloads and the prepared dataset if they exist.`),so=l(),ya=s("p"),ro=o("The generations modes:"),no=l(),ze=s("table"),Ta=s("thead"),J=s("tr"),ws=s("th"),oo=l(),Ia=s("th"),lo=o("Downloads"),io=l(),Ba=s("th"),po=o("Dataset"),co=l(),K=s("tbody"),Y=s("tr"),Et=s("td"),Sa=s("code"),uo=o("REUSE_DATASET_IF_EXISTS"),mo=o(" (default)"),fo=l(),Na=s("td"),go=o("Reuse"),ho=l(),ja=s("td"),_o=o("Reuse"),vo=l(),Q=s("tr"),Ra=s("td"),Ca=s("code"),bo=o("REUSE_CACHE_IF_EXISTS"),$o=l(),ka=s("td"),wo=o("Reuse"),Eo=l(),Oa=s("td"),xo=o("Fresh"),Do=l(),Z=s("tr"),Aa=s("td"),La=s("code"),yo=o("FORCE_REDOWNLOAD"),To=l(),Pa=s("td"),Io=o("Fresh"),Bo=l(),Va=s("td"),So=o("Fresh"),Es=l(),O=s("div"),c(Je.$$.fragment),No=l(),Fa=s("p"),jo=o("Defines the split information for the generator."),Ro=l(),ee=s("p"),Co=o(`This should be used as returned value of
`),Ma=s("code"),ko=o("GeneratorBasedBuilder._split_generators()"),Oo=o(`.
See `),Ua=s("code"),Ao=o("GeneratorBasedBuilder._split_generators()"),Lo=o(` for more info and example
of usage.`),xs=l(),y=s("div"),c(Ke.$$.fragment),Po=l(),xt=s("p"),qa=s("code"),Vo=o("Enum"),Fo=o(" for dataset splits."),Mo=l(),Ha=s("p"),Uo=o(`Datasets are typically split into different subsets to be used at various
stages of training and evaluation.`),qo=l(),A=s("ul"),Dt=s("li"),Ga=s("code"),Ho=o("TRAIN"),Go=o(": the training data."),Wo=l(),yt=s("li"),Wa=s("code"),Xo=o("VALIDATION"),zo=o(`: the validation data. If present, this is typically used as
evaluation data while iterating on a model (e.g. changing hyperparameters,
model architecture, etc.).`),Jo=l(),Tt=s("li"),Xa=s("code"),Ko=o("TEST"),Yo=o(`: the testing data. This is the data to report metrics on. Typically
you do not want to use this during model iteration as you may overfit to it.`),Qo=l(),It=s("li"),za=s("code"),Zo=o("ALL"),ed=o(": the union of all defined dataset splits."),td=l(),Bt=s("p"),ad=o("Note: All splits, including compositions inherit from "),Ja=s("code"),sd=o("datasets.SplitBase"),rd=l(),Ye=s("p"),nd=o("See the :doc:"),Ka=s("code"),od=o("guide on splits </loading>"),dd=o(" for more information."),Ds=l(),w=s("div"),c(Qe.$$.fragment),ld=l(),Ya=s("p"),id=o("Descriptor corresponding to a named split (train, test, \u2026)."),pd=l(),Qa=s("p"),cd=o("Example:"),ud=l(),c(Ze.$$.fragment),md=l(),Za=s("p"),fd=o(`Warning:
A split cannot be added twice, so the following will fail:`),gd=l(),c(et.$$.fragment),hd=l(),es=s("p"),_d=o(`Warning:
The slices can be applied only one time. So the following are valid:`),vd=l(),c(tt.$$.fragment),bd=l(),ts=s("p"),$d=o("But not:"),wd=l(),c(at.$$.fragment),ys=l(),te=s("div"),c(st.$$.fragment),Ed=l(),as=s("p"),xd=o("Split corresponding to the union of all defined dataset splits."),Ts=l(),T=s("div"),c(rt.$$.fragment),Dd=l(),ss=s("p"),yd=o("Reading instruction for a dataset."),Td=l(),rs=s("p"),Id=o("Examples:"),Bd=l(),c(nt.$$.fragment),Sd=l(),_e=s("div"),c(ot.$$.fragment),Nd=l(),ns=s("p"),jd=o("Creates a ReadInstruction instance out of a string spec."),Rd=l(),M=s("div"),c(dt.$$.fragment),Cd=l(),os=s("p"),kd=o("Translate instruction into a list of absolute instructions."),Od=l(),ds=s("p"),Ad=o("Those absolute instructions are then to be added together."),Is=l(),ae=s("div"),c(lt.$$.fragment),Ld=l(),ls=s("p"),Pd=o("Configuration for our cached path manager."),Bs=l(),L=s("div"),c(it.$$.fragment),Vd=l(),is=s("p"),Fd=o("Dataset version MAJOR.MINOR.PATCH."),Md=l(),ve=s("div"),c(pt.$$.fragment),Ud=l(),ps=s("p"),qd=o("Returns True if other_version matches."),this.h()},l(a){const _=Ci('[data-svelte="svelte-1phssyn"]',document.head);q=r(_,"META",{name:!0,content:!0}),_.forEach(t),us=i(a),H=r(a,"H1",{class:!0});var Ns=n(H);re=r(Ns,"A",{id:!0,class:!0,href:!0});var al=n(re);Ht=r(al,"SPAN",{});var sl=n(Ht);u(Ee.$$.fragment,sl),sl.forEach(t),al.forEach(t),er=i(Ns),Gt=r(Ns,"SPAN",{});var rl=n(Gt);tr=d(rl,"Builder classes"),rl.forEach(t),Ns.forEach(t),ms=i(a),P=r(a,"P",{});var St=n(P);ar=d(St,"\u{1F917} Datasets relies on two main classes during the dataset building process: "),ut=r(St,"A",{href:!0});var nl=n(ut);sr=d(nl,"DatasetBuilder"),nl.forEach(t),rr=d(St," and "),mt=r(St,"A",{href:!0});var ol=n(mt);nr=d(ol,"BuilderConfig"),ol.forEach(t),or=d(St,"."),St.forEach(t),fs=i(a),$=r(a,"DIV",{class:!0});var x=n($);u(xe.$$.fragment,x),dr=i(x),Wt=r(x,"P",{});var dl=n(Wt);lr=d(dl,"Abstract base class for all datasets."),dl.forEach(t),ir=i(x),ft=r(x,"P",{});var Hd=n(ft);Xt=r(Hd,"EM",{});var ll=n(Xt);pr=d(ll,"DatasetBuilder"),ll.forEach(t),cr=d(Hd," has 3 key methods:"),Hd.forEach(t),ur=i(x),G=r(x,"UL",{});var Nt=n(G);gt=r(Nt,"LI",{});var Gd=n(gt);zt=r(Gd,"CODE",{});var il=n(zt);mr=d(il,"datasets.DatasetBuilder.info"),il.forEach(t),fr=d(Gd,`: Documents the dataset, including feature
names, types, and shapes, version, splits, citation, etc.`),Gd.forEach(t),gr=i(Nt),ht=r(Nt,"LI",{});var Wd=n(ht);_t=r(Wd,"A",{href:!0});var pl=n(_t);hr=d(pl,"datasets.DatasetBuilder.download_and_prepare()"),pl.forEach(t),_r=d(Wd,`: Downloads the source data
and writes it to disk.`),Wd.forEach(t),vr=i(Nt),ne=r(Nt,"LI",{});var cs=n(ne);vt=r(cs,"A",{href:!0});var cl=n(vt);br=d(cl,"datasets.DatasetBuilder.as_dataset()"),cl.forEach(t),$r=d(cs,": Generates a "),Jt=r(cs,"EM",{});var ul=n(Jt);wr=d(ul,"Dataset"),ul.forEach(t),Er=d(cs,"."),cs.forEach(t),Nt.forEach(t),xr=i(x),N=r(x,"P",{});var se=n(N);Kt=r(se,"STRONG",{});var ml=n(Kt);Dr=d(ml,"Configuration"),ml.forEach(t),yr=d(se,": Some "),Yt=r(se,"EM",{});var fl=n(Yt);Tr=d(fl,"DatasetBuilder"),fl.forEach(t),Ir=d(se,`s expose multiple variants of the
dataset by defining a `),Qt=r(se,"EM",{});var gl=n(Qt);Br=d(gl,"datasets.BuilderConfig"),gl.forEach(t),Sr=d(se,` subclass and accepting a
config object (or name) on construction. Configurable datasets expose a
pre-defined set of configurations in `),Zt=r(se,"CODE",{});var hl=n(Zt);Nr=d(hl,"datasets.DatasetBuilder.builder_configs()"),hl.forEach(t),jr=d(se,"."),se.forEach(t),Rr=i(x),oe=r(x,"DIV",{class:!0});var js=n(oe);u(De.$$.fragment,js),Cr=i(js),ea=r(js,"P",{});var _l=n(ea);kr=d(_l,"Return a Dataset for the specified split."),_l.forEach(t),js.forEach(t),Or=i(x),de=r(x,"DIV",{class:!0});var Rs=n(de);u(ye.$$.fragment,Rs),Ar=i(Rs),ta=r(Rs,"P",{});var vl=n(ta);Lr=d(vl,"Downloads and prepares dataset for reading."),vl.forEach(t),Rs.forEach(t),Pr=i(x),le=r(x,"DIV",{class:!0});var Cs=n(le);u(Te.$$.fragment,Cs),Vr=i(Cs),aa=r(Cs,"P",{});var bl=n(aa);Fr=d(bl,"Empty dict if doesn\u2019t exist"),bl.forEach(t),Cs.forEach(t),Mr=i(x),ie=r(x,"DIV",{class:!0});var ks=n(ie);u(Ie.$$.fragment,ks),Ur=i(ks),sa=r(ks,"P",{});var $l=n(sa);qr=d($l,"Empty DatasetInfo if doesn\u2019t exist"),$l.forEach(t),ks.forEach(t),Hr=i(x),pe=r(x,"DIV",{class:!0});var Os=n(pe);u(Be.$$.fragment,Os),Gr=i(Os),ra=r(Os,"P",{});var wl=n(ra);Wr=d(wl,"Return the path of the module of this class or subclass."),wl.forEach(t),Os.forEach(t),x.forEach(t),gs=i(a),k=r(a,"DIV",{class:!0});var jt=n(k);u(Se.$$.fragment,jt),Xr=i(jt),na=r(jt,"P",{});var El=n(na);zr=d(El,"Base class for datasets with data generation based on dict generators."),El.forEach(t),Jr=i(jt),V=r(jt,"P",{});var ct=n(V);oa=r(ct,"CODE",{});var xl=n(oa);Kr=d(xl,"GeneratorBasedBuilder"),xl.forEach(t),Yr=d(ct,` is a convenience class that abstracts away much
of the data writing and reading of `),da=r(ct,"CODE",{});var Dl=n(da);Qr=d(Dl,"DatasetBuilder"),Dl.forEach(t),Zr=d(ct,`. It expects subclasses to
implement generators of feature dictionaries across the dataset splits
(`),la=r(ct,"CODE",{});var yl=n(la);en=d(yl,"_split_generators"),yl.forEach(t),tn=d(ct,"). See the method docstrings for details."),ct.forEach(t),jt.forEach(t),hs=i(a),W=r(a,"DIV",{class:!0});var As=n(W);u(Ne.$$.fragment,As),an=i(As),ia=r(As,"P",{});var Tl=n(ia);sn=d(Tl,"Beam based Builder."),Tl.forEach(t),As.forEach(t),_s=i(a),X=r(a,"DIV",{class:!0});var Ls=n(X);u(je.$$.fragment,Ls),rn=i(Ls),pa=r(Ls,"P",{});var Il=n(pa);nn=d(Il,"Base class for datasets with data generation based on Arrow loading functions (CSV/JSON/Parquet)."),Il.forEach(t),Ls.forEach(t),vs=i(a),S=r(a,"DIV",{class:!0});var be=n(S);u(Re.$$.fragment,be),on=i(be),Ce=r(be,"P",{});var Ps=n(Ce);dn=d(Ps,"Base class for "),bt=r(Ps,"A",{href:!0});var Bl=n(bt);ln=d(Bl,"DatasetBuilder"),Bl.forEach(t),pn=d(Ps," data configuration."),Ps.forEach(t),cn=i(be),ke=r(be,"P",{});var Vs=n(ke);un=d(Vs,`DatasetBuilder subclasses with data configuration options should subclass
`),$t=r(Vs,"A",{href:!0});var Sl=n($t);mn=d(Sl,"BuilderConfig"),Sl.forEach(t),fn=d(Vs," and add their own properties."),Vs.forEach(t),gn=i(be),F=r(be,"DIV",{class:!0});var Rt=n(F);u(Oe.$$.fragment,Rt),hn=i(Rt),ca=r(Rt,"P",{});var Nl=n(ca);_n=d(Nl,`The config id is used to build the cache directory.
By default it is equal to the config name.
However the name of a config is not sufficient to have a unique identifier for the dataset being generated
since it doesn\u2019t take into account:`),Nl.forEach(t),vn=i(Rt),z=r(Rt,"UL",{});var Ct=n(z);ua=r(Ct,"LI",{});var jl=n(ua);bn=d(jl,"the config kwargs that can be used to overwrite attributes"),jl.forEach(t),$n=i(Ct),ma=r(Ct,"LI",{});var Rl=n(ma);wn=d(Rl,"the custom features used to write the dataset"),Rl.forEach(t),En=i(Ct),fa=r(Ct,"LI",{});var Cl=n(fa);xn=d(Cl,`the data_files for json/text/csv/pandas datasets
Therefore the config id is just the config name with an optional suffix based on these.`),Cl.forEach(t),Ct.forEach(t),Rt.forEach(t),be.forEach(t),bs=i(a),E=r(a,"DIV",{class:!0});var I=n(E);u(Ae.$$.fragment,I),Dn=i(I),ce=r(I,"DIV",{class:!0});var Fs=n(ce);u(Le.$$.fragment,Fs),yn=i(Fs),ga=r(Fs,"P",{});var kl=n(ga);Tn=d(kl,"Download given url(s)."),kl.forEach(t),Fs.forEach(t),In=i(I),j=r(I,"DIV",{class:!0});var $e=n(j);u(Pe.$$.fragment,$e),Bn=i($e),ha=r($e,"P",{});var Ol=n(ha);Sn=d(Ol,"Download and extract given url_or_urls."),Ol.forEach(t),Nn=i($e),_a=r($e,"P",{});var Al=n(_a);jn=d(Al,"Is roughly equivalent to:"),Al.forEach(t),Rn=i($e),u(Ve.$$.fragment,$e),$e.forEach(t),Cn=i(I),ue=r(I,"DIV",{class:!0});var Ms=n(ue);u(Fe.$$.fragment,Ms),kn=i(Ms),Me=r(Ms,"P",{});var Us=n(Me);On=d(Us,"Download given urls(s) by calling "),va=r(Us,"CODE",{});var Ll=n(va);An=d(Ll,"custom_download"),Ll.forEach(t),Ln=d(Us,"."),Us.forEach(t),Ms.forEach(t),Pn=i(I),me=r(I,"DIV",{class:!0});var qs=n(me);u(Ue.$$.fragment,qs),Vn=i(qs),ba=r(qs,"P",{});var Pl=n(ba);Fn=d(Pl,"Extract given path(s)."),Pl.forEach(t),qs.forEach(t),Mn=i(I),fe=r(I,"DIV",{class:!0});var Hs=n(fe);u(qe.$$.fragment,Hs),Un=i(Hs),$a=r(Hs,"P",{});var Vl=n($a);qn=d(Vl,"Iterate over files within an archive."),Vl.forEach(t),Hs.forEach(t),Hn=i(I),ge=r(I,"DIV",{class:!0});var Gs=n(ge);u(He.$$.fragment,Gs),Gn=i(Gs),wa=r(Gs,"P",{});var Fl=n(wa);Wn=d(Fl,"Iterate over file paths."),Fl.forEach(t),Gs.forEach(t),Xn=i(I),he=r(I,"DIV",{class:!0});var Ws=n(he);u(Ge.$$.fragment,Ws),zn=i(Ws),Ea=r(Ws,"P",{});var Ml=n(Ea);Jn=d(Ml,"Ship the files using Beam FileSystems to the pipeline temp dir."),Ml.forEach(t),Ws.forEach(t),I.forEach(t),$s=i(a),B=r(a,"DIV",{class:!0});var U=n(B);u(We.$$.fragment,U),Kn=i(U),wt=r(U,"P",{});var Xd=n(wt);xa=r(Xd,"CODE",{});var Ul=n(xa);Yn=d(Ul,"Enum"),Ul.forEach(t),Qn=d(Xd," for how to treat pre-existing downloads and data."),Xd.forEach(t),Zn=i(U),Xe=r(U,"P",{});var Xs=n(Xe);eo=d(Xs,"The default mode is "),Da=r(Xs,"CODE",{});var ql=n(Da);to=d(ql,"REUSE_DATASET_IF_EXISTS"),ql.forEach(t),ao=d(Xs,`, which will reuse both
raw downloads and the prepared dataset if they exist.`),Xs.forEach(t),so=i(U),ya=r(U,"P",{});var Hl=n(ya);ro=d(Hl,"The generations modes:"),Hl.forEach(t),no=i(U),ze=r(U,"TABLE",{});var zs=n(ze);Ta=r(zs,"THEAD",{});var Gl=n(Ta);J=r(Gl,"TR",{});var kt=n(J);ws=r(kt,"TH",{}),n(ws).forEach(t),oo=i(kt),Ia=r(kt,"TH",{});var Wl=n(Ia);lo=d(Wl,"Downloads"),Wl.forEach(t),io=i(kt),Ba=r(kt,"TH",{});var Xl=n(Ba);po=d(Xl,"Dataset"),Xl.forEach(t),kt.forEach(t),Gl.forEach(t),co=i(zs),K=r(zs,"TBODY",{});var Ot=n(K);Y=r(Ot,"TR",{});var At=n(Y);Et=r(At,"TD",{});var zd=n(Et);Sa=r(zd,"CODE",{});var zl=n(Sa);uo=d(zl,"REUSE_DATASET_IF_EXISTS"),zl.forEach(t),mo=d(zd," (default)"),zd.forEach(t),fo=i(At),Na=r(At,"TD",{});var Jl=n(Na);go=d(Jl,"Reuse"),Jl.forEach(t),ho=i(At),ja=r(At,"TD",{});var Kl=n(ja);_o=d(Kl,"Reuse"),Kl.forEach(t),At.forEach(t),vo=i(Ot),Q=r(Ot,"TR",{});var Lt=n(Q);Ra=r(Lt,"TD",{});var Yl=n(Ra);Ca=r(Yl,"CODE",{});var Ql=n(Ca);bo=d(Ql,"REUSE_CACHE_IF_EXISTS"),Ql.forEach(t),Yl.forEach(t),$o=i(Lt),ka=r(Lt,"TD",{});var Zl=n(ka);wo=d(Zl,"Reuse"),Zl.forEach(t),Eo=i(Lt),Oa=r(Lt,"TD",{});var ei=n(Oa);xo=d(ei,"Fresh"),ei.forEach(t),Lt.forEach(t),Do=i(Ot),Z=r(Ot,"TR",{});var Pt=n(Z);Aa=r(Pt,"TD",{});var ti=n(Aa);La=r(ti,"CODE",{});var ai=n(La);yo=d(ai,"FORCE_REDOWNLOAD"),ai.forEach(t),ti.forEach(t),To=i(Pt),Pa=r(Pt,"TD",{});var si=n(Pa);Io=d(si,"Fresh"),si.forEach(t),Bo=i(Pt),Va=r(Pt,"TD",{});var ri=n(Va);So=d(ri,"Fresh"),ri.forEach(t),Pt.forEach(t),Ot.forEach(t),zs.forEach(t),U.forEach(t),Es=i(a),O=r(a,"DIV",{class:!0});var Vt=n(O);u(Je.$$.fragment,Vt),No=i(Vt),Fa=r(Vt,"P",{});var ni=n(Fa);jo=d(ni,"Defines the split information for the generator."),ni.forEach(t),Ro=i(Vt),ee=r(Vt,"P",{});var Ft=n(ee);Co=d(Ft,`This should be used as returned value of
`),Ma=r(Ft,"CODE",{});var oi=n(Ma);ko=d(oi,"GeneratorBasedBuilder._split_generators()"),oi.forEach(t),Oo=d(Ft,`.
See `),Ua=r(Ft,"CODE",{});var di=n(Ua);Ao=d(di,"GeneratorBasedBuilder._split_generators()"),di.forEach(t),Lo=d(Ft,` for more info and example
of usage.`),Ft.forEach(t),Vt.forEach(t),xs=i(a),y=r(a,"DIV",{class:!0});var R=n(y);u(Ke.$$.fragment,R),Po=i(R),xt=r(R,"P",{});var Jd=n(xt);qa=r(Jd,"CODE",{});var li=n(qa);Vo=d(li,"Enum"),li.forEach(t),Fo=d(Jd," for dataset splits."),Jd.forEach(t),Mo=i(R),Ha=r(R,"P",{});var ii=n(Ha);Uo=d(ii,`Datasets are typically split into different subsets to be used at various
stages of training and evaluation.`),ii.forEach(t),qo=i(R),A=r(R,"UL",{});var we=n(A);Dt=r(we,"LI",{});var Kd=n(Dt);Ga=r(Kd,"CODE",{});var pi=n(Ga);Ho=d(pi,"TRAIN"),pi.forEach(t),Go=d(Kd,": the training data."),Kd.forEach(t),Wo=i(we),yt=r(we,"LI",{});var Yd=n(yt);Wa=r(Yd,"CODE",{});var ci=n(Wa);Xo=d(ci,"VALIDATION"),ci.forEach(t),zo=d(Yd,`: the validation data. If present, this is typically used as
evaluation data while iterating on a model (e.g. changing hyperparameters,
model architecture, etc.).`),Yd.forEach(t),Jo=i(we),Tt=r(we,"LI",{});var Qd=n(Tt);Xa=r(Qd,"CODE",{});var ui=n(Xa);Ko=d(ui,"TEST"),ui.forEach(t),Yo=d(Qd,`: the testing data. This is the data to report metrics on. Typically
you do not want to use this during model iteration as you may overfit to it.`),Qd.forEach(t),Qo=i(we),It=r(we,"LI",{});var Zd=n(It);za=r(Zd,"CODE",{});var mi=n(za);Zo=d(mi,"ALL"),mi.forEach(t),ed=d(Zd,": the union of all defined dataset splits."),Zd.forEach(t),we.forEach(t),td=i(R),Bt=r(R,"P",{});var el=n(Bt);ad=d(el,"Note: All splits, including compositions inherit from "),Ja=r(el,"CODE",{});var fi=n(Ja);sd=d(fi,"datasets.SplitBase"),fi.forEach(t),el.forEach(t),rd=i(R),Ye=r(R,"P",{});var Js=n(Ye);nd=d(Js,"See the :doc:"),Ka=r(Js,"CODE",{});var gi=n(Ka);od=d(gi,"guide on splits </loading>"),gi.forEach(t),dd=d(Js," for more information."),Js.forEach(t),R.forEach(t),Ds=i(a),w=r(a,"DIV",{class:!0});var D=n(w);u(Qe.$$.fragment,D),ld=i(D),Ya=r(D,"P",{});var hi=n(Ya);id=d(hi,"Descriptor corresponding to a named split (train, test, \u2026)."),hi.forEach(t),pd=i(D),Qa=r(D,"P",{});var _i=n(Qa);cd=d(_i,"Example:"),_i.forEach(t),ud=i(D),u(Ze.$$.fragment,D),md=i(D),Za=r(D,"P",{});var vi=n(Za);fd=d(vi,`Warning:
A split cannot be added twice, so the following will fail:`),vi.forEach(t),gd=i(D),u(et.$$.fragment,D),hd=i(D),es=r(D,"P",{});var bi=n(es);_d=d(bi,`Warning:
The slices can be applied only one time. So the following are valid:`),bi.forEach(t),vd=i(D),u(tt.$$.fragment,D),bd=i(D),ts=r(D,"P",{});var $i=n(ts);$d=d($i,"But not:"),$i.forEach(t),wd=i(D),u(at.$$.fragment,D),D.forEach(t),ys=i(a),te=r(a,"DIV",{class:!0});var Ks=n(te);u(st.$$.fragment,Ks),Ed=i(Ks),as=r(Ks,"P",{});var wi=n(as);xd=d(wi,"Split corresponding to the union of all defined dataset splits."),wi.forEach(t),Ks.forEach(t),Ts=i(a),T=r(a,"DIV",{class:!0});var C=n(T);u(rt.$$.fragment,C),Dd=i(C),ss=r(C,"P",{});var Ei=n(ss);yd=d(Ei,"Reading instruction for a dataset."),Ei.forEach(t),Td=i(C),rs=r(C,"P",{});var xi=n(rs);Id=d(xi,"Examples:"),xi.forEach(t),Bd=i(C),u(nt.$$.fragment,C),Sd=i(C),_e=r(C,"DIV",{class:!0});var Ys=n(_e);u(ot.$$.fragment,Ys),Nd=i(Ys),ns=r(Ys,"P",{});var Di=n(ns);jd=d(Di,"Creates a ReadInstruction instance out of a string spec."),Di.forEach(t),Ys.forEach(t),Rd=i(C),M=r(C,"DIV",{class:!0});var Mt=n(M);u(dt.$$.fragment,Mt),Cd=i(Mt),os=r(Mt,"P",{});var yi=n(os);kd=d(yi,"Translate instruction into a list of absolute instructions."),yi.forEach(t),Od=i(Mt),ds=r(Mt,"P",{});var Ti=n(ds);Ad=d(Ti,"Those absolute instructions are then to be added together."),Ti.forEach(t),Mt.forEach(t),C.forEach(t),Is=i(a),ae=r(a,"DIV",{class:!0});var Qs=n(ae);u(lt.$$.fragment,Qs),Ld=i(Qs),ls=r(Qs,"P",{});var Ii=n(ls);Pd=d(Ii,"Configuration for our cached path manager."),Ii.forEach(t),Qs.forEach(t),Bs=i(a),L=r(a,"DIV",{class:!0});var Ut=n(L);u(it.$$.fragment,Ut),Vd=i(Ut),is=r(Ut,"P",{});var Bi=n(is);Fd=d(Bi,"Dataset version MAJOR.MINOR.PATCH."),Bi.forEach(t),Md=i(Ut),ve=r(Ut,"DIV",{class:!0});var Zs=n(ve);u(pt.$$.fragment,Zs),Ud=i(Zs),ps=r(Zs,"P",{});var Si=n(ps);qd=d(Si,"Returns True if other_version matches."),Si.forEach(t),Zs.forEach(t),Ut.forEach(t),this.h()},h(){p(q,"name","hf:doc:metadata"),p(q,"content",JSON.stringify(Pi)),p(re,"id","datasets.DatasetBuilder"),p(re,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(re,"href","#datasets.DatasetBuilder"),p(H,"class","relative group"),p(ut,"href","/docs/datasets/pr_4175/en/package_reference/builder_classes#datasets.DatasetBuilder"),p(mt,"href","/docs/datasets/pr_4175/en/package_reference/builder_classes#datasets.BuilderConfig"),p(_t,"href","/docs/datasets/pr_4175/en/package_reference/builder_classes#datasets.DatasetBuilder.download_and_prepare"),p(vt,"href","/docs/datasets/pr_4175/en/package_reference/builder_classes#datasets.DatasetBuilder.as_dataset"),p(oe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(de,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(le,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(ie,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(pe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p($,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(k,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(W,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(X,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(bt,"href","/docs/datasets/pr_4175/en/package_reference/builder_classes#datasets.DatasetBuilder"),p($t,"href","/docs/datasets/pr_4175/en/package_reference/builder_classes#datasets.BuilderConfig"),p(F,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(S,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(ce,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(j,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(ue,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(me,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(fe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(ge,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(he,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(E,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(B,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(O,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(y,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(w,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(te,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(_e,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(M,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(T,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(ae,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(ve,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(L,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(a,_){e(document.head,q),v(a,us,_),v(a,H,_),e(H,re),e(re,Ht),m(Ee,Ht,null),e(H,er),e(H,Gt),e(Gt,tr),v(a,ms,_),v(a,P,_),e(P,ar),e(P,ut),e(ut,sr),e(P,rr),e(P,mt),e(mt,nr),e(P,or),v(a,fs,_),v(a,$,_),m(xe,$,null),e($,dr),e($,Wt),e(Wt,lr),e($,ir),e($,ft),e(ft,Xt),e(Xt,pr),e(ft,cr),e($,ur),e($,G),e(G,gt),e(gt,zt),e(zt,mr),e(gt,fr),e(G,gr),e(G,ht),e(ht,_t),e(_t,hr),e(ht,_r),e(G,vr),e(G,ne),e(ne,vt),e(vt,br),e(ne,$r),e(ne,Jt),e(Jt,wr),e(ne,Er),e($,xr),e($,N),e(N,Kt),e(Kt,Dr),e(N,yr),e(N,Yt),e(Yt,Tr),e(N,Ir),e(N,Qt),e(Qt,Br),e(N,Sr),e(N,Zt),e(Zt,Nr),e(N,jr),e($,Rr),e($,oe),m(De,oe,null),e(oe,Cr),e(oe,ea),e(ea,kr),e($,Or),e($,de),m(ye,de,null),e(de,Ar),e(de,ta),e(ta,Lr),e($,Pr),e($,le),m(Te,le,null),e(le,Vr),e(le,aa),e(aa,Fr),e($,Mr),e($,ie),m(Ie,ie,null),e(ie,Ur),e(ie,sa),e(sa,qr),e($,Hr),e($,pe),m(Be,pe,null),e(pe,Gr),e(pe,ra),e(ra,Wr),v(a,gs,_),v(a,k,_),m(Se,k,null),e(k,Xr),e(k,na),e(na,zr),e(k,Jr),e(k,V),e(V,oa),e(oa,Kr),e(V,Yr),e(V,da),e(da,Qr),e(V,Zr),e(V,la),e(la,en),e(V,tn),v(a,hs,_),v(a,W,_),m(Ne,W,null),e(W,an),e(W,ia),e(ia,sn),v(a,_s,_),v(a,X,_),m(je,X,null),e(X,rn),e(X,pa),e(pa,nn),v(a,vs,_),v(a,S,_),m(Re,S,null),e(S,on),e(S,Ce),e(Ce,dn),e(Ce,bt),e(bt,ln),e(Ce,pn),e(S,cn),e(S,ke),e(ke,un),e(ke,$t),e($t,mn),e(ke,fn),e(S,gn),e(S,F),m(Oe,F,null),e(F,hn),e(F,ca),e(ca,_n),e(F,vn),e(F,z),e(z,ua),e(ua,bn),e(z,$n),e(z,ma),e(ma,wn),e(z,En),e(z,fa),e(fa,xn),v(a,bs,_),v(a,E,_),m(Ae,E,null),e(E,Dn),e(E,ce),m(Le,ce,null),e(ce,yn),e(ce,ga),e(ga,Tn),e(E,In),e(E,j),m(Pe,j,null),e(j,Bn),e(j,ha),e(ha,Sn),e(j,Nn),e(j,_a),e(_a,jn),e(j,Rn),m(Ve,j,null),e(E,Cn),e(E,ue),m(Fe,ue,null),e(ue,kn),e(ue,Me),e(Me,On),e(Me,va),e(va,An),e(Me,Ln),e(E,Pn),e(E,me),m(Ue,me,null),e(me,Vn),e(me,ba),e(ba,Fn),e(E,Mn),e(E,fe),m(qe,fe,null),e(fe,Un),e(fe,$a),e($a,qn),e(E,Hn),e(E,ge),m(He,ge,null),e(ge,Gn),e(ge,wa),e(wa,Wn),e(E,Xn),e(E,he),m(Ge,he,null),e(he,zn),e(he,Ea),e(Ea,Jn),v(a,$s,_),v(a,B,_),m(We,B,null),e(B,Kn),e(B,wt),e(wt,xa),e(xa,Yn),e(wt,Qn),e(B,Zn),e(B,Xe),e(Xe,eo),e(Xe,Da),e(Da,to),e(Xe,ao),e(B,so),e(B,ya),e(ya,ro),e(B,no),e(B,ze),e(ze,Ta),e(Ta,J),e(J,ws),e(J,oo),e(J,Ia),e(Ia,lo),e(J,io),e(J,Ba),e(Ba,po),e(ze,co),e(ze,K),e(K,Y),e(Y,Et),e(Et,Sa),e(Sa,uo),e(Et,mo),e(Y,fo),e(Y,Na),e(Na,go),e(Y,ho),e(Y,ja),e(ja,_o),e(K,vo),e(K,Q),e(Q,Ra),e(Ra,Ca),e(Ca,bo),e(Q,$o),e(Q,ka),e(ka,wo),e(Q,Eo),e(Q,Oa),e(Oa,xo),e(K,Do),e(K,Z),e(Z,Aa),e(Aa,La),e(La,yo),e(Z,To),e(Z,Pa),e(Pa,Io),e(Z,Bo),e(Z,Va),e(Va,So),v(a,Es,_),v(a,O,_),m(Je,O,null),e(O,No),e(O,Fa),e(Fa,jo),e(O,Ro),e(O,ee),e(ee,Co),e(ee,Ma),e(Ma,ko),e(ee,Oo),e(ee,Ua),e(Ua,Ao),e(ee,Lo),v(a,xs,_),v(a,y,_),m(Ke,y,null),e(y,Po),e(y,xt),e(xt,qa),e(qa,Vo),e(xt,Fo),e(y,Mo),e(y,Ha),e(Ha,Uo),e(y,qo),e(y,A),e(A,Dt),e(Dt,Ga),e(Ga,Ho),e(Dt,Go),e(A,Wo),e(A,yt),e(yt,Wa),e(Wa,Xo),e(yt,zo),e(A,Jo),e(A,Tt),e(Tt,Xa),e(Xa,Ko),e(Tt,Yo),e(A,Qo),e(A,It),e(It,za),e(za,Zo),e(It,ed),e(y,td),e(y,Bt),e(Bt,ad),e(Bt,Ja),e(Ja,sd),e(y,rd),e(y,Ye),e(Ye,nd),e(Ye,Ka),e(Ka,od),e(Ye,dd),v(a,Ds,_),v(a,w,_),m(Qe,w,null),e(w,ld),e(w,Ya),e(Ya,id),e(w,pd),e(w,Qa),e(Qa,cd),e(w,ud),m(Ze,w,null),e(w,md),e(w,Za),e(Za,fd),e(w,gd),m(et,w,null),e(w,hd),e(w,es),e(es,_d),e(w,vd),m(tt,w,null),e(w,bd),e(w,ts),e(ts,$d),e(w,wd),m(at,w,null),v(a,ys,_),v(a,te,_),m(st,te,null),e(te,Ed),e(te,as),e(as,xd),v(a,Ts,_),v(a,T,_),m(rt,T,null),e(T,Dd),e(T,ss),e(ss,yd),e(T,Td),e(T,rs),e(rs,Id),e(T,Bd),m(nt,T,null),e(T,Sd),e(T,_e),m(ot,_e,null),e(_e,Nd),e(_e,ns),e(ns,jd),e(T,Rd),e(T,M),m(dt,M,null),e(M,Cd),e(M,os),e(os,kd),e(M,Od),e(M,ds),e(ds,Ad),v(a,Is,_),v(a,ae,_),m(lt,ae,null),e(ae,Ld),e(ae,ls),e(ls,Pd),v(a,Bs,_),v(a,L,_),m(it,L,null),e(L,Vd),e(L,is),e(is,Fd),e(L,Md),e(L,ve),m(pt,ve,null),e(ve,Ud),e(ve,ps),e(ps,qd),Ss=!0},p:ki,i(a){Ss||(f(Ee.$$.fragment,a),f(xe.$$.fragment,a),f(De.$$.fragment,a),f(ye.$$.fragment,a),f(Te.$$.fragment,a),f(Ie.$$.fragment,a),f(Be.$$.fragment,a),f(Se.$$.fragment,a),f(Ne.$$.fragment,a),f(je.$$.fragment,a),f(Re.$$.fragment,a),f(Oe.$$.fragment,a),f(Ae.$$.fragment,a),f(Le.$$.fragment,a),f(Pe.$$.fragment,a),f(Ve.$$.fragment,a),f(Fe.$$.fragment,a),f(Ue.$$.fragment,a),f(qe.$$.fragment,a),f(He.$$.fragment,a),f(Ge.$$.fragment,a),f(We.$$.fragment,a),f(Je.$$.fragment,a),f(Ke.$$.fragment,a),f(Qe.$$.fragment,a),f(Ze.$$.fragment,a),f(et.$$.fragment,a),f(tt.$$.fragment,a),f(at.$$.fragment,a),f(st.$$.fragment,a),f(rt.$$.fragment,a),f(nt.$$.fragment,a),f(ot.$$.fragment,a),f(dt.$$.fragment,a),f(lt.$$.fragment,a),f(it.$$.fragment,a),f(pt.$$.fragment,a),Ss=!0)},o(a){g(Ee.$$.fragment,a),g(xe.$$.fragment,a),g(De.$$.fragment,a),g(ye.$$.fragment,a),g(Te.$$.fragment,a),g(Ie.$$.fragment,a),g(Be.$$.fragment,a),g(Se.$$.fragment,a),g(Ne.$$.fragment,a),g(je.$$.fragment,a),g(Re.$$.fragment,a),g(Oe.$$.fragment,a),g(Ae.$$.fragment,a),g(Le.$$.fragment,a),g(Pe.$$.fragment,a),g(Ve.$$.fragment,a),g(Fe.$$.fragment,a),g(Ue.$$.fragment,a),g(qe.$$.fragment,a),g(He.$$.fragment,a),g(Ge.$$.fragment,a),g(We.$$.fragment,a),g(Je.$$.fragment,a),g(Ke.$$.fragment,a),g(Qe.$$.fragment,a),g(Ze.$$.fragment,a),g(et.$$.fragment,a),g(tt.$$.fragment,a),g(at.$$.fragment,a),g(st.$$.fragment,a),g(rt.$$.fragment,a),g(nt.$$.fragment,a),g(ot.$$.fragment,a),g(dt.$$.fragment,a),g(lt.$$.fragment,a),g(it.$$.fragment,a),g(pt.$$.fragment,a),Ss=!1},d(a){t(q),a&&t(us),a&&t(H),h(Ee),a&&t(ms),a&&t(P),a&&t(fs),a&&t($),h(xe),h(De),h(ye),h(Te),h(Ie),h(Be),a&&t(gs),a&&t(k),h(Se),a&&t(hs),a&&t(W),h(Ne),a&&t(_s),a&&t(X),h(je),a&&t(vs),a&&t(S),h(Re),h(Oe),a&&t(bs),a&&t(E),h(Ae),h(Le),h(Pe),h(Ve),h(Fe),h(Ue),h(qe),h(He),h(Ge),a&&t($s),a&&t(B),h(We),a&&t(Es),a&&t(O),h(Je),a&&t(xs),a&&t(y),h(Ke),a&&t(Ds),a&&t(w),h(Qe),h(Ze),h(et),h(tt),h(at),a&&t(ys),a&&t(te),h(st),a&&t(Ts),a&&t(T),h(rt),h(nt),h(ot),h(dt),a&&t(Is),a&&t(ae),h(lt),a&&t(Bs),a&&t(L),h(it),h(pt)}}}const Pi={local:"datasets.DatasetBuilder",title:"Builder classes"};function Vi(tl){return Oi(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Hi extends Ni{constructor(q){super();ji(this,q,Vi,Li,Ri,{})}}export{Hi as default,Pi as metadata};
