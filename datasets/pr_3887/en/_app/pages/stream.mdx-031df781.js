import{S as Gp,i as Kp,s as Qp,e as t,k as d,w as u,t as n,M as Xp,c as l,d as a,m as f,a as i,x as _,h as r,b as h,N as Wp,F as e,g as o,y as g,q as b,o as v,B as j}from"../chunks/vendor-e67aec41.js";import{T as nl}from"../chunks/Tip-76459d1c.js";import{I as F}from"../chunks/IconCopyLink-ffd7f84e.js";import{C as S}from"../chunks/CodeBlock-e2bcf023.js";import{C as Zp}from"../chunks/CodeBlockFw-1e02e2ba.js";function si(N){let c,$,m,w,k,x,y,E;return{c(){c=t("p"),$=n("An "),m=t("a"),w=n("datasets.IterableDataset"),k=n(" is useful for iterative jobs like training a model. You shouldn\u2019t use a "),x=t("a"),y=n("datasets.IterableDataset"),E=n(" for jobs that require random access to examples because you have to iterate all over it using a for loop. Getting the last example in an iterable dataset would require you to iterate over all the previous examples."),this.h()},l(q){c=l(q,"P",{});var D=i(c);$=r(D,"An "),m=l(D,"A",{href:!0});var T=i(m);w=r(T,"datasets.IterableDataset"),T.forEach(a),k=r(D," is useful for iterative jobs like training a model. You shouldn\u2019t use a "),x=l(D,"A",{href:!0});var Y=i(x);y=r(Y,"datasets.IterableDataset"),Y.forEach(a),E=r(D," for jobs that require random access to examples because you have to iterate all over it using a for loop. Getting the last example in an iterable dataset would require you to iterate over all the previous examples."),D.forEach(a),this.h()},h(){h(m,"href","/docs/datasets/pr_3887/en/package_reference/main_classes#datasets.IterableDataset"),h(x,"href","/docs/datasets/pr_3887/en/package_reference/main_classes#datasets.IterableDataset")},m(q,D){o(q,c,D),e(c,$),e(c,m),e(m,w),e(c,k),e(c,x),e(x,y),e(c,E)},d(q){q&&a(c)}}}function ei(N){let c,$,m,w;return{c(){c=t("p"),$=t("a"),m=n("datasets.IterableDataset.shuffle()"),w=n(" will also shuffle the order of the shards if the dataset is sharded into multiple sets."),this.h()},l(k){c=l(k,"P",{});var x=i(c);$=l(x,"A",{href:!0});var y=i($);m=r(y,"datasets.IterableDataset.shuffle()"),y.forEach(a),w=r(x," will also shuffle the order of the shards if the dataset is sharded into multiple sets."),x.forEach(a),this.h()},h(){h($,"href","/docs/datasets/pr_3887/en/package_reference/main_classes#datasets.IterableDataset.shuffle")},m(k,x){o(k,c,x),e(c,$),e($,m),e(c,w)},d(k){k&&a(c)}}}function ai(N){let c,$,m,w,k,x,y,E,q,D,T,Y,ns;return{c(){c=t("p"),$=t("code"),m=n("take"),w=n(" and "),k=t("code"),x=n("skip"),y=n(" prevent future calls to "),E=t("code"),q=n("shuffle"),D=n(" because they lock in the order of the shards. You should "),T=t("code"),Y=n("shuffle"),ns=n(" your dataset before splitting it.")},l(z){c=l(z,"P",{});var A=i(c);$=l(A,"CODE",{});var oe=i($);m=r(oe,"take"),oe.forEach(a),w=r(A," and "),k=l(A,"CODE",{});var he=i(k);x=r(he,"skip"),he.forEach(a),y=r(A," prevent future calls to "),E=l(A,"CODE",{});var rs=i(E);q=r(rs,"shuffle"),rs.forEach(a),D=r(A," because they lock in the order of the shards. You should "),T=l(A,"CODE",{});var de=i(T);Y=r(de,"shuffle"),de.forEach(a),ns=r(A," your dataset before splitting it."),A.forEach(a)},m(z,A){o(z,c,A),e(c,$),e($,m),e(c,w),e(c,k),e(k,x),e(c,y),e(c,E),e(E,q),e(c,D),e(c,T),e(T,Y),e(c,ns)},d(z){z&&a(c)}}}function ti(N){let c,$,m,w,k;return{c(){c=t("p"),$=n("See other examples of batch processing in "),m=t("a"),w=n("the batched map processing documentation"),k=n(". They work the same for iterable datasets."),this.h()},l(x){c=l(x,"P",{});var y=i(c);$=r(y,"See other examples of batch processing in "),m=l(y,"A",{href:!0});var E=i(m);w=r(E,"the batched map processing documentation"),E.forEach(a),k=r(y,". They work the same for iterable datasets."),y.forEach(a),this.h()},h(){h(m,"href","./process#batch-processing")},m(x,y){o(x,c,y),e(c,$),e(c,m),e(m,w),e(c,k)},d(x){x&&a(c)}}}function li(N){let c,$,m,w,k,x,y,E,q,D,T,Y,ns,z,A,oe,he,rs,de,La,J,fe,xr,rl,ce,$r,Na,P,pl,qs,il,ol,Ge,hl,dl,me,fl,cl,Oa,Ps,Fa,H,ml,ue,ul,_l,_e,gl,bl,Ya,ps,Ha,U,is,Ke,Ms,vl,Qe,jl,Ra,M,xl,ge,$l,wl,be,kl,yl,ve,El,Dl,Ba,C,Al,Xe,Il,Tl,Ze,Sl,zl,je,ql,Pl,Va,Cs,Ja,os,Ua,W,hs,sa,Ls,Ml,ea,Cl,Wa,ds,Ll,aa,Nl,Ol,Ga,fs,Fl,ta,Yl,Hl,Ka,Ns,Qa,G,cs,la,Os,Rl,na,Bl,Xa,xe,Vl,Za,$e,ms,we,Jl,Ul,ra,Wl,Gl,st,Fs,et,ke,us,ye,Kl,Ql,pa,Xl,Zl,at,Ys,tt,_s,lt,Ee,nt,K,gs,ia,Hs,sn,oa,en,rt,Q,De,an,tn,Ae,ln,nn,pt,Rs,it,bs,rn,ha,pn,on,ot,Bs,ht,R,hn,da,dn,fn,fa,cn,mn,dt,X,vs,ca,Vs,un,ma,_n,ft,js,gn,Ie,bn,vn,ct,Js,mt,Z,xs,ua,Us,jn,_a,xn,ut,I,$n,Te,wn,kn,Se,yn,En,ze,Dn,An,qe,In,Tn,Pe,Sn,zn,_t,Me,qn,gt,B,Pn,Ce,Mn,Cn,ga,Ln,Nn,bt,Ws,vt,$s,On,Le,Fn,Yn,jt,Gs,xt,ws,Hn,Ne,Rn,Bn,$t,V,Vn,ba,Jn,Un,Oe,Wn,Gn,wt,Ks,kt,ss,ks,va,Qs,Kn,ja,Qn,yt,O,Fe,Xn,Zn,xa,sr,er,$a,ar,tr,Et,es,ys,wa,Xs,lr,ka,nr,Dt,Zs,At,Es,It,as,Ds,ya,se,rr,Ea,pr,Tt,As,ir,Ye,or,hr,St,ee,zt,ts,He,dr,fr,Da,cr,mr,qt,ae,Pt,ls,Is,Aa,te,ur,Ia,_r,Mt,le,Re,gr,br,Ct,ne,Lt,Be,vr,Nt,re,Ot;return x=new F({}),Ps=new S({props:{code:`from datasets import load_dataset
dataset = load_dataset('oscar', "unshuffled_deduplicated_en", split='train', streaming=True)
print(next(iter(dataset)))`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;oscar&#x27;</span>, <span class="hljs-string">&quot;unshuffled_deduplicated_en&quot;</span>, split=<span class="hljs-string">&#x27;train&#x27;</span>, streaming=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(<span class="hljs-built_in">next</span>(<span class="hljs-built_in">iter</span>(dataset)))
{<span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;Mtendere Village was inspired by the vision of Chief Napoleon Dzombe, which he shared with John Blanchard during his first visit to Malawi. Chief Napoleon conveyed the desperate need for a program to intervene and care for the orphans and vulnerable children (OVC) in Malawi, and John committed to help...</span>`}}),ps=new nl({props:{$$slots:{default:[si]},$$scope:{ctx:N}}}),Ms=new F({}),Cs=new S({props:{code:`from datasets import load_dataset
dataset = load_dataset('oscar', "unshuffled_deduplicated_en", split='train', streaming=True)
shuffled_dataset = dataset.shuffle(seed=42, buffer_size=10_000)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;oscar&#x27;</span>, <span class="hljs-string">&quot;unshuffled_deduplicated_en&quot;</span>, split=<span class="hljs-string">&#x27;train&#x27;</span>, streaming=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>shuffled_dataset = dataset.shuffle(seed=<span class="hljs-number">42</span>, buffer_size=<span class="hljs-number">10_000</span>)`}}),os=new nl({props:{$$slots:{default:[ei]},$$scope:{ctx:N}}}),Ls=new F({}),Ns=new S({props:{code:`for epoch in range(epochs):
    shuffled_dataset.set_epoch(epoch)
    for example in shuffled_dataset:
        ...`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(epochs):
<span class="hljs-meta">... </span>    shuffled_dataset.set_epoch(epoch)
<span class="hljs-meta">... </span>    <span class="hljs-keyword">for</span> example <span class="hljs-keyword">in</span> shuffled_dataset:
<span class="hljs-meta">... </span>        ...`}}),Os=new F({}),Fs=new S({props:{code:`dataset = load_dataset('oscar', "unshuffled_deduplicated_en", split='train', streaming=True)
dataset_head = dataset.take(2)
list(dataset_head)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;oscar&#x27;</span>, <span class="hljs-string">&quot;unshuffled_deduplicated_en&quot;</span>, split=<span class="hljs-string">&#x27;train&#x27;</span>, streaming=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset_head = dataset.take(<span class="hljs-number">2</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">list</span>(dataset_head)
[{<span class="hljs-string">&#x27;id&#x27;</span>: <span class="hljs-number">0</span>, <span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;Mtendere Village was...&#x27;</span>}, {<span class="hljs-string">&#x27;id&#x27;</span>: <span class="hljs-number">1</span>, <span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;Lily James cannot fight the music...&#x27;</span>}]`}}),Ys=new S({props:{code:"train_dataset = shuffled_dataset.skip(1000)",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>train_dataset = shuffled_dataset.skip(<span class="hljs-number">1000</span>)'}}),_s=new nl({props:{warning:"&lcub;true}",$$slots:{default:[ai]},$$scope:{ctx:N}}}),Hs=new F({}),Rs=new S({props:{code:`from datasets import interleave_datasets
from itertools import islice
en_dataset = load_dataset('oscar', "unshuffled_deduplicated_en", split='train', streaming=True)
fr_dataset = load_dataset('oscar', "unshuffled_deduplicated_fr", split='train', streaming=True)

multilingual_dataset = interleave_datasets([en_dataset, fr_dataset])
print(list(islice(multilingual_dataset, 2)))`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> interleave_datasets
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> itertools <span class="hljs-keyword">import</span> islice
<span class="hljs-meta">&gt;&gt;&gt; </span>en_dataset = load_dataset(<span class="hljs-string">&#x27;oscar&#x27;</span>, <span class="hljs-string">&quot;unshuffled_deduplicated_en&quot;</span>, split=<span class="hljs-string">&#x27;train&#x27;</span>, streaming=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>fr_dataset = load_dataset(<span class="hljs-string">&#x27;oscar&#x27;</span>, <span class="hljs-string">&quot;unshuffled_deduplicated_fr&quot;</span>, split=<span class="hljs-string">&#x27;train&#x27;</span>, streaming=<span class="hljs-literal">True</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>multilingual_dataset = interleave_datasets([en_dataset, fr_dataset])
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(<span class="hljs-built_in">list</span>(islice(multilingual_dataset, <span class="hljs-number">2</span>)))
[{<span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;Mtendere Village was inspired by the vision...&#x27;</span>}, {<span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&quot;M\xE9dia de d\xE9bat d&#x27;id\xE9es, de culture et de litt\xE9rature...&quot;</span>}]`}}),Bs=new S({props:{code:`multilingual_dataset_with_oversampling = interleave_datasets([en_dataset, fr_dataset], probabilities=[0.8, 0.2], seed=42)
print(list(islice(multilingual_dataset_with_oversampling, 2)))`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>multilingual_dataset_with_oversampling = interleave_datasets([en_dataset, fr_dataset], probabilities=[<span class="hljs-number">0.8</span>, <span class="hljs-number">0.2</span>], seed=<span class="hljs-number">42</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(<span class="hljs-built_in">list</span>(islice(multilingual_dataset_with_oversampling, <span class="hljs-number">2</span>)))
[{<span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;Mtendere Village was inspired by the vision...&#x27;</span>}, {<span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;Lily James cannot fight the music...&#x27;</span>}]`}}),Vs=new F({}),Js=new S({props:{code:`from datasets import load_dataset
dataset = load_dataset('mc4', 'en', streaming=True, split='train')
dataset = dataset.remove_columns('timestamp')`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;mc4&#x27;</span>, <span class="hljs-string">&#x27;en&#x27;</span>, streaming=<span class="hljs-literal">True</span>, split=<span class="hljs-string">&#x27;train&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = dataset.remove_columns(<span class="hljs-string">&#x27;timestamp&#x27;</span>)`}}),Us=new F({}),Ws=new S({props:{code:`def add_prefix(example):
    example['text'] = 'My text: ' + example['text']
    return example`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">def</span> <span class="hljs-title function_">add_prefix</span>(<span class="hljs-params">example</span>):
<span class="hljs-meta">... </span>    example[<span class="hljs-string">&#x27;text&#x27;</span>] = <span class="hljs-string">&#x27;My text: &#x27;</span> + example[<span class="hljs-string">&#x27;text&#x27;</span>]
<span class="hljs-meta">... </span>    <span class="hljs-keyword">return</span> example`}}),Gs=new S({props:{code:`from datasets import load_dataset
dataset = load_dataset('oscar', 'unshuffled_deduplicated_en', streaming=True, split='train')
updated_dataset = dataset.map(add_prefix)
list(updated_dataset.take(3))`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;oscar&#x27;</span>, <span class="hljs-string">&#x27;unshuffled_deduplicated_en&#x27;</span>, streaming=<span class="hljs-literal">True</span>, split=<span class="hljs-string">&#x27;train&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>updated_dataset = dataset.<span class="hljs-built_in">map</span>(add_prefix)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">list</span>(updated_dataset.take(<span class="hljs-number">3</span>))
[{<span class="hljs-string">&#x27;id&#x27;</span>: <span class="hljs-number">0</span>, <span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;My text: Mtendere Village was inspired by...&#x27;</span>},
 {<span class="hljs-string">&#x27;id&#x27;</span>: <span class="hljs-number">1</span>, <span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;My text: Lily James cannot fight the music...&#x27;</span>},
 {<span class="hljs-string">&#x27;id&#x27;</span>: <span class="hljs-number">2</span>, <span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;My text: &quot;I\\&#x27;d love to help kickstart...&#x27;</span>}]`}}),Ks=new S({props:{code:`updated_dataset = dataset.map(add_prefix, remove_columns=["id"])
list(updated_dataset.take(3))`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>updated_dataset = dataset.<span class="hljs-built_in">map</span>(add_prefix, remove_columns=[<span class="hljs-string">&quot;id&quot;</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">list</span>(updated_dataset.take(<span class="hljs-number">3</span>))
[{<span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;My text: Mtendere Village was inspired by...&#x27;</span>},
 {<span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;My text: Lily James cannot fight the music...&#x27;</span>},
 {<span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;My text: &quot;I\\&#x27;d love to help kickstart...&#x27;</span>}]`}}),Qs=new F({}),Xs=new F({}),Zs=new S({props:{code:`from datasets import load_dataset
from transformers import AutoTokenizer
dataset = load_dataset("mc4", "en", streaming=True, split="train")
tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')
def encode(examples):
    return tokenizer(examples['text'], truncation=True, padding='max_length')
dataset = dataset.map(encode, batched=True, remove_columns=["text", "timestamp", "url"])
next(iter(dataset))`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;mc4&quot;</span>, <span class="hljs-string">&quot;en&quot;</span>, streaming=<span class="hljs-literal">True</span>, split=<span class="hljs-string">&quot;train&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&#x27;distilbert-base-uncased&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">def</span> <span class="hljs-title function_">encode</span>(<span class="hljs-params">examples</span>):
<span class="hljs-meta">... </span>    <span class="hljs-keyword">return</span> tokenizer(examples[<span class="hljs-string">&#x27;text&#x27;</span>], truncation=<span class="hljs-literal">True</span>, padding=<span class="hljs-string">&#x27;max_length&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = dataset.<span class="hljs-built_in">map</span>(encode, batched=<span class="hljs-literal">True</span>, remove_columns=[<span class="hljs-string">&quot;text&quot;</span>, <span class="hljs-string">&quot;timestamp&quot;</span>, <span class="hljs-string">&quot;url&quot;</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">next</span>(<span class="hljs-built_in">iter</span>(dataset))
{<span class="hljs-string">&#x27;input_ids&#x27;</span>: <span class="hljs-number">101</span>, <span class="hljs-number">8466</span>, <span class="hljs-number">1018</span>, <span class="hljs-number">1010</span>, <span class="hljs-number">4029</span>, <span class="hljs-number">2475</span>, <span class="hljs-number">2062</span>, <span class="hljs-number">18558</span>, <span class="hljs-number">3100</span>, <span class="hljs-number">2061</span>, ...,<span class="hljs-number">1106</span>, <span class="hljs-number">3739</span>, <span class="hljs-number">102</span>],
<span class="hljs-string">&#x27;attention_mask&#x27;</span>: [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, ..., <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]}`}}),Es=new nl({props:{$$slots:{default:[ti]},$$scope:{ctx:N}}}),se=new F({}),ee=new S({props:{code:`from datasets import load_dataset
dataset = load_dataset('oscar', 'unshuffled_deduplicated_en', streaming=True, split='train')
start_with_ar = dataset.filter(lambda example: example['text'].startswith('Ar'))
next(iter(start_with_ar))`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;oscar&#x27;</span>, <span class="hljs-string">&#x27;unshuffled_deduplicated_en&#x27;</span>, streaming=<span class="hljs-literal">True</span>, split=<span class="hljs-string">&#x27;train&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>start_with_ar = dataset.<span class="hljs-built_in">filter</span>(<span class="hljs-keyword">lambda</span> example: example[<span class="hljs-string">&#x27;text&#x27;</span>].startswith(<span class="hljs-string">&#x27;Ar&#x27;</span>))
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">next</span>(<span class="hljs-built_in">iter</span>(start_with_ar))
{<span class="hljs-string">&#x27;id&#x27;</span>: <span class="hljs-number">4</span>, <span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;Are you looking for Number the Stars (Essential Modern Classics)?...&#x27;</span>}`}}),ae=new S({props:{code:`even_dataset = dataset.filter(lambda example, idx: idx % 2 == 0, with_indices=True)
list(even_dataset.take(3))`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>even_dataset = dataset.<span class="hljs-built_in">filter</span>(<span class="hljs-keyword">lambda</span> example, idx: idx % <span class="hljs-number">2</span> == <span class="hljs-number">0</span>, with_indices=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">list</span>(even_dataset.take(<span class="hljs-number">3</span>))
[{<span class="hljs-string">&#x27;id&#x27;</span>: <span class="hljs-number">0</span>, <span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;Mtendere Village was inspired by the vision of Chief Napoleon Dzombe, ...&#x27;</span>},
 {<span class="hljs-string">&#x27;id&#x27;</span>: <span class="hljs-number">2</span>, <span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;&quot;I\\&#x27;d love to help kickstart continued development! And 0 EUR/month...&#x27;</span>},
 {<span class="hljs-string">&#x27;id&#x27;</span>: <span class="hljs-number">4</span>, <span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;Are you looking for Number the Stars (Essential Modern Classics)? Normally, ...&#x27;</span>}]`}}),te=new F({}),ne=new S({props:{code:`seed, buffer_size = 42, 10_000
dataset = dataset.shuffle(seed, buffer_size=buffer_size)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>seed, buffer_size = <span class="hljs-number">42</span>, <span class="hljs-number">10_000</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = dataset.shuffle(seed, buffer_size=buffer_size)`}}),re=new Zp({props:{group1:{id:"pt",code:`import torch
from torch.utils.data import DataLoader
from transformers import AutoModelForMaskedLM, DataCollatorForLanguageModeling
from tqdm import tqdm
dataset = dataset.with_format("torch")
dataloader = DataLoader(dataset, collate_fn=DataCollatorForLanguageModeling(tokenizer))
device = 'cuda' if torch.cuda.is_available() else 'cpu' 
model = AutoModelForMaskedLM.from_pretrained("distilbert-base-uncased")
model.train().to(device)
optimizer = torch.optim.AdamW(params=model.parameters(), lr=1e-5)
for epoch in range(3):
    dataset.set_epoch(epoch)
    for i, batch in enumerate(tqdm(dataloader, total=5)):
        if i == 5:
            break
        batch = {k: v.to(device) for k, v in batch.items()}
        outputs = model(**batch)
        loss = outputs[0]
        loss.backward()
        optimizer.step()
        optimizer.zero_grad()
        if i % 10 == 0:
            print(f"loss: {loss}")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> DataLoader
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForMaskedLM, DataCollatorForLanguageModeling
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> tqdm <span class="hljs-keyword">import</span> tqdm
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = dataset.with_format(<span class="hljs-string">&quot;torch&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>dataloader = DataLoader(dataset, collate_fn=DataCollatorForLanguageModeling(tokenizer))
<span class="hljs-meta">&gt;&gt;&gt; </span>device = <span class="hljs-string">&#x27;cuda&#x27;</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">&#x27;cpu&#x27;</span> 
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMaskedLM.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.train().to(device)
<span class="hljs-meta">&gt;&gt;&gt; </span>optimizer = torch.optim.AdamW(params=model.parameters(), lr=<span class="hljs-number">1e-5</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">3</span>):
<span class="hljs-meta">... </span>    dataset.set_epoch(epoch)
<span class="hljs-meta">... </span>    <span class="hljs-keyword">for</span> i, batch <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(tqdm(dataloader, total=<span class="hljs-number">5</span>)):
<span class="hljs-meta">... </span>        <span class="hljs-keyword">if</span> i == <span class="hljs-number">5</span>:
<span class="hljs-meta">... </span>            <span class="hljs-keyword">break</span>
<span class="hljs-meta">... </span>        batch = {k: v.to(device) <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> batch.items()}
<span class="hljs-meta">... </span>        outputs = model(**batch)
<span class="hljs-meta">... </span>        loss = outputs[<span class="hljs-number">0</span>]
<span class="hljs-meta">... </span>        loss.backward()
<span class="hljs-meta">... </span>        optimizer.step()
<span class="hljs-meta">... </span>        optimizer.zero_grad()
<span class="hljs-meta">... </span>        <span class="hljs-keyword">if</span> i % <span class="hljs-number">10</span> == <span class="hljs-number">0</span>:
<span class="hljs-meta">... </span>            <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;loss: <span class="hljs-subst">{loss}</span>&quot;</span>)`},group2:{id:"tf",code:"# WIP",highlighted:'<span class="hljs-comment"># WIP</span>'}}}),{c(){c=t("meta"),$=d(),m=t("h1"),w=t("a"),k=t("span"),u(x.$$.fragment),y=d(),E=t("span"),q=n("Stream"),D=d(),T=t("p"),Y=n("Dataset streaming lets you get started with a dataset without waiting for the entire dataset to download. The data is downloaded progressively as you iterate over the dataset. This is especially helpful when:"),ns=d(),z=t("ul"),A=t("li"),oe=n("You don\u2019t want to wait for an extremely large dataset to download."),he=d(),rs=t("li"),de=n("The dataset size exceeds the amount of disk space on your computer."),La=d(),J=t("div"),fe=t("img"),rl=d(),ce=t("img"),Na=d(),P=t("p"),pl=n("For example, the English split of the "),qs=t("a"),il=n("OSCAR"),ol=n(" dataset is 1.2 terabytes, but you can use it instantly with streaming. Stream a dataset by setting "),Ge=t("code"),hl=n("streaming=True"),dl=n(" in "),me=t("a"),fl=n("datasets.load_dataset()"),cl=n(" as shown below:"),Oa=d(),u(Ps.$$.fragment),Fa=d(),H=t("p"),ml=n("Loading a dataset in streaming mode creates a new dataset type instance (instead of the classic "),ue=t("a"),ul=n("datasets.Dataset"),_l=n(" object), known as an "),_e=t("a"),gl=n("datasets.IterableDataset"),bl=n(". This special type of dataset has its own set of processing methods shown below."),Ya=d(),u(ps.$$.fragment),Ha=d(),U=t("h2"),is=t("a"),Ke=t("span"),u(Ms.$$.fragment),vl=d(),Qe=t("span"),jl=n("Shuffle"),Ra=d(),M=t("p"),xl=n("Like a regular "),ge=t("a"),$l=n("datasets.Dataset"),wl=n(" object, you can also shuffle a "),be=t("a"),kl=n("datasets.IterableDataset"),yl=n(" with "),ve=t("a"),El=n("datasets.IterableDataset.shuffle()"),Dl=n("."),Ba=d(),C=t("p"),Al=n("The "),Xe=t("code"),Il=n("buffer_size"),Tl=n(" argument controls the size of the buffer to randomly sample examples from. Let\u2019s say your dataset has one million examples, and you set the "),Ze=t("code"),Sl=n("buffer_size"),zl=n(" to ten thousand. "),je=t("a"),ql=n("datasets.IterableDataset.shuffle()"),Pl=n(" will randomly select examples from the first ten thousand examples in the buffer. Selected examples in the buffer are replaced with new examples. By default, the buffer size is 1,000."),Va=d(),u(Cs.$$.fragment),Ja=d(),u(os.$$.fragment),Ua=d(),W=t("h2"),hs=t("a"),sa=t("span"),u(Ls.$$.fragment),Ml=d(),ea=t("span"),Cl=n("Reshuffle"),Wa=d(),ds=t("p"),Ll=n("Sometimes you may want to reshuffle the dataset after each epoch. This will require you to set a different seed for each epoch. Use "),aa=t("code"),Nl=n("datasets.IterableDataset.set_epoch()"),Ol=n("in between epochs to tell the dataset what epoch you\u2019re on."),Ga=d(),fs=t("p"),Fl=n("Your seed effectively becomes: "),ta=t("code"),Yl=n("initial seed + current epoch"),Hl=n("."),Ka=d(),u(Ns.$$.fragment),Qa=d(),G=t("h2"),cs=t("a"),la=t("span"),u(Os.$$.fragment),Rl=d(),na=t("span"),Bl=n("Split dataset"),Xa=d(),xe=t("p"),Vl=n("You can split your dataset one of two ways:"),Za=d(),$e=t("ul"),ms=t("li"),we=t("a"),Jl=n("datasets.IterableDataset.take()"),Ul=n(" returns the first "),ra=t("code"),Wl=n("n"),Gl=n(" examples in a dataset:"),st=d(),u(Fs.$$.fragment),et=d(),ke=t("ul"),us=t("li"),ye=t("a"),Kl=n("datasets.IterableDataset.skip()"),Ql=n(" omits the first "),pa=t("code"),Xl=n("n"),Zl=n(" examples in a dataset and returns the remaining examples:"),at=d(),u(Ys.$$.fragment),tt=d(),u(_s.$$.fragment),lt=d(),Ee=t("a"),nt=d(),K=t("h2"),gs=t("a"),ia=t("span"),u(Hs.$$.fragment),sn=d(),oa=t("span"),en=n("Interleave"),rt=d(),Q=t("p"),De=t("a"),an=n("datasets.interleave_datasets()"),tn=n(" can combine an "),Ae=t("a"),ln=n("datasets.IterableDataset"),nn=n(" with other datasets. The combined dataset returns alternating examples from each of the original datasets."),pt=d(),u(Rs.$$.fragment),it=d(),bs=t("p"),rn=n("Define sampling probabilities from each of the original datasets for more control over how each of them are sampled and combined. Set the "),ha=t("code"),pn=n("probabilities"),on=n(" argument with your desired sampling probabilities:"),ot=d(),u(Bs.$$.fragment),ht=d(),R=t("p"),hn=n("Around 80% of the final dataset is made of the "),da=t("code"),dn=n("en_dataset"),fn=n(", and 20% of the "),fa=t("code"),cn=n("fr_dataset"),mn=n("."),dt=d(),X=t("h2"),vs=t("a"),ca=t("span"),u(Vs.$$.fragment),un=d(),ma=t("span"),_n=n("Remove"),ft=d(),js=t("p"),gn=n("Remove columns on-the-fly with "),Ie=t("a"),bn=n("datasets.IterableDataset.remove_columns()"),vn=n(". Specify the name of the column to remove:"),ct=d(),u(Js.$$.fragment),mt=d(),Z=t("h2"),xs=t("a"),ua=t("span"),u(Us.$$.fragment),jn=d(),_a=t("span"),xn=n("Map"),ut=d(),I=t("p"),$n=n("Similar to the "),Te=t("a"),wn=n("datasets.Dataset.map()"),kn=n(" function for a regular "),Se=t("a"),yn=n("datasets.Dataset"),En=n(", \u{1F917}  Datasets features "),ze=t("a"),Dn=n("datasets.IterableDataset.map()"),An=n(" for processing "),qe=t("a"),In=n("datasets.IterableDataset"),Tn=n(`\\s.
`),Pe=t("a"),Sn=n("datasets.IterableDataset.map()"),zn=n(" applies processing on-the-fly when examples are streamed."),_t=d(),Me=t("p"),qn=n("It allows you to apply a processing function to each example in a dataset, independently or in batches. This function can even create new rows and columns."),gt=d(),B=t("p"),Pn=n("The following example demonstrates how to tokenize a "),Ce=t("a"),Mn=n("datasets.IterableDataset"),Cn=n(". The function needs to accept and output a "),ga=t("code"),Ln=n("dict"),Nn=n(":"),bt=d(),u(Ws.$$.fragment),vt=d(),$s=t("p"),On=n("Next, apply this function to the dataset with "),Le=t("a"),Fn=n("datasets.IterableDataset.map()"),Yn=n(":"),jt=d(),u(Gs.$$.fragment),xt=d(),ws=t("p"),Hn=n("Let\u2019s take a look at another example, except this time, you will remove a column with "),Ne=t("a"),Rn=n("datasets.IterableDataset.map()"),Bn=n(". When you remove a column, it is only removed after the example has been provided to the mapped function. This allows the mapped function to use the content of the columns before they are removed."),$t=d(),V=t("p"),Vn=n("Specify the column to remove with the "),ba=t("code"),Jn=n("remove_columns"),Un=n(" argument in "),Oe=t("a"),Wn=n("datasets.IterableDataset.map()"),Gn=n(":"),wt=d(),u(Ks.$$.fragment),kt=d(),ss=t("h3"),ks=t("a"),va=t("span"),u(Qs.$$.fragment),Kn=d(),ja=t("span"),Qn=n("Batch processing"),yt=d(),O=t("p"),Fe=t("a"),Xn=n("datasets.IterableDataset.map()"),Zn=n(" also supports working with batches of examples. Operate on batches by setting "),xa=t("code"),sr=n("batched=True"),er=n(". The default batch size is 1000, but you can adjust it with the "),$a=t("code"),ar=n("batch_size"),tr=n(" argument. This opens the door to many interesting applications such as tokenization, splitting long sentences into shorter chunks, and data augmentation."),Et=d(),es=t("h4"),ys=t("a"),wa=t("span"),u(Xs.$$.fragment),lr=d(),ka=t("span"),nr=n("Tokenization"),Dt=d(),u(Zs.$$.fragment),At=d(),u(Es.$$.fragment),It=d(),as=t("h3"),Ds=t("a"),ya=t("span"),u(se.$$.fragment),rr=d(),Ea=t("span"),pr=n("Filter"),Tt=d(),As=t("p"),ir=n("You can filter rows in the dataset based on a predicate function using "),Ye=t("a"),or=n("datasets.Dataset.filter()"),hr=n(". It returns rows that match a specified condition:"),St=d(),u(ee.$$.fragment),zt=d(),ts=t("p"),He=t("a"),dr=n("datasets.Dataset.filter()"),fr=n(" can also filter by indices if you set "),Da=t("code"),cr=n("with_indices=True"),mr=n(":"),qt=d(),u(ae.$$.fragment),Pt=d(),ls=t("h2"),Is=t("a"),Aa=t("span"),u(te.$$.fragment),ur=d(),Ia=t("span"),_r=n("Stream in a training loop"),Mt=d(),le=t("p"),Re=t("a"),gr=n("datasets.IterableDataset"),br=n(" can be integrated into a training loop. First, shuffle the dataset:"),Ct=d(),u(ne.$$.fragment),Lt=d(),Be=t("p"),vr=n("Lastly, create a simple training loop and start training:"),Nt=d(),u(re.$$.fragment),this.h()},l(s){const p=Xp('[data-svelte="svelte-1phssyn"]',document.head);c=l(p,"META",{name:!0,content:!0}),p.forEach(a),$=f(s),m=l(s,"H1",{class:!0});var pe=i(m);w=l(pe,"A",{id:!0,class:!0,href:!0});var Ta=i(w);k=l(Ta,"SPAN",{});var Sa=i(k);_(x.$$.fragment,Sa),Sa.forEach(a),Ta.forEach(a),y=f(pe),E=l(pe,"SPAN",{});var za=i(E);q=r(za,"Stream"),za.forEach(a),pe.forEach(a),D=f(s),T=l(s,"P",{});var wr=i(T);Y=r(wr,"Dataset streaming lets you get started with a dataset without waiting for the entire dataset to download. The data is downloaded progressively as you iterate over the dataset. This is especially helpful when:"),wr.forEach(a),ns=f(s),z=l(s,"UL",{});var Ft=i(z);A=l(Ft,"LI",{});var kr=i(A);oe=r(kr,"You don\u2019t want to wait for an extremely large dataset to download."),kr.forEach(a),he=f(Ft),rs=l(Ft,"LI",{});var yr=i(rs);de=r(yr,"The dataset size exceeds the amount of disk space on your computer."),yr.forEach(a),Ft.forEach(a),La=f(s),J=l(s,"DIV",{class:!0});var Yt=i(J);fe=l(Yt,"IMG",{class:!0,src:!0}),rl=f(Yt),ce=l(Yt,"IMG",{class:!0,src:!0}),Yt.forEach(a),Na=f(s),P=l(s,"P",{});var Ts=i(P);pl=r(Ts,"For example, the English split of the "),qs=l(Ts,"A",{href:!0,rel:!0});var Er=i(qs);il=r(Er,"OSCAR"),Er.forEach(a),ol=r(Ts," dataset is 1.2 terabytes, but you can use it instantly with streaming. Stream a dataset by setting "),Ge=l(Ts,"CODE",{});var Dr=i(Ge);hl=r(Dr,"streaming=True"),Dr.forEach(a),dl=r(Ts," in "),me=l(Ts,"A",{href:!0});var Ar=i(me);fl=r(Ar,"datasets.load_dataset()"),Ar.forEach(a),cl=r(Ts," as shown below:"),Ts.forEach(a),Oa=f(s),_(Ps.$$.fragment,s),Fa=f(s),H=l(s,"P",{});var Ve=i(H);ml=r(Ve,"Loading a dataset in streaming mode creates a new dataset type instance (instead of the classic "),ue=l(Ve,"A",{href:!0});var Ir=i(ue);ul=r(Ir,"datasets.Dataset"),Ir.forEach(a),_l=r(Ve," object), known as an "),_e=l(Ve,"A",{href:!0});var Tr=i(_e);gl=r(Tr,"datasets.IterableDataset"),Tr.forEach(a),bl=r(Ve,". This special type of dataset has its own set of processing methods shown below."),Ve.forEach(a),Ya=f(s),_(ps.$$.fragment,s),Ha=f(s),U=l(s,"H2",{class:!0});var Ht=i(U);is=l(Ht,"A",{id:!0,class:!0,href:!0});var Sr=i(is);Ke=l(Sr,"SPAN",{});var zr=i(Ke);_(Ms.$$.fragment,zr),zr.forEach(a),Sr.forEach(a),vl=f(Ht),Qe=l(Ht,"SPAN",{});var qr=i(Qe);jl=r(qr,"Shuffle"),qr.forEach(a),Ht.forEach(a),Ra=f(s),M=l(s,"P",{});var Ss=i(M);xl=r(Ss,"Like a regular "),ge=l(Ss,"A",{href:!0});var Pr=i(ge);$l=r(Pr,"datasets.Dataset"),Pr.forEach(a),wl=r(Ss," object, you can also shuffle a "),be=l(Ss,"A",{href:!0});var Mr=i(be);kl=r(Mr,"datasets.IterableDataset"),Mr.forEach(a),yl=r(Ss," with "),ve=l(Ss,"A",{href:!0});var Cr=i(ve);El=r(Cr,"datasets.IterableDataset.shuffle()"),Cr.forEach(a),Dl=r(Ss,"."),Ss.forEach(a),Ba=f(s),C=l(s,"P",{});var zs=i(C);Al=r(zs,"The "),Xe=l(zs,"CODE",{});var Lr=i(Xe);Il=r(Lr,"buffer_size"),Lr.forEach(a),Tl=r(zs," argument controls the size of the buffer to randomly sample examples from. Let\u2019s say your dataset has one million examples, and you set the "),Ze=l(zs,"CODE",{});var Nr=i(Ze);Sl=r(Nr,"buffer_size"),Nr.forEach(a),zl=r(zs," to ten thousand. "),je=l(zs,"A",{href:!0});var Or=i(je);ql=r(Or,"datasets.IterableDataset.shuffle()"),Or.forEach(a),Pl=r(zs," will randomly select examples from the first ten thousand examples in the buffer. Selected examples in the buffer are replaced with new examples. By default, the buffer size is 1,000."),zs.forEach(a),Va=f(s),_(Cs.$$.fragment,s),Ja=f(s),_(os.$$.fragment,s),Ua=f(s),W=l(s,"H2",{class:!0});var Rt=i(W);hs=l(Rt,"A",{id:!0,class:!0,href:!0});var Fr=i(hs);sa=l(Fr,"SPAN",{});var Yr=i(sa);_(Ls.$$.fragment,Yr),Yr.forEach(a),Fr.forEach(a),Ml=f(Rt),ea=l(Rt,"SPAN",{});var Hr=i(ea);Cl=r(Hr,"Reshuffle"),Hr.forEach(a),Rt.forEach(a),Wa=f(s),ds=l(s,"P",{});var Bt=i(ds);Ll=r(Bt,"Sometimes you may want to reshuffle the dataset after each epoch. This will require you to set a different seed for each epoch. Use "),aa=l(Bt,"CODE",{});var Rr=i(aa);Nl=r(Rr,"datasets.IterableDataset.set_epoch()"),Rr.forEach(a),Ol=r(Bt,"in between epochs to tell the dataset what epoch you\u2019re on."),Bt.forEach(a),Ga=f(s),fs=l(s,"P",{});var Vt=i(fs);Fl=r(Vt,"Your seed effectively becomes: "),ta=l(Vt,"CODE",{});var Br=i(ta);Yl=r(Br,"initial seed + current epoch"),Br.forEach(a),Hl=r(Vt,"."),Vt.forEach(a),Ka=f(s),_(Ns.$$.fragment,s),Qa=f(s),G=l(s,"H2",{class:!0});var Jt=i(G);cs=l(Jt,"A",{id:!0,class:!0,href:!0});var Vr=i(cs);la=l(Vr,"SPAN",{});var Jr=i(la);_(Os.$$.fragment,Jr),Jr.forEach(a),Vr.forEach(a),Rl=f(Jt),na=l(Jt,"SPAN",{});var Ur=i(na);Bl=r(Ur,"Split dataset"),Ur.forEach(a),Jt.forEach(a),Xa=f(s),xe=l(s,"P",{});var Wr=i(xe);Vl=r(Wr,"You can split your dataset one of two ways:"),Wr.forEach(a),Za=f(s),$e=l(s,"UL",{});var Gr=i($e);ms=l(Gr,"LI",{});var qa=i(ms);we=l(qa,"A",{href:!0});var Kr=i(we);Jl=r(Kr,"datasets.IterableDataset.take()"),Kr.forEach(a),Ul=r(qa," returns the first "),ra=l(qa,"CODE",{});var Qr=i(ra);Wl=r(Qr,"n"),Qr.forEach(a),Gl=r(qa," examples in a dataset:"),qa.forEach(a),Gr.forEach(a),st=f(s),_(Fs.$$.fragment,s),et=f(s),ke=l(s,"UL",{});var Xr=i(ke);us=l(Xr,"LI",{});var Pa=i(us);ye=l(Pa,"A",{href:!0});var Zr=i(ye);Kl=r(Zr,"datasets.IterableDataset.skip()"),Zr.forEach(a),Ql=r(Pa," omits the first "),pa=l(Pa,"CODE",{});var sp=i(pa);Xl=r(sp,"n"),sp.forEach(a),Zl=r(Pa," examples in a dataset and returns the remaining examples:"),Pa.forEach(a),Xr.forEach(a),at=f(s),_(Ys.$$.fragment,s),tt=f(s),_(_s.$$.fragment,s),lt=f(s),Ee=l(s,"A",{id:!0}),i(Ee).forEach(a),nt=f(s),K=l(s,"H2",{class:!0});var Ut=i(K);gs=l(Ut,"A",{id:!0,class:!0,href:!0});var ep=i(gs);ia=l(ep,"SPAN",{});var ap=i(ia);_(Hs.$$.fragment,ap),ap.forEach(a),ep.forEach(a),sn=f(Ut),oa=l(Ut,"SPAN",{});var tp=i(oa);en=r(tp,"Interleave"),tp.forEach(a),Ut.forEach(a),rt=f(s),Q=l(s,"P",{});var Ma=i(Q);De=l(Ma,"A",{href:!0});var lp=i(De);an=r(lp,"datasets.interleave_datasets()"),lp.forEach(a),tn=r(Ma," can combine an "),Ae=l(Ma,"A",{href:!0});var np=i(Ae);ln=r(np,"datasets.IterableDataset"),np.forEach(a),nn=r(Ma," with other datasets. The combined dataset returns alternating examples from each of the original datasets."),Ma.forEach(a),pt=f(s),_(Rs.$$.fragment,s),it=f(s),bs=l(s,"P",{});var Wt=i(bs);rn=r(Wt,"Define sampling probabilities from each of the original datasets for more control over how each of them are sampled and combined. Set the "),ha=l(Wt,"CODE",{});var rp=i(ha);pn=r(rp,"probabilities"),rp.forEach(a),on=r(Wt," argument with your desired sampling probabilities:"),Wt.forEach(a),ot=f(s),_(Bs.$$.fragment,s),ht=f(s),R=l(s,"P",{});var Je=i(R);hn=r(Je,"Around 80% of the final dataset is made of the "),da=l(Je,"CODE",{});var pp=i(da);dn=r(pp,"en_dataset"),pp.forEach(a),fn=r(Je,", and 20% of the "),fa=l(Je,"CODE",{});var ip=i(fa);cn=r(ip,"fr_dataset"),ip.forEach(a),mn=r(Je,"."),Je.forEach(a),dt=f(s),X=l(s,"H2",{class:!0});var Gt=i(X);vs=l(Gt,"A",{id:!0,class:!0,href:!0});var op=i(vs);ca=l(op,"SPAN",{});var hp=i(ca);_(Vs.$$.fragment,hp),hp.forEach(a),op.forEach(a),un=f(Gt),ma=l(Gt,"SPAN",{});var dp=i(ma);_n=r(dp,"Remove"),dp.forEach(a),Gt.forEach(a),ft=f(s),js=l(s,"P",{});var Kt=i(js);gn=r(Kt,"Remove columns on-the-fly with "),Ie=l(Kt,"A",{href:!0});var fp=i(Ie);bn=r(fp,"datasets.IterableDataset.remove_columns()"),fp.forEach(a),vn=r(Kt,". Specify the name of the column to remove:"),Kt.forEach(a),ct=f(s),_(Js.$$.fragment,s),mt=f(s),Z=l(s,"H2",{class:!0});var Qt=i(Z);xs=l(Qt,"A",{id:!0,class:!0,href:!0});var cp=i(xs);ua=l(cp,"SPAN",{});var mp=i(ua);_(Us.$$.fragment,mp),mp.forEach(a),cp.forEach(a),jn=f(Qt),_a=l(Qt,"SPAN",{});var up=i(_a);xn=r(up,"Map"),up.forEach(a),Qt.forEach(a),ut=f(s),I=l(s,"P",{});var L=i(I);$n=r(L,"Similar to the "),Te=l(L,"A",{href:!0});var _p=i(Te);wn=r(_p,"datasets.Dataset.map()"),_p.forEach(a),kn=r(L," function for a regular "),Se=l(L,"A",{href:!0});var gp=i(Se);yn=r(gp,"datasets.Dataset"),gp.forEach(a),En=r(L,", \u{1F917}  Datasets features "),ze=l(L,"A",{href:!0});var bp=i(ze);Dn=r(bp,"datasets.IterableDataset.map()"),bp.forEach(a),An=r(L," for processing "),qe=l(L,"A",{href:!0});var vp=i(qe);In=r(vp,"datasets.IterableDataset"),vp.forEach(a),Tn=r(L,`\\s.
`),Pe=l(L,"A",{href:!0});var jp=i(Pe);Sn=r(jp,"datasets.IterableDataset.map()"),jp.forEach(a),zn=r(L," applies processing on-the-fly when examples are streamed."),L.forEach(a),_t=f(s),Me=l(s,"P",{});var xp=i(Me);qn=r(xp,"It allows you to apply a processing function to each example in a dataset, independently or in batches. This function can even create new rows and columns."),xp.forEach(a),gt=f(s),B=l(s,"P",{});var Ue=i(B);Pn=r(Ue,"The following example demonstrates how to tokenize a "),Ce=l(Ue,"A",{href:!0});var $p=i(Ce);Mn=r($p,"datasets.IterableDataset"),$p.forEach(a),Cn=r(Ue,". The function needs to accept and output a "),ga=l(Ue,"CODE",{});var wp=i(ga);Ln=r(wp,"dict"),wp.forEach(a),Nn=r(Ue,":"),Ue.forEach(a),bt=f(s),_(Ws.$$.fragment,s),vt=f(s),$s=l(s,"P",{});var Xt=i($s);On=r(Xt,"Next, apply this function to the dataset with "),Le=l(Xt,"A",{href:!0});var kp=i(Le);Fn=r(kp,"datasets.IterableDataset.map()"),kp.forEach(a),Yn=r(Xt,":"),Xt.forEach(a),jt=f(s),_(Gs.$$.fragment,s),xt=f(s),ws=l(s,"P",{});var Zt=i(ws);Hn=r(Zt,"Let\u2019s take a look at another example, except this time, you will remove a column with "),Ne=l(Zt,"A",{href:!0});var yp=i(Ne);Rn=r(yp,"datasets.IterableDataset.map()"),yp.forEach(a),Bn=r(Zt,". When you remove a column, it is only removed after the example has been provided to the mapped function. This allows the mapped function to use the content of the columns before they are removed."),Zt.forEach(a),$t=f(s),V=l(s,"P",{});var We=i(V);Vn=r(We,"Specify the column to remove with the "),ba=l(We,"CODE",{});var Ep=i(ba);Jn=r(Ep,"remove_columns"),Ep.forEach(a),Un=r(We," argument in "),Oe=l(We,"A",{href:!0});var Dp=i(Oe);Wn=r(Dp,"datasets.IterableDataset.map()"),Dp.forEach(a),Gn=r(We,":"),We.forEach(a),wt=f(s),_(Ks.$$.fragment,s),kt=f(s),ss=l(s,"H3",{class:!0});var sl=i(ss);ks=l(sl,"A",{id:!0,class:!0,href:!0});var Ap=i(ks);va=l(Ap,"SPAN",{});var Ip=i(va);_(Qs.$$.fragment,Ip),Ip.forEach(a),Ap.forEach(a),Kn=f(sl),ja=l(sl,"SPAN",{});var Tp=i(ja);Qn=r(Tp,"Batch processing"),Tp.forEach(a),sl.forEach(a),yt=f(s),O=l(s,"P",{});var ie=i(O);Fe=l(ie,"A",{href:!0});var Sp=i(Fe);Xn=r(Sp,"datasets.IterableDataset.map()"),Sp.forEach(a),Zn=r(ie," also supports working with batches of examples. Operate on batches by setting "),xa=l(ie,"CODE",{});var zp=i(xa);sr=r(zp,"batched=True"),zp.forEach(a),er=r(ie,". The default batch size is 1000, but you can adjust it with the "),$a=l(ie,"CODE",{});var qp=i($a);ar=r(qp,"batch_size"),qp.forEach(a),tr=r(ie," argument. This opens the door to many interesting applications such as tokenization, splitting long sentences into shorter chunks, and data augmentation."),ie.forEach(a),Et=f(s),es=l(s,"H4",{class:!0});var el=i(es);ys=l(el,"A",{id:!0,class:!0,href:!0});var Pp=i(ys);wa=l(Pp,"SPAN",{});var Mp=i(wa);_(Xs.$$.fragment,Mp),Mp.forEach(a),Pp.forEach(a),lr=f(el),ka=l(el,"SPAN",{});var Cp=i(ka);nr=r(Cp,"Tokenization"),Cp.forEach(a),el.forEach(a),Dt=f(s),_(Zs.$$.fragment,s),At=f(s),_(Es.$$.fragment,s),It=f(s),as=l(s,"H3",{class:!0});var al=i(as);Ds=l(al,"A",{id:!0,class:!0,href:!0});var Lp=i(Ds);ya=l(Lp,"SPAN",{});var Np=i(ya);_(se.$$.fragment,Np),Np.forEach(a),Lp.forEach(a),rr=f(al),Ea=l(al,"SPAN",{});var Op=i(Ea);pr=r(Op,"Filter"),Op.forEach(a),al.forEach(a),Tt=f(s),As=l(s,"P",{});var tl=i(As);ir=r(tl,"You can filter rows in the dataset based on a predicate function using "),Ye=l(tl,"A",{href:!0});var Fp=i(Ye);or=r(Fp,"datasets.Dataset.filter()"),Fp.forEach(a),hr=r(tl,". It returns rows that match a specified condition:"),tl.forEach(a),St=f(s),_(ee.$$.fragment,s),zt=f(s),ts=l(s,"P",{});var Ca=i(ts);He=l(Ca,"A",{href:!0});var Yp=i(He);dr=r(Yp,"datasets.Dataset.filter()"),Yp.forEach(a),fr=r(Ca," can also filter by indices if you set "),Da=l(Ca,"CODE",{});var Hp=i(Da);cr=r(Hp,"with_indices=True"),Hp.forEach(a),mr=r(Ca,":"),Ca.forEach(a),qt=f(s),_(ae.$$.fragment,s),Pt=f(s),ls=l(s,"H2",{class:!0});var ll=i(ls);Is=l(ll,"A",{id:!0,class:!0,href:!0});var Rp=i(Is);Aa=l(Rp,"SPAN",{});var Bp=i(Aa);_(te.$$.fragment,Bp),Bp.forEach(a),Rp.forEach(a),ur=f(ll),Ia=l(ll,"SPAN",{});var Vp=i(Ia);_r=r(Vp,"Stream in a training loop"),Vp.forEach(a),ll.forEach(a),Mt=f(s),le=l(s,"P",{});var jr=i(le);Re=l(jr,"A",{href:!0});var Jp=i(Re);gr=r(Jp,"datasets.IterableDataset"),Jp.forEach(a),br=r(jr," can be integrated into a training loop. First, shuffle the dataset:"),jr.forEach(a),Ct=f(s),_(ne.$$.fragment,s),Lt=f(s),Be=l(s,"P",{});var Up=i(Be);vr=r(Up,"Lastly, create a simple training loop and start training:"),Up.forEach(a),Nt=f(s),_(re.$$.fragment,s),this.h()},h(){h(c,"name","hf:doc:metadata"),h(c,"content",JSON.stringify(ni)),h(w,"id","stream"),h(w,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(w,"href","#stream"),h(m,"class","relative group"),h(fe,"class","block dark:hidden"),Wp(fe.src,xr="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/datasets/streaming.gif")||h(fe,"src",xr),h(ce,"class","hidden dark:block"),Wp(ce.src,$r="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/datasets/streaming-dark.gif")||h(ce,"src",$r),h(J,"class","flex justify-center"),h(qs,"href","https://huggingface.co/datasets/oscar"),h(qs,"rel","nofollow"),h(me,"href","/docs/datasets/pr_3887/en/package_reference/loading_methods#datasets.load_dataset"),h(ue,"href","/docs/datasets/pr_3887/en/package_reference/main_classes#datasets.Dataset"),h(_e,"href","/docs/datasets/pr_3887/en/package_reference/main_classes#datasets.IterableDataset"),h(is,"id","shuffle"),h(is,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(is,"href","#shuffle"),h(U,"class","relative group"),h(ge,"href","/docs/datasets/pr_3887/en/package_reference/main_classes#datasets.Dataset"),h(be,"href","/docs/datasets/pr_3887/en/package_reference/main_classes#datasets.IterableDataset"),h(ve,"href","/docs/datasets/pr_3887/en/package_reference/main_classes#datasets.IterableDataset.shuffle"),h(je,"href","/docs/datasets/pr_3887/en/package_reference/main_classes#datasets.IterableDataset.shuffle"),h(hs,"id","reshuffle"),h(hs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(hs,"href","#reshuffle"),h(W,"class","relative group"),h(cs,"id","split-dataset"),h(cs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(cs,"href","#split-dataset"),h(G,"class","relative group"),h(we,"href","/docs/datasets/pr_3887/en/package_reference/main_classes#datasets.IterableDataset.take"),h(ye,"href","/docs/datasets/pr_3887/en/package_reference/main_classes#datasets.IterableDataset.skip"),h(Ee,"id","interleave_datasets"),h(gs,"id","interleave"),h(gs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(gs,"href","#interleave"),h(K,"class","relative group"),h(De,"href","/docs/datasets/pr_3887/en/package_reference/main_classes#datasets.interleave_datasets"),h(Ae,"href","/docs/datasets/pr_3887/en/package_reference/main_classes#datasets.IterableDataset"),h(vs,"id","remove"),h(vs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(vs,"href","#remove"),h(X,"class","relative group"),h(Ie,"href","/docs/datasets/pr_3887/en/package_reference/main_classes#datasets.IterableDataset.remove_columns"),h(xs,"id","map"),h(xs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(xs,"href","#map"),h(Z,"class","relative group"),h(Te,"href","/docs/datasets/pr_3887/en/package_reference/main_classes#datasets.Dataset.map"),h(Se,"href","/docs/datasets/pr_3887/en/package_reference/main_classes#datasets.Dataset"),h(ze,"href","/docs/datasets/pr_3887/en/package_reference/main_classes#datasets.IterableDataset.map"),h(qe,"href","/docs/datasets/pr_3887/en/package_reference/main_classes#datasets.IterableDataset"),h(Pe,"href","/docs/datasets/pr_3887/en/package_reference/main_classes#datasets.IterableDataset.map"),h(Ce,"href","/docs/datasets/pr_3887/en/package_reference/main_classes#datasets.IterableDataset"),h(Le,"href","/docs/datasets/pr_3887/en/package_reference/main_classes#datasets.IterableDataset.map"),h(Ne,"href","/docs/datasets/pr_3887/en/package_reference/main_classes#datasets.IterableDataset.map"),h(Oe,"href","/docs/datasets/pr_3887/en/package_reference/main_classes#datasets.IterableDataset.map"),h(ks,"id","batch-processing"),h(ks,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(ks,"href","#batch-processing"),h(ss,"class","relative group"),h(Fe,"href","/docs/datasets/pr_3887/en/package_reference/main_classes#datasets.IterableDataset.map"),h(ys,"id","tokenization"),h(ys,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(ys,"href","#tokenization"),h(es,"class","relative group"),h(Ds,"id","filter"),h(Ds,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(Ds,"href","#filter"),h(as,"class","relative group"),h(Ye,"href","/docs/datasets/pr_3887/en/package_reference/main_classes#datasets.Dataset.filter"),h(He,"href","/docs/datasets/pr_3887/en/package_reference/main_classes#datasets.Dataset.filter"),h(Is,"id","stream-in-a-training-loop"),h(Is,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(Is,"href","#stream-in-a-training-loop"),h(ls,"class","relative group"),h(Re,"href","/docs/datasets/pr_3887/en/package_reference/main_classes#datasets.IterableDataset")},m(s,p){e(document.head,c),o(s,$,p),o(s,m,p),e(m,w),e(w,k),g(x,k,null),e(m,y),e(m,E),e(E,q),o(s,D,p),o(s,T,p),e(T,Y),o(s,ns,p),o(s,z,p),e(z,A),e(A,oe),e(z,he),e(z,rs),e(rs,de),o(s,La,p),o(s,J,p),e(J,fe),e(J,rl),e(J,ce),o(s,Na,p),o(s,P,p),e(P,pl),e(P,qs),e(qs,il),e(P,ol),e(P,Ge),e(Ge,hl),e(P,dl),e(P,me),e(me,fl),e(P,cl),o(s,Oa,p),g(Ps,s,p),o(s,Fa,p),o(s,H,p),e(H,ml),e(H,ue),e(ue,ul),e(H,_l),e(H,_e),e(_e,gl),e(H,bl),o(s,Ya,p),g(ps,s,p),o(s,Ha,p),o(s,U,p),e(U,is),e(is,Ke),g(Ms,Ke,null),e(U,vl),e(U,Qe),e(Qe,jl),o(s,Ra,p),o(s,M,p),e(M,xl),e(M,ge),e(ge,$l),e(M,wl),e(M,be),e(be,kl),e(M,yl),e(M,ve),e(ve,El),e(M,Dl),o(s,Ba,p),o(s,C,p),e(C,Al),e(C,Xe),e(Xe,Il),e(C,Tl),e(C,Ze),e(Ze,Sl),e(C,zl),e(C,je),e(je,ql),e(C,Pl),o(s,Va,p),g(Cs,s,p),o(s,Ja,p),g(os,s,p),o(s,Ua,p),o(s,W,p),e(W,hs),e(hs,sa),g(Ls,sa,null),e(W,Ml),e(W,ea),e(ea,Cl),o(s,Wa,p),o(s,ds,p),e(ds,Ll),e(ds,aa),e(aa,Nl),e(ds,Ol),o(s,Ga,p),o(s,fs,p),e(fs,Fl),e(fs,ta),e(ta,Yl),e(fs,Hl),o(s,Ka,p),g(Ns,s,p),o(s,Qa,p),o(s,G,p),e(G,cs),e(cs,la),g(Os,la,null),e(G,Rl),e(G,na),e(na,Bl),o(s,Xa,p),o(s,xe,p),e(xe,Vl),o(s,Za,p),o(s,$e,p),e($e,ms),e(ms,we),e(we,Jl),e(ms,Ul),e(ms,ra),e(ra,Wl),e(ms,Gl),o(s,st,p),g(Fs,s,p),o(s,et,p),o(s,ke,p),e(ke,us),e(us,ye),e(ye,Kl),e(us,Ql),e(us,pa),e(pa,Xl),e(us,Zl),o(s,at,p),g(Ys,s,p),o(s,tt,p),g(_s,s,p),o(s,lt,p),o(s,Ee,p),o(s,nt,p),o(s,K,p),e(K,gs),e(gs,ia),g(Hs,ia,null),e(K,sn),e(K,oa),e(oa,en),o(s,rt,p),o(s,Q,p),e(Q,De),e(De,an),e(Q,tn),e(Q,Ae),e(Ae,ln),e(Q,nn),o(s,pt,p),g(Rs,s,p),o(s,it,p),o(s,bs,p),e(bs,rn),e(bs,ha),e(ha,pn),e(bs,on),o(s,ot,p),g(Bs,s,p),o(s,ht,p),o(s,R,p),e(R,hn),e(R,da),e(da,dn),e(R,fn),e(R,fa),e(fa,cn),e(R,mn),o(s,dt,p),o(s,X,p),e(X,vs),e(vs,ca),g(Vs,ca,null),e(X,un),e(X,ma),e(ma,_n),o(s,ft,p),o(s,js,p),e(js,gn),e(js,Ie),e(Ie,bn),e(js,vn),o(s,ct,p),g(Js,s,p),o(s,mt,p),o(s,Z,p),e(Z,xs),e(xs,ua),g(Us,ua,null),e(Z,jn),e(Z,_a),e(_a,xn),o(s,ut,p),o(s,I,p),e(I,$n),e(I,Te),e(Te,wn),e(I,kn),e(I,Se),e(Se,yn),e(I,En),e(I,ze),e(ze,Dn),e(I,An),e(I,qe),e(qe,In),e(I,Tn),e(I,Pe),e(Pe,Sn),e(I,zn),o(s,_t,p),o(s,Me,p),e(Me,qn),o(s,gt,p),o(s,B,p),e(B,Pn),e(B,Ce),e(Ce,Mn),e(B,Cn),e(B,ga),e(ga,Ln),e(B,Nn),o(s,bt,p),g(Ws,s,p),o(s,vt,p),o(s,$s,p),e($s,On),e($s,Le),e(Le,Fn),e($s,Yn),o(s,jt,p),g(Gs,s,p),o(s,xt,p),o(s,ws,p),e(ws,Hn),e(ws,Ne),e(Ne,Rn),e(ws,Bn),o(s,$t,p),o(s,V,p),e(V,Vn),e(V,ba),e(ba,Jn),e(V,Un),e(V,Oe),e(Oe,Wn),e(V,Gn),o(s,wt,p),g(Ks,s,p),o(s,kt,p),o(s,ss,p),e(ss,ks),e(ks,va),g(Qs,va,null),e(ss,Kn),e(ss,ja),e(ja,Qn),o(s,yt,p),o(s,O,p),e(O,Fe),e(Fe,Xn),e(O,Zn),e(O,xa),e(xa,sr),e(O,er),e(O,$a),e($a,ar),e(O,tr),o(s,Et,p),o(s,es,p),e(es,ys),e(ys,wa),g(Xs,wa,null),e(es,lr),e(es,ka),e(ka,nr),o(s,Dt,p),g(Zs,s,p),o(s,At,p),g(Es,s,p),o(s,It,p),o(s,as,p),e(as,Ds),e(Ds,ya),g(se,ya,null),e(as,rr),e(as,Ea),e(Ea,pr),o(s,Tt,p),o(s,As,p),e(As,ir),e(As,Ye),e(Ye,or),e(As,hr),o(s,St,p),g(ee,s,p),o(s,zt,p),o(s,ts,p),e(ts,He),e(He,dr),e(ts,fr),e(ts,Da),e(Da,cr),e(ts,mr),o(s,qt,p),g(ae,s,p),o(s,Pt,p),o(s,ls,p),e(ls,Is),e(Is,Aa),g(te,Aa,null),e(ls,ur),e(ls,Ia),e(Ia,_r),o(s,Mt,p),o(s,le,p),e(le,Re),e(Re,gr),e(le,br),o(s,Ct,p),g(ne,s,p),o(s,Lt,p),o(s,Be,p),e(Be,vr),o(s,Nt,p),g(re,s,p),Ot=!0},p(s,[p]){const pe={};p&2&&(pe.$$scope={dirty:p,ctx:s}),ps.$set(pe);const Ta={};p&2&&(Ta.$$scope={dirty:p,ctx:s}),os.$set(Ta);const Sa={};p&2&&(Sa.$$scope={dirty:p,ctx:s}),_s.$set(Sa);const za={};p&2&&(za.$$scope={dirty:p,ctx:s}),Es.$set(za)},i(s){Ot||(b(x.$$.fragment,s),b(Ps.$$.fragment,s),b(ps.$$.fragment,s),b(Ms.$$.fragment,s),b(Cs.$$.fragment,s),b(os.$$.fragment,s),b(Ls.$$.fragment,s),b(Ns.$$.fragment,s),b(Os.$$.fragment,s),b(Fs.$$.fragment,s),b(Ys.$$.fragment,s),b(_s.$$.fragment,s),b(Hs.$$.fragment,s),b(Rs.$$.fragment,s),b(Bs.$$.fragment,s),b(Vs.$$.fragment,s),b(Js.$$.fragment,s),b(Us.$$.fragment,s),b(Ws.$$.fragment,s),b(Gs.$$.fragment,s),b(Ks.$$.fragment,s),b(Qs.$$.fragment,s),b(Xs.$$.fragment,s),b(Zs.$$.fragment,s),b(Es.$$.fragment,s),b(se.$$.fragment,s),b(ee.$$.fragment,s),b(ae.$$.fragment,s),b(te.$$.fragment,s),b(ne.$$.fragment,s),b(re.$$.fragment,s),Ot=!0)},o(s){v(x.$$.fragment,s),v(Ps.$$.fragment,s),v(ps.$$.fragment,s),v(Ms.$$.fragment,s),v(Cs.$$.fragment,s),v(os.$$.fragment,s),v(Ls.$$.fragment,s),v(Ns.$$.fragment,s),v(Os.$$.fragment,s),v(Fs.$$.fragment,s),v(Ys.$$.fragment,s),v(_s.$$.fragment,s),v(Hs.$$.fragment,s),v(Rs.$$.fragment,s),v(Bs.$$.fragment,s),v(Vs.$$.fragment,s),v(Js.$$.fragment,s),v(Us.$$.fragment,s),v(Ws.$$.fragment,s),v(Gs.$$.fragment,s),v(Ks.$$.fragment,s),v(Qs.$$.fragment,s),v(Xs.$$.fragment,s),v(Zs.$$.fragment,s),v(Es.$$.fragment,s),v(se.$$.fragment,s),v(ee.$$.fragment,s),v(ae.$$.fragment,s),v(te.$$.fragment,s),v(ne.$$.fragment,s),v(re.$$.fragment,s),Ot=!1},d(s){a(c),s&&a($),s&&a(m),j(x),s&&a(D),s&&a(T),s&&a(ns),s&&a(z),s&&a(La),s&&a(J),s&&a(Na),s&&a(P),s&&a(Oa),j(Ps,s),s&&a(Fa),s&&a(H),s&&a(Ya),j(ps,s),s&&a(Ha),s&&a(U),j(Ms),s&&a(Ra),s&&a(M),s&&a(Ba),s&&a(C),s&&a(Va),j(Cs,s),s&&a(Ja),j(os,s),s&&a(Ua),s&&a(W),j(Ls),s&&a(Wa),s&&a(ds),s&&a(Ga),s&&a(fs),s&&a(Ka),j(Ns,s),s&&a(Qa),s&&a(G),j(Os),s&&a(Xa),s&&a(xe),s&&a(Za),s&&a($e),s&&a(st),j(Fs,s),s&&a(et),s&&a(ke),s&&a(at),j(Ys,s),s&&a(tt),j(_s,s),s&&a(lt),s&&a(Ee),s&&a(nt),s&&a(K),j(Hs),s&&a(rt),s&&a(Q),s&&a(pt),j(Rs,s),s&&a(it),s&&a(bs),s&&a(ot),j(Bs,s),s&&a(ht),s&&a(R),s&&a(dt),s&&a(X),j(Vs),s&&a(ft),s&&a(js),s&&a(ct),j(Js,s),s&&a(mt),s&&a(Z),j(Us),s&&a(ut),s&&a(I),s&&a(_t),s&&a(Me),s&&a(gt),s&&a(B),s&&a(bt),j(Ws,s),s&&a(vt),s&&a($s),s&&a(jt),j(Gs,s),s&&a(xt),s&&a(ws),s&&a($t),s&&a(V),s&&a(wt),j(Ks,s),s&&a(kt),s&&a(ss),j(Qs),s&&a(yt),s&&a(O),s&&a(Et),s&&a(es),j(Xs),s&&a(Dt),j(Zs,s),s&&a(At),j(Es,s),s&&a(It),s&&a(as),j(se),s&&a(Tt),s&&a(As),s&&a(St),j(ee,s),s&&a(zt),s&&a(ts),s&&a(qt),j(ae,s),s&&a(Pt),s&&a(ls),j(te),s&&a(Mt),s&&a(le),s&&a(Ct),j(ne,s),s&&a(Lt),s&&a(Be),s&&a(Nt),j(re,s)}}}const ni={local:"stream",sections:[{local:"shuffle",title:"Shuffle"},{local:"reshuffle",title:"Reshuffle"},{local:"split-dataset",title:"Split dataset"},{local:"interleave",title:"Interleave"},{local:"remove",title:"Remove"},{local:"map",sections:[{local:"batch-processing",sections:[{local:"tokenization",title:"Tokenization"}],title:"Batch processing"},{local:"filter",title:"Filter"}],title:"Map"},{local:"stream-in-a-training-loop",title:"Stream in a training loop"}],title:"Stream"};function ri(N,c,$){let{fw:m}=c;return N.$$set=w=>{"fw"in w&&$(0,m=w.fw)},[m]}class fi extends Gp{constructor(c){super();Kp(this,c,ri,li,Qp,{fw:0})}}export{fi as default,ni as metadata};
