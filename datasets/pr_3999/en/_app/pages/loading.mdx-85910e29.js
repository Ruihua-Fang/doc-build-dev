import{S as Yd,i as Wd,s as Gd,e as l,k as f,t as i,c as o,a as n,m as c,h as p,d as t,b as h,g as r,F as a,Q as Cl,q as b,l as wh,n as wr,o as j,B as k,p as br,w as E,y as q,j as Th,G as Dh,$ as kh,x as P,a0 as Nh,T as Ih,Y as Eh,Z as qh,M as Ch,v as Oh}from"../chunks/vendor-aa873a46.js";import{T as ua}from"../chunks/Tip-f7f252ab.js";import{I as C}from"../chunks/IconCopyLink-d0ca3106.js";import{a as Ph,C as D}from"../chunks/CodeBlock-1f14baf3.js";import{b as Ah,I as Hh,a as Fh}from"../chunks/IconTensorflow-b9816778.js";function bh(x,d,g){const u=x.slice();return u[8]=d[g],u[10]=g,u}function jh(x){let d,g,u;var y=x[8].icon;function $(_){return{props:{classNames:"mr-1.5"}}}return y&&(d=new y($())),{c(){d&&E(d.$$.fragment),g=wh()},l(_){d&&P(d.$$.fragment,_),g=wh()},m(_,m){d&&q(d,_,m),r(_,g,m),u=!0},p(_,m){if(y!==(y=_[8].icon)){if(d){wr();const w=d;j(w.$$.fragment,1,0,()=>{k(w,1)}),br()}y?(d=new y($()),E(d.$$.fragment),b(d.$$.fragment,1),q(d,g.parentNode,g)):d=null}},i(_){u||(d&&b(d.$$.fragment,_),u=!0)},o(_){d&&j(d.$$.fragment,_),u=!1},d(_){_&&t(g),d&&k(d,_)}}}function xh(x){let d,g,u,y=x[8].name+"",$,_,m,w,v,A,S,T=x[8].icon&&jh(x);function B(){return x[6](x[8])}return{c(){d=l("button"),T&&T.c(),g=f(),u=l("p"),$=i(y),m=f(),this.h()},l(I){d=o(I,"BUTTON",{class:!0});var N=n(d);T&&T.l(N),g=c(N),u=o(N,"P",{class:!0});var U=n(u);$=p(U,y),U.forEach(t),m=c(N),N.forEach(t),this.h()},h(){h(u,"class",_="!m-0 "+x[8].classNames),h(d,"class",w="flex justify-center py-1.5 px-2.5 focus:outline-none rounded-"+(x[10]?"r":"l")+" "+(x[8].group!==x[1]&&"text-gray-500 filter grayscale"))},m(I,N){r(I,d,N),T&&T.m(d,null),a(d,g),a(d,u),a(u,$),a(d,m),v=!0,A||(S=Cl(d,"click",B),A=!0)},p(I,N){x=I,x[8].icon?T?(T.p(x,N),N&1&&b(T,1)):(T=jh(x),T.c(),b(T,1),T.m(d,g)):T&&(wr(),j(T,1,1,()=>{T=null}),br()),(!v||N&1)&&y!==(y=x[8].name+"")&&Th($,y),(!v||N&1&&_!==(_="!m-0 "+x[8].classNames))&&h(u,"class",_),(!v||N&3&&w!==(w="flex justify-center py-1.5 px-2.5 focus:outline-none rounded-"+(x[10]?"r":"l")+" "+(x[8].group!==x[1]&&"text-gray-500 filter grayscale")))&&h(d,"class",w)},i(I){v||(b(T),v=!0)},o(I){j(T),v=!1},d(I){I&&t(d),T&&T.d(),A=!1,S()}}}function Lh(x){let d,g,u,y=x[3].filter(x[5]),$=[];for(let m=0;m<y.length;m+=1)$[m]=xh(bh(x,y,m));const _=m=>j($[m],1,1,()=>{$[m]=null});return{c(){d=l("div"),g=l("div");for(let m=0;m<$.length;m+=1)$[m].c();this.h()},l(m){d=o(m,"DIV",{});var w=n(d);g=o(w,"DIV",{class:!0});var v=n(g);for(let A=0;A<$.length;A+=1)$[A].l(v);v.forEach(t),w.forEach(t),this.h()},h(){h(g,"class","bg-white leading-none border border-gray-100 rounded-lg inline-flex p-0.5 text-sm mb-4 select-none")},m(m,w){r(m,d,w),a(d,g);for(let v=0;v<$.length;v+=1)$[v].m(g,null);u=!0},p(m,[w]){if(w&27){y=m[3].filter(m[5]);let v;for(v=0;v<y.length;v+=1){const A=bh(m,y,v);$[v]?($[v].p(A,w),b($[v],1)):($[v]=xh(A),$[v].c(),b($[v],1),$[v].m(g,null))}for(wr(),v=y.length;v<$.length;v+=1)_(v);br()}},i(m){if(!u){for(let w=0;w<y.length;w+=1)b($[w]);u=!0}},o(m){$=$.filter(Boolean);for(let w=0;w<$.length;w+=1)j($[w]);u=!1},d(m){m&&t(d),Dh($,m)}}}function Rh(x,d,g){let u,{ids:y}=d;const $=y.join("-"),_=Ah($);kh(x,_,S=>g(1,u=S));const m=[{id:"pt",classNames:"",icon:Hh,name:"Pytorch",group:"group1"},{id:"tf",classNames:"",icon:Fh,name:"TensorFlow",group:"group2"},{id:"stringapi",classNames:"text-blue-600",name:"String API",group:"group1"},{id:"readinstruction",classNames:"text-blue-600",name:"ReadInstruction",group:"group2"}];function w(S){Nh(_,u=S,u)}const v=S=>y.includes(S.id),A=S=>w(S.group);return x.$$set=S=>{"ids"in S&&g(0,y=S.ids)},[y,u,_,m,w,v,A]}class Sh extends Yd{constructor(d){super();Wd(this,d,Rh,Lh,Gd,{ids:0})}}function Mh(x){let d,g,u,y,$,_,m=x[1].highlighted+"",w;return g=new Ph({props:{classNames:"transition duration-200 ease-in-out "+(x[2]&&"opacity-0"),title:"Copy code excerpt to clipboard",value:x[1].code}}),$=new Sh({props:{ids:x[4]}}),{c(){d=l("div"),E(g.$$.fragment),u=f(),y=l("pre"),E($.$$.fragment),_=new Eh,this.h()},l(v){d=o(v,"DIV",{class:!0});var A=n(d);P(g.$$.fragment,A),A.forEach(t),u=c(v),y=o(v,"PRE",{});var S=n(y);P($.$$.fragment,S),_=qh(S),S.forEach(t),this.h()},h(){h(d,"class","absolute top-2.5 right-4"),_.a=null},m(v,A){r(v,d,A),q(g,d,null),r(v,u,A),r(v,y,A),q($,y,null),_.m(m,y),w=!0},p(v,A){const S={};A&4&&(S.classNames="transition duration-200 ease-in-out "+(v[2]&&"opacity-0")),A&2&&(S.value=v[1].code),g.$set(S),(!w||A&2)&&m!==(m=v[1].highlighted+"")&&_.p(m)},i(v){w||(b(g.$$.fragment,v),b($.$$.fragment,v),w=!0)},o(v){j(g.$$.fragment,v),j($.$$.fragment,v),w=!1},d(v){v&&t(d),k(g),v&&t(u),v&&t(y),k($)}}}function Vh(x){let d,g,u,y,$,_,m=x[0].highlighted+"",w;return g=new Ph({props:{classNames:"transition duration-200 ease-in-out "+(x[2]&&"opacity-0"),title:"Copy code excerpt to clipboard",value:x[0].code}}),$=new Sh({props:{ids:x[4]}}),{c(){d=l("div"),E(g.$$.fragment),u=f(),y=l("pre"),E($.$$.fragment),_=new Eh,this.h()},l(v){d=o(v,"DIV",{class:!0});var A=n(d);P(g.$$.fragment,A),A.forEach(t),u=c(v),y=o(v,"PRE",{});var S=n(y);P($.$$.fragment,S),_=qh(S),S.forEach(t),this.h()},h(){h(d,"class","absolute top-2.5 right-4"),_.a=null},m(v,A){r(v,d,A),q(g,d,null),r(v,u,A),r(v,y,A),q($,y,null),_.m(m,y),w=!0},p(v,A){const S={};A&4&&(S.classNames="transition duration-200 ease-in-out "+(v[2]&&"opacity-0")),A&1&&(S.value=v[0].code),g.$set(S),(!w||A&1)&&m!==(m=v[0].highlighted+"")&&_.p(m)},i(v){w||(b(g.$$.fragment,v),b($.$$.fragment,v),w=!0)},o(v){j(g.$$.fragment,v),j($.$$.fragment,v),w=!1},d(v){v&&t(d),k(g),v&&t(u),v&&t(y),k($)}}}function zh(x){let d,g,u,y,$,_;const m=[Vh,Mh],w=[];function v(A,S){return A[3]==="group1"?0:1}return g=v(x),u=w[g]=m[g](x),{c(){d=l("div"),u.c(),this.h()},l(A){d=o(A,"DIV",{class:!0});var S=n(d);u.l(S),S.forEach(t),this.h()},h(){h(d,"class","code-block relative")},m(A,S){r(A,d,S),w[g].m(d,null),y=!0,$||(_=[Cl(d,"mouseover",x[6]),Cl(d,"focus",x[6]),Cl(d,"mouseout",x[7]),Cl(d,"focus",x[7])],$=!0)},p(A,[S]){let T=g;g=v(A),g===T?w[g].p(A,S):(wr(),j(w[T],1,1,()=>{w[T]=null}),br(),u=w[g],u?u.p(A,S):(u=w[g]=m[g](A),u.c()),b(u,1),u.m(d,null))},i(A){y||(b(u),y=!0)},o(A){j(u),y=!1},d(A){A&&t(d),w[g].d(),$=!1,Ih(_)}}}function Uh(x,d,g){let u,{group1:y}=d,{group2:$}=d;const _=[y.id,$.id],m=_.join("-"),w=Ah(m);kh(x,w,T=>g(3,u=T));let v=!0;function A(){g(2,v=!1)}function S(){g(2,v=!0)}return x.$$set=T=>{"group1"in T&&g(0,y=T.group1),"group2"in T&&g(1,$=T.group2)},[y,$,v,u,_,w,A,S]}class Il extends Yd{constructor(d){super();Wd(this,d,Uh,zh,Gd,{group1:0,group2:1})}}function Jh(x){let d,g,u,y,$;return{c(){d=l("p"),g=i("Refer to the "),u=l("a"),y=i("Upload"),$=i(" guide for more instructions on how to create a dataset repository on the Hub, and how to upload your data files."),this.h()},l(_){d=o(_,"P",{});var m=n(d);g=p(m,"Refer to the "),u=o(m,"A",{href:!0});var w=n(u);y=p(w,"Upload"),w.forEach(t),$=p(m," guide for more instructions on how to create a dataset repository on the Hub, and how to upload your data files."),m.forEach(t),this.h()},h(){h(u,"href","./upload_dataset")},m(_,m){r(_,d,m),a(d,g),a(d,u),a(u,y),a(d,$)},d(_){_&&t(d)}}}function Bh(x){let d,g,u,y,$;return{c(){d=l("p"),g=i("If you don\u2019t specify which data files to use, "),u=l("code"),y=i("load_dataset"),$=i(" will return all the data files. This can take a long time if you are loading a large dataset like C4, which is approximately 13TB of data.")},l(_){d=o(_,"P",{});var m=n(d);g=p(m,"If you don\u2019t specify which data files to use, "),u=o(m,"CODE",{});var w=n(u);y=p(w,"load_dataset"),w.forEach(t),$=p(m," will return all the data files. This can take a long time if you are loading a large dataset like C4, which is approximately 13TB of data."),m.forEach(t)},m(_,m){r(_,d,m),a(d,g),a(d,u),a(u,y),a(d,$)},d(_){_&&t(d)}}}function Yh(x){let d,g,u,y,$;return{c(){d=l("p"),g=i("Curious about how to load datasets for vision? Check out the image loading guide "),u=l("a"),y=i("here"),$=i("!"),this.h()},l(_){d=o(_,"P",{});var m=n(d);g=p(m,"Curious about how to load datasets for vision? Check out the image loading guide "),u=o(m,"A",{href:!0});var w=n(u);y=p(w,"here"),w.forEach(t),$=p(m,"!"),m.forEach(t),this.h()},h(){h(u,"href","./image_process")},m(_,m){r(_,d,m),a(d,g),a(d,u),a(u,y),a(d,$)},d(_){_&&t(d)}}}function Wh(x){let d,g,u,y,$,_,m,w,v,A,S,T,B,I,N,U,O;return{c(){d=l("p"),g=i("An object data type in "),u=l("a"),y=i("pandas.Series"),$=i(" doesn\u2019t always carry enough information for Arrow to automatically infer a data type. For example, if a DataFrame is of length 0 or the Series only contains None/nan objects, the type is set to null. Avoid potential errors by constructing an explicit schema with "),_=l("a"),m=i("Features"),w=i(" using the "),v=l("code"),A=i("from_dict"),S=i(" or "),T=l("code"),B=i("from_pandas"),I=i(" methods. See the "),N=l("a"),U=i("troubleshoot"),O=i(" for more details on how to explicitly specify your own features."),this.h()},l(J){d=o(J,"P",{});var H=n(d);g=p(H,"An object data type in "),u=o(H,"A",{href:!0,rel:!0});var ma=n(u);y=p(ma,"pandas.Series"),ma.forEach(t),$=p(H," doesn\u2019t always carry enough information for Arrow to automatically infer a data type. For example, if a DataFrame is of length 0 or the Series only contains None/nan objects, the type is set to null. Avoid potential errors by constructing an explicit schema with "),_=o(H,"A",{href:!0});var ws=n(_);m=p(ws,"Features"),ws.forEach(t),w=p(H," using the "),v=o(H,"CODE",{});var ga=n(v);A=p(ga,"from_dict"),ga.forEach(t),S=p(H," or "),T=o(H,"CODE",{});var _a=n(T);B=p(_a,"from_pandas"),_a.forEach(t),I=p(H," methods. See the "),N=o(H,"A",{href:!0});var bs=n(N);U=p(bs,"troubleshoot"),bs.forEach(t),O=p(H," for more details on how to explicitly specify your own features."),H.forEach(t),this.h()},h(){h(u,"href","https://pandas.pydata.org/docs/reference/api/pandas.Series.html"),h(u,"rel","nofollow"),h(_,"href","/docs/datasets/pr_3999/en/package_reference/main_classes#datasets.Features"),h(N,"href","./loading#specify-features")},m(J,H){r(J,d,H),a(d,g),a(d,u),a(u,y),a(d,$),a(d,_),a(_,m),a(d,w),a(d,v),a(v,A),a(d,S),a(d,T),a(T,B),a(d,I),a(d,N),a(N,U),a(d,O)},d(J){J&&t(d)}}}function Gh(x){let d,g,u,y,$;return{c(){d=l("p"),g=i("Using "),u=l("code"),y=i("pct1_dropremainder"),$=i(" rounding may truncate the last examples in a dataset if the number of examples in your dataset don\u2019t divide evenly by 100.")},l(_){d=o(_,"P",{});var m=n(d);g=p(m,"Using "),u=o(m,"CODE",{});var w=n(u);y=p(w,"pct1_dropremainder"),w.forEach(t),$=p(m," rounding may truncate the last examples in a dataset if the number of examples in your dataset don\u2019t divide evenly by 100."),m.forEach(t)},m(_,m){r(_,d,m),a(d,g),a(d,u),a(u,y),a(d,$)},d(_){_&&t(d)}}}function Qh(x){let d,g,u,y,$;return{c(){d=l("p"),g=i("See the "),u=l("a"),y=i("Metrics"),$=i(" guide for more details on how to write your own metric loading script."),this.h()},l(_){d=o(_,"P",{});var m=n(d);g=p(m,"See the "),u=o(m,"A",{href:!0});var w=n(u);y=p(w,"Metrics"),w.forEach(t),$=p(m," guide for more details on how to write your own metric loading script."),m.forEach(t),this.h()},h(){h(u,"href","./how_to_metrics#custom-metric-loading-script")},m(_,m){r(_,d,m),a(d,g),a(d,u),a(u,y),a(d,$)},d(_){_&&t(d)}}}function Kh(x){let d,g,u,y,$;return{c(){d=l("p"),g=i("Once you\u2019ve loaded a metric for distributed usage, you can compute the metric as usual. Behind the scenes, "),u=l("a"),y=i("Metric.compute()"),$=i(" gathers all the predictions and references from the nodes, and computes the final metric."),this.h()},l(_){d=o(_,"P",{});var m=n(d);g=p(m,"Once you\u2019ve loaded a metric for distributed usage, you can compute the metric as usual. Behind the scenes, "),u=o(m,"A",{href:!0});var w=n(u);y=p(w,"Metric.compute()"),w.forEach(t),$=p(m," gathers all the predictions and references from the nodes, and computes the final metric."),m.forEach(t),this.h()},h(){h(u,"href","/docs/datasets/pr_3999/en/package_reference/main_classes#datasets.Metric.compute")},m(_,m){r(_,d,m),a(d,g),a(d,u),a(u,y),a(d,$)},d(_){_&&t(d)}}}function Xh(x){let d,g,u,y,$,_,m,w,v,A,S,T,B,I,N,U,O,J,H,ma,ws,ga,_a,bs,jr,xr,ue,kr,Er,me,qr,Ol,va,Pr,Hl,$a,Fl,as,js,ge,it,Ar,_e,Sr,Ll,xs,Tr,ve,Dr,Nr,Rl,Y,Ir,ya,Cr,Or,pt,Hr,Fr,Ml,dt,Vl,wa,Lr,zl,ks,Rr,$e,Mr,Vr,Ul,ft,Jl,Es,Bl,F,zr,ye,Ur,Jr,we,Br,Yr,be,Wr,Gr,je,Qr,Kr,xe,Xr,Zr,Yl,ct,Wl,qs,Gl,W,si,ke,ti,ai,ht,ei,li,Ql,ut,Kl,Ps,oi,Ee,ni,ri,Xl,mt,Zl,es,As,qe,gt,ii,Pe,pi,so,L,di,Ae,fi,ci,Se,hi,ui,Te,mi,gi,De,_i,vi,ba,$i,yi,to,Ss,ao,ls,Ts,Ne,_t,wi,Ie,bi,eo,ja,ji,lo,vt,oo,xa,xi,no,$t,ro,ka,ki,io,yt,po,Ea,Ei,fo,wt,co,qa,qi,ho,bt,uo,os,Ds,Ce,jt,Pi,Oe,Ai,mo,Ns,Si,Pa,Ti,Di,go,xt,_o,Aa,Ni,vo,kt,$o,Is,Ii,He,Ci,Oi,yo,Et,wo,Sa,Hi,bo,qt,jo,Ta,Fi,xo,ns,Cs,Fe,Pt,Li,Le,Ri,ko,Da,Mi,Eo,At,qo,Na,Vi,Po,St,Ao,rs,Os,Re,Tt,zi,Me,Ui,So,Ia,Ji,To,Dt,Do,Ca,Bi,No,Nt,Io,is,Hs,Ve,It,Yi,ze,Wi,Co,Fs,Gi,Oa,Qi,Ki,Oo,ps,Ls,Ue,Ct,Xi,Je,Zi,Ho,Rs,sp,Ha,tp,ap,Fo,Ot,Lo,ds,Ms,Be,Ht,ep,Ye,lp,Ro,Vs,op,Fa,np,rp,Mo,Ft,Vo,zs,zo,fs,Us,We,Lt,ip,Ge,pp,Uo,La,dp,Jo,G,fp,Qe,cp,hp,Ke,up,mp,Bo,cs,Js,Xe,Rt,gp,Ze,_p,Yo,Q,vp,Ra,$p,yp,Ma,wp,bp,Wo,K,jp,sl,xp,kp,tl,Ep,qp,Go,Mt,Qo,Bs,Pp,al,Ap,Sp,Ko,Vt,Xo,Va,Tp,Zo,zt,sn,za,Dp,tn,Ut,an,Ua,Np,en,Jt,ln,hs,Ys,el,Bt,Ip,ll,Cp,on,Ja,Op,nn,Yt,rn,Ws,Hp,ol,Fp,Lp,pn,Wt,dn,Gs,fn,Ba,cn,us,Qs,nl,Gt,Rp,rl,Mp,hn,Ya,Vp,un,ms,Ks,il,Qt,zp,pl,Up,mn,M,Jp,Wa,Bp,Yp,dl,Wp,Gp,fl,Qp,Kp,gn,Xs,Xp,Kt,Zp,sd,_n,Xt,vn,gs,Zs,cl,Zt,td,hl,ad,$n,X,ed,Ga,ld,od,sa,nd,rd,yn,Z,id,Qa,pd,dd,Ka,fd,cd,wn,ta,bn,ss,hd,ul,ud,md,Xa,gd,_d,jn,aa,xn,Za,vd,kn,ea,En,_s,st,ml,la,$d,gl,yd,qn,se,wd,Pn,oa,An,tt,Sn,vs,at,_l,na,bd,vl,jd,Tn,et,xd,$l,kd,Ed,Dn,ra,Nn,$s,lt,yl,ia,qd,wl,Pd,In,te,Ad,Cn,ae,Sd,On,ts,bl,pa,Td,jl,Dd,Nd,Id,xl,ys,Cd,kl,Od,Hd,El,Fd,Ld,Rd,ql,da,Md,ee,Vd,zd,Hn,fa,Fn,ot,Ln,nt,Ud,Pl,Jd,Bd,Rn,ca,Mn;return _=new C({}),it=new C({}),dt=new D({props:{code:`from datasets import load_dataset
dataset = load_dataset('lhoestq/demo1')`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;lhoestq/demo1&#x27;</span>)`}}),ft=new D({props:{code:`dataset = load_dataset(
  "lhoestq/custom_squad",
  revision="main"  # tag name, or branch name, or commit hash
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(
<span class="hljs-meta">&gt;&gt;&gt; </span>  <span class="hljs-string">&quot;lhoestq/custom_squad&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>  revision=<span class="hljs-string">&quot;main&quot;</span>  <span class="hljs-comment"># tag name, or branch name, or commit hash</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>)`}}),Es=new ua({props:{$$slots:{default:[Jh]},$$scope:{ctx:x}}}),ct=new D({props:{code:`data_files = {"train": "train.csv", "test": "test.csv"}
dataset = load_dataset("namespace/your_dataset_name", data_files=data_files)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>data_files = {<span class="hljs-string">&quot;train&quot;</span>: <span class="hljs-string">&quot;train.csv&quot;</span>, <span class="hljs-string">&quot;test&quot;</span>: <span class="hljs-string">&quot;test.csv&quot;</span>}
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;namespace/your_dataset_name&quot;</span>, data_files=data_files)`}}),qs=new ua({props:{warning:!0,$$slots:{default:[Bh]},$$scope:{ctx:x}}}),ut=new D({props:{code:`from datasets import load_dataset
c4_subset = load_dataset('allenai/c4', data_files='en/c4-train.0000*-of-01024.json.gz')`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>c4_subset = load_dataset(<span class="hljs-string">&#x27;allenai/c4&#x27;</span>, data_files=<span class="hljs-string">&#x27;en/c4-train.0000*-of-01024.json.gz&#x27;</span>)`}}),mt=new D({props:{code:`data_files = {"validation": "en/c4-validation.*.json.gz"}
c4_validation = load_dataset("allenai/c4", data_files=data_files, split="validation")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>data_files = {<span class="hljs-string">&quot;validation&quot;</span>: <span class="hljs-string">&quot;en/c4-validation.*.json.gz&quot;</span>}
<span class="hljs-meta">&gt;&gt;&gt; </span>c4_validation = load_dataset(<span class="hljs-string">&quot;allenai/c4&quot;</span>, data_files=data_files, split=<span class="hljs-string">&quot;validation&quot;</span>)`}}),gt=new C({}),Ss=new ua({props:{$$slots:{default:[Yh]},$$scope:{ctx:x}}}),_t=new C({}),vt=new D({props:{code:`from datasets import load_dataset
dataset = load_dataset('csv', data_files='my_file.csv')`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;csv&#x27;</span>, data_files=<span class="hljs-string">&#x27;my_file.csv&#x27;</span>)`}}),$t=new D({props:{code:"dataset = load_dataset('csv', data_files=['my_file_1.csv', 'my_file_2.csv', 'my_file_3.csv'])",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;csv&#x27;</span>, data_files=[<span class="hljs-string">&#x27;my_file_1.csv&#x27;</span>, <span class="hljs-string">&#x27;my_file_2.csv&#x27;</span>, <span class="hljs-string">&#x27;my_file_3.csv&#x27;</span>])'}}),yt=new D({props:{code:"dataset = load_dataset('csv', data_files={'train': ['my_train_file_1.csv', 'my_train_file_2.csv'] 'test': 'my_test_file.csv'})",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;csv&#x27;</span>, data_files={<span class="hljs-string">&#x27;train&#x27;</span>: [<span class="hljs-string">&#x27;my_train_file_1.csv&#x27;</span>, <span class="hljs-string">&#x27;my_train_file_2.csv&#x27;</span>] <span class="hljs-string">&#x27;test&#x27;</span>: <span class="hljs-string">&#x27;my_test_file.csv&#x27;</span>})'}}),wt=new D({props:{code:`base_url = "https://huggingface.co/datasets/lhoestq/demo1/resolve/main/data/"
dataset = load_dataset('csv', data_files={'train': base_url + 'train.csv', 'test': base_url + 'test.csv'})`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>base_url = <span class="hljs-string">&quot;https://huggingface.co/datasets/lhoestq/demo1/resolve/main/data/&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;csv&#x27;</span>, data_files={<span class="hljs-string">&#x27;train&#x27;</span>: base_url + <span class="hljs-string">&#x27;train.csv&#x27;</span>, <span class="hljs-string">&#x27;test&#x27;</span>: base_url + <span class="hljs-string">&#x27;test.csv&#x27;</span>})`}}),bt=new D({props:{code:`url = "https://domain.org/train_data.zip"
data_files = {"train": url}
dataset = load_dataset("csv", data_files=data_files)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;https://domain.org/train_data.zip&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>data_files = {<span class="hljs-string">&quot;train&quot;</span>: url}
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;csv&quot;</span>, data_files=data_files)`}}),jt=new C({}),xt=new D({props:{code:`from datasets import load_dataset
dataset = load_dataset('json', data_files='my_file.json')`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;json&#x27;</span>, data_files=<span class="hljs-string">&#x27;my_file.json&#x27;</span>)`}}),kt=new D({props:{code:`{"a": 1, "b": 2.0, "c": "foo", "d": false}
{"a": 4, "b": -5.5, "c": null, "d": true}`,highlighted:`{<span class="hljs-string">&quot;a&quot;</span>: <span class="hljs-number">1</span>, <span class="hljs-string">&quot;b&quot;</span>: <span class="hljs-number">2.0</span>, <span class="hljs-string">&quot;c&quot;</span>: <span class="hljs-string">&quot;foo&quot;</span>, <span class="hljs-string">&quot;d&quot;</span>: false}
{<span class="hljs-string">&quot;a&quot;</span>: <span class="hljs-number">4</span>, <span class="hljs-string">&quot;b&quot;</span>: -<span class="hljs-number">5.5</span>, <span class="hljs-string">&quot;c&quot;</span>: null, <span class="hljs-string">&quot;d&quot;</span>: true}`}}),Et=new D({props:{code:`{"version": "0.1.0",
    "data": [{"a": 1, "b": 2.0, "c": "foo", "d": false},
            {"a": 4, "b": -5.5, "c": null, "d": true}]
}

>>> from datasets import load_dataset
>>> dataset = load_dataset('json', data_files='my_file.json', field='data')`,highlighted:`{<span class="hljs-string">&quot;version&quot;</span>: <span class="hljs-string">&quot;0.1.0&quot;</span>,
    <span class="hljs-string">&quot;data&quot;</span>: [{<span class="hljs-string">&quot;a&quot;</span>: <span class="hljs-number">1</span>, <span class="hljs-string">&quot;b&quot;</span>: <span class="hljs-number">2.0</span>, <span class="hljs-string">&quot;c&quot;</span>: <span class="hljs-string">&quot;foo&quot;</span>, <span class="hljs-string">&quot;d&quot;</span>: false},
            {<span class="hljs-string">&quot;a&quot;</span>: <span class="hljs-number">4</span>, <span class="hljs-string">&quot;b&quot;</span>: -<span class="hljs-number">5.5</span>, <span class="hljs-string">&quot;c&quot;</span>: null, <span class="hljs-string">&quot;d&quot;</span>: true}]
}

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;json&#x27;</span>, data_files=<span class="hljs-string">&#x27;my_file.json&#x27;</span>, field=<span class="hljs-string">&#x27;data&#x27;</span>)`}}),qt=new D({props:{code:`base_url = "https://rajpurkar.github.io/SQuAD-explorer/dataset/"
dataset = load_dataset('json', data_files={'train': base_url + 'train-v1.1.json', 'validation': base_url + 'dev-v1.1.json'}, field="data")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>base_url = <span class="hljs-string">&quot;https://rajpurkar.github.io/SQuAD-explorer/dataset/&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;json&#x27;</span>, data_files={<span class="hljs-string">&#x27;train&#x27;</span>: base_url + <span class="hljs-string">&#x27;train-v1.1.json&#x27;</span>, <span class="hljs-string">&#x27;validation&#x27;</span>: base_url + <span class="hljs-string">&#x27;dev-v1.1.json&#x27;</span>}, field=<span class="hljs-string">&quot;data&quot;</span>)`}}),Pt=new C({}),At=new D({props:{code:`from datasets import load_dataset
dataset = load_dataset('text', data_files={'train': ['my_text_1.txt', 'my_text_2.txt'], 'test': 'my_test_file.txt'})`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;text&#x27;</span>, data_files={<span class="hljs-string">&#x27;train&#x27;</span>: [<span class="hljs-string">&#x27;my_text_1.txt&#x27;</span>, <span class="hljs-string">&#x27;my_text_2.txt&#x27;</span>], <span class="hljs-string">&#x27;test&#x27;</span>: <span class="hljs-string">&#x27;my_test_file.txt&#x27;</span>})`}}),St=new D({props:{code:"dataset = load_dataset('text', data_files='https://huggingface.co/datasets/lhoestq/test/resolve/main/some_text.txt')",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;text&#x27;</span>, data_files=<span class="hljs-string">&#x27;https://huggingface.co/datasets/lhoestq/test/resolve/main/some_text.txt&#x27;</span>)'}}),Tt=new C({}),Dt=new D({props:{code:`from datasets import load_dataset
dataset = load_dataset("parquet", data_files={'train': 'train.parquet', 'test': 'test.parquet'})`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;parquet&quot;</span>, data_files={<span class="hljs-string">&#x27;train&#x27;</span>: <span class="hljs-string">&#x27;train.parquet&#x27;</span>, <span class="hljs-string">&#x27;test&#x27;</span>: <span class="hljs-string">&#x27;test.parquet&#x27;</span>})`}}),Nt=new D({props:{code:`base_url = "https://storage.googleapis.com/huggingface-nlp/cache/datasets/wikipedia/20200501.en/1.0.0/"
data_files = {"train": base_url + "wikipedia-train.parquet"}
wiki = load_dataset("parquet", data_files=data_files, split="train")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>base_url = <span class="hljs-string">&quot;https://storage.googleapis.com/huggingface-nlp/cache/datasets/wikipedia/20200501.en/1.0.0/&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>data_files = {<span class="hljs-string">&quot;train&quot;</span>: base_url + <span class="hljs-string">&quot;wikipedia-train.parquet&quot;</span>}
<span class="hljs-meta">&gt;&gt;&gt; </span>wiki = load_dataset(<span class="hljs-string">&quot;parquet&quot;</span>, data_files=data_files, split=<span class="hljs-string">&quot;train&quot;</span>)`}}),It=new C({}),Ct=new C({}),Ot=new D({props:{code:`from datasets import Dataset
my_dict = {"a": [1, 2, 3]}
dataset = Dataset.from_dict(my_dict)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> Dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>my_dict = {<span class="hljs-string">&quot;a&quot;</span>: [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>]}
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = Dataset.from_dict(my_dict)`}}),Ht=new C({}),Ft=new D({props:{code:`from datasets import Dataset
import pandas as pd
df = pd.DataFrame({"a": [1, 2, 3]})
dataset = Dataset.from_pandas(df)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> Dataset
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd
<span class="hljs-meta">&gt;&gt;&gt; </span>df = pd.DataFrame({<span class="hljs-string">&quot;a&quot;</span>: [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>]})
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = Dataset.from_pandas(df)`}}),zs=new ua({props:{warning:!0,$$slots:{default:[Wh]},$$scope:{ctx:x}}}),Lt=new C({}),Rt=new C({}),Mt=new Il({props:{group1:{id:"stringapi",code:"train_test_ds = datasets.load_dataset('bookcorpus', split='train+test')",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>train_test_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=<span class="hljs-string">&#x27;train+test&#x27;</span>)'},group2:{id:"readinstruction",code:`ri = datasets.ReadInstruction('train') + datasets.ReadInstruction('test')
train_test_ds = datasets.load_dataset('bookcorpus', split=ri)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>ri = datasets.ReadInstruction(<span class="hljs-string">&#x27;train&#x27;</span>) + datasets.ReadInstruction(<span class="hljs-string">&#x27;test&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>train_test_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=ri)`}}}),Vt=new Il({props:{group1:{id:"stringapi",code:"train_10_20_ds = datasets.load_dataset('bookcorpus', split='train[10:20]')",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>train_10_20_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=<span class="hljs-string">&#x27;train[10:20]&#x27;</span>)'},group2:{id:"readinstruction",code:"train_10_20_ds = datasets.load_dataset('bookcorpus', split=datasets.ReadInstruction('train', from_=10, to=20, unit='abs'))",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>train_10_20_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=datasets.ReadInstruction(<span class="hljs-string">&#x27;train&#x27;</span>, from_=<span class="hljs-number">10</span>, to=<span class="hljs-number">20</span>, unit=<span class="hljs-string">&#x27;abs&#x27;</span>))'}}}),zt=new Il({props:{group1:{id:"stringapi",code:"train_10pct_ds = datasets.load_dataset('bookcorpus', split='train[:10%]')",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>train_10pct_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=<span class="hljs-string">&#x27;train[:10%]&#x27;</span>)'},group2:{id:"readinstruction",code:"train_10_20_ds = datasets.load_dataset('bookcorpus', split=datasets.ReadInstruction('train', to=10, unit='%'))",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>train_10_20_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=datasets.ReadInstruction(<span class="hljs-string">&#x27;train&#x27;</span>, to=<span class="hljs-number">10</span>, unit=<span class="hljs-string">&#x27;%&#x27;</span>))'}}}),Ut=new Il({props:{group1:{id:"stringapi",code:"train_10_80pct_ds = datasets.load_dataset('bookcorpus', split='train[:10%]+train[-80%:]')",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>train_10_80pct_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=<span class="hljs-string">&#x27;train[:10%]+train[-80%:]&#x27;</span>)'},group2:{id:"readinstruction",code:`ri = (datasets.ReadInstruction('train', to=10, unit='%') + datasets.ReadInstruction('train', from_=-80, unit='%'))
train_10_80pct_ds = datasets.load_dataset('bookcorpus', split=ri)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>ri = (datasets.ReadInstruction(<span class="hljs-string">&#x27;train&#x27;</span>, to=<span class="hljs-number">10</span>, unit=<span class="hljs-string">&#x27;%&#x27;</span>) + datasets.ReadInstruction(<span class="hljs-string">&#x27;train&#x27;</span>, from_=-<span class="hljs-number">80</span>, unit=<span class="hljs-string">&#x27;%&#x27;</span>))
<span class="hljs-meta">&gt;&gt;&gt; </span>train_10_80pct_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=ri)`}}}),Jt=new Il({props:{group1:{id:"stringapi",code:`# 10-fold cross-validation (see also next section on rounding behavior):
# The validation datasets are each going to be 10%:
# [0%:10%], [10%:20%], ..., [90%:100%].
# And the training datasets are each going to be the complementary 90%:
# [10%:100%] (for a corresponding validation set of [0%:10%]),
# [0%:10%] + [20%:100%] (for a validation set of [10%:20%]), ...,
# [0%:90%] (for a validation set of [90%:100%]).
vals_ds = datasets.load_dataset('bookcorpus', split=[f'train[{k}%:{k+10}%]' for k in range(0, 100, 10)])
trains_ds = datasets.load_dataset('bookcorpus', split=[f'train[:{k}%]+train[{k+10}%:]' for k in range(0, 100, 10)])`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># 10-fold cross-validation (see also next section on rounding behavior):</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># The validation datasets are each going to be 10%:</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># [0%:10%], [10%:20%], ..., [90%:100%].</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># And the training datasets are each going to be the complementary 90%:</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># [10%:100%] (for a corresponding validation set of [0%:10%]),</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># [0%:10%] + [20%:100%] (for a validation set of [10%:20%]), ...,</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># [0%:90%] (for a validation set of [90%:100%]).</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>vals_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=[<span class="hljs-string">f&#x27;train[<span class="hljs-subst">{k}</span>%:<span class="hljs-subst">{k+<span class="hljs-number">10</span>}</span>%]&#x27;</span> <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, <span class="hljs-number">100</span>, <span class="hljs-number">10</span>)])
<span class="hljs-meta">&gt;&gt;&gt; </span>trains_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=[<span class="hljs-string">f&#x27;train[:<span class="hljs-subst">{k}</span>%]+train[<span class="hljs-subst">{k+<span class="hljs-number">10</span>}</span>%:]&#x27;</span> <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, <span class="hljs-number">100</span>, <span class="hljs-number">10</span>)])`},group2:{id:"readinstruction",code:`# 10-fold cross-validation (see also next section on rounding behavior):
# The validation datasets are each going to be 10%:
# [0%:10%], [10%:20%], ..., [90%:100%].
# And the training datasets are each going to be the complementary 90%:
# [10%:100%] (for a corresponding validation set of [0%:10%]),
# [0%:10%] + [20%:100%] (for a validation set of [10%:20%]), ...,
# [0%:90%] (for a validation set of [90%:100%]).
vals_ds = datasets.load_dataset('bookcorpus', [datasets.ReadInstruction('train', from_=k, to=k+10, unit='%') for k in range(0, 100, 10)])
trains_ds = datasets.load_dataset('bookcorpus', [(datasets.ReadInstruction('train', to=k, unit='%') + datasets.ReadInstruction('train', from_=k+10, unit='%')) for k in range(0, 100, 10)])`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># 10-fold cross-validation (see also next section on rounding behavior):</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># The validation datasets are each going to be 10%:</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># [0%:10%], [10%:20%], ..., [90%:100%].</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># And the training datasets are each going to be the complementary 90%:</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># [10%:100%] (for a corresponding validation set of [0%:10%]),</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># [0%:10%] + [20%:100%] (for a validation set of [10%:20%]), ...,</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># [0%:90%] (for a validation set of [90%:100%]).</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>vals_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, [datasets.ReadInstruction(<span class="hljs-string">&#x27;train&#x27;</span>, from_=k, to=k+<span class="hljs-number">10</span>, unit=<span class="hljs-string">&#x27;%&#x27;</span>) <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, <span class="hljs-number">100</span>, <span class="hljs-number">10</span>)])
<span class="hljs-meta">&gt;&gt;&gt; </span>trains_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, [(datasets.ReadInstruction(<span class="hljs-string">&#x27;train&#x27;</span>, to=k, unit=<span class="hljs-string">&#x27;%&#x27;</span>) + datasets.ReadInstruction(<span class="hljs-string">&#x27;train&#x27;</span>, from_=k+<span class="hljs-number">10</span>, unit=<span class="hljs-string">&#x27;%&#x27;</span>)) <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, <span class="hljs-number">100</span>, <span class="hljs-number">10</span>)])`}}}),Bt=new C({}),Yt=new D({props:{code:`# Assuming *train* split contains 999 records.
# 19 records, from 500 (included) to 519 (excluded).
>>> train_50_52_ds = datasets.load_dataset('bookcorpus', split='train[50%:52%]')
# 20 records, from 519 (included) to 539 (excluded).
>>> train_52_54_ds = datasets.load_dataset('bookcorpus', split='train[52%:54%]')`,highlighted:`<span class="hljs-comment"># Assuming *train* split contains 999 records.</span>
<span class="hljs-comment"># 19 records, from 500 (included) to 519 (excluded).</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>train_50_52_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=<span class="hljs-string">&#x27;train[50%:52%]&#x27;</span>)
<span class="hljs-comment"># 20 records, from 519 (included) to 539 (excluded).</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>train_52_54_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=<span class="hljs-string">&#x27;train[52%:54%]&#x27;</span>)`}}),Wt=new D({props:{code:`# 18 records, from 450 (included) to 468 (excluded).
>>> train_50_52pct1_ds = datasets.load_dataset('bookcorpus', split=datasets.ReadInstruction( 'train', from_=50, to=52, unit='%', rounding='pct1_dropremainder'))
# 18 records, from 468 (included) to 486 (excluded).
>>> train_52_54pct1_ds = datasets.load_dataset('bookcorpus', split=datasets.ReadInstruction('train',from_=52, to=54, unit='%', rounding='pct1_dropremainder'))
# Or equivalently:
>>> train_50_52pct1_ds = datasets.load_dataset('bookcorpus', split='train[50%:52%](pct1_dropremainder)')
>>> train_52_54pct1_ds = datasets.load_dataset('bookcorpus', split='train[52%:54%](pct1_dropremainder)')`,highlighted:`<span class="hljs-comment"># 18 records, from 450 (included) to 468 (excluded).</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>train_50_52pct1_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=datasets.ReadInstruction( <span class="hljs-string">&#x27;train&#x27;</span>, from_=<span class="hljs-number">50</span>, to=<span class="hljs-number">52</span>, unit=<span class="hljs-string">&#x27;%&#x27;</span>, rounding=<span class="hljs-string">&#x27;pct1_dropremainder&#x27;</span>))
<span class="hljs-comment"># 18 records, from 468 (included) to 486 (excluded).</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>train_52_54pct1_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=datasets.ReadInstruction(<span class="hljs-string">&#x27;train&#x27;</span>,from_=<span class="hljs-number">52</span>, to=<span class="hljs-number">54</span>, unit=<span class="hljs-string">&#x27;%&#x27;</span>, rounding=<span class="hljs-string">&#x27;pct1_dropremainder&#x27;</span>))
<span class="hljs-comment"># Or equivalently:</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>train_50_52pct1_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=<span class="hljs-string">&#x27;train[50%:52%](pct1_dropremainder)&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>train_52_54pct1_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=<span class="hljs-string">&#x27;train[52%:54%](pct1_dropremainder)&#x27;</span>)`}}),Gs=new ua({props:{warning:!0,$$slots:{default:[Gh]},$$scope:{ctx:x}}}),Gt=new C({}),Qt=new C({}),Xt=new D({props:{code:'dataset = load_dataset("matinf", "summarization")',highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;matinf&quot;</span>, <span class="hljs-string">&quot;summarization&quot;</span>)
Downloading <span class="hljs-keyword">and</span> preparing dataset matinf/summarization (download: Unknown size, generated: <span class="hljs-number">246.89</span> MiB, post-processed: Unknown size, total: <span class="hljs-number">246.89</span> MiB) to /root/.cache/huggingface/datasets/matinf/summarization/<span class="hljs-number">1.0</span><span class="hljs-number">.0</span>/82eee5e71c3ceaf20d909bca36ff237452b4e4ab195d3be7ee1c78b53e6f540e...
AssertionError: The dataset matinf <span class="hljs-keyword">with</span> config summarization requires manual data. 
Please follow the manual download instructions: To use MATINF you have to download it manually. Please fill this google form (https://forms.gle/nkH4LVE4iNQeDzsc9). You will receive a download link <span class="hljs-keyword">and</span> a password once you complete the form. Please extract <span class="hljs-built_in">all</span> files <span class="hljs-keyword">in</span> one folder <span class="hljs-keyword">and</span> load the dataset <span class="hljs-keyword">with</span>: *datasets.load_dataset(<span class="hljs-string">&#x27;matinf&#x27;</span>, data_dir=<span class="hljs-string">&#x27;path/to/folder/folder_name&#x27;</span>)*. 
Manual data can be loaded <span class="hljs-keyword">with</span> \`datasets.load_dataset(matinf, data_dir=<span class="hljs-string">&#x27;&lt;path/to/manual/data&gt;&#x27;</span>) `}}),Zt=new C({}),ta=new D({props:{code:`class_names = ["sadness", "joy", "love", "anger", "fear", "surprise"]
emotion_features = Features({'text': Value('string'), 'label': ClassLabel(names=class_names)})`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>class_names = [<span class="hljs-string">&quot;sadness&quot;</span>, <span class="hljs-string">&quot;joy&quot;</span>, <span class="hljs-string">&quot;love&quot;</span>, <span class="hljs-string">&quot;anger&quot;</span>, <span class="hljs-string">&quot;fear&quot;</span>, <span class="hljs-string">&quot;surprise&quot;</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>emotion_features = Features({<span class="hljs-string">&#x27;text&#x27;</span>: Value(<span class="hljs-string">&#x27;string&#x27;</span>), <span class="hljs-string">&#x27;label&#x27;</span>: ClassLabel(names=class_names)})`}}),aa=new D({props:{code:"dataset = load_dataset('csv', data_files=file_dict, delimiter=';', column_names=['text', 'label'], features=emotion_features)",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;csv&#x27;</span>, data_files=file_dict, delimiter=<span class="hljs-string">&#x27;;&#x27;</span>, column_names=[<span class="hljs-string">&#x27;text&#x27;</span>, <span class="hljs-string">&#x27;label&#x27;</span>], features=emotion_features)'}}),ea=new D({props:{code:"dataset['train'].features",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>dataset[<span class="hljs-string">&#x27;train&#x27;</span>].features
{<span class="hljs-string">&#x27;text&#x27;</span>: Value(dtype=<span class="hljs-string">&#x27;string&#x27;</span>, <span class="hljs-built_in">id</span>=<span class="hljs-literal">None</span>),
<span class="hljs-string">&#x27;label&#x27;</span>: ClassLabel(num_classes=<span class="hljs-number">6</span>, names=[<span class="hljs-string">&#x27;sadness&#x27;</span>, <span class="hljs-string">&#x27;joy&#x27;</span>, <span class="hljs-string">&#x27;love&#x27;</span>, <span class="hljs-string">&#x27;anger&#x27;</span>, <span class="hljs-string">&#x27;fear&#x27;</span>, <span class="hljs-string">&#x27;surprise&#x27;</span>], names_file=<span class="hljs-literal">None</span>, <span class="hljs-built_in">id</span>=<span class="hljs-literal">None</span>)}`}}),la=new C({}),oa=new D({props:{code:`from datasets import load_metric
metric = load_metric('PATH/TO/MY/METRIC/SCRIPT')

# Example of typical usage
for batch in dataset:
    inputs, references = batch
    predictions = model(inputs)
    metric.add_batch(predictions=predictions, references=references)
score = metric.compute()`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_metric
<span class="hljs-meta">&gt;&gt;&gt; </span>metric = load_metric(<span class="hljs-string">&#x27;PATH/TO/MY/METRIC/SCRIPT&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Example of typical usage</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">for</span> batch <span class="hljs-keyword">in</span> dataset:
<span class="hljs-meta">... </span>    inputs, references = batch
<span class="hljs-meta">... </span>    predictions = model(inputs)
<span class="hljs-meta">... </span>    metric.add_batch(predictions=predictions, references=references)
<span class="hljs-meta">&gt;&gt;&gt; </span>score = metric.compute()`}}),tt=new ua({props:{$$slots:{default:[Qh]},$$scope:{ctx:x}}}),na=new C({}),ra=new D({props:{code:`from datasets import load_metric
metric = load_metric('bleurt', name='bleurt-base-128')
metric = load_metric('bleurt', name='bleurt-base-512')`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python"><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_metric</span>
<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python">metric = load_metric(<span class="hljs-string">&#x27;bleurt&#x27;</span>, name=<span class="hljs-string">&#x27;bleurt-base-128&#x27;</span>)</span>
<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python">metric = load_metric(<span class="hljs-string">&#x27;bleurt&#x27;</span>, name=<span class="hljs-string">&#x27;bleurt-base-512&#x27;</span>)</span>`}}),ia=new C({}),fa=new D({props:{code:`from datasets import load_metric
metric = load_metric('glue', 'mrpc', num_process=num_process, process_id=rank)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_metric
<span class="hljs-meta">&gt;&gt;&gt; </span>metric = load_metric(<span class="hljs-string">&#x27;glue&#x27;</span>, <span class="hljs-string">&#x27;mrpc&#x27;</span>, num_process=num_process, process_id=rank)`}}),ot=new ua({props:{$$slots:{default:[Kh]},$$scope:{ctx:x}}}),ca=new D({props:{code:`from datasets import load_metric
metric = load_metric('glue', 'mrpc', num_process=num_process, process_id=process_id, experiment_id="My_experiment_10")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_metric
<span class="hljs-meta">&gt;&gt;&gt; </span>metric = load_metric(<span class="hljs-string">&#x27;glue&#x27;</span>, <span class="hljs-string">&#x27;mrpc&#x27;</span>, num_process=num_process, process_id=process_id, experiment_id=<span class="hljs-string">&quot;My_experiment_10&quot;</span>)`}}),{c(){d=l("meta"),g=f(),u=l("h1"),y=l("a"),$=l("span"),E(_.$$.fragment),m=f(),w=l("span"),v=i("Load"),A=f(),S=l("p"),T=i("You have already seen how to load a dataset from the Hugging Face Hub. But datasets are stored in a variety of places, and sometimes you won\u2019t find the one you want on the Hub. A dataset can be on disk on your local machine, in a Github repository, and in in-memory data structures like Python dictionaries and Pandas DataFrames. Wherever your dataset may be stored, \u{1F917} Datasets provides a way for you to load and use it for training."),B=f(),I=l("p"),N=i("This guide will show you how to load a dataset from:"),U=f(),O=l("ul"),J=l("li"),H=i("The Hub without a dataset loading script"),ma=f(),ws=l("li"),ga=i("Local files"),_a=f(),bs=l("li"),jr=i("In-memory data"),xr=f(),ue=l("li"),kr=i("Offline"),Er=f(),me=l("li"),qr=i("A specific slice of a split"),Ol=f(),va=l("p"),Pr=i("You will also learn how to troubleshoot common errors, and how to load specific configurations of a metric."),Hl=f(),$a=l("a"),Fl=f(),as=l("h2"),js=l("a"),ge=l("span"),E(it.$$.fragment),Ar=f(),_e=l("span"),Sr=i("Hugging Face Hub"),Ll=f(),xs=l("p"),Tr=i("In the tutorial, you learned how to load a dataset from the Hub. This method relies on a dataset loading script that downloads and builds the dataset. However, you can also load a dataset from any dataset repository on the Hub "),ve=l("strong"),Dr=i("without"),Nr=i(" a loading script!"),Rl=f(),Y=l("p"),Ir=i("First, create a dataset repository and upload your data files. Then you can use "),ya=l("a"),Cr=i("load_dataset()"),Or=i(" like you learned in the tutorial. For example, load the files from this "),pt=l("a"),Hr=i("demo repository"),Fr=i(" by providing the repository namespace and dataset name:"),Ml=f(),E(dt.$$.fragment),Vl=f(),wa=l("p"),Lr=i("This dataset repository contains CSV files, and this code loads all the data from the CSV files."),zl=f(),ks=l("p"),Rr=i("Some datasets may have more than one version, based on Git tags, branches or commits. Use the "),$e=l("code"),Mr=i("revision"),Vr=i(" flag to specify which dataset version you want to load:"),Ul=f(),E(ft.$$.fragment),Jl=f(),E(Es.$$.fragment),Bl=f(),F=l("p"),zr=i("If the dataset doesn\u2019t have a dataset loading script, then by default, all the data will be loaded in the "),ye=l("code"),Ur=i("train"),Jr=i(" split. Use the "),we=l("code"),Br=i("data_files"),Yr=i(" parameter to map data files to splits like "),be=l("code"),Wr=i("train"),Gr=i(", "),je=l("code"),Qr=i("validation"),Kr=i(" and "),xe=l("code"),Xr=i("test"),Zr=i(":"),Yl=f(),E(ct.$$.fragment),Wl=f(),E(qs.$$.fragment),Gl=f(),W=l("p"),si=i("You can also load a specific subset of the files with the "),ke=l("code"),ti=i("data_files"),ai=i(" parameter. The example below loads files from the "),ht=l("a"),ei=i("C4 dataset"),li=i(":"),Ql=f(),E(ut.$$.fragment),Kl=f(),Ps=l("p"),oi=i("Specify a custom split with the "),Ee=l("code"),ni=i("split"),ri=i(" parameter:"),Xl=f(),E(mt.$$.fragment),Zl=f(),es=l("h2"),As=l("a"),qe=l("span"),E(gt.$$.fragment),ii=f(),Pe=l("span"),pi=i("Local and remote files"),so=f(),L=l("p"),di=i("\u{1F917} Datasets can be loaded from local files stored on your computer, and also from remote files. The datasets are most likely stored as a "),Ae=l("code"),fi=i("csv"),ci=i(", "),Se=l("code"),hi=i("json"),ui=i(", "),Te=l("code"),mi=i("txt"),gi=i(" or "),De=l("code"),_i=i("parquet"),vi=i(" file. The "),ba=l("a"),$i=i("load_dataset()"),yi=i(" method is able to load each of these file types."),to=f(),E(Ss.$$.fragment),ao=f(),ls=l("h3"),Ts=l("a"),Ne=l("span"),E(_t.$$.fragment),wi=f(),Ie=l("span"),bi=i("CSV"),eo=f(),ja=l("p"),ji=i("\u{1F917} Datasets can read a dataset made up of one or several CSV files:"),lo=f(),E(vt.$$.fragment),oo=f(),xa=l("p"),xi=i("If you have more than one CSV file:"),no=f(),E($t.$$.fragment),ro=f(),ka=l("p"),ki=i("You can also map the training and test splits to specific CSV files:"),io=f(),E(yt.$$.fragment),po=f(),Ea=l("p"),Ei=i("To load remote CSV files via HTTP, you can pass the URLs:"),fo=f(),E(wt.$$.fragment),co=f(),qa=l("p"),qi=i("To load zipped CSV files:"),ho=f(),E(bt.$$.fragment),uo=f(),os=l("h3"),Ds=l("a"),Ce=l("span"),E(jt.$$.fragment),Pi=f(),Oe=l("span"),Ai=i("JSON"),mo=f(),Ns=l("p"),Si=i("JSON files are loaded directly with "),Pa=l("a"),Ti=i("load_dataset()"),Di=i(" as shown below:"),go=f(),E(xt.$$.fragment),_o=f(),Aa=l("p"),Ni=i("JSON files can have diverse formats, but we think the most efficient format is to have multiple JSON objects; each line represents an individual row of data. For example:"),vo=f(),E(kt.$$.fragment),$o=f(),Is=l("p"),Ii=i("Another JSON format you may encounter is a nested field, in which case you will need to specify the "),He=l("code"),Ci=i("field"),Oi=i(" argument as shown in the following:"),yo=f(),E(Et.$$.fragment),wo=f(),Sa=l("p"),Hi=i("To load remote JSON files via HTTP, you can pass the URLs:"),bo=f(),E(qt.$$.fragment),jo=f(),Ta=l("p"),Fi=i("While these are the most common JSON formats, you will see other datasets that are formatted differently. \u{1F917} Datasets recognizes these other formats, and will fallback accordingly on the Python JSON loading methods to handle them."),xo=f(),ns=l("h3"),Cs=l("a"),Fe=l("span"),E(Pt.$$.fragment),Li=f(),Le=l("span"),Ri=i("Text files"),ko=f(),Da=l("p"),Mi=i("Text files are one of the most common file types for storing a dataset. \u{1F917} Datasets will read the text file line by line to build the dataset."),Eo=f(),E(At.$$.fragment),qo=f(),Na=l("p"),Vi=i("To load remote TXT files via HTTP, you can pass the URLs:"),Po=f(),E(St.$$.fragment),Ao=f(),rs=l("h3"),Os=l("a"),Re=l("span"),E(Tt.$$.fragment),zi=f(),Me=l("span"),Ui=i("Parquet"),So=f(),Ia=l("p"),Ji=i("Parquet files are stored in a columnar format unlike row-based files like CSV. Large datasets may be stored in a Parquet file because it is more efficient, and faster at returning your query. Load a Parquet file as shown in the following example:"),To=f(),E(Dt.$$.fragment),Do=f(),Ca=l("p"),Bi=i("To load remote parquet files via HTTP, you can pass the URLs:"),No=f(),E(Nt.$$.fragment),Io=f(),is=l("h2"),Hs=l("a"),Ve=l("span"),E(It.$$.fragment),Yi=f(),ze=l("span"),Wi=i("In-memory data"),Co=f(),Fs=l("p"),Gi=i("\u{1F917} Datasets will also allow you to create a "),Oa=l("a"),Qi=i("Dataset"),Ki=i(" directly from in-memory data structures like Python dictionaries and Pandas DataFrames."),Oo=f(),ps=l("h3"),Ls=l("a"),Ue=l("span"),E(Ct.$$.fragment),Xi=f(),Je=l("span"),Zi=i("Python dictionary"),Ho=f(),Rs=l("p"),sp=i("Load Python dictionaries with "),Ha=l("a"),tp=i("Dataset.from_dict()"),ap=i(":"),Fo=f(),E(Ot.$$.fragment),Lo=f(),ds=l("h3"),Ms=l("a"),Be=l("span"),E(Ht.$$.fragment),ep=f(),Ye=l("span"),lp=i("Pandas DataFrame"),Ro=f(),Vs=l("p"),op=i("Load Pandas DataFrames with "),Fa=l("a"),np=i("Dataset.from_pandas()"),rp=i(":"),Mo=f(),E(Ft.$$.fragment),Vo=f(),E(zs.$$.fragment),zo=f(),fs=l("h2"),Us=l("a"),We=l("span"),E(Lt.$$.fragment),ip=f(),Ge=l("span"),pp=i("Offline"),Uo=f(),La=l("p"),dp=i("Even if you don\u2019t have an internet connection, it is still possible to load a dataset. As long as you\u2019ve downloaded a dataset from the Hub or \u{1F917} Datasets GitHub repository before, it should be cached. This means you can reload the dataset from the cache and use it offline."),Jo=f(),G=l("p"),fp=i("If you know you won\u2019t have internet access, you can run \u{1F917} Datasets in full offline mode. This saves time because instead of waiting for the Dataset builder download to time out, \u{1F917} Datasets will look directly in the cache. Set the environment variable "),Qe=l("code"),cp=i("HF_DATASETS_OFFLINE"),hp=i(" to "),Ke=l("code"),up=i("1"),mp=i(" to enable full offline mode."),Bo=f(),cs=l("h2"),Js=l("a"),Xe=l("span"),E(Rt.$$.fragment),gp=f(),Ze=l("span"),_p=i("Slice splits"),Yo=f(),Q=l("p"),vp=i("For even greater control over how to load a split, you can choose to only load specific slices of a split. There are two options for slicing a split: using strings or "),Ra=l("a"),$p=i("ReadInstruction"),yp=i(". Strings are more compact and readable for simple cases, while "),Ma=l("a"),wp=i("ReadInstruction"),bp=i(" is easier to use with variable slicing parameters."),Wo=f(),K=l("p"),jp=i("Concatenate the "),sl=l("code"),xp=i("train"),kp=i(" and "),tl=l("code"),Ep=i("test"),qp=i(" split by:"),Go=f(),E(Mt.$$.fragment),Qo=f(),Bs=l("p"),Pp=i("Select specific rows of the "),al=l("code"),Ap=i("train"),Sp=i(" split:"),Ko=f(),E(Vt.$$.fragment),Xo=f(),Va=l("p"),Tp=i("Or select a percentage of the split with:"),Zo=f(),E(zt.$$.fragment),sn=f(),za=l("p"),Dp=i("You can even select a combination of percentages from each split:"),tn=f(),E(Ut.$$.fragment),an=f(),Ua=l("p"),Np=i("Finally, create cross-validated dataset splits by:"),en=f(),E(Jt.$$.fragment),ln=f(),hs=l("h3"),Ys=l("a"),el=l("span"),E(Bt.$$.fragment),Ip=f(),ll=l("span"),Cp=i("Percent slicing and rounding"),on=f(),Ja=l("p"),Op=i("For datasets where the requested slice boundaries do not divide evenly by 100, the default behavior is to round the boundaries to the nearest integer. As a result, some slices may contain more examples than others as shown in the following example:"),nn=f(),E(Yt.$$.fragment),rn=f(),Ws=l("p"),Hp=i("If you want equal sized splits, use "),ol=l("code"),Fp=i("pct1_dropremainder"),Lp=i(" rounding instead. This will treat the specified percentage boundaries as multiples of 1%."),pn=f(),E(Wt.$$.fragment),dn=f(),E(Gs.$$.fragment),fn=f(),Ba=l("a"),cn=f(),us=l("h2"),Qs=l("a"),nl=l("span"),E(Gt.$$.fragment),Rp=f(),rl=l("span"),Mp=i("Troubleshooting"),hn=f(),Ya=l("p"),Vp=i("Sometimes, you may get unexpected results when you load a dataset. In this section, you will learn how to solve two common issues you may encounter when you load a dataset: manually download a dataset, and specify features of a dataset."),un=f(),ms=l("h3"),Ks=l("a"),il=l("span"),E(Qt.$$.fragment),zp=f(),pl=l("span"),Up=i("Manual download"),mn=f(),M=l("p"),Jp=i("Certain datasets require you to manually download the dataset files due to licensing incompatibility, or if the files are hidden behind a login page. This will cause "),Wa=l("a"),Bp=i("load_dataset()"),Yp=i(" to throw an "),dl=l("code"),Wp=i("AssertionError"),Gp=i(". But \u{1F917} Datasets provides detailed instructions for downloading the missing files. After you have downloaded the files, use the "),fl=l("code"),Qp=i("data_dir"),Kp=i(" argument to specify the path to the files you just downloaded."),gn=f(),Xs=l("p"),Xp=i("For example, if you try to download a configuration from the "),Kt=l("a"),Zp=i("MATINF"),sd=i(" dataset:"),_n=f(),E(Xt.$$.fragment),vn=f(),gs=l("h3"),Zs=l("a"),cl=l("span"),E(Zt.$$.fragment),td=f(),hl=l("span"),ad=i("Specify features"),$n=f(),X=l("p"),ed=i("When you create a dataset from local files, the "),Ga=l("a"),ld=i("Features"),od=i(" are automatically inferred by "),sa=l("a"),nd=i("Apache Arrow"),rd=i(". However, the features of the dataset may not always align with your expectations or you may want to define the features yourself."),yn=f(),Z=l("p"),id=i("The following example shows how you can add custom labels with "),Qa=l("a"),pd=i("ClassLabel"),dd=i(". First, define your own labels using the "),Ka=l("a"),fd=i("Features"),cd=i(" class:"),wn=f(),E(ta.$$.fragment),bn=f(),ss=l("p"),hd=i("Next, specify the "),ul=l("code"),ud=i("features"),md=i(" argument in "),Xa=l("a"),gd=i("load_dataset()"),_d=i(" with the features you just created:"),jn=f(),E(aa.$$.fragment),xn=f(),Za=l("p"),vd=i("Now when you look at your dataset features, you can see it uses the custom labels you defined:"),kn=f(),E(ea.$$.fragment),En=f(),_s=l("h2"),st=l("a"),ml=l("span"),E(la.$$.fragment),$d=f(),gl=l("span"),yd=i("Metrics"),qn=f(),se=l("p"),wd=i("When the metric you want to use is not supported by \u{1F917} Datasets, you can write and use your own metric script. Load your metric by providing the path to your local metric loading script:"),Pn=f(),E(oa.$$.fragment),An=f(),E(tt.$$.fragment),Sn=f(),vs=l("h3"),at=l("a"),_l=l("span"),E(na.$$.fragment),bd=f(),vl=l("span"),jd=i("Load configurations"),Tn=f(),et=l("p"),xd=i("It is possible for a metric to have different configurations. The configurations are stored in the "),$l=l("code"),kd=i("config_name"),Ed=i(" parameter in [\u2018datasets.MetricInfo\u2019] attribute. When you load a metric, provide the configuration name as shown in the following:"),Dn=f(),E(ra.$$.fragment),Nn=f(),$s=l("h3"),lt=l("a"),yl=l("span"),E(ia.$$.fragment),qd=f(),wl=l("span"),Pd=i("Distributed setup"),In=f(),te=l("p"),Ad=i("When you work in a distributed or parallel processing environment, loading and computing a metric can be tricky because these processes are executed in parallel on separate subsets of the data. \u{1F917} Datasets supports distributed usage with a few additional arguments when you load a metric."),Cn=f(),ae=l("p"),Sd=i("For example, imagine you are training and evaluating on eight parallel processes. Here\u2019s how you would load a metric in this distributed setting:"),On=f(),ts=l("ol"),bl=l("li"),pa=l("p"),Td=i("Define the total number of processes with the "),jl=l("code"),Dd=i("num_process"),Nd=i(" argument."),Id=f(),xl=l("li"),ys=l("p"),Cd=i("Set the process "),kl=l("code"),Od=i("rank"),Hd=i(" as an integer between zero and "),El=l("code"),Fd=i("num_process - 1"),Ld=i("."),Rd=f(),ql=l("li"),da=l("p"),Md=i("Load your metric with "),ee=l("a"),Vd=i("load_metric()"),zd=i(" with these arguments:"),Hn=f(),E(fa.$$.fragment),Fn=f(),E(ot.$$.fragment),Ln=f(),nt=l("p"),Ud=i("In some instances, you may be simultaneously running multiple independent distributed evaluations on the same server and files. To avoid any conflicts, it is important to provide an "),Pl=l("code"),Jd=i("experiment_id"),Bd=i(" to distinguish the separate evaluations:"),Rn=f(),E(ca.$$.fragment),this.h()},l(s){const e=Ch('[data-svelte="svelte-1phssyn"]',document.head);d=o(e,"META",{name:!0,content:!0}),e.forEach(t),g=c(s),u=o(s,"H1",{class:!0});var ha=n(u);y=o(ha,"A",{id:!0,class:!0,href:!0});var Al=n(y);$=o(Al,"SPAN",{});var Sl=n($);P(_.$$.fragment,Sl),Sl.forEach(t),Al.forEach(t),m=c(ha),w=o(ha,"SPAN",{});var Tl=n(w);v=p(Tl,"Load"),Tl.forEach(t),ha.forEach(t),A=c(s),S=o(s,"P",{});var Dl=n(S);T=p(Dl,"You have already seen how to load a dataset from the Hugging Face Hub. But datasets are stored in a variety of places, and sometimes you won\u2019t find the one you want on the Hub. A dataset can be on disk on your local machine, in a Github repository, and in in-memory data structures like Python dictionaries and Pandas DataFrames. Wherever your dataset may be stored, \u{1F917} Datasets provides a way for you to load and use it for training."),Dl.forEach(t),B=c(s),I=o(s,"P",{});var Nl=n(I);N=p(Nl,"This guide will show you how to load a dataset from:"),Nl.forEach(t),U=c(s),O=o(s,"UL",{});var R=n(O);J=o(R,"LI",{});var Qd=n(J);H=p(Qd,"The Hub without a dataset loading script"),Qd.forEach(t),ma=c(R),ws=o(R,"LI",{});var Kd=n(ws);ga=p(Kd,"Local files"),Kd.forEach(t),_a=c(R),bs=o(R,"LI",{});var Xd=n(bs);jr=p(Xd,"In-memory data"),Xd.forEach(t),xr=c(R),ue=o(R,"LI",{});var Zd=n(ue);kr=p(Zd,"Offline"),Zd.forEach(t),Er=c(R),me=o(R,"LI",{});var sf=n(me);qr=p(sf,"A specific slice of a split"),sf.forEach(t),R.forEach(t),Ol=c(s),va=o(s,"P",{});var tf=n(va);Pr=p(tf,"You will also learn how to troubleshoot common errors, and how to load specific configurations of a metric."),tf.forEach(t),Hl=c(s),$a=o(s,"A",{id:!0}),n($a).forEach(t),Fl=c(s),as=o(s,"H2",{class:!0});var Vn=n(as);js=o(Vn,"A",{id:!0,class:!0,href:!0});var af=n(js);ge=o(af,"SPAN",{});var ef=n(ge);P(it.$$.fragment,ef),ef.forEach(t),af.forEach(t),Ar=c(Vn),_e=o(Vn,"SPAN",{});var lf=n(_e);Sr=p(lf,"Hugging Face Hub"),lf.forEach(t),Vn.forEach(t),Ll=c(s),xs=o(s,"P",{});var zn=n(xs);Tr=p(zn,"In the tutorial, you learned how to load a dataset from the Hub. This method relies on a dataset loading script that downloads and builds the dataset. However, you can also load a dataset from any dataset repository on the Hub "),ve=o(zn,"STRONG",{});var of=n(ve);Dr=p(of,"without"),of.forEach(t),Nr=p(zn," a loading script!"),zn.forEach(t),Rl=c(s),Y=o(s,"P",{});var le=n(Y);Ir=p(le,"First, create a dataset repository and upload your data files. Then you can use "),ya=o(le,"A",{href:!0});var nf=n(ya);Cr=p(nf,"load_dataset()"),nf.forEach(t),Or=p(le," like you learned in the tutorial. For example, load the files from this "),pt=o(le,"A",{href:!0,rel:!0});var rf=n(pt);Hr=p(rf,"demo repository"),rf.forEach(t),Fr=p(le," by providing the repository namespace and dataset name:"),le.forEach(t),Ml=c(s),P(dt.$$.fragment,s),Vl=c(s),wa=o(s,"P",{});var pf=n(wa);Lr=p(pf,"This dataset repository contains CSV files, and this code loads all the data from the CSV files."),pf.forEach(t),zl=c(s),ks=o(s,"P",{});var Un=n(ks);Rr=p(Un,"Some datasets may have more than one version, based on Git tags, branches or commits. Use the "),$e=o(Un,"CODE",{});var df=n($e);Mr=p(df,"revision"),df.forEach(t),Vr=p(Un," flag to specify which dataset version you want to load:"),Un.forEach(t),Ul=c(s),P(ft.$$.fragment,s),Jl=c(s),P(Es.$$.fragment,s),Bl=c(s),F=o(s,"P",{});var V=n(F);zr=p(V,"If the dataset doesn\u2019t have a dataset loading script, then by default, all the data will be loaded in the "),ye=o(V,"CODE",{});var ff=n(ye);Ur=p(ff,"train"),ff.forEach(t),Jr=p(V," split. Use the "),we=o(V,"CODE",{});var cf=n(we);Br=p(cf,"data_files"),cf.forEach(t),Yr=p(V," parameter to map data files to splits like "),be=o(V,"CODE",{});var hf=n(be);Wr=p(hf,"train"),hf.forEach(t),Gr=p(V,", "),je=o(V,"CODE",{});var uf=n(je);Qr=p(uf,"validation"),uf.forEach(t),Kr=p(V," and "),xe=o(V,"CODE",{});var mf=n(xe);Xr=p(mf,"test"),mf.forEach(t),Zr=p(V,":"),V.forEach(t),Yl=c(s),P(ct.$$.fragment,s),Wl=c(s),P(qs.$$.fragment,s),Gl=c(s),W=o(s,"P",{});var oe=n(W);si=p(oe,"You can also load a specific subset of the files with the "),ke=o(oe,"CODE",{});var gf=n(ke);ti=p(gf,"data_files"),gf.forEach(t),ai=p(oe," parameter. The example below loads files from the "),ht=o(oe,"A",{href:!0,rel:!0});var _f=n(ht);ei=p(_f,"C4 dataset"),_f.forEach(t),li=p(oe,":"),oe.forEach(t),Ql=c(s),P(ut.$$.fragment,s),Kl=c(s),Ps=o(s,"P",{});var Jn=n(Ps);oi=p(Jn,"Specify a custom split with the "),Ee=o(Jn,"CODE",{});var vf=n(Ee);ni=p(vf,"split"),vf.forEach(t),ri=p(Jn," parameter:"),Jn.forEach(t),Xl=c(s),P(mt.$$.fragment,s),Zl=c(s),es=o(s,"H2",{class:!0});var Bn=n(es);As=o(Bn,"A",{id:!0,class:!0,href:!0});var $f=n(As);qe=o($f,"SPAN",{});var yf=n(qe);P(gt.$$.fragment,yf),yf.forEach(t),$f.forEach(t),ii=c(Bn),Pe=o(Bn,"SPAN",{});var wf=n(Pe);pi=p(wf,"Local and remote files"),wf.forEach(t),Bn.forEach(t),so=c(s),L=o(s,"P",{});var z=n(L);di=p(z,"\u{1F917} Datasets can be loaded from local files stored on your computer, and also from remote files. The datasets are most likely stored as a "),Ae=o(z,"CODE",{});var bf=n(Ae);fi=p(bf,"csv"),bf.forEach(t),ci=p(z,", "),Se=o(z,"CODE",{});var jf=n(Se);hi=p(jf,"json"),jf.forEach(t),ui=p(z,", "),Te=o(z,"CODE",{});var xf=n(Te);mi=p(xf,"txt"),xf.forEach(t),gi=p(z," or "),De=o(z,"CODE",{});var kf=n(De);_i=p(kf,"parquet"),kf.forEach(t),vi=p(z," file. The "),ba=o(z,"A",{href:!0});var Ef=n(ba);$i=p(Ef,"load_dataset()"),Ef.forEach(t),yi=p(z," method is able to load each of these file types."),z.forEach(t),to=c(s),P(Ss.$$.fragment,s),ao=c(s),ls=o(s,"H3",{class:!0});var Yn=n(ls);Ts=o(Yn,"A",{id:!0,class:!0,href:!0});var qf=n(Ts);Ne=o(qf,"SPAN",{});var Pf=n(Ne);P(_t.$$.fragment,Pf),Pf.forEach(t),qf.forEach(t),wi=c(Yn),Ie=o(Yn,"SPAN",{});var Af=n(Ie);bi=p(Af,"CSV"),Af.forEach(t),Yn.forEach(t),eo=c(s),ja=o(s,"P",{});var Sf=n(ja);ji=p(Sf,"\u{1F917} Datasets can read a dataset made up of one or several CSV files:"),Sf.forEach(t),lo=c(s),P(vt.$$.fragment,s),oo=c(s),xa=o(s,"P",{});var Tf=n(xa);xi=p(Tf,"If you have more than one CSV file:"),Tf.forEach(t),no=c(s),P($t.$$.fragment,s),ro=c(s),ka=o(s,"P",{});var Df=n(ka);ki=p(Df,"You can also map the training and test splits to specific CSV files:"),Df.forEach(t),io=c(s),P(yt.$$.fragment,s),po=c(s),Ea=o(s,"P",{});var Nf=n(Ea);Ei=p(Nf,"To load remote CSV files via HTTP, you can pass the URLs:"),Nf.forEach(t),fo=c(s),P(wt.$$.fragment,s),co=c(s),qa=o(s,"P",{});var If=n(qa);qi=p(If,"To load zipped CSV files:"),If.forEach(t),ho=c(s),P(bt.$$.fragment,s),uo=c(s),os=o(s,"H3",{class:!0});var Wn=n(os);Ds=o(Wn,"A",{id:!0,class:!0,href:!0});var Cf=n(Ds);Ce=o(Cf,"SPAN",{});var Of=n(Ce);P(jt.$$.fragment,Of),Of.forEach(t),Cf.forEach(t),Pi=c(Wn),Oe=o(Wn,"SPAN",{});var Hf=n(Oe);Ai=p(Hf,"JSON"),Hf.forEach(t),Wn.forEach(t),mo=c(s),Ns=o(s,"P",{});var Gn=n(Ns);Si=p(Gn,"JSON files are loaded directly with "),Pa=o(Gn,"A",{href:!0});var Ff=n(Pa);Ti=p(Ff,"load_dataset()"),Ff.forEach(t),Di=p(Gn," as shown below:"),Gn.forEach(t),go=c(s),P(xt.$$.fragment,s),_o=c(s),Aa=o(s,"P",{});var Lf=n(Aa);Ni=p(Lf,"JSON files can have diverse formats, but we think the most efficient format is to have multiple JSON objects; each line represents an individual row of data. For example:"),Lf.forEach(t),vo=c(s),P(kt.$$.fragment,s),$o=c(s),Is=o(s,"P",{});var Qn=n(Is);Ii=p(Qn,"Another JSON format you may encounter is a nested field, in which case you will need to specify the "),He=o(Qn,"CODE",{});var Rf=n(He);Ci=p(Rf,"field"),Rf.forEach(t),Oi=p(Qn," argument as shown in the following:"),Qn.forEach(t),yo=c(s),P(Et.$$.fragment,s),wo=c(s),Sa=o(s,"P",{});var Mf=n(Sa);Hi=p(Mf,"To load remote JSON files via HTTP, you can pass the URLs:"),Mf.forEach(t),bo=c(s),P(qt.$$.fragment,s),jo=c(s),Ta=o(s,"P",{});var Vf=n(Ta);Fi=p(Vf,"While these are the most common JSON formats, you will see other datasets that are formatted differently. \u{1F917} Datasets recognizes these other formats, and will fallback accordingly on the Python JSON loading methods to handle them."),Vf.forEach(t),xo=c(s),ns=o(s,"H3",{class:!0});var Kn=n(ns);Cs=o(Kn,"A",{id:!0,class:!0,href:!0});var zf=n(Cs);Fe=o(zf,"SPAN",{});var Uf=n(Fe);P(Pt.$$.fragment,Uf),Uf.forEach(t),zf.forEach(t),Li=c(Kn),Le=o(Kn,"SPAN",{});var Jf=n(Le);Ri=p(Jf,"Text files"),Jf.forEach(t),Kn.forEach(t),ko=c(s),Da=o(s,"P",{});var Bf=n(Da);Mi=p(Bf,"Text files are one of the most common file types for storing a dataset. \u{1F917} Datasets will read the text file line by line to build the dataset."),Bf.forEach(t),Eo=c(s),P(At.$$.fragment,s),qo=c(s),Na=o(s,"P",{});var Yf=n(Na);Vi=p(Yf,"To load remote TXT files via HTTP, you can pass the URLs:"),Yf.forEach(t),Po=c(s),P(St.$$.fragment,s),Ao=c(s),rs=o(s,"H3",{class:!0});var Xn=n(rs);Os=o(Xn,"A",{id:!0,class:!0,href:!0});var Wf=n(Os);Re=o(Wf,"SPAN",{});var Gf=n(Re);P(Tt.$$.fragment,Gf),Gf.forEach(t),Wf.forEach(t),zi=c(Xn),Me=o(Xn,"SPAN",{});var Qf=n(Me);Ui=p(Qf,"Parquet"),Qf.forEach(t),Xn.forEach(t),So=c(s),Ia=o(s,"P",{});var Kf=n(Ia);Ji=p(Kf,"Parquet files are stored in a columnar format unlike row-based files like CSV. Large datasets may be stored in a Parquet file because it is more efficient, and faster at returning your query. Load a Parquet file as shown in the following example:"),Kf.forEach(t),To=c(s),P(Dt.$$.fragment,s),Do=c(s),Ca=o(s,"P",{});var Xf=n(Ca);Bi=p(Xf,"To load remote parquet files via HTTP, you can pass the URLs:"),Xf.forEach(t),No=c(s),P(Nt.$$.fragment,s),Io=c(s),is=o(s,"H2",{class:!0});var Zn=n(is);Hs=o(Zn,"A",{id:!0,class:!0,href:!0});var Zf=n(Hs);Ve=o(Zf,"SPAN",{});var sc=n(Ve);P(It.$$.fragment,sc),sc.forEach(t),Zf.forEach(t),Yi=c(Zn),ze=o(Zn,"SPAN",{});var tc=n(ze);Wi=p(tc,"In-memory data"),tc.forEach(t),Zn.forEach(t),Co=c(s),Fs=o(s,"P",{});var sr=n(Fs);Gi=p(sr,"\u{1F917} Datasets will also allow you to create a "),Oa=o(sr,"A",{href:!0});var ac=n(Oa);Qi=p(ac,"Dataset"),ac.forEach(t),Ki=p(sr," directly from in-memory data structures like Python dictionaries and Pandas DataFrames."),sr.forEach(t),Oo=c(s),ps=o(s,"H3",{class:!0});var tr=n(ps);Ls=o(tr,"A",{id:!0,class:!0,href:!0});var ec=n(Ls);Ue=o(ec,"SPAN",{});var lc=n(Ue);P(Ct.$$.fragment,lc),lc.forEach(t),ec.forEach(t),Xi=c(tr),Je=o(tr,"SPAN",{});var oc=n(Je);Zi=p(oc,"Python dictionary"),oc.forEach(t),tr.forEach(t),Ho=c(s),Rs=o(s,"P",{});var ar=n(Rs);sp=p(ar,"Load Python dictionaries with "),Ha=o(ar,"A",{href:!0});var nc=n(Ha);tp=p(nc,"Dataset.from_dict()"),nc.forEach(t),ap=p(ar,":"),ar.forEach(t),Fo=c(s),P(Ot.$$.fragment,s),Lo=c(s),ds=o(s,"H3",{class:!0});var er=n(ds);Ms=o(er,"A",{id:!0,class:!0,href:!0});var rc=n(Ms);Be=o(rc,"SPAN",{});var ic=n(Be);P(Ht.$$.fragment,ic),ic.forEach(t),rc.forEach(t),ep=c(er),Ye=o(er,"SPAN",{});var pc=n(Ye);lp=p(pc,"Pandas DataFrame"),pc.forEach(t),er.forEach(t),Ro=c(s),Vs=o(s,"P",{});var lr=n(Vs);op=p(lr,"Load Pandas DataFrames with "),Fa=o(lr,"A",{href:!0});var dc=n(Fa);np=p(dc,"Dataset.from_pandas()"),dc.forEach(t),rp=p(lr,":"),lr.forEach(t),Mo=c(s),P(Ft.$$.fragment,s),Vo=c(s),P(zs.$$.fragment,s),zo=c(s),fs=o(s,"H2",{class:!0});var or=n(fs);Us=o(or,"A",{id:!0,class:!0,href:!0});var fc=n(Us);We=o(fc,"SPAN",{});var cc=n(We);P(Lt.$$.fragment,cc),cc.forEach(t),fc.forEach(t),ip=c(or),Ge=o(or,"SPAN",{});var hc=n(Ge);pp=p(hc,"Offline"),hc.forEach(t),or.forEach(t),Uo=c(s),La=o(s,"P",{});var uc=n(La);dp=p(uc,"Even if you don\u2019t have an internet connection, it is still possible to load a dataset. As long as you\u2019ve downloaded a dataset from the Hub or \u{1F917} Datasets GitHub repository before, it should be cached. This means you can reload the dataset from the cache and use it offline."),uc.forEach(t),Jo=c(s),G=o(s,"P",{});var ne=n(G);fp=p(ne,"If you know you won\u2019t have internet access, you can run \u{1F917} Datasets in full offline mode. This saves time because instead of waiting for the Dataset builder download to time out, \u{1F917} Datasets will look directly in the cache. Set the environment variable "),Qe=o(ne,"CODE",{});var mc=n(Qe);cp=p(mc,"HF_DATASETS_OFFLINE"),mc.forEach(t),hp=p(ne," to "),Ke=o(ne,"CODE",{});var gc=n(Ke);up=p(gc,"1"),gc.forEach(t),mp=p(ne," to enable full offline mode."),ne.forEach(t),Bo=c(s),cs=o(s,"H2",{class:!0});var nr=n(cs);Js=o(nr,"A",{id:!0,class:!0,href:!0});var _c=n(Js);Xe=o(_c,"SPAN",{});var vc=n(Xe);P(Rt.$$.fragment,vc),vc.forEach(t),_c.forEach(t),gp=c(nr),Ze=o(nr,"SPAN",{});var $c=n(Ze);_p=p($c,"Slice splits"),$c.forEach(t),nr.forEach(t),Yo=c(s),Q=o(s,"P",{});var re=n(Q);vp=p(re,"For even greater control over how to load a split, you can choose to only load specific slices of a split. There are two options for slicing a split: using strings or "),Ra=o(re,"A",{href:!0});var yc=n(Ra);$p=p(yc,"ReadInstruction"),yc.forEach(t),yp=p(re,". Strings are more compact and readable for simple cases, while "),Ma=o(re,"A",{href:!0});var wc=n(Ma);wp=p(wc,"ReadInstruction"),wc.forEach(t),bp=p(re," is easier to use with variable slicing parameters."),re.forEach(t),Wo=c(s),K=o(s,"P",{});var ie=n(K);jp=p(ie,"Concatenate the "),sl=o(ie,"CODE",{});var bc=n(sl);xp=p(bc,"train"),bc.forEach(t),kp=p(ie," and "),tl=o(ie,"CODE",{});var jc=n(tl);Ep=p(jc,"test"),jc.forEach(t),qp=p(ie," split by:"),ie.forEach(t),Go=c(s),P(Mt.$$.fragment,s),Qo=c(s),Bs=o(s,"P",{});var rr=n(Bs);Pp=p(rr,"Select specific rows of the "),al=o(rr,"CODE",{});var xc=n(al);Ap=p(xc,"train"),xc.forEach(t),Sp=p(rr," split:"),rr.forEach(t),Ko=c(s),P(Vt.$$.fragment,s),Xo=c(s),Va=o(s,"P",{});var kc=n(Va);Tp=p(kc,"Or select a percentage of the split with:"),kc.forEach(t),Zo=c(s),P(zt.$$.fragment,s),sn=c(s),za=o(s,"P",{});var Ec=n(za);Dp=p(Ec,"You can even select a combination of percentages from each split:"),Ec.forEach(t),tn=c(s),P(Ut.$$.fragment,s),an=c(s),Ua=o(s,"P",{});var qc=n(Ua);Np=p(qc,"Finally, create cross-validated dataset splits by:"),qc.forEach(t),en=c(s),P(Jt.$$.fragment,s),ln=c(s),hs=o(s,"H3",{class:!0});var ir=n(hs);Ys=o(ir,"A",{id:!0,class:!0,href:!0});var Pc=n(Ys);el=o(Pc,"SPAN",{});var Ac=n(el);P(Bt.$$.fragment,Ac),Ac.forEach(t),Pc.forEach(t),Ip=c(ir),ll=o(ir,"SPAN",{});var Sc=n(ll);Cp=p(Sc,"Percent slicing and rounding"),Sc.forEach(t),ir.forEach(t),on=c(s),Ja=o(s,"P",{});var Tc=n(Ja);Op=p(Tc,"For datasets where the requested slice boundaries do not divide evenly by 100, the default behavior is to round the boundaries to the nearest integer. As a result, some slices may contain more examples than others as shown in the following example:"),Tc.forEach(t),nn=c(s),P(Yt.$$.fragment,s),rn=c(s),Ws=o(s,"P",{});var pr=n(Ws);Hp=p(pr,"If you want equal sized splits, use "),ol=o(pr,"CODE",{});var Dc=n(ol);Fp=p(Dc,"pct1_dropremainder"),Dc.forEach(t),Lp=p(pr," rounding instead. This will treat the specified percentage boundaries as multiples of 1%."),pr.forEach(t),pn=c(s),P(Wt.$$.fragment,s),dn=c(s),P(Gs.$$.fragment,s),fn=c(s),Ba=o(s,"A",{id:!0}),n(Ba).forEach(t),cn=c(s),us=o(s,"H2",{class:!0});var dr=n(us);Qs=o(dr,"A",{id:!0,class:!0,href:!0});var Nc=n(Qs);nl=o(Nc,"SPAN",{});var Ic=n(nl);P(Gt.$$.fragment,Ic),Ic.forEach(t),Nc.forEach(t),Rp=c(dr),rl=o(dr,"SPAN",{});var Cc=n(rl);Mp=p(Cc,"Troubleshooting"),Cc.forEach(t),dr.forEach(t),hn=c(s),Ya=o(s,"P",{});var Oc=n(Ya);Vp=p(Oc,"Sometimes, you may get unexpected results when you load a dataset. In this section, you will learn how to solve two common issues you may encounter when you load a dataset: manually download a dataset, and specify features of a dataset."),Oc.forEach(t),un=c(s),ms=o(s,"H3",{class:!0});var fr=n(ms);Ks=o(fr,"A",{id:!0,class:!0,href:!0});var Hc=n(Ks);il=o(Hc,"SPAN",{});var Fc=n(il);P(Qt.$$.fragment,Fc),Fc.forEach(t),Hc.forEach(t),zp=c(fr),pl=o(fr,"SPAN",{});var Lc=n(pl);Up=p(Lc,"Manual download"),Lc.forEach(t),fr.forEach(t),mn=c(s),M=o(s,"P",{});var rt=n(M);Jp=p(rt,"Certain datasets require you to manually download the dataset files due to licensing incompatibility, or if the files are hidden behind a login page. This will cause "),Wa=o(rt,"A",{href:!0});var Rc=n(Wa);Bp=p(Rc,"load_dataset()"),Rc.forEach(t),Yp=p(rt," to throw an "),dl=o(rt,"CODE",{});var Mc=n(dl);Wp=p(Mc,"AssertionError"),Mc.forEach(t),Gp=p(rt,". But \u{1F917} Datasets provides detailed instructions for downloading the missing files. After you have downloaded the files, use the "),fl=o(rt,"CODE",{});var Vc=n(fl);Qp=p(Vc,"data_dir"),Vc.forEach(t),Kp=p(rt," argument to specify the path to the files you just downloaded."),rt.forEach(t),gn=c(s),Xs=o(s,"P",{});var cr=n(Xs);Xp=p(cr,"For example, if you try to download a configuration from the "),Kt=o(cr,"A",{href:!0,rel:!0});var zc=n(Kt);Zp=p(zc,"MATINF"),zc.forEach(t),sd=p(cr," dataset:"),cr.forEach(t),_n=c(s),P(Xt.$$.fragment,s),vn=c(s),gs=o(s,"H3",{class:!0});var hr=n(gs);Zs=o(hr,"A",{id:!0,class:!0,href:!0});var Uc=n(Zs);cl=o(Uc,"SPAN",{});var Jc=n(cl);P(Zt.$$.fragment,Jc),Jc.forEach(t),Uc.forEach(t),td=c(hr),hl=o(hr,"SPAN",{});var Bc=n(hl);ad=p(Bc,"Specify features"),Bc.forEach(t),hr.forEach(t),$n=c(s),X=o(s,"P",{});var pe=n(X);ed=p(pe,"When you create a dataset from local files, the "),Ga=o(pe,"A",{href:!0});var Yc=n(Ga);ld=p(Yc,"Features"),Yc.forEach(t),od=p(pe," are automatically inferred by "),sa=o(pe,"A",{href:!0,rel:!0});var Wc=n(sa);nd=p(Wc,"Apache Arrow"),Wc.forEach(t),rd=p(pe,". However, the features of the dataset may not always align with your expectations or you may want to define the features yourself."),pe.forEach(t),yn=c(s),Z=o(s,"P",{});var de=n(Z);id=p(de,"The following example shows how you can add custom labels with "),Qa=o(de,"A",{href:!0});var Gc=n(Qa);pd=p(Gc,"ClassLabel"),Gc.forEach(t),dd=p(de,". First, define your own labels using the "),Ka=o(de,"A",{href:!0});var Qc=n(Ka);fd=p(Qc,"Features"),Qc.forEach(t),cd=p(de," class:"),de.forEach(t),wn=c(s),P(ta.$$.fragment,s),bn=c(s),ss=o(s,"P",{});var fe=n(ss);hd=p(fe,"Next, specify the "),ul=o(fe,"CODE",{});var Kc=n(ul);ud=p(Kc,"features"),Kc.forEach(t),md=p(fe," argument in "),Xa=o(fe,"A",{href:!0});var Xc=n(Xa);gd=p(Xc,"load_dataset()"),Xc.forEach(t),_d=p(fe," with the features you just created:"),fe.forEach(t),jn=c(s),P(aa.$$.fragment,s),xn=c(s),Za=o(s,"P",{});var Zc=n(Za);vd=p(Zc,"Now when you look at your dataset features, you can see it uses the custom labels you defined:"),Zc.forEach(t),kn=c(s),P(ea.$$.fragment,s),En=c(s),_s=o(s,"H2",{class:!0});var ur=n(_s);st=o(ur,"A",{id:!0,class:!0,href:!0});var sh=n(st);ml=o(sh,"SPAN",{});var th=n(ml);P(la.$$.fragment,th),th.forEach(t),sh.forEach(t),$d=c(ur),gl=o(ur,"SPAN",{});var ah=n(gl);yd=p(ah,"Metrics"),ah.forEach(t),ur.forEach(t),qn=c(s),se=o(s,"P",{});var eh=n(se);wd=p(eh,"When the metric you want to use is not supported by \u{1F917} Datasets, you can write and use your own metric script. Load your metric by providing the path to your local metric loading script:"),eh.forEach(t),Pn=c(s),P(oa.$$.fragment,s),An=c(s),P(tt.$$.fragment,s),Sn=c(s),vs=o(s,"H3",{class:!0});var mr=n(vs);at=o(mr,"A",{id:!0,class:!0,href:!0});var lh=n(at);_l=o(lh,"SPAN",{});var oh=n(_l);P(na.$$.fragment,oh),oh.forEach(t),lh.forEach(t),bd=c(mr),vl=o(mr,"SPAN",{});var nh=n(vl);jd=p(nh,"Load configurations"),nh.forEach(t),mr.forEach(t),Tn=c(s),et=o(s,"P",{});var gr=n(et);xd=p(gr,"It is possible for a metric to have different configurations. The configurations are stored in the "),$l=o(gr,"CODE",{});var rh=n($l);kd=p(rh,"config_name"),rh.forEach(t),Ed=p(gr," parameter in [\u2018datasets.MetricInfo\u2019] attribute. When you load a metric, provide the configuration name as shown in the following:"),gr.forEach(t),Dn=c(s),P(ra.$$.fragment,s),Nn=c(s),$s=o(s,"H3",{class:!0});var _r=n($s);lt=o(_r,"A",{id:!0,class:!0,href:!0});var ih=n(lt);yl=o(ih,"SPAN",{});var ph=n(yl);P(ia.$$.fragment,ph),ph.forEach(t),ih.forEach(t),qd=c(_r),wl=o(_r,"SPAN",{});var dh=n(wl);Pd=p(dh,"Distributed setup"),dh.forEach(t),_r.forEach(t),In=c(s),te=o(s,"P",{});var fh=n(te);Ad=p(fh,"When you work in a distributed or parallel processing environment, loading and computing a metric can be tricky because these processes are executed in parallel on separate subsets of the data. \u{1F917} Datasets supports distributed usage with a few additional arguments when you load a metric."),fh.forEach(t),Cn=c(s),ae=o(s,"P",{});var ch=n(ae);Sd=p(ch,"For example, imagine you are training and evaluating on eight parallel processes. Here\u2019s how you would load a metric in this distributed setting:"),ch.forEach(t),On=c(s),ts=o(s,"OL",{});var ce=n(ts);bl=o(ce,"LI",{});var hh=n(bl);pa=o(hh,"P",{});var vr=n(pa);Td=p(vr,"Define the total number of processes with the "),jl=o(vr,"CODE",{});var uh=n(jl);Dd=p(uh,"num_process"),uh.forEach(t),Nd=p(vr," argument."),vr.forEach(t),hh.forEach(t),Id=c(ce),xl=o(ce,"LI",{});var mh=n(xl);ys=o(mh,"P",{});var he=n(ys);Cd=p(he,"Set the process "),kl=o(he,"CODE",{});var gh=n(kl);Od=p(gh,"rank"),gh.forEach(t),Hd=p(he," as an integer between zero and "),El=o(he,"CODE",{});var _h=n(El);Fd=p(_h,"num_process - 1"),_h.forEach(t),Ld=p(he,"."),he.forEach(t),mh.forEach(t),Rd=c(ce),ql=o(ce,"LI",{});var vh=n(ql);da=o(vh,"P",{});var $r=n(da);Md=p($r,"Load your metric with "),ee=o($r,"A",{href:!0});var $h=n(ee);Vd=p($h,"load_metric()"),$h.forEach(t),zd=p($r," with these arguments:"),$r.forEach(t),vh.forEach(t),ce.forEach(t),Hn=c(s),P(fa.$$.fragment,s),Fn=c(s),P(ot.$$.fragment,s),Ln=c(s),nt=o(s,"P",{});var yr=n(nt);Ud=p(yr,"In some instances, you may be simultaneously running multiple independent distributed evaluations on the same server and files. To avoid any conflicts, it is important to provide an "),Pl=o(yr,"CODE",{});var yh=n(Pl);Jd=p(yh,"experiment_id"),yh.forEach(t),Bd=p(yr," to distinguish the separate evaluations:"),yr.forEach(t),Rn=c(s),P(ca.$$.fragment,s),this.h()},h(){h(d,"name","hf:doc:metadata"),h(d,"content",JSON.stringify(Zh)),h(y,"id","load"),h(y,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(y,"href","#load"),h(u,"class","relative group"),h($a,"id","load-from-the-hub"),h(js,"id","hugging-face-hub"),h(js,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(js,"href","#hugging-face-hub"),h(as,"class","relative group"),h(ya,"href","/docs/datasets/pr_3999/en/package_reference/loading_methods#datasets.load_dataset"),h(pt,"href","https://huggingface.co/datasets/lhoestq/demo1"),h(pt,"rel","nofollow"),h(ht,"href","https://huggingface.co/datasets/allenai/c4"),h(ht,"rel","nofollow"),h(As,"id","local-and-remote-files"),h(As,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(As,"href","#local-and-remote-files"),h(es,"class","relative group"),h(ba,"href","/docs/datasets/pr_3999/en/package_reference/loading_methods#datasets.load_dataset"),h(Ts,"id","csv"),h(Ts,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(Ts,"href","#csv"),h(ls,"class","relative group"),h(Ds,"id","json"),h(Ds,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(Ds,"href","#json"),h(os,"class","relative group"),h(Pa,"href","/docs/datasets/pr_3999/en/package_reference/loading_methods#datasets.load_dataset"),h(Cs,"id","text-files"),h(Cs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(Cs,"href","#text-files"),h(ns,"class","relative group"),h(Os,"id","parquet"),h(Os,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(Os,"href","#parquet"),h(rs,"class","relative group"),h(Hs,"id","inmemory-data"),h(Hs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(Hs,"href","#inmemory-data"),h(is,"class","relative group"),h(Oa,"href","/docs/datasets/pr_3999/en/package_reference/main_classes#datasets.Dataset"),h(Ls,"id","python-dictionary"),h(Ls,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(Ls,"href","#python-dictionary"),h(ps,"class","relative group"),h(Ha,"href","/docs/datasets/pr_3999/en/package_reference/main_classes#datasets.Dataset.from_dict"),h(Ms,"id","pandas-dataframe"),h(Ms,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(Ms,"href","#pandas-dataframe"),h(ds,"class","relative group"),h(Fa,"href","/docs/datasets/pr_3999/en/package_reference/main_classes#datasets.Dataset.from_pandas"),h(Us,"id","offline"),h(Us,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(Us,"href","#offline"),h(fs,"class","relative group"),h(Js,"id","slice-splits"),h(Js,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(Js,"href","#slice-splits"),h(cs,"class","relative group"),h(Ra,"href","/docs/datasets/pr_3999/en/package_reference/builder_classes#datasets.ReadInstruction"),h(Ma,"href","/docs/datasets/pr_3999/en/package_reference/builder_classes#datasets.ReadInstruction"),h(Ys,"id","percent-slicing-and-rounding"),h(Ys,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(Ys,"href","#percent-slicing-and-rounding"),h(hs,"class","relative group"),h(Ba,"id","troubleshoot"),h(Qs,"id","troubleshooting"),h(Qs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(Qs,"href","#troubleshooting"),h(us,"class","relative group"),h(Ks,"id","manual-download"),h(Ks,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(Ks,"href","#manual-download"),h(ms,"class","relative group"),h(Wa,"href","/docs/datasets/pr_3999/en/package_reference/loading_methods#datasets.load_dataset"),h(Kt,"href","https://huggingface.co/datasets/matinf"),h(Kt,"rel","nofollow"),h(Zs,"id","specify-features"),h(Zs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(Zs,"href","#specify-features"),h(gs,"class","relative group"),h(Ga,"href","/docs/datasets/pr_3999/en/package_reference/main_classes#datasets.Features"),h(sa,"href","https://arrow.apache.org/docs/"),h(sa,"rel","nofollow"),h(Qa,"href","/docs/datasets/pr_3999/en/package_reference/main_classes#datasets.ClassLabel"),h(Ka,"href","/docs/datasets/pr_3999/en/package_reference/main_classes#datasets.Features"),h(Xa,"href","/docs/datasets/pr_3999/en/package_reference/loading_methods#datasets.load_dataset"),h(st,"id","metrics"),h(st,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(st,"href","#metrics"),h(_s,"class","relative group"),h(at,"id","load-configurations"),h(at,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(at,"href","#load-configurations"),h(vs,"class","relative group"),h(lt,"id","distributed-setup"),h(lt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(lt,"href","#distributed-setup"),h($s,"class","relative group"),h(ee,"href","/docs/datasets/pr_3999/en/package_reference/loading_methods#datasets.load_metric")},m(s,e){a(document.head,d),r(s,g,e),r(s,u,e),a(u,y),a(y,$),q(_,$,null),a(u,m),a(u,w),a(w,v),r(s,A,e),r(s,S,e),a(S,T),r(s,B,e),r(s,I,e),a(I,N),r(s,U,e),r(s,O,e),a(O,J),a(J,H),a(O,ma),a(O,ws),a(ws,ga),a(O,_a),a(O,bs),a(bs,jr),a(O,xr),a(O,ue),a(ue,kr),a(O,Er),a(O,me),a(me,qr),r(s,Ol,e),r(s,va,e),a(va,Pr),r(s,Hl,e),r(s,$a,e),r(s,Fl,e),r(s,as,e),a(as,js),a(js,ge),q(it,ge,null),a(as,Ar),a(as,_e),a(_e,Sr),r(s,Ll,e),r(s,xs,e),a(xs,Tr),a(xs,ve),a(ve,Dr),a(xs,Nr),r(s,Rl,e),r(s,Y,e),a(Y,Ir),a(Y,ya),a(ya,Cr),a(Y,Or),a(Y,pt),a(pt,Hr),a(Y,Fr),r(s,Ml,e),q(dt,s,e),r(s,Vl,e),r(s,wa,e),a(wa,Lr),r(s,zl,e),r(s,ks,e),a(ks,Rr),a(ks,$e),a($e,Mr),a(ks,Vr),r(s,Ul,e),q(ft,s,e),r(s,Jl,e),q(Es,s,e),r(s,Bl,e),r(s,F,e),a(F,zr),a(F,ye),a(ye,Ur),a(F,Jr),a(F,we),a(we,Br),a(F,Yr),a(F,be),a(be,Wr),a(F,Gr),a(F,je),a(je,Qr),a(F,Kr),a(F,xe),a(xe,Xr),a(F,Zr),r(s,Yl,e),q(ct,s,e),r(s,Wl,e),q(qs,s,e),r(s,Gl,e),r(s,W,e),a(W,si),a(W,ke),a(ke,ti),a(W,ai),a(W,ht),a(ht,ei),a(W,li),r(s,Ql,e),q(ut,s,e),r(s,Kl,e),r(s,Ps,e),a(Ps,oi),a(Ps,Ee),a(Ee,ni),a(Ps,ri),r(s,Xl,e),q(mt,s,e),r(s,Zl,e),r(s,es,e),a(es,As),a(As,qe),q(gt,qe,null),a(es,ii),a(es,Pe),a(Pe,pi),r(s,so,e),r(s,L,e),a(L,di),a(L,Ae),a(Ae,fi),a(L,ci),a(L,Se),a(Se,hi),a(L,ui),a(L,Te),a(Te,mi),a(L,gi),a(L,De),a(De,_i),a(L,vi),a(L,ba),a(ba,$i),a(L,yi),r(s,to,e),q(Ss,s,e),r(s,ao,e),r(s,ls,e),a(ls,Ts),a(Ts,Ne),q(_t,Ne,null),a(ls,wi),a(ls,Ie),a(Ie,bi),r(s,eo,e),r(s,ja,e),a(ja,ji),r(s,lo,e),q(vt,s,e),r(s,oo,e),r(s,xa,e),a(xa,xi),r(s,no,e),q($t,s,e),r(s,ro,e),r(s,ka,e),a(ka,ki),r(s,io,e),q(yt,s,e),r(s,po,e),r(s,Ea,e),a(Ea,Ei),r(s,fo,e),q(wt,s,e),r(s,co,e),r(s,qa,e),a(qa,qi),r(s,ho,e),q(bt,s,e),r(s,uo,e),r(s,os,e),a(os,Ds),a(Ds,Ce),q(jt,Ce,null),a(os,Pi),a(os,Oe),a(Oe,Ai),r(s,mo,e),r(s,Ns,e),a(Ns,Si),a(Ns,Pa),a(Pa,Ti),a(Ns,Di),r(s,go,e),q(xt,s,e),r(s,_o,e),r(s,Aa,e),a(Aa,Ni),r(s,vo,e),q(kt,s,e),r(s,$o,e),r(s,Is,e),a(Is,Ii),a(Is,He),a(He,Ci),a(Is,Oi),r(s,yo,e),q(Et,s,e),r(s,wo,e),r(s,Sa,e),a(Sa,Hi),r(s,bo,e),q(qt,s,e),r(s,jo,e),r(s,Ta,e),a(Ta,Fi),r(s,xo,e),r(s,ns,e),a(ns,Cs),a(Cs,Fe),q(Pt,Fe,null),a(ns,Li),a(ns,Le),a(Le,Ri),r(s,ko,e),r(s,Da,e),a(Da,Mi),r(s,Eo,e),q(At,s,e),r(s,qo,e),r(s,Na,e),a(Na,Vi),r(s,Po,e),q(St,s,e),r(s,Ao,e),r(s,rs,e),a(rs,Os),a(Os,Re),q(Tt,Re,null),a(rs,zi),a(rs,Me),a(Me,Ui),r(s,So,e),r(s,Ia,e),a(Ia,Ji),r(s,To,e),q(Dt,s,e),r(s,Do,e),r(s,Ca,e),a(Ca,Bi),r(s,No,e),q(Nt,s,e),r(s,Io,e),r(s,is,e),a(is,Hs),a(Hs,Ve),q(It,Ve,null),a(is,Yi),a(is,ze),a(ze,Wi),r(s,Co,e),r(s,Fs,e),a(Fs,Gi),a(Fs,Oa),a(Oa,Qi),a(Fs,Ki),r(s,Oo,e),r(s,ps,e),a(ps,Ls),a(Ls,Ue),q(Ct,Ue,null),a(ps,Xi),a(ps,Je),a(Je,Zi),r(s,Ho,e),r(s,Rs,e),a(Rs,sp),a(Rs,Ha),a(Ha,tp),a(Rs,ap),r(s,Fo,e),q(Ot,s,e),r(s,Lo,e),r(s,ds,e),a(ds,Ms),a(Ms,Be),q(Ht,Be,null),a(ds,ep),a(ds,Ye),a(Ye,lp),r(s,Ro,e),r(s,Vs,e),a(Vs,op),a(Vs,Fa),a(Fa,np),a(Vs,rp),r(s,Mo,e),q(Ft,s,e),r(s,Vo,e),q(zs,s,e),r(s,zo,e),r(s,fs,e),a(fs,Us),a(Us,We),q(Lt,We,null),a(fs,ip),a(fs,Ge),a(Ge,pp),r(s,Uo,e),r(s,La,e),a(La,dp),r(s,Jo,e),r(s,G,e),a(G,fp),a(G,Qe),a(Qe,cp),a(G,hp),a(G,Ke),a(Ke,up),a(G,mp),r(s,Bo,e),r(s,cs,e),a(cs,Js),a(Js,Xe),q(Rt,Xe,null),a(cs,gp),a(cs,Ze),a(Ze,_p),r(s,Yo,e),r(s,Q,e),a(Q,vp),a(Q,Ra),a(Ra,$p),a(Q,yp),a(Q,Ma),a(Ma,wp),a(Q,bp),r(s,Wo,e),r(s,K,e),a(K,jp),a(K,sl),a(sl,xp),a(K,kp),a(K,tl),a(tl,Ep),a(K,qp),r(s,Go,e),q(Mt,s,e),r(s,Qo,e),r(s,Bs,e),a(Bs,Pp),a(Bs,al),a(al,Ap),a(Bs,Sp),r(s,Ko,e),q(Vt,s,e),r(s,Xo,e),r(s,Va,e),a(Va,Tp),r(s,Zo,e),q(zt,s,e),r(s,sn,e),r(s,za,e),a(za,Dp),r(s,tn,e),q(Ut,s,e),r(s,an,e),r(s,Ua,e),a(Ua,Np),r(s,en,e),q(Jt,s,e),r(s,ln,e),r(s,hs,e),a(hs,Ys),a(Ys,el),q(Bt,el,null),a(hs,Ip),a(hs,ll),a(ll,Cp),r(s,on,e),r(s,Ja,e),a(Ja,Op),r(s,nn,e),q(Yt,s,e),r(s,rn,e),r(s,Ws,e),a(Ws,Hp),a(Ws,ol),a(ol,Fp),a(Ws,Lp),r(s,pn,e),q(Wt,s,e),r(s,dn,e),q(Gs,s,e),r(s,fn,e),r(s,Ba,e),r(s,cn,e),r(s,us,e),a(us,Qs),a(Qs,nl),q(Gt,nl,null),a(us,Rp),a(us,rl),a(rl,Mp),r(s,hn,e),r(s,Ya,e),a(Ya,Vp),r(s,un,e),r(s,ms,e),a(ms,Ks),a(Ks,il),q(Qt,il,null),a(ms,zp),a(ms,pl),a(pl,Up),r(s,mn,e),r(s,M,e),a(M,Jp),a(M,Wa),a(Wa,Bp),a(M,Yp),a(M,dl),a(dl,Wp),a(M,Gp),a(M,fl),a(fl,Qp),a(M,Kp),r(s,gn,e),r(s,Xs,e),a(Xs,Xp),a(Xs,Kt),a(Kt,Zp),a(Xs,sd),r(s,_n,e),q(Xt,s,e),r(s,vn,e),r(s,gs,e),a(gs,Zs),a(Zs,cl),q(Zt,cl,null),a(gs,td),a(gs,hl),a(hl,ad),r(s,$n,e),r(s,X,e),a(X,ed),a(X,Ga),a(Ga,ld),a(X,od),a(X,sa),a(sa,nd),a(X,rd),r(s,yn,e),r(s,Z,e),a(Z,id),a(Z,Qa),a(Qa,pd),a(Z,dd),a(Z,Ka),a(Ka,fd),a(Z,cd),r(s,wn,e),q(ta,s,e),r(s,bn,e),r(s,ss,e),a(ss,hd),a(ss,ul),a(ul,ud),a(ss,md),a(ss,Xa),a(Xa,gd),a(ss,_d),r(s,jn,e),q(aa,s,e),r(s,xn,e),r(s,Za,e),a(Za,vd),r(s,kn,e),q(ea,s,e),r(s,En,e),r(s,_s,e),a(_s,st),a(st,ml),q(la,ml,null),a(_s,$d),a(_s,gl),a(gl,yd),r(s,qn,e),r(s,se,e),a(se,wd),r(s,Pn,e),q(oa,s,e),r(s,An,e),q(tt,s,e),r(s,Sn,e),r(s,vs,e),a(vs,at),a(at,_l),q(na,_l,null),a(vs,bd),a(vs,vl),a(vl,jd),r(s,Tn,e),r(s,et,e),a(et,xd),a(et,$l),a($l,kd),a(et,Ed),r(s,Dn,e),q(ra,s,e),r(s,Nn,e),r(s,$s,e),a($s,lt),a(lt,yl),q(ia,yl,null),a($s,qd),a($s,wl),a(wl,Pd),r(s,In,e),r(s,te,e),a(te,Ad),r(s,Cn,e),r(s,ae,e),a(ae,Sd),r(s,On,e),r(s,ts,e),a(ts,bl),a(bl,pa),a(pa,Td),a(pa,jl),a(jl,Dd),a(pa,Nd),a(ts,Id),a(ts,xl),a(xl,ys),a(ys,Cd),a(ys,kl),a(kl,Od),a(ys,Hd),a(ys,El),a(El,Fd),a(ys,Ld),a(ts,Rd),a(ts,ql),a(ql,da),a(da,Md),a(da,ee),a(ee,Vd),a(da,zd),r(s,Hn,e),q(fa,s,e),r(s,Fn,e),q(ot,s,e),r(s,Ln,e),r(s,nt,e),a(nt,Ud),a(nt,Pl),a(Pl,Jd),a(nt,Bd),r(s,Rn,e),q(ca,s,e),Mn=!0},p(s,[e]){const ha={};e&2&&(ha.$$scope={dirty:e,ctx:s}),Es.$set(ha);const Al={};e&2&&(Al.$$scope={dirty:e,ctx:s}),qs.$set(Al);const Sl={};e&2&&(Sl.$$scope={dirty:e,ctx:s}),Ss.$set(Sl);const Tl={};e&2&&(Tl.$$scope={dirty:e,ctx:s}),zs.$set(Tl);const Dl={};e&2&&(Dl.$$scope={dirty:e,ctx:s}),Gs.$set(Dl);const Nl={};e&2&&(Nl.$$scope={dirty:e,ctx:s}),tt.$set(Nl);const R={};e&2&&(R.$$scope={dirty:e,ctx:s}),ot.$set(R)},i(s){Mn||(b(_.$$.fragment,s),b(it.$$.fragment,s),b(dt.$$.fragment,s),b(ft.$$.fragment,s),b(Es.$$.fragment,s),b(ct.$$.fragment,s),b(qs.$$.fragment,s),b(ut.$$.fragment,s),b(mt.$$.fragment,s),b(gt.$$.fragment,s),b(Ss.$$.fragment,s),b(_t.$$.fragment,s),b(vt.$$.fragment,s),b($t.$$.fragment,s),b(yt.$$.fragment,s),b(wt.$$.fragment,s),b(bt.$$.fragment,s),b(jt.$$.fragment,s),b(xt.$$.fragment,s),b(kt.$$.fragment,s),b(Et.$$.fragment,s),b(qt.$$.fragment,s),b(Pt.$$.fragment,s),b(At.$$.fragment,s),b(St.$$.fragment,s),b(Tt.$$.fragment,s),b(Dt.$$.fragment,s),b(Nt.$$.fragment,s),b(It.$$.fragment,s),b(Ct.$$.fragment,s),b(Ot.$$.fragment,s),b(Ht.$$.fragment,s),b(Ft.$$.fragment,s),b(zs.$$.fragment,s),b(Lt.$$.fragment,s),b(Rt.$$.fragment,s),b(Mt.$$.fragment,s),b(Vt.$$.fragment,s),b(zt.$$.fragment,s),b(Ut.$$.fragment,s),b(Jt.$$.fragment,s),b(Bt.$$.fragment,s),b(Yt.$$.fragment,s),b(Wt.$$.fragment,s),b(Gs.$$.fragment,s),b(Gt.$$.fragment,s),b(Qt.$$.fragment,s),b(Xt.$$.fragment,s),b(Zt.$$.fragment,s),b(ta.$$.fragment,s),b(aa.$$.fragment,s),b(ea.$$.fragment,s),b(la.$$.fragment,s),b(oa.$$.fragment,s),b(tt.$$.fragment,s),b(na.$$.fragment,s),b(ra.$$.fragment,s),b(ia.$$.fragment,s),b(fa.$$.fragment,s),b(ot.$$.fragment,s),b(ca.$$.fragment,s),Mn=!0)},o(s){j(_.$$.fragment,s),j(it.$$.fragment,s),j(dt.$$.fragment,s),j(ft.$$.fragment,s),j(Es.$$.fragment,s),j(ct.$$.fragment,s),j(qs.$$.fragment,s),j(ut.$$.fragment,s),j(mt.$$.fragment,s),j(gt.$$.fragment,s),j(Ss.$$.fragment,s),j(_t.$$.fragment,s),j(vt.$$.fragment,s),j($t.$$.fragment,s),j(yt.$$.fragment,s),j(wt.$$.fragment,s),j(bt.$$.fragment,s),j(jt.$$.fragment,s),j(xt.$$.fragment,s),j(kt.$$.fragment,s),j(Et.$$.fragment,s),j(qt.$$.fragment,s),j(Pt.$$.fragment,s),j(At.$$.fragment,s),j(St.$$.fragment,s),j(Tt.$$.fragment,s),j(Dt.$$.fragment,s),j(Nt.$$.fragment,s),j(It.$$.fragment,s),j(Ct.$$.fragment,s),j(Ot.$$.fragment,s),j(Ht.$$.fragment,s),j(Ft.$$.fragment,s),j(zs.$$.fragment,s),j(Lt.$$.fragment,s),j(Rt.$$.fragment,s),j(Mt.$$.fragment,s),j(Vt.$$.fragment,s),j(zt.$$.fragment,s),j(Ut.$$.fragment,s),j(Jt.$$.fragment,s),j(Bt.$$.fragment,s),j(Yt.$$.fragment,s),j(Wt.$$.fragment,s),j(Gs.$$.fragment,s),j(Gt.$$.fragment,s),j(Qt.$$.fragment,s),j(Xt.$$.fragment,s),j(Zt.$$.fragment,s),j(ta.$$.fragment,s),j(aa.$$.fragment,s),j(ea.$$.fragment,s),j(la.$$.fragment,s),j(oa.$$.fragment,s),j(tt.$$.fragment,s),j(na.$$.fragment,s),j(ra.$$.fragment,s),j(ia.$$.fragment,s),j(fa.$$.fragment,s),j(ot.$$.fragment,s),j(ca.$$.fragment,s),Mn=!1},d(s){t(d),s&&t(g),s&&t(u),k(_),s&&t(A),s&&t(S),s&&t(B),s&&t(I),s&&t(U),s&&t(O),s&&t(Ol),s&&t(va),s&&t(Hl),s&&t($a),s&&t(Fl),s&&t(as),k(it),s&&t(Ll),s&&t(xs),s&&t(Rl),s&&t(Y),s&&t(Ml),k(dt,s),s&&t(Vl),s&&t(wa),s&&t(zl),s&&t(ks),s&&t(Ul),k(ft,s),s&&t(Jl),k(Es,s),s&&t(Bl),s&&t(F),s&&t(Yl),k(ct,s),s&&t(Wl),k(qs,s),s&&t(Gl),s&&t(W),s&&t(Ql),k(ut,s),s&&t(Kl),s&&t(Ps),s&&t(Xl),k(mt,s),s&&t(Zl),s&&t(es),k(gt),s&&t(so),s&&t(L),s&&t(to),k(Ss,s),s&&t(ao),s&&t(ls),k(_t),s&&t(eo),s&&t(ja),s&&t(lo),k(vt,s),s&&t(oo),s&&t(xa),s&&t(no),k($t,s),s&&t(ro),s&&t(ka),s&&t(io),k(yt,s),s&&t(po),s&&t(Ea),s&&t(fo),k(wt,s),s&&t(co),s&&t(qa),s&&t(ho),k(bt,s),s&&t(uo),s&&t(os),k(jt),s&&t(mo),s&&t(Ns),s&&t(go),k(xt,s),s&&t(_o),s&&t(Aa),s&&t(vo),k(kt,s),s&&t($o),s&&t(Is),s&&t(yo),k(Et,s),s&&t(wo),s&&t(Sa),s&&t(bo),k(qt,s),s&&t(jo),s&&t(Ta),s&&t(xo),s&&t(ns),k(Pt),s&&t(ko),s&&t(Da),s&&t(Eo),k(At,s),s&&t(qo),s&&t(Na),s&&t(Po),k(St,s),s&&t(Ao),s&&t(rs),k(Tt),s&&t(So),s&&t(Ia),s&&t(To),k(Dt,s),s&&t(Do),s&&t(Ca),s&&t(No),k(Nt,s),s&&t(Io),s&&t(is),k(It),s&&t(Co),s&&t(Fs),s&&t(Oo),s&&t(ps),k(Ct),s&&t(Ho),s&&t(Rs),s&&t(Fo),k(Ot,s),s&&t(Lo),s&&t(ds),k(Ht),s&&t(Ro),s&&t(Vs),s&&t(Mo),k(Ft,s),s&&t(Vo),k(zs,s),s&&t(zo),s&&t(fs),k(Lt),s&&t(Uo),s&&t(La),s&&t(Jo),s&&t(G),s&&t(Bo),s&&t(cs),k(Rt),s&&t(Yo),s&&t(Q),s&&t(Wo),s&&t(K),s&&t(Go),k(Mt,s),s&&t(Qo),s&&t(Bs),s&&t(Ko),k(Vt,s),s&&t(Xo),s&&t(Va),s&&t(Zo),k(zt,s),s&&t(sn),s&&t(za),s&&t(tn),k(Ut,s),s&&t(an),s&&t(Ua),s&&t(en),k(Jt,s),s&&t(ln),s&&t(hs),k(Bt),s&&t(on),s&&t(Ja),s&&t(nn),k(Yt,s),s&&t(rn),s&&t(Ws),s&&t(pn),k(Wt,s),s&&t(dn),k(Gs,s),s&&t(fn),s&&t(Ba),s&&t(cn),s&&t(us),k(Gt),s&&t(hn),s&&t(Ya),s&&t(un),s&&t(ms),k(Qt),s&&t(mn),s&&t(M),s&&t(gn),s&&t(Xs),s&&t(_n),k(Xt,s),s&&t(vn),s&&t(gs),k(Zt),s&&t($n),s&&t(X),s&&t(yn),s&&t(Z),s&&t(wn),k(ta,s),s&&t(bn),s&&t(ss),s&&t(jn),k(aa,s),s&&t(xn),s&&t(Za),s&&t(kn),k(ea,s),s&&t(En),s&&t(_s),k(la),s&&t(qn),s&&t(se),s&&t(Pn),k(oa,s),s&&t(An),k(tt,s),s&&t(Sn),s&&t(vs),k(na),s&&t(Tn),s&&t(et),s&&t(Dn),k(ra,s),s&&t(Nn),s&&t($s),k(ia),s&&t(In),s&&t(te),s&&t(Cn),s&&t(ae),s&&t(On),s&&t(ts),s&&t(Hn),k(fa,s),s&&t(Fn),k(ot,s),s&&t(Ln),s&&t(nt),s&&t(Rn),k(ca,s)}}}const Zh={local:"load",sections:[{local:"hugging-face-hub",title:"Hugging Face Hub"},{local:"local-and-remote-files",sections:[{local:"csv",title:"CSV"},{local:"json",title:"JSON"},{local:"text-files",title:"Text files"},{local:"parquet",title:"Parquet"}],title:"Local and remote files"},{local:"inmemory-data",sections:[{local:"python-dictionary",title:"Python dictionary"},{local:"pandas-dataframe",title:"Pandas DataFrame"}],title:"In-memory data"},{local:"offline",title:"Offline"},{local:"slice-splits",sections:[{local:"percent-slicing-and-rounding",title:"Percent slicing and rounding"}],title:"Slice splits"},{local:"troubleshooting",sections:[{local:"manual-download",title:"Manual download"},{local:"specify-features",title:"Specify features"}],title:"Troubleshooting"},{local:"metrics",sections:[{local:"load-configurations",title:"Load configurations"},{local:"distributed-setup",title:"Distributed setup"}],title:"Metrics"}],title:"Load"};function su(x){return Oh(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class nu extends Yd{constructor(d){super();Wd(this,d,su,Xh,Gd,{})}}export{nu as default,Zh as metadata};
