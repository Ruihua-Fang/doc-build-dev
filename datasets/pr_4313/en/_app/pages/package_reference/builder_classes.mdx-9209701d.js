import{S as Oi,i as Vi,s as qi,e as n,k as d,w as v,t as c,M as Fi,c as r,d as a,m as i,a as o,x as b,h as m,b as y,F as e,g as $,y as x,q as w,o as E,B as D,v as Mi,L as C}from"../../chunks/vendor-8138ceec.js";import{D as j}from"../../chunks/Docstring-6fa3bd37.js";import{C as P}from"../../chunks/CodeBlock-fc89709f.js";import{I as zi}from"../../chunks/IconCopyLink-2dd3a6ac.js";import{E as R}from"../../chunks/ExampleCodeBlock-25dbadc2.js";function Ui(T){let p,_,g,l,u;return l=new P({props:{code:`from datasets import load_dataset_builder
builder = load_dataset_builder('rotten_tomatoes')
ds = builder.as_dataset(split='train')
ds`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset_builder
<span class="hljs-meta">&gt;&gt;&gt; </span>builder = load_dataset_builder(<span class="hljs-string">&#x27;rotten_tomatoes&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>ds = builder.as_dataset(split=<span class="hljs-string">&#x27;train&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>ds
Dataset({
    features: [<span class="hljs-string">&#x27;text&#x27;</span>, <span class="hljs-string">&#x27;label&#x27;</span>],
    num_rows: <span class="hljs-number">8530</span>
})`}}),{c(){p=n("p"),_=c("Example:"),g=d(),v(l.$$.fragment)},l(t){p=r(t,"P",{});var f=o(p);_=m(f,"Example:"),f.forEach(a),g=i(t),b(l.$$.fragment,t)},m(t,f){$(t,p,f),e(p,_),$(t,g,f),x(l,t,f),u=!0},p:C,i(t){u||(w(l.$$.fragment,t),u=!0)},o(t){E(l.$$.fragment,t),u=!1},d(t){t&&a(p),t&&a(g),D(l,t)}}}function Gi(T){let p,_,g,l,u;return l=new P({props:{code:`from datasets import load_dataset_builder
ds_builder = load_dataset_builder('rotten_tomatoes')
ds_builder.get_all_exported_dataset_infos()
`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset_builder
<span class="hljs-meta">&gt;&gt;&gt; </span>ds_builder = load_dataset_builder(<span class="hljs-string">&#x27;rotten_tomatoes&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>ds_builder.get_all_exported_dataset_infos()
{<span class="hljs-string">&#x27;default&#x27;</span>: DatasetInfo(description=<span class="hljs-string">&quot;Movie Review Dataset.
a dataset of containing 5,331 positive and 5,331 negative processed
s from Rotten Tomatoes movie reviews. This data was first used in Bo
 Lillian Lee, \`\`Seeing stars: Exploiting class relationships for
t categorization with respect to rating scales.&#x27;&#x27;, Proceedings of the
5.
ion=&#x27;@InProceedings{Pang+Lee:05a,
 =       {Bo Pang and Lillian Lee},
=        {Seeing stars: Exploiting class relationships for sentiment
          categorization with respect to rating scales},
tle =    {Proceedings of the ACL},
         2005

age=&#x27;http://www.cs.cornell.edu/people/pabo/movie-review-data/&#x27;, license=&#x27;&#x27;, features={&#x27;text&#x27;: Value(dtype=&#x27;string&#x27;, id=None), &#x27;label&#x27;: ClassLabel(num_classes=2, names=[&#x27;neg&#x27;, &#x27;pos&#x27;], id=None)}, post_processed=None, supervised_keys=SupervisedKeysData(input=&#x27;&#x27;, output=&#x27;&#x27;), task_templates=[TextClassification(task=&#x27;text-classification&#x27;, text_column=&#x27;text&#x27;, label_column=&#x27;label&#x27;)], builder_name=&#x27;rotten_tomatoes_movie_review&#x27;, config_name=&#x27;default&#x27;, version=1.0.0, splits={&#x27;train&#x27;: SplitInfo(name=&#x27;train&#x27;, num_bytes=1074810, num_examples=8530, dataset_name=&#x27;rotten_tomatoes_movie_review&#x27;), &#x27;validation&#x27;: SplitInfo(name=&#x27;validation&#x27;, num_bytes=134679, num_examples=1066, dataset_name=&#x27;rotten_tomatoes_movie_review&#x27;), &#x27;test&#x27;: SplitInfo(name=&#x27;test&#x27;, num_bytes=135972, num_examples=1066, dataset_name=&#x27;rotten_tomatoes_movie_review&#x27;)}, download_checksums={&#x27;https://storage.googleapis.com/seldon-datasets/sentence_polarity_v1/rt-polaritydata.tar.gz&#x27;: {&#x27;num_bytes&#x27;: 487770, &#x27;checksum&#x27;: &#x27;a05befe52aafda71d458d188a1c54506a998b1308613ba76bbda2e5029409ce9&#x27;}}, download_size=487770, post_processing_size=None, dataset_size=1345461, size_in_bytes=1833231)}</span>`}}),{c(){p=n("p"),_=c("Example:"),g=d(),v(l.$$.fragment)},l(t){p=r(t,"P",{});var f=o(p);_=m(f,"Example:"),f.forEach(a),g=i(t),b(l.$$.fragment,t)},m(t,f){$(t,p,f),e(p,_),$(t,g,f),x(l,t,f),u=!0},p:C,i(t){u||(w(l.$$.fragment,t),u=!0)},o(t){E(l.$$.fragment,t),u=!1},d(t){t&&a(p),t&&a(g),D(l,t)}}}function Hi(T){let p,_,g,l,u;return l=new P({props:{code:`from datasets import load_dataset_builder
ds_builder = load_dataset_builder('rotten_tomatoes')
ds_builder.get_exported_dataset_info()
`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset_builder
<span class="hljs-meta">&gt;&gt;&gt; </span>ds_builder = load_dataset_builder(<span class="hljs-string">&#x27;rotten_tomatoes&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>ds_builder.get_exported_dataset_info()
DatasetInfo(description=<span class="hljs-string">&quot;Movie Review Dataset.
a dataset of containing 5,331 positive and 5,331 negative processed
s from Rotten Tomatoes movie reviews. This data was first used in Bo
 Lillian Lee, \`\`Seeing stars: Exploiting class relationships for
t categorization with respect to rating scales.&#x27;&#x27;, Proceedings of the
5.
ion=&#x27;@InProceedings{Pang+Lee:05a,
 =       {Bo Pang and Lillian Lee},
=        {Seeing stars: Exploiting class relationships for sentiment
          categorization with respect to rating scales},
tle =    {Proceedings of the ACL},
         2005

age=&#x27;http://www.cs.cornell.edu/people/pabo/movie-review-data/&#x27;, license=&#x27;&#x27;, features={&#x27;text&#x27;: Value(dtype=&#x27;string&#x27;, id=None), &#x27;label&#x27;: ClassLabel(num_classes=2, names=[&#x27;neg&#x27;, &#x27;pos&#x27;], id=None)}, post_processed=None, supervised_keys=SupervisedKeysData(input=&#x27;&#x27;, output=&#x27;&#x27;), task_templates=[TextClassification(task=&#x27;text-classification&#x27;, text_column=&#x27;text&#x27;, label_column=&#x27;label&#x27;)], builder_name=&#x27;rotten_tomatoes_movie_review&#x27;, config_name=&#x27;default&#x27;, version=1.0.0, splits={&#x27;train&#x27;: SplitInfo(name=&#x27;train&#x27;, num_bytes=1074810, num_examples=8530, dataset_name=&#x27;rotten_tomatoes_movie_review&#x27;), &#x27;validation&#x27;: SplitInfo(name=&#x27;validation&#x27;, num_bytes=134679, num_examples=1066, dataset_name=&#x27;rotten_tomatoes_movie_review&#x27;), &#x27;test&#x27;: SplitInfo(name=&#x27;test&#x27;, num_bytes=135972, num_examples=1066, dataset_name=&#x27;rotten_tomatoes_movie_review&#x27;)}, download_checksums={&#x27;https://storage.googleapis.com/seldon-datasets/sentence_polarity_v1/rt-polaritydata.tar.gz&#x27;: {&#x27;num_bytes&#x27;: 487770, &#x27;checksum&#x27;: &#x27;a05befe52aafda71d458d188a1c54506a998b1308613ba76bbda2e5029409ce9&#x27;}}, download_size=487770, post_processing_size=None, dataset_size=1345461, size_in_bytes=1833231)</span>`}}),{c(){p=n("p"),_=c("Example:"),g=d(),v(l.$$.fragment)},l(t){p=r(t,"P",{});var f=o(p);_=m(f,"Example:"),f.forEach(a),g=i(t),b(l.$$.fragment,t)},m(t,f){$(t,p,f),e(p,_),$(t,g,f),x(l,t,f),u=!0},p:C,i(t){u||(w(l.$$.fragment,t),u=!0)},o(t){E(l.$$.fragment,t),u=!1},d(t){t&&a(p),t&&a(g),D(l,t)}}}function Wi(T){let p,_,g,l,u;return l=new P({props:{code:"downloaded_files = dl_manager.download('https://storage.googleapis.com/seldon-datasets/sentence_polarity_v1/rt-polaritydata.tar.gz')",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>downloaded_files = dl_manager.download(<span class="hljs-string">&#x27;https://storage.googleapis.com/seldon-datasets/sentence_polarity_v1/rt-polaritydata.tar.gz&#x27;</span>)'}}),{c(){p=n("p"),_=c("Example:"),g=d(),v(l.$$.fragment)},l(t){p=r(t,"P",{});var f=o(p);_=m(f,"Example:"),f.forEach(a),g=i(t),b(l.$$.fragment,t)},m(t,f){$(t,p,f),e(p,_),$(t,g,f),x(l,t,f),u=!0},p:C,i(t){u||(w(l.$$.fragment,t),u=!0)},o(t){E(l.$$.fragment,t),u=!1},d(t){t&&a(p),t&&a(g),D(l,t)}}}function Xi(T){let p,_,g,l,u;return l=new P({props:{code:"extracted_paths = dl_manager.extract(dl_manager.download(url_or_urls))",highlighted:'<span class="hljs-attr">extracted_paths</span> = dl_manager.extract(dl_manager.download(url_or_urls))'}}),{c(){p=n("p"),_=c("Is roughly equivalent to:"),g=d(),v(l.$$.fragment)},l(t){p=r(t,"P",{});var f=o(p);_=m(f,"Is roughly equivalent to:"),f.forEach(a),g=i(t),b(l.$$.fragment,t)},m(t,f){$(t,p,f),e(p,_),$(t,g,f),x(l,t,f),u=!0},p:C,i(t){u||(w(l.$$.fragment,t),u=!0)},o(t){E(l.$$.fragment,t),u=!1},d(t){t&&a(p),t&&a(g),D(l,t)}}}function Ji(T){let p,_,g,l,u;return l=new P({props:{code:"downloaded_files = dl_manager.download_custom('https://storage.googleapis.com/seldon-datasets/sentence_polarity_v1/rt-polaritydata.tar.gz')",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>downloaded_files = dl_manager.download_custom(<span class="hljs-string">&#x27;https://storage.googleapis.com/seldon-datasets/sentence_polarity_v1/rt-polaritydata.tar.gz&#x27;</span>)'}}),{c(){p=n("p"),_=c("Example:"),g=d(),v(l.$$.fragment)},l(t){p=r(t,"P",{});var f=o(p);_=m(f,"Example:"),f.forEach(a),g=i(t),b(l.$$.fragment,t)},m(t,f){$(t,p,f),e(p,_),$(t,g,f),x(l,t,f),u=!0},p:C,i(t){u||(w(l.$$.fragment,t),u=!0)},o(t){E(l.$$.fragment,t),u=!1},d(t){t&&a(p),t&&a(g),D(l,t)}}}function Ki(T){let p,_,g,l,u;return l=new P({props:{code:`downloaded_files = dl_manager.download('https://storage.googleapis.com/seldon-datasets/sentence_polarity_v1/rt-polaritydata.tar.gz')
extracted_files = dl_manager.extract(downloaded_files)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>downloaded_files = dl_manager.download(<span class="hljs-string">&#x27;https://storage.googleapis.com/seldon-datasets/sentence_polarity_v1/rt-polaritydata.tar.gz&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>extracted_files = dl_manager.extract(downloaded_files)`}}),{c(){p=n("p"),_=c("Example:"),g=d(),v(l.$$.fragment)},l(t){p=r(t,"P",{});var f=o(p);_=m(f,"Example:"),f.forEach(a),g=i(t),b(l.$$.fragment,t)},m(t,f){$(t,p,f),e(p,_),$(t,g,f),x(l,t,f),u=!0},p:C,i(t){u||(w(l.$$.fragment,t),u=!0)},o(t){E(l.$$.fragment,t),u=!1},d(t){t&&a(p),t&&a(g),D(l,t)}}}function Yi(T){let p,_,g,l,u;return l=new P({props:{code:`archive = dl_manager.download('https://storage.googleapis.com/seldon-datasets/sentence_polarity_v1/rt-polaritydata.tar.gz')
files = dl_manager.iter_archive(archive)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>archive = dl_manager.download(<span class="hljs-string">&#x27;https://storage.googleapis.com/seldon-datasets/sentence_polarity_v1/rt-polaritydata.tar.gz&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>files = dl_manager.iter_archive(archive)`}}),{c(){p=n("p"),_=c("Example:"),g=d(),v(l.$$.fragment)},l(t){p=r(t,"P",{});var f=o(p);_=m(f,"Example:"),f.forEach(a),g=i(t),b(l.$$.fragment,t)},m(t,f){$(t,p,f),e(p,_),$(t,g,f),x(l,t,f),u=!0},p:C,i(t){u||(w(l.$$.fragment,t),u=!0)},o(t){E(l.$$.fragment,t),u=!1},d(t){t&&a(p),t&&a(g),D(l,t)}}}function Qi(T){let p,_,g,l,u;return l=new P({props:{code:`files = dl_manager.download_and_extract('https://huggingface.co/datasets/beans/resolve/main/data/train.zip')
files = dl_manager.iter_files(files)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>files = dl_manager.download_and_extract(<span class="hljs-string">&#x27;https://huggingface.co/datasets/beans/resolve/main/data/train.zip&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>files = dl_manager.iter_files(files)`}}),{c(){p=n("p"),_=c("Example:"),g=d(),v(l.$$.fragment)},l(t){p=r(t,"P",{});var f=o(p);_=m(f,"Example:"),f.forEach(a),g=i(t),b(l.$$.fragment,t)},m(t,f){$(t,p,f),e(p,_),$(t,g,f),x(l,t,f),u=!0},p:C,i(t){u||(w(l.$$.fragment,t),u=!0)},o(t){E(l.$$.fragment,t),u=!1},d(t){t&&a(p),t&&a(g),D(l,t)}}}function Zi(T){let p,_,g,l,u;return l=new P({props:{code:`datasets.SplitGenerator(
    name=datasets.Split.TRAIN,
    gen_kwargs={"split_key": "train", "files": dl_manager.iter_archive(archive)},
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>datasets.SplitGenerator(
<span class="hljs-meta">... </span>    name=datasets.Split.TRAIN,
<span class="hljs-meta">... </span>    gen_kwargs={<span class="hljs-string">&quot;split_key&quot;</span>: <span class="hljs-string">&quot;train&quot;</span>, <span class="hljs-string">&quot;files&quot;</span>: dl_manager.iter_archive(archive)},
<span class="hljs-meta">... </span>)`}}),{c(){p=n("p"),_=c("Example:"),g=d(),v(l.$$.fragment)},l(t){p=r(t,"P",{});var f=o(p);_=m(f,"Example:"),f.forEach(a),g=i(t),b(l.$$.fragment,t)},m(t,f){$(t,p,f),e(p,_),$(t,g,f),x(l,t,f),u=!0},p:C,i(t){u||(w(l.$$.fragment,t),u=!0)},o(t){E(l.$$.fragment,t),u=!1},d(t){t&&a(p),t&&a(g),D(l,t)}}}function ep(T){let p,_,g,l,u;return l=new P({props:{code:`datasets.SplitGenerator(
    name=datasets.Split.TRAIN,
    gen_kwargs={"split_key": "train", "files": dl_manager.iter_archive(archive)},
),
datasets.SplitGenerator(
    name=datasets.Split.VALIDATION,
    gen_kwargs={"split_key": "validation", "files": dl_manager.iter_archive(archive)},
),
datasets.SplitGenerator(
    name=datasets.Split.TEST,
    gen_kwargs={"split_key": "test", "files": dl_manager.iter_archive(archive)},
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>datasets.SplitGenerator(
<span class="hljs-meta">... </span>    name=datasets.Split.TRAIN,
<span class="hljs-meta">... </span>    gen_kwargs={<span class="hljs-string">&quot;split_key&quot;</span>: <span class="hljs-string">&quot;train&quot;</span>, <span class="hljs-string">&quot;files&quot;</span>: dl_manager.iter_archive(archive)},
<span class="hljs-meta">... </span>),
<span class="hljs-meta">... </span>datasets.SplitGenerator(
<span class="hljs-meta">... </span>    name=datasets.Split.VALIDATION,
<span class="hljs-meta">... </span>    gen_kwargs={<span class="hljs-string">&quot;split_key&quot;</span>: <span class="hljs-string">&quot;validation&quot;</span>, <span class="hljs-string">&quot;files&quot;</span>: dl_manager.iter_archive(archive)},
<span class="hljs-meta">... </span>),
<span class="hljs-meta">... </span>datasets.SplitGenerator(
<span class="hljs-meta">... </span>    name=datasets.Split.TEST,
<span class="hljs-meta">... </span>    gen_kwargs={<span class="hljs-string">&quot;split_key&quot;</span>: <span class="hljs-string">&quot;test&quot;</span>, <span class="hljs-string">&quot;files&quot;</span>: dl_manager.iter_archive(archive)},
<span class="hljs-meta">... </span>)`}}),{c(){p=n("p"),_=c("Example:"),g=d(),v(l.$$.fragment)},l(t){p=r(t,"P",{});var f=o(p);_=m(f,"Example:"),f.forEach(a),g=i(t),b(l.$$.fragment,t)},m(t,f){$(t,p,f),e(p,_),$(t,g,f),x(l,t,f),u=!0},p:C,i(t){u||(w(l.$$.fragment,t),u=!0)},o(t){E(l.$$.fragment,t),u=!1},d(t){t&&a(p),t&&a(g),D(l,t)}}}function tp(T){let p,_,g,l,u;return l=new P({props:{code:`Each descriptor can be composed with other using addition or slice. Ex
split = datasets.Split.TRAIN.subsplit(datasets.percent[0:25]) + datasets.Split.TEST

The resulting split will correspond to 25% of the train split merged with
100% of the test split.`,highlighted:`Each descriptor can be composed <span class="hljs-keyword">with</span> other using addition <span class="hljs-keyword">or</span> <span class="hljs-built_in">slice</span>. Ex
split = datasets.Split.TRAIN.subsplit(datasets.percent[<span class="hljs-number">0</span>:<span class="hljs-number">25</span>]) + datasets.Split.TEST

The resulting split will correspond to <span class="hljs-number">25</span>% of the train split merged <span class="hljs-keyword">with</span>
<span class="hljs-number">100</span>% of the test split.`}}),{c(){p=n("p"),_=c("Example:"),g=d(),v(l.$$.fragment)},l(t){p=r(t,"P",{});var f=o(p);_=m(f,"Example:"),f.forEach(a),g=i(t),b(l.$$.fragment,t)},m(t,f){$(t,p,f),e(p,_),$(t,g,f),x(l,t,f),u=!0},p:C,i(t){u||(w(l.$$.fragment,t),u=!0)},o(t){E(l.$$.fragment,t),u=!1},d(t){t&&a(p),t&&a(g),D(l,t)}}}function ap(T){let p,_,g,l,u;return l=new P({props:{code:`split = (
        datasets.Split.TRAIN.subsplit(datasets.percent[:25]) +
        datasets.Split.TRAIN.subsplit(datasets.percent[75:])
)  # Error
split = datasets.Split.TEST + datasets.Split.ALL  # Error`,highlighted:`split = (
        datasets.Split.TRAIN.subsplit(datasets.percent[:<span class="hljs-number">25</span>]) +
        datasets.Split.TRAIN.subsplit(datasets.percent[<span class="hljs-number">75</span>:])
)  <span class="hljs-comment"># Error</span>
split = datasets.Split.TEST + datasets.Split.ALL  <span class="hljs-comment"># Error</span>`}}),{c(){p=n("p"),_=c("A split cannot be added twice, so the following will fail:"),g=d(),v(l.$$.fragment)},l(t){p=r(t,"P",{});var f=o(p);_=m(f,"A split cannot be added twice, so the following will fail:"),f.forEach(a),g=i(t),b(l.$$.fragment,t)},m(t,f){$(t,p,f),e(p,_),$(t,g,f),x(l,t,f),u=!0},p:C,i(t){u||(w(l.$$.fragment,t),u=!0)},o(t){E(l.$$.fragment,t),u=!1},d(t){t&&a(p),t&&a(g),D(l,t)}}}function sp(T){let p,_,g,l,u;return l=new P({props:{code:`split = (
datasets.Split.TRAIN.subsplit(datasets.percent[:25]) +
datasets.Split.TEST.subsplit(datasets.percent[:50])
)
split = (datasets.Split.TRAIN + datasets.Split.TEST).subsplit(datasets.percent[:50])`,highlighted:`split = (
datasets.Split.TRAIN.subsplit(datasets.percent[:<span class="hljs-number">25</span>]) +
datasets.Split.TEST.subsplit(datasets.percent[:<span class="hljs-number">50</span>])
)
split = (datasets.Split.TRAIN + datasets.Split.TEST).subsplit(datasets.percent[:<span class="hljs-number">50</span>])`}}),{c(){p=n("p"),_=c("The slices can be applied only one time. So the following are valid:"),g=d(),v(l.$$.fragment)},l(t){p=r(t,"P",{});var f=o(p);_=m(f,"The slices can be applied only one time. So the following are valid:"),f.forEach(a),g=i(t),b(l.$$.fragment,t)},m(t,f){$(t,p,f),e(p,_),$(t,g,f),x(l,t,f),u=!0},p:C,i(t){u||(w(l.$$.fragment,t),u=!0)},o(t){E(l.$$.fragment,t),u=!1},d(t){t&&a(p),t&&a(g),D(l,t)}}}function np(T){let p,_,g,l,u;return l=new P({props:{code:`train = datasets.Split.TRAIN
test = datasets.Split.TEST
split = train.subsplit(datasets.percent[:25]).subsplit(datasets.percent[:25])
split = (train.subsplit(datasets.percent[:25]) + test).subsplit(datasets.percent[:50])`,highlighted:`train = datasets.Split.TRAIN
test = datasets.Split.TEST
split = train.subsplit(datasets.percent[:<span class="hljs-number">25</span>]).subsplit(datasets.percent[:<span class="hljs-number">25</span>])
split = (train.subsplit(datasets.percent[:<span class="hljs-number">25</span>]) + test).subsplit(datasets.percent[:<span class="hljs-number">50</span>])`}}),{c(){p=n("p"),_=c("But not:"),g=d(),v(l.$$.fragment)},l(t){p=r(t,"P",{});var f=o(p);_=m(f,"But not:"),f.forEach(a),g=i(t),b(l.$$.fragment,t)},m(t,f){$(t,p,f),e(p,_),$(t,g,f),x(l,t,f),u=!0},p:C,i(t){u||(w(l.$$.fragment,t),u=!0)},o(t){E(l.$$.fragment,t),u=!1},d(t){t&&a(p),t&&a(g),D(l,t)}}}function rp(T){let p,_,g,l,u;return l=new P({props:{code:`# The following lines are equivalent:
ds = datasets.load_dataset('mnist', split='test[:33%]')
ds = datasets.load_dataset('mnist', split=datasets.ReadInstruction.from_spec('test[:33%]'))
ds = datasets.load_dataset('mnist', split=datasets.ReadInstruction('test', to=33, unit='%'))
ds = datasets.load_dataset('mnist', split=datasets.ReadInstruction(
'test', from_=0, to=33, unit='%'))

# The following lines are equivalent:
ds = datasets.load_dataset('mnist', split='test[:33%]+train[1:-1]')
ds = datasets.load_dataset('mnist', split=datasets.ReadInstruction.from_spec(
'test[:33%]+train[1:-1]'))
ds = datasets.load_dataset('mnist', split=(
datasets.ReadInstruction('test', to=33, unit='%') +
datasets.ReadInstruction('train', from_=1, to=-1, unit='abs')))

# The following lines are equivalent:
ds = datasets.load_dataset('mnist', split='test[:33%](pct1_dropremainder)')
ds = datasets.load_dataset('mnist', split=datasets.ReadInstruction.from_spec(
'test[:33%](pct1_dropremainder)'))
ds = datasets.load_dataset('mnist', split=datasets.ReadInstruction(
'test', from_=0, to=33, unit='%', rounding="pct1_dropremainder"))

# 10-fold validation:
tests = datasets.load_dataset(
'mnist',
[datasets.ReadInstruction('train', from_=k, to=k+10, unit='%')
for k in range(0, 100, 10)])
trains = datasets.load_dataset(
'mnist',
[datasets.ReadInstruction('train', to=k, unit='%') + datasets.ReadInstruction('train', from_=k+10, unit='%')
for k in range(0, 100, 10)])`,highlighted:`<span class="hljs-comment"># The following lines are equivalent:</span>
ds = datasets.load_dataset(<span class="hljs-string">&#x27;mnist&#x27;</span>, split=<span class="hljs-string">&#x27;test[:33%]&#x27;</span>)
ds = datasets.load_dataset(<span class="hljs-string">&#x27;mnist&#x27;</span>, split=datasets.ReadInstruction.from_spec(<span class="hljs-string">&#x27;test[:33%]&#x27;</span>))
ds = datasets.load_dataset(<span class="hljs-string">&#x27;mnist&#x27;</span>, split=datasets.ReadInstruction(<span class="hljs-string">&#x27;test&#x27;</span>, to=<span class="hljs-number">33</span>, unit=<span class="hljs-string">&#x27;%&#x27;</span>))
ds = datasets.load_dataset(<span class="hljs-string">&#x27;mnist&#x27;</span>, split=datasets.ReadInstruction(
<span class="hljs-string">&#x27;test&#x27;</span>, from_=<span class="hljs-number">0</span>, to=<span class="hljs-number">33</span>, unit=<span class="hljs-string">&#x27;%&#x27;</span>))

<span class="hljs-comment"># The following lines are equivalent:</span>
ds = datasets.load_dataset(<span class="hljs-string">&#x27;mnist&#x27;</span>, split=<span class="hljs-string">&#x27;test[:33%]+train[1:-1]&#x27;</span>)
ds = datasets.load_dataset(<span class="hljs-string">&#x27;mnist&#x27;</span>, split=datasets.ReadInstruction.from_spec(
<span class="hljs-string">&#x27;test[:33%]+train[1:-1]&#x27;</span>))
ds = datasets.load_dataset(<span class="hljs-string">&#x27;mnist&#x27;</span>, split=(
datasets.ReadInstruction(<span class="hljs-string">&#x27;test&#x27;</span>, to=<span class="hljs-number">33</span>, unit=<span class="hljs-string">&#x27;%&#x27;</span>) +
datasets.ReadInstruction(<span class="hljs-string">&#x27;train&#x27;</span>, from_=<span class="hljs-number">1</span>, to=-<span class="hljs-number">1</span>, unit=<span class="hljs-string">&#x27;abs&#x27;</span>)))

<span class="hljs-comment"># The following lines are equivalent:</span>
ds = datasets.load_dataset(<span class="hljs-string">&#x27;mnist&#x27;</span>, split=<span class="hljs-string">&#x27;test[:33%](pct1_dropremainder)&#x27;</span>)
ds = datasets.load_dataset(<span class="hljs-string">&#x27;mnist&#x27;</span>, split=datasets.ReadInstruction.from_spec(
<span class="hljs-string">&#x27;test[:33%](pct1_dropremainder)&#x27;</span>))
ds = datasets.load_dataset(<span class="hljs-string">&#x27;mnist&#x27;</span>, split=datasets.ReadInstruction(
<span class="hljs-string">&#x27;test&#x27;</span>, from_=<span class="hljs-number">0</span>, to=<span class="hljs-number">33</span>, unit=<span class="hljs-string">&#x27;%&#x27;</span>, rounding=<span class="hljs-string">&quot;pct1_dropremainder&quot;</span>))

<span class="hljs-comment"># 10-fold validation:</span>
tests = datasets.load_dataset(
<span class="hljs-string">&#x27;mnist&#x27;</span>,
[datasets.ReadInstruction(<span class="hljs-string">&#x27;train&#x27;</span>, from_=k, to=k+<span class="hljs-number">10</span>, unit=<span class="hljs-string">&#x27;%&#x27;</span>)
<span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, <span class="hljs-number">100</span>, <span class="hljs-number">10</span>)])
trains = datasets.load_dataset(
<span class="hljs-string">&#x27;mnist&#x27;</span>,
[datasets.ReadInstruction(<span class="hljs-string">&#x27;train&#x27;</span>, to=k, unit=<span class="hljs-string">&#x27;%&#x27;</span>) + datasets.ReadInstruction(<span class="hljs-string">&#x27;train&#x27;</span>, from_=k+<span class="hljs-number">10</span>, unit=<span class="hljs-string">&#x27;%&#x27;</span>)
<span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, <span class="hljs-number">100</span>, <span class="hljs-number">10</span>)])`}}),{c(){p=n("p"),_=c("Examples:"),g=d(),v(l.$$.fragment)},l(t){p=r(t,"P",{});var f=o(p);_=m(f,"Examples:"),f.forEach(a),g=i(t),b(l.$$.fragment,t)},m(t,f){$(t,p,f),e(p,_),$(t,g,f),x(l,t,f),u=!0},p:C,i(t){u||(w(l.$$.fragment,t),u=!0)},o(t){E(l.$$.fragment,t),u=!1},d(t){t&&a(p),t&&a(g),D(l,t)}}}function op(T){let p,_,g,l,u;return l=new P({props:{code:'VERSION = datasets.Version("1.0.0")',highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>VERSION = datasets.Version(<span class="hljs-string">&quot;1.0.0&quot;</span>)'}}),{c(){p=n("p"),_=c("Example:"),g=d(),v(l.$$.fragment)},l(t){p=r(t,"P",{});var f=o(p);_=m(f,"Example:"),f.forEach(a),g=i(t),b(l.$$.fragment,t)},m(t,f){$(t,p,f),e(p,_),$(t,g,f),x(l,t,f),u=!0},p:C,i(t){u||(w(l.$$.fragment,t),u=!0)},o(t){E(l.$$.fragment,t),u=!1},d(t){t&&a(p),t&&a(g),D(l,t)}}}function lp(T){let p,_,g,l,u,t,f,ba,Dn,Gs,W,yn,Pt,Tn,jn,At,kn,In,Hs,k,Ye,Sn,xa,Bn,Nn,Lt,wa,Rn,Cn,Pn,de,Ot,Ea,An,Ln,On,Vt,qt,Vn,qn,Fn,Ee,Ft,Mn,zn,Da,Un,Gn,Hn,U,ya,Wn,Xn,Ta,Jn,Kn,ja,Yn,Qn,ka,Zn,er,tr,X,Qe,ar,Ia,sr,nr,De,rr,ye,Ze,or,Sa,lr,dr,J,et,ir,Ba,pr,cr,Te,mr,K,tt,fr,Na,gr,ur,je,_r,ke,at,hr,Ra,$r,Ws,G,st,vr,Ca,br,xr,Y,Pa,wr,Er,Aa,Dr,yr,La,Tr,jr,Xs,ie,nt,kr,Oa,Ir,Js,pe,rt,Sr,Va,Br,Ks,F,ot,Nr,lt,Rr,Mt,Cr,Pr,Ar,dt,Lr,zt,Or,Vr,qr,Q,it,Fr,qa,Mr,zr,ce,Fa,Ur,Gr,Ma,Hr,Wr,za,Xr,Ys,S,pt,Jr,Z,ct,Kr,Ua,Yr,Qr,Ie,Zr,ee,mt,eo,Ga,to,ao,Se,so,te,ft,no,gt,ro,Ha,oo,lo,io,Be,po,ae,ut,co,Wa,mo,fo,Ne,go,se,_t,uo,Xa,_o,ho,Re,$o,ne,ht,vo,Ja,bo,xo,Ce,wo,Pe,$t,Eo,Ka,Do,Qs,O,vt,yo,Ut,Ya,To,jo,ko,bt,Io,Qa,So,Bo,No,Za,Ro,Co,xt,es,me,Zs,Po,ts,Ao,Lo,as,Oo,Vo,fe,ge,Gt,ss,qo,Fo,Mo,ns,zo,Uo,rs,Go,Ho,ue,os,ls,Wo,Xo,ds,Jo,Ko,is,Yo,Qo,_e,ps,cs,Zo,el,ms,tl,al,fs,sl,en,M,wt,nl,gs,rl,ol,he,ll,us,dl,il,_s,pl,cl,ml,Ae,tn,N,Et,fl,Ht,hs,gl,ul,_l,$s,hl,$l,H,Wt,vs,vl,bl,xl,Xt,bs,wl,El,Dl,Jt,xs,yl,Tl,jl,Kt,ws,kl,Il,Sl,Yt,Bl,Es,Nl,Rl,Dt,Cl,Ds,Pl,Al,Ll,Le,an,B,yt,Ol,ys,Vl,ql,Oe,Fl,Ts,Ml,zl,Ve,Ul,js,Gl,Hl,qe,Wl,Fe,sn,$e,Tt,Xl,ks,Jl,nn,V,jt,Kl,Is,Yl,Ql,Me,Zl,ze,kt,ed,Ss,td,ad,re,It,sd,Bs,nd,rd,Ns,od,rn,ve,St,ld,Rs,dd,on,z,Bt,id,Cs,pd,cd,Ue,md,Ge,Nt,fd,Ps,gd,ln;return t=new zi({}),Ye=new j({props:{name:"class datasets.DatasetBuilder",anchor:"datasets.DatasetBuilder",parameters:[{name:"cache_dir",val:": typing.Optional[str] = None"},{name:"name",val:": typing.Optional[str] = None"},{name:"hash",val:": typing.Optional[str] = None"},{name:"base_path",val:": typing.Optional[str] = None"},{name:"info",val:": typing.Optional[datasets.info.DatasetInfo] = None"},{name:"features",val:": typing.Optional[datasets.features.features.Features] = None"},{name:"use_auth_token",val:": typing.Union[str, bool, NoneType] = None"},{name:"repo_id",val:": typing.Optional[str] = None"},{name:"data_files",val:": typing.Union[str, list, dict, datasets.data_files.DataFilesDict, NoneType] = None"},{name:"data_dir",val:": typing.Optional[str] = None"},{name:"**config_kwargs",val:""}],source:"https://github.com/huggingface/datasets/blob/r_4313/src/datasets/builder.py#L177"}}),Qe=new j({props:{name:"as_dataset",anchor:"datasets.DatasetBuilder.as_dataset",parameters:[{name:"split",val:": typing.Optional[datasets.splits.Split] = None"},{name:"run_post_process",val:" = True"},{name:"ignore_verifications",val:" = False"},{name:"in_memory",val:" = False"}],parametersDescription:[{anchor:"datasets.DatasetBuilder.as_dataset.split",description:"<strong>split</strong> (<code>datasets.Split</code>) &#x2014; Which subset of the data to return.",name:"split"},{anchor:"datasets.DatasetBuilder.as_dataset.run_post_process",description:`<strong>run_post_process</strong> (bool, default=True) &#x2014; Whether to run post-processing dataset transforms and/or add
indexes.`,name:"run_post_process"},{anchor:"datasets.DatasetBuilder.as_dataset.ignore_verifications",description:`<strong>ignore_verifications</strong> (bool, default=False) &#x2014; Whether to ignore the verifications of the
downloaded/processed dataset information (checksums/size/splits/&#x2026;).`,name:"ignore_verifications"},{anchor:"datasets.DatasetBuilder.as_dataset.in_memory",description:"<strong>in_memory</strong> (bool, default=False) &#x2014; Whether to copy the data in-memory.",name:"in_memory"}],source:"https://github.com/huggingface/datasets/blob/r_4313/src/datasets/builder.py#L772",returnDescription:`
<p>datasets.Dataset</p>
`}}),De=new R({props:{anchor:"datasets.DatasetBuilder.as_dataset.example",$$slots:{default:[Ui]},$$scope:{ctx:T}}}),Ze=new j({props:{name:"download_and_prepare",anchor:"datasets.DatasetBuilder.download_and_prepare",parameters:[{name:"download_config",val:": typing.Optional[datasets.utils.file_utils.DownloadConfig] = None"},{name:"download_mode",val:": typing.Optional[datasets.utils.download_manager.DownloadMode] = None"},{name:"ignore_verifications",val:": bool = False"},{name:"try_from_hf_gcs",val:": bool = True"},{name:"dl_manager",val:": typing.Optional[datasets.utils.download_manager.DownloadManager] = None"},{name:"base_path",val:": typing.Optional[str] = None"},{name:"use_auth_token",val:": typing.Union[str, bool, NoneType] = None"},{name:"**download_and_prepare_kwargs",val:""}],parametersDescription:[{anchor:"datasets.DatasetBuilder.download_and_prepare.download_config",description:'<strong>download_config</strong> (<a href="/docs/datasets/pr_4313/en/package_reference/builder_classes#datasets.DownloadConfig">DownloadConfig</a>, optional) &#x2014; specific download configuration parameters.',name:"download_config"},{anchor:"datasets.DatasetBuilder.download_and_prepare.download_mode",description:'<strong>download_mode</strong> (<a href="/docs/datasets/pr_4313/en/package_reference/builder_classes#datasets.DownloadMode">DownloadMode</a>, optional) &#x2014; select the download/generate mode - Default to <code>REUSE_DATASET_IF_EXISTS</code>',name:"download_mode"},{anchor:"datasets.DatasetBuilder.download_and_prepare.ignore_verifications",description:"<strong>ignore_verifications</strong> (<code>bool</code>) &#x2014; Ignore the verifications of the downloaded/processed dataset information (checksums/size/splits/&#x2026;)",name:"ignore_verifications"},{anchor:"datasets.DatasetBuilder.download_and_prepare.try_from_hf_gcs",description:"<strong>try_from_hf_gcs</strong> (<code>bool</code>) &#x2014; If True, it will try to download the already prepared dataset from the Hf google cloud storage",name:"try_from_hf_gcs"},{anchor:"datasets.DatasetBuilder.download_and_prepare.dl_manager",description:'<strong>dl_manager</strong> (<a href="/docs/datasets/pr_4313/en/package_reference/builder_classes#datasets.DownloadManager">DownloadManager</a>, optional) &#x2014; specific Download Manger to use',name:"dl_manager"},{anchor:"datasets.DatasetBuilder.download_and_prepare.base_path",description:`<strong>base_path</strong> (<code>str</code>, optional) &#x2014; base path for relative paths that are used to download files. This can be a remote url.
If not specified, the value of the <em>base_path</em> attribute (<em>self.base_path</em>) will be used instead.`,name:"base_path"},{anchor:"datasets.DatasetBuilder.download_and_prepare.use_auth_token",description:`<strong>use_auth_token</strong> (<code>Union[str, bool]</code>, optional) &#x2014; Optional string or boolean to use as Bearer token for remote files on the Datasets Hub.
If True, will get token from ~/.huggingface.`,name:"use_auth_token"}],source:"https://github.com/huggingface/datasets/blob/r_4313/src/datasets/builder.py#L512"}}),et=new j({props:{name:"get_all_exported_dataset_infos",anchor:"datasets.DatasetBuilder.get_all_exported_dataset_infos",parameters:[],source:"https://github.com/huggingface/datasets/blob/r_4313/src/datasets/builder.py#L324"}}),Te=new R({props:{anchor:"datasets.DatasetBuilder.get_all_exported_dataset_infos.example",$$slots:{default:[Gi]},$$scope:{ctx:T}}}),tt=new j({props:{name:"get_exported_dataset_info",anchor:"datasets.DatasetBuilder.get_exported_dataset_info",parameters:[],source:"https://github.com/huggingface/datasets/blob/r_4313/src/datasets/builder.py#L342"}}),je=new R({props:{anchor:"datasets.DatasetBuilder.get_exported_dataset_info.example",$$slots:{default:[Hi]},$$scope:{ctx:T}}}),at=new j({props:{name:"get_imported_module_dir",anchor:"datasets.DatasetBuilder.get_imported_module_dir",parameters:[],source:"https://github.com/huggingface/datasets/blob/r_4313/src/datasets/builder.py#L507"}}),st=new j({props:{name:"class datasets.GeneratorBasedBuilder",anchor:"datasets.GeneratorBasedBuilder",parameters:[{name:"*args",val:""},{name:"writer_batch_size",val:" = None"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/datasets/blob/r_4313/src/datasets/builder.py#L1058"}}),nt=new j({props:{name:"class datasets.BeamBasedBuilder",anchor:"datasets.BeamBasedBuilder",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/datasets/blob/r_4313/src/datasets/builder.py#L1218"}}),rt=new j({props:{name:"class datasets.ArrowBasedBuilder",anchor:"datasets.ArrowBasedBuilder",parameters:[{name:"cache_dir",val:": typing.Optional[str] = None"},{name:"name",val:": typing.Optional[str] = None"},{name:"hash",val:": typing.Optional[str] = None"},{name:"base_path",val:": typing.Optional[str] = None"},{name:"info",val:": typing.Optional[datasets.info.DatasetInfo] = None"},{name:"features",val:": typing.Optional[datasets.features.features.Features] = None"},{name:"use_auth_token",val:": typing.Union[str, bool, NoneType] = None"},{name:"repo_id",val:": typing.Optional[str] = None"},{name:"data_files",val:": typing.Union[str, list, dict, datasets.data_files.DataFilesDict, NoneType] = None"},{name:"data_dir",val:": typing.Optional[str] = None"},{name:"**config_kwargs",val:""}],source:"https://github.com/huggingface/datasets/blob/r_4313/src/datasets/builder.py#L1156"}}),ot=new j({props:{name:"class datasets.BuilderConfig",anchor:"datasets.BuilderConfig",parameters:[{name:"name",val:": str = 'default'"},{name:"version",val:": typing.Union[str, datasets.utils.version.Version, NoneType] = '0.0.0'"},{name:"data_dir",val:": typing.Optional[str] = None"},{name:"data_files",val:": typing.Optional[datasets.data_files.DataFilesDict] = None"},{name:"description",val:": typing.Optional[str] = None"}],parametersDescription:[{anchor:"datasets.BuilderConfig.name",description:"<strong>name</strong> (<code>str</code>, default <code>&quot;default&quot;</code>) &#x2014;",name:"name"},{anchor:"datasets.BuilderConfig.version",description:'<strong>version</strong> (<a href="/docs/datasets/pr_4313/en/package_reference/builder_classes#datasets.Version">Version</a> or <code>str</code>, optional) &#x2014;',name:"version"},{anchor:"datasets.BuilderConfig.data_dir",description:"<strong>data_dir</strong> (<code>str</code>, optional) &#x2014;",name:"data_dir"},{anchor:"datasets.BuilderConfig.data_files",description:"<strong>data_files</strong> (<code>str</code> or <code>Sequence</code> or <code>Mapping</code>, optional) &#x2014; Path(s) to source data file(s).",name:"data_files"},{anchor:"datasets.BuilderConfig.description",description:"<strong>description</strong> (<code>str</code>, optional) &#x2014;",name:"description"}],source:"https://github.com/huggingface/datasets/blob/r_4313/src/datasets/builder.py#L81"}}),it=new j({props:{name:"create_config_id",anchor:"datasets.BuilderConfig.create_config_id",parameters:[{name:"config_kwargs",val:": dict"},{name:"custom_features",val:": typing.Optional[datasets.features.features.Features] = None"}],source:"https://github.com/huggingface/datasets/blob/r_4313/src/datasets/builder.py#L120"}}),pt=new j({props:{name:"class datasets.DownloadManager",anchor:"datasets.DownloadManager",parameters:[{name:"dataset_name",val:": typing.Optional[str] = None"},{name:"data_dir",val:": typing.Optional[str] = None"},{name:"download_config",val:": typing.Optional[datasets.utils.file_utils.DownloadConfig] = None"},{name:"base_path",val:": typing.Optional[str] = None"},{name:"record_checksums",val:" = True"}],source:"https://github.com/huggingface/datasets/blob/r_4313/src/datasets/utils/download_manager.py#L141"}}),ct=new j({props:{name:"download",anchor:"datasets.DownloadManager.download",parameters:[{name:"url_or_urls",val:""}],source:"https://github.com/huggingface/datasets/blob/r_4313/src/datasets/utils/download_manager.py#L265",returnDescription:`
<p><code>str</code>, The downloaded paths matching the given input
url_or_urls.</p>
`,returnType:`
<p>downloaded_path(s)</p>
`}}),Ie=new R({props:{anchor:"datasets.DownloadManager.download.example",$$slots:{default:[Wi]},$$scope:{ctx:T}}}),mt=new j({props:{name:"download_and_extract",anchor:"datasets.DownloadManager.download_and_extract",parameters:[{name:"url_or_urls",val:""}],source:"https://github.com/huggingface/datasets/blob/r_4313/src/datasets/utils/download_manager.py#L400",returnDescription:`
<p><code>str</code>, extracted paths of given URL(s).</p>
`,returnType:`
<p>extracted_path(s)</p>
`}}),Se=new R({props:{anchor:"datasets.DownloadManager.download_and_extract.example",$$slots:{default:[Xi]},$$scope:{ctx:T}}}),ft=new j({props:{name:"download_custom",anchor:"datasets.DownloadManager.download_custom",parameters:[{name:"url_or_urls",val:""},{name:"custom_download",val:""}],source:"https://github.com/huggingface/datasets/blob/r_4313/src/datasets/utils/download_manager.py#L218",returnDescription:`
<p><code>str</code>, The downloaded paths matching the given input
url_or_urls.</p>
`,returnType:`
<p>downloaded_path(s)</p>
`}}),Be=new R({props:{anchor:"datasets.DownloadManager.download_custom.example",$$slots:{default:[Ji]},$$scope:{ctx:T}}}),ut=new j({props:{name:"extract",anchor:"datasets.DownloadManager.extract",parameters:[{name:"path_or_paths",val:""},{name:"num_proc",val:" = None"}],source:"https://github.com/huggingface/datasets/blob/r_4313/src/datasets/utils/download_manager.py#L363",returnDescription:`
<p><code>str</code>, The extracted paths matching the given input
path_or_paths.</p>
`,returnType:`
<p>extracted_path(s)</p>
`}}),Ne=new R({props:{anchor:"datasets.DownloadManager.extract.example",$$slots:{default:[Ki]},$$scope:{ctx:T}}}),_t=new j({props:{name:"iter_archive",anchor:"datasets.DownloadManager.iter_archive",parameters:[{name:"path_or_buf",val:": typing.Union[str, _io.BufferedReader]"}],parametersDescription:[{anchor:"datasets.DownloadManager.iter_archive.path_or_buf",description:"<strong>path_or_buf</strong> (<code>str</code> or <code>io.BufferedReader</code>) &#x2014; Archive path or archive binary file object.",name:"path_or_buf"}],source:"https://github.com/huggingface/datasets/blob/r_4313/src/datasets/utils/download_manager.py#L322"}}),Re=new R({props:{anchor:"datasets.DownloadManager.iter_archive.example",$$slots:{default:[Yi]},$$scope:{ctx:T}}}),ht=new j({props:{name:"iter_files",anchor:"datasets.DownloadManager.iter_files",parameters:[{name:"paths",val:": typing.Union[str, typing.List[str]]"}],parametersDescription:[{anchor:"datasets.DownloadManager.iter_files.paths",description:"<strong>paths</strong> (<code>str</code> or <code>list</code> of <code>str</code>) &#x2014; Root paths.",name:"paths"}],source:"https://github.com/huggingface/datasets/blob/r_4313/src/datasets/utils/download_manager.py#L345"}}),Ce=new R({props:{anchor:"datasets.DownloadManager.iter_files.example",$$slots:{default:[Qi]},$$scope:{ctx:T}}}),$t=new j({props:{name:"ship_files_with_pipeline",anchor:"datasets.DownloadManager.ship_files_with_pipeline",parameters:[{name:"downloaded_path_or_paths",val:""},{name:"pipeline",val:""}],source:"https://github.com/huggingface/datasets/blob/r_4313/src/datasets/utils/download_manager.py#L183"}}),vt=new j({props:{name:"class datasets.DownloadMode",anchor:"datasets.DownloadMode",parameters:[{name:"value",val:""},{name:"names",val:" = None"},{name:"module",val:" = None"},{name:"qualname",val:" = None"},{name:"type",val:" = None"},{name:"start",val:" = 1"}],source:"https://github.com/huggingface/datasets/blob/r_4313/src/datasets/utils/download_manager.py#L44"}}),wt=new j({props:{name:"class datasets.SplitGenerator",anchor:"datasets.SplitGenerator",parameters:[{name:"name",val:": str"},{name:"gen_kwargs",val:": typing.Dict = <factory>"}],parametersDescription:[{anchor:"datasets.SplitGenerator.name",description:`<strong>name</strong> (str) &#x2014; Name of the Split for which the generator will
create the examples.
**gen_kwargs &#x2014; Keyword arguments to forward to the <code>DatasetBuilder._generate_examples</code> method
of the builder.`,name:"name"}],source:"https://github.com/huggingface/datasets/blob/r_4313/src/datasets/splits.py#L566"}}),Ae=new R({props:{anchor:"datasets.SplitGenerator.example",$$slots:{default:[Zi]},$$scope:{ctx:T}}}),Et=new j({props:{name:"class datasets.Split",anchor:"datasets.Split",parameters:[{name:"name",val:""}],source:"https://github.com/huggingface/datasets/blob/r_4313/src/datasets/splits.py#L387"}}),Le=new R({props:{anchor:"datasets.Split.example",$$slots:{default:[ep]},$$scope:{ctx:T}}}),yt=new j({props:{name:"class datasets.NamedSplit",anchor:"datasets.NamedSplit",parameters:[{name:"name",val:""}],source:"https://github.com/huggingface/datasets/blob/r_4313/src/datasets/splits.py#L303"}}),Oe=new R({props:{anchor:"datasets.NamedSplit.example",$$slots:{default:[tp]},$$scope:{ctx:T}}}),Ve=new R({props:{anchor:"datasets.NamedSplit.example-2",$$slots:{default:[ap]},$$scope:{ctx:T}}}),qe=new R({props:{anchor:"datasets.NamedSplit.example-3",$$slots:{default:[sp]},$$scope:{ctx:T}}}),Fe=new R({props:{anchor:"datasets.NamedSplit.example-4",$$slots:{default:[np]},$$scope:{ctx:T}}}),Tt=new j({props:{name:"class datasets.NamedSplitAll",anchor:"datasets.NamedSplitAll",parameters:[],source:"https://github.com/huggingface/datasets/blob/r_4313/src/datasets/splits.py#L372"}}),jt=new j({props:{name:"class datasets.ReadInstruction",anchor:"datasets.ReadInstruction",parameters:[{name:"split_name",val:""},{name:"rounding",val:" = None"},{name:"from_",val:" = None"},{name:"to",val:" = None"},{name:"unit",val:" = None"}],source:"https://github.com/huggingface/datasets/blob/r_4313/src/datasets/arrow_reader.py#L456"}}),Me=new R({props:{anchor:"datasets.ReadInstruction.example",$$slots:{default:[rp]},$$scope:{ctx:T}}}),kt=new j({props:{name:"from_spec",anchor:"datasets.ReadInstruction.from_spec",parameters:[{name:"spec",val:""}],parametersDescription:[{anchor:"datasets.ReadInstruction.from_spec.spec",description:`<strong>spec</strong> (str) &#x2014; split(s) + optional slice(s) to read + optional rounding
if percents are used as the slicing unit. A slice can be specified,
using absolute numbers (int) or percentages (int). E.g.
<code>test</code>: test split.
<code>test + validation</code>: test split + validation split.
<code>test[10:]</code>: test split, minus its first 10 records.
<code>test[:10%]</code>: first 10% records of test split.
<code>test[:20%](pct1_dropremainder)</code>: first 10% records, rounded with
the <code>pct1_dropremainder</code> rounding.
<code>test[:-5%]+train[40%:60%]</code>: first 95% of test + middle 20% of
train.`,name:"spec"}],source:"https://github.com/huggingface/datasets/blob/r_4313/src/datasets/arrow_reader.py#L536",returnDescription:`
<p>ReadInstruction instance.</p>
`}}),It=new j({props:{name:"to_absolute",anchor:"datasets.ReadInstruction.to_absolute",parameters:[{name:"name2len",val:""}],source:"https://github.com/huggingface/datasets/blob/r_4313/src/datasets/arrow_reader.py#L604",returnDescription:`
<p>list of _AbsoluteInstruction instances (corresponds to the + in spec).</p>
`}}),St=new j({props:{name:"class datasets.DownloadConfig",anchor:"datasets.DownloadConfig",parameters:[{name:"cache_dir",val:": typing.Union[pathlib.Path, str, NoneType] = None"},{name:"force_download",val:": bool = False"},{name:"resume_download",val:": bool = False"},{name:"local_files_only",val:": bool = False"},{name:"proxies",val:": typing.Optional[typing.Dict] = None"},{name:"user_agent",val:": typing.Optional[str] = None"},{name:"extract_compressed_file",val:": bool = False"},{name:"force_extract",val:": bool = False"},{name:"delete_extracted",val:": bool = False"},{name:"use_etag",val:": bool = True"},{name:"num_proc",val:": typing.Optional[int] = None"},{name:"max_retries",val:": int = 1"},{name:"use_auth_token",val:": typing.Union[bool, str, NoneType] = None"},{name:"ignore_url_params",val:": bool = False"},{name:"download_desc",val:": typing.Optional[str] = None"}],parametersDescription:[{anchor:"datasets.DownloadConfig.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>Path</code>, optional) &#x2014; Specify a cache directory to save the file to (overwrite the
default cache dir).`,name:"cache_dir"},{anchor:"datasets.DownloadConfig.force_download",description:`<strong>force_download</strong> (<code>bool</code>, default <code>False</code>) &#x2014; If True, re-dowload the file even if it&#x2019;s already cached in
the cache dir.`,name:"force_download"},{anchor:"datasets.DownloadConfig.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, default <code>False</code>) &#x2014; If True, resume the download if incompletly recieved file is
found.`,name:"resume_download"},{anchor:"datasets.DownloadConfig.proxies",description:"<strong>proxies</strong> (<code>dict</code>, optional) &#x2014;",name:"proxies"},{anchor:"datasets.DownloadConfig.user_agent",description:`<strong>user_agent</strong> (<code>str</code>, optional) &#x2014; Optional string or dict that will be appended to the user-agent on remote
requests.`,name:"user_agent"},{anchor:"datasets.DownloadConfig.extract_compressed_file",description:`<strong>extract_compressed_file</strong> (<code>bool</code>, default <code>False</code>) &#x2014; If True and the path point to a zip or tar file,
extract the compressed file in a folder along the archive.`,name:"extract_compressed_file"},{anchor:"datasets.DownloadConfig.force_extract",description:`<strong>force_extract</strong> (<code>bool</code>, default <code>False</code>) &#x2014; If True when extract_compressed_file is True and the archive
was already extracted, re-extract the archive and override the folder where it was extracted.`,name:"force_extract"},{anchor:"datasets.DownloadConfig.delete_extracted",description:"<strong>delete_extracted</strong> (<code>bool</code>, default <code>False</code>) &#x2014; Whether to delete (or keep) the extracted files.",name:"delete_extracted"},{anchor:"datasets.DownloadConfig.use_etag",description:"<strong>use_etag</strong> (<code>bool</code>, default <code>True</code>) &#x2014; Whether to use the ETag HTTP response header to validate the cached files.",name:"use_etag"},{anchor:"datasets.DownloadConfig.num_proc",description:"<strong>num_proc</strong> (<code>int</code>, optional) &#x2014; The number of processes to launch to download the files in parallel.",name:"num_proc"},{anchor:"datasets.DownloadConfig.max_retries",description:"<strong>max_retries</strong> (<code>int</code>, default <code>1</code>) &#x2014; The number of times to retry an HTTP request if it fails.",name:"max_retries"},{anchor:"datasets.DownloadConfig.use_auth_token",description:`<strong>use_auth_token</strong> (<code>str</code> or <code>bool</code>, optional) &#x2014; Optional string or boolean to use as Bearer token
for remote files on the Datasets Hub. If True, will get token from ~/.huggingface.`,name:"use_auth_token"},{anchor:"datasets.DownloadConfig.ignore_url_params",description:`<strong>ignore_url_params</strong> (<code>bool</code>, default <code>False</code>) &#x2014; Whether to strip all query parameters and #fragments from
the download URL before using it for caching the file.`,name:"ignore_url_params"},{anchor:"datasets.DownloadConfig.download_desc",description:"<strong>download_desc</strong> (<code>str</code>, optional) &#x2014; A description to be displayed alongside with the progress bar while downloading the files.",name:"download_desc"}],source:"https://github.com/huggingface/datasets/blob/r_4313/src/datasets/utils/file_utils.py#L153"}}),Bt=new j({props:{name:"class datasets.Version",anchor:"datasets.Version",parameters:[{name:"version_str",val:": str"},{name:"description",val:": typing.Optional[str] = None"},{name:"major",val:": typing.Union[str, int, NoneType] = None"},{name:"minor",val:": typing.Union[str, int, NoneType] = None"},{name:"patch",val:": typing.Union[str, int, NoneType] = None"}],parametersDescription:[{anchor:"datasets.Version.version_str",description:"<strong>version_str</strong> (<code>str</code>) &#x2014; Eg: &#x201C;1.2.3&#x201D;.",name:"version_str"},{anchor:"datasets.Version.description",description:"<strong>description</strong> (<code>str</code>) &#x2014; A description of what is new in this version.",name:"description"},{anchor:"datasets.Version.version_str",description:"<strong>version_str</strong> (<code>str</code>) &#x2014; Eg: &#x201C;1.2.3&#x201D;.",name:"version_str"},{anchor:"datasets.Version.description",description:"<strong>description</strong> (<code>str</code>) &#x2014; A description of what is new in this version.",name:"description"},{anchor:"datasets.Version.major",description:"<strong>major</strong> (<code>str</code>) &#x2014;",name:"major"},{anchor:"datasets.Version.minor",description:"<strong>minor</strong> (<code>str</code>) &#x2014;",name:"minor"},{anchor:"datasets.Version.patch",description:"<strong>patch</strong> (<code>str</code>) &#x2014;",name:"patch"}],source:"https://github.com/huggingface/datasets/blob/r_4313/src/datasets/utils/version.py#L30"}}),Ue=new R({props:{anchor:"datasets.Version.example",$$slots:{default:[op]},$$scope:{ctx:T}}}),Nt=new j({props:{name:"match",anchor:"datasets.Version.match",parameters:[{name:"other_version",val:""}],source:"https://github.com/huggingface/datasets/blob/r_4313/src/datasets/utils/version.py#L98"}}),{c(){p=n("meta"),_=d(),g=n("h1"),l=n("a"),u=n("span"),v(t.$$.fragment),f=d(),ba=n("span"),Dn=c("Builder classes"),Gs=d(),W=n("p"),yn=c("\u{1F917} Datasets relies on two main classes during the dataset building process: "),Pt=n("a"),Tn=c("DatasetBuilder"),jn=c(" and "),At=n("a"),kn=c("BuilderConfig"),In=c("."),Hs=d(),k=n("div"),v(Ye.$$.fragment),Sn=d(),xa=n("p"),Bn=c("Abstract base class for all datasets."),Nn=d(),Lt=n("p"),wa=n("em"),Rn=c("DatasetBuilder"),Cn=c(" has 3 key methods:"),Pn=d(),de=n("ul"),Ot=n("li"),Ea=n("code"),An=c("datasets.DatasetBuilder.info"),Ln=c(`: Documents the dataset, including feature
names, types, and shapes, version, splits, citation, etc.`),On=d(),Vt=n("li"),qt=n("a"),Vn=c("datasets.DatasetBuilder.download_and_prepare()"),qn=c(`: Downloads the source data
and writes it to disk.`),Fn=d(),Ee=n("li"),Ft=n("a"),Mn=c("datasets.DatasetBuilder.as_dataset()"),zn=c(": Generates a "),Da=n("em"),Un=c("Dataset"),Gn=c("."),Hn=d(),U=n("p"),ya=n("strong"),Wn=c("Configuration"),Xn=c(": Some "),Ta=n("em"),Jn=c("DatasetBuilder"),Kn=c(`s expose multiple variants of the
dataset by defining a `),ja=n("em"),Yn=c("datasets.BuilderConfig"),Qn=c(` subclass and accepting a
config object (or name) on construction. Configurable datasets expose a
pre-defined set of configurations in `),ka=n("code"),Zn=c("datasets.DatasetBuilder.builder_configs()"),er=c("."),tr=d(),X=n("div"),v(Qe.$$.fragment),ar=d(),Ia=n("p"),sr=c("Return a Dataset for the specified split."),nr=d(),v(De.$$.fragment),rr=d(),ye=n("div"),v(Ze.$$.fragment),or=d(),Sa=n("p"),lr=c("Downloads and prepares dataset for reading."),dr=d(),J=n("div"),v(et.$$.fragment),ir=d(),Ba=n("p"),pr=c("Empty dict if doesn\u2019t exist"),cr=d(),v(Te.$$.fragment),mr=d(),K=n("div"),v(tt.$$.fragment),fr=d(),Na=n("p"),gr=c("Empty DatasetInfo if doesn\u2019t exist"),ur=d(),v(je.$$.fragment),_r=d(),ke=n("div"),v(at.$$.fragment),hr=d(),Ra=n("p"),$r=c("Return the path of the module of this class or subclass."),Ws=d(),G=n("div"),v(st.$$.fragment),vr=d(),Ca=n("p"),br=c("Base class for datasets with data generation based on dict generators."),xr=d(),Y=n("p"),Pa=n("code"),wr=c("GeneratorBasedBuilder"),Er=c(` is a convenience class that abstracts away much
of the data writing and reading of `),Aa=n("code"),Dr=c("DatasetBuilder"),yr=c(`. It expects subclasses to
implement generators of feature dictionaries across the dataset splits
(`),La=n("code"),Tr=c("_split_generators"),jr=c("). See the method docstrings for details."),Xs=d(),ie=n("div"),v(nt.$$.fragment),kr=d(),Oa=n("p"),Ir=c("Beam based Builder."),Js=d(),pe=n("div"),v(rt.$$.fragment),Sr=d(),Va=n("p"),Br=c("Base class for datasets with data generation based on Arrow loading functions (CSV/JSON/Parquet)."),Ks=d(),F=n("div"),v(ot.$$.fragment),Nr=d(),lt=n("p"),Rr=c("Base class for "),Mt=n("a"),Cr=c("DatasetBuilder"),Pr=c(" data configuration."),Ar=d(),dt=n("p"),Lr=c(`DatasetBuilder subclasses with data configuration options should subclass
`),zt=n("a"),Or=c("BuilderConfig"),Vr=c(" and add their own properties."),qr=d(),Q=n("div"),v(it.$$.fragment),Fr=d(),qa=n("p"),Mr=c(`The config id is used to build the cache directory.
By default it is equal to the config name.
However the name of a config is not sufficient to have a unique identifier for the dataset being generated
since it doesn\u2019t take into account:`),zr=d(),ce=n("ul"),Fa=n("li"),Ur=c("the config kwargs that can be used to overwrite attributes"),Gr=d(),Ma=n("li"),Hr=c("the custom features used to write the dataset"),Wr=d(),za=n("li"),Xr=c(`the data_files for json/text/csv/pandas datasets
Therefore the config id is just the config name with an optional suffix based on these.`),Ys=d(),S=n("div"),v(pt.$$.fragment),Jr=d(),Z=n("div"),v(ct.$$.fragment),Kr=d(),Ua=n("p"),Yr=c("Download given url(s)."),Qr=d(),v(Ie.$$.fragment),Zr=d(),ee=n("div"),v(mt.$$.fragment),eo=d(),Ga=n("p"),to=c("Download and extract given url_or_urls."),ao=d(),v(Se.$$.fragment),so=d(),te=n("div"),v(ft.$$.fragment),no=d(),gt=n("p"),ro=c("Download given urls(s) by calling "),Ha=n("code"),oo=c("custom_download"),lo=c("."),io=d(),v(Be.$$.fragment),po=d(),ae=n("div"),v(ut.$$.fragment),co=d(),Wa=n("p"),mo=c("Extract given path(s)."),fo=d(),v(Ne.$$.fragment),go=d(),se=n("div"),v(_t.$$.fragment),uo=d(),Xa=n("p"),_o=c("Iterate over files within an archive."),ho=d(),v(Re.$$.fragment),$o=d(),ne=n("div"),v(ht.$$.fragment),vo=d(),Ja=n("p"),bo=c("Iterate over file paths."),xo=d(),v(Ce.$$.fragment),wo=d(),Pe=n("div"),v($t.$$.fragment),Eo=d(),Ka=n("p"),Do=c("Ship the files using Beam FileSystems to the pipeline temp dir."),Qs=d(),O=n("div"),v(vt.$$.fragment),yo=d(),Ut=n("p"),Ya=n("code"),To=c("Enum"),jo=c(" for how to treat pre-existing downloads and data."),ko=d(),bt=n("p"),Io=c("The default mode is "),Qa=n("code"),So=c("REUSE_DATASET_IF_EXISTS"),Bo=c(`, which will reuse both
raw downloads and the prepared dataset if they exist.`),No=d(),Za=n("p"),Ro=c("The generations modes:"),Co=d(),xt=n("table"),es=n("thead"),me=n("tr"),Zs=n("th"),Po=d(),ts=n("th"),Ao=c("Downloads"),Lo=d(),as=n("th"),Oo=c("Dataset"),Vo=d(),fe=n("tbody"),ge=n("tr"),Gt=n("td"),ss=n("code"),qo=c("REUSE_DATASET_IF_EXISTS"),Fo=c(" (default)"),Mo=d(),ns=n("td"),zo=c("Reuse"),Uo=d(),rs=n("td"),Go=c("Reuse"),Ho=d(),ue=n("tr"),os=n("td"),ls=n("code"),Wo=c("REUSE_CACHE_IF_EXISTS"),Xo=d(),ds=n("td"),Jo=c("Reuse"),Ko=d(),is=n("td"),Yo=c("Fresh"),Qo=d(),_e=n("tr"),ps=n("td"),cs=n("code"),Zo=c("FORCE_REDOWNLOAD"),el=d(),ms=n("td"),tl=c("Fresh"),al=d(),fs=n("td"),sl=c("Fresh"),en=d(),M=n("div"),v(wt.$$.fragment),nl=d(),gs=n("p"),rl=c("Defines the split information for the generator."),ol=d(),he=n("p"),ll=c(`This should be used as returned value of
`),us=n("code"),dl=c("GeneratorBasedBuilder._split_generators()"),il=c(`.
See `),_s=n("code"),pl=c("GeneratorBasedBuilder._split_generators()"),cl=c(` for more info and example
of usage.`),ml=d(),v(Ae.$$.fragment),tn=d(),N=n("div"),v(Et.$$.fragment),fl=d(),Ht=n("p"),hs=n("code"),gl=c("Enum"),ul=c(" for dataset splits."),_l=d(),$s=n("p"),hl=c(`Datasets are typically split into different subsets to be used at various
stages of training and evaluation.`),$l=d(),H=n("ul"),Wt=n("li"),vs=n("code"),vl=c("TRAIN"),bl=c(": the training data."),xl=d(),Xt=n("li"),bs=n("code"),wl=c("VALIDATION"),El=c(`: the validation data. If present, this is typically used as
evaluation data while iterating on a model (e.g. changing hyperparameters,
model architecture, etc.).`),Dl=d(),Jt=n("li"),xs=n("code"),yl=c("TEST"),Tl=c(`: the testing data. This is the data to report metrics on. Typically
you do not want to use this during model iteration as you may overfit to it.`),jl=d(),Kt=n("li"),ws=n("code"),kl=c("ALL"),Il=c(": the union of all defined dataset splits."),Sl=d(),Yt=n("p"),Bl=c("Note: All splits, including compositions inherit from "),Es=n("code"),Nl=c("datasets.SplitBase"),Rl=d(),Dt=n("p"),Cl=c("See the :doc:"),Ds=n("code"),Pl=c("guide on splits </loading>"),Al=c(" for more information."),Ll=d(),v(Le.$$.fragment),an=d(),B=n("div"),v(yt.$$.fragment),Ol=d(),ys=n("p"),Vl=c("Descriptor corresponding to a named split (train, test, \u2026)."),ql=d(),v(Oe.$$.fragment),Fl=d(),Ts=n("p"),Ml=c("Warning:"),zl=d(),v(Ve.$$.fragment),Ul=d(),js=n("p"),Gl=c("Warning:"),Hl=d(),v(qe.$$.fragment),Wl=d(),v(Fe.$$.fragment),sn=d(),$e=n("div"),v(Tt.$$.fragment),Xl=d(),ks=n("p"),Jl=c("Split corresponding to the union of all defined dataset splits."),nn=d(),V=n("div"),v(jt.$$.fragment),Kl=d(),Is=n("p"),Yl=c("Reading instruction for a dataset."),Ql=d(),v(Me.$$.fragment),Zl=d(),ze=n("div"),v(kt.$$.fragment),ed=d(),Ss=n("p"),td=c("Creates a ReadInstruction instance out of a string spec."),ad=d(),re=n("div"),v(It.$$.fragment),sd=d(),Bs=n("p"),nd=c("Translate instruction into a list of absolute instructions."),rd=d(),Ns=n("p"),od=c("Those absolute instructions are then to be added together."),rn=d(),ve=n("div"),v(St.$$.fragment),ld=d(),Rs=n("p"),dd=c("Configuration for our cached path manager."),on=d(),z=n("div"),v(Bt.$$.fragment),id=d(),Cs=n("p"),pd=c("Dataset version MAJOR.MINOR.PATCH."),cd=d(),v(Ue.$$.fragment),md=d(),Ge=n("div"),v(Nt.$$.fragment),fd=d(),Ps=n("p"),gd=c("Returns True if other_version matches."),this.h()},l(s){const h=Fi('[data-svelte="svelte-1phssyn"]',document.head);p=r(h,"META",{name:!0,content:!0}),h.forEach(a),_=i(s),g=r(s,"H1",{class:!0});var Rt=o(g);l=r(Rt,"A",{id:!0,class:!0,href:!0});var As=o(l);u=r(As,"SPAN",{});var Ls=o(u);b(t.$$.fragment,Ls),Ls.forEach(a),As.forEach(a),f=i(Rt),ba=r(Rt,"SPAN",{});var Os=o(ba);Dn=m(Os,"Builder classes"),Os.forEach(a),Rt.forEach(a),Gs=i(s),W=r(s,"P",{});var be=o(W);yn=m(be,"\u{1F917} Datasets relies on two main classes during the dataset building process: "),Pt=r(be,"A",{href:!0});var Vs=o(Pt);Tn=m(Vs,"DatasetBuilder"),Vs.forEach(a),jn=m(be," and "),At=r(be,"A",{href:!0});var qs=o(At);kn=m(qs,"BuilderConfig"),qs.forEach(a),In=m(be,"."),be.forEach(a),Hs=i(s),k=r(s,"DIV",{class:!0});var I=o(k);b(Ye.$$.fragment,I),Sn=i(I),xa=r(I,"P",{});var Fs=o(xa);Bn=m(Fs,"Abstract base class for all datasets."),Fs.forEach(a),Nn=i(I),Lt=r(I,"P",{});var Qt=o(Lt);wa=r(Qt,"EM",{});var Ms=o(wa);Rn=m(Ms,"DatasetBuilder"),Ms.forEach(a),Cn=m(Qt," has 3 key methods:"),Qt.forEach(a),Pn=i(I),de=r(I,"UL",{});var xe=o(de);Ot=r(xe,"LI",{});var Zt=o(Ot);Ea=r(Zt,"CODE",{});var zs=o(Ea);An=m(zs,"datasets.DatasetBuilder.info"),zs.forEach(a),Ln=m(Zt,`: Documents the dataset, including feature
names, types, and shapes, version, splits, citation, etc.`),Zt.forEach(a),On=i(xe),Vt=r(xe,"LI",{});var ea=o(Vt);qt=r(ea,"A",{href:!0});var Us=o(qt);Vn=m(Us,"datasets.DatasetBuilder.download_and_prepare()"),Us.forEach(a),qn=m(ea,`: Downloads the source data
and writes it to disk.`),ea.forEach(a),Fn=i(xe),Ee=r(xe,"LI",{});var He=o(Ee);Ft=r(He,"A",{href:!0});var Ed=o(Ft);Mn=m(Ed,"datasets.DatasetBuilder.as_dataset()"),Ed.forEach(a),zn=m(He,": Generates a "),Da=r(He,"EM",{});var Dd=o(Da);Un=m(Dd,"Dataset"),Dd.forEach(a),Gn=m(He,"."),He.forEach(a),xe.forEach(a),Hn=i(I),U=r(I,"P",{});var we=o(U);ya=r(we,"STRONG",{});var yd=o(ya);Wn=m(yd,"Configuration"),yd.forEach(a),Xn=m(we,": Some "),Ta=r(we,"EM",{});var Td=o(Ta);Jn=m(Td,"DatasetBuilder"),Td.forEach(a),Kn=m(we,`s expose multiple variants of the
dataset by defining a `),ja=r(we,"EM",{});var jd=o(ja);Yn=m(jd,"datasets.BuilderConfig"),jd.forEach(a),Qn=m(we,` subclass and accepting a
config object (or name) on construction. Configurable datasets expose a
pre-defined set of configurations in `),ka=r(we,"CODE",{});var kd=o(ka);Zn=m(kd,"datasets.DatasetBuilder.builder_configs()"),kd.forEach(a),er=m(we,"."),we.forEach(a),tr=i(I),X=r(I,"DIV",{class:!0});var ta=o(X);b(Qe.$$.fragment,ta),ar=i(ta),Ia=r(ta,"P",{});var Id=o(Ia);sr=m(Id,"Return a Dataset for the specified split."),Id.forEach(a),nr=i(ta),b(De.$$.fragment,ta),ta.forEach(a),rr=i(I),ye=r(I,"DIV",{class:!0});var dn=o(ye);b(Ze.$$.fragment,dn),or=i(dn),Sa=r(dn,"P",{});var Sd=o(Sa);lr=m(Sd,"Downloads and prepares dataset for reading."),Sd.forEach(a),dn.forEach(a),dr=i(I),J=r(I,"DIV",{class:!0});var aa=o(J);b(et.$$.fragment,aa),ir=i(aa),Ba=r(aa,"P",{});var Bd=o(Ba);pr=m(Bd,"Empty dict if doesn\u2019t exist"),Bd.forEach(a),cr=i(aa),b(Te.$$.fragment,aa),aa.forEach(a),mr=i(I),K=r(I,"DIV",{class:!0});var sa=o(K);b(tt.$$.fragment,sa),fr=i(sa),Na=r(sa,"P",{});var Nd=o(Na);gr=m(Nd,"Empty DatasetInfo if doesn\u2019t exist"),Nd.forEach(a),ur=i(sa),b(je.$$.fragment,sa),sa.forEach(a),_r=i(I),ke=r(I,"DIV",{class:!0});var pn=o(ke);b(at.$$.fragment,pn),hr=i(pn),Ra=r(pn,"P",{});var Rd=o(Ra);$r=m(Rd,"Return the path of the module of this class or subclass."),Rd.forEach(a),pn.forEach(a),I.forEach(a),Ws=i(s),G=r(s,"DIV",{class:!0});var na=o(G);b(st.$$.fragment,na),vr=i(na),Ca=r(na,"P",{});var Cd=o(Ca);br=m(Cd,"Base class for datasets with data generation based on dict generators."),Cd.forEach(a),xr=i(na),Y=r(na,"P",{});var Ct=o(Y);Pa=r(Ct,"CODE",{});var Pd=o(Pa);wr=m(Pd,"GeneratorBasedBuilder"),Pd.forEach(a),Er=m(Ct,` is a convenience class that abstracts away much
of the data writing and reading of `),Aa=r(Ct,"CODE",{});var Ad=o(Aa);Dr=m(Ad,"DatasetBuilder"),Ad.forEach(a),yr=m(Ct,`. It expects subclasses to
implement generators of feature dictionaries across the dataset splits
(`),La=r(Ct,"CODE",{});var Ld=o(La);Tr=m(Ld,"_split_generators"),Ld.forEach(a),jr=m(Ct,"). See the method docstrings for details."),Ct.forEach(a),na.forEach(a),Xs=i(s),ie=r(s,"DIV",{class:!0});var cn=o(ie);b(nt.$$.fragment,cn),kr=i(cn),Oa=r(cn,"P",{});var Od=o(Oa);Ir=m(Od,"Beam based Builder."),Od.forEach(a),cn.forEach(a),Js=i(s),pe=r(s,"DIV",{class:!0});var mn=o(pe);b(rt.$$.fragment,mn),Sr=i(mn),Va=r(mn,"P",{});var Vd=o(Va);Br=m(Vd,"Base class for datasets with data generation based on Arrow loading functions (CSV/JSON/Parquet)."),Vd.forEach(a),mn.forEach(a),Ks=i(s),F=r(s,"DIV",{class:!0});var We=o(F);b(ot.$$.fragment,We),Nr=i(We),lt=r(We,"P",{});var fn=o(lt);Rr=m(fn,"Base class for "),Mt=r(fn,"A",{href:!0});var qd=o(Mt);Cr=m(qd,"DatasetBuilder"),qd.forEach(a),Pr=m(fn," data configuration."),fn.forEach(a),Ar=i(We),dt=r(We,"P",{});var gn=o(dt);Lr=m(gn,`DatasetBuilder subclasses with data configuration options should subclass
`),zt=r(gn,"A",{href:!0});var Fd=o(zt);Or=m(Fd,"BuilderConfig"),Fd.forEach(a),Vr=m(gn," and add their own properties."),gn.forEach(a),qr=i(We),Q=r(We,"DIV",{class:!0});var ra=o(Q);b(it.$$.fragment,ra),Fr=i(ra),qa=r(ra,"P",{});var Md=o(qa);Mr=m(Md,`The config id is used to build the cache directory.
By default it is equal to the config name.
However the name of a config is not sufficient to have a unique identifier for the dataset being generated
since it doesn\u2019t take into account:`),Md.forEach(a),zr=i(ra),ce=r(ra,"UL",{});var oa=o(ce);Fa=r(oa,"LI",{});var zd=o(Fa);Ur=m(zd,"the config kwargs that can be used to overwrite attributes"),zd.forEach(a),Gr=i(oa),Ma=r(oa,"LI",{});var Ud=o(Ma);Hr=m(Ud,"the custom features used to write the dataset"),Ud.forEach(a),Wr=i(oa),za=r(oa,"LI",{});var Gd=o(za);Xr=m(Gd,`the data_files for json/text/csv/pandas datasets
Therefore the config id is just the config name with an optional suffix based on these.`),Gd.forEach(a),oa.forEach(a),ra.forEach(a),We.forEach(a),Ys=i(s),S=r(s,"DIV",{class:!0});var A=o(S);b(pt.$$.fragment,A),Jr=i(A),Z=r(A,"DIV",{class:!0});var la=o(Z);b(ct.$$.fragment,la),Kr=i(la),Ua=r(la,"P",{});var Hd=o(Ua);Yr=m(Hd,"Download given url(s)."),Hd.forEach(a),Qr=i(la),b(Ie.$$.fragment,la),la.forEach(a),Zr=i(A),ee=r(A,"DIV",{class:!0});var da=o(ee);b(mt.$$.fragment,da),eo=i(da),Ga=r(da,"P",{});var Wd=o(Ga);to=m(Wd,"Download and extract given url_or_urls."),Wd.forEach(a),ao=i(da),b(Se.$$.fragment,da),da.forEach(a),so=i(A),te=r(A,"DIV",{class:!0});var ia=o(te);b(ft.$$.fragment,ia),no=i(ia),gt=r(ia,"P",{});var un=o(gt);ro=m(un,"Download given urls(s) by calling "),Ha=r(un,"CODE",{});var Xd=o(Ha);oo=m(Xd,"custom_download"),Xd.forEach(a),lo=m(un,"."),un.forEach(a),io=i(ia),b(Be.$$.fragment,ia),ia.forEach(a),po=i(A),ae=r(A,"DIV",{class:!0});var pa=o(ae);b(ut.$$.fragment,pa),co=i(pa),Wa=r(pa,"P",{});var Jd=o(Wa);mo=m(Jd,"Extract given path(s)."),Jd.forEach(a),fo=i(pa),b(Ne.$$.fragment,pa),pa.forEach(a),go=i(A),se=r(A,"DIV",{class:!0});var ca=o(se);b(_t.$$.fragment,ca),uo=i(ca),Xa=r(ca,"P",{});var Kd=o(Xa);_o=m(Kd,"Iterate over files within an archive."),Kd.forEach(a),ho=i(ca),b(Re.$$.fragment,ca),ca.forEach(a),$o=i(A),ne=r(A,"DIV",{class:!0});var ma=o(ne);b(ht.$$.fragment,ma),vo=i(ma),Ja=r(ma,"P",{});var Yd=o(Ja);bo=m(Yd,"Iterate over file paths."),Yd.forEach(a),xo=i(ma),b(Ce.$$.fragment,ma),ma.forEach(a),wo=i(A),Pe=r(A,"DIV",{class:!0});var _n=o(Pe);b($t.$$.fragment,_n),Eo=i(_n),Ka=r(_n,"P",{});var Qd=o(Ka);Do=m(Qd,"Ship the files using Beam FileSystems to the pipeline temp dir."),Qd.forEach(a),_n.forEach(a),A.forEach(a),Qs=i(s),O=r(s,"DIV",{class:!0});var oe=o(O);b(vt.$$.fragment,oe),yo=i(oe),Ut=r(oe,"P",{});var ud=o(Ut);Ya=r(ud,"CODE",{});var Zd=o(Ya);To=m(Zd,"Enum"),Zd.forEach(a),jo=m(ud," for how to treat pre-existing downloads and data."),ud.forEach(a),ko=i(oe),bt=r(oe,"P",{});var hn=o(bt);Io=m(hn,"The default mode is "),Qa=r(hn,"CODE",{});var ei=o(Qa);So=m(ei,"REUSE_DATASET_IF_EXISTS"),ei.forEach(a),Bo=m(hn,`, which will reuse both
raw downloads and the prepared dataset if they exist.`),hn.forEach(a),No=i(oe),Za=r(oe,"P",{});var ti=o(Za);Ro=m(ti,"The generations modes:"),ti.forEach(a),Co=i(oe),xt=r(oe,"TABLE",{});var $n=o(xt);es=r($n,"THEAD",{});var ai=o(es);me=r(ai,"TR",{});var fa=o(me);Zs=r(fa,"TH",{}),o(Zs).forEach(a),Po=i(fa),ts=r(fa,"TH",{});var si=o(ts);Ao=m(si,"Downloads"),si.forEach(a),Lo=i(fa),as=r(fa,"TH",{});var ni=o(as);Oo=m(ni,"Dataset"),ni.forEach(a),fa.forEach(a),ai.forEach(a),Vo=i($n),fe=r($n,"TBODY",{});var ga=o(fe);ge=r(ga,"TR",{});var ua=o(ge);Gt=r(ua,"TD",{});var _d=o(Gt);ss=r(_d,"CODE",{});var ri=o(ss);qo=m(ri,"REUSE_DATASET_IF_EXISTS"),ri.forEach(a),Fo=m(_d," (default)"),_d.forEach(a),Mo=i(ua),ns=r(ua,"TD",{});var oi=o(ns);zo=m(oi,"Reuse"),oi.forEach(a),Uo=i(ua),rs=r(ua,"TD",{});var li=o(rs);Go=m(li,"Reuse"),li.forEach(a),ua.forEach(a),Ho=i(ga),ue=r(ga,"TR",{});var _a=o(ue);os=r(_a,"TD",{});var di=o(os);ls=r(di,"CODE",{});var ii=o(ls);Wo=m(ii,"REUSE_CACHE_IF_EXISTS"),ii.forEach(a),di.forEach(a),Xo=i(_a),ds=r(_a,"TD",{});var pi=o(ds);Jo=m(pi,"Reuse"),pi.forEach(a),Ko=i(_a),is=r(_a,"TD",{});var ci=o(is);Yo=m(ci,"Fresh"),ci.forEach(a),_a.forEach(a),Qo=i(ga),_e=r(ga,"TR",{});var ha=o(_e);ps=r(ha,"TD",{});var mi=o(ps);cs=r(mi,"CODE",{});var fi=o(cs);Zo=m(fi,"FORCE_REDOWNLOAD"),fi.forEach(a),mi.forEach(a),el=i(ha),ms=r(ha,"TD",{});var gi=o(ms);tl=m(gi,"Fresh"),gi.forEach(a),al=i(ha),fs=r(ha,"TD",{});var ui=o(fs);sl=m(ui,"Fresh"),ui.forEach(a),ha.forEach(a),ga.forEach(a),$n.forEach(a),oe.forEach(a),en=i(s),M=r(s,"DIV",{class:!0});var Xe=o(M);b(wt.$$.fragment,Xe),nl=i(Xe),gs=r(Xe,"P",{});var _i=o(gs);rl=m(_i,"Defines the split information for the generator."),_i.forEach(a),ol=i(Xe),he=r(Xe,"P",{});var $a=o(he);ll=m($a,`This should be used as returned value of
`),us=r($a,"CODE",{});var hi=o(us);dl=m(hi,"GeneratorBasedBuilder._split_generators()"),hi.forEach(a),il=m($a,`.
See `),_s=r($a,"CODE",{});var $i=o(_s);pl=m($i,"GeneratorBasedBuilder._split_generators()"),$i.forEach(a),cl=m($a,` for more info and example
of usage.`),$a.forEach(a),ml=i(Xe),b(Ae.$$.fragment,Xe),Xe.forEach(a),tn=i(s),N=r(s,"DIV",{class:!0});var q=o(N);b(Et.$$.fragment,q),fl=i(q),Ht=r(q,"P",{});var hd=o(Ht);hs=r(hd,"CODE",{});var vi=o(hs);gl=m(vi,"Enum"),vi.forEach(a),ul=m(hd," for dataset splits."),hd.forEach(a),_l=i(q),$s=r(q,"P",{});var bi=o($s);hl=m(bi,`Datasets are typically split into different subsets to be used at various
stages of training and evaluation.`),bi.forEach(a),$l=i(q),H=r(q,"UL",{});var Je=o(H);Wt=r(Je,"LI",{});var $d=o(Wt);vs=r($d,"CODE",{});var xi=o(vs);vl=m(xi,"TRAIN"),xi.forEach(a),bl=m($d,": the training data."),$d.forEach(a),xl=i(Je),Xt=r(Je,"LI",{});var vd=o(Xt);bs=r(vd,"CODE",{});var wi=o(bs);wl=m(wi,"VALIDATION"),wi.forEach(a),El=m(vd,`: the validation data. If present, this is typically used as
evaluation data while iterating on a model (e.g. changing hyperparameters,
model architecture, etc.).`),vd.forEach(a),Dl=i(Je),Jt=r(Je,"LI",{});var bd=o(Jt);xs=r(bd,"CODE",{});var Ei=o(xs);yl=m(Ei,"TEST"),Ei.forEach(a),Tl=m(bd,`: the testing data. This is the data to report metrics on. Typically
you do not want to use this during model iteration as you may overfit to it.`),bd.forEach(a),jl=i(Je),Kt=r(Je,"LI",{});var xd=o(Kt);ws=r(xd,"CODE",{});var Di=o(ws);kl=m(Di,"ALL"),Di.forEach(a),Il=m(xd,": the union of all defined dataset splits."),xd.forEach(a),Je.forEach(a),Sl=i(q),Yt=r(q,"P",{});var wd=o(Yt);Bl=m(wd,"Note: All splits, including compositions inherit from "),Es=r(wd,"CODE",{});var yi=o(Es);Nl=m(yi,"datasets.SplitBase"),yi.forEach(a),wd.forEach(a),Rl=i(q),Dt=r(q,"P",{});var vn=o(Dt);Cl=m(vn,"See the :doc:"),Ds=r(vn,"CODE",{});var Ti=o(Ds);Pl=m(Ti,"guide on splits </loading>"),Ti.forEach(a),Al=m(vn," for more information."),vn.forEach(a),Ll=i(q),b(Le.$$.fragment,q),q.forEach(a),an=i(s),B=r(s,"DIV",{class:!0});var L=o(B);b(yt.$$.fragment,L),Ol=i(L),ys=r(L,"P",{});var ji=o(ys);Vl=m(ji,"Descriptor corresponding to a named split (train, test, \u2026)."),ji.forEach(a),ql=i(L),b(Oe.$$.fragment,L),Fl=i(L),Ts=r(L,"P",{});var ki=o(Ts);Ml=m(ki,"Warning:"),ki.forEach(a),zl=i(L),b(Ve.$$.fragment,L),Ul=i(L),js=r(L,"P",{});var Ii=o(js);Gl=m(Ii,"Warning:"),Ii.forEach(a),Hl=i(L),b(qe.$$.fragment,L),Wl=i(L),b(Fe.$$.fragment,L),L.forEach(a),sn=i(s),$e=r(s,"DIV",{class:!0});var bn=o($e);b(Tt.$$.fragment,bn),Xl=i(bn),ks=r(bn,"P",{});var Si=o(ks);Jl=m(Si,"Split corresponding to the union of all defined dataset splits."),Si.forEach(a),bn.forEach(a),nn=i(s),V=r(s,"DIV",{class:!0});var le=o(V);b(jt.$$.fragment,le),Kl=i(le),Is=r(le,"P",{});var Bi=o(Is);Yl=m(Bi,"Reading instruction for a dataset."),Bi.forEach(a),Ql=i(le),b(Me.$$.fragment,le),Zl=i(le),ze=r(le,"DIV",{class:!0});var xn=o(ze);b(kt.$$.fragment,xn),ed=i(xn),Ss=r(xn,"P",{});var Ni=o(Ss);td=m(Ni,"Creates a ReadInstruction instance out of a string spec."),Ni.forEach(a),xn.forEach(a),ad=i(le),re=r(le,"DIV",{class:!0});var va=o(re);b(It.$$.fragment,va),sd=i(va),Bs=r(va,"P",{});var Ri=o(Bs);nd=m(Ri,"Translate instruction into a list of absolute instructions."),Ri.forEach(a),rd=i(va),Ns=r(va,"P",{});var Ci=o(Ns);od=m(Ci,"Those absolute instructions are then to be added together."),Ci.forEach(a),va.forEach(a),le.forEach(a),rn=i(s),ve=r(s,"DIV",{class:!0});var wn=o(ve);b(St.$$.fragment,wn),ld=i(wn),Rs=r(wn,"P",{});var Pi=o(Rs);dd=m(Pi,"Configuration for our cached path manager."),Pi.forEach(a),wn.forEach(a),on=i(s),z=r(s,"DIV",{class:!0});var Ke=o(z);b(Bt.$$.fragment,Ke),id=i(Ke),Cs=r(Ke,"P",{});var Ai=o(Cs);pd=m(Ai,"Dataset version MAJOR.MINOR.PATCH."),Ai.forEach(a),cd=i(Ke),b(Ue.$$.fragment,Ke),md=i(Ke),Ge=r(Ke,"DIV",{class:!0});var En=o(Ge);b(Nt.$$.fragment,En),fd=i(En),Ps=r(En,"P",{});var Li=o(Ps);gd=m(Li,"Returns True if other_version matches."),Li.forEach(a),En.forEach(a),Ke.forEach(a),this.h()},h(){y(p,"name","hf:doc:metadata"),y(p,"content",JSON.stringify(dp)),y(l,"id","datasets.DatasetBuilder"),y(l,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),y(l,"href","#datasets.DatasetBuilder"),y(g,"class","relative group"),y(Pt,"href","/docs/datasets/pr_4313/en/package_reference/builder_classes#datasets.DatasetBuilder"),y(At,"href","/docs/datasets/pr_4313/en/package_reference/builder_classes#datasets.BuilderConfig"),y(qt,"href","/docs/datasets/pr_4313/en/package_reference/builder_classes#datasets.DatasetBuilder.download_and_prepare"),y(Ft,"href","/docs/datasets/pr_4313/en/package_reference/builder_classes#datasets.DatasetBuilder.as_dataset"),y(X,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),y(ye,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),y(J,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),y(K,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),y(ke,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),y(k,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),y(G,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),y(ie,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),y(pe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),y(Mt,"href","/docs/datasets/pr_4313/en/package_reference/builder_classes#datasets.DatasetBuilder"),y(zt,"href","/docs/datasets/pr_4313/en/package_reference/builder_classes#datasets.BuilderConfig"),y(Q,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),y(F,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),y(Z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),y(ee,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),y(te,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),y(ae,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),y(se,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),y(ne,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),y(Pe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),y(S,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),y(O,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),y(M,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),y(N,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),y(B,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),y($e,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),y(ze,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),y(re,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),y(V,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),y(ve,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),y(Ge,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),y(z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(s,h){e(document.head,p),$(s,_,h),$(s,g,h),e(g,l),e(l,u),x(t,u,null),e(g,f),e(g,ba),e(ba,Dn),$(s,Gs,h),$(s,W,h),e(W,yn),e(W,Pt),e(Pt,Tn),e(W,jn),e(W,At),e(At,kn),e(W,In),$(s,Hs,h),$(s,k,h),x(Ye,k,null),e(k,Sn),e(k,xa),e(xa,Bn),e(k,Nn),e(k,Lt),e(Lt,wa),e(wa,Rn),e(Lt,Cn),e(k,Pn),e(k,de),e(de,Ot),e(Ot,Ea),e(Ea,An),e(Ot,Ln),e(de,On),e(de,Vt),e(Vt,qt),e(qt,Vn),e(Vt,qn),e(de,Fn),e(de,Ee),e(Ee,Ft),e(Ft,Mn),e(Ee,zn),e(Ee,Da),e(Da,Un),e(Ee,Gn),e(k,Hn),e(k,U),e(U,ya),e(ya,Wn),e(U,Xn),e(U,Ta),e(Ta,Jn),e(U,Kn),e(U,ja),e(ja,Yn),e(U,Qn),e(U,ka),e(ka,Zn),e(U,er),e(k,tr),e(k,X),x(Qe,X,null),e(X,ar),e(X,Ia),e(Ia,sr),e(X,nr),x(De,X,null),e(k,rr),e(k,ye),x(Ze,ye,null),e(ye,or),e(ye,Sa),e(Sa,lr),e(k,dr),e(k,J),x(et,J,null),e(J,ir),e(J,Ba),e(Ba,pr),e(J,cr),x(Te,J,null),e(k,mr),e(k,K),x(tt,K,null),e(K,fr),e(K,Na),e(Na,gr),e(K,ur),x(je,K,null),e(k,_r),e(k,ke),x(at,ke,null),e(ke,hr),e(ke,Ra),e(Ra,$r),$(s,Ws,h),$(s,G,h),x(st,G,null),e(G,vr),e(G,Ca),e(Ca,br),e(G,xr),e(G,Y),e(Y,Pa),e(Pa,wr),e(Y,Er),e(Y,Aa),e(Aa,Dr),e(Y,yr),e(Y,La),e(La,Tr),e(Y,jr),$(s,Xs,h),$(s,ie,h),x(nt,ie,null),e(ie,kr),e(ie,Oa),e(Oa,Ir),$(s,Js,h),$(s,pe,h),x(rt,pe,null),e(pe,Sr),e(pe,Va),e(Va,Br),$(s,Ks,h),$(s,F,h),x(ot,F,null),e(F,Nr),e(F,lt),e(lt,Rr),e(lt,Mt),e(Mt,Cr),e(lt,Pr),e(F,Ar),e(F,dt),e(dt,Lr),e(dt,zt),e(zt,Or),e(dt,Vr),e(F,qr),e(F,Q),x(it,Q,null),e(Q,Fr),e(Q,qa),e(qa,Mr),e(Q,zr),e(Q,ce),e(ce,Fa),e(Fa,Ur),e(ce,Gr),e(ce,Ma),e(Ma,Hr),e(ce,Wr),e(ce,za),e(za,Xr),$(s,Ys,h),$(s,S,h),x(pt,S,null),e(S,Jr),e(S,Z),x(ct,Z,null),e(Z,Kr),e(Z,Ua),e(Ua,Yr),e(Z,Qr),x(Ie,Z,null),e(S,Zr),e(S,ee),x(mt,ee,null),e(ee,eo),e(ee,Ga),e(Ga,to),e(ee,ao),x(Se,ee,null),e(S,so),e(S,te),x(ft,te,null),e(te,no),e(te,gt),e(gt,ro),e(gt,Ha),e(Ha,oo),e(gt,lo),e(te,io),x(Be,te,null),e(S,po),e(S,ae),x(ut,ae,null),e(ae,co),e(ae,Wa),e(Wa,mo),e(ae,fo),x(Ne,ae,null),e(S,go),e(S,se),x(_t,se,null),e(se,uo),e(se,Xa),e(Xa,_o),e(se,ho),x(Re,se,null),e(S,$o),e(S,ne),x(ht,ne,null),e(ne,vo),e(ne,Ja),e(Ja,bo),e(ne,xo),x(Ce,ne,null),e(S,wo),e(S,Pe),x($t,Pe,null),e(Pe,Eo),e(Pe,Ka),e(Ka,Do),$(s,Qs,h),$(s,O,h),x(vt,O,null),e(O,yo),e(O,Ut),e(Ut,Ya),e(Ya,To),e(Ut,jo),e(O,ko),e(O,bt),e(bt,Io),e(bt,Qa),e(Qa,So),e(bt,Bo),e(O,No),e(O,Za),e(Za,Ro),e(O,Co),e(O,xt),e(xt,es),e(es,me),e(me,Zs),e(me,Po),e(me,ts),e(ts,Ao),e(me,Lo),e(me,as),e(as,Oo),e(xt,Vo),e(xt,fe),e(fe,ge),e(ge,Gt),e(Gt,ss),e(ss,qo),e(Gt,Fo),e(ge,Mo),e(ge,ns),e(ns,zo),e(ge,Uo),e(ge,rs),e(rs,Go),e(fe,Ho),e(fe,ue),e(ue,os),e(os,ls),e(ls,Wo),e(ue,Xo),e(ue,ds),e(ds,Jo),e(ue,Ko),e(ue,is),e(is,Yo),e(fe,Qo),e(fe,_e),e(_e,ps),e(ps,cs),e(cs,Zo),e(_e,el),e(_e,ms),e(ms,tl),e(_e,al),e(_e,fs),e(fs,sl),$(s,en,h),$(s,M,h),x(wt,M,null),e(M,nl),e(M,gs),e(gs,rl),e(M,ol),e(M,he),e(he,ll),e(he,us),e(us,dl),e(he,il),e(he,_s),e(_s,pl),e(he,cl),e(M,ml),x(Ae,M,null),$(s,tn,h),$(s,N,h),x(Et,N,null),e(N,fl),e(N,Ht),e(Ht,hs),e(hs,gl),e(Ht,ul),e(N,_l),e(N,$s),e($s,hl),e(N,$l),e(N,H),e(H,Wt),e(Wt,vs),e(vs,vl),e(Wt,bl),e(H,xl),e(H,Xt),e(Xt,bs),e(bs,wl),e(Xt,El),e(H,Dl),e(H,Jt),e(Jt,xs),e(xs,yl),e(Jt,Tl),e(H,jl),e(H,Kt),e(Kt,ws),e(ws,kl),e(Kt,Il),e(N,Sl),e(N,Yt),e(Yt,Bl),e(Yt,Es),e(Es,Nl),e(N,Rl),e(N,Dt),e(Dt,Cl),e(Dt,Ds),e(Ds,Pl),e(Dt,Al),e(N,Ll),x(Le,N,null),$(s,an,h),$(s,B,h),x(yt,B,null),e(B,Ol),e(B,ys),e(ys,Vl),e(B,ql),x(Oe,B,null),e(B,Fl),e(B,Ts),e(Ts,Ml),e(B,zl),x(Ve,B,null),e(B,Ul),e(B,js),e(js,Gl),e(B,Hl),x(qe,B,null),e(B,Wl),x(Fe,B,null),$(s,sn,h),$(s,$e,h),x(Tt,$e,null),e($e,Xl),e($e,ks),e(ks,Jl),$(s,nn,h),$(s,V,h),x(jt,V,null),e(V,Kl),e(V,Is),e(Is,Yl),e(V,Ql),x(Me,V,null),e(V,Zl),e(V,ze),x(kt,ze,null),e(ze,ed),e(ze,Ss),e(Ss,td),e(V,ad),e(V,re),x(It,re,null),e(re,sd),e(re,Bs),e(Bs,nd),e(re,rd),e(re,Ns),e(Ns,od),$(s,rn,h),$(s,ve,h),x(St,ve,null),e(ve,ld),e(ve,Rs),e(Rs,dd),$(s,on,h),$(s,z,h),x(Bt,z,null),e(z,id),e(z,Cs),e(Cs,pd),e(z,cd),x(Ue,z,null),e(z,md),e(z,Ge),x(Nt,Ge,null),e(Ge,fd),e(Ge,Ps),e(Ps,gd),ln=!0},p(s,[h]){const Rt={};h&2&&(Rt.$$scope={dirty:h,ctx:s}),De.$set(Rt);const As={};h&2&&(As.$$scope={dirty:h,ctx:s}),Te.$set(As);const Ls={};h&2&&(Ls.$$scope={dirty:h,ctx:s}),je.$set(Ls);const Os={};h&2&&(Os.$$scope={dirty:h,ctx:s}),Ie.$set(Os);const be={};h&2&&(be.$$scope={dirty:h,ctx:s}),Se.$set(be);const Vs={};h&2&&(Vs.$$scope={dirty:h,ctx:s}),Be.$set(Vs);const qs={};h&2&&(qs.$$scope={dirty:h,ctx:s}),Ne.$set(qs);const I={};h&2&&(I.$$scope={dirty:h,ctx:s}),Re.$set(I);const Fs={};h&2&&(Fs.$$scope={dirty:h,ctx:s}),Ce.$set(Fs);const Qt={};h&2&&(Qt.$$scope={dirty:h,ctx:s}),Ae.$set(Qt);const Ms={};h&2&&(Ms.$$scope={dirty:h,ctx:s}),Le.$set(Ms);const xe={};h&2&&(xe.$$scope={dirty:h,ctx:s}),Oe.$set(xe);const Zt={};h&2&&(Zt.$$scope={dirty:h,ctx:s}),Ve.$set(Zt);const zs={};h&2&&(zs.$$scope={dirty:h,ctx:s}),qe.$set(zs);const ea={};h&2&&(ea.$$scope={dirty:h,ctx:s}),Fe.$set(ea);const Us={};h&2&&(Us.$$scope={dirty:h,ctx:s}),Me.$set(Us);const He={};h&2&&(He.$$scope={dirty:h,ctx:s}),Ue.$set(He)},i(s){ln||(w(t.$$.fragment,s),w(Ye.$$.fragment,s),w(Qe.$$.fragment,s),w(De.$$.fragment,s),w(Ze.$$.fragment,s),w(et.$$.fragment,s),w(Te.$$.fragment,s),w(tt.$$.fragment,s),w(je.$$.fragment,s),w(at.$$.fragment,s),w(st.$$.fragment,s),w(nt.$$.fragment,s),w(rt.$$.fragment,s),w(ot.$$.fragment,s),w(it.$$.fragment,s),w(pt.$$.fragment,s),w(ct.$$.fragment,s),w(Ie.$$.fragment,s),w(mt.$$.fragment,s),w(Se.$$.fragment,s),w(ft.$$.fragment,s),w(Be.$$.fragment,s),w(ut.$$.fragment,s),w(Ne.$$.fragment,s),w(_t.$$.fragment,s),w(Re.$$.fragment,s),w(ht.$$.fragment,s),w(Ce.$$.fragment,s),w($t.$$.fragment,s),w(vt.$$.fragment,s),w(wt.$$.fragment,s),w(Ae.$$.fragment,s),w(Et.$$.fragment,s),w(Le.$$.fragment,s),w(yt.$$.fragment,s),w(Oe.$$.fragment,s),w(Ve.$$.fragment,s),w(qe.$$.fragment,s),w(Fe.$$.fragment,s),w(Tt.$$.fragment,s),w(jt.$$.fragment,s),w(Me.$$.fragment,s),w(kt.$$.fragment,s),w(It.$$.fragment,s),w(St.$$.fragment,s),w(Bt.$$.fragment,s),w(Ue.$$.fragment,s),w(Nt.$$.fragment,s),ln=!0)},o(s){E(t.$$.fragment,s),E(Ye.$$.fragment,s),E(Qe.$$.fragment,s),E(De.$$.fragment,s),E(Ze.$$.fragment,s),E(et.$$.fragment,s),E(Te.$$.fragment,s),E(tt.$$.fragment,s),E(je.$$.fragment,s),E(at.$$.fragment,s),E(st.$$.fragment,s),E(nt.$$.fragment,s),E(rt.$$.fragment,s),E(ot.$$.fragment,s),E(it.$$.fragment,s),E(pt.$$.fragment,s),E(ct.$$.fragment,s),E(Ie.$$.fragment,s),E(mt.$$.fragment,s),E(Se.$$.fragment,s),E(ft.$$.fragment,s),E(Be.$$.fragment,s),E(ut.$$.fragment,s),E(Ne.$$.fragment,s),E(_t.$$.fragment,s),E(Re.$$.fragment,s),E(ht.$$.fragment,s),E(Ce.$$.fragment,s),E($t.$$.fragment,s),E(vt.$$.fragment,s),E(wt.$$.fragment,s),E(Ae.$$.fragment,s),E(Et.$$.fragment,s),E(Le.$$.fragment,s),E(yt.$$.fragment,s),E(Oe.$$.fragment,s),E(Ve.$$.fragment,s),E(qe.$$.fragment,s),E(Fe.$$.fragment,s),E(Tt.$$.fragment,s),E(jt.$$.fragment,s),E(Me.$$.fragment,s),E(kt.$$.fragment,s),E(It.$$.fragment,s),E(St.$$.fragment,s),E(Bt.$$.fragment,s),E(Ue.$$.fragment,s),E(Nt.$$.fragment,s),ln=!1},d(s){a(p),s&&a(_),s&&a(g),D(t),s&&a(Gs),s&&a(W),s&&a(Hs),s&&a(k),D(Ye),D(Qe),D(De),D(Ze),D(et),D(Te),D(tt),D(je),D(at),s&&a(Ws),s&&a(G),D(st),s&&a(Xs),s&&a(ie),D(nt),s&&a(Js),s&&a(pe),D(rt),s&&a(Ks),s&&a(F),D(ot),D(it),s&&a(Ys),s&&a(S),D(pt),D(ct),D(Ie),D(mt),D(Se),D(ft),D(Be),D(ut),D(Ne),D(_t),D(Re),D(ht),D(Ce),D($t),s&&a(Qs),s&&a(O),D(vt),s&&a(en),s&&a(M),D(wt),D(Ae),s&&a(tn),s&&a(N),D(Et),D(Le),s&&a(an),s&&a(B),D(yt),D(Oe),D(Ve),D(qe),D(Fe),s&&a(sn),s&&a($e),D(Tt),s&&a(nn),s&&a(V),D(jt),D(Me),D(kt),D(It),s&&a(rn),s&&a(ve),D(St),s&&a(on),s&&a(z),D(Bt),D(Ue),D(Nt)}}}const dp={local:"datasets.DatasetBuilder",title:"Builder classes"};function ip(T){return Mi(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class up extends Oi{constructor(p){super();Vi(this,p,ip,lp,qi,{})}}export{up as default,dp as metadata};
