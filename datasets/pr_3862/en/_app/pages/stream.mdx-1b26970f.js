import{S as Xp,i as Zp,s as ei,e as t,k as c,w as g,t as n,M as si,c as l,d as a,m as d,a as o,x as _,h as r,b as h,N as Qp,F as s,g as i,y as v,q as j,o as b,B as $}from"../chunks/vendor-e67aec41.js";import{T as tt}from"../chunks/Tip-76459d1c.js";import{I as L}from"../chunks/IconCopyLink-ffd7f84e.js";import{C as q}from"../chunks/CodeBlock-e2bcf023.js";import{C as ai}from"../chunks/CodeBlockFw-1e02e2ba.js";function ti(C){let f,w,m,x,y,u,E,D;return{c(){f=t("p"),w=n("An "),m=t("a"),x=n("datasets.IterableDataset"),y=n(" is useful for iterative jobs like training a model. You shouldn\u2019t use a "),u=t("a"),E=n("datasets.IterableDataset"),D=n(" for jobs that require random access to examples because you have to iterate all over it using a for loop. Getting the last example in an iterable dataset would require you to iterate over all the previous examples."),this.h()},l(A){f=l(A,"P",{});var k=o(f);w=r(k,"An "),m=l(k,"A",{href:!0});var I=o(m);x=r(I,"datasets.IterableDataset"),I.forEach(a),y=r(k," is useful for iterative jobs like training a model. You shouldn\u2019t use a "),u=l(k,"A",{href:!0});var z=o(u);E=r(z,"datasets.IterableDataset"),z.forEach(a),D=r(k," for jobs that require random access to examples because you have to iterate all over it using a for loop. Getting the last example in an iterable dataset would require you to iterate over all the previous examples."),k.forEach(a),this.h()},h(){h(m,"href","/docs/datasets/pr_3862/en/package_reference/main_classes#datasets.IterableDataset"),h(u,"href","/docs/datasets/pr_3862/en/package_reference/main_classes#datasets.IterableDataset")},m(A,k){i(A,f,k),s(f,w),s(f,m),s(m,x),s(f,y),s(f,u),s(u,E),s(f,D)},d(A){A&&a(f)}}}function li(C){let f,w,m,x;return{c(){f=t("p"),w=t("a"),m=n("datasets.IterableDataset.shuffle()"),x=n(" will also shuffle the order of the shards if the dataset is sharded into multiple sets."),this.h()},l(y){f=l(y,"P",{});var u=o(f);w=l(u,"A",{href:!0});var E=o(w);m=r(E,"datasets.IterableDataset.shuffle()"),E.forEach(a),x=r(u," will also shuffle the order of the shards if the dataset is sharded into multiple sets."),u.forEach(a),this.h()},h(){h(w,"href","/docs/datasets/pr_3862/en/package_reference/main_classes#datasets.IterableDataset.shuffle")},m(y,u){i(y,f,u),s(f,w),s(w,m),s(f,x)},d(y){y&&a(f)}}}function ni(C){let f,w,m,x,y,u,E,D,A,k,I,z,pe;return{c(){f=t("p"),w=t("code"),m=n("take"),x=n(" and "),y=t("code"),u=n("skip"),E=n(" prevent future calls to "),D=t("code"),A=n("shuffle"),k=n(" because they lock in the order of the shards. You should "),I=t("code"),z=n("shuffle"),pe=n(" your dataset before splitting it.")},l(S){f=l(S,"P",{});var T=o(f);w=l(T,"CODE",{});var bs=o(w);m=r(bs,"take"),bs.forEach(a),x=r(T," and "),y=l(T,"CODE",{});var $s=o(y);u=r($s,"skip"),$s.forEach(a),E=r(T," prevent future calls to "),D=l(T,"CODE",{});var ie=o(D);A=r(ie,"shuffle"),ie.forEach(a),k=r(T," because they lock in the order of the shards. You should "),I=l(T,"CODE",{});var ws=o(I);z=r(ws,"shuffle"),ws.forEach(a),pe=r(T," your dataset before splitting it."),T.forEach(a)},m(S,T){i(S,f,T),s(f,w),s(w,m),s(f,x),s(f,y),s(y,u),s(f,E),s(f,D),s(D,A),s(f,k),s(f,I),s(I,z),s(f,pe)},d(S){S&&a(f)}}}function ri(C){let f,w,m,x,y,u,E,D;return{c(){f=t("p"),w=n("Casting only works if the original feature type and new feature type are compatible. For example, you can cast a column with the feature type "),m=t("code"),x=n("Value('int32')"),y=n(" to "),u=t("code"),E=n("Value('bool')"),D=n(" if the original column only contains ones and zeros.")},l(A){f=l(A,"P",{});var k=o(f);w=r(k,"Casting only works if the original feature type and new feature type are compatible. For example, you can cast a column with the feature type "),m=l(k,"CODE",{});var I=o(m);x=r(I,"Value('int32')"),I.forEach(a),y=r(k," to "),u=l(k,"CODE",{});var z=o(u);E=r(z,"Value('bool')"),z.forEach(a),D=r(k," if the original column only contains ones and zeros."),k.forEach(a)},m(A,k){i(A,f,k),s(f,w),s(f,m),s(m,x),s(f,y),s(f,u),s(u,E),s(f,D)},d(A){A&&a(f)}}}function oi(C){let f,w,m,x,y;return{c(){f=t("p"),w=n("See other examples of batch processing in "),m=t("a"),x=n("the batched map processing documentation"),y=n(". They work the same for iterable datasets."),this.h()},l(u){f=l(u,"P",{});var E=o(f);w=r(E,"See other examples of batch processing in "),m=l(E,"A",{href:!0});var D=o(m);x=r(D,"the batched map processing documentation"),D.forEach(a),y=r(E,". They work the same for iterable datasets."),E.forEach(a),this.h()},h(){h(m,"href","./process#batch-processing")},m(u,E){i(u,f,E),s(f,w),s(f,m),s(m,x),s(f,y)},d(u){u&&a(f)}}}function pi(C){let f,w,m,x,y,u,E,D,A,k,I,z,pe,S,T,bs,$s,ie,ws,lt,W,xs,mo,Ll,ys,uo,nt,M,Ml,Fe,Ol,Vl,ra,Fl,Rl,ks,Hl,Yl,rt,Re,ot,H,Bl,Es,Ul,Wl,Ds,Jl,Gl,pt,he,it,J,ce,oa,He,Kl,pa,Ql,ht,O,Xl,As,Zl,en,Is,sn,an,Ts,tn,ln,ct,V,nn,ia,rn,on,ha,pn,hn,Ps,cn,dn,dt,Ye,ft,de,mt,G,fe,ca,Be,fn,da,mn,ut,me,un,fa,gn,_n,gt,ue,vn,ma,jn,bn,_t,Ue,vt,K,ge,ua,We,$n,ga,wn,jt,qs,xn,bt,Ss,_e,Cs,yn,kn,_a,En,Dn,$t,Je,wt,zs,ve,Ns,An,In,va,Tn,Pn,xt,Ge,yt,je,kt,Ls,Et,Q,be,ja,Ke,qn,ba,Sn,Dt,X,Ms,Cn,zn,Os,Nn,Ln,At,Qe,It,$e,Mn,$a,On,Vn,Tt,Xe,Pt,Y,Fn,wa,Rn,Hn,xa,Yn,Bn,qt,Z,we,ya,Ze,Un,ka,Wn,St,Vs,Jn,Ct,ee,xe,Ea,es,Gn,Da,Kn,zt,ye,Qn,Aa,Xn,Zn,Nt,ke,er,Ia,sr,ar,Lt,ss,Mt,se,Ee,Ta,as,tr,Pa,lr,Ot,De,nr,Fs,rr,or,Vt,ts,Ft,ae,Ae,qa,ls,pr,Sa,ir,Rt,N,Ca,hr,cr,za,dr,fr,Na,mr,ur,La,gr,_r,Ht,ns,Yt,Ie,Bt,Te,vr,Rs,jr,br,Ut,rs,Wt,te,Pe,Ma,os,$r,Oa,wr,Jt,P,xr,Hs,yr,kr,Ys,Er,Dr,Bs,Ar,Ir,Us,Tr,Pr,Ws,qr,Sr,Gt,Js,Cr,Kt,B,zr,Gs,Nr,Lr,Va,Mr,Or,Qt,ps,Xt,qe,Vr,Ks,Fr,Rr,Zt,is,el,Se,Hr,Qs,Yr,Br,sl,U,Ur,Fa,Wr,Jr,Xs,Gr,Kr,al,hs,tl,le,Ce,Ra,cs,Qr,Ha,Xr,ll,R,Zs,Zr,eo,Ya,so,ao,Ba,to,lo,nl,ne,ze,Ua,ds,no,Wa,ro,rl,fs,ol,Ne,pl,re,Le,Ja,ms,oo,Ga,po,il,us,ea,io,ho,hl,gs,cl,sa,co,dl,_s,fl;return u=new L({}),Re=new q({props:{code:`from datasets import load_dataset
dataset = load_dataset('oscar', "unshuffled_deduplicated_en", split='train', streaming=True)
print(next(iter(dataset)))`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;oscar&#x27;</span>, <span class="hljs-string">&quot;unshuffled_deduplicated_en&quot;</span>, split=<span class="hljs-string">&#x27;train&#x27;</span>, streaming=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(<span class="hljs-built_in">next</span>(<span class="hljs-built_in">iter</span>(dataset)))
{<span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;Mtendere Village was inspired by the vision of Chief Napoleon Dzombe, which he shared with John Blanchard during his first visit to Malawi. Chief Napoleon conveyed the desperate need for a program to intervene and care for the orphans and vulnerable children (OVC) in Malawi, and John committed to help...</span>`}}),he=new tt({props:{$$slots:{default:[ti]},$$scope:{ctx:C}}}),He=new L({}),Ye=new q({props:{code:`from datasets import load_dataset
dataset = load_dataset('oscar', "unshuffled_deduplicated_en", split='train', streaming=True)
shuffled_dataset = dataset.shuffle(seed=42, buffer_size=10_000)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;oscar&#x27;</span>, <span class="hljs-string">&quot;unshuffled_deduplicated_en&quot;</span>, split=<span class="hljs-string">&#x27;train&#x27;</span>, streaming=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>shuffled_dataset = dataset.shuffle(seed=<span class="hljs-number">42</span>, buffer_size=<span class="hljs-number">10_000</span>)`}}),de=new tt({props:{$$slots:{default:[li]},$$scope:{ctx:C}}}),Be=new L({}),Ue=new q({props:{code:`for epoch in range(epochs):
    shuffled_dataset.set_epoch(epoch)
    for example in shuffled_dataset:
        ...`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(epochs):
<span class="hljs-meta">... </span>    shuffled_dataset.set_epoch(epoch)
<span class="hljs-meta">... </span>    <span class="hljs-keyword">for</span> example <span class="hljs-keyword">in</span> shuffled_dataset:
<span class="hljs-meta">... </span>        ...`}}),We=new L({}),Je=new q({props:{code:`dataset = load_dataset('oscar', "unshuffled_deduplicated_en", split='train', streaming=True)
dataset_head = dataset.take(2)
list(dataset_head)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;oscar&#x27;</span>, <span class="hljs-string">&quot;unshuffled_deduplicated_en&quot;</span>, split=<span class="hljs-string">&#x27;train&#x27;</span>, streaming=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset_head = dataset.take(<span class="hljs-number">2</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">list</span>(dataset_head)
[{<span class="hljs-string">&#x27;id&#x27;</span>: <span class="hljs-number">0</span>, <span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;Mtendere Village was...&#x27;</span>}, {<span class="hljs-string">&#x27;id&#x27;</span>: <span class="hljs-number">1</span>, <span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;Lily James cannot fight the music...&#x27;</span>}]`}}),Ge=new q({props:{code:"train_dataset = shuffled_dataset.skip(1000)",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>train_dataset = shuffled_dataset.skip(<span class="hljs-number">1000</span>)'}}),je=new tt({props:{warning:"&lcub;true}",$$slots:{default:[ni]},$$scope:{ctx:C}}}),Ke=new L({}),Qe=new q({props:{code:`from datasets import interleave_datasets
en_dataset = load_dataset('oscar', "unshuffled_deduplicated_en", split='train', streaming=True)
fr_dataset = load_dataset('oscar', "unshuffled_deduplicated_fr", split='train', streaming=True)

multilingual_dataset = interleave_datasets([en_dataset, fr_dataset])
list(multilingual_dataset.take(2))`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> interleave_datasets
<span class="hljs-meta">&gt;&gt;&gt; </span>en_dataset = load_dataset(<span class="hljs-string">&#x27;oscar&#x27;</span>, <span class="hljs-string">&quot;unshuffled_deduplicated_en&quot;</span>, split=<span class="hljs-string">&#x27;train&#x27;</span>, streaming=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>fr_dataset = load_dataset(<span class="hljs-string">&#x27;oscar&#x27;</span>, <span class="hljs-string">&quot;unshuffled_deduplicated_fr&quot;</span>, split=<span class="hljs-string">&#x27;train&#x27;</span>, streaming=<span class="hljs-literal">True</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>multilingual_dataset = interleave_datasets([en_dataset, fr_dataset])
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">list</span>(multilingual_dataset.take(<span class="hljs-number">2</span>))
[{<span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;Mtendere Village was inspired by the vision...&#x27;</span>}, {<span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&quot;M\xE9dia de d\xE9bat d&#x27;id\xE9es, de culture et de litt\xE9rature...&quot;</span>}]`}}),Xe=new q({props:{code:`multilingual_dataset_with_oversampling = interleave_datasets([en_dataset, fr_dataset], probabilities=[0.8, 0.2], seed=42)
list(multilingual_dataset_with_oversampling.take(2))`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>multilingual_dataset_with_oversampling = interleave_datasets([en_dataset, fr_dataset], probabilities=[<span class="hljs-number">0.8</span>, <span class="hljs-number">0.2</span>], seed=<span class="hljs-number">42</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">list</span>(multilingual_dataset_with_oversampling.take(<span class="hljs-number">2</span>))
[{<span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;Mtendere Village was inspired by the vision...&#x27;</span>}, {<span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;Lily James cannot fight the music...&#x27;</span>}]`}}),Ze=new L({}),es=new L({}),ss=new q({props:{code:`from datasets import load_dataset
dataset = load_dataset('mc4', 'en', streaming=True, split='train')
dataset = dataset.rename_column("text", "content")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;mc4&#x27;</span>, <span class="hljs-string">&#x27;en&#x27;</span>, streaming=<span class="hljs-literal">True</span>, split=<span class="hljs-string">&#x27;train&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = dataset.rename_column(<span class="hljs-string">&quot;text&quot;</span>, <span class="hljs-string">&quot;content&quot;</span>)`}}),as=new L({}),ts=new q({props:{code:`from datasets import load_dataset
dataset = load_dataset('mc4', 'en', streaming=True, split='train')
dataset = dataset.remove_columns('timestamp')`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;mc4&#x27;</span>, <span class="hljs-string">&#x27;en&#x27;</span>, streaming=<span class="hljs-literal">True</span>, split=<span class="hljs-string">&#x27;train&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = dataset.remove_columns(<span class="hljs-string">&#x27;timestamp&#x27;</span>)`}}),ls=new L({}),ns=new q({props:{code:`from datasets import load_dataset
dataset = load_dataset('glue', 'mrpc', split='train')features

from datasets import ClassLabel, Value
new_features = dataset.features.copy()
new_features["label"] = ClassLabel(names=['negative', 'positive'])
new_features["idx"] = Value('int64')
dataset = dataset.cast(new_features)
dataset.features`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;glue&#x27;</span>, <span class="hljs-string">&#x27;mrpc&#x27;</span>, split=<span class="hljs-string">&#x27;train&#x27;</span>)features
{<span class="hljs-string">&#x27;sentence1&#x27;</span>: Value(dtype=<span class="hljs-string">&#x27;string&#x27;</span>, <span class="hljs-built_in">id</span>=<span class="hljs-literal">None</span>),
<span class="hljs-string">&#x27;sentence2&#x27;</span>: Value(dtype=<span class="hljs-string">&#x27;string&#x27;</span>, <span class="hljs-built_in">id</span>=<span class="hljs-literal">None</span>),
<span class="hljs-string">&#x27;label&#x27;</span>: ClassLabel(num_classes=<span class="hljs-number">2</span>, names=[<span class="hljs-string">&#x27;not_equivalent&#x27;</span>, <span class="hljs-string">&#x27;equivalent&#x27;</span>], names_file=<span class="hljs-literal">None</span>, <span class="hljs-built_in">id</span>=<span class="hljs-literal">None</span>),
<span class="hljs-string">&#x27;idx&#x27;</span>: Value(dtype=<span class="hljs-string">&#x27;int32&#x27;</span>, <span class="hljs-built_in">id</span>=<span class="hljs-literal">None</span>)}

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> ClassLabel, Value
<span class="hljs-meta">&gt;&gt;&gt; </span>new_features = dataset.features.copy()
<span class="hljs-meta">&gt;&gt;&gt; </span>new_features[<span class="hljs-string">&quot;label&quot;</span>] = ClassLabel(names=[<span class="hljs-string">&#x27;negative&#x27;</span>, <span class="hljs-string">&#x27;positive&#x27;</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>new_features[<span class="hljs-string">&quot;idx&quot;</span>] = Value(<span class="hljs-string">&#x27;int64&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = dataset.cast(new_features)
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset.features
{<span class="hljs-string">&#x27;sentence1&#x27;</span>: Value(dtype=<span class="hljs-string">&#x27;string&#x27;</span>, <span class="hljs-built_in">id</span>=<span class="hljs-literal">None</span>),
<span class="hljs-string">&#x27;sentence2&#x27;</span>: Value(dtype=<span class="hljs-string">&#x27;string&#x27;</span>, <span class="hljs-built_in">id</span>=<span class="hljs-literal">None</span>),
<span class="hljs-string">&#x27;label&#x27;</span>: ClassLabel(num_classes=<span class="hljs-number">2</span>, names=[<span class="hljs-string">&#x27;negative&#x27;</span>, <span class="hljs-string">&#x27;positive&#x27;</span>], names_file=<span class="hljs-literal">None</span>, <span class="hljs-built_in">id</span>=<span class="hljs-literal">None</span>),
<span class="hljs-string">&#x27;idx&#x27;</span>: Value(dtype=<span class="hljs-string">&#x27;int64&#x27;</span>, <span class="hljs-built_in">id</span>=<span class="hljs-literal">None</span>)}`}}),Ie=new tt({props:{$$slots:{default:[ri]},$$scope:{ctx:C}}}),rs=new q({props:{code:`dataset.features

dataset = dataset.cast_column("audio", Audio(sampling_rate=16000))
dataset.features`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>dataset.features
{<span class="hljs-string">&#x27;audio&#x27;</span>: Audio(sampling_rate=<span class="hljs-number">44100</span>, mono=<span class="hljs-literal">True</span>, <span class="hljs-built_in">id</span>=<span class="hljs-literal">None</span>)}

<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = dataset.cast_column(<span class="hljs-string">&quot;audio&quot;</span>, Audio(sampling_rate=<span class="hljs-number">16000</span>))
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset.features
{<span class="hljs-string">&#x27;audio&#x27;</span>: Audio(sampling_rate=<span class="hljs-number">16000</span>, mono=<span class="hljs-literal">True</span>, <span class="hljs-built_in">id</span>=<span class="hljs-literal">None</span>)}`}}),os=new L({}),ps=new q({props:{code:`def add_prefix(example):
    example['text'] = 'My text: ' + example['text']
    return example`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">def</span> <span class="hljs-title function_">add_prefix</span>(<span class="hljs-params">example</span>):
<span class="hljs-meta">... </span>    example[<span class="hljs-string">&#x27;text&#x27;</span>] = <span class="hljs-string">&#x27;My text: &#x27;</span> + example[<span class="hljs-string">&#x27;text&#x27;</span>]
<span class="hljs-meta">... </span>    <span class="hljs-keyword">return</span> example`}}),is=new q({props:{code:`from datasets import load_dataset
dataset = load_dataset('oscar', 'unshuffled_deduplicated_en', streaming=True, split='train')
updated_dataset = dataset.map(add_prefix)
list(updated_dataset.take(3))`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;oscar&#x27;</span>, <span class="hljs-string">&#x27;unshuffled_deduplicated_en&#x27;</span>, streaming=<span class="hljs-literal">True</span>, split=<span class="hljs-string">&#x27;train&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>updated_dataset = dataset.<span class="hljs-built_in">map</span>(add_prefix)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">list</span>(updated_dataset.take(<span class="hljs-number">3</span>))
[{<span class="hljs-string">&#x27;id&#x27;</span>: <span class="hljs-number">0</span>, <span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;My text: Mtendere Village was inspired by...&#x27;</span>},
 {<span class="hljs-string">&#x27;id&#x27;</span>: <span class="hljs-number">1</span>, <span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;My text: Lily James cannot fight the music...&#x27;</span>},
 {<span class="hljs-string">&#x27;id&#x27;</span>: <span class="hljs-number">2</span>, <span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;My text: &quot;I\\&#x27;d love to help kickstart...&#x27;</span>}]`}}),hs=new q({props:{code:`updated_dataset = dataset.map(add_prefix, remove_columns=["id"])
list(updated_dataset.take(3))`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>updated_dataset = dataset.<span class="hljs-built_in">map</span>(add_prefix, remove_columns=[<span class="hljs-string">&quot;id&quot;</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">list</span>(updated_dataset.take(<span class="hljs-number">3</span>))
[{<span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;My text: Mtendere Village was inspired by...&#x27;</span>},
 {<span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;My text: Lily James cannot fight the music...&#x27;</span>},
 {<span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;My text: &quot;I\\&#x27;d love to help kickstart...&#x27;</span>}]`}}),cs=new L({}),ds=new L({}),fs=new q({props:{code:`from datasets import load_dataset
from transformers import AutoTokenizer
dataset = load_dataset("mc4", "en", streaming=True, split="train")
tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')
def encode(examples):
    return tokenizer(examples['text'], truncation=True, padding='max_length')
dataset = dataset.map(encode, batched=True, remove_columns=["text", "timestamp", "url"])
next(iter(dataset))`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;mc4&quot;</span>, <span class="hljs-string">&quot;en&quot;</span>, streaming=<span class="hljs-literal">True</span>, split=<span class="hljs-string">&quot;train&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&#x27;distilbert-base-uncased&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">def</span> <span class="hljs-title function_">encode</span>(<span class="hljs-params">examples</span>):
<span class="hljs-meta">... </span>    <span class="hljs-keyword">return</span> tokenizer(examples[<span class="hljs-string">&#x27;text&#x27;</span>], truncation=<span class="hljs-literal">True</span>, padding=<span class="hljs-string">&#x27;max_length&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = dataset.<span class="hljs-built_in">map</span>(encode, batched=<span class="hljs-literal">True</span>, remove_columns=[<span class="hljs-string">&quot;text&quot;</span>, <span class="hljs-string">&quot;timestamp&quot;</span>, <span class="hljs-string">&quot;url&quot;</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">next</span>(<span class="hljs-built_in">iter</span>(dataset))
{<span class="hljs-string">&#x27;input_ids&#x27;</span>: <span class="hljs-number">101</span>, <span class="hljs-number">8466</span>, <span class="hljs-number">1018</span>, <span class="hljs-number">1010</span>, <span class="hljs-number">4029</span>, <span class="hljs-number">2475</span>, <span class="hljs-number">2062</span>, <span class="hljs-number">18558</span>, <span class="hljs-number">3100</span>, <span class="hljs-number">2061</span>, ...,<span class="hljs-number">1106</span>, <span class="hljs-number">3739</span>, <span class="hljs-number">102</span>],
<span class="hljs-string">&#x27;attention_mask&#x27;</span>: [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, ..., <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]}`}}),Ne=new tt({props:{$$slots:{default:[oi]},$$scope:{ctx:C}}}),ms=new L({}),gs=new q({props:{code:`buffer_size, seed = 10_000, 42
dataset = dataset.shuffle(buffer_size, seed)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>buffer_size, seed = <span class="hljs-number">10_000</span>, <span class="hljs-number">42</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = dataset.shuffle(buffer_size, seed)`}}),_s=new ai({props:{group1:{id:"pt",code:`import torch
from torch.utils.data import DataLoader
from transformers import AutoModelForMaskedLM, DataCollatorForLanguageModeling
from tqdm import tqdm
dataset = dataset.with_format("torch")
dataloader = DataLoader(dataset, collate_fn=DataCollatorForLanguageModeling(tokenizer))
device = 'cuda' if torch.cuda.is_available() else 'cpu' 
model = AutoModelForMaskedLM.from_pretrained("distilbert-base-uncased")
model.train().to(device)
optimizer = torch.optim.AdamW(params=model.parameters(), lr=1e-5)
for epoch in range(3):
    dataset.set_epoch(epoch)
    for i, batch in enumerate(tqdm(dataloader, total=5)):
        if i == 5:
            break
        batch = {k: v.to(device) for k, v in batch.items()}
        outputs = model(**batch)
        loss = outputs[0]
        loss.backward()
        optimizer.step()
        optimizer.zero_grad()
        if i % 10 == 0:
            print(f"loss: {loss}")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> DataLoader
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForMaskedLM, DataCollatorForLanguageModeling
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> tqdm <span class="hljs-keyword">import</span> tqdm
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = dataset.with_format(<span class="hljs-string">&quot;torch&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>dataloader = DataLoader(dataset, collate_fn=DataCollatorForLanguageModeling(tokenizer))
<span class="hljs-meta">&gt;&gt;&gt; </span>device = <span class="hljs-string">&#x27;cuda&#x27;</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">&#x27;cpu&#x27;</span> 
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMaskedLM.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.train().to(device)
<span class="hljs-meta">&gt;&gt;&gt; </span>optimizer = torch.optim.AdamW(params=model.parameters(), lr=<span class="hljs-number">1e-5</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">3</span>):
<span class="hljs-meta">... </span>    dataset.set_epoch(epoch)
<span class="hljs-meta">... </span>    <span class="hljs-keyword">for</span> i, batch <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(tqdm(dataloader, total=<span class="hljs-number">5</span>)):
<span class="hljs-meta">... </span>        <span class="hljs-keyword">if</span> i == <span class="hljs-number">5</span>:
<span class="hljs-meta">... </span>            <span class="hljs-keyword">break</span>
<span class="hljs-meta">... </span>        batch = {k: v.to(device) <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> batch.items()}
<span class="hljs-meta">... </span>        outputs = model(**batch)
<span class="hljs-meta">... </span>        loss = outputs[<span class="hljs-number">0</span>]
<span class="hljs-meta">... </span>        loss.backward()
<span class="hljs-meta">... </span>        optimizer.step()
<span class="hljs-meta">... </span>        optimizer.zero_grad()
<span class="hljs-meta">... </span>        <span class="hljs-keyword">if</span> i % <span class="hljs-number">10</span> == <span class="hljs-number">0</span>:
<span class="hljs-meta">... </span>            <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;loss: <span class="hljs-subst">{loss}</span>&quot;</span>)`},group2:{id:"tf",code:"# WIP",highlighted:'<span class="hljs-comment"># WIP</span>'}}}),{c(){f=t("meta"),w=c(),m=t("h1"),x=t("a"),y=t("span"),g(u.$$.fragment),E=c(),D=t("span"),A=n("Stream"),k=c(),I=t("p"),z=n("Dataset streaming lets you get started with a dataset without waiting for the entire dataset to download. The data is downloaded progressively as you iterate over the dataset. This is especially helpful when:"),pe=c(),S=t("ul"),T=t("li"),bs=n("You don\u2019t want to wait for an extremely large dataset to download."),$s=c(),ie=t("li"),ws=n("The dataset size exceeds the amount of disk space on your computer."),lt=c(),W=t("div"),xs=t("img"),Ll=c(),ys=t("img"),nt=c(),M=t("p"),Ml=n("For example, the English split of the "),Fe=t("a"),Ol=n("OSCAR"),Vl=n(" dataset is 1.2 terabytes, but you can use it instantly with streaming. Stream a dataset by setting "),ra=t("code"),Fl=n("streaming=True"),Rl=n(" in "),ks=t("a"),Hl=n("datasets.load_dataset()"),Yl=n(" as shown below:"),rt=c(),g(Re.$$.fragment),ot=c(),H=t("p"),Bl=n("Loading a dataset in streaming mode creates a new dataset type instance (instead of the classic "),Es=t("a"),Ul=n("datasets.Dataset"),Wl=n(" object), known as an "),Ds=t("a"),Jl=n("datasets.IterableDataset"),Gl=n(". This special type of dataset has its own set of processing methods shown below."),pt=c(),g(he.$$.fragment),it=c(),J=t("h2"),ce=t("a"),oa=t("span"),g(He.$$.fragment),Kl=c(),pa=t("span"),Ql=n("Shuffle"),ht=c(),O=t("p"),Xl=n("Like a regular "),As=t("a"),Zl=n("datasets.Dataset"),en=n(" object, you can also shuffle a "),Is=t("a"),sn=n("datasets.IterableDataset"),an=n(" with "),Ts=t("a"),tn=n("datasets.IterableDataset.shuffle()"),ln=n("."),ct=c(),V=t("p"),nn=n("The "),ia=t("code"),rn=n("buffer_size"),on=n(" argument controls the size of the buffer to randomly sample examples from. Let\u2019s say your dataset has one million examples, and you set the "),ha=t("code"),pn=n("buffer_size"),hn=n(" to ten thousand. "),Ps=t("a"),cn=n("datasets.IterableDataset.shuffle()"),dn=n(" will randomly select examples from the first ten thousand examples in the buffer. Selected examples in the buffer are replaced with new examples. By default, the buffer size is 1,000."),dt=c(),g(Ye.$$.fragment),ft=c(),g(de.$$.fragment),mt=c(),G=t("h2"),fe=t("a"),ca=t("span"),g(Be.$$.fragment),fn=c(),da=t("span"),mn=n("Reshuffle"),ut=c(),me=t("p"),un=n("Sometimes you may want to reshuffle the dataset after each epoch. This will require you to set a different seed for each epoch. Use "),fa=t("code"),gn=n("datasets.IterableDataset.set_epoch()"),_n=n("in between epochs to tell the dataset what epoch you\u2019re on."),gt=c(),ue=t("p"),vn=n("Your seed effectively becomes: "),ma=t("code"),jn=n("initial seed + current epoch"),bn=n("."),_t=c(),g(Ue.$$.fragment),vt=c(),K=t("h2"),ge=t("a"),ua=t("span"),g(We.$$.fragment),$n=c(),ga=t("span"),wn=n("Split dataset"),jt=c(),qs=t("p"),xn=n("You can split your dataset one of two ways:"),bt=c(),Ss=t("ul"),_e=t("li"),Cs=t("a"),yn=n("datasets.IterableDataset.take()"),kn=n(" returns the first "),_a=t("code"),En=n("n"),Dn=n(" examples in a dataset:"),$t=c(),g(Je.$$.fragment),wt=c(),zs=t("ul"),ve=t("li"),Ns=t("a"),An=n("datasets.IterableDataset.skip()"),In=n(" omits the first "),va=t("code"),Tn=n("n"),Pn=n(" examples in a dataset and returns the remaining examples:"),xt=c(),g(Ge.$$.fragment),yt=c(),g(je.$$.fragment),kt=c(),Ls=t("a"),Et=c(),Q=t("h2"),be=t("a"),ja=t("span"),g(Ke.$$.fragment),qn=c(),ba=t("span"),Sn=n("Interleave"),Dt=c(),X=t("p"),Ms=t("a"),Cn=n("datasets.interleave_datasets()"),zn=n(" can combine an "),Os=t("a"),Nn=n("datasets.IterableDataset"),Ln=n(" with other datasets. The combined dataset returns alternating examples from each of the original datasets."),At=c(),g(Qe.$$.fragment),It=c(),$e=t("p"),Mn=n("Define sampling probabilities from each of the original datasets for more control over how each of them are sampled and combined. Set the "),$a=t("code"),On=n("probabilities"),Vn=n(" argument with your desired sampling probabilities:"),Tt=c(),g(Xe.$$.fragment),Pt=c(),Y=t("p"),Fn=n("Around 80% of the final dataset is made of the "),wa=t("code"),Rn=n("en_dataset"),Hn=n(", and 20% of the "),xa=t("code"),Yn=n("fr_dataset"),Bn=n("."),qt=c(),Z=t("h2"),we=t("a"),ya=t("span"),g(Ze.$$.fragment),Un=c(),ka=t("span"),Wn=n("Rename, remove, cast, and flatten"),St=c(),Vs=t("p"),Jn=n("The following methods allow you to modify the columns of a dataset. These methods are useful for renaming or removing columns, changing columns to a new set of features, and flattening nested column structures."),Ct=c(),ee=t("h3"),xe=t("a"),Ea=t("span"),g(es.$$.fragment),Gn=c(),Da=t("span"),Kn=n("Rename"),zt=c(),ye=t("p"),Qn=n("Use "),Aa=t("code"),Xn=n("datasets.IterableDataset.rename_column()"),Zn=n("when you need to rename a column in your dataset. Features associated with the original column are actually moved under the new column name, instead of just replacing the original column in-place."),Nt=c(),ke=t("p"),er=n("Provide "),Ia=t("code"),sr=n("datasets.IterableDataset.rename_column()"),ar=n("with the name of the original column, and the new column name:"),Lt=c(),g(ss.$$.fragment),Mt=c(),se=t("h3"),Ee=t("a"),Ta=t("span"),g(as.$$.fragment),tr=c(),Pa=t("span"),lr=n("Remove"),Ot=c(),De=t("p"),nr=n("When you need to remove one or more columns, give "),Fs=t("a"),rr=n("datasets.IterableDataset.remove_columns()"),or=n(" the name of the column to remove. Remove more than one column by providing a list of column names:"),Vt=c(),g(ts.$$.fragment),Ft=c(),ae=t("h3"),Ae=t("a"),qa=t("span"),g(ls.$$.fragment),pr=c(),Sa=t("span"),ir=n("Cast"),Rt=c(),N=t("p"),Ca=t("code"),hr=n("datasets.IterableDataset.cast()"),cr=n("changes the feature type of one or more columns. This method takes your new "),za=t("code"),dr=n("datasets.Features"),fr=n(" as its argument. The following sample code shows how to change the feature types of "),Na=t("code"),mr=n("datasets.ClassLabel"),ur=n(" and "),La=t("code"),gr=n("datasets.Value"),_r=n(":"),Ht=c(),g(ns.$$.fragment),Yt=c(),g(Ie.$$.fragment),Bt=c(),Te=t("p"),vr=n("Use "),Rs=t("a"),jr=n("datasets.Dataset.cast_column()"),br=n(" to change the feature type of just one column. Pass the column name and its new feature type as arguments:"),Ut=c(),g(rs.$$.fragment),Wt=c(),te=t("h2"),Pe=t("a"),Ma=t("span"),g(os.$$.fragment),$r=c(),Oa=t("span"),wr=n("Map"),Jt=c(),P=t("p"),xr=n("Similar to the "),Hs=t("a"),yr=n("datasets.Dataset.map()"),kr=n(" function for a regular "),Ys=t("a"),Er=n("datasets.Dataset"),Dr=n(", \u{1F917}  Datasets features "),Bs=t("a"),Ar=n("datasets.IterableDataset.map()"),Ir=n(" for processing "),Us=t("a"),Tr=n("datasets.IterableDataset"),Pr=n(`\\s.
`),Ws=t("a"),qr=n("datasets.IterableDataset.map()"),Sr=n(" applies processing on-the-fly when examples are streamed."),Gt=c(),Js=t("p"),Cr=n("It allows you to apply a processing function to each example in a dataset, independently or in batches. This function can even create new rows and columns."),Kt=c(),B=t("p"),zr=n("The following example demonstrates how to tokenize a "),Gs=t("a"),Nr=n("datasets.IterableDataset"),Lr=n(". The function needs to accept and output a "),Va=t("code"),Mr=n("dict"),Or=n(":"),Qt=c(),g(ps.$$.fragment),Xt=c(),qe=t("p"),Vr=n("Next, apply this function to the dataset with "),Ks=t("a"),Fr=n("datasets.IterableDataset.map()"),Rr=n(":"),Zt=c(),g(is.$$.fragment),el=c(),Se=t("p"),Hr=n("Let\u2019s take a look at another example, except this time, you will remove a column with "),Qs=t("a"),Yr=n("datasets.IterableDataset.map()"),Br=n(". When you remove a column, it is only removed after the example has been provided to the mapped function. This allows the mapped function to use the content of the columns before they are removed."),sl=c(),U=t("p"),Ur=n("Specify the column to remove with the "),Fa=t("code"),Wr=n("remove_columns"),Jr=n(" argument in "),Xs=t("a"),Gr=n("datasets.IterableDataset.map()"),Kr=n(":"),al=c(),g(hs.$$.fragment),tl=c(),le=t("h3"),Ce=t("a"),Ra=t("span"),g(cs.$$.fragment),Qr=c(),Ha=t("span"),Xr=n("Batch processing"),ll=c(),R=t("p"),Zs=t("a"),Zr=n("datasets.IterableDataset.map()"),eo=n(" also supports working with batches of examples. Operate on batches by setting "),Ya=t("code"),so=n("batched=True"),ao=n(". The default batch size is 1000, but you can adjust it with the "),Ba=t("code"),to=n("batch_size"),lo=n(" argument. This opens the door to many interesting applications such as tokenization, splitting long sentences into shorter chunks, and data augmentation."),nl=c(),ne=t("h4"),ze=t("a"),Ua=t("span"),g(ds.$$.fragment),no=c(),Wa=t("span"),ro=n("Tokenization"),rl=c(),g(fs.$$.fragment),ol=c(),g(Ne.$$.fragment),pl=c(),re=t("h2"),Le=t("a"),Ja=t("span"),g(ms.$$.fragment),oo=c(),Ga=t("span"),po=n("Stream in a training loop"),il=c(),us=t("p"),ea=t("a"),io=n("datasets.IterableDataset"),ho=n(" can be integrated into a training loop. First, shuffle the dataset:"),hl=c(),g(gs.$$.fragment),cl=c(),sa=t("p"),co=n("Lastly, create a simple training loop and start training:"),dl=c(),g(_s.$$.fragment),this.h()},l(e){const p=si('[data-svelte="svelte-1phssyn"]',document.head);f=l(p,"META",{name:!0,content:!0}),p.forEach(a),w=d(e),m=l(e,"H1",{class:!0});var vs=o(m);x=l(vs,"A",{id:!0,class:!0,href:!0});var Ka=o(x);y=l(Ka,"SPAN",{});var Qa=o(y);_(u.$$.fragment,Qa),Qa.forEach(a),Ka.forEach(a),E=d(vs),D=l(vs,"SPAN",{});var Xa=o(D);A=r(Xa,"Stream"),Xa.forEach(a),vs.forEach(a),k=d(e),I=l(e,"P",{});var Za=o(I);z=r(Za,"Dataset streaming lets you get started with a dataset without waiting for the entire dataset to download. The data is downloaded progressively as you iterate over the dataset. This is especially helpful when:"),Za.forEach(a),pe=d(e),S=l(e,"UL",{});var ml=o(S);T=l(ml,"LI",{});var go=o(T);bs=r(go,"You don\u2019t want to wait for an extremely large dataset to download."),go.forEach(a),$s=d(ml),ie=l(ml,"LI",{});var _o=o(ie);ws=r(_o,"The dataset size exceeds the amount of disk space on your computer."),_o.forEach(a),ml.forEach(a),lt=d(e),W=l(e,"DIV",{class:!0});var ul=o(W);xs=l(ul,"IMG",{class:!0,src:!0}),Ll=d(ul),ys=l(ul,"IMG",{class:!0,src:!0}),ul.forEach(a),nt=d(e),M=l(e,"P",{});var Me=o(M);Ml=r(Me,"For example, the English split of the "),Fe=l(Me,"A",{href:!0,rel:!0});var vo=o(Fe);Ol=r(vo,"OSCAR"),vo.forEach(a),Vl=r(Me," dataset is 1.2 terabytes, but you can use it instantly with streaming. Stream a dataset by setting "),ra=l(Me,"CODE",{});var jo=o(ra);Fl=r(jo,"streaming=True"),jo.forEach(a),Rl=r(Me," in "),ks=l(Me,"A",{href:!0});var bo=o(ks);Hl=r(bo,"datasets.load_dataset()"),bo.forEach(a),Yl=r(Me," as shown below:"),Me.forEach(a),rt=d(e),_(Re.$$.fragment,e),ot=d(e),H=l(e,"P",{});var aa=o(H);Bl=r(aa,"Loading a dataset in streaming mode creates a new dataset type instance (instead of the classic "),Es=l(aa,"A",{href:!0});var $o=o(Es);Ul=r($o,"datasets.Dataset"),$o.forEach(a),Wl=r(aa," object), known as an "),Ds=l(aa,"A",{href:!0});var wo=o(Ds);Jl=r(wo,"datasets.IterableDataset"),wo.forEach(a),Gl=r(aa,". This special type of dataset has its own set of processing methods shown below."),aa.forEach(a),pt=d(e),_(he.$$.fragment,e),it=d(e),J=l(e,"H2",{class:!0});var gl=o(J);ce=l(gl,"A",{id:!0,class:!0,href:!0});var xo=o(ce);oa=l(xo,"SPAN",{});var yo=o(oa);_(He.$$.fragment,yo),yo.forEach(a),xo.forEach(a),Kl=d(gl),pa=l(gl,"SPAN",{});var ko=o(pa);Ql=r(ko,"Shuffle"),ko.forEach(a),gl.forEach(a),ht=d(e),O=l(e,"P",{});var Oe=o(O);Xl=r(Oe,"Like a regular "),As=l(Oe,"A",{href:!0});var Eo=o(As);Zl=r(Eo,"datasets.Dataset"),Eo.forEach(a),en=r(Oe," object, you can also shuffle a "),Is=l(Oe,"A",{href:!0});var Do=o(Is);sn=r(Do,"datasets.IterableDataset"),Do.forEach(a),an=r(Oe," with "),Ts=l(Oe,"A",{href:!0});var Ao=o(Ts);tn=r(Ao,"datasets.IterableDataset.shuffle()"),Ao.forEach(a),ln=r(Oe,"."),Oe.forEach(a),ct=d(e),V=l(e,"P",{});var Ve=o(V);nn=r(Ve,"The "),ia=l(Ve,"CODE",{});var Io=o(ia);rn=r(Io,"buffer_size"),Io.forEach(a),on=r(Ve," argument controls the size of the buffer to randomly sample examples from. Let\u2019s say your dataset has one million examples, and you set the "),ha=l(Ve,"CODE",{});var To=o(ha);pn=r(To,"buffer_size"),To.forEach(a),hn=r(Ve," to ten thousand. "),Ps=l(Ve,"A",{href:!0});var Po=o(Ps);cn=r(Po,"datasets.IterableDataset.shuffle()"),Po.forEach(a),dn=r(Ve," will randomly select examples from the first ten thousand examples in the buffer. Selected examples in the buffer are replaced with new examples. By default, the buffer size is 1,000."),Ve.forEach(a),dt=d(e),_(Ye.$$.fragment,e),ft=d(e),_(de.$$.fragment,e),mt=d(e),G=l(e,"H2",{class:!0});var _l=o(G);fe=l(_l,"A",{id:!0,class:!0,href:!0});var qo=o(fe);ca=l(qo,"SPAN",{});var So=o(ca);_(Be.$$.fragment,So),So.forEach(a),qo.forEach(a),fn=d(_l),da=l(_l,"SPAN",{});var Co=o(da);mn=r(Co,"Reshuffle"),Co.forEach(a),_l.forEach(a),ut=d(e),me=l(e,"P",{});var vl=o(me);un=r(vl,"Sometimes you may want to reshuffle the dataset after each epoch. This will require you to set a different seed for each epoch. Use "),fa=l(vl,"CODE",{});var zo=o(fa);gn=r(zo,"datasets.IterableDataset.set_epoch()"),zo.forEach(a),_n=r(vl,"in between epochs to tell the dataset what epoch you\u2019re on."),vl.forEach(a),gt=d(e),ue=l(e,"P",{});var jl=o(ue);vn=r(jl,"Your seed effectively becomes: "),ma=l(jl,"CODE",{});var No=o(ma);jn=r(No,"initial seed + current epoch"),No.forEach(a),bn=r(jl,"."),jl.forEach(a),_t=d(e),_(Ue.$$.fragment,e),vt=d(e),K=l(e,"H2",{class:!0});var bl=o(K);ge=l(bl,"A",{id:!0,class:!0,href:!0});var Lo=o(ge);ua=l(Lo,"SPAN",{});var Mo=o(ua);_(We.$$.fragment,Mo),Mo.forEach(a),Lo.forEach(a),$n=d(bl),ga=l(bl,"SPAN",{});var Oo=o(ga);wn=r(Oo,"Split dataset"),Oo.forEach(a),bl.forEach(a),jt=d(e),qs=l(e,"P",{});var Vo=o(qs);xn=r(Vo,"You can split your dataset one of two ways:"),Vo.forEach(a),bt=d(e),Ss=l(e,"UL",{});var Fo=o(Ss);_e=l(Fo,"LI",{});var et=o(_e);Cs=l(et,"A",{href:!0});var Ro=o(Cs);yn=r(Ro,"datasets.IterableDataset.take()"),Ro.forEach(a),kn=r(et," returns the first "),_a=l(et,"CODE",{});var Ho=o(_a);En=r(Ho,"n"),Ho.forEach(a),Dn=r(et," examples in a dataset:"),et.forEach(a),Fo.forEach(a),$t=d(e),_(Je.$$.fragment,e),wt=d(e),zs=l(e,"UL",{});var Yo=o(zs);ve=l(Yo,"LI",{});var st=o(ve);Ns=l(st,"A",{href:!0});var Bo=o(Ns);An=r(Bo,"datasets.IterableDataset.skip()"),Bo.forEach(a),In=r(st," omits the first "),va=l(st,"CODE",{});var Uo=o(va);Tn=r(Uo,"n"),Uo.forEach(a),Pn=r(st," examples in a dataset and returns the remaining examples:"),st.forEach(a),Yo.forEach(a),xt=d(e),_(Ge.$$.fragment,e),yt=d(e),_(je.$$.fragment,e),kt=d(e),Ls=l(e,"A",{id:!0}),o(Ls).forEach(a),Et=d(e),Q=l(e,"H2",{class:!0});var $l=o(Q);be=l($l,"A",{id:!0,class:!0,href:!0});var Wo=o(be);ja=l(Wo,"SPAN",{});var Jo=o(ja);_(Ke.$$.fragment,Jo),Jo.forEach(a),Wo.forEach(a),qn=d($l),ba=l($l,"SPAN",{});var Go=o(ba);Sn=r(Go,"Interleave"),Go.forEach(a),$l.forEach(a),Dt=d(e),X=l(e,"P",{});var at=o(X);Ms=l(at,"A",{href:!0});var Ko=o(Ms);Cn=r(Ko,"datasets.interleave_datasets()"),Ko.forEach(a),zn=r(at," can combine an "),Os=l(at,"A",{href:!0});var Qo=o(Os);Nn=r(Qo,"datasets.IterableDataset"),Qo.forEach(a),Ln=r(at," with other datasets. The combined dataset returns alternating examples from each of the original datasets."),at.forEach(a),At=d(e),_(Qe.$$.fragment,e),It=d(e),$e=l(e,"P",{});var wl=o($e);Mn=r(wl,"Define sampling probabilities from each of the original datasets for more control over how each of them are sampled and combined. Set the "),$a=l(wl,"CODE",{});var Xo=o($a);On=r(Xo,"probabilities"),Xo.forEach(a),Vn=r(wl," argument with your desired sampling probabilities:"),wl.forEach(a),Tt=d(e),_(Xe.$$.fragment,e),Pt=d(e),Y=l(e,"P",{});var ta=o(Y);Fn=r(ta,"Around 80% of the final dataset is made of the "),wa=l(ta,"CODE",{});var Zo=o(wa);Rn=r(Zo,"en_dataset"),Zo.forEach(a),Hn=r(ta,", and 20% of the "),xa=l(ta,"CODE",{});var ep=o(xa);Yn=r(ep,"fr_dataset"),ep.forEach(a),Bn=r(ta,"."),ta.forEach(a),qt=d(e),Z=l(e,"H2",{class:!0});var xl=o(Z);we=l(xl,"A",{id:!0,class:!0,href:!0});var sp=o(we);ya=l(sp,"SPAN",{});var ap=o(ya);_(Ze.$$.fragment,ap),ap.forEach(a),sp.forEach(a),Un=d(xl),ka=l(xl,"SPAN",{});var tp=o(ka);Wn=r(tp,"Rename, remove, cast, and flatten"),tp.forEach(a),xl.forEach(a),St=d(e),Vs=l(e,"P",{});var lp=o(Vs);Jn=r(lp,"The following methods allow you to modify the columns of a dataset. These methods are useful for renaming or removing columns, changing columns to a new set of features, and flattening nested column structures."),lp.forEach(a),Ct=d(e),ee=l(e,"H3",{class:!0});var yl=o(ee);xe=l(yl,"A",{id:!0,class:!0,href:!0});var np=o(xe);Ea=l(np,"SPAN",{});var rp=o(Ea);_(es.$$.fragment,rp),rp.forEach(a),np.forEach(a),Gn=d(yl),Da=l(yl,"SPAN",{});var op=o(Da);Kn=r(op,"Rename"),op.forEach(a),yl.forEach(a),zt=d(e),ye=l(e,"P",{});var kl=o(ye);Qn=r(kl,"Use "),Aa=l(kl,"CODE",{});var pp=o(Aa);Xn=r(pp,"datasets.IterableDataset.rename_column()"),pp.forEach(a),Zn=r(kl,"when you need to rename a column in your dataset. Features associated with the original column are actually moved under the new column name, instead of just replacing the original column in-place."),kl.forEach(a),Nt=d(e),ke=l(e,"P",{});var El=o(ke);er=r(El,"Provide "),Ia=l(El,"CODE",{});var ip=o(Ia);sr=r(ip,"datasets.IterableDataset.rename_column()"),ip.forEach(a),ar=r(El,"with the name of the original column, and the new column name:"),El.forEach(a),Lt=d(e),_(ss.$$.fragment,e),Mt=d(e),se=l(e,"H3",{class:!0});var Dl=o(se);Ee=l(Dl,"A",{id:!0,class:!0,href:!0});var hp=o(Ee);Ta=l(hp,"SPAN",{});var cp=o(Ta);_(as.$$.fragment,cp),cp.forEach(a),hp.forEach(a),tr=d(Dl),Pa=l(Dl,"SPAN",{});var dp=o(Pa);lr=r(dp,"Remove"),dp.forEach(a),Dl.forEach(a),Ot=d(e),De=l(e,"P",{});var Al=o(De);nr=r(Al,"When you need to remove one or more columns, give "),Fs=l(Al,"A",{href:!0});var fp=o(Fs);rr=r(fp,"datasets.IterableDataset.remove_columns()"),fp.forEach(a),or=r(Al," the name of the column to remove. Remove more than one column by providing a list of column names:"),Al.forEach(a),Vt=d(e),_(ts.$$.fragment,e),Ft=d(e),ae=l(e,"H3",{class:!0});var Il=o(ae);Ae=l(Il,"A",{id:!0,class:!0,href:!0});var mp=o(Ae);qa=l(mp,"SPAN",{});var up=o(qa);_(ls.$$.fragment,up),up.forEach(a),mp.forEach(a),pr=d(Il),Sa=l(Il,"SPAN",{});var gp=o(Sa);ir=r(gp,"Cast"),gp.forEach(a),Il.forEach(a),Rt=d(e),N=l(e,"P",{});var oe=o(N);Ca=l(oe,"CODE",{});var _p=o(Ca);hr=r(_p,"datasets.IterableDataset.cast()"),_p.forEach(a),cr=r(oe,"changes the feature type of one or more columns. This method takes your new "),za=l(oe,"CODE",{});var vp=o(za);dr=r(vp,"datasets.Features"),vp.forEach(a),fr=r(oe," as its argument. The following sample code shows how to change the feature types of "),Na=l(oe,"CODE",{});var jp=o(Na);mr=r(jp,"datasets.ClassLabel"),jp.forEach(a),ur=r(oe," and "),La=l(oe,"CODE",{});var bp=o(La);gr=r(bp,"datasets.Value"),bp.forEach(a),_r=r(oe,":"),oe.forEach(a),Ht=d(e),_(ns.$$.fragment,e),Yt=d(e),_(Ie.$$.fragment,e),Bt=d(e),Te=l(e,"P",{});var Tl=o(Te);vr=r(Tl,"Use "),Rs=l(Tl,"A",{href:!0});var $p=o(Rs);jr=r($p,"datasets.Dataset.cast_column()"),$p.forEach(a),br=r(Tl," to change the feature type of just one column. Pass the column name and its new feature type as arguments:"),Tl.forEach(a),Ut=d(e),_(rs.$$.fragment,e),Wt=d(e),te=l(e,"H2",{class:!0});var Pl=o(te);Pe=l(Pl,"A",{id:!0,class:!0,href:!0});var wp=o(Pe);Ma=l(wp,"SPAN",{});var xp=o(Ma);_(os.$$.fragment,xp),xp.forEach(a),wp.forEach(a),$r=d(Pl),Oa=l(Pl,"SPAN",{});var yp=o(Oa);wr=r(yp,"Map"),yp.forEach(a),Pl.forEach(a),Jt=d(e),P=l(e,"P",{});var F=o(P);xr=r(F,"Similar to the "),Hs=l(F,"A",{href:!0});var kp=o(Hs);yr=r(kp,"datasets.Dataset.map()"),kp.forEach(a),kr=r(F," function for a regular "),Ys=l(F,"A",{href:!0});var Ep=o(Ys);Er=r(Ep,"datasets.Dataset"),Ep.forEach(a),Dr=r(F,", \u{1F917}  Datasets features "),Bs=l(F,"A",{href:!0});var Dp=o(Bs);Ar=r(Dp,"datasets.IterableDataset.map()"),Dp.forEach(a),Ir=r(F," for processing "),Us=l(F,"A",{href:!0});var Ap=o(Us);Tr=r(Ap,"datasets.IterableDataset"),Ap.forEach(a),Pr=r(F,`\\s.
`),Ws=l(F,"A",{href:!0});var Ip=o(Ws);qr=r(Ip,"datasets.IterableDataset.map()"),Ip.forEach(a),Sr=r(F," applies processing on-the-fly when examples are streamed."),F.forEach(a),Gt=d(e),Js=l(e,"P",{});var Tp=o(Js);Cr=r(Tp,"It allows you to apply a processing function to each example in a dataset, independently or in batches. This function can even create new rows and columns."),Tp.forEach(a),Kt=d(e),B=l(e,"P",{});var la=o(B);zr=r(la,"The following example demonstrates how to tokenize a "),Gs=l(la,"A",{href:!0});var Pp=o(Gs);Nr=r(Pp,"datasets.IterableDataset"),Pp.forEach(a),Lr=r(la,". The function needs to accept and output a "),Va=l(la,"CODE",{});var qp=o(Va);Mr=r(qp,"dict"),qp.forEach(a),Or=r(la,":"),la.forEach(a),Qt=d(e),_(ps.$$.fragment,e),Xt=d(e),qe=l(e,"P",{});var ql=o(qe);Vr=r(ql,"Next, apply this function to the dataset with "),Ks=l(ql,"A",{href:!0});var Sp=o(Ks);Fr=r(Sp,"datasets.IterableDataset.map()"),Sp.forEach(a),Rr=r(ql,":"),ql.forEach(a),Zt=d(e),_(is.$$.fragment,e),el=d(e),Se=l(e,"P",{});var Sl=o(Se);Hr=r(Sl,"Let\u2019s take a look at another example, except this time, you will remove a column with "),Qs=l(Sl,"A",{href:!0});var Cp=o(Qs);Yr=r(Cp,"datasets.IterableDataset.map()"),Cp.forEach(a),Br=r(Sl,". When you remove a column, it is only removed after the example has been provided to the mapped function. This allows the mapped function to use the content of the columns before they are removed."),Sl.forEach(a),sl=d(e),U=l(e,"P",{});var na=o(U);Ur=r(na,"Specify the column to remove with the "),Fa=l(na,"CODE",{});var zp=o(Fa);Wr=r(zp,"remove_columns"),zp.forEach(a),Jr=r(na," argument in "),Xs=l(na,"A",{href:!0});var Np=o(Xs);Gr=r(Np,"datasets.IterableDataset.map()"),Np.forEach(a),Kr=r(na,":"),na.forEach(a),al=d(e),_(hs.$$.fragment,e),tl=d(e),le=l(e,"H3",{class:!0});var Cl=o(le);Ce=l(Cl,"A",{id:!0,class:!0,href:!0});var Lp=o(Ce);Ra=l(Lp,"SPAN",{});var Mp=o(Ra);_(cs.$$.fragment,Mp),Mp.forEach(a),Lp.forEach(a),Qr=d(Cl),Ha=l(Cl,"SPAN",{});var Op=o(Ha);Xr=r(Op,"Batch processing"),Op.forEach(a),Cl.forEach(a),ll=d(e),R=l(e,"P",{});var js=o(R);Zs=l(js,"A",{href:!0});var Vp=o(Zs);Zr=r(Vp,"datasets.IterableDataset.map()"),Vp.forEach(a),eo=r(js," also supports working with batches of examples. Operate on batches by setting "),Ya=l(js,"CODE",{});var Fp=o(Ya);so=r(Fp,"batched=True"),Fp.forEach(a),ao=r(js,". The default batch size is 1000, but you can adjust it with the "),Ba=l(js,"CODE",{});var Rp=o(Ba);to=r(Rp,"batch_size"),Rp.forEach(a),lo=r(js," argument. This opens the door to many interesting applications such as tokenization, splitting long sentences into shorter chunks, and data augmentation."),js.forEach(a),nl=d(e),ne=l(e,"H4",{class:!0});var zl=o(ne);ze=l(zl,"A",{id:!0,class:!0,href:!0});var Hp=o(ze);Ua=l(Hp,"SPAN",{});var Yp=o(Ua);_(ds.$$.fragment,Yp),Yp.forEach(a),Hp.forEach(a),no=d(zl),Wa=l(zl,"SPAN",{});var Bp=o(Wa);ro=r(Bp,"Tokenization"),Bp.forEach(a),zl.forEach(a),rl=d(e),_(fs.$$.fragment,e),ol=d(e),_(Ne.$$.fragment,e),pl=d(e),re=l(e,"H2",{class:!0});var Nl=o(re);Le=l(Nl,"A",{id:!0,class:!0,href:!0});var Up=o(Le);Ja=l(Up,"SPAN",{});var Wp=o(Ja);_(ms.$$.fragment,Wp),Wp.forEach(a),Up.forEach(a),oo=d(Nl),Ga=l(Nl,"SPAN",{});var Jp=o(Ga);po=r(Jp,"Stream in a training loop"),Jp.forEach(a),Nl.forEach(a),il=d(e),us=l(e,"P",{});var fo=o(us);ea=l(fo,"A",{href:!0});var Gp=o(ea);io=r(Gp,"datasets.IterableDataset"),Gp.forEach(a),ho=r(fo," can be integrated into a training loop. First, shuffle the dataset:"),fo.forEach(a),hl=d(e),_(gs.$$.fragment,e),cl=d(e),sa=l(e,"P",{});var Kp=o(sa);co=r(Kp,"Lastly, create a simple training loop and start training:"),Kp.forEach(a),dl=d(e),_(_s.$$.fragment,e),this.h()},h(){h(f,"name","hf:doc:metadata"),h(f,"content",JSON.stringify(ii)),h(x,"id","stream"),h(x,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(x,"href","#stream"),h(m,"class","relative group"),h(xs,"class","block dark:hidden"),Qp(xs.src,mo="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/datasets/streaming.gif")||h(xs,"src",mo),h(ys,"class","hidden dark:block"),Qp(ys.src,uo="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/datasets/streaming-dark.gif")||h(ys,"src",uo),h(W,"class","flex justify-center"),h(Fe,"href","https://huggingface.co/datasets/oscar"),h(Fe,"rel","nofollow"),h(ks,"href","/docs/datasets/pr_3862/en/package_reference/loading_methods#datasets.load_dataset"),h(Es,"href","/docs/datasets/pr_3862/en/package_reference/main_classes#datasets.Dataset"),h(Ds,"href","/docs/datasets/pr_3862/en/package_reference/main_classes#datasets.IterableDataset"),h(ce,"id","shuffle"),h(ce,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(ce,"href","#shuffle"),h(J,"class","relative group"),h(As,"href","/docs/datasets/pr_3862/en/package_reference/main_classes#datasets.Dataset"),h(Is,"href","/docs/datasets/pr_3862/en/package_reference/main_classes#datasets.IterableDataset"),h(Ts,"href","/docs/datasets/pr_3862/en/package_reference/main_classes#datasets.IterableDataset.shuffle"),h(Ps,"href","/docs/datasets/pr_3862/en/package_reference/main_classes#datasets.IterableDataset.shuffle"),h(fe,"id","reshuffle"),h(fe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(fe,"href","#reshuffle"),h(G,"class","relative group"),h(ge,"id","split-dataset"),h(ge,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(ge,"href","#split-dataset"),h(K,"class","relative group"),h(Cs,"href","/docs/datasets/pr_3862/en/package_reference/main_classes#datasets.IterableDataset.take"),h(Ns,"href","/docs/datasets/pr_3862/en/package_reference/main_classes#datasets.IterableDataset.skip"),h(Ls,"id","interleave_datasets"),h(be,"id","interleave"),h(be,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(be,"href","#interleave"),h(Q,"class","relative group"),h(Ms,"href","/docs/datasets/pr_3862/en/package_reference/main_classes#datasets.interleave_datasets"),h(Os,"href","/docs/datasets/pr_3862/en/package_reference/main_classes#datasets.IterableDataset"),h(we,"id","rename-remove-cast-and-flatten"),h(we,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(we,"href","#rename-remove-cast-and-flatten"),h(Z,"class","relative group"),h(xe,"id","rename"),h(xe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(xe,"href","#rename"),h(ee,"class","relative group"),h(Ee,"id","remove"),h(Ee,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(Ee,"href","#remove"),h(se,"class","relative group"),h(Fs,"href","/docs/datasets/pr_3862/en/package_reference/main_classes#datasets.IterableDataset.remove_columns"),h(Ae,"id","cast"),h(Ae,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(Ae,"href","#cast"),h(ae,"class","relative group"),h(Rs,"href","/docs/datasets/pr_3862/en/package_reference/main_classes#datasets.Dataset.cast_column"),h(Pe,"id","map"),h(Pe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(Pe,"href","#map"),h(te,"class","relative group"),h(Hs,"href","/docs/datasets/pr_3862/en/package_reference/main_classes#datasets.Dataset.map"),h(Ys,"href","/docs/datasets/pr_3862/en/package_reference/main_classes#datasets.Dataset"),h(Bs,"href","/docs/datasets/pr_3862/en/package_reference/main_classes#datasets.IterableDataset.map"),h(Us,"href","/docs/datasets/pr_3862/en/package_reference/main_classes#datasets.IterableDataset"),h(Ws,"href","/docs/datasets/pr_3862/en/package_reference/main_classes#datasets.IterableDataset.map"),h(Gs,"href","/docs/datasets/pr_3862/en/package_reference/main_classes#datasets.IterableDataset"),h(Ks,"href","/docs/datasets/pr_3862/en/package_reference/main_classes#datasets.IterableDataset.map"),h(Qs,"href","/docs/datasets/pr_3862/en/package_reference/main_classes#datasets.IterableDataset.map"),h(Xs,"href","/docs/datasets/pr_3862/en/package_reference/main_classes#datasets.IterableDataset.map"),h(Ce,"id","batch-processing"),h(Ce,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(Ce,"href","#batch-processing"),h(le,"class","relative group"),h(Zs,"href","/docs/datasets/pr_3862/en/package_reference/main_classes#datasets.IterableDataset.map"),h(ze,"id","tokenization"),h(ze,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(ze,"href","#tokenization"),h(ne,"class","relative group"),h(Le,"id","stream-in-a-training-loop"),h(Le,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(Le,"href","#stream-in-a-training-loop"),h(re,"class","relative group"),h(ea,"href","/docs/datasets/pr_3862/en/package_reference/main_classes#datasets.IterableDataset")},m(e,p){s(document.head,f),i(e,w,p),i(e,m,p),s(m,x),s(x,y),v(u,y,null),s(m,E),s(m,D),s(D,A),i(e,k,p),i(e,I,p),s(I,z),i(e,pe,p),i(e,S,p),s(S,T),s(T,bs),s(S,$s),s(S,ie),s(ie,ws),i(e,lt,p),i(e,W,p),s(W,xs),s(W,Ll),s(W,ys),i(e,nt,p),i(e,M,p),s(M,Ml),s(M,Fe),s(Fe,Ol),s(M,Vl),s(M,ra),s(ra,Fl),s(M,Rl),s(M,ks),s(ks,Hl),s(M,Yl),i(e,rt,p),v(Re,e,p),i(e,ot,p),i(e,H,p),s(H,Bl),s(H,Es),s(Es,Ul),s(H,Wl),s(H,Ds),s(Ds,Jl),s(H,Gl),i(e,pt,p),v(he,e,p),i(e,it,p),i(e,J,p),s(J,ce),s(ce,oa),v(He,oa,null),s(J,Kl),s(J,pa),s(pa,Ql),i(e,ht,p),i(e,O,p),s(O,Xl),s(O,As),s(As,Zl),s(O,en),s(O,Is),s(Is,sn),s(O,an),s(O,Ts),s(Ts,tn),s(O,ln),i(e,ct,p),i(e,V,p),s(V,nn),s(V,ia),s(ia,rn),s(V,on),s(V,ha),s(ha,pn),s(V,hn),s(V,Ps),s(Ps,cn),s(V,dn),i(e,dt,p),v(Ye,e,p),i(e,ft,p),v(de,e,p),i(e,mt,p),i(e,G,p),s(G,fe),s(fe,ca),v(Be,ca,null),s(G,fn),s(G,da),s(da,mn),i(e,ut,p),i(e,me,p),s(me,un),s(me,fa),s(fa,gn),s(me,_n),i(e,gt,p),i(e,ue,p),s(ue,vn),s(ue,ma),s(ma,jn),s(ue,bn),i(e,_t,p),v(Ue,e,p),i(e,vt,p),i(e,K,p),s(K,ge),s(ge,ua),v(We,ua,null),s(K,$n),s(K,ga),s(ga,wn),i(e,jt,p),i(e,qs,p),s(qs,xn),i(e,bt,p),i(e,Ss,p),s(Ss,_e),s(_e,Cs),s(Cs,yn),s(_e,kn),s(_e,_a),s(_a,En),s(_e,Dn),i(e,$t,p),v(Je,e,p),i(e,wt,p),i(e,zs,p),s(zs,ve),s(ve,Ns),s(Ns,An),s(ve,In),s(ve,va),s(va,Tn),s(ve,Pn),i(e,xt,p),v(Ge,e,p),i(e,yt,p),v(je,e,p),i(e,kt,p),i(e,Ls,p),i(e,Et,p),i(e,Q,p),s(Q,be),s(be,ja),v(Ke,ja,null),s(Q,qn),s(Q,ba),s(ba,Sn),i(e,Dt,p),i(e,X,p),s(X,Ms),s(Ms,Cn),s(X,zn),s(X,Os),s(Os,Nn),s(X,Ln),i(e,At,p),v(Qe,e,p),i(e,It,p),i(e,$e,p),s($e,Mn),s($e,$a),s($a,On),s($e,Vn),i(e,Tt,p),v(Xe,e,p),i(e,Pt,p),i(e,Y,p),s(Y,Fn),s(Y,wa),s(wa,Rn),s(Y,Hn),s(Y,xa),s(xa,Yn),s(Y,Bn),i(e,qt,p),i(e,Z,p),s(Z,we),s(we,ya),v(Ze,ya,null),s(Z,Un),s(Z,ka),s(ka,Wn),i(e,St,p),i(e,Vs,p),s(Vs,Jn),i(e,Ct,p),i(e,ee,p),s(ee,xe),s(xe,Ea),v(es,Ea,null),s(ee,Gn),s(ee,Da),s(Da,Kn),i(e,zt,p),i(e,ye,p),s(ye,Qn),s(ye,Aa),s(Aa,Xn),s(ye,Zn),i(e,Nt,p),i(e,ke,p),s(ke,er),s(ke,Ia),s(Ia,sr),s(ke,ar),i(e,Lt,p),v(ss,e,p),i(e,Mt,p),i(e,se,p),s(se,Ee),s(Ee,Ta),v(as,Ta,null),s(se,tr),s(se,Pa),s(Pa,lr),i(e,Ot,p),i(e,De,p),s(De,nr),s(De,Fs),s(Fs,rr),s(De,or),i(e,Vt,p),v(ts,e,p),i(e,Ft,p),i(e,ae,p),s(ae,Ae),s(Ae,qa),v(ls,qa,null),s(ae,pr),s(ae,Sa),s(Sa,ir),i(e,Rt,p),i(e,N,p),s(N,Ca),s(Ca,hr),s(N,cr),s(N,za),s(za,dr),s(N,fr),s(N,Na),s(Na,mr),s(N,ur),s(N,La),s(La,gr),s(N,_r),i(e,Ht,p),v(ns,e,p),i(e,Yt,p),v(Ie,e,p),i(e,Bt,p),i(e,Te,p),s(Te,vr),s(Te,Rs),s(Rs,jr),s(Te,br),i(e,Ut,p),v(rs,e,p),i(e,Wt,p),i(e,te,p),s(te,Pe),s(Pe,Ma),v(os,Ma,null),s(te,$r),s(te,Oa),s(Oa,wr),i(e,Jt,p),i(e,P,p),s(P,xr),s(P,Hs),s(Hs,yr),s(P,kr),s(P,Ys),s(Ys,Er),s(P,Dr),s(P,Bs),s(Bs,Ar),s(P,Ir),s(P,Us),s(Us,Tr),s(P,Pr),s(P,Ws),s(Ws,qr),s(P,Sr),i(e,Gt,p),i(e,Js,p),s(Js,Cr),i(e,Kt,p),i(e,B,p),s(B,zr),s(B,Gs),s(Gs,Nr),s(B,Lr),s(B,Va),s(Va,Mr),s(B,Or),i(e,Qt,p),v(ps,e,p),i(e,Xt,p),i(e,qe,p),s(qe,Vr),s(qe,Ks),s(Ks,Fr),s(qe,Rr),i(e,Zt,p),v(is,e,p),i(e,el,p),i(e,Se,p),s(Se,Hr),s(Se,Qs),s(Qs,Yr),s(Se,Br),i(e,sl,p),i(e,U,p),s(U,Ur),s(U,Fa),s(Fa,Wr),s(U,Jr),s(U,Xs),s(Xs,Gr),s(U,Kr),i(e,al,p),v(hs,e,p),i(e,tl,p),i(e,le,p),s(le,Ce),s(Ce,Ra),v(cs,Ra,null),s(le,Qr),s(le,Ha),s(Ha,Xr),i(e,ll,p),i(e,R,p),s(R,Zs),s(Zs,Zr),s(R,eo),s(R,Ya),s(Ya,so),s(R,ao),s(R,Ba),s(Ba,to),s(R,lo),i(e,nl,p),i(e,ne,p),s(ne,ze),s(ze,Ua),v(ds,Ua,null),s(ne,no),s(ne,Wa),s(Wa,ro),i(e,rl,p),v(fs,e,p),i(e,ol,p),v(Ne,e,p),i(e,pl,p),i(e,re,p),s(re,Le),s(Le,Ja),v(ms,Ja,null),s(re,oo),s(re,Ga),s(Ga,po),i(e,il,p),i(e,us,p),s(us,ea),s(ea,io),s(us,ho),i(e,hl,p),v(gs,e,p),i(e,cl,p),i(e,sa,p),s(sa,co),i(e,dl,p),v(_s,e,p),fl=!0},p(e,[p]){const vs={};p&2&&(vs.$$scope={dirty:p,ctx:e}),he.$set(vs);const Ka={};p&2&&(Ka.$$scope={dirty:p,ctx:e}),de.$set(Ka);const Qa={};p&2&&(Qa.$$scope={dirty:p,ctx:e}),je.$set(Qa);const Xa={};p&2&&(Xa.$$scope={dirty:p,ctx:e}),Ie.$set(Xa);const Za={};p&2&&(Za.$$scope={dirty:p,ctx:e}),Ne.$set(Za)},i(e){fl||(j(u.$$.fragment,e),j(Re.$$.fragment,e),j(he.$$.fragment,e),j(He.$$.fragment,e),j(Ye.$$.fragment,e),j(de.$$.fragment,e),j(Be.$$.fragment,e),j(Ue.$$.fragment,e),j(We.$$.fragment,e),j(Je.$$.fragment,e),j(Ge.$$.fragment,e),j(je.$$.fragment,e),j(Ke.$$.fragment,e),j(Qe.$$.fragment,e),j(Xe.$$.fragment,e),j(Ze.$$.fragment,e),j(es.$$.fragment,e),j(ss.$$.fragment,e),j(as.$$.fragment,e),j(ts.$$.fragment,e),j(ls.$$.fragment,e),j(ns.$$.fragment,e),j(Ie.$$.fragment,e),j(rs.$$.fragment,e),j(os.$$.fragment,e),j(ps.$$.fragment,e),j(is.$$.fragment,e),j(hs.$$.fragment,e),j(cs.$$.fragment,e),j(ds.$$.fragment,e),j(fs.$$.fragment,e),j(Ne.$$.fragment,e),j(ms.$$.fragment,e),j(gs.$$.fragment,e),j(_s.$$.fragment,e),fl=!0)},o(e){b(u.$$.fragment,e),b(Re.$$.fragment,e),b(he.$$.fragment,e),b(He.$$.fragment,e),b(Ye.$$.fragment,e),b(de.$$.fragment,e),b(Be.$$.fragment,e),b(Ue.$$.fragment,e),b(We.$$.fragment,e),b(Je.$$.fragment,e),b(Ge.$$.fragment,e),b(je.$$.fragment,e),b(Ke.$$.fragment,e),b(Qe.$$.fragment,e),b(Xe.$$.fragment,e),b(Ze.$$.fragment,e),b(es.$$.fragment,e),b(ss.$$.fragment,e),b(as.$$.fragment,e),b(ts.$$.fragment,e),b(ls.$$.fragment,e),b(ns.$$.fragment,e),b(Ie.$$.fragment,e),b(rs.$$.fragment,e),b(os.$$.fragment,e),b(ps.$$.fragment,e),b(is.$$.fragment,e),b(hs.$$.fragment,e),b(cs.$$.fragment,e),b(ds.$$.fragment,e),b(fs.$$.fragment,e),b(Ne.$$.fragment,e),b(ms.$$.fragment,e),b(gs.$$.fragment,e),b(_s.$$.fragment,e),fl=!1},d(e){a(f),e&&a(w),e&&a(m),$(u),e&&a(k),e&&a(I),e&&a(pe),e&&a(S),e&&a(lt),e&&a(W),e&&a(nt),e&&a(M),e&&a(rt),$(Re,e),e&&a(ot),e&&a(H),e&&a(pt),$(he,e),e&&a(it),e&&a(J),$(He),e&&a(ht),e&&a(O),e&&a(ct),e&&a(V),e&&a(dt),$(Ye,e),e&&a(ft),$(de,e),e&&a(mt),e&&a(G),$(Be),e&&a(ut),e&&a(me),e&&a(gt),e&&a(ue),e&&a(_t),$(Ue,e),e&&a(vt),e&&a(K),$(We),e&&a(jt),e&&a(qs),e&&a(bt),e&&a(Ss),e&&a($t),$(Je,e),e&&a(wt),e&&a(zs),e&&a(xt),$(Ge,e),e&&a(yt),$(je,e),e&&a(kt),e&&a(Ls),e&&a(Et),e&&a(Q),$(Ke),e&&a(Dt),e&&a(X),e&&a(At),$(Qe,e),e&&a(It),e&&a($e),e&&a(Tt),$(Xe,e),e&&a(Pt),e&&a(Y),e&&a(qt),e&&a(Z),$(Ze),e&&a(St),e&&a(Vs),e&&a(Ct),e&&a(ee),$(es),e&&a(zt),e&&a(ye),e&&a(Nt),e&&a(ke),e&&a(Lt),$(ss,e),e&&a(Mt),e&&a(se),$(as),e&&a(Ot),e&&a(De),e&&a(Vt),$(ts,e),e&&a(Ft),e&&a(ae),$(ls),e&&a(Rt),e&&a(N),e&&a(Ht),$(ns,e),e&&a(Yt),$(Ie,e),e&&a(Bt),e&&a(Te),e&&a(Ut),$(rs,e),e&&a(Wt),e&&a(te),$(os),e&&a(Jt),e&&a(P),e&&a(Gt),e&&a(Js),e&&a(Kt),e&&a(B),e&&a(Qt),$(ps,e),e&&a(Xt),e&&a(qe),e&&a(Zt),$(is,e),e&&a(el),e&&a(Se),e&&a(sl),e&&a(U),e&&a(al),$(hs,e),e&&a(tl),e&&a(le),$(cs),e&&a(ll),e&&a(R),e&&a(nl),e&&a(ne),$(ds),e&&a(rl),$(fs,e),e&&a(ol),$(Ne,e),e&&a(pl),e&&a(re),$(ms),e&&a(il),e&&a(us),e&&a(hl),$(gs,e),e&&a(cl),e&&a(sa),e&&a(dl),$(_s,e)}}}const ii={local:"stream",sections:[{local:"shuffle",title:"Shuffle"},{local:"reshuffle",title:"Reshuffle"},{local:"split-dataset",title:"Split dataset"},{local:"interleave",title:"Interleave"},{local:"rename-remove-cast-and-flatten",sections:[{local:"rename",title:"Rename"},{local:"remove",title:"Remove"},{local:"cast",title:"Cast"}],title:"Rename, remove, cast, and flatten"},{local:"map",sections:[{local:"batch-processing",sections:[{local:"tokenization",title:"Tokenization"}],title:"Batch processing"}],title:"Map"},{local:"stream-in-a-training-loop",title:"Stream in a training loop"}],title:"Stream"};function hi(C,f,w){let{fw:m}=f;return C.$$set=x=>{"fw"in x&&w(0,m=x.fw)},[m]}class gi extends Xp{constructor(f){super();Zp(this,f,hi,pi,ei,{fw:0})}}export{gi as default,ii as metadata};
