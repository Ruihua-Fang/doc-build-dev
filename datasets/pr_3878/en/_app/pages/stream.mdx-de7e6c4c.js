import{S as Gp,i as Kp,s as Qp,e as t,k as d,w as u,t as n,M as Xp,c as l,d as e,m as f,a as i,x as _,h as r,b as h,N as Wp,F as a,g as o,y as g,q as b,o as v,B as j}from"../chunks/vendor-e67aec41.js";import{T as nl}from"../chunks/Tip-76459d1c.js";import{I as F}from"../chunks/IconCopyLink-ffd7f84e.js";import{C as S}from"../chunks/CodeBlock-e2bcf023.js";import{C as Zp}from"../chunks/CodeBlockFw-1e02e2ba.js";function si(N){let c,$,m,w,k,x,y,E;return{c(){c=t("p"),$=n("An "),m=t("a"),w=n("datasets.IterableDataset"),k=n(" is useful for iterative jobs like training a model. You shouldn\u2019t use a "),x=t("a"),y=n("datasets.IterableDataset"),E=n(" for jobs that require random access to examples because you have to iterate all over it using a for loop. Getting the last example in an iterable dataset would require you to iterate over all the previous examples."),this.h()},l(P){c=l(P,"P",{});var D=i(c);$=r(D,"An "),m=l(D,"A",{href:!0});var T=i(m);w=r(T,"datasets.IterableDataset"),T.forEach(e),k=r(D," is useful for iterative jobs like training a model. You shouldn\u2019t use a "),x=l(D,"A",{href:!0});var Y=i(x);y=r(Y,"datasets.IterableDataset"),Y.forEach(e),E=r(D," for jobs that require random access to examples because you have to iterate all over it using a for loop. Getting the last example in an iterable dataset would require you to iterate over all the previous examples."),D.forEach(e),this.h()},h(){h(m,"href","/docs/datasets/pr_3878/en/package_reference/main_classes#datasets.IterableDataset"),h(x,"href","/docs/datasets/pr_3878/en/package_reference/main_classes#datasets.IterableDataset")},m(P,D){o(P,c,D),a(c,$),a(c,m),a(m,w),a(c,k),a(c,x),a(x,y),a(c,E)},d(P){P&&e(c)}}}function ai(N){let c,$,m,w;return{c(){c=t("p"),$=t("a"),m=n("datasets.IterableDataset.shuffle()"),w=n(" will also shuffle the order of the shards if the dataset is sharded into multiple sets."),this.h()},l(k){c=l(k,"P",{});var x=i(c);$=l(x,"A",{href:!0});var y=i($);m=r(y,"datasets.IterableDataset.shuffle()"),y.forEach(e),w=r(x," will also shuffle the order of the shards if the dataset is sharded into multiple sets."),x.forEach(e),this.h()},h(){h($,"href","/docs/datasets/pr_3878/en/package_reference/main_classes#datasets.IterableDataset.shuffle")},m(k,x){o(k,c,x),a(c,$),a($,m),a(c,w)},d(k){k&&e(c)}}}function ei(N){let c,$,m,w,k,x,y,E,P,D,T,Y,ns;return{c(){c=t("p"),$=t("code"),m=n("take"),w=n(" and "),k=t("code"),x=n("skip"),y=n(" prevent future calls to "),E=t("code"),P=n("shuffle"),D=n(" because they lock in the order of the shards. You should "),T=t("code"),Y=n("shuffle"),ns=n(" your dataset before splitting it.")},l(q){c=l(q,"P",{});var A=i(c);$=l(A,"CODE",{});var oa=i($);m=r(oa,"take"),oa.forEach(e),w=r(A," and "),k=l(A,"CODE",{});var ha=i(k);x=r(ha,"skip"),ha.forEach(e),y=r(A," prevent future calls to "),E=l(A,"CODE",{});var rs=i(E);P=r(rs,"shuffle"),rs.forEach(e),D=r(A," because they lock in the order of the shards. You should "),T=l(A,"CODE",{});var da=i(T);Y=r(da,"shuffle"),da.forEach(e),ns=r(A," your dataset before splitting it."),A.forEach(e)},m(q,A){o(q,c,A),a(c,$),a($,m),a(c,w),a(c,k),a(k,x),a(c,y),a(c,E),a(E,P),a(c,D),a(c,T),a(T,Y),a(c,ns)},d(q){q&&e(c)}}}function ti(N){let c,$,m,w,k;return{c(){c=t("p"),$=n("See other examples of batch processing in "),m=t("a"),w=n("the batched map processing documentation"),k=n(". They work the same for iterable datasets."),this.h()},l(x){c=l(x,"P",{});var y=i(c);$=r(y,"See other examples of batch processing in "),m=l(y,"A",{href:!0});var E=i(m);w=r(E,"the batched map processing documentation"),E.forEach(e),k=r(y,". They work the same for iterable datasets."),y.forEach(e),this.h()},h(){h(m,"href","./process#batch-processing")},m(x,y){o(x,c,y),a(c,$),a(c,m),a(m,w),a(c,k)},d(x){x&&e(c)}}}function li(N){let c,$,m,w,k,x,y,E,P,D,T,Y,ns,q,A,oa,ha,rs,da,Le,J,fa,xr,rl,ca,$r,Ne,z,pl,Ps,il,ol,Ga,hl,dl,ma,fl,cl,Oe,zs,Fe,H,ml,ua,ul,_l,_a,gl,bl,Ye,ps,He,U,is,Ka,Ms,vl,Qa,jl,Re,M,xl,ga,$l,wl,ba,kl,yl,va,El,Dl,Be,C,Al,Xa,Il,Tl,Za,Sl,ql,ja,Pl,zl,Ve,Cs,Je,os,Ue,W,hs,se,Ls,Ml,ae,Cl,We,ds,Ll,ee,Nl,Ol,Ge,fs,Fl,te,Yl,Hl,Ke,Ns,Qe,G,cs,le,Os,Rl,ne,Bl,Xe,xa,Vl,Ze,$a,ms,wa,Jl,Ul,re,Wl,Gl,st,Fs,at,ka,us,ya,Kl,Ql,pe,Xl,Zl,et,Ys,tt,_s,lt,Ea,nt,K,gs,ie,Hs,sn,oe,an,rt,Q,Da,en,tn,Aa,ln,nn,pt,Rs,it,bs,rn,he,pn,on,ot,Bs,ht,R,hn,de,dn,fn,fe,cn,mn,dt,X,vs,ce,Vs,un,me,_n,ft,js,gn,Ia,bn,vn,ct,Js,mt,Z,xs,ue,Us,jn,_e,xn,ut,I,$n,Ta,wn,kn,Sa,yn,En,qa,Dn,An,Pa,In,Tn,za,Sn,qn,_t,Ma,Pn,gt,B,zn,Ca,Mn,Cn,ge,Ln,Nn,bt,Ws,vt,$s,On,La,Fn,Yn,jt,Gs,xt,ws,Hn,Na,Rn,Bn,$t,V,Vn,be,Jn,Un,Oa,Wn,Gn,wt,Ks,kt,ss,ks,ve,Qs,Kn,je,Qn,yt,O,Fa,Xn,Zn,xe,sr,ar,$e,er,tr,Et,as,ys,we,Xs,lr,ke,nr,Dt,Zs,At,Es,It,es,Ds,ye,sa,rr,Ee,pr,Tt,As,ir,Ya,or,hr,St,aa,qt,ts,Ha,dr,fr,De,cr,mr,Pt,ea,zt,ls,Is,Ae,ta,ur,Ie,_r,Mt,la,Ra,gr,br,Ct,na,Lt,Ba,vr,Nt,ra,Ot;return x=new F({}),zs=new S({props:{code:`from datasets import load_dataset
dataset = load_dataset('oscar', "unshuffled_deduplicated_en", split='train', streaming=True)
print(next(iter(dataset)))`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;oscar&#x27;</span>, <span class="hljs-string">&quot;unshuffled_deduplicated_en&quot;</span>, split=<span class="hljs-string">&#x27;train&#x27;</span>, streaming=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(<span class="hljs-built_in">next</span>(<span class="hljs-built_in">iter</span>(dataset)))
{<span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;Mtendere Village was inspired by the vision of Chief Napoleon Dzombe, which he shared with John Blanchard during his first visit to Malawi. Chief Napoleon conveyed the desperate need for a program to intervene and care for the orphans and vulnerable children (OVC) in Malawi, and John committed to help...</span>`}}),ps=new nl({props:{$$slots:{default:[si]},$$scope:{ctx:N}}}),Ms=new F({}),Cs=new S({props:{code:`from datasets import load_dataset
dataset = load_dataset('oscar', "unshuffled_deduplicated_en", split='train', streaming=True)
shuffled_dataset = dataset.shuffle(seed=42, buffer_size=10_000)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;oscar&#x27;</span>, <span class="hljs-string">&quot;unshuffled_deduplicated_en&quot;</span>, split=<span class="hljs-string">&#x27;train&#x27;</span>, streaming=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>shuffled_dataset = dataset.shuffle(seed=<span class="hljs-number">42</span>, buffer_size=<span class="hljs-number">10_000</span>)`}}),os=new nl({props:{$$slots:{default:[ai]},$$scope:{ctx:N}}}),Ls=new F({}),Ns=new S({props:{code:`for epoch in range(epochs):
    shuffled_dataset.set_epoch(epoch)
    for example in shuffled_dataset:
        ...`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(epochs):
<span class="hljs-meta">... </span>    shuffled_dataset.set_epoch(epoch)
<span class="hljs-meta">... </span>    <span class="hljs-keyword">for</span> example <span class="hljs-keyword">in</span> shuffled_dataset:
<span class="hljs-meta">... </span>        ...`}}),Os=new F({}),Fs=new S({props:{code:`dataset = load_dataset('oscar', "unshuffled_deduplicated_en", split='train', streaming=True)
dataset_head = dataset.take(2)
list(dataset_head)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;oscar&#x27;</span>, <span class="hljs-string">&quot;unshuffled_deduplicated_en&quot;</span>, split=<span class="hljs-string">&#x27;train&#x27;</span>, streaming=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset_head = dataset.take(<span class="hljs-number">2</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">list</span>(dataset_head)
[{<span class="hljs-string">&#x27;id&#x27;</span>: <span class="hljs-number">0</span>, <span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;Mtendere Village was...&#x27;</span>}, {<span class="hljs-string">&#x27;id&#x27;</span>: <span class="hljs-number">1</span>, <span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;Lily James cannot fight the music...&#x27;</span>}]`}}),Ys=new S({props:{code:"train_dataset = shuffled_dataset.skip(1000)",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>train_dataset = shuffled_dataset.skip(<span class="hljs-number">1000</span>)'}}),_s=new nl({props:{warning:"&lcub;true}",$$slots:{default:[ei]},$$scope:{ctx:N}}}),Hs=new F({}),Rs=new S({props:{code:`from datasets import interleave_datasets
from itertools import islice
en_dataset = load_dataset('oscar', "unshuffled_deduplicated_en", split='train', streaming=True)
fr_dataset = load_dataset('oscar', "unshuffled_deduplicated_fr", split='train', streaming=True)

multilingual_dataset = interleave_datasets([en_dataset, fr_dataset])
print(list(islice(multilingual_dataset, 2)))`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> interleave_datasets
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> itertools <span class="hljs-keyword">import</span> islice
<span class="hljs-meta">&gt;&gt;&gt; </span>en_dataset = load_dataset(<span class="hljs-string">&#x27;oscar&#x27;</span>, <span class="hljs-string">&quot;unshuffled_deduplicated_en&quot;</span>, split=<span class="hljs-string">&#x27;train&#x27;</span>, streaming=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>fr_dataset = load_dataset(<span class="hljs-string">&#x27;oscar&#x27;</span>, <span class="hljs-string">&quot;unshuffled_deduplicated_fr&quot;</span>, split=<span class="hljs-string">&#x27;train&#x27;</span>, streaming=<span class="hljs-literal">True</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>multilingual_dataset = interleave_datasets([en_dataset, fr_dataset])
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(<span class="hljs-built_in">list</span>(islice(multilingual_dataset, <span class="hljs-number">2</span>)))
[{<span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;Mtendere Village was inspired by the vision...&#x27;</span>}, {<span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&quot;M\xE9dia de d\xE9bat d&#x27;id\xE9es, de culture et de litt\xE9rature...&quot;</span>}]`}}),Bs=new S({props:{code:`multilingual_dataset_with_oversampling = interleave_datasets([en_dataset, fr_dataset], probabilities=[0.8, 0.2], seed=42)
print(list(islice(multilingual_dataset_with_oversampling, 2)))`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>multilingual_dataset_with_oversampling = interleave_datasets([en_dataset, fr_dataset], probabilities=[<span class="hljs-number">0.8</span>, <span class="hljs-number">0.2</span>], seed=<span class="hljs-number">42</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(<span class="hljs-built_in">list</span>(islice(multilingual_dataset_with_oversampling, <span class="hljs-number">2</span>)))
[{<span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;Mtendere Village was inspired by the vision...&#x27;</span>}, {<span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;Lily James cannot fight the music...&#x27;</span>}]`}}),Vs=new F({}),Js=new S({props:{code:`from datasets import load_dataset
dataset = load_dataset('mc4', 'en', streaming=True, split='train')
dataset = dataset.remove_columns('timestamp')`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;mc4&#x27;</span>, <span class="hljs-string">&#x27;en&#x27;</span>, streaming=<span class="hljs-literal">True</span>, split=<span class="hljs-string">&#x27;train&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = dataset.remove_columns(<span class="hljs-string">&#x27;timestamp&#x27;</span>)`}}),Us=new F({}),Ws=new S({props:{code:`def add_prefix(example):
    example['text'] = 'My text: ' + example['text']
    return example`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">def</span> <span class="hljs-title function_">add_prefix</span>(<span class="hljs-params">example</span>):
<span class="hljs-meta">... </span>    example[<span class="hljs-string">&#x27;text&#x27;</span>] = <span class="hljs-string">&#x27;My text: &#x27;</span> + example[<span class="hljs-string">&#x27;text&#x27;</span>]
<span class="hljs-meta">... </span>    <span class="hljs-keyword">return</span> example`}}),Gs=new S({props:{code:`from datasets import load_dataset
dataset = load_dataset('oscar', 'unshuffled_deduplicated_en', streaming=True, split='train')
updated_dataset = dataset.map(add_prefix)
list(updated_dataset.take(3))`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;oscar&#x27;</span>, <span class="hljs-string">&#x27;unshuffled_deduplicated_en&#x27;</span>, streaming=<span class="hljs-literal">True</span>, split=<span class="hljs-string">&#x27;train&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>updated_dataset = dataset.<span class="hljs-built_in">map</span>(add_prefix)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">list</span>(updated_dataset.take(<span class="hljs-number">3</span>))
[{<span class="hljs-string">&#x27;id&#x27;</span>: <span class="hljs-number">0</span>, <span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;My text: Mtendere Village was inspired by...&#x27;</span>},
 {<span class="hljs-string">&#x27;id&#x27;</span>: <span class="hljs-number">1</span>, <span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;My text: Lily James cannot fight the music...&#x27;</span>},
 {<span class="hljs-string">&#x27;id&#x27;</span>: <span class="hljs-number">2</span>, <span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;My text: &quot;I\\&#x27;d love to help kickstart...&#x27;</span>}]`}}),Ks=new S({props:{code:`updated_dataset = dataset.map(add_prefix, remove_columns=["id"])
list(updated_dataset.take(3))`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>updated_dataset = dataset.<span class="hljs-built_in">map</span>(add_prefix, remove_columns=[<span class="hljs-string">&quot;id&quot;</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">list</span>(updated_dataset.take(<span class="hljs-number">3</span>))
[{<span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;My text: Mtendere Village was inspired by...&#x27;</span>},
 {<span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;My text: Lily James cannot fight the music...&#x27;</span>},
 {<span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;My text: &quot;I\\&#x27;d love to help kickstart...&#x27;</span>}]`}}),Qs=new F({}),Xs=new F({}),Zs=new S({props:{code:`from datasets import load_dataset
from transformers import AutoTokenizer
dataset = load_dataset("mc4", "en", streaming=True, split="train")
tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')
def encode(examples):
    return tokenizer(examples['text'], truncation=True, padding='max_length')
dataset = dataset.map(encode, batched=True, remove_columns=["text", "timestamp", "url"])
next(iter(dataset))`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;mc4&quot;</span>, <span class="hljs-string">&quot;en&quot;</span>, streaming=<span class="hljs-literal">True</span>, split=<span class="hljs-string">&quot;train&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&#x27;distilbert-base-uncased&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">def</span> <span class="hljs-title function_">encode</span>(<span class="hljs-params">examples</span>):
<span class="hljs-meta">... </span>    <span class="hljs-keyword">return</span> tokenizer(examples[<span class="hljs-string">&#x27;text&#x27;</span>], truncation=<span class="hljs-literal">True</span>, padding=<span class="hljs-string">&#x27;max_length&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = dataset.<span class="hljs-built_in">map</span>(encode, batched=<span class="hljs-literal">True</span>, remove_columns=[<span class="hljs-string">&quot;text&quot;</span>, <span class="hljs-string">&quot;timestamp&quot;</span>, <span class="hljs-string">&quot;url&quot;</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">next</span>(<span class="hljs-built_in">iter</span>(dataset))
{<span class="hljs-string">&#x27;input_ids&#x27;</span>: <span class="hljs-number">101</span>, <span class="hljs-number">8466</span>, <span class="hljs-number">1018</span>, <span class="hljs-number">1010</span>, <span class="hljs-number">4029</span>, <span class="hljs-number">2475</span>, <span class="hljs-number">2062</span>, <span class="hljs-number">18558</span>, <span class="hljs-number">3100</span>, <span class="hljs-number">2061</span>, ...,<span class="hljs-number">1106</span>, <span class="hljs-number">3739</span>, <span class="hljs-number">102</span>],
<span class="hljs-string">&#x27;attention_mask&#x27;</span>: [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, ..., <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]}`}}),Es=new nl({props:{$$slots:{default:[ti]},$$scope:{ctx:N}}}),sa=new F({}),aa=new S({props:{code:`from datasets import load_dataset
dataset = load_dataset('oscar', 'unshuffled_deduplicated_en', streaming=True, split='train')
start_with_ar = dataset.filter(lambda example: example['text'].startswith('Ar'))
next(iter(start_with_ar))`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;oscar&#x27;</span>, <span class="hljs-string">&#x27;unshuffled_deduplicated_en&#x27;</span>, streaming=<span class="hljs-literal">True</span>, split=<span class="hljs-string">&#x27;train&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>start_with_ar = dataset.<span class="hljs-built_in">filter</span>(<span class="hljs-keyword">lambda</span> example: example[<span class="hljs-string">&#x27;text&#x27;</span>].startswith(<span class="hljs-string">&#x27;Ar&#x27;</span>))
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">next</span>(<span class="hljs-built_in">iter</span>(start_with_ar))
{<span class="hljs-string">&#x27;id&#x27;</span>: <span class="hljs-number">4</span>, <span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;Are you looking for Number the Stars (Essential Modern Classics)?...&#x27;</span>}`}}),ea=new S({props:{code:`even_dataset = dataset.filter(lambda example, idx: idx % 2 == 0, with_indices=True)
list(even_dataset.take(3))`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>even_dataset = dataset.<span class="hljs-built_in">filter</span>(<span class="hljs-keyword">lambda</span> example, idx: idx % <span class="hljs-number">2</span> == <span class="hljs-number">0</span>, with_indices=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">list</span>(even_dataset.take(<span class="hljs-number">3</span>))
[{<span class="hljs-string">&#x27;id&#x27;</span>: <span class="hljs-number">0</span>, <span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;Mtendere Village was inspired by the vision of Chief Napoleon Dzombe, ...&#x27;</span>},
 {<span class="hljs-string">&#x27;id&#x27;</span>: <span class="hljs-number">2</span>, <span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;&quot;I\\&#x27;d love to help kickstart continued development! And 0 EUR/month...&#x27;</span>},
 {<span class="hljs-string">&#x27;id&#x27;</span>: <span class="hljs-number">4</span>, <span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;Are you looking for Number the Stars (Essential Modern Classics)? Normally, ...&#x27;</span>}]`}}),ta=new F({}),na=new S({props:{code:`buffer_size, seed = 10_000, 42
dataset = dataset.shuffle(buffer_size, seed)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>buffer_size, seed = <span class="hljs-number">10_000</span>, <span class="hljs-number">42</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = dataset.shuffle(buffer_size, seed)`}}),ra=new Zp({props:{group1:{id:"pt",code:`import torch
from torch.utils.data import DataLoader
from transformers import AutoModelForMaskedLM, DataCollatorForLanguageModeling
from tqdm import tqdm
dataset = dataset.with_format("torch")
dataloader = DataLoader(dataset, collate_fn=DataCollatorForLanguageModeling(tokenizer))
device = 'cuda' if torch.cuda.is_available() else 'cpu' 
model = AutoModelForMaskedLM.from_pretrained("distilbert-base-uncased")
model.train().to(device)
optimizer = torch.optim.AdamW(params=model.parameters(), lr=1e-5)
for epoch in range(3):
    dataset.set_epoch(epoch)
    for i, batch in enumerate(tqdm(dataloader, total=5)):
        if i == 5:
            break
        batch = {k: v.to(device) for k, v in batch.items()}
        outputs = model(**batch)
        loss = outputs[0]
        loss.backward()
        optimizer.step()
        optimizer.zero_grad()
        if i % 10 == 0:
            print(f"loss: {loss}")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> DataLoader
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForMaskedLM, DataCollatorForLanguageModeling
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> tqdm <span class="hljs-keyword">import</span> tqdm
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = dataset.with_format(<span class="hljs-string">&quot;torch&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>dataloader = DataLoader(dataset, collate_fn=DataCollatorForLanguageModeling(tokenizer))
<span class="hljs-meta">&gt;&gt;&gt; </span>device = <span class="hljs-string">&#x27;cuda&#x27;</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">&#x27;cpu&#x27;</span> 
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMaskedLM.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.train().to(device)
<span class="hljs-meta">&gt;&gt;&gt; </span>optimizer = torch.optim.AdamW(params=model.parameters(), lr=<span class="hljs-number">1e-5</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">3</span>):
<span class="hljs-meta">... </span>    dataset.set_epoch(epoch)
<span class="hljs-meta">... </span>    <span class="hljs-keyword">for</span> i, batch <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(tqdm(dataloader, total=<span class="hljs-number">5</span>)):
<span class="hljs-meta">... </span>        <span class="hljs-keyword">if</span> i == <span class="hljs-number">5</span>:
<span class="hljs-meta">... </span>            <span class="hljs-keyword">break</span>
<span class="hljs-meta">... </span>        batch = {k: v.to(device) <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> batch.items()}
<span class="hljs-meta">... </span>        outputs = model(**batch)
<span class="hljs-meta">... </span>        loss = outputs[<span class="hljs-number">0</span>]
<span class="hljs-meta">... </span>        loss.backward()
<span class="hljs-meta">... </span>        optimizer.step()
<span class="hljs-meta">... </span>        optimizer.zero_grad()
<span class="hljs-meta">... </span>        <span class="hljs-keyword">if</span> i % <span class="hljs-number">10</span> == <span class="hljs-number">0</span>:
<span class="hljs-meta">... </span>            <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;loss: <span class="hljs-subst">{loss}</span>&quot;</span>)`},group2:{id:"tf",code:"# WIP",highlighted:'<span class="hljs-comment"># WIP</span>'}}}),{c(){c=t("meta"),$=d(),m=t("h1"),w=t("a"),k=t("span"),u(x.$$.fragment),y=d(),E=t("span"),P=n("Stream"),D=d(),T=t("p"),Y=n("Dataset streaming lets you get started with a dataset without waiting for the entire dataset to download. The data is downloaded progressively as you iterate over the dataset. This is especially helpful when:"),ns=d(),q=t("ul"),A=t("li"),oa=n("You don\u2019t want to wait for an extremely large dataset to download."),ha=d(),rs=t("li"),da=n("The dataset size exceeds the amount of disk space on your computer."),Le=d(),J=t("div"),fa=t("img"),rl=d(),ca=t("img"),Ne=d(),z=t("p"),pl=n("For example, the English split of the "),Ps=t("a"),il=n("OSCAR"),ol=n(" dataset is 1.2 terabytes, but you can use it instantly with streaming. Stream a dataset by setting "),Ga=t("code"),hl=n("streaming=True"),dl=n(" in "),ma=t("a"),fl=n("datasets.load_dataset()"),cl=n(" as shown below:"),Oe=d(),u(zs.$$.fragment),Fe=d(),H=t("p"),ml=n("Loading a dataset in streaming mode creates a new dataset type instance (instead of the classic "),ua=t("a"),ul=n("datasets.Dataset"),_l=n(" object), known as an "),_a=t("a"),gl=n("datasets.IterableDataset"),bl=n(". This special type of dataset has its own set of processing methods shown below."),Ye=d(),u(ps.$$.fragment),He=d(),U=t("h2"),is=t("a"),Ka=t("span"),u(Ms.$$.fragment),vl=d(),Qa=t("span"),jl=n("Shuffle"),Re=d(),M=t("p"),xl=n("Like a regular "),ga=t("a"),$l=n("datasets.Dataset"),wl=n(" object, you can also shuffle a "),ba=t("a"),kl=n("datasets.IterableDataset"),yl=n(" with "),va=t("a"),El=n("datasets.IterableDataset.shuffle()"),Dl=n("."),Be=d(),C=t("p"),Al=n("The "),Xa=t("code"),Il=n("buffer_size"),Tl=n(" argument controls the size of the buffer to randomly sample examples from. Let\u2019s say your dataset has one million examples, and you set the "),Za=t("code"),Sl=n("buffer_size"),ql=n(" to ten thousand. "),ja=t("a"),Pl=n("datasets.IterableDataset.shuffle()"),zl=n(" will randomly select examples from the first ten thousand examples in the buffer. Selected examples in the buffer are replaced with new examples. By default, the buffer size is 1,000."),Ve=d(),u(Cs.$$.fragment),Je=d(),u(os.$$.fragment),Ue=d(),W=t("h2"),hs=t("a"),se=t("span"),u(Ls.$$.fragment),Ml=d(),ae=t("span"),Cl=n("Reshuffle"),We=d(),ds=t("p"),Ll=n("Sometimes you may want to reshuffle the dataset after each epoch. This will require you to set a different seed for each epoch. Use "),ee=t("code"),Nl=n("datasets.IterableDataset.set_epoch()"),Ol=n("in between epochs to tell the dataset what epoch you\u2019re on."),Ge=d(),fs=t("p"),Fl=n("Your seed effectively becomes: "),te=t("code"),Yl=n("initial seed + current epoch"),Hl=n("."),Ke=d(),u(Ns.$$.fragment),Qe=d(),G=t("h2"),cs=t("a"),le=t("span"),u(Os.$$.fragment),Rl=d(),ne=t("span"),Bl=n("Split dataset"),Xe=d(),xa=t("p"),Vl=n("You can split your dataset one of two ways:"),Ze=d(),$a=t("ul"),ms=t("li"),wa=t("a"),Jl=n("datasets.IterableDataset.take()"),Ul=n(" returns the first "),re=t("code"),Wl=n("n"),Gl=n(" examples in a dataset:"),st=d(),u(Fs.$$.fragment),at=d(),ka=t("ul"),us=t("li"),ya=t("a"),Kl=n("datasets.IterableDataset.skip()"),Ql=n(" omits the first "),pe=t("code"),Xl=n("n"),Zl=n(" examples in a dataset and returns the remaining examples:"),et=d(),u(Ys.$$.fragment),tt=d(),u(_s.$$.fragment),lt=d(),Ea=t("a"),nt=d(),K=t("h2"),gs=t("a"),ie=t("span"),u(Hs.$$.fragment),sn=d(),oe=t("span"),an=n("Interleave"),rt=d(),Q=t("p"),Da=t("a"),en=n("datasets.interleave_datasets()"),tn=n(" can combine an "),Aa=t("a"),ln=n("datasets.IterableDataset"),nn=n(" with other datasets. The combined dataset returns alternating examples from each of the original datasets."),pt=d(),u(Rs.$$.fragment),it=d(),bs=t("p"),rn=n("Define sampling probabilities from each of the original datasets for more control over how each of them are sampled and combined. Set the "),he=t("code"),pn=n("probabilities"),on=n(" argument with your desired sampling probabilities:"),ot=d(),u(Bs.$$.fragment),ht=d(),R=t("p"),hn=n("Around 80% of the final dataset is made of the "),de=t("code"),dn=n("en_dataset"),fn=n(", and 20% of the "),fe=t("code"),cn=n("fr_dataset"),mn=n("."),dt=d(),X=t("h2"),vs=t("a"),ce=t("span"),u(Vs.$$.fragment),un=d(),me=t("span"),_n=n("Remove"),ft=d(),js=t("p"),gn=n("Remove columns on-the-fly with "),Ia=t("a"),bn=n("datasets.IterableDataset.remove_columns()"),vn=n(". Specify the name of the column to remove:"),ct=d(),u(Js.$$.fragment),mt=d(),Z=t("h2"),xs=t("a"),ue=t("span"),u(Us.$$.fragment),jn=d(),_e=t("span"),xn=n("Map"),ut=d(),I=t("p"),$n=n("Similar to the "),Ta=t("a"),wn=n("datasets.Dataset.map()"),kn=n(" function for a regular "),Sa=t("a"),yn=n("datasets.Dataset"),En=n(", \u{1F917}  Datasets features "),qa=t("a"),Dn=n("datasets.IterableDataset.map()"),An=n(" for processing "),Pa=t("a"),In=n("datasets.IterableDataset"),Tn=n(`\\s.
`),za=t("a"),Sn=n("datasets.IterableDataset.map()"),qn=n(" applies processing on-the-fly when examples are streamed."),_t=d(),Ma=t("p"),Pn=n("It allows you to apply a processing function to each example in a dataset, independently or in batches. This function can even create new rows and columns."),gt=d(),B=t("p"),zn=n("The following example demonstrates how to tokenize a "),Ca=t("a"),Mn=n("datasets.IterableDataset"),Cn=n(". The function needs to accept and output a "),ge=t("code"),Ln=n("dict"),Nn=n(":"),bt=d(),u(Ws.$$.fragment),vt=d(),$s=t("p"),On=n("Next, apply this function to the dataset with "),La=t("a"),Fn=n("datasets.IterableDataset.map()"),Yn=n(":"),jt=d(),u(Gs.$$.fragment),xt=d(),ws=t("p"),Hn=n("Let\u2019s take a look at another example, except this time, you will remove a column with "),Na=t("a"),Rn=n("datasets.IterableDataset.map()"),Bn=n(". When you remove a column, it is only removed after the example has been provided to the mapped function. This allows the mapped function to use the content of the columns before they are removed."),$t=d(),V=t("p"),Vn=n("Specify the column to remove with the "),be=t("code"),Jn=n("remove_columns"),Un=n(" argument in "),Oa=t("a"),Wn=n("datasets.IterableDataset.map()"),Gn=n(":"),wt=d(),u(Ks.$$.fragment),kt=d(),ss=t("h3"),ks=t("a"),ve=t("span"),u(Qs.$$.fragment),Kn=d(),je=t("span"),Qn=n("Batch processing"),yt=d(),O=t("p"),Fa=t("a"),Xn=n("datasets.IterableDataset.map()"),Zn=n(" also supports working with batches of examples. Operate on batches by setting "),xe=t("code"),sr=n("batched=True"),ar=n(". The default batch size is 1000, but you can adjust it with the "),$e=t("code"),er=n("batch_size"),tr=n(" argument. This opens the door to many interesting applications such as tokenization, splitting long sentences into shorter chunks, and data augmentation."),Et=d(),as=t("h4"),ys=t("a"),we=t("span"),u(Xs.$$.fragment),lr=d(),ke=t("span"),nr=n("Tokenization"),Dt=d(),u(Zs.$$.fragment),At=d(),u(Es.$$.fragment),It=d(),es=t("h3"),Ds=t("a"),ye=t("span"),u(sa.$$.fragment),rr=d(),Ee=t("span"),pr=n("Filter"),Tt=d(),As=t("p"),ir=n("You can filter rows in the dataset based on a predicate function using "),Ya=t("a"),or=n("datasets.Dataset.filter()"),hr=n(". It returns rows that match a specified condition:"),St=d(),u(aa.$$.fragment),qt=d(),ts=t("p"),Ha=t("a"),dr=n("datasets.Dataset.filter()"),fr=n(" can also filter by indices if you set "),De=t("code"),cr=n("with_indices=True"),mr=n(":"),Pt=d(),u(ea.$$.fragment),zt=d(),ls=t("h2"),Is=t("a"),Ae=t("span"),u(ta.$$.fragment),ur=d(),Ie=t("span"),_r=n("Stream in a training loop"),Mt=d(),la=t("p"),Ra=t("a"),gr=n("datasets.IterableDataset"),br=n(" can be integrated into a training loop. First, shuffle the dataset:"),Ct=d(),u(na.$$.fragment),Lt=d(),Ba=t("p"),vr=n("Lastly, create a simple training loop and start training:"),Nt=d(),u(ra.$$.fragment),this.h()},l(s){const p=Xp('[data-svelte="svelte-1phssyn"]',document.head);c=l(p,"META",{name:!0,content:!0}),p.forEach(e),$=f(s),m=l(s,"H1",{class:!0});var pa=i(m);w=l(pa,"A",{id:!0,class:!0,href:!0});var Te=i(w);k=l(Te,"SPAN",{});var Se=i(k);_(x.$$.fragment,Se),Se.forEach(e),Te.forEach(e),y=f(pa),E=l(pa,"SPAN",{});var qe=i(E);P=r(qe,"Stream"),qe.forEach(e),pa.forEach(e),D=f(s),T=l(s,"P",{});var wr=i(T);Y=r(wr,"Dataset streaming lets you get started with a dataset without waiting for the entire dataset to download. The data is downloaded progressively as you iterate over the dataset. This is especially helpful when:"),wr.forEach(e),ns=f(s),q=l(s,"UL",{});var Ft=i(q);A=l(Ft,"LI",{});var kr=i(A);oa=r(kr,"You don\u2019t want to wait for an extremely large dataset to download."),kr.forEach(e),ha=f(Ft),rs=l(Ft,"LI",{});var yr=i(rs);da=r(yr,"The dataset size exceeds the amount of disk space on your computer."),yr.forEach(e),Ft.forEach(e),Le=f(s),J=l(s,"DIV",{class:!0});var Yt=i(J);fa=l(Yt,"IMG",{class:!0,src:!0}),rl=f(Yt),ca=l(Yt,"IMG",{class:!0,src:!0}),Yt.forEach(e),Ne=f(s),z=l(s,"P",{});var Ts=i(z);pl=r(Ts,"For example, the English split of the "),Ps=l(Ts,"A",{href:!0,rel:!0});var Er=i(Ps);il=r(Er,"OSCAR"),Er.forEach(e),ol=r(Ts," dataset is 1.2 terabytes, but you can use it instantly with streaming. Stream a dataset by setting "),Ga=l(Ts,"CODE",{});var Dr=i(Ga);hl=r(Dr,"streaming=True"),Dr.forEach(e),dl=r(Ts," in "),ma=l(Ts,"A",{href:!0});var Ar=i(ma);fl=r(Ar,"datasets.load_dataset()"),Ar.forEach(e),cl=r(Ts," as shown below:"),Ts.forEach(e),Oe=f(s),_(zs.$$.fragment,s),Fe=f(s),H=l(s,"P",{});var Va=i(H);ml=r(Va,"Loading a dataset in streaming mode creates a new dataset type instance (instead of the classic "),ua=l(Va,"A",{href:!0});var Ir=i(ua);ul=r(Ir,"datasets.Dataset"),Ir.forEach(e),_l=r(Va," object), known as an "),_a=l(Va,"A",{href:!0});var Tr=i(_a);gl=r(Tr,"datasets.IterableDataset"),Tr.forEach(e),bl=r(Va,". This special type of dataset has its own set of processing methods shown below."),Va.forEach(e),Ye=f(s),_(ps.$$.fragment,s),He=f(s),U=l(s,"H2",{class:!0});var Ht=i(U);is=l(Ht,"A",{id:!0,class:!0,href:!0});var Sr=i(is);Ka=l(Sr,"SPAN",{});var qr=i(Ka);_(Ms.$$.fragment,qr),qr.forEach(e),Sr.forEach(e),vl=f(Ht),Qa=l(Ht,"SPAN",{});var Pr=i(Qa);jl=r(Pr,"Shuffle"),Pr.forEach(e),Ht.forEach(e),Re=f(s),M=l(s,"P",{});var Ss=i(M);xl=r(Ss,"Like a regular "),ga=l(Ss,"A",{href:!0});var zr=i(ga);$l=r(zr,"datasets.Dataset"),zr.forEach(e),wl=r(Ss," object, you can also shuffle a "),ba=l(Ss,"A",{href:!0});var Mr=i(ba);kl=r(Mr,"datasets.IterableDataset"),Mr.forEach(e),yl=r(Ss," with "),va=l(Ss,"A",{href:!0});var Cr=i(va);El=r(Cr,"datasets.IterableDataset.shuffle()"),Cr.forEach(e),Dl=r(Ss,"."),Ss.forEach(e),Be=f(s),C=l(s,"P",{});var qs=i(C);Al=r(qs,"The "),Xa=l(qs,"CODE",{});var Lr=i(Xa);Il=r(Lr,"buffer_size"),Lr.forEach(e),Tl=r(qs," argument controls the size of the buffer to randomly sample examples from. Let\u2019s say your dataset has one million examples, and you set the "),Za=l(qs,"CODE",{});var Nr=i(Za);Sl=r(Nr,"buffer_size"),Nr.forEach(e),ql=r(qs," to ten thousand. "),ja=l(qs,"A",{href:!0});var Or=i(ja);Pl=r(Or,"datasets.IterableDataset.shuffle()"),Or.forEach(e),zl=r(qs," will randomly select examples from the first ten thousand examples in the buffer. Selected examples in the buffer are replaced with new examples. By default, the buffer size is 1,000."),qs.forEach(e),Ve=f(s),_(Cs.$$.fragment,s),Je=f(s),_(os.$$.fragment,s),Ue=f(s),W=l(s,"H2",{class:!0});var Rt=i(W);hs=l(Rt,"A",{id:!0,class:!0,href:!0});var Fr=i(hs);se=l(Fr,"SPAN",{});var Yr=i(se);_(Ls.$$.fragment,Yr),Yr.forEach(e),Fr.forEach(e),Ml=f(Rt),ae=l(Rt,"SPAN",{});var Hr=i(ae);Cl=r(Hr,"Reshuffle"),Hr.forEach(e),Rt.forEach(e),We=f(s),ds=l(s,"P",{});var Bt=i(ds);Ll=r(Bt,"Sometimes you may want to reshuffle the dataset after each epoch. This will require you to set a different seed for each epoch. Use "),ee=l(Bt,"CODE",{});var Rr=i(ee);Nl=r(Rr,"datasets.IterableDataset.set_epoch()"),Rr.forEach(e),Ol=r(Bt,"in between epochs to tell the dataset what epoch you\u2019re on."),Bt.forEach(e),Ge=f(s),fs=l(s,"P",{});var Vt=i(fs);Fl=r(Vt,"Your seed effectively becomes: "),te=l(Vt,"CODE",{});var Br=i(te);Yl=r(Br,"initial seed + current epoch"),Br.forEach(e),Hl=r(Vt,"."),Vt.forEach(e),Ke=f(s),_(Ns.$$.fragment,s),Qe=f(s),G=l(s,"H2",{class:!0});var Jt=i(G);cs=l(Jt,"A",{id:!0,class:!0,href:!0});var Vr=i(cs);le=l(Vr,"SPAN",{});var Jr=i(le);_(Os.$$.fragment,Jr),Jr.forEach(e),Vr.forEach(e),Rl=f(Jt),ne=l(Jt,"SPAN",{});var Ur=i(ne);Bl=r(Ur,"Split dataset"),Ur.forEach(e),Jt.forEach(e),Xe=f(s),xa=l(s,"P",{});var Wr=i(xa);Vl=r(Wr,"You can split your dataset one of two ways:"),Wr.forEach(e),Ze=f(s),$a=l(s,"UL",{});var Gr=i($a);ms=l(Gr,"LI",{});var Pe=i(ms);wa=l(Pe,"A",{href:!0});var Kr=i(wa);Jl=r(Kr,"datasets.IterableDataset.take()"),Kr.forEach(e),Ul=r(Pe," returns the first "),re=l(Pe,"CODE",{});var Qr=i(re);Wl=r(Qr,"n"),Qr.forEach(e),Gl=r(Pe," examples in a dataset:"),Pe.forEach(e),Gr.forEach(e),st=f(s),_(Fs.$$.fragment,s),at=f(s),ka=l(s,"UL",{});var Xr=i(ka);us=l(Xr,"LI",{});var ze=i(us);ya=l(ze,"A",{href:!0});var Zr=i(ya);Kl=r(Zr,"datasets.IterableDataset.skip()"),Zr.forEach(e),Ql=r(ze," omits the first "),pe=l(ze,"CODE",{});var sp=i(pe);Xl=r(sp,"n"),sp.forEach(e),Zl=r(ze," examples in a dataset and returns the remaining examples:"),ze.forEach(e),Xr.forEach(e),et=f(s),_(Ys.$$.fragment,s),tt=f(s),_(_s.$$.fragment,s),lt=f(s),Ea=l(s,"A",{id:!0}),i(Ea).forEach(e),nt=f(s),K=l(s,"H2",{class:!0});var Ut=i(K);gs=l(Ut,"A",{id:!0,class:!0,href:!0});var ap=i(gs);ie=l(ap,"SPAN",{});var ep=i(ie);_(Hs.$$.fragment,ep),ep.forEach(e),ap.forEach(e),sn=f(Ut),oe=l(Ut,"SPAN",{});var tp=i(oe);an=r(tp,"Interleave"),tp.forEach(e),Ut.forEach(e),rt=f(s),Q=l(s,"P",{});var Me=i(Q);Da=l(Me,"A",{href:!0});var lp=i(Da);en=r(lp,"datasets.interleave_datasets()"),lp.forEach(e),tn=r(Me," can combine an "),Aa=l(Me,"A",{href:!0});var np=i(Aa);ln=r(np,"datasets.IterableDataset"),np.forEach(e),nn=r(Me," with other datasets. The combined dataset returns alternating examples from each of the original datasets."),Me.forEach(e),pt=f(s),_(Rs.$$.fragment,s),it=f(s),bs=l(s,"P",{});var Wt=i(bs);rn=r(Wt,"Define sampling probabilities from each of the original datasets for more control over how each of them are sampled and combined. Set the "),he=l(Wt,"CODE",{});var rp=i(he);pn=r(rp,"probabilities"),rp.forEach(e),on=r(Wt," argument with your desired sampling probabilities:"),Wt.forEach(e),ot=f(s),_(Bs.$$.fragment,s),ht=f(s),R=l(s,"P",{});var Ja=i(R);hn=r(Ja,"Around 80% of the final dataset is made of the "),de=l(Ja,"CODE",{});var pp=i(de);dn=r(pp,"en_dataset"),pp.forEach(e),fn=r(Ja,", and 20% of the "),fe=l(Ja,"CODE",{});var ip=i(fe);cn=r(ip,"fr_dataset"),ip.forEach(e),mn=r(Ja,"."),Ja.forEach(e),dt=f(s),X=l(s,"H2",{class:!0});var Gt=i(X);vs=l(Gt,"A",{id:!0,class:!0,href:!0});var op=i(vs);ce=l(op,"SPAN",{});var hp=i(ce);_(Vs.$$.fragment,hp),hp.forEach(e),op.forEach(e),un=f(Gt),me=l(Gt,"SPAN",{});var dp=i(me);_n=r(dp,"Remove"),dp.forEach(e),Gt.forEach(e),ft=f(s),js=l(s,"P",{});var Kt=i(js);gn=r(Kt,"Remove columns on-the-fly with "),Ia=l(Kt,"A",{href:!0});var fp=i(Ia);bn=r(fp,"datasets.IterableDataset.remove_columns()"),fp.forEach(e),vn=r(Kt,". Specify the name of the column to remove:"),Kt.forEach(e),ct=f(s),_(Js.$$.fragment,s),mt=f(s),Z=l(s,"H2",{class:!0});var Qt=i(Z);xs=l(Qt,"A",{id:!0,class:!0,href:!0});var cp=i(xs);ue=l(cp,"SPAN",{});var mp=i(ue);_(Us.$$.fragment,mp),mp.forEach(e),cp.forEach(e),jn=f(Qt),_e=l(Qt,"SPAN",{});var up=i(_e);xn=r(up,"Map"),up.forEach(e),Qt.forEach(e),ut=f(s),I=l(s,"P",{});var L=i(I);$n=r(L,"Similar to the "),Ta=l(L,"A",{href:!0});var _p=i(Ta);wn=r(_p,"datasets.Dataset.map()"),_p.forEach(e),kn=r(L," function for a regular "),Sa=l(L,"A",{href:!0});var gp=i(Sa);yn=r(gp,"datasets.Dataset"),gp.forEach(e),En=r(L,", \u{1F917}  Datasets features "),qa=l(L,"A",{href:!0});var bp=i(qa);Dn=r(bp,"datasets.IterableDataset.map()"),bp.forEach(e),An=r(L," for processing "),Pa=l(L,"A",{href:!0});var vp=i(Pa);In=r(vp,"datasets.IterableDataset"),vp.forEach(e),Tn=r(L,`\\s.
`),za=l(L,"A",{href:!0});var jp=i(za);Sn=r(jp,"datasets.IterableDataset.map()"),jp.forEach(e),qn=r(L," applies processing on-the-fly when examples are streamed."),L.forEach(e),_t=f(s),Ma=l(s,"P",{});var xp=i(Ma);Pn=r(xp,"It allows you to apply a processing function to each example in a dataset, independently or in batches. This function can even create new rows and columns."),xp.forEach(e),gt=f(s),B=l(s,"P",{});var Ua=i(B);zn=r(Ua,"The following example demonstrates how to tokenize a "),Ca=l(Ua,"A",{href:!0});var $p=i(Ca);Mn=r($p,"datasets.IterableDataset"),$p.forEach(e),Cn=r(Ua,". The function needs to accept and output a "),ge=l(Ua,"CODE",{});var wp=i(ge);Ln=r(wp,"dict"),wp.forEach(e),Nn=r(Ua,":"),Ua.forEach(e),bt=f(s),_(Ws.$$.fragment,s),vt=f(s),$s=l(s,"P",{});var Xt=i($s);On=r(Xt,"Next, apply this function to the dataset with "),La=l(Xt,"A",{href:!0});var kp=i(La);Fn=r(kp,"datasets.IterableDataset.map()"),kp.forEach(e),Yn=r(Xt,":"),Xt.forEach(e),jt=f(s),_(Gs.$$.fragment,s),xt=f(s),ws=l(s,"P",{});var Zt=i(ws);Hn=r(Zt,"Let\u2019s take a look at another example, except this time, you will remove a column with "),Na=l(Zt,"A",{href:!0});var yp=i(Na);Rn=r(yp,"datasets.IterableDataset.map()"),yp.forEach(e),Bn=r(Zt,". When you remove a column, it is only removed after the example has been provided to the mapped function. This allows the mapped function to use the content of the columns before they are removed."),Zt.forEach(e),$t=f(s),V=l(s,"P",{});var Wa=i(V);Vn=r(Wa,"Specify the column to remove with the "),be=l(Wa,"CODE",{});var Ep=i(be);Jn=r(Ep,"remove_columns"),Ep.forEach(e),Un=r(Wa," argument in "),Oa=l(Wa,"A",{href:!0});var Dp=i(Oa);Wn=r(Dp,"datasets.IterableDataset.map()"),Dp.forEach(e),Gn=r(Wa,":"),Wa.forEach(e),wt=f(s),_(Ks.$$.fragment,s),kt=f(s),ss=l(s,"H3",{class:!0});var sl=i(ss);ks=l(sl,"A",{id:!0,class:!0,href:!0});var Ap=i(ks);ve=l(Ap,"SPAN",{});var Ip=i(ve);_(Qs.$$.fragment,Ip),Ip.forEach(e),Ap.forEach(e),Kn=f(sl),je=l(sl,"SPAN",{});var Tp=i(je);Qn=r(Tp,"Batch processing"),Tp.forEach(e),sl.forEach(e),yt=f(s),O=l(s,"P",{});var ia=i(O);Fa=l(ia,"A",{href:!0});var Sp=i(Fa);Xn=r(Sp,"datasets.IterableDataset.map()"),Sp.forEach(e),Zn=r(ia," also supports working with batches of examples. Operate on batches by setting "),xe=l(ia,"CODE",{});var qp=i(xe);sr=r(qp,"batched=True"),qp.forEach(e),ar=r(ia,". The default batch size is 1000, but you can adjust it with the "),$e=l(ia,"CODE",{});var Pp=i($e);er=r(Pp,"batch_size"),Pp.forEach(e),tr=r(ia," argument. This opens the door to many interesting applications such as tokenization, splitting long sentences into shorter chunks, and data augmentation."),ia.forEach(e),Et=f(s),as=l(s,"H4",{class:!0});var al=i(as);ys=l(al,"A",{id:!0,class:!0,href:!0});var zp=i(ys);we=l(zp,"SPAN",{});var Mp=i(we);_(Xs.$$.fragment,Mp),Mp.forEach(e),zp.forEach(e),lr=f(al),ke=l(al,"SPAN",{});var Cp=i(ke);nr=r(Cp,"Tokenization"),Cp.forEach(e),al.forEach(e),Dt=f(s),_(Zs.$$.fragment,s),At=f(s),_(Es.$$.fragment,s),It=f(s),es=l(s,"H3",{class:!0});var el=i(es);Ds=l(el,"A",{id:!0,class:!0,href:!0});var Lp=i(Ds);ye=l(Lp,"SPAN",{});var Np=i(ye);_(sa.$$.fragment,Np),Np.forEach(e),Lp.forEach(e),rr=f(el),Ee=l(el,"SPAN",{});var Op=i(Ee);pr=r(Op,"Filter"),Op.forEach(e),el.forEach(e),Tt=f(s),As=l(s,"P",{});var tl=i(As);ir=r(tl,"You can filter rows in the dataset based on a predicate function using "),Ya=l(tl,"A",{href:!0});var Fp=i(Ya);or=r(Fp,"datasets.Dataset.filter()"),Fp.forEach(e),hr=r(tl,". It returns rows that match a specified condition:"),tl.forEach(e),St=f(s),_(aa.$$.fragment,s),qt=f(s),ts=l(s,"P",{});var Ce=i(ts);Ha=l(Ce,"A",{href:!0});var Yp=i(Ha);dr=r(Yp,"datasets.Dataset.filter()"),Yp.forEach(e),fr=r(Ce," can also filter by indices if you set "),De=l(Ce,"CODE",{});var Hp=i(De);cr=r(Hp,"with_indices=True"),Hp.forEach(e),mr=r(Ce,":"),Ce.forEach(e),Pt=f(s),_(ea.$$.fragment,s),zt=f(s),ls=l(s,"H2",{class:!0});var ll=i(ls);Is=l(ll,"A",{id:!0,class:!0,href:!0});var Rp=i(Is);Ae=l(Rp,"SPAN",{});var Bp=i(Ae);_(ta.$$.fragment,Bp),Bp.forEach(e),Rp.forEach(e),ur=f(ll),Ie=l(ll,"SPAN",{});var Vp=i(Ie);_r=r(Vp,"Stream in a training loop"),Vp.forEach(e),ll.forEach(e),Mt=f(s),la=l(s,"P",{});var jr=i(la);Ra=l(jr,"A",{href:!0});var Jp=i(Ra);gr=r(Jp,"datasets.IterableDataset"),Jp.forEach(e),br=r(jr," can be integrated into a training loop. First, shuffle the dataset:"),jr.forEach(e),Ct=f(s),_(na.$$.fragment,s),Lt=f(s),Ba=l(s,"P",{});var Up=i(Ba);vr=r(Up,"Lastly, create a simple training loop and start training:"),Up.forEach(e),Nt=f(s),_(ra.$$.fragment,s),this.h()},h(){h(c,"name","hf:doc:metadata"),h(c,"content",JSON.stringify(ni)),h(w,"id","stream"),h(w,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(w,"href","#stream"),h(m,"class","relative group"),h(fa,"class","block dark:hidden"),Wp(fa.src,xr="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/datasets/streaming.gif")||h(fa,"src",xr),h(ca,"class","hidden dark:block"),Wp(ca.src,$r="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/datasets/streaming-dark.gif")||h(ca,"src",$r),h(J,"class","flex justify-center"),h(Ps,"href","https://huggingface.co/datasets/oscar"),h(Ps,"rel","nofollow"),h(ma,"href","/docs/datasets/pr_3878/en/package_reference/loading_methods#datasets.load_dataset"),h(ua,"href","/docs/datasets/pr_3878/en/package_reference/main_classes#datasets.Dataset"),h(_a,"href","/docs/datasets/pr_3878/en/package_reference/main_classes#datasets.IterableDataset"),h(is,"id","shuffle"),h(is,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(is,"href","#shuffle"),h(U,"class","relative group"),h(ga,"href","/docs/datasets/pr_3878/en/package_reference/main_classes#datasets.Dataset"),h(ba,"href","/docs/datasets/pr_3878/en/package_reference/main_classes#datasets.IterableDataset"),h(va,"href","/docs/datasets/pr_3878/en/package_reference/main_classes#datasets.IterableDataset.shuffle"),h(ja,"href","/docs/datasets/pr_3878/en/package_reference/main_classes#datasets.IterableDataset.shuffle"),h(hs,"id","reshuffle"),h(hs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(hs,"href","#reshuffle"),h(W,"class","relative group"),h(cs,"id","split-dataset"),h(cs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(cs,"href","#split-dataset"),h(G,"class","relative group"),h(wa,"href","/docs/datasets/pr_3878/en/package_reference/main_classes#datasets.IterableDataset.take"),h(ya,"href","/docs/datasets/pr_3878/en/package_reference/main_classes#datasets.IterableDataset.skip"),h(Ea,"id","interleave_datasets"),h(gs,"id","interleave"),h(gs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(gs,"href","#interleave"),h(K,"class","relative group"),h(Da,"href","/docs/datasets/pr_3878/en/package_reference/main_classes#datasets.interleave_datasets"),h(Aa,"href","/docs/datasets/pr_3878/en/package_reference/main_classes#datasets.IterableDataset"),h(vs,"id","remove"),h(vs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(vs,"href","#remove"),h(X,"class","relative group"),h(Ia,"href","/docs/datasets/pr_3878/en/package_reference/main_classes#datasets.IterableDataset.remove_columns"),h(xs,"id","map"),h(xs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(xs,"href","#map"),h(Z,"class","relative group"),h(Ta,"href","/docs/datasets/pr_3878/en/package_reference/main_classes#datasets.Dataset.map"),h(Sa,"href","/docs/datasets/pr_3878/en/package_reference/main_classes#datasets.Dataset"),h(qa,"href","/docs/datasets/pr_3878/en/package_reference/main_classes#datasets.IterableDataset.map"),h(Pa,"href","/docs/datasets/pr_3878/en/package_reference/main_classes#datasets.IterableDataset"),h(za,"href","/docs/datasets/pr_3878/en/package_reference/main_classes#datasets.IterableDataset.map"),h(Ca,"href","/docs/datasets/pr_3878/en/package_reference/main_classes#datasets.IterableDataset"),h(La,"href","/docs/datasets/pr_3878/en/package_reference/main_classes#datasets.IterableDataset.map"),h(Na,"href","/docs/datasets/pr_3878/en/package_reference/main_classes#datasets.IterableDataset.map"),h(Oa,"href","/docs/datasets/pr_3878/en/package_reference/main_classes#datasets.IterableDataset.map"),h(ks,"id","batch-processing"),h(ks,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(ks,"href","#batch-processing"),h(ss,"class","relative group"),h(Fa,"href","/docs/datasets/pr_3878/en/package_reference/main_classes#datasets.IterableDataset.map"),h(ys,"id","tokenization"),h(ys,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(ys,"href","#tokenization"),h(as,"class","relative group"),h(Ds,"id","filter"),h(Ds,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(Ds,"href","#filter"),h(es,"class","relative group"),h(Ya,"href","/docs/datasets/pr_3878/en/package_reference/main_classes#datasets.Dataset.filter"),h(Ha,"href","/docs/datasets/pr_3878/en/package_reference/main_classes#datasets.Dataset.filter"),h(Is,"id","stream-in-a-training-loop"),h(Is,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(Is,"href","#stream-in-a-training-loop"),h(ls,"class","relative group"),h(Ra,"href","/docs/datasets/pr_3878/en/package_reference/main_classes#datasets.IterableDataset")},m(s,p){a(document.head,c),o(s,$,p),o(s,m,p),a(m,w),a(w,k),g(x,k,null),a(m,y),a(m,E),a(E,P),o(s,D,p),o(s,T,p),a(T,Y),o(s,ns,p),o(s,q,p),a(q,A),a(A,oa),a(q,ha),a(q,rs),a(rs,da),o(s,Le,p),o(s,J,p),a(J,fa),a(J,rl),a(J,ca),o(s,Ne,p),o(s,z,p),a(z,pl),a(z,Ps),a(Ps,il),a(z,ol),a(z,Ga),a(Ga,hl),a(z,dl),a(z,ma),a(ma,fl),a(z,cl),o(s,Oe,p),g(zs,s,p),o(s,Fe,p),o(s,H,p),a(H,ml),a(H,ua),a(ua,ul),a(H,_l),a(H,_a),a(_a,gl),a(H,bl),o(s,Ye,p),g(ps,s,p),o(s,He,p),o(s,U,p),a(U,is),a(is,Ka),g(Ms,Ka,null),a(U,vl),a(U,Qa),a(Qa,jl),o(s,Re,p),o(s,M,p),a(M,xl),a(M,ga),a(ga,$l),a(M,wl),a(M,ba),a(ba,kl),a(M,yl),a(M,va),a(va,El),a(M,Dl),o(s,Be,p),o(s,C,p),a(C,Al),a(C,Xa),a(Xa,Il),a(C,Tl),a(C,Za),a(Za,Sl),a(C,ql),a(C,ja),a(ja,Pl),a(C,zl),o(s,Ve,p),g(Cs,s,p),o(s,Je,p),g(os,s,p),o(s,Ue,p),o(s,W,p),a(W,hs),a(hs,se),g(Ls,se,null),a(W,Ml),a(W,ae),a(ae,Cl),o(s,We,p),o(s,ds,p),a(ds,Ll),a(ds,ee),a(ee,Nl),a(ds,Ol),o(s,Ge,p),o(s,fs,p),a(fs,Fl),a(fs,te),a(te,Yl),a(fs,Hl),o(s,Ke,p),g(Ns,s,p),o(s,Qe,p),o(s,G,p),a(G,cs),a(cs,le),g(Os,le,null),a(G,Rl),a(G,ne),a(ne,Bl),o(s,Xe,p),o(s,xa,p),a(xa,Vl),o(s,Ze,p),o(s,$a,p),a($a,ms),a(ms,wa),a(wa,Jl),a(ms,Ul),a(ms,re),a(re,Wl),a(ms,Gl),o(s,st,p),g(Fs,s,p),o(s,at,p),o(s,ka,p),a(ka,us),a(us,ya),a(ya,Kl),a(us,Ql),a(us,pe),a(pe,Xl),a(us,Zl),o(s,et,p),g(Ys,s,p),o(s,tt,p),g(_s,s,p),o(s,lt,p),o(s,Ea,p),o(s,nt,p),o(s,K,p),a(K,gs),a(gs,ie),g(Hs,ie,null),a(K,sn),a(K,oe),a(oe,an),o(s,rt,p),o(s,Q,p),a(Q,Da),a(Da,en),a(Q,tn),a(Q,Aa),a(Aa,ln),a(Q,nn),o(s,pt,p),g(Rs,s,p),o(s,it,p),o(s,bs,p),a(bs,rn),a(bs,he),a(he,pn),a(bs,on),o(s,ot,p),g(Bs,s,p),o(s,ht,p),o(s,R,p),a(R,hn),a(R,de),a(de,dn),a(R,fn),a(R,fe),a(fe,cn),a(R,mn),o(s,dt,p),o(s,X,p),a(X,vs),a(vs,ce),g(Vs,ce,null),a(X,un),a(X,me),a(me,_n),o(s,ft,p),o(s,js,p),a(js,gn),a(js,Ia),a(Ia,bn),a(js,vn),o(s,ct,p),g(Js,s,p),o(s,mt,p),o(s,Z,p),a(Z,xs),a(xs,ue),g(Us,ue,null),a(Z,jn),a(Z,_e),a(_e,xn),o(s,ut,p),o(s,I,p),a(I,$n),a(I,Ta),a(Ta,wn),a(I,kn),a(I,Sa),a(Sa,yn),a(I,En),a(I,qa),a(qa,Dn),a(I,An),a(I,Pa),a(Pa,In),a(I,Tn),a(I,za),a(za,Sn),a(I,qn),o(s,_t,p),o(s,Ma,p),a(Ma,Pn),o(s,gt,p),o(s,B,p),a(B,zn),a(B,Ca),a(Ca,Mn),a(B,Cn),a(B,ge),a(ge,Ln),a(B,Nn),o(s,bt,p),g(Ws,s,p),o(s,vt,p),o(s,$s,p),a($s,On),a($s,La),a(La,Fn),a($s,Yn),o(s,jt,p),g(Gs,s,p),o(s,xt,p),o(s,ws,p),a(ws,Hn),a(ws,Na),a(Na,Rn),a(ws,Bn),o(s,$t,p),o(s,V,p),a(V,Vn),a(V,be),a(be,Jn),a(V,Un),a(V,Oa),a(Oa,Wn),a(V,Gn),o(s,wt,p),g(Ks,s,p),o(s,kt,p),o(s,ss,p),a(ss,ks),a(ks,ve),g(Qs,ve,null),a(ss,Kn),a(ss,je),a(je,Qn),o(s,yt,p),o(s,O,p),a(O,Fa),a(Fa,Xn),a(O,Zn),a(O,xe),a(xe,sr),a(O,ar),a(O,$e),a($e,er),a(O,tr),o(s,Et,p),o(s,as,p),a(as,ys),a(ys,we),g(Xs,we,null),a(as,lr),a(as,ke),a(ke,nr),o(s,Dt,p),g(Zs,s,p),o(s,At,p),g(Es,s,p),o(s,It,p),o(s,es,p),a(es,Ds),a(Ds,ye),g(sa,ye,null),a(es,rr),a(es,Ee),a(Ee,pr),o(s,Tt,p),o(s,As,p),a(As,ir),a(As,Ya),a(Ya,or),a(As,hr),o(s,St,p),g(aa,s,p),o(s,qt,p),o(s,ts,p),a(ts,Ha),a(Ha,dr),a(ts,fr),a(ts,De),a(De,cr),a(ts,mr),o(s,Pt,p),g(ea,s,p),o(s,zt,p),o(s,ls,p),a(ls,Is),a(Is,Ae),g(ta,Ae,null),a(ls,ur),a(ls,Ie),a(Ie,_r),o(s,Mt,p),o(s,la,p),a(la,Ra),a(Ra,gr),a(la,br),o(s,Ct,p),g(na,s,p),o(s,Lt,p),o(s,Ba,p),a(Ba,vr),o(s,Nt,p),g(ra,s,p),Ot=!0},p(s,[p]){const pa={};p&2&&(pa.$$scope={dirty:p,ctx:s}),ps.$set(pa);const Te={};p&2&&(Te.$$scope={dirty:p,ctx:s}),os.$set(Te);const Se={};p&2&&(Se.$$scope={dirty:p,ctx:s}),_s.$set(Se);const qe={};p&2&&(qe.$$scope={dirty:p,ctx:s}),Es.$set(qe)},i(s){Ot||(b(x.$$.fragment,s),b(zs.$$.fragment,s),b(ps.$$.fragment,s),b(Ms.$$.fragment,s),b(Cs.$$.fragment,s),b(os.$$.fragment,s),b(Ls.$$.fragment,s),b(Ns.$$.fragment,s),b(Os.$$.fragment,s),b(Fs.$$.fragment,s),b(Ys.$$.fragment,s),b(_s.$$.fragment,s),b(Hs.$$.fragment,s),b(Rs.$$.fragment,s),b(Bs.$$.fragment,s),b(Vs.$$.fragment,s),b(Js.$$.fragment,s),b(Us.$$.fragment,s),b(Ws.$$.fragment,s),b(Gs.$$.fragment,s),b(Ks.$$.fragment,s),b(Qs.$$.fragment,s),b(Xs.$$.fragment,s),b(Zs.$$.fragment,s),b(Es.$$.fragment,s),b(sa.$$.fragment,s),b(aa.$$.fragment,s),b(ea.$$.fragment,s),b(ta.$$.fragment,s),b(na.$$.fragment,s),b(ra.$$.fragment,s),Ot=!0)},o(s){v(x.$$.fragment,s),v(zs.$$.fragment,s),v(ps.$$.fragment,s),v(Ms.$$.fragment,s),v(Cs.$$.fragment,s),v(os.$$.fragment,s),v(Ls.$$.fragment,s),v(Ns.$$.fragment,s),v(Os.$$.fragment,s),v(Fs.$$.fragment,s),v(Ys.$$.fragment,s),v(_s.$$.fragment,s),v(Hs.$$.fragment,s),v(Rs.$$.fragment,s),v(Bs.$$.fragment,s),v(Vs.$$.fragment,s),v(Js.$$.fragment,s),v(Us.$$.fragment,s),v(Ws.$$.fragment,s),v(Gs.$$.fragment,s),v(Ks.$$.fragment,s),v(Qs.$$.fragment,s),v(Xs.$$.fragment,s),v(Zs.$$.fragment,s),v(Es.$$.fragment,s),v(sa.$$.fragment,s),v(aa.$$.fragment,s),v(ea.$$.fragment,s),v(ta.$$.fragment,s),v(na.$$.fragment,s),v(ra.$$.fragment,s),Ot=!1},d(s){e(c),s&&e($),s&&e(m),j(x),s&&e(D),s&&e(T),s&&e(ns),s&&e(q),s&&e(Le),s&&e(J),s&&e(Ne),s&&e(z),s&&e(Oe),j(zs,s),s&&e(Fe),s&&e(H),s&&e(Ye),j(ps,s),s&&e(He),s&&e(U),j(Ms),s&&e(Re),s&&e(M),s&&e(Be),s&&e(C),s&&e(Ve),j(Cs,s),s&&e(Je),j(os,s),s&&e(Ue),s&&e(W),j(Ls),s&&e(We),s&&e(ds),s&&e(Ge),s&&e(fs),s&&e(Ke),j(Ns,s),s&&e(Qe),s&&e(G),j(Os),s&&e(Xe),s&&e(xa),s&&e(Ze),s&&e($a),s&&e(st),j(Fs,s),s&&e(at),s&&e(ka),s&&e(et),j(Ys,s),s&&e(tt),j(_s,s),s&&e(lt),s&&e(Ea),s&&e(nt),s&&e(K),j(Hs),s&&e(rt),s&&e(Q),s&&e(pt),j(Rs,s),s&&e(it),s&&e(bs),s&&e(ot),j(Bs,s),s&&e(ht),s&&e(R),s&&e(dt),s&&e(X),j(Vs),s&&e(ft),s&&e(js),s&&e(ct),j(Js,s),s&&e(mt),s&&e(Z),j(Us),s&&e(ut),s&&e(I),s&&e(_t),s&&e(Ma),s&&e(gt),s&&e(B),s&&e(bt),j(Ws,s),s&&e(vt),s&&e($s),s&&e(jt),j(Gs,s),s&&e(xt),s&&e(ws),s&&e($t),s&&e(V),s&&e(wt),j(Ks,s),s&&e(kt),s&&e(ss),j(Qs),s&&e(yt),s&&e(O),s&&e(Et),s&&e(as),j(Xs),s&&e(Dt),j(Zs,s),s&&e(At),j(Es,s),s&&e(It),s&&e(es),j(sa),s&&e(Tt),s&&e(As),s&&e(St),j(aa,s),s&&e(qt),s&&e(ts),s&&e(Pt),j(ea,s),s&&e(zt),s&&e(ls),j(ta),s&&e(Mt),s&&e(la),s&&e(Ct),j(na,s),s&&e(Lt),s&&e(Ba),s&&e(Nt),j(ra,s)}}}const ni={local:"stream",sections:[{local:"shuffle",title:"Shuffle"},{local:"reshuffle",title:"Reshuffle"},{local:"split-dataset",title:"Split dataset"},{local:"interleave",title:"Interleave"},{local:"remove",title:"Remove"},{local:"map",sections:[{local:"batch-processing",sections:[{local:"tokenization",title:"Tokenization"}],title:"Batch processing"},{local:"filter",title:"Filter"}],title:"Map"},{local:"stream-in-a-training-loop",title:"Stream in a training loop"}],title:"Stream"};function ri(N,c,$){let{fw:m}=c;return N.$$set=w=>{"fw"in w&&$(0,m=w.fw)},[m]}class fi extends Gp{constructor(c){super();Kp(this,c,ri,li,Qp,{fw:0})}}export{fi as default,ni as metadata};
