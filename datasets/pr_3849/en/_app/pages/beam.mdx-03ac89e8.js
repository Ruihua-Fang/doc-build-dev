import{S as Ve,i as Xe,s as Ze,e as r,k as _,w as M,t as n,M as ea,c as o,d as a,m as d,a as i,x as R,h as p,b as l,F as t,g as f,y as F,q as L,o as K,B as U}from"../chunks/vendor-aa873a46.js";import{T as aa}from"../chunks/Tip-f7f252ab.js";import{I as ta}from"../chunks/IconCopyLink-d0ca3106.js";import{C as fe}from"../chunks/CodeBlock-1f14baf3.js";function sa(J){let c,E;return{c(){c=r("p"),E=n("When you run your pipeline, you can adjust the parameters to change the runner (Flink or Spark), output location (S3 bucket or HDFS), and the number of workers.")},l(u){c=o(u,"P",{});var m=i(c);E=p(m,"When you run your pipeline, you can adjust the parameters to change the runner (Flink or Spark), output location (S3 bucket or HDFS), and the number of workers."),m.forEach(a)},m(u,m){f(u,c,m),t(c,E)},d(u){u&&a(c)}}}function ra(J){let c,E,u,m,W,y,ue,H,ce,X,h,me,v,he,_e,k,de,$e,A,Ee,be,j,ge,we,Z,$,ye,T,ve,ke,S,Ae,je,P,Te,Se,ee,G,Y,Ce,ae,C,te,q,z,qe,se,B,re,x,Q,Be,oe,N,le,I,V,xe,ne,D,ie,g,pe;return y=new ta({}),C=new fe({props:{code:`DATASET_NAME=your_dataset_name  # ex: wikipedia
CONFIG_NAME=your_config_name    # ex: 20200501.en`,highlighted:`<span class="hljs-attr">DATASET_NAME</span>=your_dataset_name  <span class="hljs-comment"># ex: wikipedia</span>
<span class="hljs-attr">CONFIG_NAME</span>=your_config_name    <span class="hljs-comment"># ex: 20200501.en</span>`}}),B=new fe({props:{code:`PROJECT=your_project
BUCKET=your_bucket
REGION=your_region`,highlighted:`<span class="hljs-attribute">PROJECT</span><span class="hljs-operator">=</span>your_project
<span class="hljs-attribute">BUCKET</span><span class="hljs-operator">=</span>your_bucket
<span class="hljs-attribute">REGION</span><span class="hljs-operator">=</span>your_region`}}),N=new fe({props:{code:`echo "datasets" > /tmp/beam_requirements.txt
echo "apache_beam" >> /tmp/beam_requirements.txt`,highlighted:`echo <span class="hljs-string">&quot;datasets&quot;</span> &gt; <span class="hljs-regexp">/tmp/</span>beam_requirements.txt
echo <span class="hljs-string">&quot;apache_beam&quot;</span> &gt;&gt; <span class="hljs-regexp">/tmp/</span>beam_requirements.txt`}}),D=new fe({props:{code:`datasets-cli run_beam datasets/$DATASET_NAME \\
--name $CONFIG_NAME \\
--save_infos \\
--cache_dir gs://$BUCKET/cache/datasets \\
--beam_pipeline_options=\\
"runner=DataflowRunner,project=$PROJECT,job_name=$DATASET_NAME-gen,"\\
"staging_location=gs://$BUCKET/binaries,temp_location=gs://$BUCKET/temp,"\\
"region=$REGION,requirements_file=/tmp/beam_requirements.txt"`,highlighted:`datasets-cli run_beam datasets/<span class="hljs-variable">$DATASET_NAME</span> \\
--name <span class="hljs-variable">$CONFIG_NAME</span> \\
--save_infos \\
--cache_dir gs://<span class="hljs-variable">$BUCKET</span>/cache/datasets \\
--beam_pipeline_options=\\
<span class="hljs-string">&quot;runner=DataflowRunner,project=<span class="hljs-variable">$PROJECT</span>,job_name=<span class="hljs-variable">$DATASET_NAME</span>-gen,&quot;</span>\\
<span class="hljs-string">&quot;staging_location=gs://<span class="hljs-variable">$BUCKET</span>/binaries,temp_location=gs://<span class="hljs-variable">$BUCKET</span>/temp,&quot;</span>\\
<span class="hljs-string">&quot;region=<span class="hljs-variable">$REGION</span>,requirements_file=/tmp/beam_requirements.txt&quot;</span>`}}),g=new aa({props:{$$slots:{default:[sa]},$$scope:{ctx:J}}}),{c(){c=r("meta"),E=_(),u=r("h1"),m=r("a"),W=r("span"),M(y.$$.fragment),ue=_(),H=r("span"),ce=n("Beam Datasets"),X=_(),h=r("p"),me=n("Some datasets are too large to be processed on a single machine. Instead, you can process them with "),v=r("a"),he=n("Apache Beam"),_e=n(", a library for parallel data processing. The processing pipeline is executed on a distributed processing backend such as "),k=r("a"),de=n("Apache Flink"),$e=n(", "),A=r("a"),Ee=n("Apache Spark"),be=n(", or "),j=r("a"),ge=n("Google Cloud Dataflow"),we=n("."),Z=_(),$=r("p"),ye=n("We have already created Beam pipelines for some of the larger datasets like "),T=r("a"),ve=n("wikipedia"),ke=n(", and "),S=r("a"),Ae=n("wiki40b"),je=n(". You can load these normally with "),P=r("a"),Te=n("load_dataset()"),Se=n(". But if you want to run your own Beam pipeline with Dataflow, here is how:"),ee=_(),G=r("ol"),Y=r("li"),Ce=n("Specify the dataset and configuration you want to process:"),ae=_(),M(C.$$.fragment),te=_(),q=r("ol"),z=r("li"),qe=n("Input your Google Cloud Platform information:"),se=_(),M(B.$$.fragment),re=_(),x=r("ol"),Q=r("li"),Be=n("Specify your Python requirements:"),oe=_(),M(N.$$.fragment),le=_(),I=r("ol"),V=r("li"),xe=n("Run the pipeline:"),ne=_(),M(D.$$.fragment),ie=_(),M(g.$$.fragment),this.h()},l(e){const s=ea('[data-svelte="svelte-1phssyn"]',document.head);c=o(s,"META",{name:!0,content:!0}),s.forEach(a),E=d(e),u=o(e,"H1",{class:!0});var O=i(u);m=o(O,"A",{id:!0,class:!0,href:!0});var Ne=i(m);W=o(Ne,"SPAN",{});var Ie=i(W);R(y.$$.fragment,Ie),Ie.forEach(a),Ne.forEach(a),ue=d(O),H=o(O,"SPAN",{});var De=i(H);ce=p(De,"Beam Datasets"),De.forEach(a),O.forEach(a),X=d(e),h=o(e,"P",{});var b=i(h);me=p(b,"Some datasets are too large to be processed on a single machine. Instead, you can process them with "),v=o(b,"A",{href:!0,rel:!0});var Oe=i(v);he=p(Oe,"Apache Beam"),Oe.forEach(a),_e=p(b,", a library for parallel data processing. The processing pipeline is executed on a distributed processing backend such as "),k=o(b,"A",{href:!0,rel:!0});var Pe=i(k);de=p(Pe,"Apache Flink"),Pe.forEach(a),$e=p(b,", "),A=o(b,"A",{href:!0,rel:!0});var Ge=i(A);Ee=p(Ge,"Apache Spark"),Ge.forEach(a),be=p(b,", or "),j=o(b,"A",{href:!0,rel:!0});var Me=i(j);ge=p(Me,"Google Cloud Dataflow"),Me.forEach(a),we=p(b,"."),b.forEach(a),Z=d(e),$=o(e,"P",{});var w=i($);ye=p(w,"We have already created Beam pipelines for some of the larger datasets like "),T=o(w,"A",{href:!0,rel:!0});var Re=i(T);ve=p(Re,"wikipedia"),Re.forEach(a),ke=p(w,", and "),S=o(w,"A",{href:!0,rel:!0});var Fe=i(S);Ae=p(Fe,"wiki40b"),Fe.forEach(a),je=p(w,". You can load these normally with "),P=o(w,"A",{href:!0});var Le=i(P);Te=p(Le,"load_dataset()"),Le.forEach(a),Se=p(w,". But if you want to run your own Beam pipeline with Dataflow, here is how:"),w.forEach(a),ee=d(e),G=o(e,"OL",{});var Ke=i(G);Y=o(Ke,"LI",{});var Ue=i(Y);Ce=p(Ue,"Specify the dataset and configuration you want to process:"),Ue.forEach(a),Ke.forEach(a),ae=d(e),R(C.$$.fragment,e),te=d(e),q=o(e,"OL",{start:!0});var Je=i(q);z=o(Je,"LI",{});var We=i(z);qe=p(We,"Input your Google Cloud Platform information:"),We.forEach(a),Je.forEach(a),se=d(e),R(B.$$.fragment,e),re=d(e),x=o(e,"OL",{start:!0});var He=i(x);Q=o(He,"LI",{});var Ye=i(Q);Be=p(Ye,"Specify your Python requirements:"),Ye.forEach(a),He.forEach(a),oe=d(e),R(N.$$.fragment,e),le=d(e),I=o(e,"OL",{start:!0});var ze=i(I);V=o(ze,"LI",{});var Qe=i(V);xe=p(Qe,"Run the pipeline:"),Qe.forEach(a),ze.forEach(a),ne=d(e),R(D.$$.fragment,e),ie=d(e),R(g.$$.fragment,e),this.h()},h(){l(c,"name","hf:doc:metadata"),l(c,"content",JSON.stringify(oa)),l(m,"id","beam-datasets"),l(m,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(m,"href","#beam-datasets"),l(u,"class","relative group"),l(v,"href","https://beam.apache.org/"),l(v,"rel","nofollow"),l(k,"href","https://flink.apache.org/"),l(k,"rel","nofollow"),l(A,"href","https://spark.apache.org/"),l(A,"rel","nofollow"),l(j,"href","https://cloud.google.com/dataflow"),l(j,"rel","nofollow"),l(T,"href","https://huggingface.co/datasets/wikipedia"),l(T,"rel","nofollow"),l(S,"href","https://huggingface.co/datasets/wiki40b"),l(S,"rel","nofollow"),l(P,"href","/docs/datasets/pr_3849/en/package_reference/loading_methods#datasets.load_dataset"),l(q,"start","2"),l(x,"start","3"),l(I,"start","4")},m(e,s){t(document.head,c),f(e,E,s),f(e,u,s),t(u,m),t(m,W),F(y,W,null),t(u,ue),t(u,H),t(H,ce),f(e,X,s),f(e,h,s),t(h,me),t(h,v),t(v,he),t(h,_e),t(h,k),t(k,de),t(h,$e),t(h,A),t(A,Ee),t(h,be),t(h,j),t(j,ge),t(h,we),f(e,Z,s),f(e,$,s),t($,ye),t($,T),t(T,ve),t($,ke),t($,S),t(S,Ae),t($,je),t($,P),t(P,Te),t($,Se),f(e,ee,s),f(e,G,s),t(G,Y),t(Y,Ce),f(e,ae,s),F(C,e,s),f(e,te,s),f(e,q,s),t(q,z),t(z,qe),f(e,se,s),F(B,e,s),f(e,re,s),f(e,x,s),t(x,Q),t(Q,Be),f(e,oe,s),F(N,e,s),f(e,le,s),f(e,I,s),t(I,V),t(V,xe),f(e,ne,s),F(D,e,s),f(e,ie,s),F(g,e,s),pe=!0},p(e,[s]){const O={};s&2&&(O.$$scope={dirty:s,ctx:e}),g.$set(O)},i(e){pe||(L(y.$$.fragment,e),L(C.$$.fragment,e),L(B.$$.fragment,e),L(N.$$.fragment,e),L(D.$$.fragment,e),L(g.$$.fragment,e),pe=!0)},o(e){K(y.$$.fragment,e),K(C.$$.fragment,e),K(B.$$.fragment,e),K(N.$$.fragment,e),K(D.$$.fragment,e),K(g.$$.fragment,e),pe=!1},d(e){a(c),e&&a(E),e&&a(u),U(y),e&&a(X),e&&a(h),e&&a(Z),e&&a($),e&&a(ee),e&&a(G),e&&a(ae),U(C,e),e&&a(te),e&&a(q),e&&a(se),U(B,e),e&&a(re),e&&a(x),e&&a(oe),U(N,e),e&&a(le),e&&a(I),e&&a(ne),U(D,e),e&&a(ie),U(g,e)}}}const oa={local:"beam-datasets",title:"Beam Datasets"};function la(J,c,E){let{fw:u}=c;return J.$$set=m=>{"fw"in m&&E(0,u=m.fw)},[u]}class ua extends Ve{constructor(c){super();Xe(this,c,la,ra,Ze,{fw:0})}}export{ua as default,oa as metadata};
