import{S as mp,i as up,s as _p,e as n,k as d,w as _,t,M as gp,c as r,d as a,m as f,a as o,x as g,h as l,b as h,N as cp,F as s,g as i,y as b,q as v,o as j,B as $}from"../chunks/vendor-e67aec41.js";import{T as Ot}from"../chunks/Tip-76459d1c.js";import{I as B}from"../chunks/IconCopyLink-ffd7f84e.js";import{C as z}from"../chunks/CodeBlock-e2bcf023.js";import{C as bp}from"../chunks/CodeBlockFw-1e02e2ba.js";function vp(N){let c,w,m,x,k,u,y,E;return{c(){c=n("p"),w=t("An "),m=n("a"),x=t("datasets.IterableDataset"),k=t(" is useful for iterative jobs like training a model. You shouldn\u2019t use a "),u=n("a"),y=t("datasets.IterableDataset"),E=t(" for jobs that require random access to examples because you have to iterate all over it using a for loop. Getting the last example in an iterable dataset would require you to iterate over all the previous examples."),this.h()},l(q){c=r(q,"P",{});var D=o(c);w=l(D,"An "),m=r(D,"A",{href:!0});var T=o(m);x=l(T,"datasets.IterableDataset"),T.forEach(a),k=l(D," is useful for iterative jobs like training a model. You shouldn\u2019t use a "),u=r(D,"A",{href:!0});var F=o(u);y=l(F,"datasets.IterableDataset"),F.forEach(a),E=l(D," for jobs that require random access to examples because you have to iterate all over it using a for loop. Getting the last example in an iterable dataset would require you to iterate over all the previous examples."),D.forEach(a),this.h()},h(){h(m,"href","/docs/datasets/pr_3801/en/package_reference/main_classes#datasets.IterableDataset"),h(u,"href","/docs/datasets/pr_3801/en/package_reference/main_classes#datasets.IterableDataset")},m(q,D){i(q,c,D),s(c,w),s(c,m),s(m,x),s(c,k),s(c,u),s(u,y),s(c,E)},d(q){q&&a(c)}}}function jp(N){let c,w,m,x;return{c(){c=n("p"),w=n("a"),m=t("datasets.IterableDataset.shuffle()"),x=t(" will also shuffle the order of the shards if the dataset is sharded into multiple sets."),this.h()},l(k){c=r(k,"P",{});var u=o(c);w=r(u,"A",{href:!0});var y=o(w);m=l(y,"datasets.IterableDataset.shuffle()"),y.forEach(a),x=l(u," will also shuffle the order of the shards if the dataset is sharded into multiple sets."),u.forEach(a),this.h()},h(){h(w,"href","/docs/datasets/pr_3801/en/package_reference/main_classes#datasets.IterableDataset.shuffle")},m(k,u){i(k,c,u),s(c,w),s(w,m),s(c,x)},d(k){k&&a(c)}}}function $p(N){let c,w,m,x,k,u,y,E,q,D,T,F,te;return{c(){c=n("p"),w=n("code"),m=t("take"),x=t(" and "),k=n("code"),u=t("skip"),y=t(" prevent future calls to "),E=n("code"),q=t("shuffle"),D=t(" because they lock in the order of the shards. You should "),T=n("code"),F=t("shuffle"),te=t(" your dataset before splitting it.")},l(S){c=r(S,"P",{});var I=o(c);w=r(I,"CODE",{});var as=o(w);m=l(as,"take"),as.forEach(a),x=l(I," and "),k=r(I,"CODE",{});var ts=o(k);u=l(ts,"skip"),ts.forEach(a),y=l(I," prevent future calls to "),E=r(I,"CODE",{});var le=o(E);q=l(le,"shuffle"),le.forEach(a),D=l(I," because they lock in the order of the shards. You should "),T=r(I,"CODE",{});var ls=o(T);F=l(ls,"shuffle"),ls.forEach(a),te=l(I," your dataset before splitting it."),I.forEach(a)},m(S,I){i(S,c,I),s(c,w),s(w,m),s(c,x),s(c,k),s(k,u),s(c,y),s(c,E),s(E,q),s(c,D),s(c,T),s(T,F),s(c,te)},d(S){S&&a(c)}}}function wp(N){let c,w,m,x,k;return{c(){c=n("p"),w=t("See other examples of batch processing in "),m=n("a"),x=t("the batched map processing documentation"),k=t(". They work the same for iterable datasets."),this.h()},l(u){c=r(u,"P",{});var y=o(c);w=l(y,"See other examples of batch processing in "),m=r(y,"A",{href:!0});var E=o(m);x=l(E,"the batched map processing documentation"),E.forEach(a),k=l(y,". They work the same for iterable datasets."),y.forEach(a),this.h()},h(){h(m,"href","./process#batch-processing")},m(u,y){i(u,c,y),s(c,w),s(c,m),s(m,x),s(c,k)},d(u){u&&a(c)}}}function xp(N){let c,w,m,x,k,u,y,E,q,D,T,F,te,S,I,as,ts,le,ls,ka,J,ns,Jn,Ft,rs,Wn,ya,P,Ht,Ae,Rt,Yt,Fs,Vt,Bt,ps,Jt,Wt,Ea,Te,Da,H,Ut,os,Gt,Kt,is,Qt,Xt,Ia,ne,Aa,W,re,Hs,Se,Zt,Rs,el,Ta,M,sl,hs,al,tl,ds,ll,nl,fs,rl,pl,Sa,L,ol,Ys,il,hl,Vs,dl,fl,cs,cl,ml,qa,qe,za,pe,Pa,U,oe,Bs,ze,ul,Js,_l,Ma,ie,gl,Ws,bl,vl,La,he,jl,Us,$l,wl,Ca,Pe,Na,G,de,Gs,Me,xl,Ks,kl,Oa,ms,yl,Fa,us,fe,_s,El,Dl,Qs,Il,Al,Ha,Le,Ra,gs,ce,bs,Tl,Sl,Xs,ql,zl,Ya,Ce,Va,me,Ba,vs,Ja,K,ue,Zs,Ne,Pl,ea,Ml,Wa,Q,js,Ll,Cl,$s,Nl,Ol,Ua,Oe,Ga,_e,Fl,sa,Hl,Rl,Ka,Fe,Qa,R,Yl,aa,Vl,Bl,ta,Jl,Wl,Xa,X,ge,la,He,Ul,na,Gl,Za,be,Kl,ws,Ql,Xl,et,Re,st,Z,ve,ra,Ye,Zl,pa,en,at,A,sn,xs,an,tn,ks,ln,nn,ys,rn,pn,Es,on,hn,Ds,dn,fn,tt,Is,cn,lt,Y,mn,As,un,_n,oa,gn,bn,nt,Ve,rt,je,vn,Ts,jn,$n,pt,Be,ot,$e,wn,Ss,xn,kn,it,V,yn,ia,En,Dn,qs,In,An,ht,Je,dt,ee,we,ha,We,Tn,da,Sn,ft,O,zs,qn,zn,fa,Pn,Mn,ca,Ln,Cn,ct,se,xe,ma,Ue,Nn,ua,On,mt,Ge,ut,ke,_t,ae,ye,_a,Ke,Fn,ga,Hn,gt,Qe,Ps,Rn,Yn,bt,Xe,vt,Ms,Vn,jt,Ze,$t;return u=new B({}),Te=new z({props:{code:`from datasets import load_dataset
dataset = load_dataset('oscar', "unshuffled_deduplicated_en", split='train', streaming=True)
print(next(iter(dataset)))`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;oscar&#x27;</span>, <span class="hljs-string">&quot;unshuffled_deduplicated_en&quot;</span>, split=<span class="hljs-string">&#x27;train&#x27;</span>, streaming=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(<span class="hljs-built_in">next</span>(<span class="hljs-built_in">iter</span>(dataset)))
{<span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;Mtendere Village was inspired by the vision of Chief Napoleon Dzombe, which he shared with John Blanchard during his first visit to Malawi. Chief Napoleon conveyed the desperate need for a program to intervene and care for the orphans and vulnerable children (OVC) in Malawi, and John committed to help...</span>`}}),ne=new Ot({props:{$$slots:{default:[vp]},$$scope:{ctx:N}}}),Se=new B({}),qe=new z({props:{code:`from datasets import load_dataset
dataset = load_dataset('oscar', "unshuffled_deduplicated_en", split='train', streaming=True)
shuffled_dataset = dataset.shuffle(buffer_size=10_000, seed=42)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;oscar&#x27;</span>, <span class="hljs-string">&quot;unshuffled_deduplicated_en&quot;</span>, split=<span class="hljs-string">&#x27;train&#x27;</span>, streaming=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>shuffled_dataset = dataset.shuffle(buffer_size=<span class="hljs-number">10_000</span>, seed=<span class="hljs-number">42</span>)`}}),pe=new Ot({props:{$$slots:{default:[jp]},$$scope:{ctx:N}}}),ze=new B({}),Pe=new z({props:{code:`for epoch in range(epochs):
    shuffled_dataset.set_epoch(epoch)
    for example in shuffled_dataset:
        ...`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(epochs):
<span class="hljs-meta">... </span>    shuffled_dataset.set_epoch(epoch)
<span class="hljs-meta">... </span>    <span class="hljs-keyword">for</span> example <span class="hljs-keyword">in</span> shuffled_dataset:
<span class="hljs-meta">... </span>        ...`}}),Me=new B({}),Le=new z({props:{code:`dataset = load_dataset('oscar', "unshuffled_deduplicated_en", split='train', streaming=True)
dataset_head = dataset.take(2)
list(dataset_head)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;oscar&#x27;</span>, <span class="hljs-string">&quot;unshuffled_deduplicated_en&quot;</span>, split=<span class="hljs-string">&#x27;train&#x27;</span>, streaming=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset_head = dataset.take(<span class="hljs-number">2</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">list</span>(dataset_head)
[{<span class="hljs-string">&#x27;id&#x27;</span>: <span class="hljs-number">0</span>, <span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;Mtendere Village was...&#x27;</span>}, {<span class="hljs-string">&#x27;id&#x27;</span>: <span class="hljs-number">1</span>, <span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;Lily James cannot fight the music...&#x27;</span>}]`}}),Ce=new z({props:{code:"train_dataset = shuffled_dataset.skip(1000)",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>train_dataset = shuffled_dataset.skip(<span class="hljs-number">1000</span>)'}}),me=new Ot({props:{warning:"&lcub;true}",$$slots:{default:[$p]},$$scope:{ctx:N}}}),Ne=new B({}),Oe=new z({props:{code:`from datasets import interleave_datasets
from itertools import islice
en_dataset = load_dataset('oscar', "unshuffled_deduplicated_en", split='train', streaming=True)
fr_dataset = load_dataset('oscar', "unshuffled_deduplicated_fr", split='train', streaming=True)

multilingual_dataset = interleave_datasets([en_dataset, fr_dataset])
print(list(islice(multilingual_dataset, 2)))`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> interleave_datasets
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> itertools <span class="hljs-keyword">import</span> islice
<span class="hljs-meta">&gt;&gt;&gt; </span>en_dataset = load_dataset(<span class="hljs-string">&#x27;oscar&#x27;</span>, <span class="hljs-string">&quot;unshuffled_deduplicated_en&quot;</span>, split=<span class="hljs-string">&#x27;train&#x27;</span>, streaming=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>fr_dataset = load_dataset(<span class="hljs-string">&#x27;oscar&#x27;</span>, <span class="hljs-string">&quot;unshuffled_deduplicated_fr&quot;</span>, split=<span class="hljs-string">&#x27;train&#x27;</span>, streaming=<span class="hljs-literal">True</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>multilingual_dataset = interleave_datasets([en_dataset, fr_dataset])
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(<span class="hljs-built_in">list</span>(islice(multilingual_dataset, <span class="hljs-number">2</span>)))
[{<span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;Mtendere Village was inspired by the vision...&#x27;</span>}, {<span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&quot;M\xE9dia de d\xE9bat d&#x27;id\xE9es, de culture et de litt\xE9rature...&quot;</span>}]`}}),Fe=new z({props:{code:`multilingual_dataset_with_oversampling = interleave_datasets([en_dataset, fr_dataset], probabilities=[0.8, 0.2], seed=42)
print(list(islice(multilingual_dataset_with_oversampling, 2)))`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>multilingual_dataset_with_oversampling = interleave_datasets([en_dataset, fr_dataset], probabilities=[<span class="hljs-number">0.8</span>, <span class="hljs-number">0.2</span>], seed=<span class="hljs-number">42</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(<span class="hljs-built_in">list</span>(islice(multilingual_dataset_with_oversampling, <span class="hljs-number">2</span>)))
[{<span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;Mtendere Village was inspired by the vision...&#x27;</span>}, {<span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;Lily James cannot fight the music...&#x27;</span>}]`}}),He=new B({}),Re=new z({props:{code:`from datasets import load_dataset
dataset = load_dataset('mc4', 'en', streaming=True, split='train')
dataset = dataset.remove_columns('timestamp')`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;mc4&#x27;</span>, <span class="hljs-string">&#x27;en&#x27;</span>, streaming=<span class="hljs-literal">True</span>, split=<span class="hljs-string">&#x27;train&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = dataset.remove_columns(<span class="hljs-string">&#x27;timestamp&#x27;</span>)`}}),Ye=new B({}),Ve=new z({props:{code:`def add_prefix(example):
    example['text'] = 'My text: ' + example['text']
    return example`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">def</span> <span class="hljs-title function_">add_prefix</span>(<span class="hljs-params">example</span>):
<span class="hljs-meta">... </span>    example[<span class="hljs-string">&#x27;text&#x27;</span>] = <span class="hljs-string">&#x27;My text: &#x27;</span> + example[<span class="hljs-string">&#x27;text&#x27;</span>]
<span class="hljs-meta">... </span>    <span class="hljs-keyword">return</span> example`}}),Be=new z({props:{code:`from datasets import load_dataset
dataset = load_dataset('oscar', 'unshuffled_deduplicated_en', streaming=True, split='train')
updated_dataset = dataset.map(add_prefix)
list(updated_dataset.take(3))`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;oscar&#x27;</span>, <span class="hljs-string">&#x27;unshuffled_deduplicated_en&#x27;</span>, streaming=<span class="hljs-literal">True</span>, split=<span class="hljs-string">&#x27;train&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>updated_dataset = dataset.<span class="hljs-built_in">map</span>(add_prefix)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">list</span>(updated_dataset.take(<span class="hljs-number">3</span>))
[{<span class="hljs-string">&#x27;id&#x27;</span>: <span class="hljs-number">0</span>, <span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;My text: Mtendere Village was inspired by...&#x27;</span>},
 {<span class="hljs-string">&#x27;id&#x27;</span>: <span class="hljs-number">1</span>, <span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;My text: Lily James cannot fight the music...&#x27;</span>},
 {<span class="hljs-string">&#x27;id&#x27;</span>: <span class="hljs-number">2</span>, <span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;My text: &quot;I\\&#x27;d love to help kickstart...&#x27;</span>}]`}}),Je=new z({props:{code:`updated_dataset = dataset.map(add_prefix, remove_columns=["id"])
list(updated_dataset.take(3))`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>updated_dataset = dataset.<span class="hljs-built_in">map</span>(add_prefix, remove_columns=[<span class="hljs-string">&quot;id&quot;</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">list</span>(updated_dataset.take(<span class="hljs-number">3</span>))
[{<span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;My text: Mtendere Village was inspired by...&#x27;</span>},
 {<span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;My text: Lily James cannot fight the music...&#x27;</span>},
 {<span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;My text: &quot;I\\&#x27;d love to help kickstart...&#x27;</span>}]`}}),We=new B({}),Ue=new B({}),Ge=new z({props:{code:`from datasets import load_dataset
from transformers import AutoTokenizer
dataset = load_dataset("mc4", "en", streaming=True, split="train")
tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')
def encode(examples):
    return tokenizer(examples['text'], truncation=True, padding='max_length')
dataset = dataset.map(encode, batched=True, remove_columns=["text", "timestamp", "url"])
next(iter(dataset))`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;mc4&quot;</span>, <span class="hljs-string">&quot;en&quot;</span>, streaming=<span class="hljs-literal">True</span>, split=<span class="hljs-string">&quot;train&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&#x27;distilbert-base-uncased&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">def</span> <span class="hljs-title function_">encode</span>(<span class="hljs-params">examples</span>):
<span class="hljs-meta">... </span>    <span class="hljs-keyword">return</span> tokenizer(examples[<span class="hljs-string">&#x27;text&#x27;</span>], truncation=<span class="hljs-literal">True</span>, padding=<span class="hljs-string">&#x27;max_length&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = dataset.<span class="hljs-built_in">map</span>(encode, batched=<span class="hljs-literal">True</span>, remove_columns=[<span class="hljs-string">&quot;text&quot;</span>, <span class="hljs-string">&quot;timestamp&quot;</span>, <span class="hljs-string">&quot;url&quot;</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">next</span>(<span class="hljs-built_in">iter</span>(dataset))
{<span class="hljs-string">&#x27;input_ids&#x27;</span>: <span class="hljs-number">101</span>, <span class="hljs-number">8466</span>, <span class="hljs-number">1018</span>, <span class="hljs-number">1010</span>, <span class="hljs-number">4029</span>, <span class="hljs-number">2475</span>, <span class="hljs-number">2062</span>, <span class="hljs-number">18558</span>, <span class="hljs-number">3100</span>, <span class="hljs-number">2061</span>, ...,<span class="hljs-number">1106</span>, <span class="hljs-number">3739</span>, <span class="hljs-number">102</span>],
<span class="hljs-string">&#x27;attention_mask&#x27;</span>: [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, ..., <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]}`}}),ke=new Ot({props:{$$slots:{default:[wp]},$$scope:{ctx:N}}}),Ke=new B({}),Xe=new z({props:{code:`buffer_size, seed = 10_000, 42
dataset = dataset.shuffle(buffer_size, seed)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>buffer_size, seed = <span class="hljs-number">10_000</span>, <span class="hljs-number">42</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = dataset.shuffle(buffer_size, seed)`}}),Ze=new bp({props:{group1:{id:"pt",code:`import torch
from torch.utils.data import DataLoader
from transformers import AutoModelForMaskedLM, DataCollatorForLanguageModeling
from tqdm import tqdm
dataset = dataset.with_format("torch")
dataloader = DataLoader(dataset, collate_fn=DataCollatorForLanguageModeling(tokenizer))
device = 'cuda' if torch.cuda.is_available() else 'cpu' 
model = AutoModelForMaskedLM.from_pretrained("distilbert-base-uncased")
model.train().to(device)
optimizer = torch.optim.AdamW(params=model.parameters(), lr=1e-5)
for epoch in range(3):
    dataset.set_epoch(epoch)
    for i, batch in enumerate(tqdm(dataloader, total=5)):
        if i == 5:
            break
        batch = {k: v.to(device) for k, v in batch.items()}
        outputs = model(**batch)
        loss = outputs[0]
        loss.backward()
        optimizer.step()
        optimizer.zero_grad()
        if i % 10 == 0:
            print(f"loss: {loss}")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> DataLoader
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForMaskedLM, DataCollatorForLanguageModeling
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> tqdm <span class="hljs-keyword">import</span> tqdm
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = dataset.with_format(<span class="hljs-string">&quot;torch&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>dataloader = DataLoader(dataset, collate_fn=DataCollatorForLanguageModeling(tokenizer))
<span class="hljs-meta">&gt;&gt;&gt; </span>device = <span class="hljs-string">&#x27;cuda&#x27;</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">&#x27;cpu&#x27;</span> 
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMaskedLM.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.train().to(device)
<span class="hljs-meta">&gt;&gt;&gt; </span>optimizer = torch.optim.AdamW(params=model.parameters(), lr=<span class="hljs-number">1e-5</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">3</span>):
<span class="hljs-meta">... </span>    dataset.set_epoch(epoch)
<span class="hljs-meta">... </span>    <span class="hljs-keyword">for</span> i, batch <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(tqdm(dataloader, total=<span class="hljs-number">5</span>)):
<span class="hljs-meta">... </span>        <span class="hljs-keyword">if</span> i == <span class="hljs-number">5</span>:
<span class="hljs-meta">... </span>            <span class="hljs-keyword">break</span>
<span class="hljs-meta">... </span>        batch = {k: v.to(device) <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> batch.items()}
<span class="hljs-meta">... </span>        outputs = model(**batch)
<span class="hljs-meta">... </span>        loss = outputs[<span class="hljs-number">0</span>]
<span class="hljs-meta">... </span>        loss.backward()
<span class="hljs-meta">... </span>        optimizer.step()
<span class="hljs-meta">... </span>        optimizer.zero_grad()
<span class="hljs-meta">... </span>        <span class="hljs-keyword">if</span> i % <span class="hljs-number">10</span> == <span class="hljs-number">0</span>:
<span class="hljs-meta">... </span>            <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;loss: <span class="hljs-subst">{loss}</span>&quot;</span>)`},group2:{id:"tf",code:"# WIP",highlighted:'<span class="hljs-comment"># WIP</span>'}}}),{c(){c=n("meta"),w=d(),m=n("h1"),x=n("a"),k=n("span"),_(u.$$.fragment),y=d(),E=n("span"),q=t("Stream"),D=d(),T=n("p"),F=t("Dataset streaming lets you get started with a dataset without waiting for the entire dataset to download. The data is downloaded progressively as you iterate over the dataset. This is especially helpful when:"),te=d(),S=n("ul"),I=n("li"),as=t("You don\u2019t want to wait for an extremely large dataset to download."),ts=d(),le=n("li"),ls=t("The dataset size exceeds the amount of disk space on your computer."),ka=d(),J=n("div"),ns=n("img"),Ft=d(),rs=n("img"),ya=d(),P=n("p"),Ht=t("For example, the English split of the "),Ae=n("a"),Rt=t("OSCAR"),Yt=t(" dataset is 1.2 terabytes, but you can use it instantly with streaming. Stream a dataset by setting "),Fs=n("code"),Vt=t("streaming=True"),Bt=t(" in "),ps=n("a"),Jt=t("datasets.load_dataset()"),Wt=t(" as shown below:"),Ea=d(),_(Te.$$.fragment),Da=d(),H=n("p"),Ut=t("Loading a dataset in streaming mode creates a new dataset type instance (instead of the classic "),os=n("a"),Gt=t("datasets.Dataset"),Kt=t(" object), known as an "),is=n("a"),Qt=t("datasets.IterableDataset"),Xt=t(". This special type of dataset has its own set of processing methods shown below."),Ia=d(),_(ne.$$.fragment),Aa=d(),W=n("h2"),re=n("a"),Hs=n("span"),_(Se.$$.fragment),Zt=d(),Rs=n("span"),el=t("Shuffle"),Ta=d(),M=n("p"),sl=t("Like a regular "),hs=n("a"),al=t("datasets.Dataset"),tl=t(" object, you can also shuffle a "),ds=n("a"),ll=t("datasets.IterableDataset"),nl=t(" with "),fs=n("a"),rl=t("datasets.IterableDataset.shuffle()"),pl=t("."),Sa=d(),L=n("p"),ol=t("The "),Ys=n("code"),il=t("buffer_size"),hl=t(" argument controls the size of the buffer to randomly sample examples from. Let\u2019s say your dataset has one million examples, and you set the "),Vs=n("code"),dl=t("buffer_size"),fl=t(" to ten thousand. "),cs=n("a"),cl=t("datasets.IterableDataset.shuffle()"),ml=t(" will randomly select examples from the first ten thousand examples in the buffer. Selected examples in the buffer are replaced with new examples."),qa=d(),_(qe.$$.fragment),za=d(),_(pe.$$.fragment),Pa=d(),U=n("h2"),oe=n("a"),Bs=n("span"),_(ze.$$.fragment),ul=d(),Js=n("span"),_l=t("Reshuffle"),Ma=d(),ie=n("p"),gl=t("Sometimes you may want to reshuffle the dataset after each epoch. This will require you to set a different seed for each epoch. Use "),Ws=n("code"),bl=t("datasets.IterableDataset.set_epoch()"),vl=t("in between epochs to tell the dataset what epoch you\u2019re on."),La=d(),he=n("p"),jl=t("Your seed effectively becomes: "),Us=n("code"),$l=t("initial seed + current epoch"),wl=t("."),Ca=d(),_(Pe.$$.fragment),Na=d(),G=n("h2"),de=n("a"),Gs=n("span"),_(Me.$$.fragment),xl=d(),Ks=n("span"),kl=t("Split dataset"),Oa=d(),ms=n("p"),yl=t("You can split your dataset one of two ways:"),Fa=d(),us=n("ul"),fe=n("li"),_s=n("a"),El=t("datasets.IterableDataset.take()"),Dl=t(" returns the first "),Qs=n("code"),Il=t("n"),Al=t(" examples in a dataset:"),Ha=d(),_(Le.$$.fragment),Ra=d(),gs=n("ul"),ce=n("li"),bs=n("a"),Tl=t("datasets.IterableDataset.skip()"),Sl=t(" omits the first "),Xs=n("code"),ql=t("n"),zl=t(" examples in a dataset and returns the remaining examples:"),Ya=d(),_(Ce.$$.fragment),Va=d(),_(me.$$.fragment),Ba=d(),vs=n("a"),Ja=d(),K=n("h2"),ue=n("a"),Zs=n("span"),_(Ne.$$.fragment),Pl=d(),ea=n("span"),Ml=t("Interleave"),Wa=d(),Q=n("p"),js=n("a"),Ll=t("datasets.interleave_datasets()"),Cl=t(" can combine an "),$s=n("a"),Nl=t("datasets.IterableDataset"),Ol=t(" with other datasets. The combined dataset returns alternating examples from each of the original datasets."),Ua=d(),_(Oe.$$.fragment),Ga=d(),_e=n("p"),Fl=t("Define sampling probabilities from each of the original datasets for more control over how each of them are sampled and combined. Set the "),sa=n("code"),Hl=t("probabilities"),Rl=t(" argument with your desired sampling probabilities:"),Ka=d(),_(Fe.$$.fragment),Qa=d(),R=n("p"),Yl=t("Around 80% of the final dataset is made of the "),aa=n("code"),Vl=t("en_dataset"),Bl=t(", and 20% of the "),ta=n("code"),Jl=t("fr_dataset"),Wl=t("."),Xa=d(),X=n("h2"),ge=n("a"),la=n("span"),_(He.$$.fragment),Ul=d(),na=n("span"),Gl=t("Remove"),Za=d(),be=n("p"),Kl=t("Remove columns on-the-fly with "),ws=n("a"),Ql=t("datasets.IterableDataset.remove_columns()"),Xl=t(". Specify the name of the column to remove:"),et=d(),_(Re.$$.fragment),st=d(),Z=n("h2"),ve=n("a"),ra=n("span"),_(Ye.$$.fragment),Zl=d(),pa=n("span"),en=t("Map"),at=d(),A=n("p"),sn=t("Similar to the "),xs=n("a"),an=t("datasets.Dataset.map()"),tn=t(" function for a regular "),ks=n("a"),ln=t("datasets.Dataset"),nn=t(", \u{1F917}  Datasets features "),ys=n("a"),rn=t("datasets.IterableDataset.map()"),pn=t(" for processing "),Es=n("a"),on=t("datasets.IterableDataset"),hn=t(`\\s.
`),Ds=n("a"),dn=t("datasets.IterableDataset.map()"),fn=t(" applies processing on-the-fly when examples are streamed."),tt=d(),Is=n("p"),cn=t("It allows you to apply a processing function to each example in a dataset, independently or in batches. This function can even create new rows and columns."),lt=d(),Y=n("p"),mn=t("The following example demonstrates how to tokenize a "),As=n("a"),un=t("datasets.IterableDataset"),_n=t(". The function needs to accept and output a "),oa=n("code"),gn=t("dict"),bn=t(":"),nt=d(),_(Ve.$$.fragment),rt=d(),je=n("p"),vn=t("Next, apply this function to the dataset with "),Ts=n("a"),jn=t("datasets.IterableDataset.map()"),$n=t(":"),pt=d(),_(Be.$$.fragment),ot=d(),$e=n("p"),wn=t("Let\u2019s take a look at another example, except this time, you will remove a column with "),Ss=n("a"),xn=t("datasets.IterableDataset.map()"),kn=t(". When you remove a column, it is only removed after the example has been provided to the mapped function. This allows the mapped function to use the content of the columns before they are removed."),it=d(),V=n("p"),yn=t("Specify the column to remove with the "),ia=n("code"),En=t("remove_columns"),Dn=t(" argument in "),qs=n("a"),In=t("datasets.IterableDataset.map()"),An=t(":"),ht=d(),_(Je.$$.fragment),dt=d(),ee=n("h3"),we=n("a"),ha=n("span"),_(We.$$.fragment),Tn=d(),da=n("span"),Sn=t("Batch processing"),ft=d(),O=n("p"),zs=n("a"),qn=t("datasets.IterableDataset.map()"),zn=t(" also supports working with batches of examples. Operate on batches by setting "),fa=n("code"),Pn=t("batched=True"),Mn=t(". The default batch size is 1000, but you can adjust it with the "),ca=n("code"),Ln=t("batch_size"),Cn=t(" argument. This opens the door to many interesting applications such as tokenization, splitting long sentences into shorter chunks, and data augmentation."),ct=d(),se=n("h4"),xe=n("a"),ma=n("span"),_(Ue.$$.fragment),Nn=d(),ua=n("span"),On=t("Tokenization"),mt=d(),_(Ge.$$.fragment),ut=d(),_(ke.$$.fragment),_t=d(),ae=n("h2"),ye=n("a"),_a=n("span"),_(Ke.$$.fragment),Fn=d(),ga=n("span"),Hn=t("Stream in a training loop"),gt=d(),Qe=n("p"),Ps=n("a"),Rn=t("datasets.IterableDataset"),Yn=t(" can be integrated into a training loop. First, shuffle the dataset:"),bt=d(),_(Xe.$$.fragment),vt=d(),Ms=n("p"),Vn=t("Lastly, create a simple training loop and start training:"),jt=d(),_(Ze.$$.fragment),this.h()},l(e){const p=gp('[data-svelte="svelte-1phssyn"]',document.head);c=r(p,"META",{name:!0,content:!0}),p.forEach(a),w=f(e),m=r(e,"H1",{class:!0});var es=o(m);x=r(es,"A",{id:!0,class:!0,href:!0});var ba=o(x);k=r(ba,"SPAN",{});var va=o(k);g(u.$$.fragment,va),va.forEach(a),ba.forEach(a),y=f(es),E=r(es,"SPAN",{});var ja=o(E);q=l(ja,"Stream"),ja.forEach(a),es.forEach(a),D=f(e),T=r(e,"P",{});var Un=o(T);F=l(Un,"Dataset streaming lets you get started with a dataset without waiting for the entire dataset to download. The data is downloaded progressively as you iterate over the dataset. This is especially helpful when:"),Un.forEach(a),te=f(e),S=r(e,"UL",{});var wt=o(S);I=r(wt,"LI",{});var Gn=o(I);as=l(Gn,"You don\u2019t want to wait for an extremely large dataset to download."),Gn.forEach(a),ts=f(wt),le=r(wt,"LI",{});var Kn=o(le);ls=l(Kn,"The dataset size exceeds the amount of disk space on your computer."),Kn.forEach(a),wt.forEach(a),ka=f(e),J=r(e,"DIV",{class:!0});var xt=o(J);ns=r(xt,"IMG",{class:!0,src:!0}),Ft=f(xt),rs=r(xt,"IMG",{class:!0,src:!0}),xt.forEach(a),ya=f(e),P=r(e,"P",{});var Ee=o(P);Ht=l(Ee,"For example, the English split of the "),Ae=r(Ee,"A",{href:!0,rel:!0});var Qn=o(Ae);Rt=l(Qn,"OSCAR"),Qn.forEach(a),Yt=l(Ee," dataset is 1.2 terabytes, but you can use it instantly with streaming. Stream a dataset by setting "),Fs=r(Ee,"CODE",{});var Xn=o(Fs);Vt=l(Xn,"streaming=True"),Xn.forEach(a),Bt=l(Ee," in "),ps=r(Ee,"A",{href:!0});var Zn=o(ps);Jt=l(Zn,"datasets.load_dataset()"),Zn.forEach(a),Wt=l(Ee," as shown below:"),Ee.forEach(a),Ea=f(e),g(Te.$$.fragment,e),Da=f(e),H=r(e,"P",{});var Ls=o(H);Ut=l(Ls,"Loading a dataset in streaming mode creates a new dataset type instance (instead of the classic "),os=r(Ls,"A",{href:!0});var er=o(os);Gt=l(er,"datasets.Dataset"),er.forEach(a),Kt=l(Ls," object), known as an "),is=r(Ls,"A",{href:!0});var sr=o(is);Qt=l(sr,"datasets.IterableDataset"),sr.forEach(a),Xt=l(Ls,". This special type of dataset has its own set of processing methods shown below."),Ls.forEach(a),Ia=f(e),g(ne.$$.fragment,e),Aa=f(e),W=r(e,"H2",{class:!0});var kt=o(W);re=r(kt,"A",{id:!0,class:!0,href:!0});var ar=o(re);Hs=r(ar,"SPAN",{});var tr=o(Hs);g(Se.$$.fragment,tr),tr.forEach(a),ar.forEach(a),Zt=f(kt),Rs=r(kt,"SPAN",{});var lr=o(Rs);el=l(lr,"Shuffle"),lr.forEach(a),kt.forEach(a),Ta=f(e),M=r(e,"P",{});var De=o(M);sl=l(De,"Like a regular "),hs=r(De,"A",{href:!0});var nr=o(hs);al=l(nr,"datasets.Dataset"),nr.forEach(a),tl=l(De," object, you can also shuffle a "),ds=r(De,"A",{href:!0});var rr=o(ds);ll=l(rr,"datasets.IterableDataset"),rr.forEach(a),nl=l(De," with "),fs=r(De,"A",{href:!0});var pr=o(fs);rl=l(pr,"datasets.IterableDataset.shuffle()"),pr.forEach(a),pl=l(De,"."),De.forEach(a),Sa=f(e),L=r(e,"P",{});var Ie=o(L);ol=l(Ie,"The "),Ys=r(Ie,"CODE",{});var or=o(Ys);il=l(or,"buffer_size"),or.forEach(a),hl=l(Ie," argument controls the size of the buffer to randomly sample examples from. Let\u2019s say your dataset has one million examples, and you set the "),Vs=r(Ie,"CODE",{});var ir=o(Vs);dl=l(ir,"buffer_size"),ir.forEach(a),fl=l(Ie," to ten thousand. "),cs=r(Ie,"A",{href:!0});var hr=o(cs);cl=l(hr,"datasets.IterableDataset.shuffle()"),hr.forEach(a),ml=l(Ie," will randomly select examples from the first ten thousand examples in the buffer. Selected examples in the buffer are replaced with new examples."),Ie.forEach(a),qa=f(e),g(qe.$$.fragment,e),za=f(e),g(pe.$$.fragment,e),Pa=f(e),U=r(e,"H2",{class:!0});var yt=o(U);oe=r(yt,"A",{id:!0,class:!0,href:!0});var dr=o(oe);Bs=r(dr,"SPAN",{});var fr=o(Bs);g(ze.$$.fragment,fr),fr.forEach(a),dr.forEach(a),ul=f(yt),Js=r(yt,"SPAN",{});var cr=o(Js);_l=l(cr,"Reshuffle"),cr.forEach(a),yt.forEach(a),Ma=f(e),ie=r(e,"P",{});var Et=o(ie);gl=l(Et,"Sometimes you may want to reshuffle the dataset after each epoch. This will require you to set a different seed for each epoch. Use "),Ws=r(Et,"CODE",{});var mr=o(Ws);bl=l(mr,"datasets.IterableDataset.set_epoch()"),mr.forEach(a),vl=l(Et,"in between epochs to tell the dataset what epoch you\u2019re on."),Et.forEach(a),La=f(e),he=r(e,"P",{});var Dt=o(he);jl=l(Dt,"Your seed effectively becomes: "),Us=r(Dt,"CODE",{});var ur=o(Us);$l=l(ur,"initial seed + current epoch"),ur.forEach(a),wl=l(Dt,"."),Dt.forEach(a),Ca=f(e),g(Pe.$$.fragment,e),Na=f(e),G=r(e,"H2",{class:!0});var It=o(G);de=r(It,"A",{id:!0,class:!0,href:!0});var _r=o(de);Gs=r(_r,"SPAN",{});var gr=o(Gs);g(Me.$$.fragment,gr),gr.forEach(a),_r.forEach(a),xl=f(It),Ks=r(It,"SPAN",{});var br=o(Ks);kl=l(br,"Split dataset"),br.forEach(a),It.forEach(a),Oa=f(e),ms=r(e,"P",{});var vr=o(ms);yl=l(vr,"You can split your dataset one of two ways:"),vr.forEach(a),Fa=f(e),us=r(e,"UL",{});var jr=o(us);fe=r(jr,"LI",{});var $a=o(fe);_s=r($a,"A",{href:!0});var $r=o(_s);El=l($r,"datasets.IterableDataset.take()"),$r.forEach(a),Dl=l($a," returns the first "),Qs=r($a,"CODE",{});var wr=o(Qs);Il=l(wr,"n"),wr.forEach(a),Al=l($a," examples in a dataset:"),$a.forEach(a),jr.forEach(a),Ha=f(e),g(Le.$$.fragment,e),Ra=f(e),gs=r(e,"UL",{});var xr=o(gs);ce=r(xr,"LI",{});var wa=o(ce);bs=r(wa,"A",{href:!0});var kr=o(bs);Tl=l(kr,"datasets.IterableDataset.skip()"),kr.forEach(a),Sl=l(wa," omits the first "),Xs=r(wa,"CODE",{});var yr=o(Xs);ql=l(yr,"n"),yr.forEach(a),zl=l(wa," examples in a dataset and returns the remaining examples:"),wa.forEach(a),xr.forEach(a),Ya=f(e),g(Ce.$$.fragment,e),Va=f(e),g(me.$$.fragment,e),Ba=f(e),vs=r(e,"A",{id:!0}),o(vs).forEach(a),Ja=f(e),K=r(e,"H2",{class:!0});var At=o(K);ue=r(At,"A",{id:!0,class:!0,href:!0});var Er=o(ue);Zs=r(Er,"SPAN",{});var Dr=o(Zs);g(Ne.$$.fragment,Dr),Dr.forEach(a),Er.forEach(a),Pl=f(At),ea=r(At,"SPAN",{});var Ir=o(ea);Ml=l(Ir,"Interleave"),Ir.forEach(a),At.forEach(a),Wa=f(e),Q=r(e,"P",{});var xa=o(Q);js=r(xa,"A",{href:!0});var Ar=o(js);Ll=l(Ar,"datasets.interleave_datasets()"),Ar.forEach(a),Cl=l(xa," can combine an "),$s=r(xa,"A",{href:!0});var Tr=o($s);Nl=l(Tr,"datasets.IterableDataset"),Tr.forEach(a),Ol=l(xa," with other datasets. The combined dataset returns alternating examples from each of the original datasets."),xa.forEach(a),Ua=f(e),g(Oe.$$.fragment,e),Ga=f(e),_e=r(e,"P",{});var Tt=o(_e);Fl=l(Tt,"Define sampling probabilities from each of the original datasets for more control over how each of them are sampled and combined. Set the "),sa=r(Tt,"CODE",{});var Sr=o(sa);Hl=l(Sr,"probabilities"),Sr.forEach(a),Rl=l(Tt," argument with your desired sampling probabilities:"),Tt.forEach(a),Ka=f(e),g(Fe.$$.fragment,e),Qa=f(e),R=r(e,"P",{});var Cs=o(R);Yl=l(Cs,"Around 80% of the final dataset is made of the "),aa=r(Cs,"CODE",{});var qr=o(aa);Vl=l(qr,"en_dataset"),qr.forEach(a),Bl=l(Cs,", and 20% of the "),ta=r(Cs,"CODE",{});var zr=o(ta);Jl=l(zr,"fr_dataset"),zr.forEach(a),Wl=l(Cs,"."),Cs.forEach(a),Xa=f(e),X=r(e,"H2",{class:!0});var St=o(X);ge=r(St,"A",{id:!0,class:!0,href:!0});var Pr=o(ge);la=r(Pr,"SPAN",{});var Mr=o(la);g(He.$$.fragment,Mr),Mr.forEach(a),Pr.forEach(a),Ul=f(St),na=r(St,"SPAN",{});var Lr=o(na);Gl=l(Lr,"Remove"),Lr.forEach(a),St.forEach(a),Za=f(e),be=r(e,"P",{});var qt=o(be);Kl=l(qt,"Remove columns on-the-fly with "),ws=r(qt,"A",{href:!0});var Cr=o(ws);Ql=l(Cr,"datasets.IterableDataset.remove_columns()"),Cr.forEach(a),Xl=l(qt,". Specify the name of the column to remove:"),qt.forEach(a),et=f(e),g(Re.$$.fragment,e),st=f(e),Z=r(e,"H2",{class:!0});var zt=o(Z);ve=r(zt,"A",{id:!0,class:!0,href:!0});var Nr=o(ve);ra=r(Nr,"SPAN",{});var Or=o(ra);g(Ye.$$.fragment,Or),Or.forEach(a),Nr.forEach(a),Zl=f(zt),pa=r(zt,"SPAN",{});var Fr=o(pa);en=l(Fr,"Map"),Fr.forEach(a),zt.forEach(a),at=f(e),A=r(e,"P",{});var C=o(A);sn=l(C,"Similar to the "),xs=r(C,"A",{href:!0});var Hr=o(xs);an=l(Hr,"datasets.Dataset.map()"),Hr.forEach(a),tn=l(C," function for a regular "),ks=r(C,"A",{href:!0});var Rr=o(ks);ln=l(Rr,"datasets.Dataset"),Rr.forEach(a),nn=l(C,", \u{1F917}  Datasets features "),ys=r(C,"A",{href:!0});var Yr=o(ys);rn=l(Yr,"datasets.IterableDataset.map()"),Yr.forEach(a),pn=l(C," for processing "),Es=r(C,"A",{href:!0});var Vr=o(Es);on=l(Vr,"datasets.IterableDataset"),Vr.forEach(a),hn=l(C,`\\s.
`),Ds=r(C,"A",{href:!0});var Br=o(Ds);dn=l(Br,"datasets.IterableDataset.map()"),Br.forEach(a),fn=l(C," applies processing on-the-fly when examples are streamed."),C.forEach(a),tt=f(e),Is=r(e,"P",{});var Jr=o(Is);cn=l(Jr,"It allows you to apply a processing function to each example in a dataset, independently or in batches. This function can even create new rows and columns."),Jr.forEach(a),lt=f(e),Y=r(e,"P",{});var Ns=o(Y);mn=l(Ns,"The following example demonstrates how to tokenize a "),As=r(Ns,"A",{href:!0});var Wr=o(As);un=l(Wr,"datasets.IterableDataset"),Wr.forEach(a),_n=l(Ns,". The function needs to accept and output a "),oa=r(Ns,"CODE",{});var Ur=o(oa);gn=l(Ur,"dict"),Ur.forEach(a),bn=l(Ns,":"),Ns.forEach(a),nt=f(e),g(Ve.$$.fragment,e),rt=f(e),je=r(e,"P",{});var Pt=o(je);vn=l(Pt,"Next, apply this function to the dataset with "),Ts=r(Pt,"A",{href:!0});var Gr=o(Ts);jn=l(Gr,"datasets.IterableDataset.map()"),Gr.forEach(a),$n=l(Pt,":"),Pt.forEach(a),pt=f(e),g(Be.$$.fragment,e),ot=f(e),$e=r(e,"P",{});var Mt=o($e);wn=l(Mt,"Let\u2019s take a look at another example, except this time, you will remove a column with "),Ss=r(Mt,"A",{href:!0});var Kr=o(Ss);xn=l(Kr,"datasets.IterableDataset.map()"),Kr.forEach(a),kn=l(Mt,". When you remove a column, it is only removed after the example has been provided to the mapped function. This allows the mapped function to use the content of the columns before they are removed."),Mt.forEach(a),it=f(e),V=r(e,"P",{});var Os=o(V);yn=l(Os,"Specify the column to remove with the "),ia=r(Os,"CODE",{});var Qr=o(ia);En=l(Qr,"remove_columns"),Qr.forEach(a),Dn=l(Os," argument in "),qs=r(Os,"A",{href:!0});var Xr=o(qs);In=l(Xr,"datasets.IterableDataset.map()"),Xr.forEach(a),An=l(Os,":"),Os.forEach(a),ht=f(e),g(Je.$$.fragment,e),dt=f(e),ee=r(e,"H3",{class:!0});var Lt=o(ee);we=r(Lt,"A",{id:!0,class:!0,href:!0});var Zr=o(we);ha=r(Zr,"SPAN",{});var ep=o(ha);g(We.$$.fragment,ep),ep.forEach(a),Zr.forEach(a),Tn=f(Lt),da=r(Lt,"SPAN",{});var sp=o(da);Sn=l(sp,"Batch processing"),sp.forEach(a),Lt.forEach(a),ft=f(e),O=r(e,"P",{});var ss=o(O);zs=r(ss,"A",{href:!0});var ap=o(zs);qn=l(ap,"datasets.IterableDataset.map()"),ap.forEach(a),zn=l(ss," also supports working with batches of examples. Operate on batches by setting "),fa=r(ss,"CODE",{});var tp=o(fa);Pn=l(tp,"batched=True"),tp.forEach(a),Mn=l(ss,". The default batch size is 1000, but you can adjust it with the "),ca=r(ss,"CODE",{});var lp=o(ca);Ln=l(lp,"batch_size"),lp.forEach(a),Cn=l(ss," argument. This opens the door to many interesting applications such as tokenization, splitting long sentences into shorter chunks, and data augmentation."),ss.forEach(a),ct=f(e),se=r(e,"H4",{class:!0});var Ct=o(se);xe=r(Ct,"A",{id:!0,class:!0,href:!0});var np=o(xe);ma=r(np,"SPAN",{});var rp=o(ma);g(Ue.$$.fragment,rp),rp.forEach(a),np.forEach(a),Nn=f(Ct),ua=r(Ct,"SPAN",{});var pp=o(ua);On=l(pp,"Tokenization"),pp.forEach(a),Ct.forEach(a),mt=f(e),g(Ge.$$.fragment,e),ut=f(e),g(ke.$$.fragment,e),_t=f(e),ae=r(e,"H2",{class:!0});var Nt=o(ae);ye=r(Nt,"A",{id:!0,class:!0,href:!0});var op=o(ye);_a=r(op,"SPAN",{});var ip=o(_a);g(Ke.$$.fragment,ip),ip.forEach(a),op.forEach(a),Fn=f(Nt),ga=r(Nt,"SPAN",{});var hp=o(ga);Hn=l(hp,"Stream in a training loop"),hp.forEach(a),Nt.forEach(a),gt=f(e),Qe=r(e,"P",{});var Bn=o(Qe);Ps=r(Bn,"A",{href:!0});var dp=o(Ps);Rn=l(dp,"datasets.IterableDataset"),dp.forEach(a),Yn=l(Bn," can be integrated into a training loop. First, shuffle the dataset:"),Bn.forEach(a),bt=f(e),g(Xe.$$.fragment,e),vt=f(e),Ms=r(e,"P",{});var fp=o(Ms);Vn=l(fp,"Lastly, create a simple training loop and start training:"),fp.forEach(a),jt=f(e),g(Ze.$$.fragment,e),this.h()},h(){h(c,"name","hf:doc:metadata"),h(c,"content",JSON.stringify(kp)),h(x,"id","stream"),h(x,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(x,"href","#stream"),h(m,"class","relative group"),h(ns,"class","block dark:hidden"),cp(ns.src,Jn="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/datasets/streaming.gif")||h(ns,"src",Jn),h(rs,"class","hidden dark:block"),cp(rs.src,Wn="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/datasets/streaming-dark.gif")||h(rs,"src",Wn),h(J,"class","flex justify-center"),h(Ae,"href","https://huggingface.co/datasets/oscar"),h(Ae,"rel","nofollow"),h(ps,"href","/docs/datasets/pr_3801/en/package_reference/loading_methods#datasets.load_dataset"),h(os,"href","/docs/datasets/pr_3801/en/package_reference/main_classes#datasets.Dataset"),h(is,"href","/docs/datasets/pr_3801/en/package_reference/main_classes#datasets.IterableDataset"),h(re,"id","shuffle"),h(re,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(re,"href","#shuffle"),h(W,"class","relative group"),h(hs,"href","/docs/datasets/pr_3801/en/package_reference/main_classes#datasets.Dataset"),h(ds,"href","/docs/datasets/pr_3801/en/package_reference/main_classes#datasets.IterableDataset"),h(fs,"href","/docs/datasets/pr_3801/en/package_reference/main_classes#datasets.IterableDataset.shuffle"),h(cs,"href","/docs/datasets/pr_3801/en/package_reference/main_classes#datasets.IterableDataset.shuffle"),h(oe,"id","reshuffle"),h(oe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(oe,"href","#reshuffle"),h(U,"class","relative group"),h(de,"id","split-dataset"),h(de,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(de,"href","#split-dataset"),h(G,"class","relative group"),h(_s,"href","/docs/datasets/pr_3801/en/package_reference/main_classes#datasets.IterableDataset.take"),h(bs,"href","/docs/datasets/pr_3801/en/package_reference/main_classes#datasets.IterableDataset.skip"),h(vs,"id","interleave_datasets"),h(ue,"id","interleave"),h(ue,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(ue,"href","#interleave"),h(K,"class","relative group"),h(js,"href","/docs/datasets/pr_3801/en/package_reference/main_classes#datasets.interleave_datasets"),h($s,"href","/docs/datasets/pr_3801/en/package_reference/main_classes#datasets.IterableDataset"),h(ge,"id","remove"),h(ge,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(ge,"href","#remove"),h(X,"class","relative group"),h(ws,"href","/docs/datasets/pr_3801/en/package_reference/main_classes#datasets.IterableDataset.remove_columns"),h(ve,"id","map"),h(ve,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(ve,"href","#map"),h(Z,"class","relative group"),h(xs,"href","/docs/datasets/pr_3801/en/package_reference/main_classes#datasets.Dataset.map"),h(ks,"href","/docs/datasets/pr_3801/en/package_reference/main_classes#datasets.Dataset"),h(ys,"href","/docs/datasets/pr_3801/en/package_reference/main_classes#datasets.IterableDataset.map"),h(Es,"href","/docs/datasets/pr_3801/en/package_reference/main_classes#datasets.IterableDataset"),h(Ds,"href","/docs/datasets/pr_3801/en/package_reference/main_classes#datasets.IterableDataset.map"),h(As,"href","/docs/datasets/pr_3801/en/package_reference/main_classes#datasets.IterableDataset"),h(Ts,"href","/docs/datasets/pr_3801/en/package_reference/main_classes#datasets.IterableDataset.map"),h(Ss,"href","/docs/datasets/pr_3801/en/package_reference/main_classes#datasets.IterableDataset.map"),h(qs,"href","/docs/datasets/pr_3801/en/package_reference/main_classes#datasets.IterableDataset.map"),h(we,"id","batch-processing"),h(we,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(we,"href","#batch-processing"),h(ee,"class","relative group"),h(zs,"href","/docs/datasets/pr_3801/en/package_reference/main_classes#datasets.IterableDataset.map"),h(xe,"id","tokenization"),h(xe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(xe,"href","#tokenization"),h(se,"class","relative group"),h(ye,"id","stream-in-a-training-loop"),h(ye,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(ye,"href","#stream-in-a-training-loop"),h(ae,"class","relative group"),h(Ps,"href","/docs/datasets/pr_3801/en/package_reference/main_classes#datasets.IterableDataset")},m(e,p){s(document.head,c),i(e,w,p),i(e,m,p),s(m,x),s(x,k),b(u,k,null),s(m,y),s(m,E),s(E,q),i(e,D,p),i(e,T,p),s(T,F),i(e,te,p),i(e,S,p),s(S,I),s(I,as),s(S,ts),s(S,le),s(le,ls),i(e,ka,p),i(e,J,p),s(J,ns),s(J,Ft),s(J,rs),i(e,ya,p),i(e,P,p),s(P,Ht),s(P,Ae),s(Ae,Rt),s(P,Yt),s(P,Fs),s(Fs,Vt),s(P,Bt),s(P,ps),s(ps,Jt),s(P,Wt),i(e,Ea,p),b(Te,e,p),i(e,Da,p),i(e,H,p),s(H,Ut),s(H,os),s(os,Gt),s(H,Kt),s(H,is),s(is,Qt),s(H,Xt),i(e,Ia,p),b(ne,e,p),i(e,Aa,p),i(e,W,p),s(W,re),s(re,Hs),b(Se,Hs,null),s(W,Zt),s(W,Rs),s(Rs,el),i(e,Ta,p),i(e,M,p),s(M,sl),s(M,hs),s(hs,al),s(M,tl),s(M,ds),s(ds,ll),s(M,nl),s(M,fs),s(fs,rl),s(M,pl),i(e,Sa,p),i(e,L,p),s(L,ol),s(L,Ys),s(Ys,il),s(L,hl),s(L,Vs),s(Vs,dl),s(L,fl),s(L,cs),s(cs,cl),s(L,ml),i(e,qa,p),b(qe,e,p),i(e,za,p),b(pe,e,p),i(e,Pa,p),i(e,U,p),s(U,oe),s(oe,Bs),b(ze,Bs,null),s(U,ul),s(U,Js),s(Js,_l),i(e,Ma,p),i(e,ie,p),s(ie,gl),s(ie,Ws),s(Ws,bl),s(ie,vl),i(e,La,p),i(e,he,p),s(he,jl),s(he,Us),s(Us,$l),s(he,wl),i(e,Ca,p),b(Pe,e,p),i(e,Na,p),i(e,G,p),s(G,de),s(de,Gs),b(Me,Gs,null),s(G,xl),s(G,Ks),s(Ks,kl),i(e,Oa,p),i(e,ms,p),s(ms,yl),i(e,Fa,p),i(e,us,p),s(us,fe),s(fe,_s),s(_s,El),s(fe,Dl),s(fe,Qs),s(Qs,Il),s(fe,Al),i(e,Ha,p),b(Le,e,p),i(e,Ra,p),i(e,gs,p),s(gs,ce),s(ce,bs),s(bs,Tl),s(ce,Sl),s(ce,Xs),s(Xs,ql),s(ce,zl),i(e,Ya,p),b(Ce,e,p),i(e,Va,p),b(me,e,p),i(e,Ba,p),i(e,vs,p),i(e,Ja,p),i(e,K,p),s(K,ue),s(ue,Zs),b(Ne,Zs,null),s(K,Pl),s(K,ea),s(ea,Ml),i(e,Wa,p),i(e,Q,p),s(Q,js),s(js,Ll),s(Q,Cl),s(Q,$s),s($s,Nl),s(Q,Ol),i(e,Ua,p),b(Oe,e,p),i(e,Ga,p),i(e,_e,p),s(_e,Fl),s(_e,sa),s(sa,Hl),s(_e,Rl),i(e,Ka,p),b(Fe,e,p),i(e,Qa,p),i(e,R,p),s(R,Yl),s(R,aa),s(aa,Vl),s(R,Bl),s(R,ta),s(ta,Jl),s(R,Wl),i(e,Xa,p),i(e,X,p),s(X,ge),s(ge,la),b(He,la,null),s(X,Ul),s(X,na),s(na,Gl),i(e,Za,p),i(e,be,p),s(be,Kl),s(be,ws),s(ws,Ql),s(be,Xl),i(e,et,p),b(Re,e,p),i(e,st,p),i(e,Z,p),s(Z,ve),s(ve,ra),b(Ye,ra,null),s(Z,Zl),s(Z,pa),s(pa,en),i(e,at,p),i(e,A,p),s(A,sn),s(A,xs),s(xs,an),s(A,tn),s(A,ks),s(ks,ln),s(A,nn),s(A,ys),s(ys,rn),s(A,pn),s(A,Es),s(Es,on),s(A,hn),s(A,Ds),s(Ds,dn),s(A,fn),i(e,tt,p),i(e,Is,p),s(Is,cn),i(e,lt,p),i(e,Y,p),s(Y,mn),s(Y,As),s(As,un),s(Y,_n),s(Y,oa),s(oa,gn),s(Y,bn),i(e,nt,p),b(Ve,e,p),i(e,rt,p),i(e,je,p),s(je,vn),s(je,Ts),s(Ts,jn),s(je,$n),i(e,pt,p),b(Be,e,p),i(e,ot,p),i(e,$e,p),s($e,wn),s($e,Ss),s(Ss,xn),s($e,kn),i(e,it,p),i(e,V,p),s(V,yn),s(V,ia),s(ia,En),s(V,Dn),s(V,qs),s(qs,In),s(V,An),i(e,ht,p),b(Je,e,p),i(e,dt,p),i(e,ee,p),s(ee,we),s(we,ha),b(We,ha,null),s(ee,Tn),s(ee,da),s(da,Sn),i(e,ft,p),i(e,O,p),s(O,zs),s(zs,qn),s(O,zn),s(O,fa),s(fa,Pn),s(O,Mn),s(O,ca),s(ca,Ln),s(O,Cn),i(e,ct,p),i(e,se,p),s(se,xe),s(xe,ma),b(Ue,ma,null),s(se,Nn),s(se,ua),s(ua,On),i(e,mt,p),b(Ge,e,p),i(e,ut,p),b(ke,e,p),i(e,_t,p),i(e,ae,p),s(ae,ye),s(ye,_a),b(Ke,_a,null),s(ae,Fn),s(ae,ga),s(ga,Hn),i(e,gt,p),i(e,Qe,p),s(Qe,Ps),s(Ps,Rn),s(Qe,Yn),i(e,bt,p),b(Xe,e,p),i(e,vt,p),i(e,Ms,p),s(Ms,Vn),i(e,jt,p),b(Ze,e,p),$t=!0},p(e,[p]){const es={};p&2&&(es.$$scope={dirty:p,ctx:e}),ne.$set(es);const ba={};p&2&&(ba.$$scope={dirty:p,ctx:e}),pe.$set(ba);const va={};p&2&&(va.$$scope={dirty:p,ctx:e}),me.$set(va);const ja={};p&2&&(ja.$$scope={dirty:p,ctx:e}),ke.$set(ja)},i(e){$t||(v(u.$$.fragment,e),v(Te.$$.fragment,e),v(ne.$$.fragment,e),v(Se.$$.fragment,e),v(qe.$$.fragment,e),v(pe.$$.fragment,e),v(ze.$$.fragment,e),v(Pe.$$.fragment,e),v(Me.$$.fragment,e),v(Le.$$.fragment,e),v(Ce.$$.fragment,e),v(me.$$.fragment,e),v(Ne.$$.fragment,e),v(Oe.$$.fragment,e),v(Fe.$$.fragment,e),v(He.$$.fragment,e),v(Re.$$.fragment,e),v(Ye.$$.fragment,e),v(Ve.$$.fragment,e),v(Be.$$.fragment,e),v(Je.$$.fragment,e),v(We.$$.fragment,e),v(Ue.$$.fragment,e),v(Ge.$$.fragment,e),v(ke.$$.fragment,e),v(Ke.$$.fragment,e),v(Xe.$$.fragment,e),v(Ze.$$.fragment,e),$t=!0)},o(e){j(u.$$.fragment,e),j(Te.$$.fragment,e),j(ne.$$.fragment,e),j(Se.$$.fragment,e),j(qe.$$.fragment,e),j(pe.$$.fragment,e),j(ze.$$.fragment,e),j(Pe.$$.fragment,e),j(Me.$$.fragment,e),j(Le.$$.fragment,e),j(Ce.$$.fragment,e),j(me.$$.fragment,e),j(Ne.$$.fragment,e),j(Oe.$$.fragment,e),j(Fe.$$.fragment,e),j(He.$$.fragment,e),j(Re.$$.fragment,e),j(Ye.$$.fragment,e),j(Ve.$$.fragment,e),j(Be.$$.fragment,e),j(Je.$$.fragment,e),j(We.$$.fragment,e),j(Ue.$$.fragment,e),j(Ge.$$.fragment,e),j(ke.$$.fragment,e),j(Ke.$$.fragment,e),j(Xe.$$.fragment,e),j(Ze.$$.fragment,e),$t=!1},d(e){a(c),e&&a(w),e&&a(m),$(u),e&&a(D),e&&a(T),e&&a(te),e&&a(S),e&&a(ka),e&&a(J),e&&a(ya),e&&a(P),e&&a(Ea),$(Te,e),e&&a(Da),e&&a(H),e&&a(Ia),$(ne,e),e&&a(Aa),e&&a(W),$(Se),e&&a(Ta),e&&a(M),e&&a(Sa),e&&a(L),e&&a(qa),$(qe,e),e&&a(za),$(pe,e),e&&a(Pa),e&&a(U),$(ze),e&&a(Ma),e&&a(ie),e&&a(La),e&&a(he),e&&a(Ca),$(Pe,e),e&&a(Na),e&&a(G),$(Me),e&&a(Oa),e&&a(ms),e&&a(Fa),e&&a(us),e&&a(Ha),$(Le,e),e&&a(Ra),e&&a(gs),e&&a(Ya),$(Ce,e),e&&a(Va),$(me,e),e&&a(Ba),e&&a(vs),e&&a(Ja),e&&a(K),$(Ne),e&&a(Wa),e&&a(Q),e&&a(Ua),$(Oe,e),e&&a(Ga),e&&a(_e),e&&a(Ka),$(Fe,e),e&&a(Qa),e&&a(R),e&&a(Xa),e&&a(X),$(He),e&&a(Za),e&&a(be),e&&a(et),$(Re,e),e&&a(st),e&&a(Z),$(Ye),e&&a(at),e&&a(A),e&&a(tt),e&&a(Is),e&&a(lt),e&&a(Y),e&&a(nt),$(Ve,e),e&&a(rt),e&&a(je),e&&a(pt),$(Be,e),e&&a(ot),e&&a($e),e&&a(it),e&&a(V),e&&a(ht),$(Je,e),e&&a(dt),e&&a(ee),$(We),e&&a(ft),e&&a(O),e&&a(ct),e&&a(se),$(Ue),e&&a(mt),$(Ge,e),e&&a(ut),$(ke,e),e&&a(_t),e&&a(ae),$(Ke),e&&a(gt),e&&a(Qe),e&&a(bt),$(Xe,e),e&&a(vt),e&&a(Ms),e&&a(jt),$(Ze,e)}}}const kp={local:"stream",sections:[{local:"shuffle",title:"Shuffle"},{local:"reshuffle",title:"Reshuffle"},{local:"split-dataset",title:"Split dataset"},{local:"interleave",title:"Interleave"},{local:"remove",title:"Remove"},{local:"map",sections:[{local:"batch-processing",sections:[{local:"tokenization",title:"Tokenization"}],title:"Batch processing"}],title:"Map"},{local:"stream-in-a-training-loop",title:"Stream in a training loop"}],title:"Stream"};function yp(N,c,w){let{fw:m}=c;return N.$$set=x=>{"fw"in x&&w(0,m=x.fw)},[m]}class Sp extends mp{constructor(c){super();up(this,c,yp,xp,_p,{fw:0})}}export{Sp as default,kp as metadata};
