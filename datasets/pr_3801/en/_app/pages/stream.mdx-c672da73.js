import{S as yr,i as Er,s as Dr,e as n,k as d,w as _,t,M as Ir,c as r,d as s,m as f,a as o,x as g,h as l,b as h,N as kr,F as a,g as i,y as b,q as j,o as v,B as w}from"../chunks/vendor-e67aec41.js";import{T as hn}from"../chunks/Tip-76459d1c.js";import{I as $e}from"../chunks/IconCopyLink-ffd7f84e.js";import{C}from"../chunks/CodeBlock-e2bcf023.js";import{C as Ar}from"../chunks/CodeBlockFw-1e02e2ba.js";function Tr(H){let c,$,m,x,k,u,T,I;return{c(){c=n("p"),$=t("An "),m=n("a"),x=t("datasets.IterableDataset"),k=t(" is useful for iterative jobs like training a model. You shouldn\u2019t use a "),u=n("a"),T=t("datasets.IterableDataset"),I=t(" for jobs that require random access to examples because you have to iterate all over it using a for loop. Getting the last example in an iterable dataset would require you to iterate over all the previous examples."),this.h()},l(q){c=r(q,"P",{});var y=o(c);$=l(y,"An "),m=r(y,"A",{href:!0});var A=o(m);x=l(A,"datasets.IterableDataset"),A.forEach(s),k=l(y," is useful for iterative jobs like training a model. You shouldn\u2019t use a "),u=r(y,"A",{href:!0});var N=o(u);T=l(N,"datasets.IterableDataset"),N.forEach(s),I=l(y," for jobs that require random access to examples because you have to iterate all over it using a for loop. Getting the last example in an iterable dataset would require you to iterate over all the previous examples."),y.forEach(s),this.h()},h(){h(m,"href","/docs/datasets/pr_3801/en/package_reference/main_classes#datasets.IterableDataset"),h(u,"href","/docs/datasets/pr_3801/en/package_reference/main_classes#datasets.IterableDataset")},m(q,y){i(q,c,y),a(c,$),a(c,m),a(m,x),a(c,k),a(c,u),a(u,T),a(c,I)},d(q){q&&s(c)}}}function Sr(H){let c,$,m,x;return{c(){c=n("p"),$=n("a"),m=t("datasets.IterableDataset.shuffle()"),x=t(" will also shuffle the order of the shards if the dataset is sharded into multiple sets."),this.h()},l(k){c=r(k,"P",{});var u=o(c);$=r(u,"A",{href:!0});var T=o($);m=l(T,"datasets.IterableDataset.shuffle()"),T.forEach(s),x=l(u," will also shuffle the order of the shards if the dataset is sharded into multiple sets."),u.forEach(s),this.h()},h(){h($,"href","/docs/datasets/pr_3801/en/package_reference/main_classes#datasets.IterableDataset.shuffle")},m(k,u){i(k,c,u),a(c,$),a($,m),a(c,x)},d(k){k&&s(c)}}}function qr(H){let c,$,m,x,k,u,T,I,q,y,A,N,Z;return{c(){c=n("p"),$=n("code"),m=t("take"),x=t(" and "),k=n("code"),u=t("skip"),T=t(" prevent future calls to "),I=n("code"),q=t("shuffle"),y=t(" because they lock in the order of the shards. You should "),A=n("code"),N=t("shuffle"),Z=t(" your dataset before splitting it.")},l(S){c=r(S,"P",{});var E=o(c);$=r(E,"CODE",{});var We=o($);m=l(We,"take"),We.forEach(s),x=l(E," and "),k=r(E,"CODE",{});var Ue=o(k);u=l(Ue,"skip"),Ue.forEach(s),T=l(E," prevent future calls to "),I=r(E,"CODE",{});var ee=o(I);q=l(ee,"shuffle"),ee.forEach(s),y=l(E," because they lock in the order of the shards. You should "),A=r(E,"CODE",{});var Be=o(A);N=l(Be,"shuffle"),Be.forEach(s),Z=l(E," your dataset before splitting it."),E.forEach(s)},m(S,E){i(S,c,E),a(c,$),a($,m),a(c,x),a(c,k),a(k,u),a(c,T),a(c,I),a(I,q),a(c,y),a(c,A),a(A,N),a(c,Z)},d(S){S&&s(c)}}}function Pr(H){let c,$,m,x,k,u,T,I,q,y,A,N,Z,S,E,We,Ue,ee,Be,rs,V,Ge,dn,mt,Ke,fn,ps,P,ut,xe,_t,gt,Aa,bt,jt,Qe,vt,wt,os,ke,is,O,$t,Xe,xt,kt,Ze,yt,Et,hs,ae,ds,J,se,Ta,ye,Dt,Sa,It,fs,M,At,ea,Tt,St,aa,qt,Pt,sa,Mt,zt,cs,z,Lt,qa,Ct,Nt,Pa,Ot,Ft,ta,Rt,Yt,ms,Ee,us,te,_s,W,le,Ma,De,Ht,za,Vt,gs,ne,Jt,La,Wt,Ut,bs,re,Bt,Ca,Gt,Kt,js,Ie,vs,U,pe,Na,Ae,Qt,Oa,Xt,ws,la,Zt,$s,na,oe,ra,el,al,Fa,sl,tl,xs,Te,ks,pa,ie,oa,ll,nl,Ra,rl,pl,ys,Se,Es,he,Ds,ia,Is,B,de,Ya,qe,ol,Ha,il,As,G,ha,hl,dl,da,fl,cl,Ts,Pe,Ss,fe,ml,Va,ul,_l,qs,Me,Ps,F,gl,Ja,bl,jl,Wa,vl,wl,Ms,K,ce,Ua,ze,$l,Ba,xl,zs,me,kl,fa,yl,El,Ls,Le,Cs,Q,ue,Ga,Ce,Dl,Ka,Il,Ns,D,Al,ca,Tl,Sl,ma,ql,Pl,ua,Ml,zl,_a,Ll,Cl,ga,Nl,Ol,Os,ba,Fl,Fs,R,Rl,ja,Yl,Hl,Qa,Vl,Jl,Rs,Ne,Ys,_e,Wl,va,Ul,Bl,Hs,Oe,Vs,ge,Gl,wa,Kl,Ql,Js,Y,Xl,Xa,Zl,en,$a,an,sn,Ws,Fe,Us,X,be,Za,Re,tn,es,ln,Bs,Ye,xa,nn,rn,Gs,He,Ks,ka,pn,Qs,Ve,Xs;return u=new $e({}),ke=new C({props:{code:`from datasets import load_dataset
dataset = load_dataset('oscar', "unshuffled_deduplicated_en", split='train', streaming=True)
print(next(iter(dataset)))`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;oscar&#x27;</span>, <span class="hljs-string">&quot;unshuffled_deduplicated_en&quot;</span>, split=<span class="hljs-string">&#x27;train&#x27;</span>, streaming=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(<span class="hljs-built_in">next</span>(<span class="hljs-built_in">iter</span>(dataset)))
{<span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;Mtendere Village was inspired by the vision of Chief Napoleon Dzombe, which he shared with John Blanchard during his first visit to Malawi. Chief Napoleon conveyed the desperate need for a program to intervene and care for the orphans and vulnerable children (OVC) in Malawi, and John committed to help...</span>`}}),ae=new hn({props:{$$slots:{default:[Tr]},$$scope:{ctx:H}}}),ye=new $e({}),Ee=new C({props:{code:`from datasets import load_dataset
dataset = load_dataset('oscar', "unshuffled_deduplicated_en", split='train', streaming=True)
shuffled_dataset = dataset.shuffle(buffer_size=10_000, seed=42)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;oscar&#x27;</span>, <span class="hljs-string">&quot;unshuffled_deduplicated_en&quot;</span>, split=<span class="hljs-string">&#x27;train&#x27;</span>, streaming=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>shuffled_dataset = dataset.shuffle(buffer_size=<span class="hljs-number">10_000</span>, seed=<span class="hljs-number">42</span>)`}}),te=new hn({props:{$$slots:{default:[Sr]},$$scope:{ctx:H}}}),De=new $e({}),Ie=new C({props:{code:`for epoch in range(epochs):
    shuffled_dataset.set_epoch(epoch)
    for example in shuffled_dataset:
        ...`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(epochs):
<span class="hljs-meta">... </span>    shuffled_dataset.set_epoch(epoch)
<span class="hljs-meta">... </span>    <span class="hljs-keyword">for</span> example <span class="hljs-keyword">in</span> shuffled_dataset:
<span class="hljs-meta">... </span>        ...`}}),Ae=new $e({}),Te=new C({props:{code:`dataset = load_dataset('oscar', "unshuffled_deduplicated_en", split='train', streaming=True)
dataset_head = dataset.take(2)
list(dataset_head)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;oscar&#x27;</span>, <span class="hljs-string">&quot;unshuffled_deduplicated_en&quot;</span>, split=<span class="hljs-string">&#x27;train&#x27;</span>, streaming=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset_head = dataset.take(<span class="hljs-number">2</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">list</span>(dataset_head)
[{<span class="hljs-string">&#x27;id&#x27;</span>: <span class="hljs-number">0</span>, <span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;Mtendere Village was...&#x27;</span>}, {<span class="hljs-string">&#x27;id&#x27;</span>: <span class="hljs-number">1</span>, <span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;Lily James cannot fight the music...&#x27;</span>}]`}}),Se=new C({props:{code:"train_dataset = shuffled_dataset.skip(1000)",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>train_dataset = shuffled_dataset.skip(<span class="hljs-number">1000</span>)'}}),he=new hn({props:{warning:"&lcub;true}",$$slots:{default:[qr]},$$scope:{ctx:H}}}),qe=new $e({}),Pe=new C({props:{code:`from datasets import interleave_datasets
from itertools import islice
en_dataset = load_dataset('oscar', "unshuffled_deduplicated_en", split='train', streaming=True)
fr_dataset = load_dataset('oscar', "unshuffled_deduplicated_fr", split='train', streaming=True)

multilingual_dataset = interleave_datasets([en_dataset, fr_dataset])
print(list(islice(multilingual_dataset, 2)))`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> interleave_datasets
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> itertools <span class="hljs-keyword">import</span> islice
<span class="hljs-meta">&gt;&gt;&gt; </span>en_dataset = load_dataset(<span class="hljs-string">&#x27;oscar&#x27;</span>, <span class="hljs-string">&quot;unshuffled_deduplicated_en&quot;</span>, split=<span class="hljs-string">&#x27;train&#x27;</span>, streaming=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>fr_dataset = load_dataset(<span class="hljs-string">&#x27;oscar&#x27;</span>, <span class="hljs-string">&quot;unshuffled_deduplicated_fr&quot;</span>, split=<span class="hljs-string">&#x27;train&#x27;</span>, streaming=<span class="hljs-literal">True</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>multilingual_dataset = interleave_datasets([en_dataset, fr_dataset])
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(<span class="hljs-built_in">list</span>(islice(multilingual_dataset, <span class="hljs-number">2</span>)))
[{<span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;Mtendere Village was inspired by the vision...&#x27;</span>}, {<span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&quot;M\xE9dia de d\xE9bat d&#x27;id\xE9es, de culture et de litt\xE9rature...&quot;</span>}]`}}),Me=new C({props:{code:`multilingual_dataset_with_oversampling = interleave_datasets([en_dataset, fr_dataset], probabilities=[0.8, 0.2], seed=42)
print(list(islice(multilingual_dataset_with_oversampling, 2)))`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>multilingual_dataset_with_oversampling = interleave_datasets([en_dataset, fr_dataset], probabilities=[<span class="hljs-number">0.8</span>, <span class="hljs-number">0.2</span>], seed=<span class="hljs-number">42</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(<span class="hljs-built_in">list</span>(islice(multilingual_dataset_with_oversampling, <span class="hljs-number">2</span>)))
[{<span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;Mtendere Village was inspired by the vision...&#x27;</span>}, {<span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;Lily James cannot fight the music...&#x27;</span>}]`}}),ze=new $e({}),Le=new C({props:{code:`from datasets import load_dataset
dataset = load_dataset('mc4', 'en', streaming=True, split='train')
dataset = dataset.remove_columns('timestamp')`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;mc4&#x27;</span>, <span class="hljs-string">&#x27;en&#x27;</span>, streaming=<span class="hljs-literal">True</span>, split=<span class="hljs-string">&#x27;train&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = dataset.remove_columns(<span class="hljs-string">&#x27;timestamp&#x27;</span>)`}}),Ce=new $e({}),Ne=new C({props:{code:`def add_prefix(example):
    example['text'] = 'My text: ' + example['text']
    return example`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">def</span> <span class="hljs-title function_">add_prefix</span>(<span class="hljs-params">example</span>):
<span class="hljs-meta">... </span>    example[<span class="hljs-string">&#x27;text&#x27;</span>] = <span class="hljs-string">&#x27;My text: &#x27;</span> + example[<span class="hljs-string">&#x27;text&#x27;</span>]
<span class="hljs-meta">... </span>    <span class="hljs-keyword">return</span> example`}}),Oe=new C({props:{code:`from datasets import load_dataset
dataset = load_dataset('oscar', 'unshuffled_deduplicated_en', streaming=True, split='train')
updated_dataset = dataset.map(add_prefix)
list(updated_dataset.take(3))`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;oscar&#x27;</span>, <span class="hljs-string">&#x27;unshuffled_deduplicated_en&#x27;</span>, streaming=<span class="hljs-literal">True</span>, split=<span class="hljs-string">&#x27;train&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>updated_dataset = dataset.<span class="hljs-built_in">map</span>(add_prefix)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">list</span>(updated_dataset.take(<span class="hljs-number">3</span>))
[{<span class="hljs-string">&#x27;id&#x27;</span>: <span class="hljs-number">0</span>, <span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;My text: Mtendere Village was inspired by...&#x27;</span>},
 {<span class="hljs-string">&#x27;id&#x27;</span>: <span class="hljs-number">1</span>, <span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;My text: Lily James cannot fight the music...&#x27;</span>},
 {<span class="hljs-string">&#x27;id&#x27;</span>: <span class="hljs-number">2</span>, <span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;My text: &quot;I\\&#x27;d love to help kickstart...&#x27;</span>}]`}}),Fe=new C({props:{code:`from datasets import load_dataset
from transformers import AutoTokenizer
dataset = load_dataset("mc4", "en", streaming=True, split="train")
tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')
def encode(examples):
    return tokenizer(examples['text'], truncation=True, padding='max_length')
dataset = dataset.map(encode, batched=True, remove_columns=["text", "timestamp", "url"])
next(iter(dataset))`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;mc4&quot;</span>, <span class="hljs-string">&quot;en&quot;</span>, streaming=<span class="hljs-literal">True</span>, split=<span class="hljs-string">&quot;train&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&#x27;distilbert-base-uncased&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">def</span> <span class="hljs-title function_">encode</span>(<span class="hljs-params">examples</span>):
<span class="hljs-meta">... </span>    <span class="hljs-keyword">return</span> tokenizer(examples[<span class="hljs-string">&#x27;text&#x27;</span>], truncation=<span class="hljs-literal">True</span>, padding=<span class="hljs-string">&#x27;max_length&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = dataset.<span class="hljs-built_in">map</span>(encode, batched=<span class="hljs-literal">True</span>, remove_columns=[<span class="hljs-string">&quot;text&quot;</span>, <span class="hljs-string">&quot;timestamp&quot;</span>, <span class="hljs-string">&quot;url&quot;</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">next</span>(<span class="hljs-built_in">iter</span>(dataset))
{<span class="hljs-string">&#x27;input_ids&#x27;</span>: <span class="hljs-number">101</span>, <span class="hljs-number">8466</span>, <span class="hljs-number">1018</span>, <span class="hljs-number">1010</span>, <span class="hljs-number">4029</span>, <span class="hljs-number">2475</span>, <span class="hljs-number">2062</span>, <span class="hljs-number">18558</span>, <span class="hljs-number">3100</span>, <span class="hljs-number">2061</span>, ...,<span class="hljs-number">1106</span>, <span class="hljs-number">3739</span>, <span class="hljs-number">102</span>],
<span class="hljs-string">&#x27;attention_mask&#x27;</span>: [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, ..., <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]}`}}),Re=new $e({}),He=new C({props:{code:`buffer_size, seed = 10_000, 42
dataset = dataset.shuffle(buffer_size, seed)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>buffer_size, seed = <span class="hljs-number">10_000</span>, <span class="hljs-number">42</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = dataset.shuffle(buffer_size, seed)`}}),Ve=new Ar({props:{group1:{id:"pt",code:`import torch
from torch.utils.data import DataLoader
from transformers import AutoModelForMaskedLM, DataCollatorForLanguageModeling
from tqdm import tqdm
dataset = dataset.with_format("torch")
dataloader = DataLoader(dataset, collate_fn=DataCollatorForLanguageModeling(tokenizer))
device = 'cuda' if torch.cuda.is_available() else 'cpu' 
model = AutoModelForMaskedLM.from_pretrained("distilbert-base-uncased")
model.train().to(device)
optimizer = torch.optim.AdamW(params=model.parameters(), lr=1e-5)
for epoch in range(3):
    dataset.set_epoch(epoch)
    for i, batch in enumerate(tqdm(dataloader, total=5)):
        if i == 5:
            break
        batch = {k: v.to(device) for k, v in batch.items()}
        outputs = model(**batch)
        loss = outputs[0]
        loss.backward()
        optimizer.step()
        optimizer.zero_grad()
        if i % 10 == 0:
            print(f"loss: {loss}")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> DataLoader
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForMaskedLM, DataCollatorForLanguageModeling
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> tqdm <span class="hljs-keyword">import</span> tqdm
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = dataset.with_format(<span class="hljs-string">&quot;torch&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>dataloader = DataLoader(dataset, collate_fn=DataCollatorForLanguageModeling(tokenizer))
<span class="hljs-meta">&gt;&gt;&gt; </span>device = <span class="hljs-string">&#x27;cuda&#x27;</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">&#x27;cpu&#x27;</span> 
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMaskedLM.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.train().to(device)
<span class="hljs-meta">&gt;&gt;&gt; </span>optimizer = torch.optim.AdamW(params=model.parameters(), lr=<span class="hljs-number">1e-5</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">3</span>):
<span class="hljs-meta">... </span>    dataset.set_epoch(epoch)
<span class="hljs-meta">... </span>    <span class="hljs-keyword">for</span> i, batch <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(tqdm(dataloader, total=<span class="hljs-number">5</span>)):
<span class="hljs-meta">... </span>        <span class="hljs-keyword">if</span> i == <span class="hljs-number">5</span>:
<span class="hljs-meta">... </span>            <span class="hljs-keyword">break</span>
<span class="hljs-meta">... </span>        batch = {k: v.to(device) <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> batch.items()}
<span class="hljs-meta">... </span>        outputs = model(**batch)
<span class="hljs-meta">... </span>        loss = outputs[<span class="hljs-number">0</span>]
<span class="hljs-meta">... </span>        loss.backward()
<span class="hljs-meta">... </span>        optimizer.step()
<span class="hljs-meta">... </span>        optimizer.zero_grad()
<span class="hljs-meta">... </span>        <span class="hljs-keyword">if</span> i % <span class="hljs-number">10</span> == <span class="hljs-number">0</span>:
<span class="hljs-meta">... </span>            <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;loss: <span class="hljs-subst">{loss}</span>&quot;</span>)`},group2:{id:"tf",code:"# WIP",highlighted:'<span class="hljs-comment"># WIP</span>'}}}),{c(){c=n("meta"),$=d(),m=n("h1"),x=n("a"),k=n("span"),_(u.$$.fragment),T=d(),I=n("span"),q=t("Stream"),y=d(),A=n("p"),N=t("Dataset streaming lets you get started with a dataset without waiting for the entire dataset to download. The data is downloaded progressively as you iterate over the dataset. This is especially helpful when:"),Z=d(),S=n("ul"),E=n("li"),We=t("You don\u2019t want to wait for an extremely large dataset to download."),Ue=d(),ee=n("li"),Be=t("The dataset size exceeds the amount of disk space on your computer."),rs=d(),V=n("div"),Ge=n("img"),mt=d(),Ke=n("img"),ps=d(),P=n("p"),ut=t("For example, the English split of the "),xe=n("a"),_t=t("OSCAR"),gt=t(" dataset is 1.2 terabytes, but you can use it instantly with streaming. Stream a dataset by setting "),Aa=n("code"),bt=t("streaming=True"),jt=t(" in "),Qe=n("a"),vt=t("datasets.load_dataset()"),wt=t(" as shown below:"),os=d(),_(ke.$$.fragment),is=d(),O=n("p"),$t=t("Loading a dataset in streaming mode creates a new dataset type instance (instead of the classic "),Xe=n("a"),xt=t("datasets.Dataset"),kt=t(" object), known as an "),Ze=n("a"),yt=t("datasets.IterableDataset"),Et=t(". This special type of dataset has its own set of processing methods shown below."),hs=d(),_(ae.$$.fragment),ds=d(),J=n("h2"),se=n("a"),Ta=n("span"),_(ye.$$.fragment),Dt=d(),Sa=n("span"),It=t("Shuffle"),fs=d(),M=n("p"),At=t("Like a regular "),ea=n("a"),Tt=t("datasets.Dataset"),St=t(" object, you can also shuffle a "),aa=n("a"),qt=t("datasets.IterableDataset"),Pt=t(" with "),sa=n("a"),Mt=t("datasets.IterableDataset.shuffle()"),zt=t("."),cs=d(),z=n("p"),Lt=t("The "),qa=n("code"),Ct=t("buffer_size"),Nt=t(" argument controls the size of the buffer to randomly sample examples from. Let\u2019s say your dataset has one million examples, and you set the "),Pa=n("code"),Ot=t("buffer_size"),Ft=t(" to ten thousand. "),ta=n("a"),Rt=t("datasets.IterableDataset.shuffle()"),Yt=t(" will randomly select examples from the first ten thousand examples in the buffer. Selected examples in the buffer are replaced with new examples."),ms=d(),_(Ee.$$.fragment),us=d(),_(te.$$.fragment),_s=d(),W=n("h2"),le=n("a"),Ma=n("span"),_(De.$$.fragment),Ht=d(),za=n("span"),Vt=t("Reshuffle"),gs=d(),ne=n("p"),Jt=t("Sometimes you may want to reshuffle the dataset after each epoch. This will require you to set a different seed for each epoch. Use "),La=n("code"),Wt=t("datasets.IterableDataset.set_epoch()"),Ut=t("in between epochs to tell the dataset what epoch you\u2019re on."),bs=d(),re=n("p"),Bt=t("Your seed effectively becomes: "),Ca=n("code"),Gt=t("initial seed + current epoch"),Kt=t("."),js=d(),_(Ie.$$.fragment),vs=d(),U=n("h2"),pe=n("a"),Na=n("span"),_(Ae.$$.fragment),Qt=d(),Oa=n("span"),Xt=t("Split dataset"),ws=d(),la=n("p"),Zt=t("You can split your dataset one of two ways:"),$s=d(),na=n("ul"),oe=n("li"),ra=n("a"),el=t("datasets.IterableDataset.take()"),al=t(" returns the first "),Fa=n("code"),sl=t("n"),tl=t(" examples in a dataset:"),xs=d(),_(Te.$$.fragment),ks=d(),pa=n("ul"),ie=n("li"),oa=n("a"),ll=t("datasets.IterableDataset.skip()"),nl=t(" omits the first "),Ra=n("code"),rl=t("n"),pl=t(" examples in a dataset and returns the remaining examples:"),ys=d(),_(Se.$$.fragment),Es=d(),_(he.$$.fragment),Ds=d(),ia=n("a"),Is=d(),B=n("h2"),de=n("a"),Ya=n("span"),_(qe.$$.fragment),ol=d(),Ha=n("span"),il=t("Interleave"),As=d(),G=n("p"),ha=n("a"),hl=t("datasets.interleave_datasets()"),dl=t(" can combine an "),da=n("a"),fl=t("datasets.IterableDataset"),cl=t(" with other datasets. The combined dataset returns alternating examples from each of the original datasets."),Ts=d(),_(Pe.$$.fragment),Ss=d(),fe=n("p"),ml=t("Define sampling probabilities from each of the original datasets for more control over how each of them are sampled and combined. Set the "),Va=n("code"),ul=t("probabilities"),_l=t(" argument with your desired sampling probabilities:"),qs=d(),_(Me.$$.fragment),Ps=d(),F=n("p"),gl=t("Around 80% of the final dataset is made of the "),Ja=n("code"),bl=t("en_dataset"),jl=t(", and 20% of the "),Wa=n("code"),vl=t("fr_dataset"),wl=t("."),Ms=d(),K=n("h2"),ce=n("a"),Ua=n("span"),_(ze.$$.fragment),$l=d(),Ba=n("span"),xl=t("Remove"),zs=d(),me=n("p"),kl=t("Remove columns on-the-fly with "),fa=n("a"),yl=t("datasets.IterableDataset.remove_columns()"),El=t(". Specify the name of the column to remove:"),Ls=d(),_(Le.$$.fragment),Cs=d(),Q=n("h2"),ue=n("a"),Ga=n("span"),_(Ce.$$.fragment),Dl=d(),Ka=n("span"),Il=t("Map"),Ns=d(),D=n("p"),Al=t("Similar to the "),ca=n("a"),Tl=t("datasets.Dataset.map()"),Sl=t(" function for a regular "),ma=n("a"),ql=t("datasets.Dataset"),Pl=t(", \u{1F917}  Datasets features "),ua=n("a"),Ml=t("datasets.IterableDataset.map()"),zl=t(" for processing "),_a=n("a"),Ll=t("datasets.IterableDataset"),Cl=t(`\\s.
`),ga=n("a"),Nl=t("datasets.IterableDataset.map()"),Ol=t(" applies processing on-the-fly when examples are streamed."),Os=d(),ba=n("p"),Fl=t("It allows you to apply a processing function to each example in a dataset, independently or in batches. This function can even create new rows and columns."),Fs=d(),R=n("p"),Rl=t("The following example demonstrates how to tokenize a "),ja=n("a"),Yl=t("datasets.IterableDataset"),Hl=t(". The function needs to accept and output a "),Qa=n("code"),Vl=t("dict"),Jl=t(":"),Rs=d(),_(Ne.$$.fragment),Ys=d(),_e=n("p"),Wl=t("Next, apply this function to the dataset with "),va=n("a"),Ul=t("datasets.IterableDataset.map()"),Bl=t(":"),Hs=d(),_(Oe.$$.fragment),Vs=d(),ge=n("p"),Gl=t("Let\u2019s take a look at another example, except this time, you will remove a column with "),wa=n("a"),Kl=t("datasets.IterableDataset.map()"),Ql=t(". When you remove a column, it is only removed after the example has been provided to the mapped function. This allows the mapped function to use the content of the columns before they are removed."),Js=d(),Y=n("p"),Xl=t("Specify the column to remove with the "),Xa=n("code"),Zl=t("remove_columns"),en=t(" argument in "),$a=n("a"),an=t("datasets.IterableDataset.map()"),sn=t(":"),Ws=d(),_(Fe.$$.fragment),Us=d(),X=n("h2"),be=n("a"),Za=n("span"),_(Re.$$.fragment),tn=d(),es=n("span"),ln=t("Stream in a training loop"),Bs=d(),Ye=n("p"),xa=n("a"),nn=t("datasets.IterableDataset"),rn=t(" can be integrated into a training loop. First, shuffle the dataset:"),Gs=d(),_(He.$$.fragment),Ks=d(),ka=n("p"),pn=t("Lastly, create a simple training loop and start training:"),Qs=d(),_(Ve.$$.fragment),this.h()},l(e){const p=Ir('[data-svelte="svelte-1phssyn"]',document.head);c=r(p,"META",{name:!0,content:!0}),p.forEach(s),$=f(e),m=r(e,"H1",{class:!0});var Je=o(m);x=r(Je,"A",{id:!0,class:!0,href:!0});var as=o(x);k=r(as,"SPAN",{});var ss=o(k);g(u.$$.fragment,ss),ss.forEach(s),as.forEach(s),T=f(Je),I=r(Je,"SPAN",{});var cn=o(I);q=l(cn,"Stream"),cn.forEach(s),Je.forEach(s),y=f(e),A=r(e,"P",{});var mn=o(A);N=l(mn,"Dataset streaming lets you get started with a dataset without waiting for the entire dataset to download. The data is downloaded progressively as you iterate over the dataset. This is especially helpful when:"),mn.forEach(s),Z=f(e),S=r(e,"UL",{});var Zs=o(S);E=r(Zs,"LI",{});var un=o(E);We=l(un,"You don\u2019t want to wait for an extremely large dataset to download."),un.forEach(s),Ue=f(Zs),ee=r(Zs,"LI",{});var _n=o(ee);Be=l(_n,"The dataset size exceeds the amount of disk space on your computer."),_n.forEach(s),Zs.forEach(s),rs=f(e),V=r(e,"DIV",{class:!0});var et=o(V);Ge=r(et,"IMG",{class:!0,src:!0}),mt=f(et),Ke=r(et,"IMG",{class:!0,src:!0}),et.forEach(s),ps=f(e),P=r(e,"P",{});var je=o(P);ut=l(je,"For example, the English split of the "),xe=r(je,"A",{href:!0,rel:!0});var gn=o(xe);_t=l(gn,"OSCAR"),gn.forEach(s),gt=l(je," dataset is 1.2 terabytes, but you can use it instantly with streaming. Stream a dataset by setting "),Aa=r(je,"CODE",{});var bn=o(Aa);bt=l(bn,"streaming=True"),bn.forEach(s),jt=l(je," in "),Qe=r(je,"A",{href:!0});var jn=o(Qe);vt=l(jn,"datasets.load_dataset()"),jn.forEach(s),wt=l(je," as shown below:"),je.forEach(s),os=f(e),g(ke.$$.fragment,e),is=f(e),O=r(e,"P",{});var ya=o(O);$t=l(ya,"Loading a dataset in streaming mode creates a new dataset type instance (instead of the classic "),Xe=r(ya,"A",{href:!0});var vn=o(Xe);xt=l(vn,"datasets.Dataset"),vn.forEach(s),kt=l(ya," object), known as an "),Ze=r(ya,"A",{href:!0});var wn=o(Ze);yt=l(wn,"datasets.IterableDataset"),wn.forEach(s),Et=l(ya,". This special type of dataset has its own set of processing methods shown below."),ya.forEach(s),hs=f(e),g(ae.$$.fragment,e),ds=f(e),J=r(e,"H2",{class:!0});var at=o(J);se=r(at,"A",{id:!0,class:!0,href:!0});var $n=o(se);Ta=r($n,"SPAN",{});var xn=o(Ta);g(ye.$$.fragment,xn),xn.forEach(s),$n.forEach(s),Dt=f(at),Sa=r(at,"SPAN",{});var kn=o(Sa);It=l(kn,"Shuffle"),kn.forEach(s),at.forEach(s),fs=f(e),M=r(e,"P",{});var ve=o(M);At=l(ve,"Like a regular "),ea=r(ve,"A",{href:!0});var yn=o(ea);Tt=l(yn,"datasets.Dataset"),yn.forEach(s),St=l(ve," object, you can also shuffle a "),aa=r(ve,"A",{href:!0});var En=o(aa);qt=l(En,"datasets.IterableDataset"),En.forEach(s),Pt=l(ve," with "),sa=r(ve,"A",{href:!0});var Dn=o(sa);Mt=l(Dn,"datasets.IterableDataset.shuffle()"),Dn.forEach(s),zt=l(ve,"."),ve.forEach(s),cs=f(e),z=r(e,"P",{});var we=o(z);Lt=l(we,"The "),qa=r(we,"CODE",{});var In=o(qa);Ct=l(In,"buffer_size"),In.forEach(s),Nt=l(we," argument controls the size of the buffer to randomly sample examples from. Let\u2019s say your dataset has one million examples, and you set the "),Pa=r(we,"CODE",{});var An=o(Pa);Ot=l(An,"buffer_size"),An.forEach(s),Ft=l(we," to ten thousand. "),ta=r(we,"A",{href:!0});var Tn=o(ta);Rt=l(Tn,"datasets.IterableDataset.shuffle()"),Tn.forEach(s),Yt=l(we," will randomly select examples from the first ten thousand examples in the buffer. Selected examples in the buffer are replaced with new examples."),we.forEach(s),ms=f(e),g(Ee.$$.fragment,e),us=f(e),g(te.$$.fragment,e),_s=f(e),W=r(e,"H2",{class:!0});var st=o(W);le=r(st,"A",{id:!0,class:!0,href:!0});var Sn=o(le);Ma=r(Sn,"SPAN",{});var qn=o(Ma);g(De.$$.fragment,qn),qn.forEach(s),Sn.forEach(s),Ht=f(st),za=r(st,"SPAN",{});var Pn=o(za);Vt=l(Pn,"Reshuffle"),Pn.forEach(s),st.forEach(s),gs=f(e),ne=r(e,"P",{});var tt=o(ne);Jt=l(tt,"Sometimes you may want to reshuffle the dataset after each epoch. This will require you to set a different seed for each epoch. Use "),La=r(tt,"CODE",{});var Mn=o(La);Wt=l(Mn,"datasets.IterableDataset.set_epoch()"),Mn.forEach(s),Ut=l(tt,"in between epochs to tell the dataset what epoch you\u2019re on."),tt.forEach(s),bs=f(e),re=r(e,"P",{});var lt=o(re);Bt=l(lt,"Your seed effectively becomes: "),Ca=r(lt,"CODE",{});var zn=o(Ca);Gt=l(zn,"initial seed + current epoch"),zn.forEach(s),Kt=l(lt,"."),lt.forEach(s),js=f(e),g(Ie.$$.fragment,e),vs=f(e),U=r(e,"H2",{class:!0});var nt=o(U);pe=r(nt,"A",{id:!0,class:!0,href:!0});var Ln=o(pe);Na=r(Ln,"SPAN",{});var Cn=o(Na);g(Ae.$$.fragment,Cn),Cn.forEach(s),Ln.forEach(s),Qt=f(nt),Oa=r(nt,"SPAN",{});var Nn=o(Oa);Xt=l(Nn,"Split dataset"),Nn.forEach(s),nt.forEach(s),ws=f(e),la=r(e,"P",{});var On=o(la);Zt=l(On,"You can split your dataset one of two ways:"),On.forEach(s),$s=f(e),na=r(e,"UL",{});var Fn=o(na);oe=r(Fn,"LI",{});var ts=o(oe);ra=r(ts,"A",{href:!0});var Rn=o(ra);el=l(Rn,"datasets.IterableDataset.take()"),Rn.forEach(s),al=l(ts," returns the first "),Fa=r(ts,"CODE",{});var Yn=o(Fa);sl=l(Yn,"n"),Yn.forEach(s),tl=l(ts," examples in a dataset:"),ts.forEach(s),Fn.forEach(s),xs=f(e),g(Te.$$.fragment,e),ks=f(e),pa=r(e,"UL",{});var Hn=o(pa);ie=r(Hn,"LI",{});var ls=o(ie);oa=r(ls,"A",{href:!0});var Vn=o(oa);ll=l(Vn,"datasets.IterableDataset.skip()"),Vn.forEach(s),nl=l(ls," omits the first "),Ra=r(ls,"CODE",{});var Jn=o(Ra);rl=l(Jn,"n"),Jn.forEach(s),pl=l(ls," examples in a dataset and returns the remaining examples:"),ls.forEach(s),Hn.forEach(s),ys=f(e),g(Se.$$.fragment,e),Es=f(e),g(he.$$.fragment,e),Ds=f(e),ia=r(e,"A",{id:!0}),o(ia).forEach(s),Is=f(e),B=r(e,"H2",{class:!0});var rt=o(B);de=r(rt,"A",{id:!0,class:!0,href:!0});var Wn=o(de);Ya=r(Wn,"SPAN",{});var Un=o(Ya);g(qe.$$.fragment,Un),Un.forEach(s),Wn.forEach(s),ol=f(rt),Ha=r(rt,"SPAN",{});var Bn=o(Ha);il=l(Bn,"Interleave"),Bn.forEach(s),rt.forEach(s),As=f(e),G=r(e,"P",{});var ns=o(G);ha=r(ns,"A",{href:!0});var Gn=o(ha);hl=l(Gn,"datasets.interleave_datasets()"),Gn.forEach(s),dl=l(ns," can combine an "),da=r(ns,"A",{href:!0});var Kn=o(da);fl=l(Kn,"datasets.IterableDataset"),Kn.forEach(s),cl=l(ns," with other datasets. The combined dataset returns alternating examples from each of the original datasets."),ns.forEach(s),Ts=f(e),g(Pe.$$.fragment,e),Ss=f(e),fe=r(e,"P",{});var pt=o(fe);ml=l(pt,"Define sampling probabilities from each of the original datasets for more control over how each of them are sampled and combined. Set the "),Va=r(pt,"CODE",{});var Qn=o(Va);ul=l(Qn,"probabilities"),Qn.forEach(s),_l=l(pt," argument with your desired sampling probabilities:"),pt.forEach(s),qs=f(e),g(Me.$$.fragment,e),Ps=f(e),F=r(e,"P",{});var Ea=o(F);gl=l(Ea,"Around 80% of the final dataset is made of the "),Ja=r(Ea,"CODE",{});var Xn=o(Ja);bl=l(Xn,"en_dataset"),Xn.forEach(s),jl=l(Ea,", and 20% of the "),Wa=r(Ea,"CODE",{});var Zn=o(Wa);vl=l(Zn,"fr_dataset"),Zn.forEach(s),wl=l(Ea,"."),Ea.forEach(s),Ms=f(e),K=r(e,"H2",{class:!0});var ot=o(K);ce=r(ot,"A",{id:!0,class:!0,href:!0});var er=o(ce);Ua=r(er,"SPAN",{});var ar=o(Ua);g(ze.$$.fragment,ar),ar.forEach(s),er.forEach(s),$l=f(ot),Ba=r(ot,"SPAN",{});var sr=o(Ba);xl=l(sr,"Remove"),sr.forEach(s),ot.forEach(s),zs=f(e),me=r(e,"P",{});var it=o(me);kl=l(it,"Remove columns on-the-fly with "),fa=r(it,"A",{href:!0});var tr=o(fa);yl=l(tr,"datasets.IterableDataset.remove_columns()"),tr.forEach(s),El=l(it,". Specify the name of the column to remove:"),it.forEach(s),Ls=f(e),g(Le.$$.fragment,e),Cs=f(e),Q=r(e,"H2",{class:!0});var ht=o(Q);ue=r(ht,"A",{id:!0,class:!0,href:!0});var lr=o(ue);Ga=r(lr,"SPAN",{});var nr=o(Ga);g(Ce.$$.fragment,nr),nr.forEach(s),lr.forEach(s),Dl=f(ht),Ka=r(ht,"SPAN",{});var rr=o(Ka);Il=l(rr,"Map"),rr.forEach(s),ht.forEach(s),Ns=f(e),D=r(e,"P",{});var L=o(D);Al=l(L,"Similar to the "),ca=r(L,"A",{href:!0});var pr=o(ca);Tl=l(pr,"datasets.Dataset.map()"),pr.forEach(s),Sl=l(L," function for a regular "),ma=r(L,"A",{href:!0});var or=o(ma);ql=l(or,"datasets.Dataset"),or.forEach(s),Pl=l(L,", \u{1F917}  Datasets features "),ua=r(L,"A",{href:!0});var ir=o(ua);Ml=l(ir,"datasets.IterableDataset.map()"),ir.forEach(s),zl=l(L," for processing "),_a=r(L,"A",{href:!0});var hr=o(_a);Ll=l(hr,"datasets.IterableDataset"),hr.forEach(s),Cl=l(L,`\\s.
`),ga=r(L,"A",{href:!0});var dr=o(ga);Nl=l(dr,"datasets.IterableDataset.map()"),dr.forEach(s),Ol=l(L," applies processing on-the-fly when examples are streamed."),L.forEach(s),Os=f(e),ba=r(e,"P",{});var fr=o(ba);Fl=l(fr,"It allows you to apply a processing function to each example in a dataset, independently or in batches. This function can even create new rows and columns."),fr.forEach(s),Fs=f(e),R=r(e,"P",{});var Da=o(R);Rl=l(Da,"The following example demonstrates how to tokenize a "),ja=r(Da,"A",{href:!0});var cr=o(ja);Yl=l(cr,"datasets.IterableDataset"),cr.forEach(s),Hl=l(Da,". The function needs to accept and output a "),Qa=r(Da,"CODE",{});var mr=o(Qa);Vl=l(mr,"dict"),mr.forEach(s),Jl=l(Da,":"),Da.forEach(s),Rs=f(e),g(Ne.$$.fragment,e),Ys=f(e),_e=r(e,"P",{});var dt=o(_e);Wl=l(dt,"Next, apply this function to the dataset with "),va=r(dt,"A",{href:!0});var ur=o(va);Ul=l(ur,"datasets.IterableDataset.map()"),ur.forEach(s),Bl=l(dt,":"),dt.forEach(s),Hs=f(e),g(Oe.$$.fragment,e),Vs=f(e),ge=r(e,"P",{});var ft=o(ge);Gl=l(ft,"Let\u2019s take a look at another example, except this time, you will remove a column with "),wa=r(ft,"A",{href:!0});var _r=o(wa);Kl=l(_r,"datasets.IterableDataset.map()"),_r.forEach(s),Ql=l(ft,". When you remove a column, it is only removed after the example has been provided to the mapped function. This allows the mapped function to use the content of the columns before they are removed."),ft.forEach(s),Js=f(e),Y=r(e,"P",{});var Ia=o(Y);Xl=l(Ia,"Specify the column to remove with the "),Xa=r(Ia,"CODE",{});var gr=o(Xa);Zl=l(gr,"remove_columns"),gr.forEach(s),en=l(Ia," argument in "),$a=r(Ia,"A",{href:!0});var br=o($a);an=l(br,"datasets.IterableDataset.map()"),br.forEach(s),sn=l(Ia,":"),Ia.forEach(s),Ws=f(e),g(Fe.$$.fragment,e),Us=f(e),X=r(e,"H2",{class:!0});var ct=o(X);be=r(ct,"A",{id:!0,class:!0,href:!0});var jr=o(be);Za=r(jr,"SPAN",{});var vr=o(Za);g(Re.$$.fragment,vr),vr.forEach(s),jr.forEach(s),tn=f(ct),es=r(ct,"SPAN",{});var wr=o(es);ln=l(wr,"Stream in a training loop"),wr.forEach(s),ct.forEach(s),Bs=f(e),Ye=r(e,"P",{});var on=o(Ye);xa=r(on,"A",{href:!0});var $r=o(xa);nn=l($r,"datasets.IterableDataset"),$r.forEach(s),rn=l(on," can be integrated into a training loop. First, shuffle the dataset:"),on.forEach(s),Gs=f(e),g(He.$$.fragment,e),Ks=f(e),ka=r(e,"P",{});var xr=o(ka);pn=l(xr,"Lastly, create a simple training loop and start training:"),xr.forEach(s),Qs=f(e),g(Ve.$$.fragment,e),this.h()},h(){h(c,"name","hf:doc:metadata"),h(c,"content",JSON.stringify(Mr)),h(x,"id","stream"),h(x,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(x,"href","#stream"),h(m,"class","relative group"),h(Ge,"class","block dark:hidden"),kr(Ge.src,dn="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/datasets/streaming.gif")||h(Ge,"src",dn),h(Ke,"class","hidden dark:block"),kr(Ke.src,fn="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/datasets/streaming-dark.gif")||h(Ke,"src",fn),h(V,"class","flex justify-center"),h(xe,"href","https://huggingface.co/datasets/oscar"),h(xe,"rel","nofollow"),h(Qe,"href","/docs/datasets/pr_3801/en/package_reference/loading_methods#datasets.load_dataset"),h(Xe,"href","/docs/datasets/pr_3801/en/package_reference/main_classes#datasets.Dataset"),h(Ze,"href","/docs/datasets/pr_3801/en/package_reference/main_classes#datasets.IterableDataset"),h(se,"id","shuffle"),h(se,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(se,"href","#shuffle"),h(J,"class","relative group"),h(ea,"href","/docs/datasets/pr_3801/en/package_reference/main_classes#datasets.Dataset"),h(aa,"href","/docs/datasets/pr_3801/en/package_reference/main_classes#datasets.IterableDataset"),h(sa,"href","/docs/datasets/pr_3801/en/package_reference/main_classes#datasets.IterableDataset.shuffle"),h(ta,"href","/docs/datasets/pr_3801/en/package_reference/main_classes#datasets.IterableDataset.shuffle"),h(le,"id","reshuffle"),h(le,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(le,"href","#reshuffle"),h(W,"class","relative group"),h(pe,"id","split-dataset"),h(pe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(pe,"href","#split-dataset"),h(U,"class","relative group"),h(ra,"href","/docs/datasets/pr_3801/en/package_reference/main_classes#datasets.IterableDataset.take"),h(oa,"href","/docs/datasets/pr_3801/en/package_reference/main_classes#datasets.IterableDataset.skip"),h(ia,"id","interleave_datasets"),h(de,"id","interleave"),h(de,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(de,"href","#interleave"),h(B,"class","relative group"),h(ha,"href","/docs/datasets/pr_3801/en/package_reference/main_classes#datasets.interleave_datasets"),h(da,"href","/docs/datasets/pr_3801/en/package_reference/main_classes#datasets.IterableDataset"),h(ce,"id","remove"),h(ce,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(ce,"href","#remove"),h(K,"class","relative group"),h(fa,"href","/docs/datasets/pr_3801/en/package_reference/main_classes#datasets.IterableDataset.remove_columns"),h(ue,"id","map"),h(ue,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(ue,"href","#map"),h(Q,"class","relative group"),h(ca,"href","/docs/datasets/pr_3801/en/package_reference/main_classes#datasets.Dataset.map"),h(ma,"href","/docs/datasets/pr_3801/en/package_reference/main_classes#datasets.Dataset"),h(ua,"href","/docs/datasets/pr_3801/en/package_reference/main_classes#datasets.IterableDataset.map"),h(_a,"href","/docs/datasets/pr_3801/en/package_reference/main_classes#datasets.IterableDataset"),h(ga,"href","/docs/datasets/pr_3801/en/package_reference/main_classes#datasets.IterableDataset.map"),h(ja,"href","/docs/datasets/pr_3801/en/package_reference/main_classes#datasets.IterableDataset"),h(va,"href","/docs/datasets/pr_3801/en/package_reference/main_classes#datasets.IterableDataset.map"),h(wa,"href","/docs/datasets/pr_3801/en/package_reference/main_classes#datasets.IterableDataset.map"),h($a,"href","/docs/datasets/pr_3801/en/package_reference/main_classes#datasets.IterableDataset.map"),h(be,"id","stream-in-a-training-loop"),h(be,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(be,"href","#stream-in-a-training-loop"),h(X,"class","relative group"),h(xa,"href","/docs/datasets/pr_3801/en/package_reference/main_classes#datasets.IterableDataset")},m(e,p){a(document.head,c),i(e,$,p),i(e,m,p),a(m,x),a(x,k),b(u,k,null),a(m,T),a(m,I),a(I,q),i(e,y,p),i(e,A,p),a(A,N),i(e,Z,p),i(e,S,p),a(S,E),a(E,We),a(S,Ue),a(S,ee),a(ee,Be),i(e,rs,p),i(e,V,p),a(V,Ge),a(V,mt),a(V,Ke),i(e,ps,p),i(e,P,p),a(P,ut),a(P,xe),a(xe,_t),a(P,gt),a(P,Aa),a(Aa,bt),a(P,jt),a(P,Qe),a(Qe,vt),a(P,wt),i(e,os,p),b(ke,e,p),i(e,is,p),i(e,O,p),a(O,$t),a(O,Xe),a(Xe,xt),a(O,kt),a(O,Ze),a(Ze,yt),a(O,Et),i(e,hs,p),b(ae,e,p),i(e,ds,p),i(e,J,p),a(J,se),a(se,Ta),b(ye,Ta,null),a(J,Dt),a(J,Sa),a(Sa,It),i(e,fs,p),i(e,M,p),a(M,At),a(M,ea),a(ea,Tt),a(M,St),a(M,aa),a(aa,qt),a(M,Pt),a(M,sa),a(sa,Mt),a(M,zt),i(e,cs,p),i(e,z,p),a(z,Lt),a(z,qa),a(qa,Ct),a(z,Nt),a(z,Pa),a(Pa,Ot),a(z,Ft),a(z,ta),a(ta,Rt),a(z,Yt),i(e,ms,p),b(Ee,e,p),i(e,us,p),b(te,e,p),i(e,_s,p),i(e,W,p),a(W,le),a(le,Ma),b(De,Ma,null),a(W,Ht),a(W,za),a(za,Vt),i(e,gs,p),i(e,ne,p),a(ne,Jt),a(ne,La),a(La,Wt),a(ne,Ut),i(e,bs,p),i(e,re,p),a(re,Bt),a(re,Ca),a(Ca,Gt),a(re,Kt),i(e,js,p),b(Ie,e,p),i(e,vs,p),i(e,U,p),a(U,pe),a(pe,Na),b(Ae,Na,null),a(U,Qt),a(U,Oa),a(Oa,Xt),i(e,ws,p),i(e,la,p),a(la,Zt),i(e,$s,p),i(e,na,p),a(na,oe),a(oe,ra),a(ra,el),a(oe,al),a(oe,Fa),a(Fa,sl),a(oe,tl),i(e,xs,p),b(Te,e,p),i(e,ks,p),i(e,pa,p),a(pa,ie),a(ie,oa),a(oa,ll),a(ie,nl),a(ie,Ra),a(Ra,rl),a(ie,pl),i(e,ys,p),b(Se,e,p),i(e,Es,p),b(he,e,p),i(e,Ds,p),i(e,ia,p),i(e,Is,p),i(e,B,p),a(B,de),a(de,Ya),b(qe,Ya,null),a(B,ol),a(B,Ha),a(Ha,il),i(e,As,p),i(e,G,p),a(G,ha),a(ha,hl),a(G,dl),a(G,da),a(da,fl),a(G,cl),i(e,Ts,p),b(Pe,e,p),i(e,Ss,p),i(e,fe,p),a(fe,ml),a(fe,Va),a(Va,ul),a(fe,_l),i(e,qs,p),b(Me,e,p),i(e,Ps,p),i(e,F,p),a(F,gl),a(F,Ja),a(Ja,bl),a(F,jl),a(F,Wa),a(Wa,vl),a(F,wl),i(e,Ms,p),i(e,K,p),a(K,ce),a(ce,Ua),b(ze,Ua,null),a(K,$l),a(K,Ba),a(Ba,xl),i(e,zs,p),i(e,me,p),a(me,kl),a(me,fa),a(fa,yl),a(me,El),i(e,Ls,p),b(Le,e,p),i(e,Cs,p),i(e,Q,p),a(Q,ue),a(ue,Ga),b(Ce,Ga,null),a(Q,Dl),a(Q,Ka),a(Ka,Il),i(e,Ns,p),i(e,D,p),a(D,Al),a(D,ca),a(ca,Tl),a(D,Sl),a(D,ma),a(ma,ql),a(D,Pl),a(D,ua),a(ua,Ml),a(D,zl),a(D,_a),a(_a,Ll),a(D,Cl),a(D,ga),a(ga,Nl),a(D,Ol),i(e,Os,p),i(e,ba,p),a(ba,Fl),i(e,Fs,p),i(e,R,p),a(R,Rl),a(R,ja),a(ja,Yl),a(R,Hl),a(R,Qa),a(Qa,Vl),a(R,Jl),i(e,Rs,p),b(Ne,e,p),i(e,Ys,p),i(e,_e,p),a(_e,Wl),a(_e,va),a(va,Ul),a(_e,Bl),i(e,Hs,p),b(Oe,e,p),i(e,Vs,p),i(e,ge,p),a(ge,Gl),a(ge,wa),a(wa,Kl),a(ge,Ql),i(e,Js,p),i(e,Y,p),a(Y,Xl),a(Y,Xa),a(Xa,Zl),a(Y,en),a(Y,$a),a($a,an),a(Y,sn),i(e,Ws,p),b(Fe,e,p),i(e,Us,p),i(e,X,p),a(X,be),a(be,Za),b(Re,Za,null),a(X,tn),a(X,es),a(es,ln),i(e,Bs,p),i(e,Ye,p),a(Ye,xa),a(xa,nn),a(Ye,rn),i(e,Gs,p),b(He,e,p),i(e,Ks,p),i(e,ka,p),a(ka,pn),i(e,Qs,p),b(Ve,e,p),Xs=!0},p(e,[p]){const Je={};p&2&&(Je.$$scope={dirty:p,ctx:e}),ae.$set(Je);const as={};p&2&&(as.$$scope={dirty:p,ctx:e}),te.$set(as);const ss={};p&2&&(ss.$$scope={dirty:p,ctx:e}),he.$set(ss)},i(e){Xs||(j(u.$$.fragment,e),j(ke.$$.fragment,e),j(ae.$$.fragment,e),j(ye.$$.fragment,e),j(Ee.$$.fragment,e),j(te.$$.fragment,e),j(De.$$.fragment,e),j(Ie.$$.fragment,e),j(Ae.$$.fragment,e),j(Te.$$.fragment,e),j(Se.$$.fragment,e),j(he.$$.fragment,e),j(qe.$$.fragment,e),j(Pe.$$.fragment,e),j(Me.$$.fragment,e),j(ze.$$.fragment,e),j(Le.$$.fragment,e),j(Ce.$$.fragment,e),j(Ne.$$.fragment,e),j(Oe.$$.fragment,e),j(Fe.$$.fragment,e),j(Re.$$.fragment,e),j(He.$$.fragment,e),j(Ve.$$.fragment,e),Xs=!0)},o(e){v(u.$$.fragment,e),v(ke.$$.fragment,e),v(ae.$$.fragment,e),v(ye.$$.fragment,e),v(Ee.$$.fragment,e),v(te.$$.fragment,e),v(De.$$.fragment,e),v(Ie.$$.fragment,e),v(Ae.$$.fragment,e),v(Te.$$.fragment,e),v(Se.$$.fragment,e),v(he.$$.fragment,e),v(qe.$$.fragment,e),v(Pe.$$.fragment,e),v(Me.$$.fragment,e),v(ze.$$.fragment,e),v(Le.$$.fragment,e),v(Ce.$$.fragment,e),v(Ne.$$.fragment,e),v(Oe.$$.fragment,e),v(Fe.$$.fragment,e),v(Re.$$.fragment,e),v(He.$$.fragment,e),v(Ve.$$.fragment,e),Xs=!1},d(e){s(c),e&&s($),e&&s(m),w(u),e&&s(y),e&&s(A),e&&s(Z),e&&s(S),e&&s(rs),e&&s(V),e&&s(ps),e&&s(P),e&&s(os),w(ke,e),e&&s(is),e&&s(O),e&&s(hs),w(ae,e),e&&s(ds),e&&s(J),w(ye),e&&s(fs),e&&s(M),e&&s(cs),e&&s(z),e&&s(ms),w(Ee,e),e&&s(us),w(te,e),e&&s(_s),e&&s(W),w(De),e&&s(gs),e&&s(ne),e&&s(bs),e&&s(re),e&&s(js),w(Ie,e),e&&s(vs),e&&s(U),w(Ae),e&&s(ws),e&&s(la),e&&s($s),e&&s(na),e&&s(xs),w(Te,e),e&&s(ks),e&&s(pa),e&&s(ys),w(Se,e),e&&s(Es),w(he,e),e&&s(Ds),e&&s(ia),e&&s(Is),e&&s(B),w(qe),e&&s(As),e&&s(G),e&&s(Ts),w(Pe,e),e&&s(Ss),e&&s(fe),e&&s(qs),w(Me,e),e&&s(Ps),e&&s(F),e&&s(Ms),e&&s(K),w(ze),e&&s(zs),e&&s(me),e&&s(Ls),w(Le,e),e&&s(Cs),e&&s(Q),w(Ce),e&&s(Ns),e&&s(D),e&&s(Os),e&&s(ba),e&&s(Fs),e&&s(R),e&&s(Rs),w(Ne,e),e&&s(Ys),e&&s(_e),e&&s(Hs),w(Oe,e),e&&s(Vs),e&&s(ge),e&&s(Js),e&&s(Y),e&&s(Ws),w(Fe,e),e&&s(Us),e&&s(X),w(Re),e&&s(Bs),e&&s(Ye),e&&s(Gs),w(He,e),e&&s(Ks),e&&s(ka),e&&s(Qs),w(Ve,e)}}}const Mr={local:"stream",sections:[{local:"shuffle",title:"Shuffle"},{local:"reshuffle",title:"Reshuffle"},{local:"split-dataset",title:"Split dataset"},{local:"interleave",title:"Interleave"},{local:"remove",title:"Remove"},{local:"map",title:"Map"},{local:"stream-in-a-training-loop",title:"Stream in a training loop"}],title:"Stream"};function zr(H,c,$){let{fw:m}=c;return H.$$set=x=>{"fw"in x&&$(0,m=x.fw)},[m]}class Rr extends yr{constructor(c){super();Er(this,c,zr,Pr,Dr,{fw:0})}}export{Rr as default,Mr as metadata};
