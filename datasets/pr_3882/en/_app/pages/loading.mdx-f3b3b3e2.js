import{S as fh,i as ch,s as hh,e as l,k as d,w as u,t as r,M as uh,c as o,d as t,m as f,a as n,x as m,h as i,b as c,F as a,g as p,y as g,q as _,o as v,B as $}from"../chunks/vendor-e67aec41.js";import{T as ha}from"../chunks/Tip-76459d1c.js";import{I as P}from"../chunks/IconCopyLink-ffd7f84e.js";import{C as E}from"../chunks/CodeBlock-e2bcf023.js";import{C as Cl}from"../chunks/CodeBlockFw-1e02e2ba.js";function mh(T){let h,x,y,b,k;return{c(){h=l("p"),x=r("Refer to the "),y=l("a"),b=r("upload_dataset_repo"),k=r(" guide for more instructions on how to create a dataset repository on the Hub, and how to upload your data files."),this.h()},l(w){h=o(w,"P",{});var j=n(h);x=i(j,"Refer to the "),y=o(j,"A",{href:!0});var q=n(y);b=i(q,"upload_dataset_repo"),q.forEach(t),k=i(j," guide for more instructions on how to create a dataset repository on the Hub, and how to upload your data files."),j.forEach(t),this.h()},h(){c(y,"href","#upload_dataset_repo")},m(w,j){p(w,h,j),a(h,x),a(h,y),a(y,b),a(h,k)},d(w){w&&t(h)}}}function gh(T){let h,x,y,b,k;return{c(){h=l("p"),x=r("If you don\u2019t specify which data files to use, "),y=l("code"),b=r("load_dataset"),k=r(" will return all the data files. This can take a long time if you are loading a large dataset like C4, which is approximately 13TB of data.")},l(w){h=o(w,"P",{});var j=n(h);x=i(j,"If you don\u2019t specify which data files to use, "),y=o(j,"CODE",{});var q=n(y);b=i(q,"load_dataset"),q.forEach(t),k=i(j," will return all the data files. This can take a long time if you are loading a large dataset like C4, which is approximately 13TB of data."),j.forEach(t)},m(w,j){p(w,h,j),a(h,x),a(h,y),a(y,b),a(h,k)},d(w){w&&t(h)}}}function _h(T){let h,x,y,b,k;return{c(){h=l("p"),x=r("Curious about how to load datasets for vision? Check out the image loading guide "),y=l("a"),b=r("here"),k=r("!"),this.h()},l(w){h=o(w,"P",{});var j=n(h);x=i(j,"Curious about how to load datasets for vision? Check out the image loading guide "),y=o(j,"A",{href:!0});var q=n(y);b=i(q,"here"),q.forEach(t),k=i(j,"!"),j.forEach(t),this.h()},h(){c(y,"href","./image_process")},m(w,j){p(w,h,j),a(h,x),a(h,y),a(y,b),a(h,k)},d(w){w&&t(h)}}}function vh(T){let h,x,y,b,k,w,j,q,X,vs,L,K,$s,R,M,ys,A;return{c(){h=l("p"),x=r("An object data type in "),y=l("a"),b=r("pandas.Series"),k=r(" doesn\u2019t always carry enough information for Arrow to automatically infer a data type. For example, if a DataFrame is of length 0 or the Series only contains None/nan objects, the type is set to null. Avoid potential errors by constructing an explicit schema with "),w=l("a"),j=r("datasets.Features"),q=r(" using the "),X=l("code"),vs=r("from_dict"),L=r(" or "),K=l("code"),$s=r("from_pandas"),R=r(" methods. See the "),M=l("a"),ys=r("troubleshoot"),A=r(" for more details on how to explicitly specify your own features."),this.h()},l(F){h=o(F,"P",{});var S=n(h);x=i(S,"An object data type in "),y=o(S,"A",{href:!0,rel:!0});var ua=n(y);b=i(ua,"pandas.Series"),ua.forEach(t),k=i(S," doesn\u2019t always carry enough information for Arrow to automatically infer a data type. For example, if a DataFrame is of length 0 or the Series only contains None/nan objects, the type is set to null. Avoid potential errors by constructing an explicit schema with "),w=o(S,"A",{href:!0});var ws=n(w);j=i(ws,"datasets.Features"),ws.forEach(t),q=i(S," using the "),X=o(S,"CODE",{});var ma=n(X);vs=i(ma,"from_dict"),ma.forEach(t),L=i(S," or "),K=o(S,"CODE",{});var ga=n(K);$s=i(ga,"from_pandas"),ga.forEach(t),R=i(S," methods. See the "),M=o(S,"A",{href:!0});var js=n(M);ys=i(js,"troubleshoot"),js.forEach(t),A=i(S," for more details on how to explicitly specify your own features."),S.forEach(t),this.h()},h(){c(y,"href","https://pandas.pydata.org/docs/reference/api/pandas.Series.html"),c(y,"rel","nofollow"),c(w,"href","/docs/datasets/pr_3882/en/package_reference/main_classes#datasets.Features"),c(M,"href","#troubleshoot")},m(F,S){p(F,h,S),a(h,x),a(h,y),a(y,b),a(h,k),a(h,w),a(w,j),a(h,q),a(h,X),a(X,vs),a(h,L),a(h,K),a(K,$s),a(h,R),a(h,M),a(M,ys),a(h,A)},d(F){F&&t(h)}}}function $h(T){let h,x,y,b,k;return{c(){h=l("p"),x=r("Using "),y=l("code"),b=r("pct1_dropremainder"),k=r(" rounding may truncate the last examples in a dataset if the number of examples in your dataset don\u2019t divide evenly by 100.")},l(w){h=o(w,"P",{});var j=n(h);x=i(j,"Using "),y=o(j,"CODE",{});var q=n(y);b=i(q,"pct1_dropremainder"),q.forEach(t),k=i(j," rounding may truncate the last examples in a dataset if the number of examples in your dataset don\u2019t divide evenly by 100."),j.forEach(t)},m(w,j){p(w,h,j),a(h,x),a(h,y),a(y,b),a(h,k)},d(w){w&&t(h)}}}function yh(T){let h,x,y,b,k;return{c(){h=l("p"),x=r("See the "),y=l("a"),b=r("metric_script"),k=r(" guide for more details on how to write your own metric loading script."),this.h()},l(w){h=o(w,"P",{});var j=n(h);x=i(j,"See the "),y=o(j,"A",{href:!0});var q=n(y);b=i(q,"metric_script"),q.forEach(t),k=i(j," guide for more details on how to write your own metric loading script."),j.forEach(t),this.h()},h(){c(y,"href","#metric_script")},m(w,j){p(w,h,j),a(h,x),a(h,y),a(y,b),a(h,k)},d(w){w&&t(h)}}}function wh(T){let h,x,y,b,k;return{c(){h=l("p"),x=r("Once you\u2019ve loaded a metric for distributed usage, you can compute the metric as usual. Behind the scenes, "),y=l("a"),b=r("datasets.Metric.compute()"),k=r(" gathers all the predictions and references from the nodes, and computes the final metric."),this.h()},l(w){h=o(w,"P",{});var j=n(h);x=i(j,"Once you\u2019ve loaded a metric for distributed usage, you can compute the metric as usual. Behind the scenes, "),y=o(j,"A",{href:!0});var q=n(y);b=i(q,"datasets.Metric.compute()"),q.forEach(t),k=i(j," gathers all the predictions and references from the nodes, and computes the final metric."),j.forEach(t),this.h()},h(){c(y,"href","/docs/datasets/pr_3882/en/package_reference/main_classes#datasets.Metric.compute")},m(w,j){p(w,h,j),a(h,x),a(h,y),a(y,b),a(h,k)},d(w){w&&t(h)}}}function jh(T){let h,x,y,b,k,w,j,q,X,vs,L,K,$s,R,M,ys,A,F,S,ua,ws,ma,ga,js,vr,$r,ue,yr,wr,me,jr,Nl,_a,br,Il,va,Ol,Z,bs,ge,rt,xr,_e,kr,Hl,xs,Er,ve,qr,Pr,Fl,V,Ar,$a,Sr,Tr,it,Dr,Cr,Ll,pt,Rl,ya,Nr,Ml,ks,Ir,$e,Or,Hr,Vl,dt,zl,Es,Jl,D,Fr,ye,Lr,Rr,we,Mr,Vr,je,zr,Jr,be,Ur,Br,xe,Yr,Wr,Ul,ft,Bl,qs,Yl,z,Gr,ke,Qr,Xr,ct,Kr,Zr,Wl,ht,Gl,Ps,si,Ee,ti,ai,Ql,ut,Xl,ss,As,qe,mt,ei,Pe,li,Kl,C,oi,Ae,ni,ri,Se,ii,pi,Te,di,fi,De,ci,hi,wa,ui,mi,Zl,Ss,so,ts,Ts,Ce,gt,gi,Ne,_i,to,ja,vi,ao,_t,eo,ba,$i,lo,vt,oo,xa,yi,no,$t,ro,ka,wi,io,yt,po,Ea,ji,fo,wt,co,as,Ds,Ie,jt,bi,Oe,xi,ho,Cs,ki,qa,Ei,qi,uo,bt,mo,Pa,Pi,go,xt,_o,Ns,Ai,He,Si,Ti,vo,kt,$o,Aa,Di,yo,Et,wo,Sa,Ci,jo,es,Is,Fe,qt,Ni,Le,Ii,bo,Ta,Oi,xo,Pt,ko,Da,Hi,Eo,At,qo,ls,Os,Re,St,Fi,Me,Li,Po,Ca,Ri,Ao,Tt,So,Na,Mi,To,Dt,Do,os,Hs,Ve,Ct,Vi,ze,zi,Co,Fs,Ji,Ia,Ui,Bi,No,ns,Ls,Je,Nt,Yi,Ue,Wi,Io,Rs,Gi,Oa,Qi,Xi,Oo,It,Ho,rs,Ms,Be,Ot,Ki,Ye,Zi,Fo,Vs,sp,Ha,tp,ap,Lo,Ht,Ro,zs,Mo,is,Js,We,Ft,ep,Ge,lp,Vo,Fa,op,zo,J,np,Qe,rp,ip,Xe,pp,dp,Jo,ps,Us,Ke,Lt,fp,Ze,cp,Uo,U,hp,La,up,mp,Ra,gp,_p,Bo,B,vp,sl,$p,yp,tl,wp,jp,Yo,Rt,Wo,Bs,bp,al,xp,kp,Go,Mt,Qo,Ma,Ep,Xo,Vt,Ko,Va,qp,Zo,zt,sn,za,Pp,tn,Jt,an,ds,Ys,el,Ut,Ap,ll,Sp,en,Ja,Tp,ln,Bt,on,Ws,Dp,ol,Cp,Np,nn,Yt,rn,Gs,pn,Ua,dn,fs,Qs,nl,Wt,Ip,rl,Op,fn,Ba,Hp,cn,cs,Xs,il,Gt,Fp,pl,Lp,hn,I,Rp,Ya,Mp,Vp,dl,zp,Jp,fl,Up,Bp,un,Ks,Yp,Qt,Wp,Gp,mn,Xt,gn,hs,Zs,cl,Kt,Qp,hl,Xp,_n,Y,Kp,Wa,Zp,sd,Zt,td,ad,vn,W,ed,Ga,ld,od,Qa,nd,rd,$n,sa,yn,G,id,ul,pd,dd,Xa,fd,cd,wn,ta,jn,Ka,hd,bn,aa,xn,us,st,ml,ea,ud,gl,md,kn,Za,gd,En,la,qn,tt,Pn,ms,at,_l,oa,_d,vl,vd,An,se,$d,Sn,na,Tn,gs,et,$l,ra,yd,yl,wd,Dn,te,jd,Cn,ae,bd,Nn,Q,wl,ia,xd,jl,kd,Ed,qd,bl,_s,Pd,xl,Ad,Sd,kl,Td,Dd,Cd,El,pa,Nd,ee,Id,Od,In,da,On,lt,Hn,ot,Hd,ql,Fd,Ld,Fn,fa,Ln;return w=new P({}),rt=new P({}),pt=new E({props:{code:`from datasets import load_dataset
dataset = load_dataset('lhoestq/demo1')`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;lhoestq/demo1&#x27;</span>)`}}),dt=new E({props:{code:`dataset = load_dataset(
  "lhoestq/custom_squad",
  revision="main"  # tag name, or branch name, or commit hash
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(
<span class="hljs-meta">&gt;&gt;&gt; </span>  <span class="hljs-string">&quot;lhoestq/custom_squad&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>  revision=<span class="hljs-string">&quot;main&quot;</span>  <span class="hljs-comment"># tag name, or branch name, or commit hash</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>)`}}),Es=new ha({props:{$$slots:{default:[mh]},$$scope:{ctx:T}}}),ft=new E({props:{code:`data_files = {"train": "train.csv", "test": "test.csv"}
dataset = load_dataset("namespace/your_dataset_name", data_files=data_files)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>data_files = {<span class="hljs-string">&quot;train&quot;</span>: <span class="hljs-string">&quot;train.csv&quot;</span>, <span class="hljs-string">&quot;test&quot;</span>: <span class="hljs-string">&quot;test.csv&quot;</span>}
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;namespace/your_dataset_name&quot;</span>, data_files=data_files)`}}),qs=new ha({props:{warning:"&lcub;true}",$$slots:{default:[gh]},$$scope:{ctx:T}}}),ht=new E({props:{code:`from datasets import load_dataset
c4_subset = load_dataset('allenai/c4', data_files='en/c4-train.0000*-of-01024.json.gz')`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>c4_subset = load_dataset(<span class="hljs-string">&#x27;allenai/c4&#x27;</span>, data_files=<span class="hljs-string">&#x27;en/c4-train.0000*-of-01024.json.gz&#x27;</span>)`}}),ut=new E({props:{code:`data_files = {"validation": "en/c4-validation.*.json.gz"}
c4_validation = load_dataset("allenai/c4", data_files=data_files, split="validation")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>data_files = {<span class="hljs-string">&quot;validation&quot;</span>: <span class="hljs-string">&quot;en/c4-validation.*.json.gz&quot;</span>}
<span class="hljs-meta">&gt;&gt;&gt; </span>c4_validation = load_dataset(<span class="hljs-string">&quot;allenai/c4&quot;</span>, data_files=data_files, split=<span class="hljs-string">&quot;validation&quot;</span>)`}}),mt=new P({}),Ss=new ha({props:{$$slots:{default:[_h]},$$scope:{ctx:T}}}),gt=new P({}),_t=new E({props:{code:`from datasets import load_dataset
dataset = load_dataset('csv', data_files='my_file.csv')`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;csv&#x27;</span>, data_files=<span class="hljs-string">&#x27;my_file.csv&#x27;</span>)`}}),vt=new E({props:{code:"dataset = load_dataset('csv', data_files=['my_file_1.csv', 'my_file_2.csv', 'my_file_3.csv'])",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;csv&#x27;</span>, data_files=[<span class="hljs-string">&#x27;my_file_1.csv&#x27;</span>, <span class="hljs-string">&#x27;my_file_2.csv&#x27;</span>, <span class="hljs-string">&#x27;my_file_3.csv&#x27;</span>])'}}),$t=new E({props:{code:"dataset = load_dataset('csv', data_files={'train': ['my_train_file_1.csv', 'my_train_file_2.csv'] 'test': 'my_test_file.csv'})",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;csv&#x27;</span>, data_files={<span class="hljs-string">&#x27;train&#x27;</span>: [<span class="hljs-string">&#x27;my_train_file_1.csv&#x27;</span>, <span class="hljs-string">&#x27;my_train_file_2.csv&#x27;</span>] <span class="hljs-string">&#x27;test&#x27;</span>: <span class="hljs-string">&#x27;my_test_file.csv&#x27;</span>})'}}),yt=new E({props:{code:`base_url = "https://huggingface.co/datasets/lhoestq/demo1/resolve/main/data/"
dataset = load_dataset('csv', data_files={'train': base_url + 'train.csv', 'test': base_url + 'test.csv'})`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>base_url = <span class="hljs-string">&quot;https://huggingface.co/datasets/lhoestq/demo1/resolve/main/data/&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;csv&#x27;</span>, data_files={<span class="hljs-string">&#x27;train&#x27;</span>: base_url + <span class="hljs-string">&#x27;train.csv&#x27;</span>, <span class="hljs-string">&#x27;test&#x27;</span>: base_url + <span class="hljs-string">&#x27;test.csv&#x27;</span>})`}}),wt=new E({props:{code:`url = "https://domain.org/train_data.zip"
data_files = {"train": url}
dataset = load_dataset("csv", data_files=data_files)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;https://domain.org/train_data.zip&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>data_files = {<span class="hljs-string">&quot;train&quot;</span>: url}
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;csv&quot;</span>, data_files=data_files)`}}),jt=new P({}),bt=new E({props:{code:`from datasets import load_dataset
dataset = load_dataset('json', data_files='my_file.json')`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;json&#x27;</span>, data_files=<span class="hljs-string">&#x27;my_file.json&#x27;</span>)`}}),xt=new E({props:{code:`{"a": 1, "b": 2.0, "c": "foo", "d": false}
{"a": 4, "b": -5.5, "c": null, "d": true}`,highlighted:`{<span class="hljs-string">&quot;a&quot;</span>: <span class="hljs-number">1</span>, <span class="hljs-string">&quot;b&quot;</span>: <span class="hljs-number">2.0</span>, <span class="hljs-string">&quot;c&quot;</span>: <span class="hljs-string">&quot;foo&quot;</span>, <span class="hljs-string">&quot;d&quot;</span>: false}
{<span class="hljs-string">&quot;a&quot;</span>: <span class="hljs-number">4</span>, <span class="hljs-string">&quot;b&quot;</span>: -<span class="hljs-number">5.5</span>, <span class="hljs-string">&quot;c&quot;</span>: null, <span class="hljs-string">&quot;d&quot;</span>: true}`}}),kt=new E({props:{code:`{"version": "0.1.0",
    "data": [{"a": 1, "b": 2.0, "c": "foo", "d": false},
            {"a": 4, "b": -5.5, "c": null, "d": true}]
}

>>> from datasets import load_dataset
>>> dataset = load_dataset('json', data_files='my_file.json', field='data')`,highlighted:`{<span class="hljs-string">&quot;version&quot;</span>: <span class="hljs-string">&quot;0.1.0&quot;</span>,
    <span class="hljs-string">&quot;data&quot;</span>: [{<span class="hljs-string">&quot;a&quot;</span>: <span class="hljs-number">1</span>, <span class="hljs-string">&quot;b&quot;</span>: <span class="hljs-number">2.0</span>, <span class="hljs-string">&quot;c&quot;</span>: <span class="hljs-string">&quot;foo&quot;</span>, <span class="hljs-string">&quot;d&quot;</span>: false},
            {<span class="hljs-string">&quot;a&quot;</span>: <span class="hljs-number">4</span>, <span class="hljs-string">&quot;b&quot;</span>: -<span class="hljs-number">5.5</span>, <span class="hljs-string">&quot;c&quot;</span>: null, <span class="hljs-string">&quot;d&quot;</span>: true}]
}

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;json&#x27;</span>, data_files=<span class="hljs-string">&#x27;my_file.json&#x27;</span>, field=<span class="hljs-string">&#x27;data&#x27;</span>)`}}),Et=new E({props:{code:`base_url = "https://rajpurkar.github.io/SQuAD-explorer/dataset/"
dataset = load_dataset('json', data_files={'train': base_url + 'train-v1.1.json', 'validation': base_url + 'dev-v1.1.json'}, field="data")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>base_url = <span class="hljs-string">&quot;https://rajpurkar.github.io/SQuAD-explorer/dataset/&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;json&#x27;</span>, data_files={<span class="hljs-string">&#x27;train&#x27;</span>: base_url + <span class="hljs-string">&#x27;train-v1.1.json&#x27;</span>, <span class="hljs-string">&#x27;validation&#x27;</span>: base_url + <span class="hljs-string">&#x27;dev-v1.1.json&#x27;</span>}, field=<span class="hljs-string">&quot;data&quot;</span>)`}}),qt=new P({}),Pt=new E({props:{code:`from datasets import load_dataset
dataset = load_dataset('text', data_files={'train': ['my_text_1.txt', 'my_text_2.txt'], 'test': 'my_test_file.txt'})`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;text&#x27;</span>, data_files={<span class="hljs-string">&#x27;train&#x27;</span>: [<span class="hljs-string">&#x27;my_text_1.txt&#x27;</span>, <span class="hljs-string">&#x27;my_text_2.txt&#x27;</span>], <span class="hljs-string">&#x27;test&#x27;</span>: <span class="hljs-string">&#x27;my_test_file.txt&#x27;</span>})`}}),At=new E({props:{code:"dataset = load_dataset('text', data_files='https://huggingface.co/datasets/lhoestq/test/resolve/main/some_text.txt')",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;text&#x27;</span>, data_files=<span class="hljs-string">&#x27;https://huggingface.co/datasets/lhoestq/test/resolve/main/some_text.txt&#x27;</span>)'}}),St=new P({}),Tt=new E({props:{code:`from datasets import load_dataset
dataset = load_dataset("parquet", data_files={'train': 'train.parquet', 'test': 'test.parquet'})`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;parquet&quot;</span>, data_files={<span class="hljs-string">&#x27;train&#x27;</span>: <span class="hljs-string">&#x27;train.parquet&#x27;</span>, <span class="hljs-string">&#x27;test&#x27;</span>: <span class="hljs-string">&#x27;test.parquet&#x27;</span>})`}}),Dt=new E({props:{code:`base_url = "https://storage.googleapis.com/huggingface-nlp/cache/datasets/wikipedia/20200501.en/1.0.0/"
data_files = {"train": base_url + "wikipedia-train.parquet"}
wiki = load_dataset("parquet", data_files=data_files, split="train")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>base_url = <span class="hljs-string">&quot;https://storage.googleapis.com/huggingface-nlp/cache/datasets/wikipedia/20200501.en/1.0.0/&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>data_files = {<span class="hljs-string">&quot;train&quot;</span>: base_url + <span class="hljs-string">&quot;wikipedia-train.parquet&quot;</span>}
<span class="hljs-meta">&gt;&gt;&gt; </span>wiki = load_dataset(<span class="hljs-string">&quot;parquet&quot;</span>, data_files=data_files, split=<span class="hljs-string">&quot;train&quot;</span>)`}}),Ct=new P({}),Nt=new P({}),It=new E({props:{code:`from datasets import Dataset
my_dict = {"a": [1, 2, 3]}
dataset = Dataset.from_dict(my_dict)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> Dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>my_dict = {<span class="hljs-string">&quot;a&quot;</span>: [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>]}
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = Dataset.from_dict(my_dict)`}}),Ot=new P({}),Ht=new E({props:{code:`from datasets import Dataset
import pandas as pd
df = pd.DataFrame({"a": [1, 2, 3]})
dataset = Dataset.from_pandas(df)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> Dataset
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd
<span class="hljs-meta">&gt;&gt;&gt; </span>df = pd.DataFrame({<span class="hljs-string">&quot;a&quot;</span>: [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>]})
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = Dataset.from_pandas(df)`}}),zs=new ha({props:{warning:"&lcub;true}",$$slots:{default:[vh]},$$scope:{ctx:T}}}),Ft=new P({}),Lt=new P({}),Rt=new Cl({props:{group1:{id:"stringapi",code:"train_test_ds = datasets.load_dataset('bookcorpus', split='train+test')",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>train_test_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=<span class="hljs-string">&#x27;train+test&#x27;</span>)'},group2:{id:"readinstruction",code:`ri = datasets.ReadInstruction('train') + datasets.ReadInstruction('test')
train_test_ds = datasets.load_dataset('bookcorpus', split=ri)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>ri = datasets.ReadInstruction(<span class="hljs-string">&#x27;train&#x27;</span>) + datasets.ReadInstruction(<span class="hljs-string">&#x27;test&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>train_test_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=ri)`}}}),Mt=new Cl({props:{group1:{id:"stringapi",code:"train_10_20_ds = datasets.load_dataset('bookcorpus', split='train[10:20]')",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>train_10_20_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=<span class="hljs-string">&#x27;train[10:20]&#x27;</span>)'},group2:{id:"readinstruction",code:"train_10_20_ds = datasets.load_dataset('bookcorpus', split=datasets.ReadInstruction('train', from_=10, to=20, unit='abs'))",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>train_10_20_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=datasets.ReadInstruction(<span class="hljs-string">&#x27;train&#x27;</span>, from_=<span class="hljs-number">10</span>, to=<span class="hljs-number">20</span>, unit=<span class="hljs-string">&#x27;abs&#x27;</span>))'}}}),Vt=new Cl({props:{group1:{id:"stringapi",code:"train_10pct_ds = datasets.load_dataset('bookcorpus', split='train[:10%]')",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>train_10pct_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=<span class="hljs-string">&#x27;train[:10%]&#x27;</span>)'},group2:{id:"readinstruction",code:"train_10_20_ds = datasets.load_dataset('bookcorpus', split=datasets.ReadInstruction('train', to=10, unit='%'))",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>train_10_20_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=datasets.ReadInstruction(<span class="hljs-string">&#x27;train&#x27;</span>, to=<span class="hljs-number">10</span>, unit=<span class="hljs-string">&#x27;%&#x27;</span>))'}}}),zt=new Cl({props:{group1:{id:"stringapi",code:"train_10_80pct_ds = datasets.load_dataset('bookcorpus', split='train[:10%]+train[-80%:]')",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>train_10_80pct_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=<span class="hljs-string">&#x27;train[:10%]+train[-80%:]&#x27;</span>)'},group2:{id:"readinstruction",code:`ri = (datasets.ReadInstruction('train', to=10, unit='%') + datasets.ReadInstruction('train', from_=-80, unit='%'))
train_10_80pct_ds = datasets.load_dataset('bookcorpus', split=ri)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>ri = (datasets.ReadInstruction(<span class="hljs-string">&#x27;train&#x27;</span>, to=<span class="hljs-number">10</span>, unit=<span class="hljs-string">&#x27;%&#x27;</span>) + datasets.ReadInstruction(<span class="hljs-string">&#x27;train&#x27;</span>, from_=-<span class="hljs-number">80</span>, unit=<span class="hljs-string">&#x27;%&#x27;</span>))
<span class="hljs-meta">&gt;&gt;&gt; </span>train_10_80pct_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=ri)`}}}),Jt=new Cl({props:{group1:{id:"stringapi",code:`# 10-fold cross-validation (see also next section on rounding behavior):
# The validation datasets are each going to be 10%:
# [0%:10%], [10%:20%], ..., [90%:100%].
# And the training datasets are each going to be the complementary 90%:
# [10%:100%] (for a corresponding validation set of [0%:10%]),
# [0%:10%] + [20%:100%] (for a validation set of [10%:20%]), ...,
# [0%:90%] (for a validation set of [90%:100%]).
vals_ds = datasets.load_dataset('bookcorpus', split=[f'train[{k}%:{k+10}%]' for k in range(0, 100, 10)])
trains_ds = datasets.load_dataset('bookcorpus', split=[f'train[:{k}%]+train[{k+10}%:]' for k in range(0, 100, 10)])`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># 10-fold cross-validation (see also next section on rounding behavior):</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># The validation datasets are each going to be 10%:</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># [0%:10%], [10%:20%], ..., [90%:100%].</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># And the training datasets are each going to be the complementary 90%:</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># [10%:100%] (for a corresponding validation set of [0%:10%]),</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># [0%:10%] + [20%:100%] (for a validation set of [10%:20%]), ...,</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># [0%:90%] (for a validation set of [90%:100%]).</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>vals_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=[<span class="hljs-string">f&#x27;train[<span class="hljs-subst">{k}</span>%:<span class="hljs-subst">{k+<span class="hljs-number">10</span>}</span>%]&#x27;</span> <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, <span class="hljs-number">100</span>, <span class="hljs-number">10</span>)])
<span class="hljs-meta">&gt;&gt;&gt; </span>trains_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=[<span class="hljs-string">f&#x27;train[:<span class="hljs-subst">{k}</span>%]+train[<span class="hljs-subst">{k+<span class="hljs-number">10</span>}</span>%:]&#x27;</span> <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, <span class="hljs-number">100</span>, <span class="hljs-number">10</span>)])`},group2:{id:"readinstruction",code:`# 10-fold cross-validation (see also next section on rounding behavior):
# The validation datasets are each going to be 10%:
# [0%:10%], [10%:20%], ..., [90%:100%].
# And the training datasets are each going to be the complementary 90%:
# [10%:100%] (for a corresponding validation set of [0%:10%]),
# [0%:10%] + [20%:100%] (for a validation set of [10%:20%]), ...,
# [0%:90%] (for a validation set of [90%:100%]).
vals_ds = datasets.load_dataset('bookcorpus', [datasets.ReadInstruction('train', from_=k, to=k+10, unit='%') for k in range(0, 100, 10)])
trains_ds = datasets.load_dataset('bookcorpus', [(datasets.ReadInstruction('train', to=k, unit='%') + datasets.ReadInstruction('train', from_=k+10, unit='%')) for k in range(0, 100, 10)])`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># 10-fold cross-validation (see also next section on rounding behavior):</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># The validation datasets are each going to be 10%:</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># [0%:10%], [10%:20%], ..., [90%:100%].</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># And the training datasets are each going to be the complementary 90%:</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># [10%:100%] (for a corresponding validation set of [0%:10%]),</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># [0%:10%] + [20%:100%] (for a validation set of [10%:20%]), ...,</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># [0%:90%] (for a validation set of [90%:100%]).</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>vals_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, [datasets.ReadInstruction(<span class="hljs-string">&#x27;train&#x27;</span>, from_=k, to=k+<span class="hljs-number">10</span>, unit=<span class="hljs-string">&#x27;%&#x27;</span>) <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, <span class="hljs-number">100</span>, <span class="hljs-number">10</span>)])
<span class="hljs-meta">&gt;&gt;&gt; </span>trains_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, [(datasets.ReadInstruction(<span class="hljs-string">&#x27;train&#x27;</span>, to=k, unit=<span class="hljs-string">&#x27;%&#x27;</span>) + datasets.ReadInstruction(<span class="hljs-string">&#x27;train&#x27;</span>, from_=k+<span class="hljs-number">10</span>, unit=<span class="hljs-string">&#x27;%&#x27;</span>)) <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, <span class="hljs-number">100</span>, <span class="hljs-number">10</span>)])`}}}),Ut=new P({}),Bt=new E({props:{code:`# Assuming *train* split contains 999 records.
# 19 records, from 500 (included) to 519 (excluded).
>>> train_50_52_ds = datasets.load_dataset('bookcorpus', split='train[50%:52%]')
# 20 records, from 519 (included) to 539 (excluded).
>>> train_52_54_ds = datasets.load_dataset('bookcorpus', split='train[52%:54%]')`,highlighted:`<span class="hljs-comment"># Assuming *train* split contains 999 records.</span>
<span class="hljs-comment"># 19 records, from 500 (included) to 519 (excluded).</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>train_50_52_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=<span class="hljs-string">&#x27;train[50%:52%]&#x27;</span>)
<span class="hljs-comment"># 20 records, from 519 (included) to 539 (excluded).</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>train_52_54_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=<span class="hljs-string">&#x27;train[52%:54%]&#x27;</span>)`}}),Yt=new E({props:{code:`# 18 records, from 450 (included) to 468 (excluded).
>>> train_50_52pct1_ds = datasets.load_dataset('bookcorpus', split=datasets.ReadInstruction( 'train', from_=50, to=52, unit='%', rounding='pct1_dropremainder'))
# 18 records, from 468 (included) to 486 (excluded).
>>> train_52_54pct1_ds = datasets.load_dataset('bookcorpus', split=datasets.ReadInstruction('train',from_=52, to=54, unit='%', rounding='pct1_dropremainder'))
# Or equivalently:
>>> train_50_52pct1_ds = datasets.load_dataset('bookcorpus', split='train[50%:52%](pct1_dropremainder)')
>>> train_52_54pct1_ds = datasets.load_dataset('bookcorpus', split='train[52%:54%](pct1_dropremainder)')`,highlighted:`<span class="hljs-comment"># 18 records, from 450 (included) to 468 (excluded).</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>train_50_52pct1_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=datasets.ReadInstruction( <span class="hljs-string">&#x27;train&#x27;</span>, from_=<span class="hljs-number">50</span>, to=<span class="hljs-number">52</span>, unit=<span class="hljs-string">&#x27;%&#x27;</span>, rounding=<span class="hljs-string">&#x27;pct1_dropremainder&#x27;</span>))
<span class="hljs-comment"># 18 records, from 468 (included) to 486 (excluded).</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>train_52_54pct1_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=datasets.ReadInstruction(<span class="hljs-string">&#x27;train&#x27;</span>,from_=<span class="hljs-number">52</span>, to=<span class="hljs-number">54</span>, unit=<span class="hljs-string">&#x27;%&#x27;</span>, rounding=<span class="hljs-string">&#x27;pct1_dropremainder&#x27;</span>))
<span class="hljs-comment"># Or equivalently:</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>train_50_52pct1_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=<span class="hljs-string">&#x27;train[50%:52%](pct1_dropremainder)&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>train_52_54pct1_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=<span class="hljs-string">&#x27;train[52%:54%](pct1_dropremainder)&#x27;</span>)`}}),Gs=new ha({props:{warning:"&lcub;true}",$$slots:{default:[$h]},$$scope:{ctx:T}}}),Wt=new P({}),Gt=new P({}),Xt=new E({props:{code:'dataset = load_dataset("matinf", "summarization")',highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;matinf&quot;</span>, <span class="hljs-string">&quot;summarization&quot;</span>)
Downloading <span class="hljs-keyword">and</span> preparing dataset matinf/summarization (download: Unknown size, generated: <span class="hljs-number">246.89</span> MiB, post-processed: Unknown size, total: <span class="hljs-number">246.89</span> MiB) to /root/.cache/huggingface/datasets/matinf/summarization/<span class="hljs-number">1.0</span><span class="hljs-number">.0</span>/82eee5e71c3ceaf20d909bca36ff237452b4e4ab195d3be7ee1c78b53e6f540e...
AssertionError: The dataset matinf <span class="hljs-keyword">with</span> config summarization requires manual data. 
Please follow the manual download instructions: To use MATINF you have to download it manually. Please fill this google form (https://forms.gle/nkH4LVE4iNQeDzsc9). You will receive a download link <span class="hljs-keyword">and</span> a password once you complete the form. Please extract <span class="hljs-built_in">all</span> files <span class="hljs-keyword">in</span> one folder <span class="hljs-keyword">and</span> load the dataset <span class="hljs-keyword">with</span>: *datasets.load_dataset(<span class="hljs-string">&#x27;matinf&#x27;</span>, data_dir=<span class="hljs-string">&#x27;path/to/folder/folder_name&#x27;</span>)*. 
Manual data can be loaded <span class="hljs-keyword">with</span> \`datasets.load_dataset(matinf, data_dir=<span class="hljs-string">&#x27;&lt;path/to/manual/data&gt;&#x27;</span>) `}}),Kt=new P({}),sa=new E({props:{code:`class_names = ["sadness", "joy", "love", "anger", "fear", "surprise"]
emotion_features = Features({'text': Value('string'), 'label': ClassLabel(names=class_names)})`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>class_names = [<span class="hljs-string">&quot;sadness&quot;</span>, <span class="hljs-string">&quot;joy&quot;</span>, <span class="hljs-string">&quot;love&quot;</span>, <span class="hljs-string">&quot;anger&quot;</span>, <span class="hljs-string">&quot;fear&quot;</span>, <span class="hljs-string">&quot;surprise&quot;</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>emotion_features = Features({<span class="hljs-string">&#x27;text&#x27;</span>: Value(<span class="hljs-string">&#x27;string&#x27;</span>), <span class="hljs-string">&#x27;label&#x27;</span>: ClassLabel(names=class_names)})`}}),ta=new E({props:{code:"dataset = load_dataset('csv', data_files=file_dict, delimiter=';', column_names=['text', 'label'], features=emotion_features)",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;csv&#x27;</span>, data_files=file_dict, delimiter=<span class="hljs-string">&#x27;;&#x27;</span>, column_names=[<span class="hljs-string">&#x27;text&#x27;</span>, <span class="hljs-string">&#x27;label&#x27;</span>], features=emotion_features)'}}),aa=new E({props:{code:"dataset['train'].features",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>dataset[<span class="hljs-string">&#x27;train&#x27;</span>].features
{<span class="hljs-string">&#x27;text&#x27;</span>: Value(dtype=<span class="hljs-string">&#x27;string&#x27;</span>, <span class="hljs-built_in">id</span>=<span class="hljs-literal">None</span>),
<span class="hljs-string">&#x27;label&#x27;</span>: ClassLabel(num_classes=<span class="hljs-number">6</span>, names=[<span class="hljs-string">&#x27;sadness&#x27;</span>, <span class="hljs-string">&#x27;joy&#x27;</span>, <span class="hljs-string">&#x27;love&#x27;</span>, <span class="hljs-string">&#x27;anger&#x27;</span>, <span class="hljs-string">&#x27;fear&#x27;</span>, <span class="hljs-string">&#x27;surprise&#x27;</span>], names_file=<span class="hljs-literal">None</span>, <span class="hljs-built_in">id</span>=<span class="hljs-literal">None</span>)}`}}),ea=new P({}),la=new E({props:{code:`from datasets import load_metric
metric = load_metric('PATH/TO/MY/METRIC/SCRIPT')

# Example of typical usage
for batch in dataset:
    inputs, references = batch
    predictions = model(inputs)
    metric.add_batch(predictions=predictions, references=references)
score = metric.compute()`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_metric
<span class="hljs-meta">&gt;&gt;&gt; </span>metric = load_metric(<span class="hljs-string">&#x27;PATH/TO/MY/METRIC/SCRIPT&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Example of typical usage</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">for</span> batch <span class="hljs-keyword">in</span> dataset:
<span class="hljs-meta">... </span>    inputs, references = batch
<span class="hljs-meta">... </span>    predictions = model(inputs)
<span class="hljs-meta">... </span>    metric.add_batch(predictions=predictions, references=references)
<span class="hljs-meta">&gt;&gt;&gt; </span>score = metric.compute()`}}),tt=new ha({props:{$$slots:{default:[yh]},$$scope:{ctx:T}}}),oa=new P({}),na=new E({props:{code:`from datasets import load_metric
metric = load_metric('bleurt', name='bleurt-base-128')
metric = load_metric('bleurt', name='bleurt-base-512')`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python"><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_metric</span>
<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python">metric = load_metric(<span class="hljs-string">&#x27;bleurt&#x27;</span>, name=<span class="hljs-string">&#x27;bleurt-base-128&#x27;</span>)</span>
<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python">metric = load_metric(<span class="hljs-string">&#x27;bleurt&#x27;</span>, name=<span class="hljs-string">&#x27;bleurt-base-512&#x27;</span>)</span>`}}),ra=new P({}),da=new E({props:{code:`from datasets import load_metric
metric = load_metric('glue', 'mrpc', num_process=num_process, process_id=rank)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_metric
<span class="hljs-meta">&gt;&gt;&gt; </span>metric = load_metric(<span class="hljs-string">&#x27;glue&#x27;</span>, <span class="hljs-string">&#x27;mrpc&#x27;</span>, num_process=num_process, process_id=rank)`}}),lt=new ha({props:{$$slots:{default:[wh]},$$scope:{ctx:T}}}),fa=new E({props:{code:`from datasets import load_metric
metric = load_metric('glue', 'mrpc', num_process=num_process, process_id=process_id, experiment_id="My_experiment_10")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_metric
<span class="hljs-meta">&gt;&gt;&gt; </span>metric = load_metric(<span class="hljs-string">&#x27;glue&#x27;</span>, <span class="hljs-string">&#x27;mrpc&#x27;</span>, num_process=num_process, process_id=process_id, experiment_id=<span class="hljs-string">&quot;My_experiment_10&quot;</span>)`}}),{c(){h=l("meta"),x=d(),y=l("h1"),b=l("a"),k=l("span"),u(w.$$.fragment),j=d(),q=l("span"),X=r("Load"),vs=d(),L=l("p"),K=r("You have already seen how to load a dataset from the Hugging Face Hub. But datasets are stored in a variety of places, and sometimes you won\u2019t find the one you want on the Hub. A dataset can be on disk on your local machine, in a Github repository, and in in-memory data structures like Python dictionaries and Pandas DataFrames. Wherever your dataset may be stored, \u{1F917} Datasets provides a way for you to load and use it for training."),$s=d(),R=l("p"),M=r("This guide will show you how to load a dataset from:"),ys=d(),A=l("ul"),F=l("li"),S=r("The Hub without a dataset loading script"),ua=d(),ws=l("li"),ma=r("Local files"),ga=d(),js=l("li"),vr=r("In-memory data"),$r=d(),ue=l("li"),yr=r("Offline"),wr=d(),me=l("li"),jr=r("A specific slice of a split"),Nl=d(),_a=l("p"),br=r("You will also learn how to troubleshoot common errors, and how to load specific configurations of a metric."),Il=d(),va=l("a"),Ol=d(),Z=l("h2"),bs=l("a"),ge=l("span"),u(rt.$$.fragment),xr=d(),_e=l("span"),kr=r("Hugging Face Hub"),Hl=d(),xs=l("p"),Er=r("In the tutorial, you learned how to load a dataset from the Hub. This method relies on a dataset loading script that downloads and builds the dataset. However, you can also load a dataset from any dataset repository on the Hub "),ve=l("strong"),qr=r("without"),Pr=r(" a loading script!"),Fl=d(),V=l("p"),Ar=r("First, create a dataset repository and upload your data files. Then you can use "),$a=l("a"),Sr=r("datasets.load_dataset()"),Tr=r(" like you learned in the tutorial. For example, load the files from this "),it=l("a"),Dr=r("demo repository"),Cr=r(" by providing the repository namespace and dataset name:"),Ll=d(),u(pt.$$.fragment),Rl=d(),ya=l("p"),Nr=r("This dataset repository contains CSV files, and this code loads all the data from the CSV files."),Ml=d(),ks=l("p"),Ir=r("Some datasets may have more than one version, based on Git tags, branches or commits. Use the "),$e=l("code"),Or=r("revision"),Hr=r(" flag to specify which dataset version you want to load:"),Vl=d(),u(dt.$$.fragment),zl=d(),u(Es.$$.fragment),Jl=d(),D=l("p"),Fr=r("If the dataset doesn\u2019t have a dataset loading script, then by default, all the data will be loaded in the "),ye=l("code"),Lr=r("train"),Rr=r(" split. Use the "),we=l("code"),Mr=r("data_files"),Vr=r(" parameter to map data files to splits like "),je=l("code"),zr=r("train"),Jr=r(", "),be=l("code"),Ur=r("validation"),Br=r(" and "),xe=l("code"),Yr=r("test"),Wr=r(":"),Ul=d(),u(ft.$$.fragment),Bl=d(),u(qs.$$.fragment),Yl=d(),z=l("p"),Gr=r("You can also load a specific subset of the files with the "),ke=l("code"),Qr=r("data_files"),Xr=r(" parameter. The example below loads files from the "),ct=l("a"),Kr=r("C4 dataset"),Zr=r(":"),Wl=d(),u(ht.$$.fragment),Gl=d(),Ps=l("p"),si=r("Specify a custom split with the "),Ee=l("code"),ti=r("split"),ai=r(" parameter:"),Ql=d(),u(ut.$$.fragment),Xl=d(),ss=l("h2"),As=l("a"),qe=l("span"),u(mt.$$.fragment),ei=d(),Pe=l("span"),li=r("Local and remote files"),Kl=d(),C=l("p"),oi=r("\u{1F917} Datasets can be loaded from local files stored on your computer, and also from remote files. The datasets are most likely stored as a "),Ae=l("code"),ni=r("csv"),ri=r(", "),Se=l("code"),ii=r("json"),pi=r(", "),Te=l("code"),di=r("txt"),fi=r(" or "),De=l("code"),ci=r("parquet"),hi=r(" file. The "),wa=l("a"),ui=r("datasets.load_dataset()"),mi=r(" method is able to load each of these file types."),Zl=d(),u(Ss.$$.fragment),so=d(),ts=l("h3"),Ts=l("a"),Ce=l("span"),u(gt.$$.fragment),gi=d(),Ne=l("span"),_i=r("CSV"),to=d(),ja=l("p"),vi=r("\u{1F917} Datasets can read a dataset made up of one or several CSV files:"),ao=d(),u(_t.$$.fragment),eo=d(),ba=l("p"),$i=r("If you have more than one CSV file:"),lo=d(),u(vt.$$.fragment),oo=d(),xa=l("p"),yi=r("You can also map the training and test splits to specific CSV files:"),no=d(),u($t.$$.fragment),ro=d(),ka=l("p"),wi=r("To load remote CSV files via HTTP, you can pass the URLs:"),io=d(),u(yt.$$.fragment),po=d(),Ea=l("p"),ji=r("To load zipped CSV files:"),fo=d(),u(wt.$$.fragment),co=d(),as=l("h3"),Ds=l("a"),Ie=l("span"),u(jt.$$.fragment),bi=d(),Oe=l("span"),xi=r("JSON"),ho=d(),Cs=l("p"),ki=r("JSON files are loaded directly with "),qa=l("a"),Ei=r("datasets.load_dataset()"),qi=r(" as shown below:"),uo=d(),u(bt.$$.fragment),mo=d(),Pa=l("p"),Pi=r("JSON files can have diverse formats, but we think the most efficient format is to have multiple JSON objects; each line represents an individual row of data. For example:"),go=d(),u(xt.$$.fragment),_o=d(),Ns=l("p"),Ai=r("Another JSON format you may encounter is a nested field, in which case you will need to specify the "),He=l("code"),Si=r("field"),Ti=r(" argument as shown in the following:"),vo=d(),u(kt.$$.fragment),$o=d(),Aa=l("p"),Di=r("To load remote JSON files via HTTP, you can pass the URLs:"),yo=d(),u(Et.$$.fragment),wo=d(),Sa=l("p"),Ci=r("While these are the most common JSON formats, you will see other datasets that are formatted differently. \u{1F917} Datasets recognizes these other formats, and will fallback accordingly on the Python JSON loading methods to handle them."),jo=d(),es=l("h3"),Is=l("a"),Fe=l("span"),u(qt.$$.fragment),Ni=d(),Le=l("span"),Ii=r("Text files"),bo=d(),Ta=l("p"),Oi=r("Text files are one of the most common file types for storing a dataset. \u{1F917} Datasets will read the text file line by line to build the dataset."),xo=d(),u(Pt.$$.fragment),ko=d(),Da=l("p"),Hi=r("To load remote TXT files via HTTP, you can pass the URLs:"),Eo=d(),u(At.$$.fragment),qo=d(),ls=l("h3"),Os=l("a"),Re=l("span"),u(St.$$.fragment),Fi=d(),Me=l("span"),Li=r("Parquet"),Po=d(),Ca=l("p"),Ri=r("Parquet files are stored in a columnar format unlike row-based files like CSV. Large datasets may be stored in a Parquet file because it is more efficient, and faster at returning your query. Load a Parquet file as shown in the following example:"),Ao=d(),u(Tt.$$.fragment),So=d(),Na=l("p"),Mi=r("To load remote parquet files via HTTP, you can pass the URLs:"),To=d(),u(Dt.$$.fragment),Do=d(),os=l("h2"),Hs=l("a"),Ve=l("span"),u(Ct.$$.fragment),Vi=d(),ze=l("span"),zi=r("In-memory data"),Co=d(),Fs=l("p"),Ji=r("\u{1F917} Datasets will also allow you to create a "),Ia=l("a"),Ui=r("datasets.Dataset"),Bi=r(" directly from in-memory data structures like Python dictionaries and Pandas DataFrames."),No=d(),ns=l("h3"),Ls=l("a"),Je=l("span"),u(Nt.$$.fragment),Yi=d(),Ue=l("span"),Wi=r("Python dictionary"),Io=d(),Rs=l("p"),Gi=r("Load Python dictionaries with "),Oa=l("a"),Qi=r("datasets.Dataset.from_dict()"),Xi=r(":"),Oo=d(),u(It.$$.fragment),Ho=d(),rs=l("h3"),Ms=l("a"),Be=l("span"),u(Ot.$$.fragment),Ki=d(),Ye=l("span"),Zi=r("Pandas DataFrame"),Fo=d(),Vs=l("p"),sp=r("Load Pandas DataFrames with "),Ha=l("a"),tp=r("datasets.Dataset.from_pandas()"),ap=r(":"),Lo=d(),u(Ht.$$.fragment),Ro=d(),u(zs.$$.fragment),Mo=d(),is=l("h2"),Js=l("a"),We=l("span"),u(Ft.$$.fragment),ep=d(),Ge=l("span"),lp=r("Offline"),Vo=d(),Fa=l("p"),op=r("Even if you don\u2019t have an internet connection, it is still possible to load a dataset. As long as you\u2019ve downloaded a dataset from the Hub or \u{1F917} Datasets GitHub repository before, it should be cached. This means you can reload the dataset from the cache and use it offline."),zo=d(),J=l("p"),np=r("If you know you won\u2019t have internet access, you can run \u{1F917} Datasets in full offline mode. This saves time because instead of waiting for the Dataset builder download to time out, \u{1F917} Datasets will look directly in the cache. Set the environment variable "),Qe=l("code"),rp=r("HF_DATASETS_OFFLINE"),ip=r(" to "),Xe=l("code"),pp=r("1"),dp=r(" to enable full offline mode."),Jo=d(),ps=l("h2"),Us=l("a"),Ke=l("span"),u(Lt.$$.fragment),fp=d(),Ze=l("span"),cp=r("Slice splits"),Uo=d(),U=l("p"),hp=r("For even greater control over how to load a split, you can choose to only load specific slices of a split. There are two options for slicing a split: using strings or "),La=l("a"),up=r("datasets.ReadInstruction"),mp=r(". Strings are more compact and readable for simple cases, while "),Ra=l("a"),gp=r("datasets.ReadInstruction"),_p=r(" is easier to use with variable slicing parameters."),Bo=d(),B=l("p"),vp=r("Concatenate the "),sl=l("code"),$p=r("train"),yp=r(" and "),tl=l("code"),wp=r("test"),jp=r(" split by:"),Yo=d(),u(Rt.$$.fragment),Wo=d(),Bs=l("p"),bp=r("Select specific rows of the "),al=l("code"),xp=r("train"),kp=r(" split:"),Go=d(),u(Mt.$$.fragment),Qo=d(),Ma=l("p"),Ep=r("Or select a percentage of the split with:"),Xo=d(),u(Vt.$$.fragment),Ko=d(),Va=l("p"),qp=r("You can even select a combination of percentages from each split:"),Zo=d(),u(zt.$$.fragment),sn=d(),za=l("p"),Pp=r("Finally, create cross-validated dataset splits by:"),tn=d(),u(Jt.$$.fragment),an=d(),ds=l("h3"),Ys=l("a"),el=l("span"),u(Ut.$$.fragment),Ap=d(),ll=l("span"),Sp=r("Percent slicing and rounding"),en=d(),Ja=l("p"),Tp=r("For datasets where the requested slice boundaries do not divide evenly by 100, the default behavior is to round the boundaries to the nearest integer. As a result, some slices may contain more examples than others as shown in the following example:"),ln=d(),u(Bt.$$.fragment),on=d(),Ws=l("p"),Dp=r("If you want equal sized splits, use "),ol=l("code"),Cp=r("pct1_dropremainder"),Np=r(" rounding instead. This will treat the specified percentage boundaries as multiples of 1%."),nn=d(),u(Yt.$$.fragment),rn=d(),u(Gs.$$.fragment),pn=d(),Ua=l("a"),dn=d(),fs=l("h2"),Qs=l("a"),nl=l("span"),u(Wt.$$.fragment),Ip=d(),rl=l("span"),Op=r("Troubleshooting"),fn=d(),Ba=l("p"),Hp=r("Sometimes, you may get unexpected results when you load a dataset. In this section, you will learn how to solve two common issues you may encounter when you load a dataset: manually download a dataset, and specify features of a dataset."),cn=d(),cs=l("h3"),Xs=l("a"),il=l("span"),u(Gt.$$.fragment),Fp=d(),pl=l("span"),Lp=r("Manual download"),hn=d(),I=l("p"),Rp=r("Certain datasets require you to manually download the dataset files due to licensing incompatibility, or if the files are hidden behind a login page. This will cause "),Ya=l("a"),Mp=r("datasets.load_dataset()"),Vp=r(" to throw an "),dl=l("code"),zp=r("AssertionError"),Jp=r(". But \u{1F917} Datasets provides detailed instructions for downloading the missing files. After you have downloaded the files, use the "),fl=l("code"),Up=r("data_dir"),Bp=r(" argument to specify the path to the files you just downloaded."),un=d(),Ks=l("p"),Yp=r("For example, if you try to download a configuration from the "),Qt=l("a"),Wp=r("MATINF"),Gp=r(" dataset:"),mn=d(),u(Xt.$$.fragment),gn=d(),hs=l("h3"),Zs=l("a"),cl=l("span"),u(Kt.$$.fragment),Qp=d(),hl=l("span"),Xp=r("Specify features"),_n=d(),Y=l("p"),Kp=r("When you create a dataset from local files, the "),Wa=l("a"),Zp=r("datasets.Features"),sd=r(" are automatically inferred by "),Zt=l("a"),td=r("Apache Arrow"),ad=r(". However, the features of the dataset may not always align with your expectations or you may want to define the features yourself."),vn=d(),W=l("p"),ed=r("The following example shows how you can add custom labels with "),Ga=l("a"),ld=r("datasets.ClassLabel"),od=r(". First, define your own labels using the "),Qa=l("a"),nd=r("datasets.Features"),rd=r(" class:"),$n=d(),u(sa.$$.fragment),yn=d(),G=l("p"),id=r("Next, specify the "),ul=l("code"),pd=r("features"),dd=r(" argument in "),Xa=l("a"),fd=r("datasets.load_dataset()"),cd=r(" with the features you just created:"),wn=d(),u(ta.$$.fragment),jn=d(),Ka=l("p"),hd=r("Now when you look at your dataset features, you can see it uses the custom labels you defined:"),bn=d(),u(aa.$$.fragment),xn=d(),us=l("h2"),st=l("a"),ml=l("span"),u(ea.$$.fragment),ud=d(),gl=l("span"),md=r("Metrics"),kn=d(),Za=l("p"),gd=r("When the metric you want to use is not supported by \u{1F917} Datasets, you can write and use your own metric script. Load your metric by providing the path to your local metric loading script:"),En=d(),u(la.$$.fragment),qn=d(),u(tt.$$.fragment),Pn=d(),ms=l("h3"),at=l("a"),_l=l("span"),u(oa.$$.fragment),_d=d(),vl=l("span"),vd=r("Load configurations"),An=d(),se=l("p"),$d=r("It is possible for a metric to have different configurations. The configurations are stored in the [\u2018datasets.Metric.config_name\u2019] attribute. When you load a metric, provide the configuration name as shown in the following:"),Sn=d(),u(na.$$.fragment),Tn=d(),gs=l("h3"),et=l("a"),$l=l("span"),u(ra.$$.fragment),yd=d(),yl=l("span"),wd=r("Distributed setup"),Dn=d(),te=l("p"),jd=r("When you work in a distributed or parallel processing environment, loading and computing a metric can be tricky because these processes are executed in parallel on separate subsets of the data. \u{1F917} Datasets supports distributed usage with a few additional arguments when you load a metric."),Cn=d(),ae=l("p"),bd=r("For example, imagine you are training and evaluating on eight parallel processes. Here\u2019s how you would load a metric in this distributed setting:"),Nn=d(),Q=l("ol"),wl=l("li"),ia=l("p"),xd=r("Define the total number of processes with the "),jl=l("code"),kd=r("num_process"),Ed=r(" argument."),qd=d(),bl=l("li"),_s=l("p"),Pd=r("Set the process "),xl=l("code"),Ad=r("rank"),Sd=r(" as an integer between zero and "),kl=l("code"),Td=r("num_process - 1"),Dd=r("."),Cd=d(),El=l("li"),pa=l("p"),Nd=r("Load your metric with "),ee=l("a"),Id=r("datasets.load_metric()"),Od=r(" with these arguments:"),In=d(),u(da.$$.fragment),On=d(),u(lt.$$.fragment),Hn=d(),ot=l("p"),Hd=r("In some instances, you may be simultaneously running multiple independent distributed evaluations on the same server and files. To avoid any conflicts, it is important to provide an "),ql=l("code"),Fd=r("experiment_id"),Ld=r(" to distinguish the separate evaluations:"),Fn=d(),u(fa.$$.fragment),this.h()},l(s){const e=uh('[data-svelte="svelte-1phssyn"]',document.head);h=o(e,"META",{name:!0,content:!0}),e.forEach(t),x=f(s),y=o(s,"H1",{class:!0});var ca=n(y);b=o(ca,"A",{id:!0,class:!0,href:!0});var Pl=n(b);k=o(Pl,"SPAN",{});var Al=n(k);m(w.$$.fragment,Al),Al.forEach(t),Pl.forEach(t),j=f(ca),q=o(ca,"SPAN",{});var Sl=n(q);X=i(Sl,"Load"),Sl.forEach(t),ca.forEach(t),vs=f(s),L=o(s,"P",{});var Tl=n(L);K=i(Tl,"You have already seen how to load a dataset from the Hugging Face Hub. But datasets are stored in a variety of places, and sometimes you won\u2019t find the one you want on the Hub. A dataset can be on disk on your local machine, in a Github repository, and in in-memory data structures like Python dictionaries and Pandas DataFrames. Wherever your dataset may be stored, \u{1F917} Datasets provides a way for you to load and use it for training."),Tl.forEach(t),$s=f(s),R=o(s,"P",{});var Dl=n(R);M=i(Dl,"This guide will show you how to load a dataset from:"),Dl.forEach(t),ys=f(s),A=o(s,"UL",{});var N=n(A);F=o(N,"LI",{});var Rd=n(F);S=i(Rd,"The Hub without a dataset loading script"),Rd.forEach(t),ua=f(N),ws=o(N,"LI",{});var Md=n(ws);ma=i(Md,"Local files"),Md.forEach(t),ga=f(N),js=o(N,"LI",{});var Vd=n(js);vr=i(Vd,"In-memory data"),Vd.forEach(t),$r=f(N),ue=o(N,"LI",{});var zd=n(ue);yr=i(zd,"Offline"),zd.forEach(t),wr=f(N),me=o(N,"LI",{});var Jd=n(me);jr=i(Jd,"A specific slice of a split"),Jd.forEach(t),N.forEach(t),Nl=f(s),_a=o(s,"P",{});var Ud=n(_a);br=i(Ud,"You will also learn how to troubleshoot common errors, and how to load specific configurations of a metric."),Ud.forEach(t),Il=f(s),va=o(s,"A",{id:!0}),n(va).forEach(t),Ol=f(s),Z=o(s,"H2",{class:!0});var Rn=n(Z);bs=o(Rn,"A",{id:!0,class:!0,href:!0});var Bd=n(bs);ge=o(Bd,"SPAN",{});var Yd=n(ge);m(rt.$$.fragment,Yd),Yd.forEach(t),Bd.forEach(t),xr=f(Rn),_e=o(Rn,"SPAN",{});var Wd=n(_e);kr=i(Wd,"Hugging Face Hub"),Wd.forEach(t),Rn.forEach(t),Hl=f(s),xs=o(s,"P",{});var Mn=n(xs);Er=i(Mn,"In the tutorial, you learned how to load a dataset from the Hub. This method relies on a dataset loading script that downloads and builds the dataset. However, you can also load a dataset from any dataset repository on the Hub "),ve=o(Mn,"STRONG",{});var Gd=n(ve);qr=i(Gd,"without"),Gd.forEach(t),Pr=i(Mn," a loading script!"),Mn.forEach(t),Fl=f(s),V=o(s,"P",{});var le=n(V);Ar=i(le,"First, create a dataset repository and upload your data files. Then you can use "),$a=o(le,"A",{href:!0});var Qd=n($a);Sr=i(Qd,"datasets.load_dataset()"),Qd.forEach(t),Tr=i(le," like you learned in the tutorial. For example, load the files from this "),it=o(le,"A",{href:!0,rel:!0});var Xd=n(it);Dr=i(Xd,"demo repository"),Xd.forEach(t),Cr=i(le," by providing the repository namespace and dataset name:"),le.forEach(t),Ll=f(s),m(pt.$$.fragment,s),Rl=f(s),ya=o(s,"P",{});var Kd=n(ya);Nr=i(Kd,"This dataset repository contains CSV files, and this code loads all the data from the CSV files."),Kd.forEach(t),Ml=f(s),ks=o(s,"P",{});var Vn=n(ks);Ir=i(Vn,"Some datasets may have more than one version, based on Git tags, branches or commits. Use the "),$e=o(Vn,"CODE",{});var Zd=n($e);Or=i(Zd,"revision"),Zd.forEach(t),Hr=i(Vn," flag to specify which dataset version you want to load:"),Vn.forEach(t),Vl=f(s),m(dt.$$.fragment,s),zl=f(s),m(Es.$$.fragment,s),Jl=f(s),D=o(s,"P",{});var O=n(D);Fr=i(O,"If the dataset doesn\u2019t have a dataset loading script, then by default, all the data will be loaded in the "),ye=o(O,"CODE",{});var sf=n(ye);Lr=i(sf,"train"),sf.forEach(t),Rr=i(O," split. Use the "),we=o(O,"CODE",{});var tf=n(we);Mr=i(tf,"data_files"),tf.forEach(t),Vr=i(O," parameter to map data files to splits like "),je=o(O,"CODE",{});var af=n(je);zr=i(af,"train"),af.forEach(t),Jr=i(O,", "),be=o(O,"CODE",{});var ef=n(be);Ur=i(ef,"validation"),ef.forEach(t),Br=i(O," and "),xe=o(O,"CODE",{});var lf=n(xe);Yr=i(lf,"test"),lf.forEach(t),Wr=i(O,":"),O.forEach(t),Ul=f(s),m(ft.$$.fragment,s),Bl=f(s),m(qs.$$.fragment,s),Yl=f(s),z=o(s,"P",{});var oe=n(z);Gr=i(oe,"You can also load a specific subset of the files with the "),ke=o(oe,"CODE",{});var of=n(ke);Qr=i(of,"data_files"),of.forEach(t),Xr=i(oe," parameter. The example below loads files from the "),ct=o(oe,"A",{href:!0,rel:!0});var nf=n(ct);Kr=i(nf,"C4 dataset"),nf.forEach(t),Zr=i(oe,":"),oe.forEach(t),Wl=f(s),m(ht.$$.fragment,s),Gl=f(s),Ps=o(s,"P",{});var zn=n(Ps);si=i(zn,"Specify a custom split with the "),Ee=o(zn,"CODE",{});var rf=n(Ee);ti=i(rf,"split"),rf.forEach(t),ai=i(zn," parameter:"),zn.forEach(t),Ql=f(s),m(ut.$$.fragment,s),Xl=f(s),ss=o(s,"H2",{class:!0});var Jn=n(ss);As=o(Jn,"A",{id:!0,class:!0,href:!0});var pf=n(As);qe=o(pf,"SPAN",{});var df=n(qe);m(mt.$$.fragment,df),df.forEach(t),pf.forEach(t),ei=f(Jn),Pe=o(Jn,"SPAN",{});var ff=n(Pe);li=i(ff,"Local and remote files"),ff.forEach(t),Jn.forEach(t),Kl=f(s),C=o(s,"P",{});var H=n(C);oi=i(H,"\u{1F917} Datasets can be loaded from local files stored on your computer, and also from remote files. The datasets are most likely stored as a "),Ae=o(H,"CODE",{});var cf=n(Ae);ni=i(cf,"csv"),cf.forEach(t),ri=i(H,", "),Se=o(H,"CODE",{});var hf=n(Se);ii=i(hf,"json"),hf.forEach(t),pi=i(H,", "),Te=o(H,"CODE",{});var uf=n(Te);di=i(uf,"txt"),uf.forEach(t),fi=i(H," or "),De=o(H,"CODE",{});var mf=n(De);ci=i(mf,"parquet"),mf.forEach(t),hi=i(H," file. The "),wa=o(H,"A",{href:!0});var gf=n(wa);ui=i(gf,"datasets.load_dataset()"),gf.forEach(t),mi=i(H," method is able to load each of these file types."),H.forEach(t),Zl=f(s),m(Ss.$$.fragment,s),so=f(s),ts=o(s,"H3",{class:!0});var Un=n(ts);Ts=o(Un,"A",{id:!0,class:!0,href:!0});var _f=n(Ts);Ce=o(_f,"SPAN",{});var vf=n(Ce);m(gt.$$.fragment,vf),vf.forEach(t),_f.forEach(t),gi=f(Un),Ne=o(Un,"SPAN",{});var $f=n(Ne);_i=i($f,"CSV"),$f.forEach(t),Un.forEach(t),to=f(s),ja=o(s,"P",{});var yf=n(ja);vi=i(yf,"\u{1F917} Datasets can read a dataset made up of one or several CSV files:"),yf.forEach(t),ao=f(s),m(_t.$$.fragment,s),eo=f(s),ba=o(s,"P",{});var wf=n(ba);$i=i(wf,"If you have more than one CSV file:"),wf.forEach(t),lo=f(s),m(vt.$$.fragment,s),oo=f(s),xa=o(s,"P",{});var jf=n(xa);yi=i(jf,"You can also map the training and test splits to specific CSV files:"),jf.forEach(t),no=f(s),m($t.$$.fragment,s),ro=f(s),ka=o(s,"P",{});var bf=n(ka);wi=i(bf,"To load remote CSV files via HTTP, you can pass the URLs:"),bf.forEach(t),io=f(s),m(yt.$$.fragment,s),po=f(s),Ea=o(s,"P",{});var xf=n(Ea);ji=i(xf,"To load zipped CSV files:"),xf.forEach(t),fo=f(s),m(wt.$$.fragment,s),co=f(s),as=o(s,"H3",{class:!0});var Bn=n(as);Ds=o(Bn,"A",{id:!0,class:!0,href:!0});var kf=n(Ds);Ie=o(kf,"SPAN",{});var Ef=n(Ie);m(jt.$$.fragment,Ef),Ef.forEach(t),kf.forEach(t),bi=f(Bn),Oe=o(Bn,"SPAN",{});var qf=n(Oe);xi=i(qf,"JSON"),qf.forEach(t),Bn.forEach(t),ho=f(s),Cs=o(s,"P",{});var Yn=n(Cs);ki=i(Yn,"JSON files are loaded directly with "),qa=o(Yn,"A",{href:!0});var Pf=n(qa);Ei=i(Pf,"datasets.load_dataset()"),Pf.forEach(t),qi=i(Yn," as shown below:"),Yn.forEach(t),uo=f(s),m(bt.$$.fragment,s),mo=f(s),Pa=o(s,"P",{});var Af=n(Pa);Pi=i(Af,"JSON files can have diverse formats, but we think the most efficient format is to have multiple JSON objects; each line represents an individual row of data. For example:"),Af.forEach(t),go=f(s),m(xt.$$.fragment,s),_o=f(s),Ns=o(s,"P",{});var Wn=n(Ns);Ai=i(Wn,"Another JSON format you may encounter is a nested field, in which case you will need to specify the "),He=o(Wn,"CODE",{});var Sf=n(He);Si=i(Sf,"field"),Sf.forEach(t),Ti=i(Wn," argument as shown in the following:"),Wn.forEach(t),vo=f(s),m(kt.$$.fragment,s),$o=f(s),Aa=o(s,"P",{});var Tf=n(Aa);Di=i(Tf,"To load remote JSON files via HTTP, you can pass the URLs:"),Tf.forEach(t),yo=f(s),m(Et.$$.fragment,s),wo=f(s),Sa=o(s,"P",{});var Df=n(Sa);Ci=i(Df,"While these are the most common JSON formats, you will see other datasets that are formatted differently. \u{1F917} Datasets recognizes these other formats, and will fallback accordingly on the Python JSON loading methods to handle them."),Df.forEach(t),jo=f(s),es=o(s,"H3",{class:!0});var Gn=n(es);Is=o(Gn,"A",{id:!0,class:!0,href:!0});var Cf=n(Is);Fe=o(Cf,"SPAN",{});var Nf=n(Fe);m(qt.$$.fragment,Nf),Nf.forEach(t),Cf.forEach(t),Ni=f(Gn),Le=o(Gn,"SPAN",{});var If=n(Le);Ii=i(If,"Text files"),If.forEach(t),Gn.forEach(t),bo=f(s),Ta=o(s,"P",{});var Of=n(Ta);Oi=i(Of,"Text files are one of the most common file types for storing a dataset. \u{1F917} Datasets will read the text file line by line to build the dataset."),Of.forEach(t),xo=f(s),m(Pt.$$.fragment,s),ko=f(s),Da=o(s,"P",{});var Hf=n(Da);Hi=i(Hf,"To load remote TXT files via HTTP, you can pass the URLs:"),Hf.forEach(t),Eo=f(s),m(At.$$.fragment,s),qo=f(s),ls=o(s,"H3",{class:!0});var Qn=n(ls);Os=o(Qn,"A",{id:!0,class:!0,href:!0});var Ff=n(Os);Re=o(Ff,"SPAN",{});var Lf=n(Re);m(St.$$.fragment,Lf),Lf.forEach(t),Ff.forEach(t),Fi=f(Qn),Me=o(Qn,"SPAN",{});var Rf=n(Me);Li=i(Rf,"Parquet"),Rf.forEach(t),Qn.forEach(t),Po=f(s),Ca=o(s,"P",{});var Mf=n(Ca);Ri=i(Mf,"Parquet files are stored in a columnar format unlike row-based files like CSV. Large datasets may be stored in a Parquet file because it is more efficient, and faster at returning your query. Load a Parquet file as shown in the following example:"),Mf.forEach(t),Ao=f(s),m(Tt.$$.fragment,s),So=f(s),Na=o(s,"P",{});var Vf=n(Na);Mi=i(Vf,"To load remote parquet files via HTTP, you can pass the URLs:"),Vf.forEach(t),To=f(s),m(Dt.$$.fragment,s),Do=f(s),os=o(s,"H2",{class:!0});var Xn=n(os);Hs=o(Xn,"A",{id:!0,class:!0,href:!0});var zf=n(Hs);Ve=o(zf,"SPAN",{});var Jf=n(Ve);m(Ct.$$.fragment,Jf),Jf.forEach(t),zf.forEach(t),Vi=f(Xn),ze=o(Xn,"SPAN",{});var Uf=n(ze);zi=i(Uf,"In-memory data"),Uf.forEach(t),Xn.forEach(t),Co=f(s),Fs=o(s,"P",{});var Kn=n(Fs);Ji=i(Kn,"\u{1F917} Datasets will also allow you to create a "),Ia=o(Kn,"A",{href:!0});var Bf=n(Ia);Ui=i(Bf,"datasets.Dataset"),Bf.forEach(t),Bi=i(Kn," directly from in-memory data structures like Python dictionaries and Pandas DataFrames."),Kn.forEach(t),No=f(s),ns=o(s,"H3",{class:!0});var Zn=n(ns);Ls=o(Zn,"A",{id:!0,class:!0,href:!0});var Yf=n(Ls);Je=o(Yf,"SPAN",{});var Wf=n(Je);m(Nt.$$.fragment,Wf),Wf.forEach(t),Yf.forEach(t),Yi=f(Zn),Ue=o(Zn,"SPAN",{});var Gf=n(Ue);Wi=i(Gf,"Python dictionary"),Gf.forEach(t),Zn.forEach(t),Io=f(s),Rs=o(s,"P",{});var sr=n(Rs);Gi=i(sr,"Load Python dictionaries with "),Oa=o(sr,"A",{href:!0});var Qf=n(Oa);Qi=i(Qf,"datasets.Dataset.from_dict()"),Qf.forEach(t),Xi=i(sr,":"),sr.forEach(t),Oo=f(s),m(It.$$.fragment,s),Ho=f(s),rs=o(s,"H3",{class:!0});var tr=n(rs);Ms=o(tr,"A",{id:!0,class:!0,href:!0});var Xf=n(Ms);Be=o(Xf,"SPAN",{});var Kf=n(Be);m(Ot.$$.fragment,Kf),Kf.forEach(t),Xf.forEach(t),Ki=f(tr),Ye=o(tr,"SPAN",{});var Zf=n(Ye);Zi=i(Zf,"Pandas DataFrame"),Zf.forEach(t),tr.forEach(t),Fo=f(s),Vs=o(s,"P",{});var ar=n(Vs);sp=i(ar,"Load Pandas DataFrames with "),Ha=o(ar,"A",{href:!0});var sc=n(Ha);tp=i(sc,"datasets.Dataset.from_pandas()"),sc.forEach(t),ap=i(ar,":"),ar.forEach(t),Lo=f(s),m(Ht.$$.fragment,s),Ro=f(s),m(zs.$$.fragment,s),Mo=f(s),is=o(s,"H2",{class:!0});var er=n(is);Js=o(er,"A",{id:!0,class:!0,href:!0});var tc=n(Js);We=o(tc,"SPAN",{});var ac=n(We);m(Ft.$$.fragment,ac),ac.forEach(t),tc.forEach(t),ep=f(er),Ge=o(er,"SPAN",{});var ec=n(Ge);lp=i(ec,"Offline"),ec.forEach(t),er.forEach(t),Vo=f(s),Fa=o(s,"P",{});var lc=n(Fa);op=i(lc,"Even if you don\u2019t have an internet connection, it is still possible to load a dataset. As long as you\u2019ve downloaded a dataset from the Hub or \u{1F917} Datasets GitHub repository before, it should be cached. This means you can reload the dataset from the cache and use it offline."),lc.forEach(t),zo=f(s),J=o(s,"P",{});var ne=n(J);np=i(ne,"If you know you won\u2019t have internet access, you can run \u{1F917} Datasets in full offline mode. This saves time because instead of waiting for the Dataset builder download to time out, \u{1F917} Datasets will look directly in the cache. Set the environment variable "),Qe=o(ne,"CODE",{});var oc=n(Qe);rp=i(oc,"HF_DATASETS_OFFLINE"),oc.forEach(t),ip=i(ne," to "),Xe=o(ne,"CODE",{});var nc=n(Xe);pp=i(nc,"1"),nc.forEach(t),dp=i(ne," to enable full offline mode."),ne.forEach(t),Jo=f(s),ps=o(s,"H2",{class:!0});var lr=n(ps);Us=o(lr,"A",{id:!0,class:!0,href:!0});var rc=n(Us);Ke=o(rc,"SPAN",{});var ic=n(Ke);m(Lt.$$.fragment,ic),ic.forEach(t),rc.forEach(t),fp=f(lr),Ze=o(lr,"SPAN",{});var pc=n(Ze);cp=i(pc,"Slice splits"),pc.forEach(t),lr.forEach(t),Uo=f(s),U=o(s,"P",{});var re=n(U);hp=i(re,"For even greater control over how to load a split, you can choose to only load specific slices of a split. There are two options for slicing a split: using strings or "),La=o(re,"A",{href:!0});var dc=n(La);up=i(dc,"datasets.ReadInstruction"),dc.forEach(t),mp=i(re,". Strings are more compact and readable for simple cases, while "),Ra=o(re,"A",{href:!0});var fc=n(Ra);gp=i(fc,"datasets.ReadInstruction"),fc.forEach(t),_p=i(re," is easier to use with variable slicing parameters."),re.forEach(t),Bo=f(s),B=o(s,"P",{});var ie=n(B);vp=i(ie,"Concatenate the "),sl=o(ie,"CODE",{});var cc=n(sl);$p=i(cc,"train"),cc.forEach(t),yp=i(ie," and "),tl=o(ie,"CODE",{});var hc=n(tl);wp=i(hc,"test"),hc.forEach(t),jp=i(ie," split by:"),ie.forEach(t),Yo=f(s),m(Rt.$$.fragment,s),Wo=f(s),Bs=o(s,"P",{});var or=n(Bs);bp=i(or,"Select specific rows of the "),al=o(or,"CODE",{});var uc=n(al);xp=i(uc,"train"),uc.forEach(t),kp=i(or," split:"),or.forEach(t),Go=f(s),m(Mt.$$.fragment,s),Qo=f(s),Ma=o(s,"P",{});var mc=n(Ma);Ep=i(mc,"Or select a percentage of the split with:"),mc.forEach(t),Xo=f(s),m(Vt.$$.fragment,s),Ko=f(s),Va=o(s,"P",{});var gc=n(Va);qp=i(gc,"You can even select a combination of percentages from each split:"),gc.forEach(t),Zo=f(s),m(zt.$$.fragment,s),sn=f(s),za=o(s,"P",{});var _c=n(za);Pp=i(_c,"Finally, create cross-validated dataset splits by:"),_c.forEach(t),tn=f(s),m(Jt.$$.fragment,s),an=f(s),ds=o(s,"H3",{class:!0});var nr=n(ds);Ys=o(nr,"A",{id:!0,class:!0,href:!0});var vc=n(Ys);el=o(vc,"SPAN",{});var $c=n(el);m(Ut.$$.fragment,$c),$c.forEach(t),vc.forEach(t),Ap=f(nr),ll=o(nr,"SPAN",{});var yc=n(ll);Sp=i(yc,"Percent slicing and rounding"),yc.forEach(t),nr.forEach(t),en=f(s),Ja=o(s,"P",{});var wc=n(Ja);Tp=i(wc,"For datasets where the requested slice boundaries do not divide evenly by 100, the default behavior is to round the boundaries to the nearest integer. As a result, some slices may contain more examples than others as shown in the following example:"),wc.forEach(t),ln=f(s),m(Bt.$$.fragment,s),on=f(s),Ws=o(s,"P",{});var rr=n(Ws);Dp=i(rr,"If you want equal sized splits, use "),ol=o(rr,"CODE",{});var jc=n(ol);Cp=i(jc,"pct1_dropremainder"),jc.forEach(t),Np=i(rr," rounding instead. This will treat the specified percentage boundaries as multiples of 1%."),rr.forEach(t),nn=f(s),m(Yt.$$.fragment,s),rn=f(s),m(Gs.$$.fragment,s),pn=f(s),Ua=o(s,"A",{id:!0}),n(Ua).forEach(t),dn=f(s),fs=o(s,"H2",{class:!0});var ir=n(fs);Qs=o(ir,"A",{id:!0,class:!0,href:!0});var bc=n(Qs);nl=o(bc,"SPAN",{});var xc=n(nl);m(Wt.$$.fragment,xc),xc.forEach(t),bc.forEach(t),Ip=f(ir),rl=o(ir,"SPAN",{});var kc=n(rl);Op=i(kc,"Troubleshooting"),kc.forEach(t),ir.forEach(t),fn=f(s),Ba=o(s,"P",{});var Ec=n(Ba);Hp=i(Ec,"Sometimes, you may get unexpected results when you load a dataset. In this section, you will learn how to solve two common issues you may encounter when you load a dataset: manually download a dataset, and specify features of a dataset."),Ec.forEach(t),cn=f(s),cs=o(s,"H3",{class:!0});var pr=n(cs);Xs=o(pr,"A",{id:!0,class:!0,href:!0});var qc=n(Xs);il=o(qc,"SPAN",{});var Pc=n(il);m(Gt.$$.fragment,Pc),Pc.forEach(t),qc.forEach(t),Fp=f(pr),pl=o(pr,"SPAN",{});var Ac=n(pl);Lp=i(Ac,"Manual download"),Ac.forEach(t),pr.forEach(t),hn=f(s),I=o(s,"P",{});var nt=n(I);Rp=i(nt,"Certain datasets require you to manually download the dataset files due to licensing incompatibility, or if the files are hidden behind a login page. This will cause "),Ya=o(nt,"A",{href:!0});var Sc=n(Ya);Mp=i(Sc,"datasets.load_dataset()"),Sc.forEach(t),Vp=i(nt," to throw an "),dl=o(nt,"CODE",{});var Tc=n(dl);zp=i(Tc,"AssertionError"),Tc.forEach(t),Jp=i(nt,". But \u{1F917} Datasets provides detailed instructions for downloading the missing files. After you have downloaded the files, use the "),fl=o(nt,"CODE",{});var Dc=n(fl);Up=i(Dc,"data_dir"),Dc.forEach(t),Bp=i(nt," argument to specify the path to the files you just downloaded."),nt.forEach(t),un=f(s),Ks=o(s,"P",{});var dr=n(Ks);Yp=i(dr,"For example, if you try to download a configuration from the "),Qt=o(dr,"A",{href:!0,rel:!0});var Cc=n(Qt);Wp=i(Cc,"MATINF"),Cc.forEach(t),Gp=i(dr," dataset:"),dr.forEach(t),mn=f(s),m(Xt.$$.fragment,s),gn=f(s),hs=o(s,"H3",{class:!0});var fr=n(hs);Zs=o(fr,"A",{id:!0,class:!0,href:!0});var Nc=n(Zs);cl=o(Nc,"SPAN",{});var Ic=n(cl);m(Kt.$$.fragment,Ic),Ic.forEach(t),Nc.forEach(t),Qp=f(fr),hl=o(fr,"SPAN",{});var Oc=n(hl);Xp=i(Oc,"Specify features"),Oc.forEach(t),fr.forEach(t),_n=f(s),Y=o(s,"P",{});var pe=n(Y);Kp=i(pe,"When you create a dataset from local files, the "),Wa=o(pe,"A",{href:!0});var Hc=n(Wa);Zp=i(Hc,"datasets.Features"),Hc.forEach(t),sd=i(pe," are automatically inferred by "),Zt=o(pe,"A",{href:!0,rel:!0});var Fc=n(Zt);td=i(Fc,"Apache Arrow"),Fc.forEach(t),ad=i(pe,". However, the features of the dataset may not always align with your expectations or you may want to define the features yourself."),pe.forEach(t),vn=f(s),W=o(s,"P",{});var de=n(W);ed=i(de,"The following example shows how you can add custom labels with "),Ga=o(de,"A",{href:!0});var Lc=n(Ga);ld=i(Lc,"datasets.ClassLabel"),Lc.forEach(t),od=i(de,". First, define your own labels using the "),Qa=o(de,"A",{href:!0});var Rc=n(Qa);nd=i(Rc,"datasets.Features"),Rc.forEach(t),rd=i(de," class:"),de.forEach(t),$n=f(s),m(sa.$$.fragment,s),yn=f(s),G=o(s,"P",{});var fe=n(G);id=i(fe,"Next, specify the "),ul=o(fe,"CODE",{});var Mc=n(ul);pd=i(Mc,"features"),Mc.forEach(t),dd=i(fe," argument in "),Xa=o(fe,"A",{href:!0});var Vc=n(Xa);fd=i(Vc,"datasets.load_dataset()"),Vc.forEach(t),cd=i(fe," with the features you just created:"),fe.forEach(t),wn=f(s),m(ta.$$.fragment,s),jn=f(s),Ka=o(s,"P",{});var zc=n(Ka);hd=i(zc,"Now when you look at your dataset features, you can see it uses the custom labels you defined:"),zc.forEach(t),bn=f(s),m(aa.$$.fragment,s),xn=f(s),us=o(s,"H2",{class:!0});var cr=n(us);st=o(cr,"A",{id:!0,class:!0,href:!0});var Jc=n(st);ml=o(Jc,"SPAN",{});var Uc=n(ml);m(ea.$$.fragment,Uc),Uc.forEach(t),Jc.forEach(t),ud=f(cr),gl=o(cr,"SPAN",{});var Bc=n(gl);md=i(Bc,"Metrics"),Bc.forEach(t),cr.forEach(t),kn=f(s),Za=o(s,"P",{});var Yc=n(Za);gd=i(Yc,"When the metric you want to use is not supported by \u{1F917} Datasets, you can write and use your own metric script. Load your metric by providing the path to your local metric loading script:"),Yc.forEach(t),En=f(s),m(la.$$.fragment,s),qn=f(s),m(tt.$$.fragment,s),Pn=f(s),ms=o(s,"H3",{class:!0});var hr=n(ms);at=o(hr,"A",{id:!0,class:!0,href:!0});var Wc=n(at);_l=o(Wc,"SPAN",{});var Gc=n(_l);m(oa.$$.fragment,Gc),Gc.forEach(t),Wc.forEach(t),_d=f(hr),vl=o(hr,"SPAN",{});var Qc=n(vl);vd=i(Qc,"Load configurations"),Qc.forEach(t),hr.forEach(t),An=f(s),se=o(s,"P",{});var Xc=n(se);$d=i(Xc,"It is possible for a metric to have different configurations. The configurations are stored in the [\u2018datasets.Metric.config_name\u2019] attribute. When you load a metric, provide the configuration name as shown in the following:"),Xc.forEach(t),Sn=f(s),m(na.$$.fragment,s),Tn=f(s),gs=o(s,"H3",{class:!0});var ur=n(gs);et=o(ur,"A",{id:!0,class:!0,href:!0});var Kc=n(et);$l=o(Kc,"SPAN",{});var Zc=n($l);m(ra.$$.fragment,Zc),Zc.forEach(t),Kc.forEach(t),yd=f(ur),yl=o(ur,"SPAN",{});var sh=n(yl);wd=i(sh,"Distributed setup"),sh.forEach(t),ur.forEach(t),Dn=f(s),te=o(s,"P",{});var th=n(te);jd=i(th,"When you work in a distributed or parallel processing environment, loading and computing a metric can be tricky because these processes are executed in parallel on separate subsets of the data. \u{1F917} Datasets supports distributed usage with a few additional arguments when you load a metric."),th.forEach(t),Cn=f(s),ae=o(s,"P",{});var ah=n(ae);bd=i(ah,"For example, imagine you are training and evaluating on eight parallel processes. Here\u2019s how you would load a metric in this distributed setting:"),ah.forEach(t),Nn=f(s),Q=o(s,"OL",{});var ce=n(Q);wl=o(ce,"LI",{});var eh=n(wl);ia=o(eh,"P",{});var mr=n(ia);xd=i(mr,"Define the total number of processes with the "),jl=o(mr,"CODE",{});var lh=n(jl);kd=i(lh,"num_process"),lh.forEach(t),Ed=i(mr," argument."),mr.forEach(t),eh.forEach(t),qd=f(ce),bl=o(ce,"LI",{});var oh=n(bl);_s=o(oh,"P",{});var he=n(_s);Pd=i(he,"Set the process "),xl=o(he,"CODE",{});var nh=n(xl);Ad=i(nh,"rank"),nh.forEach(t),Sd=i(he," as an integer between zero and "),kl=o(he,"CODE",{});var rh=n(kl);Td=i(rh,"num_process - 1"),rh.forEach(t),Dd=i(he,"."),he.forEach(t),oh.forEach(t),Cd=f(ce),El=o(ce,"LI",{});var ih=n(El);pa=o(ih,"P",{});var gr=n(pa);Nd=i(gr,"Load your metric with "),ee=o(gr,"A",{href:!0});var ph=n(ee);Id=i(ph,"datasets.load_metric()"),ph.forEach(t),Od=i(gr," with these arguments:"),gr.forEach(t),ih.forEach(t),ce.forEach(t),In=f(s),m(da.$$.fragment,s),On=f(s),m(lt.$$.fragment,s),Hn=f(s),ot=o(s,"P",{});var _r=n(ot);Hd=i(_r,"In some instances, you may be simultaneously running multiple independent distributed evaluations on the same server and files. To avoid any conflicts, it is important to provide an "),ql=o(_r,"CODE",{});var dh=n(ql);Fd=i(dh,"experiment_id"),dh.forEach(t),Ld=i(_r," to distinguish the separate evaluations:"),_r.forEach(t),Fn=f(s),m(fa.$$.fragment,s),this.h()},h(){c(h,"name","hf:doc:metadata"),c(h,"content",JSON.stringify(bh)),c(b,"id","load"),c(b,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(b,"href","#load"),c(y,"class","relative group"),c(va,"id","load-from-the-hub"),c(bs,"id","hugging-face-hub"),c(bs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(bs,"href","#hugging-face-hub"),c(Z,"class","relative group"),c($a,"href","/docs/datasets/pr_3882/en/package_reference/loading_methods#datasets.load_dataset"),c(it,"href","https://huggingface.co/datasets/lhoestq/demo1"),c(it,"rel","nofollow"),c(ct,"href","https://huggingface.co/datasets/allenai/c4"),c(ct,"rel","nofollow"),c(As,"id","local-and-remote-files"),c(As,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(As,"href","#local-and-remote-files"),c(ss,"class","relative group"),c(wa,"href","/docs/datasets/pr_3882/en/package_reference/loading_methods#datasets.load_dataset"),c(Ts,"id","csv"),c(Ts,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Ts,"href","#csv"),c(ts,"class","relative group"),c(Ds,"id","json"),c(Ds,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Ds,"href","#json"),c(as,"class","relative group"),c(qa,"href","/docs/datasets/pr_3882/en/package_reference/loading_methods#datasets.load_dataset"),c(Is,"id","text-files"),c(Is,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Is,"href","#text-files"),c(es,"class","relative group"),c(Os,"id","parquet"),c(Os,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Os,"href","#parquet"),c(ls,"class","relative group"),c(Hs,"id","inmemory-data"),c(Hs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Hs,"href","#inmemory-data"),c(os,"class","relative group"),c(Ia,"href","/docs/datasets/pr_3882/en/package_reference/main_classes#datasets.Dataset"),c(Ls,"id","python-dictionary"),c(Ls,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Ls,"href","#python-dictionary"),c(ns,"class","relative group"),c(Oa,"href","/docs/datasets/pr_3882/en/package_reference/main_classes#datasets.Dataset.from_dict"),c(Ms,"id","pandas-dataframe"),c(Ms,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Ms,"href","#pandas-dataframe"),c(rs,"class","relative group"),c(Ha,"href","/docs/datasets/pr_3882/en/package_reference/main_classes#datasets.Dataset.from_pandas"),c(Js,"id","offline"),c(Js,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Js,"href","#offline"),c(is,"class","relative group"),c(Us,"id","slice-splits"),c(Us,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Us,"href","#slice-splits"),c(ps,"class","relative group"),c(La,"href","/docs/datasets/pr_3882/en/package_reference/builder_classes#datasets.ReadInstruction"),c(Ra,"href","/docs/datasets/pr_3882/en/package_reference/builder_classes#datasets.ReadInstruction"),c(Ys,"id","percent-slicing-and-rounding"),c(Ys,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Ys,"href","#percent-slicing-and-rounding"),c(ds,"class","relative group"),c(Ua,"id","troubleshoot"),c(Qs,"id","troubleshooting"),c(Qs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Qs,"href","#troubleshooting"),c(fs,"class","relative group"),c(Xs,"id","manual-download"),c(Xs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Xs,"href","#manual-download"),c(cs,"class","relative group"),c(Ya,"href","/docs/datasets/pr_3882/en/package_reference/loading_methods#datasets.load_dataset"),c(Qt,"href","https://huggingface.co/datasets/matinf"),c(Qt,"rel","nofollow"),c(Zs,"id","specify-features"),c(Zs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Zs,"href","#specify-features"),c(hs,"class","relative group"),c(Wa,"href","/docs/datasets/pr_3882/en/package_reference/main_classes#datasets.Features"),c(Zt,"href","https://arrow.apache.org/docs/"),c(Zt,"rel","nofollow"),c(Ga,"href","/docs/datasets/pr_3882/en/package_reference/main_classes#datasets.ClassLabel"),c(Qa,"href","/docs/datasets/pr_3882/en/package_reference/main_classes#datasets.Features"),c(Xa,"href","/docs/datasets/pr_3882/en/package_reference/loading_methods#datasets.load_dataset"),c(st,"id","metrics"),c(st,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(st,"href","#metrics"),c(us,"class","relative group"),c(at,"id","load-configurations"),c(at,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(at,"href","#load-configurations"),c(ms,"class","relative group"),c(et,"id","distributed-setup"),c(et,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(et,"href","#distributed-setup"),c(gs,"class","relative group"),c(ee,"href","/docs/datasets/pr_3882/en/package_reference/loading_methods#datasets.load_metric")},m(s,e){a(document.head,h),p(s,x,e),p(s,y,e),a(y,b),a(b,k),g(w,k,null),a(y,j),a(y,q),a(q,X),p(s,vs,e),p(s,L,e),a(L,K),p(s,$s,e),p(s,R,e),a(R,M),p(s,ys,e),p(s,A,e),a(A,F),a(F,S),a(A,ua),a(A,ws),a(ws,ma),a(A,ga),a(A,js),a(js,vr),a(A,$r),a(A,ue),a(ue,yr),a(A,wr),a(A,me),a(me,jr),p(s,Nl,e),p(s,_a,e),a(_a,br),p(s,Il,e),p(s,va,e),p(s,Ol,e),p(s,Z,e),a(Z,bs),a(bs,ge),g(rt,ge,null),a(Z,xr),a(Z,_e),a(_e,kr),p(s,Hl,e),p(s,xs,e),a(xs,Er),a(xs,ve),a(ve,qr),a(xs,Pr),p(s,Fl,e),p(s,V,e),a(V,Ar),a(V,$a),a($a,Sr),a(V,Tr),a(V,it),a(it,Dr),a(V,Cr),p(s,Ll,e),g(pt,s,e),p(s,Rl,e),p(s,ya,e),a(ya,Nr),p(s,Ml,e),p(s,ks,e),a(ks,Ir),a(ks,$e),a($e,Or),a(ks,Hr),p(s,Vl,e),g(dt,s,e),p(s,zl,e),g(Es,s,e),p(s,Jl,e),p(s,D,e),a(D,Fr),a(D,ye),a(ye,Lr),a(D,Rr),a(D,we),a(we,Mr),a(D,Vr),a(D,je),a(je,zr),a(D,Jr),a(D,be),a(be,Ur),a(D,Br),a(D,xe),a(xe,Yr),a(D,Wr),p(s,Ul,e),g(ft,s,e),p(s,Bl,e),g(qs,s,e),p(s,Yl,e),p(s,z,e),a(z,Gr),a(z,ke),a(ke,Qr),a(z,Xr),a(z,ct),a(ct,Kr),a(z,Zr),p(s,Wl,e),g(ht,s,e),p(s,Gl,e),p(s,Ps,e),a(Ps,si),a(Ps,Ee),a(Ee,ti),a(Ps,ai),p(s,Ql,e),g(ut,s,e),p(s,Xl,e),p(s,ss,e),a(ss,As),a(As,qe),g(mt,qe,null),a(ss,ei),a(ss,Pe),a(Pe,li),p(s,Kl,e),p(s,C,e),a(C,oi),a(C,Ae),a(Ae,ni),a(C,ri),a(C,Se),a(Se,ii),a(C,pi),a(C,Te),a(Te,di),a(C,fi),a(C,De),a(De,ci),a(C,hi),a(C,wa),a(wa,ui),a(C,mi),p(s,Zl,e),g(Ss,s,e),p(s,so,e),p(s,ts,e),a(ts,Ts),a(Ts,Ce),g(gt,Ce,null),a(ts,gi),a(ts,Ne),a(Ne,_i),p(s,to,e),p(s,ja,e),a(ja,vi),p(s,ao,e),g(_t,s,e),p(s,eo,e),p(s,ba,e),a(ba,$i),p(s,lo,e),g(vt,s,e),p(s,oo,e),p(s,xa,e),a(xa,yi),p(s,no,e),g($t,s,e),p(s,ro,e),p(s,ka,e),a(ka,wi),p(s,io,e),g(yt,s,e),p(s,po,e),p(s,Ea,e),a(Ea,ji),p(s,fo,e),g(wt,s,e),p(s,co,e),p(s,as,e),a(as,Ds),a(Ds,Ie),g(jt,Ie,null),a(as,bi),a(as,Oe),a(Oe,xi),p(s,ho,e),p(s,Cs,e),a(Cs,ki),a(Cs,qa),a(qa,Ei),a(Cs,qi),p(s,uo,e),g(bt,s,e),p(s,mo,e),p(s,Pa,e),a(Pa,Pi),p(s,go,e),g(xt,s,e),p(s,_o,e),p(s,Ns,e),a(Ns,Ai),a(Ns,He),a(He,Si),a(Ns,Ti),p(s,vo,e),g(kt,s,e),p(s,$o,e),p(s,Aa,e),a(Aa,Di),p(s,yo,e),g(Et,s,e),p(s,wo,e),p(s,Sa,e),a(Sa,Ci),p(s,jo,e),p(s,es,e),a(es,Is),a(Is,Fe),g(qt,Fe,null),a(es,Ni),a(es,Le),a(Le,Ii),p(s,bo,e),p(s,Ta,e),a(Ta,Oi),p(s,xo,e),g(Pt,s,e),p(s,ko,e),p(s,Da,e),a(Da,Hi),p(s,Eo,e),g(At,s,e),p(s,qo,e),p(s,ls,e),a(ls,Os),a(Os,Re),g(St,Re,null),a(ls,Fi),a(ls,Me),a(Me,Li),p(s,Po,e),p(s,Ca,e),a(Ca,Ri),p(s,Ao,e),g(Tt,s,e),p(s,So,e),p(s,Na,e),a(Na,Mi),p(s,To,e),g(Dt,s,e),p(s,Do,e),p(s,os,e),a(os,Hs),a(Hs,Ve),g(Ct,Ve,null),a(os,Vi),a(os,ze),a(ze,zi),p(s,Co,e),p(s,Fs,e),a(Fs,Ji),a(Fs,Ia),a(Ia,Ui),a(Fs,Bi),p(s,No,e),p(s,ns,e),a(ns,Ls),a(Ls,Je),g(Nt,Je,null),a(ns,Yi),a(ns,Ue),a(Ue,Wi),p(s,Io,e),p(s,Rs,e),a(Rs,Gi),a(Rs,Oa),a(Oa,Qi),a(Rs,Xi),p(s,Oo,e),g(It,s,e),p(s,Ho,e),p(s,rs,e),a(rs,Ms),a(Ms,Be),g(Ot,Be,null),a(rs,Ki),a(rs,Ye),a(Ye,Zi),p(s,Fo,e),p(s,Vs,e),a(Vs,sp),a(Vs,Ha),a(Ha,tp),a(Vs,ap),p(s,Lo,e),g(Ht,s,e),p(s,Ro,e),g(zs,s,e),p(s,Mo,e),p(s,is,e),a(is,Js),a(Js,We),g(Ft,We,null),a(is,ep),a(is,Ge),a(Ge,lp),p(s,Vo,e),p(s,Fa,e),a(Fa,op),p(s,zo,e),p(s,J,e),a(J,np),a(J,Qe),a(Qe,rp),a(J,ip),a(J,Xe),a(Xe,pp),a(J,dp),p(s,Jo,e),p(s,ps,e),a(ps,Us),a(Us,Ke),g(Lt,Ke,null),a(ps,fp),a(ps,Ze),a(Ze,cp),p(s,Uo,e),p(s,U,e),a(U,hp),a(U,La),a(La,up),a(U,mp),a(U,Ra),a(Ra,gp),a(U,_p),p(s,Bo,e),p(s,B,e),a(B,vp),a(B,sl),a(sl,$p),a(B,yp),a(B,tl),a(tl,wp),a(B,jp),p(s,Yo,e),g(Rt,s,e),p(s,Wo,e),p(s,Bs,e),a(Bs,bp),a(Bs,al),a(al,xp),a(Bs,kp),p(s,Go,e),g(Mt,s,e),p(s,Qo,e),p(s,Ma,e),a(Ma,Ep),p(s,Xo,e),g(Vt,s,e),p(s,Ko,e),p(s,Va,e),a(Va,qp),p(s,Zo,e),g(zt,s,e),p(s,sn,e),p(s,za,e),a(za,Pp),p(s,tn,e),g(Jt,s,e),p(s,an,e),p(s,ds,e),a(ds,Ys),a(Ys,el),g(Ut,el,null),a(ds,Ap),a(ds,ll),a(ll,Sp),p(s,en,e),p(s,Ja,e),a(Ja,Tp),p(s,ln,e),g(Bt,s,e),p(s,on,e),p(s,Ws,e),a(Ws,Dp),a(Ws,ol),a(ol,Cp),a(Ws,Np),p(s,nn,e),g(Yt,s,e),p(s,rn,e),g(Gs,s,e),p(s,pn,e),p(s,Ua,e),p(s,dn,e),p(s,fs,e),a(fs,Qs),a(Qs,nl),g(Wt,nl,null),a(fs,Ip),a(fs,rl),a(rl,Op),p(s,fn,e),p(s,Ba,e),a(Ba,Hp),p(s,cn,e),p(s,cs,e),a(cs,Xs),a(Xs,il),g(Gt,il,null),a(cs,Fp),a(cs,pl),a(pl,Lp),p(s,hn,e),p(s,I,e),a(I,Rp),a(I,Ya),a(Ya,Mp),a(I,Vp),a(I,dl),a(dl,zp),a(I,Jp),a(I,fl),a(fl,Up),a(I,Bp),p(s,un,e),p(s,Ks,e),a(Ks,Yp),a(Ks,Qt),a(Qt,Wp),a(Ks,Gp),p(s,mn,e),g(Xt,s,e),p(s,gn,e),p(s,hs,e),a(hs,Zs),a(Zs,cl),g(Kt,cl,null),a(hs,Qp),a(hs,hl),a(hl,Xp),p(s,_n,e),p(s,Y,e),a(Y,Kp),a(Y,Wa),a(Wa,Zp),a(Y,sd),a(Y,Zt),a(Zt,td),a(Y,ad),p(s,vn,e),p(s,W,e),a(W,ed),a(W,Ga),a(Ga,ld),a(W,od),a(W,Qa),a(Qa,nd),a(W,rd),p(s,$n,e),g(sa,s,e),p(s,yn,e),p(s,G,e),a(G,id),a(G,ul),a(ul,pd),a(G,dd),a(G,Xa),a(Xa,fd),a(G,cd),p(s,wn,e),g(ta,s,e),p(s,jn,e),p(s,Ka,e),a(Ka,hd),p(s,bn,e),g(aa,s,e),p(s,xn,e),p(s,us,e),a(us,st),a(st,ml),g(ea,ml,null),a(us,ud),a(us,gl),a(gl,md),p(s,kn,e),p(s,Za,e),a(Za,gd),p(s,En,e),g(la,s,e),p(s,qn,e),g(tt,s,e),p(s,Pn,e),p(s,ms,e),a(ms,at),a(at,_l),g(oa,_l,null),a(ms,_d),a(ms,vl),a(vl,vd),p(s,An,e),p(s,se,e),a(se,$d),p(s,Sn,e),g(na,s,e),p(s,Tn,e),p(s,gs,e),a(gs,et),a(et,$l),g(ra,$l,null),a(gs,yd),a(gs,yl),a(yl,wd),p(s,Dn,e),p(s,te,e),a(te,jd),p(s,Cn,e),p(s,ae,e),a(ae,bd),p(s,Nn,e),p(s,Q,e),a(Q,wl),a(wl,ia),a(ia,xd),a(ia,jl),a(jl,kd),a(ia,Ed),a(Q,qd),a(Q,bl),a(bl,_s),a(_s,Pd),a(_s,xl),a(xl,Ad),a(_s,Sd),a(_s,kl),a(kl,Td),a(_s,Dd),a(Q,Cd),a(Q,El),a(El,pa),a(pa,Nd),a(pa,ee),a(ee,Id),a(pa,Od),p(s,In,e),g(da,s,e),p(s,On,e),g(lt,s,e),p(s,Hn,e),p(s,ot,e),a(ot,Hd),a(ot,ql),a(ql,Fd),a(ot,Ld),p(s,Fn,e),g(fa,s,e),Ln=!0},p(s,[e]){const ca={};e&2&&(ca.$$scope={dirty:e,ctx:s}),Es.$set(ca);const Pl={};e&2&&(Pl.$$scope={dirty:e,ctx:s}),qs.$set(Pl);const Al={};e&2&&(Al.$$scope={dirty:e,ctx:s}),Ss.$set(Al);const Sl={};e&2&&(Sl.$$scope={dirty:e,ctx:s}),zs.$set(Sl);const Tl={};e&2&&(Tl.$$scope={dirty:e,ctx:s}),Gs.$set(Tl);const Dl={};e&2&&(Dl.$$scope={dirty:e,ctx:s}),tt.$set(Dl);const N={};e&2&&(N.$$scope={dirty:e,ctx:s}),lt.$set(N)},i(s){Ln||(_(w.$$.fragment,s),_(rt.$$.fragment,s),_(pt.$$.fragment,s),_(dt.$$.fragment,s),_(Es.$$.fragment,s),_(ft.$$.fragment,s),_(qs.$$.fragment,s),_(ht.$$.fragment,s),_(ut.$$.fragment,s),_(mt.$$.fragment,s),_(Ss.$$.fragment,s),_(gt.$$.fragment,s),_(_t.$$.fragment,s),_(vt.$$.fragment,s),_($t.$$.fragment,s),_(yt.$$.fragment,s),_(wt.$$.fragment,s),_(jt.$$.fragment,s),_(bt.$$.fragment,s),_(xt.$$.fragment,s),_(kt.$$.fragment,s),_(Et.$$.fragment,s),_(qt.$$.fragment,s),_(Pt.$$.fragment,s),_(At.$$.fragment,s),_(St.$$.fragment,s),_(Tt.$$.fragment,s),_(Dt.$$.fragment,s),_(Ct.$$.fragment,s),_(Nt.$$.fragment,s),_(It.$$.fragment,s),_(Ot.$$.fragment,s),_(Ht.$$.fragment,s),_(zs.$$.fragment,s),_(Ft.$$.fragment,s),_(Lt.$$.fragment,s),_(Rt.$$.fragment,s),_(Mt.$$.fragment,s),_(Vt.$$.fragment,s),_(zt.$$.fragment,s),_(Jt.$$.fragment,s),_(Ut.$$.fragment,s),_(Bt.$$.fragment,s),_(Yt.$$.fragment,s),_(Gs.$$.fragment,s),_(Wt.$$.fragment,s),_(Gt.$$.fragment,s),_(Xt.$$.fragment,s),_(Kt.$$.fragment,s),_(sa.$$.fragment,s),_(ta.$$.fragment,s),_(aa.$$.fragment,s),_(ea.$$.fragment,s),_(la.$$.fragment,s),_(tt.$$.fragment,s),_(oa.$$.fragment,s),_(na.$$.fragment,s),_(ra.$$.fragment,s),_(da.$$.fragment,s),_(lt.$$.fragment,s),_(fa.$$.fragment,s),Ln=!0)},o(s){v(w.$$.fragment,s),v(rt.$$.fragment,s),v(pt.$$.fragment,s),v(dt.$$.fragment,s),v(Es.$$.fragment,s),v(ft.$$.fragment,s),v(qs.$$.fragment,s),v(ht.$$.fragment,s),v(ut.$$.fragment,s),v(mt.$$.fragment,s),v(Ss.$$.fragment,s),v(gt.$$.fragment,s),v(_t.$$.fragment,s),v(vt.$$.fragment,s),v($t.$$.fragment,s),v(yt.$$.fragment,s),v(wt.$$.fragment,s),v(jt.$$.fragment,s),v(bt.$$.fragment,s),v(xt.$$.fragment,s),v(kt.$$.fragment,s),v(Et.$$.fragment,s),v(qt.$$.fragment,s),v(Pt.$$.fragment,s),v(At.$$.fragment,s),v(St.$$.fragment,s),v(Tt.$$.fragment,s),v(Dt.$$.fragment,s),v(Ct.$$.fragment,s),v(Nt.$$.fragment,s),v(It.$$.fragment,s),v(Ot.$$.fragment,s),v(Ht.$$.fragment,s),v(zs.$$.fragment,s),v(Ft.$$.fragment,s),v(Lt.$$.fragment,s),v(Rt.$$.fragment,s),v(Mt.$$.fragment,s),v(Vt.$$.fragment,s),v(zt.$$.fragment,s),v(Jt.$$.fragment,s),v(Ut.$$.fragment,s),v(Bt.$$.fragment,s),v(Yt.$$.fragment,s),v(Gs.$$.fragment,s),v(Wt.$$.fragment,s),v(Gt.$$.fragment,s),v(Xt.$$.fragment,s),v(Kt.$$.fragment,s),v(sa.$$.fragment,s),v(ta.$$.fragment,s),v(aa.$$.fragment,s),v(ea.$$.fragment,s),v(la.$$.fragment,s),v(tt.$$.fragment,s),v(oa.$$.fragment,s),v(na.$$.fragment,s),v(ra.$$.fragment,s),v(da.$$.fragment,s),v(lt.$$.fragment,s),v(fa.$$.fragment,s),Ln=!1},d(s){t(h),s&&t(x),s&&t(y),$(w),s&&t(vs),s&&t(L),s&&t($s),s&&t(R),s&&t(ys),s&&t(A),s&&t(Nl),s&&t(_a),s&&t(Il),s&&t(va),s&&t(Ol),s&&t(Z),$(rt),s&&t(Hl),s&&t(xs),s&&t(Fl),s&&t(V),s&&t(Ll),$(pt,s),s&&t(Rl),s&&t(ya),s&&t(Ml),s&&t(ks),s&&t(Vl),$(dt,s),s&&t(zl),$(Es,s),s&&t(Jl),s&&t(D),s&&t(Ul),$(ft,s),s&&t(Bl),$(qs,s),s&&t(Yl),s&&t(z),s&&t(Wl),$(ht,s),s&&t(Gl),s&&t(Ps),s&&t(Ql),$(ut,s),s&&t(Xl),s&&t(ss),$(mt),s&&t(Kl),s&&t(C),s&&t(Zl),$(Ss,s),s&&t(so),s&&t(ts),$(gt),s&&t(to),s&&t(ja),s&&t(ao),$(_t,s),s&&t(eo),s&&t(ba),s&&t(lo),$(vt,s),s&&t(oo),s&&t(xa),s&&t(no),$($t,s),s&&t(ro),s&&t(ka),s&&t(io),$(yt,s),s&&t(po),s&&t(Ea),s&&t(fo),$(wt,s),s&&t(co),s&&t(as),$(jt),s&&t(ho),s&&t(Cs),s&&t(uo),$(bt,s),s&&t(mo),s&&t(Pa),s&&t(go),$(xt,s),s&&t(_o),s&&t(Ns),s&&t(vo),$(kt,s),s&&t($o),s&&t(Aa),s&&t(yo),$(Et,s),s&&t(wo),s&&t(Sa),s&&t(jo),s&&t(es),$(qt),s&&t(bo),s&&t(Ta),s&&t(xo),$(Pt,s),s&&t(ko),s&&t(Da),s&&t(Eo),$(At,s),s&&t(qo),s&&t(ls),$(St),s&&t(Po),s&&t(Ca),s&&t(Ao),$(Tt,s),s&&t(So),s&&t(Na),s&&t(To),$(Dt,s),s&&t(Do),s&&t(os),$(Ct),s&&t(Co),s&&t(Fs),s&&t(No),s&&t(ns),$(Nt),s&&t(Io),s&&t(Rs),s&&t(Oo),$(It,s),s&&t(Ho),s&&t(rs),$(Ot),s&&t(Fo),s&&t(Vs),s&&t(Lo),$(Ht,s),s&&t(Ro),$(zs,s),s&&t(Mo),s&&t(is),$(Ft),s&&t(Vo),s&&t(Fa),s&&t(zo),s&&t(J),s&&t(Jo),s&&t(ps),$(Lt),s&&t(Uo),s&&t(U),s&&t(Bo),s&&t(B),s&&t(Yo),$(Rt,s),s&&t(Wo),s&&t(Bs),s&&t(Go),$(Mt,s),s&&t(Qo),s&&t(Ma),s&&t(Xo),$(Vt,s),s&&t(Ko),s&&t(Va),s&&t(Zo),$(zt,s),s&&t(sn),s&&t(za),s&&t(tn),$(Jt,s),s&&t(an),s&&t(ds),$(Ut),s&&t(en),s&&t(Ja),s&&t(ln),$(Bt,s),s&&t(on),s&&t(Ws),s&&t(nn),$(Yt,s),s&&t(rn),$(Gs,s),s&&t(pn),s&&t(Ua),s&&t(dn),s&&t(fs),$(Wt),s&&t(fn),s&&t(Ba),s&&t(cn),s&&t(cs),$(Gt),s&&t(hn),s&&t(I),s&&t(un),s&&t(Ks),s&&t(mn),$(Xt,s),s&&t(gn),s&&t(hs),$(Kt),s&&t(_n),s&&t(Y),s&&t(vn),s&&t(W),s&&t($n),$(sa,s),s&&t(yn),s&&t(G),s&&t(wn),$(ta,s),s&&t(jn),s&&t(Ka),s&&t(bn),$(aa,s),s&&t(xn),s&&t(us),$(ea),s&&t(kn),s&&t(Za),s&&t(En),$(la,s),s&&t(qn),$(tt,s),s&&t(Pn),s&&t(ms),$(oa),s&&t(An),s&&t(se),s&&t(Sn),$(na,s),s&&t(Tn),s&&t(gs),$(ra),s&&t(Dn),s&&t(te),s&&t(Cn),s&&t(ae),s&&t(Nn),s&&t(Q),s&&t(In),$(da,s),s&&t(On),$(lt,s),s&&t(Hn),s&&t(ot),s&&t(Fn),$(fa,s)}}}const bh={local:"load",sections:[{local:"hugging-face-hub",title:"Hugging Face Hub"},{local:"local-and-remote-files",sections:[{local:"csv",title:"CSV"},{local:"json",title:"JSON"},{local:"text-files",title:"Text files"},{local:"parquet",title:"Parquet"}],title:"Local and remote files"},{local:"inmemory-data",sections:[{local:"python-dictionary",title:"Python dictionary"},{local:"pandas-dataframe",title:"Pandas DataFrame"}],title:"In-memory data"},{local:"offline",title:"Offline"},{local:"slice-splits",sections:[{local:"percent-slicing-and-rounding",title:"Percent slicing and rounding"}],title:"Slice splits"},{local:"troubleshooting",sections:[{local:"manual-download",title:"Manual download"},{local:"specify-features",title:"Specify features"}],title:"Troubleshooting"},{local:"metrics",sections:[{local:"load-configurations",title:"Load configurations"},{local:"distributed-setup",title:"Distributed setup"}],title:"Metrics"}],title:"Load"};function xh(T,h,x){let{fw:y}=h;return T.$$set=b=>{"fw"in b&&x(0,y=b.fw)},[y]}class Sh extends fh{constructor(h){super();ch(this,h,xh,jh,hh,{fw:0})}}export{Sh as default,bh as metadata};
