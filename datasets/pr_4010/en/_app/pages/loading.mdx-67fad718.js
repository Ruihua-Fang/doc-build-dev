import{S as zd,i as Ud,s as Jd,e as l,k as f,t as i,c as o,a as n,m as c,h as p,d as t,b as h,g as r,F as a,Q as Cl,q as b,l as _h,n as $r,o as j,B as k,p as yr,w as E,y as q,j as qh,G as Ph,$ as wh,x as P,a0 as Ah,T as Sh,Y as bh,Z as jh,M as Th,v as Dh}from"../chunks/vendor-aa873a46.js";import{T as ha}from"../chunks/Tip-f7f252ab.js";import{I}from"../chunks/IconCopyLink-d0ca3106.js";import{a as xh,C as D}from"../chunks/CodeBlock-1f14baf3.js";import{b as kh,I as Nh,a as Ch}from"../chunks/IconTensorflow-b9816778.js";function vh(x,d,g){const u=x.slice();return u[8]=d[g],u[10]=g,u}function $h(x){let d,g,u;var y=x[8].icon;function $(_){return{props:{classNames:"mr-1.5"}}}return y&&(d=new y($())),{c(){d&&E(d.$$.fragment),g=_h()},l(_){d&&P(d.$$.fragment,_),g=_h()},m(_,m){d&&q(d,_,m),r(_,g,m),u=!0},p(_,m){if(y!==(y=_[8].icon)){if(d){$r();const w=d;j(w.$$.fragment,1,0,()=>{k(w,1)}),yr()}y?(d=new y($()),E(d.$$.fragment),b(d.$$.fragment,1),q(d,g.parentNode,g)):d=null}},i(_){u||(d&&b(d.$$.fragment,_),u=!0)},o(_){d&&j(d.$$.fragment,_),u=!1},d(_){_&&t(g),d&&k(d,_)}}}function yh(x){let d,g,u,y=x[8].name+"",$,_,m,w,v,A,S,T=x[8].icon&&$h(x);function B(){return x[6](x[8])}return{c(){d=l("button"),T&&T.c(),g=f(),u=l("p"),$=i(y),m=f(),this.h()},l(C){d=o(C,"BUTTON",{class:!0});var N=n(d);T&&T.l(N),g=c(N),u=o(N,"P",{class:!0});var U=n(u);$=p(U,y),U.forEach(t),m=c(N),N.forEach(t),this.h()},h(){h(u,"class",_="!m-0 "+x[8].classNames),h(d,"class",w="flex justify-center py-1.5 px-2.5 focus:outline-none rounded-"+(x[10]?"r":"l")+" "+(x[8].group!==x[1]&&"text-gray-500 filter grayscale"))},m(C,N){r(C,d,N),T&&T.m(d,null),a(d,g),a(d,u),a(u,$),a(d,m),v=!0,A||(S=Cl(d,"click",B),A=!0)},p(C,N){x=C,x[8].icon?T?(T.p(x,N),N&1&&b(T,1)):(T=$h(x),T.c(),b(T,1),T.m(d,g)):T&&($r(),j(T,1,1,()=>{T=null}),yr()),(!v||N&1)&&y!==(y=x[8].name+"")&&qh($,y),(!v||N&1&&_!==(_="!m-0 "+x[8].classNames))&&h(u,"class",_),(!v||N&3&&w!==(w="flex justify-center py-1.5 px-2.5 focus:outline-none rounded-"+(x[10]?"r":"l")+" "+(x[8].group!==x[1]&&"text-gray-500 filter grayscale")))&&h(d,"class",w)},i(C){v||(b(T),v=!0)},o(C){j(T),v=!1},d(C){C&&t(d),T&&T.d(),A=!1,S()}}}function Ih(x){let d,g,u,y=x[3].filter(x[5]),$=[];for(let m=0;m<y.length;m+=1)$[m]=yh(vh(x,y,m));const _=m=>j($[m],1,1,()=>{$[m]=null});return{c(){d=l("div"),g=l("div");for(let m=0;m<$.length;m+=1)$[m].c();this.h()},l(m){d=o(m,"DIV",{});var w=n(d);g=o(w,"DIV",{class:!0});var v=n(g);for(let A=0;A<$.length;A+=1)$[A].l(v);v.forEach(t),w.forEach(t),this.h()},h(){h(g,"class","bg-white leading-none border border-gray-100 rounded-lg inline-flex p-0.5 text-sm mb-4 select-none")},m(m,w){r(m,d,w),a(d,g);for(let v=0;v<$.length;v+=1)$[v].m(g,null);u=!0},p(m,[w]){if(w&27){y=m[3].filter(m[5]);let v;for(v=0;v<y.length;v+=1){const A=vh(m,y,v);$[v]?($[v].p(A,w),b($[v],1)):($[v]=yh(A),$[v].c(),b($[v],1),$[v].m(g,null))}for($r(),v=y.length;v<$.length;v+=1)_(v);yr()}},i(m){if(!u){for(let w=0;w<y.length;w+=1)b($[w]);u=!0}},o(m){$=$.filter(Boolean);for(let w=0;w<$.length;w+=1)j($[w]);u=!1},d(m){m&&t(d),Ph($,m)}}}function Oh(x,d,g){let u,{ids:y}=d;const $=y.join("-"),_=kh($);wh(x,_,S=>g(1,u=S));const m=[{id:"pt",classNames:"",icon:Nh,name:"Pytorch",group:"group1"},{id:"tf",classNames:"",icon:Ch,name:"TensorFlow",group:"group2"},{id:"stringapi",classNames:"text-blue-600",name:"String API",group:"group1"},{id:"readinstruction",classNames:"text-blue-600",name:"ReadInstruction",group:"group2"}];function w(S){Ah(_,u=S,u)}const v=S=>y.includes(S.id),A=S=>w(S.group);return x.$$set=S=>{"ids"in S&&g(0,y=S.ids)},[y,u,_,m,w,v,A]}class Eh extends zd{constructor(d){super();Ud(this,d,Oh,Ih,Jd,{ids:0})}}function Hh(x){let d,g,u,y,$,_,m=x[1].highlighted+"",w;return g=new xh({props:{classNames:"transition duration-200 ease-in-out "+(x[2]&&"opacity-0"),title:"Copy code excerpt to clipboard",value:x[1].code}}),$=new Eh({props:{ids:x[4]}}),{c(){d=l("div"),E(g.$$.fragment),u=f(),y=l("pre"),E($.$$.fragment),_=new bh,this.h()},l(v){d=o(v,"DIV",{class:!0});var A=n(d);P(g.$$.fragment,A),A.forEach(t),u=c(v),y=o(v,"PRE",{});var S=n(y);P($.$$.fragment,S),_=jh(S),S.forEach(t),this.h()},h(){h(d,"class","absolute top-2.5 right-4"),_.a=null},m(v,A){r(v,d,A),q(g,d,null),r(v,u,A),r(v,y,A),q($,y,null),_.m(m,y),w=!0},p(v,A){const S={};A&4&&(S.classNames="transition duration-200 ease-in-out "+(v[2]&&"opacity-0")),A&2&&(S.value=v[1].code),g.$set(S),(!w||A&2)&&m!==(m=v[1].highlighted+"")&&_.p(m)},i(v){w||(b(g.$$.fragment,v),b($.$$.fragment,v),w=!0)},o(v){j(g.$$.fragment,v),j($.$$.fragment,v),w=!1},d(v){v&&t(d),k(g),v&&t(u),v&&t(y),k($)}}}function Fh(x){let d,g,u,y,$,_,m=x[0].highlighted+"",w;return g=new xh({props:{classNames:"transition duration-200 ease-in-out "+(x[2]&&"opacity-0"),title:"Copy code excerpt to clipboard",value:x[0].code}}),$=new Eh({props:{ids:x[4]}}),{c(){d=l("div"),E(g.$$.fragment),u=f(),y=l("pre"),E($.$$.fragment),_=new bh,this.h()},l(v){d=o(v,"DIV",{class:!0});var A=n(d);P(g.$$.fragment,A),A.forEach(t),u=c(v),y=o(v,"PRE",{});var S=n(y);P($.$$.fragment,S),_=jh(S),S.forEach(t),this.h()},h(){h(d,"class","absolute top-2.5 right-4"),_.a=null},m(v,A){r(v,d,A),q(g,d,null),r(v,u,A),r(v,y,A),q($,y,null),_.m(m,y),w=!0},p(v,A){const S={};A&4&&(S.classNames="transition duration-200 ease-in-out "+(v[2]&&"opacity-0")),A&1&&(S.value=v[0].code),g.$set(S),(!w||A&1)&&m!==(m=v[0].highlighted+"")&&_.p(m)},i(v){w||(b(g.$$.fragment,v),b($.$$.fragment,v),w=!0)},o(v){j(g.$$.fragment,v),j($.$$.fragment,v),w=!1},d(v){v&&t(d),k(g),v&&t(u),v&&t(y),k($)}}}function Lh(x){let d,g,u,y,$,_;const m=[Fh,Hh],w=[];function v(A,S){return A[3]==="group1"?0:1}return g=v(x),u=w[g]=m[g](x),{c(){d=l("div"),u.c(),this.h()},l(A){d=o(A,"DIV",{class:!0});var S=n(d);u.l(S),S.forEach(t),this.h()},h(){h(d,"class","code-block relative")},m(A,S){r(A,d,S),w[g].m(d,null),y=!0,$||(_=[Cl(d,"mouseover",x[6]),Cl(d,"focus",x[6]),Cl(d,"mouseout",x[7]),Cl(d,"focus",x[7])],$=!0)},p(A,[S]){let T=g;g=v(A),g===T?w[g].p(A,S):($r(),j(w[T],1,1,()=>{w[T]=null}),yr(),u=w[g],u?u.p(A,S):(u=w[g]=m[g](A),u.c()),b(u,1),u.m(d,null))},i(A){y||(b(u),y=!0)},o(A){j(u),y=!1},d(A){A&&t(d),w[g].d(),$=!1,Sh(_)}}}function Rh(x,d,g){let u,{group1:y}=d,{group2:$}=d;const _=[y.id,$.id],m=_.join("-"),w=kh(m);wh(x,w,T=>g(3,u=T));let v=!0;function A(){g(2,v=!1)}function S(){g(2,v=!0)}return x.$$set=T=>{"group1"in T&&g(0,y=T.group1),"group2"in T&&g(1,$=T.group2)},[y,$,v,u,_,w,A,S]}class Nl extends zd{constructor(d){super();Ud(this,d,Rh,Lh,Jd,{group1:0,group2:1})}}function Mh(x){let d,g,u,y,$;return{c(){d=l("p"),g=i("Refer to the "),u=l("a"),y=i("Upload"),$=i(" guide for more instructions on how to create a dataset repository on the Hub, and how to upload your data files."),this.h()},l(_){d=o(_,"P",{});var m=n(d);g=p(m,"Refer to the "),u=o(m,"A",{href:!0});var w=n(u);y=p(w,"Upload"),w.forEach(t),$=p(m," guide for more instructions on how to create a dataset repository on the Hub, and how to upload your data files."),m.forEach(t),this.h()},h(){h(u,"href","./upload_dataset")},m(_,m){r(_,d,m),a(d,g),a(d,u),a(u,y),a(d,$)},d(_){_&&t(d)}}}function Vh(x){let d,g,u,y,$;return{c(){d=l("p"),g=i("If you don\u2019t specify which data files to use, "),u=l("code"),y=i("load_dataset"),$=i(" will return all the data files. This can take a long time if you are loading a large dataset like C4, which is approximately 13TB of data.")},l(_){d=o(_,"P",{});var m=n(d);g=p(m,"If you don\u2019t specify which data files to use, "),u=o(m,"CODE",{});var w=n(u);y=p(w,"load_dataset"),w.forEach(t),$=p(m," will return all the data files. This can take a long time if you are loading a large dataset like C4, which is approximately 13TB of data."),m.forEach(t)},m(_,m){r(_,d,m),a(d,g),a(d,u),a(u,y),a(d,$)},d(_){_&&t(d)}}}function zh(x){let d,g,u,y,$;return{c(){d=l("p"),g=i("Curious about how to load datasets for vision? Check out the image loading guide "),u=l("a"),y=i("here"),$=i("!"),this.h()},l(_){d=o(_,"P",{});var m=n(d);g=p(m,"Curious about how to load datasets for vision? Check out the image loading guide "),u=o(m,"A",{href:!0});var w=n(u);y=p(w,"here"),w.forEach(t),$=p(m,"!"),m.forEach(t),this.h()},h(){h(u,"href","./image_process")},m(_,m){r(_,d,m),a(d,g),a(d,u),a(u,y),a(d,$)},d(_){_&&t(d)}}}function Uh(x){let d,g,u,y,$,_,m,w,v,A,S,T,B,C,N,U,O;return{c(){d=l("p"),g=i("An object data type in "),u=l("a"),y=i("pandas.Series"),$=i(" doesn\u2019t always carry enough information for Arrow to automatically infer a data type. For example, if a DataFrame is of length 0 or the Series only contains None/nan objects, the type is set to null. Avoid potential errors by constructing an explicit schema with "),_=l("a"),m=i("Features"),w=i(" using the "),v=l("code"),A=i("from_dict"),S=i(" or "),T=l("code"),B=i("from_pandas"),C=i(" methods. See the "),N=l("a"),U=i("troubleshoot"),O=i(" for more details on how to explicitly specify your own features."),this.h()},l(J){d=o(J,"P",{});var H=n(d);g=p(H,"An object data type in "),u=o(H,"A",{href:!0,rel:!0});var ua=n(u);y=p(ua,"pandas.Series"),ua.forEach(t),$=p(H," doesn\u2019t always carry enough information for Arrow to automatically infer a data type. For example, if a DataFrame is of length 0 or the Series only contains None/nan objects, the type is set to null. Avoid potential errors by constructing an explicit schema with "),_=o(H,"A",{href:!0});var ws=n(_);m=p(ws,"Features"),ws.forEach(t),w=p(H," using the "),v=o(H,"CODE",{});var ma=n(v);A=p(ma,"from_dict"),ma.forEach(t),S=p(H," or "),T=o(H,"CODE",{});var ga=n(T);B=p(ga,"from_pandas"),ga.forEach(t),C=p(H," methods. See the "),N=o(H,"A",{href:!0});var bs=n(N);U=p(bs,"troubleshoot"),bs.forEach(t),O=p(H," for more details on how to explicitly specify your own features."),H.forEach(t),this.h()},h(){h(u,"href","https://pandas.pydata.org/docs/reference/api/pandas.Series.html"),h(u,"rel","nofollow"),h(_,"href","/docs/datasets/pr_4010/en/package_reference/main_classes#datasets.Features"),h(N,"href","./loading#specify-features")},m(J,H){r(J,d,H),a(d,g),a(d,u),a(u,y),a(d,$),a(d,_),a(_,m),a(d,w),a(d,v),a(v,A),a(d,S),a(d,T),a(T,B),a(d,C),a(d,N),a(N,U),a(d,O)},d(J){J&&t(d)}}}function Jh(x){let d,g,u,y,$;return{c(){d=l("p"),g=i("Using "),u=l("code"),y=i("pct1_dropremainder"),$=i(" rounding may truncate the last examples in a dataset if the number of examples in your dataset don\u2019t divide evenly by 100.")},l(_){d=o(_,"P",{});var m=n(d);g=p(m,"Using "),u=o(m,"CODE",{});var w=n(u);y=p(w,"pct1_dropremainder"),w.forEach(t),$=p(m," rounding may truncate the last examples in a dataset if the number of examples in your dataset don\u2019t divide evenly by 100."),m.forEach(t)},m(_,m){r(_,d,m),a(d,g),a(d,u),a(u,y),a(d,$)},d(_){_&&t(d)}}}function Bh(x){let d,g,u,y,$;return{c(){d=l("p"),g=i("See the "),u=l("a"),y=i("Metrics"),$=i(" guide for more details on how to write your own metric loading script."),this.h()},l(_){d=o(_,"P",{});var m=n(d);g=p(m,"See the "),u=o(m,"A",{href:!0});var w=n(u);y=p(w,"Metrics"),w.forEach(t),$=p(m," guide for more details on how to write your own metric loading script."),m.forEach(t),this.h()},h(){h(u,"href","./how_to_metrics#custom-metric-loading-script")},m(_,m){r(_,d,m),a(d,g),a(d,u),a(u,y),a(d,$)},d(_){_&&t(d)}}}function Yh(x){let d,g,u,y,$;return{c(){d=l("p"),g=i("Once you\u2019ve loaded a metric for distributed usage, you can compute the metric as usual. Behind the scenes, "),u=l("a"),y=i("Metric.compute()"),$=i(" gathers all the predictions and references from the nodes, and computes the final metric."),this.h()},l(_){d=o(_,"P",{});var m=n(d);g=p(m,"Once you\u2019ve loaded a metric for distributed usage, you can compute the metric as usual. Behind the scenes, "),u=o(m,"A",{href:!0});var w=n(u);y=p(w,"Metric.compute()"),w.forEach(t),$=p(m," gathers all the predictions and references from the nodes, and computes the final metric."),m.forEach(t),this.h()},h(){h(u,"href","/docs/datasets/pr_4010/en/package_reference/main_classes#datasets.Metric.compute")},m(_,m){r(_,d,m),a(d,g),a(d,u),a(u,y),a(d,$)},d(_){_&&t(d)}}}function Wh(x){let d,g,u,y,$,_,m,w,v,A,S,T,B,C,N,U,O,J,H,ua,ws,ma,ga,bs,wr,br,ue,jr,xr,me,kr,Il,_a,Er,Ol,va,Hl,as,js,ge,rt,qr,_e,Pr,Fl,xs,Ar,ve,Sr,Tr,Ll,Y,Dr,$a,Nr,Cr,it,Ir,Or,Rl,pt,Ml,ya,Hr,Vl,ks,Fr,$e,Lr,Rr,zl,dt,Ul,Es,Jl,F,Mr,ye,Vr,zr,we,Ur,Jr,be,Br,Yr,je,Wr,Gr,xe,Qr,Kr,Bl,ft,Yl,qs,Wl,W,Xr,ke,Zr,si,ct,ti,ai,Gl,ht,Ql,Ps,ei,Ee,li,oi,Kl,ut,Xl,es,As,qe,mt,ni,Pe,ri,Zl,L,ii,Ae,pi,di,Se,fi,ci,Te,hi,ui,De,mi,gi,wa,_i,vi,so,Ss,to,ls,Ts,Ne,gt,$i,Ce,yi,ao,ba,wi,eo,_t,lo,ja,bi,oo,vt,no,xa,ji,ro,$t,io,ka,xi,po,yt,fo,Ea,ki,co,wt,ho,os,Ds,Ie,bt,Ei,Oe,qi,uo,Ns,Pi,qa,Ai,Si,mo,jt,go,Pa,Ti,_o,xt,vo,Cs,Di,He,Ni,Ci,$o,kt,yo,Aa,Ii,wo,Et,bo,Sa,Oi,jo,ns,Is,Fe,qt,Hi,Le,Fi,xo,Ta,Li,ko,Pt,Eo,Da,Ri,qo,At,Po,rs,Os,Re,St,Mi,Me,Vi,Ao,Na,zi,So,Tt,To,Ca,Ui,Do,Dt,No,is,Hs,Ve,Nt,Ji,ze,Bi,Co,Fs,Yi,Ia,Wi,Gi,Io,ps,Ls,Ue,Ct,Qi,Je,Ki,Oo,Rs,Xi,Oa,Zi,sp,Ho,It,Fo,ds,Ms,Be,Ot,tp,Ye,ap,Lo,Vs,ep,Ha,lp,op,Ro,Ht,Mo,zs,Vo,fs,Us,We,Ft,np,Ge,rp,zo,Fa,ip,Uo,G,pp,Qe,dp,fp,Ke,cp,hp,Jo,cs,Js,Xe,Lt,up,Ze,mp,Bo,Q,gp,La,_p,vp,Ra,$p,yp,Yo,K,wp,sl,bp,jp,tl,xp,kp,Wo,Rt,Go,Bs,Ep,al,qp,Pp,Qo,Mt,Ko,Ma,Ap,Xo,Vt,Zo,Va,Sp,sn,zt,tn,za,Tp,an,Ut,en,hs,Ys,el,Jt,Dp,ll,Np,ln,Ua,Cp,on,Bt,nn,Ws,Ip,ol,Op,Hp,rn,Yt,pn,Gs,dn,Ja,fn,us,Qs,nl,Wt,Fp,rl,Lp,cn,Ba,Rp,hn,ms,Ks,il,Gt,Mp,pl,Vp,un,M,zp,Ya,Up,Jp,dl,Bp,Yp,fl,Wp,Gp,mn,Xs,Qp,Qt,Kp,Xp,gn,Kt,_n,gs,Zs,cl,Xt,Zp,hl,sd,vn,X,td,Wa,ad,ed,Zt,ld,od,$n,Z,nd,Ga,rd,id,Qa,pd,dd,yn,sa,wn,ss,fd,ul,cd,hd,Ka,ud,md,bn,ta,jn,Xa,gd,xn,aa,kn,_s,st,ml,ea,_d,gl,vd,En,Za,$d,qn,la,Pn,tt,An,vs,at,_l,oa,yd,vl,wd,Sn,se,bd,Tn,na,Dn,$s,et,$l,ra,jd,yl,xd,Nn,te,kd,Cn,ae,Ed,In,ts,wl,ia,qd,bl,Pd,Ad,Sd,jl,ys,Td,xl,Dd,Nd,kl,Cd,Id,Od,El,pa,Hd,ee,Fd,Ld,On,da,Hn,lt,Fn,ot,Rd,ql,Md,Vd,Ln,fa,Rn;return _=new I({}),rt=new I({}),pt=new D({props:{code:`from datasets import load_dataset
dataset = load_dataset('lhoestq/demo1')`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;lhoestq/demo1&#x27;</span>)`}}),dt=new D({props:{code:`dataset = load_dataset(
  "lhoestq/custom_squad",
  revision="main"  # tag name, or branch name, or commit hash
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(
<span class="hljs-meta">&gt;&gt;&gt; </span>  <span class="hljs-string">&quot;lhoestq/custom_squad&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>  revision=<span class="hljs-string">&quot;main&quot;</span>  <span class="hljs-comment"># tag name, or branch name, or commit hash</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>)`}}),Es=new ha({props:{$$slots:{default:[Mh]},$$scope:{ctx:x}}}),ft=new D({props:{code:`data_files = {"train": "train.csv", "test": "test.csv"}
dataset = load_dataset("namespace/your_dataset_name", data_files=data_files)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>data_files = {<span class="hljs-string">&quot;train&quot;</span>: <span class="hljs-string">&quot;train.csv&quot;</span>, <span class="hljs-string">&quot;test&quot;</span>: <span class="hljs-string">&quot;test.csv&quot;</span>}
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;namespace/your_dataset_name&quot;</span>, data_files=data_files)`}}),qs=new ha({props:{warning:!0,$$slots:{default:[Vh]},$$scope:{ctx:x}}}),ht=new D({props:{code:`from datasets import load_dataset
c4_subset = load_dataset('allenai/c4', data_files='en/c4-train.0000*-of-01024.json.gz')`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>c4_subset = load_dataset(<span class="hljs-string">&#x27;allenai/c4&#x27;</span>, data_files=<span class="hljs-string">&#x27;en/c4-train.0000*-of-01024.json.gz&#x27;</span>)`}}),ut=new D({props:{code:`data_files = {"validation": "en/c4-validation.*.json.gz"}
c4_validation = load_dataset("allenai/c4", data_files=data_files, split="validation")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>data_files = {<span class="hljs-string">&quot;validation&quot;</span>: <span class="hljs-string">&quot;en/c4-validation.*.json.gz&quot;</span>}
<span class="hljs-meta">&gt;&gt;&gt; </span>c4_validation = load_dataset(<span class="hljs-string">&quot;allenai/c4&quot;</span>, data_files=data_files, split=<span class="hljs-string">&quot;validation&quot;</span>)`}}),mt=new I({}),Ss=new ha({props:{$$slots:{default:[zh]},$$scope:{ctx:x}}}),gt=new I({}),_t=new D({props:{code:`from datasets import load_dataset
dataset = load_dataset('csv', data_files='my_file.csv')`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;csv&#x27;</span>, data_files=<span class="hljs-string">&#x27;my_file.csv&#x27;</span>)`}}),vt=new D({props:{code:"dataset = load_dataset('csv', data_files=['my_file_1.csv', 'my_file_2.csv', 'my_file_3.csv'])",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;csv&#x27;</span>, data_files=[<span class="hljs-string">&#x27;my_file_1.csv&#x27;</span>, <span class="hljs-string">&#x27;my_file_2.csv&#x27;</span>, <span class="hljs-string">&#x27;my_file_3.csv&#x27;</span>])'}}),$t=new D({props:{code:"dataset = load_dataset('csv', data_files={'train': ['my_train_file_1.csv', 'my_train_file_2.csv'] 'test': 'my_test_file.csv'})",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;csv&#x27;</span>, data_files={<span class="hljs-string">&#x27;train&#x27;</span>: [<span class="hljs-string">&#x27;my_train_file_1.csv&#x27;</span>, <span class="hljs-string">&#x27;my_train_file_2.csv&#x27;</span>] <span class="hljs-string">&#x27;test&#x27;</span>: <span class="hljs-string">&#x27;my_test_file.csv&#x27;</span>})'}}),yt=new D({props:{code:`base_url = "https://huggingface.co/datasets/lhoestq/demo1/resolve/main/data/"
dataset = load_dataset('csv', data_files={'train': base_url + 'train.csv', 'test': base_url + 'test.csv'})`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>base_url = <span class="hljs-string">&quot;https://huggingface.co/datasets/lhoestq/demo1/resolve/main/data/&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;csv&#x27;</span>, data_files={<span class="hljs-string">&#x27;train&#x27;</span>: base_url + <span class="hljs-string">&#x27;train.csv&#x27;</span>, <span class="hljs-string">&#x27;test&#x27;</span>: base_url + <span class="hljs-string">&#x27;test.csv&#x27;</span>})`}}),wt=new D({props:{code:`url = "https://domain.org/train_data.zip"
data_files = {"train": url}
dataset = load_dataset("csv", data_files=data_files)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;https://domain.org/train_data.zip&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>data_files = {<span class="hljs-string">&quot;train&quot;</span>: url}
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;csv&quot;</span>, data_files=data_files)`}}),bt=new I({}),jt=new D({props:{code:`from datasets import load_dataset
dataset = load_dataset('json', data_files='my_file.json')`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;json&#x27;</span>, data_files=<span class="hljs-string">&#x27;my_file.json&#x27;</span>)`}}),xt=new D({props:{code:`{"a": 1, "b": 2.0, "c": "foo", "d": false}
{"a": 4, "b": -5.5, "c": null, "d": true}`,highlighted:`{<span class="hljs-string">&quot;a&quot;</span>: <span class="hljs-number">1</span>, <span class="hljs-string">&quot;b&quot;</span>: <span class="hljs-number">2.0</span>, <span class="hljs-string">&quot;c&quot;</span>: <span class="hljs-string">&quot;foo&quot;</span>, <span class="hljs-string">&quot;d&quot;</span>: false}
{<span class="hljs-string">&quot;a&quot;</span>: <span class="hljs-number">4</span>, <span class="hljs-string">&quot;b&quot;</span>: -<span class="hljs-number">5.5</span>, <span class="hljs-string">&quot;c&quot;</span>: null, <span class="hljs-string">&quot;d&quot;</span>: true}`}}),kt=new D({props:{code:`{"version": "0.1.0",
    "data": [{"a": 1, "b": 2.0, "c": "foo", "d": false},
            {"a": 4, "b": -5.5, "c": null, "d": true}]
}

>>> from datasets import load_dataset
>>> dataset = load_dataset('json', data_files='my_file.json', field='data')`,highlighted:`{<span class="hljs-string">&quot;version&quot;</span>: <span class="hljs-string">&quot;0.1.0&quot;</span>,
    <span class="hljs-string">&quot;data&quot;</span>: [{<span class="hljs-string">&quot;a&quot;</span>: <span class="hljs-number">1</span>, <span class="hljs-string">&quot;b&quot;</span>: <span class="hljs-number">2.0</span>, <span class="hljs-string">&quot;c&quot;</span>: <span class="hljs-string">&quot;foo&quot;</span>, <span class="hljs-string">&quot;d&quot;</span>: false},
            {<span class="hljs-string">&quot;a&quot;</span>: <span class="hljs-number">4</span>, <span class="hljs-string">&quot;b&quot;</span>: -<span class="hljs-number">5.5</span>, <span class="hljs-string">&quot;c&quot;</span>: null, <span class="hljs-string">&quot;d&quot;</span>: true}]
}

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;json&#x27;</span>, data_files=<span class="hljs-string">&#x27;my_file.json&#x27;</span>, field=<span class="hljs-string">&#x27;data&#x27;</span>)`}}),Et=new D({props:{code:`base_url = "https://rajpurkar.github.io/SQuAD-explorer/dataset/"
dataset = load_dataset('json', data_files={'train': base_url + 'train-v1.1.json', 'validation': base_url + 'dev-v1.1.json'}, field="data")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>base_url = <span class="hljs-string">&quot;https://rajpurkar.github.io/SQuAD-explorer/dataset/&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;json&#x27;</span>, data_files={<span class="hljs-string">&#x27;train&#x27;</span>: base_url + <span class="hljs-string">&#x27;train-v1.1.json&#x27;</span>, <span class="hljs-string">&#x27;validation&#x27;</span>: base_url + <span class="hljs-string">&#x27;dev-v1.1.json&#x27;</span>}, field=<span class="hljs-string">&quot;data&quot;</span>)`}}),qt=new I({}),Pt=new D({props:{code:`from datasets import load_dataset
dataset = load_dataset('text', data_files={'train': ['my_text_1.txt', 'my_text_2.txt'], 'test': 'my_test_file.txt'})`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;text&#x27;</span>, data_files={<span class="hljs-string">&#x27;train&#x27;</span>: [<span class="hljs-string">&#x27;my_text_1.txt&#x27;</span>, <span class="hljs-string">&#x27;my_text_2.txt&#x27;</span>], <span class="hljs-string">&#x27;test&#x27;</span>: <span class="hljs-string">&#x27;my_test_file.txt&#x27;</span>})`}}),At=new D({props:{code:"dataset = load_dataset('text', data_files='https://huggingface.co/datasets/lhoestq/test/resolve/main/some_text.txt')",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;text&#x27;</span>, data_files=<span class="hljs-string">&#x27;https://huggingface.co/datasets/lhoestq/test/resolve/main/some_text.txt&#x27;</span>)'}}),St=new I({}),Tt=new D({props:{code:`from datasets import load_dataset
dataset = load_dataset("parquet", data_files={'train': 'train.parquet', 'test': 'test.parquet'})`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;parquet&quot;</span>, data_files={<span class="hljs-string">&#x27;train&#x27;</span>: <span class="hljs-string">&#x27;train.parquet&#x27;</span>, <span class="hljs-string">&#x27;test&#x27;</span>: <span class="hljs-string">&#x27;test.parquet&#x27;</span>})`}}),Dt=new D({props:{code:`base_url = "https://storage.googleapis.com/huggingface-nlp/cache/datasets/wikipedia/20200501.en/1.0.0/"
data_files = {"train": base_url + "wikipedia-train.parquet"}
wiki = load_dataset("parquet", data_files=data_files, split="train")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>base_url = <span class="hljs-string">&quot;https://storage.googleapis.com/huggingface-nlp/cache/datasets/wikipedia/20200501.en/1.0.0/&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>data_files = {<span class="hljs-string">&quot;train&quot;</span>: base_url + <span class="hljs-string">&quot;wikipedia-train.parquet&quot;</span>}
<span class="hljs-meta">&gt;&gt;&gt; </span>wiki = load_dataset(<span class="hljs-string">&quot;parquet&quot;</span>, data_files=data_files, split=<span class="hljs-string">&quot;train&quot;</span>)`}}),Nt=new I({}),Ct=new I({}),It=new D({props:{code:`from datasets import Dataset
my_dict = {"a": [1, 2, 3]}
dataset = Dataset.from_dict(my_dict)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> Dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>my_dict = {<span class="hljs-string">&quot;a&quot;</span>: [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>]}
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = Dataset.from_dict(my_dict)`}}),Ot=new I({}),Ht=new D({props:{code:`from datasets import Dataset
import pandas as pd
df = pd.DataFrame({"a": [1, 2, 3]})
dataset = Dataset.from_pandas(df)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> Dataset
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd
<span class="hljs-meta">&gt;&gt;&gt; </span>df = pd.DataFrame({<span class="hljs-string">&quot;a&quot;</span>: [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>]})
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = Dataset.from_pandas(df)`}}),zs=new ha({props:{warning:!0,$$slots:{default:[Uh]},$$scope:{ctx:x}}}),Ft=new I({}),Lt=new I({}),Rt=new Nl({props:{group1:{id:"stringapi",code:"train_test_ds = datasets.load_dataset('bookcorpus', split='train+test')",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>train_test_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=<span class="hljs-string">&#x27;train+test&#x27;</span>)'},group2:{id:"readinstruction",code:`ri = datasets.ReadInstruction('train') + datasets.ReadInstruction('test')
train_test_ds = datasets.load_dataset('bookcorpus', split=ri)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>ri = datasets.ReadInstruction(<span class="hljs-string">&#x27;train&#x27;</span>) + datasets.ReadInstruction(<span class="hljs-string">&#x27;test&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>train_test_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=ri)`}}}),Mt=new Nl({props:{group1:{id:"stringapi",code:"train_10_20_ds = datasets.load_dataset('bookcorpus', split='train[10:20]')",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>train_10_20_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=<span class="hljs-string">&#x27;train[10:20]&#x27;</span>)'},group2:{id:"readinstruction",code:"train_10_20_ds = datasets.load_dataset('bookcorpus', split=datasets.ReadInstruction('train', from_=10, to=20, unit='abs'))",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>train_10_20_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=datasets.ReadInstruction(<span class="hljs-string">&#x27;train&#x27;</span>, from_=<span class="hljs-number">10</span>, to=<span class="hljs-number">20</span>, unit=<span class="hljs-string">&#x27;abs&#x27;</span>))'}}}),Vt=new Nl({props:{group1:{id:"stringapi",code:"train_10pct_ds = datasets.load_dataset('bookcorpus', split='train[:10%]')",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>train_10pct_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=<span class="hljs-string">&#x27;train[:10%]&#x27;</span>)'},group2:{id:"readinstruction",code:"train_10_20_ds = datasets.load_dataset('bookcorpus', split=datasets.ReadInstruction('train', to=10, unit='%'))",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>train_10_20_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=datasets.ReadInstruction(<span class="hljs-string">&#x27;train&#x27;</span>, to=<span class="hljs-number">10</span>, unit=<span class="hljs-string">&#x27;%&#x27;</span>))'}}}),zt=new Nl({props:{group1:{id:"stringapi",code:"train_10_80pct_ds = datasets.load_dataset('bookcorpus', split='train[:10%]+train[-80%:]')",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>train_10_80pct_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=<span class="hljs-string">&#x27;train[:10%]+train[-80%:]&#x27;</span>)'},group2:{id:"readinstruction",code:`ri = (datasets.ReadInstruction('train', to=10, unit='%') + datasets.ReadInstruction('train', from_=-80, unit='%'))
train_10_80pct_ds = datasets.load_dataset('bookcorpus', split=ri)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>ri = (datasets.ReadInstruction(<span class="hljs-string">&#x27;train&#x27;</span>, to=<span class="hljs-number">10</span>, unit=<span class="hljs-string">&#x27;%&#x27;</span>) + datasets.ReadInstruction(<span class="hljs-string">&#x27;train&#x27;</span>, from_=-<span class="hljs-number">80</span>, unit=<span class="hljs-string">&#x27;%&#x27;</span>))
<span class="hljs-meta">&gt;&gt;&gt; </span>train_10_80pct_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=ri)`}}}),Ut=new Nl({props:{group1:{id:"stringapi",code:`# 10-fold cross-validation (see also next section on rounding behavior):
# The validation datasets are each going to be 10%:
# [0%:10%], [10%:20%], ..., [90%:100%].
# And the training datasets are each going to be the complementary 90%:
# [10%:100%] (for a corresponding validation set of [0%:10%]),
# [0%:10%] + [20%:100%] (for a validation set of [10%:20%]), ...,
# [0%:90%] (for a validation set of [90%:100%]).
vals_ds = datasets.load_dataset('bookcorpus', split=[f'train[{k}%:{k+10}%]' for k in range(0, 100, 10)])
trains_ds = datasets.load_dataset('bookcorpus', split=[f'train[:{k}%]+train[{k+10}%:]' for k in range(0, 100, 10)])`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># 10-fold cross-validation (see also next section on rounding behavior):</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># The validation datasets are each going to be 10%:</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># [0%:10%], [10%:20%], ..., [90%:100%].</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># And the training datasets are each going to be the complementary 90%:</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># [10%:100%] (for a corresponding validation set of [0%:10%]),</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># [0%:10%] + [20%:100%] (for a validation set of [10%:20%]), ...,</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># [0%:90%] (for a validation set of [90%:100%]).</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>vals_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=[<span class="hljs-string">f&#x27;train[<span class="hljs-subst">{k}</span>%:<span class="hljs-subst">{k+<span class="hljs-number">10</span>}</span>%]&#x27;</span> <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, <span class="hljs-number">100</span>, <span class="hljs-number">10</span>)])
<span class="hljs-meta">&gt;&gt;&gt; </span>trains_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=[<span class="hljs-string">f&#x27;train[:<span class="hljs-subst">{k}</span>%]+train[<span class="hljs-subst">{k+<span class="hljs-number">10</span>}</span>%:]&#x27;</span> <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, <span class="hljs-number">100</span>, <span class="hljs-number">10</span>)])`},group2:{id:"readinstruction",code:`# 10-fold cross-validation (see also next section on rounding behavior):
# The validation datasets are each going to be 10%:
# [0%:10%], [10%:20%], ..., [90%:100%].
# And the training datasets are each going to be the complementary 90%:
# [10%:100%] (for a corresponding validation set of [0%:10%]),
# [0%:10%] + [20%:100%] (for a validation set of [10%:20%]), ...,
# [0%:90%] (for a validation set of [90%:100%]).
vals_ds = datasets.load_dataset('bookcorpus', [datasets.ReadInstruction('train', from_=k, to=k+10, unit='%') for k in range(0, 100, 10)])
trains_ds = datasets.load_dataset('bookcorpus', [(datasets.ReadInstruction('train', to=k, unit='%') + datasets.ReadInstruction('train', from_=k+10, unit='%')) for k in range(0, 100, 10)])`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># 10-fold cross-validation (see also next section on rounding behavior):</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># The validation datasets are each going to be 10%:</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># [0%:10%], [10%:20%], ..., [90%:100%].</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># And the training datasets are each going to be the complementary 90%:</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># [10%:100%] (for a corresponding validation set of [0%:10%]),</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># [0%:10%] + [20%:100%] (for a validation set of [10%:20%]), ...,</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># [0%:90%] (for a validation set of [90%:100%]).</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>vals_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, [datasets.ReadInstruction(<span class="hljs-string">&#x27;train&#x27;</span>, from_=k, to=k+<span class="hljs-number">10</span>, unit=<span class="hljs-string">&#x27;%&#x27;</span>) <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, <span class="hljs-number">100</span>, <span class="hljs-number">10</span>)])
<span class="hljs-meta">&gt;&gt;&gt; </span>trains_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, [(datasets.ReadInstruction(<span class="hljs-string">&#x27;train&#x27;</span>, to=k, unit=<span class="hljs-string">&#x27;%&#x27;</span>) + datasets.ReadInstruction(<span class="hljs-string">&#x27;train&#x27;</span>, from_=k+<span class="hljs-number">10</span>, unit=<span class="hljs-string">&#x27;%&#x27;</span>)) <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, <span class="hljs-number">100</span>, <span class="hljs-number">10</span>)])`}}}),Jt=new I({}),Bt=new D({props:{code:`# Assuming *train* split contains 999 records.
# 19 records, from 500 (included) to 519 (excluded).
>>> train_50_52_ds = datasets.load_dataset('bookcorpus', split='train[50%:52%]')
# 20 records, from 519 (included) to 539 (excluded).
>>> train_52_54_ds = datasets.load_dataset('bookcorpus', split='train[52%:54%]')`,highlighted:`<span class="hljs-comment"># Assuming *train* split contains 999 records.</span>
<span class="hljs-comment"># 19 records, from 500 (included) to 519 (excluded).</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>train_50_52_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=<span class="hljs-string">&#x27;train[50%:52%]&#x27;</span>)
<span class="hljs-comment"># 20 records, from 519 (included) to 539 (excluded).</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>train_52_54_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=<span class="hljs-string">&#x27;train[52%:54%]&#x27;</span>)`}}),Yt=new D({props:{code:`# 18 records, from 450 (included) to 468 (excluded).
>>> train_50_52pct1_ds = datasets.load_dataset('bookcorpus', split=datasets.ReadInstruction( 'train', from_=50, to=52, unit='%', rounding='pct1_dropremainder'))
# 18 records, from 468 (included) to 486 (excluded).
>>> train_52_54pct1_ds = datasets.load_dataset('bookcorpus', split=datasets.ReadInstruction('train',from_=52, to=54, unit='%', rounding='pct1_dropremainder'))
# Or equivalently:
>>> train_50_52pct1_ds = datasets.load_dataset('bookcorpus', split='train[50%:52%](pct1_dropremainder)')
>>> train_52_54pct1_ds = datasets.load_dataset('bookcorpus', split='train[52%:54%](pct1_dropremainder)')`,highlighted:`<span class="hljs-comment"># 18 records, from 450 (included) to 468 (excluded).</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>train_50_52pct1_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=datasets.ReadInstruction( <span class="hljs-string">&#x27;train&#x27;</span>, from_=<span class="hljs-number">50</span>, to=<span class="hljs-number">52</span>, unit=<span class="hljs-string">&#x27;%&#x27;</span>, rounding=<span class="hljs-string">&#x27;pct1_dropremainder&#x27;</span>))
<span class="hljs-comment"># 18 records, from 468 (included) to 486 (excluded).</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>train_52_54pct1_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=datasets.ReadInstruction(<span class="hljs-string">&#x27;train&#x27;</span>,from_=<span class="hljs-number">52</span>, to=<span class="hljs-number">54</span>, unit=<span class="hljs-string">&#x27;%&#x27;</span>, rounding=<span class="hljs-string">&#x27;pct1_dropremainder&#x27;</span>))
<span class="hljs-comment"># Or equivalently:</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>train_50_52pct1_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=<span class="hljs-string">&#x27;train[50%:52%](pct1_dropremainder)&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>train_52_54pct1_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=<span class="hljs-string">&#x27;train[52%:54%](pct1_dropremainder)&#x27;</span>)`}}),Gs=new ha({props:{warning:!0,$$slots:{default:[Jh]},$$scope:{ctx:x}}}),Wt=new I({}),Gt=new I({}),Kt=new D({props:{code:'dataset = load_dataset("matinf", "summarization")',highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;matinf&quot;</span>, <span class="hljs-string">&quot;summarization&quot;</span>)
Downloading <span class="hljs-keyword">and</span> preparing dataset matinf/summarization (download: Unknown size, generated: <span class="hljs-number">246.89</span> MiB, post-processed: Unknown size, total: <span class="hljs-number">246.89</span> MiB) to /root/.cache/huggingface/datasets/matinf/summarization/<span class="hljs-number">1.0</span><span class="hljs-number">.0</span>/82eee5e71c3ceaf20d909bca36ff237452b4e4ab195d3be7ee1c78b53e6f540e...
AssertionError: The dataset matinf <span class="hljs-keyword">with</span> config summarization requires manual data. 
Please follow the manual download instructions: To use MATINF you have to download it manually. Please fill this google form (https://forms.gle/nkH4LVE4iNQeDzsc9). You will receive a download link <span class="hljs-keyword">and</span> a password once you complete the form. Please extract <span class="hljs-built_in">all</span> files <span class="hljs-keyword">in</span> one folder <span class="hljs-keyword">and</span> load the dataset <span class="hljs-keyword">with</span>: *datasets.load_dataset(<span class="hljs-string">&#x27;matinf&#x27;</span>, data_dir=<span class="hljs-string">&#x27;path/to/folder/folder_name&#x27;</span>)*. 
Manual data can be loaded <span class="hljs-keyword">with</span> \`datasets.load_dataset(matinf, data_dir=<span class="hljs-string">&#x27;&lt;path/to/manual/data&gt;&#x27;</span>) `}}),Xt=new I({}),sa=new D({props:{code:`class_names = ["sadness", "joy", "love", "anger", "fear", "surprise"]
emotion_features = Features({'text': Value('string'), 'label': ClassLabel(names=class_names)})`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>class_names = [<span class="hljs-string">&quot;sadness&quot;</span>, <span class="hljs-string">&quot;joy&quot;</span>, <span class="hljs-string">&quot;love&quot;</span>, <span class="hljs-string">&quot;anger&quot;</span>, <span class="hljs-string">&quot;fear&quot;</span>, <span class="hljs-string">&quot;surprise&quot;</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>emotion_features = Features({<span class="hljs-string">&#x27;text&#x27;</span>: Value(<span class="hljs-string">&#x27;string&#x27;</span>), <span class="hljs-string">&#x27;label&#x27;</span>: ClassLabel(names=class_names)})`}}),ta=new D({props:{code:"dataset = load_dataset('csv', data_files=file_dict, delimiter=';', column_names=['text', 'label'], features=emotion_features)",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;csv&#x27;</span>, data_files=file_dict, delimiter=<span class="hljs-string">&#x27;;&#x27;</span>, column_names=[<span class="hljs-string">&#x27;text&#x27;</span>, <span class="hljs-string">&#x27;label&#x27;</span>], features=emotion_features)'}}),aa=new D({props:{code:"dataset['train'].features",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>dataset[<span class="hljs-string">&#x27;train&#x27;</span>].features
{<span class="hljs-string">&#x27;text&#x27;</span>: Value(dtype=<span class="hljs-string">&#x27;string&#x27;</span>, <span class="hljs-built_in">id</span>=<span class="hljs-literal">None</span>),
<span class="hljs-string">&#x27;label&#x27;</span>: ClassLabel(num_classes=<span class="hljs-number">6</span>, names=[<span class="hljs-string">&#x27;sadness&#x27;</span>, <span class="hljs-string">&#x27;joy&#x27;</span>, <span class="hljs-string">&#x27;love&#x27;</span>, <span class="hljs-string">&#x27;anger&#x27;</span>, <span class="hljs-string">&#x27;fear&#x27;</span>, <span class="hljs-string">&#x27;surprise&#x27;</span>], names_file=<span class="hljs-literal">None</span>, <span class="hljs-built_in">id</span>=<span class="hljs-literal">None</span>)}`}}),ea=new I({}),la=new D({props:{code:`from datasets import load_metric
metric = load_metric('PATH/TO/MY/METRIC/SCRIPT')

# Example of typical usage
for batch in dataset:
    inputs, references = batch
    predictions = model(inputs)
    metric.add_batch(predictions=predictions, references=references)
score = metric.compute()`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_metric
<span class="hljs-meta">&gt;&gt;&gt; </span>metric = load_metric(<span class="hljs-string">&#x27;PATH/TO/MY/METRIC/SCRIPT&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Example of typical usage</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">for</span> batch <span class="hljs-keyword">in</span> dataset:
<span class="hljs-meta">... </span>    inputs, references = batch
<span class="hljs-meta">... </span>    predictions = model(inputs)
<span class="hljs-meta">... </span>    metric.add_batch(predictions=predictions, references=references)
<span class="hljs-meta">&gt;&gt;&gt; </span>score = metric.compute()`}}),tt=new ha({props:{$$slots:{default:[Bh]},$$scope:{ctx:x}}}),oa=new I({}),na=new D({props:{code:`from datasets import load_metric
metric = load_metric('bleurt', name='bleurt-base-128')
metric = load_metric('bleurt', name='bleurt-base-512')`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python"><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_metric</span>
<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python">metric = load_metric(<span class="hljs-string">&#x27;bleurt&#x27;</span>, name=<span class="hljs-string">&#x27;bleurt-base-128&#x27;</span>)</span>
<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python">metric = load_metric(<span class="hljs-string">&#x27;bleurt&#x27;</span>, name=<span class="hljs-string">&#x27;bleurt-base-512&#x27;</span>)</span>`}}),ra=new I({}),da=new D({props:{code:`from datasets import load_metric
metric = load_metric('glue', 'mrpc', num_process=num_process, process_id=rank)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_metric
<span class="hljs-meta">&gt;&gt;&gt; </span>metric = load_metric(<span class="hljs-string">&#x27;glue&#x27;</span>, <span class="hljs-string">&#x27;mrpc&#x27;</span>, num_process=num_process, process_id=rank)`}}),lt=new ha({props:{$$slots:{default:[Yh]},$$scope:{ctx:x}}}),fa=new D({props:{code:`from datasets import load_metric
metric = load_metric('glue', 'mrpc', num_process=num_process, process_id=process_id, experiment_id="My_experiment_10")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_metric
<span class="hljs-meta">&gt;&gt;&gt; </span>metric = load_metric(<span class="hljs-string">&#x27;glue&#x27;</span>, <span class="hljs-string">&#x27;mrpc&#x27;</span>, num_process=num_process, process_id=process_id, experiment_id=<span class="hljs-string">&quot;My_experiment_10&quot;</span>)`}}),{c(){d=l("meta"),g=f(),u=l("h1"),y=l("a"),$=l("span"),E(_.$$.fragment),m=f(),w=l("span"),v=i("Load"),A=f(),S=l("p"),T=i("You have already seen how to load a dataset from the Hugging Face Hub. But datasets are stored in a variety of places, and sometimes you won\u2019t find the one you want on the Hub. A dataset can be on disk on your local machine, in a Github repository, and in in-memory data structures like Python dictionaries and Pandas DataFrames. Wherever your dataset may be stored, \u{1F917} Datasets provides a way for you to load and use it for training."),B=f(),C=l("p"),N=i("This guide will show you how to load a dataset from:"),U=f(),O=l("ul"),J=l("li"),H=i("The Hub without a dataset loading script"),ua=f(),ws=l("li"),ma=i("Local files"),ga=f(),bs=l("li"),wr=i("In-memory data"),br=f(),ue=l("li"),jr=i("Offline"),xr=f(),me=l("li"),kr=i("A specific slice of a split"),Il=f(),_a=l("p"),Er=i("You will also learn how to troubleshoot common errors, and how to load specific configurations of a metric."),Ol=f(),va=l("a"),Hl=f(),as=l("h2"),js=l("a"),ge=l("span"),E(rt.$$.fragment),qr=f(),_e=l("span"),Pr=i("Hugging Face Hub"),Fl=f(),xs=l("p"),Ar=i("In the tutorial, you learned how to load a dataset from the Hub. This method relies on a dataset loading script that downloads and builds the dataset. However, you can also load a dataset from any dataset repository on the Hub "),ve=l("strong"),Sr=i("without"),Tr=i(" a loading script!"),Ll=f(),Y=l("p"),Dr=i("First, create a dataset repository and upload your data files. Then you can use "),$a=l("a"),Nr=i("load_dataset()"),Cr=i(" like you learned in the tutorial. For example, load the files from this "),it=l("a"),Ir=i("demo repository"),Or=i(" by providing the repository namespace and dataset name:"),Rl=f(),E(pt.$$.fragment),Ml=f(),ya=l("p"),Hr=i("This dataset repository contains CSV files, and this code loads all the data from the CSV files."),Vl=f(),ks=l("p"),Fr=i("Some datasets may have more than one version, based on Git tags, branches or commits. Use the "),$e=l("code"),Lr=i("revision"),Rr=i(" flag to specify which dataset version you want to load:"),zl=f(),E(dt.$$.fragment),Ul=f(),E(Es.$$.fragment),Jl=f(),F=l("p"),Mr=i("If the dataset doesn\u2019t have a dataset loading script, then by default, all the data will be loaded in the "),ye=l("code"),Vr=i("train"),zr=i(" split. Use the "),we=l("code"),Ur=i("data_files"),Jr=i(" parameter to map data files to splits like "),be=l("code"),Br=i("train"),Yr=i(", "),je=l("code"),Wr=i("validation"),Gr=i(" and "),xe=l("code"),Qr=i("test"),Kr=i(":"),Bl=f(),E(ft.$$.fragment),Yl=f(),E(qs.$$.fragment),Wl=f(),W=l("p"),Xr=i("You can also load a specific subset of the files with the "),ke=l("code"),Zr=i("data_files"),si=i(" parameter. The example below loads files from the "),ct=l("a"),ti=i("C4 dataset"),ai=i(":"),Gl=f(),E(ht.$$.fragment),Ql=f(),Ps=l("p"),ei=i("Specify a custom split with the "),Ee=l("code"),li=i("split"),oi=i(" parameter:"),Kl=f(),E(ut.$$.fragment),Xl=f(),es=l("h2"),As=l("a"),qe=l("span"),E(mt.$$.fragment),ni=f(),Pe=l("span"),ri=i("Local and remote files"),Zl=f(),L=l("p"),ii=i("\u{1F917} Datasets can be loaded from local files stored on your computer, and also from remote files. The datasets are most likely stored as a "),Ae=l("code"),pi=i("csv"),di=i(", "),Se=l("code"),fi=i("json"),ci=i(", "),Te=l("code"),hi=i("txt"),ui=i(" or "),De=l("code"),mi=i("parquet"),gi=i(" file. The "),wa=l("a"),_i=i("load_dataset()"),vi=i(" method is able to load each of these file types."),so=f(),E(Ss.$$.fragment),to=f(),ls=l("h3"),Ts=l("a"),Ne=l("span"),E(gt.$$.fragment),$i=f(),Ce=l("span"),yi=i("CSV"),ao=f(),ba=l("p"),wi=i("\u{1F917} Datasets can read a dataset made up of one or several CSV files:"),eo=f(),E(_t.$$.fragment),lo=f(),ja=l("p"),bi=i("If you have more than one CSV file:"),oo=f(),E(vt.$$.fragment),no=f(),xa=l("p"),ji=i("You can also map the training and test splits to specific CSV files:"),ro=f(),E($t.$$.fragment),io=f(),ka=l("p"),xi=i("To load remote CSV files via HTTP, you can pass the URLs:"),po=f(),E(yt.$$.fragment),fo=f(),Ea=l("p"),ki=i("To load zipped CSV files:"),co=f(),E(wt.$$.fragment),ho=f(),os=l("h3"),Ds=l("a"),Ie=l("span"),E(bt.$$.fragment),Ei=f(),Oe=l("span"),qi=i("JSON"),uo=f(),Ns=l("p"),Pi=i("JSON files are loaded directly with "),qa=l("a"),Ai=i("load_dataset()"),Si=i(" as shown below:"),mo=f(),E(jt.$$.fragment),go=f(),Pa=l("p"),Ti=i("JSON files can have diverse formats, but we think the most efficient format is to have multiple JSON objects; each line represents an individual row of data. For example:"),_o=f(),E(xt.$$.fragment),vo=f(),Cs=l("p"),Di=i("Another JSON format you may encounter is a nested field, in which case you will need to specify the "),He=l("code"),Ni=i("field"),Ci=i(" argument as shown in the following:"),$o=f(),E(kt.$$.fragment),yo=f(),Aa=l("p"),Ii=i("To load remote JSON files via HTTP, you can pass the URLs:"),wo=f(),E(Et.$$.fragment),bo=f(),Sa=l("p"),Oi=i("While these are the most common JSON formats, you will see other datasets that are formatted differently. \u{1F917} Datasets recognizes these other formats, and will fallback accordingly on the Python JSON loading methods to handle them."),jo=f(),ns=l("h3"),Is=l("a"),Fe=l("span"),E(qt.$$.fragment),Hi=f(),Le=l("span"),Fi=i("Text files"),xo=f(),Ta=l("p"),Li=i("Text files are one of the most common file types for storing a dataset. \u{1F917} Datasets will read the text file line by line to build the dataset."),ko=f(),E(Pt.$$.fragment),Eo=f(),Da=l("p"),Ri=i("To load remote TXT files via HTTP, you can pass the URLs:"),qo=f(),E(At.$$.fragment),Po=f(),rs=l("h3"),Os=l("a"),Re=l("span"),E(St.$$.fragment),Mi=f(),Me=l("span"),Vi=i("Parquet"),Ao=f(),Na=l("p"),zi=i("Parquet files are stored in a columnar format unlike row-based files like CSV. Large datasets may be stored in a Parquet file because it is more efficient, and faster at returning your query. Load a Parquet file as shown in the following example:"),So=f(),E(Tt.$$.fragment),To=f(),Ca=l("p"),Ui=i("To load remote parquet files via HTTP, you can pass the URLs:"),Do=f(),E(Dt.$$.fragment),No=f(),is=l("h2"),Hs=l("a"),Ve=l("span"),E(Nt.$$.fragment),Ji=f(),ze=l("span"),Bi=i("In-memory data"),Co=f(),Fs=l("p"),Yi=i("\u{1F917} Datasets will also allow you to create a "),Ia=l("a"),Wi=i("Dataset"),Gi=i(" directly from in-memory data structures like Python dictionaries and Pandas DataFrames."),Io=f(),ps=l("h3"),Ls=l("a"),Ue=l("span"),E(Ct.$$.fragment),Qi=f(),Je=l("span"),Ki=i("Python dictionary"),Oo=f(),Rs=l("p"),Xi=i("Load Python dictionaries with "),Oa=l("a"),Zi=i("Dataset.from_dict()"),sp=i(":"),Ho=f(),E(It.$$.fragment),Fo=f(),ds=l("h3"),Ms=l("a"),Be=l("span"),E(Ot.$$.fragment),tp=f(),Ye=l("span"),ap=i("Pandas DataFrame"),Lo=f(),Vs=l("p"),ep=i("Load Pandas DataFrames with "),Ha=l("a"),lp=i("Dataset.from_pandas()"),op=i(":"),Ro=f(),E(Ht.$$.fragment),Mo=f(),E(zs.$$.fragment),Vo=f(),fs=l("h2"),Us=l("a"),We=l("span"),E(Ft.$$.fragment),np=f(),Ge=l("span"),rp=i("Offline"),zo=f(),Fa=l("p"),ip=i("Even if you don\u2019t have an internet connection, it is still possible to load a dataset. As long as you\u2019ve downloaded a dataset from the Hub or \u{1F917} Datasets GitHub repository before, it should be cached. This means you can reload the dataset from the cache and use it offline."),Uo=f(),G=l("p"),pp=i("If you know you won\u2019t have internet access, you can run \u{1F917} Datasets in full offline mode. This saves time because instead of waiting for the Dataset builder download to time out, \u{1F917} Datasets will look directly in the cache. Set the environment variable "),Qe=l("code"),dp=i("HF_DATASETS_OFFLINE"),fp=i(" to "),Ke=l("code"),cp=i("1"),hp=i(" to enable full offline mode."),Jo=f(),cs=l("h2"),Js=l("a"),Xe=l("span"),E(Lt.$$.fragment),up=f(),Ze=l("span"),mp=i("Slice splits"),Bo=f(),Q=l("p"),gp=i("For even greater control over how to load a split, you can choose to only load specific slices of a split. There are two options for slicing a split: using strings or "),La=l("a"),_p=i("ReadInstruction"),vp=i(". Strings are more compact and readable for simple cases, while "),Ra=l("a"),$p=i("ReadInstruction"),yp=i(" is easier to use with variable slicing parameters."),Yo=f(),K=l("p"),wp=i("Concatenate the "),sl=l("code"),bp=i("train"),jp=i(" and "),tl=l("code"),xp=i("test"),kp=i(" split by:"),Wo=f(),E(Rt.$$.fragment),Go=f(),Bs=l("p"),Ep=i("Select specific rows of the "),al=l("code"),qp=i("train"),Pp=i(" split:"),Qo=f(),E(Mt.$$.fragment),Ko=f(),Ma=l("p"),Ap=i("Or select a percentage of the split with:"),Xo=f(),E(Vt.$$.fragment),Zo=f(),Va=l("p"),Sp=i("You can even select a combination of percentages from each split:"),sn=f(),E(zt.$$.fragment),tn=f(),za=l("p"),Tp=i("Finally, create cross-validated dataset splits by:"),an=f(),E(Ut.$$.fragment),en=f(),hs=l("h3"),Ys=l("a"),el=l("span"),E(Jt.$$.fragment),Dp=f(),ll=l("span"),Np=i("Percent slicing and rounding"),ln=f(),Ua=l("p"),Cp=i("For datasets where the requested slice boundaries do not divide evenly by 100, the default behavior is to round the boundaries to the nearest integer. As a result, some slices may contain more examples than others as shown in the following example:"),on=f(),E(Bt.$$.fragment),nn=f(),Ws=l("p"),Ip=i("If you want equal sized splits, use "),ol=l("code"),Op=i("pct1_dropremainder"),Hp=i(" rounding instead. This will treat the specified percentage boundaries as multiples of 1%."),rn=f(),E(Yt.$$.fragment),pn=f(),E(Gs.$$.fragment),dn=f(),Ja=l("a"),fn=f(),us=l("h2"),Qs=l("a"),nl=l("span"),E(Wt.$$.fragment),Fp=f(),rl=l("span"),Lp=i("Troubleshooting"),cn=f(),Ba=l("p"),Rp=i("Sometimes, you may get unexpected results when you load a dataset. In this section, you will learn how to solve two common issues you may encounter when you load a dataset: manually download a dataset, and specify features of a dataset."),hn=f(),ms=l("h3"),Ks=l("a"),il=l("span"),E(Gt.$$.fragment),Mp=f(),pl=l("span"),Vp=i("Manual download"),un=f(),M=l("p"),zp=i("Certain datasets require you to manually download the dataset files due to licensing incompatibility, or if the files are hidden behind a login page. This will cause "),Ya=l("a"),Up=i("load_dataset()"),Jp=i(" to throw an "),dl=l("code"),Bp=i("AssertionError"),Yp=i(". But \u{1F917} Datasets provides detailed instructions for downloading the missing files. After you have downloaded the files, use the "),fl=l("code"),Wp=i("data_dir"),Gp=i(" argument to specify the path to the files you just downloaded."),mn=f(),Xs=l("p"),Qp=i("For example, if you try to download a configuration from the "),Qt=l("a"),Kp=i("MATINF"),Xp=i(" dataset:"),gn=f(),E(Kt.$$.fragment),_n=f(),gs=l("h3"),Zs=l("a"),cl=l("span"),E(Xt.$$.fragment),Zp=f(),hl=l("span"),sd=i("Specify features"),vn=f(),X=l("p"),td=i("When you create a dataset from local files, the "),Wa=l("a"),ad=i("Features"),ed=i(" are automatically inferred by "),Zt=l("a"),ld=i("Apache Arrow"),od=i(". However, the features of the dataset may not always align with your expectations or you may want to define the features yourself."),$n=f(),Z=l("p"),nd=i("The following example shows how you can add custom labels with "),Ga=l("a"),rd=i("ClassLabel"),id=i(". First, define your own labels using the "),Qa=l("a"),pd=i("Features"),dd=i(" class:"),yn=f(),E(sa.$$.fragment),wn=f(),ss=l("p"),fd=i("Next, specify the "),ul=l("code"),cd=i("features"),hd=i(" argument in "),Ka=l("a"),ud=i("load_dataset()"),md=i(" with the features you just created:"),bn=f(),E(ta.$$.fragment),jn=f(),Xa=l("p"),gd=i("Now when you look at your dataset features, you can see it uses the custom labels you defined:"),xn=f(),E(aa.$$.fragment),kn=f(),_s=l("h2"),st=l("a"),ml=l("span"),E(ea.$$.fragment),_d=f(),gl=l("span"),vd=i("Metrics"),En=f(),Za=l("p"),$d=i("When the metric you want to use is not supported by \u{1F917} Datasets, you can write and use your own metric script. Load your metric by providing the path to your local metric loading script:"),qn=f(),E(la.$$.fragment),Pn=f(),E(tt.$$.fragment),An=f(),vs=l("h3"),at=l("a"),_l=l("span"),E(oa.$$.fragment),yd=f(),vl=l("span"),wd=i("Load configurations"),Sn=f(),se=l("p"),bd=i("It is possible for a metric to have different configurations. The configurations are stored in the [\u2018datasets.Metric.config_name\u2019] attribute. When you load a metric, provide the configuration name as shown in the following:"),Tn=f(),E(na.$$.fragment),Dn=f(),$s=l("h3"),et=l("a"),$l=l("span"),E(ra.$$.fragment),jd=f(),yl=l("span"),xd=i("Distributed setup"),Nn=f(),te=l("p"),kd=i("When you work in a distributed or parallel processing environment, loading and computing a metric can be tricky because these processes are executed in parallel on separate subsets of the data. \u{1F917} Datasets supports distributed usage with a few additional arguments when you load a metric."),Cn=f(),ae=l("p"),Ed=i("For example, imagine you are training and evaluating on eight parallel processes. Here\u2019s how you would load a metric in this distributed setting:"),In=f(),ts=l("ol"),wl=l("li"),ia=l("p"),qd=i("Define the total number of processes with the "),bl=l("code"),Pd=i("num_process"),Ad=i(" argument."),Sd=f(),jl=l("li"),ys=l("p"),Td=i("Set the process "),xl=l("code"),Dd=i("rank"),Nd=i(" as an integer between zero and "),kl=l("code"),Cd=i("num_process - 1"),Id=i("."),Od=f(),El=l("li"),pa=l("p"),Hd=i("Load your metric with "),ee=l("a"),Fd=i("load_metric()"),Ld=i(" with these arguments:"),On=f(),E(da.$$.fragment),Hn=f(),E(lt.$$.fragment),Fn=f(),ot=l("p"),Rd=i("In some instances, you may be simultaneously running multiple independent distributed evaluations on the same server and files. To avoid any conflicts, it is important to provide an "),ql=l("code"),Md=i("experiment_id"),Vd=i(" to distinguish the separate evaluations:"),Ln=f(),E(fa.$$.fragment),this.h()},l(s){const e=Th('[data-svelte="svelte-1phssyn"]',document.head);d=o(e,"META",{name:!0,content:!0}),e.forEach(t),g=c(s),u=o(s,"H1",{class:!0});var ca=n(u);y=o(ca,"A",{id:!0,class:!0,href:!0});var Pl=n(y);$=o(Pl,"SPAN",{});var Al=n($);P(_.$$.fragment,Al),Al.forEach(t),Pl.forEach(t),m=c(ca),w=o(ca,"SPAN",{});var Sl=n(w);v=p(Sl,"Load"),Sl.forEach(t),ca.forEach(t),A=c(s),S=o(s,"P",{});var Tl=n(S);T=p(Tl,"You have already seen how to load a dataset from the Hugging Face Hub. But datasets are stored in a variety of places, and sometimes you won\u2019t find the one you want on the Hub. A dataset can be on disk on your local machine, in a Github repository, and in in-memory data structures like Python dictionaries and Pandas DataFrames. Wherever your dataset may be stored, \u{1F917} Datasets provides a way for you to load and use it for training."),Tl.forEach(t),B=c(s),C=o(s,"P",{});var Dl=n(C);N=p(Dl,"This guide will show you how to load a dataset from:"),Dl.forEach(t),U=c(s),O=o(s,"UL",{});var R=n(O);J=o(R,"LI",{});var Bd=n(J);H=p(Bd,"The Hub without a dataset loading script"),Bd.forEach(t),ua=c(R),ws=o(R,"LI",{});var Yd=n(ws);ma=p(Yd,"Local files"),Yd.forEach(t),ga=c(R),bs=o(R,"LI",{});var Wd=n(bs);wr=p(Wd,"In-memory data"),Wd.forEach(t),br=c(R),ue=o(R,"LI",{});var Gd=n(ue);jr=p(Gd,"Offline"),Gd.forEach(t),xr=c(R),me=o(R,"LI",{});var Qd=n(me);kr=p(Qd,"A specific slice of a split"),Qd.forEach(t),R.forEach(t),Il=c(s),_a=o(s,"P",{});var Kd=n(_a);Er=p(Kd,"You will also learn how to troubleshoot common errors, and how to load specific configurations of a metric."),Kd.forEach(t),Ol=c(s),va=o(s,"A",{id:!0}),n(va).forEach(t),Hl=c(s),as=o(s,"H2",{class:!0});var Mn=n(as);js=o(Mn,"A",{id:!0,class:!0,href:!0});var Xd=n(js);ge=o(Xd,"SPAN",{});var Zd=n(ge);P(rt.$$.fragment,Zd),Zd.forEach(t),Xd.forEach(t),qr=c(Mn),_e=o(Mn,"SPAN",{});var sf=n(_e);Pr=p(sf,"Hugging Face Hub"),sf.forEach(t),Mn.forEach(t),Fl=c(s),xs=o(s,"P",{});var Vn=n(xs);Ar=p(Vn,"In the tutorial, you learned how to load a dataset from the Hub. This method relies on a dataset loading script that downloads and builds the dataset. However, you can also load a dataset from any dataset repository on the Hub "),ve=o(Vn,"STRONG",{});var tf=n(ve);Sr=p(tf,"without"),tf.forEach(t),Tr=p(Vn," a loading script!"),Vn.forEach(t),Ll=c(s),Y=o(s,"P",{});var le=n(Y);Dr=p(le,"First, create a dataset repository and upload your data files. Then you can use "),$a=o(le,"A",{href:!0});var af=n($a);Nr=p(af,"load_dataset()"),af.forEach(t),Cr=p(le," like you learned in the tutorial. For example, load the files from this "),it=o(le,"A",{href:!0,rel:!0});var ef=n(it);Ir=p(ef,"demo repository"),ef.forEach(t),Or=p(le," by providing the repository namespace and dataset name:"),le.forEach(t),Rl=c(s),P(pt.$$.fragment,s),Ml=c(s),ya=o(s,"P",{});var lf=n(ya);Hr=p(lf,"This dataset repository contains CSV files, and this code loads all the data from the CSV files."),lf.forEach(t),Vl=c(s),ks=o(s,"P",{});var zn=n(ks);Fr=p(zn,"Some datasets may have more than one version, based on Git tags, branches or commits. Use the "),$e=o(zn,"CODE",{});var of=n($e);Lr=p(of,"revision"),of.forEach(t),Rr=p(zn," flag to specify which dataset version you want to load:"),zn.forEach(t),zl=c(s),P(dt.$$.fragment,s),Ul=c(s),P(Es.$$.fragment,s),Jl=c(s),F=o(s,"P",{});var V=n(F);Mr=p(V,"If the dataset doesn\u2019t have a dataset loading script, then by default, all the data will be loaded in the "),ye=o(V,"CODE",{});var nf=n(ye);Vr=p(nf,"train"),nf.forEach(t),zr=p(V," split. Use the "),we=o(V,"CODE",{});var rf=n(we);Ur=p(rf,"data_files"),rf.forEach(t),Jr=p(V," parameter to map data files to splits like "),be=o(V,"CODE",{});var pf=n(be);Br=p(pf,"train"),pf.forEach(t),Yr=p(V,", "),je=o(V,"CODE",{});var df=n(je);Wr=p(df,"validation"),df.forEach(t),Gr=p(V," and "),xe=o(V,"CODE",{});var ff=n(xe);Qr=p(ff,"test"),ff.forEach(t),Kr=p(V,":"),V.forEach(t),Bl=c(s),P(ft.$$.fragment,s),Yl=c(s),P(qs.$$.fragment,s),Wl=c(s),W=o(s,"P",{});var oe=n(W);Xr=p(oe,"You can also load a specific subset of the files with the "),ke=o(oe,"CODE",{});var cf=n(ke);Zr=p(cf,"data_files"),cf.forEach(t),si=p(oe," parameter. The example below loads files from the "),ct=o(oe,"A",{href:!0,rel:!0});var hf=n(ct);ti=p(hf,"C4 dataset"),hf.forEach(t),ai=p(oe,":"),oe.forEach(t),Gl=c(s),P(ht.$$.fragment,s),Ql=c(s),Ps=o(s,"P",{});var Un=n(Ps);ei=p(Un,"Specify a custom split with the "),Ee=o(Un,"CODE",{});var uf=n(Ee);li=p(uf,"split"),uf.forEach(t),oi=p(Un," parameter:"),Un.forEach(t),Kl=c(s),P(ut.$$.fragment,s),Xl=c(s),es=o(s,"H2",{class:!0});var Jn=n(es);As=o(Jn,"A",{id:!0,class:!0,href:!0});var mf=n(As);qe=o(mf,"SPAN",{});var gf=n(qe);P(mt.$$.fragment,gf),gf.forEach(t),mf.forEach(t),ni=c(Jn),Pe=o(Jn,"SPAN",{});var _f=n(Pe);ri=p(_f,"Local and remote files"),_f.forEach(t),Jn.forEach(t),Zl=c(s),L=o(s,"P",{});var z=n(L);ii=p(z,"\u{1F917} Datasets can be loaded from local files stored on your computer, and also from remote files. The datasets are most likely stored as a "),Ae=o(z,"CODE",{});var vf=n(Ae);pi=p(vf,"csv"),vf.forEach(t),di=p(z,", "),Se=o(z,"CODE",{});var $f=n(Se);fi=p($f,"json"),$f.forEach(t),ci=p(z,", "),Te=o(z,"CODE",{});var yf=n(Te);hi=p(yf,"txt"),yf.forEach(t),ui=p(z," or "),De=o(z,"CODE",{});var wf=n(De);mi=p(wf,"parquet"),wf.forEach(t),gi=p(z," file. The "),wa=o(z,"A",{href:!0});var bf=n(wa);_i=p(bf,"load_dataset()"),bf.forEach(t),vi=p(z," method is able to load each of these file types."),z.forEach(t),so=c(s),P(Ss.$$.fragment,s),to=c(s),ls=o(s,"H3",{class:!0});var Bn=n(ls);Ts=o(Bn,"A",{id:!0,class:!0,href:!0});var jf=n(Ts);Ne=o(jf,"SPAN",{});var xf=n(Ne);P(gt.$$.fragment,xf),xf.forEach(t),jf.forEach(t),$i=c(Bn),Ce=o(Bn,"SPAN",{});var kf=n(Ce);yi=p(kf,"CSV"),kf.forEach(t),Bn.forEach(t),ao=c(s),ba=o(s,"P",{});var Ef=n(ba);wi=p(Ef,"\u{1F917} Datasets can read a dataset made up of one or several CSV files:"),Ef.forEach(t),eo=c(s),P(_t.$$.fragment,s),lo=c(s),ja=o(s,"P",{});var qf=n(ja);bi=p(qf,"If you have more than one CSV file:"),qf.forEach(t),oo=c(s),P(vt.$$.fragment,s),no=c(s),xa=o(s,"P",{});var Pf=n(xa);ji=p(Pf,"You can also map the training and test splits to specific CSV files:"),Pf.forEach(t),ro=c(s),P($t.$$.fragment,s),io=c(s),ka=o(s,"P",{});var Af=n(ka);xi=p(Af,"To load remote CSV files via HTTP, you can pass the URLs:"),Af.forEach(t),po=c(s),P(yt.$$.fragment,s),fo=c(s),Ea=o(s,"P",{});var Sf=n(Ea);ki=p(Sf,"To load zipped CSV files:"),Sf.forEach(t),co=c(s),P(wt.$$.fragment,s),ho=c(s),os=o(s,"H3",{class:!0});var Yn=n(os);Ds=o(Yn,"A",{id:!0,class:!0,href:!0});var Tf=n(Ds);Ie=o(Tf,"SPAN",{});var Df=n(Ie);P(bt.$$.fragment,Df),Df.forEach(t),Tf.forEach(t),Ei=c(Yn),Oe=o(Yn,"SPAN",{});var Nf=n(Oe);qi=p(Nf,"JSON"),Nf.forEach(t),Yn.forEach(t),uo=c(s),Ns=o(s,"P",{});var Wn=n(Ns);Pi=p(Wn,"JSON files are loaded directly with "),qa=o(Wn,"A",{href:!0});var Cf=n(qa);Ai=p(Cf,"load_dataset()"),Cf.forEach(t),Si=p(Wn," as shown below:"),Wn.forEach(t),mo=c(s),P(jt.$$.fragment,s),go=c(s),Pa=o(s,"P",{});var If=n(Pa);Ti=p(If,"JSON files can have diverse formats, but we think the most efficient format is to have multiple JSON objects; each line represents an individual row of data. For example:"),If.forEach(t),_o=c(s),P(xt.$$.fragment,s),vo=c(s),Cs=o(s,"P",{});var Gn=n(Cs);Di=p(Gn,"Another JSON format you may encounter is a nested field, in which case you will need to specify the "),He=o(Gn,"CODE",{});var Of=n(He);Ni=p(Of,"field"),Of.forEach(t),Ci=p(Gn," argument as shown in the following:"),Gn.forEach(t),$o=c(s),P(kt.$$.fragment,s),yo=c(s),Aa=o(s,"P",{});var Hf=n(Aa);Ii=p(Hf,"To load remote JSON files via HTTP, you can pass the URLs:"),Hf.forEach(t),wo=c(s),P(Et.$$.fragment,s),bo=c(s),Sa=o(s,"P",{});var Ff=n(Sa);Oi=p(Ff,"While these are the most common JSON formats, you will see other datasets that are formatted differently. \u{1F917} Datasets recognizes these other formats, and will fallback accordingly on the Python JSON loading methods to handle them."),Ff.forEach(t),jo=c(s),ns=o(s,"H3",{class:!0});var Qn=n(ns);Is=o(Qn,"A",{id:!0,class:!0,href:!0});var Lf=n(Is);Fe=o(Lf,"SPAN",{});var Rf=n(Fe);P(qt.$$.fragment,Rf),Rf.forEach(t),Lf.forEach(t),Hi=c(Qn),Le=o(Qn,"SPAN",{});var Mf=n(Le);Fi=p(Mf,"Text files"),Mf.forEach(t),Qn.forEach(t),xo=c(s),Ta=o(s,"P",{});var Vf=n(Ta);Li=p(Vf,"Text files are one of the most common file types for storing a dataset. \u{1F917} Datasets will read the text file line by line to build the dataset."),Vf.forEach(t),ko=c(s),P(Pt.$$.fragment,s),Eo=c(s),Da=o(s,"P",{});var zf=n(Da);Ri=p(zf,"To load remote TXT files via HTTP, you can pass the URLs:"),zf.forEach(t),qo=c(s),P(At.$$.fragment,s),Po=c(s),rs=o(s,"H3",{class:!0});var Kn=n(rs);Os=o(Kn,"A",{id:!0,class:!0,href:!0});var Uf=n(Os);Re=o(Uf,"SPAN",{});var Jf=n(Re);P(St.$$.fragment,Jf),Jf.forEach(t),Uf.forEach(t),Mi=c(Kn),Me=o(Kn,"SPAN",{});var Bf=n(Me);Vi=p(Bf,"Parquet"),Bf.forEach(t),Kn.forEach(t),Ao=c(s),Na=o(s,"P",{});var Yf=n(Na);zi=p(Yf,"Parquet files are stored in a columnar format unlike row-based files like CSV. Large datasets may be stored in a Parquet file because it is more efficient, and faster at returning your query. Load a Parquet file as shown in the following example:"),Yf.forEach(t),So=c(s),P(Tt.$$.fragment,s),To=c(s),Ca=o(s,"P",{});var Wf=n(Ca);Ui=p(Wf,"To load remote parquet files via HTTP, you can pass the URLs:"),Wf.forEach(t),Do=c(s),P(Dt.$$.fragment,s),No=c(s),is=o(s,"H2",{class:!0});var Xn=n(is);Hs=o(Xn,"A",{id:!0,class:!0,href:!0});var Gf=n(Hs);Ve=o(Gf,"SPAN",{});var Qf=n(Ve);P(Nt.$$.fragment,Qf),Qf.forEach(t),Gf.forEach(t),Ji=c(Xn),ze=o(Xn,"SPAN",{});var Kf=n(ze);Bi=p(Kf,"In-memory data"),Kf.forEach(t),Xn.forEach(t),Co=c(s),Fs=o(s,"P",{});var Zn=n(Fs);Yi=p(Zn,"\u{1F917} Datasets will also allow you to create a "),Ia=o(Zn,"A",{href:!0});var Xf=n(Ia);Wi=p(Xf,"Dataset"),Xf.forEach(t),Gi=p(Zn," directly from in-memory data structures like Python dictionaries and Pandas DataFrames."),Zn.forEach(t),Io=c(s),ps=o(s,"H3",{class:!0});var sr=n(ps);Ls=o(sr,"A",{id:!0,class:!0,href:!0});var Zf=n(Ls);Ue=o(Zf,"SPAN",{});var sc=n(Ue);P(Ct.$$.fragment,sc),sc.forEach(t),Zf.forEach(t),Qi=c(sr),Je=o(sr,"SPAN",{});var tc=n(Je);Ki=p(tc,"Python dictionary"),tc.forEach(t),sr.forEach(t),Oo=c(s),Rs=o(s,"P",{});var tr=n(Rs);Xi=p(tr,"Load Python dictionaries with "),Oa=o(tr,"A",{href:!0});var ac=n(Oa);Zi=p(ac,"Dataset.from_dict()"),ac.forEach(t),sp=p(tr,":"),tr.forEach(t),Ho=c(s),P(It.$$.fragment,s),Fo=c(s),ds=o(s,"H3",{class:!0});var ar=n(ds);Ms=o(ar,"A",{id:!0,class:!0,href:!0});var ec=n(Ms);Be=o(ec,"SPAN",{});var lc=n(Be);P(Ot.$$.fragment,lc),lc.forEach(t),ec.forEach(t),tp=c(ar),Ye=o(ar,"SPAN",{});var oc=n(Ye);ap=p(oc,"Pandas DataFrame"),oc.forEach(t),ar.forEach(t),Lo=c(s),Vs=o(s,"P",{});var er=n(Vs);ep=p(er,"Load Pandas DataFrames with "),Ha=o(er,"A",{href:!0});var nc=n(Ha);lp=p(nc,"Dataset.from_pandas()"),nc.forEach(t),op=p(er,":"),er.forEach(t),Ro=c(s),P(Ht.$$.fragment,s),Mo=c(s),P(zs.$$.fragment,s),Vo=c(s),fs=o(s,"H2",{class:!0});var lr=n(fs);Us=o(lr,"A",{id:!0,class:!0,href:!0});var rc=n(Us);We=o(rc,"SPAN",{});var ic=n(We);P(Ft.$$.fragment,ic),ic.forEach(t),rc.forEach(t),np=c(lr),Ge=o(lr,"SPAN",{});var pc=n(Ge);rp=p(pc,"Offline"),pc.forEach(t),lr.forEach(t),zo=c(s),Fa=o(s,"P",{});var dc=n(Fa);ip=p(dc,"Even if you don\u2019t have an internet connection, it is still possible to load a dataset. As long as you\u2019ve downloaded a dataset from the Hub or \u{1F917} Datasets GitHub repository before, it should be cached. This means you can reload the dataset from the cache and use it offline."),dc.forEach(t),Uo=c(s),G=o(s,"P",{});var ne=n(G);pp=p(ne,"If you know you won\u2019t have internet access, you can run \u{1F917} Datasets in full offline mode. This saves time because instead of waiting for the Dataset builder download to time out, \u{1F917} Datasets will look directly in the cache. Set the environment variable "),Qe=o(ne,"CODE",{});var fc=n(Qe);dp=p(fc,"HF_DATASETS_OFFLINE"),fc.forEach(t),fp=p(ne," to "),Ke=o(ne,"CODE",{});var cc=n(Ke);cp=p(cc,"1"),cc.forEach(t),hp=p(ne," to enable full offline mode."),ne.forEach(t),Jo=c(s),cs=o(s,"H2",{class:!0});var or=n(cs);Js=o(or,"A",{id:!0,class:!0,href:!0});var hc=n(Js);Xe=o(hc,"SPAN",{});var uc=n(Xe);P(Lt.$$.fragment,uc),uc.forEach(t),hc.forEach(t),up=c(or),Ze=o(or,"SPAN",{});var mc=n(Ze);mp=p(mc,"Slice splits"),mc.forEach(t),or.forEach(t),Bo=c(s),Q=o(s,"P",{});var re=n(Q);gp=p(re,"For even greater control over how to load a split, you can choose to only load specific slices of a split. There are two options for slicing a split: using strings or "),La=o(re,"A",{href:!0});var gc=n(La);_p=p(gc,"ReadInstruction"),gc.forEach(t),vp=p(re,". Strings are more compact and readable for simple cases, while "),Ra=o(re,"A",{href:!0});var _c=n(Ra);$p=p(_c,"ReadInstruction"),_c.forEach(t),yp=p(re," is easier to use with variable slicing parameters."),re.forEach(t),Yo=c(s),K=o(s,"P",{});var ie=n(K);wp=p(ie,"Concatenate the "),sl=o(ie,"CODE",{});var vc=n(sl);bp=p(vc,"train"),vc.forEach(t),jp=p(ie," and "),tl=o(ie,"CODE",{});var $c=n(tl);xp=p($c,"test"),$c.forEach(t),kp=p(ie," split by:"),ie.forEach(t),Wo=c(s),P(Rt.$$.fragment,s),Go=c(s),Bs=o(s,"P",{});var nr=n(Bs);Ep=p(nr,"Select specific rows of the "),al=o(nr,"CODE",{});var yc=n(al);qp=p(yc,"train"),yc.forEach(t),Pp=p(nr," split:"),nr.forEach(t),Qo=c(s),P(Mt.$$.fragment,s),Ko=c(s),Ma=o(s,"P",{});var wc=n(Ma);Ap=p(wc,"Or select a percentage of the split with:"),wc.forEach(t),Xo=c(s),P(Vt.$$.fragment,s),Zo=c(s),Va=o(s,"P",{});var bc=n(Va);Sp=p(bc,"You can even select a combination of percentages from each split:"),bc.forEach(t),sn=c(s),P(zt.$$.fragment,s),tn=c(s),za=o(s,"P",{});var jc=n(za);Tp=p(jc,"Finally, create cross-validated dataset splits by:"),jc.forEach(t),an=c(s),P(Ut.$$.fragment,s),en=c(s),hs=o(s,"H3",{class:!0});var rr=n(hs);Ys=o(rr,"A",{id:!0,class:!0,href:!0});var xc=n(Ys);el=o(xc,"SPAN",{});var kc=n(el);P(Jt.$$.fragment,kc),kc.forEach(t),xc.forEach(t),Dp=c(rr),ll=o(rr,"SPAN",{});var Ec=n(ll);Np=p(Ec,"Percent slicing and rounding"),Ec.forEach(t),rr.forEach(t),ln=c(s),Ua=o(s,"P",{});var qc=n(Ua);Cp=p(qc,"For datasets where the requested slice boundaries do not divide evenly by 100, the default behavior is to round the boundaries to the nearest integer. As a result, some slices may contain more examples than others as shown in the following example:"),qc.forEach(t),on=c(s),P(Bt.$$.fragment,s),nn=c(s),Ws=o(s,"P",{});var ir=n(Ws);Ip=p(ir,"If you want equal sized splits, use "),ol=o(ir,"CODE",{});var Pc=n(ol);Op=p(Pc,"pct1_dropremainder"),Pc.forEach(t),Hp=p(ir," rounding instead. This will treat the specified percentage boundaries as multiples of 1%."),ir.forEach(t),rn=c(s),P(Yt.$$.fragment,s),pn=c(s),P(Gs.$$.fragment,s),dn=c(s),Ja=o(s,"A",{id:!0}),n(Ja).forEach(t),fn=c(s),us=o(s,"H2",{class:!0});var pr=n(us);Qs=o(pr,"A",{id:!0,class:!0,href:!0});var Ac=n(Qs);nl=o(Ac,"SPAN",{});var Sc=n(nl);P(Wt.$$.fragment,Sc),Sc.forEach(t),Ac.forEach(t),Fp=c(pr),rl=o(pr,"SPAN",{});var Tc=n(rl);Lp=p(Tc,"Troubleshooting"),Tc.forEach(t),pr.forEach(t),cn=c(s),Ba=o(s,"P",{});var Dc=n(Ba);Rp=p(Dc,"Sometimes, you may get unexpected results when you load a dataset. In this section, you will learn how to solve two common issues you may encounter when you load a dataset: manually download a dataset, and specify features of a dataset."),Dc.forEach(t),hn=c(s),ms=o(s,"H3",{class:!0});var dr=n(ms);Ks=o(dr,"A",{id:!0,class:!0,href:!0});var Nc=n(Ks);il=o(Nc,"SPAN",{});var Cc=n(il);P(Gt.$$.fragment,Cc),Cc.forEach(t),Nc.forEach(t),Mp=c(dr),pl=o(dr,"SPAN",{});var Ic=n(pl);Vp=p(Ic,"Manual download"),Ic.forEach(t),dr.forEach(t),un=c(s),M=o(s,"P",{});var nt=n(M);zp=p(nt,"Certain datasets require you to manually download the dataset files due to licensing incompatibility, or if the files are hidden behind a login page. This will cause "),Ya=o(nt,"A",{href:!0});var Oc=n(Ya);Up=p(Oc,"load_dataset()"),Oc.forEach(t),Jp=p(nt," to throw an "),dl=o(nt,"CODE",{});var Hc=n(dl);Bp=p(Hc,"AssertionError"),Hc.forEach(t),Yp=p(nt,". But \u{1F917} Datasets provides detailed instructions for downloading the missing files. After you have downloaded the files, use the "),fl=o(nt,"CODE",{});var Fc=n(fl);Wp=p(Fc,"data_dir"),Fc.forEach(t),Gp=p(nt," argument to specify the path to the files you just downloaded."),nt.forEach(t),mn=c(s),Xs=o(s,"P",{});var fr=n(Xs);Qp=p(fr,"For example, if you try to download a configuration from the "),Qt=o(fr,"A",{href:!0,rel:!0});var Lc=n(Qt);Kp=p(Lc,"MATINF"),Lc.forEach(t),Xp=p(fr," dataset:"),fr.forEach(t),gn=c(s),P(Kt.$$.fragment,s),_n=c(s),gs=o(s,"H3",{class:!0});var cr=n(gs);Zs=o(cr,"A",{id:!0,class:!0,href:!0});var Rc=n(Zs);cl=o(Rc,"SPAN",{});var Mc=n(cl);P(Xt.$$.fragment,Mc),Mc.forEach(t),Rc.forEach(t),Zp=c(cr),hl=o(cr,"SPAN",{});var Vc=n(hl);sd=p(Vc,"Specify features"),Vc.forEach(t),cr.forEach(t),vn=c(s),X=o(s,"P",{});var pe=n(X);td=p(pe,"When you create a dataset from local files, the "),Wa=o(pe,"A",{href:!0});var zc=n(Wa);ad=p(zc,"Features"),zc.forEach(t),ed=p(pe," are automatically inferred by "),Zt=o(pe,"A",{href:!0,rel:!0});var Uc=n(Zt);ld=p(Uc,"Apache Arrow"),Uc.forEach(t),od=p(pe,". However, the features of the dataset may not always align with your expectations or you may want to define the features yourself."),pe.forEach(t),$n=c(s),Z=o(s,"P",{});var de=n(Z);nd=p(de,"The following example shows how you can add custom labels with "),Ga=o(de,"A",{href:!0});var Jc=n(Ga);rd=p(Jc,"ClassLabel"),Jc.forEach(t),id=p(de,". First, define your own labels using the "),Qa=o(de,"A",{href:!0});var Bc=n(Qa);pd=p(Bc,"Features"),Bc.forEach(t),dd=p(de," class:"),de.forEach(t),yn=c(s),P(sa.$$.fragment,s),wn=c(s),ss=o(s,"P",{});var fe=n(ss);fd=p(fe,"Next, specify the "),ul=o(fe,"CODE",{});var Yc=n(ul);cd=p(Yc,"features"),Yc.forEach(t),hd=p(fe," argument in "),Ka=o(fe,"A",{href:!0});var Wc=n(Ka);ud=p(Wc,"load_dataset()"),Wc.forEach(t),md=p(fe," with the features you just created:"),fe.forEach(t),bn=c(s),P(ta.$$.fragment,s),jn=c(s),Xa=o(s,"P",{});var Gc=n(Xa);gd=p(Gc,"Now when you look at your dataset features, you can see it uses the custom labels you defined:"),Gc.forEach(t),xn=c(s),P(aa.$$.fragment,s),kn=c(s),_s=o(s,"H2",{class:!0});var hr=n(_s);st=o(hr,"A",{id:!0,class:!0,href:!0});var Qc=n(st);ml=o(Qc,"SPAN",{});var Kc=n(ml);P(ea.$$.fragment,Kc),Kc.forEach(t),Qc.forEach(t),_d=c(hr),gl=o(hr,"SPAN",{});var Xc=n(gl);vd=p(Xc,"Metrics"),Xc.forEach(t),hr.forEach(t),En=c(s),Za=o(s,"P",{});var Zc=n(Za);$d=p(Zc,"When the metric you want to use is not supported by \u{1F917} Datasets, you can write and use your own metric script. Load your metric by providing the path to your local metric loading script:"),Zc.forEach(t),qn=c(s),P(la.$$.fragment,s),Pn=c(s),P(tt.$$.fragment,s),An=c(s),vs=o(s,"H3",{class:!0});var ur=n(vs);at=o(ur,"A",{id:!0,class:!0,href:!0});var sh=n(at);_l=o(sh,"SPAN",{});var th=n(_l);P(oa.$$.fragment,th),th.forEach(t),sh.forEach(t),yd=c(ur),vl=o(ur,"SPAN",{});var ah=n(vl);wd=p(ah,"Load configurations"),ah.forEach(t),ur.forEach(t),Sn=c(s),se=o(s,"P",{});var eh=n(se);bd=p(eh,"It is possible for a metric to have different configurations. The configurations are stored in the [\u2018datasets.Metric.config_name\u2019] attribute. When you load a metric, provide the configuration name as shown in the following:"),eh.forEach(t),Tn=c(s),P(na.$$.fragment,s),Dn=c(s),$s=o(s,"H3",{class:!0});var mr=n($s);et=o(mr,"A",{id:!0,class:!0,href:!0});var lh=n(et);$l=o(lh,"SPAN",{});var oh=n($l);P(ra.$$.fragment,oh),oh.forEach(t),lh.forEach(t),jd=c(mr),yl=o(mr,"SPAN",{});var nh=n(yl);xd=p(nh,"Distributed setup"),nh.forEach(t),mr.forEach(t),Nn=c(s),te=o(s,"P",{});var rh=n(te);kd=p(rh,"When you work in a distributed or parallel processing environment, loading and computing a metric can be tricky because these processes are executed in parallel on separate subsets of the data. \u{1F917} Datasets supports distributed usage with a few additional arguments when you load a metric."),rh.forEach(t),Cn=c(s),ae=o(s,"P",{});var ih=n(ae);Ed=p(ih,"For example, imagine you are training and evaluating on eight parallel processes. Here\u2019s how you would load a metric in this distributed setting:"),ih.forEach(t),In=c(s),ts=o(s,"OL",{});var ce=n(ts);wl=o(ce,"LI",{});var ph=n(wl);ia=o(ph,"P",{});var gr=n(ia);qd=p(gr,"Define the total number of processes with the "),bl=o(gr,"CODE",{});var dh=n(bl);Pd=p(dh,"num_process"),dh.forEach(t),Ad=p(gr," argument."),gr.forEach(t),ph.forEach(t),Sd=c(ce),jl=o(ce,"LI",{});var fh=n(jl);ys=o(fh,"P",{});var he=n(ys);Td=p(he,"Set the process "),xl=o(he,"CODE",{});var ch=n(xl);Dd=p(ch,"rank"),ch.forEach(t),Nd=p(he," as an integer between zero and "),kl=o(he,"CODE",{});var hh=n(kl);Cd=p(hh,"num_process - 1"),hh.forEach(t),Id=p(he,"."),he.forEach(t),fh.forEach(t),Od=c(ce),El=o(ce,"LI",{});var uh=n(El);pa=o(uh,"P",{});var _r=n(pa);Hd=p(_r,"Load your metric with "),ee=o(_r,"A",{href:!0});var mh=n(ee);Fd=p(mh,"load_metric()"),mh.forEach(t),Ld=p(_r," with these arguments:"),_r.forEach(t),uh.forEach(t),ce.forEach(t),On=c(s),P(da.$$.fragment,s),Hn=c(s),P(lt.$$.fragment,s),Fn=c(s),ot=o(s,"P",{});var vr=n(ot);Rd=p(vr,"In some instances, you may be simultaneously running multiple independent distributed evaluations on the same server and files. To avoid any conflicts, it is important to provide an "),ql=o(vr,"CODE",{});var gh=n(ql);Md=p(gh,"experiment_id"),gh.forEach(t),Vd=p(vr," to distinguish the separate evaluations:"),vr.forEach(t),Ln=c(s),P(fa.$$.fragment,s),this.h()},h(){h(d,"name","hf:doc:metadata"),h(d,"content",JSON.stringify(Gh)),h(y,"id","load"),h(y,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(y,"href","#load"),h(u,"class","relative group"),h(va,"id","load-from-the-hub"),h(js,"id","hugging-face-hub"),h(js,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(js,"href","#hugging-face-hub"),h(as,"class","relative group"),h($a,"href","/docs/datasets/pr_4010/en/package_reference/loading_methods#datasets.load_dataset"),h(it,"href","https://huggingface.co/datasets/lhoestq/demo1"),h(it,"rel","nofollow"),h(ct,"href","https://huggingface.co/datasets/allenai/c4"),h(ct,"rel","nofollow"),h(As,"id","local-and-remote-files"),h(As,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(As,"href","#local-and-remote-files"),h(es,"class","relative group"),h(wa,"href","/docs/datasets/pr_4010/en/package_reference/loading_methods#datasets.load_dataset"),h(Ts,"id","csv"),h(Ts,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(Ts,"href","#csv"),h(ls,"class","relative group"),h(Ds,"id","json"),h(Ds,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(Ds,"href","#json"),h(os,"class","relative group"),h(qa,"href","/docs/datasets/pr_4010/en/package_reference/loading_methods#datasets.load_dataset"),h(Is,"id","text-files"),h(Is,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(Is,"href","#text-files"),h(ns,"class","relative group"),h(Os,"id","parquet"),h(Os,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(Os,"href","#parquet"),h(rs,"class","relative group"),h(Hs,"id","inmemory-data"),h(Hs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(Hs,"href","#inmemory-data"),h(is,"class","relative group"),h(Ia,"href","/docs/datasets/pr_4010/en/package_reference/main_classes#datasets.Dataset"),h(Ls,"id","python-dictionary"),h(Ls,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(Ls,"href","#python-dictionary"),h(ps,"class","relative group"),h(Oa,"href","/docs/datasets/pr_4010/en/package_reference/main_classes#datasets.Dataset.from_dict"),h(Ms,"id","pandas-dataframe"),h(Ms,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(Ms,"href","#pandas-dataframe"),h(ds,"class","relative group"),h(Ha,"href","/docs/datasets/pr_4010/en/package_reference/main_classes#datasets.Dataset.from_pandas"),h(Us,"id","offline"),h(Us,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(Us,"href","#offline"),h(fs,"class","relative group"),h(Js,"id","slice-splits"),h(Js,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(Js,"href","#slice-splits"),h(cs,"class","relative group"),h(La,"href","/docs/datasets/pr_4010/en/package_reference/builder_classes#datasets.ReadInstruction"),h(Ra,"href","/docs/datasets/pr_4010/en/package_reference/builder_classes#datasets.ReadInstruction"),h(Ys,"id","percent-slicing-and-rounding"),h(Ys,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(Ys,"href","#percent-slicing-and-rounding"),h(hs,"class","relative group"),h(Ja,"id","troubleshoot"),h(Qs,"id","troubleshooting"),h(Qs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(Qs,"href","#troubleshooting"),h(us,"class","relative group"),h(Ks,"id","manual-download"),h(Ks,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(Ks,"href","#manual-download"),h(ms,"class","relative group"),h(Ya,"href","/docs/datasets/pr_4010/en/package_reference/loading_methods#datasets.load_dataset"),h(Qt,"href","https://huggingface.co/datasets/matinf"),h(Qt,"rel","nofollow"),h(Zs,"id","specify-features"),h(Zs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(Zs,"href","#specify-features"),h(gs,"class","relative group"),h(Wa,"href","/docs/datasets/pr_4010/en/package_reference/main_classes#datasets.Features"),h(Zt,"href","https://arrow.apache.org/docs/"),h(Zt,"rel","nofollow"),h(Ga,"href","/docs/datasets/pr_4010/en/package_reference/main_classes#datasets.ClassLabel"),h(Qa,"href","/docs/datasets/pr_4010/en/package_reference/main_classes#datasets.Features"),h(Ka,"href","/docs/datasets/pr_4010/en/package_reference/loading_methods#datasets.load_dataset"),h(st,"id","metrics"),h(st,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(st,"href","#metrics"),h(_s,"class","relative group"),h(at,"id","load-configurations"),h(at,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(at,"href","#load-configurations"),h(vs,"class","relative group"),h(et,"id","distributed-setup"),h(et,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(et,"href","#distributed-setup"),h($s,"class","relative group"),h(ee,"href","/docs/datasets/pr_4010/en/package_reference/loading_methods#datasets.load_metric")},m(s,e){a(document.head,d),r(s,g,e),r(s,u,e),a(u,y),a(y,$),q(_,$,null),a(u,m),a(u,w),a(w,v),r(s,A,e),r(s,S,e),a(S,T),r(s,B,e),r(s,C,e),a(C,N),r(s,U,e),r(s,O,e),a(O,J),a(J,H),a(O,ua),a(O,ws),a(ws,ma),a(O,ga),a(O,bs),a(bs,wr),a(O,br),a(O,ue),a(ue,jr),a(O,xr),a(O,me),a(me,kr),r(s,Il,e),r(s,_a,e),a(_a,Er),r(s,Ol,e),r(s,va,e),r(s,Hl,e),r(s,as,e),a(as,js),a(js,ge),q(rt,ge,null),a(as,qr),a(as,_e),a(_e,Pr),r(s,Fl,e),r(s,xs,e),a(xs,Ar),a(xs,ve),a(ve,Sr),a(xs,Tr),r(s,Ll,e),r(s,Y,e),a(Y,Dr),a(Y,$a),a($a,Nr),a(Y,Cr),a(Y,it),a(it,Ir),a(Y,Or),r(s,Rl,e),q(pt,s,e),r(s,Ml,e),r(s,ya,e),a(ya,Hr),r(s,Vl,e),r(s,ks,e),a(ks,Fr),a(ks,$e),a($e,Lr),a(ks,Rr),r(s,zl,e),q(dt,s,e),r(s,Ul,e),q(Es,s,e),r(s,Jl,e),r(s,F,e),a(F,Mr),a(F,ye),a(ye,Vr),a(F,zr),a(F,we),a(we,Ur),a(F,Jr),a(F,be),a(be,Br),a(F,Yr),a(F,je),a(je,Wr),a(F,Gr),a(F,xe),a(xe,Qr),a(F,Kr),r(s,Bl,e),q(ft,s,e),r(s,Yl,e),q(qs,s,e),r(s,Wl,e),r(s,W,e),a(W,Xr),a(W,ke),a(ke,Zr),a(W,si),a(W,ct),a(ct,ti),a(W,ai),r(s,Gl,e),q(ht,s,e),r(s,Ql,e),r(s,Ps,e),a(Ps,ei),a(Ps,Ee),a(Ee,li),a(Ps,oi),r(s,Kl,e),q(ut,s,e),r(s,Xl,e),r(s,es,e),a(es,As),a(As,qe),q(mt,qe,null),a(es,ni),a(es,Pe),a(Pe,ri),r(s,Zl,e),r(s,L,e),a(L,ii),a(L,Ae),a(Ae,pi),a(L,di),a(L,Se),a(Se,fi),a(L,ci),a(L,Te),a(Te,hi),a(L,ui),a(L,De),a(De,mi),a(L,gi),a(L,wa),a(wa,_i),a(L,vi),r(s,so,e),q(Ss,s,e),r(s,to,e),r(s,ls,e),a(ls,Ts),a(Ts,Ne),q(gt,Ne,null),a(ls,$i),a(ls,Ce),a(Ce,yi),r(s,ao,e),r(s,ba,e),a(ba,wi),r(s,eo,e),q(_t,s,e),r(s,lo,e),r(s,ja,e),a(ja,bi),r(s,oo,e),q(vt,s,e),r(s,no,e),r(s,xa,e),a(xa,ji),r(s,ro,e),q($t,s,e),r(s,io,e),r(s,ka,e),a(ka,xi),r(s,po,e),q(yt,s,e),r(s,fo,e),r(s,Ea,e),a(Ea,ki),r(s,co,e),q(wt,s,e),r(s,ho,e),r(s,os,e),a(os,Ds),a(Ds,Ie),q(bt,Ie,null),a(os,Ei),a(os,Oe),a(Oe,qi),r(s,uo,e),r(s,Ns,e),a(Ns,Pi),a(Ns,qa),a(qa,Ai),a(Ns,Si),r(s,mo,e),q(jt,s,e),r(s,go,e),r(s,Pa,e),a(Pa,Ti),r(s,_o,e),q(xt,s,e),r(s,vo,e),r(s,Cs,e),a(Cs,Di),a(Cs,He),a(He,Ni),a(Cs,Ci),r(s,$o,e),q(kt,s,e),r(s,yo,e),r(s,Aa,e),a(Aa,Ii),r(s,wo,e),q(Et,s,e),r(s,bo,e),r(s,Sa,e),a(Sa,Oi),r(s,jo,e),r(s,ns,e),a(ns,Is),a(Is,Fe),q(qt,Fe,null),a(ns,Hi),a(ns,Le),a(Le,Fi),r(s,xo,e),r(s,Ta,e),a(Ta,Li),r(s,ko,e),q(Pt,s,e),r(s,Eo,e),r(s,Da,e),a(Da,Ri),r(s,qo,e),q(At,s,e),r(s,Po,e),r(s,rs,e),a(rs,Os),a(Os,Re),q(St,Re,null),a(rs,Mi),a(rs,Me),a(Me,Vi),r(s,Ao,e),r(s,Na,e),a(Na,zi),r(s,So,e),q(Tt,s,e),r(s,To,e),r(s,Ca,e),a(Ca,Ui),r(s,Do,e),q(Dt,s,e),r(s,No,e),r(s,is,e),a(is,Hs),a(Hs,Ve),q(Nt,Ve,null),a(is,Ji),a(is,ze),a(ze,Bi),r(s,Co,e),r(s,Fs,e),a(Fs,Yi),a(Fs,Ia),a(Ia,Wi),a(Fs,Gi),r(s,Io,e),r(s,ps,e),a(ps,Ls),a(Ls,Ue),q(Ct,Ue,null),a(ps,Qi),a(ps,Je),a(Je,Ki),r(s,Oo,e),r(s,Rs,e),a(Rs,Xi),a(Rs,Oa),a(Oa,Zi),a(Rs,sp),r(s,Ho,e),q(It,s,e),r(s,Fo,e),r(s,ds,e),a(ds,Ms),a(Ms,Be),q(Ot,Be,null),a(ds,tp),a(ds,Ye),a(Ye,ap),r(s,Lo,e),r(s,Vs,e),a(Vs,ep),a(Vs,Ha),a(Ha,lp),a(Vs,op),r(s,Ro,e),q(Ht,s,e),r(s,Mo,e),q(zs,s,e),r(s,Vo,e),r(s,fs,e),a(fs,Us),a(Us,We),q(Ft,We,null),a(fs,np),a(fs,Ge),a(Ge,rp),r(s,zo,e),r(s,Fa,e),a(Fa,ip),r(s,Uo,e),r(s,G,e),a(G,pp),a(G,Qe),a(Qe,dp),a(G,fp),a(G,Ke),a(Ke,cp),a(G,hp),r(s,Jo,e),r(s,cs,e),a(cs,Js),a(Js,Xe),q(Lt,Xe,null),a(cs,up),a(cs,Ze),a(Ze,mp),r(s,Bo,e),r(s,Q,e),a(Q,gp),a(Q,La),a(La,_p),a(Q,vp),a(Q,Ra),a(Ra,$p),a(Q,yp),r(s,Yo,e),r(s,K,e),a(K,wp),a(K,sl),a(sl,bp),a(K,jp),a(K,tl),a(tl,xp),a(K,kp),r(s,Wo,e),q(Rt,s,e),r(s,Go,e),r(s,Bs,e),a(Bs,Ep),a(Bs,al),a(al,qp),a(Bs,Pp),r(s,Qo,e),q(Mt,s,e),r(s,Ko,e),r(s,Ma,e),a(Ma,Ap),r(s,Xo,e),q(Vt,s,e),r(s,Zo,e),r(s,Va,e),a(Va,Sp),r(s,sn,e),q(zt,s,e),r(s,tn,e),r(s,za,e),a(za,Tp),r(s,an,e),q(Ut,s,e),r(s,en,e),r(s,hs,e),a(hs,Ys),a(Ys,el),q(Jt,el,null),a(hs,Dp),a(hs,ll),a(ll,Np),r(s,ln,e),r(s,Ua,e),a(Ua,Cp),r(s,on,e),q(Bt,s,e),r(s,nn,e),r(s,Ws,e),a(Ws,Ip),a(Ws,ol),a(ol,Op),a(Ws,Hp),r(s,rn,e),q(Yt,s,e),r(s,pn,e),q(Gs,s,e),r(s,dn,e),r(s,Ja,e),r(s,fn,e),r(s,us,e),a(us,Qs),a(Qs,nl),q(Wt,nl,null),a(us,Fp),a(us,rl),a(rl,Lp),r(s,cn,e),r(s,Ba,e),a(Ba,Rp),r(s,hn,e),r(s,ms,e),a(ms,Ks),a(Ks,il),q(Gt,il,null),a(ms,Mp),a(ms,pl),a(pl,Vp),r(s,un,e),r(s,M,e),a(M,zp),a(M,Ya),a(Ya,Up),a(M,Jp),a(M,dl),a(dl,Bp),a(M,Yp),a(M,fl),a(fl,Wp),a(M,Gp),r(s,mn,e),r(s,Xs,e),a(Xs,Qp),a(Xs,Qt),a(Qt,Kp),a(Xs,Xp),r(s,gn,e),q(Kt,s,e),r(s,_n,e),r(s,gs,e),a(gs,Zs),a(Zs,cl),q(Xt,cl,null),a(gs,Zp),a(gs,hl),a(hl,sd),r(s,vn,e),r(s,X,e),a(X,td),a(X,Wa),a(Wa,ad),a(X,ed),a(X,Zt),a(Zt,ld),a(X,od),r(s,$n,e),r(s,Z,e),a(Z,nd),a(Z,Ga),a(Ga,rd),a(Z,id),a(Z,Qa),a(Qa,pd),a(Z,dd),r(s,yn,e),q(sa,s,e),r(s,wn,e),r(s,ss,e),a(ss,fd),a(ss,ul),a(ul,cd),a(ss,hd),a(ss,Ka),a(Ka,ud),a(ss,md),r(s,bn,e),q(ta,s,e),r(s,jn,e),r(s,Xa,e),a(Xa,gd),r(s,xn,e),q(aa,s,e),r(s,kn,e),r(s,_s,e),a(_s,st),a(st,ml),q(ea,ml,null),a(_s,_d),a(_s,gl),a(gl,vd),r(s,En,e),r(s,Za,e),a(Za,$d),r(s,qn,e),q(la,s,e),r(s,Pn,e),q(tt,s,e),r(s,An,e),r(s,vs,e),a(vs,at),a(at,_l),q(oa,_l,null),a(vs,yd),a(vs,vl),a(vl,wd),r(s,Sn,e),r(s,se,e),a(se,bd),r(s,Tn,e),q(na,s,e),r(s,Dn,e),r(s,$s,e),a($s,et),a(et,$l),q(ra,$l,null),a($s,jd),a($s,yl),a(yl,xd),r(s,Nn,e),r(s,te,e),a(te,kd),r(s,Cn,e),r(s,ae,e),a(ae,Ed),r(s,In,e),r(s,ts,e),a(ts,wl),a(wl,ia),a(ia,qd),a(ia,bl),a(bl,Pd),a(ia,Ad),a(ts,Sd),a(ts,jl),a(jl,ys),a(ys,Td),a(ys,xl),a(xl,Dd),a(ys,Nd),a(ys,kl),a(kl,Cd),a(ys,Id),a(ts,Od),a(ts,El),a(El,pa),a(pa,Hd),a(pa,ee),a(ee,Fd),a(pa,Ld),r(s,On,e),q(da,s,e),r(s,Hn,e),q(lt,s,e),r(s,Fn,e),r(s,ot,e),a(ot,Rd),a(ot,ql),a(ql,Md),a(ot,Vd),r(s,Ln,e),q(fa,s,e),Rn=!0},p(s,[e]){const ca={};e&2&&(ca.$$scope={dirty:e,ctx:s}),Es.$set(ca);const Pl={};e&2&&(Pl.$$scope={dirty:e,ctx:s}),qs.$set(Pl);const Al={};e&2&&(Al.$$scope={dirty:e,ctx:s}),Ss.$set(Al);const Sl={};e&2&&(Sl.$$scope={dirty:e,ctx:s}),zs.$set(Sl);const Tl={};e&2&&(Tl.$$scope={dirty:e,ctx:s}),Gs.$set(Tl);const Dl={};e&2&&(Dl.$$scope={dirty:e,ctx:s}),tt.$set(Dl);const R={};e&2&&(R.$$scope={dirty:e,ctx:s}),lt.$set(R)},i(s){Rn||(b(_.$$.fragment,s),b(rt.$$.fragment,s),b(pt.$$.fragment,s),b(dt.$$.fragment,s),b(Es.$$.fragment,s),b(ft.$$.fragment,s),b(qs.$$.fragment,s),b(ht.$$.fragment,s),b(ut.$$.fragment,s),b(mt.$$.fragment,s),b(Ss.$$.fragment,s),b(gt.$$.fragment,s),b(_t.$$.fragment,s),b(vt.$$.fragment,s),b($t.$$.fragment,s),b(yt.$$.fragment,s),b(wt.$$.fragment,s),b(bt.$$.fragment,s),b(jt.$$.fragment,s),b(xt.$$.fragment,s),b(kt.$$.fragment,s),b(Et.$$.fragment,s),b(qt.$$.fragment,s),b(Pt.$$.fragment,s),b(At.$$.fragment,s),b(St.$$.fragment,s),b(Tt.$$.fragment,s),b(Dt.$$.fragment,s),b(Nt.$$.fragment,s),b(Ct.$$.fragment,s),b(It.$$.fragment,s),b(Ot.$$.fragment,s),b(Ht.$$.fragment,s),b(zs.$$.fragment,s),b(Ft.$$.fragment,s),b(Lt.$$.fragment,s),b(Rt.$$.fragment,s),b(Mt.$$.fragment,s),b(Vt.$$.fragment,s),b(zt.$$.fragment,s),b(Ut.$$.fragment,s),b(Jt.$$.fragment,s),b(Bt.$$.fragment,s),b(Yt.$$.fragment,s),b(Gs.$$.fragment,s),b(Wt.$$.fragment,s),b(Gt.$$.fragment,s),b(Kt.$$.fragment,s),b(Xt.$$.fragment,s),b(sa.$$.fragment,s),b(ta.$$.fragment,s),b(aa.$$.fragment,s),b(ea.$$.fragment,s),b(la.$$.fragment,s),b(tt.$$.fragment,s),b(oa.$$.fragment,s),b(na.$$.fragment,s),b(ra.$$.fragment,s),b(da.$$.fragment,s),b(lt.$$.fragment,s),b(fa.$$.fragment,s),Rn=!0)},o(s){j(_.$$.fragment,s),j(rt.$$.fragment,s),j(pt.$$.fragment,s),j(dt.$$.fragment,s),j(Es.$$.fragment,s),j(ft.$$.fragment,s),j(qs.$$.fragment,s),j(ht.$$.fragment,s),j(ut.$$.fragment,s),j(mt.$$.fragment,s),j(Ss.$$.fragment,s),j(gt.$$.fragment,s),j(_t.$$.fragment,s),j(vt.$$.fragment,s),j($t.$$.fragment,s),j(yt.$$.fragment,s),j(wt.$$.fragment,s),j(bt.$$.fragment,s),j(jt.$$.fragment,s),j(xt.$$.fragment,s),j(kt.$$.fragment,s),j(Et.$$.fragment,s),j(qt.$$.fragment,s),j(Pt.$$.fragment,s),j(At.$$.fragment,s),j(St.$$.fragment,s),j(Tt.$$.fragment,s),j(Dt.$$.fragment,s),j(Nt.$$.fragment,s),j(Ct.$$.fragment,s),j(It.$$.fragment,s),j(Ot.$$.fragment,s),j(Ht.$$.fragment,s),j(zs.$$.fragment,s),j(Ft.$$.fragment,s),j(Lt.$$.fragment,s),j(Rt.$$.fragment,s),j(Mt.$$.fragment,s),j(Vt.$$.fragment,s),j(zt.$$.fragment,s),j(Ut.$$.fragment,s),j(Jt.$$.fragment,s),j(Bt.$$.fragment,s),j(Yt.$$.fragment,s),j(Gs.$$.fragment,s),j(Wt.$$.fragment,s),j(Gt.$$.fragment,s),j(Kt.$$.fragment,s),j(Xt.$$.fragment,s),j(sa.$$.fragment,s),j(ta.$$.fragment,s),j(aa.$$.fragment,s),j(ea.$$.fragment,s),j(la.$$.fragment,s),j(tt.$$.fragment,s),j(oa.$$.fragment,s),j(na.$$.fragment,s),j(ra.$$.fragment,s),j(da.$$.fragment,s),j(lt.$$.fragment,s),j(fa.$$.fragment,s),Rn=!1},d(s){t(d),s&&t(g),s&&t(u),k(_),s&&t(A),s&&t(S),s&&t(B),s&&t(C),s&&t(U),s&&t(O),s&&t(Il),s&&t(_a),s&&t(Ol),s&&t(va),s&&t(Hl),s&&t(as),k(rt),s&&t(Fl),s&&t(xs),s&&t(Ll),s&&t(Y),s&&t(Rl),k(pt,s),s&&t(Ml),s&&t(ya),s&&t(Vl),s&&t(ks),s&&t(zl),k(dt,s),s&&t(Ul),k(Es,s),s&&t(Jl),s&&t(F),s&&t(Bl),k(ft,s),s&&t(Yl),k(qs,s),s&&t(Wl),s&&t(W),s&&t(Gl),k(ht,s),s&&t(Ql),s&&t(Ps),s&&t(Kl),k(ut,s),s&&t(Xl),s&&t(es),k(mt),s&&t(Zl),s&&t(L),s&&t(so),k(Ss,s),s&&t(to),s&&t(ls),k(gt),s&&t(ao),s&&t(ba),s&&t(eo),k(_t,s),s&&t(lo),s&&t(ja),s&&t(oo),k(vt,s),s&&t(no),s&&t(xa),s&&t(ro),k($t,s),s&&t(io),s&&t(ka),s&&t(po),k(yt,s),s&&t(fo),s&&t(Ea),s&&t(co),k(wt,s),s&&t(ho),s&&t(os),k(bt),s&&t(uo),s&&t(Ns),s&&t(mo),k(jt,s),s&&t(go),s&&t(Pa),s&&t(_o),k(xt,s),s&&t(vo),s&&t(Cs),s&&t($o),k(kt,s),s&&t(yo),s&&t(Aa),s&&t(wo),k(Et,s),s&&t(bo),s&&t(Sa),s&&t(jo),s&&t(ns),k(qt),s&&t(xo),s&&t(Ta),s&&t(ko),k(Pt,s),s&&t(Eo),s&&t(Da),s&&t(qo),k(At,s),s&&t(Po),s&&t(rs),k(St),s&&t(Ao),s&&t(Na),s&&t(So),k(Tt,s),s&&t(To),s&&t(Ca),s&&t(Do),k(Dt,s),s&&t(No),s&&t(is),k(Nt),s&&t(Co),s&&t(Fs),s&&t(Io),s&&t(ps),k(Ct),s&&t(Oo),s&&t(Rs),s&&t(Ho),k(It,s),s&&t(Fo),s&&t(ds),k(Ot),s&&t(Lo),s&&t(Vs),s&&t(Ro),k(Ht,s),s&&t(Mo),k(zs,s),s&&t(Vo),s&&t(fs),k(Ft),s&&t(zo),s&&t(Fa),s&&t(Uo),s&&t(G),s&&t(Jo),s&&t(cs),k(Lt),s&&t(Bo),s&&t(Q),s&&t(Yo),s&&t(K),s&&t(Wo),k(Rt,s),s&&t(Go),s&&t(Bs),s&&t(Qo),k(Mt,s),s&&t(Ko),s&&t(Ma),s&&t(Xo),k(Vt,s),s&&t(Zo),s&&t(Va),s&&t(sn),k(zt,s),s&&t(tn),s&&t(za),s&&t(an),k(Ut,s),s&&t(en),s&&t(hs),k(Jt),s&&t(ln),s&&t(Ua),s&&t(on),k(Bt,s),s&&t(nn),s&&t(Ws),s&&t(rn),k(Yt,s),s&&t(pn),k(Gs,s),s&&t(dn),s&&t(Ja),s&&t(fn),s&&t(us),k(Wt),s&&t(cn),s&&t(Ba),s&&t(hn),s&&t(ms),k(Gt),s&&t(un),s&&t(M),s&&t(mn),s&&t(Xs),s&&t(gn),k(Kt,s),s&&t(_n),s&&t(gs),k(Xt),s&&t(vn),s&&t(X),s&&t($n),s&&t(Z),s&&t(yn),k(sa,s),s&&t(wn),s&&t(ss),s&&t(bn),k(ta,s),s&&t(jn),s&&t(Xa),s&&t(xn),k(aa,s),s&&t(kn),s&&t(_s),k(ea),s&&t(En),s&&t(Za),s&&t(qn),k(la,s),s&&t(Pn),k(tt,s),s&&t(An),s&&t(vs),k(oa),s&&t(Sn),s&&t(se),s&&t(Tn),k(na,s),s&&t(Dn),s&&t($s),k(ra),s&&t(Nn),s&&t(te),s&&t(Cn),s&&t(ae),s&&t(In),s&&t(ts),s&&t(On),k(da,s),s&&t(Hn),k(lt,s),s&&t(Fn),s&&t(ot),s&&t(Ln),k(fa,s)}}}const Gh={local:"load",sections:[{local:"hugging-face-hub",title:"Hugging Face Hub"},{local:"local-and-remote-files",sections:[{local:"csv",title:"CSV"},{local:"json",title:"JSON"},{local:"text-files",title:"Text files"},{local:"parquet",title:"Parquet"}],title:"Local and remote files"},{local:"inmemory-data",sections:[{local:"python-dictionary",title:"Python dictionary"},{local:"pandas-dataframe",title:"Pandas DataFrame"}],title:"In-memory data"},{local:"offline",title:"Offline"},{local:"slice-splits",sections:[{local:"percent-slicing-and-rounding",title:"Percent slicing and rounding"}],title:"Slice splits"},{local:"troubleshooting",sections:[{local:"manual-download",title:"Manual download"},{local:"specify-features",title:"Specify features"}],title:"Troubleshooting"},{local:"metrics",sections:[{local:"load-configurations",title:"Load configurations"},{local:"distributed-setup",title:"Distributed setup"}],title:"Metrics"}],title:"Load"};function Qh(x){return Dh(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class au extends zd{constructor(d){super();Ud(this,d,Qh,Wh,Jd,{})}}export{au as default,Gh as metadata};
