import{S as We,i as Qe,s as Ue,e,k as i,w as b,t,M as Ye,c as l,d as n,m,a as r,x as f,h as p,b as c,F as a,g as h,y as g,q as _,o as w,B as k}from"../chunks/vendor-e67aec41.js";import{T as Je}from"../chunks/Tip-76459d1c.js";import{I as Cs}from"../chunks/IconCopyLink-ffd7f84e.js";import{C as Na}from"../chunks/CodeBlock-e2bcf023.js";import{C as Zn}from"../chunks/CodeBlockFw-1e02e2ba.js";function Xe(zs){let d,T,u,j,F;return{c(){d=e("p"),T=t("For more detailed information on loading and processing a dataset, take a look at "),u=e("a"),j=t("Chapter 3"),F=t(" of the Hugging Face course! It covers additional important topics like dynamic padding, and fine-tuning with the Trainer API."),this.h()},l(y){d=l(y,"P",{});var q=r(d);T=p(q,"For more detailed information on loading and processing a dataset, take a look at "),u=l(q,"A",{href:!0,rel:!0});var L=r(u);j=p(L,"Chapter 3"),L.forEach(n),F=p(q," of the Hugging Face course! It covers additional important topics like dynamic padding, and fine-tuning with the Trainer API."),q.forEach(n),this.h()},h(){c(u,"href","https://huggingface.co/course/chapter3/1?fw=pt"),c(u,"rel","nofollow")},m(y,q){h(y,d,q),a(d,T),a(d,u),a(u,j),a(d,F)},d(y){y&&n(d)}}}function Ke(zs){let d,T,u,j,F,y,q,L,Ra,Zs,O,La,gs,Oa,Ma,sa,M,Ia,V,Ha,Ga,aa,I,na,_s,Wa,ea,Z,la,S,H,Fs,ss,Qa,Ss,Ua,ta,A,Ya,as,Ja,Xa,ns,Ka,Va,pa,es,ra,G,Za,ls,sn,an,oa,ts,ha,P,W,Ps,ps,nn,Ds,en,ca,Q,ln,ws,tn,pn,ia,rs,ma,v,rn,Bs,on,hn,Ns,cn,mn,Rs,un,dn,ua,D,U,Ls,os,jn,Os,bn,da,ks,fn,ja,ys,x,gn,Ms,_n,wn,Is,kn,yn,hs,xn,vn,cs,$n,En,ba,is,fa,B,Hs,Tn,qn,$,An,Gs,Cn,zn,Ws,Fn,Sn,Qs,Pn,Dn,ga,E,xs,Bn,Nn,Us,Rn,Ln,Ys,On,Mn,_a,ms,wa,N,Y,Js,us,In,Xs,Hn,ka,vs,Gn,ya,ds,xa,R,J,Ks,js,Wn,Vs,Qn,va,$s,Un,$a,C,Yn,Es,Jn,Xn,Ts,Kn,Vn,Ea;return y=new Cs({}),I=new Je({props:{$$slots:{default:[Xe]},$$scope:{ctx:zs}}}),Z=new Na({props:{code:"pip install datasets",highlighted:'pip <span class="hljs-keyword">install</span> datasets'}}),ss=new Cs({}),es=new Na({props:{code:`from datasets import load_dataset
dataset = load_dataset('glue', 'mrpc', split='train')`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;glue&#x27;</span>, <span class="hljs-string">&#x27;mrpc&#x27;</span>, split=<span class="hljs-string">&#x27;train&#x27;</span>)`}}),ts=new Zn({props:{group1:{id:"pt",code:`from transformers import AutoModelForSequenceClassification, AutoTokenizer
model = AutoModelForSequenceClassification.from_pretrained('bert-base-cased')
tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForSequenceClassification, AutoTokenizer
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>)
Some weights of the model checkpoint at bert-base-cased were <span class="hljs-keyword">not</span> used when initializing BertForSequenceClassification: [<span class="hljs-string">&#x27;cls.predictions.bias&#x27;</span>, <span class="hljs-string">&#x27;cls.predictions.transform.dense.weight&#x27;</span>, <span class="hljs-string">&#x27;cls.predictions.transform.dense.bias&#x27;</span>, <span class="hljs-string">&#x27;cls.predictions.decoder.weight&#x27;</span>, <span class="hljs-string">&#x27;cls.seq_relationship.weight&#x27;</span>, <span class="hljs-string">&#x27;cls.seq_relationship.bias&#x27;</span>, <span class="hljs-string">&#x27;cls.predictions.transform.LayerNorm.weight&#x27;</span>, <span class="hljs-string">&#x27;cls.predictions.transform.LayerNorm.bias&#x27;</span>]
- This IS expected <span class="hljs-keyword">if</span> you are initializing BertForSequenceClassification <span class="hljs-keyword">from</span> the checkpoint of a model trained on another task <span class="hljs-keyword">or</span> <span class="hljs-keyword">with</span> another architecture (e.g. initializing a BertForSequenceClassification model <span class="hljs-keyword">from</span> a BertForPretraining model).
- This IS NOT expected <span class="hljs-keyword">if</span> you are initializing BertForSequenceClassification <span class="hljs-keyword">from</span> the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model <span class="hljs-keyword">from</span> a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were <span class="hljs-keyword">not</span> initialized <span class="hljs-keyword">from</span> the model checkpoint at bert-base-cased <span class="hljs-keyword">and</span> are newly initialized: [<span class="hljs-string">&#x27;classifier.weight&#x27;</span>, <span class="hljs-string">&#x27;classifier.bias&#x27;</span>]
You should probably TRAIN this model on a down-stream task to be able to use it <span class="hljs-keyword">for</span> predictions <span class="hljs-keyword">and</span> inference.
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>)`},group2:{id:"tf",code:`from transformers import TFAutoModelForSequenceClassification, AutoTokenizer
model = TFAutoModelForSequenceClassification.from_pretrained("bert-base-cased")
tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFAutoModelForSequenceClassification, AutoTokenizer
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
Some weights of the model checkpoint at bert-base-cased were <span class="hljs-keyword">not</span> used when initializing TFBertForSequenceClassification: [<span class="hljs-string">&#x27;nsp___cls&#x27;</span>, <span class="hljs-string">&#x27;mlm___cls&#x27;</span>]
- This IS expected <span class="hljs-keyword">if</span> you are initializing TFBertForSequenceClassification <span class="hljs-keyword">from</span> the checkpoint of a model trained on another task <span class="hljs-keyword">or</span> <span class="hljs-keyword">with</span> another architecture (e.g. initializing a BertForSequenceClassification model <span class="hljs-keyword">from</span> a BertForPretraining model).
- This IS NOT expected <span class="hljs-keyword">if</span> you are initializing TFBertForSequenceClassification <span class="hljs-keyword">from</span> the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model <span class="hljs-keyword">from</span> a BertForSequenceClassification model).
Some weights of TFBertForSequenceClassification were <span class="hljs-keyword">not</span> initialized <span class="hljs-keyword">from</span> the model checkpoint at bert-base-cased <span class="hljs-keyword">and</span> are newly initialized: [<span class="hljs-string">&#x27;dropout_37&#x27;</span>, <span class="hljs-string">&#x27;classifier&#x27;</span>]
You should probably TRAIN this model on a down-stream task to be able to use it <span class="hljs-keyword">for</span> predictions <span class="hljs-keyword">and</span> inference.
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>)`}}}),ps=new Cs({}),rs=new Na({props:{code:`def encode(examples):
    return tokenizer(examples['sentence1'], examples['sentence2'], truncation=True, padding='max_length')

dataset = dataset.map(encode, batched=True)
dataset[0]`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">def</span> <span class="hljs-title function_">encode</span>(<span class="hljs-params">examples</span>):
<span class="hljs-meta">... </span>    <span class="hljs-keyword">return</span> tokenizer(examples[<span class="hljs-string">&#x27;sentence1&#x27;</span>], examples[<span class="hljs-string">&#x27;sentence2&#x27;</span>], truncation=<span class="hljs-literal">True</span>, padding=<span class="hljs-string">&#x27;max_length&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = dataset.<span class="hljs-built_in">map</span>(encode, batched=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset[<span class="hljs-number">0</span>]
{<span class="hljs-string">&#x27;sentence1&#x27;</span>: <span class="hljs-string">&#x27;Amrozi accused his brother , whom he called &quot; the witness &quot; , of deliberately distorting his evidence .&#x27;</span>,
<span class="hljs-string">&#x27;sentence2&#x27;</span>: <span class="hljs-string">&#x27;Referring to him as only &quot; the witness &quot; , Amrozi accused his brother of deliberately distorting his evidence .&#x27;</span>,
<span class="hljs-string">&#x27;label&#x27;</span>: <span class="hljs-number">1</span>,
<span class="hljs-string">&#x27;idx&#x27;</span>: <span class="hljs-number">0</span>,
<span class="hljs-string">&#x27;input_ids&#x27;</span>: array([  <span class="hljs-number">101</span>,  <span class="hljs-number">7277</span>,  <span class="hljs-number">2180</span>,  <span class="hljs-number">5303</span>,  <span class="hljs-number">4806</span>,  <span class="hljs-number">1117</span>,  <span class="hljs-number">1711</span>,   <span class="hljs-number">117</span>,  <span class="hljs-number">2292</span>, <span class="hljs-number">1119</span>,  <span class="hljs-number">1270</span>,   <span class="hljs-number">107</span>,  <span class="hljs-number">1103</span>,  <span class="hljs-number">7737</span>,   <span class="hljs-number">107</span>,   <span class="hljs-number">117</span>,  <span class="hljs-number">1104</span>,  <span class="hljs-number">9938</span>, <span class="hljs-number">4267</span>, <span class="hljs-number">12223</span>, <span class="hljs-number">21811</span>,  <span class="hljs-number">1117</span>,  <span class="hljs-number">2554</span>,   <span class="hljs-number">119</span>,   <span class="hljs-number">102</span>, <span class="hljs-number">11336</span>,  <span class="hljs-number">6732</span>, <span class="hljs-number">3384</span>,  <span class="hljs-number">1106</span>,  <span class="hljs-number">1140</span>,  <span class="hljs-number">1112</span>,  <span class="hljs-number">1178</span>,   <span class="hljs-number">107</span>,  <span class="hljs-number">1103</span>,  <span class="hljs-number">7737</span>,   <span class="hljs-number">107</span>, <span class="hljs-number">117</span>,  <span class="hljs-number">7277</span>,  <span class="hljs-number">2180</span>,  <span class="hljs-number">5303</span>,  <span class="hljs-number">4806</span>,  <span class="hljs-number">1117</span>,  <span class="hljs-number">1711</span>,  <span class="hljs-number">1104</span>,  <span class="hljs-number">9938</span>, <span class="hljs-number">4267</span>, <span class="hljs-number">12223</span>, <span class="hljs-number">21811</span>,  <span class="hljs-number">1117</span>,  <span class="hljs-number">2554</span>,   <span class="hljs-number">119</span>,   <span class="hljs-number">102</span>]),
<span class="hljs-string">&#x27;token_type_ids&#x27;</span>: array([<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]),
<span class="hljs-string">&#x27;attention_mask&#x27;</span>: array([<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>])}`}}),os=new Cs({}),is=new Na({props:{code:"dataset = dataset.map(lambda examples: {'labels': examples['label']}, batched=True)",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = dataset.<span class="hljs-built_in">map</span>(<span class="hljs-keyword">lambda</span> examples: {<span class="hljs-string">&#x27;labels&#x27;</span>: examples[<span class="hljs-string">&#x27;label&#x27;</span>]}, batched=<span class="hljs-literal">True</span>)'}}),ms=new Zn({props:{group1:{id:"pt",code:`import torch
dataset.set_format(type='torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'labels'])
dataloader = torch.utils.data.DataLoader(dataset, batch_size=32)
next(iter(dataloader))`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset.set_format(<span class="hljs-built_in">type</span>=<span class="hljs-string">&#x27;torch&#x27;</span>, columns=[<span class="hljs-string">&#x27;input_ids&#x27;</span>, <span class="hljs-string">&#x27;token_type_ids&#x27;</span>, <span class="hljs-string">&#x27;attention_mask&#x27;</span>, <span class="hljs-string">&#x27;labels&#x27;</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>dataloader = torch.utils.data.DataLoader(dataset, batch_size=<span class="hljs-number">32</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">next</span>(<span class="hljs-built_in">iter</span>(dataloader))
{<span class="hljs-string">&#x27;attention_mask&#x27;</span>: tensor([[<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>,  ..., <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
                        [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>,  ..., <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
                        [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>,  ..., <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
                        ...,
                        [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>,  ..., <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
                        [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>,  ..., <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
                        [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>,  ..., <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>]]),
<span class="hljs-string">&#x27;input_ids&#x27;</span>: tensor([[  <span class="hljs-number">101</span>,  <span class="hljs-number">7277</span>,  <span class="hljs-number">2180</span>,  ...,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>],
                  [  <span class="hljs-number">101</span>, <span class="hljs-number">10684</span>,  <span class="hljs-number">2599</span>,  ...,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>],
                  [  <span class="hljs-number">101</span>,  <span class="hljs-number">1220</span>,  <span class="hljs-number">1125</span>,  ...,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>],
                  ...,
                  [  <span class="hljs-number">101</span>, <span class="hljs-number">16944</span>,  <span class="hljs-number">1107</span>,  ...,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>],
                  [  <span class="hljs-number">101</span>,  <span class="hljs-number">1109</span>, <span class="hljs-number">11896</span>,  ...,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>],
                  [  <span class="hljs-number">101</span>,  <span class="hljs-number">1109</span>,  <span class="hljs-number">4173</span>,  ...,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>]]),
<span class="hljs-string">&#x27;label&#x27;</span>: tensor([<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>]),
<span class="hljs-string">&#x27;token_type_ids&#x27;</span>: tensor([[<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>,  ..., <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
                     [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>,  ..., <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
                     [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>,  ..., <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
                     ...,
                     [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>,  ..., <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
                     [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>,  ..., <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
                     [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>,  ..., <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>]])}`},group2:{id:"tf",code:`import tensorflow as tf
dataset.set_format(type='tensorflow', columns=['input_ids', 'token_type_ids', 'attention_mask', 'labels'])
features = {x: dataset[x].to_tensor(default_value=0, shape=[None, tokenizer.model_max_length]) for x in ['input_ids', 'token_type_ids', 'attention_mask']}
tfdataset = tf.data.Dataset.from_tensor_slices((features, dataset["labels"])).batch(32)
next(iter(tfdataset))`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset.set_format(<span class="hljs-built_in">type</span>=<span class="hljs-string">&#x27;tensorflow&#x27;</span>, columns=[<span class="hljs-string">&#x27;input_ids&#x27;</span>, <span class="hljs-string">&#x27;token_type_ids&#x27;</span>, <span class="hljs-string">&#x27;attention_mask&#x27;</span>, <span class="hljs-string">&#x27;labels&#x27;</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>features = {x: dataset[x].to_tensor(default_value=<span class="hljs-number">0</span>, shape=[<span class="hljs-literal">None</span>, tokenizer.model_max_length]) <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> [<span class="hljs-string">&#x27;input_ids&#x27;</span>, <span class="hljs-string">&#x27;token_type_ids&#x27;</span>, <span class="hljs-string">&#x27;attention_mask&#x27;</span>]}
<span class="hljs-meta">&gt;&gt;&gt; </span>tfdataset = tf.data.Dataset.from_tensor_slices((features, dataset[<span class="hljs-string">&quot;labels&quot;</span>])).batch(<span class="hljs-number">32</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">next</span>(<span class="hljs-built_in">iter</span>(tfdataset))
({<span class="hljs-string">&#x27;input_ids&#x27;</span>: &lt;tf.Tensor: shape=(<span class="hljs-number">32</span>, <span class="hljs-number">512</span>), dtype=int32, numpy=
array([[  <span class="hljs-number">101</span>,  <span class="hljs-number">7277</span>,  <span class="hljs-number">2180</span>, ...,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>],
   [  <span class="hljs-number">101</span>, <span class="hljs-number">10684</span>,  <span class="hljs-number">2599</span>, ...,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>],
   [  <span class="hljs-number">101</span>,  <span class="hljs-number">1220</span>,  <span class="hljs-number">1125</span>, ...,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>],
   ...,
   [  <span class="hljs-number">101</span>,  <span class="hljs-number">1109</span>,  <span class="hljs-number">2026</span>, ...,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>],
   [  <span class="hljs-number">101</span>, <span class="hljs-number">22263</span>,  <span class="hljs-number">1107</span>, ...,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>],
   [  <span class="hljs-number">101</span>,   <span class="hljs-number">142</span>,  <span class="hljs-number">1813</span>, ...,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>]], dtype=int32)&gt;, <span class="hljs-string">&#x27;token_type_ids&#x27;</span>: &lt;tf.Tensor: shape=(<span class="hljs-number">32</span>, <span class="hljs-number">512</span>), dtype=int32, numpy=
array([[<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, ..., <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
   [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, ..., <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
   [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, ..., <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
   ...,
   [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, ..., <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
   [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, ..., <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
   [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, ..., <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>]], dtype=int32)&gt;, <span class="hljs-string">&#x27;attention_mask&#x27;</span>: &lt;tf.Tensor: shape=(<span class="hljs-number">32</span>, <span class="hljs-number">512</span>), dtype=int32, numpy=
array([[<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, ..., <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
   [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, ..., <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
   [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, ..., <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
   ...,
   [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, ..., <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
   [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, ..., <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
   [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, ..., <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>]], dtype=int32)&gt;}, &lt;tf.Tensor: shape=(<span class="hljs-number">32</span>,), dtype=int64, numpy=
array([<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>,
   <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>])&gt;)`}}}),us=new Cs({}),ds=new Zn({props:{group1:{id:"pt",code:`from tqdm import tqdm
device = 'cuda' if torch.cuda.is_available() else 'cpu' 
model.train().to(device)
optimizer = torch.optim.AdamW(params=model.parameters(), lr=1e-5)
for epoch in range(3):
    for i, batch in enumerate(tqdm(dataloader)):
        batch = {k: v.to(device) for k, v in batch.items()}
        outputs = model(**batch)
        loss = outputs[0]
        loss.backward()
        optimizer.step()
        optimizer.zero_grad()
        if i % 10 == 0:
            print(f"loss: {loss}")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> tqdm <span class="hljs-keyword">import</span> tqdm
<span class="hljs-meta">&gt;&gt;&gt; </span>device = <span class="hljs-string">&#x27;cuda&#x27;</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">&#x27;cpu&#x27;</span> 
<span class="hljs-meta">&gt;&gt;&gt; </span>model.train().to(device)
<span class="hljs-meta">&gt;&gt;&gt; </span>optimizer = torch.optim.AdamW(params=model.parameters(), lr=<span class="hljs-number">1e-5</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">3</span>):
<span class="hljs-meta">... </span>    <span class="hljs-keyword">for</span> i, batch <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(tqdm(dataloader)):
<span class="hljs-meta">... </span>        batch = {k: v.to(device) <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> batch.items()}
<span class="hljs-meta">... </span>        outputs = model(**batch)
<span class="hljs-meta">... </span>        loss = outputs[<span class="hljs-number">0</span>]
<span class="hljs-meta">... </span>        loss.backward()
<span class="hljs-meta">... </span>        optimizer.step()
<span class="hljs-meta">... </span>        optimizer.zero_grad()
<span class="hljs-meta">... </span>        <span class="hljs-keyword">if</span> i % <span class="hljs-number">10</span> == <span class="hljs-number">0</span>:
<span class="hljs-meta">... </span>            <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;loss: <span class="hljs-subst">{loss}</span>&quot;</span>)`},group2:{id:"tf",code:`loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(reduction=tf.keras.losses.Reduction.NONE, from_logits=True)
opt = tf.keras.optimizers.Adam(learning_rate=3e-5)
model.compile(optimizer=opt, loss=loss_fn, metrics=["accuracy"])
model.fit(tfdataset, epochs=3)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(reduction=tf.keras.losses.Reduction.NONE, from_logits=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>opt = tf.keras.optimizers.Adam(learning_rate=<span class="hljs-number">3e-5</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.<span class="hljs-built_in">compile</span>(optimizer=opt, loss=loss_fn, metrics=[<span class="hljs-string">&quot;accuracy&quot;</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>model.fit(tfdataset, epochs=<span class="hljs-number">3</span>)`}}}),js=new Cs({}),{c(){d=e("meta"),T=i(),u=e("h1"),j=e("a"),F=e("span"),b(y.$$.fragment),q=i(),L=e("span"),Ra=t("Quick Start"),Zs=i(),O=e("p"),La=t("The quick start is intended for developers who are ready to dive in to the code, and see an end-to-end example of how they can integrate \u{1F917} Datasets into their model training workflow. For beginners who are looking for a gentler introduction, we recommend you begin with the "),gs=e("a"),Oa=t("tutorials"),Ma=t("."),sa=i(),M=e("p"),Ia=t("In the quick start, you will walkthrough all the steps to fine-tune "),V=e("a"),Ha=t("BERT"),Ga=t(" on a paraphrase classification task. Depending on the specific dataset you use, these steps may vary, but the general steps of how to load a dataset and process it are the same."),aa=i(),b(I.$$.fragment),na=i(),_s=e("p"),Wa=t("Get started by installing \u{1F917} Datasets:"),ea=i(),b(Z.$$.fragment),la=i(),S=e("h2"),H=e("a"),Fs=e("span"),b(ss.$$.fragment),Qa=i(),Ss=e("span"),Ua=t("Load the dataset and model"),ta=i(),A=e("p"),Ya=t("Begin by loading the "),as=e("a"),Ja=t("Microsoft Research Paraphrase Corpus (MRPC)"),Xa=t(" training dataset from the "),ns=e("a"),Ka=t("General Language Understanding Evaluation (GLUE) benchmark"),Va=t(". MRPC is a corpus of human annotated sentence pairs used to train a model to determine whether sentence pairs are semantically equivalent."),pa=i(),b(es.$$.fragment),ra=i(),G=e("p"),Za=t("Next, import the pre-trained BERT model and its tokenizer from the "),ls=e("a"),sn=t("\u{1F917} Transformers"),an=t(" library:"),oa=i(),b(ts.$$.fragment),ha=i(),P=e("h2"),W=e("a"),Ps=e("span"),b(ps.$$.fragment),nn=i(),Ds=e("span"),en=t("Tokenize the dataset"),ca=i(),Q=e("p"),ln=t("The next step is to tokenize the text in order to build sequences of integers the model can understand. Encode the entire dataset with "),ws=e("a"),tn=t("datasets.Dataset.map()"),pn=t(", and truncate and pad the inputs to the maximum length of the model. This ensures the appropriate tensor batches are built."),ia=i(),b(rs.$$.fragment),ma=i(),v=e("p"),rn=t("Notice how there are three new columns in the dataset: "),Bs=e("code"),on=t("input_ids"),hn=t(", "),Ns=e("code"),cn=t("token_type_ids"),mn=t(", and "),Rs=e("code"),un=t("attention_mask"),dn=t(". These columns are the inputs to the model."),ua=i(),D=e("h2"),U=e("a"),Ls=e("span"),b(os.$$.fragment),jn=i(),Os=e("span"),bn=t("Format the dataset"),da=i(),ks=e("p"),fn=t("Depending on whether you are using PyTorch, TensorFlow, or JAX, you will need to format the dataset accordingly. There are three changes you need to make to the dataset:"),ja=i(),ys=e("ol"),x=e("li"),gn=t("Rename the "),Ms=e("code"),_n=t("label"),wn=t(" column to "),Is=e("code"),kn=t("labels"),yn=t(", the expected input name in "),hs=e("a"),xn=t("BertForSequenceClassification"),vn=t(" or "),cs=e("a"),$n=t("TFBertForSequenceClassification"),En=t(":"),ba=i(),b(is.$$.fragment),fa=i(),B=e("ol"),Hs=e("li"),Tn=t("Retrieve the actual tensors from the Dataset object instead of using the current Python objects."),qn=i(),$=e("li"),An=t("Filter the dataset to only return the model inputs: "),Gs=e("code"),Cn=t("input_ids"),zn=t(", "),Ws=e("code"),Fn=t("token_type_ids"),Sn=t(", and "),Qs=e("code"),Pn=t("attention_mask"),Dn=t("."),ga=i(),E=e("p"),xs=e("a"),Bn=t("datasets.Dataset.set_format()"),Nn=t(" completes the last two steps on-the-fly. After you set the format, wrap the dataset in "),Us=e("code"),Rn=t("torch.utils.data.DataLoader"),Ln=t(" or "),Ys=e("code"),On=t("tf.data.Dataset"),Mn=t(":"),_a=i(),b(ms.$$.fragment),wa=i(),N=e("h2"),Y=e("a"),Js=e("span"),b(us.$$.fragment),In=i(),Xs=e("span"),Hn=t("Train the model"),ka=i(),vs=e("p"),Gn=t("Lastly, create a simple training loop and start training:"),ya=i(),b(ds.$$.fragment),xa=i(),R=e("h2"),J=e("a"),Ks=e("span"),b(js.$$.fragment),Wn=i(),Vs=e("span"),Qn=t("What's next?"),va=i(),$s=e("p"),Un=t("This completes the basic steps of loading a dataset to train a model. You loaded and processed the MRPC dataset to fine-tune BERT to determine whether sentence pairs have the same meaning."),$a=i(),C=e("p"),Yn=t("For your next steps, take a look at our "),Es=e("a"),Jn=t("How-to guides"),Xn=t(" and learn how to achieve a specific task (e.g. load a dataset offline, add a dataset to the Hub, change the name of a column). Or if you want to deepen your knowledge of \u{1F917} Datasets core concepts, read our "),Ts=e("a"),Kn=t("Conceptual Guides"),Vn=t("."),this.h()},l(s){const o=Ye('[data-svelte="svelte-1phssyn"]',document.head);d=l(o,"META",{name:!0,content:!0}),o.forEach(n),T=m(s),u=l(s,"H1",{class:!0});var bs=r(u);j=l(bs,"A",{id:!0,class:!0,href:!0});var se=r(j);F=l(se,"SPAN",{});var ae=r(F);f(y.$$.fragment,ae),ae.forEach(n),se.forEach(n),q=m(bs),L=l(bs,"SPAN",{});var ne=r(L);Ra=p(ne,"Quick Start"),ne.forEach(n),bs.forEach(n),Zs=m(s),O=l(s,"P",{});var Ta=r(O);La=p(Ta,"The quick start is intended for developers who are ready to dive in to the code, and see an end-to-end example of how they can integrate \u{1F917} Datasets into their model training workflow. For beginners who are looking for a gentler introduction, we recommend you begin with the "),gs=l(Ta,"A",{href:!0});var ee=r(gs);Oa=p(ee,"tutorials"),ee.forEach(n),Ma=p(Ta,"."),Ta.forEach(n),sa=m(s),M=l(s,"P",{});var qa=r(M);Ia=p(qa,"In the quick start, you will walkthrough all the steps to fine-tune "),V=l(qa,"A",{href:!0,rel:!0});var le=r(V);Ha=p(le,"BERT"),le.forEach(n),Ga=p(qa," on a paraphrase classification task. Depending on the specific dataset you use, these steps may vary, but the general steps of how to load a dataset and process it are the same."),qa.forEach(n),aa=m(s),f(I.$$.fragment,s),na=m(s),_s=l(s,"P",{});var te=r(_s);Wa=p(te,"Get started by installing \u{1F917} Datasets:"),te.forEach(n),ea=m(s),f(Z.$$.fragment,s),la=m(s),S=l(s,"H2",{class:!0});var Aa=r(S);H=l(Aa,"A",{id:!0,class:!0,href:!0});var pe=r(H);Fs=l(pe,"SPAN",{});var re=r(Fs);f(ss.$$.fragment,re),re.forEach(n),pe.forEach(n),Qa=m(Aa),Ss=l(Aa,"SPAN",{});var oe=r(Ss);Ua=p(oe,"Load the dataset and model"),oe.forEach(n),Aa.forEach(n),ta=m(s),A=l(s,"P",{});var qs=r(A);Ya=p(qs,"Begin by loading the "),as=l(qs,"A",{href:!0,rel:!0});var he=r(as);Ja=p(he,"Microsoft Research Paraphrase Corpus (MRPC)"),he.forEach(n),Xa=p(qs," training dataset from the "),ns=l(qs,"A",{href:!0,rel:!0});var ce=r(ns);Ka=p(ce,"General Language Understanding Evaluation (GLUE) benchmark"),ce.forEach(n),Va=p(qs,". MRPC is a corpus of human annotated sentence pairs used to train a model to determine whether sentence pairs are semantically equivalent."),qs.forEach(n),pa=m(s),f(es.$$.fragment,s),ra=m(s),G=l(s,"P",{});var Ca=r(G);Za=p(Ca,"Next, import the pre-trained BERT model and its tokenizer from the "),ls=l(Ca,"A",{href:!0,rel:!0});var ie=r(ls);sn=p(ie,"\u{1F917} Transformers"),ie.forEach(n),an=p(Ca," library:"),Ca.forEach(n),oa=m(s),f(ts.$$.fragment,s),ha=m(s),P=l(s,"H2",{class:!0});var za=r(P);W=l(za,"A",{id:!0,class:!0,href:!0});var me=r(W);Ps=l(me,"SPAN",{});var ue=r(Ps);f(ps.$$.fragment,ue),ue.forEach(n),me.forEach(n),nn=m(za),Ds=l(za,"SPAN",{});var de=r(Ds);en=p(de,"Tokenize the dataset"),de.forEach(n),za.forEach(n),ca=m(s),Q=l(s,"P",{});var Fa=r(Q);ln=p(Fa,"The next step is to tokenize the text in order to build sequences of integers the model can understand. Encode the entire dataset with "),ws=l(Fa,"A",{href:!0});var je=r(ws);tn=p(je,"datasets.Dataset.map()"),je.forEach(n),pn=p(Fa,", and truncate and pad the inputs to the maximum length of the model. This ensures the appropriate tensor batches are built."),Fa.forEach(n),ia=m(s),f(rs.$$.fragment,s),ma=m(s),v=l(s,"P",{});var X=r(v);rn=p(X,"Notice how there are three new columns in the dataset: "),Bs=l(X,"CODE",{});var be=r(Bs);on=p(be,"input_ids"),be.forEach(n),hn=p(X,", "),Ns=l(X,"CODE",{});var fe=r(Ns);cn=p(fe,"token_type_ids"),fe.forEach(n),mn=p(X,", and "),Rs=l(X,"CODE",{});var ge=r(Rs);un=p(ge,"attention_mask"),ge.forEach(n),dn=p(X,". These columns are the inputs to the model."),X.forEach(n),ua=m(s),D=l(s,"H2",{class:!0});var Sa=r(D);U=l(Sa,"A",{id:!0,class:!0,href:!0});var _e=r(U);Ls=l(_e,"SPAN",{});var we=r(Ls);f(os.$$.fragment,we),we.forEach(n),_e.forEach(n),jn=m(Sa),Os=l(Sa,"SPAN",{});var ke=r(Os);bn=p(ke,"Format the dataset"),ke.forEach(n),Sa.forEach(n),da=m(s),ks=l(s,"P",{});var ye=r(ks);fn=p(ye,"Depending on whether you are using PyTorch, TensorFlow, or JAX, you will need to format the dataset accordingly. There are three changes you need to make to the dataset:"),ye.forEach(n),ja=m(s),ys=l(s,"OL",{});var xe=r(ys);x=l(xe,"LI",{});var z=r(x);gn=p(z,"Rename the "),Ms=l(z,"CODE",{});var ve=r(Ms);_n=p(ve,"label"),ve.forEach(n),wn=p(z," column to "),Is=l(z,"CODE",{});var $e=r(Is);kn=p($e,"labels"),$e.forEach(n),yn=p(z,", the expected input name in "),hs=l(z,"A",{href:!0,rel:!0});var Ee=r(hs);xn=p(Ee,"BertForSequenceClassification"),Ee.forEach(n),vn=p(z," or "),cs=l(z,"A",{href:!0,rel:!0});var Te=r(cs);$n=p(Te,"TFBertForSequenceClassification"),Te.forEach(n),En=p(z,":"),z.forEach(n),xe.forEach(n),ba=m(s),f(is.$$.fragment,s),fa=m(s),B=l(s,"OL",{start:!0});var Pa=r(B);Hs=l(Pa,"LI",{});var qe=r(Hs);Tn=p(qe,"Retrieve the actual tensors from the Dataset object instead of using the current Python objects."),qe.forEach(n),qn=m(Pa),$=l(Pa,"LI",{});var K=r($);An=p(K,"Filter the dataset to only return the model inputs: "),Gs=l(K,"CODE",{});var Ae=r(Gs);Cn=p(Ae,"input_ids"),Ae.forEach(n),zn=p(K,", "),Ws=l(K,"CODE",{});var Ce=r(Ws);Fn=p(Ce,"token_type_ids"),Ce.forEach(n),Sn=p(K,", and "),Qs=l(K,"CODE",{});var ze=r(Qs);Pn=p(ze,"attention_mask"),ze.forEach(n),Dn=p(K,"."),K.forEach(n),Pa.forEach(n),ga=m(s),E=l(s,"P",{});var fs=r(E);xs=l(fs,"A",{href:!0});var Fe=r(xs);Bn=p(Fe,"datasets.Dataset.set_format()"),Fe.forEach(n),Nn=p(fs," completes the last two steps on-the-fly. After you set the format, wrap the dataset in "),Us=l(fs,"CODE",{});var Se=r(Us);Rn=p(Se,"torch.utils.data.DataLoader"),Se.forEach(n),Ln=p(fs," or "),Ys=l(fs,"CODE",{});var Pe=r(Ys);On=p(Pe,"tf.data.Dataset"),Pe.forEach(n),Mn=p(fs,":"),fs.forEach(n),_a=m(s),f(ms.$$.fragment,s),wa=m(s),N=l(s,"H2",{class:!0});var Da=r(N);Y=l(Da,"A",{id:!0,class:!0,href:!0});var De=r(Y);Js=l(De,"SPAN",{});var Be=r(Js);f(us.$$.fragment,Be),Be.forEach(n),De.forEach(n),In=m(Da),Xs=l(Da,"SPAN",{});var Ne=r(Xs);Hn=p(Ne,"Train the model"),Ne.forEach(n),Da.forEach(n),ka=m(s),vs=l(s,"P",{});var Re=r(vs);Gn=p(Re,"Lastly, create a simple training loop and start training:"),Re.forEach(n),ya=m(s),f(ds.$$.fragment,s),xa=m(s),R=l(s,"H2",{class:!0});var Ba=r(R);J=l(Ba,"A",{id:!0,class:!0,href:!0});var Le=r(J);Ks=l(Le,"SPAN",{});var Oe=r(Ks);f(js.$$.fragment,Oe),Oe.forEach(n),Le.forEach(n),Wn=m(Ba),Vs=l(Ba,"SPAN",{});var Me=r(Vs);Qn=p(Me,"What's next?"),Me.forEach(n),Ba.forEach(n),va=m(s),$s=l(s,"P",{});var Ie=r($s);Un=p(Ie,"This completes the basic steps of loading a dataset to train a model. You loaded and processed the MRPC dataset to fine-tune BERT to determine whether sentence pairs have the same meaning."),Ie.forEach(n),$a=m(s),C=l(s,"P",{});var As=r(C);Yn=p(As,"For your next steps, take a look at our "),Es=l(As,"A",{href:!0});var He=r(Es);Jn=p(He,"How-to guides"),He.forEach(n),Xn=p(As," and learn how to achieve a specific task (e.g. load a dataset offline, add a dataset to the Hub, change the name of a column). Or if you want to deepen your knowledge of \u{1F917} Datasets core concepts, read our "),Ts=l(As,"A",{href:!0});var Ge=r(Ts);Kn=p(Ge,"Conceptual Guides"),Ge.forEach(n),Vn=p(As,"."),As.forEach(n),this.h()},h(){c(d,"name","hf:doc:metadata"),c(d,"content",JSON.stringify(Ve)),c(j,"id","quick-start"),c(j,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(j,"href","#quick-start"),c(u,"class","relative group"),c(gs,"href","./tutorial"),c(V,"href","https://huggingface.co/bert-base-cased"),c(V,"rel","nofollow"),c(H,"id","load-the-dataset-and-model"),c(H,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(H,"href","#load-the-dataset-and-model"),c(S,"class","relative group"),c(as,"href","https://huggingface.co/datasets/viewer/?dataset=glue&config=mrpc"),c(as,"rel","nofollow"),c(ns,"href","https://huggingface.co/datasets/glue"),c(ns,"rel","nofollow"),c(ls,"href","https://huggingface.co/transformers/"),c(ls,"rel","nofollow"),c(W,"id","tokenize-the-dataset"),c(W,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(W,"href","#tokenize-the-dataset"),c(P,"class","relative group"),c(ws,"href","/docs/datasets/pr_3843/en/package_reference/main_classes#datasets.Dataset.map"),c(U,"id","format-the-dataset"),c(U,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(U,"href","#format-the-dataset"),c(D,"class","relative group"),c(hs,"href","https://huggingface.co/transformers/model_doc/bert#transformers.BertForSequenceClassification.forward"),c(hs,"rel","nofollow"),c(cs,"href","https://huggingface.co/transformers/model_doc/bert#tfbertforsequenceclassification"),c(cs,"rel","nofollow"),c(B,"start","2"),c(xs,"href","/docs/datasets/pr_3843/en/package_reference/main_classes#datasets.Dataset.set_format"),c(Y,"id","train-the-model"),c(Y,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Y,"href","#train-the-model"),c(N,"class","relative group"),c(J,"id","whats-next"),c(J,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(J,"href","#whats-next"),c(R,"class","relative group"),c(Es,"href","./how_to"),c(Ts,"href","./about_arrow")},m(s,o){a(document.head,d),h(s,T,o),h(s,u,o),a(u,j),a(j,F),g(y,F,null),a(u,q),a(u,L),a(L,Ra),h(s,Zs,o),h(s,O,o),a(O,La),a(O,gs),a(gs,Oa),a(O,Ma),h(s,sa,o),h(s,M,o),a(M,Ia),a(M,V),a(V,Ha),a(M,Ga),h(s,aa,o),g(I,s,o),h(s,na,o),h(s,_s,o),a(_s,Wa),h(s,ea,o),g(Z,s,o),h(s,la,o),h(s,S,o),a(S,H),a(H,Fs),g(ss,Fs,null),a(S,Qa),a(S,Ss),a(Ss,Ua),h(s,ta,o),h(s,A,o),a(A,Ya),a(A,as),a(as,Ja),a(A,Xa),a(A,ns),a(ns,Ka),a(A,Va),h(s,pa,o),g(es,s,o),h(s,ra,o),h(s,G,o),a(G,Za),a(G,ls),a(ls,sn),a(G,an),h(s,oa,o),g(ts,s,o),h(s,ha,o),h(s,P,o),a(P,W),a(W,Ps),g(ps,Ps,null),a(P,nn),a(P,Ds),a(Ds,en),h(s,ca,o),h(s,Q,o),a(Q,ln),a(Q,ws),a(ws,tn),a(Q,pn),h(s,ia,o),g(rs,s,o),h(s,ma,o),h(s,v,o),a(v,rn),a(v,Bs),a(Bs,on),a(v,hn),a(v,Ns),a(Ns,cn),a(v,mn),a(v,Rs),a(Rs,un),a(v,dn),h(s,ua,o),h(s,D,o),a(D,U),a(U,Ls),g(os,Ls,null),a(D,jn),a(D,Os),a(Os,bn),h(s,da,o),h(s,ks,o),a(ks,fn),h(s,ja,o),h(s,ys,o),a(ys,x),a(x,gn),a(x,Ms),a(Ms,_n),a(x,wn),a(x,Is),a(Is,kn),a(x,yn),a(x,hs),a(hs,xn),a(x,vn),a(x,cs),a(cs,$n),a(x,En),h(s,ba,o),g(is,s,o),h(s,fa,o),h(s,B,o),a(B,Hs),a(Hs,Tn),a(B,qn),a(B,$),a($,An),a($,Gs),a(Gs,Cn),a($,zn),a($,Ws),a(Ws,Fn),a($,Sn),a($,Qs),a(Qs,Pn),a($,Dn),h(s,ga,o),h(s,E,o),a(E,xs),a(xs,Bn),a(E,Nn),a(E,Us),a(Us,Rn),a(E,Ln),a(E,Ys),a(Ys,On),a(E,Mn),h(s,_a,o),g(ms,s,o),h(s,wa,o),h(s,N,o),a(N,Y),a(Y,Js),g(us,Js,null),a(N,In),a(N,Xs),a(Xs,Hn),h(s,ka,o),h(s,vs,o),a(vs,Gn),h(s,ya,o),g(ds,s,o),h(s,xa,o),h(s,R,o),a(R,J),a(J,Ks),g(js,Ks,null),a(R,Wn),a(R,Vs),a(Vs,Qn),h(s,va,o),h(s,$s,o),a($s,Un),h(s,$a,o),h(s,C,o),a(C,Yn),a(C,Es),a(Es,Jn),a(C,Xn),a(C,Ts),a(Ts,Kn),a(C,Vn),Ea=!0},p(s,[o]){const bs={};o&2&&(bs.$$scope={dirty:o,ctx:s}),I.$set(bs)},i(s){Ea||(_(y.$$.fragment,s),_(I.$$.fragment,s),_(Z.$$.fragment,s),_(ss.$$.fragment,s),_(es.$$.fragment,s),_(ts.$$.fragment,s),_(ps.$$.fragment,s),_(rs.$$.fragment,s),_(os.$$.fragment,s),_(is.$$.fragment,s),_(ms.$$.fragment,s),_(us.$$.fragment,s),_(ds.$$.fragment,s),_(js.$$.fragment,s),Ea=!0)},o(s){w(y.$$.fragment,s),w(I.$$.fragment,s),w(Z.$$.fragment,s),w(ss.$$.fragment,s),w(es.$$.fragment,s),w(ts.$$.fragment,s),w(ps.$$.fragment,s),w(rs.$$.fragment,s),w(os.$$.fragment,s),w(is.$$.fragment,s),w(ms.$$.fragment,s),w(us.$$.fragment,s),w(ds.$$.fragment,s),w(js.$$.fragment,s),Ea=!1},d(s){n(d),s&&n(T),s&&n(u),k(y),s&&n(Zs),s&&n(O),s&&n(sa),s&&n(M),s&&n(aa),k(I,s),s&&n(na),s&&n(_s),s&&n(ea),k(Z,s),s&&n(la),s&&n(S),k(ss),s&&n(ta),s&&n(A),s&&n(pa),k(es,s),s&&n(ra),s&&n(G),s&&n(oa),k(ts,s),s&&n(ha),s&&n(P),k(ps),s&&n(ca),s&&n(Q),s&&n(ia),k(rs,s),s&&n(ma),s&&n(v),s&&n(ua),s&&n(D),k(os),s&&n(da),s&&n(ks),s&&n(ja),s&&n(ys),s&&n(ba),k(is,s),s&&n(fa),s&&n(B),s&&n(ga),s&&n(E),s&&n(_a),k(ms,s),s&&n(wa),s&&n(N),k(us),s&&n(ka),s&&n(vs),s&&n(ya),k(ds,s),s&&n(xa),s&&n(R),k(js),s&&n(va),s&&n($s),s&&n($a),s&&n(C)}}}const Ve={local:"quick-start",sections:[{local:"load-the-dataset-and-model",title:"Load the dataset and model"},{local:"tokenize-the-dataset",title:"Tokenize the dataset"},{local:"format-the-dataset",title:"Format the dataset"},{local:"train-the-model",title:"Train the model"},{local:"whats-next",title:"What's next?"}],title:"Quick Start"};function Ze(zs,d,T){let{fw:u}=d;return zs.$$set=j=>{"fw"in j&&T(0,u=j.fw)},[u]}class tl extends We{constructor(d){super();Qe(this,d,Ze,Ke,Ue,{fw:0})}}export{tl as default,Ve as metadata};
