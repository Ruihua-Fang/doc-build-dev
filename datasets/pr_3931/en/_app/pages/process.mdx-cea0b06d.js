import{S as Vb,i as Hb,s as Yb,e as t,k as h,w as m,t as r,M as Wb,c as n,d as e,m as c,a as l,x as f,h as p,b as d,F as a,g as i,y as u,q as g,o as _,B as j}from"../chunks/vendor-aa873a46.js";import{T as nl}from"../chunks/Tip-f7f252ab.js";import{I as E}from"../chunks/IconCopyLink-d0ca3106.js";import{C as w}from"../chunks/CodeBlock-1f14baf3.js";function Gb(z){let b,k,v,$,D;return{c(){b=t("p"),k=r("All the processing methods in this guide return a new "),v=t("a"),$=r("datasets.Dataset"),D=r(". Modification is not done in-place. Be careful about overriding your previous dataset!"),this.h()},l(x){b=n(x,"P",{});var y=l(b);k=p(y,"All the processing methods in this guide return a new "),v=n(y,"A",{href:!0});var A=l(v);$=p(A,"datasets.Dataset"),A.forEach(e),D=p(y,". Modification is not done in-place. Be careful about overriding your previous dataset!"),y.forEach(e),this.h()},h(){d(v,"href","/docs/datasets/pr_3931/en/package_reference/main_classes#datasets.Dataset")},m(x,y){i(x,b,y),a(b,k),a(b,v),a(v,$),a(b,D)},d(x){x&&e(b)}}}function Jb(z){let b,k,v,$,D,x,y,A;return{c(){b=t("p"),k=r("Casting only works if the original feature type and new feature type are compatible. For example, you can cast a column with the feature type "),v=t("code"),$=r("Value('int32')"),D=r(" to "),x=t("code"),y=r("Value('bool')"),A=r(" if the original column only contains ones and zeros.")},l(C){b=n(C,"P",{});var q=l(b);k=p(q,"Casting only works if the original feature type and new feature type are compatible. For example, you can cast a column with the feature type "),v=n(q,"CODE",{});var L=l(v);$=p(L,"Value('int32')"),L.forEach(e),D=p(q," to "),x=n(q,"CODE",{});var M=l(x);y=p(M,"Value('bool')"),M.forEach(e),A=p(q," if the original column only contains ones and zeros."),q.forEach(e)},m(C,q){i(C,b,q),a(b,k),a(b,v),a(v,$),a(b,D),a(b,x),a(x,y),a(b,A)},d(C){C&&e(b)}}}function Kb(z){let b,k,v,$,D;return{c(){b=t("p"),k=r("\u{1F917} Datasets also has a "),v=t("a"),$=r("datasets.Dataset.remove_columns()"),D=r(" method that is functionally identical, but faster, because it doesn\u2019t copy the data of the remaining columns."),this.h()},l(x){b=n(x,"P",{});var y=l(b);k=p(y,"\u{1F917} Datasets also has a "),v=n(y,"A",{href:!0});var A=l(v);$=p(A,"datasets.Dataset.remove_columns()"),A.forEach(e),D=p(y," method that is functionally identical, but faster, because it doesn\u2019t copy the data of the remaining columns."),y.forEach(e),this.h()},h(){d(v,"href","/docs/datasets/pr_3931/en/package_reference/main_classes#datasets.Dataset.remove_columns")},m(x,y){i(x,b,y),a(b,k),a(b,v),a(v,$),a(b,D)},d(x){x&&e(b)}}}function Qb(z){let b,k,v,$,D,x,y,A,C,q,L,M,Us,Q,X,Vs,P,U,Qa,Xa;return{c(){b=t("p"),k=r("You can also mix several datasets together by taking alternating examples from each one to create a new dataset. This is known as interleaving, and you can use it with "),v=t("a"),$=r("datasets.interleave_datasets()"),D=r(". Both "),x=t("a"),y=r("datasets.interleave_datasets()"),A=r(" and "),C=t("a"),q=r("datasets.concatenate_datasets()"),L=r(" will work with regular "),M=t("a"),Us=r("datasets.Dataset"),Q=r(" and "),X=t("a"),Vs=r("datasets.IterableDataset"),P=r(" objects. Refer to the "),U=t("a"),Qa=r("interleave_datasets"),Xa=r(" section for an example of how it\u2019s used."),this.h()},l(G){b=n(G,"P",{});var T=l(b);k=p(T,"You can also mix several datasets together by taking alternating examples from each one to create a new dataset. This is known as interleaving, and you can use it with "),v=n(T,"A",{href:!0});var qt=l(v);$=p(qt,"datasets.interleave_datasets()"),qt.forEach(e),D=p(T,". Both "),x=n(T,"A",{href:!0});var Hs=l(x);y=p(Hs,"datasets.interleave_datasets()"),Hs.forEach(e),A=p(T," and "),C=n(T,"A",{href:!0});var Ct=l(C);q=p(Ct,"datasets.concatenate_datasets()"),Ct.forEach(e),L=p(T," will work with regular "),M=n(T,"A",{href:!0});var Nt=l(M);Us=p(Nt,"datasets.Dataset"),Nt.forEach(e),Q=p(T," and "),X=n(T,"A",{href:!0});var Ys=l(X);Vs=p(Ys,"datasets.IterableDataset"),Ys.forEach(e),P=p(T," objects. Refer to the "),U=n(T,"A",{href:!0});var zt=l(U);Qa=p(zt,"interleave_datasets"),zt.forEach(e),Xa=p(T," section for an example of how it\u2019s used."),T.forEach(e),this.h()},h(){d(v,"href","/docs/datasets/pr_3931/en/package_reference/main_classes#datasets.interleave_datasets"),d(x,"href","/docs/datasets/pr_3931/en/package_reference/main_classes#datasets.interleave_datasets"),d(C,"href","/docs/datasets/pr_3931/en/package_reference/main_classes#datasets.concatenate_datasets"),d(M,"href","/docs/datasets/pr_3931/en/package_reference/main_classes#datasets.Dataset"),d(X,"href","/docs/datasets/pr_3931/en/package_reference/main_classes#datasets.IterableDataset"),d(U,"href","#interleave_datasets")},m(G,T){i(G,b,T),a(b,k),a(b,v),a(v,$),a(b,D),a(b,x),a(x,y),a(b,A),a(b,C),a(C,q),a(b,L),a(b,M),a(M,Us),a(b,Q),a(b,X),a(X,Vs),a(b,P),a(b,U),a(U,Qa),a(b,Xa)},d(G){G&&e(b)}}}function Xb(z){let b,k;return{c(){b=t("p"),k=r("\u{1F917} Datasets also provides support for other common data formats such as NumPy, PyTorch, Pandas, and JAX.")},l(v){b=n(v,"P",{});var $=l(b);k=p($,"\u{1F917} Datasets also provides support for other common data formats such as NumPy, PyTorch, Pandas, and JAX."),$.forEach(e)},m(v,$){i(v,b,$),a(b,k)},d(v){v&&e(b)}}}function Zb(z){let b,k,v,$,D;return{c(){b=t("p"),k=r("Want to save your dataset to a cloud storage provider? Read our "),v=t("a"),$=r("Cloud Storage"),D=r(" guide on how to save your dataset to AWS or Google Cloud Storage!"),this.h()},l(x){b=n(x,"P",{});var y=l(b);k=p(y,"Want to save your dataset to a cloud storage provider? Read our "),v=n(y,"A",{href:!0});var A=l(v);$=p(A,"Cloud Storage"),A.forEach(e),D=p(y," guide on how to save your dataset to AWS or Google Cloud Storage!"),y.forEach(e),this.h()},h(){d(v,"href","./filesystems")},m(x,y){i(x,b,y),a(b,k),a(b,v),a(v,$),a(b,D)},d(x){x&&e(b)}}}function s1(z){let b,k,v,$,D,x,y,A,C,q,L,M,Us,Q,X,Vs,P,U,Qa,Xa,G,T,qt,Hs,Ct,Nt,Ys,zt,Zh,ll,sc,ac,rl,ec,gp,Ft,tc,_p,Za,jp,Ws,bp,fs,Gs,pl,se,nc,ol,lc,vp,Ot,rc,wp,us,Js,il,ae,pc,hl,oc,xp,Ks,ic,Rt,hc,cc,$p,ee,yp,gs,Qs,cl,te,dc,dl,mc,kp,V,fc,It,uc,gc,ml,_c,jc,fl,bc,vc,Ep,ne,Dp,_s,Xs,ul,le,wc,gl,xc,Ap,Z,$c,Lt,yc,kc,Mt,Ec,Dc,Pp,Bt,Ut,Vt,Ac,Pc,Tp,re,Sp,Ht,Yt,Wt,Tc,Sc,qp,pe,Cp,js,Gt,qc,Cc,_l,Nc,zc,Np,oe,zp,bs,Zs,jl,ie,Fc,bl,Oc,Fp,vs,Jt,Rc,Ic,vl,Lc,Mc,Op,he,Rp,sa,Bc,wl,Uc,Vc,Ip,ws,aa,xl,ce,Hc,$l,Yc,Lp,H,Wc,yl,Gc,Jc,Kt,Kc,Qc,kl,Xc,Zc,Mp,ea,sd,de,ad,ed,Bp,me,Up,Qt,td,Vp,fe,Hp,xs,ta,El,ue,nd,Dl,ld,Yp,Xt,rd,Wp,$s,na,Al,ge,pd,Pl,od,Gp,la,id,Zt,hd,cd,Jp,ra,dd,sn,md,fd,Kp,_e,Qp,ys,pa,Tl,je,ud,Sl,gd,Xp,oa,_d,an,jd,bd,Zp,be,so,ks,ia,ql,ve,vd,Cl,wd,ao,B,en,xd,$d,Nl,yd,kd,zl,Ed,Dd,Fl,Ad,Pd,eo,we,to,ha,no,ca,Td,tn,Sd,qd,lo,xe,ro,Es,da,Ol,$e,Cd,Rl,Nd,po,ma,zd,nn,Fd,Od,oo,ye,io,F,Rd,Il,Id,Ld,Ll,Md,Bd,Ml,Ud,Vd,ln,Hd,Yd,ho,ke,co,ss,Wd,Bl,Gd,Jd,Ul,Kd,Qd,mo,Ds,fa,Vl,Ee,Xd,Hl,Zd,fo,ua,sm,rn,am,em,uo,ga,tm,De,nm,lm,go,Ae,_o,as,rm,Pe,pm,om,Yl,im,hm,jo,Te,bo,_a,cm,pn,dm,mm,vo,Se,wo,on,fm,xo,qe,$o,As,ja,Wl,Ce,um,Gl,gm,yo,es,_m,hn,jm,bm,cn,vm,wm,ko,O,xm,Jl,$m,ym,Kl,km,Em,Ql,Dm,Am,Xl,Pm,Tm,Eo,Ne,Do,ba,Sm,dn,qm,Cm,Ao,ze,Po,va,Nm,mn,zm,Fm,To,ts,Om,Zl,Rm,Im,fn,Lm,Mm,So,Fe,qo,wa,Co,ns,Bm,un,Um,Vm,sr,Hm,Ym,No,Oe,zo,S,Wm,gn,Gm,Jm,ar,Km,Qm,er,Xm,Zm,tr,sf,af,nr,ef,tf,lr,nf,lf,rr,rf,pf,Fo,Re,Oo,Ps,xa,pr,Ie,of,or,hf,Ro,ls,cf,ir,df,mf,_n,ff,uf,Io,Le,Lo,Ts,$a,hr,Me,gf,cr,_f,Mo,J,jn,jf,bf,dr,vf,wf,mr,xf,$f,Bo,Ss,ya,fr,Be,yf,ur,kf,Uo,bn,Ef,Vo,vn,Df,Ho,Ue,Yo,ka,Af,gr,Pf,Tf,Wo,Ve,Go,R,Sf,_r,qf,Cf,jr,Nf,zf,br,Ff,Of,vr,Rf,If,Jo,qs,Ea,wr,He,Lf,xr,Mf,Ko,wn,Bf,Qo,Da,$r,Ye,Uf,yr,Vf,Hf,Yf,kr,Er,Wf,Xo,We,Zo,Aa,Gf,xn,Jf,Kf,si,Ge,ai,$n,Qf,ei,Je,ti,Cs,Pa,Dr,Ke,Xf,Ar,Zf,ni,yn,su,li,rs,au,Qe,eu,tu,Xe,nu,lu,ri,Ze,pi,kn,ru,oi,st,ii,Ta,pu,En,ou,iu,hi,at,ci,I,hu,Pr,cu,du,Tr,mu,fu,Sr,uu,gu,qr,_u,ju,di,Ns,Sa,Cr,et,bu,Nr,vu,mi,ps,wu,Dn,xu,$u,zr,yu,ku,fi,tt,ui,zs,qa,Fr,nt,Eu,Or,Du,gi,os,Au,An,Pu,Tu,lt,Su,qu,_i,Ca,Cu,Rr,Nu,zu,ji,rt,bi,Fs,Na,Ir,pt,Fu,Lr,Ou,vi,za,Ru,Pn,Iu,Lu,wi,ot,xi,Fa,$i,Tn,Mu,yi,it,ki,Os,Oa,Mr,ht,Bu,Br,Uu,Ei,is,Vu,Sn,Hu,Yu,Ur,Wu,Gu,Di,ct,Ai,Y,Ju,Vr,Ku,Qu,dt,Xu,Zu,Hr,sg,ag,Pi,mt,Ti,Rs,qn,eg,tg,Cn,ng,lg,Si,ft,qi,Ra,Ci,Ia,rg,Nn,pg,og,Ni,ut,zi,Is,La,Yr,gt,ig,Wr,hg,Fi,_t,zn,cg,dg,Oi,jt,Ri,Fn,mg,Ii,Ls,Ma,Gr,bt,fg,Jr,ug,Li,Ba,gg,On,_g,jg,Mi,Rn,bg,Bi,vt,Ui,Ua,vg,In,wg,xg,Vi,wt,Hi,Va,Yi,Ms,Ha,Kr,xt,$g,Qr,yg,Wi,Ln,kg,Gi,Ya,Xr,$t,Zr,Eg,Dg,sp,Ag,Pg,K,yt,ap,Tg,Sg,ep,Mn,qg,Cg,kt,tp,Ng,zg,np,Bn,Fg,Og,Et,lp,Rg,Ig,rp,Un,Lg,Mg,Dt,pp,Bg,Ug,At,Vn,Vg,Hg,Hn,Yg,Ji,Yn,Wg,Ki,Pt,Qi;return x=new E({}),Za=new w({props:{code:`from datasets import load_dataset
dataset = load_dataset('glue', 'mrpc', split='train')`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;glue&#x27;</span>, <span class="hljs-string">&#x27;mrpc&#x27;</span>, split=<span class="hljs-string">&#x27;train&#x27;</span>)`}}),Ws=new nl({props:{warning:"&lcub;true}",$$slots:{default:[Gb]},$$scope:{ctx:z}}}),se=new E({}),ae=new E({}),ee=new w({props:{code:`dataset['label'][:10]
sorted_dataset = dataset.sort('label')
sorted_dataset['label'][:10]
sorted_dataset['label'][-10:]`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>dataset[<span class="hljs-string">&#x27;label&#x27;</span>][:<span class="hljs-number">10</span>]
[<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>sorted_dataset = dataset.sort(<span class="hljs-string">&#x27;label&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>sorted_dataset[<span class="hljs-string">&#x27;label&#x27;</span>][:<span class="hljs-number">10</span>]
[<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>sorted_dataset[<span class="hljs-string">&#x27;label&#x27;</span>][-<span class="hljs-number">10</span>:]
[<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]`}}),te=new E({}),ne=new w({props:{code:`shuffled_dataset = sorted_dataset.shuffle(seed=42)
shuffled_dataset['label'][:10]`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>shuffled_dataset = sorted_dataset.shuffle(seed=<span class="hljs-number">42</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>shuffled_dataset[<span class="hljs-string">&#x27;label&#x27;</span>][:<span class="hljs-number">10</span>]
[<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>]`}}),le=new E({}),re=new w({props:{code:`small_dataset = dataset.select([0, 10, 20, 30, 40, 50])
len(small_dataset)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>small_dataset = dataset.select([<span class="hljs-number">0</span>, <span class="hljs-number">10</span>, <span class="hljs-number">20</span>, <span class="hljs-number">30</span>, <span class="hljs-number">40</span>, <span class="hljs-number">50</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">len</span>(small_dataset)
<span class="hljs-number">6</span>`}}),pe=new w({props:{code:`start_with_ar = dataset.filter(lambda example: example['sentence1'].startswith('Ar'))
len(start_with_ar)
start_with_ar['sentence1']`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>start_with_ar = dataset.<span class="hljs-built_in">filter</span>(<span class="hljs-keyword">lambda</span> example: example[<span class="hljs-string">&#x27;sentence1&#x27;</span>].startswith(<span class="hljs-string">&#x27;Ar&#x27;</span>))
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">len</span>(start_with_ar)
<span class="hljs-number">6</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>start_with_ar[<span class="hljs-string">&#x27;sentence1&#x27;</span>]
[<span class="hljs-string">&#x27;Around 0335 GMT , Tab shares were up 19 cents , or 4.4 % , at A $ 4.56 , having earlier set a record high of A $ 4.57 .&#x27;</span>,
<span class="hljs-string">&#x27;Arison said Mann may have been one of the pioneers of the world music movement and he had a deep love of Brazilian music .&#x27;</span>,
<span class="hljs-string">&#x27;Arts helped coach the youth on an eighth-grade football team at Lombardi Middle School in Green Bay .&#x27;</span>,
<span class="hljs-string">&#x27;Around 9 : 00 a.m. EDT ( 1300 GMT ) , the euro was at $ 1.1566 against the dollar , up 0.07 percent on the day .&#x27;</span>,
<span class="hljs-string">&quot;Arguing that the case was an isolated example , Canada has threatened a trade backlash if Tokyo &#x27;s ban is not justified on scientific grounds .&quot;</span>,
<span class="hljs-string">&#x27;Artists are worried the plan would harm those who need help most - performers who have a difficult time lining up shows .&#x27;</span>
]`}}),oe=new w({props:{code:`even_dataset = dataset.filter(lambda example, idx: idx % 2 == 0, with_indices=True)
len(even_dataset)
len(dataset) / 2`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>even_dataset = dataset.<span class="hljs-built_in">filter</span>(<span class="hljs-keyword">lambda</span> example, idx: idx % <span class="hljs-number">2</span> == <span class="hljs-number">0</span>, with_indices=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">len</span>(even_dataset)
<span class="hljs-number">1834</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">len</span>(dataset) / <span class="hljs-number">2</span>
<span class="hljs-number">1834.0</span>`}}),ie=new E({}),he=new w({props:{code:`dataset.train_test_split(test_size=0.1)
0.1 * len(dataset)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>dataset.train_test_split(test_size=<span class="hljs-number">0.1</span>)
{<span class="hljs-string">&#x27;train&#x27;</span>: Dataset(schema: {<span class="hljs-string">&#x27;sentence1&#x27;</span>: <span class="hljs-string">&#x27;string&#x27;</span>, <span class="hljs-string">&#x27;sentence2&#x27;</span>: <span class="hljs-string">&#x27;string&#x27;</span>, <span class="hljs-string">&#x27;label&#x27;</span>: <span class="hljs-string">&#x27;int64&#x27;</span>, <span class="hljs-string">&#x27;idx&#x27;</span>: <span class="hljs-string">&#x27;int32&#x27;</span>}, num_rows: <span class="hljs-number">3301</span>),
<span class="hljs-string">&#x27;test&#x27;</span>: Dataset(schema: {<span class="hljs-string">&#x27;sentence1&#x27;</span>: <span class="hljs-string">&#x27;string&#x27;</span>, <span class="hljs-string">&#x27;sentence2&#x27;</span>: <span class="hljs-string">&#x27;string&#x27;</span>, <span class="hljs-string">&#x27;label&#x27;</span>: <span class="hljs-string">&#x27;int64&#x27;</span>, <span class="hljs-string">&#x27;idx&#x27;</span>: <span class="hljs-string">&#x27;int32&#x27;</span>}, num_rows: <span class="hljs-number">367</span>)}
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-number">0.1</span> * <span class="hljs-built_in">len</span>(dataset)
<span class="hljs-number">366.8</span>`}}),ce=new E({}),me=new w({props:{code:`from datasets import load_dataset
datasets = load_dataset('imdb', split='train')
print(dataset)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>datasets = load_dataset(<span class="hljs-string">&#x27;imdb&#x27;</span>, split=<span class="hljs-string">&#x27;train&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(dataset)
Dataset({
    features: [<span class="hljs-string">&#x27;text&#x27;</span>, <span class="hljs-string">&#x27;label&#x27;</span>],
    num_rows: <span class="hljs-number">25000</span>
})`}}),fe=new w({props:{code:`dataset.shard(num_shards=4, index=0)
print(25000/4)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>dataset.shard(num_shards=<span class="hljs-number">4</span>, index=<span class="hljs-number">0</span>)
Dataset({
    features: [<span class="hljs-string">&#x27;text&#x27;</span>, <span class="hljs-string">&#x27;label&#x27;</span>],
    num_rows: <span class="hljs-number">6250</span>
})
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(<span class="hljs-number">25000</span>/<span class="hljs-number">4</span>)
<span class="hljs-number">6250.0</span>`}}),ue=new E({}),ge=new E({}),_e=new w({props:{code:`dataset
dataset = dataset.rename_column("sentence1", "sentenceA")
dataset = dataset.rename_column("sentence2", "sentenceB")
dataset`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>dataset
Dataset({
    features: [<span class="hljs-string">&#x27;sentence1&#x27;</span>, <span class="hljs-string">&#x27;sentence2&#x27;</span>, <span class="hljs-string">&#x27;label&#x27;</span>, <span class="hljs-string">&#x27;idx&#x27;</span>],
    num_rows: <span class="hljs-number">3668</span>
})
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = dataset.rename_column(<span class="hljs-string">&quot;sentence1&quot;</span>, <span class="hljs-string">&quot;sentenceA&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = dataset.rename_column(<span class="hljs-string">&quot;sentence2&quot;</span>, <span class="hljs-string">&quot;sentenceB&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset
Dataset({
    features: [<span class="hljs-string">&#x27;sentenceA&#x27;</span>, <span class="hljs-string">&#x27;sentenceB&#x27;</span>, <span class="hljs-string">&#x27;label&#x27;</span>, <span class="hljs-string">&#x27;idx&#x27;</span>],
    num_rows: <span class="hljs-number">3668</span>
})`}}),je=new E({}),be=new w({props:{code:`dataset = dataset.remove_columns("label")
dataset
dataset = dataset.remove_columns(['sentence1', 'sentence2'])
dataset`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = dataset.remove_columns(<span class="hljs-string">&quot;label&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset
Dataset({
    features: [<span class="hljs-string">&#x27;sentence1&#x27;</span>, <span class="hljs-string">&#x27;sentence2&#x27;</span>, <span class="hljs-string">&#x27;idx&#x27;</span>],
    num_rows: <span class="hljs-number">3668</span>
})
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = dataset.remove_columns([<span class="hljs-string">&#x27;sentence1&#x27;</span>, <span class="hljs-string">&#x27;sentence2&#x27;</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset
Dataset({
    features: [<span class="hljs-string">&#x27;idx&#x27;</span>],
    num_rows: <span class="hljs-number">3668</span>
})`}}),ve=new E({}),we=new w({props:{code:`dataset.features

from datasets import ClassLabel, Value
new_features = dataset.features.copy()
new_features["label"] = ClassLabel(names=['negative', 'positive'])
new_features["idx"] = Value('int64')
dataset = dataset.cast(new_features)
dataset.features`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>dataset.features
{<span class="hljs-string">&#x27;sentence1&#x27;</span>: Value(dtype=<span class="hljs-string">&#x27;string&#x27;</span>, <span class="hljs-built_in">id</span>=<span class="hljs-literal">None</span>),
<span class="hljs-string">&#x27;sentence2&#x27;</span>: Value(dtype=<span class="hljs-string">&#x27;string&#x27;</span>, <span class="hljs-built_in">id</span>=<span class="hljs-literal">None</span>),
<span class="hljs-string">&#x27;label&#x27;</span>: ClassLabel(num_classes=<span class="hljs-number">2</span>, names=[<span class="hljs-string">&#x27;not_equivalent&#x27;</span>, <span class="hljs-string">&#x27;equivalent&#x27;</span>], names_file=<span class="hljs-literal">None</span>, <span class="hljs-built_in">id</span>=<span class="hljs-literal">None</span>),
<span class="hljs-string">&#x27;idx&#x27;</span>: Value(dtype=<span class="hljs-string">&#x27;int32&#x27;</span>, <span class="hljs-built_in">id</span>=<span class="hljs-literal">None</span>)}

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> ClassLabel, Value
<span class="hljs-meta">&gt;&gt;&gt; </span>new_features = dataset.features.copy()
<span class="hljs-meta">&gt;&gt;&gt; </span>new_features[<span class="hljs-string">&quot;label&quot;</span>] = ClassLabel(names=[<span class="hljs-string">&#x27;negative&#x27;</span>, <span class="hljs-string">&#x27;positive&#x27;</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>new_features[<span class="hljs-string">&quot;idx&quot;</span>] = Value(<span class="hljs-string">&#x27;int64&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = dataset.cast(new_features)
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset.features
{<span class="hljs-string">&#x27;sentence1&#x27;</span>: Value(dtype=<span class="hljs-string">&#x27;string&#x27;</span>, <span class="hljs-built_in">id</span>=<span class="hljs-literal">None</span>),
<span class="hljs-string">&#x27;sentence2&#x27;</span>: Value(dtype=<span class="hljs-string">&#x27;string&#x27;</span>, <span class="hljs-built_in">id</span>=<span class="hljs-literal">None</span>),
<span class="hljs-string">&#x27;label&#x27;</span>: ClassLabel(num_classes=<span class="hljs-number">2</span>, names=[<span class="hljs-string">&#x27;negative&#x27;</span>, <span class="hljs-string">&#x27;positive&#x27;</span>], names_file=<span class="hljs-literal">None</span>, <span class="hljs-built_in">id</span>=<span class="hljs-literal">None</span>),
<span class="hljs-string">&#x27;idx&#x27;</span>: Value(dtype=<span class="hljs-string">&#x27;int64&#x27;</span>, <span class="hljs-built_in">id</span>=<span class="hljs-literal">None</span>)}`}}),ha=new nl({props:{$$slots:{default:[Jb]},$$scope:{ctx:z}}}),xe=new w({props:{code:`dataset.features

dataset = dataset.cast_column("audio", Audio(sampling_rate=16000))
dataset.features`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>dataset.features
{<span class="hljs-string">&#x27;audio&#x27;</span>: Audio(sampling_rate=<span class="hljs-number">44100</span>, mono=<span class="hljs-literal">True</span>, <span class="hljs-built_in">id</span>=<span class="hljs-literal">None</span>)}

<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = dataset.cast_column(<span class="hljs-string">&quot;audio&quot;</span>, Audio(sampling_rate=<span class="hljs-number">16000</span>))
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset.features
{<span class="hljs-string">&#x27;audio&#x27;</span>: Audio(sampling_rate=<span class="hljs-number">16000</span>, mono=<span class="hljs-literal">True</span>, <span class="hljs-built_in">id</span>=<span class="hljs-literal">None</span>)}`}}),$e=new E({}),ye=new w({props:{code:`from datasets import load_dataset
dataset = load_dataset('squad', split='train')
dataset.features`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;squad&#x27;</span>, split=<span class="hljs-string">&#x27;train&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset.features
{<span class="hljs-string">&#x27;answers&#x27;</span>: <span class="hljs-type">Sequence</span>(feature={<span class="hljs-string">&#x27;text&#x27;</span>: Value(dtype=<span class="hljs-string">&#x27;string&#x27;</span>, <span class="hljs-built_in">id</span>=<span class="hljs-literal">None</span>), <span class="hljs-string">&#x27;answer_start&#x27;</span>: Value(dtype=<span class="hljs-string">&#x27;int32&#x27;</span>, <span class="hljs-built_in">id</span>=<span class="hljs-literal">None</span>)}, length=-<span class="hljs-number">1</span>, <span class="hljs-built_in">id</span>=<span class="hljs-literal">None</span>),
<span class="hljs-string">&#x27;context&#x27;</span>: Value(dtype=<span class="hljs-string">&#x27;string&#x27;</span>, <span class="hljs-built_in">id</span>=<span class="hljs-literal">None</span>),
<span class="hljs-string">&#x27;id&#x27;</span>: Value(dtype=<span class="hljs-string">&#x27;string&#x27;</span>, <span class="hljs-built_in">id</span>=<span class="hljs-literal">None</span>),
<span class="hljs-string">&#x27;question&#x27;</span>: Value(dtype=<span class="hljs-string">&#x27;string&#x27;</span>, <span class="hljs-built_in">id</span>=<span class="hljs-literal">None</span>),
<span class="hljs-string">&#x27;title&#x27;</span>: Value(dtype=<span class="hljs-string">&#x27;string&#x27;</span>, <span class="hljs-built_in">id</span>=<span class="hljs-literal">None</span>)}`}}),ke=new w({props:{code:`flat_dataset = dataset.flatten()
flat_dataset`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>flat_dataset = dataset.flatten()
<span class="hljs-meta">&gt;&gt;&gt; </span>flat_dataset
Dataset({
    features: [<span class="hljs-string">&#x27;id&#x27;</span>, <span class="hljs-string">&#x27;title&#x27;</span>, <span class="hljs-string">&#x27;context&#x27;</span>, <span class="hljs-string">&#x27;question&#x27;</span>, <span class="hljs-string">&#x27;answers.text&#x27;</span>, <span class="hljs-string">&#x27;answers.answer_start&#x27;</span>],
 num_rows: <span class="hljs-number">87599</span>
})`}}),Ee=new E({}),Ae=new w({props:{code:`from datasets import load_dataset

poem = load_dataset("poem_sentiment")
labels = poem["train"].features["label"].names
label2id, id2label = dict(), dict()
for i, label in enumerate(labels):
    label2id[label] = str(i)
    id2label[str(i)] = label
label2id`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

<span class="hljs-meta">&gt;&gt;&gt; </span>poem = load_dataset(<span class="hljs-string">&quot;poem_sentiment&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>labels = poem[<span class="hljs-string">&quot;train&quot;</span>].features[<span class="hljs-string">&quot;label&quot;</span>].names
<span class="hljs-meta">&gt;&gt;&gt; </span>label2id, id2label = <span class="hljs-built_in">dict</span>(), <span class="hljs-built_in">dict</span>()
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">for</span> i, label <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(labels):
<span class="hljs-meta">... </span>    label2id[label] = <span class="hljs-built_in">str</span>(i)
<span class="hljs-meta">... </span>    id2label[<span class="hljs-built_in">str</span>(i)] = label
<span class="hljs-meta">&gt;&gt;&gt; </span>label2id
{<span class="hljs-string">&#x27;mixed&#x27;</span>: <span class="hljs-string">&#x27;3&#x27;</span>, <span class="hljs-string">&#x27;negative&#x27;</span>: <span class="hljs-string">&#x27;0&#x27;</span>, <span class="hljs-string">&#x27;no_impact&#x27;</span>: <span class="hljs-string">&#x27;2&#x27;</span>, <span class="hljs-string">&#x27;positive&#x27;</span>: <span class="hljs-string">&#x27;1&#x27;</span>}`}}),Te=new w({props:{code:"label2id = {'negative': -1, 'no_impact': 0, 'positive': 1, 'mixed': 3}",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>label2id = {<span class="hljs-string">&#x27;negative&#x27;</span>: -<span class="hljs-number">1</span>, <span class="hljs-string">&#x27;no_impact&#x27;</span>: <span class="hljs-number">0</span>, <span class="hljs-string">&#x27;positive&#x27;</span>: <span class="hljs-number">1</span>, <span class="hljs-string">&#x27;mixed&#x27;</span>: <span class="hljs-number">3</span>}'}}),Se=new w({props:{code:'aligned_poem = poem.align_labels_with_mapping(label2id, "label")',highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>aligned_poem = poem.align_labels_with_mapping(label2id, <span class="hljs-string">&quot;label&quot;</span>)'}}),qe=new w({props:{code:"label2id",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>label2id
{<span class="hljs-string">&#x27;mixed&#x27;</span>: <span class="hljs-number">3</span>, <span class="hljs-string">&#x27;negative&#x27;</span>: -<span class="hljs-number">1</span>, <span class="hljs-string">&#x27;no_impact&#x27;</span>: <span class="hljs-number">0</span>, <span class="hljs-string">&#x27;positive&#x27;</span>: <span class="hljs-number">1</span>}`}}),Ce=new E({}),Ne=new w({props:{code:`def add_prefix(example):
    example['sentence1'] = 'My sentence: ' + example['sentence1']
    return example`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">def</span> <span class="hljs-title function_">add_prefix</span>(<span class="hljs-params">example</span>):
<span class="hljs-meta">... </span>    example[<span class="hljs-string">&#x27;sentence1&#x27;</span>] = <span class="hljs-string">&#x27;My sentence: &#x27;</span> + example[<span class="hljs-string">&#x27;sentence1&#x27;</span>]
<span class="hljs-meta">... </span>    <span class="hljs-keyword">return</span> example`}}),ze=new w({props:{code:`updated_dataset = small_dataset.map(add_prefix)
updated_dataset['sentence1'][:5]`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>updated_dataset = small_dataset.<span class="hljs-built_in">map</span>(add_prefix)
<span class="hljs-meta">&gt;&gt;&gt; </span>updated_dataset[<span class="hljs-string">&#x27;sentence1&#x27;</span>][:<span class="hljs-number">5</span>]
[<span class="hljs-string">&#x27;My sentence: Amrozi accused his brother , whom he called &quot; the witness &quot; , of deliberately distorting his evidence .&#x27;</span>,
<span class="hljs-string">&quot;My sentence: Yucaipa owned Dominick &#x27;s before selling the chain to Safeway in 1998 for $ 2.5 billion .&quot;</span>,
<span class="hljs-string">&#x27;My sentence: They had published an advertisement on the Internet on June 10 , offering the cargo for sale , he added .&#x27;</span>,
<span class="hljs-string">&#x27;My sentence: Around 0335 GMT , Tab shares were up 19 cents , or 4.4 % , at A $ 4.56 , having earlier set a record high of A $ 4.57 .&#x27;</span>,
]`}}),Fe=new w({props:{code:`updated_dataset = dataset.map(lambda example: {'new_sentence': example['sentence1']}, remove_columns=['sentence1'])
updated_dataset.column_names`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>updated_dataset = dataset.<span class="hljs-built_in">map</span>(<span class="hljs-keyword">lambda</span> example: {<span class="hljs-string">&#x27;new_sentence&#x27;</span>: example[<span class="hljs-string">&#x27;sentence1&#x27;</span>]}, remove_columns=[<span class="hljs-string">&#x27;sentence1&#x27;</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>updated_dataset.column_names
[<span class="hljs-string">&#x27;sentence2&#x27;</span>, <span class="hljs-string">&#x27;label&#x27;</span>, <span class="hljs-string">&#x27;idx&#x27;</span>, <span class="hljs-string">&#x27;new_sentence&#x27;</span>]`}}),wa=new nl({props:{$$slots:{default:[Kb]},$$scope:{ctx:z}}}),Oe=new w({props:{code:`updated_dataset = dataset.map(lambda example, idx: {'sentence2': f'{idx}: ' + example['sentence2']}, with_indices=True)
updated_dataset['sentence2'][:5]`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>updated_dataset = dataset.<span class="hljs-built_in">map</span>(<span class="hljs-keyword">lambda</span> example, idx: {<span class="hljs-string">&#x27;sentence2&#x27;</span>: <span class="hljs-string">f&#x27;<span class="hljs-subst">{idx}</span>: &#x27;</span> + example[<span class="hljs-string">&#x27;sentence2&#x27;</span>]}, with_indices=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>updated_dataset[<span class="hljs-string">&#x27;sentence2&#x27;</span>][:<span class="hljs-number">5</span>]
[<span class="hljs-string">&#x27;0: Referring to him as only &quot; the witness &quot; , Amrozi accused his brother of deliberately distorting his evidence .&#x27;</span>,
 <span class="hljs-string">&quot;1: Yucaipa bought Dominick &#x27;s in 1995 for $ 693 million and sold it to Safeway for $ 1.8 billion in 1998 .&quot;</span>,
 <span class="hljs-string">&quot;2: On June 10 , the ship &#x27;s owners had published an advertisement on the Internet , offering the explosives for sale .&quot;</span>,
 <span class="hljs-string">&#x27;3: Tab shares jumped 20 cents , or 4.6 % , to set a record closing high at A $ 4.57 .&#x27;</span>,
 <span class="hljs-string">&#x27;4: PG &amp; E Corp. shares jumped $ 1.63 or 8 percent to $ 21.03 on the New York Stock Exchange on Friday .&#x27;</span>
]`}}),Re=new w({props:{code:`from multiprocess import set_start_method
import torch
import os
set_start_method("spawn")
def gpu_computation(example, rank):
    os.environ["CUDA_VISIBLE_DEVICES"] = str(rank % torch.cuda.device_count())
    # Your big GPU call goes here
    return examples
updated_dataset = dataset.map(gpu_computation, with_rank=True)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> multiprocess <span class="hljs-keyword">import</span> set_start_method
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> os
&gt;&gt;&gt;
<span class="hljs-meta">&gt;&gt;&gt; </span>set_start_method(<span class="hljs-string">&quot;spawn&quot;</span>)
&gt;&gt;&gt;
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">def</span> <span class="hljs-title function_">gpu_computation</span>(<span class="hljs-params">example, rank</span>):
<span class="hljs-meta">&gt;&gt;&gt; </span>    os.environ[<span class="hljs-string">&quot;CUDA_VISIBLE_DEVICES&quot;</span>] = <span class="hljs-built_in">str</span>(rank % torch.cuda.device_count())
<span class="hljs-meta">&gt;&gt;&gt; </span>    <span class="hljs-comment"># Your big GPU call goes here</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>    <span class="hljs-keyword">return</span> examples
&gt;&gt;&gt;
<span class="hljs-meta">&gt;&gt;&gt; </span>updated_dataset = dataset.<span class="hljs-built_in">map</span>(gpu_computation, with_rank=<span class="hljs-literal">True</span>)`}}),Ie=new E({}),Le=new w({props:{code:"updated_dataset = dataset.map(lambda example, idx: {'sentence2': f'{idx}: ' + example['sentence2']}, num_proc=4)",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>updated_dataset = dataset.<span class="hljs-built_in">map</span>(<span class="hljs-keyword">lambda</span> example, idx: {<span class="hljs-string">&#x27;sentence2&#x27;</span>: <span class="hljs-string">f&#x27;<span class="hljs-subst">{idx}</span>: &#x27;</span> + example[<span class="hljs-string">&#x27;sentence2&#x27;</span>]}, num_proc=<span class="hljs-number">4</span>)'}}),Me=new E({}),Be=new E({}),Ue=new w({props:{code:`from transformers import BertTokenizerFast
tokenizer = BertTokenizerFast.from_pretrained('bert-base-cased')`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> BertTokenizerFast
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = BertTokenizerFast.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>)`}}),Ve=new w({props:{code:`encoded_dataset = dataset.map(lambda examples: tokenizer(examples['sentence1']), batched=True)
encoded_dataset.column_names
encoded_dataset[0]`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>encoded_dataset = dataset.<span class="hljs-built_in">map</span>(<span class="hljs-keyword">lambda</span> examples: tokenizer(examples[<span class="hljs-string">&#x27;sentence1&#x27;</span>]), batched=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>encoded_dataset.column_names
[<span class="hljs-string">&#x27;sentence1&#x27;</span>, <span class="hljs-string">&#x27;sentence2&#x27;</span>, <span class="hljs-string">&#x27;label&#x27;</span>, <span class="hljs-string">&#x27;idx&#x27;</span>, <span class="hljs-string">&#x27;input_ids&#x27;</span>, <span class="hljs-string">&#x27;token_type_ids&#x27;</span>, <span class="hljs-string">&#x27;attention_mask&#x27;</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>encoded_dataset[<span class="hljs-number">0</span>]
{<span class="hljs-string">&#x27;sentence1&#x27;</span>: <span class="hljs-string">&#x27;Amrozi accused his brother , whom he called &quot; the witness &quot; , of deliberately distorting his evidence .&#x27;</span>,
<span class="hljs-string">&#x27;sentence2&#x27;</span>: <span class="hljs-string">&#x27;Referring to him as only &quot; the witness &quot; , Amrozi accused his brother of deliberately distorting his evidence .&#x27;</span>,
<span class="hljs-string">&#x27;label&#x27;</span>: <span class="hljs-number">1</span>,
<span class="hljs-string">&#x27;idx&#x27;</span>: <span class="hljs-number">0</span>,
<span class="hljs-string">&#x27;input_ids&#x27;</span>: [  <span class="hljs-number">101</span>,  <span class="hljs-number">7277</span>,  <span class="hljs-number">2180</span>,  <span class="hljs-number">5303</span>,  <span class="hljs-number">4806</span>,  <span class="hljs-number">1117</span>,  <span class="hljs-number">1711</span>,   <span class="hljs-number">117</span>,  <span class="hljs-number">2292</span>, <span class="hljs-number">1119</span>,  <span class="hljs-number">1270</span>,   <span class="hljs-number">107</span>,  <span class="hljs-number">1103</span>,  <span class="hljs-number">7737</span>,   <span class="hljs-number">107</span>,   <span class="hljs-number">117</span>,  <span class="hljs-number">1104</span>,  <span class="hljs-number">9938</span>, <span class="hljs-number">4267</span>, <span class="hljs-number">12223</span>, <span class="hljs-number">21811</span>,  <span class="hljs-number">1117</span>,  <span class="hljs-number">2554</span>,   <span class="hljs-number">119</span>,   <span class="hljs-number">102</span>],
<span class="hljs-string">&#x27;token_type_ids&#x27;</span>: [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
<span class="hljs-string">&#x27;attention_mask&#x27;</span>: [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]
}`}}),He=new E({}),We=new w({props:{code:`def chunk_examples(examples):
    chunks = []
    for sentence in examples['sentence1']:
        chunks += [sentence[i:i + 50] for i in range(0, len(sentence), 50)]
    return {'chunks': chunks}`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">def</span> <span class="hljs-title function_">chunk_examples</span>(<span class="hljs-params">examples</span>):
<span class="hljs-meta">... </span>    chunks = []
<span class="hljs-meta">... </span>    <span class="hljs-keyword">for</span> sentence <span class="hljs-keyword">in</span> examples[<span class="hljs-string">&#x27;sentence1&#x27;</span>]:
<span class="hljs-meta">... </span>        chunks += [sentence[i:i + <span class="hljs-number">50</span>] <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, <span class="hljs-built_in">len</span>(sentence), <span class="hljs-number">50</span>)]
<span class="hljs-meta">... </span>    <span class="hljs-keyword">return</span> {<span class="hljs-string">&#x27;chunks&#x27;</span>: chunks}`}}),Ge=new w({props:{code:`chunked_dataset = dataset.map(chunk_examples, batched=True, remove_columns=dataset.column_names)
chunked_dataset[:10]`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>chunked_dataset = dataset.<span class="hljs-built_in">map</span>(chunk_examples, batched=<span class="hljs-literal">True</span>, remove_columns=dataset.column_names)
<span class="hljs-meta">&gt;&gt;&gt; </span>chunked_dataset[:<span class="hljs-number">10</span>]
{<span class="hljs-string">&#x27;chunks&#x27;</span>: [<span class="hljs-string">&#x27;Amrozi accused his brother , whom he called &quot; the &#x27;</span>,
            <span class="hljs-string">&#x27;witness &quot; , of deliberately distorting his evidenc&#x27;</span>,
            <span class="hljs-string">&#x27;e .&#x27;</span>,
            <span class="hljs-string">&quot;Yucaipa owned Dominick &#x27;s before selling the chain&quot;</span>,
            <span class="hljs-string">&#x27; to Safeway in 1998 for $ 2.5 billion .&#x27;</span>,
            <span class="hljs-string">&#x27;They had published an advertisement on the Interne&#x27;</span>,
            <span class="hljs-string">&#x27;t on June 10 , offering the cargo for sale , he ad&#x27;</span>,
            <span class="hljs-string">&#x27;ded .&#x27;</span>,
            <span class="hljs-string">&#x27;Around 0335 GMT , Tab shares were up 19 cents , or&#x27;</span>,
            <span class="hljs-string">&#x27; 4.4 % , at A $ 4.56 , having earlier set a record&#x27;</span>]}`}}),Je=new w({props:{code:`dataset
chunked_dataset`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>dataset
Dataset({
 features: [<span class="hljs-string">&#x27;sentence1&#x27;</span>, <span class="hljs-string">&#x27;sentence2&#x27;</span>, <span class="hljs-string">&#x27;label&#x27;</span>, <span class="hljs-string">&#x27;idx&#x27;</span>],
 num_rows: <span class="hljs-number">3668</span>
})
<span class="hljs-meta">&gt;&gt;&gt; </span>chunked_dataset
Dataset(schema: {<span class="hljs-string">&#x27;chunks&#x27;</span>: <span class="hljs-string">&#x27;string&#x27;</span>}, num_rows: <span class="hljs-number">10470</span>)`}}),Ke=new E({}),Ze=new w({props:{code:`from random import randint
from transformers import pipeline

fillmask = pipeline('fill-mask', model='roberta-base')
mask_token = fillmask.tokenizer.mask_token
smaller_dataset = dataset.filter(lambda e, i: i<100, with_indices=True)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> random <span class="hljs-keyword">import</span> randint
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline

<span class="hljs-meta">&gt;&gt;&gt; </span>fillmask = pipeline(<span class="hljs-string">&#x27;fill-mask&#x27;</span>, model=<span class="hljs-string">&#x27;roberta-base&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>mask_token = fillmask.tokenizer.mask_token
<span class="hljs-meta">&gt;&gt;&gt; </span>smaller_dataset = dataset.<span class="hljs-built_in">filter</span>(<span class="hljs-keyword">lambda</span> e, i: i&lt;<span class="hljs-number">100</span>, with_indices=<span class="hljs-literal">True</span>)`}}),st=new w({props:{code:`def augment_data(examples):
    outputs = []
    for sentence in examples['sentence1']:
        words = sentence.split(' ')
        K = randint(1, len(words)-1)
        masked_sentence = " ".join(words[:K]  + [mask_token] + words[K+1:])
        predictions = fillmask(masked_sentence)
        augmented_sequences = [predictions[i]['sequence'] for i in range(3)]
        outputs += [sentence] + augmented_sequences
    return {'data': outputs}`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">def</span> <span class="hljs-title function_">augment_data</span>(<span class="hljs-params">examples</span>):
<span class="hljs-meta">... </span>    outputs = []
<span class="hljs-meta">... </span>    <span class="hljs-keyword">for</span> sentence <span class="hljs-keyword">in</span> examples[<span class="hljs-string">&#x27;sentence1&#x27;</span>]:
<span class="hljs-meta">... </span>        words = sentence.split(<span class="hljs-string">&#x27; &#x27;</span>)
<span class="hljs-meta">... </span>        K = randint(<span class="hljs-number">1</span>, <span class="hljs-built_in">len</span>(words)-<span class="hljs-number">1</span>)
<span class="hljs-meta">... </span>        masked_sentence = <span class="hljs-string">&quot; &quot;</span>.join(words[:K]  + [mask_token] + words[K+<span class="hljs-number">1</span>:])
<span class="hljs-meta">... </span>        predictions = fillmask(masked_sentence)
<span class="hljs-meta">... </span>        augmented_sequences = [predictions[i][<span class="hljs-string">&#x27;sequence&#x27;</span>] <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">3</span>)]
<span class="hljs-meta">... </span>        outputs += [sentence] + augmented_sequences
...
<span class="hljs-meta">... </span>    <span class="hljs-keyword">return</span> {<span class="hljs-string">&#x27;data&#x27;</span>: outputs}`}}),at=new w({props:{code:`augmented_dataset = smaller_dataset.map(augment_data, batched=True, remove_columns=dataset.column_names, batch_size=8)
augmented_dataset[:9]['data']`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>augmented_dataset = smaller_dataset.<span class="hljs-built_in">map</span>(augment_data, batched=<span class="hljs-literal">True</span>, remove_columns=dataset.column_names, batch_size=<span class="hljs-number">8</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>augmented_dataset[:<span class="hljs-number">9</span>][<span class="hljs-string">&#x27;data&#x27;</span>]
[<span class="hljs-string">&#x27;Amrozi accused his brother , whom he called &quot; the witness &quot; , of deliberately distorting his evidence .&#x27;</span>,
 <span class="hljs-string">&#x27;Amrozi accused his brother, whom he called &quot; the witness &quot;, of deliberately withholding his evidence.&#x27;</span>,
 <span class="hljs-string">&#x27;Amrozi accused his brother, whom he called &quot; the witness &quot;, of deliberately suppressing his evidence.&#x27;</span>,
 <span class="hljs-string">&#x27;Amrozi accused his brother, whom he called &quot; the witness &quot;, of deliberately destroying his evidence.&#x27;</span>,
 <span class="hljs-string">&quot;Yucaipa owned Dominick &#x27;s before selling the chain to Safeway in 1998 for $ 2.5 billion .&quot;</span>,
 <span class="hljs-string">&#x27;Yucaipa owned Dominick Stores before selling the chain to Safeway in 1998 for $ 2.5 billion.&#x27;</span>,
 <span class="hljs-string">&quot;Yucaipa owned Dominick&#x27;s before selling the chain to Safeway in 1998 for $ 2.5 billion.&quot;</span>,
 <span class="hljs-string">&#x27;Yucaipa owned Dominick Pizza before selling the chain to Safeway in 1998 for $ 2.5 billion.&#x27;</span>
]`}}),et=new E({}),tt=new w({props:{code:`from datasets import load_dataset

dataset = load_dataset('glue', 'mrpc')
encoded_dataset = dataset.map(lambda examples: tokenizer(examples['sentence1']), batched=True)
encoded_dataset["train"][0]`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

<span class="hljs-comment"># load all the splits</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;glue&#x27;</span>, <span class="hljs-string">&#x27;mrpc&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>encoded_dataset = dataset.<span class="hljs-built_in">map</span>(<span class="hljs-keyword">lambda</span> examples: tokenizer(examples[<span class="hljs-string">&#x27;sentence1&#x27;</span>]), batched=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>encoded_dataset[<span class="hljs-string">&quot;train&quot;</span>][<span class="hljs-number">0</span>]
{<span class="hljs-string">&#x27;sentence1&#x27;</span>: <span class="hljs-string">&#x27;Amrozi accused his brother , whom he called &quot; the witness &quot; , of deliberately distorting his evidence .&#x27;</span>,
<span class="hljs-string">&#x27;sentence2&#x27;</span>: <span class="hljs-string">&#x27;Referring to him as only &quot; the witness &quot; , Amrozi accused his brother of deliberately distorting his evidence .&#x27;</span>,
<span class="hljs-string">&#x27;label&#x27;</span>: <span class="hljs-number">1</span>,
<span class="hljs-string">&#x27;idx&#x27;</span>: <span class="hljs-number">0</span>,
<span class="hljs-string">&#x27;input_ids&#x27;</span>: [  <span class="hljs-number">101</span>,  <span class="hljs-number">7277</span>,  <span class="hljs-number">2180</span>,  <span class="hljs-number">5303</span>,  <span class="hljs-number">4806</span>,  <span class="hljs-number">1117</span>,  <span class="hljs-number">1711</span>,   <span class="hljs-number">117</span>,  <span class="hljs-number">2292</span>, <span class="hljs-number">1119</span>,  <span class="hljs-number">1270</span>,   <span class="hljs-number">107</span>,  <span class="hljs-number">1103</span>,  <span class="hljs-number">7737</span>,   <span class="hljs-number">107</span>,   <span class="hljs-number">117</span>,  <span class="hljs-number">1104</span>,  <span class="hljs-number">9938</span>, <span class="hljs-number">4267</span>, <span class="hljs-number">12223</span>, <span class="hljs-number">21811</span>,  <span class="hljs-number">1117</span>,  <span class="hljs-number">2554</span>,   <span class="hljs-number">119</span>,   <span class="hljs-number">102</span>],
<span class="hljs-string">&#x27;token_type_ids&#x27;</span>: [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
<span class="hljs-string">&#x27;attention_mask&#x27;</span>: [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]
}`}}),nt=new E({}),rt=new w({props:{code:`from datasets import Dataset
import torch.distributed

dataset1 = Dataset.from_dict({"a": [0, 1, 2]})

if training_args.local_rank > 0:
    print("Waiting for main process to perform the mapping")
    torch.distributed.barrier()

dataset2 = dataset1.map(lambda x: {"a": x["a"] + 1})

if training_args.local_rank == 0:
    print("Loading results from main process")
    torch.distributed.barrier()`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> Dataset
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch.distributed

<span class="hljs-meta">&gt;&gt;&gt; </span>dataset1 = Dataset.from_dict({<span class="hljs-string">&quot;a&quot;</span>: [<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>]})

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">if</span> training_args.local_rank &gt; <span class="hljs-number">0</span>:
<span class="hljs-meta">... </span>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Waiting for main process to perform the mapping&quot;</span>)
<span class="hljs-meta">... </span>    torch.distributed.barrier()

<span class="hljs-meta">&gt;&gt;&gt; </span>dataset2 = dataset1.<span class="hljs-built_in">map</span>(<span class="hljs-keyword">lambda</span> x: {<span class="hljs-string">&quot;a&quot;</span>: x[<span class="hljs-string">&quot;a&quot;</span>] + <span class="hljs-number">1</span>})

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">if</span> training_args.local_rank == <span class="hljs-number">0</span>:
<span class="hljs-meta">... </span>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Loading results from main process&quot;</span>)
<span class="hljs-meta">... </span>    torch.distributed.barrier()`}}),pt=new E({}),ot=new w({props:{code:`from datasets import concatenate_datasets, load_dataset

bookcorpus = load_dataset("bookcorpus", split="train")
wiki = load_dataset("wikipedia", "20200501.en", split="train")
wiki = wiki.remove_columns("title")  # only keep the text

assert bookcorpus.features.type == wiki.features.type
bert_dataset = concatenate_datasets([bookcorpus, wiki])`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> concatenate_datasets, load_dataset

<span class="hljs-meta">&gt;&gt;&gt; </span>bookcorpus = load_dataset(<span class="hljs-string">&quot;bookcorpus&quot;</span>, split=<span class="hljs-string">&quot;train&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>wiki = load_dataset(<span class="hljs-string">&quot;wikipedia&quot;</span>, <span class="hljs-string">&quot;20200501.en&quot;</span>, split=<span class="hljs-string">&quot;train&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>wiki = wiki.remove_columns(<span class="hljs-string">&quot;title&quot;</span>)  <span class="hljs-comment"># only keep the text</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">assert</span> bookcorpus.features.<span class="hljs-built_in">type</span> == wiki.features.<span class="hljs-built_in">type</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>bert_dataset = concatenate_datasets([bookcorpus, wiki])`}}),Fa=new nl({props:{$$slots:{default:[Qb]},$$scope:{ctx:z}}}),it=new w({props:{code:`from datasets import Dataset
bookcorpus_ids = Dataset.from_dict({"ids": list(range(len(bookcorpus)))})
bookcorpus_with_ids = concatenate_datasets([bookcorpus, bookcorpus_ids], axis=1)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> Dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>bookcorpus_ids = Dataset.from_dict({<span class="hljs-string">&quot;ids&quot;</span>: <span class="hljs-built_in">list</span>(<span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(bookcorpus)))})
<span class="hljs-meta">&gt;&gt;&gt; </span>bookcorpus_with_ids = concatenate_datasets([bookcorpus, bookcorpus_ids], axis=<span class="hljs-number">1</span>)`}}),ht=new E({}),ct=new w({props:{code:`import tensorflow as tf
dataset.set_format(type='tensorflow', columns=['input_ids', 'token_type_ids', 'attention_mask', 'label'])`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset.set_format(<span class="hljs-built_in">type</span>=<span class="hljs-string">&#x27;tensorflow&#x27;</span>, columns=[<span class="hljs-string">&#x27;input_ids&#x27;</span>, <span class="hljs-string">&#x27;token_type_ids&#x27;</span>, <span class="hljs-string">&#x27;attention_mask&#x27;</span>, <span class="hljs-string">&#x27;label&#x27;</span>])`}}),mt=new w({props:{code:'tfdataset = tf.data.Dataset.from_tensor_slices((features, dataset["label"])).batch(32)',highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>tfdataset = tf.data.Dataset.from_tensor_slices((features, dataset[<span class="hljs-string">&quot;label&quot;</span>])).batch(<span class="hljs-number">32</span>)'}}),ft=new w({props:{code:"dataset = dataset.with_format(type='tensorflow', columns=['input_ids', 'token_type_ids', 'attention_mask', 'label'])",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = dataset.with_format(<span class="hljs-built_in">type</span>=<span class="hljs-string">&#x27;tensorflow&#x27;</span>, columns=[<span class="hljs-string">&#x27;input_ids&#x27;</span>, <span class="hljs-string">&#x27;token_type_ids&#x27;</span>, <span class="hljs-string">&#x27;attention_mask&#x27;</span>, <span class="hljs-string">&#x27;label&#x27;</span>])'}}),Ra=new nl({props:{$$slots:{default:[Xb]},$$scope:{ctx:z}}}),ut=new w({props:{code:`dataset.format
dataset.reset_format()
dataset.format`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>dataset.<span class="hljs-built_in">format</span>
{<span class="hljs-string">&#x27;type&#x27;</span>: <span class="hljs-string">&#x27;tensorflow&#x27;</span>, <span class="hljs-string">&#x27;format_kwargs&#x27;</span>: {}, <span class="hljs-string">&#x27;columns&#x27;</span>: [<span class="hljs-string">&#x27;label&#x27;</span>], <span class="hljs-string">&#x27;output_all_columns&#x27;</span>: <span class="hljs-literal">False</span>}
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset.reset_format()
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset.<span class="hljs-built_in">format</span>
{<span class="hljs-string">&#x27;type&#x27;</span>: <span class="hljs-string">&#x27;python&#x27;</span>, <span class="hljs-string">&#x27;format_kwargs&#x27;</span>: {}, <span class="hljs-string">&#x27;columns&#x27;</span>: [<span class="hljs-string">&#x27;idx&#x27;</span>, <span class="hljs-string">&#x27;label&#x27;</span>, <span class="hljs-string">&#x27;sentence1&#x27;</span>, <span class="hljs-string">&#x27;sentence2&#x27;</span>], <span class="hljs-string">&#x27;output_all_columns&#x27;</span>: <span class="hljs-literal">False</span>}`}}),gt=new E({}),jt=new w({props:{code:`from transformers import BertTokenizer
tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
def encode(batch):
    return tokenizer(batch["sentence1"], padding="longest", truncation=True, max_length=512, return_tensors="pt")
dataset.set_transform(encode)
dataset.format
dataset[:2]`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> BertTokenizer
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = BertTokenizer.from_pretrained(<span class="hljs-string">&quot;bert-base-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">def</span> <span class="hljs-title function_">encode</span>(<span class="hljs-params">batch</span>):
<span class="hljs-meta">... </span>    <span class="hljs-keyword">return</span> tokenizer(batch[<span class="hljs-string">&quot;sentence1&quot;</span>], padding=<span class="hljs-string">&quot;longest&quot;</span>, truncation=<span class="hljs-literal">True</span>, max_length=<span class="hljs-number">512</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset.set_transform(encode)
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset.<span class="hljs-built_in">format</span>
{<span class="hljs-string">&#x27;type&#x27;</span>: <span class="hljs-string">&#x27;custom&#x27;</span>, <span class="hljs-string">&#x27;format_kwargs&#x27;</span>: {<span class="hljs-string">&#x27;transform&#x27;</span>: &lt;function __main__.encode(batch)&gt;}, <span class="hljs-string">&#x27;columns&#x27;</span>: [<span class="hljs-string">&#x27;idx&#x27;</span>, <span class="hljs-string">&#x27;label&#x27;</span>, <span class="hljs-string">&#x27;sentence1&#x27;</span>, <span class="hljs-string">&#x27;sentence2&#x27;</span>], <span class="hljs-string">&#x27;output_all_columns&#x27;</span>: <span class="hljs-literal">False</span>}
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset[:<span class="hljs-number">2</span>]
{<span class="hljs-string">&#x27;input_ids&#x27;</span>: tensor([[  <span class="hljs-number">101</span>,  <span class="hljs-number">2572</span>,  <span class="hljs-number">3217</span>, ... <span class="hljs-number">102</span>]]), <span class="hljs-string">&#x27;token_type_ids&#x27;</span>: tensor([[<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, ... <span class="hljs-number">0</span>]]), <span class="hljs-string">&#x27;attention_mask&#x27;</span>: tensor([[<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, ... <span class="hljs-number">1</span>]])}`}}),bt=new E({}),vt=new w({props:{code:'encoded_dataset.save_to_disk("path/of/my/dataset/directory")',highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>encoded_dataset.save_to_disk(<span class="hljs-string">&quot;path/of/my/dataset/directory&quot;</span>)'}}),wt=new w({props:{code:`from datasets import load_from_disk
reloaded_encoded_dataset = load_from_disk("path/of/my/dataset/directory")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_from_disk
<span class="hljs-meta">&gt;&gt;&gt; </span>reloaded_encoded_dataset = load_from_disk(<span class="hljs-string">&quot;path/of/my/dataset/directory&quot;</span>)`}}),Va=new nl({props:{$$slots:{default:[Zb]},$$scope:{ctx:z}}}),xt=new E({}),Pt=new w({props:{code:'encoded_dataset.to_csv("path/of/my/dataset.csv")',highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>encoded_dataset.to_csv(<span class="hljs-string">&quot;path/of/my/dataset.csv&quot;</span>)'}}),{c(){b=t("meta"),k=h(),v=t("h1"),$=t("a"),D=t("span"),m(x.$$.fragment),y=h(),A=t("span"),C=r("Process"),q=h(),L=t("p"),M=r("\u{1F917} Datasets provides many tools for modifying the structure and content of a dataset. You can rearrange the order of rows or extract nested fields into their own columns. For more powerful processing applications, you can even alter the contents of a dataset by applying a function to the entire dataset to generate new rows and columns. These processing methods provide a lot of control and flexibility to mold your dataset into the desired shape and size with the appropriate features."),Us=h(),Q=t("p"),X=r("This guide will show you how to:"),Vs=h(),P=t("ul"),U=t("li"),Qa=r("Reorder rows and split the dataset."),Xa=h(),G=t("li"),T=r("Rename and remove columns, and other common column operations."),qt=h(),Hs=t("li"),Ct=r("Apply processing functions to each example in a dataset."),Nt=h(),Ys=t("li"),zt=r("Concatenate datasets."),Zh=h(),ll=t("li"),sc=r("Apply a custom formatting transform."),ac=h(),rl=t("li"),ec=r("Save and export processed datasets."),gp=h(),Ft=t("p"),tc=r("Load the MRPC dataset from the GLUE benchmark to follow along with our examples:"),_p=h(),m(Za.$$.fragment),jp=h(),m(Ws.$$.fragment),bp=h(),fs=t("h2"),Gs=t("a"),pl=t("span"),m(se.$$.fragment),nc=h(),ol=t("span"),lc=r("Sort, shuffle, select, split, and shard"),vp=h(),Ot=t("p"),rc=r("There are several methods for rearranging the structure of a dataset. These methods are useful for selecting only the rows you want, creating train and test splits, and sharding very large datasets into smaller chunks."),wp=h(),us=t("h3"),Js=t("a"),il=t("span"),m(ae.$$.fragment),pc=h(),hl=t("span"),oc=r("Sort"),xp=h(),Ks=t("p"),ic=r("Use "),Rt=t("a"),hc=r("datasets.Dataset.sort()"),cc=r(" to sort a columns values according to their numerical values. The provided column must be NumPy compatible."),$p=h(),m(ee.$$.fragment),yp=h(),gs=t("h3"),Qs=t("a"),cl=t("span"),m(te.$$.fragment),dc=h(),dl=t("span"),mc=r("Shuffle"),kp=h(),V=t("p"),fc=r("The "),It=t("a"),uc=r("datasets.Dataset.shuffle()"),gc=r(" method randomly rearranges the values of a column. You can specify the "),ml=t("code"),_c=r("generator"),jc=r(" argument in this method to use a different "),fl=t("code"),bc=r("numpy.random.Generator"),vc=r(" if you want more control over the algorithm used to shuffle the dataset."),Ep=h(),m(ne.$$.fragment),Dp=h(),_s=t("h3"),Xs=t("a"),ul=t("span"),m(le.$$.fragment),wc=h(),gl=t("span"),xc=r("Select and Filter"),Ap=h(),Z=t("p"),$c=r("There are two options for filtering rows in a dataset: "),Lt=t("a"),yc=r("datasets.Dataset.select()"),kc=r(" and "),Mt=t("a"),Ec=r("datasets.Dataset.filter()"),Dc=r("."),Pp=h(),Bt=t("ul"),Ut=t("li"),Vt=t("a"),Ac=r("datasets.Dataset.select()"),Pc=r(" returns rows according to a list of indices:"),Tp=h(),m(re.$$.fragment),Sp=h(),Ht=t("ul"),Yt=t("li"),Wt=t("a"),Tc=r("datasets.Dataset.filter()"),Sc=r(" returns rows that match a specified condition:"),qp=h(),m(pe.$$.fragment),Cp=h(),js=t("p"),Gt=t("a"),qc=r("datasets.Dataset.filter()"),Cc=r(" can also filter by indices if you set "),_l=t("code"),Nc=r("with_indices=True"),zc=r(":"),Np=h(),m(oe.$$.fragment),zp=h(),bs=t("h3"),Zs=t("a"),jl=t("span"),m(ie.$$.fragment),Fc=h(),bl=t("span"),Oc=r("Split"),Fp=h(),vs=t("p"),Jt=t("a"),Rc=r("datasets.Dataset.train_test_split()"),Ic=r(" creates train and test splits, if your dataset doesn\u2019t already have them. This allows you to adjust the relative proportions or absolute number of samples in each split. In the example below, use the "),vl=t("code"),Lc=r("test_size"),Mc=r(" argument to create a test split that is 10% of the original dataset:"),Op=h(),m(he.$$.fragment),Rp=h(),sa=t("p"),Bc=r("The splits are shuffled by default, but you can set "),wl=t("code"),Uc=r("shuffle=False"),Vc=r(" to prevent shuffling."),Ip=h(),ws=t("h3"),aa=t("a"),xl=t("span"),m(ce.$$.fragment),Hc=h(),$l=t("span"),Yc=r("Shard"),Lp=h(),H=t("p"),Wc=r("\u{1F917} Datasets supports sharding to divide a very large dataset into a predefined number of chunks. Specify the "),yl=t("code"),Gc=r("num_shards"),Jc=r(" argument in "),Kt=t("a"),Kc=r("datasets.Dataset.shard()"),Qc=r(" to determine the number of shards to split the dataset into. You will also need to provide the shard you want to return with the "),kl=t("code"),Xc=r("index"),Zc=r(" argument."),Mp=h(),ea=t("p"),sd=r("For example, the "),de=t("a"),ad=r("imdb"),ed=r(" dataset has 25000 examples:"),Bp=h(),m(me.$$.fragment),Up=h(),Qt=t("p"),td=r("After you shard the dataset into four chunks, the first shard only has 6250 examples:"),Vp=h(),m(fe.$$.fragment),Hp=h(),xs=t("h2"),ta=t("a"),El=t("span"),m(ue.$$.fragment),nd=h(),Dl=t("span"),ld=r("Rename, remove, cast, and flatten"),Yp=h(),Xt=t("p"),rd=r("The following methods allow you to modify the columns of a dataset. These methods are useful for renaming or removing columns, changing columns to a new set of features, and flattening nested column structures."),Wp=h(),$s=t("h3"),na=t("a"),Al=t("span"),m(ge.$$.fragment),pd=h(),Pl=t("span"),od=r("Rename"),Gp=h(),la=t("p"),id=r("Use "),Zt=t("a"),hd=r("datasets.Dataset.rename_column()"),cd=r(" when you need to rename a column in your dataset. Features associated with the original column are actually moved under the new column name, instead of just replacing the original column in-place."),Jp=h(),ra=t("p"),dd=r("Provide "),sn=t("a"),md=r("datasets.Dataset.rename_column()"),fd=r(" with the name of the original column, and the new column name:"),Kp=h(),m(_e.$$.fragment),Qp=h(),ys=t("h3"),pa=t("a"),Tl=t("span"),m(je.$$.fragment),ud=h(),Sl=t("span"),gd=r("Remove"),Xp=h(),oa=t("p"),_d=r("When you need to remove one or more columns, give "),an=t("a"),jd=r("datasets.Dataset.remove_columns()"),bd=r(" the name of the column to remove. Remove more than one column by providing a list of column names:"),Zp=h(),m(be.$$.fragment),so=h(),ks=t("h3"),ia=t("a"),ql=t("span"),m(ve.$$.fragment),vd=h(),Cl=t("span"),wd=r("Cast"),ao=h(),B=t("p"),en=t("a"),xd=r("datasets.Dataset.cast()"),$d=r(" changes the feature type of one or more columns. This method takes your new "),Nl=t("code"),yd=r("datasets.Features"),kd=r(" as its argument. The following sample code shows how to change the feature types of "),zl=t("code"),Ed=r("datasets.ClassLabel"),Dd=r(" and "),Fl=t("code"),Ad=r("datasets.Value"),Pd=r(":"),eo=h(),m(we.$$.fragment),to=h(),m(ha.$$.fragment),no=h(),ca=t("p"),Td=r("Use "),tn=t("a"),Sd=r("datasets.Dataset.cast_column()"),qd=r(" to change the feature type of just one column. Pass the column name and its new feature type as arguments:"),lo=h(),m(xe.$$.fragment),ro=h(),Es=t("h3"),da=t("a"),Ol=t("span"),m($e.$$.fragment),Cd=h(),Rl=t("span"),Nd=r("Flatten"),po=h(),ma=t("p"),zd=r("Sometimes a column can be a nested structure of several types. Use "),nn=t("a"),Fd=r("datasets.Dataset.flatten()"),Od=r(" to extract the subfields into their own separate columns. Take a look at the nested structure below from the SQuAD dataset:"),oo=h(),m(ye.$$.fragment),io=h(),F=t("p"),Rd=r("The "),Il=t("code"),Id=r("answers"),Ld=r(" field contains two subfields: "),Ll=t("code"),Md=r("text"),Bd=r(" and "),Ml=t("code"),Ud=r("answer_start"),Vd=r(". Flatten them with "),ln=t("a"),Hd=r("datasets.Dataset.flatten()"),Yd=r(":"),ho=h(),m(ke.$$.fragment),co=h(),ss=t("p"),Wd=r("Notice how the subfields are now their own independent columns: "),Bl=t("code"),Gd=r("answers.text"),Jd=r(" and "),Ul=t("code"),Kd=r("answers.answer_start"),Qd=r("."),mo=h(),Ds=t("h2"),fa=t("a"),Vl=t("span"),m(Ee.$$.fragment),Xd=h(),Hl=t("span"),Zd=r("Align"),fo=h(),ua=t("p"),sm=r("The "),rn=t("a"),am=r("align_labels_with_mapping()"),em=r(" function is used to align a dataset label id with the label name, or you can also assign a different mapping of label to ids."),uo=h(),ga=t("p"),tm=r("For example, the "),De=t("a"),nm=r("Poem Sentiment"),lm=r(" dataset uses the following label mapping:"),go=h(),m(Ae.$$.fragment),_o=h(),as=t("p"),rm=r("According to the Poem Sentiment "),Pe=t("a"),pm=r("dataset card"),om=r(", this mapping is different from the original label mapping. To use the original label mappings, create a dictionary of the name and id to align on. There was no "),Yl=t("code"),im=r("mixed"),hm=r(" label in the original dataset, so you don\u2019t have to change it:"),jo=h(),m(Te.$$.fragment),bo=h(),_a=t("p"),cm=r("Pass the dictionary of the original label mappings to the "),pn=t("a"),dm=r("align_labels_with_mapping()"),mm=r(" function, and the column to align on:"),vo=h(),m(Se.$$.fragment),wo=h(),on=t("p"),fm=r("Now when you look at the label mapping, it is aligned with the mappings from the original dataset:"),xo=h(),m(qe.$$.fragment),$o=h(),As=t("h2"),ja=t("a"),Wl=t("span"),m(Ce.$$.fragment),um=h(),Gl=t("span"),gm=r("Map"),yo=h(),es=t("p"),_m=r("Some of the more powerful applications of \u{1F917} Datasets come from using "),hn=t("a"),jm=r("datasets.Dataset.map()"),bm=r(". The primary purpose of "),cn=t("a"),vm=r("datasets.Dataset.map()"),wm=r(" is to speed up processing functions. It allows you to apply a processing function to each example in a dataset, independently or in batches. This function can even create new rows and columns."),ko=h(),O=t("p"),xm=r("In the following example, you will prefix each "),Jl=t("code"),$m=r("sentence1"),ym=r(" value in the dataset with "),Kl=t("code"),km=r("'My sentence: '"),Em=r(". First, create a function that adds "),Ql=t("code"),Dm=r("'My sentence: '"),Am=r(" to the beginning of each sentence. The function needs to accept and output a "),Xl=t("code"),Pm=r("dict"),Tm=r(":"),Eo=h(),m(Ne.$$.fragment),Do=h(),ba=t("p"),Sm=r("Next, apply this function to the dataset with "),dn=t("a"),qm=r("datasets.Dataset.map()"),Cm=r(":"),Ao=h(),m(ze.$$.fragment),Po=h(),va=t("p"),Nm=r("Let\u2019s take a look at another example, except this time, you will remove a column with "),mn=t("a"),zm=r("datasets.Dataset.map()"),Fm=r(". When you remove a column, it is only removed after the example has been provided to the mapped function. This allows the mapped function to use the content of the columns before they are removed."),To=h(),ts=t("p"),Om=r("Specify the column to remove with the "),Zl=t("code"),Rm=r("remove_columns"),Im=r(" argument in "),fn=t("a"),Lm=r("datasets.Dataset.map()"),Mm=r(":"),So=h(),m(Fe.$$.fragment),qo=h(),m(wa.$$.fragment),Co=h(),ns=t("p"),Bm=r("You can also use "),un=t("a"),Um=r("datasets.Dataset.map()"),Vm=r(" with indices if you set "),sr=t("code"),Hm=r("with_indices=True"),Ym=r(". The example below adds the index to the beginning of each sentence:"),No=h(),m(Oe.$$.fragment),zo=h(),S=t("p"),Wm=r("You can also use "),gn=t("a"),Gm=r("datasets.Dataset.map()"),Jm=r(" with the rank of the process if you set "),ar=t("code"),Km=r("with_rank=True"),Qm=r(". This is analogous to "),er=t("code"),Xm=r("with_indices"),Zm=r(". The "),tr=t("code"),sf=r("rank"),af=r(" argument in the mapped function goes after the "),nr=t("code"),ef=r("index"),tf=r(" one if it is already present. The main use-case for it is to parallelize your computation across several GPUs. This requires setting "),lr=t("em"),nf=r("multiprocess.set_start_method(\u201Cspawn\u201D)"),lf=r(", without which you will receive a CUDA error: "),rr=t("em"),rf=r("RuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the \u2018spawn\u2019 start method"),pf=r("."),Fo=h(),m(Re.$$.fragment),Oo=h(),Ps=t("h3"),xa=t("a"),pr=t("span"),m(Ie.$$.fragment),of=h(),or=t("span"),hf=r("Multiprocessing"),Ro=h(),ls=t("p"),cf=r("Multiprocessing can significantly speed up processing by parallelizing the processes on your CPU. Set the "),ir=t("code"),df=r("num_proc"),mf=r(" argument in "),_n=t("a"),ff=r("datasets.Dataset.map()"),uf=r(" to set the number of processes to use:"),Io=h(),m(Le.$$.fragment),Lo=h(),Ts=t("h3"),$a=t("a"),hr=t("span"),m(Me.$$.fragment),gf=h(),cr=t("span"),_f=r("Batch processing"),Mo=h(),J=t("p"),jn=t("a"),jf=r("datasets.Dataset.map()"),bf=r(" also supports working with batches of examples. Operate on batches by setting "),dr=t("code"),vf=r("batched=True"),wf=r(". The default batch size is 1000, but you can adjust it with the "),mr=t("code"),xf=r("batch_size"),$f=r(" argument. This opens the door to many interesting applications such as tokenization, splitting long sentences into shorter chunks, and data augmentation."),Bo=h(),Ss=t("h4"),ya=t("a"),fr=t("span"),m(Be.$$.fragment),yf=h(),ur=t("span"),kf=r("Tokenization"),Uo=h(),bn=t("p"),Ef=r("One of the most obvious use-cases for batch processing is tokenization, which accepts batches of inputs."),Vo=h(),vn=t("p"),Df=r("First, load the tokenizer from the BERT model:"),Ho=h(),m(Ue.$$.fragment),Yo=h(),ka=t("p"),Af=r("Apply the tokenizer to batches of the "),gr=t("code"),Pf=r("sentence1"),Tf=r(" field:"),Wo=h(),m(Ve.$$.fragment),Go=h(),R=t("p"),Sf=r("Now you have three new columns, "),_r=t("code"),qf=r("input_ids"),Cf=r(", "),jr=t("code"),Nf=r("token_type_ids"),zf=r(", "),br=t("code"),Ff=r("attention_mask"),Of=r(", that contain the encoded version of the "),vr=t("code"),Rf=r("sentence1"),If=r(" field."),Jo=h(),qs=t("h4"),Ea=t("a"),wr=t("span"),m(He.$$.fragment),Lf=h(),xr=t("span"),Mf=r("Split long examples"),Ko=h(),wn=t("p"),Bf=r("When your examples are too long, you may want to split them into several smaller snippets. Begin by creating a function that:"),Qo=h(),Da=t("ol"),$r=t("li"),Ye=t("p"),Uf=r("Splits the "),yr=t("code"),Vf=r("sentence1"),Hf=r(" field into snippets of 50 characters."),Yf=h(),kr=t("li"),Er=t("p"),Wf=r("Stacks all the snippets together to create the new dataset."),Xo=h(),m(We.$$.fragment),Zo=h(),Aa=t("p"),Gf=r("Apply the function with "),xn=t("a"),Jf=r("datasets.Dataset.map()"),Kf=r(":"),si=h(),m(Ge.$$.fragment),ai=h(),$n=t("p"),Qf=r("Notice how the sentences are split into shorter chunks now, and there are more rows in the dataset."),ei=h(),m(Je.$$.fragment),ti=h(),Cs=t("h4"),Pa=t("a"),Dr=t("span"),m(Ke.$$.fragment),Xf=h(),Ar=t("span"),Zf=r("Data augmentation"),ni=h(),yn=t("p"),su=r("With batch processing, you can even augment your dataset with additional examples. In the following example, you will generate additional words for a masked token in a sentence."),li=h(),rs=t("p"),au=r("Load the "),Qe=t("a"),eu=r("RoBERTA"),tu=r(" model for use in the \u{1F917} Transformer "),Xe=t("a"),nu=r("FillMaskPipeline"),lu=r(":"),ri=h(),m(Ze.$$.fragment),pi=h(),kn=t("p"),ru=r("Create a function to randomly select a word to mask in the sentence. The function should also return the original sentence and the top two replacements generated by RoBERTA."),oi=h(),m(st.$$.fragment),ii=h(),Ta=t("p"),pu=r("Use "),En=t("a"),ou=r("datasets.Dataset.map()"),iu=r(" to apply the function over the whole dataset:"),hi=h(),m(at.$$.fragment),ci=h(),I=t("p"),hu=r("For each original sentence, RoBERTA augmented a random word with three alternatives. In the first sentence, the word "),Pr=t("code"),cu=r("distorting"),du=r(" is augmented with "),Tr=t("code"),mu=r("withholding"),fu=r(", "),Sr=t("code"),uu=r("suppressing"),gu=r(", and "),qr=t("code"),_u=r("destroying"),ju=r("."),di=h(),Ns=t("h3"),Sa=t("a"),Cr=t("span"),m(et.$$.fragment),bu=h(),Nr=t("span"),vu=r("Process multiple splits"),mi=h(),ps=t("p"),wu=r("Many datasets have splits that you can process simultaneously with "),Dn=t("a"),xu=r("datasets.DatasetDict.map()"),$u=r(". For example, tokenize the "),zr=t("code"),yu=r("sentence1"),ku=r(" field in the train and test split by:"),fi=h(),m(tt.$$.fragment),ui=h(),zs=t("h3"),qa=t("a"),Fr=t("span"),m(nt.$$.fragment),Eu=h(),Or=t("span"),Du=r("Distributed usage"),gi=h(),os=t("p"),Au=r("When you use "),An=t("a"),Pu=r("datasets.Dataset.map()"),Tu=r(" in a distributed setting, you should also use "),lt=t("a"),Su=r("torch.distributed.barrier"),qu=r(". This ensures the main process performs the mapping, while the other processes load the results, thereby avoiding duplicate work."),_i=h(),Ca=t("p"),Cu=r("The following example shows how you can use "),Rr=t("code"),Nu=r("torch.distributed.barrier"),zu=r(" to synchronize the processes:"),ji=h(),m(rt.$$.fragment),bi=h(),Fs=t("h2"),Na=t("a"),Ir=t("span"),m(pt.$$.fragment),Fu=h(),Lr=t("span"),Ou=r("Concatenate"),vi=h(),za=t("p"),Ru=r("Separate datasets can be concatenated if they share the same column types. Concatenate datasets with "),Pn=t("a"),Iu=r("datasets.concatenate_datasets()"),Lu=r(":"),wi=h(),m(ot.$$.fragment),xi=h(),m(Fa.$$.fragment),$i=h(),Tn=t("p"),Mu=r("You can also concatenate two datasets horizontally (axis=1) as long as they have the same number of rows:"),yi=h(),m(it.$$.fragment),ki=h(),Os=t("h2"),Oa=t("a"),Mr=t("span"),m(ht.$$.fragment),Bu=h(),Br=t("span"),Uu=r("Format"),Ei=h(),is=t("p"),Vu=r("Set a dataset to a TensorFlow compatible format with "),Sn=t("a"),Hu=r("datasets.Dataset.set_format()"),Yu=r(". Specify "),Ur=t("code"),Wu=r("type=tensorflow"),Gu=r(" and the columns that should be formatted:"),Di=h(),m(ct.$$.fragment),Ai=h(),Y=t("p"),Ju=r("Then you can wrap the dataset with "),Vr=t("code"),Ku=r("tf.data.Dataset"),Qu=r(". This method gives you more control over how to create a "),dt=t("a"),Xu=r("TensorFlow Dataset"),Zu=r(". In the example below, the dataset is created "),Hr=t("code"),sg=r("from_tensor_slices"),ag=r(":"),Pi=h(),m(mt.$$.fragment),Ti=h(),Rs=t("p"),qn=t("a"),eg=r("datasets.Dataset.with_format()"),tg=r(" provides an alternative method to set the format. This method will return a new "),Cn=t("a"),ng=r("datasets.Dataset"),lg=r(" object with your specified format:"),Si=h(),m(ft.$$.fragment),qi=h(),m(Ra.$$.fragment),Ci=h(),Ia=t("p"),rg=r("Use "),Nn=t("a"),pg=r("datasets.Dataset.reset_format()"),og=r(" if you need to reset the dataset to the original format:"),Ni=h(),m(ut.$$.fragment),zi=h(),Is=t("h3"),La=t("a"),Yr=t("span"),m(gt.$$.fragment),ig=h(),Wr=t("span"),hg=r("Format transform"),Fi=h(),_t=t("p"),zn=t("a"),cg=r("datasets.Dataset.set_transform()"),dg=r(" allows you to apply a custom formatting transform on-the-fly. This will replace any previously specified format. For example, you can use this method to tokenize and pad tokens on-the-fly:"),Oi=h(),m(jt.$$.fragment),Ri=h(),Fn=t("p"),mg=r("In this case, the tokenization is applied only when the examples are accessed."),Ii=h(),Ls=t("h2"),Ma=t("a"),Gr=t("span"),m(bt.$$.fragment),fg=h(),Jr=t("span"),ug=r("Save"),Li=h(),Ba=t("p"),gg=r("Once you are done processing your dataset, you can save and reuse it later with "),On=t("a"),_g=r("datasets.Dataset.save_to_disk()"),jg=r("."),Mi=h(),Rn=t("p"),bg=r("Save your dataset by providing the path to the directory you wish to save it to:"),Bi=h(),m(vt.$$.fragment),Ui=h(),Ua=t("p"),vg=r("When you want to use your dataset again, use "),In=t("a"),wg=r("datasets.load_from_disk()"),xg=r(" to reload it:"),Vi=h(),m(wt.$$.fragment),Hi=h(),m(Va.$$.fragment),Yi=h(),Ms=t("h2"),Ha=t("a"),Kr=t("span"),m(xt.$$.fragment),$g=h(),Qr=t("span"),yg=r("Export"),Wi=h(),Ln=t("p"),kg=r("\u{1F917} Datasets supports exporting as well, so you can work with your dataset in other applications. The following table shows currently supported file formats you can export to:"),Gi=h(),Ya=t("table"),Xr=t("thead"),$t=t("tr"),Zr=t("th"),Eg=r("File type"),Dg=h(),sp=t("th"),Ag=r("Export method"),Pg=h(),K=t("tbody"),yt=t("tr"),ap=t("td"),Tg=r("CSV"),Sg=h(),ep=t("td"),Mn=t("a"),qg=r("datasets.Dataset.to_csv()"),Cg=h(),kt=t("tr"),tp=t("td"),Ng=r("JSON"),zg=h(),np=t("td"),Bn=t("a"),Fg=r("datasets.Dataset.to_json()"),Og=h(),Et=t("tr"),lp=t("td"),Rg=r("Parquet"),Ig=h(),rp=t("td"),Un=t("a"),Lg=r("datasets.Dataset.to_parquet()"),Mg=h(),Dt=t("tr"),pp=t("td"),Bg=r("In-memory Python object"),Ug=h(),At=t("td"),Vn=t("a"),Vg=r("datasets.Dataset.to_pandas()"),Hg=r(" or "),Hn=t("a"),Yg=r("datasets.Dataset.to_dict()"),Ji=h(),Yn=t("p"),Wg=r("For example, export your dataset to a CSV file like this:"),Ki=h(),m(Pt.$$.fragment),this.h()},l(s){const o=Wb('[data-svelte="svelte-1phssyn"]',document.head);b=n(o,"META",{name:!0,content:!0}),o.forEach(e),k=c(s),v=n(s,"H1",{class:!0});var Tt=l(v);$=n(Tt,"A",{id:!0,class:!0,href:!0});var op=l($);D=n(op,"SPAN",{});var ip=l(D);f(x.$$.fragment,ip),ip.forEach(e),op.forEach(e),y=c(Tt),A=n(Tt,"SPAN",{});var hp=l(A);C=p(hp,"Process"),hp.forEach(e),Tt.forEach(e),q=c(s),L=n(s,"P",{});var cp=l(L);M=p(cp,"\u{1F917} Datasets provides many tools for modifying the structure and content of a dataset. You can rearrange the order of rows or extract nested fields into their own columns. For more powerful processing applications, you can even alter the contents of a dataset by applying a function to the entire dataset to generate new rows and columns. These processing methods provide a lot of control and flexibility to mold your dataset into the desired shape and size with the appropriate features."),cp.forEach(e),Us=c(s),Q=n(s,"P",{});var dp=l(Q);X=p(dp,"This guide will show you how to:"),dp.forEach(e),Vs=c(s),P=n(s,"UL",{});var W=l(P);U=n(W,"LI",{});var Qg=l(U);Qa=p(Qg,"Reorder rows and split the dataset."),Qg.forEach(e),Xa=c(W),G=n(W,"LI",{});var Xg=l(G);T=p(Xg,"Rename and remove columns, and other common column operations."),Xg.forEach(e),qt=c(W),Hs=n(W,"LI",{});var Zg=l(Hs);Ct=p(Zg,"Apply processing functions to each example in a dataset."),Zg.forEach(e),Nt=c(W),Ys=n(W,"LI",{});var s_=l(Ys);zt=p(s_,"Concatenate datasets."),s_.forEach(e),Zh=c(W),ll=n(W,"LI",{});var a_=l(ll);sc=p(a_,"Apply a custom formatting transform."),a_.forEach(e),ac=c(W),rl=n(W,"LI",{});var e_=l(rl);ec=p(e_,"Save and export processed datasets."),e_.forEach(e),W.forEach(e),gp=c(s),Ft=n(s,"P",{});var t_=l(Ft);tc=p(t_,"Load the MRPC dataset from the GLUE benchmark to follow along with our examples:"),t_.forEach(e),_p=c(s),f(Za.$$.fragment,s),jp=c(s),f(Ws.$$.fragment,s),bp=c(s),fs=n(s,"H2",{class:!0});var Xi=l(fs);Gs=n(Xi,"A",{id:!0,class:!0,href:!0});var n_=l(Gs);pl=n(n_,"SPAN",{});var l_=l(pl);f(se.$$.fragment,l_),l_.forEach(e),n_.forEach(e),nc=c(Xi),ol=n(Xi,"SPAN",{});var r_=l(ol);lc=p(r_,"Sort, shuffle, select, split, and shard"),r_.forEach(e),Xi.forEach(e),vp=c(s),Ot=n(s,"P",{});var p_=l(Ot);rc=p(p_,"There are several methods for rearranging the structure of a dataset. These methods are useful for selecting only the rows you want, creating train and test splits, and sharding very large datasets into smaller chunks."),p_.forEach(e),wp=c(s),us=n(s,"H3",{class:!0});var Zi=l(us);Js=n(Zi,"A",{id:!0,class:!0,href:!0});var o_=l(Js);il=n(o_,"SPAN",{});var i_=l(il);f(ae.$$.fragment,i_),i_.forEach(e),o_.forEach(e),pc=c(Zi),hl=n(Zi,"SPAN",{});var h_=l(hl);oc=p(h_,"Sort"),h_.forEach(e),Zi.forEach(e),xp=c(s),Ks=n(s,"P",{});var sh=l(Ks);ic=p(sh,"Use "),Rt=n(sh,"A",{href:!0});var c_=l(Rt);hc=p(c_,"datasets.Dataset.sort()"),c_.forEach(e),cc=p(sh," to sort a columns values according to their numerical values. The provided column must be NumPy compatible."),sh.forEach(e),$p=c(s),f(ee.$$.fragment,s),yp=c(s),gs=n(s,"H3",{class:!0});var ah=l(gs);Qs=n(ah,"A",{id:!0,class:!0,href:!0});var d_=l(Qs);cl=n(d_,"SPAN",{});var m_=l(cl);f(te.$$.fragment,m_),m_.forEach(e),d_.forEach(e),dc=c(ah),dl=n(ah,"SPAN",{});var f_=l(dl);mc=p(f_,"Shuffle"),f_.forEach(e),ah.forEach(e),kp=c(s),V=n(s,"P",{});var Wa=l(V);fc=p(Wa,"The "),It=n(Wa,"A",{href:!0});var u_=l(It);uc=p(u_,"datasets.Dataset.shuffle()"),u_.forEach(e),gc=p(Wa," method randomly rearranges the values of a column. You can specify the "),ml=n(Wa,"CODE",{});var g_=l(ml);_c=p(g_,"generator"),g_.forEach(e),jc=p(Wa," argument in this method to use a different "),fl=n(Wa,"CODE",{});var __=l(fl);bc=p(__,"numpy.random.Generator"),__.forEach(e),vc=p(Wa," if you want more control over the algorithm used to shuffle the dataset."),Wa.forEach(e),Ep=c(s),f(ne.$$.fragment,s),Dp=c(s),_s=n(s,"H3",{class:!0});var eh=l(_s);Xs=n(eh,"A",{id:!0,class:!0,href:!0});var j_=l(Xs);ul=n(j_,"SPAN",{});var b_=l(ul);f(le.$$.fragment,b_),b_.forEach(e),j_.forEach(e),wc=c(eh),gl=n(eh,"SPAN",{});var v_=l(gl);xc=p(v_,"Select and Filter"),v_.forEach(e),eh.forEach(e),Ap=c(s),Z=n(s,"P",{});var Wn=l(Z);$c=p(Wn,"There are two options for filtering rows in a dataset: "),Lt=n(Wn,"A",{href:!0});var w_=l(Lt);yc=p(w_,"datasets.Dataset.select()"),w_.forEach(e),kc=p(Wn," and "),Mt=n(Wn,"A",{href:!0});var x_=l(Mt);Ec=p(x_,"datasets.Dataset.filter()"),x_.forEach(e),Dc=p(Wn,"."),Wn.forEach(e),Pp=c(s),Bt=n(s,"UL",{});var $_=l(Bt);Ut=n($_,"LI",{});var Gg=l(Ut);Vt=n(Gg,"A",{href:!0});var y_=l(Vt);Ac=p(y_,"datasets.Dataset.select()"),y_.forEach(e),Pc=p(Gg," returns rows according to a list of indices:"),Gg.forEach(e),$_.forEach(e),Tp=c(s),f(re.$$.fragment,s),Sp=c(s),Ht=n(s,"UL",{});var k_=l(Ht);Yt=n(k_,"LI",{});var Jg=l(Yt);Wt=n(Jg,"A",{href:!0});var E_=l(Wt);Tc=p(E_,"datasets.Dataset.filter()"),E_.forEach(e),Sc=p(Jg," returns rows that match a specified condition:"),Jg.forEach(e),k_.forEach(e),qp=c(s),f(pe.$$.fragment,s),Cp=c(s),js=n(s,"P",{});var mp=l(js);Gt=n(mp,"A",{href:!0});var D_=l(Gt);qc=p(D_,"datasets.Dataset.filter()"),D_.forEach(e),Cc=p(mp," can also filter by indices if you set "),_l=n(mp,"CODE",{});var A_=l(_l);Nc=p(A_,"with_indices=True"),A_.forEach(e),zc=p(mp,":"),mp.forEach(e),Np=c(s),f(oe.$$.fragment,s),zp=c(s),bs=n(s,"H3",{class:!0});var th=l(bs);Zs=n(th,"A",{id:!0,class:!0,href:!0});var P_=l(Zs);jl=n(P_,"SPAN",{});var T_=l(jl);f(ie.$$.fragment,T_),T_.forEach(e),P_.forEach(e),Fc=c(th),bl=n(th,"SPAN",{});var S_=l(bl);Oc=p(S_,"Split"),S_.forEach(e),th.forEach(e),Fp=c(s),vs=n(s,"P",{});var fp=l(vs);Jt=n(fp,"A",{href:!0});var q_=l(Jt);Rc=p(q_,"datasets.Dataset.train_test_split()"),q_.forEach(e),Ic=p(fp," creates train and test splits, if your dataset doesn\u2019t already have them. This allows you to adjust the relative proportions or absolute number of samples in each split. In the example below, use the "),vl=n(fp,"CODE",{});var C_=l(vl);Lc=p(C_,"test_size"),C_.forEach(e),Mc=p(fp," argument to create a test split that is 10% of the original dataset:"),fp.forEach(e),Op=c(s),f(he.$$.fragment,s),Rp=c(s),sa=n(s,"P",{});var nh=l(sa);Bc=p(nh,"The splits are shuffled by default, but you can set "),wl=n(nh,"CODE",{});var N_=l(wl);Uc=p(N_,"shuffle=False"),N_.forEach(e),Vc=p(nh," to prevent shuffling."),nh.forEach(e),Ip=c(s),ws=n(s,"H3",{class:!0});var lh=l(ws);aa=n(lh,"A",{id:!0,class:!0,href:!0});var z_=l(aa);xl=n(z_,"SPAN",{});var F_=l(xl);f(ce.$$.fragment,F_),F_.forEach(e),z_.forEach(e),Hc=c(lh),$l=n(lh,"SPAN",{});var O_=l($l);Yc=p(O_,"Shard"),O_.forEach(e),lh.forEach(e),Lp=c(s),H=n(s,"P",{});var Ga=l(H);Wc=p(Ga,"\u{1F917} Datasets supports sharding to divide a very large dataset into a predefined number of chunks. Specify the "),yl=n(Ga,"CODE",{});var R_=l(yl);Gc=p(R_,"num_shards"),R_.forEach(e),Jc=p(Ga," argument in "),Kt=n(Ga,"A",{href:!0});var I_=l(Kt);Kc=p(I_,"datasets.Dataset.shard()"),I_.forEach(e),Qc=p(Ga," to determine the number of shards to split the dataset into. You will also need to provide the shard you want to return with the "),kl=n(Ga,"CODE",{});var L_=l(kl);Xc=p(L_,"index"),L_.forEach(e),Zc=p(Ga," argument."),Ga.forEach(e),Mp=c(s),ea=n(s,"P",{});var rh=l(ea);sd=p(rh,"For example, the "),de=n(rh,"A",{href:!0,rel:!0});var M_=l(de);ad=p(M_,"imdb"),M_.forEach(e),ed=p(rh," dataset has 25000 examples:"),rh.forEach(e),Bp=c(s),f(me.$$.fragment,s),Up=c(s),Qt=n(s,"P",{});var B_=l(Qt);td=p(B_,"After you shard the dataset into four chunks, the first shard only has 6250 examples:"),B_.forEach(e),Vp=c(s),f(fe.$$.fragment,s),Hp=c(s),xs=n(s,"H2",{class:!0});var ph=l(xs);ta=n(ph,"A",{id:!0,class:!0,href:!0});var U_=l(ta);El=n(U_,"SPAN",{});var V_=l(El);f(ue.$$.fragment,V_),V_.forEach(e),U_.forEach(e),nd=c(ph),Dl=n(ph,"SPAN",{});var H_=l(Dl);ld=p(H_,"Rename, remove, cast, and flatten"),H_.forEach(e),ph.forEach(e),Yp=c(s),Xt=n(s,"P",{});var Y_=l(Xt);rd=p(Y_,"The following methods allow you to modify the columns of a dataset. These methods are useful for renaming or removing columns, changing columns to a new set of features, and flattening nested column structures."),Y_.forEach(e),Wp=c(s),$s=n(s,"H3",{class:!0});var oh=l($s);na=n(oh,"A",{id:!0,class:!0,href:!0});var W_=l(na);Al=n(W_,"SPAN",{});var G_=l(Al);f(ge.$$.fragment,G_),G_.forEach(e),W_.forEach(e),pd=c(oh),Pl=n(oh,"SPAN",{});var J_=l(Pl);od=p(J_,"Rename"),J_.forEach(e),oh.forEach(e),Gp=c(s),la=n(s,"P",{});var ih=l(la);id=p(ih,"Use "),Zt=n(ih,"A",{href:!0});var K_=l(Zt);hd=p(K_,"datasets.Dataset.rename_column()"),K_.forEach(e),cd=p(ih," when you need to rename a column in your dataset. Features associated with the original column are actually moved under the new column name, instead of just replacing the original column in-place."),ih.forEach(e),Jp=c(s),ra=n(s,"P",{});var hh=l(ra);dd=p(hh,"Provide "),sn=n(hh,"A",{href:!0});var Q_=l(sn);md=p(Q_,"datasets.Dataset.rename_column()"),Q_.forEach(e),fd=p(hh," with the name of the original column, and the new column name:"),hh.forEach(e),Kp=c(s),f(_e.$$.fragment,s),Qp=c(s),ys=n(s,"H3",{class:!0});var ch=l(ys);pa=n(ch,"A",{id:!0,class:!0,href:!0});var X_=l(pa);Tl=n(X_,"SPAN",{});var Z_=l(Tl);f(je.$$.fragment,Z_),Z_.forEach(e),X_.forEach(e),ud=c(ch),Sl=n(ch,"SPAN",{});var s2=l(Sl);gd=p(s2,"Remove"),s2.forEach(e),ch.forEach(e),Xp=c(s),oa=n(s,"P",{});var dh=l(oa);_d=p(dh,"When you need to remove one or more columns, give "),an=n(dh,"A",{href:!0});var a2=l(an);jd=p(a2,"datasets.Dataset.remove_columns()"),a2.forEach(e),bd=p(dh," the name of the column to remove. Remove more than one column by providing a list of column names:"),dh.forEach(e),Zp=c(s),f(be.$$.fragment,s),so=c(s),ks=n(s,"H3",{class:!0});var mh=l(ks);ia=n(mh,"A",{id:!0,class:!0,href:!0});var e2=l(ia);ql=n(e2,"SPAN",{});var t2=l(ql);f(ve.$$.fragment,t2),t2.forEach(e),e2.forEach(e),vd=c(mh),Cl=n(mh,"SPAN",{});var n2=l(Cl);wd=p(n2,"Cast"),n2.forEach(e),mh.forEach(e),ao=c(s),B=n(s,"P",{});var Bs=l(B);en=n(Bs,"A",{href:!0});var l2=l(en);xd=p(l2,"datasets.Dataset.cast()"),l2.forEach(e),$d=p(Bs," changes the feature type of one or more columns. This method takes your new "),Nl=n(Bs,"CODE",{});var r2=l(Nl);yd=p(r2,"datasets.Features"),r2.forEach(e),kd=p(Bs," as its argument. The following sample code shows how to change the feature types of "),zl=n(Bs,"CODE",{});var p2=l(zl);Ed=p(p2,"datasets.ClassLabel"),p2.forEach(e),Dd=p(Bs," and "),Fl=n(Bs,"CODE",{});var o2=l(Fl);Ad=p(o2,"datasets.Value"),o2.forEach(e),Pd=p(Bs,":"),Bs.forEach(e),eo=c(s),f(we.$$.fragment,s),to=c(s),f(ha.$$.fragment,s),no=c(s),ca=n(s,"P",{});var fh=l(ca);Td=p(fh,"Use "),tn=n(fh,"A",{href:!0});var i2=l(tn);Sd=p(i2,"datasets.Dataset.cast_column()"),i2.forEach(e),qd=p(fh," to change the feature type of just one column. Pass the column name and its new feature type as arguments:"),fh.forEach(e),lo=c(s),f(xe.$$.fragment,s),ro=c(s),Es=n(s,"H3",{class:!0});var uh=l(Es);da=n(uh,"A",{id:!0,class:!0,href:!0});var h2=l(da);Ol=n(h2,"SPAN",{});var c2=l(Ol);f($e.$$.fragment,c2),c2.forEach(e),h2.forEach(e),Cd=c(uh),Rl=n(uh,"SPAN",{});var d2=l(Rl);Nd=p(d2,"Flatten"),d2.forEach(e),uh.forEach(e),po=c(s),ma=n(s,"P",{});var gh=l(ma);zd=p(gh,"Sometimes a column can be a nested structure of several types. Use "),nn=n(gh,"A",{href:!0});var m2=l(nn);Fd=p(m2,"datasets.Dataset.flatten()"),m2.forEach(e),Od=p(gh," to extract the subfields into their own separate columns. Take a look at the nested structure below from the SQuAD dataset:"),gh.forEach(e),oo=c(s),f(ye.$$.fragment,s),io=c(s),F=n(s,"P",{});var hs=l(F);Rd=p(hs,"The "),Il=n(hs,"CODE",{});var f2=l(Il);Id=p(f2,"answers"),f2.forEach(e),Ld=p(hs," field contains two subfields: "),Ll=n(hs,"CODE",{});var u2=l(Ll);Md=p(u2,"text"),u2.forEach(e),Bd=p(hs," and "),Ml=n(hs,"CODE",{});var g2=l(Ml);Ud=p(g2,"answer_start"),g2.forEach(e),Vd=p(hs,". Flatten them with "),ln=n(hs,"A",{href:!0});var _2=l(ln);Hd=p(_2,"datasets.Dataset.flatten()"),_2.forEach(e),Yd=p(hs,":"),hs.forEach(e),ho=c(s),f(ke.$$.fragment,s),co=c(s),ss=n(s,"P",{});var Gn=l(ss);Wd=p(Gn,"Notice how the subfields are now their own independent columns: "),Bl=n(Gn,"CODE",{});var j2=l(Bl);Gd=p(j2,"answers.text"),j2.forEach(e),Jd=p(Gn," and "),Ul=n(Gn,"CODE",{});var b2=l(Ul);Kd=p(b2,"answers.answer_start"),b2.forEach(e),Qd=p(Gn,"."),Gn.forEach(e),mo=c(s),Ds=n(s,"H2",{class:!0});var _h=l(Ds);fa=n(_h,"A",{id:!0,class:!0,href:!0});var v2=l(fa);Vl=n(v2,"SPAN",{});var w2=l(Vl);f(Ee.$$.fragment,w2),w2.forEach(e),v2.forEach(e),Xd=c(_h),Hl=n(_h,"SPAN",{});var x2=l(Hl);Zd=p(x2,"Align"),x2.forEach(e),_h.forEach(e),fo=c(s),ua=n(s,"P",{});var jh=l(ua);sm=p(jh,"The "),rn=n(jh,"A",{href:!0});var $2=l(rn);am=p($2,"align_labels_with_mapping()"),$2.forEach(e),em=p(jh," function is used to align a dataset label id with the label name, or you can also assign a different mapping of label to ids."),jh.forEach(e),uo=c(s),ga=n(s,"P",{});var bh=l(ga);tm=p(bh,"For example, the "),De=n(bh,"A",{href:!0,rel:!0});var y2=l(De);nm=p(y2,"Poem Sentiment"),y2.forEach(e),lm=p(bh," dataset uses the following label mapping:"),bh.forEach(e),go=c(s),f(Ae.$$.fragment,s),_o=c(s),as=n(s,"P",{});var Jn=l(as);rm=p(Jn,"According to the Poem Sentiment "),Pe=n(Jn,"A",{href:!0,rel:!0});var k2=l(Pe);pm=p(k2,"dataset card"),k2.forEach(e),om=p(Jn,", this mapping is different from the original label mapping. To use the original label mappings, create a dictionary of the name and id to align on. There was no "),Yl=n(Jn,"CODE",{});var E2=l(Yl);im=p(E2,"mixed"),E2.forEach(e),hm=p(Jn," label in the original dataset, so you don\u2019t have to change it:"),Jn.forEach(e),jo=c(s),f(Te.$$.fragment,s),bo=c(s),_a=n(s,"P",{});var vh=l(_a);cm=p(vh,"Pass the dictionary of the original label mappings to the "),pn=n(vh,"A",{href:!0});var D2=l(pn);dm=p(D2,"align_labels_with_mapping()"),D2.forEach(e),mm=p(vh," function, and the column to align on:"),vh.forEach(e),vo=c(s),f(Se.$$.fragment,s),wo=c(s),on=n(s,"P",{});var A2=l(on);fm=p(A2,"Now when you look at the label mapping, it is aligned with the mappings from the original dataset:"),A2.forEach(e),xo=c(s),f(qe.$$.fragment,s),$o=c(s),As=n(s,"H2",{class:!0});var wh=l(As);ja=n(wh,"A",{id:!0,class:!0,href:!0});var P2=l(ja);Wl=n(P2,"SPAN",{});var T2=l(Wl);f(Ce.$$.fragment,T2),T2.forEach(e),P2.forEach(e),um=c(wh),Gl=n(wh,"SPAN",{});var S2=l(Gl);gm=p(S2,"Map"),S2.forEach(e),wh.forEach(e),yo=c(s),es=n(s,"P",{});var Kn=l(es);_m=p(Kn,"Some of the more powerful applications of \u{1F917} Datasets come from using "),hn=n(Kn,"A",{href:!0});var q2=l(hn);jm=p(q2,"datasets.Dataset.map()"),q2.forEach(e),bm=p(Kn,". The primary purpose of "),cn=n(Kn,"A",{href:!0});var C2=l(cn);vm=p(C2,"datasets.Dataset.map()"),C2.forEach(e),wm=p(Kn," is to speed up processing functions. It allows you to apply a processing function to each example in a dataset, independently or in batches. This function can even create new rows and columns."),Kn.forEach(e),ko=c(s),O=n(s,"P",{});var cs=l(O);xm=p(cs,"In the following example, you will prefix each "),Jl=n(cs,"CODE",{});var N2=l(Jl);$m=p(N2,"sentence1"),N2.forEach(e),ym=p(cs," value in the dataset with "),Kl=n(cs,"CODE",{});var z2=l(Kl);km=p(z2,"'My sentence: '"),z2.forEach(e),Em=p(cs,". First, create a function that adds "),Ql=n(cs,"CODE",{});var F2=l(Ql);Dm=p(F2,"'My sentence: '"),F2.forEach(e),Am=p(cs," to the beginning of each sentence. The function needs to accept and output a "),Xl=n(cs,"CODE",{});var O2=l(Xl);Pm=p(O2,"dict"),O2.forEach(e),Tm=p(cs,":"),cs.forEach(e),Eo=c(s),f(Ne.$$.fragment,s),Do=c(s),ba=n(s,"P",{});var xh=l(ba);Sm=p(xh,"Next, apply this function to the dataset with "),dn=n(xh,"A",{href:!0});var R2=l(dn);qm=p(R2,"datasets.Dataset.map()"),R2.forEach(e),Cm=p(xh,":"),xh.forEach(e),Ao=c(s),f(ze.$$.fragment,s),Po=c(s),va=n(s,"P",{});var $h=l(va);Nm=p($h,"Let\u2019s take a look at another example, except this time, you will remove a column with "),mn=n($h,"A",{href:!0});var I2=l(mn);zm=p(I2,"datasets.Dataset.map()"),I2.forEach(e),Fm=p($h,". When you remove a column, it is only removed after the example has been provided to the mapped function. This allows the mapped function to use the content of the columns before they are removed."),$h.forEach(e),To=c(s),ts=n(s,"P",{});var Qn=l(ts);Om=p(Qn,"Specify the column to remove with the "),Zl=n(Qn,"CODE",{});var L2=l(Zl);Rm=p(L2,"remove_columns"),L2.forEach(e),Im=p(Qn," argument in "),fn=n(Qn,"A",{href:!0});var M2=l(fn);Lm=p(M2,"datasets.Dataset.map()"),M2.forEach(e),Mm=p(Qn,":"),Qn.forEach(e),So=c(s),f(Fe.$$.fragment,s),qo=c(s),f(wa.$$.fragment,s),Co=c(s),ns=n(s,"P",{});var Xn=l(ns);Bm=p(Xn,"You can also use "),un=n(Xn,"A",{href:!0});var B2=l(un);Um=p(B2,"datasets.Dataset.map()"),B2.forEach(e),Vm=p(Xn," with indices if you set "),sr=n(Xn,"CODE",{});var U2=l(sr);Hm=p(U2,"with_indices=True"),U2.forEach(e),Ym=p(Xn,". The example below adds the index to the beginning of each sentence:"),Xn.forEach(e),No=c(s),f(Oe.$$.fragment,s),zo=c(s),S=n(s,"P",{});var N=l(S);Wm=p(N,"You can also use "),gn=n(N,"A",{href:!0});var V2=l(gn);Gm=p(V2,"datasets.Dataset.map()"),V2.forEach(e),Jm=p(N," with the rank of the process if you set "),ar=n(N,"CODE",{});var H2=l(ar);Km=p(H2,"with_rank=True"),H2.forEach(e),Qm=p(N,". This is analogous to "),er=n(N,"CODE",{});var Y2=l(er);Xm=p(Y2,"with_indices"),Y2.forEach(e),Zm=p(N,". The "),tr=n(N,"CODE",{});var W2=l(tr);sf=p(W2,"rank"),W2.forEach(e),af=p(N," argument in the mapped function goes after the "),nr=n(N,"CODE",{});var G2=l(nr);ef=p(G2,"index"),G2.forEach(e),tf=p(N," one if it is already present. The main use-case for it is to parallelize your computation across several GPUs. This requires setting "),lr=n(N,"EM",{});var J2=l(lr);nf=p(J2,"multiprocess.set_start_method(\u201Cspawn\u201D)"),J2.forEach(e),lf=p(N,", without which you will receive a CUDA error: "),rr=n(N,"EM",{});var K2=l(rr);rf=p(K2,"RuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the \u2018spawn\u2019 start method"),K2.forEach(e),pf=p(N,"."),N.forEach(e),Fo=c(s),f(Re.$$.fragment,s),Oo=c(s),Ps=n(s,"H3",{class:!0});var yh=l(Ps);xa=n(yh,"A",{id:!0,class:!0,href:!0});var Q2=l(xa);pr=n(Q2,"SPAN",{});var X2=l(pr);f(Ie.$$.fragment,X2),X2.forEach(e),Q2.forEach(e),of=c(yh),or=n(yh,"SPAN",{});var Z2=l(or);hf=p(Z2,"Multiprocessing"),Z2.forEach(e),yh.forEach(e),Ro=c(s),ls=n(s,"P",{});var Zn=l(ls);cf=p(Zn,"Multiprocessing can significantly speed up processing by parallelizing the processes on your CPU. Set the "),ir=n(Zn,"CODE",{});var sj=l(ir);df=p(sj,"num_proc"),sj.forEach(e),mf=p(Zn," argument in "),_n=n(Zn,"A",{href:!0});var aj=l(_n);ff=p(aj,"datasets.Dataset.map()"),aj.forEach(e),uf=p(Zn," to set the number of processes to use:"),Zn.forEach(e),Io=c(s),f(Le.$$.fragment,s),Lo=c(s),Ts=n(s,"H3",{class:!0});var kh=l(Ts);$a=n(kh,"A",{id:!0,class:!0,href:!0});var ej=l($a);hr=n(ej,"SPAN",{});var tj=l(hr);f(Me.$$.fragment,tj),tj.forEach(e),ej.forEach(e),gf=c(kh),cr=n(kh,"SPAN",{});var nj=l(cr);_f=p(nj,"Batch processing"),nj.forEach(e),kh.forEach(e),Mo=c(s),J=n(s,"P",{});var St=l(J);jn=n(St,"A",{href:!0});var lj=l(jn);jf=p(lj,"datasets.Dataset.map()"),lj.forEach(e),bf=p(St," also supports working with batches of examples. Operate on batches by setting "),dr=n(St,"CODE",{});var rj=l(dr);vf=p(rj,"batched=True"),rj.forEach(e),wf=p(St,". The default batch size is 1000, but you can adjust it with the "),mr=n(St,"CODE",{});var pj=l(mr);xf=p(pj,"batch_size"),pj.forEach(e),$f=p(St," argument. This opens the door to many interesting applications such as tokenization, splitting long sentences into shorter chunks, and data augmentation."),St.forEach(e),Bo=c(s),Ss=n(s,"H4",{class:!0});var Eh=l(Ss);ya=n(Eh,"A",{id:!0,class:!0,href:!0});var oj=l(ya);fr=n(oj,"SPAN",{});var ij=l(fr);f(Be.$$.fragment,ij),ij.forEach(e),oj.forEach(e),yf=c(Eh),ur=n(Eh,"SPAN",{});var hj=l(ur);kf=p(hj,"Tokenization"),hj.forEach(e),Eh.forEach(e),Uo=c(s),bn=n(s,"P",{});var cj=l(bn);Ef=p(cj,"One of the most obvious use-cases for batch processing is tokenization, which accepts batches of inputs."),cj.forEach(e),Vo=c(s),vn=n(s,"P",{});var dj=l(vn);Df=p(dj,"First, load the tokenizer from the BERT model:"),dj.forEach(e),Ho=c(s),f(Ue.$$.fragment,s),Yo=c(s),ka=n(s,"P",{});var Dh=l(ka);Af=p(Dh,"Apply the tokenizer to batches of the "),gr=n(Dh,"CODE",{});var mj=l(gr);Pf=p(mj,"sentence1"),mj.forEach(e),Tf=p(Dh," field:"),Dh.forEach(e),Wo=c(s),f(Ve.$$.fragment,s),Go=c(s),R=n(s,"P",{});var ds=l(R);Sf=p(ds,"Now you have three new columns, "),_r=n(ds,"CODE",{});var fj=l(_r);qf=p(fj,"input_ids"),fj.forEach(e),Cf=p(ds,", "),jr=n(ds,"CODE",{});var uj=l(jr);Nf=p(uj,"token_type_ids"),uj.forEach(e),zf=p(ds,", "),br=n(ds,"CODE",{});var gj=l(br);Ff=p(gj,"attention_mask"),gj.forEach(e),Of=p(ds,", that contain the encoded version of the "),vr=n(ds,"CODE",{});var _j=l(vr);Rf=p(_j,"sentence1"),_j.forEach(e),If=p(ds," field."),ds.forEach(e),Jo=c(s),qs=n(s,"H4",{class:!0});var Ah=l(qs);Ea=n(Ah,"A",{id:!0,class:!0,href:!0});var jj=l(Ea);wr=n(jj,"SPAN",{});var bj=l(wr);f(He.$$.fragment,bj),bj.forEach(e),jj.forEach(e),Lf=c(Ah),xr=n(Ah,"SPAN",{});var vj=l(xr);Mf=p(vj,"Split long examples"),vj.forEach(e),Ah.forEach(e),Ko=c(s),wn=n(s,"P",{});var wj=l(wn);Bf=p(wj,"When your examples are too long, you may want to split them into several smaller snippets. Begin by creating a function that:"),wj.forEach(e),Qo=c(s),Da=n(s,"OL",{});var Ph=l(Da);$r=n(Ph,"LI",{});var xj=l($r);Ye=n(xj,"P",{});var Th=l(Ye);Uf=p(Th,"Splits the "),yr=n(Th,"CODE",{});var $j=l(yr);Vf=p($j,"sentence1"),$j.forEach(e),Hf=p(Th," field into snippets of 50 characters."),Th.forEach(e),xj.forEach(e),Yf=c(Ph),kr=n(Ph,"LI",{});var yj=l(kr);Er=n(yj,"P",{});var kj=l(Er);Wf=p(kj,"Stacks all the snippets together to create the new dataset."),kj.forEach(e),yj.forEach(e),Ph.forEach(e),Xo=c(s),f(We.$$.fragment,s),Zo=c(s),Aa=n(s,"P",{});var Sh=l(Aa);Gf=p(Sh,"Apply the function with "),xn=n(Sh,"A",{href:!0});var Ej=l(xn);Jf=p(Ej,"datasets.Dataset.map()"),Ej.forEach(e),Kf=p(Sh,":"),Sh.forEach(e),si=c(s),f(Ge.$$.fragment,s),ai=c(s),$n=n(s,"P",{});var Dj=l($n);Qf=p(Dj,"Notice how the sentences are split into shorter chunks now, and there are more rows in the dataset."),Dj.forEach(e),ei=c(s),f(Je.$$.fragment,s),ti=c(s),Cs=n(s,"H4",{class:!0});var qh=l(Cs);Pa=n(qh,"A",{id:!0,class:!0,href:!0});var Aj=l(Pa);Dr=n(Aj,"SPAN",{});var Pj=l(Dr);f(Ke.$$.fragment,Pj),Pj.forEach(e),Aj.forEach(e),Xf=c(qh),Ar=n(qh,"SPAN",{});var Tj=l(Ar);Zf=p(Tj,"Data augmentation"),Tj.forEach(e),qh.forEach(e),ni=c(s),yn=n(s,"P",{});var Sj=l(yn);su=p(Sj,"With batch processing, you can even augment your dataset with additional examples. In the following example, you will generate additional words for a masked token in a sentence."),Sj.forEach(e),li=c(s),rs=n(s,"P",{});var sl=l(rs);au=p(sl,"Load the "),Qe=n(sl,"A",{href:!0,rel:!0});var qj=l(Qe);eu=p(qj,"RoBERTA"),qj.forEach(e),tu=p(sl," model for use in the \u{1F917} Transformer "),Xe=n(sl,"A",{href:!0,rel:!0});var Cj=l(Xe);nu=p(Cj,"FillMaskPipeline"),Cj.forEach(e),lu=p(sl,":"),sl.forEach(e),ri=c(s),f(Ze.$$.fragment,s),pi=c(s),kn=n(s,"P",{});var Nj=l(kn);ru=p(Nj,"Create a function to randomly select a word to mask in the sentence. The function should also return the original sentence and the top two replacements generated by RoBERTA."),Nj.forEach(e),oi=c(s),f(st.$$.fragment,s),ii=c(s),Ta=n(s,"P",{});var Ch=l(Ta);pu=p(Ch,"Use "),En=n(Ch,"A",{href:!0});var zj=l(En);ou=p(zj,"datasets.Dataset.map()"),zj.forEach(e),iu=p(Ch," to apply the function over the whole dataset:"),Ch.forEach(e),hi=c(s),f(at.$$.fragment,s),ci=c(s),I=n(s,"P",{});var ms=l(I);hu=p(ms,"For each original sentence, RoBERTA augmented a random word with three alternatives. In the first sentence, the word "),Pr=n(ms,"CODE",{});var Fj=l(Pr);cu=p(Fj,"distorting"),Fj.forEach(e),du=p(ms," is augmented with "),Tr=n(ms,"CODE",{});var Oj=l(Tr);mu=p(Oj,"withholding"),Oj.forEach(e),fu=p(ms,", "),Sr=n(ms,"CODE",{});var Rj=l(Sr);uu=p(Rj,"suppressing"),Rj.forEach(e),gu=p(ms,", and "),qr=n(ms,"CODE",{});var Ij=l(qr);_u=p(Ij,"destroying"),Ij.forEach(e),ju=p(ms,"."),ms.forEach(e),di=c(s),Ns=n(s,"H3",{class:!0});var Nh=l(Ns);Sa=n(Nh,"A",{id:!0,class:!0,href:!0});var Lj=l(Sa);Cr=n(Lj,"SPAN",{});var Mj=l(Cr);f(et.$$.fragment,Mj),Mj.forEach(e),Lj.forEach(e),bu=c(Nh),Nr=n(Nh,"SPAN",{});var Bj=l(Nr);vu=p(Bj,"Process multiple splits"),Bj.forEach(e),Nh.forEach(e),mi=c(s),ps=n(s,"P",{});var al=l(ps);wu=p(al,"Many datasets have splits that you can process simultaneously with "),Dn=n(al,"A",{href:!0});var Uj=l(Dn);xu=p(Uj,"datasets.DatasetDict.map()"),Uj.forEach(e),$u=p(al,". For example, tokenize the "),zr=n(al,"CODE",{});var Vj=l(zr);yu=p(Vj,"sentence1"),Vj.forEach(e),ku=p(al," field in the train and test split by:"),al.forEach(e),fi=c(s),f(tt.$$.fragment,s),ui=c(s),zs=n(s,"H3",{class:!0});var zh=l(zs);qa=n(zh,"A",{id:!0,class:!0,href:!0});var Hj=l(qa);Fr=n(Hj,"SPAN",{});var Yj=l(Fr);f(nt.$$.fragment,Yj),Yj.forEach(e),Hj.forEach(e),Eu=c(zh),Or=n(zh,"SPAN",{});var Wj=l(Or);Du=p(Wj,"Distributed usage"),Wj.forEach(e),zh.forEach(e),gi=c(s),os=n(s,"P",{});var el=l(os);Au=p(el,"When you use "),An=n(el,"A",{href:!0});var Gj=l(An);Pu=p(Gj,"datasets.Dataset.map()"),Gj.forEach(e),Tu=p(el," in a distributed setting, you should also use "),lt=n(el,"A",{href:!0,rel:!0});var Jj=l(lt);Su=p(Jj,"torch.distributed.barrier"),Jj.forEach(e),qu=p(el,". This ensures the main process performs the mapping, while the other processes load the results, thereby avoiding duplicate work."),el.forEach(e),_i=c(s),Ca=n(s,"P",{});var Fh=l(Ca);Cu=p(Fh,"The following example shows how you can use "),Rr=n(Fh,"CODE",{});var Kj=l(Rr);Nu=p(Kj,"torch.distributed.barrier"),Kj.forEach(e),zu=p(Fh," to synchronize the processes:"),Fh.forEach(e),ji=c(s),f(rt.$$.fragment,s),bi=c(s),Fs=n(s,"H2",{class:!0});var Oh=l(Fs);Na=n(Oh,"A",{id:!0,class:!0,href:!0});var Qj=l(Na);Ir=n(Qj,"SPAN",{});var Xj=l(Ir);f(pt.$$.fragment,Xj),Xj.forEach(e),Qj.forEach(e),Fu=c(Oh),Lr=n(Oh,"SPAN",{});var Zj=l(Lr);Ou=p(Zj,"Concatenate"),Zj.forEach(e),Oh.forEach(e),vi=c(s),za=n(s,"P",{});var Rh=l(za);Ru=p(Rh,"Separate datasets can be concatenated if they share the same column types. Concatenate datasets with "),Pn=n(Rh,"A",{href:!0});var sb=l(Pn);Iu=p(sb,"datasets.concatenate_datasets()"),sb.forEach(e),Lu=p(Rh,":"),Rh.forEach(e),wi=c(s),f(ot.$$.fragment,s),xi=c(s),f(Fa.$$.fragment,s),$i=c(s),Tn=n(s,"P",{});var ab=l(Tn);Mu=p(ab,"You can also concatenate two datasets horizontally (axis=1) as long as they have the same number of rows:"),ab.forEach(e),yi=c(s),f(it.$$.fragment,s),ki=c(s),Os=n(s,"H2",{class:!0});var Ih=l(Os);Oa=n(Ih,"A",{id:!0,class:!0,href:!0});var eb=l(Oa);Mr=n(eb,"SPAN",{});var tb=l(Mr);f(ht.$$.fragment,tb),tb.forEach(e),eb.forEach(e),Bu=c(Ih),Br=n(Ih,"SPAN",{});var nb=l(Br);Uu=p(nb,"Format"),nb.forEach(e),Ih.forEach(e),Ei=c(s),is=n(s,"P",{});var tl=l(is);Vu=p(tl,"Set a dataset to a TensorFlow compatible format with "),Sn=n(tl,"A",{href:!0});var lb=l(Sn);Hu=p(lb,"datasets.Dataset.set_format()"),lb.forEach(e),Yu=p(tl,". Specify "),Ur=n(tl,"CODE",{});var rb=l(Ur);Wu=p(rb,"type=tensorflow"),rb.forEach(e),Gu=p(tl," and the columns that should be formatted:"),tl.forEach(e),Di=c(s),f(ct.$$.fragment,s),Ai=c(s),Y=n(s,"P",{});var Ja=l(Y);Ju=p(Ja,"Then you can wrap the dataset with "),Vr=n(Ja,"CODE",{});var pb=l(Vr);Ku=p(pb,"tf.data.Dataset"),pb.forEach(e),Qu=p(Ja,". This method gives you more control over how to create a "),dt=n(Ja,"A",{href:!0,rel:!0});var ob=l(dt);Xu=p(ob,"TensorFlow Dataset"),ob.forEach(e),Zu=p(Ja,". In the example below, the dataset is created "),Hr=n(Ja,"CODE",{});var ib=l(Hr);sg=p(ib,"from_tensor_slices"),ib.forEach(e),ag=p(Ja,":"),Ja.forEach(e),Pi=c(s),f(mt.$$.fragment,s),Ti=c(s),Rs=n(s,"P",{});var up=l(Rs);qn=n(up,"A",{href:!0});var hb=l(qn);eg=p(hb,"datasets.Dataset.with_format()"),hb.forEach(e),tg=p(up," provides an alternative method to set the format. This method will return a new "),Cn=n(up,"A",{href:!0});var cb=l(Cn);ng=p(cb,"datasets.Dataset"),cb.forEach(e),lg=p(up," object with your specified format:"),up.forEach(e),Si=c(s),f(ft.$$.fragment,s),qi=c(s),f(Ra.$$.fragment,s),Ci=c(s),Ia=n(s,"P",{});var Lh=l(Ia);rg=p(Lh,"Use "),Nn=n(Lh,"A",{href:!0});var db=l(Nn);pg=p(db,"datasets.Dataset.reset_format()"),db.forEach(e),og=p(Lh," if you need to reset the dataset to the original format:"),Lh.forEach(e),Ni=c(s),f(ut.$$.fragment,s),zi=c(s),Is=n(s,"H3",{class:!0});var Mh=l(Is);La=n(Mh,"A",{id:!0,class:!0,href:!0});var mb=l(La);Yr=n(mb,"SPAN",{});var fb=l(Yr);f(gt.$$.fragment,fb),fb.forEach(e),mb.forEach(e),ig=c(Mh),Wr=n(Mh,"SPAN",{});var ub=l(Wr);hg=p(ub,"Format transform"),ub.forEach(e),Mh.forEach(e),Fi=c(s),_t=n(s,"P",{});var Kg=l(_t);zn=n(Kg,"A",{href:!0});var gb=l(zn);cg=p(gb,"datasets.Dataset.set_transform()"),gb.forEach(e),dg=p(Kg," allows you to apply a custom formatting transform on-the-fly. This will replace any previously specified format. For example, you can use this method to tokenize and pad tokens on-the-fly:"),Kg.forEach(e),Oi=c(s),f(jt.$$.fragment,s),Ri=c(s),Fn=n(s,"P",{});var _b=l(Fn);mg=p(_b,"In this case, the tokenization is applied only when the examples are accessed."),_b.forEach(e),Ii=c(s),Ls=n(s,"H2",{class:!0});var Bh=l(Ls);Ma=n(Bh,"A",{id:!0,class:!0,href:!0});var jb=l(Ma);Gr=n(jb,"SPAN",{});var bb=l(Gr);f(bt.$$.fragment,bb),bb.forEach(e),jb.forEach(e),fg=c(Bh),Jr=n(Bh,"SPAN",{});var vb=l(Jr);ug=p(vb,"Save"),vb.forEach(e),Bh.forEach(e),Li=c(s),Ba=n(s,"P",{});var Uh=l(Ba);gg=p(Uh,"Once you are done processing your dataset, you can save and reuse it later with "),On=n(Uh,"A",{href:!0});var wb=l(On);_g=p(wb,"datasets.Dataset.save_to_disk()"),wb.forEach(e),jg=p(Uh,"."),Uh.forEach(e),Mi=c(s),Rn=n(s,"P",{});var xb=l(Rn);bg=p(xb,"Save your dataset by providing the path to the directory you wish to save it to:"),xb.forEach(e),Bi=c(s),f(vt.$$.fragment,s),Ui=c(s),Ua=n(s,"P",{});var Vh=l(Ua);vg=p(Vh,"When you want to use your dataset again, use "),In=n(Vh,"A",{href:!0});var $b=l(In);wg=p($b,"datasets.load_from_disk()"),$b.forEach(e),xg=p(Vh," to reload it:"),Vh.forEach(e),Vi=c(s),f(wt.$$.fragment,s),Hi=c(s),f(Va.$$.fragment,s),Yi=c(s),Ms=n(s,"H2",{class:!0});var Hh=l(Ms);Ha=n(Hh,"A",{id:!0,class:!0,href:!0});var yb=l(Ha);Kr=n(yb,"SPAN",{});var kb=l(Kr);f(xt.$$.fragment,kb),kb.forEach(e),yb.forEach(e),$g=c(Hh),Qr=n(Hh,"SPAN",{});var Eb=l(Qr);yg=p(Eb,"Export"),Eb.forEach(e),Hh.forEach(e),Wi=c(s),Ln=n(s,"P",{});var Db=l(Ln);kg=p(Db,"\u{1F917} Datasets supports exporting as well, so you can work with your dataset in other applications. The following table shows currently supported file formats you can export to:"),Db.forEach(e),Gi=c(s),Ya=n(s,"TABLE",{});var Yh=l(Ya);Xr=n(Yh,"THEAD",{});var Ab=l(Xr);$t=n(Ab,"TR",{});var Wh=l($t);Zr=n(Wh,"TH",{});var Pb=l(Zr);Eg=p(Pb,"File type"),Pb.forEach(e),Dg=c(Wh),sp=n(Wh,"TH",{});var Tb=l(sp);Ag=p(Tb,"Export method"),Tb.forEach(e),Wh.forEach(e),Ab.forEach(e),Pg=c(Yh),K=n(Yh,"TBODY",{});var Ka=l(K);yt=n(Ka,"TR",{});var Gh=l(yt);ap=n(Gh,"TD",{});var Sb=l(ap);Tg=p(Sb,"CSV"),Sb.forEach(e),Sg=c(Gh),ep=n(Gh,"TD",{});var qb=l(ep);Mn=n(qb,"A",{href:!0});var Cb=l(Mn);qg=p(Cb,"datasets.Dataset.to_csv()"),Cb.forEach(e),qb.forEach(e),Gh.forEach(e),Cg=c(Ka),kt=n(Ka,"TR",{});var Jh=l(kt);tp=n(Jh,"TD",{});var Nb=l(tp);Ng=p(Nb,"JSON"),Nb.forEach(e),zg=c(Jh),np=n(Jh,"TD",{});var zb=l(np);Bn=n(zb,"A",{href:!0});var Fb=l(Bn);Fg=p(Fb,"datasets.Dataset.to_json()"),Fb.forEach(e),zb.forEach(e),Jh.forEach(e),Og=c(Ka),Et=n(Ka,"TR",{});var Kh=l(Et);lp=n(Kh,"TD",{});var Ob=l(lp);Rg=p(Ob,"Parquet"),Ob.forEach(e),Ig=c(Kh),rp=n(Kh,"TD",{});var Rb=l(rp);Un=n(Rb,"A",{href:!0});var Ib=l(Un);Lg=p(Ib,"datasets.Dataset.to_parquet()"),Ib.forEach(e),Rb.forEach(e),Kh.forEach(e),Mg=c(Ka),Dt=n(Ka,"TR",{});var Qh=l(Dt);pp=n(Qh,"TD",{});var Lb=l(pp);Bg=p(Lb,"In-memory Python object"),Lb.forEach(e),Ug=c(Qh),At=n(Qh,"TD",{});var Xh=l(At);Vn=n(Xh,"A",{href:!0});var Mb=l(Vn);Vg=p(Mb,"datasets.Dataset.to_pandas()"),Mb.forEach(e),Hg=p(Xh," or "),Hn=n(Xh,"A",{href:!0});var Bb=l(Hn);Yg=p(Bb,"datasets.Dataset.to_dict()"),Bb.forEach(e),Xh.forEach(e),Qh.forEach(e),Ka.forEach(e),Yh.forEach(e),Ji=c(s),Yn=n(s,"P",{});var Ub=l(Yn);Wg=p(Ub,"For example, export your dataset to a CSV file like this:"),Ub.forEach(e),Ki=c(s),f(Pt.$$.fragment,s),this.h()},h(){d(b,"name","hf:doc:metadata"),d(b,"content",JSON.stringify(a1)),d($,"id","process"),d($,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d($,"href","#process"),d(v,"class","relative group"),d(Gs,"id","sort-shuffle-select-split-and-shard"),d(Gs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Gs,"href","#sort-shuffle-select-split-and-shard"),d(fs,"class","relative group"),d(Js,"id","sort"),d(Js,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Js,"href","#sort"),d(us,"class","relative group"),d(Rt,"href","/docs/datasets/pr_3931/en/package_reference/main_classes#datasets.Dataset.sort"),d(Qs,"id","shuffle"),d(Qs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Qs,"href","#shuffle"),d(gs,"class","relative group"),d(It,"href","/docs/datasets/pr_3931/en/package_reference/main_classes#datasets.Dataset.shuffle"),d(Xs,"id","select-and-filter"),d(Xs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Xs,"href","#select-and-filter"),d(_s,"class","relative group"),d(Lt,"href","/docs/datasets/pr_3931/en/package_reference/main_classes#datasets.Dataset.select"),d(Mt,"href","/docs/datasets/pr_3931/en/package_reference/main_classes#datasets.Dataset.filter"),d(Vt,"href","/docs/datasets/pr_3931/en/package_reference/main_classes#datasets.Dataset.select"),d(Wt,"href","/docs/datasets/pr_3931/en/package_reference/main_classes#datasets.Dataset.filter"),d(Gt,"href","/docs/datasets/pr_3931/en/package_reference/main_classes#datasets.Dataset.filter"),d(Zs,"id","split"),d(Zs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Zs,"href","#split"),d(bs,"class","relative group"),d(Jt,"href","/docs/datasets/pr_3931/en/package_reference/main_classes#datasets.Dataset.train_test_split"),d(aa,"id","shard"),d(aa,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(aa,"href","#shard"),d(ws,"class","relative group"),d(Kt,"href","/docs/datasets/pr_3931/en/package_reference/main_classes#datasets.Dataset.shard"),d(de,"href","https://huggingface.co/datasets/imdb"),d(de,"rel","nofollow"),d(ta,"id","rename-remove-cast-and-flatten"),d(ta,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(ta,"href","#rename-remove-cast-and-flatten"),d(xs,"class","relative group"),d(na,"id","rename"),d(na,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(na,"href","#rename"),d($s,"class","relative group"),d(Zt,"href","/docs/datasets/pr_3931/en/package_reference/main_classes#datasets.Dataset.rename_column"),d(sn,"href","/docs/datasets/pr_3931/en/package_reference/main_classes#datasets.Dataset.rename_column"),d(pa,"id","remove"),d(pa,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(pa,"href","#remove"),d(ys,"class","relative group"),d(an,"href","/docs/datasets/pr_3931/en/package_reference/main_classes#datasets.Dataset.remove_columns"),d(ia,"id","cast"),d(ia,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(ia,"href","#cast"),d(ks,"class","relative group"),d(en,"href","/docs/datasets/pr_3931/en/package_reference/main_classes#datasets.Dataset.cast"),d(tn,"href","/docs/datasets/pr_3931/en/package_reference/main_classes#datasets.Dataset.cast_column"),d(da,"id","flatten"),d(da,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(da,"href","#flatten"),d(Es,"class","relative group"),d(nn,"href","/docs/datasets/pr_3931/en/package_reference/main_classes#datasets.Dataset.flatten"),d(ln,"href","/docs/datasets/pr_3931/en/package_reference/main_classes#datasets.Dataset.flatten"),d(fa,"id","align"),d(fa,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(fa,"href","#align"),d(Ds,"class","relative group"),d(rn,"href","/docs/datasets/pr_3931/en/package_reference/main_classes#datasets.Dataset.align_labels_with_mapping"),d(De,"href","https://huggingface.co/datasets/poem_sentiment"),d(De,"rel","nofollow"),d(Pe,"href","https://huggingface.co/datasets/poem_sentiment#data-fields"),d(Pe,"rel","nofollow"),d(pn,"href","/docs/datasets/pr_3931/en/package_reference/main_classes#datasets.Dataset.align_labels_with_mapping"),d(ja,"id","map"),d(ja,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(ja,"href","#map"),d(As,"class","relative group"),d(hn,"href","/docs/datasets/pr_3931/en/package_reference/main_classes#datasets.Dataset.map"),d(cn,"href","/docs/datasets/pr_3931/en/package_reference/main_classes#datasets.Dataset.map"),d(dn,"href","/docs/datasets/pr_3931/en/package_reference/main_classes#datasets.Dataset.map"),d(mn,"href","/docs/datasets/pr_3931/en/package_reference/main_classes#datasets.Dataset.map"),d(fn,"href","/docs/datasets/pr_3931/en/package_reference/main_classes#datasets.Dataset.map"),d(un,"href","/docs/datasets/pr_3931/en/package_reference/main_classes#datasets.Dataset.map"),d(gn,"href","/docs/datasets/pr_3931/en/package_reference/main_classes#datasets.Dataset.map"),d(xa,"id","multiprocessing"),d(xa,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(xa,"href","#multiprocessing"),d(Ps,"class","relative group"),d(_n,"href","/docs/datasets/pr_3931/en/package_reference/main_classes#datasets.Dataset.map"),d($a,"id","batch-processing"),d($a,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d($a,"href","#batch-processing"),d(Ts,"class","relative group"),d(jn,"href","/docs/datasets/pr_3931/en/package_reference/main_classes#datasets.Dataset.map"),d(ya,"id","tokenization"),d(ya,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(ya,"href","#tokenization"),d(Ss,"class","relative group"),d(Ea,"id","split-long-examples"),d(Ea,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Ea,"href","#split-long-examples"),d(qs,"class","relative group"),d(xn,"href","/docs/datasets/pr_3931/en/package_reference/main_classes#datasets.Dataset.map"),d(Pa,"id","data-augmentation"),d(Pa,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Pa,"href","#data-augmentation"),d(Cs,"class","relative group"),d(Qe,"href","https://huggingface.co/roberta-base"),d(Qe,"rel","nofollow"),d(Xe,"href","https://huggingface.co/transformers/main_classes/pipelines#transformers.FillMaskPipeline"),d(Xe,"rel","nofollow"),d(En,"href","/docs/datasets/pr_3931/en/package_reference/main_classes#datasets.Dataset.map"),d(Sa,"id","process-multiple-splits"),d(Sa,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Sa,"href","#process-multiple-splits"),d(Ns,"class","relative group"),d(Dn,"href","/docs/datasets/pr_3931/en/package_reference/main_classes#datasets.DatasetDict.map"),d(qa,"id","distributed-usage"),d(qa,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(qa,"href","#distributed-usage"),d(zs,"class","relative group"),d(An,"href","/docs/datasets/pr_3931/en/package_reference/main_classes#datasets.Dataset.map"),d(lt,"href","https://pytorch.org/docs/stable/distributed?highlight=barrier#torch.distributed.barrier"),d(lt,"rel","nofollow"),d(Na,"id","concatenate"),d(Na,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Na,"href","#concatenate"),d(Fs,"class","relative group"),d(Pn,"href","/docs/datasets/pr_3931/en/package_reference/main_classes#datasets.concatenate_datasets"),d(Oa,"id","format"),d(Oa,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Oa,"href","#format"),d(Os,"class","relative group"),d(Sn,"href","/docs/datasets/pr_3931/en/package_reference/main_classes#datasets.Dataset.set_format"),d(dt,"href","https://www.tensorflow.org/api_docs/python/tf/data/Dataset"),d(dt,"rel","nofollow"),d(qn,"href","/docs/datasets/pr_3931/en/package_reference/main_classes#datasets.Dataset.with_format"),d(Cn,"href","/docs/datasets/pr_3931/en/package_reference/main_classes#datasets.Dataset"),d(Nn,"href","/docs/datasets/pr_3931/en/package_reference/main_classes#datasets.Dataset.reset_format"),d(La,"id","format-transform"),d(La,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(La,"href","#format-transform"),d(Is,"class","relative group"),d(zn,"href","/docs/datasets/pr_3931/en/package_reference/main_classes#datasets.Dataset.set_transform"),d(Ma,"id","save"),d(Ma,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Ma,"href","#save"),d(Ls,"class","relative group"),d(On,"href","/docs/datasets/pr_3931/en/package_reference/main_classes#datasets.Dataset.save_to_disk"),d(In,"href","/docs/datasets/pr_3931/en/package_reference/loading_methods#datasets.load_from_disk"),d(Ha,"id","export"),d(Ha,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Ha,"href","#export"),d(Ms,"class","relative group"),d(Mn,"href","/docs/datasets/pr_3931/en/package_reference/main_classes#datasets.Dataset.to_csv"),d(Bn,"href","/docs/datasets/pr_3931/en/package_reference/main_classes#datasets.Dataset.to_json"),d(Un,"href","/docs/datasets/pr_3931/en/package_reference/main_classes#datasets.Dataset.to_parquet"),d(Vn,"href","/docs/datasets/pr_3931/en/package_reference/main_classes#datasets.Dataset.to_pandas"),d(Hn,"href","/docs/datasets/pr_3931/en/package_reference/main_classes#datasets.Dataset.to_dict")},m(s,o){a(document.head,b),i(s,k,o),i(s,v,o),a(v,$),a($,D),u(x,D,null),a(v,y),a(v,A),a(A,C),i(s,q,o),i(s,L,o),a(L,M),i(s,Us,o),i(s,Q,o),a(Q,X),i(s,Vs,o),i(s,P,o),a(P,U),a(U,Qa),a(P,Xa),a(P,G),a(G,T),a(P,qt),a(P,Hs),a(Hs,Ct),a(P,Nt),a(P,Ys),a(Ys,zt),a(P,Zh),a(P,ll),a(ll,sc),a(P,ac),a(P,rl),a(rl,ec),i(s,gp,o),i(s,Ft,o),a(Ft,tc),i(s,_p,o),u(Za,s,o),i(s,jp,o),u(Ws,s,o),i(s,bp,o),i(s,fs,o),a(fs,Gs),a(Gs,pl),u(se,pl,null),a(fs,nc),a(fs,ol),a(ol,lc),i(s,vp,o),i(s,Ot,o),a(Ot,rc),i(s,wp,o),i(s,us,o),a(us,Js),a(Js,il),u(ae,il,null),a(us,pc),a(us,hl),a(hl,oc),i(s,xp,o),i(s,Ks,o),a(Ks,ic),a(Ks,Rt),a(Rt,hc),a(Ks,cc),i(s,$p,o),u(ee,s,o),i(s,yp,o),i(s,gs,o),a(gs,Qs),a(Qs,cl),u(te,cl,null),a(gs,dc),a(gs,dl),a(dl,mc),i(s,kp,o),i(s,V,o),a(V,fc),a(V,It),a(It,uc),a(V,gc),a(V,ml),a(ml,_c),a(V,jc),a(V,fl),a(fl,bc),a(V,vc),i(s,Ep,o),u(ne,s,o),i(s,Dp,o),i(s,_s,o),a(_s,Xs),a(Xs,ul),u(le,ul,null),a(_s,wc),a(_s,gl),a(gl,xc),i(s,Ap,o),i(s,Z,o),a(Z,$c),a(Z,Lt),a(Lt,yc),a(Z,kc),a(Z,Mt),a(Mt,Ec),a(Z,Dc),i(s,Pp,o),i(s,Bt,o),a(Bt,Ut),a(Ut,Vt),a(Vt,Ac),a(Ut,Pc),i(s,Tp,o),u(re,s,o),i(s,Sp,o),i(s,Ht,o),a(Ht,Yt),a(Yt,Wt),a(Wt,Tc),a(Yt,Sc),i(s,qp,o),u(pe,s,o),i(s,Cp,o),i(s,js,o),a(js,Gt),a(Gt,qc),a(js,Cc),a(js,_l),a(_l,Nc),a(js,zc),i(s,Np,o),u(oe,s,o),i(s,zp,o),i(s,bs,o),a(bs,Zs),a(Zs,jl),u(ie,jl,null),a(bs,Fc),a(bs,bl),a(bl,Oc),i(s,Fp,o),i(s,vs,o),a(vs,Jt),a(Jt,Rc),a(vs,Ic),a(vs,vl),a(vl,Lc),a(vs,Mc),i(s,Op,o),u(he,s,o),i(s,Rp,o),i(s,sa,o),a(sa,Bc),a(sa,wl),a(wl,Uc),a(sa,Vc),i(s,Ip,o),i(s,ws,o),a(ws,aa),a(aa,xl),u(ce,xl,null),a(ws,Hc),a(ws,$l),a($l,Yc),i(s,Lp,o),i(s,H,o),a(H,Wc),a(H,yl),a(yl,Gc),a(H,Jc),a(H,Kt),a(Kt,Kc),a(H,Qc),a(H,kl),a(kl,Xc),a(H,Zc),i(s,Mp,o),i(s,ea,o),a(ea,sd),a(ea,de),a(de,ad),a(ea,ed),i(s,Bp,o),u(me,s,o),i(s,Up,o),i(s,Qt,o),a(Qt,td),i(s,Vp,o),u(fe,s,o),i(s,Hp,o),i(s,xs,o),a(xs,ta),a(ta,El),u(ue,El,null),a(xs,nd),a(xs,Dl),a(Dl,ld),i(s,Yp,o),i(s,Xt,o),a(Xt,rd),i(s,Wp,o),i(s,$s,o),a($s,na),a(na,Al),u(ge,Al,null),a($s,pd),a($s,Pl),a(Pl,od),i(s,Gp,o),i(s,la,o),a(la,id),a(la,Zt),a(Zt,hd),a(la,cd),i(s,Jp,o),i(s,ra,o),a(ra,dd),a(ra,sn),a(sn,md),a(ra,fd),i(s,Kp,o),u(_e,s,o),i(s,Qp,o),i(s,ys,o),a(ys,pa),a(pa,Tl),u(je,Tl,null),a(ys,ud),a(ys,Sl),a(Sl,gd),i(s,Xp,o),i(s,oa,o),a(oa,_d),a(oa,an),a(an,jd),a(oa,bd),i(s,Zp,o),u(be,s,o),i(s,so,o),i(s,ks,o),a(ks,ia),a(ia,ql),u(ve,ql,null),a(ks,vd),a(ks,Cl),a(Cl,wd),i(s,ao,o),i(s,B,o),a(B,en),a(en,xd),a(B,$d),a(B,Nl),a(Nl,yd),a(B,kd),a(B,zl),a(zl,Ed),a(B,Dd),a(B,Fl),a(Fl,Ad),a(B,Pd),i(s,eo,o),u(we,s,o),i(s,to,o),u(ha,s,o),i(s,no,o),i(s,ca,o),a(ca,Td),a(ca,tn),a(tn,Sd),a(ca,qd),i(s,lo,o),u(xe,s,o),i(s,ro,o),i(s,Es,o),a(Es,da),a(da,Ol),u($e,Ol,null),a(Es,Cd),a(Es,Rl),a(Rl,Nd),i(s,po,o),i(s,ma,o),a(ma,zd),a(ma,nn),a(nn,Fd),a(ma,Od),i(s,oo,o),u(ye,s,o),i(s,io,o),i(s,F,o),a(F,Rd),a(F,Il),a(Il,Id),a(F,Ld),a(F,Ll),a(Ll,Md),a(F,Bd),a(F,Ml),a(Ml,Ud),a(F,Vd),a(F,ln),a(ln,Hd),a(F,Yd),i(s,ho,o),u(ke,s,o),i(s,co,o),i(s,ss,o),a(ss,Wd),a(ss,Bl),a(Bl,Gd),a(ss,Jd),a(ss,Ul),a(Ul,Kd),a(ss,Qd),i(s,mo,o),i(s,Ds,o),a(Ds,fa),a(fa,Vl),u(Ee,Vl,null),a(Ds,Xd),a(Ds,Hl),a(Hl,Zd),i(s,fo,o),i(s,ua,o),a(ua,sm),a(ua,rn),a(rn,am),a(ua,em),i(s,uo,o),i(s,ga,o),a(ga,tm),a(ga,De),a(De,nm),a(ga,lm),i(s,go,o),u(Ae,s,o),i(s,_o,o),i(s,as,o),a(as,rm),a(as,Pe),a(Pe,pm),a(as,om),a(as,Yl),a(Yl,im),a(as,hm),i(s,jo,o),u(Te,s,o),i(s,bo,o),i(s,_a,o),a(_a,cm),a(_a,pn),a(pn,dm),a(_a,mm),i(s,vo,o),u(Se,s,o),i(s,wo,o),i(s,on,o),a(on,fm),i(s,xo,o),u(qe,s,o),i(s,$o,o),i(s,As,o),a(As,ja),a(ja,Wl),u(Ce,Wl,null),a(As,um),a(As,Gl),a(Gl,gm),i(s,yo,o),i(s,es,o),a(es,_m),a(es,hn),a(hn,jm),a(es,bm),a(es,cn),a(cn,vm),a(es,wm),i(s,ko,o),i(s,O,o),a(O,xm),a(O,Jl),a(Jl,$m),a(O,ym),a(O,Kl),a(Kl,km),a(O,Em),a(O,Ql),a(Ql,Dm),a(O,Am),a(O,Xl),a(Xl,Pm),a(O,Tm),i(s,Eo,o),u(Ne,s,o),i(s,Do,o),i(s,ba,o),a(ba,Sm),a(ba,dn),a(dn,qm),a(ba,Cm),i(s,Ao,o),u(ze,s,o),i(s,Po,o),i(s,va,o),a(va,Nm),a(va,mn),a(mn,zm),a(va,Fm),i(s,To,o),i(s,ts,o),a(ts,Om),a(ts,Zl),a(Zl,Rm),a(ts,Im),a(ts,fn),a(fn,Lm),a(ts,Mm),i(s,So,o),u(Fe,s,o),i(s,qo,o),u(wa,s,o),i(s,Co,o),i(s,ns,o),a(ns,Bm),a(ns,un),a(un,Um),a(ns,Vm),a(ns,sr),a(sr,Hm),a(ns,Ym),i(s,No,o),u(Oe,s,o),i(s,zo,o),i(s,S,o),a(S,Wm),a(S,gn),a(gn,Gm),a(S,Jm),a(S,ar),a(ar,Km),a(S,Qm),a(S,er),a(er,Xm),a(S,Zm),a(S,tr),a(tr,sf),a(S,af),a(S,nr),a(nr,ef),a(S,tf),a(S,lr),a(lr,nf),a(S,lf),a(S,rr),a(rr,rf),a(S,pf),i(s,Fo,o),u(Re,s,o),i(s,Oo,o),i(s,Ps,o),a(Ps,xa),a(xa,pr),u(Ie,pr,null),a(Ps,of),a(Ps,or),a(or,hf),i(s,Ro,o),i(s,ls,o),a(ls,cf),a(ls,ir),a(ir,df),a(ls,mf),a(ls,_n),a(_n,ff),a(ls,uf),i(s,Io,o),u(Le,s,o),i(s,Lo,o),i(s,Ts,o),a(Ts,$a),a($a,hr),u(Me,hr,null),a(Ts,gf),a(Ts,cr),a(cr,_f),i(s,Mo,o),i(s,J,o),a(J,jn),a(jn,jf),a(J,bf),a(J,dr),a(dr,vf),a(J,wf),a(J,mr),a(mr,xf),a(J,$f),i(s,Bo,o),i(s,Ss,o),a(Ss,ya),a(ya,fr),u(Be,fr,null),a(Ss,yf),a(Ss,ur),a(ur,kf),i(s,Uo,o),i(s,bn,o),a(bn,Ef),i(s,Vo,o),i(s,vn,o),a(vn,Df),i(s,Ho,o),u(Ue,s,o),i(s,Yo,o),i(s,ka,o),a(ka,Af),a(ka,gr),a(gr,Pf),a(ka,Tf),i(s,Wo,o),u(Ve,s,o),i(s,Go,o),i(s,R,o),a(R,Sf),a(R,_r),a(_r,qf),a(R,Cf),a(R,jr),a(jr,Nf),a(R,zf),a(R,br),a(br,Ff),a(R,Of),a(R,vr),a(vr,Rf),a(R,If),i(s,Jo,o),i(s,qs,o),a(qs,Ea),a(Ea,wr),u(He,wr,null),a(qs,Lf),a(qs,xr),a(xr,Mf),i(s,Ko,o),i(s,wn,o),a(wn,Bf),i(s,Qo,o),i(s,Da,o),a(Da,$r),a($r,Ye),a(Ye,Uf),a(Ye,yr),a(yr,Vf),a(Ye,Hf),a(Da,Yf),a(Da,kr),a(kr,Er),a(Er,Wf),i(s,Xo,o),u(We,s,o),i(s,Zo,o),i(s,Aa,o),a(Aa,Gf),a(Aa,xn),a(xn,Jf),a(Aa,Kf),i(s,si,o),u(Ge,s,o),i(s,ai,o),i(s,$n,o),a($n,Qf),i(s,ei,o),u(Je,s,o),i(s,ti,o),i(s,Cs,o),a(Cs,Pa),a(Pa,Dr),u(Ke,Dr,null),a(Cs,Xf),a(Cs,Ar),a(Ar,Zf),i(s,ni,o),i(s,yn,o),a(yn,su),i(s,li,o),i(s,rs,o),a(rs,au),a(rs,Qe),a(Qe,eu),a(rs,tu),a(rs,Xe),a(Xe,nu),a(rs,lu),i(s,ri,o),u(Ze,s,o),i(s,pi,o),i(s,kn,o),a(kn,ru),i(s,oi,o),u(st,s,o),i(s,ii,o),i(s,Ta,o),a(Ta,pu),a(Ta,En),a(En,ou),a(Ta,iu),i(s,hi,o),u(at,s,o),i(s,ci,o),i(s,I,o),a(I,hu),a(I,Pr),a(Pr,cu),a(I,du),a(I,Tr),a(Tr,mu),a(I,fu),a(I,Sr),a(Sr,uu),a(I,gu),a(I,qr),a(qr,_u),a(I,ju),i(s,di,o),i(s,Ns,o),a(Ns,Sa),a(Sa,Cr),u(et,Cr,null),a(Ns,bu),a(Ns,Nr),a(Nr,vu),i(s,mi,o),i(s,ps,o),a(ps,wu),a(ps,Dn),a(Dn,xu),a(ps,$u),a(ps,zr),a(zr,yu),a(ps,ku),i(s,fi,o),u(tt,s,o),i(s,ui,o),i(s,zs,o),a(zs,qa),a(qa,Fr),u(nt,Fr,null),a(zs,Eu),a(zs,Or),a(Or,Du),i(s,gi,o),i(s,os,o),a(os,Au),a(os,An),a(An,Pu),a(os,Tu),a(os,lt),a(lt,Su),a(os,qu),i(s,_i,o),i(s,Ca,o),a(Ca,Cu),a(Ca,Rr),a(Rr,Nu),a(Ca,zu),i(s,ji,o),u(rt,s,o),i(s,bi,o),i(s,Fs,o),a(Fs,Na),a(Na,Ir),u(pt,Ir,null),a(Fs,Fu),a(Fs,Lr),a(Lr,Ou),i(s,vi,o),i(s,za,o),a(za,Ru),a(za,Pn),a(Pn,Iu),a(za,Lu),i(s,wi,o),u(ot,s,o),i(s,xi,o),u(Fa,s,o),i(s,$i,o),i(s,Tn,o),a(Tn,Mu),i(s,yi,o),u(it,s,o),i(s,ki,o),i(s,Os,o),a(Os,Oa),a(Oa,Mr),u(ht,Mr,null),a(Os,Bu),a(Os,Br),a(Br,Uu),i(s,Ei,o),i(s,is,o),a(is,Vu),a(is,Sn),a(Sn,Hu),a(is,Yu),a(is,Ur),a(Ur,Wu),a(is,Gu),i(s,Di,o),u(ct,s,o),i(s,Ai,o),i(s,Y,o),a(Y,Ju),a(Y,Vr),a(Vr,Ku),a(Y,Qu),a(Y,dt),a(dt,Xu),a(Y,Zu),a(Y,Hr),a(Hr,sg),a(Y,ag),i(s,Pi,o),u(mt,s,o),i(s,Ti,o),i(s,Rs,o),a(Rs,qn),a(qn,eg),a(Rs,tg),a(Rs,Cn),a(Cn,ng),a(Rs,lg),i(s,Si,o),u(ft,s,o),i(s,qi,o),u(Ra,s,o),i(s,Ci,o),i(s,Ia,o),a(Ia,rg),a(Ia,Nn),a(Nn,pg),a(Ia,og),i(s,Ni,o),u(ut,s,o),i(s,zi,o),i(s,Is,o),a(Is,La),a(La,Yr),u(gt,Yr,null),a(Is,ig),a(Is,Wr),a(Wr,hg),i(s,Fi,o),i(s,_t,o),a(_t,zn),a(zn,cg),a(_t,dg),i(s,Oi,o),u(jt,s,o),i(s,Ri,o),i(s,Fn,o),a(Fn,mg),i(s,Ii,o),i(s,Ls,o),a(Ls,Ma),a(Ma,Gr),u(bt,Gr,null),a(Ls,fg),a(Ls,Jr),a(Jr,ug),i(s,Li,o),i(s,Ba,o),a(Ba,gg),a(Ba,On),a(On,_g),a(Ba,jg),i(s,Mi,o),i(s,Rn,o),a(Rn,bg),i(s,Bi,o),u(vt,s,o),i(s,Ui,o),i(s,Ua,o),a(Ua,vg),a(Ua,In),a(In,wg),a(Ua,xg),i(s,Vi,o),u(wt,s,o),i(s,Hi,o),u(Va,s,o),i(s,Yi,o),i(s,Ms,o),a(Ms,Ha),a(Ha,Kr),u(xt,Kr,null),a(Ms,$g),a(Ms,Qr),a(Qr,yg),i(s,Wi,o),i(s,Ln,o),a(Ln,kg),i(s,Gi,o),i(s,Ya,o),a(Ya,Xr),a(Xr,$t),a($t,Zr),a(Zr,Eg),a($t,Dg),a($t,sp),a(sp,Ag),a(Ya,Pg),a(Ya,K),a(K,yt),a(yt,ap),a(ap,Tg),a(yt,Sg),a(yt,ep),a(ep,Mn),a(Mn,qg),a(K,Cg),a(K,kt),a(kt,tp),a(tp,Ng),a(kt,zg),a(kt,np),a(np,Bn),a(Bn,Fg),a(K,Og),a(K,Et),a(Et,lp),a(lp,Rg),a(Et,Ig),a(Et,rp),a(rp,Un),a(Un,Lg),a(K,Mg),a(K,Dt),a(Dt,pp),a(pp,Bg),a(Dt,Ug),a(Dt,At),a(At,Vn),a(Vn,Vg),a(At,Hg),a(At,Hn),a(Hn,Yg),i(s,Ji,o),i(s,Yn,o),a(Yn,Wg),i(s,Ki,o),u(Pt,s,o),Qi=!0},p(s,[o]){const Tt={};o&2&&(Tt.$$scope={dirty:o,ctx:s}),Ws.$set(Tt);const op={};o&2&&(op.$$scope={dirty:o,ctx:s}),ha.$set(op);const ip={};o&2&&(ip.$$scope={dirty:o,ctx:s}),wa.$set(ip);const hp={};o&2&&(hp.$$scope={dirty:o,ctx:s}),Fa.$set(hp);const cp={};o&2&&(cp.$$scope={dirty:o,ctx:s}),Ra.$set(cp);const dp={};o&2&&(dp.$$scope={dirty:o,ctx:s}),Va.$set(dp)},i(s){Qi||(g(x.$$.fragment,s),g(Za.$$.fragment,s),g(Ws.$$.fragment,s),g(se.$$.fragment,s),g(ae.$$.fragment,s),g(ee.$$.fragment,s),g(te.$$.fragment,s),g(ne.$$.fragment,s),g(le.$$.fragment,s),g(re.$$.fragment,s),g(pe.$$.fragment,s),g(oe.$$.fragment,s),g(ie.$$.fragment,s),g(he.$$.fragment,s),g(ce.$$.fragment,s),g(me.$$.fragment,s),g(fe.$$.fragment,s),g(ue.$$.fragment,s),g(ge.$$.fragment,s),g(_e.$$.fragment,s),g(je.$$.fragment,s),g(be.$$.fragment,s),g(ve.$$.fragment,s),g(we.$$.fragment,s),g(ha.$$.fragment,s),g(xe.$$.fragment,s),g($e.$$.fragment,s),g(ye.$$.fragment,s),g(ke.$$.fragment,s),g(Ee.$$.fragment,s),g(Ae.$$.fragment,s),g(Te.$$.fragment,s),g(Se.$$.fragment,s),g(qe.$$.fragment,s),g(Ce.$$.fragment,s),g(Ne.$$.fragment,s),g(ze.$$.fragment,s),g(Fe.$$.fragment,s),g(wa.$$.fragment,s),g(Oe.$$.fragment,s),g(Re.$$.fragment,s),g(Ie.$$.fragment,s),g(Le.$$.fragment,s),g(Me.$$.fragment,s),g(Be.$$.fragment,s),g(Ue.$$.fragment,s),g(Ve.$$.fragment,s),g(He.$$.fragment,s),g(We.$$.fragment,s),g(Ge.$$.fragment,s),g(Je.$$.fragment,s),g(Ke.$$.fragment,s),g(Ze.$$.fragment,s),g(st.$$.fragment,s),g(at.$$.fragment,s),g(et.$$.fragment,s),g(tt.$$.fragment,s),g(nt.$$.fragment,s),g(rt.$$.fragment,s),g(pt.$$.fragment,s),g(ot.$$.fragment,s),g(Fa.$$.fragment,s),g(it.$$.fragment,s),g(ht.$$.fragment,s),g(ct.$$.fragment,s),g(mt.$$.fragment,s),g(ft.$$.fragment,s),g(Ra.$$.fragment,s),g(ut.$$.fragment,s),g(gt.$$.fragment,s),g(jt.$$.fragment,s),g(bt.$$.fragment,s),g(vt.$$.fragment,s),g(wt.$$.fragment,s),g(Va.$$.fragment,s),g(xt.$$.fragment,s),g(Pt.$$.fragment,s),Qi=!0)},o(s){_(x.$$.fragment,s),_(Za.$$.fragment,s),_(Ws.$$.fragment,s),_(se.$$.fragment,s),_(ae.$$.fragment,s),_(ee.$$.fragment,s),_(te.$$.fragment,s),_(ne.$$.fragment,s),_(le.$$.fragment,s),_(re.$$.fragment,s),_(pe.$$.fragment,s),_(oe.$$.fragment,s),_(ie.$$.fragment,s),_(he.$$.fragment,s),_(ce.$$.fragment,s),_(me.$$.fragment,s),_(fe.$$.fragment,s),_(ue.$$.fragment,s),_(ge.$$.fragment,s),_(_e.$$.fragment,s),_(je.$$.fragment,s),_(be.$$.fragment,s),_(ve.$$.fragment,s),_(we.$$.fragment,s),_(ha.$$.fragment,s),_(xe.$$.fragment,s),_($e.$$.fragment,s),_(ye.$$.fragment,s),_(ke.$$.fragment,s),_(Ee.$$.fragment,s),_(Ae.$$.fragment,s),_(Te.$$.fragment,s),_(Se.$$.fragment,s),_(qe.$$.fragment,s),_(Ce.$$.fragment,s),_(Ne.$$.fragment,s),_(ze.$$.fragment,s),_(Fe.$$.fragment,s),_(wa.$$.fragment,s),_(Oe.$$.fragment,s),_(Re.$$.fragment,s),_(Ie.$$.fragment,s),_(Le.$$.fragment,s),_(Me.$$.fragment,s),_(Be.$$.fragment,s),_(Ue.$$.fragment,s),_(Ve.$$.fragment,s),_(He.$$.fragment,s),_(We.$$.fragment,s),_(Ge.$$.fragment,s),_(Je.$$.fragment,s),_(Ke.$$.fragment,s),_(Ze.$$.fragment,s),_(st.$$.fragment,s),_(at.$$.fragment,s),_(et.$$.fragment,s),_(tt.$$.fragment,s),_(nt.$$.fragment,s),_(rt.$$.fragment,s),_(pt.$$.fragment,s),_(ot.$$.fragment,s),_(Fa.$$.fragment,s),_(it.$$.fragment,s),_(ht.$$.fragment,s),_(ct.$$.fragment,s),_(mt.$$.fragment,s),_(ft.$$.fragment,s),_(Ra.$$.fragment,s),_(ut.$$.fragment,s),_(gt.$$.fragment,s),_(jt.$$.fragment,s),_(bt.$$.fragment,s),_(vt.$$.fragment,s),_(wt.$$.fragment,s),_(Va.$$.fragment,s),_(xt.$$.fragment,s),_(Pt.$$.fragment,s),Qi=!1},d(s){e(b),s&&e(k),s&&e(v),j(x),s&&e(q),s&&e(L),s&&e(Us),s&&e(Q),s&&e(Vs),s&&e(P),s&&e(gp),s&&e(Ft),s&&e(_p),j(Za,s),s&&e(jp),j(Ws,s),s&&e(bp),s&&e(fs),j(se),s&&e(vp),s&&e(Ot),s&&e(wp),s&&e(us),j(ae),s&&e(xp),s&&e(Ks),s&&e($p),j(ee,s),s&&e(yp),s&&e(gs),j(te),s&&e(kp),s&&e(V),s&&e(Ep),j(ne,s),s&&e(Dp),s&&e(_s),j(le),s&&e(Ap),s&&e(Z),s&&e(Pp),s&&e(Bt),s&&e(Tp),j(re,s),s&&e(Sp),s&&e(Ht),s&&e(qp),j(pe,s),s&&e(Cp),s&&e(js),s&&e(Np),j(oe,s),s&&e(zp),s&&e(bs),j(ie),s&&e(Fp),s&&e(vs),s&&e(Op),j(he,s),s&&e(Rp),s&&e(sa),s&&e(Ip),s&&e(ws),j(ce),s&&e(Lp),s&&e(H),s&&e(Mp),s&&e(ea),s&&e(Bp),j(me,s),s&&e(Up),s&&e(Qt),s&&e(Vp),j(fe,s),s&&e(Hp),s&&e(xs),j(ue),s&&e(Yp),s&&e(Xt),s&&e(Wp),s&&e($s),j(ge),s&&e(Gp),s&&e(la),s&&e(Jp),s&&e(ra),s&&e(Kp),j(_e,s),s&&e(Qp),s&&e(ys),j(je),s&&e(Xp),s&&e(oa),s&&e(Zp),j(be,s),s&&e(so),s&&e(ks),j(ve),s&&e(ao),s&&e(B),s&&e(eo),j(we,s),s&&e(to),j(ha,s),s&&e(no),s&&e(ca),s&&e(lo),j(xe,s),s&&e(ro),s&&e(Es),j($e),s&&e(po),s&&e(ma),s&&e(oo),j(ye,s),s&&e(io),s&&e(F),s&&e(ho),j(ke,s),s&&e(co),s&&e(ss),s&&e(mo),s&&e(Ds),j(Ee),s&&e(fo),s&&e(ua),s&&e(uo),s&&e(ga),s&&e(go),j(Ae,s),s&&e(_o),s&&e(as),s&&e(jo),j(Te,s),s&&e(bo),s&&e(_a),s&&e(vo),j(Se,s),s&&e(wo),s&&e(on),s&&e(xo),j(qe,s),s&&e($o),s&&e(As),j(Ce),s&&e(yo),s&&e(es),s&&e(ko),s&&e(O),s&&e(Eo),j(Ne,s),s&&e(Do),s&&e(ba),s&&e(Ao),j(ze,s),s&&e(Po),s&&e(va),s&&e(To),s&&e(ts),s&&e(So),j(Fe,s),s&&e(qo),j(wa,s),s&&e(Co),s&&e(ns),s&&e(No),j(Oe,s),s&&e(zo),s&&e(S),s&&e(Fo),j(Re,s),s&&e(Oo),s&&e(Ps),j(Ie),s&&e(Ro),s&&e(ls),s&&e(Io),j(Le,s),s&&e(Lo),s&&e(Ts),j(Me),s&&e(Mo),s&&e(J),s&&e(Bo),s&&e(Ss),j(Be),s&&e(Uo),s&&e(bn),s&&e(Vo),s&&e(vn),s&&e(Ho),j(Ue,s),s&&e(Yo),s&&e(ka),s&&e(Wo),j(Ve,s),s&&e(Go),s&&e(R),s&&e(Jo),s&&e(qs),j(He),s&&e(Ko),s&&e(wn),s&&e(Qo),s&&e(Da),s&&e(Xo),j(We,s),s&&e(Zo),s&&e(Aa),s&&e(si),j(Ge,s),s&&e(ai),s&&e($n),s&&e(ei),j(Je,s),s&&e(ti),s&&e(Cs),j(Ke),s&&e(ni),s&&e(yn),s&&e(li),s&&e(rs),s&&e(ri),j(Ze,s),s&&e(pi),s&&e(kn),s&&e(oi),j(st,s),s&&e(ii),s&&e(Ta),s&&e(hi),j(at,s),s&&e(ci),s&&e(I),s&&e(di),s&&e(Ns),j(et),s&&e(mi),s&&e(ps),s&&e(fi),j(tt,s),s&&e(ui),s&&e(zs),j(nt),s&&e(gi),s&&e(os),s&&e(_i),s&&e(Ca),s&&e(ji),j(rt,s),s&&e(bi),s&&e(Fs),j(pt),s&&e(vi),s&&e(za),s&&e(wi),j(ot,s),s&&e(xi),j(Fa,s),s&&e($i),s&&e(Tn),s&&e(yi),j(it,s),s&&e(ki),s&&e(Os),j(ht),s&&e(Ei),s&&e(is),s&&e(Di),j(ct,s),s&&e(Ai),s&&e(Y),s&&e(Pi),j(mt,s),s&&e(Ti),s&&e(Rs),s&&e(Si),j(ft,s),s&&e(qi),j(Ra,s),s&&e(Ci),s&&e(Ia),s&&e(Ni),j(ut,s),s&&e(zi),s&&e(Is),j(gt),s&&e(Fi),s&&e(_t),s&&e(Oi),j(jt,s),s&&e(Ri),s&&e(Fn),s&&e(Ii),s&&e(Ls),j(bt),s&&e(Li),s&&e(Ba),s&&e(Mi),s&&e(Rn),s&&e(Bi),j(vt,s),s&&e(Ui),s&&e(Ua),s&&e(Vi),j(wt,s),s&&e(Hi),j(Va,s),s&&e(Yi),s&&e(Ms),j(xt),s&&e(Wi),s&&e(Ln),s&&e(Gi),s&&e(Ya),s&&e(Ji),s&&e(Yn),s&&e(Ki),j(Pt,s)}}}const a1={local:"process",sections:[{local:"sort-shuffle-select-split-and-shard",sections:[{local:"sort",title:"Sort"},{local:"shuffle",title:"Shuffle"},{local:"select-and-filter",title:"Select and Filter"},{local:"split",title:"Split"},{local:"shard",title:"Shard"}],title:"Sort, shuffle, select, split, and shard"},{local:"rename-remove-cast-and-flatten",sections:[{local:"rename",title:"Rename"},{local:"remove",title:"Remove"},{local:"cast",title:"Cast"},{local:"flatten",title:"Flatten"}],title:"Rename, remove, cast, and flatten"},{local:"align",title:"Align"},{local:"map",sections:[{local:"multiprocessing",title:"Multiprocessing"},{local:"batch-processing",sections:[{local:"tokenization",title:"Tokenization"},{local:"split-long-examples",title:"Split long examples"},{local:"data-augmentation",title:"Data augmentation"}],title:"Batch processing"},{local:"process-multiple-splits",title:"Process multiple splits"},{local:"distributed-usage",title:"Distributed usage"}],title:"Map"},{local:"concatenate",title:"Concatenate"},{local:"format",sections:[{local:"format-transform",title:"Format transform"}],title:"Format"},{local:"save",title:"Save"},{local:"export",title:"Export"}],title:"Process"};function e1(z,b,k){let{fw:v}=b;return z.$$set=$=>{"fw"in $&&k(0,v=$.fw)},[v]}class p1 extends Vb{constructor(b){super();Hb(this,b,e1,s1,Yb,{fw:0})}}export{p1 as default,a1 as metadata};
