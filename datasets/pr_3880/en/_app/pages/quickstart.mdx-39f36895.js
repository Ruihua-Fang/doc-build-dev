import{S as Ne,i as De,s as Fe,e as t,k as c,w as j,t as p,M as Re,c as l,d as a,m as u,a as r,x as g,h as o,b as i,F as e,g as h,y as w,q as v,o as $,B as _}from"../chunks/vendor-aa873a46.js";import{T as ze}from"../chunks/Tip-f7f252ab.js";import{I as js}from"../chunks/IconCopyLink-d0ca3106.js";import{C as Za}from"../chunks/CodeBlock-1f14baf3.js";import{F as se}from"../chunks/FrameworkContent-9b4630ee.js";import"../chunks/IconTensorflow-b9816778.js";function He(gs){let f,y,m,d,P;return{c(){f=t("p"),y=p("For more detailed information on loading and processing a dataset, take a look at "),m=t("a"),d=p("Chapter 3"),P=p(" of the Hugging Face course! It covers additional important topics like dynamic padding, and fine-tuning with the Trainer API."),this.h()},l(b){f=l(b,"P",{});var x=r(f);y=o(x,"For more detailed information on loading and processing a dataset, take a look at "),m=l(x,"A",{href:!0,rel:!0});var D=r(m);d=o(D,"Chapter 3"),D.forEach(a),P=o(x," of the Hugging Face course! It covers additional important topics like dynamic padding, and fine-tuning with the Trainer API."),x.forEach(a),this.h()},h(){i(m,"href","https://huggingface.co/course/chapter3/1?fw=pt"),i(m,"rel","nofollow")},m(b,x){h(b,f,x),e(f,y),e(f,m),e(m,d),e(f,P)},d(b){b&&a(f)}}}function Be(gs){let f,y,m,d,P,b,x,D,ia,Ss,F,ca,os,ua,ma,Ns,R,fa,U,da,ba,Ds,z,Fs,hs,ja,Rs,J,zs,A,H,ws,W,ga,vs,wa,Hs,E,va,X,$a,_a,Y,ka,ya,Bs,K,Ms,B,xa,V,Ea,Ta,Gs,Z,Is,C,M,$s,ss,Pa,_s,Aa,Ls,G,Ca,is,qa,Sa,Os,as,Qs,k,Na,ks,Da,Fa,ys,Ra,za,xs,Ha,Ba,Us,q,I,Es,es,Ma,Ts,Ga,Js,cs,Ia,Ws,ns,Xs,S,L,Ps,ts,La,As,Oa,Ys,ls,Ks,N,O,Cs,rs,Qa,qs,Ua,Vs,us,Ja,Zs,T,Wa,ms,Xa,Ya,fs,Ka,Va,sa;return b=new js({}),z=new ze({props:{$$slots:{default:[He]},$$scope:{ctx:gs}}}),J=new Za({props:{code:"pip install datasets",highlighted:'pip <span class="hljs-keyword">install</span> datasets'}}),W=new js({}),K=new Za({props:{code:`from datasets import load_dataset
dataset = load_dataset('glue', 'mrpc', split='train')`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;glue&#x27;</span>, <span class="hljs-string">&#x27;mrpc&#x27;</span>, split=<span class="hljs-string">&#x27;train&#x27;</span>)`}}),Z=new se({props:{pytorch:!1,tensorflow:!1,jax:!1}}),ss=new js({}),as=new Za({props:{code:`def encode(examples):
    return tokenizer(examples['sentence1'], examples['sentence2'], truncation=True, padding='max_length')

dataset = dataset.map(encode, batched=True)
dataset[0]`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">def</span> <span class="hljs-title function_">encode</span>(<span class="hljs-params">examples</span>):
<span class="hljs-meta">... </span>    <span class="hljs-keyword">return</span> tokenizer(examples[<span class="hljs-string">&#x27;sentence1&#x27;</span>], examples[<span class="hljs-string">&#x27;sentence2&#x27;</span>], truncation=<span class="hljs-literal">True</span>, padding=<span class="hljs-string">&#x27;max_length&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = dataset.<span class="hljs-built_in">map</span>(encode, batched=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset[<span class="hljs-number">0</span>]
{<span class="hljs-string">&#x27;sentence1&#x27;</span>: <span class="hljs-string">&#x27;Amrozi accused his brother , whom he called &quot; the witness &quot; , of deliberately distorting his evidence .&#x27;</span>,
<span class="hljs-string">&#x27;sentence2&#x27;</span>: <span class="hljs-string">&#x27;Referring to him as only &quot; the witness &quot; , Amrozi accused his brother of deliberately distorting his evidence .&#x27;</span>,
<span class="hljs-string">&#x27;label&#x27;</span>: <span class="hljs-number">1</span>,
<span class="hljs-string">&#x27;idx&#x27;</span>: <span class="hljs-number">0</span>,
<span class="hljs-string">&#x27;input_ids&#x27;</span>: array([  <span class="hljs-number">101</span>,  <span class="hljs-number">7277</span>,  <span class="hljs-number">2180</span>,  <span class="hljs-number">5303</span>,  <span class="hljs-number">4806</span>,  <span class="hljs-number">1117</span>,  <span class="hljs-number">1711</span>,   <span class="hljs-number">117</span>,  <span class="hljs-number">2292</span>, <span class="hljs-number">1119</span>,  <span class="hljs-number">1270</span>,   <span class="hljs-number">107</span>,  <span class="hljs-number">1103</span>,  <span class="hljs-number">7737</span>,   <span class="hljs-number">107</span>,   <span class="hljs-number">117</span>,  <span class="hljs-number">1104</span>,  <span class="hljs-number">9938</span>, <span class="hljs-number">4267</span>, <span class="hljs-number">12223</span>, <span class="hljs-number">21811</span>,  <span class="hljs-number">1117</span>,  <span class="hljs-number">2554</span>,   <span class="hljs-number">119</span>,   <span class="hljs-number">102</span>, <span class="hljs-number">11336</span>,  <span class="hljs-number">6732</span>, <span class="hljs-number">3384</span>,  <span class="hljs-number">1106</span>,  <span class="hljs-number">1140</span>,  <span class="hljs-number">1112</span>,  <span class="hljs-number">1178</span>,   <span class="hljs-number">107</span>,  <span class="hljs-number">1103</span>,  <span class="hljs-number">7737</span>,   <span class="hljs-number">107</span>, <span class="hljs-number">117</span>,  <span class="hljs-number">7277</span>,  <span class="hljs-number">2180</span>,  <span class="hljs-number">5303</span>,  <span class="hljs-number">4806</span>,  <span class="hljs-number">1117</span>,  <span class="hljs-number">1711</span>,  <span class="hljs-number">1104</span>,  <span class="hljs-number">9938</span>, <span class="hljs-number">4267</span>, <span class="hljs-number">12223</span>, <span class="hljs-number">21811</span>,  <span class="hljs-number">1117</span>,  <span class="hljs-number">2554</span>,   <span class="hljs-number">119</span>,   <span class="hljs-number">102</span>]),
<span class="hljs-string">&#x27;token_type_ids&#x27;</span>: array([<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]),
<span class="hljs-string">&#x27;attention_mask&#x27;</span>: array([<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>])}`}}),es=new js({}),ns=new se({props:{pytorch:!1,tensorflow:!1,jax:!1}}),ts=new js({}),ls=new se({props:{pytorch:!1,tensorflow:!1,jax:!1}}),rs=new js({}),{c(){f=t("meta"),y=c(),m=t("h1"),d=t("a"),P=t("span"),j(b.$$.fragment),x=c(),D=t("span"),ia=p("Quick Start"),Ss=c(),F=t("p"),ca=p("The quick start is intended for developers who are ready to dive in to the code, and see an end-to-end example of how they can integrate \u{1F917} Datasets into their model training workflow. For beginners who are looking for a gentler introduction, we recommend you begin with the "),os=t("a"),ua=p("tutorials"),ma=p("."),Ns=c(),R=t("p"),fa=p("In the quick start, you will walkthrough all the steps to fine-tune "),U=t("a"),da=p("BERT"),ba=p(" on a paraphrase classification task. Depending on the specific dataset you use, these steps may vary, but the general steps of how to load a dataset and process it are the same."),Ds=c(),j(z.$$.fragment),Fs=c(),hs=t("p"),ja=p("Get started by installing \u{1F917} Datasets:"),Rs=c(),j(J.$$.fragment),zs=c(),A=t("h2"),H=t("a"),ws=t("span"),j(W.$$.fragment),ga=c(),vs=t("span"),wa=p("Load the dataset and model"),Hs=c(),E=t("p"),va=p("Begin by loading the "),X=t("a"),$a=p("Microsoft Research Paraphrase Corpus (MRPC)"),_a=p(" training dataset from the "),Y=t("a"),ka=p("General Language Understanding Evaluation (GLUE) benchmark"),ya=p(". MRPC is a corpus of human annotated sentence pairs used to train a model to determine whether sentence pairs are semantically equivalent."),Bs=c(),j(K.$$.fragment),Ms=c(),B=t("p"),xa=p("Next, import the pre-trained BERT model and its tokenizer from the "),V=t("a"),Ea=p("\u{1F917} Transformers"),Ta=p(" library:"),Gs=c(),j(Z.$$.fragment),Is=c(),C=t("h2"),M=t("a"),$s=t("span"),j(ss.$$.fragment),Pa=c(),_s=t("span"),Aa=p("Tokenize the dataset"),Ls=c(),G=t("p"),Ca=p("The next step is to tokenize the text in order to build sequences of integers the model can understand. Encode the entire dataset with "),is=t("a"),qa=p("datasets.Dataset.map()"),Sa=p(", and truncate and pad the inputs to the maximum length of the model. This ensures the appropriate tensor batches are built."),Os=c(),j(as.$$.fragment),Qs=c(),k=t("p"),Na=p("Notice how there are three new columns in the dataset: "),ks=t("code"),Da=p("input_ids"),Fa=p(", "),ys=t("code"),Ra=p("token_type_ids"),za=p(", and "),xs=t("code"),Ha=p("attention_mask"),Ba=p(". These columns are the inputs to the model."),Us=c(),q=t("h2"),I=t("a"),Es=t("span"),j(es.$$.fragment),Ma=c(),Ts=t("span"),Ga=p("Format the dataset"),Js=c(),cs=t("p"),Ia=p("Depending on whether you are using PyTorch, TensorFlow, or JAX, you will need to format the dataset accordingly. There are three changes you need to make to the dataset:"),Ws=c(),j(ns.$$.fragment),Xs=c(),S=t("h2"),L=t("a"),Ps=t("span"),j(ts.$$.fragment),La=c(),As=t("span"),Oa=p("Train the model"),Ys=c(),j(ls.$$.fragment),Ks=c(),N=t("h2"),O=t("a"),Cs=t("span"),j(rs.$$.fragment),Qa=c(),qs=t("span"),Ua=p("What's next?"),Vs=c(),us=t("p"),Ja=p("This completes the basic steps of loading a dataset to train a model. You loaded and processed the MRPC dataset to fine-tune BERT to determine whether sentence pairs have the same meaning."),Zs=c(),T=t("p"),Wa=p("For your next steps, take a look at our "),ms=t("a"),Xa=p("How-to guides"),Ya=p(" and learn how to achieve a specific task (e.g. load a dataset offline, add a dataset to the Hub, change the name of a column). Or if you want to deepen your knowledge of \u{1F917} Datasets core concepts, read our "),fs=t("a"),Ka=p("Conceptual Guides"),Va=p("."),this.h()},l(s){const n=Re('[data-svelte="svelte-1phssyn"]',document.head);f=l(n,"META",{name:!0,content:!0}),n.forEach(a),y=u(s),m=l(s,"H1",{class:!0});var ps=r(m);d=l(ps,"A",{id:!0,class:!0,href:!0});var ae=r(d);P=l(ae,"SPAN",{});var ee=r(P);g(b.$$.fragment,ee),ee.forEach(a),ae.forEach(a),x=u(ps),D=l(ps,"SPAN",{});var ne=r(D);ia=o(ne,"Quick Start"),ne.forEach(a),ps.forEach(a),Ss=u(s),F=l(s,"P",{});var aa=r(F);ca=o(aa,"The quick start is intended for developers who are ready to dive in to the code, and see an end-to-end example of how they can integrate \u{1F917} Datasets into their model training workflow. For beginners who are looking for a gentler introduction, we recommend you begin with the "),os=l(aa,"A",{href:!0});var te=r(os);ua=o(te,"tutorials"),te.forEach(a),ma=o(aa,"."),aa.forEach(a),Ns=u(s),R=l(s,"P",{});var ea=r(R);fa=o(ea,"In the quick start, you will walkthrough all the steps to fine-tune "),U=l(ea,"A",{href:!0,rel:!0});var le=r(U);da=o(le,"BERT"),le.forEach(a),ba=o(ea," on a paraphrase classification task. Depending on the specific dataset you use, these steps may vary, but the general steps of how to load a dataset and process it are the same."),ea.forEach(a),Ds=u(s),g(z.$$.fragment,s),Fs=u(s),hs=l(s,"P",{});var re=r(hs);ja=o(re,"Get started by installing \u{1F917} Datasets:"),re.forEach(a),Rs=u(s),g(J.$$.fragment,s),zs=u(s),A=l(s,"H2",{class:!0});var na=r(A);H=l(na,"A",{id:!0,class:!0,href:!0});var pe=r(H);ws=l(pe,"SPAN",{});var oe=r(ws);g(W.$$.fragment,oe),oe.forEach(a),pe.forEach(a),ga=u(na),vs=l(na,"SPAN",{});var he=r(vs);wa=o(he,"Load the dataset and model"),he.forEach(a),na.forEach(a),Hs=u(s),E=l(s,"P",{});var ds=r(E);va=o(ds,"Begin by loading the "),X=l(ds,"A",{href:!0,rel:!0});var ie=r(X);$a=o(ie,"Microsoft Research Paraphrase Corpus (MRPC)"),ie.forEach(a),_a=o(ds," training dataset from the "),Y=l(ds,"A",{href:!0,rel:!0});var ce=r(Y);ka=o(ce,"General Language Understanding Evaluation (GLUE) benchmark"),ce.forEach(a),ya=o(ds,". MRPC is a corpus of human annotated sentence pairs used to train a model to determine whether sentence pairs are semantically equivalent."),ds.forEach(a),Bs=u(s),g(K.$$.fragment,s),Ms=u(s),B=l(s,"P",{});var ta=r(B);xa=o(ta,"Next, import the pre-trained BERT model and its tokenizer from the "),V=l(ta,"A",{href:!0,rel:!0});var ue=r(V);Ea=o(ue,"\u{1F917} Transformers"),ue.forEach(a),Ta=o(ta," library:"),ta.forEach(a),Gs=u(s),g(Z.$$.fragment,s),Is=u(s),C=l(s,"H2",{class:!0});var la=r(C);M=l(la,"A",{id:!0,class:!0,href:!0});var me=r(M);$s=l(me,"SPAN",{});var fe=r($s);g(ss.$$.fragment,fe),fe.forEach(a),me.forEach(a),Pa=u(la),_s=l(la,"SPAN",{});var de=r(_s);Aa=o(de,"Tokenize the dataset"),de.forEach(a),la.forEach(a),Ls=u(s),G=l(s,"P",{});var ra=r(G);Ca=o(ra,"The next step is to tokenize the text in order to build sequences of integers the model can understand. Encode the entire dataset with "),is=l(ra,"A",{href:!0});var be=r(is);qa=o(be,"datasets.Dataset.map()"),be.forEach(a),Sa=o(ra,", and truncate and pad the inputs to the maximum length of the model. This ensures the appropriate tensor batches are built."),ra.forEach(a),Os=u(s),g(as.$$.fragment,s),Qs=u(s),k=l(s,"P",{});var Q=r(k);Na=o(Q,"Notice how there are three new columns in the dataset: "),ks=l(Q,"CODE",{});var je=r(ks);Da=o(je,"input_ids"),je.forEach(a),Fa=o(Q,", "),ys=l(Q,"CODE",{});var ge=r(ys);Ra=o(ge,"token_type_ids"),ge.forEach(a),za=o(Q,", and "),xs=l(Q,"CODE",{});var we=r(xs);Ha=o(we,"attention_mask"),we.forEach(a),Ba=o(Q,". These columns are the inputs to the model."),Q.forEach(a),Us=u(s),q=l(s,"H2",{class:!0});var pa=r(q);I=l(pa,"A",{id:!0,class:!0,href:!0});var ve=r(I);Es=l(ve,"SPAN",{});var $e=r(Es);g(es.$$.fragment,$e),$e.forEach(a),ve.forEach(a),Ma=u(pa),Ts=l(pa,"SPAN",{});var _e=r(Ts);Ga=o(_e,"Format the dataset"),_e.forEach(a),pa.forEach(a),Js=u(s),cs=l(s,"P",{});var ke=r(cs);Ia=o(ke,"Depending on whether you are using PyTorch, TensorFlow, or JAX, you will need to format the dataset accordingly. There are three changes you need to make to the dataset:"),ke.forEach(a),Ws=u(s),g(ns.$$.fragment,s),Xs=u(s),S=l(s,"H2",{class:!0});var oa=r(S);L=l(oa,"A",{id:!0,class:!0,href:!0});var ye=r(L);Ps=l(ye,"SPAN",{});var xe=r(Ps);g(ts.$$.fragment,xe),xe.forEach(a),ye.forEach(a),La=u(oa),As=l(oa,"SPAN",{});var Ee=r(As);Oa=o(Ee,"Train the model"),Ee.forEach(a),oa.forEach(a),Ys=u(s),g(ls.$$.fragment,s),Ks=u(s),N=l(s,"H2",{class:!0});var ha=r(N);O=l(ha,"A",{id:!0,class:!0,href:!0});var Te=r(O);Cs=l(Te,"SPAN",{});var Pe=r(Cs);g(rs.$$.fragment,Pe),Pe.forEach(a),Te.forEach(a),Qa=u(ha),qs=l(ha,"SPAN",{});var Ae=r(qs);Ua=o(Ae,"What's next?"),Ae.forEach(a),ha.forEach(a),Vs=u(s),us=l(s,"P",{});var Ce=r(us);Ja=o(Ce,"This completes the basic steps of loading a dataset to train a model. You loaded and processed the MRPC dataset to fine-tune BERT to determine whether sentence pairs have the same meaning."),Ce.forEach(a),Zs=u(s),T=l(s,"P",{});var bs=r(T);Wa=o(bs,"For your next steps, take a look at our "),ms=l(bs,"A",{href:!0});var qe=r(ms);Xa=o(qe,"How-to guides"),qe.forEach(a),Ya=o(bs," and learn how to achieve a specific task (e.g. load a dataset offline, add a dataset to the Hub, change the name of a column). Or if you want to deepen your knowledge of \u{1F917} Datasets core concepts, read our "),fs=l(bs,"A",{href:!0});var Se=r(fs);Ka=o(Se,"Conceptual Guides"),Se.forEach(a),Va=o(bs,"."),bs.forEach(a),this.h()},h(){i(f,"name","hf:doc:metadata"),i(f,"content",JSON.stringify(Me)),i(d,"id","quick-start"),i(d,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(d,"href","#quick-start"),i(m,"class","relative group"),i(os,"href","./tutorial"),i(U,"href","https://huggingface.co/bert-base-cased"),i(U,"rel","nofollow"),i(H,"id","load-the-dataset-and-model"),i(H,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(H,"href","#load-the-dataset-and-model"),i(A,"class","relative group"),i(X,"href","https://huggingface.co/datasets/viewer/?dataset=glue&config=mrpc"),i(X,"rel","nofollow"),i(Y,"href","https://huggingface.co/datasets/glue"),i(Y,"rel","nofollow"),i(V,"href","https://huggingface.co/transformers/"),i(V,"rel","nofollow"),i(M,"id","tokenize-the-dataset"),i(M,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(M,"href","#tokenize-the-dataset"),i(C,"class","relative group"),i(is,"href","/docs/datasets/pr_3880/en/package_reference/main_classes#datasets.Dataset.map"),i(I,"id","format-the-dataset"),i(I,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(I,"href","#format-the-dataset"),i(q,"class","relative group"),i(L,"id","train-the-model"),i(L,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(L,"href","#train-the-model"),i(S,"class","relative group"),i(O,"id","whats-next"),i(O,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(O,"href","#whats-next"),i(N,"class","relative group"),i(ms,"href","./how_to"),i(fs,"href","./about_arrow")},m(s,n){e(document.head,f),h(s,y,n),h(s,m,n),e(m,d),e(d,P),w(b,P,null),e(m,x),e(m,D),e(D,ia),h(s,Ss,n),h(s,F,n),e(F,ca),e(F,os),e(os,ua),e(F,ma),h(s,Ns,n),h(s,R,n),e(R,fa),e(R,U),e(U,da),e(R,ba),h(s,Ds,n),w(z,s,n),h(s,Fs,n),h(s,hs,n),e(hs,ja),h(s,Rs,n),w(J,s,n),h(s,zs,n),h(s,A,n),e(A,H),e(H,ws),w(W,ws,null),e(A,ga),e(A,vs),e(vs,wa),h(s,Hs,n),h(s,E,n),e(E,va),e(E,X),e(X,$a),e(E,_a),e(E,Y),e(Y,ka),e(E,ya),h(s,Bs,n),w(K,s,n),h(s,Ms,n),h(s,B,n),e(B,xa),e(B,V),e(V,Ea),e(B,Ta),h(s,Gs,n),w(Z,s,n),h(s,Is,n),h(s,C,n),e(C,M),e(M,$s),w(ss,$s,null),e(C,Pa),e(C,_s),e(_s,Aa),h(s,Ls,n),h(s,G,n),e(G,Ca),e(G,is),e(is,qa),e(G,Sa),h(s,Os,n),w(as,s,n),h(s,Qs,n),h(s,k,n),e(k,Na),e(k,ks),e(ks,Da),e(k,Fa),e(k,ys),e(ys,Ra),e(k,za),e(k,xs),e(xs,Ha),e(k,Ba),h(s,Us,n),h(s,q,n),e(q,I),e(I,Es),w(es,Es,null),e(q,Ma),e(q,Ts),e(Ts,Ga),h(s,Js,n),h(s,cs,n),e(cs,Ia),h(s,Ws,n),w(ns,s,n),h(s,Xs,n),h(s,S,n),e(S,L),e(L,Ps),w(ts,Ps,null),e(S,La),e(S,As),e(As,Oa),h(s,Ys,n),w(ls,s,n),h(s,Ks,n),h(s,N,n),e(N,O),e(O,Cs),w(rs,Cs,null),e(N,Qa),e(N,qs),e(qs,Ua),h(s,Vs,n),h(s,us,n),e(us,Ja),h(s,Zs,n),h(s,T,n),e(T,Wa),e(T,ms),e(ms,Xa),e(T,Ya),e(T,fs),e(fs,Ka),e(T,Va),sa=!0},p(s,[n]){const ps={};n&2&&(ps.$$scope={dirty:n,ctx:s}),z.$set(ps)},i(s){sa||(v(b.$$.fragment,s),v(z.$$.fragment,s),v(J.$$.fragment,s),v(W.$$.fragment,s),v(K.$$.fragment,s),v(Z.$$.fragment,s),v(ss.$$.fragment,s),v(as.$$.fragment,s),v(es.$$.fragment,s),v(ns.$$.fragment,s),v(ts.$$.fragment,s),v(ls.$$.fragment,s),v(rs.$$.fragment,s),sa=!0)},o(s){$(b.$$.fragment,s),$(z.$$.fragment,s),$(J.$$.fragment,s),$(W.$$.fragment,s),$(K.$$.fragment,s),$(Z.$$.fragment,s),$(ss.$$.fragment,s),$(as.$$.fragment,s),$(es.$$.fragment,s),$(ns.$$.fragment,s),$(ts.$$.fragment,s),$(ls.$$.fragment,s),$(rs.$$.fragment,s),sa=!1},d(s){a(f),s&&a(y),s&&a(m),_(b),s&&a(Ss),s&&a(F),s&&a(Ns),s&&a(R),s&&a(Ds),_(z,s),s&&a(Fs),s&&a(hs),s&&a(Rs),_(J,s),s&&a(zs),s&&a(A),_(W),s&&a(Hs),s&&a(E),s&&a(Bs),_(K,s),s&&a(Ms),s&&a(B),s&&a(Gs),_(Z,s),s&&a(Is),s&&a(C),_(ss),s&&a(Ls),s&&a(G),s&&a(Os),_(as,s),s&&a(Qs),s&&a(k),s&&a(Us),s&&a(q),_(es),s&&a(Js),s&&a(cs),s&&a(Ws),_(ns,s),s&&a(Xs),s&&a(S),_(ts),s&&a(Ys),_(ls,s),s&&a(Ks),s&&a(N),_(rs),s&&a(Vs),s&&a(us),s&&a(Zs),s&&a(T)}}}const Me={local:"quick-start",sections:[{local:"load-the-dataset-and-model",title:"Load the dataset and model"},{local:"tokenize-the-dataset",title:"Tokenize the dataset"},{local:"format-the-dataset",title:"Format the dataset"},{local:"train-the-model",title:"Train the model"},{local:"whats-next",title:"What's next?"}],title:"Quick Start"};function Ge(gs,f,y){let{fw:m}=f;return gs.$$set=d=>{"fw"in d&&y(0,m=d.fw)},[m]}class We extends Ne{constructor(f){super();De(this,f,Ge,Be,Fe,{fw:0})}}export{We as default,Me as metadata};
