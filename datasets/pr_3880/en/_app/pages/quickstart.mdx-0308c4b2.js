import{S as vt,i as xt,s as $t,e as n,k as h,w as b,t as r,M as Et,c as t,d as e,m,a as l,x as f,h as p,b as c,F as a,g as i,y as j,q as g,o as _,B as w}from"../chunks/vendor-e67aec41.js";import{T as Tt}from"../chunks/Tip-76459d1c.js";import{I as Ls}from"../chunks/IconCopyLink-ffd7f84e.js";import{C as L}from"../chunks/CodeBlock-e2bcf023.js";function Ft(Is){let d,F,u,k,z;return{c(){d=n("p"),F=r("For more detailed information on loading and processing a dataset, take a look at "),u=n("a"),k=r("Chapter 3"),z=r(" of the Hugging Face course! It covers additional important topics like dynamic padding, and fine-tuning with the Trainer API."),this.h()},l(y){d=t(y,"P",{});var q=l(d);F=p(q,"For more detailed information on loading and processing a dataset, take a look at "),u=t(q,"A",{href:!0,rel:!0});var I=l(u);k=p(I,"Chapter 3"),I.forEach(e),z=p(q," of the Hugging Face course! It covers additional important topics like dynamic padding, and fine-tuning with the Trainer API."),q.forEach(e),this.h()},h(){c(u,"href","https://huggingface.co/course/chapter3/1?fw=pt"),c(u,"rel","nofollow")},m(y,q){i(y,d,q),a(d,F),a(d,u),a(u,k),a(d,z)},d(y){y&&e(d)}}}function qt(Is){let d,F,u,k,z,y,q,I,Va,ba,M,Za,Ts,se,ae,fa,H,ee,os,ne,te,ja,G,ga,Fs,le,_a,cs,wa,A,W,Ms,is,re,Hs,pe,ka,S,oe,hs,ce,ie,ms,he,me,ya,us,va,Y,ue,ds,de,be,xa,Q,Gs,fe,je,Ws,ge,$a,B,U,Ys,bs,_e,Qs,we,Ea,J,ke,qs,ye,ve,Ta,fs,Fa,v,xe,Us,$e,Ee,Js,Te,Fe,Ks,qe,Se,qa,P,K,Xs,js,Ce,Vs,ze,Sa,Ss,Ae,Ca,X,x,Be,gs,Pe,V,Zs,De,Ne,E,Re,sa,Oe,Le,aa,Ie,Me,ea,He,Ge,We,Z,Cs,Ye,Qe,na,Ue,Je,Ke,_s,Xe,$,Ve,ws,Ze,ss,ta,sn,an,T,en,la,nn,tn,ra,ln,rn,pa,pn,on,cn,as,zs,hn,mn,oa,un,dn,bn,ks,za,D,es,ca,ys,fn,ia,jn,Aa,ns,As,gn,vs,_n,Bs,wn,xs,Ba,N,ts,ha,$s,kn,ma,yn,Pa,Ps,vn,Da,C,xn,Ds,$n,En,Ns,Tn,Fn,Na;return y=new Ls({}),G=new Tt({props:{$$slots:{default:[Ft]},$$scope:{ctx:Is}}}),cs=new L({props:{code:"pip install datasets",highlighted:'pip <span class="hljs-keyword">install</span> datasets'}}),is=new Ls({}),us=new L({props:{code:`from datasets import load_dataset
dataset = load_dataset('glue', 'mrpc', split='train')`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;glue&#x27;</span>, <span class="hljs-string">&#x27;mrpc&#x27;</span>, split=<span class="hljs-string">&#x27;train&#x27;</span>)`}}),bs=new Ls({}),fs=new L({props:{code:`def encode(examples):
    return tokenizer(examples['sentence1'], examples['sentence2'], truncation=True, padding='max_length')

dataset = dataset.map(encode, batched=True)
dataset[0]`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">def</span> <span class="hljs-title function_">encode</span>(<span class="hljs-params">examples</span>):
<span class="hljs-meta">... </span>    <span class="hljs-keyword">return</span> tokenizer(examples[<span class="hljs-string">&#x27;sentence1&#x27;</span>], examples[<span class="hljs-string">&#x27;sentence2&#x27;</span>], truncation=<span class="hljs-literal">True</span>, padding=<span class="hljs-string">&#x27;max_length&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = dataset.<span class="hljs-built_in">map</span>(encode, batched=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset[<span class="hljs-number">0</span>]
{<span class="hljs-string">&#x27;sentence1&#x27;</span>: <span class="hljs-string">&#x27;Amrozi accused his brother , whom he called &quot; the witness &quot; , of deliberately distorting his evidence .&#x27;</span>,
<span class="hljs-string">&#x27;sentence2&#x27;</span>: <span class="hljs-string">&#x27;Referring to him as only &quot; the witness &quot; , Amrozi accused his brother of deliberately distorting his evidence .&#x27;</span>,
<span class="hljs-string">&#x27;label&#x27;</span>: <span class="hljs-number">1</span>,
<span class="hljs-string">&#x27;idx&#x27;</span>: <span class="hljs-number">0</span>,
<span class="hljs-string">&#x27;input_ids&#x27;</span>: array([  <span class="hljs-number">101</span>,  <span class="hljs-number">7277</span>,  <span class="hljs-number">2180</span>,  <span class="hljs-number">5303</span>,  <span class="hljs-number">4806</span>,  <span class="hljs-number">1117</span>,  <span class="hljs-number">1711</span>,   <span class="hljs-number">117</span>,  <span class="hljs-number">2292</span>, <span class="hljs-number">1119</span>,  <span class="hljs-number">1270</span>,   <span class="hljs-number">107</span>,  <span class="hljs-number">1103</span>,  <span class="hljs-number">7737</span>,   <span class="hljs-number">107</span>,   <span class="hljs-number">117</span>,  <span class="hljs-number">1104</span>,  <span class="hljs-number">9938</span>, <span class="hljs-number">4267</span>, <span class="hljs-number">12223</span>, <span class="hljs-number">21811</span>,  <span class="hljs-number">1117</span>,  <span class="hljs-number">2554</span>,   <span class="hljs-number">119</span>,   <span class="hljs-number">102</span>, <span class="hljs-number">11336</span>,  <span class="hljs-number">6732</span>, <span class="hljs-number">3384</span>,  <span class="hljs-number">1106</span>,  <span class="hljs-number">1140</span>,  <span class="hljs-number">1112</span>,  <span class="hljs-number">1178</span>,   <span class="hljs-number">107</span>,  <span class="hljs-number">1103</span>,  <span class="hljs-number">7737</span>,   <span class="hljs-number">107</span>, <span class="hljs-number">117</span>,  <span class="hljs-number">7277</span>,  <span class="hljs-number">2180</span>,  <span class="hljs-number">5303</span>,  <span class="hljs-number">4806</span>,  <span class="hljs-number">1117</span>,  <span class="hljs-number">1711</span>,  <span class="hljs-number">1104</span>,  <span class="hljs-number">9938</span>, <span class="hljs-number">4267</span>, <span class="hljs-number">12223</span>, <span class="hljs-number">21811</span>,  <span class="hljs-number">1117</span>,  <span class="hljs-number">2554</span>,   <span class="hljs-number">119</span>,   <span class="hljs-number">102</span>]),
<span class="hljs-string">&#x27;token_type_ids&#x27;</span>: array([<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]),
<span class="hljs-string">&#x27;attention_mask&#x27;</span>: array([<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>])}`}}),js=new Ls({}),gs=new L({props:{code:"dataset = dataset.map(lambda examples: {'labels': examples['label']}, batched=True)",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = dataset.<span class="hljs-built_in">map</span>(<span class="hljs-keyword">lambda</span> examples: {<span class="hljs-string">&#x27;labels&#x27;</span>: examples[<span class="hljs-string">&#x27;label&#x27;</span>]}, batched=<span class="hljs-literal">True</span>)'}}),_s=new L({props:{code:`import torch
dataset.set_format(type='torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'labels'])
dataloader = torch.utils.data.DataLoader(dataset, batch_size=32)
next(iter(dataloader))`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset.set_format(<span class="hljs-built_in">type</span>=<span class="hljs-string">&#x27;torch&#x27;</span>, columns=[<span class="hljs-string">&#x27;input_ids&#x27;</span>, <span class="hljs-string">&#x27;token_type_ids&#x27;</span>, <span class="hljs-string">&#x27;attention_mask&#x27;</span>, <span class="hljs-string">&#x27;labels&#x27;</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>dataloader = torch.utils.data.DataLoader(dataset, batch_size=<span class="hljs-number">32</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">next</span>(<span class="hljs-built_in">iter</span>(dataloader))
{<span class="hljs-string">&#x27;attention_mask&#x27;</span>: tensor([[<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>,  ..., <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
                        [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>,  ..., <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
                        [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>,  ..., <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
                        ...,
                        [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>,  ..., <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
                        [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>,  ..., <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
                        [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>,  ..., <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>]]),
<span class="hljs-string">&#x27;input_ids&#x27;</span>: tensor([[  <span class="hljs-number">101</span>,  <span class="hljs-number">7277</span>,  <span class="hljs-number">2180</span>,  ...,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>],
                  [  <span class="hljs-number">101</span>, <span class="hljs-number">10684</span>,  <span class="hljs-number">2599</span>,  ...,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>],
                  [  <span class="hljs-number">101</span>,  <span class="hljs-number">1220</span>,  <span class="hljs-number">1125</span>,  ...,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>],
                  ...,
                  [  <span class="hljs-number">101</span>, <span class="hljs-number">16944</span>,  <span class="hljs-number">1107</span>,  ...,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>],
                  [  <span class="hljs-number">101</span>,  <span class="hljs-number">1109</span>, <span class="hljs-number">11896</span>,  ...,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>],
                  [  <span class="hljs-number">101</span>,  <span class="hljs-number">1109</span>,  <span class="hljs-number">4173</span>,  ...,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>]]),
<span class="hljs-string">&#x27;label&#x27;</span>: tensor([<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>]),
<span class="hljs-string">&#x27;token_type_ids&#x27;</span>: tensor([[<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>,  ..., <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
                     [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>,  ..., <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
                     [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>,  ..., <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
                     ...,
                     [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>,  ..., <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
                     [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>,  ..., <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
                     [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>,  ..., <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>]])}`}}),ws=new L({props:{code:"dataset = dataset.map(lambda examples: {'labels': examples['label']}, batched=True)",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = dataset.<span class="hljs-built_in">map</span>(<span class="hljs-keyword">lambda</span> examples: {<span class="hljs-string">&#x27;labels&#x27;</span>: examples[<span class="hljs-string">&#x27;label&#x27;</span>]}, batched=<span class="hljs-literal">True</span>)'}}),ks=new L({props:{code:`import tensorflow as tf
dataset.set_format(type='tensorflow', columns=['input_ids', 'token_type_ids', 'attention_mask', 'labels'])
features = {x: dataset[x].to_tensor(default_value=0, shape=[None, tokenizer.model_max_length]) for x in ['input_ids', 'token_type_ids', 'attention_mask']}
tfdataset = tf.data.Dataset.from_tensor_slices((features, dataset["labels"])).batch(32)
next(iter(tfdataset))`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset.set_format(<span class="hljs-built_in">type</span>=<span class="hljs-string">&#x27;tensorflow&#x27;</span>, columns=[<span class="hljs-string">&#x27;input_ids&#x27;</span>, <span class="hljs-string">&#x27;token_type_ids&#x27;</span>, <span class="hljs-string">&#x27;attention_mask&#x27;</span>, <span class="hljs-string">&#x27;labels&#x27;</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>features = {x: dataset[x].to_tensor(default_value=<span class="hljs-number">0</span>, shape=[<span class="hljs-literal">None</span>, tokenizer.model_max_length]) <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> [<span class="hljs-string">&#x27;input_ids&#x27;</span>, <span class="hljs-string">&#x27;token_type_ids&#x27;</span>, <span class="hljs-string">&#x27;attention_mask&#x27;</span>]}
<span class="hljs-meta">&gt;&gt;&gt; </span>tfdataset = tf.data.Dataset.from_tensor_slices((features, dataset[<span class="hljs-string">&quot;labels&quot;</span>])).batch(<span class="hljs-number">32</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">next</span>(<span class="hljs-built_in">iter</span>(tfdataset))
({<span class="hljs-string">&#x27;input_ids&#x27;</span>: &lt;tf.Tensor: shape=(<span class="hljs-number">32</span>, <span class="hljs-number">512</span>), dtype=int32, numpy=
array([[  <span class="hljs-number">101</span>,  <span class="hljs-number">7277</span>,  <span class="hljs-number">2180</span>, ...,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>],
   [  <span class="hljs-number">101</span>, <span class="hljs-number">10684</span>,  <span class="hljs-number">2599</span>, ...,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>],
   [  <span class="hljs-number">101</span>,  <span class="hljs-number">1220</span>,  <span class="hljs-number">1125</span>, ...,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>],
   ...,
   [  <span class="hljs-number">101</span>,  <span class="hljs-number">1109</span>,  <span class="hljs-number">2026</span>, ...,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>],
   [  <span class="hljs-number">101</span>, <span class="hljs-number">22263</span>,  <span class="hljs-number">1107</span>, ...,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>],
   [  <span class="hljs-number">101</span>,   <span class="hljs-number">142</span>,  <span class="hljs-number">1813</span>, ...,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>]], dtype=int32)&gt;, <span class="hljs-string">&#x27;token_type_ids&#x27;</span>: &lt;tf.Tensor: shape=(<span class="hljs-number">32</span>, <span class="hljs-number">512</span>), dtype=int32, numpy=
array([[<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, ..., <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
   [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, ..., <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
   [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, ..., <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
   ...,
   [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, ..., <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
   [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, ..., <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
   [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, ..., <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>]], dtype=int32)&gt;, <span class="hljs-string">&#x27;attention_mask&#x27;</span>: &lt;tf.Tensor: shape=(<span class="hljs-number">32</span>, <span class="hljs-number">512</span>), dtype=int32, numpy=
array([[<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, ..., <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
   [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, ..., <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
   [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, ..., <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
   ...,
   [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, ..., <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
   [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, ..., <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
   [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, ..., <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>]], dtype=int32)&gt;}, &lt;tf.Tensor: shape=(<span class="hljs-number">32</span>,), dtype=int64, numpy=
array([<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>,
   <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>])&gt;)`}}),ys=new Ls({}),vs=new L({props:{code:`from tqdm import tqdm
device = 'cuda' if torch.cuda.is_available() else 'cpu' 
model.train().to(device)
optimizer = torch.optim.AdamW(params=model.parameters(), lr=1e-5)
for epoch in range(3):
    for i, batch in enumerate(tqdm(dataloader)):
        batch = {k: v.to(device) for k, v in batch.items()}
        outputs = model(**batch)
        loss = outputs[0]
        loss.backward()
        optimizer.step()
        optimizer.zero_grad()
        if i % 10 == 0:
            print(f"loss: {loss}")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> tqdm <span class="hljs-keyword">import</span> tqdm
<span class="hljs-meta">&gt;&gt;&gt; </span>device = <span class="hljs-string">&#x27;cuda&#x27;</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">&#x27;cpu&#x27;</span> 
<span class="hljs-meta">&gt;&gt;&gt; </span>model.train().to(device)
<span class="hljs-meta">&gt;&gt;&gt; </span>optimizer = torch.optim.AdamW(params=model.parameters(), lr=<span class="hljs-number">1e-5</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">3</span>):
<span class="hljs-meta">... </span>    <span class="hljs-keyword">for</span> i, batch <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(tqdm(dataloader)):
<span class="hljs-meta">... </span>        batch = {k: v.to(device) <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> batch.items()}
<span class="hljs-meta">... </span>        outputs = model(**batch)
<span class="hljs-meta">... </span>        loss = outputs[<span class="hljs-number">0</span>]
<span class="hljs-meta">... </span>        loss.backward()
<span class="hljs-meta">... </span>        optimizer.step()
<span class="hljs-meta">... </span>        optimizer.zero_grad()
<span class="hljs-meta">... </span>        <span class="hljs-keyword">if</span> i % <span class="hljs-number">10</span> == <span class="hljs-number">0</span>:
<span class="hljs-meta">... </span>            <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;loss: <span class="hljs-subst">{loss}</span>&quot;</span>)`}}),xs=new L({props:{code:`loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(reduction=tf.keras.losses.Reduction.NONE, from_logits=True)
opt = tf.keras.optimizers.Adam(learning_rate=3e-5)
model.compile(optimizer=opt, loss=loss_fn, metrics=["accuracy"])
model.fit(tfdataset, epochs=3)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(reduction=tf.keras.losses.Reduction.NONE, from_logits=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>opt = tf.keras.optimizers.Adam(learning_rate=<span class="hljs-number">3e-5</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.<span class="hljs-built_in">compile</span>(optimizer=opt, loss=loss_fn, metrics=[<span class="hljs-string">&quot;accuracy&quot;</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>model.fit(tfdataset, epochs=<span class="hljs-number">3</span>)`}}),$s=new Ls({}),{c(){d=n("meta"),F=h(),u=n("h1"),k=n("a"),z=n("span"),b(y.$$.fragment),q=h(),I=n("span"),Va=r("Quick Start"),ba=h(),M=n("p"),Za=r("The quick start is intended for developers who are ready to dive in to the code, and see an end-to-end example of how they can integrate \u{1F917} Datasets into their model training workflow. For beginners who are looking for a gentler introduction, we recommend you begin with the "),Ts=n("a"),se=r("tutorials"),ae=r("."),fa=h(),H=n("p"),ee=r("In the quick start, you will walkthrough all the steps to fine-tune "),os=n("a"),ne=r("BERT"),te=r(" on a paraphrase classification task. Depending on the specific dataset you use, these steps may vary, but the general steps of how to load a dataset and process it are the same."),ja=h(),b(G.$$.fragment),ga=h(),Fs=n("p"),le=r("Get started by installing \u{1F917} Datasets:"),_a=h(),b(cs.$$.fragment),wa=h(),A=n("h2"),W=n("a"),Ms=n("span"),b(is.$$.fragment),re=h(),Hs=n("span"),pe=r("Load the dataset and model"),ka=h(),S=n("p"),oe=r("Begin by loading the "),hs=n("a"),ce=r("Microsoft Research Paraphrase Corpus (MRPC)"),ie=r(" training dataset from the "),ms=n("a"),he=r("General Language Understanding Evaluation (GLUE) benchmark"),me=r(". MRPC is a corpus of human annotated sentence pairs used to train a model to determine whether sentence pairs are semantically equivalent."),ya=h(),b(us.$$.fragment),va=h(),Y=n("p"),ue=r("Next, import the pre-trained BERT model and its tokenizer from the "),ds=n("a"),de=r("\u{1F917} Transformers"),be=r(" library:"),xa=h(),Q=n("frameworkcontent"),Gs=n("pt"),fe=r(`\`\`\`py
>>> from transformers import AutoModelForSequenceClassification, AutoTokenizer
>>> model = AutoModelForSequenceClassification.from_pretrained('bert-base-cased')
Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
>>> tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')
\`\`\``),je=h(),Ws=n("tf"),ge=r(`\`\`\`pt
>>> from transformers import TFAutoModelForSequenceClassification, AutoTokenizer
>>> model = TFAutoModelForSequenceClassification.from_pretrained("bert-base-cased")
Some weights of the model checkpoint at bert-base-cased were not used when initializing TFBertForSequenceClassification: ['nsp___cls', 'mlm___cls']
- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['dropout_37', 'classifier']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
>>> tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')
\`\`\``),$a=h(),B=n("h2"),U=n("a"),Ys=n("span"),b(bs.$$.fragment),_e=h(),Qs=n("span"),we=r("Tokenize the dataset"),Ea=h(),J=n("p"),ke=r("The next step is to tokenize the text in order to build sequences of integers the model can understand. Encode the entire dataset with "),qs=n("a"),ye=r("datasets.Dataset.map()"),ve=r(", and truncate and pad the inputs to the maximum length of the model. This ensures the appropriate tensor batches are built."),Ta=h(),b(fs.$$.fragment),Fa=h(),v=n("p"),xe=r("Notice how there are three new columns in the dataset: "),Us=n("code"),$e=r("input_ids"),Ee=r(", "),Js=n("code"),Te=r("token_type_ids"),Fe=r(", and "),Ks=n("code"),qe=r("attention_mask"),Se=r(". These columns are the inputs to the model."),qa=h(),P=n("h2"),K=n("a"),Xs=n("span"),b(js.$$.fragment),Ce=h(),Vs=n("span"),ze=r("Format the dataset"),Sa=h(),Ss=n("p"),Ae=r("Depending on whether you are using PyTorch, TensorFlow, or JAX, you will need to format the dataset accordingly. There are three changes you need to make to the dataset:"),Ca=h(),X=n("frameworkcontent"),x=n("pt"),Be=r("1. Rename the `label` column to `labels`, the expected input name in [BertForSequenceClassification](https://huggingface.co/transformers/model_doc/bert#transformers.BertForSequenceClassification.forward):\n\n	"),b(gs.$$.fragment),Pe=h(),V=n("ol"),Zs=n("li"),De=r("Retrieve the actual tensors from the Dataset object instead of using the current Python objects."),Ne=h(),E=n("li"),Re=r("Filter the dataset to only return the model inputs: "),sa=n("code"),Oe=r("input_ids"),Le=r(", "),aa=n("code"),Ie=r("token_type_ids"),Me=r(", and "),ea=n("code"),He=r("attention_mask"),Ge=r("."),We=h(),Z=n("p"),Cs=n("a"),Ye=r("datasets.Dataset.set_format()"),Qe=r(" completes the last two steps on-the-fly. After you set the format, wrap the dataset in "),na=n("code"),Ue=r("torch.utils.data.DataLoader"),Je=r(":"),Ke=h(),b(_s.$$.fragment),Xe=h(),$=n("tf"),Ve=r("1. Rename the `label` column to `labels`, the expected input name in [TFBertForSequenceClassification](https://huggingface.co/transformers/model_doc/bert#tfbertforsequenceclassification):\n\n	"),b(ws.$$.fragment),Ze=h(),ss=n("ol"),ta=n("li"),sn=r("Retrieve the actual tensors from the Dataset object instead of using the current Python objects."),an=h(),T=n("li"),en=r("Filter the dataset to only return the model inputs: "),la=n("code"),nn=r("input_ids"),tn=r(", "),ra=n("code"),ln=r("token_type_ids"),rn=r(", and "),pa=n("code"),pn=r("attention_mask"),on=r("."),cn=h(),as=n("p"),zs=n("a"),hn=r("datasets.Dataset.set_format()"),mn=r(" completes the last two steps on-the-fly. After you set the format, wrap the dataset in "),oa=n("code"),un=r("tf.data.Dataset"),dn=r(":"),bn=h(),b(ks.$$.fragment),za=h(),D=n("h2"),es=n("a"),ca=n("span"),b(ys.$$.fragment),fn=h(),ia=n("span"),jn=r("Train the model"),Aa=h(),ns=n("frameworkcontent"),As=n("pt"),gn=r(`Lastly, create a simple training loop and start training:

	`),b(vs.$$.fragment),_n=h(),Bs=n("tf"),wn=r(`Lastly, compile the model and start training:

	`),b(xs.$$.fragment),Ba=h(),N=n("h2"),ts=n("a"),ha=n("span"),b($s.$$.fragment),kn=h(),ma=n("span"),yn=r("What's next?"),Pa=h(),Ps=n("p"),vn=r("This completes the basic steps of loading a dataset to train a model. You loaded and processed the MRPC dataset to fine-tune BERT to determine whether sentence pairs have the same meaning."),Da=h(),C=n("p"),xn=r("For your next steps, take a look at our "),Ds=n("a"),$n=r("How-to guides"),En=r(" and learn how to achieve a specific task (e.g. load a dataset offline, add a dataset to the Hub, change the name of a column). Or if you want to deepen your knowledge of \u{1F917} Datasets core concepts, read our "),Ns=n("a"),Tn=r("Conceptual Guides"),Fn=r("."),this.h()},l(s){const o=Et('[data-svelte="svelte-1phssyn"]',document.head);d=t(o,"META",{name:!0,content:!0}),o.forEach(e),F=m(s),u=t(s,"H1",{class:!0});var Es=l(u);k=t(Es,"A",{id:!0,class:!0,href:!0});var Cn=l(k);z=t(Cn,"SPAN",{});var zn=l(z);f(y.$$.fragment,zn),zn.forEach(e),Cn.forEach(e),q=m(Es),I=t(Es,"SPAN",{});var An=l(I);Va=p(An,"Quick Start"),An.forEach(e),Es.forEach(e),ba=m(s),M=t(s,"P",{});var Ra=l(M);Za=p(Ra,"The quick start is intended for developers who are ready to dive in to the code, and see an end-to-end example of how they can integrate \u{1F917} Datasets into their model training workflow. For beginners who are looking for a gentler introduction, we recommend you begin with the "),Ts=t(Ra,"A",{href:!0});var Bn=l(Ts);se=p(Bn,"tutorials"),Bn.forEach(e),ae=p(Ra,"."),Ra.forEach(e),fa=m(s),H=t(s,"P",{});var Oa=l(H);ee=p(Oa,"In the quick start, you will walkthrough all the steps to fine-tune "),os=t(Oa,"A",{href:!0,rel:!0});var Pn=l(os);ne=p(Pn,"BERT"),Pn.forEach(e),te=p(Oa," on a paraphrase classification task. Depending on the specific dataset you use, these steps may vary, but the general steps of how to load a dataset and process it are the same."),Oa.forEach(e),ja=m(s),f(G.$$.fragment,s),ga=m(s),Fs=t(s,"P",{});var Dn=l(Fs);le=p(Dn,"Get started by installing \u{1F917} Datasets:"),Dn.forEach(e),_a=m(s),f(cs.$$.fragment,s),wa=m(s),A=t(s,"H2",{class:!0});var La=l(A);W=t(La,"A",{id:!0,class:!0,href:!0});var Nn=l(W);Ms=t(Nn,"SPAN",{});var Rn=l(Ms);f(is.$$.fragment,Rn),Rn.forEach(e),Nn.forEach(e),re=m(La),Hs=t(La,"SPAN",{});var On=l(Hs);pe=p(On,"Load the dataset and model"),On.forEach(e),La.forEach(e),ka=m(s),S=t(s,"P",{});var Rs=l(S);oe=p(Rs,"Begin by loading the "),hs=t(Rs,"A",{href:!0,rel:!0});var Ln=l(hs);ce=p(Ln,"Microsoft Research Paraphrase Corpus (MRPC)"),Ln.forEach(e),ie=p(Rs," training dataset from the "),ms=t(Rs,"A",{href:!0,rel:!0});var In=l(ms);he=p(In,"General Language Understanding Evaluation (GLUE) benchmark"),In.forEach(e),me=p(Rs,". MRPC is a corpus of human annotated sentence pairs used to train a model to determine whether sentence pairs are semantically equivalent."),Rs.forEach(e),ya=m(s),f(us.$$.fragment,s),va=m(s),Y=t(s,"P",{});var Ia=l(Y);ue=p(Ia,"Next, import the pre-trained BERT model and its tokenizer from the "),ds=t(Ia,"A",{href:!0,rel:!0});var Mn=l(ds);de=p(Mn,"\u{1F917} Transformers"),Mn.forEach(e),be=p(Ia," library:"),Ia.forEach(e),xa=m(s),Q=t(s,"FRAMEWORKCONTENT",{});var Ma=l(Q);Gs=t(Ma,"PT",{});var Hn=l(Gs);fe=p(Hn,`\`\`\`py
>>> from transformers import AutoModelForSequenceClassification, AutoTokenizer
>>> model = AutoModelForSequenceClassification.from_pretrained('bert-base-cased')
Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
>>> tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')
\`\`\``),Hn.forEach(e),je=m(Ma),Ws=t(Ma,"TF",{});var Gn=l(Ws);ge=p(Gn,`\`\`\`pt
>>> from transformers import TFAutoModelForSequenceClassification, AutoTokenizer
>>> model = TFAutoModelForSequenceClassification.from_pretrained("bert-base-cased")
Some weights of the model checkpoint at bert-base-cased were not used when initializing TFBertForSequenceClassification: ['nsp___cls', 'mlm___cls']
- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['dropout_37', 'classifier']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
>>> tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')
\`\`\``),Gn.forEach(e),Ma.forEach(e),$a=m(s),B=t(s,"H2",{class:!0});var Ha=l(B);U=t(Ha,"A",{id:!0,class:!0,href:!0});var Wn=l(U);Ys=t(Wn,"SPAN",{});var Yn=l(Ys);f(bs.$$.fragment,Yn),Yn.forEach(e),Wn.forEach(e),_e=m(Ha),Qs=t(Ha,"SPAN",{});var Qn=l(Qs);we=p(Qn,"Tokenize the dataset"),Qn.forEach(e),Ha.forEach(e),Ea=m(s),J=t(s,"P",{});var Ga=l(J);ke=p(Ga,"The next step is to tokenize the text in order to build sequences of integers the model can understand. Encode the entire dataset with "),qs=t(Ga,"A",{href:!0});var Un=l(qs);ye=p(Un,"datasets.Dataset.map()"),Un.forEach(e),ve=p(Ga,", and truncate and pad the inputs to the maximum length of the model. This ensures the appropriate tensor batches are built."),Ga.forEach(e),Ta=m(s),f(fs.$$.fragment,s),Fa=m(s),v=t(s,"P",{});var ls=l(v);xe=p(ls,"Notice how there are three new columns in the dataset: "),Us=t(ls,"CODE",{});var Jn=l(Us);$e=p(Jn,"input_ids"),Jn.forEach(e),Ee=p(ls,", "),Js=t(ls,"CODE",{});var Kn=l(Js);Te=p(Kn,"token_type_ids"),Kn.forEach(e),Fe=p(ls,", and "),Ks=t(ls,"CODE",{});var Xn=l(Ks);qe=p(Xn,"attention_mask"),Xn.forEach(e),Se=p(ls,". These columns are the inputs to the model."),ls.forEach(e),qa=m(s),P=t(s,"H2",{class:!0});var Wa=l(P);K=t(Wa,"A",{id:!0,class:!0,href:!0});var Vn=l(K);Xs=t(Vn,"SPAN",{});var Zn=l(Xs);f(js.$$.fragment,Zn),Zn.forEach(e),Vn.forEach(e),Ce=m(Wa),Vs=t(Wa,"SPAN",{});var st=l(Vs);ze=p(st,"Format the dataset"),st.forEach(e),Wa.forEach(e),Sa=m(s),Ss=t(s,"P",{});var at=l(Ss);Ae=p(at,"Depending on whether you are using PyTorch, TensorFlow, or JAX, you will need to format the dataset accordingly. There are three changes you need to make to the dataset:"),at.forEach(e),Ca=m(s),X=t(s,"FRAMEWORKCONTENT",{});var Ya=l(X);x=t(Ya,"PT",{});var R=l(x);Be=p(R,"1. Rename the `label` column to `labels`, the expected input name in [BertForSequenceClassification](https://huggingface.co/transformers/model_doc/bert#transformers.BertForSequenceClassification.forward):\n\n	"),f(gs.$$.fragment,R),Pe=m(R),V=t(R,"OL",{start:!0});var Qa=l(V);Zs=t(Qa,"LI",{});var et=l(Zs);De=p(et,"Retrieve the actual tensors from the Dataset object instead of using the current Python objects."),et.forEach(e),Ne=m(Qa),E=t(Qa,"LI",{});var rs=l(E);Re=p(rs,"Filter the dataset to only return the model inputs: "),sa=t(rs,"CODE",{});var nt=l(sa);Oe=p(nt,"input_ids"),nt.forEach(e),Le=p(rs,", "),aa=t(rs,"CODE",{});var tt=l(aa);Ie=p(tt,"token_type_ids"),tt.forEach(e),Me=p(rs,", and "),ea=t(rs,"CODE",{});var lt=l(ea);He=p(lt,"attention_mask"),lt.forEach(e),Ge=p(rs,"."),rs.forEach(e),Qa.forEach(e),We=m(R),Z=t(R,"P",{});var ua=l(Z);Cs=t(ua,"A",{href:!0});var rt=l(Cs);Ye=p(rt,"datasets.Dataset.set_format()"),rt.forEach(e),Qe=p(ua," completes the last two steps on-the-fly. After you set the format, wrap the dataset in "),na=t(ua,"CODE",{});var pt=l(na);Ue=p(pt,"torch.utils.data.DataLoader"),pt.forEach(e),Je=p(ua,":"),ua.forEach(e),Ke=m(R),f(_s.$$.fragment,R),R.forEach(e),Xe=m(Ya),$=t(Ya,"TF",{});var O=l($);Ve=p(O,"1. Rename the `label` column to `labels`, the expected input name in [TFBertForSequenceClassification](https://huggingface.co/transformers/model_doc/bert#tfbertforsequenceclassification):\n\n	"),f(ws.$$.fragment,O),Ze=m(O),ss=t(O,"OL",{start:!0});var Ua=l(ss);ta=t(Ua,"LI",{});var ot=l(ta);sn=p(ot,"Retrieve the actual tensors from the Dataset object instead of using the current Python objects."),ot.forEach(e),an=m(Ua),T=t(Ua,"LI",{});var ps=l(T);en=p(ps,"Filter the dataset to only return the model inputs: "),la=t(ps,"CODE",{});var ct=l(la);nn=p(ct,"input_ids"),ct.forEach(e),tn=p(ps,", "),ra=t(ps,"CODE",{});var it=l(ra);ln=p(it,"token_type_ids"),it.forEach(e),rn=p(ps,", and "),pa=t(ps,"CODE",{});var ht=l(pa);pn=p(ht,"attention_mask"),ht.forEach(e),on=p(ps,"."),ps.forEach(e),Ua.forEach(e),cn=m(O),as=t(O,"P",{});var da=l(as);zs=t(da,"A",{href:!0});var mt=l(zs);hn=p(mt,"datasets.Dataset.set_format()"),mt.forEach(e),mn=p(da," completes the last two steps on-the-fly. After you set the format, wrap the dataset in "),oa=t(da,"CODE",{});var ut=l(oa);un=p(ut,"tf.data.Dataset"),ut.forEach(e),dn=p(da,":"),da.forEach(e),bn=m(O),f(ks.$$.fragment,O),O.forEach(e),Ya.forEach(e),za=m(s),D=t(s,"H2",{class:!0});var Ja=l(D);es=t(Ja,"A",{id:!0,class:!0,href:!0});var dt=l(es);ca=t(dt,"SPAN",{});var bt=l(ca);f(ys.$$.fragment,bt),bt.forEach(e),dt.forEach(e),fn=m(Ja),ia=t(Ja,"SPAN",{});var ft=l(ia);jn=p(ft,"Train the model"),ft.forEach(e),Ja.forEach(e),Aa=m(s),ns=t(s,"FRAMEWORKCONTENT",{});var Ka=l(ns);As=t(Ka,"PT",{});var qn=l(As);gn=p(qn,`Lastly, create a simple training loop and start training:

	`),f(vs.$$.fragment,qn),qn.forEach(e),_n=m(Ka),Bs=t(Ka,"TF",{});var Sn=l(Bs);wn=p(Sn,`Lastly, compile the model and start training:

	`),f(xs.$$.fragment,Sn),Sn.forEach(e),Ka.forEach(e),Ba=m(s),N=t(s,"H2",{class:!0});var Xa=l(N);ts=t(Xa,"A",{id:!0,class:!0,href:!0});var jt=l(ts);ha=t(jt,"SPAN",{});var gt=l(ha);f($s.$$.fragment,gt),gt.forEach(e),jt.forEach(e),kn=m(Xa),ma=t(Xa,"SPAN",{});var _t=l(ma);yn=p(_t,"What's next?"),_t.forEach(e),Xa.forEach(e),Pa=m(s),Ps=t(s,"P",{});var wt=l(Ps);vn=p(wt,"This completes the basic steps of loading a dataset to train a model. You loaded and processed the MRPC dataset to fine-tune BERT to determine whether sentence pairs have the same meaning."),wt.forEach(e),Da=m(s),C=t(s,"P",{});var Os=l(C);xn=p(Os,"For your next steps, take a look at our "),Ds=t(Os,"A",{href:!0});var kt=l(Ds);$n=p(kt,"How-to guides"),kt.forEach(e),En=p(Os," and learn how to achieve a specific task (e.g. load a dataset offline, add a dataset to the Hub, change the name of a column). Or if you want to deepen your knowledge of \u{1F917} Datasets core concepts, read our "),Ns=t(Os,"A",{href:!0});var yt=l(Ns);Tn=p(yt,"Conceptual Guides"),yt.forEach(e),Fn=p(Os,"."),Os.forEach(e),this.h()},h(){c(d,"name","hf:doc:metadata"),c(d,"content",JSON.stringify(St)),c(k,"id","quick-start"),c(k,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(k,"href","#quick-start"),c(u,"class","relative group"),c(Ts,"href","./tutorial"),c(os,"href","https://huggingface.co/bert-base-cased"),c(os,"rel","nofollow"),c(W,"id","load-the-dataset-and-model"),c(W,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(W,"href","#load-the-dataset-and-model"),c(A,"class","relative group"),c(hs,"href","https://huggingface.co/datasets/viewer/?dataset=glue&config=mrpc"),c(hs,"rel","nofollow"),c(ms,"href","https://huggingface.co/datasets/glue"),c(ms,"rel","nofollow"),c(ds,"href","https://huggingface.co/transformers/"),c(ds,"rel","nofollow"),c(U,"id","tokenize-the-dataset"),c(U,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(U,"href","#tokenize-the-dataset"),c(B,"class","relative group"),c(qs,"href","/docs/datasets/pr_3880/en/package_reference/main_classes#datasets.Dataset.map"),c(K,"id","format-the-dataset"),c(K,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(K,"href","#format-the-dataset"),c(P,"class","relative group"),c(V,"start","2"),c(Cs,"href","/docs/datasets/pr_3880/en/package_reference/main_classes#datasets.Dataset.set_format"),c(ss,"start","2"),c(zs,"href","/docs/datasets/pr_3880/en/package_reference/main_classes#datasets.Dataset.set_format"),c(es,"id","train-the-model"),c(es,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(es,"href","#train-the-model"),c(D,"class","relative group"),c(ts,"id","whats-next"),c(ts,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(ts,"href","#whats-next"),c(N,"class","relative group"),c(Ds,"href","./how_to"),c(Ns,"href","./about_arrow")},m(s,o){a(document.head,d),i(s,F,o),i(s,u,o),a(u,k),a(k,z),j(y,z,null),a(u,q),a(u,I),a(I,Va),i(s,ba,o),i(s,M,o),a(M,Za),a(M,Ts),a(Ts,se),a(M,ae),i(s,fa,o),i(s,H,o),a(H,ee),a(H,os),a(os,ne),a(H,te),i(s,ja,o),j(G,s,o),i(s,ga,o),i(s,Fs,o),a(Fs,le),i(s,_a,o),j(cs,s,o),i(s,wa,o),i(s,A,o),a(A,W),a(W,Ms),j(is,Ms,null),a(A,re),a(A,Hs),a(Hs,pe),i(s,ka,o),i(s,S,o),a(S,oe),a(S,hs),a(hs,ce),a(S,ie),a(S,ms),a(ms,he),a(S,me),i(s,ya,o),j(us,s,o),i(s,va,o),i(s,Y,o),a(Y,ue),a(Y,ds),a(ds,de),a(Y,be),i(s,xa,o),i(s,Q,o),a(Q,Gs),a(Gs,fe),a(Q,je),a(Q,Ws),a(Ws,ge),i(s,$a,o),i(s,B,o),a(B,U),a(U,Ys),j(bs,Ys,null),a(B,_e),a(B,Qs),a(Qs,we),i(s,Ea,o),i(s,J,o),a(J,ke),a(J,qs),a(qs,ye),a(J,ve),i(s,Ta,o),j(fs,s,o),i(s,Fa,o),i(s,v,o),a(v,xe),a(v,Us),a(Us,$e),a(v,Ee),a(v,Js),a(Js,Te),a(v,Fe),a(v,Ks),a(Ks,qe),a(v,Se),i(s,qa,o),i(s,P,o),a(P,K),a(K,Xs),j(js,Xs,null),a(P,Ce),a(P,Vs),a(Vs,ze),i(s,Sa,o),i(s,Ss,o),a(Ss,Ae),i(s,Ca,o),i(s,X,o),a(X,x),a(x,Be),j(gs,x,null),a(x,Pe),a(x,V),a(V,Zs),a(Zs,De),a(V,Ne),a(V,E),a(E,Re),a(E,sa),a(sa,Oe),a(E,Le),a(E,aa),a(aa,Ie),a(E,Me),a(E,ea),a(ea,He),a(E,Ge),a(x,We),a(x,Z),a(Z,Cs),a(Cs,Ye),a(Z,Qe),a(Z,na),a(na,Ue),a(Z,Je),a(x,Ke),j(_s,x,null),a(X,Xe),a(X,$),a($,Ve),j(ws,$,null),a($,Ze),a($,ss),a(ss,ta),a(ta,sn),a(ss,an),a(ss,T),a(T,en),a(T,la),a(la,nn),a(T,tn),a(T,ra),a(ra,ln),a(T,rn),a(T,pa),a(pa,pn),a(T,on),a($,cn),a($,as),a(as,zs),a(zs,hn),a(as,mn),a(as,oa),a(oa,un),a(as,dn),a($,bn),j(ks,$,null),i(s,za,o),i(s,D,o),a(D,es),a(es,ca),j(ys,ca,null),a(D,fn),a(D,ia),a(ia,jn),i(s,Aa,o),i(s,ns,o),a(ns,As),a(As,gn),j(vs,As,null),a(ns,_n),a(ns,Bs),a(Bs,wn),j(xs,Bs,null),i(s,Ba,o),i(s,N,o),a(N,ts),a(ts,ha),j($s,ha,null),a(N,kn),a(N,ma),a(ma,yn),i(s,Pa,o),i(s,Ps,o),a(Ps,vn),i(s,Da,o),i(s,C,o),a(C,xn),a(C,Ds),a(Ds,$n),a(C,En),a(C,Ns),a(Ns,Tn),a(C,Fn),Na=!0},p(s,[o]){const Es={};o&2&&(Es.$$scope={dirty:o,ctx:s}),G.$set(Es)},i(s){Na||(g(y.$$.fragment,s),g(G.$$.fragment,s),g(cs.$$.fragment,s),g(is.$$.fragment,s),g(us.$$.fragment,s),g(bs.$$.fragment,s),g(fs.$$.fragment,s),g(js.$$.fragment,s),g(gs.$$.fragment,s),g(_s.$$.fragment,s),g(ws.$$.fragment,s),g(ks.$$.fragment,s),g(ys.$$.fragment,s),g(vs.$$.fragment,s),g(xs.$$.fragment,s),g($s.$$.fragment,s),Na=!0)},o(s){_(y.$$.fragment,s),_(G.$$.fragment,s),_(cs.$$.fragment,s),_(is.$$.fragment,s),_(us.$$.fragment,s),_(bs.$$.fragment,s),_(fs.$$.fragment,s),_(js.$$.fragment,s),_(gs.$$.fragment,s),_(_s.$$.fragment,s),_(ws.$$.fragment,s),_(ks.$$.fragment,s),_(ys.$$.fragment,s),_(vs.$$.fragment,s),_(xs.$$.fragment,s),_($s.$$.fragment,s),Na=!1},d(s){e(d),s&&e(F),s&&e(u),w(y),s&&e(ba),s&&e(M),s&&e(fa),s&&e(H),s&&e(ja),w(G,s),s&&e(ga),s&&e(Fs),s&&e(_a),w(cs,s),s&&e(wa),s&&e(A),w(is),s&&e(ka),s&&e(S),s&&e(ya),w(us,s),s&&e(va),s&&e(Y),s&&e(xa),s&&e(Q),s&&e($a),s&&e(B),w(bs),s&&e(Ea),s&&e(J),s&&e(Ta),w(fs,s),s&&e(Fa),s&&e(v),s&&e(qa),s&&e(P),w(js),s&&e(Sa),s&&e(Ss),s&&e(Ca),s&&e(X),w(gs),w(_s),w(ws),w(ks),s&&e(za),s&&e(D),w(ys),s&&e(Aa),s&&e(ns),w(vs),w(xs),s&&e(Ba),s&&e(N),w($s),s&&e(Pa),s&&e(Ps),s&&e(Da),s&&e(C)}}}const St={local:"quick-start",sections:[{local:"load-the-dataset-and-model",title:"Load the dataset and model"},{local:"tokenize-the-dataset",title:"Tokenize the dataset"},{local:"format-the-dataset",title:"Format the dataset"},{local:"train-the-model",title:"Train the model"},{local:"whats-next",title:"What's next?"}],title:"Quick Start"};function Ct(Is,d,F){let{fw:u}=d;return Is.$$set=k=>{"fw"in k&&F(0,u=k.fw)},[u]}class Dt extends vt{constructor(d){super();xt(this,d,Ct,qt,$t,{fw:0})}}export{Dt as default,St as metadata};
