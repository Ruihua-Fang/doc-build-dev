import{S as Fp,i as Vp,s as Bp,e as t,k as f,w as _,t as n,M as Jp,c as l,d as a,m as d,a as p,x as g,h as r,b as h,N as Rp,F as s,g as o,y as v,q as b,o as x,B as $}from"../chunks/vendor-aa873a46.js";import{T as sl}from"../chunks/Tip-f7f252ab.js";import{I as Y}from"../chunks/IconCopyLink-d0ca3106.js";import{C as P}from"../chunks/CodeBlock-1f14baf3.js";import{F as Up}from"../chunks/FrameworkContent-9b4630ee.js";import"../chunks/IconTensorflow-b9816778.js";function Gp(O){let c,j,m,w,y,u,k,E;return{c(){c=t("p"),j=n("An "),m=t("a"),w=n("datasets.IterableDataset"),y=n(" is useful for iterative jobs like training a model. You shouldn\u2019t use a "),u=t("a"),k=n("datasets.IterableDataset"),E=n(" for jobs that require random access to examples because you have to iterate all over it using a for loop. Getting the last example in an iterable dataset would require you to iterate over all the previous examples."),this.h()},l(q){c=l(q,"P",{});var D=p(c);j=r(D,"An "),m=l(D,"A",{href:!0});var T=p(m);w=r(T,"datasets.IterableDataset"),T.forEach(a),y=r(D," is useful for iterative jobs like training a model. You shouldn\u2019t use a "),u=l(D,"A",{href:!0});var H=p(u);k=r(H,"datasets.IterableDataset"),H.forEach(a),E=r(D," for jobs that require random access to examples because you have to iterate all over it using a for loop. Getting the last example in an iterable dataset would require you to iterate over all the previous examples."),D.forEach(a),this.h()},h(){h(m,"href","/docs/datasets/pr_3880/en/package_reference/main_classes#datasets.IterableDataset"),h(u,"href","/docs/datasets/pr_3880/en/package_reference/main_classes#datasets.IterableDataset")},m(q,D){o(q,c,D),s(c,j),s(c,m),s(m,w),s(c,y),s(c,u),s(u,k),s(c,E)},d(q){q&&a(c)}}}function Wp(O){let c,j,m,w;return{c(){c=t("p"),j=t("a"),m=n("datasets.IterableDataset.shuffle()"),w=n(" will also shuffle the order of the shards if the dataset is sharded into multiple sets."),this.h()},l(y){c=l(y,"P",{});var u=p(c);j=l(u,"A",{href:!0});var k=p(j);m=r(k,"datasets.IterableDataset.shuffle()"),k.forEach(a),w=r(u," will also shuffle the order of the shards if the dataset is sharded into multiple sets."),u.forEach(a),this.h()},h(){h(j,"href","/docs/datasets/pr_3880/en/package_reference/main_classes#datasets.IterableDataset.shuffle")},m(y,u){o(y,c,u),s(c,j),s(j,m),s(c,w)},d(y){y&&a(c)}}}function Kp(O){let c,j,m,w,y,u,k,E,q,D,T,H,ne;return{c(){c=t("p"),j=t("code"),m=n("take"),w=n(" and "),y=t("code"),u=n("skip"),k=n(" prevent future calls to "),E=t("code"),q=n("shuffle"),D=n(" because they lock in the order of the shards. You should "),T=t("code"),H=n("shuffle"),ne=n(" your dataset before splitting it.")},l(S){c=l(S,"P",{});var I=p(c);j=l(I,"CODE",{});var is=p(j);m=r(is,"take"),is.forEach(a),w=r(I," and "),y=l(I,"CODE",{});var os=p(y);u=r(os,"skip"),os.forEach(a),k=r(I," prevent future calls to "),E=l(I,"CODE",{});var re=p(E);q=r(re,"shuffle"),re.forEach(a),D=r(I," because they lock in the order of the shards. You should "),T=l(I,"CODE",{});var hs=p(T);H=r(hs,"shuffle"),hs.forEach(a),ne=r(I," your dataset before splitting it."),I.forEach(a)},m(S,I){o(S,c,I),s(c,j),s(j,m),s(c,w),s(c,y),s(y,u),s(c,k),s(c,E),s(E,q),s(c,D),s(c,T),s(T,H),s(c,ne)},d(S){S&&a(c)}}}function Qp(O){let c,j,m,w,y;return{c(){c=t("p"),j=n("See other examples of batch processing in "),m=t("a"),w=n("the batched map processing documentation"),y=n(". They work the same for iterable datasets."),this.h()},l(u){c=l(u,"P",{});var k=p(c);j=r(k,"See other examples of batch processing in "),m=l(k,"A",{href:!0});var E=p(m);w=r(E,"the batched map processing documentation"),E.forEach(a),y=r(k,". They work the same for iterable datasets."),k.forEach(a),this.h()},h(){h(m,"href","./process#batch-processing")},m(u,k){o(u,c,k),s(c,j),s(c,m),s(m,w),s(c,y)},d(u){u&&a(c)}}}function Xp(O){let c,j,m,w,y,u,k,E,q,D,T,H,ne,S,I,is,os,re,hs,Ca,J,fs,_r,al,ds,gr,Na,z,tl,qe,ll,nl,Us,rl,pl,cs,il,ol,Ma,ze,Oa,R,hl,ms,fl,dl,us,cl,ml,La,pe,Ya,U,ie,Gs,Ce,ul,Ws,_l,Ha,C,gl,_s,vl,bl,gs,xl,$l,vs,jl,wl,Ra,N,yl,Ks,kl,El,Qs,Dl,Il,bs,Al,Tl,Fa,Ne,Va,oe,Ba,G,he,Xs,Me,Sl,Zs,Pl,Ja,fe,ql,ea,zl,Cl,Ua,de,Nl,sa,Ml,Ol,Ga,Oe,Wa,W,ce,aa,Le,Ll,ta,Yl,Ka,xs,Hl,Qa,$s,me,js,Rl,Fl,la,Vl,Bl,Xa,Ye,Za,ws,ue,ys,Jl,Ul,na,Gl,Wl,et,He,st,_e,at,ks,tt,K,ge,ra,Re,Kl,pa,Ql,lt,Q,Es,Xl,Zl,Ds,en,sn,nt,Fe,rt,ve,an,ia,tn,ln,pt,Ve,it,F,nn,oa,rn,pn,ha,on,hn,ot,X,be,fa,Be,fn,da,dn,ht,xe,cn,Is,mn,un,ft,Je,dt,Z,$e,ca,Ue,_n,ma,gn,ct,A,vn,As,bn,xn,Ts,$n,jn,Ss,wn,yn,Ps,kn,En,qs,Dn,In,mt,zs,An,ut,V,Tn,Cs,Sn,Pn,ua,qn,zn,_t,Ge,gt,je,Cn,Ns,Nn,Mn,vt,We,bt,we,On,Ms,Ln,Yn,xt,B,Hn,_a,Rn,Fn,Os,Vn,Bn,$t,Ke,jt,ee,ye,ga,Qe,Jn,va,Un,wt,L,Ls,Gn,Wn,ba,Kn,Qn,xa,Xn,Zn,yt,se,ke,$a,Xe,er,ja,sr,kt,Ze,Et,Ee,Dt,ae,De,wa,es,ar,ya,tr,It,Ie,lr,Ys,nr,rr,At,ss,Tt,te,Hs,pr,ir,ka,or,hr,St,as,Pt,le,Ae,Ea,ts,fr,Da,dr,qt,ls,Rs,cr,mr,zt,ns,Ct;return u=new Y({}),ze=new P({props:{code:`from datasets import load_dataset
dataset = load_dataset('oscar', "unshuffled_deduplicated_en", split='train', streaming=True)
print(next(iter(dataset)))`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;oscar&#x27;</span>, <span class="hljs-string">&quot;unshuffled_deduplicated_en&quot;</span>, split=<span class="hljs-string">&#x27;train&#x27;</span>, streaming=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(<span class="hljs-built_in">next</span>(<span class="hljs-built_in">iter</span>(dataset)))
{<span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;Mtendere Village was inspired by the vision of Chief Napoleon Dzombe, which he shared with John Blanchard during his first visit to Malawi. Chief Napoleon conveyed the desperate need for a program to intervene and care for the orphans and vulnerable children (OVC) in Malawi, and John committed to help...</span>`}}),pe=new sl({props:{$$slots:{default:[Gp]},$$scope:{ctx:O}}}),Ce=new Y({}),Ne=new P({props:{code:`from datasets import load_dataset
dataset = load_dataset('oscar', "unshuffled_deduplicated_en", split='train', streaming=True)
shuffled_dataset = dataset.shuffle(seed=42, buffer_size=10_000)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;oscar&#x27;</span>, <span class="hljs-string">&quot;unshuffled_deduplicated_en&quot;</span>, split=<span class="hljs-string">&#x27;train&#x27;</span>, streaming=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>shuffled_dataset = dataset.shuffle(seed=<span class="hljs-number">42</span>, buffer_size=<span class="hljs-number">10_000</span>)`}}),oe=new sl({props:{$$slots:{default:[Wp]},$$scope:{ctx:O}}}),Me=new Y({}),Oe=new P({props:{code:`for epoch in range(epochs):
    shuffled_dataset.set_epoch(epoch)
    for example in shuffled_dataset:
        ...`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(epochs):
<span class="hljs-meta">... </span>    shuffled_dataset.set_epoch(epoch)
<span class="hljs-meta">... </span>    <span class="hljs-keyword">for</span> example <span class="hljs-keyword">in</span> shuffled_dataset:
<span class="hljs-meta">... </span>        ...`}}),Le=new Y({}),Ye=new P({props:{code:`dataset = load_dataset('oscar', "unshuffled_deduplicated_en", split='train', streaming=True)
dataset_head = dataset.take(2)
list(dataset_head)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;oscar&#x27;</span>, <span class="hljs-string">&quot;unshuffled_deduplicated_en&quot;</span>, split=<span class="hljs-string">&#x27;train&#x27;</span>, streaming=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset_head = dataset.take(<span class="hljs-number">2</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">list</span>(dataset_head)
[{<span class="hljs-string">&#x27;id&#x27;</span>: <span class="hljs-number">0</span>, <span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;Mtendere Village was...&#x27;</span>}, {<span class="hljs-string">&#x27;id&#x27;</span>: <span class="hljs-number">1</span>, <span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;Lily James cannot fight the music...&#x27;</span>}]`}}),He=new P({props:{code:"train_dataset = shuffled_dataset.skip(1000)",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>train_dataset = shuffled_dataset.skip(<span class="hljs-number">1000</span>)'}}),_e=new sl({props:{warning:"&lcub;true}",$$slots:{default:[Kp]},$$scope:{ctx:O}}}),Re=new Y({}),Fe=new P({props:{code:`from datasets import interleave_datasets
from itertools import islice
en_dataset = load_dataset('oscar', "unshuffled_deduplicated_en", split='train', streaming=True)
fr_dataset = load_dataset('oscar', "unshuffled_deduplicated_fr", split='train', streaming=True)

multilingual_dataset = interleave_datasets([en_dataset, fr_dataset])
print(list(islice(multilingual_dataset, 2)))`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> interleave_datasets
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> itertools <span class="hljs-keyword">import</span> islice
<span class="hljs-meta">&gt;&gt;&gt; </span>en_dataset = load_dataset(<span class="hljs-string">&#x27;oscar&#x27;</span>, <span class="hljs-string">&quot;unshuffled_deduplicated_en&quot;</span>, split=<span class="hljs-string">&#x27;train&#x27;</span>, streaming=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>fr_dataset = load_dataset(<span class="hljs-string">&#x27;oscar&#x27;</span>, <span class="hljs-string">&quot;unshuffled_deduplicated_fr&quot;</span>, split=<span class="hljs-string">&#x27;train&#x27;</span>, streaming=<span class="hljs-literal">True</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>multilingual_dataset = interleave_datasets([en_dataset, fr_dataset])
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(<span class="hljs-built_in">list</span>(islice(multilingual_dataset, <span class="hljs-number">2</span>)))
[{<span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;Mtendere Village was inspired by the vision...&#x27;</span>}, {<span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&quot;M\xE9dia de d\xE9bat d&#x27;id\xE9es, de culture et de litt\xE9rature...&quot;</span>}]`}}),Ve=new P({props:{code:`multilingual_dataset_with_oversampling = interleave_datasets([en_dataset, fr_dataset], probabilities=[0.8, 0.2], seed=42)
print(list(islice(multilingual_dataset_with_oversampling, 2)))`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>multilingual_dataset_with_oversampling = interleave_datasets([en_dataset, fr_dataset], probabilities=[<span class="hljs-number">0.8</span>, <span class="hljs-number">0.2</span>], seed=<span class="hljs-number">42</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(<span class="hljs-built_in">list</span>(islice(multilingual_dataset_with_oversampling, <span class="hljs-number">2</span>)))
[{<span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;Mtendere Village was inspired by the vision...&#x27;</span>}, {<span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;Lily James cannot fight the music...&#x27;</span>}]`}}),Be=new Y({}),Je=new P({props:{code:`from datasets import load_dataset
dataset = load_dataset('mc4', 'en', streaming=True, split='train')
dataset = dataset.remove_columns('timestamp')`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;mc4&#x27;</span>, <span class="hljs-string">&#x27;en&#x27;</span>, streaming=<span class="hljs-literal">True</span>, split=<span class="hljs-string">&#x27;train&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = dataset.remove_columns(<span class="hljs-string">&#x27;timestamp&#x27;</span>)`}}),Ue=new Y({}),Ge=new P({props:{code:`def add_prefix(example):
    example['text'] = 'My text: ' + example['text']
    return example`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">def</span> <span class="hljs-title function_">add_prefix</span>(<span class="hljs-params">example</span>):
<span class="hljs-meta">... </span>    example[<span class="hljs-string">&#x27;text&#x27;</span>] = <span class="hljs-string">&#x27;My text: &#x27;</span> + example[<span class="hljs-string">&#x27;text&#x27;</span>]
<span class="hljs-meta">... </span>    <span class="hljs-keyword">return</span> example`}}),We=new P({props:{code:`from datasets import load_dataset
dataset = load_dataset('oscar', 'unshuffled_deduplicated_en', streaming=True, split='train')
updated_dataset = dataset.map(add_prefix)
list(updated_dataset.take(3))`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;oscar&#x27;</span>, <span class="hljs-string">&#x27;unshuffled_deduplicated_en&#x27;</span>, streaming=<span class="hljs-literal">True</span>, split=<span class="hljs-string">&#x27;train&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>updated_dataset = dataset.<span class="hljs-built_in">map</span>(add_prefix)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">list</span>(updated_dataset.take(<span class="hljs-number">3</span>))
[{<span class="hljs-string">&#x27;id&#x27;</span>: <span class="hljs-number">0</span>, <span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;My text: Mtendere Village was inspired by...&#x27;</span>},
 {<span class="hljs-string">&#x27;id&#x27;</span>: <span class="hljs-number">1</span>, <span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;My text: Lily James cannot fight the music...&#x27;</span>},
 {<span class="hljs-string">&#x27;id&#x27;</span>: <span class="hljs-number">2</span>, <span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;My text: &quot;I\\&#x27;d love to help kickstart...&#x27;</span>}]`}}),Ke=new P({props:{code:`updated_dataset = dataset.map(add_prefix, remove_columns=["id"])
list(updated_dataset.take(3))`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>updated_dataset = dataset.<span class="hljs-built_in">map</span>(add_prefix, remove_columns=[<span class="hljs-string">&quot;id&quot;</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">list</span>(updated_dataset.take(<span class="hljs-number">3</span>))
[{<span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;My text: Mtendere Village was inspired by...&#x27;</span>},
 {<span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;My text: Lily James cannot fight the music...&#x27;</span>},
 {<span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;My text: &quot;I\\&#x27;d love to help kickstart...&#x27;</span>}]`}}),Qe=new Y({}),Xe=new Y({}),Ze=new P({props:{code:`from datasets import load_dataset
from transformers import AutoTokenizer
dataset = load_dataset("mc4", "en", streaming=True, split="train")
tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')
def encode(examples):
    return tokenizer(examples['text'], truncation=True, padding='max_length')
dataset = dataset.map(encode, batched=True, remove_columns=["text", "timestamp", "url"])
next(iter(dataset))`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;mc4&quot;</span>, <span class="hljs-string">&quot;en&quot;</span>, streaming=<span class="hljs-literal">True</span>, split=<span class="hljs-string">&quot;train&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&#x27;distilbert-base-uncased&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">def</span> <span class="hljs-title function_">encode</span>(<span class="hljs-params">examples</span>):
<span class="hljs-meta">... </span>    <span class="hljs-keyword">return</span> tokenizer(examples[<span class="hljs-string">&#x27;text&#x27;</span>], truncation=<span class="hljs-literal">True</span>, padding=<span class="hljs-string">&#x27;max_length&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = dataset.<span class="hljs-built_in">map</span>(encode, batched=<span class="hljs-literal">True</span>, remove_columns=[<span class="hljs-string">&quot;text&quot;</span>, <span class="hljs-string">&quot;timestamp&quot;</span>, <span class="hljs-string">&quot;url&quot;</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">next</span>(<span class="hljs-built_in">iter</span>(dataset))
{<span class="hljs-string">&#x27;input_ids&#x27;</span>: <span class="hljs-number">101</span>, <span class="hljs-number">8466</span>, <span class="hljs-number">1018</span>, <span class="hljs-number">1010</span>, <span class="hljs-number">4029</span>, <span class="hljs-number">2475</span>, <span class="hljs-number">2062</span>, <span class="hljs-number">18558</span>, <span class="hljs-number">3100</span>, <span class="hljs-number">2061</span>, ...,<span class="hljs-number">1106</span>, <span class="hljs-number">3739</span>, <span class="hljs-number">102</span>],
<span class="hljs-string">&#x27;attention_mask&#x27;</span>: [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, ..., <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]}`}}),Ee=new sl({props:{$$slots:{default:[Qp]},$$scope:{ctx:O}}}),es=new Y({}),ss=new P({props:{code:`from datasets import load_dataset
dataset = load_dataset('oscar', 'unshuffled_deduplicated_en', streaming=True, split='train')
start_with_ar = dataset.filter(lambda example: example['text'].startswith('Ar'))
next(iter(start_with_ar))`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;oscar&#x27;</span>, <span class="hljs-string">&#x27;unshuffled_deduplicated_en&#x27;</span>, streaming=<span class="hljs-literal">True</span>, split=<span class="hljs-string">&#x27;train&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>start_with_ar = dataset.<span class="hljs-built_in">filter</span>(<span class="hljs-keyword">lambda</span> example: example[<span class="hljs-string">&#x27;text&#x27;</span>].startswith(<span class="hljs-string">&#x27;Ar&#x27;</span>))
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">next</span>(<span class="hljs-built_in">iter</span>(start_with_ar))
{<span class="hljs-string">&#x27;id&#x27;</span>: <span class="hljs-number">4</span>, <span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;Are you looking for Number the Stars (Essential Modern Classics)?...&#x27;</span>}`}}),as=new P({props:{code:`even_dataset = dataset.filter(lambda example, idx: idx % 2 == 0, with_indices=True)
list(even_dataset.take(3))`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>even_dataset = dataset.<span class="hljs-built_in">filter</span>(<span class="hljs-keyword">lambda</span> example, idx: idx % <span class="hljs-number">2</span> == <span class="hljs-number">0</span>, with_indices=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">list</span>(even_dataset.take(<span class="hljs-number">3</span>))
[{<span class="hljs-string">&#x27;id&#x27;</span>: <span class="hljs-number">0</span>, <span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;Mtendere Village was inspired by the vision of Chief Napoleon Dzombe, ...&#x27;</span>},
 {<span class="hljs-string">&#x27;id&#x27;</span>: <span class="hljs-number">2</span>, <span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;&quot;I\\&#x27;d love to help kickstart continued development! And 0 EUR/month...&#x27;</span>},
 {<span class="hljs-string">&#x27;id&#x27;</span>: <span class="hljs-number">4</span>, <span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;Are you looking for Number the Stars (Essential Modern Classics)? Normally, ...&#x27;</span>}]`}}),ts=new Y({}),ns=new Up({props:{pytorch:!1,tensorflow:!1,jax:!1}}),{c(){c=t("meta"),j=f(),m=t("h1"),w=t("a"),y=t("span"),_(u.$$.fragment),k=f(),E=t("span"),q=n("Stream"),D=f(),T=t("p"),H=n("Dataset streaming lets you get started with a dataset without waiting for the entire dataset to download. The data is downloaded progressively as you iterate over the dataset. This is especially helpful when:"),ne=f(),S=t("ul"),I=t("li"),is=n("You don\u2019t want to wait for an extremely large dataset to download."),os=f(),re=t("li"),hs=n("The dataset size exceeds the amount of disk space on your computer."),Ca=f(),J=t("div"),fs=t("img"),al=f(),ds=t("img"),Na=f(),z=t("p"),tl=n("For example, the English split of the "),qe=t("a"),ll=n("OSCAR"),nl=n(" dataset is 1.2 terabytes, but you can use it instantly with streaming. Stream a dataset by setting "),Us=t("code"),rl=n("streaming=True"),pl=n(" in "),cs=t("a"),il=n("datasets.load_dataset()"),ol=n(" as shown below:"),Ma=f(),_(ze.$$.fragment),Oa=f(),R=t("p"),hl=n("Loading a dataset in streaming mode creates a new dataset type instance (instead of the classic "),ms=t("a"),fl=n("datasets.Dataset"),dl=n(" object), known as an "),us=t("a"),cl=n("datasets.IterableDataset"),ml=n(". This special type of dataset has its own set of processing methods shown below."),La=f(),_(pe.$$.fragment),Ya=f(),U=t("h2"),ie=t("a"),Gs=t("span"),_(Ce.$$.fragment),ul=f(),Ws=t("span"),_l=n("Shuffle"),Ha=f(),C=t("p"),gl=n("Like a regular "),_s=t("a"),vl=n("datasets.Dataset"),bl=n(" object, you can also shuffle a "),gs=t("a"),xl=n("datasets.IterableDataset"),$l=n(" with "),vs=t("a"),jl=n("datasets.IterableDataset.shuffle()"),wl=n("."),Ra=f(),N=t("p"),yl=n("The "),Ks=t("code"),kl=n("buffer_size"),El=n(" argument controls the size of the buffer to randomly sample examples from. Let\u2019s say your dataset has one million examples, and you set the "),Qs=t("code"),Dl=n("buffer_size"),Il=n(" to ten thousand. "),bs=t("a"),Al=n("datasets.IterableDataset.shuffle()"),Tl=n(" will randomly select examples from the first ten thousand examples in the buffer. Selected examples in the buffer are replaced with new examples. By default, the buffer size is 1,000."),Fa=f(),_(Ne.$$.fragment),Va=f(),_(oe.$$.fragment),Ba=f(),G=t("h2"),he=t("a"),Xs=t("span"),_(Me.$$.fragment),Sl=f(),Zs=t("span"),Pl=n("Reshuffle"),Ja=f(),fe=t("p"),ql=n("Sometimes you may want to reshuffle the dataset after each epoch. This will require you to set a different seed for each epoch. Use "),ea=t("code"),zl=n("datasets.IterableDataset.set_epoch()"),Cl=n("in between epochs to tell the dataset what epoch you\u2019re on."),Ua=f(),de=t("p"),Nl=n("Your seed effectively becomes: "),sa=t("code"),Ml=n("initial seed + current epoch"),Ol=n("."),Ga=f(),_(Oe.$$.fragment),Wa=f(),W=t("h2"),ce=t("a"),aa=t("span"),_(Le.$$.fragment),Ll=f(),ta=t("span"),Yl=n("Split dataset"),Ka=f(),xs=t("p"),Hl=n("You can split your dataset one of two ways:"),Qa=f(),$s=t("ul"),me=t("li"),js=t("a"),Rl=n("datasets.IterableDataset.take()"),Fl=n(" returns the first "),la=t("code"),Vl=n("n"),Bl=n(" examples in a dataset:"),Xa=f(),_(Ye.$$.fragment),Za=f(),ws=t("ul"),ue=t("li"),ys=t("a"),Jl=n("datasets.IterableDataset.skip()"),Ul=n(" omits the first "),na=t("code"),Gl=n("n"),Wl=n(" examples in a dataset and returns the remaining examples:"),et=f(),_(He.$$.fragment),st=f(),_(_e.$$.fragment),at=f(),ks=t("a"),tt=f(),K=t("h2"),ge=t("a"),ra=t("span"),_(Re.$$.fragment),Kl=f(),pa=t("span"),Ql=n("Interleave"),lt=f(),Q=t("p"),Es=t("a"),Xl=n("datasets.interleave_datasets()"),Zl=n(" can combine an "),Ds=t("a"),en=n("datasets.IterableDataset"),sn=n(" with other datasets. The combined dataset returns alternating examples from each of the original datasets."),nt=f(),_(Fe.$$.fragment),rt=f(),ve=t("p"),an=n("Define sampling probabilities from each of the original datasets for more control over how each of them are sampled and combined. Set the "),ia=t("code"),tn=n("probabilities"),ln=n(" argument with your desired sampling probabilities:"),pt=f(),_(Ve.$$.fragment),it=f(),F=t("p"),nn=n("Around 80% of the final dataset is made of the "),oa=t("code"),rn=n("en_dataset"),pn=n(", and 20% of the "),ha=t("code"),on=n("fr_dataset"),hn=n("."),ot=f(),X=t("h2"),be=t("a"),fa=t("span"),_(Be.$$.fragment),fn=f(),da=t("span"),dn=n("Remove"),ht=f(),xe=t("p"),cn=n("Remove columns on-the-fly with "),Is=t("a"),mn=n("datasets.IterableDataset.remove_columns()"),un=n(". Specify the name of the column to remove:"),ft=f(),_(Je.$$.fragment),dt=f(),Z=t("h2"),$e=t("a"),ca=t("span"),_(Ue.$$.fragment),_n=f(),ma=t("span"),gn=n("Map"),ct=f(),A=t("p"),vn=n("Similar to the "),As=t("a"),bn=n("datasets.Dataset.map()"),xn=n(" function for a regular "),Ts=t("a"),$n=n("datasets.Dataset"),jn=n(", \u{1F917}  Datasets features "),Ss=t("a"),wn=n("datasets.IterableDataset.map()"),yn=n(" for processing "),Ps=t("a"),kn=n("datasets.IterableDataset"),En=n(`\\s.
`),qs=t("a"),Dn=n("datasets.IterableDataset.map()"),In=n(" applies processing on-the-fly when examples are streamed."),mt=f(),zs=t("p"),An=n("It allows you to apply a processing function to each example in a dataset, independently or in batches. This function can even create new rows and columns."),ut=f(),V=t("p"),Tn=n("The following example demonstrates how to tokenize a "),Cs=t("a"),Sn=n("datasets.IterableDataset"),Pn=n(". The function needs to accept and output a "),ua=t("code"),qn=n("dict"),zn=n(":"),_t=f(),_(Ge.$$.fragment),gt=f(),je=t("p"),Cn=n("Next, apply this function to the dataset with "),Ns=t("a"),Nn=n("datasets.IterableDataset.map()"),Mn=n(":"),vt=f(),_(We.$$.fragment),bt=f(),we=t("p"),On=n("Let\u2019s take a look at another example, except this time, you will remove a column with "),Ms=t("a"),Ln=n("datasets.IterableDataset.map()"),Yn=n(". When you remove a column, it is only removed after the example has been provided to the mapped function. This allows the mapped function to use the content of the columns before they are removed."),xt=f(),B=t("p"),Hn=n("Specify the column to remove with the "),_a=t("code"),Rn=n("remove_columns"),Fn=n(" argument in "),Os=t("a"),Vn=n("datasets.IterableDataset.map()"),Bn=n(":"),$t=f(),_(Ke.$$.fragment),jt=f(),ee=t("h3"),ye=t("a"),ga=t("span"),_(Qe.$$.fragment),Jn=f(),va=t("span"),Un=n("Batch processing"),wt=f(),L=t("p"),Ls=t("a"),Gn=n("datasets.IterableDataset.map()"),Wn=n(" also supports working with batches of examples. Operate on batches by setting "),ba=t("code"),Kn=n("batched=True"),Qn=n(". The default batch size is 1000, but you can adjust it with the "),xa=t("code"),Xn=n("batch_size"),Zn=n(" argument. This opens the door to many interesting applications such as tokenization, splitting long sentences into shorter chunks, and data augmentation."),yt=f(),se=t("h4"),ke=t("a"),$a=t("span"),_(Xe.$$.fragment),er=f(),ja=t("span"),sr=n("Tokenization"),kt=f(),_(Ze.$$.fragment),Et=f(),_(Ee.$$.fragment),Dt=f(),ae=t("h3"),De=t("a"),wa=t("span"),_(es.$$.fragment),ar=f(),ya=t("span"),tr=n("Filter"),It=f(),Ie=t("p"),lr=n("You can filter rows in the dataset based on a predicate function using "),Ys=t("a"),nr=n("datasets.Dataset.filter()"),rr=n(". It returns rows that match a specified condition:"),At=f(),_(ss.$$.fragment),Tt=f(),te=t("p"),Hs=t("a"),pr=n("datasets.Dataset.filter()"),ir=n(" can also filter by indices if you set "),ka=t("code"),or=n("with_indices=True"),hr=n(":"),St=f(),_(as.$$.fragment),Pt=f(),le=t("h2"),Ae=t("a"),Ea=t("span"),_(ts.$$.fragment),fr=f(),Da=t("span"),dr=n("Stream in a training loop"),qt=f(),ls=t("p"),Rs=t("a"),cr=n("datasets.IterableDataset"),mr=n(" can be integrated into a training loop. First, shuffle the dataset:"),zt=f(),_(ns.$$.fragment),this.h()},l(e){const i=Jp('[data-svelte="svelte-1phssyn"]',document.head);c=l(i,"META",{name:!0,content:!0}),i.forEach(a),j=d(e),m=l(e,"H1",{class:!0});var rs=p(m);w=l(rs,"A",{id:!0,class:!0,href:!0});var Ia=p(w);y=l(Ia,"SPAN",{});var Aa=p(y);g(u.$$.fragment,Aa),Aa.forEach(a),Ia.forEach(a),k=d(rs),E=l(rs,"SPAN",{});var Ta=p(E);q=r(Ta,"Stream"),Ta.forEach(a),rs.forEach(a),D=d(e),T=l(e,"P",{});var vr=p(T);H=r(vr,"Dataset streaming lets you get started with a dataset without waiting for the entire dataset to download. The data is downloaded progressively as you iterate over the dataset. This is especially helpful when:"),vr.forEach(a),ne=d(e),S=l(e,"UL",{});var Nt=p(S);I=l(Nt,"LI",{});var br=p(I);is=r(br,"You don\u2019t want to wait for an extremely large dataset to download."),br.forEach(a),os=d(Nt),re=l(Nt,"LI",{});var xr=p(re);hs=r(xr,"The dataset size exceeds the amount of disk space on your computer."),xr.forEach(a),Nt.forEach(a),Ca=d(e),J=l(e,"DIV",{class:!0});var Mt=p(J);fs=l(Mt,"IMG",{class:!0,src:!0}),al=d(Mt),ds=l(Mt,"IMG",{class:!0,src:!0}),Mt.forEach(a),Na=d(e),z=l(e,"P",{});var Te=p(z);tl=r(Te,"For example, the English split of the "),qe=l(Te,"A",{href:!0,rel:!0});var $r=p(qe);ll=r($r,"OSCAR"),$r.forEach(a),nl=r(Te," dataset is 1.2 terabytes, but you can use it instantly with streaming. Stream a dataset by setting "),Us=l(Te,"CODE",{});var jr=p(Us);rl=r(jr,"streaming=True"),jr.forEach(a),pl=r(Te," in "),cs=l(Te,"A",{href:!0});var wr=p(cs);il=r(wr,"datasets.load_dataset()"),wr.forEach(a),ol=r(Te," as shown below:"),Te.forEach(a),Ma=d(e),g(ze.$$.fragment,e),Oa=d(e),R=l(e,"P",{});var Fs=p(R);hl=r(Fs,"Loading a dataset in streaming mode creates a new dataset type instance (instead of the classic "),ms=l(Fs,"A",{href:!0});var yr=p(ms);fl=r(yr,"datasets.Dataset"),yr.forEach(a),dl=r(Fs," object), known as an "),us=l(Fs,"A",{href:!0});var kr=p(us);cl=r(kr,"datasets.IterableDataset"),kr.forEach(a),ml=r(Fs,". This special type of dataset has its own set of processing methods shown below."),Fs.forEach(a),La=d(e),g(pe.$$.fragment,e),Ya=d(e),U=l(e,"H2",{class:!0});var Ot=p(U);ie=l(Ot,"A",{id:!0,class:!0,href:!0});var Er=p(ie);Gs=l(Er,"SPAN",{});var Dr=p(Gs);g(Ce.$$.fragment,Dr),Dr.forEach(a),Er.forEach(a),ul=d(Ot),Ws=l(Ot,"SPAN",{});var Ir=p(Ws);_l=r(Ir,"Shuffle"),Ir.forEach(a),Ot.forEach(a),Ha=d(e),C=l(e,"P",{});var Se=p(C);gl=r(Se,"Like a regular "),_s=l(Se,"A",{href:!0});var Ar=p(_s);vl=r(Ar,"datasets.Dataset"),Ar.forEach(a),bl=r(Se," object, you can also shuffle a "),gs=l(Se,"A",{href:!0});var Tr=p(gs);xl=r(Tr,"datasets.IterableDataset"),Tr.forEach(a),$l=r(Se," with "),vs=l(Se,"A",{href:!0});var Sr=p(vs);jl=r(Sr,"datasets.IterableDataset.shuffle()"),Sr.forEach(a),wl=r(Se,"."),Se.forEach(a),Ra=d(e),N=l(e,"P",{});var Pe=p(N);yl=r(Pe,"The "),Ks=l(Pe,"CODE",{});var Pr=p(Ks);kl=r(Pr,"buffer_size"),Pr.forEach(a),El=r(Pe," argument controls the size of the buffer to randomly sample examples from. Let\u2019s say your dataset has one million examples, and you set the "),Qs=l(Pe,"CODE",{});var qr=p(Qs);Dl=r(qr,"buffer_size"),qr.forEach(a),Il=r(Pe," to ten thousand. "),bs=l(Pe,"A",{href:!0});var zr=p(bs);Al=r(zr,"datasets.IterableDataset.shuffle()"),zr.forEach(a),Tl=r(Pe," will randomly select examples from the first ten thousand examples in the buffer. Selected examples in the buffer are replaced with new examples. By default, the buffer size is 1,000."),Pe.forEach(a),Fa=d(e),g(Ne.$$.fragment,e),Va=d(e),g(oe.$$.fragment,e),Ba=d(e),G=l(e,"H2",{class:!0});var Lt=p(G);he=l(Lt,"A",{id:!0,class:!0,href:!0});var Cr=p(he);Xs=l(Cr,"SPAN",{});var Nr=p(Xs);g(Me.$$.fragment,Nr),Nr.forEach(a),Cr.forEach(a),Sl=d(Lt),Zs=l(Lt,"SPAN",{});var Mr=p(Zs);Pl=r(Mr,"Reshuffle"),Mr.forEach(a),Lt.forEach(a),Ja=d(e),fe=l(e,"P",{});var Yt=p(fe);ql=r(Yt,"Sometimes you may want to reshuffle the dataset after each epoch. This will require you to set a different seed for each epoch. Use "),ea=l(Yt,"CODE",{});var Or=p(ea);zl=r(Or,"datasets.IterableDataset.set_epoch()"),Or.forEach(a),Cl=r(Yt,"in between epochs to tell the dataset what epoch you\u2019re on."),Yt.forEach(a),Ua=d(e),de=l(e,"P",{});var Ht=p(de);Nl=r(Ht,"Your seed effectively becomes: "),sa=l(Ht,"CODE",{});var Lr=p(sa);Ml=r(Lr,"initial seed + current epoch"),Lr.forEach(a),Ol=r(Ht,"."),Ht.forEach(a),Ga=d(e),g(Oe.$$.fragment,e),Wa=d(e),W=l(e,"H2",{class:!0});var Rt=p(W);ce=l(Rt,"A",{id:!0,class:!0,href:!0});var Yr=p(ce);aa=l(Yr,"SPAN",{});var Hr=p(aa);g(Le.$$.fragment,Hr),Hr.forEach(a),Yr.forEach(a),Ll=d(Rt),ta=l(Rt,"SPAN",{});var Rr=p(ta);Yl=r(Rr,"Split dataset"),Rr.forEach(a),Rt.forEach(a),Ka=d(e),xs=l(e,"P",{});var Fr=p(xs);Hl=r(Fr,"You can split your dataset one of two ways:"),Fr.forEach(a),Qa=d(e),$s=l(e,"UL",{});var Vr=p($s);me=l(Vr,"LI",{});var Sa=p(me);js=l(Sa,"A",{href:!0});var Br=p(js);Rl=r(Br,"datasets.IterableDataset.take()"),Br.forEach(a),Fl=r(Sa," returns the first "),la=l(Sa,"CODE",{});var Jr=p(la);Vl=r(Jr,"n"),Jr.forEach(a),Bl=r(Sa," examples in a dataset:"),Sa.forEach(a),Vr.forEach(a),Xa=d(e),g(Ye.$$.fragment,e),Za=d(e),ws=l(e,"UL",{});var Ur=p(ws);ue=l(Ur,"LI",{});var Pa=p(ue);ys=l(Pa,"A",{href:!0});var Gr=p(ys);Jl=r(Gr,"datasets.IterableDataset.skip()"),Gr.forEach(a),Ul=r(Pa," omits the first "),na=l(Pa,"CODE",{});var Wr=p(na);Gl=r(Wr,"n"),Wr.forEach(a),Wl=r(Pa," examples in a dataset and returns the remaining examples:"),Pa.forEach(a),Ur.forEach(a),et=d(e),g(He.$$.fragment,e),st=d(e),g(_e.$$.fragment,e),at=d(e),ks=l(e,"A",{id:!0}),p(ks).forEach(a),tt=d(e),K=l(e,"H2",{class:!0});var Ft=p(K);ge=l(Ft,"A",{id:!0,class:!0,href:!0});var Kr=p(ge);ra=l(Kr,"SPAN",{});var Qr=p(ra);g(Re.$$.fragment,Qr),Qr.forEach(a),Kr.forEach(a),Kl=d(Ft),pa=l(Ft,"SPAN",{});var Xr=p(pa);Ql=r(Xr,"Interleave"),Xr.forEach(a),Ft.forEach(a),lt=d(e),Q=l(e,"P",{});var qa=p(Q);Es=l(qa,"A",{href:!0});var Zr=p(Es);Xl=r(Zr,"datasets.interleave_datasets()"),Zr.forEach(a),Zl=r(qa," can combine an "),Ds=l(qa,"A",{href:!0});var ep=p(Ds);en=r(ep,"datasets.IterableDataset"),ep.forEach(a),sn=r(qa," with other datasets. The combined dataset returns alternating examples from each of the original datasets."),qa.forEach(a),nt=d(e),g(Fe.$$.fragment,e),rt=d(e),ve=l(e,"P",{});var Vt=p(ve);an=r(Vt,"Define sampling probabilities from each of the original datasets for more control over how each of them are sampled and combined. Set the "),ia=l(Vt,"CODE",{});var sp=p(ia);tn=r(sp,"probabilities"),sp.forEach(a),ln=r(Vt," argument with your desired sampling probabilities:"),Vt.forEach(a),pt=d(e),g(Ve.$$.fragment,e),it=d(e),F=l(e,"P",{});var Vs=p(F);nn=r(Vs,"Around 80% of the final dataset is made of the "),oa=l(Vs,"CODE",{});var ap=p(oa);rn=r(ap,"en_dataset"),ap.forEach(a),pn=r(Vs,", and 20% of the "),ha=l(Vs,"CODE",{});var tp=p(ha);on=r(tp,"fr_dataset"),tp.forEach(a),hn=r(Vs,"."),Vs.forEach(a),ot=d(e),X=l(e,"H2",{class:!0});var Bt=p(X);be=l(Bt,"A",{id:!0,class:!0,href:!0});var lp=p(be);fa=l(lp,"SPAN",{});var np=p(fa);g(Be.$$.fragment,np),np.forEach(a),lp.forEach(a),fn=d(Bt),da=l(Bt,"SPAN",{});var rp=p(da);dn=r(rp,"Remove"),rp.forEach(a),Bt.forEach(a),ht=d(e),xe=l(e,"P",{});var Jt=p(xe);cn=r(Jt,"Remove columns on-the-fly with "),Is=l(Jt,"A",{href:!0});var pp=p(Is);mn=r(pp,"datasets.IterableDataset.remove_columns()"),pp.forEach(a),un=r(Jt,". Specify the name of the column to remove:"),Jt.forEach(a),ft=d(e),g(Je.$$.fragment,e),dt=d(e),Z=l(e,"H2",{class:!0});var Ut=p(Z);$e=l(Ut,"A",{id:!0,class:!0,href:!0});var ip=p($e);ca=l(ip,"SPAN",{});var op=p(ca);g(Ue.$$.fragment,op),op.forEach(a),ip.forEach(a),_n=d(Ut),ma=l(Ut,"SPAN",{});var hp=p(ma);gn=r(hp,"Map"),hp.forEach(a),Ut.forEach(a),ct=d(e),A=l(e,"P",{});var M=p(A);vn=r(M,"Similar to the "),As=l(M,"A",{href:!0});var fp=p(As);bn=r(fp,"datasets.Dataset.map()"),fp.forEach(a),xn=r(M," function for a regular "),Ts=l(M,"A",{href:!0});var dp=p(Ts);$n=r(dp,"datasets.Dataset"),dp.forEach(a),jn=r(M,", \u{1F917}  Datasets features "),Ss=l(M,"A",{href:!0});var cp=p(Ss);wn=r(cp,"datasets.IterableDataset.map()"),cp.forEach(a),yn=r(M," for processing "),Ps=l(M,"A",{href:!0});var mp=p(Ps);kn=r(mp,"datasets.IterableDataset"),mp.forEach(a),En=r(M,`\\s.
`),qs=l(M,"A",{href:!0});var up=p(qs);Dn=r(up,"datasets.IterableDataset.map()"),up.forEach(a),In=r(M," applies processing on-the-fly when examples are streamed."),M.forEach(a),mt=d(e),zs=l(e,"P",{});var _p=p(zs);An=r(_p,"It allows you to apply a processing function to each example in a dataset, independently or in batches. This function can even create new rows and columns."),_p.forEach(a),ut=d(e),V=l(e,"P",{});var Bs=p(V);Tn=r(Bs,"The following example demonstrates how to tokenize a "),Cs=l(Bs,"A",{href:!0});var gp=p(Cs);Sn=r(gp,"datasets.IterableDataset"),gp.forEach(a),Pn=r(Bs,". The function needs to accept and output a "),ua=l(Bs,"CODE",{});var vp=p(ua);qn=r(vp,"dict"),vp.forEach(a),zn=r(Bs,":"),Bs.forEach(a),_t=d(e),g(Ge.$$.fragment,e),gt=d(e),je=l(e,"P",{});var Gt=p(je);Cn=r(Gt,"Next, apply this function to the dataset with "),Ns=l(Gt,"A",{href:!0});var bp=p(Ns);Nn=r(bp,"datasets.IterableDataset.map()"),bp.forEach(a),Mn=r(Gt,":"),Gt.forEach(a),vt=d(e),g(We.$$.fragment,e),bt=d(e),we=l(e,"P",{});var Wt=p(we);On=r(Wt,"Let\u2019s take a look at another example, except this time, you will remove a column with "),Ms=l(Wt,"A",{href:!0});var xp=p(Ms);Ln=r(xp,"datasets.IterableDataset.map()"),xp.forEach(a),Yn=r(Wt,". When you remove a column, it is only removed after the example has been provided to the mapped function. This allows the mapped function to use the content of the columns before they are removed."),Wt.forEach(a),xt=d(e),B=l(e,"P",{});var Js=p(B);Hn=r(Js,"Specify the column to remove with the "),_a=l(Js,"CODE",{});var $p=p(_a);Rn=r($p,"remove_columns"),$p.forEach(a),Fn=r(Js," argument in "),Os=l(Js,"A",{href:!0});var jp=p(Os);Vn=r(jp,"datasets.IterableDataset.map()"),jp.forEach(a),Bn=r(Js,":"),Js.forEach(a),$t=d(e),g(Ke.$$.fragment,e),jt=d(e),ee=l(e,"H3",{class:!0});var Kt=p(ee);ye=l(Kt,"A",{id:!0,class:!0,href:!0});var wp=p(ye);ga=l(wp,"SPAN",{});var yp=p(ga);g(Qe.$$.fragment,yp),yp.forEach(a),wp.forEach(a),Jn=d(Kt),va=l(Kt,"SPAN",{});var kp=p(va);Un=r(kp,"Batch processing"),kp.forEach(a),Kt.forEach(a),wt=d(e),L=l(e,"P",{});var ps=p(L);Ls=l(ps,"A",{href:!0});var Ep=p(Ls);Gn=r(Ep,"datasets.IterableDataset.map()"),Ep.forEach(a),Wn=r(ps," also supports working with batches of examples. Operate on batches by setting "),ba=l(ps,"CODE",{});var Dp=p(ba);Kn=r(Dp,"batched=True"),Dp.forEach(a),Qn=r(ps,". The default batch size is 1000, but you can adjust it with the "),xa=l(ps,"CODE",{});var Ip=p(xa);Xn=r(Ip,"batch_size"),Ip.forEach(a),Zn=r(ps," argument. This opens the door to many interesting applications such as tokenization, splitting long sentences into shorter chunks, and data augmentation."),ps.forEach(a),yt=d(e),se=l(e,"H4",{class:!0});var Qt=p(se);ke=l(Qt,"A",{id:!0,class:!0,href:!0});var Ap=p(ke);$a=l(Ap,"SPAN",{});var Tp=p($a);g(Xe.$$.fragment,Tp),Tp.forEach(a),Ap.forEach(a),er=d(Qt),ja=l(Qt,"SPAN",{});var Sp=p(ja);sr=r(Sp,"Tokenization"),Sp.forEach(a),Qt.forEach(a),kt=d(e),g(Ze.$$.fragment,e),Et=d(e),g(Ee.$$.fragment,e),Dt=d(e),ae=l(e,"H3",{class:!0});var Xt=p(ae);De=l(Xt,"A",{id:!0,class:!0,href:!0});var Pp=p(De);wa=l(Pp,"SPAN",{});var qp=p(wa);g(es.$$.fragment,qp),qp.forEach(a),Pp.forEach(a),ar=d(Xt),ya=l(Xt,"SPAN",{});var zp=p(ya);tr=r(zp,"Filter"),zp.forEach(a),Xt.forEach(a),It=d(e),Ie=l(e,"P",{});var Zt=p(Ie);lr=r(Zt,"You can filter rows in the dataset based on a predicate function using "),Ys=l(Zt,"A",{href:!0});var Cp=p(Ys);nr=r(Cp,"datasets.Dataset.filter()"),Cp.forEach(a),rr=r(Zt,". It returns rows that match a specified condition:"),Zt.forEach(a),At=d(e),g(ss.$$.fragment,e),Tt=d(e),te=l(e,"P",{});var za=p(te);Hs=l(za,"A",{href:!0});var Np=p(Hs);pr=r(Np,"datasets.Dataset.filter()"),Np.forEach(a),ir=r(za," can also filter by indices if you set "),ka=l(za,"CODE",{});var Mp=p(ka);or=r(Mp,"with_indices=True"),Mp.forEach(a),hr=r(za,":"),za.forEach(a),St=d(e),g(as.$$.fragment,e),Pt=d(e),le=l(e,"H2",{class:!0});var el=p(le);Ae=l(el,"A",{id:!0,class:!0,href:!0});var Op=p(Ae);Ea=l(Op,"SPAN",{});var Lp=p(Ea);g(ts.$$.fragment,Lp),Lp.forEach(a),Op.forEach(a),fr=d(el),Da=l(el,"SPAN",{});var Yp=p(Da);dr=r(Yp,"Stream in a training loop"),Yp.forEach(a),el.forEach(a),qt=d(e),ls=l(e,"P",{});var ur=p(ls);Rs=l(ur,"A",{href:!0});var Hp=p(Rs);cr=r(Hp,"datasets.IterableDataset"),Hp.forEach(a),mr=r(ur," can be integrated into a training loop. First, shuffle the dataset:"),ur.forEach(a),zt=d(e),g(ns.$$.fragment,e),this.h()},h(){h(c,"name","hf:doc:metadata"),h(c,"content",JSON.stringify(Zp)),h(w,"id","stream"),h(w,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(w,"href","#stream"),h(m,"class","relative group"),h(fs,"class","block dark:hidden"),Rp(fs.src,_r="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/datasets/streaming.gif")||h(fs,"src",_r),h(ds,"class","hidden dark:block"),Rp(ds.src,gr="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/datasets/streaming-dark.gif")||h(ds,"src",gr),h(J,"class","flex justify-center"),h(qe,"href","https://huggingface.co/datasets/oscar"),h(qe,"rel","nofollow"),h(cs,"href","/docs/datasets/pr_3880/en/package_reference/loading_methods#datasets.load_dataset"),h(ms,"href","/docs/datasets/pr_3880/en/package_reference/main_classes#datasets.Dataset"),h(us,"href","/docs/datasets/pr_3880/en/package_reference/main_classes#datasets.IterableDataset"),h(ie,"id","shuffle"),h(ie,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(ie,"href","#shuffle"),h(U,"class","relative group"),h(_s,"href","/docs/datasets/pr_3880/en/package_reference/main_classes#datasets.Dataset"),h(gs,"href","/docs/datasets/pr_3880/en/package_reference/main_classes#datasets.IterableDataset"),h(vs,"href","/docs/datasets/pr_3880/en/package_reference/main_classes#datasets.IterableDataset.shuffle"),h(bs,"href","/docs/datasets/pr_3880/en/package_reference/main_classes#datasets.IterableDataset.shuffle"),h(he,"id","reshuffle"),h(he,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(he,"href","#reshuffle"),h(G,"class","relative group"),h(ce,"id","split-dataset"),h(ce,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(ce,"href","#split-dataset"),h(W,"class","relative group"),h(js,"href","/docs/datasets/pr_3880/en/package_reference/main_classes#datasets.IterableDataset.take"),h(ys,"href","/docs/datasets/pr_3880/en/package_reference/main_classes#datasets.IterableDataset.skip"),h(ks,"id","interleave_datasets"),h(ge,"id","interleave"),h(ge,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(ge,"href","#interleave"),h(K,"class","relative group"),h(Es,"href","/docs/datasets/pr_3880/en/package_reference/main_classes#datasets.interleave_datasets"),h(Ds,"href","/docs/datasets/pr_3880/en/package_reference/main_classes#datasets.IterableDataset"),h(be,"id","remove"),h(be,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(be,"href","#remove"),h(X,"class","relative group"),h(Is,"href","/docs/datasets/pr_3880/en/package_reference/main_classes#datasets.IterableDataset.remove_columns"),h($e,"id","map"),h($e,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h($e,"href","#map"),h(Z,"class","relative group"),h(As,"href","/docs/datasets/pr_3880/en/package_reference/main_classes#datasets.Dataset.map"),h(Ts,"href","/docs/datasets/pr_3880/en/package_reference/main_classes#datasets.Dataset"),h(Ss,"href","/docs/datasets/pr_3880/en/package_reference/main_classes#datasets.IterableDataset.map"),h(Ps,"href","/docs/datasets/pr_3880/en/package_reference/main_classes#datasets.IterableDataset"),h(qs,"href","/docs/datasets/pr_3880/en/package_reference/main_classes#datasets.IterableDataset.map"),h(Cs,"href","/docs/datasets/pr_3880/en/package_reference/main_classes#datasets.IterableDataset"),h(Ns,"href","/docs/datasets/pr_3880/en/package_reference/main_classes#datasets.IterableDataset.map"),h(Ms,"href","/docs/datasets/pr_3880/en/package_reference/main_classes#datasets.IterableDataset.map"),h(Os,"href","/docs/datasets/pr_3880/en/package_reference/main_classes#datasets.IterableDataset.map"),h(ye,"id","batch-processing"),h(ye,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(ye,"href","#batch-processing"),h(ee,"class","relative group"),h(Ls,"href","/docs/datasets/pr_3880/en/package_reference/main_classes#datasets.IterableDataset.map"),h(ke,"id","tokenization"),h(ke,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(ke,"href","#tokenization"),h(se,"class","relative group"),h(De,"id","filter"),h(De,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(De,"href","#filter"),h(ae,"class","relative group"),h(Ys,"href","/docs/datasets/pr_3880/en/package_reference/main_classes#datasets.Dataset.filter"),h(Hs,"href","/docs/datasets/pr_3880/en/package_reference/main_classes#datasets.Dataset.filter"),h(Ae,"id","stream-in-a-training-loop"),h(Ae,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(Ae,"href","#stream-in-a-training-loop"),h(le,"class","relative group"),h(Rs,"href","/docs/datasets/pr_3880/en/package_reference/main_classes#datasets.IterableDataset")},m(e,i){s(document.head,c),o(e,j,i),o(e,m,i),s(m,w),s(w,y),v(u,y,null),s(m,k),s(m,E),s(E,q),o(e,D,i),o(e,T,i),s(T,H),o(e,ne,i),o(e,S,i),s(S,I),s(I,is),s(S,os),s(S,re),s(re,hs),o(e,Ca,i),o(e,J,i),s(J,fs),s(J,al),s(J,ds),o(e,Na,i),o(e,z,i),s(z,tl),s(z,qe),s(qe,ll),s(z,nl),s(z,Us),s(Us,rl),s(z,pl),s(z,cs),s(cs,il),s(z,ol),o(e,Ma,i),v(ze,e,i),o(e,Oa,i),o(e,R,i),s(R,hl),s(R,ms),s(ms,fl),s(R,dl),s(R,us),s(us,cl),s(R,ml),o(e,La,i),v(pe,e,i),o(e,Ya,i),o(e,U,i),s(U,ie),s(ie,Gs),v(Ce,Gs,null),s(U,ul),s(U,Ws),s(Ws,_l),o(e,Ha,i),o(e,C,i),s(C,gl),s(C,_s),s(_s,vl),s(C,bl),s(C,gs),s(gs,xl),s(C,$l),s(C,vs),s(vs,jl),s(C,wl),o(e,Ra,i),o(e,N,i),s(N,yl),s(N,Ks),s(Ks,kl),s(N,El),s(N,Qs),s(Qs,Dl),s(N,Il),s(N,bs),s(bs,Al),s(N,Tl),o(e,Fa,i),v(Ne,e,i),o(e,Va,i),v(oe,e,i),o(e,Ba,i),o(e,G,i),s(G,he),s(he,Xs),v(Me,Xs,null),s(G,Sl),s(G,Zs),s(Zs,Pl),o(e,Ja,i),o(e,fe,i),s(fe,ql),s(fe,ea),s(ea,zl),s(fe,Cl),o(e,Ua,i),o(e,de,i),s(de,Nl),s(de,sa),s(sa,Ml),s(de,Ol),o(e,Ga,i),v(Oe,e,i),o(e,Wa,i),o(e,W,i),s(W,ce),s(ce,aa),v(Le,aa,null),s(W,Ll),s(W,ta),s(ta,Yl),o(e,Ka,i),o(e,xs,i),s(xs,Hl),o(e,Qa,i),o(e,$s,i),s($s,me),s(me,js),s(js,Rl),s(me,Fl),s(me,la),s(la,Vl),s(me,Bl),o(e,Xa,i),v(Ye,e,i),o(e,Za,i),o(e,ws,i),s(ws,ue),s(ue,ys),s(ys,Jl),s(ue,Ul),s(ue,na),s(na,Gl),s(ue,Wl),o(e,et,i),v(He,e,i),o(e,st,i),v(_e,e,i),o(e,at,i),o(e,ks,i),o(e,tt,i),o(e,K,i),s(K,ge),s(ge,ra),v(Re,ra,null),s(K,Kl),s(K,pa),s(pa,Ql),o(e,lt,i),o(e,Q,i),s(Q,Es),s(Es,Xl),s(Q,Zl),s(Q,Ds),s(Ds,en),s(Q,sn),o(e,nt,i),v(Fe,e,i),o(e,rt,i),o(e,ve,i),s(ve,an),s(ve,ia),s(ia,tn),s(ve,ln),o(e,pt,i),v(Ve,e,i),o(e,it,i),o(e,F,i),s(F,nn),s(F,oa),s(oa,rn),s(F,pn),s(F,ha),s(ha,on),s(F,hn),o(e,ot,i),o(e,X,i),s(X,be),s(be,fa),v(Be,fa,null),s(X,fn),s(X,da),s(da,dn),o(e,ht,i),o(e,xe,i),s(xe,cn),s(xe,Is),s(Is,mn),s(xe,un),o(e,ft,i),v(Je,e,i),o(e,dt,i),o(e,Z,i),s(Z,$e),s($e,ca),v(Ue,ca,null),s(Z,_n),s(Z,ma),s(ma,gn),o(e,ct,i),o(e,A,i),s(A,vn),s(A,As),s(As,bn),s(A,xn),s(A,Ts),s(Ts,$n),s(A,jn),s(A,Ss),s(Ss,wn),s(A,yn),s(A,Ps),s(Ps,kn),s(A,En),s(A,qs),s(qs,Dn),s(A,In),o(e,mt,i),o(e,zs,i),s(zs,An),o(e,ut,i),o(e,V,i),s(V,Tn),s(V,Cs),s(Cs,Sn),s(V,Pn),s(V,ua),s(ua,qn),s(V,zn),o(e,_t,i),v(Ge,e,i),o(e,gt,i),o(e,je,i),s(je,Cn),s(je,Ns),s(Ns,Nn),s(je,Mn),o(e,vt,i),v(We,e,i),o(e,bt,i),o(e,we,i),s(we,On),s(we,Ms),s(Ms,Ln),s(we,Yn),o(e,xt,i),o(e,B,i),s(B,Hn),s(B,_a),s(_a,Rn),s(B,Fn),s(B,Os),s(Os,Vn),s(B,Bn),o(e,$t,i),v(Ke,e,i),o(e,jt,i),o(e,ee,i),s(ee,ye),s(ye,ga),v(Qe,ga,null),s(ee,Jn),s(ee,va),s(va,Un),o(e,wt,i),o(e,L,i),s(L,Ls),s(Ls,Gn),s(L,Wn),s(L,ba),s(ba,Kn),s(L,Qn),s(L,xa),s(xa,Xn),s(L,Zn),o(e,yt,i),o(e,se,i),s(se,ke),s(ke,$a),v(Xe,$a,null),s(se,er),s(se,ja),s(ja,sr),o(e,kt,i),v(Ze,e,i),o(e,Et,i),v(Ee,e,i),o(e,Dt,i),o(e,ae,i),s(ae,De),s(De,wa),v(es,wa,null),s(ae,ar),s(ae,ya),s(ya,tr),o(e,It,i),o(e,Ie,i),s(Ie,lr),s(Ie,Ys),s(Ys,nr),s(Ie,rr),o(e,At,i),v(ss,e,i),o(e,Tt,i),o(e,te,i),s(te,Hs),s(Hs,pr),s(te,ir),s(te,ka),s(ka,or),s(te,hr),o(e,St,i),v(as,e,i),o(e,Pt,i),o(e,le,i),s(le,Ae),s(Ae,Ea),v(ts,Ea,null),s(le,fr),s(le,Da),s(Da,dr),o(e,qt,i),o(e,ls,i),s(ls,Rs),s(Rs,cr),s(ls,mr),o(e,zt,i),v(ns,e,i),Ct=!0},p(e,[i]){const rs={};i&2&&(rs.$$scope={dirty:i,ctx:e}),pe.$set(rs);const Ia={};i&2&&(Ia.$$scope={dirty:i,ctx:e}),oe.$set(Ia);const Aa={};i&2&&(Aa.$$scope={dirty:i,ctx:e}),_e.$set(Aa);const Ta={};i&2&&(Ta.$$scope={dirty:i,ctx:e}),Ee.$set(Ta)},i(e){Ct||(b(u.$$.fragment,e),b(ze.$$.fragment,e),b(pe.$$.fragment,e),b(Ce.$$.fragment,e),b(Ne.$$.fragment,e),b(oe.$$.fragment,e),b(Me.$$.fragment,e),b(Oe.$$.fragment,e),b(Le.$$.fragment,e),b(Ye.$$.fragment,e),b(He.$$.fragment,e),b(_e.$$.fragment,e),b(Re.$$.fragment,e),b(Fe.$$.fragment,e),b(Ve.$$.fragment,e),b(Be.$$.fragment,e),b(Je.$$.fragment,e),b(Ue.$$.fragment,e),b(Ge.$$.fragment,e),b(We.$$.fragment,e),b(Ke.$$.fragment,e),b(Qe.$$.fragment,e),b(Xe.$$.fragment,e),b(Ze.$$.fragment,e),b(Ee.$$.fragment,e),b(es.$$.fragment,e),b(ss.$$.fragment,e),b(as.$$.fragment,e),b(ts.$$.fragment,e),b(ns.$$.fragment,e),Ct=!0)},o(e){x(u.$$.fragment,e),x(ze.$$.fragment,e),x(pe.$$.fragment,e),x(Ce.$$.fragment,e),x(Ne.$$.fragment,e),x(oe.$$.fragment,e),x(Me.$$.fragment,e),x(Oe.$$.fragment,e),x(Le.$$.fragment,e),x(Ye.$$.fragment,e),x(He.$$.fragment,e),x(_e.$$.fragment,e),x(Re.$$.fragment,e),x(Fe.$$.fragment,e),x(Ve.$$.fragment,e),x(Be.$$.fragment,e),x(Je.$$.fragment,e),x(Ue.$$.fragment,e),x(Ge.$$.fragment,e),x(We.$$.fragment,e),x(Ke.$$.fragment,e),x(Qe.$$.fragment,e),x(Xe.$$.fragment,e),x(Ze.$$.fragment,e),x(Ee.$$.fragment,e),x(es.$$.fragment,e),x(ss.$$.fragment,e),x(as.$$.fragment,e),x(ts.$$.fragment,e),x(ns.$$.fragment,e),Ct=!1},d(e){a(c),e&&a(j),e&&a(m),$(u),e&&a(D),e&&a(T),e&&a(ne),e&&a(S),e&&a(Ca),e&&a(J),e&&a(Na),e&&a(z),e&&a(Ma),$(ze,e),e&&a(Oa),e&&a(R),e&&a(La),$(pe,e),e&&a(Ya),e&&a(U),$(Ce),e&&a(Ha),e&&a(C),e&&a(Ra),e&&a(N),e&&a(Fa),$(Ne,e),e&&a(Va),$(oe,e),e&&a(Ba),e&&a(G),$(Me),e&&a(Ja),e&&a(fe),e&&a(Ua),e&&a(de),e&&a(Ga),$(Oe,e),e&&a(Wa),e&&a(W),$(Le),e&&a(Ka),e&&a(xs),e&&a(Qa),e&&a($s),e&&a(Xa),$(Ye,e),e&&a(Za),e&&a(ws),e&&a(et),$(He,e),e&&a(st),$(_e,e),e&&a(at),e&&a(ks),e&&a(tt),e&&a(K),$(Re),e&&a(lt),e&&a(Q),e&&a(nt),$(Fe,e),e&&a(rt),e&&a(ve),e&&a(pt),$(Ve,e),e&&a(it),e&&a(F),e&&a(ot),e&&a(X),$(Be),e&&a(ht),e&&a(xe),e&&a(ft),$(Je,e),e&&a(dt),e&&a(Z),$(Ue),e&&a(ct),e&&a(A),e&&a(mt),e&&a(zs),e&&a(ut),e&&a(V),e&&a(_t),$(Ge,e),e&&a(gt),e&&a(je),e&&a(vt),$(We,e),e&&a(bt),e&&a(we),e&&a(xt),e&&a(B),e&&a($t),$(Ke,e),e&&a(jt),e&&a(ee),$(Qe),e&&a(wt),e&&a(L),e&&a(yt),e&&a(se),$(Xe),e&&a(kt),$(Ze,e),e&&a(Et),$(Ee,e),e&&a(Dt),e&&a(ae),$(es),e&&a(It),e&&a(Ie),e&&a(At),$(ss,e),e&&a(Tt),e&&a(te),e&&a(St),$(as,e),e&&a(Pt),e&&a(le),$(ts),e&&a(qt),e&&a(ls),e&&a(zt),$(ns,e)}}}const Zp={local:"stream",sections:[{local:"shuffle",title:"Shuffle"},{local:"reshuffle",title:"Reshuffle"},{local:"split-dataset",title:"Split dataset"},{local:"interleave",title:"Interleave"},{local:"remove",title:"Remove"},{local:"map",sections:[{local:"batch-processing",sections:[{local:"tokenization",title:"Tokenization"}],title:"Batch processing"},{local:"filter",title:"Filter"}],title:"Map"},{local:"stream-in-a-training-loop",title:"Stream in a training loop"}],title:"Stream"};function ei(O,c,j){let{fw:m}=c;return O.$$set=w=>{"fw"in w&&j(0,m=w.fw)},[m]}class pi extends Fp{constructor(c){super();Vp(this,c,ei,Xp,Bp,{fw:0})}}export{pi as default,Zp as metadata};
