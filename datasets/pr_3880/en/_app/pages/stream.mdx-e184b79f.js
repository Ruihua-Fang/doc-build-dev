import{S as Xp,i as Zp,s as so,e as t,k as d,w as _,t as n,M as eo,c as l,d as a,m as c,a as p,x as g,h as r,b as h,N as Qp,F as e,g as i,y as b,q as v,o as j,B as x}from"../chunks/vendor-e67aec41.js";import{T as nl}from"../chunks/Tip-76459d1c.js";import{I as F}from"../chunks/IconCopyLink-ffd7f84e.js";import{C as S}from"../chunks/CodeBlock-e2bcf023.js";function ao(N){let f,w,m,$,k,u,y,E;return{c(){f=t("p"),w=n("An "),m=t("a"),$=n("datasets.IterableDataset"),k=n(" is useful for iterative jobs like training a model. You shouldn\u2019t use a "),u=t("a"),y=n("datasets.IterableDataset"),E=n(" for jobs that require random access to examples because you have to iterate all over it using a for loop. Getting the last example in an iterable dataset would require you to iterate over all the previous examples."),this.h()},l(z){f=l(z,"P",{});var D=p(f);w=r(D,"An "),m=l(D,"A",{href:!0});var T=p(m);$=r(T,"datasets.IterableDataset"),T.forEach(a),k=r(D," is useful for iterative jobs like training a model. You shouldn\u2019t use a "),u=l(D,"A",{href:!0});var R=p(u);y=r(R,"datasets.IterableDataset"),R.forEach(a),E=r(D," for jobs that require random access to examples because you have to iterate all over it using a for loop. Getting the last example in an iterable dataset would require you to iterate over all the previous examples."),D.forEach(a),this.h()},h(){h(m,"href","/docs/datasets/pr_3880/en/package_reference/main_classes#datasets.IterableDataset"),h(u,"href","/docs/datasets/pr_3880/en/package_reference/main_classes#datasets.IterableDataset")},m(z,D){i(z,f,D),e(f,w),e(f,m),e(m,$),e(f,k),e(f,u),e(u,y),e(f,E)},d(z){z&&a(f)}}}function to(N){let f,w,m,$;return{c(){f=t("p"),w=t("a"),m=n("datasets.IterableDataset.shuffle()"),$=n(" will also shuffle the order of the shards if the dataset is sharded into multiple sets."),this.h()},l(k){f=l(k,"P",{});var u=p(f);w=l(u,"A",{href:!0});var y=p(w);m=r(y,"datasets.IterableDataset.shuffle()"),y.forEach(a),$=r(u," will also shuffle the order of the shards if the dataset is sharded into multiple sets."),u.forEach(a),this.h()},h(){h(w,"href","/docs/datasets/pr_3880/en/package_reference/main_classes#datasets.IterableDataset.shuffle")},m(k,u){i(k,f,u),e(f,w),e(w,m),e(f,$)},d(k){k&&a(f)}}}function lo(N){let f,w,m,$,k,u,y,E,z,D,T,R,ns;return{c(){f=t("p"),w=t("code"),m=n("take"),$=n(" and "),k=t("code"),u=n("skip"),y=n(" prevent future calls to "),E=t("code"),z=n("shuffle"),D=n(" because they lock in the order of the shards. You should "),T=t("code"),R=n("shuffle"),ns=n(" your dataset before splitting it.")},l(q){f=l(q,"P",{});var A=p(f);w=l(A,"CODE",{});var ie=p(w);m=r(ie,"take"),ie.forEach(a),$=r(A," and "),k=l(A,"CODE",{});var he=p(k);u=r(he,"skip"),he.forEach(a),y=r(A," prevent future calls to "),E=l(A,"CODE",{});var rs=p(E);z=r(rs,"shuffle"),rs.forEach(a),D=r(A," because they lock in the order of the shards. You should "),T=l(A,"CODE",{});var de=p(T);R=r(de,"shuffle"),de.forEach(a),ns=r(A," your dataset before splitting it."),A.forEach(a)},m(q,A){i(q,f,A),e(f,w),e(w,m),e(f,$),e(f,k),e(k,u),e(f,y),e(f,E),e(E,z),e(f,D),e(f,T),e(T,R),e(f,ns)},d(q){q&&a(f)}}}function no(N){let f,w,m,$,k;return{c(){f=t("p"),w=n("See other examples of batch processing in "),m=t("a"),$=n("the batched map processing documentation"),k=n(". They work the same for iterable datasets."),this.h()},l(u){f=l(u,"P",{});var y=p(f);w=r(y,"See other examples of batch processing in "),m=l(y,"A",{href:!0});var E=p(m);$=r(E,"the batched map processing documentation"),E.forEach(a),k=r(y,". They work the same for iterable datasets."),y.forEach(a),this.h()},h(){h(m,"href","./process#batch-processing")},m(u,y){i(u,f,y),e(f,w),e(f,m),e(m,$),e(f,k)},d(u){u&&a(f)}}}function ro(N){let f,w,m,$,k,u,y,E,z,D,T,R,ns,q,A,ie,he,rs,de,Oa,J,ce,$r,rl,fe,kr,Fa,P,pl,Ps,ol,il,Ge,hl,dl,me,cl,fl,Ra,Ms,Ya,Y,ml,ue,ul,_l,_e,gl,bl,Ha,ps,Va,U,os,Ke,Cs,vl,Qe,jl,Ba,M,xl,ge,wl,$l,be,kl,yl,ve,El,Dl,Ja,C,Al,Xe,Il,Tl,Ze,Sl,ql,je,zl,Pl,Ua,Ls,Wa,is,Ga,W,hs,sa,Ns,Ml,ea,Cl,Ka,ds,Ll,aa,Nl,Ol,Qa,cs,Fl,ta,Rl,Yl,Xa,Os,Za,G,fs,la,Fs,Hl,na,Vl,st,xe,Bl,et,we,ms,$e,Jl,Ul,ra,Wl,Gl,at,Rs,tt,ke,us,ye,Kl,Ql,pa,Xl,Zl,lt,Ys,nt,_s,rt,Ee,pt,K,gs,oa,Hs,sn,ia,en,ot,Q,De,an,tn,Ae,ln,nn,it,Vs,ht,bs,rn,ha,pn,on,dt,Bs,ct,H,hn,da,dn,cn,ca,fn,mn,ft,X,vs,fa,Js,un,ma,_n,mt,js,gn,Ie,bn,vn,ut,Us,_t,Z,xs,ua,Ws,jn,_a,xn,gt,I,wn,Te,$n,kn,Se,yn,En,qe,Dn,An,ze,In,Tn,Pe,Sn,qn,bt,Me,zn,vt,V,Pn,Ce,Mn,Cn,ga,Ln,Nn,jt,Gs,xt,ws,On,Le,Fn,Rn,wt,Ks,$t,$s,Yn,Ne,Hn,Vn,kt,B,Bn,ba,Jn,Un,Oe,Wn,Gn,yt,Qs,Et,ss,ks,va,Xs,Kn,ja,Qn,Dt,O,Fe,Xn,Zn,xa,sr,er,wa,ar,tr,At,es,ys,$a,Zs,lr,ka,nr,It,se,Tt,Es,St,as,Ds,ya,ee,rr,Ea,pr,qt,As,or,Re,ir,hr,zt,ae,Pt,ts,Ye,dr,cr,Da,fr,mr,Mt,te,Ct,ls,Is,Aa,le,ur,Ia,_r,Lt,ne,He,gr,br,Nt,Ve,Ts,vr,Ta,jr,xr,re,Ot;return u=new F({}),Ms=new S({props:{code:`from datasets import load_dataset
dataset = load_dataset('oscar', "unshuffled_deduplicated_en", split='train', streaming=True)
print(next(iter(dataset)))`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;oscar&#x27;</span>, <span class="hljs-string">&quot;unshuffled_deduplicated_en&quot;</span>, split=<span class="hljs-string">&#x27;train&#x27;</span>, streaming=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(<span class="hljs-built_in">next</span>(<span class="hljs-built_in">iter</span>(dataset)))
{<span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;Mtendere Village was inspired by the vision of Chief Napoleon Dzombe, which he shared with John Blanchard during his first visit to Malawi. Chief Napoleon conveyed the desperate need for a program to intervene and care for the orphans and vulnerable children (OVC) in Malawi, and John committed to help...</span>`}}),ps=new nl({props:{$$slots:{default:[ao]},$$scope:{ctx:N}}}),Cs=new F({}),Ls=new S({props:{code:`from datasets import load_dataset
dataset = load_dataset('oscar', "unshuffled_deduplicated_en", split='train', streaming=True)
shuffled_dataset = dataset.shuffle(seed=42, buffer_size=10_000)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;oscar&#x27;</span>, <span class="hljs-string">&quot;unshuffled_deduplicated_en&quot;</span>, split=<span class="hljs-string">&#x27;train&#x27;</span>, streaming=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>shuffled_dataset = dataset.shuffle(seed=<span class="hljs-number">42</span>, buffer_size=<span class="hljs-number">10_000</span>)`}}),is=new nl({props:{$$slots:{default:[to]},$$scope:{ctx:N}}}),Ns=new F({}),Os=new S({props:{code:`for epoch in range(epochs):
    shuffled_dataset.set_epoch(epoch)
    for example in shuffled_dataset:
        ...`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(epochs):
<span class="hljs-meta">... </span>    shuffled_dataset.set_epoch(epoch)
<span class="hljs-meta">... </span>    <span class="hljs-keyword">for</span> example <span class="hljs-keyword">in</span> shuffled_dataset:
<span class="hljs-meta">... </span>        ...`}}),Fs=new F({}),Rs=new S({props:{code:`dataset = load_dataset('oscar', "unshuffled_deduplicated_en", split='train', streaming=True)
dataset_head = dataset.take(2)
list(dataset_head)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;oscar&#x27;</span>, <span class="hljs-string">&quot;unshuffled_deduplicated_en&quot;</span>, split=<span class="hljs-string">&#x27;train&#x27;</span>, streaming=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset_head = dataset.take(<span class="hljs-number">2</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">list</span>(dataset_head)
[{<span class="hljs-string">&#x27;id&#x27;</span>: <span class="hljs-number">0</span>, <span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;Mtendere Village was...&#x27;</span>}, {<span class="hljs-string">&#x27;id&#x27;</span>: <span class="hljs-number">1</span>, <span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;Lily James cannot fight the music...&#x27;</span>}]`}}),Ys=new S({props:{code:"train_dataset = shuffled_dataset.skip(1000)",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>train_dataset = shuffled_dataset.skip(<span class="hljs-number">1000</span>)'}}),_s=new nl({props:{warning:"&lcub;true}",$$slots:{default:[lo]},$$scope:{ctx:N}}}),Hs=new F({}),Vs=new S({props:{code:`from datasets import interleave_datasets
from itertools import islice
en_dataset = load_dataset('oscar', "unshuffled_deduplicated_en", split='train', streaming=True)
fr_dataset = load_dataset('oscar', "unshuffled_deduplicated_fr", split='train', streaming=True)

multilingual_dataset = interleave_datasets([en_dataset, fr_dataset])
print(list(islice(multilingual_dataset, 2)))`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> interleave_datasets
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> itertools <span class="hljs-keyword">import</span> islice
<span class="hljs-meta">&gt;&gt;&gt; </span>en_dataset = load_dataset(<span class="hljs-string">&#x27;oscar&#x27;</span>, <span class="hljs-string">&quot;unshuffled_deduplicated_en&quot;</span>, split=<span class="hljs-string">&#x27;train&#x27;</span>, streaming=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>fr_dataset = load_dataset(<span class="hljs-string">&#x27;oscar&#x27;</span>, <span class="hljs-string">&quot;unshuffled_deduplicated_fr&quot;</span>, split=<span class="hljs-string">&#x27;train&#x27;</span>, streaming=<span class="hljs-literal">True</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>multilingual_dataset = interleave_datasets([en_dataset, fr_dataset])
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(<span class="hljs-built_in">list</span>(islice(multilingual_dataset, <span class="hljs-number">2</span>)))
[{<span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;Mtendere Village was inspired by the vision...&#x27;</span>}, {<span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&quot;M\xE9dia de d\xE9bat d&#x27;id\xE9es, de culture et de litt\xE9rature...&quot;</span>}]`}}),Bs=new S({props:{code:`multilingual_dataset_with_oversampling = interleave_datasets([en_dataset, fr_dataset], probabilities=[0.8, 0.2], seed=42)
print(list(islice(multilingual_dataset_with_oversampling, 2)))`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>multilingual_dataset_with_oversampling = interleave_datasets([en_dataset, fr_dataset], probabilities=[<span class="hljs-number">0.8</span>, <span class="hljs-number">0.2</span>], seed=<span class="hljs-number">42</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(<span class="hljs-built_in">list</span>(islice(multilingual_dataset_with_oversampling, <span class="hljs-number">2</span>)))
[{<span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;Mtendere Village was inspired by the vision...&#x27;</span>}, {<span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;Lily James cannot fight the music...&#x27;</span>}]`}}),Js=new F({}),Us=new S({props:{code:`from datasets import load_dataset
dataset = load_dataset('mc4', 'en', streaming=True, split='train')
dataset = dataset.remove_columns('timestamp')`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;mc4&#x27;</span>, <span class="hljs-string">&#x27;en&#x27;</span>, streaming=<span class="hljs-literal">True</span>, split=<span class="hljs-string">&#x27;train&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = dataset.remove_columns(<span class="hljs-string">&#x27;timestamp&#x27;</span>)`}}),Ws=new F({}),Gs=new S({props:{code:`def add_prefix(example):
    example['text'] = 'My text: ' + example['text']
    return example`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">def</span> <span class="hljs-title function_">add_prefix</span>(<span class="hljs-params">example</span>):
<span class="hljs-meta">... </span>    example[<span class="hljs-string">&#x27;text&#x27;</span>] = <span class="hljs-string">&#x27;My text: &#x27;</span> + example[<span class="hljs-string">&#x27;text&#x27;</span>]
<span class="hljs-meta">... </span>    <span class="hljs-keyword">return</span> example`}}),Ks=new S({props:{code:`from datasets import load_dataset
dataset = load_dataset('oscar', 'unshuffled_deduplicated_en', streaming=True, split='train')
updated_dataset = dataset.map(add_prefix)
list(updated_dataset.take(3))`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;oscar&#x27;</span>, <span class="hljs-string">&#x27;unshuffled_deduplicated_en&#x27;</span>, streaming=<span class="hljs-literal">True</span>, split=<span class="hljs-string">&#x27;train&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>updated_dataset = dataset.<span class="hljs-built_in">map</span>(add_prefix)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">list</span>(updated_dataset.take(<span class="hljs-number">3</span>))
[{<span class="hljs-string">&#x27;id&#x27;</span>: <span class="hljs-number">0</span>, <span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;My text: Mtendere Village was inspired by...&#x27;</span>},
 {<span class="hljs-string">&#x27;id&#x27;</span>: <span class="hljs-number">1</span>, <span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;My text: Lily James cannot fight the music...&#x27;</span>},
 {<span class="hljs-string">&#x27;id&#x27;</span>: <span class="hljs-number">2</span>, <span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;My text: &quot;I\\&#x27;d love to help kickstart...&#x27;</span>}]`}}),Qs=new S({props:{code:`updated_dataset = dataset.map(add_prefix, remove_columns=["id"])
list(updated_dataset.take(3))`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>updated_dataset = dataset.<span class="hljs-built_in">map</span>(add_prefix, remove_columns=[<span class="hljs-string">&quot;id&quot;</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">list</span>(updated_dataset.take(<span class="hljs-number">3</span>))
[{<span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;My text: Mtendere Village was inspired by...&#x27;</span>},
 {<span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;My text: Lily James cannot fight the music...&#x27;</span>},
 {<span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;My text: &quot;I\\&#x27;d love to help kickstart...&#x27;</span>}]`}}),Xs=new F({}),Zs=new F({}),se=new S({props:{code:`from datasets import load_dataset
from transformers import AutoTokenizer
dataset = load_dataset("mc4", "en", streaming=True, split="train")
tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')
def encode(examples):
    return tokenizer(examples['text'], truncation=True, padding='max_length')
dataset = dataset.map(encode, batched=True, remove_columns=["text", "timestamp", "url"])
next(iter(dataset))`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;mc4&quot;</span>, <span class="hljs-string">&quot;en&quot;</span>, streaming=<span class="hljs-literal">True</span>, split=<span class="hljs-string">&quot;train&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&#x27;distilbert-base-uncased&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">def</span> <span class="hljs-title function_">encode</span>(<span class="hljs-params">examples</span>):
<span class="hljs-meta">... </span>    <span class="hljs-keyword">return</span> tokenizer(examples[<span class="hljs-string">&#x27;text&#x27;</span>], truncation=<span class="hljs-literal">True</span>, padding=<span class="hljs-string">&#x27;max_length&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = dataset.<span class="hljs-built_in">map</span>(encode, batched=<span class="hljs-literal">True</span>, remove_columns=[<span class="hljs-string">&quot;text&quot;</span>, <span class="hljs-string">&quot;timestamp&quot;</span>, <span class="hljs-string">&quot;url&quot;</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">next</span>(<span class="hljs-built_in">iter</span>(dataset))
{<span class="hljs-string">&#x27;input_ids&#x27;</span>: <span class="hljs-number">101</span>, <span class="hljs-number">8466</span>, <span class="hljs-number">1018</span>, <span class="hljs-number">1010</span>, <span class="hljs-number">4029</span>, <span class="hljs-number">2475</span>, <span class="hljs-number">2062</span>, <span class="hljs-number">18558</span>, <span class="hljs-number">3100</span>, <span class="hljs-number">2061</span>, ...,<span class="hljs-number">1106</span>, <span class="hljs-number">3739</span>, <span class="hljs-number">102</span>],
<span class="hljs-string">&#x27;attention_mask&#x27;</span>: [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, ..., <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]}`}}),Es=new nl({props:{$$slots:{default:[no]},$$scope:{ctx:N}}}),ee=new F({}),ae=new S({props:{code:`from datasets import load_dataset
dataset = load_dataset('oscar', 'unshuffled_deduplicated_en', streaming=True, split='train')
start_with_ar = dataset.filter(lambda example: example['text'].startswith('Ar'))
next(iter(start_with_ar))`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;oscar&#x27;</span>, <span class="hljs-string">&#x27;unshuffled_deduplicated_en&#x27;</span>, streaming=<span class="hljs-literal">True</span>, split=<span class="hljs-string">&#x27;train&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>start_with_ar = dataset.<span class="hljs-built_in">filter</span>(<span class="hljs-keyword">lambda</span> example: example[<span class="hljs-string">&#x27;text&#x27;</span>].startswith(<span class="hljs-string">&#x27;Ar&#x27;</span>))
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">next</span>(<span class="hljs-built_in">iter</span>(start_with_ar))
{<span class="hljs-string">&#x27;id&#x27;</span>: <span class="hljs-number">4</span>, <span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;Are you looking for Number the Stars (Essential Modern Classics)?...&#x27;</span>}`}}),te=new S({props:{code:`even_dataset = dataset.filter(lambda example, idx: idx % 2 == 0, with_indices=True)
list(even_dataset.take(3))`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>even_dataset = dataset.<span class="hljs-built_in">filter</span>(<span class="hljs-keyword">lambda</span> example, idx: idx % <span class="hljs-number">2</span> == <span class="hljs-number">0</span>, with_indices=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">list</span>(even_dataset.take(<span class="hljs-number">3</span>))
[{<span class="hljs-string">&#x27;id&#x27;</span>: <span class="hljs-number">0</span>, <span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;Mtendere Village was inspired by the vision of Chief Napoleon Dzombe, ...&#x27;</span>},
 {<span class="hljs-string">&#x27;id&#x27;</span>: <span class="hljs-number">2</span>, <span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;&quot;I\\&#x27;d love to help kickstart continued development! And 0 EUR/month...&#x27;</span>},
 {<span class="hljs-string">&#x27;id&#x27;</span>: <span class="hljs-number">4</span>, <span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;Are you looking for Number the Stars (Essential Modern Classics)? Normally, ...&#x27;</span>}]`}}),le=new F({}),re=new S({props:{code:`import torch
from torch.utils.data import DataLoader
from transformers import AutoModelForMaskedLM, DataCollatorForLanguageModeling
from tqdm import tqdm
dataset = dataset.with_format("torch")
dataloader = DataLoader(dataset, collate_fn=DataCollatorForLanguageModeling(tokenizer))
device = 'cuda' if torch.cuda.is_available() else 'cpu' 
model = AutoModelForMaskedLM.from_pretrained("distilbert-base-uncased")
model.train().to(device)
optimizer = torch.optim.AdamW(params=model.parameters(), lr=1e-5)
for epoch in range(3):
    dataset.set_epoch(epoch)
    for i, batch in enumerate(tqdm(dataloader, total=5)):
        if i == 5:
            break
        batch = {k: v.to(device) for k, v in batch.items()}
        outputs = model(**batch)
        loss = outputs[0]
        loss.backward()
        optimizer.step()
        optimizer.zero_grad()
        if i % 10 == 0:
            print(f"loss: {loss}")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> DataLoader
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForMaskedLM, DataCollatorForLanguageModeling
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> tqdm <span class="hljs-keyword">import</span> tqdm
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = dataset.with_format(<span class="hljs-string">&quot;torch&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>dataloader = DataLoader(dataset, collate_fn=DataCollatorForLanguageModeling(tokenizer))
<span class="hljs-meta">&gt;&gt;&gt; </span>device = <span class="hljs-string">&#x27;cuda&#x27;</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">&#x27;cpu&#x27;</span> 
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMaskedLM.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.train().to(device)
<span class="hljs-meta">&gt;&gt;&gt; </span>optimizer = torch.optim.AdamW(params=model.parameters(), lr=<span class="hljs-number">1e-5</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">3</span>):
<span class="hljs-meta">... </span>    dataset.set_epoch(epoch)
<span class="hljs-meta">... </span>    <span class="hljs-keyword">for</span> i, batch <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(tqdm(dataloader, total=<span class="hljs-number">5</span>)):
<span class="hljs-meta">... </span>        <span class="hljs-keyword">if</span> i == <span class="hljs-number">5</span>:
<span class="hljs-meta">... </span>            <span class="hljs-keyword">break</span>
<span class="hljs-meta">... </span>        batch = {k: v.to(device) <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> batch.items()}
<span class="hljs-meta">... </span>        outputs = model(**batch)
<span class="hljs-meta">... </span>        loss = outputs[<span class="hljs-number">0</span>]
<span class="hljs-meta">... </span>        loss.backward()
<span class="hljs-meta">... </span>        optimizer.step()
<span class="hljs-meta">... </span>        optimizer.zero_grad()
<span class="hljs-meta">... </span>        <span class="hljs-keyword">if</span> i % <span class="hljs-number">10</span> == <span class="hljs-number">0</span>:
<span class="hljs-meta">... </span>            <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;loss: <span class="hljs-subst">{loss}</span>&quot;</span>)`}}),{c(){f=t("meta"),w=d(),m=t("h1"),$=t("a"),k=t("span"),_(u.$$.fragment),y=d(),E=t("span"),z=n("Stream"),D=d(),T=t("p"),R=n("Dataset streaming lets you get started with a dataset without waiting for the entire dataset to download. The data is downloaded progressively as you iterate over the dataset. This is especially helpful when:"),ns=d(),q=t("ul"),A=t("li"),ie=n("You don\u2019t want to wait for an extremely large dataset to download."),he=d(),rs=t("li"),de=n("The dataset size exceeds the amount of disk space on your computer."),Oa=d(),J=t("div"),ce=t("img"),rl=d(),fe=t("img"),Fa=d(),P=t("p"),pl=n("For example, the English split of the "),Ps=t("a"),ol=n("OSCAR"),il=n(" dataset is 1.2 terabytes, but you can use it instantly with streaming. Stream a dataset by setting "),Ge=t("code"),hl=n("streaming=True"),dl=n(" in "),me=t("a"),cl=n("datasets.load_dataset()"),fl=n(" as shown below:"),Ra=d(),_(Ms.$$.fragment),Ya=d(),Y=t("p"),ml=n("Loading a dataset in streaming mode creates a new dataset type instance (instead of the classic "),ue=t("a"),ul=n("datasets.Dataset"),_l=n(" object), known as an "),_e=t("a"),gl=n("datasets.IterableDataset"),bl=n(". This special type of dataset has its own set of processing methods shown below."),Ha=d(),_(ps.$$.fragment),Va=d(),U=t("h2"),os=t("a"),Ke=t("span"),_(Cs.$$.fragment),vl=d(),Qe=t("span"),jl=n("Shuffle"),Ba=d(),M=t("p"),xl=n("Like a regular "),ge=t("a"),wl=n("datasets.Dataset"),$l=n(" object, you can also shuffle a "),be=t("a"),kl=n("datasets.IterableDataset"),yl=n(" with "),ve=t("a"),El=n("datasets.IterableDataset.shuffle()"),Dl=n("."),Ja=d(),C=t("p"),Al=n("The "),Xe=t("code"),Il=n("buffer_size"),Tl=n(" argument controls the size of the buffer to randomly sample examples from. Let\u2019s say your dataset has one million examples, and you set the "),Ze=t("code"),Sl=n("buffer_size"),ql=n(" to ten thousand. "),je=t("a"),zl=n("datasets.IterableDataset.shuffle()"),Pl=n(" will randomly select examples from the first ten thousand examples in the buffer. Selected examples in the buffer are replaced with new examples. By default, the buffer size is 1,000."),Ua=d(),_(Ls.$$.fragment),Wa=d(),_(is.$$.fragment),Ga=d(),W=t("h2"),hs=t("a"),sa=t("span"),_(Ns.$$.fragment),Ml=d(),ea=t("span"),Cl=n("Reshuffle"),Ka=d(),ds=t("p"),Ll=n("Sometimes you may want to reshuffle the dataset after each epoch. This will require you to set a different seed for each epoch. Use "),aa=t("code"),Nl=n("datasets.IterableDataset.set_epoch()"),Ol=n("in between epochs to tell the dataset what epoch you\u2019re on."),Qa=d(),cs=t("p"),Fl=n("Your seed effectively becomes: "),ta=t("code"),Rl=n("initial seed + current epoch"),Yl=n("."),Xa=d(),_(Os.$$.fragment),Za=d(),G=t("h2"),fs=t("a"),la=t("span"),_(Fs.$$.fragment),Hl=d(),na=t("span"),Vl=n("Split dataset"),st=d(),xe=t("p"),Bl=n("You can split your dataset one of two ways:"),et=d(),we=t("ul"),ms=t("li"),$e=t("a"),Jl=n("datasets.IterableDataset.take()"),Ul=n(" returns the first "),ra=t("code"),Wl=n("n"),Gl=n(" examples in a dataset:"),at=d(),_(Rs.$$.fragment),tt=d(),ke=t("ul"),us=t("li"),ye=t("a"),Kl=n("datasets.IterableDataset.skip()"),Ql=n(" omits the first "),pa=t("code"),Xl=n("n"),Zl=n(" examples in a dataset and returns the remaining examples:"),lt=d(),_(Ys.$$.fragment),nt=d(),_(_s.$$.fragment),rt=d(),Ee=t("a"),pt=d(),K=t("h2"),gs=t("a"),oa=t("span"),_(Hs.$$.fragment),sn=d(),ia=t("span"),en=n("Interleave"),ot=d(),Q=t("p"),De=t("a"),an=n("datasets.interleave_datasets()"),tn=n(" can combine an "),Ae=t("a"),ln=n("datasets.IterableDataset"),nn=n(" with other datasets. The combined dataset returns alternating examples from each of the original datasets."),it=d(),_(Vs.$$.fragment),ht=d(),bs=t("p"),rn=n("Define sampling probabilities from each of the original datasets for more control over how each of them are sampled and combined. Set the "),ha=t("code"),pn=n("probabilities"),on=n(" argument with your desired sampling probabilities:"),dt=d(),_(Bs.$$.fragment),ct=d(),H=t("p"),hn=n("Around 80% of the final dataset is made of the "),da=t("code"),dn=n("en_dataset"),cn=n(", and 20% of the "),ca=t("code"),fn=n("fr_dataset"),mn=n("."),ft=d(),X=t("h2"),vs=t("a"),fa=t("span"),_(Js.$$.fragment),un=d(),ma=t("span"),_n=n("Remove"),mt=d(),js=t("p"),gn=n("Remove columns on-the-fly with "),Ie=t("a"),bn=n("datasets.IterableDataset.remove_columns()"),vn=n(". Specify the name of the column to remove:"),ut=d(),_(Us.$$.fragment),_t=d(),Z=t("h2"),xs=t("a"),ua=t("span"),_(Ws.$$.fragment),jn=d(),_a=t("span"),xn=n("Map"),gt=d(),I=t("p"),wn=n("Similar to the "),Te=t("a"),$n=n("datasets.Dataset.map()"),kn=n(" function for a regular "),Se=t("a"),yn=n("datasets.Dataset"),En=n(", \u{1F917}  Datasets features "),qe=t("a"),Dn=n("datasets.IterableDataset.map()"),An=n(" for processing "),ze=t("a"),In=n("datasets.IterableDataset"),Tn=n(`\\s.
`),Pe=t("a"),Sn=n("datasets.IterableDataset.map()"),qn=n(" applies processing on-the-fly when examples are streamed."),bt=d(),Me=t("p"),zn=n("It allows you to apply a processing function to each example in a dataset, independently or in batches. This function can even create new rows and columns."),vt=d(),V=t("p"),Pn=n("The following example demonstrates how to tokenize a "),Ce=t("a"),Mn=n("datasets.IterableDataset"),Cn=n(". The function needs to accept and output a "),ga=t("code"),Ln=n("dict"),Nn=n(":"),jt=d(),_(Gs.$$.fragment),xt=d(),ws=t("p"),On=n("Next, apply this function to the dataset with "),Le=t("a"),Fn=n("datasets.IterableDataset.map()"),Rn=n(":"),wt=d(),_(Ks.$$.fragment),$t=d(),$s=t("p"),Yn=n("Let\u2019s take a look at another example, except this time, you will remove a column with "),Ne=t("a"),Hn=n("datasets.IterableDataset.map()"),Vn=n(". When you remove a column, it is only removed after the example has been provided to the mapped function. This allows the mapped function to use the content of the columns before they are removed."),kt=d(),B=t("p"),Bn=n("Specify the column to remove with the "),ba=t("code"),Jn=n("remove_columns"),Un=n(" argument in "),Oe=t("a"),Wn=n("datasets.IterableDataset.map()"),Gn=n(":"),yt=d(),_(Qs.$$.fragment),Et=d(),ss=t("h3"),ks=t("a"),va=t("span"),_(Xs.$$.fragment),Kn=d(),ja=t("span"),Qn=n("Batch processing"),Dt=d(),O=t("p"),Fe=t("a"),Xn=n("datasets.IterableDataset.map()"),Zn=n(" also supports working with batches of examples. Operate on batches by setting "),xa=t("code"),sr=n("batched=True"),er=n(". The default batch size is 1000, but you can adjust it with the "),wa=t("code"),ar=n("batch_size"),tr=n(" argument. This opens the door to many interesting applications such as tokenization, splitting long sentences into shorter chunks, and data augmentation."),At=d(),es=t("h4"),ys=t("a"),$a=t("span"),_(Zs.$$.fragment),lr=d(),ka=t("span"),nr=n("Tokenization"),It=d(),_(se.$$.fragment),Tt=d(),_(Es.$$.fragment),St=d(),as=t("h3"),Ds=t("a"),ya=t("span"),_(ee.$$.fragment),rr=d(),Ea=t("span"),pr=n("Filter"),qt=d(),As=t("p"),or=n("You can filter rows in the dataset based on a predicate function using "),Re=t("a"),ir=n("datasets.Dataset.filter()"),hr=n(". It returns rows that match a specified condition:"),zt=d(),_(ae.$$.fragment),Pt=d(),ts=t("p"),Ye=t("a"),dr=n("datasets.Dataset.filter()"),cr=n(" can also filter by indices if you set "),Da=t("code"),fr=n("with_indices=True"),mr=n(":"),Mt=d(),_(te.$$.fragment),Ct=d(),ls=t("h2"),Is=t("a"),Aa=t("span"),_(le.$$.fragment),ur=d(),Ia=t("span"),_r=n("Stream in a training loop"),Lt=d(),ne=t("p"),He=t("a"),gr=n("datasets.IterableDataset"),br=n(" can be integrated into a training loop. First, shuffle the dataset:"),Nt=d(),Ve=t("frameworkcontent"),Ts=t("pt"),vr=n("```py\n>>> buffer_size, seed = 10_000, 42\n>>> dataset = dataset.shuffle(buffer_size, seed)\n```\n"),Ta=t("p"),jr=n("Lastly, create a simple training loop and start training:"),xr=d(),_(re.$$.fragment),this.h()},l(s){const o=eo('[data-svelte="svelte-1phssyn"]',document.head);f=l(o,"META",{name:!0,content:!0}),o.forEach(a),w=c(s),m=l(s,"H1",{class:!0});var pe=p(m);$=l(pe,"A",{id:!0,class:!0,href:!0});var Sa=p($);k=l(Sa,"SPAN",{});var qa=p(k);g(u.$$.fragment,qa),qa.forEach(a),Sa.forEach(a),y=c(pe),E=l(pe,"SPAN",{});var za=p(E);z=r(za,"Stream"),za.forEach(a),pe.forEach(a),D=c(s),T=l(s,"P",{});var yr=p(T);R=r(yr,"Dataset streaming lets you get started with a dataset without waiting for the entire dataset to download. The data is downloaded progressively as you iterate over the dataset. This is especially helpful when:"),yr.forEach(a),ns=c(s),q=l(s,"UL",{});var Ft=p(q);A=l(Ft,"LI",{});var Er=p(A);ie=r(Er,"You don\u2019t want to wait for an extremely large dataset to download."),Er.forEach(a),he=c(Ft),rs=l(Ft,"LI",{});var Dr=p(rs);de=r(Dr,"The dataset size exceeds the amount of disk space on your computer."),Dr.forEach(a),Ft.forEach(a),Oa=c(s),J=l(s,"DIV",{class:!0});var Rt=p(J);ce=l(Rt,"IMG",{class:!0,src:!0}),rl=c(Rt),fe=l(Rt,"IMG",{class:!0,src:!0}),Rt.forEach(a),Fa=c(s),P=l(s,"P",{});var Ss=p(P);pl=r(Ss,"For example, the English split of the "),Ps=l(Ss,"A",{href:!0,rel:!0});var Ar=p(Ps);ol=r(Ar,"OSCAR"),Ar.forEach(a),il=r(Ss," dataset is 1.2 terabytes, but you can use it instantly with streaming. Stream a dataset by setting "),Ge=l(Ss,"CODE",{});var Ir=p(Ge);hl=r(Ir,"streaming=True"),Ir.forEach(a),dl=r(Ss," in "),me=l(Ss,"A",{href:!0});var Tr=p(me);cl=r(Tr,"datasets.load_dataset()"),Tr.forEach(a),fl=r(Ss," as shown below:"),Ss.forEach(a),Ra=c(s),g(Ms.$$.fragment,s),Ya=c(s),Y=l(s,"P",{});var Be=p(Y);ml=r(Be,"Loading a dataset in streaming mode creates a new dataset type instance (instead of the classic "),ue=l(Be,"A",{href:!0});var Sr=p(ue);ul=r(Sr,"datasets.Dataset"),Sr.forEach(a),_l=r(Be," object), known as an "),_e=l(Be,"A",{href:!0});var qr=p(_e);gl=r(qr,"datasets.IterableDataset"),qr.forEach(a),bl=r(Be,". This special type of dataset has its own set of processing methods shown below."),Be.forEach(a),Ha=c(s),g(ps.$$.fragment,s),Va=c(s),U=l(s,"H2",{class:!0});var Yt=p(U);os=l(Yt,"A",{id:!0,class:!0,href:!0});var zr=p(os);Ke=l(zr,"SPAN",{});var Pr=p(Ke);g(Cs.$$.fragment,Pr),Pr.forEach(a),zr.forEach(a),vl=c(Yt),Qe=l(Yt,"SPAN",{});var Mr=p(Qe);jl=r(Mr,"Shuffle"),Mr.forEach(a),Yt.forEach(a),Ba=c(s),M=l(s,"P",{});var qs=p(M);xl=r(qs,"Like a regular "),ge=l(qs,"A",{href:!0});var Cr=p(ge);wl=r(Cr,"datasets.Dataset"),Cr.forEach(a),$l=r(qs," object, you can also shuffle a "),be=l(qs,"A",{href:!0});var Lr=p(be);kl=r(Lr,"datasets.IterableDataset"),Lr.forEach(a),yl=r(qs," with "),ve=l(qs,"A",{href:!0});var Nr=p(ve);El=r(Nr,"datasets.IterableDataset.shuffle()"),Nr.forEach(a),Dl=r(qs,"."),qs.forEach(a),Ja=c(s),C=l(s,"P",{});var zs=p(C);Al=r(zs,"The "),Xe=l(zs,"CODE",{});var Or=p(Xe);Il=r(Or,"buffer_size"),Or.forEach(a),Tl=r(zs," argument controls the size of the buffer to randomly sample examples from. Let\u2019s say your dataset has one million examples, and you set the "),Ze=l(zs,"CODE",{});var Fr=p(Ze);Sl=r(Fr,"buffer_size"),Fr.forEach(a),ql=r(zs," to ten thousand. "),je=l(zs,"A",{href:!0});var Rr=p(je);zl=r(Rr,"datasets.IterableDataset.shuffle()"),Rr.forEach(a),Pl=r(zs," will randomly select examples from the first ten thousand examples in the buffer. Selected examples in the buffer are replaced with new examples. By default, the buffer size is 1,000."),zs.forEach(a),Ua=c(s),g(Ls.$$.fragment,s),Wa=c(s),g(is.$$.fragment,s),Ga=c(s),W=l(s,"H2",{class:!0});var Ht=p(W);hs=l(Ht,"A",{id:!0,class:!0,href:!0});var Yr=p(hs);sa=l(Yr,"SPAN",{});var Hr=p(sa);g(Ns.$$.fragment,Hr),Hr.forEach(a),Yr.forEach(a),Ml=c(Ht),ea=l(Ht,"SPAN",{});var Vr=p(ea);Cl=r(Vr,"Reshuffle"),Vr.forEach(a),Ht.forEach(a),Ka=c(s),ds=l(s,"P",{});var Vt=p(ds);Ll=r(Vt,"Sometimes you may want to reshuffle the dataset after each epoch. This will require you to set a different seed for each epoch. Use "),aa=l(Vt,"CODE",{});var Br=p(aa);Nl=r(Br,"datasets.IterableDataset.set_epoch()"),Br.forEach(a),Ol=r(Vt,"in between epochs to tell the dataset what epoch you\u2019re on."),Vt.forEach(a),Qa=c(s),cs=l(s,"P",{});var Bt=p(cs);Fl=r(Bt,"Your seed effectively becomes: "),ta=l(Bt,"CODE",{});var Jr=p(ta);Rl=r(Jr,"initial seed + current epoch"),Jr.forEach(a),Yl=r(Bt,"."),Bt.forEach(a),Xa=c(s),g(Os.$$.fragment,s),Za=c(s),G=l(s,"H2",{class:!0});var Jt=p(G);fs=l(Jt,"A",{id:!0,class:!0,href:!0});var Ur=p(fs);la=l(Ur,"SPAN",{});var Wr=p(la);g(Fs.$$.fragment,Wr),Wr.forEach(a),Ur.forEach(a),Hl=c(Jt),na=l(Jt,"SPAN",{});var Gr=p(na);Vl=r(Gr,"Split dataset"),Gr.forEach(a),Jt.forEach(a),st=c(s),xe=l(s,"P",{});var Kr=p(xe);Bl=r(Kr,"You can split your dataset one of two ways:"),Kr.forEach(a),et=c(s),we=l(s,"UL",{});var Qr=p(we);ms=l(Qr,"LI",{});var Pa=p(ms);$e=l(Pa,"A",{href:!0});var Xr=p($e);Jl=r(Xr,"datasets.IterableDataset.take()"),Xr.forEach(a),Ul=r(Pa," returns the first "),ra=l(Pa,"CODE",{});var Zr=p(ra);Wl=r(Zr,"n"),Zr.forEach(a),Gl=r(Pa," examples in a dataset:"),Pa.forEach(a),Qr.forEach(a),at=c(s),g(Rs.$$.fragment,s),tt=c(s),ke=l(s,"UL",{});var sp=p(ke);us=l(sp,"LI",{});var Ma=p(us);ye=l(Ma,"A",{href:!0});var ep=p(ye);Kl=r(ep,"datasets.IterableDataset.skip()"),ep.forEach(a),Ql=r(Ma," omits the first "),pa=l(Ma,"CODE",{});var ap=p(pa);Xl=r(ap,"n"),ap.forEach(a),Zl=r(Ma," examples in a dataset and returns the remaining examples:"),Ma.forEach(a),sp.forEach(a),lt=c(s),g(Ys.$$.fragment,s),nt=c(s),g(_s.$$.fragment,s),rt=c(s),Ee=l(s,"A",{id:!0}),p(Ee).forEach(a),pt=c(s),K=l(s,"H2",{class:!0});var Ut=p(K);gs=l(Ut,"A",{id:!0,class:!0,href:!0});var tp=p(gs);oa=l(tp,"SPAN",{});var lp=p(oa);g(Hs.$$.fragment,lp),lp.forEach(a),tp.forEach(a),sn=c(Ut),ia=l(Ut,"SPAN",{});var np=p(ia);en=r(np,"Interleave"),np.forEach(a),Ut.forEach(a),ot=c(s),Q=l(s,"P",{});var Ca=p(Q);De=l(Ca,"A",{href:!0});var rp=p(De);an=r(rp,"datasets.interleave_datasets()"),rp.forEach(a),tn=r(Ca," can combine an "),Ae=l(Ca,"A",{href:!0});var pp=p(Ae);ln=r(pp,"datasets.IterableDataset"),pp.forEach(a),nn=r(Ca," with other datasets. The combined dataset returns alternating examples from each of the original datasets."),Ca.forEach(a),it=c(s),g(Vs.$$.fragment,s),ht=c(s),bs=l(s,"P",{});var Wt=p(bs);rn=r(Wt,"Define sampling probabilities from each of the original datasets for more control over how each of them are sampled and combined. Set the "),ha=l(Wt,"CODE",{});var op=p(ha);pn=r(op,"probabilities"),op.forEach(a),on=r(Wt," argument with your desired sampling probabilities:"),Wt.forEach(a),dt=c(s),g(Bs.$$.fragment,s),ct=c(s),H=l(s,"P",{});var Je=p(H);hn=r(Je,"Around 80% of the final dataset is made of the "),da=l(Je,"CODE",{});var ip=p(da);dn=r(ip,"en_dataset"),ip.forEach(a),cn=r(Je,", and 20% of the "),ca=l(Je,"CODE",{});var hp=p(ca);fn=r(hp,"fr_dataset"),hp.forEach(a),mn=r(Je,"."),Je.forEach(a),ft=c(s),X=l(s,"H2",{class:!0});var Gt=p(X);vs=l(Gt,"A",{id:!0,class:!0,href:!0});var dp=p(vs);fa=l(dp,"SPAN",{});var cp=p(fa);g(Js.$$.fragment,cp),cp.forEach(a),dp.forEach(a),un=c(Gt),ma=l(Gt,"SPAN",{});var fp=p(ma);_n=r(fp,"Remove"),fp.forEach(a),Gt.forEach(a),mt=c(s),js=l(s,"P",{});var Kt=p(js);gn=r(Kt,"Remove columns on-the-fly with "),Ie=l(Kt,"A",{href:!0});var mp=p(Ie);bn=r(mp,"datasets.IterableDataset.remove_columns()"),mp.forEach(a),vn=r(Kt,". Specify the name of the column to remove:"),Kt.forEach(a),ut=c(s),g(Us.$$.fragment,s),_t=c(s),Z=l(s,"H2",{class:!0});var Qt=p(Z);xs=l(Qt,"A",{id:!0,class:!0,href:!0});var up=p(xs);ua=l(up,"SPAN",{});var _p=p(ua);g(Ws.$$.fragment,_p),_p.forEach(a),up.forEach(a),jn=c(Qt),_a=l(Qt,"SPAN",{});var gp=p(_a);xn=r(gp,"Map"),gp.forEach(a),Qt.forEach(a),gt=c(s),I=l(s,"P",{});var L=p(I);wn=r(L,"Similar to the "),Te=l(L,"A",{href:!0});var bp=p(Te);$n=r(bp,"datasets.Dataset.map()"),bp.forEach(a),kn=r(L," function for a regular "),Se=l(L,"A",{href:!0});var vp=p(Se);yn=r(vp,"datasets.Dataset"),vp.forEach(a),En=r(L,", \u{1F917}  Datasets features "),qe=l(L,"A",{href:!0});var jp=p(qe);Dn=r(jp,"datasets.IterableDataset.map()"),jp.forEach(a),An=r(L," for processing "),ze=l(L,"A",{href:!0});var xp=p(ze);In=r(xp,"datasets.IterableDataset"),xp.forEach(a),Tn=r(L,`\\s.
`),Pe=l(L,"A",{href:!0});var wp=p(Pe);Sn=r(wp,"datasets.IterableDataset.map()"),wp.forEach(a),qn=r(L," applies processing on-the-fly when examples are streamed."),L.forEach(a),bt=c(s),Me=l(s,"P",{});var $p=p(Me);zn=r($p,"It allows you to apply a processing function to each example in a dataset, independently or in batches. This function can even create new rows and columns."),$p.forEach(a),vt=c(s),V=l(s,"P",{});var Ue=p(V);Pn=r(Ue,"The following example demonstrates how to tokenize a "),Ce=l(Ue,"A",{href:!0});var kp=p(Ce);Mn=r(kp,"datasets.IterableDataset"),kp.forEach(a),Cn=r(Ue,". The function needs to accept and output a "),ga=l(Ue,"CODE",{});var yp=p(ga);Ln=r(yp,"dict"),yp.forEach(a),Nn=r(Ue,":"),Ue.forEach(a),jt=c(s),g(Gs.$$.fragment,s),xt=c(s),ws=l(s,"P",{});var Xt=p(ws);On=r(Xt,"Next, apply this function to the dataset with "),Le=l(Xt,"A",{href:!0});var Ep=p(Le);Fn=r(Ep,"datasets.IterableDataset.map()"),Ep.forEach(a),Rn=r(Xt,":"),Xt.forEach(a),wt=c(s),g(Ks.$$.fragment,s),$t=c(s),$s=l(s,"P",{});var Zt=p($s);Yn=r(Zt,"Let\u2019s take a look at another example, except this time, you will remove a column with "),Ne=l(Zt,"A",{href:!0});var Dp=p(Ne);Hn=r(Dp,"datasets.IterableDataset.map()"),Dp.forEach(a),Vn=r(Zt,". When you remove a column, it is only removed after the example has been provided to the mapped function. This allows the mapped function to use the content of the columns before they are removed."),Zt.forEach(a),kt=c(s),B=l(s,"P",{});var We=p(B);Bn=r(We,"Specify the column to remove with the "),ba=l(We,"CODE",{});var Ap=p(ba);Jn=r(Ap,"remove_columns"),Ap.forEach(a),Un=r(We," argument in "),Oe=l(We,"A",{href:!0});var Ip=p(Oe);Wn=r(Ip,"datasets.IterableDataset.map()"),Ip.forEach(a),Gn=r(We,":"),We.forEach(a),yt=c(s),g(Qs.$$.fragment,s),Et=c(s),ss=l(s,"H3",{class:!0});var sl=p(ss);ks=l(sl,"A",{id:!0,class:!0,href:!0});var Tp=p(ks);va=l(Tp,"SPAN",{});var Sp=p(va);g(Xs.$$.fragment,Sp),Sp.forEach(a),Tp.forEach(a),Kn=c(sl),ja=l(sl,"SPAN",{});var qp=p(ja);Qn=r(qp,"Batch processing"),qp.forEach(a),sl.forEach(a),Dt=c(s),O=l(s,"P",{});var oe=p(O);Fe=l(oe,"A",{href:!0});var zp=p(Fe);Xn=r(zp,"datasets.IterableDataset.map()"),zp.forEach(a),Zn=r(oe," also supports working with batches of examples. Operate on batches by setting "),xa=l(oe,"CODE",{});var Pp=p(xa);sr=r(Pp,"batched=True"),Pp.forEach(a),er=r(oe,". The default batch size is 1000, but you can adjust it with the "),wa=l(oe,"CODE",{});var Mp=p(wa);ar=r(Mp,"batch_size"),Mp.forEach(a),tr=r(oe," argument. This opens the door to many interesting applications such as tokenization, splitting long sentences into shorter chunks, and data augmentation."),oe.forEach(a),At=c(s),es=l(s,"H4",{class:!0});var el=p(es);ys=l(el,"A",{id:!0,class:!0,href:!0});var Cp=p(ys);$a=l(Cp,"SPAN",{});var Lp=p($a);g(Zs.$$.fragment,Lp),Lp.forEach(a),Cp.forEach(a),lr=c(el),ka=l(el,"SPAN",{});var Np=p(ka);nr=r(Np,"Tokenization"),Np.forEach(a),el.forEach(a),It=c(s),g(se.$$.fragment,s),Tt=c(s),g(Es.$$.fragment,s),St=c(s),as=l(s,"H3",{class:!0});var al=p(as);Ds=l(al,"A",{id:!0,class:!0,href:!0});var Op=p(Ds);ya=l(Op,"SPAN",{});var Fp=p(ya);g(ee.$$.fragment,Fp),Fp.forEach(a),Op.forEach(a),rr=c(al),Ea=l(al,"SPAN",{});var Rp=p(Ea);pr=r(Rp,"Filter"),Rp.forEach(a),al.forEach(a),qt=c(s),As=l(s,"P",{});var tl=p(As);or=r(tl,"You can filter rows in the dataset based on a predicate function using "),Re=l(tl,"A",{href:!0});var Yp=p(Re);ir=r(Yp,"datasets.Dataset.filter()"),Yp.forEach(a),hr=r(tl,". It returns rows that match a specified condition:"),tl.forEach(a),zt=c(s),g(ae.$$.fragment,s),Pt=c(s),ts=l(s,"P",{});var La=p(ts);Ye=l(La,"A",{href:!0});var Hp=p(Ye);dr=r(Hp,"datasets.Dataset.filter()"),Hp.forEach(a),cr=r(La," can also filter by indices if you set "),Da=l(La,"CODE",{});var Vp=p(Da);fr=r(Vp,"with_indices=True"),Vp.forEach(a),mr=r(La,":"),La.forEach(a),Mt=c(s),g(te.$$.fragment,s),Ct=c(s),ls=l(s,"H2",{class:!0});var ll=p(ls);Is=l(ll,"A",{id:!0,class:!0,href:!0});var Bp=p(Is);Aa=l(Bp,"SPAN",{});var Jp=p(Aa);g(le.$$.fragment,Jp),Jp.forEach(a),Bp.forEach(a),ur=c(ll),Ia=l(ll,"SPAN",{});var Up=p(Ia);_r=r(Up,"Stream in a training loop"),Up.forEach(a),ll.forEach(a),Lt=c(s),ne=l(s,"P",{});var wr=p(ne);He=l(wr,"A",{href:!0});var Wp=p(He);gr=r(Wp,"datasets.IterableDataset"),Wp.forEach(a),br=r(wr," can be integrated into a training loop. First, shuffle the dataset:"),wr.forEach(a),Nt=c(s),Ve=l(s,"FRAMEWORKCONTENT",{});var Gp=p(Ve);Ts=l(Gp,"PT",{});var Na=p(Ts);vr=r(Na,"```py\n>>> buffer_size, seed = 10_000, 42\n>>> dataset = dataset.shuffle(buffer_size, seed)\n```\n"),Ta=l(Na,"P",{});var Kp=p(Ta);jr=r(Kp,"Lastly, create a simple training loop and start training:"),Kp.forEach(a),xr=c(Na),g(re.$$.fragment,Na),Na.forEach(a),Gp.forEach(a),this.h()},h(){h(f,"name","hf:doc:metadata"),h(f,"content",JSON.stringify(po)),h($,"id","stream"),h($,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h($,"href","#stream"),h(m,"class","relative group"),h(ce,"class","block dark:hidden"),Qp(ce.src,$r="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/datasets/streaming.gif")||h(ce,"src",$r),h(fe,"class","hidden dark:block"),Qp(fe.src,kr="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/datasets/streaming-dark.gif")||h(fe,"src",kr),h(J,"class","flex justify-center"),h(Ps,"href","https://huggingface.co/datasets/oscar"),h(Ps,"rel","nofollow"),h(me,"href","/docs/datasets/pr_3880/en/package_reference/loading_methods#datasets.load_dataset"),h(ue,"href","/docs/datasets/pr_3880/en/package_reference/main_classes#datasets.Dataset"),h(_e,"href","/docs/datasets/pr_3880/en/package_reference/main_classes#datasets.IterableDataset"),h(os,"id","shuffle"),h(os,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(os,"href","#shuffle"),h(U,"class","relative group"),h(ge,"href","/docs/datasets/pr_3880/en/package_reference/main_classes#datasets.Dataset"),h(be,"href","/docs/datasets/pr_3880/en/package_reference/main_classes#datasets.IterableDataset"),h(ve,"href","/docs/datasets/pr_3880/en/package_reference/main_classes#datasets.IterableDataset.shuffle"),h(je,"href","/docs/datasets/pr_3880/en/package_reference/main_classes#datasets.IterableDataset.shuffle"),h(hs,"id","reshuffle"),h(hs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(hs,"href","#reshuffle"),h(W,"class","relative group"),h(fs,"id","split-dataset"),h(fs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(fs,"href","#split-dataset"),h(G,"class","relative group"),h($e,"href","/docs/datasets/pr_3880/en/package_reference/main_classes#datasets.IterableDataset.take"),h(ye,"href","/docs/datasets/pr_3880/en/package_reference/main_classes#datasets.IterableDataset.skip"),h(Ee,"id","interleave_datasets"),h(gs,"id","interleave"),h(gs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(gs,"href","#interleave"),h(K,"class","relative group"),h(De,"href","/docs/datasets/pr_3880/en/package_reference/main_classes#datasets.interleave_datasets"),h(Ae,"href","/docs/datasets/pr_3880/en/package_reference/main_classes#datasets.IterableDataset"),h(vs,"id","remove"),h(vs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(vs,"href","#remove"),h(X,"class","relative group"),h(Ie,"href","/docs/datasets/pr_3880/en/package_reference/main_classes#datasets.IterableDataset.remove_columns"),h(xs,"id","map"),h(xs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(xs,"href","#map"),h(Z,"class","relative group"),h(Te,"href","/docs/datasets/pr_3880/en/package_reference/main_classes#datasets.Dataset.map"),h(Se,"href","/docs/datasets/pr_3880/en/package_reference/main_classes#datasets.Dataset"),h(qe,"href","/docs/datasets/pr_3880/en/package_reference/main_classes#datasets.IterableDataset.map"),h(ze,"href","/docs/datasets/pr_3880/en/package_reference/main_classes#datasets.IterableDataset"),h(Pe,"href","/docs/datasets/pr_3880/en/package_reference/main_classes#datasets.IterableDataset.map"),h(Ce,"href","/docs/datasets/pr_3880/en/package_reference/main_classes#datasets.IterableDataset"),h(Le,"href","/docs/datasets/pr_3880/en/package_reference/main_classes#datasets.IterableDataset.map"),h(Ne,"href","/docs/datasets/pr_3880/en/package_reference/main_classes#datasets.IterableDataset.map"),h(Oe,"href","/docs/datasets/pr_3880/en/package_reference/main_classes#datasets.IterableDataset.map"),h(ks,"id","batch-processing"),h(ks,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(ks,"href","#batch-processing"),h(ss,"class","relative group"),h(Fe,"href","/docs/datasets/pr_3880/en/package_reference/main_classes#datasets.IterableDataset.map"),h(ys,"id","tokenization"),h(ys,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(ys,"href","#tokenization"),h(es,"class","relative group"),h(Ds,"id","filter"),h(Ds,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(Ds,"href","#filter"),h(as,"class","relative group"),h(Re,"href","/docs/datasets/pr_3880/en/package_reference/main_classes#datasets.Dataset.filter"),h(Ye,"href","/docs/datasets/pr_3880/en/package_reference/main_classes#datasets.Dataset.filter"),h(Is,"id","stream-in-a-training-loop"),h(Is,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(Is,"href","#stream-in-a-training-loop"),h(ls,"class","relative group"),h(He,"href","/docs/datasets/pr_3880/en/package_reference/main_classes#datasets.IterableDataset")},m(s,o){e(document.head,f),i(s,w,o),i(s,m,o),e(m,$),e($,k),b(u,k,null),e(m,y),e(m,E),e(E,z),i(s,D,o),i(s,T,o),e(T,R),i(s,ns,o),i(s,q,o),e(q,A),e(A,ie),e(q,he),e(q,rs),e(rs,de),i(s,Oa,o),i(s,J,o),e(J,ce),e(J,rl),e(J,fe),i(s,Fa,o),i(s,P,o),e(P,pl),e(P,Ps),e(Ps,ol),e(P,il),e(P,Ge),e(Ge,hl),e(P,dl),e(P,me),e(me,cl),e(P,fl),i(s,Ra,o),b(Ms,s,o),i(s,Ya,o),i(s,Y,o),e(Y,ml),e(Y,ue),e(ue,ul),e(Y,_l),e(Y,_e),e(_e,gl),e(Y,bl),i(s,Ha,o),b(ps,s,o),i(s,Va,o),i(s,U,o),e(U,os),e(os,Ke),b(Cs,Ke,null),e(U,vl),e(U,Qe),e(Qe,jl),i(s,Ba,o),i(s,M,o),e(M,xl),e(M,ge),e(ge,wl),e(M,$l),e(M,be),e(be,kl),e(M,yl),e(M,ve),e(ve,El),e(M,Dl),i(s,Ja,o),i(s,C,o),e(C,Al),e(C,Xe),e(Xe,Il),e(C,Tl),e(C,Ze),e(Ze,Sl),e(C,ql),e(C,je),e(je,zl),e(C,Pl),i(s,Ua,o),b(Ls,s,o),i(s,Wa,o),b(is,s,o),i(s,Ga,o),i(s,W,o),e(W,hs),e(hs,sa),b(Ns,sa,null),e(W,Ml),e(W,ea),e(ea,Cl),i(s,Ka,o),i(s,ds,o),e(ds,Ll),e(ds,aa),e(aa,Nl),e(ds,Ol),i(s,Qa,o),i(s,cs,o),e(cs,Fl),e(cs,ta),e(ta,Rl),e(cs,Yl),i(s,Xa,o),b(Os,s,o),i(s,Za,o),i(s,G,o),e(G,fs),e(fs,la),b(Fs,la,null),e(G,Hl),e(G,na),e(na,Vl),i(s,st,o),i(s,xe,o),e(xe,Bl),i(s,et,o),i(s,we,o),e(we,ms),e(ms,$e),e($e,Jl),e(ms,Ul),e(ms,ra),e(ra,Wl),e(ms,Gl),i(s,at,o),b(Rs,s,o),i(s,tt,o),i(s,ke,o),e(ke,us),e(us,ye),e(ye,Kl),e(us,Ql),e(us,pa),e(pa,Xl),e(us,Zl),i(s,lt,o),b(Ys,s,o),i(s,nt,o),b(_s,s,o),i(s,rt,o),i(s,Ee,o),i(s,pt,o),i(s,K,o),e(K,gs),e(gs,oa),b(Hs,oa,null),e(K,sn),e(K,ia),e(ia,en),i(s,ot,o),i(s,Q,o),e(Q,De),e(De,an),e(Q,tn),e(Q,Ae),e(Ae,ln),e(Q,nn),i(s,it,o),b(Vs,s,o),i(s,ht,o),i(s,bs,o),e(bs,rn),e(bs,ha),e(ha,pn),e(bs,on),i(s,dt,o),b(Bs,s,o),i(s,ct,o),i(s,H,o),e(H,hn),e(H,da),e(da,dn),e(H,cn),e(H,ca),e(ca,fn),e(H,mn),i(s,ft,o),i(s,X,o),e(X,vs),e(vs,fa),b(Js,fa,null),e(X,un),e(X,ma),e(ma,_n),i(s,mt,o),i(s,js,o),e(js,gn),e(js,Ie),e(Ie,bn),e(js,vn),i(s,ut,o),b(Us,s,o),i(s,_t,o),i(s,Z,o),e(Z,xs),e(xs,ua),b(Ws,ua,null),e(Z,jn),e(Z,_a),e(_a,xn),i(s,gt,o),i(s,I,o),e(I,wn),e(I,Te),e(Te,$n),e(I,kn),e(I,Se),e(Se,yn),e(I,En),e(I,qe),e(qe,Dn),e(I,An),e(I,ze),e(ze,In),e(I,Tn),e(I,Pe),e(Pe,Sn),e(I,qn),i(s,bt,o),i(s,Me,o),e(Me,zn),i(s,vt,o),i(s,V,o),e(V,Pn),e(V,Ce),e(Ce,Mn),e(V,Cn),e(V,ga),e(ga,Ln),e(V,Nn),i(s,jt,o),b(Gs,s,o),i(s,xt,o),i(s,ws,o),e(ws,On),e(ws,Le),e(Le,Fn),e(ws,Rn),i(s,wt,o),b(Ks,s,o),i(s,$t,o),i(s,$s,o),e($s,Yn),e($s,Ne),e(Ne,Hn),e($s,Vn),i(s,kt,o),i(s,B,o),e(B,Bn),e(B,ba),e(ba,Jn),e(B,Un),e(B,Oe),e(Oe,Wn),e(B,Gn),i(s,yt,o),b(Qs,s,o),i(s,Et,o),i(s,ss,o),e(ss,ks),e(ks,va),b(Xs,va,null),e(ss,Kn),e(ss,ja),e(ja,Qn),i(s,Dt,o),i(s,O,o),e(O,Fe),e(Fe,Xn),e(O,Zn),e(O,xa),e(xa,sr),e(O,er),e(O,wa),e(wa,ar),e(O,tr),i(s,At,o),i(s,es,o),e(es,ys),e(ys,$a),b(Zs,$a,null),e(es,lr),e(es,ka),e(ka,nr),i(s,It,o),b(se,s,o),i(s,Tt,o),b(Es,s,o),i(s,St,o),i(s,as,o),e(as,Ds),e(Ds,ya),b(ee,ya,null),e(as,rr),e(as,Ea),e(Ea,pr),i(s,qt,o),i(s,As,o),e(As,or),e(As,Re),e(Re,ir),e(As,hr),i(s,zt,o),b(ae,s,o),i(s,Pt,o),i(s,ts,o),e(ts,Ye),e(Ye,dr),e(ts,cr),e(ts,Da),e(Da,fr),e(ts,mr),i(s,Mt,o),b(te,s,o),i(s,Ct,o),i(s,ls,o),e(ls,Is),e(Is,Aa),b(le,Aa,null),e(ls,ur),e(ls,Ia),e(Ia,_r),i(s,Lt,o),i(s,ne,o),e(ne,He),e(He,gr),e(ne,br),i(s,Nt,o),i(s,Ve,o),e(Ve,Ts),e(Ts,vr),e(Ts,Ta),e(Ta,jr),e(Ts,xr),b(re,Ts,null),Ot=!0},p(s,[o]){const pe={};o&2&&(pe.$$scope={dirty:o,ctx:s}),ps.$set(pe);const Sa={};o&2&&(Sa.$$scope={dirty:o,ctx:s}),is.$set(Sa);const qa={};o&2&&(qa.$$scope={dirty:o,ctx:s}),_s.$set(qa);const za={};o&2&&(za.$$scope={dirty:o,ctx:s}),Es.$set(za)},i(s){Ot||(v(u.$$.fragment,s),v(Ms.$$.fragment,s),v(ps.$$.fragment,s),v(Cs.$$.fragment,s),v(Ls.$$.fragment,s),v(is.$$.fragment,s),v(Ns.$$.fragment,s),v(Os.$$.fragment,s),v(Fs.$$.fragment,s),v(Rs.$$.fragment,s),v(Ys.$$.fragment,s),v(_s.$$.fragment,s),v(Hs.$$.fragment,s),v(Vs.$$.fragment,s),v(Bs.$$.fragment,s),v(Js.$$.fragment,s),v(Us.$$.fragment,s),v(Ws.$$.fragment,s),v(Gs.$$.fragment,s),v(Ks.$$.fragment,s),v(Qs.$$.fragment,s),v(Xs.$$.fragment,s),v(Zs.$$.fragment,s),v(se.$$.fragment,s),v(Es.$$.fragment,s),v(ee.$$.fragment,s),v(ae.$$.fragment,s),v(te.$$.fragment,s),v(le.$$.fragment,s),v(re.$$.fragment,s),Ot=!0)},o(s){j(u.$$.fragment,s),j(Ms.$$.fragment,s),j(ps.$$.fragment,s),j(Cs.$$.fragment,s),j(Ls.$$.fragment,s),j(is.$$.fragment,s),j(Ns.$$.fragment,s),j(Os.$$.fragment,s),j(Fs.$$.fragment,s),j(Rs.$$.fragment,s),j(Ys.$$.fragment,s),j(_s.$$.fragment,s),j(Hs.$$.fragment,s),j(Vs.$$.fragment,s),j(Bs.$$.fragment,s),j(Js.$$.fragment,s),j(Us.$$.fragment,s),j(Ws.$$.fragment,s),j(Gs.$$.fragment,s),j(Ks.$$.fragment,s),j(Qs.$$.fragment,s),j(Xs.$$.fragment,s),j(Zs.$$.fragment,s),j(se.$$.fragment,s),j(Es.$$.fragment,s),j(ee.$$.fragment,s),j(ae.$$.fragment,s),j(te.$$.fragment,s),j(le.$$.fragment,s),j(re.$$.fragment,s),Ot=!1},d(s){a(f),s&&a(w),s&&a(m),x(u),s&&a(D),s&&a(T),s&&a(ns),s&&a(q),s&&a(Oa),s&&a(J),s&&a(Fa),s&&a(P),s&&a(Ra),x(Ms,s),s&&a(Ya),s&&a(Y),s&&a(Ha),x(ps,s),s&&a(Va),s&&a(U),x(Cs),s&&a(Ba),s&&a(M),s&&a(Ja),s&&a(C),s&&a(Ua),x(Ls,s),s&&a(Wa),x(is,s),s&&a(Ga),s&&a(W),x(Ns),s&&a(Ka),s&&a(ds),s&&a(Qa),s&&a(cs),s&&a(Xa),x(Os,s),s&&a(Za),s&&a(G),x(Fs),s&&a(st),s&&a(xe),s&&a(et),s&&a(we),s&&a(at),x(Rs,s),s&&a(tt),s&&a(ke),s&&a(lt),x(Ys,s),s&&a(nt),x(_s,s),s&&a(rt),s&&a(Ee),s&&a(pt),s&&a(K),x(Hs),s&&a(ot),s&&a(Q),s&&a(it),x(Vs,s),s&&a(ht),s&&a(bs),s&&a(dt),x(Bs,s),s&&a(ct),s&&a(H),s&&a(ft),s&&a(X),x(Js),s&&a(mt),s&&a(js),s&&a(ut),x(Us,s),s&&a(_t),s&&a(Z),x(Ws),s&&a(gt),s&&a(I),s&&a(bt),s&&a(Me),s&&a(vt),s&&a(V),s&&a(jt),x(Gs,s),s&&a(xt),s&&a(ws),s&&a(wt),x(Ks,s),s&&a($t),s&&a($s),s&&a(kt),s&&a(B),s&&a(yt),x(Qs,s),s&&a(Et),s&&a(ss),x(Xs),s&&a(Dt),s&&a(O),s&&a(At),s&&a(es),x(Zs),s&&a(It),x(se,s),s&&a(Tt),x(Es,s),s&&a(St),s&&a(as),x(ee),s&&a(qt),s&&a(As),s&&a(zt),x(ae,s),s&&a(Pt),s&&a(ts),s&&a(Mt),x(te,s),s&&a(Ct),s&&a(ls),x(le),s&&a(Lt),s&&a(ne),s&&a(Nt),s&&a(Ve),x(re)}}}const po={local:"stream",sections:[{local:"shuffle",title:"Shuffle"},{local:"reshuffle",title:"Reshuffle"},{local:"split-dataset",title:"Split dataset"},{local:"interleave",title:"Interleave"},{local:"remove",title:"Remove"},{local:"map",sections:[{local:"batch-processing",sections:[{local:"tokenization",title:"Tokenization"}],title:"Batch processing"},{local:"filter",title:"Filter"}],title:"Map"},{local:"stream-in-a-training-loop",title:"Stream in a training loop"}],title:"Stream"};function oo(N,f,w){let{fw:m}=f;return N.$$set=$=>{"fw"in $&&w(0,m=$.fw)},[m]}class mo extends Xp{constructor(f){super();Zp(this,f,oo,ro,so,{fw:0})}}export{mo as default,po as metadata};
