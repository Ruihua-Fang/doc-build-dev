import{D as Mu,S as lo,i as oo,s as no,O as ot,P as nt,a as l,d as s,b as f,g as r,F as a,L as ti,e as o,k as c,t as i,c as n,m as h,h as p,Q as eo,q as j,l as Du,n as si,o as x,B as k,p as ai,w as E,y as q,j as zu,G as Vu,$ as Iu,x as P,a0 as Bu,T as Uu,Y as Ou,Z as Fu,M as Ju}from"../chunks/vendor-e67aec41.js";import{T as Pe}from"../chunks/Tip-76459d1c.js";import{I}from"../chunks/IconCopyLink-ffd7f84e.js";import{a as Hu,C as T}from"../chunks/CodeBlock-e2bcf023.js";const Yf={};function Lu(b){return Yf[b]||(Yf[b]=Mu("group1")),Yf[b]}function Yu(b){let d,m,u,g,v,_;return{c(){d=ot("svg"),m=ot("defs"),u=ot("clipPath"),g=ot("rect"),v=ot("g"),_=ot("path"),this.h()},l($){d=nt($,"svg",{class:!0,xmlns:!0,"xmlns:xlink":!0,"aria-hidden":!0,focusable:!0,role:!0,width:!0,height:!0,preserveAspectRatio:!0,viewBox:!0});var y=l(d);m=nt(y,"defs",{});var w=l(m);u=nt(w,"clipPath",{id:!0});var A=l(u);g=nt(A,"rect",{x:!0,y:!0,width:!0,height:!0,fill:!0}),l(g).forEach(s),A.forEach(s),w.forEach(s),v=nt(y,"g",{"clip-path":!0});var S=l(v);_=nt(S,"path",{d:!0,fill:!0}),l(_).forEach(s),S.forEach(s),y.forEach(s),this.h()},h(){f(g,"x","3.05"),f(g,"y","0.5"),f(g,"width","25.73"),f(g,"height","31"),f(g,"fill","none"),f(u,"id","a"),f(_,"d","M24.94,9.51a12.81,12.81,0,0,1,0,18.16,12.68,12.68,0,0,1-18,0,12.81,12.81,0,0,1,0-18.16l9-9V5l-.84.83-6,6a9.58,9.58,0,1,0,13.55,0ZM20.44,9a1.68,1.68,0,1,1,1.67-1.67A1.68,1.68,0,0,1,20.44,9Z"),f(_,"fill","#ee4c2c"),f(v,"clip-path","url(#a)"),f(d,"class",b[0]),f(d,"xmlns","http://www.w3.org/2000/svg"),f(d,"xmlns:xlink","http://www.w3.org/1999/xlink"),f(d,"aria-hidden","true"),f(d,"focusable","false"),f(d,"role","img"),f(d,"width","1em"),f(d,"height","1em"),f(d,"preserveAspectRatio","xMidYMid meet"),f(d,"viewBox","0 0 32 32")},m($,y){r($,d,y),a(d,m),a(m,u),a(u,g),a(d,v),a(v,_)},p($,[y]){y&1&&f(d,"class",$[0])},i:ti,o:ti,d($){$&&s(d)}}}function Wu(b,d,m){let{classNames:u=""}=d;return b.$$set=g=>{"classNames"in g&&m(0,u=g.classNames)},[u]}class Gu extends lo{constructor(d){super();oo(this,d,Wu,Yu,no,{classNames:0})}}function Qu(b){let d,m,u,g;return{c(){d=ot("svg"),m=ot("path"),u=ot("path"),g=ot("path"),this.h()},l(v){d=nt(v,"svg",{class:!0,xmlns:!0,"xmlns:xlink":!0,"aria-hidden":!0,focusable:!0,role:!0,width:!0,height:!0,preserveAspectRatio:!0,viewBox:!0});var _=l(d);m=nt(_,"path",{d:!0,fill:!0}),l(m).forEach(s),u=nt(_,"path",{d:!0,fill:!0}),l(u).forEach(s),g=nt(_,"path",{d:!0,fill:!0}),l(g).forEach(s),_.forEach(s),this.h()},h(){f(m,"d","M145.726 42.065v42.07l72.861 42.07v-42.07l-72.86-42.07zM0 84.135v42.07l36.43 21.03V105.17L0 84.135zm109.291 21.035l-36.43 21.034v126.2l36.43 21.035v-84.135l36.435 21.035v-42.07l-36.435-21.034V105.17z"),f(m,"fill","#E55B2D"),f(u,"d","M145.726 42.065L36.43 105.17v42.065l72.861-42.065v42.065l36.435-21.03v-84.14zM255.022 63.1l-36.435 21.035v42.07l36.435-21.035V63.1zm-72.865 84.135l-36.43 21.035v42.07l36.43-21.036v-42.07zm-36.43 63.104l-36.436-21.035v84.135l36.435-21.035V210.34z"),f(u,"fill","#ED8E24"),f(g,"d","M145.726 0L0 84.135l36.43 21.035l109.296-63.105l72.861 42.07L255.022 63.1L145.726 0zm0 126.204l-36.435 21.03l36.435 21.036l36.43-21.035l-36.43-21.03z"),f(g,"fill","#F8BF3C"),f(d,"class",b[0]),f(d,"xmlns","http://www.w3.org/2000/svg"),f(d,"xmlns:xlink","http://www.w3.org/1999/xlink"),f(d,"aria-hidden","true"),f(d,"focusable","false"),f(d,"role","img"),f(d,"width","0.94em"),f(d,"height","1em"),f(d,"preserveAspectRatio","xMidYMid meet"),f(d,"viewBox","0 0 256 274")},m(v,_){r(v,d,_),a(d,m),a(d,u),a(d,g)},p(v,[_]){_&1&&f(d,"class",v[0])},i:ti,o:ti,d(v){v&&s(d)}}}function Zu(b,d,m){let{classNames:u=""}=d;return b.$$set=g=>{"classNames"in g&&m(0,u=g.classNames)},[u]}class Ku extends lo{constructor(d){super();oo(this,d,Zu,Qu,no,{classNames:0})}}function Tu(b,d,m){const u=b.slice();return u[8]=d[m],u[10]=m,u}function Nu(b){let d,m,u;var g=b[8].icon;function v(_){return{props:{classNames:"mr-1.5"}}}return g&&(d=new g(v())),{c(){d&&E(d.$$.fragment),m=Du()},l(_){d&&P(d.$$.fragment,_),m=Du()},m(_,$){d&&q(d,_,$),r(_,m,$),u=!0},p(_,$){if(g!==(g=_[8].icon)){if(d){si();const y=d;x(y.$$.fragment,1,0,()=>{k(y,1)}),ai()}g?(d=new g(v()),E(d.$$.fragment),j(d.$$.fragment,1),q(d,m.parentNode,m)):d=null}},i(_){u||(d&&j(d.$$.fragment,_),u=!0)},o(_){d&&x(d.$$.fragment,_),u=!1},d(_){_&&s(m),d&&k(d,_)}}}function Cu(b){let d,m,u,g=b[8].name+"",v,_,$,y,w,A,S,D=b[8].icon&&Nu(b);function Y(){return b[6](b[8])}return{c(){d=o("button"),D&&D.c(),m=c(),u=o("p"),v=i(g),$=c(),this.h()},l(C){d=n(C,"BUTTON",{class:!0});var N=l(d);D&&D.l(N),m=h(N),u=n(N,"P",{class:!0});var U=l(u);v=p(U,g),U.forEach(s),$=h(N),N.forEach(s),this.h()},h(){f(u,"class",_="!m-0 "+b[8].classNames),f(d,"class",y="flex justify-center py-1.5 px-2.5 focus:outline-none rounded-"+(b[10]?"r":"l")+" "+(b[8].group!==b[1]&&"text-gray-500 filter grayscale"))},m(C,N){r(C,d,N),D&&D.m(d,null),a(d,m),a(d,u),a(u,v),a(d,$),w=!0,A||(S=eo(d,"click",Y),A=!0)},p(C,N){b=C,b[8].icon?D?(D.p(b,N),N&1&&j(D,1)):(D=Nu(b),D.c(),j(D,1),D.m(d,m)):D&&(si(),x(D,1,1,()=>{D=null}),ai()),(!w||N&1)&&g!==(g=b[8].name+"")&&zu(v,g),(!w||N&1&&_!==(_="!m-0 "+b[8].classNames))&&f(u,"class",_),(!w||N&3&&y!==(y="flex justify-center py-1.5 px-2.5 focus:outline-none rounded-"+(b[10]?"r":"l")+" "+(b[8].group!==b[1]&&"text-gray-500 filter grayscale")))&&f(d,"class",y)},i(C){w||(j(D),w=!0)},o(C){x(D),w=!1},d(C){C&&s(d),D&&D.d(),A=!1,S()}}}function Xu(b){let d,m,u,g=b[3].filter(b[5]),v=[];for(let $=0;$<g.length;$+=1)v[$]=Cu(Tu(b,g,$));const _=$=>x(v[$],1,1,()=>{v[$]=null});return{c(){d=o("div"),m=o("div");for(let $=0;$<v.length;$+=1)v[$].c();this.h()},l($){d=n($,"DIV",{});var y=l(d);m=n(y,"DIV",{class:!0});var w=l(m);for(let A=0;A<v.length;A+=1)v[A].l(w);w.forEach(s),y.forEach(s),this.h()},h(){f(m,"class","bg-white leading-none border border-gray-100 rounded-lg inline-flex p-0.5 text-sm mb-4 select-none")},m($,y){r($,d,y),a(d,m);for(let w=0;w<v.length;w+=1)v[w].m(m,null);u=!0},p($,[y]){if(y&27){g=$[3].filter($[5]);let w;for(w=0;w<g.length;w+=1){const A=Tu($,g,w);v[w]?(v[w].p(A,y),j(v[w],1)):(v[w]=Cu(A),v[w].c(),j(v[w],1),v[w].m(m,null))}for(si(),w=g.length;w<v.length;w+=1)_(w);ai()}},i($){if(!u){for(let y=0;y<g.length;y+=1)j(v[y]);u=!0}},o($){v=v.filter(Boolean);for(let y=0;y<v.length;y+=1)x(v[y]);u=!1},d($){$&&s(d),Vu(v,$)}}}function tm(b,d,m){let u,{ids:g}=d;const v=g.join("-"),_=Lu(v);Iu(b,_,S=>m(1,u=S));const $=[{id:"pt",classNames:"",icon:Gu,name:"Pytorch",group:"group1"},{id:"tf",classNames:"",icon:Ku,name:"TensorFlow",group:"group2"},{id:"stringapi",classNames:"text-blue-600",name:"String API",group:"group1"},{id:"readinstruction",classNames:"text-blue-600",name:"ReadInstruction",group:"group2"}];function y(S){Bu(_,u=S,u)}const w=S=>g.includes(S.id),A=S=>y(S.group);return b.$$set=S=>{"ids"in S&&m(0,g=S.ids)},[g,u,_,$,y,w,A]}class Ru extends lo{constructor(d){super();oo(this,d,tm,Xu,no,{ids:0})}}function sm(b){let d,m,u,g,v,_,$=b[1].highlighted+"",y;return m=new Hu({props:{classNames:"transition duration-200 ease-in-out "+(b[2]&&"opacity-0"),title:"Copy code excerpt to clipboard",value:b[1].code}}),v=new Ru({props:{ids:b[4]}}),{c(){d=o("div"),E(m.$$.fragment),u=c(),g=o("pre"),E(v.$$.fragment),_=new Ou,this.h()},l(w){d=n(w,"DIV",{class:!0});var A=l(d);P(m.$$.fragment,A),A.forEach(s),u=h(w),g=n(w,"PRE",{});var S=l(g);P(v.$$.fragment,S),_=Fu(S),S.forEach(s),this.h()},h(){f(d,"class","absolute top-2.5 right-4"),_.a=null},m(w,A){r(w,d,A),q(m,d,null),r(w,u,A),r(w,g,A),q(v,g,null),_.m($,g),y=!0},p(w,A){const S={};A&4&&(S.classNames="transition duration-200 ease-in-out "+(w[2]&&"opacity-0")),A&2&&(S.value=w[1].code),m.$set(S),(!y||A&2)&&$!==($=w[1].highlighted+"")&&_.p($)},i(w){y||(j(m.$$.fragment,w),j(v.$$.fragment,w),y=!0)},o(w){x(m.$$.fragment,w),x(v.$$.fragment,w),y=!1},d(w){w&&s(d),k(m),w&&s(u),w&&s(g),k(v)}}}function am(b){let d,m,u,g,v,_,$=b[0].highlighted+"",y;return m=new Hu({props:{classNames:"transition duration-200 ease-in-out "+(b[2]&&"opacity-0"),title:"Copy code excerpt to clipboard",value:b[0].code}}),v=new Ru({props:{ids:b[4]}}),{c(){d=o("div"),E(m.$$.fragment),u=c(),g=o("pre"),E(v.$$.fragment),_=new Ou,this.h()},l(w){d=n(w,"DIV",{class:!0});var A=l(d);P(m.$$.fragment,A),A.forEach(s),u=h(w),g=n(w,"PRE",{});var S=l(g);P(v.$$.fragment,S),_=Fu(S),S.forEach(s),this.h()},h(){f(d,"class","absolute top-2.5 right-4"),_.a=null},m(w,A){r(w,d,A),q(m,d,null),r(w,u,A),r(w,g,A),q(v,g,null),_.m($,g),y=!0},p(w,A){const S={};A&4&&(S.classNames="transition duration-200 ease-in-out "+(w[2]&&"opacity-0")),A&1&&(S.value=w[0].code),m.$set(S),(!y||A&1)&&$!==($=w[0].highlighted+"")&&_.p($)},i(w){y||(j(m.$$.fragment,w),j(v.$$.fragment,w),y=!0)},o(w){x(m.$$.fragment,w),x(v.$$.fragment,w),y=!1},d(w){w&&s(d),k(m),w&&s(u),w&&s(g),k(v)}}}function em(b){let d,m,u,g,v,_;const $=[am,sm],y=[];function w(A,S){return A[3]==="group1"?0:1}return m=w(b),u=y[m]=$[m](b),{c(){d=o("div"),u.c(),this.h()},l(A){d=n(A,"DIV",{class:!0});var S=l(d);u.l(S),S.forEach(s),this.h()},h(){f(d,"class","code-block relative")},m(A,S){r(A,d,S),y[m].m(d,null),g=!0,v||(_=[eo(d,"mouseover",b[6]),eo(d,"focus",b[6]),eo(d,"mouseout",b[7]),eo(d,"focus",b[7])],v=!0)},p(A,[S]){let D=m;m=w(A),m===D?y[m].p(A,S):(si(),x(y[D],1,1,()=>{y[D]=null}),ai(),u=y[m],u?u.p(A,S):(u=y[m]=$[m](A),u.c()),j(u,1),u.m(d,null))},i(A){g||(j(u),g=!0)},o(A){x(u),g=!1},d(A){A&&s(d),y[m].d(),v=!1,Uu(_)}}}function lm(b,d,m){let u,{group1:g}=d,{group2:v}=d;const _=[g.id,v.id],$=_.join("-"),y=Lu($);Iu(b,y,D=>m(3,u=D));let w=!0;function A(){m(2,w=!1)}function S(){m(2,w=!0)}return b.$$set=D=>{"group1"in D&&m(0,g=D.group1),"group2"in D&&m(1,v=D.group2)},[g,v,w,u,_,y,A,S]}class ao extends lo{constructor(d){super();oo(this,d,lm,em,no,{group1:0,group2:1})}}function om(b){let d,m,u,g,v;return{c(){d=o("p"),m=i("Refer to the "),u=o("a"),g=i("upload_dataset_repo"),v=i(" guide for more instructions on how to create a dataset repository on the Hub, and how to upload your data files."),this.h()},l(_){d=n(_,"P",{});var $=l(d);m=p($,"Refer to the "),u=n($,"A",{href:!0});var y=l(u);g=p(y,"upload_dataset_repo"),y.forEach(s),v=p($," guide for more instructions on how to create a dataset repository on the Hub, and how to upload your data files."),$.forEach(s),this.h()},h(){f(u,"href","#upload_dataset_repo")},m(_,$){r(_,d,$),a(d,m),a(d,u),a(u,g),a(d,v)},d(_){_&&s(d)}}}function nm(b){let d,m,u,g,v;return{c(){d=o("p"),m=i("If you don\u2019t specify which data files to use, "),u=o("code"),g=i("load_dataset"),v=i(" will return all the data files. This can take a long time if you are loading a large dataset like C4, which is approximately 13TB of data.")},l(_){d=n(_,"P",{});var $=l(d);m=p($,"If you don\u2019t specify which data files to use, "),u=n($,"CODE",{});var y=l(u);g=p(y,"load_dataset"),y.forEach(s),v=p($," will return all the data files. This can take a long time if you are loading a large dataset like C4, which is approximately 13TB of data."),$.forEach(s)},m(_,$){r(_,d,$),a(d,m),a(d,u),a(u,g),a(d,v)},d(_){_&&s(d)}}}function rm(b){let d,m,u,g,v,_,$,y,w,A,S,D,Y,C,N,U,O;return{c(){d=o("p"),m=i("An object data type in "),u=o("a"),g=i("pandas.Series"),v=i(" doesn\u2019t always carry enough information for Arrow to automatically infer a data type. For example, if a DataFrame is of length 0 or the Series only contains None/nan objects, the type is set to null. Avoid potential errors by constructing an explicit schema with "),_=o("a"),$=i("datasets.Features"),y=i(" using the "),w=o("code"),A=i("from_dict"),S=i(" or "),D=o("code"),Y=i("from_pandas"),C=i(" methods. See the "),N=o("a"),U=i("troubleshoot"),O=i(" for more details on how to explicitly specify your own features."),this.h()},l(J){d=n(J,"P",{});var F=l(d);m=p(F,"An object data type in "),u=n(F,"A",{href:!0,rel:!0});var xa=l(u);g=p(xa,"pandas.Series"),xa.forEach(s),v=p(F," doesn\u2019t always carry enough information for Arrow to automatically infer a data type. For example, if a DataFrame is of length 0 or the Series only contains None/nan objects, the type is set to null. Avoid potential errors by constructing an explicit schema with "),_=n(F,"A",{href:!0});var qt=l(_);$=p(qt,"datasets.Features"),qt.forEach(s),y=p(F," using the "),w=n(F,"CODE",{});var ka=l(w);A=p(ka,"from_dict"),ka.forEach(s),S=p(F," or "),D=n(F,"CODE",{});var Ea=l(D);Y=p(Ea,"from_pandas"),Ea.forEach(s),C=p(F," methods. See the "),N=n(F,"A",{href:!0});var Pt=l(N);U=p(Pt,"troubleshoot"),Pt.forEach(s),O=p(F," for more details on how to explicitly specify your own features."),F.forEach(s),this.h()},h(){f(u,"href","https://pandas.pydata.org/docs/reference/api/pandas.Series.html"),f(u,"rel","nofollow"),f(_,"href","/docs/datasets/pr_3880/en/package_reference/main_classes#datasets.Features"),f(N,"href","#troubleshoot")},m(J,F){r(J,d,F),a(d,m),a(d,u),a(u,g),a(d,v),a(d,_),a(_,$),a(d,y),a(d,w),a(w,A),a(d,S),a(d,D),a(D,Y),a(d,C),a(d,N),a(N,U),a(d,O)},d(J){J&&s(d)}}}function im(b){let d,m,u,g,v;return{c(){d=o("p"),m=i("Using "),u=o("code"),g=i("pct1_dropremainder"),v=i(" rounding may truncate the last examples in a dataset if the number of examples in your dataset don\u2019t divide evenly by 100.")},l(_){d=n(_,"P",{});var $=l(d);m=p($,"Using "),u=n($,"CODE",{});var y=l(u);g=p(y,"pct1_dropremainder"),y.forEach(s),v=p($," rounding may truncate the last examples in a dataset if the number of examples in your dataset don\u2019t divide evenly by 100."),$.forEach(s)},m(_,$){r(_,d,$),a(d,m),a(d,u),a(u,g),a(d,v)},d(_){_&&s(d)}}}function pm(b){let d,m,u,g,v;return{c(){d=o("p"),m=i("See the "),u=o("a"),g=i("metric_script"),v=i(" guide for more details on how to write your own metric loading script."),this.h()},l(_){d=n(_,"P",{});var $=l(d);m=p($,"See the "),u=n($,"A",{href:!0});var y=l(u);g=p(y,"metric_script"),y.forEach(s),v=p($," guide for more details on how to write your own metric loading script."),$.forEach(s),this.h()},h(){f(u,"href","#metric_script")},m(_,$){r(_,d,$),a(d,m),a(d,u),a(u,g),a(d,v)},d(_){_&&s(d)}}}function dm(b){let d,m,u,g,v;return{c(){d=o("p"),m=i("Once you\u2019ve loaded a metric for distributed usage, you can compute the metric as usual. Behind the scenes, "),u=o("a"),g=i("datasets.Metric.compute()"),v=i(" gathers all the predictions and references from the nodes, and computes the final metric."),this.h()},l(_){d=n(_,"P",{});var $=l(d);m=p($,"Once you\u2019ve loaded a metric for distributed usage, you can compute the metric as usual. Behind the scenes, "),u=n($,"A",{href:!0});var y=l(u);g=p(y,"datasets.Metric.compute()"),y.forEach(s),v=p($," gathers all the predictions and references from the nodes, and computes the final metric."),$.forEach(s),this.h()},h(){f(u,"href","/docs/datasets/pr_3880/en/package_reference/main_classes#datasets.Metric.compute")},m(_,$){r(_,d,$),a(d,m),a(d,u),a(u,g),a(d,v)},d(_){_&&s(d)}}}function fm(b){let d,m,u,g,v,_,$,y,w,A,S,D,Y,C,N,U,O,J,F,xa,qt,ka,Ea,Pt,ei,li,Ae,oi,ni,Se,ri,ro,qa,ii,io,Pa,po,rt,At,De,us,pi,Te,di,fo,St,fi,Ne,ci,hi,co,W,ui,Aa,mi,gi,ms,_i,vi,ho,gs,uo,Sa,$i,mo,Dt,wi,Ce,yi,bi,go,_s,_o,Tt,vo,H,ji,Ie,xi,ki,Oe,Ei,qi,Fe,Pi,Ai,He,Si,Di,Le,Ti,Ni,$o,vs,wo,Nt,yo,G,Ci,Re,Ii,Oi,$s,Fi,Hi,bo,ws,jo,Ct,Li,Me,Ri,Mi,xo,ys,ko,it,It,ze,bs,zi,Ve,Vi,Eo,L,Bi,Be,Ui,Ji,Ue,Yi,Wi,Je,Gi,Qi,Ye,Zi,Ki,Da,Xi,tp,qo,pt,Ot,We,js,sp,Ge,ap,Po,Ta,ep,Ao,xs,So,Na,lp,Do,ks,To,Ca,op,No,Es,Co,Ia,np,Io,qs,Oo,Oa,rp,Fo,Ps,Ho,dt,Ft,Qe,As,ip,Ze,pp,Lo,Ht,dp,Fa,fp,cp,Ro,Ss,Mo,Ha,hp,zo,Ds,Vo,Lt,up,Ke,mp,gp,Bo,Ts,Uo,La,_p,Jo,Ns,Yo,Ra,vp,Wo,ft,Rt,Xe,Cs,$p,tl,wp,Go,Ma,yp,Qo,Is,Zo,za,bp,Ko,Os,Xo,ct,Mt,sl,Fs,jp,al,xp,tn,Va,kp,sn,Hs,an,Ba,Ep,en,Ls,ln,ht,zt,el,Rs,qp,ll,Pp,on,Ua,Ap,nn,Ja,Sp,rn,Ms,pn,R,Dp,ol,Tp,Np,nl,Cp,Ip,Ya,Op,Fp,rl,Hp,Lp,dn,zs,fn,Wa,Rp,cn,Vs,hn,M,Mp,il,zp,Vp,pl,Bp,Up,dl,Jp,Yp,un,ut,Vt,fl,Bs,Wp,cl,Gp,mn,Bt,Qp,Ga,Zp,Kp,gn,mt,Ut,hl,Us,Xp,ul,td,_n,Jt,sd,Qa,ad,ed,vn,Js,$n,gt,Yt,ml,Ys,ld,gl,od,wn,Wt,nd,Za,rd,id,yn,Ws,bn,Gt,jn,_t,Qt,_l,Gs,pd,vl,dd,xn,Ka,fd,kn,Q,cd,$l,hd,ud,wl,md,gd,En,vt,Zt,yl,Qs,_d,bl,vd,qn,Z,$d,Xa,wd,yd,te,bd,jd,Pn,K,xd,jl,kd,Ed,xl,qd,Pd,An,Zs,Sn,Kt,Ad,kl,Sd,Dd,Dn,Ks,Tn,se,Td,Nn,Xs,Cn,ae,Nd,In,ta,On,ee,Cd,Fn,sa,Hn,$t,Xt,El,aa,Id,ql,Od,Ln,le,Fd,Rn,ea,Mn,ts,Hd,Pl,Ld,Rd,zn,la,Vn,ss,Bn,oe,Un,wt,as,Al,oa,Md,Sl,zd,Jn,ne,Vd,Yn,yt,es,Dl,na,Bd,Tl,Ud,Wn,z,Jd,re,Yd,Wd,Nl,Gd,Qd,Cl,Zd,Kd,Gn,ls,Xd,ra,tf,sf,Qn,ia,Zn,bt,os,Il,pa,af,Ol,ef,Kn,X,lf,ie,of,nf,da,rf,pf,Xn,tt,df,pe,ff,cf,de,hf,uf,tr,fa,sr,st,mf,Fl,gf,_f,fe,vf,$f,ar,ca,er,ce,wf,lr,ha,or,jt,ns,Hl,ua,yf,Ll,bf,nr,he,jf,rr,ma,ir,rs,pr,xt,is,Rl,ga,xf,Ml,kf,dr,ue,Ef,fr,_a,cr,kt,ps,zl,va,qf,Vl,Pf,hr,me,Af,ur,ge,Sf,mr,at,Bl,$a,Df,Ul,Tf,Nf,Cf,Jl,Et,If,Yl,Of,Ff,Wl,Hf,Lf,Rf,Gl,wa,Mf,_e,zf,Vf,gr,ya,_r,ds,vr,fs,Bf,Ql,Uf,Jf,$r,ba,wr;return _=new I({}),us=new I({}),gs=new T({props:{code:`from datasets import load_dataset
dataset = load_dataset('lhoestq/demo1')`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;lhoestq/demo1&#x27;</span>)`}}),_s=new T({props:{code:`dataset = load_dataset(
  "lhoestq/custom_squad",
  revision="main"  # tag name, or branch name, or commit hash
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(
<span class="hljs-meta">&gt;&gt;&gt; </span>  <span class="hljs-string">&quot;lhoestq/custom_squad&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>  revision=<span class="hljs-string">&quot;main&quot;</span>  <span class="hljs-comment"># tag name, or branch name, or commit hash</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>)`}}),Tt=new Pe({props:{$$slots:{default:[om]},$$scope:{ctx:b}}}),vs=new T({props:{code:`data_files = {"train": "train.csv", "test": "test.csv"}
dataset = load_dataset("namespace/your_dataset_name", data_files=data_files)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>data_files = {<span class="hljs-string">&quot;train&quot;</span>: <span class="hljs-string">&quot;train.csv&quot;</span>, <span class="hljs-string">&quot;test&quot;</span>: <span class="hljs-string">&quot;test.csv&quot;</span>}
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;namespace/your_dataset_name&quot;</span>, data_files=data_files)`}}),Nt=new Pe({props:{warning:"&lcub;true}",$$slots:{default:[nm]},$$scope:{ctx:b}}}),ws=new T({props:{code:`from datasets import load_dataset
c4_subset = load_dataset('allenai/c4', data_files='en/c4-train.0000*-of-01024.json.gz')`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>c4_subset = load_dataset(<span class="hljs-string">&#x27;allenai/c4&#x27;</span>, data_files=<span class="hljs-string">&#x27;en/c4-train.0000*-of-01024.json.gz&#x27;</span>)`}}),ys=new T({props:{code:`data_files = {"validation": "en/c4-validation.*.json.gz"}
c4_validation = load_dataset("allenai/c4", data_files=data_files, split="validation")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>data_files = {<span class="hljs-string">&quot;validation&quot;</span>: <span class="hljs-string">&quot;en/c4-validation.*.json.gz&quot;</span>}
<span class="hljs-meta">&gt;&gt;&gt; </span>c4_validation = load_dataset(<span class="hljs-string">&quot;allenai/c4&quot;</span>, data_files=data_files, split=<span class="hljs-string">&quot;validation&quot;</span>)`}}),bs=new I({}),js=new I({}),xs=new T({props:{code:`from datasets import load_dataset
dataset = load_dataset('csv', data_files='my_file.csv')`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;csv&#x27;</span>, data_files=<span class="hljs-string">&#x27;my_file.csv&#x27;</span>)`}}),ks=new T({props:{code:"dataset = load_dataset('csv', data_files=['my_file_1.csv', 'my_file_2.csv', 'my_file_3.csv'])",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;csv&#x27;</span>, data_files=[<span class="hljs-string">&#x27;my_file_1.csv&#x27;</span>, <span class="hljs-string">&#x27;my_file_2.csv&#x27;</span>, <span class="hljs-string">&#x27;my_file_3.csv&#x27;</span>])'}}),Es=new T({props:{code:"dataset = load_dataset('csv', data_files={'train': ['my_train_file_1.csv', 'my_train_file_2.csv'] 'test': 'my_test_file.csv'})",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;csv&#x27;</span>, data_files={<span class="hljs-string">&#x27;train&#x27;</span>: [<span class="hljs-string">&#x27;my_train_file_1.csv&#x27;</span>, <span class="hljs-string">&#x27;my_train_file_2.csv&#x27;</span>] <span class="hljs-string">&#x27;test&#x27;</span>: <span class="hljs-string">&#x27;my_test_file.csv&#x27;</span>})'}}),qs=new T({props:{code:`base_url = "https://huggingface.co/datasets/lhoestq/demo1/resolve/main/data/"
dataset = load_dataset('csv', data_files={'train': base_url + 'train.csv', 'test': base_url + 'test.csv'})`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>base_url = <span class="hljs-string">&quot;https://huggingface.co/datasets/lhoestq/demo1/resolve/main/data/&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;csv&#x27;</span>, data_files={<span class="hljs-string">&#x27;train&#x27;</span>: base_url + <span class="hljs-string">&#x27;train.csv&#x27;</span>, <span class="hljs-string">&#x27;test&#x27;</span>: base_url + <span class="hljs-string">&#x27;test.csv&#x27;</span>})`}}),Ps=new T({props:{code:`url = "https://domain.org/train_data.zip"
data_files = {"train": url}
dataset = load_dataset("csv", data_files=data_files)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;https://domain.org/train_data.zip&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>data_files = {<span class="hljs-string">&quot;train&quot;</span>: url}
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;csv&quot;</span>, data_files=data_files)`}}),As=new I({}),Ss=new T({props:{code:`from datasets import load_dataset
dataset = load_dataset('json', data_files='my_file.json')`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;json&#x27;</span>, data_files=<span class="hljs-string">&#x27;my_file.json&#x27;</span>)`}}),Ds=new T({props:{code:`{"a": 1, "b": 2.0, "c": "foo", "d": false}
{"a": 4, "b": -5.5, "c": null, "d": true}`,highlighted:`{<span class="hljs-string">&quot;a&quot;</span>: <span class="hljs-number">1</span>, <span class="hljs-string">&quot;b&quot;</span>: <span class="hljs-number">2.0</span>, <span class="hljs-string">&quot;c&quot;</span>: <span class="hljs-string">&quot;foo&quot;</span>, <span class="hljs-string">&quot;d&quot;</span>: false}
{<span class="hljs-string">&quot;a&quot;</span>: <span class="hljs-number">4</span>, <span class="hljs-string">&quot;b&quot;</span>: -<span class="hljs-number">5.5</span>, <span class="hljs-string">&quot;c&quot;</span>: null, <span class="hljs-string">&quot;d&quot;</span>: true}`}}),Ts=new T({props:{code:`{"version": "0.1.0",
    "data": [{"a": 1, "b": 2.0, "c": "foo", "d": false},
            {"a": 4, "b": -5.5, "c": null, "d": true}]
}

>>> from datasets import load_dataset
>>> dataset = load_dataset('json', data_files='my_file.json', field='data')`,highlighted:`{<span class="hljs-string">&quot;version&quot;</span>: <span class="hljs-string">&quot;0.1.0&quot;</span>,
    <span class="hljs-string">&quot;data&quot;</span>: [{<span class="hljs-string">&quot;a&quot;</span>: <span class="hljs-number">1</span>, <span class="hljs-string">&quot;b&quot;</span>: <span class="hljs-number">2.0</span>, <span class="hljs-string">&quot;c&quot;</span>: <span class="hljs-string">&quot;foo&quot;</span>, <span class="hljs-string">&quot;d&quot;</span>: false},
            {<span class="hljs-string">&quot;a&quot;</span>: <span class="hljs-number">4</span>, <span class="hljs-string">&quot;b&quot;</span>: -<span class="hljs-number">5.5</span>, <span class="hljs-string">&quot;c&quot;</span>: null, <span class="hljs-string">&quot;d&quot;</span>: true}]
}

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;json&#x27;</span>, data_files=<span class="hljs-string">&#x27;my_file.json&#x27;</span>, field=<span class="hljs-string">&#x27;data&#x27;</span>)`}}),Ns=new T({props:{code:`base_url = "https://rajpurkar.github.io/SQuAD-explorer/dataset/"
dataset = load_dataset('json', data_files={'train': base_url + 'train-v1.1.json', 'validation': base_url + 'dev-v1.1.json'}, field="data")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>base_url = <span class="hljs-string">&quot;https://rajpurkar.github.io/SQuAD-explorer/dataset/&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;json&#x27;</span>, data_files={<span class="hljs-string">&#x27;train&#x27;</span>: base_url + <span class="hljs-string">&#x27;train-v1.1.json&#x27;</span>, <span class="hljs-string">&#x27;validation&#x27;</span>: base_url + <span class="hljs-string">&#x27;dev-v1.1.json&#x27;</span>}, field=<span class="hljs-string">&quot;data&quot;</span>)`}}),Cs=new I({}),Is=new T({props:{code:`from datasets import load_dataset
dataset = load_dataset('text', data_files={'train': ['my_text_1.txt', 'my_text_2.txt'], 'test': 'my_test_file.txt'})`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;text&#x27;</span>, data_files={<span class="hljs-string">&#x27;train&#x27;</span>: [<span class="hljs-string">&#x27;my_text_1.txt&#x27;</span>, <span class="hljs-string">&#x27;my_text_2.txt&#x27;</span>], <span class="hljs-string">&#x27;test&#x27;</span>: <span class="hljs-string">&#x27;my_test_file.txt&#x27;</span>})`}}),Os=new T({props:{code:"dataset = load_dataset('text', data_files='https://huggingface.co/datasets/lhoestq/test/resolve/main/some_text.txt')",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;text&#x27;</span>, data_files=<span class="hljs-string">&#x27;https://huggingface.co/datasets/lhoestq/test/resolve/main/some_text.txt&#x27;</span>)'}}),Fs=new I({}),Hs=new T({props:{code:`from datasets import load_dataset
dataset = load_dataset("parquet", data_files={'train': 'train.parquet', 'test': 'test.parquet'})`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;parquet&quot;</span>, data_files={<span class="hljs-string">&#x27;train&#x27;</span>: <span class="hljs-string">&#x27;train.parquet&#x27;</span>, <span class="hljs-string">&#x27;test&#x27;</span>: <span class="hljs-string">&#x27;test.parquet&#x27;</span>})`}}),Ls=new T({props:{code:`base_url = "https://storage.googleapis.com/huggingface-nlp/cache/datasets/wikipedia/20200501.en/1.0.0/"
data_files = {"train": base_url + "wikipedia-train.parquet"}
wiki = load_dataset("parquet", data_files=data_files, split="train")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>base_url = <span class="hljs-string">&quot;https://storage.googleapis.com/huggingface-nlp/cache/datasets/wikipedia/20200501.en/1.0.0/&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>data_files = {<span class="hljs-string">&quot;train&quot;</span>: base_url + <span class="hljs-string">&quot;wikipedia-train.parquet&quot;</span>}
<span class="hljs-meta">&gt;&gt;&gt; </span>wiki = load_dataset(<span class="hljs-string">&quot;parquet&quot;</span>, data_files=data_files, split=<span class="hljs-string">&quot;train&quot;</span>)`}}),Rs=new I({}),Ms=new T({props:{code:`   data/dog/xxx.png
   data/dog/xxy.png
   data/dog/xxz.png

   data/cat/123.png
   data/cat/nsdf3.png
   data/cat/asd932_.png`,highlighted:`   data<span class="hljs-regexp">/dog/</span>xxx.png
   data<span class="hljs-regexp">/dog/</span>xxy.png
   data<span class="hljs-regexp">/dog/</span>xxz.png

   data<span class="hljs-regexp">/cat/</span><span class="hljs-number">123</span>.png
   data<span class="hljs-regexp">/cat/</span>nsdf3.png
   data<span class="hljs-regexp">/cat/</span>asd932_.png`}}),zs=new T({props:{code:`from datasets import load_dataset
dataset = load_dataset("imagefolder", data_dir="/path/to/data")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;imagefolder&quot;</span>, data_dir=<span class="hljs-string">&quot;/path/to/data&quot;</span>)`}}),Vs=new T({props:{code:'dataset = load_dataset("imagefolder", data_files="https://download.microsoft.com/download/3/E/1/3E1C3F21-ECDB-4869-8368-6DEBA77B919F/kagglecatsanddogs_3367a.zip", split="train")',highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;imagefolder&quot;</span>, data_files=<span class="hljs-string">&quot;https://download.microsoft.com/download/3/E/1/3E1C3F21-ECDB-4869-8368-6DEBA77B919F/kagglecatsanddogs_3367a.zip&quot;</span>, split=<span class="hljs-string">&quot;train&quot;</span>)'}}),Bs=new I({}),Us=new I({}),Js=new T({props:{code:`from datasets import Dataset
my_dict = {"a": [1, 2, 3]}
dataset = Dataset.from_dict(my_dict)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> Dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>my_dict = {<span class="hljs-string">&quot;a&quot;</span>: [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>]}
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = Dataset.from_dict(my_dict)`}}),Ys=new I({}),Ws=new T({props:{code:`from datasets import Dataset
import pandas as pd
df = pd.DataFrame({"a": [1, 2, 3]})
dataset = Dataset.from_pandas(df)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> Dataset
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd
<span class="hljs-meta">&gt;&gt;&gt; </span>df = pd.DataFrame({<span class="hljs-string">&quot;a&quot;</span>: [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>]})
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = Dataset.from_pandas(df)`}}),Gt=new Pe({props:{warning:"&lcub;true}",$$slots:{default:[rm]},$$scope:{ctx:b}}}),Gs=new I({}),Qs=new I({}),Zs=new ao({props:{group1:{id:"stringapi",code:"train_test_ds = datasets.load_dataset('bookcorpus', split='train+test')",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>train_test_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=<span class="hljs-string">&#x27;train+test&#x27;</span>)'},group2:{id:"readinstruction",code:`ri = datasets.ReadInstruction('train') + datasets.ReadInstruction('test')
train_test_ds = datasets.load_dataset('bookcorpus', split=ri)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>ri = datasets.ReadInstruction(<span class="hljs-string">&#x27;train&#x27;</span>) + datasets.ReadInstruction(<span class="hljs-string">&#x27;test&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>train_test_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=ri)`}}}),Ks=new ao({props:{group1:{id:"stringapi",code:"train_10_20_ds = datasets.load_dataset('bookcorpus', split='train[10:20]')",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>train_10_20_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=<span class="hljs-string">&#x27;train[10:20]&#x27;</span>)'},group2:{id:"readinstruction",code:"train_10_20_ds = datasets.load_dataset('bookcorpus', split=datasets.ReadInstruction('train', from_=10, to=20, unit='abs'))",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>train_10_20_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=datasets.ReadInstruction(<span class="hljs-string">&#x27;train&#x27;</span>, from_=<span class="hljs-number">10</span>, to=<span class="hljs-number">20</span>, unit=<span class="hljs-string">&#x27;abs&#x27;</span>))'}}}),Xs=new ao({props:{group1:{id:"stringapi",code:"train_10pct_ds = datasets.load_dataset('bookcorpus', split='train[:10%]')",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>train_10pct_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=<span class="hljs-string">&#x27;train[:10%]&#x27;</span>)'},group2:{id:"readinstruction",code:"train_10_20_ds = datasets.load_dataset('bookcorpus', split=datasets.ReadInstruction('train', to=10, unit='%'))",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>train_10_20_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=datasets.ReadInstruction(<span class="hljs-string">&#x27;train&#x27;</span>, to=<span class="hljs-number">10</span>, unit=<span class="hljs-string">&#x27;%&#x27;</span>))'}}}),ta=new ao({props:{group1:{id:"stringapi",code:"train_10_80pct_ds = datasets.load_dataset('bookcorpus', split='train[:10%]+train[-80%:]')",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>train_10_80pct_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=<span class="hljs-string">&#x27;train[:10%]+train[-80%:]&#x27;</span>)'},group2:{id:"readinstruction",code:`ri = (datasets.ReadInstruction('train', to=10, unit='%') + datasets.ReadInstruction('train', from_=-80, unit='%'))
train_10_80pct_ds = datasets.load_dataset('bookcorpus', split=ri)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>ri = (datasets.ReadInstruction(<span class="hljs-string">&#x27;train&#x27;</span>, to=<span class="hljs-number">10</span>, unit=<span class="hljs-string">&#x27;%&#x27;</span>) + datasets.ReadInstruction(<span class="hljs-string">&#x27;train&#x27;</span>, from_=-<span class="hljs-number">80</span>, unit=<span class="hljs-string">&#x27;%&#x27;</span>))
<span class="hljs-meta">&gt;&gt;&gt; </span>train_10_80pct_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=ri)`}}}),sa=new ao({props:{group1:{id:"stringapi",code:`# 10-fold cross-validation (see also next section on rounding behavior):
# The validation datasets are each going to be 10%:
# [0%:10%], [10%:20%], ..., [90%:100%].
# And the training datasets are each going to be the complementary 90%:
# [10%:100%] (for a corresponding validation set of [0%:10%]),
# [0%:10%] + [20%:100%] (for a validation set of [10%:20%]), ...,
# [0%:90%] (for a validation set of [90%:100%]).
vals_ds = datasets.load_dataset('bookcorpus', split=[f'train[{k}%:{k+10}%]' for k in range(0, 100, 10)])
trains_ds = datasets.load_dataset('bookcorpus', split=[f'train[:{k}%]+train[{k+10}%:]' for k in range(0, 100, 10)])`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># 10-fold cross-validation (see also next section on rounding behavior):</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># The validation datasets are each going to be 10%:</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># [0%:10%], [10%:20%], ..., [90%:100%].</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># And the training datasets are each going to be the complementary 90%:</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># [10%:100%] (for a corresponding validation set of [0%:10%]),</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># [0%:10%] + [20%:100%] (for a validation set of [10%:20%]), ...,</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># [0%:90%] (for a validation set of [90%:100%]).</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>vals_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=[<span class="hljs-string">f&#x27;train[<span class="hljs-subst">{k}</span>%:<span class="hljs-subst">{k+<span class="hljs-number">10</span>}</span>%]&#x27;</span> <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, <span class="hljs-number">100</span>, <span class="hljs-number">10</span>)])
<span class="hljs-meta">&gt;&gt;&gt; </span>trains_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=[<span class="hljs-string">f&#x27;train[:<span class="hljs-subst">{k}</span>%]+train[<span class="hljs-subst">{k+<span class="hljs-number">10</span>}</span>%:]&#x27;</span> <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, <span class="hljs-number">100</span>, <span class="hljs-number">10</span>)])`},group2:{id:"readinstruction",code:`# 10-fold cross-validation (see also next section on rounding behavior):
# The validation datasets are each going to be 10%:
# [0%:10%], [10%:20%], ..., [90%:100%].
# And the training datasets are each going to be the complementary 90%:
# [10%:100%] (for a corresponding validation set of [0%:10%]),
# [0%:10%] + [20%:100%] (for a validation set of [10%:20%]), ...,
# [0%:90%] (for a validation set of [90%:100%]).
vals_ds = datasets.load_dataset('bookcorpus', [datasets.ReadInstruction('train', from_=k, to=k+10, unit='%') for k in range(0, 100, 10)])
trains_ds = datasets.load_dataset('bookcorpus', [(datasets.ReadInstruction('train', to=k, unit='%') + datasets.ReadInstruction('train', from_=k+10, unit='%')) for k in range(0, 100, 10)])`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># 10-fold cross-validation (see also next section on rounding behavior):</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># The validation datasets are each going to be 10%:</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># [0%:10%], [10%:20%], ..., [90%:100%].</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># And the training datasets are each going to be the complementary 90%:</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># [10%:100%] (for a corresponding validation set of [0%:10%]),</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># [0%:10%] + [20%:100%] (for a validation set of [10%:20%]), ...,</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># [0%:90%] (for a validation set of [90%:100%]).</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>vals_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, [datasets.ReadInstruction(<span class="hljs-string">&#x27;train&#x27;</span>, from_=k, to=k+<span class="hljs-number">10</span>, unit=<span class="hljs-string">&#x27;%&#x27;</span>) <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, <span class="hljs-number">100</span>, <span class="hljs-number">10</span>)])
<span class="hljs-meta">&gt;&gt;&gt; </span>trains_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, [(datasets.ReadInstruction(<span class="hljs-string">&#x27;train&#x27;</span>, to=k, unit=<span class="hljs-string">&#x27;%&#x27;</span>) + datasets.ReadInstruction(<span class="hljs-string">&#x27;train&#x27;</span>, from_=k+<span class="hljs-number">10</span>, unit=<span class="hljs-string">&#x27;%&#x27;</span>)) <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, <span class="hljs-number">100</span>, <span class="hljs-number">10</span>)])`}}}),aa=new I({}),ea=new T({props:{code:`# Assuming *train* split contains 999 records.
# 19 records, from 500 (included) to 519 (excluded).
>>> train_50_52_ds = datasets.load_dataset('bookcorpus', split='train[50%:52%]')
# 20 records, from 519 (included) to 539 (excluded).
>>> train_52_54_ds = datasets.load_dataset('bookcorpus', split='train[52%:54%]')`,highlighted:`<span class="hljs-comment"># Assuming *train* split contains 999 records.</span>
<span class="hljs-comment"># 19 records, from 500 (included) to 519 (excluded).</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>train_50_52_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=<span class="hljs-string">&#x27;train[50%:52%]&#x27;</span>)
<span class="hljs-comment"># 20 records, from 519 (included) to 539 (excluded).</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>train_52_54_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=<span class="hljs-string">&#x27;train[52%:54%]&#x27;</span>)`}}),la=new T({props:{code:`# 18 records, from 450 (included) to 468 (excluded).
>>> train_50_52pct1_ds = datasets.load_dataset('bookcorpus', split=datasets.ReadInstruction( 'train', from_=50, to=52, unit='%', rounding='pct1_dropremainder'))
# 18 records, from 468 (included) to 486 (excluded).
>>> train_52_54pct1_ds = datasets.load_dataset('bookcorpus', split=datasets.ReadInstruction('train',from_=52, to=54, unit='%', rounding='pct1_dropremainder'))
# Or equivalently:
>>> train_50_52pct1_ds = datasets.load_dataset('bookcorpus', split='train[50%:52%](pct1_dropremainder)')
>>> train_52_54pct1_ds = datasets.load_dataset('bookcorpus', split='train[52%:54%](pct1_dropremainder)')`,highlighted:`<span class="hljs-comment"># 18 records, from 450 (included) to 468 (excluded).</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>train_50_52pct1_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=datasets.ReadInstruction( <span class="hljs-string">&#x27;train&#x27;</span>, from_=<span class="hljs-number">50</span>, to=<span class="hljs-number">52</span>, unit=<span class="hljs-string">&#x27;%&#x27;</span>, rounding=<span class="hljs-string">&#x27;pct1_dropremainder&#x27;</span>))
<span class="hljs-comment"># 18 records, from 468 (included) to 486 (excluded).</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>train_52_54pct1_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=datasets.ReadInstruction(<span class="hljs-string">&#x27;train&#x27;</span>,from_=<span class="hljs-number">52</span>, to=<span class="hljs-number">54</span>, unit=<span class="hljs-string">&#x27;%&#x27;</span>, rounding=<span class="hljs-string">&#x27;pct1_dropremainder&#x27;</span>))
<span class="hljs-comment"># Or equivalently:</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>train_50_52pct1_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=<span class="hljs-string">&#x27;train[50%:52%](pct1_dropremainder)&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>train_52_54pct1_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=<span class="hljs-string">&#x27;train[52%:54%](pct1_dropremainder)&#x27;</span>)`}}),ss=new Pe({props:{warning:"&lcub;true}",$$slots:{default:[im]},$$scope:{ctx:b}}}),oa=new I({}),na=new I({}),ia=new T({props:{code:'dataset = load_dataset("matinf", "summarization")',highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;matinf&quot;</span>, <span class="hljs-string">&quot;summarization&quot;</span>)
Downloading <span class="hljs-keyword">and</span> preparing dataset matinf/summarization (download: Unknown size, generated: <span class="hljs-number">246.89</span> MiB, post-processed: Unknown size, total: <span class="hljs-number">246.89</span> MiB) to /root/.cache/huggingface/datasets/matinf/summarization/<span class="hljs-number">1.0</span><span class="hljs-number">.0</span>/82eee5e71c3ceaf20d909bca36ff237452b4e4ab195d3be7ee1c78b53e6f540e...
AssertionError: The dataset matinf <span class="hljs-keyword">with</span> config summarization requires manual data. 
Please follow the manual download instructions: To use MATINF you have to download it manually. Please fill this google form (https://forms.gle/nkH4LVE4iNQeDzsc9). You will receive a download link <span class="hljs-keyword">and</span> a password once you complete the form. Please extract <span class="hljs-built_in">all</span> files <span class="hljs-keyword">in</span> one folder <span class="hljs-keyword">and</span> load the dataset <span class="hljs-keyword">with</span>: *datasets.load_dataset(<span class="hljs-string">&#x27;matinf&#x27;</span>, data_dir=<span class="hljs-string">&#x27;path/to/folder/folder_name&#x27;</span>)*. 
Manual data can be loaded <span class="hljs-keyword">with</span> \`datasets.load_dataset(matinf, data_dir=<span class="hljs-string">&#x27;&lt;path/to/manual/data&gt;&#x27;</span>) `}}),pa=new I({}),fa=new T({props:{code:`class_names = ["sadness", "joy", "love", "anger", "fear", "surprise"]
emotion_features = Features({'text': Value('string'), 'label': ClassLabel(names=class_names)})`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>class_names = [<span class="hljs-string">&quot;sadness&quot;</span>, <span class="hljs-string">&quot;joy&quot;</span>, <span class="hljs-string">&quot;love&quot;</span>, <span class="hljs-string">&quot;anger&quot;</span>, <span class="hljs-string">&quot;fear&quot;</span>, <span class="hljs-string">&quot;surprise&quot;</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>emotion_features = Features({<span class="hljs-string">&#x27;text&#x27;</span>: Value(<span class="hljs-string">&#x27;string&#x27;</span>), <span class="hljs-string">&#x27;label&#x27;</span>: ClassLabel(names=class_names)})`}}),ca=new T({props:{code:"dataset = load_dataset('csv', data_files=file_dict, delimiter=';', column_names=['text', 'label'], features=emotion_features)",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;csv&#x27;</span>, data_files=file_dict, delimiter=<span class="hljs-string">&#x27;;&#x27;</span>, column_names=[<span class="hljs-string">&#x27;text&#x27;</span>, <span class="hljs-string">&#x27;label&#x27;</span>], features=emotion_features)'}}),ha=new T({props:{code:"dataset['train'].features",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>dataset[<span class="hljs-string">&#x27;train&#x27;</span>].features
{<span class="hljs-string">&#x27;text&#x27;</span>: Value(dtype=<span class="hljs-string">&#x27;string&#x27;</span>, <span class="hljs-built_in">id</span>=<span class="hljs-literal">None</span>),
<span class="hljs-string">&#x27;label&#x27;</span>: ClassLabel(num_classes=<span class="hljs-number">6</span>, names=[<span class="hljs-string">&#x27;sadness&#x27;</span>, <span class="hljs-string">&#x27;joy&#x27;</span>, <span class="hljs-string">&#x27;love&#x27;</span>, <span class="hljs-string">&#x27;anger&#x27;</span>, <span class="hljs-string">&#x27;fear&#x27;</span>, <span class="hljs-string">&#x27;surprise&#x27;</span>], names_file=<span class="hljs-literal">None</span>, <span class="hljs-built_in">id</span>=<span class="hljs-literal">None</span>)}`}}),ua=new I({}),ma=new T({props:{code:`from datasets import load_metric
metric = load_metric('PATH/TO/MY/METRIC/SCRIPT')

# Example of typical usage
for batch in dataset:
    inputs, references = batch
    predictions = model(inputs)
    metric.add_batch(predictions=predictions, references=references)
score = metric.compute()`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_metric
<span class="hljs-meta">&gt;&gt;&gt; </span>metric = load_metric(<span class="hljs-string">&#x27;PATH/TO/MY/METRIC/SCRIPT&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Example of typical usage</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">for</span> batch <span class="hljs-keyword">in</span> dataset:
<span class="hljs-meta">... </span>    inputs, references = batch
<span class="hljs-meta">... </span>    predictions = model(inputs)
<span class="hljs-meta">... </span>    metric.add_batch(predictions=predictions, references=references)
<span class="hljs-meta">&gt;&gt;&gt; </span>score = metric.compute()`}}),rs=new Pe({props:{$$slots:{default:[pm]},$$scope:{ctx:b}}}),ga=new I({}),_a=new T({props:{code:`from datasets import load_metric
metric = load_metric('bleurt', name='bleurt-base-128')
metric = load_metric('bleurt', name='bleurt-base-512')`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python"><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_metric</span>
<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python">metric = load_metric(<span class="hljs-string">&#x27;bleurt&#x27;</span>, name=<span class="hljs-string">&#x27;bleurt-base-128&#x27;</span>)</span>
<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python">metric = load_metric(<span class="hljs-string">&#x27;bleurt&#x27;</span>, name=<span class="hljs-string">&#x27;bleurt-base-512&#x27;</span>)</span>`}}),va=new I({}),ya=new T({props:{code:`from datasets import load_metric
metric = load_metric('glue', 'mrpc', num_process=num_process, process_id=rank)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_metric
<span class="hljs-meta">&gt;&gt;&gt; </span>metric = load_metric(<span class="hljs-string">&#x27;glue&#x27;</span>, <span class="hljs-string">&#x27;mrpc&#x27;</span>, num_process=num_process, process_id=rank)`}}),ds=new Pe({props:{$$slots:{default:[dm]},$$scope:{ctx:b}}}),ba=new T({props:{code:`from datasets import load_metric
metric = load_metric('glue', 'mrpc', num_process=num_process, process_id=process_id, experiment_id="My_experiment_10")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_metric
<span class="hljs-meta">&gt;&gt;&gt; </span>metric = load_metric(<span class="hljs-string">&#x27;glue&#x27;</span>, <span class="hljs-string">&#x27;mrpc&#x27;</span>, num_process=num_process, process_id=process_id, experiment_id=<span class="hljs-string">&quot;My_experiment_10&quot;</span>)`}}),{c(){d=o("meta"),m=c(),u=o("h1"),g=o("a"),v=o("span"),E(_.$$.fragment),$=c(),y=o("span"),w=i("Load"),A=c(),S=o("p"),D=i("You have already seen how to load a dataset from the Hugging Face Hub. But datasets are stored in a variety of places, and sometimes you won\u2019t find the one you want on the Hub. A dataset can be on disk on your local machine, in a Github repository, and in in-memory data structures like Python dictionaries and Pandas DataFrames. Wherever your dataset may be stored, \u{1F917} Datasets provides a way for you to load and use it for training."),Y=c(),C=o("p"),N=i("This guide will show you how to load a dataset from:"),U=c(),O=o("ul"),J=o("li"),F=i("The Hub without a dataset loading script"),xa=c(),qt=o("li"),ka=i("Local files"),Ea=c(),Pt=o("li"),ei=i("In-memory data"),li=c(),Ae=o("li"),oi=i("Offline"),ni=c(),Se=o("li"),ri=i("A specific slice of a split"),ro=c(),qa=o("p"),ii=i("You will also learn how to troubleshoot common errors, and how to load specific configurations of a metric."),io=c(),Pa=o("a"),po=c(),rt=o("h2"),At=o("a"),De=o("span"),E(us.$$.fragment),pi=c(),Te=o("span"),di=i("Hugging Face Hub"),fo=c(),St=o("p"),fi=i("In the tutorial, you learned how to load a dataset from the Hub. This method relies on a dataset loading script that downloads and builds the dataset. However, you can also load a dataset from any dataset repository on the Hub "),Ne=o("strong"),ci=i("without"),hi=i(" a loading script!"),co=c(),W=o("p"),ui=i("First, create a dataset repository and upload your data files. Then you can use "),Aa=o("a"),mi=i("datasets.load_dataset()"),gi=i(" like you learned in the tutorial. For example, load the files from this "),ms=o("a"),_i=i("demo repository"),vi=i(" by providing the repository namespace and dataset name:"),ho=c(),E(gs.$$.fragment),uo=c(),Sa=o("p"),$i=i("This dataset repository contains CSV files, and this code loads all the data from the CSV files."),mo=c(),Dt=o("p"),wi=i("Some datasets may have more than one version, based on Git tags, branches or commits. Use the "),Ce=o("code"),yi=i("revision"),bi=i(" flag to specify which dataset version you want to load:"),go=c(),E(_s.$$.fragment),_o=c(),E(Tt.$$.fragment),vo=c(),H=o("p"),ji=i("If the dataset doesn\u2019t have a dataset loading script, then by default, all the data will be loaded in the "),Ie=o("code"),xi=i("train"),ki=i(" split. Use the "),Oe=o("code"),Ei=i("data_files"),qi=i(" parameter to map data files to splits like "),Fe=o("code"),Pi=i("train"),Ai=i(", "),He=o("code"),Si=i("validation"),Di=i(" and "),Le=o("code"),Ti=i("test"),Ni=i(":"),$o=c(),E(vs.$$.fragment),wo=c(),E(Nt.$$.fragment),yo=c(),G=o("p"),Ci=i("You can also load a specific subset of the files with the "),Re=o("code"),Ii=i("data_files"),Oi=i(" parameter. The example below loads files from the "),$s=o("a"),Fi=i("C4 dataset"),Hi=i(":"),bo=c(),E(ws.$$.fragment),jo=c(),Ct=o("p"),Li=i("Specify a custom split with the "),Me=o("code"),Ri=i("split"),Mi=i(" parameter:"),xo=c(),E(ys.$$.fragment),ko=c(),it=o("h2"),It=o("a"),ze=o("span"),E(bs.$$.fragment),zi=c(),Ve=o("span"),Vi=i("Local and remote files"),Eo=c(),L=o("p"),Bi=i("\u{1F917} Datasets can be loaded from local files stored on your computer, and also from remote files. The datasets are most likely stored as a "),Be=o("code"),Ui=i("csv"),Ji=i(", "),Ue=o("code"),Yi=i("json"),Wi=i(", "),Je=o("code"),Gi=i("txt"),Qi=i(" or "),Ye=o("code"),Zi=i("parquet"),Ki=i(" file. The "),Da=o("a"),Xi=i("datasets.load_dataset()"),tp=i(" method is able to load each of these file types."),qo=c(),pt=o("h3"),Ot=o("a"),We=o("span"),E(js.$$.fragment),sp=c(),Ge=o("span"),ap=i("CSV"),Po=c(),Ta=o("p"),ep=i("\u{1F917} Datasets can read a dataset made up of one or several CSV files:"),Ao=c(),E(xs.$$.fragment),So=c(),Na=o("p"),lp=i("If you have more than one CSV file:"),Do=c(),E(ks.$$.fragment),To=c(),Ca=o("p"),op=i("You can also map the training and test splits to specific CSV files:"),No=c(),E(Es.$$.fragment),Co=c(),Ia=o("p"),np=i("To load remote CSV files via HTTP, you can pass the URLs:"),Io=c(),E(qs.$$.fragment),Oo=c(),Oa=o("p"),rp=i("To load zipped CSV files:"),Fo=c(),E(Ps.$$.fragment),Ho=c(),dt=o("h3"),Ft=o("a"),Qe=o("span"),E(As.$$.fragment),ip=c(),Ze=o("span"),pp=i("JSON"),Lo=c(),Ht=o("p"),dp=i("JSON files are loaded directly with "),Fa=o("a"),fp=i("datasets.load_dataset()"),cp=i(" as shown below:"),Ro=c(),E(Ss.$$.fragment),Mo=c(),Ha=o("p"),hp=i("JSON files can have diverse formats, but we think the most efficient format is to have multiple JSON objects; each line represents an individual row of data. For example:"),zo=c(),E(Ds.$$.fragment),Vo=c(),Lt=o("p"),up=i("Another JSON format you may encounter is a nested field, in which case you will need to specify the "),Ke=o("code"),mp=i("field"),gp=i(" argument as shown in the following:"),Bo=c(),E(Ts.$$.fragment),Uo=c(),La=o("p"),_p=i("To load remote JSON files via HTTP, you can pass the URLs:"),Jo=c(),E(Ns.$$.fragment),Yo=c(),Ra=o("p"),vp=i("While these are the most common JSON formats, you will see other datasets that are formatted differently. \u{1F917} Datasets recognizes these other formats, and will fallback accordingly on the Python JSON loading methods to handle them."),Wo=c(),ft=o("h3"),Rt=o("a"),Xe=o("span"),E(Cs.$$.fragment),$p=c(),tl=o("span"),wp=i("Text files"),Go=c(),Ma=o("p"),yp=i("Text files are one of the most common file types for storing a dataset. \u{1F917} Datasets will read the text file line by line to build the dataset."),Qo=c(),E(Is.$$.fragment),Zo=c(),za=o("p"),bp=i("To load remote TXT files via HTTP, you can pass the URLs:"),Ko=c(),E(Os.$$.fragment),Xo=c(),ct=o("h3"),Mt=o("a"),sl=o("span"),E(Fs.$$.fragment),jp=c(),al=o("span"),xp=i("Parquet"),tn=c(),Va=o("p"),kp=i("Parquet files are stored in a columnar format unlike row-based files like CSV. Large datasets may be stored in a Parquet file because it is more efficient, and faster at returning your query. Load a Parquet file as shown in the following example:"),sn=c(),E(Hs.$$.fragment),an=c(),Ba=o("p"),Ep=i("To load remote parquet files via HTTP, you can pass the URLs:"),en=c(),E(Ls.$$.fragment),ln=c(),ht=o("h3"),zt=o("a"),el=o("span"),E(Rs.$$.fragment),qp=c(),ll=o("span"),Pp=i("Image folders"),on=c(),Ua=o("p"),Ap=i("\u{1F917} Datasets can also load generic image folders."),nn=c(),Ja=o("p"),Sp=i("The folder structure should look like this:"),rn=c(),E(Ms.$$.fragment),pn=c(),R=o("p"),Dp=i("To load an "),ol=o("code"),Tp=i("imagefolder"),Np=i(" dataset, simply pass the root path of the image folder to the "),nl=o("code"),Cp=i("data_dir"),Ip=i(" kwarg of "),Ya=o("a"),Op=i("datasets.load_dataset()"),Fp=i(", which is a shorthand syntax for "),rl=o("code"),Hp=i("data_files=os.path.join(data_dir, **)"),Lp=i("."),dn=c(),E(zs.$$.fragment),fn=c(),Wa=o("p"),Rp=i("To load remote image folders via HTTP, you can pass the URLs:"),cn=c(),E(Vs.$$.fragment),hn=c(),M=o("p"),Mp=i("The resulting dataset will include an "),il=o("code"),zp=i("image"),Vp=i(" feature, which is a "),pl=o("code"),Bp=i("PIL.Image"),Up=i(" loaded from the image file, and the corresponding "),dl=o("code"),Jp=i("label"),Yp=i(" inferred from the directory structure."),un=c(),ut=o("h2"),Vt=o("a"),fl=o("span"),E(Bs.$$.fragment),Wp=c(),cl=o("span"),Gp=i("In-memory data"),mn=c(),Bt=o("p"),Qp=i("\u{1F917} Datasets will also allow you to create a "),Ga=o("a"),Zp=i("datasets.Dataset"),Kp=i(" directly from in-memory data structures like Python dictionaries and Pandas DataFrames."),gn=c(),mt=o("h3"),Ut=o("a"),hl=o("span"),E(Us.$$.fragment),Xp=c(),ul=o("span"),td=i("Python dictionary"),_n=c(),Jt=o("p"),sd=i("Load Python dictionaries with "),Qa=o("a"),ad=i("datasets.Dataset.from_dict()"),ed=i(":"),vn=c(),E(Js.$$.fragment),$n=c(),gt=o("h3"),Yt=o("a"),ml=o("span"),E(Ys.$$.fragment),ld=c(),gl=o("span"),od=i("Pandas DataFrame"),wn=c(),Wt=o("p"),nd=i("Load Pandas DataFrames with "),Za=o("a"),rd=i("datasets.Dataset.from_pandas()"),id=i(":"),yn=c(),E(Ws.$$.fragment),bn=c(),E(Gt.$$.fragment),jn=c(),_t=o("h2"),Qt=o("a"),_l=o("span"),E(Gs.$$.fragment),pd=c(),vl=o("span"),dd=i("Offline"),xn=c(),Ka=o("p"),fd=i("Even if you don\u2019t have an internet connection, it is still possible to load a dataset. As long as you\u2019ve downloaded a dataset from the Hub or \u{1F917} Datasets GitHub repository before, it should be cached. This means you can reload the dataset from the cache and use it offline."),kn=c(),Q=o("p"),cd=i("If you know you won\u2019t have internet access, you can run \u{1F917} Datasets in full offline mode. This saves time because instead of waiting for the Dataset builder download to time out, \u{1F917} Datasets will look directly in the cache. Set the environment variable "),$l=o("code"),hd=i("HF_DATASETS_OFFLINE"),ud=i(" to "),wl=o("code"),md=i("1"),gd=i(" to enable full offline mode."),En=c(),vt=o("h2"),Zt=o("a"),yl=o("span"),E(Qs.$$.fragment),_d=c(),bl=o("span"),vd=i("Slice splits"),qn=c(),Z=o("p"),$d=i("For even greater control over how to load a split, you can choose to only load specific slices of a split. There are two options for slicing a split: using strings or "),Xa=o("a"),wd=i("datasets.ReadInstruction"),yd=i(". Strings are more compact and readable for simple cases, while "),te=o("a"),bd=i("datasets.ReadInstruction"),jd=i(" is easier to use with variable slicing parameters."),Pn=c(),K=o("p"),xd=i("Concatenate the "),jl=o("code"),kd=i("train"),Ed=i(" and "),xl=o("code"),qd=i("test"),Pd=i(" split by:"),An=c(),E(Zs.$$.fragment),Sn=c(),Kt=o("p"),Ad=i("Select specific rows of the "),kl=o("code"),Sd=i("train"),Dd=i(" split:"),Dn=c(),E(Ks.$$.fragment),Tn=c(),se=o("p"),Td=i("Or select a percentage of the split with:"),Nn=c(),E(Xs.$$.fragment),Cn=c(),ae=o("p"),Nd=i("You can even select a combination of percentages from each split:"),In=c(),E(ta.$$.fragment),On=c(),ee=o("p"),Cd=i("Finally, create cross-validated dataset splits by:"),Fn=c(),E(sa.$$.fragment),Hn=c(),$t=o("h3"),Xt=o("a"),El=o("span"),E(aa.$$.fragment),Id=c(),ql=o("span"),Od=i("Percent slicing and rounding"),Ln=c(),le=o("p"),Fd=i("For datasets where the requested slice boundaries do not divide evenly by 100, the default behavior is to round the boundaries to the nearest integer. As a result, some slices may contain more examples than others as shown in the following example:"),Rn=c(),E(ea.$$.fragment),Mn=c(),ts=o("p"),Hd=i("If you want equal sized splits, use "),Pl=o("code"),Ld=i("pct1_dropremainder"),Rd=i(" rounding instead. This will treat the specified percentage boundaries as multiples of 1%."),zn=c(),E(la.$$.fragment),Vn=c(),E(ss.$$.fragment),Bn=c(),oe=o("a"),Un=c(),wt=o("h2"),as=o("a"),Al=o("span"),E(oa.$$.fragment),Md=c(),Sl=o("span"),zd=i("Troubleshooting"),Jn=c(),ne=o("p"),Vd=i("Sometimes, you may get unexpected results when you load a dataset. In this section, you will learn how to solve two common issues you may encounter when you load a dataset: manually download a dataset, and specify features of a dataset."),Yn=c(),yt=o("h3"),es=o("a"),Dl=o("span"),E(na.$$.fragment),Bd=c(),Tl=o("span"),Ud=i("Manual download"),Wn=c(),z=o("p"),Jd=i("Certain datasets require you to manually download the dataset files due to licensing incompatibility, or if the files are hidden behind a login page. This will cause "),re=o("a"),Yd=i("datasets.load_dataset()"),Wd=i(" to throw an "),Nl=o("code"),Gd=i("AssertionError"),Qd=i(". But \u{1F917} Datasets provides detailed instructions for downloading the missing files. After you have downloaded the files, use the "),Cl=o("code"),Zd=i("data_dir"),Kd=i(" argument to specify the path to the files you just downloaded."),Gn=c(),ls=o("p"),Xd=i("For example, if you try to download a configuration from the "),ra=o("a"),tf=i("MATINF"),sf=i(" dataset:"),Qn=c(),E(ia.$$.fragment),Zn=c(),bt=o("h3"),os=o("a"),Il=o("span"),E(pa.$$.fragment),af=c(),Ol=o("span"),ef=i("Specify features"),Kn=c(),X=o("p"),lf=i("When you create a dataset from local files, the "),ie=o("a"),of=i("datasets.Features"),nf=i(" are automatically inferred by "),da=o("a"),rf=i("Apache Arrow"),pf=i(". However, the features of the dataset may not always align with your expectations or you may want to define the features yourself."),Xn=c(),tt=o("p"),df=i("The following example shows how you can add custom labels with "),pe=o("a"),ff=i("datasets.ClassLabel"),cf=i(". First, define your own labels using the "),de=o("a"),hf=i("datasets.Features"),uf=i(" class:"),tr=c(),E(fa.$$.fragment),sr=c(),st=o("p"),mf=i("Next, specify the "),Fl=o("code"),gf=i("features"),_f=i(" argument in "),fe=o("a"),vf=i("datasets.load_dataset()"),$f=i(" with the features you just created:"),ar=c(),E(ca.$$.fragment),er=c(),ce=o("p"),wf=i("Now when you look at your dataset features, you can see it uses the custom labels you defined:"),lr=c(),E(ha.$$.fragment),or=c(),jt=o("h2"),ns=o("a"),Hl=o("span"),E(ua.$$.fragment),yf=c(),Ll=o("span"),bf=i("Metrics"),nr=c(),he=o("p"),jf=i("When the metric you want to use is not supported by \u{1F917} Datasets, you can write and use your own metric script. Load your metric by providing the path to your local metric loading script:"),rr=c(),E(ma.$$.fragment),ir=c(),E(rs.$$.fragment),pr=c(),xt=o("h3"),is=o("a"),Rl=o("span"),E(ga.$$.fragment),xf=c(),Ml=o("span"),kf=i("Load configurations"),dr=c(),ue=o("p"),Ef=i("It is possible for a metric to have different configurations. The configurations are stored in the [\u2018datasets.Metric.config_name\u2019] attribute. When you load a metric, provide the configuration name as shown in the following:"),fr=c(),E(_a.$$.fragment),cr=c(),kt=o("h3"),ps=o("a"),zl=o("span"),E(va.$$.fragment),qf=c(),Vl=o("span"),Pf=i("Distributed setup"),hr=c(),me=o("p"),Af=i("When you work in a distributed or parallel processing environment, loading and computing a metric can be tricky because these processes are executed in parallel on separate subsets of the data. \u{1F917} Datasets supports distributed usage with a few additional arguments when you load a metric."),ur=c(),ge=o("p"),Sf=i("For example, imagine you are training and evaluating on eight parallel processes. Here\u2019s how you would load a metric in this distributed setting:"),mr=c(),at=o("ol"),Bl=o("li"),$a=o("p"),Df=i("Define the total number of processes with the "),Ul=o("code"),Tf=i("num_process"),Nf=i(" argument."),Cf=c(),Jl=o("li"),Et=o("p"),If=i("Set the process "),Yl=o("code"),Of=i("rank"),Ff=i(" as an integer between zero and "),Wl=o("code"),Hf=i("num_process - 1"),Lf=i("."),Rf=c(),Gl=o("li"),wa=o("p"),Mf=i("Load your metric with "),_e=o("a"),zf=i("datasets.load_metric()"),Vf=i(" with these arguments:"),gr=c(),E(ya.$$.fragment),_r=c(),E(ds.$$.fragment),vr=c(),fs=o("p"),Bf=i("In some instances, you may be simultaneously running multiple independent distributed evaluations on the same server and files. To avoid any conflicts, it is important to provide an "),Ql=o("code"),Uf=i("experiment_id"),Jf=i(" to distinguish the separate evaluations:"),$r=c(),E(ba.$$.fragment),this.h()},l(t){const e=Ju('[data-svelte="svelte-1phssyn"]',document.head);d=n(e,"META",{name:!0,content:!0}),e.forEach(s),m=h(t),u=n(t,"H1",{class:!0});var ja=l(u);g=n(ja,"A",{id:!0,class:!0,href:!0});var Zl=l(g);v=n(Zl,"SPAN",{});var Kl=l(v);P(_.$$.fragment,Kl),Kl.forEach(s),Zl.forEach(s),$=h(ja),y=n(ja,"SPAN",{});var Xl=l(y);w=p(Xl,"Load"),Xl.forEach(s),ja.forEach(s),A=h(t),S=n(t,"P",{});var to=l(S);D=p(to,"You have already seen how to load a dataset from the Hugging Face Hub. But datasets are stored in a variety of places, and sometimes you won\u2019t find the one you want on the Hub. A dataset can be on disk on your local machine, in a Github repository, and in in-memory data structures like Python dictionaries and Pandas DataFrames. Wherever your dataset may be stored, \u{1F917} Datasets provides a way for you to load and use it for training."),to.forEach(s),Y=h(t),C=n(t,"P",{});var so=l(C);N=p(so,"This guide will show you how to load a dataset from:"),so.forEach(s),U=h(t),O=n(t,"UL",{});var et=l(O);J=n(et,"LI",{});var Wf=l(J);F=p(Wf,"The Hub without a dataset loading script"),Wf.forEach(s),xa=h(et),qt=n(et,"LI",{});var Gf=l(qt);ka=p(Gf,"Local files"),Gf.forEach(s),Ea=h(et),Pt=n(et,"LI",{});var Qf=l(Pt);ei=p(Qf,"In-memory data"),Qf.forEach(s),li=h(et),Ae=n(et,"LI",{});var Zf=l(Ae);oi=p(Zf,"Offline"),Zf.forEach(s),ni=h(et),Se=n(et,"LI",{});var Kf=l(Se);ri=p(Kf,"A specific slice of a split"),Kf.forEach(s),et.forEach(s),ro=h(t),qa=n(t,"P",{});var Xf=l(qa);ii=p(Xf,"You will also learn how to troubleshoot common errors, and how to load specific configurations of a metric."),Xf.forEach(s),io=h(t),Pa=n(t,"A",{id:!0}),l(Pa).forEach(s),po=h(t),rt=n(t,"H2",{class:!0});var yr=l(rt);At=n(yr,"A",{id:!0,class:!0,href:!0});var tc=l(At);De=n(tc,"SPAN",{});var sc=l(De);P(us.$$.fragment,sc),sc.forEach(s),tc.forEach(s),pi=h(yr),Te=n(yr,"SPAN",{});var ac=l(Te);di=p(ac,"Hugging Face Hub"),ac.forEach(s),yr.forEach(s),fo=h(t),St=n(t,"P",{});var br=l(St);fi=p(br,"In the tutorial, you learned how to load a dataset from the Hub. This method relies on a dataset loading script that downloads and builds the dataset. However, you can also load a dataset from any dataset repository on the Hub "),Ne=n(br,"STRONG",{});var ec=l(Ne);ci=p(ec,"without"),ec.forEach(s),hi=p(br," a loading script!"),br.forEach(s),co=h(t),W=n(t,"P",{});var ve=l(W);ui=p(ve,"First, create a dataset repository and upload your data files. Then you can use "),Aa=n(ve,"A",{href:!0});var lc=l(Aa);mi=p(lc,"datasets.load_dataset()"),lc.forEach(s),gi=p(ve," like you learned in the tutorial. For example, load the files from this "),ms=n(ve,"A",{href:!0,rel:!0});var oc=l(ms);_i=p(oc,"demo repository"),oc.forEach(s),vi=p(ve," by providing the repository namespace and dataset name:"),ve.forEach(s),ho=h(t),P(gs.$$.fragment,t),uo=h(t),Sa=n(t,"P",{});var nc=l(Sa);$i=p(nc,"This dataset repository contains CSV files, and this code loads all the data from the CSV files."),nc.forEach(s),mo=h(t),Dt=n(t,"P",{});var jr=l(Dt);wi=p(jr,"Some datasets may have more than one version, based on Git tags, branches or commits. Use the "),Ce=n(jr,"CODE",{});var rc=l(Ce);yi=p(rc,"revision"),rc.forEach(s),bi=p(jr," flag to specify which dataset version you want to load:"),jr.forEach(s),go=h(t),P(_s.$$.fragment,t),_o=h(t),P(Tt.$$.fragment,t),vo=h(t),H=n(t,"P",{});var V=l(H);ji=p(V,"If the dataset doesn\u2019t have a dataset loading script, then by default, all the data will be loaded in the "),Ie=n(V,"CODE",{});var ic=l(Ie);xi=p(ic,"train"),ic.forEach(s),ki=p(V," split. Use the "),Oe=n(V,"CODE",{});var pc=l(Oe);Ei=p(pc,"data_files"),pc.forEach(s),qi=p(V," parameter to map data files to splits like "),Fe=n(V,"CODE",{});var dc=l(Fe);Pi=p(dc,"train"),dc.forEach(s),Ai=p(V,", "),He=n(V,"CODE",{});var fc=l(He);Si=p(fc,"validation"),fc.forEach(s),Di=p(V," and "),Le=n(V,"CODE",{});var cc=l(Le);Ti=p(cc,"test"),cc.forEach(s),Ni=p(V,":"),V.forEach(s),$o=h(t),P(vs.$$.fragment,t),wo=h(t),P(Nt.$$.fragment,t),yo=h(t),G=n(t,"P",{});var $e=l(G);Ci=p($e,"You can also load a specific subset of the files with the "),Re=n($e,"CODE",{});var hc=l(Re);Ii=p(hc,"data_files"),hc.forEach(s),Oi=p($e," parameter. The example below loads files from the "),$s=n($e,"A",{href:!0,rel:!0});var uc=l($s);Fi=p(uc,"C4 dataset"),uc.forEach(s),Hi=p($e,":"),$e.forEach(s),bo=h(t),P(ws.$$.fragment,t),jo=h(t),Ct=n(t,"P",{});var xr=l(Ct);Li=p(xr,"Specify a custom split with the "),Me=n(xr,"CODE",{});var mc=l(Me);Ri=p(mc,"split"),mc.forEach(s),Mi=p(xr," parameter:"),xr.forEach(s),xo=h(t),P(ys.$$.fragment,t),ko=h(t),it=n(t,"H2",{class:!0});var kr=l(it);It=n(kr,"A",{id:!0,class:!0,href:!0});var gc=l(It);ze=n(gc,"SPAN",{});var _c=l(ze);P(bs.$$.fragment,_c),_c.forEach(s),gc.forEach(s),zi=h(kr),Ve=n(kr,"SPAN",{});var vc=l(Ve);Vi=p(vc,"Local and remote files"),vc.forEach(s),kr.forEach(s),Eo=h(t),L=n(t,"P",{});var B=l(L);Bi=p(B,"\u{1F917} Datasets can be loaded from local files stored on your computer, and also from remote files. The datasets are most likely stored as a "),Be=n(B,"CODE",{});var $c=l(Be);Ui=p($c,"csv"),$c.forEach(s),Ji=p(B,", "),Ue=n(B,"CODE",{});var wc=l(Ue);Yi=p(wc,"json"),wc.forEach(s),Wi=p(B,", "),Je=n(B,"CODE",{});var yc=l(Je);Gi=p(yc,"txt"),yc.forEach(s),Qi=p(B," or "),Ye=n(B,"CODE",{});var bc=l(Ye);Zi=p(bc,"parquet"),bc.forEach(s),Ki=p(B," file. The "),Da=n(B,"A",{href:!0});var jc=l(Da);Xi=p(jc,"datasets.load_dataset()"),jc.forEach(s),tp=p(B," method is able to load each of these file types."),B.forEach(s),qo=h(t),pt=n(t,"H3",{class:!0});var Er=l(pt);Ot=n(Er,"A",{id:!0,class:!0,href:!0});var xc=l(Ot);We=n(xc,"SPAN",{});var kc=l(We);P(js.$$.fragment,kc),kc.forEach(s),xc.forEach(s),sp=h(Er),Ge=n(Er,"SPAN",{});var Ec=l(Ge);ap=p(Ec,"CSV"),Ec.forEach(s),Er.forEach(s),Po=h(t),Ta=n(t,"P",{});var qc=l(Ta);ep=p(qc,"\u{1F917} Datasets can read a dataset made up of one or several CSV files:"),qc.forEach(s),Ao=h(t),P(xs.$$.fragment,t),So=h(t),Na=n(t,"P",{});var Pc=l(Na);lp=p(Pc,"If you have more than one CSV file:"),Pc.forEach(s),Do=h(t),P(ks.$$.fragment,t),To=h(t),Ca=n(t,"P",{});var Ac=l(Ca);op=p(Ac,"You can also map the training and test splits to specific CSV files:"),Ac.forEach(s),No=h(t),P(Es.$$.fragment,t),Co=h(t),Ia=n(t,"P",{});var Sc=l(Ia);np=p(Sc,"To load remote CSV files via HTTP, you can pass the URLs:"),Sc.forEach(s),Io=h(t),P(qs.$$.fragment,t),Oo=h(t),Oa=n(t,"P",{});var Dc=l(Oa);rp=p(Dc,"To load zipped CSV files:"),Dc.forEach(s),Fo=h(t),P(Ps.$$.fragment,t),Ho=h(t),dt=n(t,"H3",{class:!0});var qr=l(dt);Ft=n(qr,"A",{id:!0,class:!0,href:!0});var Tc=l(Ft);Qe=n(Tc,"SPAN",{});var Nc=l(Qe);P(As.$$.fragment,Nc),Nc.forEach(s),Tc.forEach(s),ip=h(qr),Ze=n(qr,"SPAN",{});var Cc=l(Ze);pp=p(Cc,"JSON"),Cc.forEach(s),qr.forEach(s),Lo=h(t),Ht=n(t,"P",{});var Pr=l(Ht);dp=p(Pr,"JSON files are loaded directly with "),Fa=n(Pr,"A",{href:!0});var Ic=l(Fa);fp=p(Ic,"datasets.load_dataset()"),Ic.forEach(s),cp=p(Pr," as shown below:"),Pr.forEach(s),Ro=h(t),P(Ss.$$.fragment,t),Mo=h(t),Ha=n(t,"P",{});var Oc=l(Ha);hp=p(Oc,"JSON files can have diverse formats, but we think the most efficient format is to have multiple JSON objects; each line represents an individual row of data. For example:"),Oc.forEach(s),zo=h(t),P(Ds.$$.fragment,t),Vo=h(t),Lt=n(t,"P",{});var Ar=l(Lt);up=p(Ar,"Another JSON format you may encounter is a nested field, in which case you will need to specify the "),Ke=n(Ar,"CODE",{});var Fc=l(Ke);mp=p(Fc,"field"),Fc.forEach(s),gp=p(Ar," argument as shown in the following:"),Ar.forEach(s),Bo=h(t),P(Ts.$$.fragment,t),Uo=h(t),La=n(t,"P",{});var Hc=l(La);_p=p(Hc,"To load remote JSON files via HTTP, you can pass the URLs:"),Hc.forEach(s),Jo=h(t),P(Ns.$$.fragment,t),Yo=h(t),Ra=n(t,"P",{});var Lc=l(Ra);vp=p(Lc,"While these are the most common JSON formats, you will see other datasets that are formatted differently. \u{1F917} Datasets recognizes these other formats, and will fallback accordingly on the Python JSON loading methods to handle them."),Lc.forEach(s),Wo=h(t),ft=n(t,"H3",{class:!0});var Sr=l(ft);Rt=n(Sr,"A",{id:!0,class:!0,href:!0});var Rc=l(Rt);Xe=n(Rc,"SPAN",{});var Mc=l(Xe);P(Cs.$$.fragment,Mc),Mc.forEach(s),Rc.forEach(s),$p=h(Sr),tl=n(Sr,"SPAN",{});var zc=l(tl);wp=p(zc,"Text files"),zc.forEach(s),Sr.forEach(s),Go=h(t),Ma=n(t,"P",{});var Vc=l(Ma);yp=p(Vc,"Text files are one of the most common file types for storing a dataset. \u{1F917} Datasets will read the text file line by line to build the dataset."),Vc.forEach(s),Qo=h(t),P(Is.$$.fragment,t),Zo=h(t),za=n(t,"P",{});var Bc=l(za);bp=p(Bc,"To load remote TXT files via HTTP, you can pass the URLs:"),Bc.forEach(s),Ko=h(t),P(Os.$$.fragment,t),Xo=h(t),ct=n(t,"H3",{class:!0});var Dr=l(ct);Mt=n(Dr,"A",{id:!0,class:!0,href:!0});var Uc=l(Mt);sl=n(Uc,"SPAN",{});var Jc=l(sl);P(Fs.$$.fragment,Jc),Jc.forEach(s),Uc.forEach(s),jp=h(Dr),al=n(Dr,"SPAN",{});var Yc=l(al);xp=p(Yc,"Parquet"),Yc.forEach(s),Dr.forEach(s),tn=h(t),Va=n(t,"P",{});var Wc=l(Va);kp=p(Wc,"Parquet files are stored in a columnar format unlike row-based files like CSV. Large datasets may be stored in a Parquet file because it is more efficient, and faster at returning your query. Load a Parquet file as shown in the following example:"),Wc.forEach(s),sn=h(t),P(Hs.$$.fragment,t),an=h(t),Ba=n(t,"P",{});var Gc=l(Ba);Ep=p(Gc,"To load remote parquet files via HTTP, you can pass the URLs:"),Gc.forEach(s),en=h(t),P(Ls.$$.fragment,t),ln=h(t),ht=n(t,"H3",{class:!0});var Tr=l(ht);zt=n(Tr,"A",{id:!0,class:!0,href:!0});var Qc=l(zt);el=n(Qc,"SPAN",{});var Zc=l(el);P(Rs.$$.fragment,Zc),Zc.forEach(s),Qc.forEach(s),qp=h(Tr),ll=n(Tr,"SPAN",{});var Kc=l(ll);Pp=p(Kc,"Image folders"),Kc.forEach(s),Tr.forEach(s),on=h(t),Ua=n(t,"P",{});var Xc=l(Ua);Ap=p(Xc,"\u{1F917} Datasets can also load generic image folders."),Xc.forEach(s),nn=h(t),Ja=n(t,"P",{});var th=l(Ja);Sp=p(th,"The folder structure should look like this:"),th.forEach(s),rn=h(t),P(Ms.$$.fragment,t),pn=h(t),R=n(t,"P",{});var lt=l(R);Dp=p(lt,"To load an "),ol=n(lt,"CODE",{});var sh=l(ol);Tp=p(sh,"imagefolder"),sh.forEach(s),Np=p(lt," dataset, simply pass the root path of the image folder to the "),nl=n(lt,"CODE",{});var ah=l(nl);Cp=p(ah,"data_dir"),ah.forEach(s),Ip=p(lt," kwarg of "),Ya=n(lt,"A",{href:!0});var eh=l(Ya);Op=p(eh,"datasets.load_dataset()"),eh.forEach(s),Fp=p(lt,", which is a shorthand syntax for "),rl=n(lt,"CODE",{});var lh=l(rl);Hp=p(lh,"data_files=os.path.join(data_dir, **)"),lh.forEach(s),Lp=p(lt,"."),lt.forEach(s),dn=h(t),P(zs.$$.fragment,t),fn=h(t),Wa=n(t,"P",{});var oh=l(Wa);Rp=p(oh,"To load remote image folders via HTTP, you can pass the URLs:"),oh.forEach(s),cn=h(t),P(Vs.$$.fragment,t),hn=h(t),M=n(t,"P",{});var cs=l(M);Mp=p(cs,"The resulting dataset will include an "),il=n(cs,"CODE",{});var nh=l(il);zp=p(nh,"image"),nh.forEach(s),Vp=p(cs," feature, which is a "),pl=n(cs,"CODE",{});var rh=l(pl);Bp=p(rh,"PIL.Image"),rh.forEach(s),Up=p(cs," loaded from the image file, and the corresponding "),dl=n(cs,"CODE",{});var ih=l(dl);Jp=p(ih,"label"),ih.forEach(s),Yp=p(cs," inferred from the directory structure."),cs.forEach(s),un=h(t),ut=n(t,"H2",{class:!0});var Nr=l(ut);Vt=n(Nr,"A",{id:!0,class:!0,href:!0});var ph=l(Vt);fl=n(ph,"SPAN",{});var dh=l(fl);P(Bs.$$.fragment,dh),dh.forEach(s),ph.forEach(s),Wp=h(Nr),cl=n(Nr,"SPAN",{});var fh=l(cl);Gp=p(fh,"In-memory data"),fh.forEach(s),Nr.forEach(s),mn=h(t),Bt=n(t,"P",{});var Cr=l(Bt);Qp=p(Cr,"\u{1F917} Datasets will also allow you to create a "),Ga=n(Cr,"A",{href:!0});var ch=l(Ga);Zp=p(ch,"datasets.Dataset"),ch.forEach(s),Kp=p(Cr," directly from in-memory data structures like Python dictionaries and Pandas DataFrames."),Cr.forEach(s),gn=h(t),mt=n(t,"H3",{class:!0});var Ir=l(mt);Ut=n(Ir,"A",{id:!0,class:!0,href:!0});var hh=l(Ut);hl=n(hh,"SPAN",{});var uh=l(hl);P(Us.$$.fragment,uh),uh.forEach(s),hh.forEach(s),Xp=h(Ir),ul=n(Ir,"SPAN",{});var mh=l(ul);td=p(mh,"Python dictionary"),mh.forEach(s),Ir.forEach(s),_n=h(t),Jt=n(t,"P",{});var Or=l(Jt);sd=p(Or,"Load Python dictionaries with "),Qa=n(Or,"A",{href:!0});var gh=l(Qa);ad=p(gh,"datasets.Dataset.from_dict()"),gh.forEach(s),ed=p(Or,":"),Or.forEach(s),vn=h(t),P(Js.$$.fragment,t),$n=h(t),gt=n(t,"H3",{class:!0});var Fr=l(gt);Yt=n(Fr,"A",{id:!0,class:!0,href:!0});var _h=l(Yt);ml=n(_h,"SPAN",{});var vh=l(ml);P(Ys.$$.fragment,vh),vh.forEach(s),_h.forEach(s),ld=h(Fr),gl=n(Fr,"SPAN",{});var $h=l(gl);od=p($h,"Pandas DataFrame"),$h.forEach(s),Fr.forEach(s),wn=h(t),Wt=n(t,"P",{});var Hr=l(Wt);nd=p(Hr,"Load Pandas DataFrames with "),Za=n(Hr,"A",{href:!0});var wh=l(Za);rd=p(wh,"datasets.Dataset.from_pandas()"),wh.forEach(s),id=p(Hr,":"),Hr.forEach(s),yn=h(t),P(Ws.$$.fragment,t),bn=h(t),P(Gt.$$.fragment,t),jn=h(t),_t=n(t,"H2",{class:!0});var Lr=l(_t);Qt=n(Lr,"A",{id:!0,class:!0,href:!0});var yh=l(Qt);_l=n(yh,"SPAN",{});var bh=l(_l);P(Gs.$$.fragment,bh),bh.forEach(s),yh.forEach(s),pd=h(Lr),vl=n(Lr,"SPAN",{});var jh=l(vl);dd=p(jh,"Offline"),jh.forEach(s),Lr.forEach(s),xn=h(t),Ka=n(t,"P",{});var xh=l(Ka);fd=p(xh,"Even if you don\u2019t have an internet connection, it is still possible to load a dataset. As long as you\u2019ve downloaded a dataset from the Hub or \u{1F917} Datasets GitHub repository before, it should be cached. This means you can reload the dataset from the cache and use it offline."),xh.forEach(s),kn=h(t),Q=n(t,"P",{});var we=l(Q);cd=p(we,"If you know you won\u2019t have internet access, you can run \u{1F917} Datasets in full offline mode. This saves time because instead of waiting for the Dataset builder download to time out, \u{1F917} Datasets will look directly in the cache. Set the environment variable "),$l=n(we,"CODE",{});var kh=l($l);hd=p(kh,"HF_DATASETS_OFFLINE"),kh.forEach(s),ud=p(we," to "),wl=n(we,"CODE",{});var Eh=l(wl);md=p(Eh,"1"),Eh.forEach(s),gd=p(we," to enable full offline mode."),we.forEach(s),En=h(t),vt=n(t,"H2",{class:!0});var Rr=l(vt);Zt=n(Rr,"A",{id:!0,class:!0,href:!0});var qh=l(Zt);yl=n(qh,"SPAN",{});var Ph=l(yl);P(Qs.$$.fragment,Ph),Ph.forEach(s),qh.forEach(s),_d=h(Rr),bl=n(Rr,"SPAN",{});var Ah=l(bl);vd=p(Ah,"Slice splits"),Ah.forEach(s),Rr.forEach(s),qn=h(t),Z=n(t,"P",{});var ye=l(Z);$d=p(ye,"For even greater control over how to load a split, you can choose to only load specific slices of a split. There are two options for slicing a split: using strings or "),Xa=n(ye,"A",{href:!0});var Sh=l(Xa);wd=p(Sh,"datasets.ReadInstruction"),Sh.forEach(s),yd=p(ye,". Strings are more compact and readable for simple cases, while "),te=n(ye,"A",{href:!0});var Dh=l(te);bd=p(Dh,"datasets.ReadInstruction"),Dh.forEach(s),jd=p(ye," is easier to use with variable slicing parameters."),ye.forEach(s),Pn=h(t),K=n(t,"P",{});var be=l(K);xd=p(be,"Concatenate the "),jl=n(be,"CODE",{});var Th=l(jl);kd=p(Th,"train"),Th.forEach(s),Ed=p(be," and "),xl=n(be,"CODE",{});var Nh=l(xl);qd=p(Nh,"test"),Nh.forEach(s),Pd=p(be," split by:"),be.forEach(s),An=h(t),P(Zs.$$.fragment,t),Sn=h(t),Kt=n(t,"P",{});var Mr=l(Kt);Ad=p(Mr,"Select specific rows of the "),kl=n(Mr,"CODE",{});var Ch=l(kl);Sd=p(Ch,"train"),Ch.forEach(s),Dd=p(Mr," split:"),Mr.forEach(s),Dn=h(t),P(Ks.$$.fragment,t),Tn=h(t),se=n(t,"P",{});var Ih=l(se);Td=p(Ih,"Or select a percentage of the split with:"),Ih.forEach(s),Nn=h(t),P(Xs.$$.fragment,t),Cn=h(t),ae=n(t,"P",{});var Oh=l(ae);Nd=p(Oh,"You can even select a combination of percentages from each split:"),Oh.forEach(s),In=h(t),P(ta.$$.fragment,t),On=h(t),ee=n(t,"P",{});var Fh=l(ee);Cd=p(Fh,"Finally, create cross-validated dataset splits by:"),Fh.forEach(s),Fn=h(t),P(sa.$$.fragment,t),Hn=h(t),$t=n(t,"H3",{class:!0});var zr=l($t);Xt=n(zr,"A",{id:!0,class:!0,href:!0});var Hh=l(Xt);El=n(Hh,"SPAN",{});var Lh=l(El);P(aa.$$.fragment,Lh),Lh.forEach(s),Hh.forEach(s),Id=h(zr),ql=n(zr,"SPAN",{});var Rh=l(ql);Od=p(Rh,"Percent slicing and rounding"),Rh.forEach(s),zr.forEach(s),Ln=h(t),le=n(t,"P",{});var Mh=l(le);Fd=p(Mh,"For datasets where the requested slice boundaries do not divide evenly by 100, the default behavior is to round the boundaries to the nearest integer. As a result, some slices may contain more examples than others as shown in the following example:"),Mh.forEach(s),Rn=h(t),P(ea.$$.fragment,t),Mn=h(t),ts=n(t,"P",{});var Vr=l(ts);Hd=p(Vr,"If you want equal sized splits, use "),Pl=n(Vr,"CODE",{});var zh=l(Pl);Ld=p(zh,"pct1_dropremainder"),zh.forEach(s),Rd=p(Vr," rounding instead. This will treat the specified percentage boundaries as multiples of 1%."),Vr.forEach(s),zn=h(t),P(la.$$.fragment,t),Vn=h(t),P(ss.$$.fragment,t),Bn=h(t),oe=n(t,"A",{id:!0}),l(oe).forEach(s),Un=h(t),wt=n(t,"H2",{class:!0});var Br=l(wt);as=n(Br,"A",{id:!0,class:!0,href:!0});var Vh=l(as);Al=n(Vh,"SPAN",{});var Bh=l(Al);P(oa.$$.fragment,Bh),Bh.forEach(s),Vh.forEach(s),Md=h(Br),Sl=n(Br,"SPAN",{});var Uh=l(Sl);zd=p(Uh,"Troubleshooting"),Uh.forEach(s),Br.forEach(s),Jn=h(t),ne=n(t,"P",{});var Jh=l(ne);Vd=p(Jh,"Sometimes, you may get unexpected results when you load a dataset. In this section, you will learn how to solve two common issues you may encounter when you load a dataset: manually download a dataset, and specify features of a dataset."),Jh.forEach(s),Yn=h(t),yt=n(t,"H3",{class:!0});var Ur=l(yt);es=n(Ur,"A",{id:!0,class:!0,href:!0});var Yh=l(es);Dl=n(Yh,"SPAN",{});var Wh=l(Dl);P(na.$$.fragment,Wh),Wh.forEach(s),Yh.forEach(s),Bd=h(Ur),Tl=n(Ur,"SPAN",{});var Gh=l(Tl);Ud=p(Gh,"Manual download"),Gh.forEach(s),Ur.forEach(s),Wn=h(t),z=n(t,"P",{});var hs=l(z);Jd=p(hs,"Certain datasets require you to manually download the dataset files due to licensing incompatibility, or if the files are hidden behind a login page. This will cause "),re=n(hs,"A",{href:!0});var Qh=l(re);Yd=p(Qh,"datasets.load_dataset()"),Qh.forEach(s),Wd=p(hs," to throw an "),Nl=n(hs,"CODE",{});var Zh=l(Nl);Gd=p(Zh,"AssertionError"),Zh.forEach(s),Qd=p(hs,". But \u{1F917} Datasets provides detailed instructions for downloading the missing files. After you have downloaded the files, use the "),Cl=n(hs,"CODE",{});var Kh=l(Cl);Zd=p(Kh,"data_dir"),Kh.forEach(s),Kd=p(hs," argument to specify the path to the files you just downloaded."),hs.forEach(s),Gn=h(t),ls=n(t,"P",{});var Jr=l(ls);Xd=p(Jr,"For example, if you try to download a configuration from the "),ra=n(Jr,"A",{href:!0,rel:!0});var Xh=l(ra);tf=p(Xh,"MATINF"),Xh.forEach(s),sf=p(Jr," dataset:"),Jr.forEach(s),Qn=h(t),P(ia.$$.fragment,t),Zn=h(t),bt=n(t,"H3",{class:!0});var Yr=l(bt);os=n(Yr,"A",{id:!0,class:!0,href:!0});var tu=l(os);Il=n(tu,"SPAN",{});var su=l(Il);P(pa.$$.fragment,su),su.forEach(s),tu.forEach(s),af=h(Yr),Ol=n(Yr,"SPAN",{});var au=l(Ol);ef=p(au,"Specify features"),au.forEach(s),Yr.forEach(s),Kn=h(t),X=n(t,"P",{});var je=l(X);lf=p(je,"When you create a dataset from local files, the "),ie=n(je,"A",{href:!0});var eu=l(ie);of=p(eu,"datasets.Features"),eu.forEach(s),nf=p(je," are automatically inferred by "),da=n(je,"A",{href:!0,rel:!0});var lu=l(da);rf=p(lu,"Apache Arrow"),lu.forEach(s),pf=p(je,". However, the features of the dataset may not always align with your expectations or you may want to define the features yourself."),je.forEach(s),Xn=h(t),tt=n(t,"P",{});var xe=l(tt);df=p(xe,"The following example shows how you can add custom labels with "),pe=n(xe,"A",{href:!0});var ou=l(pe);ff=p(ou,"datasets.ClassLabel"),ou.forEach(s),cf=p(xe,". First, define your own labels using the "),de=n(xe,"A",{href:!0});var nu=l(de);hf=p(nu,"datasets.Features"),nu.forEach(s),uf=p(xe," class:"),xe.forEach(s),tr=h(t),P(fa.$$.fragment,t),sr=h(t),st=n(t,"P",{});var ke=l(st);mf=p(ke,"Next, specify the "),Fl=n(ke,"CODE",{});var ru=l(Fl);gf=p(ru,"features"),ru.forEach(s),_f=p(ke," argument in "),fe=n(ke,"A",{href:!0});var iu=l(fe);vf=p(iu,"datasets.load_dataset()"),iu.forEach(s),$f=p(ke," with the features you just created:"),ke.forEach(s),ar=h(t),P(ca.$$.fragment,t),er=h(t),ce=n(t,"P",{});var pu=l(ce);wf=p(pu,"Now when you look at your dataset features, you can see it uses the custom labels you defined:"),pu.forEach(s),lr=h(t),P(ha.$$.fragment,t),or=h(t),jt=n(t,"H2",{class:!0});var Wr=l(jt);ns=n(Wr,"A",{id:!0,class:!0,href:!0});var du=l(ns);Hl=n(du,"SPAN",{});var fu=l(Hl);P(ua.$$.fragment,fu),fu.forEach(s),du.forEach(s),yf=h(Wr),Ll=n(Wr,"SPAN",{});var cu=l(Ll);bf=p(cu,"Metrics"),cu.forEach(s),Wr.forEach(s),nr=h(t),he=n(t,"P",{});var hu=l(he);jf=p(hu,"When the metric you want to use is not supported by \u{1F917} Datasets, you can write and use your own metric script. Load your metric by providing the path to your local metric loading script:"),hu.forEach(s),rr=h(t),P(ma.$$.fragment,t),ir=h(t),P(rs.$$.fragment,t),pr=h(t),xt=n(t,"H3",{class:!0});var Gr=l(xt);is=n(Gr,"A",{id:!0,class:!0,href:!0});var uu=l(is);Rl=n(uu,"SPAN",{});var mu=l(Rl);P(ga.$$.fragment,mu),mu.forEach(s),uu.forEach(s),xf=h(Gr),Ml=n(Gr,"SPAN",{});var gu=l(Ml);kf=p(gu,"Load configurations"),gu.forEach(s),Gr.forEach(s),dr=h(t),ue=n(t,"P",{});var _u=l(ue);Ef=p(_u,"It is possible for a metric to have different configurations. The configurations are stored in the [\u2018datasets.Metric.config_name\u2019] attribute. When you load a metric, provide the configuration name as shown in the following:"),_u.forEach(s),fr=h(t),P(_a.$$.fragment,t),cr=h(t),kt=n(t,"H3",{class:!0});var Qr=l(kt);ps=n(Qr,"A",{id:!0,class:!0,href:!0});var vu=l(ps);zl=n(vu,"SPAN",{});var $u=l(zl);P(va.$$.fragment,$u),$u.forEach(s),vu.forEach(s),qf=h(Qr),Vl=n(Qr,"SPAN",{});var wu=l(Vl);Pf=p(wu,"Distributed setup"),wu.forEach(s),Qr.forEach(s),hr=h(t),me=n(t,"P",{});var yu=l(me);Af=p(yu,"When you work in a distributed or parallel processing environment, loading and computing a metric can be tricky because these processes are executed in parallel on separate subsets of the data. \u{1F917} Datasets supports distributed usage with a few additional arguments when you load a metric."),yu.forEach(s),ur=h(t),ge=n(t,"P",{});var bu=l(ge);Sf=p(bu,"For example, imagine you are training and evaluating on eight parallel processes. Here\u2019s how you would load a metric in this distributed setting:"),bu.forEach(s),mr=h(t),at=n(t,"OL",{});var Ee=l(at);Bl=n(Ee,"LI",{});var ju=l(Bl);$a=n(ju,"P",{});var Zr=l($a);Df=p(Zr,"Define the total number of processes with the "),Ul=n(Zr,"CODE",{});var xu=l(Ul);Tf=p(xu,"num_process"),xu.forEach(s),Nf=p(Zr," argument."),Zr.forEach(s),ju.forEach(s),Cf=h(Ee),Jl=n(Ee,"LI",{});var ku=l(Jl);Et=n(ku,"P",{});var qe=l(Et);If=p(qe,"Set the process "),Yl=n(qe,"CODE",{});var Eu=l(Yl);Of=p(Eu,"rank"),Eu.forEach(s),Ff=p(qe," as an integer between zero and "),Wl=n(qe,"CODE",{});var qu=l(Wl);Hf=p(qu,"num_process - 1"),qu.forEach(s),Lf=p(qe,"."),qe.forEach(s),ku.forEach(s),Rf=h(Ee),Gl=n(Ee,"LI",{});var Pu=l(Gl);wa=n(Pu,"P",{});var Kr=l(wa);Mf=p(Kr,"Load your metric with "),_e=n(Kr,"A",{href:!0});var Au=l(_e);zf=p(Au,"datasets.load_metric()"),Au.forEach(s),Vf=p(Kr," with these arguments:"),Kr.forEach(s),Pu.forEach(s),Ee.forEach(s),gr=h(t),P(ya.$$.fragment,t),_r=h(t),P(ds.$$.fragment,t),vr=h(t),fs=n(t,"P",{});var Xr=l(fs);Bf=p(Xr,"In some instances, you may be simultaneously running multiple independent distributed evaluations on the same server and files. To avoid any conflicts, it is important to provide an "),Ql=n(Xr,"CODE",{});var Su=l(Ql);Uf=p(Su,"experiment_id"),Su.forEach(s),Jf=p(Xr," to distinguish the separate evaluations:"),Xr.forEach(s),$r=h(t),P(ba.$$.fragment,t),this.h()},h(){f(d,"name","hf:doc:metadata"),f(d,"content",JSON.stringify(cm)),f(g,"id","load"),f(g,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(g,"href","#load"),f(u,"class","relative group"),f(Pa,"id","load-from-the-hub"),f(At,"id","hugging-face-hub"),f(At,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(At,"href","#hugging-face-hub"),f(rt,"class","relative group"),f(Aa,"href","/docs/datasets/pr_3880/en/package_reference/loading_methods#datasets.load_dataset"),f(ms,"href","https://huggingface.co/datasets/lhoestq/demo1"),f(ms,"rel","nofollow"),f($s,"href","https://huggingface.co/datasets/allenai/c4"),f($s,"rel","nofollow"),f(It,"id","local-and-remote-files"),f(It,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(It,"href","#local-and-remote-files"),f(it,"class","relative group"),f(Da,"href","/docs/datasets/pr_3880/en/package_reference/loading_methods#datasets.load_dataset"),f(Ot,"id","csv"),f(Ot,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(Ot,"href","#csv"),f(pt,"class","relative group"),f(Ft,"id","json"),f(Ft,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(Ft,"href","#json"),f(dt,"class","relative group"),f(Fa,"href","/docs/datasets/pr_3880/en/package_reference/loading_methods#datasets.load_dataset"),f(Rt,"id","text-files"),f(Rt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(Rt,"href","#text-files"),f(ft,"class","relative group"),f(Mt,"id","parquet"),f(Mt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(Mt,"href","#parquet"),f(ct,"class","relative group"),f(zt,"id","image-folders"),f(zt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(zt,"href","#image-folders"),f(ht,"class","relative group"),f(Ya,"href","/docs/datasets/pr_3880/en/package_reference/loading_methods#datasets.load_dataset"),f(Vt,"id","inmemory-data"),f(Vt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(Vt,"href","#inmemory-data"),f(ut,"class","relative group"),f(Ga,"href","/docs/datasets/pr_3880/en/package_reference/main_classes#datasets.Dataset"),f(Ut,"id","python-dictionary"),f(Ut,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(Ut,"href","#python-dictionary"),f(mt,"class","relative group"),f(Qa,"href","/docs/datasets/pr_3880/en/package_reference/main_classes#datasets.Dataset.from_dict"),f(Yt,"id","pandas-dataframe"),f(Yt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(Yt,"href","#pandas-dataframe"),f(gt,"class","relative group"),f(Za,"href","/docs/datasets/pr_3880/en/package_reference/main_classes#datasets.Dataset.from_pandas"),f(Qt,"id","offline"),f(Qt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(Qt,"href","#offline"),f(_t,"class","relative group"),f(Zt,"id","slice-splits"),f(Zt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(Zt,"href","#slice-splits"),f(vt,"class","relative group"),f(Xa,"href","/docs/datasets/pr_3880/en/package_reference/builder_classes#datasets.ReadInstruction"),f(te,"href","/docs/datasets/pr_3880/en/package_reference/builder_classes#datasets.ReadInstruction"),f(Xt,"id","percent-slicing-and-rounding"),f(Xt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(Xt,"href","#percent-slicing-and-rounding"),f($t,"class","relative group"),f(oe,"id","troubleshoot"),f(as,"id","troubleshooting"),f(as,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(as,"href","#troubleshooting"),f(wt,"class","relative group"),f(es,"id","manual-download"),f(es,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(es,"href","#manual-download"),f(yt,"class","relative group"),f(re,"href","/docs/datasets/pr_3880/en/package_reference/loading_methods#datasets.load_dataset"),f(ra,"href","https://huggingface.co/datasets/matinf"),f(ra,"rel","nofollow"),f(os,"id","specify-features"),f(os,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(os,"href","#specify-features"),f(bt,"class","relative group"),f(ie,"href","/docs/datasets/pr_3880/en/package_reference/main_classes#datasets.Features"),f(da,"href","https://arrow.apache.org/docs/"),f(da,"rel","nofollow"),f(pe,"href","/docs/datasets/pr_3880/en/package_reference/main_classes#datasets.ClassLabel"),f(de,"href","/docs/datasets/pr_3880/en/package_reference/main_classes#datasets.Features"),f(fe,"href","/docs/datasets/pr_3880/en/package_reference/loading_methods#datasets.load_dataset"),f(ns,"id","metrics"),f(ns,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(ns,"href","#metrics"),f(jt,"class","relative group"),f(is,"id","load-configurations"),f(is,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(is,"href","#load-configurations"),f(xt,"class","relative group"),f(ps,"id","distributed-setup"),f(ps,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(ps,"href","#distributed-setup"),f(kt,"class","relative group"),f(_e,"href","/docs/datasets/pr_3880/en/package_reference/loading_methods#datasets.load_metric")},m(t,e){a(document.head,d),r(t,m,e),r(t,u,e),a(u,g),a(g,v),q(_,v,null),a(u,$),a(u,y),a(y,w),r(t,A,e),r(t,S,e),a(S,D),r(t,Y,e),r(t,C,e),a(C,N),r(t,U,e),r(t,O,e),a(O,J),a(J,F),a(O,xa),a(O,qt),a(qt,ka),a(O,Ea),a(O,Pt),a(Pt,ei),a(O,li),a(O,Ae),a(Ae,oi),a(O,ni),a(O,Se),a(Se,ri),r(t,ro,e),r(t,qa,e),a(qa,ii),r(t,io,e),r(t,Pa,e),r(t,po,e),r(t,rt,e),a(rt,At),a(At,De),q(us,De,null),a(rt,pi),a(rt,Te),a(Te,di),r(t,fo,e),r(t,St,e),a(St,fi),a(St,Ne),a(Ne,ci),a(St,hi),r(t,co,e),r(t,W,e),a(W,ui),a(W,Aa),a(Aa,mi),a(W,gi),a(W,ms),a(ms,_i),a(W,vi),r(t,ho,e),q(gs,t,e),r(t,uo,e),r(t,Sa,e),a(Sa,$i),r(t,mo,e),r(t,Dt,e),a(Dt,wi),a(Dt,Ce),a(Ce,yi),a(Dt,bi),r(t,go,e),q(_s,t,e),r(t,_o,e),q(Tt,t,e),r(t,vo,e),r(t,H,e),a(H,ji),a(H,Ie),a(Ie,xi),a(H,ki),a(H,Oe),a(Oe,Ei),a(H,qi),a(H,Fe),a(Fe,Pi),a(H,Ai),a(H,He),a(He,Si),a(H,Di),a(H,Le),a(Le,Ti),a(H,Ni),r(t,$o,e),q(vs,t,e),r(t,wo,e),q(Nt,t,e),r(t,yo,e),r(t,G,e),a(G,Ci),a(G,Re),a(Re,Ii),a(G,Oi),a(G,$s),a($s,Fi),a(G,Hi),r(t,bo,e),q(ws,t,e),r(t,jo,e),r(t,Ct,e),a(Ct,Li),a(Ct,Me),a(Me,Ri),a(Ct,Mi),r(t,xo,e),q(ys,t,e),r(t,ko,e),r(t,it,e),a(it,It),a(It,ze),q(bs,ze,null),a(it,zi),a(it,Ve),a(Ve,Vi),r(t,Eo,e),r(t,L,e),a(L,Bi),a(L,Be),a(Be,Ui),a(L,Ji),a(L,Ue),a(Ue,Yi),a(L,Wi),a(L,Je),a(Je,Gi),a(L,Qi),a(L,Ye),a(Ye,Zi),a(L,Ki),a(L,Da),a(Da,Xi),a(L,tp),r(t,qo,e),r(t,pt,e),a(pt,Ot),a(Ot,We),q(js,We,null),a(pt,sp),a(pt,Ge),a(Ge,ap),r(t,Po,e),r(t,Ta,e),a(Ta,ep),r(t,Ao,e),q(xs,t,e),r(t,So,e),r(t,Na,e),a(Na,lp),r(t,Do,e),q(ks,t,e),r(t,To,e),r(t,Ca,e),a(Ca,op),r(t,No,e),q(Es,t,e),r(t,Co,e),r(t,Ia,e),a(Ia,np),r(t,Io,e),q(qs,t,e),r(t,Oo,e),r(t,Oa,e),a(Oa,rp),r(t,Fo,e),q(Ps,t,e),r(t,Ho,e),r(t,dt,e),a(dt,Ft),a(Ft,Qe),q(As,Qe,null),a(dt,ip),a(dt,Ze),a(Ze,pp),r(t,Lo,e),r(t,Ht,e),a(Ht,dp),a(Ht,Fa),a(Fa,fp),a(Ht,cp),r(t,Ro,e),q(Ss,t,e),r(t,Mo,e),r(t,Ha,e),a(Ha,hp),r(t,zo,e),q(Ds,t,e),r(t,Vo,e),r(t,Lt,e),a(Lt,up),a(Lt,Ke),a(Ke,mp),a(Lt,gp),r(t,Bo,e),q(Ts,t,e),r(t,Uo,e),r(t,La,e),a(La,_p),r(t,Jo,e),q(Ns,t,e),r(t,Yo,e),r(t,Ra,e),a(Ra,vp),r(t,Wo,e),r(t,ft,e),a(ft,Rt),a(Rt,Xe),q(Cs,Xe,null),a(ft,$p),a(ft,tl),a(tl,wp),r(t,Go,e),r(t,Ma,e),a(Ma,yp),r(t,Qo,e),q(Is,t,e),r(t,Zo,e),r(t,za,e),a(za,bp),r(t,Ko,e),q(Os,t,e),r(t,Xo,e),r(t,ct,e),a(ct,Mt),a(Mt,sl),q(Fs,sl,null),a(ct,jp),a(ct,al),a(al,xp),r(t,tn,e),r(t,Va,e),a(Va,kp),r(t,sn,e),q(Hs,t,e),r(t,an,e),r(t,Ba,e),a(Ba,Ep),r(t,en,e),q(Ls,t,e),r(t,ln,e),r(t,ht,e),a(ht,zt),a(zt,el),q(Rs,el,null),a(ht,qp),a(ht,ll),a(ll,Pp),r(t,on,e),r(t,Ua,e),a(Ua,Ap),r(t,nn,e),r(t,Ja,e),a(Ja,Sp),r(t,rn,e),q(Ms,t,e),r(t,pn,e),r(t,R,e),a(R,Dp),a(R,ol),a(ol,Tp),a(R,Np),a(R,nl),a(nl,Cp),a(R,Ip),a(R,Ya),a(Ya,Op),a(R,Fp),a(R,rl),a(rl,Hp),a(R,Lp),r(t,dn,e),q(zs,t,e),r(t,fn,e),r(t,Wa,e),a(Wa,Rp),r(t,cn,e),q(Vs,t,e),r(t,hn,e),r(t,M,e),a(M,Mp),a(M,il),a(il,zp),a(M,Vp),a(M,pl),a(pl,Bp),a(M,Up),a(M,dl),a(dl,Jp),a(M,Yp),r(t,un,e),r(t,ut,e),a(ut,Vt),a(Vt,fl),q(Bs,fl,null),a(ut,Wp),a(ut,cl),a(cl,Gp),r(t,mn,e),r(t,Bt,e),a(Bt,Qp),a(Bt,Ga),a(Ga,Zp),a(Bt,Kp),r(t,gn,e),r(t,mt,e),a(mt,Ut),a(Ut,hl),q(Us,hl,null),a(mt,Xp),a(mt,ul),a(ul,td),r(t,_n,e),r(t,Jt,e),a(Jt,sd),a(Jt,Qa),a(Qa,ad),a(Jt,ed),r(t,vn,e),q(Js,t,e),r(t,$n,e),r(t,gt,e),a(gt,Yt),a(Yt,ml),q(Ys,ml,null),a(gt,ld),a(gt,gl),a(gl,od),r(t,wn,e),r(t,Wt,e),a(Wt,nd),a(Wt,Za),a(Za,rd),a(Wt,id),r(t,yn,e),q(Ws,t,e),r(t,bn,e),q(Gt,t,e),r(t,jn,e),r(t,_t,e),a(_t,Qt),a(Qt,_l),q(Gs,_l,null),a(_t,pd),a(_t,vl),a(vl,dd),r(t,xn,e),r(t,Ka,e),a(Ka,fd),r(t,kn,e),r(t,Q,e),a(Q,cd),a(Q,$l),a($l,hd),a(Q,ud),a(Q,wl),a(wl,md),a(Q,gd),r(t,En,e),r(t,vt,e),a(vt,Zt),a(Zt,yl),q(Qs,yl,null),a(vt,_d),a(vt,bl),a(bl,vd),r(t,qn,e),r(t,Z,e),a(Z,$d),a(Z,Xa),a(Xa,wd),a(Z,yd),a(Z,te),a(te,bd),a(Z,jd),r(t,Pn,e),r(t,K,e),a(K,xd),a(K,jl),a(jl,kd),a(K,Ed),a(K,xl),a(xl,qd),a(K,Pd),r(t,An,e),q(Zs,t,e),r(t,Sn,e),r(t,Kt,e),a(Kt,Ad),a(Kt,kl),a(kl,Sd),a(Kt,Dd),r(t,Dn,e),q(Ks,t,e),r(t,Tn,e),r(t,se,e),a(se,Td),r(t,Nn,e),q(Xs,t,e),r(t,Cn,e),r(t,ae,e),a(ae,Nd),r(t,In,e),q(ta,t,e),r(t,On,e),r(t,ee,e),a(ee,Cd),r(t,Fn,e),q(sa,t,e),r(t,Hn,e),r(t,$t,e),a($t,Xt),a(Xt,El),q(aa,El,null),a($t,Id),a($t,ql),a(ql,Od),r(t,Ln,e),r(t,le,e),a(le,Fd),r(t,Rn,e),q(ea,t,e),r(t,Mn,e),r(t,ts,e),a(ts,Hd),a(ts,Pl),a(Pl,Ld),a(ts,Rd),r(t,zn,e),q(la,t,e),r(t,Vn,e),q(ss,t,e),r(t,Bn,e),r(t,oe,e),r(t,Un,e),r(t,wt,e),a(wt,as),a(as,Al),q(oa,Al,null),a(wt,Md),a(wt,Sl),a(Sl,zd),r(t,Jn,e),r(t,ne,e),a(ne,Vd),r(t,Yn,e),r(t,yt,e),a(yt,es),a(es,Dl),q(na,Dl,null),a(yt,Bd),a(yt,Tl),a(Tl,Ud),r(t,Wn,e),r(t,z,e),a(z,Jd),a(z,re),a(re,Yd),a(z,Wd),a(z,Nl),a(Nl,Gd),a(z,Qd),a(z,Cl),a(Cl,Zd),a(z,Kd),r(t,Gn,e),r(t,ls,e),a(ls,Xd),a(ls,ra),a(ra,tf),a(ls,sf),r(t,Qn,e),q(ia,t,e),r(t,Zn,e),r(t,bt,e),a(bt,os),a(os,Il),q(pa,Il,null),a(bt,af),a(bt,Ol),a(Ol,ef),r(t,Kn,e),r(t,X,e),a(X,lf),a(X,ie),a(ie,of),a(X,nf),a(X,da),a(da,rf),a(X,pf),r(t,Xn,e),r(t,tt,e),a(tt,df),a(tt,pe),a(pe,ff),a(tt,cf),a(tt,de),a(de,hf),a(tt,uf),r(t,tr,e),q(fa,t,e),r(t,sr,e),r(t,st,e),a(st,mf),a(st,Fl),a(Fl,gf),a(st,_f),a(st,fe),a(fe,vf),a(st,$f),r(t,ar,e),q(ca,t,e),r(t,er,e),r(t,ce,e),a(ce,wf),r(t,lr,e),q(ha,t,e),r(t,or,e),r(t,jt,e),a(jt,ns),a(ns,Hl),q(ua,Hl,null),a(jt,yf),a(jt,Ll),a(Ll,bf),r(t,nr,e),r(t,he,e),a(he,jf),r(t,rr,e),q(ma,t,e),r(t,ir,e),q(rs,t,e),r(t,pr,e),r(t,xt,e),a(xt,is),a(is,Rl),q(ga,Rl,null),a(xt,xf),a(xt,Ml),a(Ml,kf),r(t,dr,e),r(t,ue,e),a(ue,Ef),r(t,fr,e),q(_a,t,e),r(t,cr,e),r(t,kt,e),a(kt,ps),a(ps,zl),q(va,zl,null),a(kt,qf),a(kt,Vl),a(Vl,Pf),r(t,hr,e),r(t,me,e),a(me,Af),r(t,ur,e),r(t,ge,e),a(ge,Sf),r(t,mr,e),r(t,at,e),a(at,Bl),a(Bl,$a),a($a,Df),a($a,Ul),a(Ul,Tf),a($a,Nf),a(at,Cf),a(at,Jl),a(Jl,Et),a(Et,If),a(Et,Yl),a(Yl,Of),a(Et,Ff),a(Et,Wl),a(Wl,Hf),a(Et,Lf),a(at,Rf),a(at,Gl),a(Gl,wa),a(wa,Mf),a(wa,_e),a(_e,zf),a(wa,Vf),r(t,gr,e),q(ya,t,e),r(t,_r,e),q(ds,t,e),r(t,vr,e),r(t,fs,e),a(fs,Bf),a(fs,Ql),a(Ql,Uf),a(fs,Jf),r(t,$r,e),q(ba,t,e),wr=!0},p(t,[e]){const ja={};e&2&&(ja.$$scope={dirty:e,ctx:t}),Tt.$set(ja);const Zl={};e&2&&(Zl.$$scope={dirty:e,ctx:t}),Nt.$set(Zl);const Kl={};e&2&&(Kl.$$scope={dirty:e,ctx:t}),Gt.$set(Kl);const Xl={};e&2&&(Xl.$$scope={dirty:e,ctx:t}),ss.$set(Xl);const to={};e&2&&(to.$$scope={dirty:e,ctx:t}),rs.$set(to);const so={};e&2&&(so.$$scope={dirty:e,ctx:t}),ds.$set(so)},i(t){wr||(j(_.$$.fragment,t),j(us.$$.fragment,t),j(gs.$$.fragment,t),j(_s.$$.fragment,t),j(Tt.$$.fragment,t),j(vs.$$.fragment,t),j(Nt.$$.fragment,t),j(ws.$$.fragment,t),j(ys.$$.fragment,t),j(bs.$$.fragment,t),j(js.$$.fragment,t),j(xs.$$.fragment,t),j(ks.$$.fragment,t),j(Es.$$.fragment,t),j(qs.$$.fragment,t),j(Ps.$$.fragment,t),j(As.$$.fragment,t),j(Ss.$$.fragment,t),j(Ds.$$.fragment,t),j(Ts.$$.fragment,t),j(Ns.$$.fragment,t),j(Cs.$$.fragment,t),j(Is.$$.fragment,t),j(Os.$$.fragment,t),j(Fs.$$.fragment,t),j(Hs.$$.fragment,t),j(Ls.$$.fragment,t),j(Rs.$$.fragment,t),j(Ms.$$.fragment,t),j(zs.$$.fragment,t),j(Vs.$$.fragment,t),j(Bs.$$.fragment,t),j(Us.$$.fragment,t),j(Js.$$.fragment,t),j(Ys.$$.fragment,t),j(Ws.$$.fragment,t),j(Gt.$$.fragment,t),j(Gs.$$.fragment,t),j(Qs.$$.fragment,t),j(Zs.$$.fragment,t),j(Ks.$$.fragment,t),j(Xs.$$.fragment,t),j(ta.$$.fragment,t),j(sa.$$.fragment,t),j(aa.$$.fragment,t),j(ea.$$.fragment,t),j(la.$$.fragment,t),j(ss.$$.fragment,t),j(oa.$$.fragment,t),j(na.$$.fragment,t),j(ia.$$.fragment,t),j(pa.$$.fragment,t),j(fa.$$.fragment,t),j(ca.$$.fragment,t),j(ha.$$.fragment,t),j(ua.$$.fragment,t),j(ma.$$.fragment,t),j(rs.$$.fragment,t),j(ga.$$.fragment,t),j(_a.$$.fragment,t),j(va.$$.fragment,t),j(ya.$$.fragment,t),j(ds.$$.fragment,t),j(ba.$$.fragment,t),wr=!0)},o(t){x(_.$$.fragment,t),x(us.$$.fragment,t),x(gs.$$.fragment,t),x(_s.$$.fragment,t),x(Tt.$$.fragment,t),x(vs.$$.fragment,t),x(Nt.$$.fragment,t),x(ws.$$.fragment,t),x(ys.$$.fragment,t),x(bs.$$.fragment,t),x(js.$$.fragment,t),x(xs.$$.fragment,t),x(ks.$$.fragment,t),x(Es.$$.fragment,t),x(qs.$$.fragment,t),x(Ps.$$.fragment,t),x(As.$$.fragment,t),x(Ss.$$.fragment,t),x(Ds.$$.fragment,t),x(Ts.$$.fragment,t),x(Ns.$$.fragment,t),x(Cs.$$.fragment,t),x(Is.$$.fragment,t),x(Os.$$.fragment,t),x(Fs.$$.fragment,t),x(Hs.$$.fragment,t),x(Ls.$$.fragment,t),x(Rs.$$.fragment,t),x(Ms.$$.fragment,t),x(zs.$$.fragment,t),x(Vs.$$.fragment,t),x(Bs.$$.fragment,t),x(Us.$$.fragment,t),x(Js.$$.fragment,t),x(Ys.$$.fragment,t),x(Ws.$$.fragment,t),x(Gt.$$.fragment,t),x(Gs.$$.fragment,t),x(Qs.$$.fragment,t),x(Zs.$$.fragment,t),x(Ks.$$.fragment,t),x(Xs.$$.fragment,t),x(ta.$$.fragment,t),x(sa.$$.fragment,t),x(aa.$$.fragment,t),x(ea.$$.fragment,t),x(la.$$.fragment,t),x(ss.$$.fragment,t),x(oa.$$.fragment,t),x(na.$$.fragment,t),x(ia.$$.fragment,t),x(pa.$$.fragment,t),x(fa.$$.fragment,t),x(ca.$$.fragment,t),x(ha.$$.fragment,t),x(ua.$$.fragment,t),x(ma.$$.fragment,t),x(rs.$$.fragment,t),x(ga.$$.fragment,t),x(_a.$$.fragment,t),x(va.$$.fragment,t),x(ya.$$.fragment,t),x(ds.$$.fragment,t),x(ba.$$.fragment,t),wr=!1},d(t){s(d),t&&s(m),t&&s(u),k(_),t&&s(A),t&&s(S),t&&s(Y),t&&s(C),t&&s(U),t&&s(O),t&&s(ro),t&&s(qa),t&&s(io),t&&s(Pa),t&&s(po),t&&s(rt),k(us),t&&s(fo),t&&s(St),t&&s(co),t&&s(W),t&&s(ho),k(gs,t),t&&s(uo),t&&s(Sa),t&&s(mo),t&&s(Dt),t&&s(go),k(_s,t),t&&s(_o),k(Tt,t),t&&s(vo),t&&s(H),t&&s($o),k(vs,t),t&&s(wo),k(Nt,t),t&&s(yo),t&&s(G),t&&s(bo),k(ws,t),t&&s(jo),t&&s(Ct),t&&s(xo),k(ys,t),t&&s(ko),t&&s(it),k(bs),t&&s(Eo),t&&s(L),t&&s(qo),t&&s(pt),k(js),t&&s(Po),t&&s(Ta),t&&s(Ao),k(xs,t),t&&s(So),t&&s(Na),t&&s(Do),k(ks,t),t&&s(To),t&&s(Ca),t&&s(No),k(Es,t),t&&s(Co),t&&s(Ia),t&&s(Io),k(qs,t),t&&s(Oo),t&&s(Oa),t&&s(Fo),k(Ps,t),t&&s(Ho),t&&s(dt),k(As),t&&s(Lo),t&&s(Ht),t&&s(Ro),k(Ss,t),t&&s(Mo),t&&s(Ha),t&&s(zo),k(Ds,t),t&&s(Vo),t&&s(Lt),t&&s(Bo),k(Ts,t),t&&s(Uo),t&&s(La),t&&s(Jo),k(Ns,t),t&&s(Yo),t&&s(Ra),t&&s(Wo),t&&s(ft),k(Cs),t&&s(Go),t&&s(Ma),t&&s(Qo),k(Is,t),t&&s(Zo),t&&s(za),t&&s(Ko),k(Os,t),t&&s(Xo),t&&s(ct),k(Fs),t&&s(tn),t&&s(Va),t&&s(sn),k(Hs,t),t&&s(an),t&&s(Ba),t&&s(en),k(Ls,t),t&&s(ln),t&&s(ht),k(Rs),t&&s(on),t&&s(Ua),t&&s(nn),t&&s(Ja),t&&s(rn),k(Ms,t),t&&s(pn),t&&s(R),t&&s(dn),k(zs,t),t&&s(fn),t&&s(Wa),t&&s(cn),k(Vs,t),t&&s(hn),t&&s(M),t&&s(un),t&&s(ut),k(Bs),t&&s(mn),t&&s(Bt),t&&s(gn),t&&s(mt),k(Us),t&&s(_n),t&&s(Jt),t&&s(vn),k(Js,t),t&&s($n),t&&s(gt),k(Ys),t&&s(wn),t&&s(Wt),t&&s(yn),k(Ws,t),t&&s(bn),k(Gt,t),t&&s(jn),t&&s(_t),k(Gs),t&&s(xn),t&&s(Ka),t&&s(kn),t&&s(Q),t&&s(En),t&&s(vt),k(Qs),t&&s(qn),t&&s(Z),t&&s(Pn),t&&s(K),t&&s(An),k(Zs,t),t&&s(Sn),t&&s(Kt),t&&s(Dn),k(Ks,t),t&&s(Tn),t&&s(se),t&&s(Nn),k(Xs,t),t&&s(Cn),t&&s(ae),t&&s(In),k(ta,t),t&&s(On),t&&s(ee),t&&s(Fn),k(sa,t),t&&s(Hn),t&&s($t),k(aa),t&&s(Ln),t&&s(le),t&&s(Rn),k(ea,t),t&&s(Mn),t&&s(ts),t&&s(zn),k(la,t),t&&s(Vn),k(ss,t),t&&s(Bn),t&&s(oe),t&&s(Un),t&&s(wt),k(oa),t&&s(Jn),t&&s(ne),t&&s(Yn),t&&s(yt),k(na),t&&s(Wn),t&&s(z),t&&s(Gn),t&&s(ls),t&&s(Qn),k(ia,t),t&&s(Zn),t&&s(bt),k(pa),t&&s(Kn),t&&s(X),t&&s(Xn),t&&s(tt),t&&s(tr),k(fa,t),t&&s(sr),t&&s(st),t&&s(ar),k(ca,t),t&&s(er),t&&s(ce),t&&s(lr),k(ha,t),t&&s(or),t&&s(jt),k(ua),t&&s(nr),t&&s(he),t&&s(rr),k(ma,t),t&&s(ir),k(rs,t),t&&s(pr),t&&s(xt),k(ga),t&&s(dr),t&&s(ue),t&&s(fr),k(_a,t),t&&s(cr),t&&s(kt),k(va),t&&s(hr),t&&s(me),t&&s(ur),t&&s(ge),t&&s(mr),t&&s(at),t&&s(gr),k(ya,t),t&&s(_r),k(ds,t),t&&s(vr),t&&s(fs),t&&s($r),k(ba,t)}}}const cm={local:"load",sections:[{local:"hugging-face-hub",title:"Hugging Face Hub"},{local:"local-and-remote-files",sections:[{local:"csv",title:"CSV"},{local:"json",title:"JSON"},{local:"text-files",title:"Text files"},{local:"parquet",title:"Parquet"},{local:"image-folders",title:"Image folders"}],title:"Local and remote files"},{local:"inmemory-data",sections:[{local:"python-dictionary",title:"Python dictionary"},{local:"pandas-dataframe",title:"Pandas DataFrame"}],title:"In-memory data"},{local:"offline",title:"Offline"},{local:"slice-splits",sections:[{local:"percent-slicing-and-rounding",title:"Percent slicing and rounding"}],title:"Slice splits"},{local:"troubleshooting",sections:[{local:"manual-download",title:"Manual download"},{local:"specify-features",title:"Specify features"}],title:"Troubleshooting"},{local:"metrics",sections:[{local:"load-configurations",title:"Load configurations"},{local:"distributed-setup",title:"Distributed setup"}],title:"Metrics"}],title:"Load"};function hm(b,d,m){let{fw:u}=d;return b.$$set=g=>{"fw"in g&&m(0,u=g.fw)},[u]}class vm extends lo{constructor(d){super();oo(this,d,hm,fm,no,{fw:0})}}export{vm as default,cm as metadata};
