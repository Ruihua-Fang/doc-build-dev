import{S as WC,i as YC,s as KC,e as r,k as f,w as _,t as i,M as VC,c as o,d as t,m as h,a as l,x as v,h as u,b as p,N as JC,F as e,g as m,y as E,q as w,o as y,B as b,v as XC,L as x}from"../chunks/vendor-7c454903.js";import{T as W}from"../chunks/Tip-735285fc.js";import{I as U}from"../chunks/IconCopyLink-5457534b.js";import{I as C,M as R,C as N}from"../chunks/InferenceApi-143e589f.js";function QC(q){let n,c,s,d,$,k,A;return{c(){n=r("p"),c=r("strong"),s=i("Recommended model"),d=i(`:
`),$=r("a"),k=i("facebook/bart-large-mnli"),A=i("."),this.h()},l(T){n=o(T,"P",{});var j=l(n);c=o(j,"STRONG",{});var O=l(c);s=u(O,"Recommended model"),O.forEach(t),d=u(j,`:
`),$=o(j,"A",{href:!0,rel:!0});var D=l($);k=u(D,"facebook/bart-large-mnli"),D.forEach(t),A=u(j,"."),j.forEach(t),this.h()},h(){p($,"href","https://huggingface.co/facebook/bart-large-mnli"),p($,"rel","nofollow")},m(T,j){m(T,n,j),e(n,c),e(c,s),e(n,d),e(n,$),e($,k),e(n,A)},d(T){T&&t(n)}}}function ZC(q){let n,c;return n=new N({props:{code:`self.assertEqual(
    deep_round(data),
    {
        "sequence": "Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!",
        "labels": ["refund", "faq", "legal"],
        "scores": [
            # 88% refund
            0.8778,
            0.1052,
            0.017,
        ],
    },
)`,highlighted:`self.assertEqual(
    deep_round(data),
    {
        <span class="hljs-string">&quot;sequence&quot;</span>: <span class="hljs-string">&quot;Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!&quot;</span>,
        <span class="hljs-string">&quot;labels&quot;</span>: [<span class="hljs-string">&quot;refund&quot;</span>, <span class="hljs-string">&quot;faq&quot;</span>, <span class="hljs-string">&quot;legal&quot;</span>],
        <span class="hljs-string">&quot;scores&quot;</span>: [
            <span class="hljs-comment"># 88% refund</span>
            <span class="hljs-number">0.8778</span>,
            <span class="hljs-number">0.1052</span>,
            <span class="hljs-number">0.017</span>,
        ],
    },
)`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){E(n,s,d),c=!0},p:x,i(s){c||(w(n.$$.fragment,s),c=!0)},o(s){y(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function eM(q){let n,c;return n=new R({props:{$$slots:{default:[ZC]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){E(n,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){c||(w(n.$$.fragment,s),c=!0)},o(s){y(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function tM(q){let n,c;return n=new N({props:{code:`import fetch from "node-fetch";
async function query(data) {
    const response = await fetch(
        "https://api-inference.huggingface.co/models/facebook/bart-large-mnli",
        {
            headers: { Authorization: \`Bearer $}{API_TOKEN}\` },
            method: "POST",
            body: JSON.stringify(data),
        }
    );
    const result = await response.json();
    return result;
}
query({inputs: "Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!", parameters: {candidate_labels: ["refund", "legal", "faq"]}}).then((response) => {
    console.log(JSON.stringify(response));
});
// {"sequence":"Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!","labels":["refund","faq","legal"],"scores":[0.8778, 0.1052, 0.017]}`,highlighted:`<span class="hljs-keyword">import</span> fetch <span class="hljs-keyword">from</span> <span class="hljs-string">&quot;node-fetch&quot;</span>;
<span class="hljs-keyword">async</span> <span class="hljs-keyword">function</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">data</span>) {
    <span class="hljs-keyword">const</span> response = <span class="hljs-keyword">await</span> <span class="hljs-title function_">fetch</span>(
        <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/facebook/bart-large-mnli&quot;</span>,
        {
            <span class="hljs-attr">headers</span>: { <span class="hljs-title class_">Authorization</span>: <span class="hljs-string">\`Bearer <span class="hljs-subst">$}{API_TOKEN}</span>\`</span> },
            <span class="hljs-attr">method</span>: <span class="hljs-string">&quot;POST&quot;</span>,
            <span class="hljs-attr">body</span>: <span class="hljs-title class_">JSON</span>.<span class="hljs-title function_">stringify</span>(data),
        }
    );
    <span class="hljs-keyword">const</span> result = <span class="hljs-keyword">await</span> response.<span class="hljs-title function_">json</span>();
    <span class="hljs-keyword">return</span> result;
}
<span class="hljs-title function_">query</span>({<span class="hljs-attr">inputs</span>: <span class="hljs-string">&quot;Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!&quot;</span>, <span class="hljs-attr">parameters</span>: {<span class="hljs-attr">candidate_labels</span>: [<span class="hljs-string">&quot;refund&quot;</span>, <span class="hljs-string">&quot;legal&quot;</span>, <span class="hljs-string">&quot;faq&quot;</span>]}}).<span class="hljs-title function_">then</span>(<span class="hljs-function">(<span class="hljs-params">response</span>) =&gt;</span> {
    <span class="hljs-variable language_">console</span>.<span class="hljs-title function_">log</span>(<span class="hljs-title class_">JSON</span>.<span class="hljs-title function_">stringify</span>(response));
});
<span class="hljs-comment">// {&quot;sequence&quot;:&quot;Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!&quot;,&quot;labels&quot;:[&quot;refund&quot;,&quot;faq&quot;,&quot;legal&quot;],&quot;scores&quot;:[0.8778, 0.1052, 0.017]}</span>`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){E(n,s,d),c=!0},p:x,i(s){c||(w(n.$$.fragment,s),c=!0)},o(s){y(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function sM(q){let n,c;return n=new R({props:{$$slots:{default:[tM]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){E(n,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){c||(w(n.$$.fragment,s),c=!0)},o(s){y(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function aM(q){let n,c;return n=new N({props:{code:`curl https://api-inference.huggingface.co/models/facebook/bart-large-mnli \\
        -X POST \\
        -d '{"inputs": "Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!", "parameters": {"candidate_labels": ["refund", "legal", "faq"]}}' \\
        -H "Authorization: Bearer $}{HF_API_TOKEN}"
# {"sequence":"Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!","labels":["refund","faq","legal"],"scores":[0.8778, 0.1052, 0.017]}`,highlighted:`curl https:<span class="hljs-comment">//api-inference.huggingface.co/models/facebook/bart-large-mnli \\</span>
        -X <span class="hljs-keyword">POST</span> \\
        -<span class="hljs-keyword">d</span> &#x27;{<span class="hljs-string">&quot;inputs&quot;</span>: <span class="hljs-string">&quot;Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!&quot;</span>, <span class="hljs-string">&quot;parameters&quot;</span>: {<span class="hljs-string">&quot;candidate_labels&quot;</span>: [<span class="hljs-string">&quot;refund&quot;</span>, <span class="hljs-string">&quot;legal&quot;</span>, <span class="hljs-string">&quot;faq&quot;</span>]}}&#x27; \\
        -<span class="hljs-keyword">H</span> <span class="hljs-string">&quot;Authorization: Bearer $}{HF_API_TOKEN}&quot;</span>
# {<span class="hljs-string">&quot;sequence&quot;</span>:<span class="hljs-string">&quot;Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!&quot;</span>,<span class="hljs-string">&quot;labels&quot;</span>:[<span class="hljs-string">&quot;refund&quot;</span>,<span class="hljs-string">&quot;faq&quot;</span>,<span class="hljs-string">&quot;legal&quot;</span>],<span class="hljs-string">&quot;scores&quot;</span>:[0.8778, 0.1052, 0.017]}`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){E(n,s,d),c=!0},p:x,i(s){c||(w(n.$$.fragment,s),c=!0)},o(s){y(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function nM(q){let n,c;return n=new R({props:{$$slots:{default:[aM]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){E(n,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){c||(w(n.$$.fragment,s),c=!0)},o(s){y(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function rM(q){let n,c;return n=new N({props:{code:`self.assertEqual(
    deep_round(data),
    {
        "sequence": "Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!",
        "labels": ["refund", "faq", "legal"],
        "scores": [
            # 88% refund
            0.8778,
            0.1052,
            0.017,
        ],
    },
)`,highlighted:`self.assertEqual(
    deep_round(data),
    {
        <span class="hljs-string">&quot;sequence&quot;</span>: <span class="hljs-string">&quot;Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!&quot;</span>,
        <span class="hljs-string">&quot;labels&quot;</span>: [<span class="hljs-string">&quot;refund&quot;</span>, <span class="hljs-string">&quot;faq&quot;</span>, <span class="hljs-string">&quot;legal&quot;</span>],
        <span class="hljs-string">&quot;scores&quot;</span>: [
            <span class="hljs-comment"># 88% refund</span>
            <span class="hljs-number">0.8778</span>,
            <span class="hljs-number">0.1052</span>,
            <span class="hljs-number">0.017</span>,
        ],
    },
)`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){E(n,s,d),c=!0},p:x,i(s){c||(w(n.$$.fragment,s),c=!0)},o(s){y(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function oM(q){let n,c;return n=new R({props:{$$slots:{default:[rM]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){E(n,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){c||(w(n.$$.fragment,s),c=!0)},o(s){y(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function lM(q){let n,c,s,d,$,k,A,T,j,O,D,se,De;return{c(){n=r("p"),c=r("strong"),s=i("Recommended model"),d=i(`:
`),$=r("a"),k=i("Helsinki-NLP/opus-mt-ru-en"),A=i(`.
Helsinki-NLP uploaded many models with many language pairs.
`),T=r("strong"),j=i("Recommended model"),O=i(": "),D=r("a"),se=i("t5-base"),De=i("."),this.h()},l(V){n=o(V,"P",{});var J=l(n);c=o(J,"STRONG",{});var et=l(c);s=u(et,"Recommended model"),et.forEach(t),d=u(J,`:
`),$=o(J,"A",{href:!0,rel:!0});var Ll=l($);k=u(Ll,"Helsinki-NLP/opus-mt-ru-en"),Ll.forEach(t),A=u(J,`.
Helsinki-NLP uploaded many models with many language pairs.
`),T=o(J,"STRONG",{});var Ra=l(T);j=u(Ra,"Recommended model"),Ra.forEach(t),O=u(J,": "),D=o(J,"A",{href:!0,rel:!0});var Oe=l(D);se=u(Oe,"t5-base"),Oe.forEach(t),De=u(J,"."),J.forEach(t),this.h()},h(){p($,"href","https://huggingface.co/Helsinki-NLP/opus-mt-ru-en"),p($,"rel","nofollow"),p(D,"href","https://huggingface.co/t5-base"),p(D,"rel","nofollow")},m(V,J){m(V,n,J),e(n,c),e(c,s),e(n,d),e(n,$),e($,k),e(n,A),e(n,T),e(T,j),e(n,O),e(n,D),e(D,se),e(n,De)},d(V){V&&t(n)}}}function iM(q){let n,c;return n=new N({props:{code:`import json
import requests
headers = {"Authorization": f"Bearer {API_TOKEN}"}
API_URL = "https://api-inference.huggingface.co/models/Helsinki-NLP/opus-mt-ru-en"
def query(payload):
    data = json.dumps(payload)
    response = requests.request("POST", API_URL, headers=headers, data=data)
    return json.loads(response.content.decode("utf-8"))
data = query(
    {
        "inputs": "\u041C\u0435\u043D\u044F \u0437\u043E\u0432\u0443\u0442 \u0412\u043E\u043B\u044C\u0444\u0433\u0430\u043D\u0433 \u0438 \u044F \u0436\u0438\u0432\u0443 \u0432 \u0411\u0435\u0440\u043B\u0438\u043D\u0435",
    }
)
# Response
self.assertEqual(
    data,
    [
        {
            "translation_text": "My name is Wolfgang and I live in Berlin.",
        },
    ],
)`,highlighted:`<span class="hljs-keyword">import</span> json
<span class="hljs-keyword">import</span> requests
headers = {<span class="hljs-string">&quot;Authorization&quot;</span>: <span class="hljs-string">f&quot;Bearer <span class="hljs-subst">{API_TOKEN}</span>&quot;</span>}
API_URL = <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/Helsinki-NLP/opus-mt-ru-en&quot;</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">payload</span>):
    data = json.dumps(payload)
    response = requests.request(<span class="hljs-string">&quot;POST&quot;</span>, API_URL, headers=headers, data=data)
    <span class="hljs-keyword">return</span> json.loads(response.content.decode(<span class="hljs-string">&quot;utf-8&quot;</span>))
data = query(
    {
        <span class="hljs-string">&quot;inputs&quot;</span>: <span class="hljs-string">&quot;\u041C\u0435\u043D\u044F \u0437\u043E\u0432\u0443\u0442 \u0412\u043E\u043B\u044C\u0444\u0433\u0430\u043D\u0433 \u0438 \u044F \u0436\u0438\u0432\u0443 \u0432 \u0411\u0435\u0440\u043B\u0438\u043D\u0435&quot;</span>,
    }
)
<span class="hljs-comment"># Response</span>
self.assertEqual(
    data,
    [
        {
            <span class="hljs-string">&quot;translation_text&quot;</span>: <span class="hljs-string">&quot;My name is Wolfgang and I live in Berlin.&quot;</span>,
        },
    ],
)`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){E(n,s,d),c=!0},p:x,i(s){c||(w(n.$$.fragment,s),c=!0)},o(s){y(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function uM(q){let n,c;return n=new R({props:{$$slots:{default:[iM]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){E(n,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){c||(w(n.$$.fragment,s),c=!0)},o(s){y(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function cM(q){let n,c;return n=new N({props:{code:`import fetch from "node-fetch";
async function query(data) {
    const response = await fetch(
        "https://api-inference.huggingface.co/models/Helsinki-NLP/opus-mt-ru-en",
        {
            headers: { Authorization: \`Bearer $}{API_TOKEN}\` },
            method: "POST",
            body: JSON.stringify(data),
        }
    );
    const result = await response.json();
    return result;
}
query({inputs: "\u041C\u0435\u043D\u044F \u0437\u043E\u0432\u0443\u0442 \u0412\u043E\u043B\u044C\u0444\u0433\u0430\u043D\u0433 \u0438 \u044F \u0436\u0438\u0432\u0443 \u0432 \u0411\u0435\u0440\u043B\u0438\u043D\u0435"}).then((response) => {
    console.log(JSON.stringify(response));
});
// [{"translation_text":"My name is Wolfgang and I live in Berlin."}]`,highlighted:`<span class="hljs-keyword">import</span> fetch <span class="hljs-keyword">from</span> <span class="hljs-string">&quot;node-fetch&quot;</span>;
<span class="hljs-keyword">async</span> <span class="hljs-keyword">function</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">data</span>) {
    <span class="hljs-keyword">const</span> response = <span class="hljs-keyword">await</span> <span class="hljs-title function_">fetch</span>(
        <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/Helsinki-NLP/opus-mt-ru-en&quot;</span>,
        {
            <span class="hljs-attr">headers</span>: { <span class="hljs-title class_">Authorization</span>: <span class="hljs-string">\`Bearer <span class="hljs-subst">$}{API_TOKEN}</span>\`</span> },
            <span class="hljs-attr">method</span>: <span class="hljs-string">&quot;POST&quot;</span>,
            <span class="hljs-attr">body</span>: <span class="hljs-title class_">JSON</span>.<span class="hljs-title function_">stringify</span>(data),
        }
    );
    <span class="hljs-keyword">const</span> result = <span class="hljs-keyword">await</span> response.<span class="hljs-title function_">json</span>();
    <span class="hljs-keyword">return</span> result;
}
<span class="hljs-title function_">query</span>({<span class="hljs-attr">inputs</span>: <span class="hljs-string">&quot;\u041C\u0435\u043D\u044F \u0437\u043E\u0432\u0443\u0442 \u0412\u043E\u043B\u044C\u0444\u0433\u0430\u043D\u0433 \u0438 \u044F \u0436\u0438\u0432\u0443 \u0432 \u0411\u0435\u0440\u043B\u0438\u043D\u0435&quot;</span>}).<span class="hljs-title function_">then</span>(<span class="hljs-function">(<span class="hljs-params">response</span>) =&gt;</span> {
    <span class="hljs-variable language_">console</span>.<span class="hljs-title function_">log</span>(<span class="hljs-title class_">JSON</span>.<span class="hljs-title function_">stringify</span>(response));
});
<span class="hljs-comment">// [{&quot;translation_text&quot;:&quot;My name is Wolfgang and I live in Berlin.&quot;}]</span>`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){E(n,s,d),c=!0},p:x,i(s){c||(w(n.$$.fragment,s),c=!0)},o(s){y(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function pM(q){let n,c;return n=new R({props:{$$slots:{default:[cM]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){E(n,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){c||(w(n.$$.fragment,s),c=!0)},o(s){y(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function fM(q){let n,c;return n=new N({props:{code:`curl https://api-inference.huggingface.co/models/Helsinki-NLP/opus-mt-ru-en \\
        -X POST \\
        -d '{"inputs": "\u041C\u0435\u043D\u044F \u0437\u043E\u0432\u0443\u0442 \u0412\u043E\u043B\u044C\u0444\u0433\u0430\u043D\u0433 \u0438 \u044F \u0436\u0438\u0432\u0443 \u0432 \u0411\u0435\u0440\u043B\u0438\u043D\u0435"}' \\
        -H "Authorization: Bearer $}{HF_API_TOKEN}"
# [{"translation_text":"My name is Wolfgang and I live in Berlin."}]`,highlighted:`curl https:<span class="hljs-regexp">//</span>api-inference.huggingface.co<span class="hljs-regexp">/models/</span>Helsinki-NLP/opus-mt-ru-en \\
        -X POST \\
        -d <span class="hljs-string">&#x27;{&quot;inputs&quot;: &quot;\u041C\u0435\u043D\u044F \u0437\u043E\u0432\u0443\u0442 \u0412\u043E\u043B\u044C\u0444\u0433\u0430\u043D\u0433 \u0438 \u044F \u0436\u0438\u0432\u0443 \u0432 \u0411\u0435\u0440\u043B\u0438\u043D\u0435&quot;}&#x27;</span> \\
        -H <span class="hljs-string">&quot;Authorization: Bearer $}{HF_API_TOKEN}&quot;</span>
<span class="hljs-comment"># [{&quot;translation_text&quot;:&quot;My name is Wolfgang and I live in Berlin.&quot;}]</span>`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){E(n,s,d),c=!0},p:x,i(s){c||(w(n.$$.fragment,s),c=!0)},o(s){y(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function hM(q){let n,c;return n=new R({props:{$$slots:{default:[fM]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){E(n,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){c||(w(n.$$.fragment,s),c=!0)},o(s){y(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function dM(q){let n,c,s,d,$,k,A;return{c(){n=r("p"),c=r("strong"),s=i("Recommended model"),d=i(`:
`),$=r("a"),k=i("facebook/bart-large-cnn"),A=i("."),this.h()},l(T){n=o(T,"P",{});var j=l(n);c=o(j,"STRONG",{});var O=l(c);s=u(O,"Recommended model"),O.forEach(t),d=u(j,`:
`),$=o(j,"A",{href:!0,rel:!0});var D=l($);k=u(D,"facebook/bart-large-cnn"),D.forEach(t),A=u(j,"."),j.forEach(t),this.h()},h(){p($,"href","https://huggingface.co/facebook/bart-large-cnn"),p($,"rel","nofollow")},m(T,j){m(T,n,j),e(n,c),e(c,s),e(n,d),e(n,$),e($,k),e(n,A)},d(T){T&&t(n)}}}function gM(q){let n,c;return n=new N({props:{code:`import json
import requests
headers = {"Authorization": f"Bearer {API_TOKEN}"}
API_URL = "https://api-inference.huggingface.co/models/facebook/bart-large-cnn"
def query(payload):
    data = json.dumps(payload)
    response = requests.request("POST", API_URL, headers=headers, data=data)
    return json.loads(response.content.decode("utf-8"))
data = query(
    {
        "inputs": "The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world, a title it held for 41 years until the Chrysler Building in New York City was finished in 1930. It was the first structure to reach a height of 300 metres. Due to the addition of a broadcasting aerial at the top of the tower in 1957, it is now taller than the Chrysler Building by 5.2 metres (17 ft). Excluding transmitters, the Eiffel Tower is the second tallest free-standing structure in France after the Millau Viaduct.",
        "parameters": {"do_sample": False},
    }
)
# Response
self.assertEqual(
    data,
    [
        {
            "summary_text": "The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world.",
        },
    ],
)`,highlighted:`<span class="hljs-keyword">import</span> json
<span class="hljs-keyword">import</span> requests
headers = {<span class="hljs-string">&quot;Authorization&quot;</span>: <span class="hljs-string">f&quot;Bearer <span class="hljs-subst">{API_TOKEN}</span>&quot;</span>}
API_URL = <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/facebook/bart-large-cnn&quot;</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">payload</span>):
    data = json.dumps(payload)
    response = requests.request(<span class="hljs-string">&quot;POST&quot;</span>, API_URL, headers=headers, data=data)
    <span class="hljs-keyword">return</span> json.loads(response.content.decode(<span class="hljs-string">&quot;utf-8&quot;</span>))
data = query(
    {
        <span class="hljs-string">&quot;inputs&quot;</span>: <span class="hljs-string">&quot;The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world, a title it held for 41 years until the Chrysler Building in New York City was finished in 1930. It was the first structure to reach a height of 300 metres. Due to the addition of a broadcasting aerial at the top of the tower in 1957, it is now taller than the Chrysler Building by 5.2 metres (17 ft). Excluding transmitters, the Eiffel Tower is the second tallest free-standing structure in France after the Millau Viaduct.&quot;</span>,
        <span class="hljs-string">&quot;parameters&quot;</span>: {<span class="hljs-string">&quot;do_sample&quot;</span>: <span class="hljs-literal">False</span>},
    }
)
<span class="hljs-comment"># Response</span>
self.assertEqual(
    data,
    [
        {
            <span class="hljs-string">&quot;summary_text&quot;</span>: <span class="hljs-string">&quot;The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world.&quot;</span>,
        },
    ],
)`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){E(n,s,d),c=!0},p:x,i(s){c||(w(n.$$.fragment,s),c=!0)},o(s){y(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function mM(q){let n,c;return n=new R({props:{$$slots:{default:[gM]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){E(n,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){c||(w(n.$$.fragment,s),c=!0)},o(s){y(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function $M(q){let n,c;return n=new N({props:{code:`import fetch from "node-fetch";
async function query(data) {
    const response = await fetch(
        "https://api-inference.huggingface.co/models/facebook/bart-large-cnn",
        {
            headers: { Authorization: \`Bearer $}{API_TOKEN}\` },
            method: "POST",
            body: JSON.stringify(data),
        }
    );
    const result = await response.json();
    return result;
}
query({inputs: "The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world, a title it held for 41 years until the Chrysler Building in New York City was finished in 1930. It was the first structure to reach a height of 300 metres. Due to the addition of a broadcasting aerial at the top of the tower in 1957, it is now taller than the Chrysler Building by 5.2 metres (17 ft). Excluding transmitters, the Eiffel Tower is the second tallest free-standing structure in France after the Millau Viaduct."}).then((response) => {
    console.log(JSON.stringify(response));
});
// [{"summary_text":"The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side. It was the first structure to reach a height of 300 metres."}]`,highlighted:`import fetch from <span class="hljs-comment">&quot;node-fetch&quot;</span>;
async function query(data) {
    const response = await fetch(
        <span class="hljs-comment">&quot;https://api-inference.huggingface.co/models/facebook/bart-large-cnn&quot;</span>,
        {
            headers: { <span class="hljs-type">Authorization</span>: \`<span class="hljs-type">Bearer</span> <span class="hljs-string">$}{</span><span class="hljs-type">API_TOKEN</span>}\` },
            method: <span class="hljs-comment">&quot;POST&quot;</span>,
            body: <span class="hljs-type">JSON</span>.stringify(data),
        }
    );
    const result = await response.json();
    return result;
}
query({inputs: <span class="hljs-comment">&quot;The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world, a title it held for 41 years until the Chrysler Building in New York City was finished in 1930. It was the first structure to reach a height of 300 metres. Due to the addition of a broadcasting aerial at the top of the tower in 1957, it is now taller than the Chrysler Building by 5.2 metres (17 ft). Excluding transmitters, the Eiffel Tower is the second tallest free-standing structure in France after the Millau Viaduct.&quot;</span>}).then((response) =&gt; {
    console.log(<span class="hljs-type">JSON</span>.stringify(response));
});
// [{<span class="hljs-comment">&quot;summary_text&quot;</span>:<span class="hljs-comment">&quot;The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side. It was the first structure to reach a height of 300 metres.&quot;</span>}]`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){E(n,s,d),c=!0},p:x,i(s){c||(w(n.$$.fragment,s),c=!0)},o(s){y(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function qM(q){let n,c;return n=new R({props:{$$slots:{default:[$M]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){E(n,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){c||(w(n.$$.fragment,s),c=!0)},o(s){y(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function _M(q){let n,c;return n=new N({props:{code:`curl https://api-inference.huggingface.co/models/facebook/bart-large-cnn \\
        -X POST \\
        -d '{"inputs": "The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world, a title it held for 41 years until the Chrysler Building in New York City was finished in 1930. It was the first structure to reach a height of 300 metres. Due to the addition of a broadcasting aerial at the top of the tower in 1957, it is now taller than the Chrysler Building by 5.2 metres (17 ft). Excluding transmitters, the Eiffel Tower is the second tallest free-standing structure in France after the Millau Viaduct.", "parameters": {"do_sample": false}}' \\
        -H "Authorization: Bearer $}{HF_API_TOKEN}"
# [{"summary_text":"The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world."}]`,highlighted:`curl https:<span class="hljs-comment">//api-inference.huggingface.co/models/facebook/bart-large-cnn \\
        -X POST \\
        -d &#x27;{&quot;inputs&quot;: &quot;The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world, a title it held for 41 years until the Chrysler Building in New York City was finished in 1930. It was the first structure to reach a height of 300 metres. Due to the addition of a broadcasting aerial at the top of the tower in 1957, it is now taller than the Chrysler Building by 5.2 metres (17 ft). Excluding transmitters, the Eiffel Tower is the second tallest free-standing structure in France after the Millau Viaduct.&quot;, &quot;parameters&quot;: {&quot;do_sample&quot;: false}}&#x27; \\
        -H &quot;Authorization: Bearer $}{HF_API_TOKEN}&quot;</span>
# [{<span class="hljs-string">&quot;summary_text&quot;</span>:<span class="hljs-string">&quot;The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world.&quot;</span>}]`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){E(n,s,d),c=!0},p:x,i(s){c||(w(n.$$.fragment,s),c=!0)},o(s){y(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function vM(q){let n,c;return n=new R({props:{$$slots:{default:[_M]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){E(n,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){c||(w(n.$$.fragment,s),c=!0)},o(s){y(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function EM(q){let n,c,s,d,$,k,A;return{c(){n=r("p"),c=r("strong"),s=i("Recommended model"),d=i(`:
`),$=r("a"),k=i("microsoft/DialoGPT-large"),A=i("."),this.h()},l(T){n=o(T,"P",{});var j=l(n);c=o(j,"STRONG",{});var O=l(c);s=u(O,"Recommended model"),O.forEach(t),d=u(j,`:
`),$=o(j,"A",{href:!0,rel:!0});var D=l($);k=u(D,"microsoft/DialoGPT-large"),D.forEach(t),A=u(j,"."),j.forEach(t),this.h()},h(){p($,"href","https://huggingface.co/microsoft/DialoGPT-large"),p($,"rel","nofollow")},m(T,j){m(T,n,j),e(n,c),e(c,s),e(n,d),e(n,$),e($,k),e(n,A)},d(T){T&&t(n)}}}function wM(q){let n,c;return n=new N({props:{code:`import json
import requests
headers = {"Authorization": f"Bearer {API_TOKEN}"}
API_URL = "https://api-inference.huggingface.co/models/microsoft/DialoGPT-large"
def query(payload):
    data = json.dumps(payload)
    response = requests.request("POST", API_URL, headers=headers, data=data)
    return json.loads(response.content.decode("utf-8"))
data = query(
    {
        "inputs": {
            "past_user_inputs": ["Which movie is the best ?"],
            "generated_responses": ["It's Die Hard for sure."],
            "text": "Can you explain why ?",
        },
    }
)
# Response
self.assertEqual(
    data,
    {
        "generated_text": "It's the best movie ever.",
        "conversation": {
            "past_user_inputs": [
                "Which movie is the best ?",
                "Can you explain why ?",
            ],
            "generated_responses": [
                "It's Die Hard for sure.",
                "It's the best movie ever.",
            ],
        },
        "warnings": ["Setting \`pad_token_id\` to \`eos_token_id\`:50256 for open-end generation."],
    },
)`,highlighted:`<span class="hljs-keyword">import</span> json
<span class="hljs-keyword">import</span> requests
headers = {<span class="hljs-string">&quot;Authorization&quot;</span>: <span class="hljs-string">f&quot;Bearer <span class="hljs-subst">{API_TOKEN}</span>&quot;</span>}
API_URL = <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/microsoft/DialoGPT-large&quot;</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">payload</span>):
    data = json.dumps(payload)
    response = requests.request(<span class="hljs-string">&quot;POST&quot;</span>, API_URL, headers=headers, data=data)
    <span class="hljs-keyword">return</span> json.loads(response.content.decode(<span class="hljs-string">&quot;utf-8&quot;</span>))
data = query(
    {
        <span class="hljs-string">&quot;inputs&quot;</span>: {
            <span class="hljs-string">&quot;past_user_inputs&quot;</span>: [<span class="hljs-string">&quot;Which movie is the best ?&quot;</span>],
            <span class="hljs-string">&quot;generated_responses&quot;</span>: [<span class="hljs-string">&quot;It&#x27;s Die Hard for sure.&quot;</span>],
            <span class="hljs-string">&quot;text&quot;</span>: <span class="hljs-string">&quot;Can you explain why ?&quot;</span>,
        },
    }
)
<span class="hljs-comment"># Response</span>
self.assertEqual(
    data,
    {
        <span class="hljs-string">&quot;generated_text&quot;</span>: <span class="hljs-string">&quot;It&#x27;s the best movie ever.&quot;</span>,
        <span class="hljs-string">&quot;conversation&quot;</span>: {
            <span class="hljs-string">&quot;past_user_inputs&quot;</span>: [
                <span class="hljs-string">&quot;Which movie is the best ?&quot;</span>,
                <span class="hljs-string">&quot;Can you explain why ?&quot;</span>,
            ],
            <span class="hljs-string">&quot;generated_responses&quot;</span>: [
                <span class="hljs-string">&quot;It&#x27;s Die Hard for sure.&quot;</span>,
                <span class="hljs-string">&quot;It&#x27;s the best movie ever.&quot;</span>,
            ],
        },
        <span class="hljs-string">&quot;warnings&quot;</span>: [<span class="hljs-string">&quot;Setting \`pad_token_id\` to \`eos_token_id\`:50256 for open-end generation.&quot;</span>],
    },
)`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){E(n,s,d),c=!0},p:x,i(s){c||(w(n.$$.fragment,s),c=!0)},o(s){y(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function yM(q){let n,c;return n=new R({props:{$$slots:{default:[wM]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){E(n,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){c||(w(n.$$.fragment,s),c=!0)},o(s){y(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function bM(q){let n,c;return n=new N({props:{code:`import fetch from "node-fetch";
async function query(data) {
    const response = await fetch(
        "https://api-inference.huggingface.co/models/microsoft/DialoGPT-large",
        {
            headers: { Authorization: \`Bearer $}{API_TOKEN}\` },
            method: "POST",
            body: JSON.stringify(data),
        }
    );
    const result = await response.json();
    return result;
}
query({inputs: {past_user_inputs: ["Which movie is the best ?"], generated_responses: ["It is Die Hard for sure."], text:"Can you explain why ?"}}).then((response) => {
    console.log(JSON.stringify(response));
});
// {"generated_text":"It's the best movie ever.","conversation":{"past_user_inputs":["Which movie is the best ?","Can you explain why ?"],"generated_responses":["It is Die Hard for sure.","It's the best movie ever."]},"warnings":["Setting \`pad_token_id\` to \`eos_token_id\`:50256 for open-end generation."]}`,highlighted:`<span class="hljs-keyword">import</span> <span class="hljs-keyword">fetch</span> <span class="hljs-keyword">from</span> &quot;node-fetch&quot;;
async <span class="hljs-keyword">function</span> query(data) {
    const response = await <span class="hljs-keyword">fetch</span>(
        &quot;https://api-inference.huggingface.co/models/microsoft/DialoGPT-large&quot;,
        {
            headers: { <span class="hljs-keyword">Authorization</span>: \`Bearer $}{API_TOKEN}\` },
            <span class="hljs-keyword">method</span>: &quot;POST&quot;,
            body: <span class="hljs-type">JSON</span>.stringify(data),
        }
    );
    const result = await response.json();
    <span class="hljs-keyword">return</span> result;
}
query({inputs: {past_user_inputs: [&quot;Which movie is the best ?&quot;], generated_responses: [&quot;It is Die Hard for sure.&quot;], <span class="hljs-type">text</span>:&quot;Can you explain why ?&quot;}}).<span class="hljs-keyword">then</span>((response) =&gt; {
    console.log(<span class="hljs-type">JSON</span>.stringify(response));
});
// {&quot;generated_text&quot;:&quot;It&#x27;s the best movie ever.&quot;,&quot;conversation&quot;:{&quot;past_user_inputs&quot;:[&quot;Which movie is the best ?&quot;,&quot;Can you explain why ?&quot;],&quot;generated_responses&quot;:[&quot;It is Die Hard for sure.&quot;,&quot;It&#x27;s the best movie ever.&quot;]},&quot;warnings&quot;:[&quot;Setting \`pad_token_id\` to \`eos_token_id\`:50256 for open-end generation.&quot;]}`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){E(n,s,d),c=!0},p:x,i(s){c||(w(n.$$.fragment,s),c=!0)},o(s){y(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function jM(q){let n,c;return n=new R({props:{$$slots:{default:[bM]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){E(n,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){c||(w(n.$$.fragment,s),c=!0)},o(s){y(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function TM(q){let n,c;return n=new N({props:{code:`curl https://api-inference.huggingface.co/models/microsoft/DialoGPT-large \\
        -X POST \\
        -d '{"inputs": {"past_user_inputs": ["Which movie is the best ?"], "generated_responses": ["It is Die Hard for sure."], "text":"Can you explain why ?"}}' \\
        -H "Authorization: Bearer $}{HF_API_TOKEN}"
# {"generated_text":"It's the best movie ever.","conversation":{"past_user_inputs":["Which movie is the best ?","Can you explain why ?"],"generated_responses":["It is Die Hard for sure.","It's the best movie ever."]},"warnings":["Setting \`pad_token_id\` to \`eos_token_id\`:50256 for open-end generation."]}`,highlighted:'<span class="hljs-title">curl https:</span>//api-inference.huggingface.co/models/microsoft/DialoGPT-large \\\n        -X POST \\\n        -d &#x27;{<span class="hljs-string">&quot;inputs&quot;</span>: {<span class="hljs-string">&quot;past_user_inputs&quot;</span>: [<span class="hljs-string">&quot;Which movie is the best ?&quot;</span>], <span class="hljs-string">&quot;generated_responses&quot;</span>: [<span class="hljs-string">&quot;It is Die Hard for sure.&quot;</span>], <span class="hljs-string">&quot;text&quot;</span>:<span class="hljs-string">&quot;Can you explain why ?&quot;</span>}}&#x27; \\\n        -H <span class="hljs-string">&quot;Authorization: Bearer $}{HF_API_TOKEN}&quot;</span>\n# {<span class="hljs-string">&quot;generated_text&quot;</span>:<span class="hljs-string">&quot;It&#x27;s the best movie ever.&quot;</span>,<span class="hljs-string">&quot;conversation&quot;</span>:{<span class="hljs-string">&quot;past_user_inputs&quot;</span>:[<span class="hljs-string">&quot;Which movie is the best ?&quot;</span>,<span class="hljs-string">&quot;Can you explain why ?&quot;</span>],<span class="hljs-string">&quot;generated_responses&quot;</span>:[<span class="hljs-string">&quot;It is Die Hard for sure.&quot;</span>,<span class="hljs-string">&quot;It&#x27;s the best movie ever.&quot;</span>]},<span class="hljs-string">&quot;warnings&quot;</span>:[<span class="hljs-string">&quot;Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.&quot;</span>]}'}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){E(n,s,d),c=!0},p:x,i(s){c||(w(n.$$.fragment,s),c=!0)},o(s){y(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function kM(q){let n,c;return n=new R({props:{$$slots:{default:[TM]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){E(n,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){c||(w(n.$$.fragment,s),c=!0)},o(s){y(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function AM(q){let n,c,s,d,$,k,A;return{c(){n=r("p"),c=r("strong"),s=i("Recommended model"),d=i(`:
`),$=r("a"),k=i("google/tapas-base-finetuned-wtq"),A=i("."),this.h()},l(T){n=o(T,"P",{});var j=l(n);c=o(j,"STRONG",{});var O=l(c);s=u(O,"Recommended model"),O.forEach(t),d=u(j,`:
`),$=o(j,"A",{href:!0,rel:!0});var D=l($);k=u(D,"google/tapas-base-finetuned-wtq"),D.forEach(t),A=u(j,"."),j.forEach(t),this.h()},h(){p($,"href","https://huggingface.co/google/tapas-base-finetuned-wtq"),p($,"rel","nofollow")},m(T,j){m(T,n,j),e(n,c),e(c,s),e(n,d),e(n,$),e($,k),e(n,A)},d(T){T&&t(n)}}}function DM(q){let n,c;return n=new N({props:{code:`self.assertEqual(
    data,
    {
        "answer": "AVERAGE > 36542",
        "coordinates": [[0, 1]],
        "cells": ["36542"],
        "aggregator": "AVERAGE",
    },
)`,highlighted:`self.assertEqual(
    data,
    {
        <span class="hljs-string">&quot;answer&quot;</span>: <span class="hljs-string">&quot;AVERAGE &gt; 36542&quot;</span>,
        <span class="hljs-string">&quot;coordinates&quot;</span>: [[<span class="hljs-number">0</span>, <span class="hljs-number">1</span>]],
        <span class="hljs-string">&quot;cells&quot;</span>: [<span class="hljs-string">&quot;36542&quot;</span>],
        <span class="hljs-string">&quot;aggregator&quot;</span>: <span class="hljs-string">&quot;AVERAGE&quot;</span>,
    },
)`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){E(n,s,d),c=!0},p:x,i(s){c||(w(n.$$.fragment,s),c=!0)},o(s){y(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function OM(q){let n,c;return n=new R({props:{$$slots:{default:[DM]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){E(n,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){c||(w(n.$$.fragment,s),c=!0)},o(s){y(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function xM(q){let n,c;return n=new N({props:{code:`import fetch from "node-fetch";
async function query(data) {
    const response = await fetch(
        "https://api-inference.huggingface.co/models/google/tapas-base-finetuned-wtq",
        {
            headers: { Authorization: \`Bearer $}{API_TOKEN}\` },
            method: "POST",
            body: JSON.stringify(data),
        }
    );
    const result = await response.json();
    return result;
}
query({inputs:{query:"How many stars does the transformers repository have?",table:{Repository:["Transformers","Datasets","Tokenizers"],Stars:["36542","4512","3934"],Contributors:["651","77","34"],"Programming language":["Python","Python","Rust, Python and NodeJS"]}}}).then((response) => {
    console.log(JSON.stringify(response));
});
// {"answer":"AVERAGE > 36542","coordinates":[[0,1]],"cells":["36542"],"aggregator":"AVERAGE"}`,highlighted:`<span class="hljs-keyword">import</span> <span class="hljs-keyword">fetch</span> <span class="hljs-keyword">from</span> &quot;node-fetch&quot;;
async <span class="hljs-keyword">function</span> query(data) {
    const response = await <span class="hljs-keyword">fetch</span>(
        &quot;https://api-inference.huggingface.co/models/google/tapas-base-finetuned-wtq&quot;,
        {
            headers: { <span class="hljs-keyword">Authorization</span>: \`Bearer $}{API_TOKEN}\` },
            <span class="hljs-keyword">method</span>: &quot;POST&quot;,
            body: <span class="hljs-type">JSON</span>.stringify(data),
        }
    );
    const result = await response.json();
    <span class="hljs-keyword">return</span> result;
}
query({inputs:{query:&quot;How many stars does the transformers repository have?&quot;,<span class="hljs-keyword">table</span>:{Repository:[&quot;Transformers&quot;,&quot;Datasets&quot;,&quot;Tokenizers&quot;],Stars:[&quot;36542&quot;,&quot;4512&quot;,&quot;3934&quot;],Contributors:[&quot;651&quot;,&quot;77&quot;,&quot;34&quot;],&quot;Programming language&quot;:[&quot;Python&quot;,&quot;Python&quot;,&quot;Rust, Python and NodeJS&quot;]}}}).<span class="hljs-keyword">then</span>((response) =&gt; {
    console.log(<span class="hljs-type">JSON</span>.stringify(response));
});
// {&quot;answer&quot;:&quot;AVERAGE &gt; 36542&quot;,&quot;coordinates&quot;:[[<span class="hljs-number">0</span>,<span class="hljs-number">1</span>]],&quot;cells&quot;:[&quot;36542&quot;],&quot;aggregator&quot;:&quot;AVERAGE&quot;}`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){E(n,s,d),c=!0},p:x,i(s){c||(w(n.$$.fragment,s),c=!0)},o(s){y(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function RM(q){let n,c;return n=new R({props:{$$slots:{default:[xM]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){E(n,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){c||(w(n.$$.fragment,s),c=!0)},o(s){y(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function NM(q){let n,c;return n=new N({props:{code:`curl https://api-inference.huggingface.co/models/google/tapas-base-finetuned-wtq \\
        -X POST \\
        -d '{"inputs":{"query":"How many stars does the transformers repository have?","table":{"Repository":["Transformers","Datasets","Tokenizers"],"Stars":["36542","4512","3934"],"Contributors":["651","77","34"],"Programming language":["Python","Python","Rust, Python and NodeJS"]}}}' \\
        -H "Authorization: Bearer $}{HF_API_TOKEN}"
# {"answer":"AVERAGE > 36542","coordinates":[[0,1]],"cells":["36542"],"aggregator":"AVERAGE"}`,highlighted:`curl https:<span class="hljs-comment">//api-inference.huggingface.co/models/google/tapas-base-finetuned-wtq \\</span>
        -X <span class="hljs-keyword">POST</span> \\
        -<span class="hljs-keyword">d</span> &#x27;{<span class="hljs-string">&quot;inputs&quot;</span>:{<span class="hljs-string">&quot;query&quot;</span>:<span class="hljs-string">&quot;How many stars does the transformers repository have?&quot;</span>,<span class="hljs-string">&quot;table&quot;</span>:{<span class="hljs-string">&quot;Repository&quot;</span>:[<span class="hljs-string">&quot;Transformers&quot;</span>,<span class="hljs-string">&quot;Datasets&quot;</span>,<span class="hljs-string">&quot;Tokenizers&quot;</span>],<span class="hljs-string">&quot;Stars&quot;</span>:[<span class="hljs-string">&quot;36542&quot;</span>,<span class="hljs-string">&quot;4512&quot;</span>,<span class="hljs-string">&quot;3934&quot;</span>],<span class="hljs-string">&quot;Contributors&quot;</span>:[<span class="hljs-string">&quot;651&quot;</span>,<span class="hljs-string">&quot;77&quot;</span>,<span class="hljs-string">&quot;34&quot;</span>],<span class="hljs-string">&quot;Programming language&quot;</span>:[<span class="hljs-string">&quot;Python&quot;</span>,<span class="hljs-string">&quot;Python&quot;</span>,<span class="hljs-string">&quot;Rust, Python and NodeJS&quot;</span>]}}}&#x27; \\
        -<span class="hljs-keyword">H</span> <span class="hljs-string">&quot;Authorization: Bearer $}{HF_API_TOKEN}&quot;</span>
# {<span class="hljs-string">&quot;answer&quot;</span>:<span class="hljs-string">&quot;AVERAGE &gt; 36542&quot;</span>,<span class="hljs-string">&quot;coordinates&quot;</span>:[[0,1]],<span class="hljs-string">&quot;cells&quot;</span>:[<span class="hljs-string">&quot;36542&quot;</span>],<span class="hljs-string">&quot;aggregator&quot;</span>:<span class="hljs-string">&quot;AVERAGE&quot;</span>}`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){E(n,s,d),c=!0},p:x,i(s){c||(w(n.$$.fragment,s),c=!0)},o(s){y(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function PM(q){let n,c;return n=new R({props:{$$slots:{default:[NM]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){E(n,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){c||(w(n.$$.fragment,s),c=!0)},o(s){y(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function SM(q){let n,c;return n=new N({props:{code:`self.assertEqual(
    data,
    {
        "answer": "AVERAGE > 36542",
        "coordinates": [[0, 1]],
        "cells": ["36542"],
        "aggregator": "AVERAGE",
    },
)`,highlighted:`self.assertEqual(
    data,
    {
        <span class="hljs-string">&quot;answer&quot;</span>: <span class="hljs-string">&quot;AVERAGE &gt; 36542&quot;</span>,
        <span class="hljs-string">&quot;coordinates&quot;</span>: [[<span class="hljs-number">0</span>, <span class="hljs-number">1</span>]],
        <span class="hljs-string">&quot;cells&quot;</span>: [<span class="hljs-string">&quot;36542&quot;</span>],
        <span class="hljs-string">&quot;aggregator&quot;</span>: <span class="hljs-string">&quot;AVERAGE&quot;</span>,
    },
)`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){E(n,s,d),c=!0},p:x,i(s){c||(w(n.$$.fragment,s),c=!0)},o(s){y(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function IM(q){let n,c;return n=new R({props:{$$slots:{default:[SM]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){E(n,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){c||(w(n.$$.fragment,s),c=!0)},o(s){y(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function HM(q){let n,c,s,d,$,k,A;return{c(){n=r("p"),c=r("strong"),s=i("Recommended model"),d=i(`:
`),$=r("a"),k=i("deepset/roberta-base-squad2"),A=i("."),this.h()},l(T){n=o(T,"P",{});var j=l(n);c=o(j,"STRONG",{});var O=l(c);s=u(O,"Recommended model"),O.forEach(t),d=u(j,`:
`),$=o(j,"A",{href:!0,rel:!0});var D=l($);k=u(D,"deepset/roberta-base-squad2"),D.forEach(t),A=u(j,"."),j.forEach(t),this.h()},h(){p($,"href","https://huggingface.co/deepset/roberta-base-squad2"),p($,"rel","nofollow")},m(T,j){m(T,n,j),e(n,c),e(c,s),e(n,d),e(n,$),e($,k),e(n,A)},d(T){T&&t(n)}}}function BM(q){let n,c;return n=new N({props:{code:`self.assertEqual(
    deep_round(data),
    {"score": 0.9327, "start": 11, "end": 16, "answer": "Clara"},
)`,highlighted:`self.assertEqual(
    deep_round(data),
    {<span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.9327</span>, <span class="hljs-string">&quot;start&quot;</span>: <span class="hljs-number">11</span>, <span class="hljs-string">&quot;end&quot;</span>: <span class="hljs-number">16</span>, <span class="hljs-string">&quot;answer&quot;</span>: <span class="hljs-string">&quot;Clara&quot;</span>},
)`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){E(n,s,d),c=!0},p:x,i(s){c||(w(n.$$.fragment,s),c=!0)},o(s){y(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function GM(q){let n,c;return n=new R({props:{$$slots:{default:[BM]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){E(n,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){c||(w(n.$$.fragment,s),c=!0)},o(s){y(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function CM(q){let n,c;return n=new N({props:{code:`import fetch from "node-fetch";
async function query(data) {
    const response = await fetch(
        "https://api-inference.huggingface.co/models/deepset/roberta-base-squad2",
        {
            headers: { Authorization: \`Bearer $}{API_TOKEN}\` },
            method: "POST",
            body: JSON.stringify(data),
        }
    );
    const result = await response.json();
    return result;
}
query({inputs:{question:"What is my name?",context:"My name is Clara and I live in Berkeley."}}).then((response) => {
    console.log(JSON.stringify(response));
});
// {"score":0.933128833770752,"start":11,"end":16,"answer":"Clara"}`,highlighted:`<span class="hljs-keyword">import</span> fetch <span class="hljs-keyword">from</span> <span class="hljs-string">&quot;node-fetch&quot;</span>;
<span class="hljs-keyword">async</span> <span class="hljs-keyword">function</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">data</span>) {
    <span class="hljs-keyword">const</span> response = <span class="hljs-keyword">await</span> <span class="hljs-title function_">fetch</span>(
        <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/deepset/roberta-base-squad2&quot;</span>,
        {
            <span class="hljs-attr">headers</span>: { <span class="hljs-title class_">Authorization</span>: <span class="hljs-string">\`Bearer <span class="hljs-subst">$}{API_TOKEN}</span>\`</span> },
            <span class="hljs-attr">method</span>: <span class="hljs-string">&quot;POST&quot;</span>,
            <span class="hljs-attr">body</span>: <span class="hljs-title class_">JSON</span>.<span class="hljs-title function_">stringify</span>(data),
        }
    );
    <span class="hljs-keyword">const</span> result = <span class="hljs-keyword">await</span> response.<span class="hljs-title function_">json</span>();
    <span class="hljs-keyword">return</span> result;
}
<span class="hljs-title function_">query</span>({<span class="hljs-attr">inputs</span>:{<span class="hljs-attr">question</span>:<span class="hljs-string">&quot;What is my name?&quot;</span>,<span class="hljs-attr">context</span>:<span class="hljs-string">&quot;My name is Clara and I live in Berkeley.&quot;</span>}}).<span class="hljs-title function_">then</span>(<span class="hljs-function">(<span class="hljs-params">response</span>) =&gt;</span> {
    <span class="hljs-variable language_">console</span>.<span class="hljs-title function_">log</span>(<span class="hljs-title class_">JSON</span>.<span class="hljs-title function_">stringify</span>(response));
});
<span class="hljs-comment">// {&quot;score&quot;:0.933128833770752,&quot;start&quot;:11,&quot;end&quot;:16,&quot;answer&quot;:&quot;Clara&quot;}</span>`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){E(n,s,d),c=!0},p:x,i(s){c||(w(n.$$.fragment,s),c=!0)},o(s){y(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function MM(q){let n,c;return n=new R({props:{$$slots:{default:[CM]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){E(n,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){c||(w(n.$$.fragment,s),c=!0)},o(s){y(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function FM(q){let n,c;return n=new N({props:{code:`curl https://api-inference.huggingface.co/models/deepset/roberta-base-squad2 \\
        -X POST \\
        -d '{"inputs":{"question":"What is my name?","context":"My name is Clara and I live in Berkeley."}}' \\
        -H "Authorization: Bearer $}{HF_API_TOKEN}"
# {"score":0.933128833770752,"start":11,"end":16,"answer":"Clara"}`,highlighted:`curl https:<span class="hljs-comment">//api-inference.huggingface.co/models/deepset/roberta-base-squad2 \\</span>
        -X <span class="hljs-keyword">POST</span> \\
        -<span class="hljs-keyword">d</span> &#x27;{<span class="hljs-string">&quot;inputs&quot;</span>:{<span class="hljs-string">&quot;question&quot;</span>:<span class="hljs-string">&quot;What is my name?&quot;</span>,<span class="hljs-string">&quot;context&quot;</span>:<span class="hljs-string">&quot;My name is Clara and I live in Berkeley.&quot;</span>}}&#x27; \\
        -<span class="hljs-keyword">H</span> <span class="hljs-string">&quot;Authorization: Bearer $}{HF_API_TOKEN}&quot;</span>
# {<span class="hljs-string">&quot;score&quot;</span>:0.933128833770752,<span class="hljs-string">&quot;start&quot;</span>:11,<span class="hljs-string">&quot;end&quot;</span>:16,<span class="hljs-string">&quot;answer&quot;</span>:<span class="hljs-string">&quot;Clara&quot;</span>}`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){E(n,s,d),c=!0},p:x,i(s){c||(w(n.$$.fragment,s),c=!0)},o(s){y(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function UM(q){let n,c;return n=new R({props:{$$slots:{default:[FM]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){E(n,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){c||(w(n.$$.fragment,s),c=!0)},o(s){y(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function LM(q){let n,c;return n=new N({props:{code:`self.assertEqual(
    deep_round(data),
    {"score": 0.9327, "start": 11, "end": 16, "answer": "Clara"},
)`,highlighted:`self.assertEqual(
    deep_round(data),
    {<span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.9327</span>, <span class="hljs-string">&quot;start&quot;</span>: <span class="hljs-number">11</span>, <span class="hljs-string">&quot;end&quot;</span>: <span class="hljs-number">16</span>, <span class="hljs-string">&quot;answer&quot;</span>: <span class="hljs-string">&quot;Clara&quot;</span>},
)`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){E(n,s,d),c=!0},p:x,i(s){c||(w(n.$$.fragment,s),c=!0)},o(s){y(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function zM(q){let n,c;return n=new R({props:{$$slots:{default:[LM]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){E(n,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){c||(w(n.$$.fragment,s),c=!0)},o(s){y(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function JM(q){let n,c,s,d,$,k;return{c(){n=r("p"),c=r("strong"),s=i("Recommended model"),d=i(`:
`),$=r("a"),k=i("distilbert-base-uncased-finetuned-sst-2-english"),this.h()},l(A){n=o(A,"P",{});var T=l(n);c=o(T,"STRONG",{});var j=l(c);s=u(j,"Recommended model"),j.forEach(t),d=u(T,`:
`),$=o(T,"A",{href:!0,rel:!0});var O=l($);k=u(O,"distilbert-base-uncased-finetuned-sst-2-english"),O.forEach(t),T.forEach(t),this.h()},h(){p($,"href","https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english"),p($,"rel","nofollow")},m(A,T){m(A,n,T),e(n,c),e(c,s),e(n,d),e(n,$),e($,k)},d(A){A&&t(n)}}}function WM(q){let n,c;return n=new N({props:{code:`self.assertEqual(
    deep_round(data),
    [
        [
            {"label": "NEGATIVE", "score": 0.0001},
            {"label": "POSITIVE", "score": 0.9999},
        ]
    ],
)`,highlighted:`self.assertEqual(
    deep_round(data),
    [
        [
            {<span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;NEGATIVE&quot;</span>, <span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.0001</span>},
            {<span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;POSITIVE&quot;</span>, <span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.9999</span>},
        ]
    ],
)`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){E(n,s,d),c=!0},p:x,i(s){c||(w(n.$$.fragment,s),c=!0)},o(s){y(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function YM(q){let n,c;return n=new R({props:{$$slots:{default:[WM]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){E(n,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){c||(w(n.$$.fragment,s),c=!0)},o(s){y(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function KM(q){let n,c;return n=new N({props:{code:`import fetch from "node-fetch";
async function query(data) {
    const response = await fetch(
        "https://api-inference.huggingface.co/models/distilbert-base-uncased-finetuned-sst-2-english",
        {
            headers: { Authorization: \`Bearer $}{API_TOKEN}\` },
            method: "POST",
            body: JSON.stringify(data),
        }
    );
    const result = await response.json();
    return result;
}
query({inputs:"I like you. I love you"}).then((response) => {
    console.log(JSON.stringify(response));
});
// [[{"label":"NEGATIVE","score":0.0001261125144083053},{"label":"POSITIVE","score":0.9998738765716553}]]`,highlighted:`<span class="hljs-keyword">import</span> fetch <span class="hljs-keyword">from</span> <span class="hljs-string">&quot;node-fetch&quot;</span>;
<span class="hljs-keyword">async</span> <span class="hljs-keyword">function</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">data</span>) {
    <span class="hljs-keyword">const</span> response = <span class="hljs-keyword">await</span> <span class="hljs-title function_">fetch</span>(
        <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/distilbert-base-uncased-finetuned-sst-2-english&quot;</span>,
        {
            <span class="hljs-attr">headers</span>: { <span class="hljs-title class_">Authorization</span>: <span class="hljs-string">\`Bearer <span class="hljs-subst">$}{API_TOKEN}</span>\`</span> },
            <span class="hljs-attr">method</span>: <span class="hljs-string">&quot;POST&quot;</span>,
            <span class="hljs-attr">body</span>: <span class="hljs-title class_">JSON</span>.<span class="hljs-title function_">stringify</span>(data),
        }
    );
    <span class="hljs-keyword">const</span> result = <span class="hljs-keyword">await</span> response.<span class="hljs-title function_">json</span>();
    <span class="hljs-keyword">return</span> result;
}
<span class="hljs-title function_">query</span>({<span class="hljs-attr">inputs</span>:<span class="hljs-string">&quot;I like you. I love you&quot;</span>}).<span class="hljs-title function_">then</span>(<span class="hljs-function">(<span class="hljs-params">response</span>) =&gt;</span> {
    <span class="hljs-variable language_">console</span>.<span class="hljs-title function_">log</span>(<span class="hljs-title class_">JSON</span>.<span class="hljs-title function_">stringify</span>(response));
});
<span class="hljs-comment">// [[{&quot;label&quot;:&quot;NEGATIVE&quot;,&quot;score&quot;:0.0001261125144083053},{&quot;label&quot;:&quot;POSITIVE&quot;,&quot;score&quot;:0.9998738765716553}]]</span>`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){E(n,s,d),c=!0},p:x,i(s){c||(w(n.$$.fragment,s),c=!0)},o(s){y(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function VM(q){let n,c;return n=new R({props:{$$slots:{default:[KM]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){E(n,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){c||(w(n.$$.fragment,s),c=!0)},o(s){y(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function XM(q){let n,c;return n=new N({props:{code:`curl https://api-inference.huggingface.co/models/distilbert-base-uncased-finetuned-sst-2-english \\
        -X POST \\
        -d '{"inputs":"I like you. I love you"}' \\
        -H "Authorization: Bearer $}{HF_API_TOKEN}"
# [[{"label":"NEGATIVE","score":0.0001261125144083053},{"label":"POSITIVE","score":0.9998738765716553}]]`,highlighted:`curl https:<span class="hljs-comment">//api-inference.huggingface.co/models/distilbert-base-uncased-finetuned-sst-2-english \\
        -X POST \\
        -d &#x27;{&quot;inputs&quot;:&quot;I like you. I love you&quot;}&#x27; \\
        -H &quot;Authorization: Bearer $}{HF_API_TOKEN}&quot;</span>
# [[{<span class="hljs-string">&quot;label&quot;</span>:<span class="hljs-string">&quot;NEGATIVE&quot;</span>,<span class="hljs-string">&quot;score&quot;</span>:<span class="hljs-number">0.0001261125144083053</span>},{<span class="hljs-string">&quot;label&quot;</span>:<span class="hljs-string">&quot;POSITIVE&quot;</span>,<span class="hljs-string">&quot;score&quot;</span>:<span class="hljs-number">0.9998738765716553</span>}]]`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){E(n,s,d),c=!0},p:x,i(s){c||(w(n.$$.fragment,s),c=!0)},o(s){y(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function QM(q){let n,c;return n=new R({props:{$$slots:{default:[XM]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){E(n,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){c||(w(n.$$.fragment,s),c=!0)},o(s){y(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function ZM(q){let n,c;return n=new N({props:{code:`self.assertEqual(
    deep_round(data),
    [
        [
            {"label": "NEGATIVE", "score": 0.0001},
            {"label": "POSITIVE", "score": 0.9999},
        ]
    ],
)`,highlighted:`self.assertEqual(
    deep_round(data),
    [
        [
            {<span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;NEGATIVE&quot;</span>, <span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.0001</span>},
            {<span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;POSITIVE&quot;</span>, <span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.9999</span>},
        ]
    ],
)`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){E(n,s,d),c=!0},p:x,i(s){c||(w(n.$$.fragment,s),c=!0)},o(s){y(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function eF(q){let n,c;return n=new R({props:{$$slots:{default:[ZM]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){E(n,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){c||(w(n.$$.fragment,s),c=!0)},o(s){y(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function tF(q){let n,c,s,d,$,k;return{c(){n=r("p"),c=r("strong"),s=i("Recommended model"),d=i(`:
`),$=r("a"),k=i("dbmdz/bert-large-cased-finetuned-conll03-english"),this.h()},l(A){n=o(A,"P",{});var T=l(n);c=o(T,"STRONG",{});var j=l(c);s=u(j,"Recommended model"),j.forEach(t),d=u(T,`:
`),$=o(T,"A",{href:!0,rel:!0});var O=l($);k=u(O,"dbmdz/bert-large-cased-finetuned-conll03-english"),O.forEach(t),T.forEach(t),this.h()},h(){p($,"href","https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english"),p($,"rel","nofollow")},m(A,T){m(A,n,T),e(n,c),e(c,s),e(n,d),e(n,$),e($,k)},d(A){A&&t(n)}}}function sF(q){let n,c;return n=new N({props:{code:`self.assertEqual(
    deep_round(data),
    [
        {
            "entity_group": "PER",
            "score": 0.9991,
            "word": "Sarah Jessica Parker",
            "start": 11,
            "end": 31,
        },
        {
            "entity_group": "PER",
            "score": 0.998,
            "word": "Jessica",
            "start": 52,
            "end": 59,
        },
    ],
)`,highlighted:`self.assertEqual(
    deep_round(data),
    [
        {
            <span class="hljs-string">&quot;entity_group&quot;</span>: <span class="hljs-string">&quot;PER&quot;</span>,
            <span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.9991</span>,
            <span class="hljs-string">&quot;word&quot;</span>: <span class="hljs-string">&quot;Sarah Jessica Parker&quot;</span>,
            <span class="hljs-string">&quot;start&quot;</span>: <span class="hljs-number">11</span>,
            <span class="hljs-string">&quot;end&quot;</span>: <span class="hljs-number">31</span>,
        },
        {
            <span class="hljs-string">&quot;entity_group&quot;</span>: <span class="hljs-string">&quot;PER&quot;</span>,
            <span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.998</span>,
            <span class="hljs-string">&quot;word&quot;</span>: <span class="hljs-string">&quot;Jessica&quot;</span>,
            <span class="hljs-string">&quot;start&quot;</span>: <span class="hljs-number">52</span>,
            <span class="hljs-string">&quot;end&quot;</span>: <span class="hljs-number">59</span>,
        },
    ],
)`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){E(n,s,d),c=!0},p:x,i(s){c||(w(n.$$.fragment,s),c=!0)},o(s){y(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function aF(q){let n,c;return n=new R({props:{$$slots:{default:[sF]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){E(n,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){c||(w(n.$$.fragment,s),c=!0)},o(s){y(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function nF(q){let n,c;return n=new N({props:{code:`import fetch from "node-fetch";
async function query(data) {
    const response = await fetch(
        "https://api-inference.huggingface.co/models/dbmdz/bert-large-cased-finetuned-conll03-english",
        {
            headers: { Authorization: \`Bearer $}{API_TOKEN}\` },
            method: "POST",
            body: JSON.stringify(data),
        }
    );
    const result = await response.json();
    return result;
}
query({inputs:"My name is Sarah Jessica Parker but you can call me Jessica"}).then((response) => {
    console.log(JSON.stringify(response));
});
// [{"entity_group":"PER","score":0.9991337060928345,"word":"Sarah Jessica Parker","start":11,"end":31},{"entity_group":"PER","score":0.9979912042617798,"word":"Jessica","start":52,"end":59}]`,highlighted:`<span class="hljs-keyword">import</span> <span class="hljs-keyword">fetch</span> <span class="hljs-keyword">from</span> &quot;node-fetch&quot;;
async <span class="hljs-keyword">function</span> query(data) {
    const response = await <span class="hljs-keyword">fetch</span>(
        &quot;https://api-inference.huggingface.co/models/dbmdz/bert-large-cased-finetuned-conll03-english&quot;,
        {
            headers: { <span class="hljs-keyword">Authorization</span>: \`Bearer $}{API_TOKEN}\` },
            <span class="hljs-keyword">method</span>: &quot;POST&quot;,
            body: <span class="hljs-type">JSON</span>.stringify(data),
        }
    );
    const result = await response.json();
    <span class="hljs-keyword">return</span> result;
}
query({inputs:&quot;My name is Sarah Jessica Parker but you can call me Jessica&quot;}).<span class="hljs-keyword">then</span>((response) =&gt; {
    console.log(<span class="hljs-type">JSON</span>.stringify(response));
});
// [{&quot;entity_group&quot;:&quot;PER&quot;,&quot;score&quot;:<span class="hljs-number">0.9991337060928345</span>,&quot;word&quot;:&quot;Sarah Jessica Parker&quot;,&quot;start&quot;:<span class="hljs-number">11</span>,&quot;end&quot;:<span class="hljs-number">31</span>},{&quot;entity_group&quot;:&quot;PER&quot;,&quot;score&quot;:<span class="hljs-number">0.9979912042617798</span>,&quot;word&quot;:&quot;Jessica&quot;,&quot;start&quot;:<span class="hljs-number">52</span>,&quot;end&quot;:<span class="hljs-number">59</span>}]`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){E(n,s,d),c=!0},p:x,i(s){c||(w(n.$$.fragment,s),c=!0)},o(s){y(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function rF(q){let n,c;return n=new R({props:{$$slots:{default:[nF]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){E(n,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){c||(w(n.$$.fragment,s),c=!0)},o(s){y(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function oF(q){let n,c;return n=new N({props:{code:`curl https://api-inference.huggingface.co/models/dbmdz/bert-large-cased-finetuned-conll03-english \\
        -X POST \\
        -d '{"inputs":"My name is Sarah Jessica Parker but you can call me Jessica"}' \\
        -H "Authorization: Bearer $}{HF_API_TOKEN}"
# [{"entity_group":"PER","score":0.9991337060928345,"word":"Sarah Jessica Parker","start":11,"end":31},{"entity_group":"PER","score":0.9979912042617798,"word":"Jessica","start":52,"end":59}]`,highlighted:`curl https:<span class="hljs-comment">//api-inference.huggingface.co/models/dbmdz/bert-large-cased-finetuned-conll03-english \\
        -X POST \\
        -d &#x27;{&quot;inputs&quot;:&quot;My name is Sarah Jessica Parker but you can call me Jessica&quot;}&#x27; \\
        -H &quot;Authorization: Bearer $}{HF_API_TOKEN}&quot;</span>
# [{<span class="hljs-string">&quot;entity_group&quot;</span>:<span class="hljs-string">&quot;PER&quot;</span>,<span class="hljs-string">&quot;score&quot;</span>:<span class="hljs-number">0.9991337060928345</span>,<span class="hljs-string">&quot;word&quot;</span>:<span class="hljs-string">&quot;Sarah Jessica Parker&quot;</span>,<span class="hljs-string">&quot;start&quot;</span>:<span class="hljs-number">11</span>,<span class="hljs-string">&quot;end&quot;</span>:<span class="hljs-number">31</span>},{<span class="hljs-string">&quot;entity_group&quot;</span>:<span class="hljs-string">&quot;PER&quot;</span>,<span class="hljs-string">&quot;score&quot;</span>:<span class="hljs-number">0.9979912042617798</span>,<span class="hljs-string">&quot;word&quot;</span>:<span class="hljs-string">&quot;Jessica&quot;</span>,<span class="hljs-string">&quot;start&quot;</span>:<span class="hljs-number">52</span>,<span class="hljs-string">&quot;end&quot;</span>:<span class="hljs-number">59</span>}]`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){E(n,s,d),c=!0},p:x,i(s){c||(w(n.$$.fragment,s),c=!0)},o(s){y(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function lF(q){let n,c;return n=new R({props:{$$slots:{default:[oF]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){E(n,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){c||(w(n.$$.fragment,s),c=!0)},o(s){y(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function iF(q){let n,c;return n=new N({props:{code:`self.assertEqual(
    deep_round(data),
    [
        {
            "entity_group": "PER",
            "score": 0.9991,
            "word": "Sarah Jessica Parker",
            "start": 11,
            "end": 31,
        },
        {
            "entity_group": "PER",
            "score": 0.998,
            "word": "Jessica",
            "start": 52,
            "end": 59,
        },
    ],
)`,highlighted:`self.assertEqual(
    deep_round(data),
    [
        {
            <span class="hljs-string">&quot;entity_group&quot;</span>: <span class="hljs-string">&quot;PER&quot;</span>,
            <span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.9991</span>,
            <span class="hljs-string">&quot;word&quot;</span>: <span class="hljs-string">&quot;Sarah Jessica Parker&quot;</span>,
            <span class="hljs-string">&quot;start&quot;</span>: <span class="hljs-number">11</span>,
            <span class="hljs-string">&quot;end&quot;</span>: <span class="hljs-number">31</span>,
        },
        {
            <span class="hljs-string">&quot;entity_group&quot;</span>: <span class="hljs-string">&quot;PER&quot;</span>,
            <span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.998</span>,
            <span class="hljs-string">&quot;word&quot;</span>: <span class="hljs-string">&quot;Jessica&quot;</span>,
            <span class="hljs-string">&quot;start&quot;</span>: <span class="hljs-number">52</span>,
            <span class="hljs-string">&quot;end&quot;</span>: <span class="hljs-number">59</span>,
        },
    ],
)`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){E(n,s,d),c=!0},p:x,i(s){c||(w(n.$$.fragment,s),c=!0)},o(s){y(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function uF(q){let n,c;return n=new R({props:{$$slots:{default:[iF]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){E(n,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){c||(w(n.$$.fragment,s),c=!0)},o(s){y(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function cF(q){let n,c,s,d,$,k,A;return{c(){n=r("p"),c=r("strong"),s=i("Recommended model"),d=i(": "),$=r("a"),k=i("gpt2"),A=i(" (it\u2019s a simple model, but fun to play with)."),this.h()},l(T){n=o(T,"P",{});var j=l(n);c=o(j,"STRONG",{});var O=l(c);s=u(O,"Recommended model"),O.forEach(t),d=u(j,": "),$=o(j,"A",{href:!0,rel:!0});var D=l($);k=u(D,"gpt2"),D.forEach(t),A=u(j," (it\u2019s a simple model, but fun to play with)."),j.forEach(t),this.h()},h(){p($,"href","https://huggingface.co/gpt2"),p($,"rel","nofollow")},m(T,j){m(T,n,j),e(n,c),e(c,s),e(n,d),e(n,$),e($,k),e(n,A)},d(T){T&&t(n)}}}function pF(q){let n,c;return n=new N({props:{code:`data == [
    {
        "generated_text": 'The answer to the universe is that we are the creation of the entire universe," says Fitch.\\n\\nAs of the 1960s, six times as many Americans still make fewer than six bucks ($}17) per year on their way to retirement.'
    }
]`,highlighted:`data == [
    {
        <span class="hljs-string">&quot;generated_text&quot;</span>: <span class="hljs-string">&#x27;The answer to the universe is that we are the creation of the entire universe,&quot; says Fitch.\\n\\nAs of the 1960s, six times as many Americans still make fewer than six bucks ($}17) per year on their way to retirement.&#x27;</span>
    }
]`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){E(n,s,d),c=!0},p:x,i(s){c||(w(n.$$.fragment,s),c=!0)},o(s){y(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function fF(q){let n,c;return n=new R({props:{$$slots:{default:[pF]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){E(n,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){c||(w(n.$$.fragment,s),c=!0)},o(s){y(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function hF(q){let n,c;return n=new N({props:{code:`import fetch from "node-fetch";
async function query(data) {
    const response = await fetch(
        "https://api-inference.huggingface.co/models/gpt2",
        {
            headers: { Authorization: \`Bearer $}{API_TOKEN}\` },
            method: "POST",
            body: JSON.stringify(data),
        }
    );
    const result = await response.json();
    return result;
}
query({inputs:"The answer to the universe is"}).then((response) => {
    console.log(JSON.stringify(response));
});
// [{"generated_text":"The answer to the universe is in a different shape (or shapeless) than"}]`,highlighted:`<span class="hljs-keyword">import</span> fetch <span class="hljs-keyword">from</span> <span class="hljs-string">&quot;node-fetch&quot;</span>;
<span class="hljs-keyword">async</span> <span class="hljs-keyword">function</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">data</span>) {
    <span class="hljs-keyword">const</span> response = <span class="hljs-keyword">await</span> <span class="hljs-title function_">fetch</span>(
        <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/gpt2&quot;</span>,
        {
            <span class="hljs-attr">headers</span>: { <span class="hljs-title class_">Authorization</span>: <span class="hljs-string">\`Bearer <span class="hljs-subst">$}{API_TOKEN}</span>\`</span> },
            <span class="hljs-attr">method</span>: <span class="hljs-string">&quot;POST&quot;</span>,
            <span class="hljs-attr">body</span>: <span class="hljs-title class_">JSON</span>.<span class="hljs-title function_">stringify</span>(data),
        }
    );
    <span class="hljs-keyword">const</span> result = <span class="hljs-keyword">await</span> response.<span class="hljs-title function_">json</span>();
    <span class="hljs-keyword">return</span> result;
}
<span class="hljs-title function_">query</span>({<span class="hljs-attr">inputs</span>:<span class="hljs-string">&quot;The answer to the universe is&quot;</span>}).<span class="hljs-title function_">then</span>(<span class="hljs-function">(<span class="hljs-params">response</span>) =&gt;</span> {
    <span class="hljs-variable language_">console</span>.<span class="hljs-title function_">log</span>(<span class="hljs-title class_">JSON</span>.<span class="hljs-title function_">stringify</span>(response));
});
<span class="hljs-comment">// [{&quot;generated_text&quot;:&quot;The answer to the universe is in a different shape (or shapeless) than&quot;}]</span>`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){E(n,s,d),c=!0},p:x,i(s){c||(w(n.$$.fragment,s),c=!0)},o(s){y(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function dF(q){let n,c;return n=new R({props:{$$slots:{default:[hF]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){E(n,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){c||(w(n.$$.fragment,s),c=!0)},o(s){y(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function gF(q){let n,c;return n=new N({props:{code:`curl https://api-inference.huggingface.co/models/gpt2 \\
        -X POST \\
        -d '{"inputs":"The answer to the universe is"}' \\
        -H "Authorization: Bearer $}{HF_API_TOKEN}"
# [{"generated_text":"The answer to the universe is in a different shape (or shapeless) than"}]`,highlighted:`curl https:<span class="hljs-regexp">//</span>api-inference.huggingface.co<span class="hljs-regexp">/models/g</span>pt2 \\
        -X POST \\
        -d <span class="hljs-string">&#x27;{&quot;inputs&quot;:&quot;The answer to the universe is&quot;}&#x27;</span> \\
        -H <span class="hljs-string">&quot;Authorization: Bearer $}{HF_API_TOKEN}&quot;</span>
<span class="hljs-comment"># [{&quot;generated_text&quot;:&quot;The answer to the universe is in a different shape (or shapeless) than&quot;}]</span>`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){E(n,s,d),c=!0},p:x,i(s){c||(w(n.$$.fragment,s),c=!0)},o(s){y(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function mF(q){let n,c;return n=new R({props:{$$slots:{default:[gF]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){E(n,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){c||(w(n.$$.fragment,s),c=!0)},o(s){y(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function $F(q){let n,c;return n=new N({props:{code:`data == [
    {
        "generated_text": 'The answer to the universe is that we are the creation of the entire universe," says Fitch.\\n\\nAs of the 1960s, six times as many Americans still make fewer than six bucks ($}17) per year on their way to retirement.'
    }
]`,highlighted:`data == [
    {
        <span class="hljs-string">&quot;generated_text&quot;</span>: <span class="hljs-string">&#x27;The answer to the universe is that we are the creation of the entire universe,&quot; says Fitch.\\n\\nAs of the 1960s, six times as many Americans still make fewer than six bucks ($}17) per year on their way to retirement.&#x27;</span>
    }
]`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){E(n,s,d),c=!0},p:x,i(s){c||(w(n.$$.fragment,s),c=!0)},o(s){y(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function qF(q){let n,c;return n=new R({props:{$$slots:{default:[$F]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){E(n,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){c||(w(n.$$.fragment,s),c=!0)},o(s){y(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function _F(q){let n,c,s,d,$,k,A;return{c(){n=r("p"),c=r("strong"),s=i("Recommended model"),d=i(`:
`),$=r("a"),k=i("bert-base-uncased"),A=i(" (it\u2019s a simple model, but fun to play with)."),this.h()},l(T){n=o(T,"P",{});var j=l(n);c=o(j,"STRONG",{});var O=l(c);s=u(O,"Recommended model"),O.forEach(t),d=u(j,`:
`),$=o(j,"A",{href:!0,rel:!0});var D=l($);k=u(D,"bert-base-uncased"),D.forEach(t),A=u(j," (it\u2019s a simple model, but fun to play with)."),j.forEach(t),this.h()},h(){p($,"href","https://huggingface.co/bert-base-uncased"),p($,"rel","nofollow")},m(T,j){m(T,n,j),e(n,c),e(c,s),e(n,d),e(n,$),e($,k),e(n,A)},d(T){T&&t(n)}}}function vF(q){let n,c;return n=new N({props:{code:`self.assertEqual(
    deep_round(data),
    [
        {
            "sequence": "the answer to the universe is no.",
            "score": 0.1696,
            "token": 2053,
            "token_str": "no",
        },
        {
            "sequence": "the answer to the universe is nothing.",
            "score": 0.0734,
            "token": 2498,
            "token_str": "nothing",
        },
        {
            "sequence": "the answer to the universe is yes.",
            "score": 0.0580,
            "token": 2748,
            "token_str": "yes",
        },
        {
            "sequence": "the answer to the universe is unknown.",
            "score": 0.044,
            "token": 4242,
            "token_str": "unknown",
        },
        {
            "sequence": "the answer to the universe is simple.",
            "score": 0.0402,
            "token": 3722,
            "token_str": "simple",
        },
    ],
)`,highlighted:`self.assertEqual(
    deep_round(data),
    [
        {
            <span class="hljs-string">&quot;sequence&quot;</span>: <span class="hljs-string">&quot;the answer to the universe is no.&quot;</span>,
            <span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.1696</span>,
            <span class="hljs-string">&quot;token&quot;</span>: <span class="hljs-number">2053</span>,
            <span class="hljs-string">&quot;token_str&quot;</span>: <span class="hljs-string">&quot;no&quot;</span>,
        },
        {
            <span class="hljs-string">&quot;sequence&quot;</span>: <span class="hljs-string">&quot;the answer to the universe is nothing.&quot;</span>,
            <span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.0734</span>,
            <span class="hljs-string">&quot;token&quot;</span>: <span class="hljs-number">2498</span>,
            <span class="hljs-string">&quot;token_str&quot;</span>: <span class="hljs-string">&quot;nothing&quot;</span>,
        },
        {
            <span class="hljs-string">&quot;sequence&quot;</span>: <span class="hljs-string">&quot;the answer to the universe is yes.&quot;</span>,
            <span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.0580</span>,
            <span class="hljs-string">&quot;token&quot;</span>: <span class="hljs-number">2748</span>,
            <span class="hljs-string">&quot;token_str&quot;</span>: <span class="hljs-string">&quot;yes&quot;</span>,
        },
        {
            <span class="hljs-string">&quot;sequence&quot;</span>: <span class="hljs-string">&quot;the answer to the universe is unknown.&quot;</span>,
            <span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.044</span>,
            <span class="hljs-string">&quot;token&quot;</span>: <span class="hljs-number">4242</span>,
            <span class="hljs-string">&quot;token_str&quot;</span>: <span class="hljs-string">&quot;unknown&quot;</span>,
        },
        {
            <span class="hljs-string">&quot;sequence&quot;</span>: <span class="hljs-string">&quot;the answer to the universe is simple.&quot;</span>,
            <span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.0402</span>,
            <span class="hljs-string">&quot;token&quot;</span>: <span class="hljs-number">3722</span>,
            <span class="hljs-string">&quot;token_str&quot;</span>: <span class="hljs-string">&quot;simple&quot;</span>,
        },
    ],
)`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){E(n,s,d),c=!0},p:x,i(s){c||(w(n.$$.fragment,s),c=!0)},o(s){y(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function EF(q){let n,c;return n=new R({props:{$$slots:{default:[vF]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){E(n,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){c||(w(n.$$.fragment,s),c=!0)},o(s){y(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function wF(q){let n,c;return n=new N({props:{code:`import fetch from "node-fetch";
async function query(data) {
    const response = await fetch(
        "https://api-inference.huggingface.co/models/bert-base-uncased",
        {
            headers: { Authorization: \`Bearer $}{API_TOKEN}\` },
            method: "POST",
            body: JSON.stringify(data),
        }
    );
    const result = await response.json();
    return result;
}
query({inputs:"The answer to the universe is [MASK]."}).then((response) => {
    console.log(JSON.stringify(response));
});
// [{"sequence":"the answer to the universe is no.","score":0.16963955760002136,"token":2053,"token_str":"no"},{"sequence":"the answer to the universe is nothing.","score":0.07344776391983032,"token":2498,"token_str":"nothing"},{"sequence":"the answer to the universe is yes.","score":0.05803241208195686,"token":2748,"token_str":"yes"},{"sequence":"the answer to the universe is unknown.","score":0.043957844376564026,"token":4242,"token_str":"unknown"},{"sequence":"the answer to the universe is simple.","score":0.04015745222568512,"token":3722,"token_str":"simple"}]`,highlighted:`<span class="hljs-keyword">import</span> <span class="hljs-keyword">fetch</span> <span class="hljs-keyword">from</span> &quot;node-fetch&quot;;
async <span class="hljs-keyword">function</span> query(data) {
    const response = await <span class="hljs-keyword">fetch</span>(
        &quot;https://api-inference.huggingface.co/models/bert-base-uncased&quot;,
        {
            headers: { <span class="hljs-keyword">Authorization</span>: \`Bearer $}{API_TOKEN}\` },
            <span class="hljs-keyword">method</span>: &quot;POST&quot;,
            body: <span class="hljs-type">JSON</span>.stringify(data),
        }
    );
    const result = await response.json();
    <span class="hljs-keyword">return</span> result;
}
query({inputs:&quot;The answer to the universe is [MASK].&quot;}).<span class="hljs-keyword">then</span>((response) =&gt; {
    console.log(<span class="hljs-type">JSON</span>.stringify(response));
});
// [{&quot;sequence&quot;:&quot;the answer to the universe is no.&quot;,&quot;score&quot;:<span class="hljs-number">0.16963955760002136</span>,&quot;token&quot;:<span class="hljs-number">2053</span>,&quot;token_str&quot;:&quot;no&quot;},{&quot;sequence&quot;:&quot;the answer to the universe is nothing.&quot;,&quot;score&quot;:<span class="hljs-number">0.07344776391983032</span>,&quot;token&quot;:<span class="hljs-number">2498</span>,&quot;token_str&quot;:&quot;nothing&quot;},{&quot;sequence&quot;:&quot;the answer to the universe is yes.&quot;,&quot;score&quot;:<span class="hljs-number">0.05803241208195686</span>,&quot;token&quot;:<span class="hljs-number">2748</span>,&quot;token_str&quot;:&quot;yes&quot;},{&quot;sequence&quot;:&quot;the answer to the universe is unknown.&quot;,&quot;score&quot;:<span class="hljs-number">0.043957844376564026</span>,&quot;token&quot;:<span class="hljs-number">4242</span>,&quot;token_str&quot;:&quot;unknown&quot;},{&quot;sequence&quot;:&quot;the answer to the universe is simple.&quot;,&quot;score&quot;:<span class="hljs-number">0.04015745222568512</span>,&quot;token&quot;:<span class="hljs-number">3722</span>,&quot;token_str&quot;:&quot;simple&quot;}]`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){E(n,s,d),c=!0},p:x,i(s){c||(w(n.$$.fragment,s),c=!0)},o(s){y(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function yF(q){let n,c;return n=new R({props:{$$slots:{default:[wF]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){E(n,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){c||(w(n.$$.fragment,s),c=!0)},o(s){y(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function bF(q){let n,c;return n=new N({props:{code:`curl https://api-inference.huggingface.co/models/bert-base-uncased \\
        -X POST \\
        -d '{"inputs":"The answer to the universe is [MASK]."}' \\
        -H "Authorization: Bearer $}{HF_API_TOKEN}"
# [{"sequence":"the answer to the universe is no.","score":0.16963955760002136,"token":2053,"token_str":"no"},{"sequence":"the answer to the universe is nothing.","score":0.07344776391983032,"token":2498,"token_str":"nothing"},{"sequence":"the answer to the universe is yes.","score":0.05803241208195686,"token":2748,"token_str":"yes"},{"sequence":"the answer to the universe is unknown.","score":0.043957844376564026,"token":4242,"token_str":"unknown"},{"sequence":"the answer to the universe is simple.","score":0.04015745222568512,"token":3722,"token_str":"simple"}]`,highlighted:`curl https://api-inference.huggingface.co/models/bert-base-uncased \\
        -<span class="hljs-type">X</span> <span class="hljs-type">POST</span> \\
        -d <span class="hljs-string">&#x27;{&quot;inputs&quot;:&quot;The answer to the universe is [MASK].&quot;}&#x27;</span> \\
        -<span class="hljs-type">H</span> <span class="hljs-comment">&quot;Authorization: Bearer $}{HF_API_TOKEN}&quot;</span>
# [{<span class="hljs-comment">&quot;sequence&quot;</span>:<span class="hljs-comment">&quot;the answer to the universe is no.&quot;</span>,<span class="hljs-comment">&quot;score&quot;</span>:<span class="hljs-number">0.16963955760002136</span>,<span class="hljs-comment">&quot;token&quot;</span>:<span class="hljs-number">2053</span>,<span class="hljs-comment">&quot;token_str&quot;</span>:<span class="hljs-comment">&quot;no&quot;</span>},{<span class="hljs-comment">&quot;sequence&quot;</span>:<span class="hljs-comment">&quot;the answer to the universe is nothing.&quot;</span>,<span class="hljs-comment">&quot;score&quot;</span>:<span class="hljs-number">0.07344776391983032</span>,<span class="hljs-comment">&quot;token&quot;</span>:<span class="hljs-number">2498</span>,<span class="hljs-comment">&quot;token_str&quot;</span>:<span class="hljs-comment">&quot;nothing&quot;</span>},{<span class="hljs-comment">&quot;sequence&quot;</span>:<span class="hljs-comment">&quot;the answer to the universe is yes.&quot;</span>,<span class="hljs-comment">&quot;score&quot;</span>:<span class="hljs-number">0.05803241208195686</span>,<span class="hljs-comment">&quot;token&quot;</span>:<span class="hljs-number">2748</span>,<span class="hljs-comment">&quot;token_str&quot;</span>:<span class="hljs-comment">&quot;yes&quot;</span>},{<span class="hljs-comment">&quot;sequence&quot;</span>:<span class="hljs-comment">&quot;the answer to the universe is unknown.&quot;</span>,<span class="hljs-comment">&quot;score&quot;</span>:<span class="hljs-number">0.043957844376564026</span>,<span class="hljs-comment">&quot;token&quot;</span>:<span class="hljs-number">4242</span>,<span class="hljs-comment">&quot;token_str&quot;</span>:<span class="hljs-comment">&quot;unknown&quot;</span>},{<span class="hljs-comment">&quot;sequence&quot;</span>:<span class="hljs-comment">&quot;the answer to the universe is simple.&quot;</span>,<span class="hljs-comment">&quot;score&quot;</span>:<span class="hljs-number">0.04015745222568512</span>,<span class="hljs-comment">&quot;token&quot;</span>:<span class="hljs-number">3722</span>,<span class="hljs-comment">&quot;token_str&quot;</span>:<span class="hljs-comment">&quot;simple&quot;</span>}]`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){E(n,s,d),c=!0},p:x,i(s){c||(w(n.$$.fragment,s),c=!0)},o(s){y(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function jF(q){let n,c;return n=new R({props:{$$slots:{default:[bF]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){E(n,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){c||(w(n.$$.fragment,s),c=!0)},o(s){y(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function TF(q){let n,c;return n=new N({props:{code:`self.assertEqual(
    deep_round(data),
    [
        {
            "sequence": "the answer to the universe is no.",
            "score": 0.1696,
            "token": 2053,
            "token_str": "no",
        },
        {
            "sequence": "the answer to the universe is nothing.",
            "score": 0.0734,
            "token": 2498,
            "token_str": "nothing",
        },
        {
            "sequence": "the answer to the universe is yes.",
            "score": 0.0580,
            "token": 2748,
            "token_str": "yes",
        },
        {
            "sequence": "the answer to the universe is unknown.",
            "score": 0.044,
            "token": 4242,
            "token_str": "unknown",
        },
        {
            "sequence": "the answer to the universe is simple.",
            "score": 0.0402,
            "token": 3722,
            "token_str": "simple",
        },
    ],
)`,highlighted:`self.assertEqual(
    deep_round(data),
    [
        {
            <span class="hljs-string">&quot;sequence&quot;</span>: <span class="hljs-string">&quot;the answer to the universe is no.&quot;</span>,
            <span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.1696</span>,
            <span class="hljs-string">&quot;token&quot;</span>: <span class="hljs-number">2053</span>,
            <span class="hljs-string">&quot;token_str&quot;</span>: <span class="hljs-string">&quot;no&quot;</span>,
        },
        {
            <span class="hljs-string">&quot;sequence&quot;</span>: <span class="hljs-string">&quot;the answer to the universe is nothing.&quot;</span>,
            <span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.0734</span>,
            <span class="hljs-string">&quot;token&quot;</span>: <span class="hljs-number">2498</span>,
            <span class="hljs-string">&quot;token_str&quot;</span>: <span class="hljs-string">&quot;nothing&quot;</span>,
        },
        {
            <span class="hljs-string">&quot;sequence&quot;</span>: <span class="hljs-string">&quot;the answer to the universe is yes.&quot;</span>,
            <span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.0580</span>,
            <span class="hljs-string">&quot;token&quot;</span>: <span class="hljs-number">2748</span>,
            <span class="hljs-string">&quot;token_str&quot;</span>: <span class="hljs-string">&quot;yes&quot;</span>,
        },
        {
            <span class="hljs-string">&quot;sequence&quot;</span>: <span class="hljs-string">&quot;the answer to the universe is unknown.&quot;</span>,
            <span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.044</span>,
            <span class="hljs-string">&quot;token&quot;</span>: <span class="hljs-number">4242</span>,
            <span class="hljs-string">&quot;token_str&quot;</span>: <span class="hljs-string">&quot;unknown&quot;</span>,
        },
        {
            <span class="hljs-string">&quot;sequence&quot;</span>: <span class="hljs-string">&quot;the answer to the universe is simple.&quot;</span>,
            <span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.0402</span>,
            <span class="hljs-string">&quot;token&quot;</span>: <span class="hljs-number">3722</span>,
            <span class="hljs-string">&quot;token_str&quot;</span>: <span class="hljs-string">&quot;simple&quot;</span>,
        },
    ],
)`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){E(n,s,d),c=!0},p:x,i(s){c||(w(n.$$.fragment,s),c=!0)},o(s){y(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function kF(q){let n,c;return n=new R({props:{$$slots:{default:[TF]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){E(n,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){c||(w(n.$$.fragment,s),c=!0)},o(s){y(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function AF(q){let n,c,s,d,$,k,A;return{c(){n=r("p"),c=r("strong"),s=i("Recommended model"),d=i(": "),$=r("a"),k=i(`Check your
langage`),A=i("."),this.h()},l(T){n=o(T,"P",{});var j=l(n);c=o(j,"STRONG",{});var O=l(c);s=u(O,"Recommended model"),O.forEach(t),d=u(j,": "),$=o(j,"A",{href:!0,rel:!0});var D=l($);k=u(D,`Check your
langage`),D.forEach(t),A=u(j,"."),j.forEach(t),this.h()},h(){p($,"href","https://huggingface.co/models?pipeline_tag=automatic-speech-recognition"),p($,"rel","nofollow")},m(T,j){m(T,n,j),e(n,c),e(c,s),e(n,d),e(n,$),e($,k),e(n,A)},d(T){T&&t(n)}}}function DF(q){let n,c,s,d,$,k,A;return{c(){n=r("p"),c=r("strong"),s=i("English"),d=i(`:
`),$=r("a"),k=i("facebook/wav2vec2-large-960h-lv60-self"),A=i("."),this.h()},l(T){n=o(T,"P",{});var j=l(n);c=o(j,"STRONG",{});var O=l(c);s=u(O,"English"),O.forEach(t),d=u(j,`:
`),$=o(j,"A",{href:!0,rel:!0});var D=l($);k=u(D,"facebook/wav2vec2-large-960h-lv60-self"),D.forEach(t),A=u(j,"."),j.forEach(t),this.h()},h(){p($,"href","https://huggingface.co/facebook/wav2vec2-large-960h-lv60-self"),p($,"rel","nofollow")},m(T,j){m(T,n,j),e(n,c),e(c,s),e(n,d),e(n,$),e($,k),e(n,A)},d(T){T&&t(n)}}}function OF(q){let n,c;return n=new N({props:{code:`    self.assertEqual(
        data,
        {
            "text": "GOING ALONG SLUSHY COUNTRY ROADS AND SPEAKING TO DAMP AUDIENCES IN DRAUGHTY SCHOOL ROOMS DAY AFTER DAY FOR A FORTNIGHT HE'LL HAVE TO PUT IN AN APPEARANCE AT SOME PLACE OF WORSHIP ON SUNDAY MORNING AND HE CAN COME TO US IMMEDIATELY AFTERWARDS"
        },
    )`,highlighted:`    self.assertEqual(
        data,
        {
            <span class="hljs-string">&quot;text&quot;</span>: <span class="hljs-string">&quot;GOING ALONG SLUSHY COUNTRY ROADS AND SPEAKING TO DAMP AUDIENCES IN DRAUGHTY SCHOOL ROOMS DAY AFTER DAY FOR A FORTNIGHT HE&#x27;LL HAVE TO PUT IN AN APPEARANCE AT SOME PLACE OF WORSHIP ON SUNDAY MORNING AND HE CAN COME TO US IMMEDIATELY AFTERWARDS&quot;</span>
        },
    )`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){E(n,s,d),c=!0},p:x,i(s){c||(w(n.$$.fragment,s),c=!0)},o(s){y(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function xF(q){let n,c;return n=new R({props:{$$slots:{default:[OF]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){E(n,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){c||(w(n.$$.fragment,s),c=!0)},o(s){y(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function RF(q){let n,c;return n=new N({props:{code:`    import fetch from "node-fetch";
    import fs from "fs";
    async function query(filename) {
        const data = fs.readFileSync(filename);
        const response = await fetch(
            "https://api-inference.huggingface.co/models/facebook/wav2vec2-base-960h",
            {
                headers: { Authorization: \`Bearer $}{API_TOKEN}\` },
                method: "POST",
                body: data,
            }
        );
        const result = await response.json();
        return result;
    }
    query("sample1.flac").then((response) => {
        console.log(JSON.stringify(response));
    });
    // {"text":"GOING ALONG SLUSHY COUNTRY ROADS AND SPEAKING TO DAMP AUDIENCES IN DRAUGHTY SCHOOL ROOMS DAY AFTER DAY FOR A FORTNIGHT HE'LL HAVE TO PUT IN AN APPEARANCE AT SOME PLACE OF WORSHIP ON SUNDAY MORNING AND HE CAN COME TO US IMMEDIATELY AFTERWARDS"}`,highlighted:`    <span class="hljs-keyword">import</span> fetch <span class="hljs-keyword">from</span> <span class="hljs-string">&quot;node-fetch&quot;</span>;
    <span class="hljs-keyword">import</span> fs <span class="hljs-keyword">from</span> <span class="hljs-string">&quot;fs&quot;</span>;
    <span class="hljs-keyword">async</span> <span class="hljs-keyword">function</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">filename</span>) {
        <span class="hljs-keyword">const</span> data = fs.<span class="hljs-title function_">readFileSync</span>(filename);
        <span class="hljs-keyword">const</span> response = <span class="hljs-keyword">await</span> <span class="hljs-title function_">fetch</span>(
            <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/facebook/wav2vec2-base-960h&quot;</span>,
            {
                <span class="hljs-attr">headers</span>: { <span class="hljs-title class_">Authorization</span>: <span class="hljs-string">\`Bearer <span class="hljs-subst">$}{API_TOKEN}</span>\`</span> },
                <span class="hljs-attr">method</span>: <span class="hljs-string">&quot;POST&quot;</span>,
                <span class="hljs-attr">body</span>: data,
            }
        );
        <span class="hljs-keyword">const</span> result = <span class="hljs-keyword">await</span> response.<span class="hljs-title function_">json</span>();
        <span class="hljs-keyword">return</span> result;
    }
    <span class="hljs-title function_">query</span>(<span class="hljs-string">&quot;sample1.flac&quot;</span>).<span class="hljs-title function_">then</span>(<span class="hljs-function">(<span class="hljs-params">response</span>) =&gt;</span> {
        <span class="hljs-variable language_">console</span>.<span class="hljs-title function_">log</span>(<span class="hljs-title class_">JSON</span>.<span class="hljs-title function_">stringify</span>(response));
    });
    <span class="hljs-comment">// {&quot;text&quot;:&quot;GOING ALONG SLUSHY COUNTRY ROADS AND SPEAKING TO DAMP AUDIENCES IN DRAUGHTY SCHOOL ROOMS DAY AFTER DAY FOR A FORTNIGHT HE&#x27;LL HAVE TO PUT IN AN APPEARANCE AT SOME PLACE OF WORSHIP ON SUNDAY MORNING AND HE CAN COME TO US IMMEDIATELY AFTERWARDS&quot;}</span>`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){E(n,s,d),c=!0},p:x,i(s){c||(w(n.$$.fragment,s),c=!0)},o(s){y(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function NF(q){let n,c;return n=new R({props:{$$slots:{default:[RF]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){E(n,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){c||(w(n.$$.fragment,s),c=!0)},o(s){y(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function PF(q){let n,c;return n=new N({props:{code:`    curl https://api-inference.huggingface.co/models/facebook/wav2vec2-base-960h \\
            -X POST \\
            --data-binary '@sample1.flac' \\
            -H "Authorization: Bearer $}{HF_API_TOKEN}"
    # {"text":"GOING ALONG SLUSHY COUNTRY ROADS AND SPEAKING TO DAMP AUDIENCES IN DRAUGHTY SCHOOL ROOMS DAY AFTER DAY FOR A FORTNIGHT HE'LL HAVE TO PUT IN AN APPEARANCE AT SOME PLACE OF WORSHIP ON SUNDAY MORNING AND HE CAN COME TO US IMMEDIATELY AFTERWARDS"}`,highlighted:`    curl https://api-inference.huggingface.co/models/facebook/wav2vec2-base-<span class="hljs-number">960</span>h \\
            -X POST \\
            --data-binary <span class="hljs-string">&#x27;@sample1.flac&#x27;</span> \\
            -H <span class="hljs-string">&quot;Authorization: Bearer <span class="hljs-variable">$}{HF_API_TOKEN}</span>&quot;</span>
    <span class="hljs-comment"># {&quot;text&quot;:&quot;GOING ALONG SLUSHY COUNTRY ROADS AND SPEAKING TO DAMP AUDIENCES IN DRAUGHTY SCHOOL ROOMS DAY AFTER DAY FOR A FORTNIGHT HE&#x27;LL HAVE TO PUT IN AN APPEARANCE AT SOME PLACE OF WORSHIP ON SUNDAY MORNING AND HE CAN COME TO US IMMEDIATELY AFTERWARDS&quot;}</span>`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){E(n,s,d),c=!0},p:x,i(s){c||(w(n.$$.fragment,s),c=!0)},o(s){y(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function SF(q){let n,c;return n=new R({props:{$$slots:{default:[PF]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){E(n,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){c||(w(n.$$.fragment,s),c=!0)},o(s){y(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function IF(q){let n,c;return n=new N({props:{code:`    self.assertEqual(
        data,
        {
            "text": "GOING ALONG SLUSHY COUNTRY ROADS AND SPEAKING TO DAMP AUDIENCES IN DRAUGHTY SCHOOL ROOMS DAY AFTER DAY FOR A FORTNIGHT HE'LL HAVE TO PUT IN AN APPEARANCE AT SOME PLACE OF WORSHIP ON SUNDAY MORNING AND HE CAN COME TO US IMMEDIATELY AFTERWARDS"
        },
    )`,highlighted:`    self.assertEqual(
        data,
        {
            <span class="hljs-string">&quot;text&quot;</span>: <span class="hljs-string">&quot;GOING ALONG SLUSHY COUNTRY ROADS AND SPEAKING TO DAMP AUDIENCES IN DRAUGHTY SCHOOL ROOMS DAY AFTER DAY FOR A FORTNIGHT HE&#x27;LL HAVE TO PUT IN AN APPEARANCE AT SOME PLACE OF WORSHIP ON SUNDAY MORNING AND HE CAN COME TO US IMMEDIATELY AFTERWARDS&quot;</span>
        },
    )`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){E(n,s,d),c=!0},p:x,i(s){c||(w(n.$$.fragment,s),c=!0)},o(s){y(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function HF(q){let n,c;return n=new R({props:{$$slots:{default:[IF]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){E(n,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){c||(w(n.$$.fragment,s),c=!0)},o(s){y(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function BF(q){let n,c,s,d,$,k,A;return{c(){n=r("p"),c=r("strong"),s=i("Recommended model"),d=i(`:
`),$=r("a"),k=i("Sentence-transformers"),A=i("."),this.h()},l(T){n=o(T,"P",{});var j=l(n);c=o(j,"STRONG",{});var O=l(c);s=u(O,"Recommended model"),O.forEach(t),d=u(j,`:
`),$=o(j,"A",{href:!0,rel:!0});var D=l($);k=u(D,"Sentence-transformers"),D.forEach(t),A=u(j,"."),j.forEach(t),this.h()},h(){p($,"href","https://huggingface.co/sentence-transformers/paraphrase-xlm-r-multilingual-v1"),p($,"rel","nofollow")},m(T,j){m(T,n,j),e(n,c),e(c,s),e(n,d),e(n,$),e($,k),e(n,A)},d(T){T&&t(n)}}}function GF(q){let n,c,s,d,$,k,A;return{c(){n=r("p"),c=r("strong"),s=i("Recommended model"),d=i(`:
`),$=r("a"),k=i("superb/hubert-large-superb-er"),A=i("."),this.h()},l(T){n=o(T,"P",{});var j=l(n);c=o(j,"STRONG",{});var O=l(c);s=u(O,"Recommended model"),O.forEach(t),d=u(j,`:
`),$=o(j,"A",{href:!0,rel:!0});var D=l($);k=u(D,"superb/hubert-large-superb-er"),D.forEach(t),A=u(j,"."),j.forEach(t),this.h()},h(){p($,"href","https://huggingface.co/superb/hubert-large-superb-er"),p($,"rel","nofollow")},m(T,j){m(T,n,j),e(n,c),e(c,s),e(n,d),e(n,$),e($,k),e(n,A)},d(T){T&&t(n)}}}function CF(q){let n,c;return n=new N({props:{code:`    self.assertEqual(
        deep_round(data, 4),
        [
            {"score": 0.5928, "label": "neu"},
            {"score": 0.2003, "label": "hap"},
            {"score": 0.128, "label": "ang"},
            {"score": 0.079, "label": "sad"},
        ],
    )`,highlighted:`    self.assertEqual(
        deep_round(data, <span class="hljs-number">4</span>),
        [
            {<span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.5928</span>, <span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;neu&quot;</span>},
            {<span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.2003</span>, <span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;hap&quot;</span>},
            {<span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.128</span>, <span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;ang&quot;</span>},
            {<span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.079</span>, <span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;sad&quot;</span>},
        ],
    )`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){E(n,s,d),c=!0},p:x,i(s){c||(w(n.$$.fragment,s),c=!0)},o(s){y(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function MF(q){let n,c;return n=new R({props:{$$slots:{default:[CF]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){E(n,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){c||(w(n.$$.fragment,s),c=!0)},o(s){y(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function FF(q){let n,c;return n=new N({props:{code:`    import fetch from "node-fetch";
    import fs from "fs";
    async function query(filename) {
        const data = fs.readFileSync(filename);
        const response = await fetch(
            "https://api-inference.huggingface.co/models/superb/hubert-large-superb-er",
            {
                headers: { Authorization: \`Bearer $}{API_TOKEN}\` },
                method: "POST",
                body: data,
            }
        );
        const result = await response.json();
        return result;
    }
    query("sample1.flac").then((response) => {
        console.log(JSON.stringify(response));
    });
    // [{"score":0.5927661657333374,"label":"neu"},{"score":0.2002529799938202,"label":"hap"},{"score":0.12795612215995789,"label":"ang"},{"score":0.07902472466230392,"label":"sad"}]`,highlighted:`    <span class="hljs-keyword">import</span> <span class="hljs-keyword">fetch</span> <span class="hljs-keyword">from</span> &quot;node-fetch&quot;;
    <span class="hljs-keyword">import</span> fs <span class="hljs-keyword">from</span> &quot;fs&quot;;
    async <span class="hljs-keyword">function</span> query(filename) {
        const data = fs.readFileSync(filename);
        const response = await <span class="hljs-keyword">fetch</span>(
            &quot;https://api-inference.huggingface.co/models/superb/hubert-large-superb-er&quot;,
            {
                headers: { <span class="hljs-keyword">Authorization</span>: \`Bearer $}{API_TOKEN}\` },
                <span class="hljs-keyword">method</span>: &quot;POST&quot;,
                body: data,
            }
        );
        const result = await response.json();
        <span class="hljs-keyword">return</span> result;
    }
    query(&quot;sample1.flac&quot;).<span class="hljs-keyword">then</span>((response) =&gt; {
        console.log(<span class="hljs-type">JSON</span>.stringify(response));
    });
    // [{&quot;score&quot;:<span class="hljs-number">0.5927661657333374</span>,&quot;label&quot;:&quot;neu&quot;},{&quot;score&quot;:<span class="hljs-number">0.2002529799938202</span>,&quot;label&quot;:&quot;hap&quot;},{&quot;score&quot;:<span class="hljs-number">0.12795612215995789</span>,&quot;label&quot;:&quot;ang&quot;},{&quot;score&quot;:<span class="hljs-number">0.07902472466230392</span>,&quot;label&quot;:&quot;sad&quot;}]`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){E(n,s,d),c=!0},p:x,i(s){c||(w(n.$$.fragment,s),c=!0)},o(s){y(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function UF(q){let n,c;return n=new R({props:{$$slots:{default:[FF]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){E(n,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){c||(w(n.$$.fragment,s),c=!0)},o(s){y(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function LF(q){let n,c;return n=new N({props:{code:`    curl https://api-inference.huggingface.co/models/superb/hubert-large-superb-er \\
            -X POST \\
            --data-binary '@sample1.flac' \\
            -H "Authorization: Bearer $}{HF_API_TOKEN}"
    # [{"score":0.5927661657333374,"label":"neu"},{"score":0.2002529799938202,"label":"hap"},{"score":0.12795612215995789,"label":"ang"},{"score":0.07902472466230392,"label":"sad"}]`,highlighted:`    curl https:<span class="hljs-comment">//api-inference.huggingface.co/models/superb/hubert-large-superb-er \\</span>
            -X POST \\
            --<span class="hljs-keyword">data</span>-binary <span class="hljs-string">&#x27;@sample1.flac&#x27;</span> \\
            -H <span class="hljs-string">&quot;Authorization: Bearer <span class="hljs-subst">$}{HF_API_TOKEN}</span>&quot;</span>
    # [{<span class="hljs-string">&quot;score&quot;</span>:<span class="hljs-number">0.5927661657333374</span>,<span class="hljs-string">&quot;label&quot;</span>:<span class="hljs-string">&quot;neu&quot;</span>},{<span class="hljs-string">&quot;score&quot;</span>:<span class="hljs-number">0.2002529799938202</span>,<span class="hljs-string">&quot;label&quot;</span>:<span class="hljs-string">&quot;hap&quot;</span>},{<span class="hljs-string">&quot;score&quot;</span>:<span class="hljs-number">0.12795612215995789</span>,<span class="hljs-string">&quot;label&quot;</span>:<span class="hljs-string">&quot;ang&quot;</span>},{<span class="hljs-string">&quot;score&quot;</span>:<span class="hljs-number">0.07902472466230392</span>,<span class="hljs-string">&quot;label&quot;</span>:<span class="hljs-string">&quot;sad&quot;</span>}]`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){E(n,s,d),c=!0},p:x,i(s){c||(w(n.$$.fragment,s),c=!0)},o(s){y(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function zF(q){let n,c;return n=new R({props:{$$slots:{default:[LF]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){E(n,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){c||(w(n.$$.fragment,s),c=!0)},o(s){y(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function JF(q){let n,c;return n=new N({props:{code:`    self.assertEqual(
        deep_round(data, 4),
        [
            {"score": 0.5928, "label": "neu"},
            {"score": 0.2003, "label": "hap"},
            {"score": 0.128, "label": "ang"},
            {"score": 0.079, "label": "sad"},
        ],
    )`,highlighted:`    self.assertEqual(
        deep_round(data, <span class="hljs-number">4</span>),
        [
            {<span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.5928</span>, <span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;neu&quot;</span>},
            {<span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.2003</span>, <span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;hap&quot;</span>},
            {<span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.128</span>, <span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;ang&quot;</span>},
            {<span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.079</span>, <span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;sad&quot;</span>},
        ],
    )`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){E(n,s,d),c=!0},p:x,i(s){c||(w(n.$$.fragment,s),c=!0)},o(s){y(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function WF(q){let n,c;return n=new R({props:{$$slots:{default:[JF]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){E(n,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){c||(w(n.$$.fragment,s),c=!0)},o(s){y(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function YF(q){let n,c,s,d,$,k,A;return{c(){n=r("p"),c=r("strong"),s=i("Recommended model"),d=i(`:
`),$=r("a"),k=i("facebook/detr-resnet-50"),A=i("."),this.h()},l(T){n=o(T,"P",{});var j=l(n);c=o(j,"STRONG",{});var O=l(c);s=u(O,"Recommended model"),O.forEach(t),d=u(j,`:
`),$=o(j,"A",{href:!0,rel:!0});var D=l($);k=u(D,"facebook/detr-resnet-50"),D.forEach(t),A=u(j,"."),j.forEach(t),this.h()},h(){p($,"href","https://huggingface.co/facebook/detr-resnet-50"),p($,"rel","nofollow")},m(T,j){m(T,n,j),e(n,c),e(c,s),e(n,d),e(n,$),e($,k),e(n,A)},d(T){T&&t(n)}}}function KF(q){let n,c;return n=new N({props:{code:`    self.assertEqual(
        deep_round(data, 4),
        [
            {
                "score": 0.9982,
                "label": "remote",
                "box": {"xmin": 40, "ymin": 70, "xmax": 175, "ymax": 117},
            },
            {
                "score": 0.9960,
                "label": "remote",
                "box": {"xmin": 333, "ymin": 72, "xmax": 368, "ymax": 187},
            },
            {
                "score": 0.9955,
                "label": "couch",
                "box": {"xmin": 0, "ymin": 1, "xmax": 639, "ymax": 473},
            },
            {
                "score": 0.9988,
                "label": "cat",
                "box": {"xmin": 13, "ymin": 52, "xmax": 314, "ymax": 470},
            },
            {
                "score": 0.9987,
                "label": "cat",
                "box": {"xmin": 345, "ymin": 23, "xmax": 640, "ymax": 368},
            },
        ],
    )`,highlighted:`    self.assertEqual(
        deep_round(data, <span class="hljs-number">4</span>),
        [
            {
                <span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.9982</span>,
                <span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;remote&quot;</span>,
                <span class="hljs-string">&quot;box&quot;</span>: {<span class="hljs-string">&quot;xmin&quot;</span>: <span class="hljs-number">40</span>, <span class="hljs-string">&quot;ymin&quot;</span>: <span class="hljs-number">70</span>, <span class="hljs-string">&quot;xmax&quot;</span>: <span class="hljs-number">175</span>, <span class="hljs-string">&quot;ymax&quot;</span>: <span class="hljs-number">117</span>},
            },
            {
                <span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.9960</span>,
                <span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;remote&quot;</span>,
                <span class="hljs-string">&quot;box&quot;</span>: {<span class="hljs-string">&quot;xmin&quot;</span>: <span class="hljs-number">333</span>, <span class="hljs-string">&quot;ymin&quot;</span>: <span class="hljs-number">72</span>, <span class="hljs-string">&quot;xmax&quot;</span>: <span class="hljs-number">368</span>, <span class="hljs-string">&quot;ymax&quot;</span>: <span class="hljs-number">187</span>},
            },
            {
                <span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.9955</span>,
                <span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;couch&quot;</span>,
                <span class="hljs-string">&quot;box&quot;</span>: {<span class="hljs-string">&quot;xmin&quot;</span>: <span class="hljs-number">0</span>, <span class="hljs-string">&quot;ymin&quot;</span>: <span class="hljs-number">1</span>, <span class="hljs-string">&quot;xmax&quot;</span>: <span class="hljs-number">639</span>, <span class="hljs-string">&quot;ymax&quot;</span>: <span class="hljs-number">473</span>},
            },
            {
                <span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.9988</span>,
                <span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;cat&quot;</span>,
                <span class="hljs-string">&quot;box&quot;</span>: {<span class="hljs-string">&quot;xmin&quot;</span>: <span class="hljs-number">13</span>, <span class="hljs-string">&quot;ymin&quot;</span>: <span class="hljs-number">52</span>, <span class="hljs-string">&quot;xmax&quot;</span>: <span class="hljs-number">314</span>, <span class="hljs-string">&quot;ymax&quot;</span>: <span class="hljs-number">470</span>},
            },
            {
                <span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.9987</span>,
                <span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;cat&quot;</span>,
                <span class="hljs-string">&quot;box&quot;</span>: {<span class="hljs-string">&quot;xmin&quot;</span>: <span class="hljs-number">345</span>, <span class="hljs-string">&quot;ymin&quot;</span>: <span class="hljs-number">23</span>, <span class="hljs-string">&quot;xmax&quot;</span>: <span class="hljs-number">640</span>, <span class="hljs-string">&quot;ymax&quot;</span>: <span class="hljs-number">368</span>},
            },
        ],
    )`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){E(n,s,d),c=!0},p:x,i(s){c||(w(n.$$.fragment,s),c=!0)},o(s){y(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function VF(q){let n,c;return n=new R({props:{$$slots:{default:[KF]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){E(n,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){c||(w(n.$$.fragment,s),c=!0)},o(s){y(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function XF(q){let n,c;return n=new N({props:{code:`    import fetch from "node-fetch";
    import fs from "fs";
    async function query(filename) {
        const data = fs.readFileSync(filename);
        const response = await fetch(
            "https://api-inference.huggingface.co/models/facebook/detr-resnet-50",
            {
                headers: { Authorization: \`Bearer $}{API_TOKEN}\` },
                method: "POST",
                body: data,
            }
        );
        const result = await response.json();
        return result;
    }
    query("cats.jpg").then((response) => {
        console.log(JSON.stringify(response));
    });
    // [{"score":0.9982201457023621,"label":"remote","box":{"xmin":40,"ymin":70,"xmax":175,"ymax":117}},{"score":0.9960021376609802,"label":"remote","box":{"xmin":333,"ymin":72,"xmax":368,"ymax":187}},{"score":0.9954745173454285,"label":"couch","box":{"xmin":0,"ymin":1,"xmax":639,"ymax":473}},{"score":0.9988006353378296,"label":"cat","box":{"xmin":13,"ymin":52,"xmax":314,"ymax":470}},{"score":0.9986783862113953,"label":"cat","box":{"xmin":345,"ymin":23,"xmax":640,"ymax":368}}]`,highlighted:`    <span class="hljs-keyword">import</span> <span class="hljs-keyword">fetch</span> <span class="hljs-keyword">from</span> &quot;node-fetch&quot;;
    <span class="hljs-keyword">import</span> fs <span class="hljs-keyword">from</span> &quot;fs&quot;;
    async <span class="hljs-keyword">function</span> query(filename) {
        const data = fs.readFileSync(filename);
        const response = await <span class="hljs-keyword">fetch</span>(
            &quot;https://api-inference.huggingface.co/models/facebook/detr-resnet-50&quot;,
            {
                headers: { <span class="hljs-keyword">Authorization</span>: \`Bearer $}{API_TOKEN}\` },
                <span class="hljs-keyword">method</span>: &quot;POST&quot;,
                body: data,
            }
        );
        const result = await response.json();
        <span class="hljs-keyword">return</span> result;
    }
    query(&quot;cats.jpg&quot;).<span class="hljs-keyword">then</span>((response) =&gt; {
        console.log(<span class="hljs-type">JSON</span>.stringify(response));
    });
    // [{&quot;score&quot;:<span class="hljs-number">0.9982201457023621</span>,&quot;label&quot;:&quot;remote&quot;,&quot;box&quot;:{&quot;xmin&quot;:<span class="hljs-number">40</span>,&quot;ymin&quot;:<span class="hljs-number">70</span>,&quot;xmax&quot;:<span class="hljs-number">175</span>,&quot;ymax&quot;:<span class="hljs-number">117</span>}},{&quot;score&quot;:<span class="hljs-number">0.9960021376609802</span>,&quot;label&quot;:&quot;remote&quot;,&quot;box&quot;:{&quot;xmin&quot;:<span class="hljs-number">333</span>,&quot;ymin&quot;:<span class="hljs-number">72</span>,&quot;xmax&quot;:<span class="hljs-number">368</span>,&quot;ymax&quot;:<span class="hljs-number">187</span>}},{&quot;score&quot;:<span class="hljs-number">0.9954745173454285</span>,&quot;label&quot;:&quot;couch&quot;,&quot;box&quot;:{&quot;xmin&quot;:<span class="hljs-number">0</span>,&quot;ymin&quot;:<span class="hljs-number">1</span>,&quot;xmax&quot;:<span class="hljs-number">639</span>,&quot;ymax&quot;:<span class="hljs-number">473</span>}},{&quot;score&quot;:<span class="hljs-number">0.9988006353378296</span>,&quot;label&quot;:&quot;cat&quot;,&quot;box&quot;:{&quot;xmin&quot;:<span class="hljs-number">13</span>,&quot;ymin&quot;:<span class="hljs-number">52</span>,&quot;xmax&quot;:<span class="hljs-number">314</span>,&quot;ymax&quot;:<span class="hljs-number">470</span>}},{&quot;score&quot;:<span class="hljs-number">0.9986783862113953</span>,&quot;label&quot;:&quot;cat&quot;,&quot;box&quot;:{&quot;xmin&quot;:<span class="hljs-number">345</span>,&quot;ymin&quot;:<span class="hljs-number">23</span>,&quot;xmax&quot;:<span class="hljs-number">640</span>,&quot;ymax&quot;:<span class="hljs-number">368</span>}}]`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){E(n,s,d),c=!0},p:x,i(s){c||(w(n.$$.fragment,s),c=!0)},o(s){y(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function QF(q){let n,c;return n=new R({props:{$$slots:{default:[XF]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){E(n,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){c||(w(n.$$.fragment,s),c=!0)},o(s){y(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function ZF(q){let n,c;return n=new N({props:{code:`    curl https://api-inference.huggingface.co/models/facebook/detr-resnet-50 \\
            -X POST \\
            --data-binary '@cats.jpg' \\
            -H "Authorization: Bearer $}{HF_API_TOKEN}"
    # [{"score":0.9982201457023621,"label":"remote","box":{"xmin":40,"ymin":70,"xmax":175,"ymax":117}},{"score":0.9960021376609802,"label":"remote","box":{"xmin":333,"ymin":72,"xmax":368,"ymax":187}},{"score":0.9954745173454285,"label":"couch","box":{"xmin":0,"ymin":1,"xmax":639,"ymax":473}},{"score":0.9988006353378296,"label":"cat","box":{"xmin":13,"ymin":52,"xmax":314,"ymax":470}},{"score":0.9986783862113953,"label":"cat","box":{"xmin":345,"ymin":23,"xmax":640,"ymax":368}}]`,highlighted:`    curl https:<span class="hljs-comment">//api-inference.huggingface.co/models/facebook/detr-resnet-50 \\</span>
            -X POST \\
            --<span class="hljs-keyword">data</span>-binary <span class="hljs-string">&#x27;@cats.jpg&#x27;</span> \\
            -H <span class="hljs-string">&quot;Authorization: Bearer <span class="hljs-subst">$}{HF_API_TOKEN}</span>&quot;</span>
    # [{<span class="hljs-string">&quot;score&quot;</span>:<span class="hljs-number">0.9982201457023621</span>,<span class="hljs-string">&quot;label&quot;</span>:<span class="hljs-string">&quot;remote&quot;</span>,<span class="hljs-string">&quot;box&quot;</span>:{<span class="hljs-string">&quot;xmin&quot;</span>:<span class="hljs-number">40</span>,<span class="hljs-string">&quot;ymin&quot;</span>:<span class="hljs-number">70</span>,<span class="hljs-string">&quot;xmax&quot;</span>:<span class="hljs-number">175</span>,<span class="hljs-string">&quot;ymax&quot;</span>:<span class="hljs-number">117</span>}},{<span class="hljs-string">&quot;score&quot;</span>:<span class="hljs-number">0.9960021376609802</span>,<span class="hljs-string">&quot;label&quot;</span>:<span class="hljs-string">&quot;remote&quot;</span>,<span class="hljs-string">&quot;box&quot;</span>:{<span class="hljs-string">&quot;xmin&quot;</span>:<span class="hljs-number">333</span>,<span class="hljs-string">&quot;ymin&quot;</span>:<span class="hljs-number">72</span>,<span class="hljs-string">&quot;xmax&quot;</span>:<span class="hljs-number">368</span>,<span class="hljs-string">&quot;ymax&quot;</span>:<span class="hljs-number">187</span>}},{<span class="hljs-string">&quot;score&quot;</span>:<span class="hljs-number">0.9954745173454285</span>,<span class="hljs-string">&quot;label&quot;</span>:<span class="hljs-string">&quot;couch&quot;</span>,<span class="hljs-string">&quot;box&quot;</span>:{<span class="hljs-string">&quot;xmin&quot;</span>:<span class="hljs-number">0</span>,<span class="hljs-string">&quot;ymin&quot;</span>:<span class="hljs-number">1</span>,<span class="hljs-string">&quot;xmax&quot;</span>:<span class="hljs-number">639</span>,<span class="hljs-string">&quot;ymax&quot;</span>:<span class="hljs-number">473</span>}},{<span class="hljs-string">&quot;score&quot;</span>:<span class="hljs-number">0.9988006353378296</span>,<span class="hljs-string">&quot;label&quot;</span>:<span class="hljs-string">&quot;cat&quot;</span>,<span class="hljs-string">&quot;box&quot;</span>:{<span class="hljs-string">&quot;xmin&quot;</span>:<span class="hljs-number">13</span>,<span class="hljs-string">&quot;ymin&quot;</span>:<span class="hljs-number">52</span>,<span class="hljs-string">&quot;xmax&quot;</span>:<span class="hljs-number">314</span>,<span class="hljs-string">&quot;ymax&quot;</span>:<span class="hljs-number">470</span>}},{<span class="hljs-string">&quot;score&quot;</span>:<span class="hljs-number">0.9986783862113953</span>,<span class="hljs-string">&quot;label&quot;</span>:<span class="hljs-string">&quot;cat&quot;</span>,<span class="hljs-string">&quot;box&quot;</span>:{<span class="hljs-string">&quot;xmin&quot;</span>:<span class="hljs-number">345</span>,<span class="hljs-string">&quot;ymin&quot;</span>:<span class="hljs-number">23</span>,<span class="hljs-string">&quot;xmax&quot;</span>:<span class="hljs-number">640</span>,<span class="hljs-string">&quot;ymax&quot;</span>:<span class="hljs-number">368</span>}}]`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){E(n,s,d),c=!0},p:x,i(s){c||(w(n.$$.fragment,s),c=!0)},o(s){y(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function eU(q){let n,c;return n=new R({props:{$$slots:{default:[ZF]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){E(n,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){c||(w(n.$$.fragment,s),c=!0)},o(s){y(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function tU(q){let n,c;return n=new N({props:{code:`    self.assertEqual(
        deep_round(data, 4),
        [
            {
                "score": 0.9982,
                "label": "remote",
                "box": {"xmin": 40, "ymin": 70, "xmax": 175, "ymax": 117},
            },
            {
                "score": 0.9960,
                "label": "remote",
                "box": {"xmin": 333, "ymin": 72, "xmax": 368, "ymax": 187},
            },
            {
                "score": 0.9955,
                "label": "couch",
                "box": {"xmin": 0, "ymin": 1, "xmax": 639, "ymax": 473},
            },
            {
                "score": 0.9988,
                "label": "cat",
                "box": {"xmin": 13, "ymin": 52, "xmax": 314, "ymax": 470},
            },
            {
                "score": 0.9987,
                "label": "cat",
                "box": {"xmin": 345, "ymin": 23, "xmax": 640, "ymax": 368},
            },
        ],
    )`,highlighted:`    self.assertEqual(
        deep_round(data, <span class="hljs-number">4</span>),
        [
            {
                <span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.9982</span>,
                <span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;remote&quot;</span>,
                <span class="hljs-string">&quot;box&quot;</span>: {<span class="hljs-string">&quot;xmin&quot;</span>: <span class="hljs-number">40</span>, <span class="hljs-string">&quot;ymin&quot;</span>: <span class="hljs-number">70</span>, <span class="hljs-string">&quot;xmax&quot;</span>: <span class="hljs-number">175</span>, <span class="hljs-string">&quot;ymax&quot;</span>: <span class="hljs-number">117</span>},
            },
            {
                <span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.9960</span>,
                <span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;remote&quot;</span>,
                <span class="hljs-string">&quot;box&quot;</span>: {<span class="hljs-string">&quot;xmin&quot;</span>: <span class="hljs-number">333</span>, <span class="hljs-string">&quot;ymin&quot;</span>: <span class="hljs-number">72</span>, <span class="hljs-string">&quot;xmax&quot;</span>: <span class="hljs-number">368</span>, <span class="hljs-string">&quot;ymax&quot;</span>: <span class="hljs-number">187</span>},
            },
            {
                <span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.9955</span>,
                <span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;couch&quot;</span>,
                <span class="hljs-string">&quot;box&quot;</span>: {<span class="hljs-string">&quot;xmin&quot;</span>: <span class="hljs-number">0</span>, <span class="hljs-string">&quot;ymin&quot;</span>: <span class="hljs-number">1</span>, <span class="hljs-string">&quot;xmax&quot;</span>: <span class="hljs-number">639</span>, <span class="hljs-string">&quot;ymax&quot;</span>: <span class="hljs-number">473</span>},
            },
            {
                <span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.9988</span>,
                <span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;cat&quot;</span>,
                <span class="hljs-string">&quot;box&quot;</span>: {<span class="hljs-string">&quot;xmin&quot;</span>: <span class="hljs-number">13</span>, <span class="hljs-string">&quot;ymin&quot;</span>: <span class="hljs-number">52</span>, <span class="hljs-string">&quot;xmax&quot;</span>: <span class="hljs-number">314</span>, <span class="hljs-string">&quot;ymax&quot;</span>: <span class="hljs-number">470</span>},
            },
            {
                <span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.9987</span>,
                <span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;cat&quot;</span>,
                <span class="hljs-string">&quot;box&quot;</span>: {<span class="hljs-string">&quot;xmin&quot;</span>: <span class="hljs-number">345</span>, <span class="hljs-string">&quot;ymin&quot;</span>: <span class="hljs-number">23</span>, <span class="hljs-string">&quot;xmax&quot;</span>: <span class="hljs-number">640</span>, <span class="hljs-string">&quot;ymax&quot;</span>: <span class="hljs-number">368</span>},
            },
        ],
    )`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){E(n,s,d),c=!0},p:x,i(s){c||(w(n.$$.fragment,s),c=!0)},o(s){y(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function sU(q){let n,c;return n=new R({props:{$$slots:{default:[tU]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){E(n,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){c||(w(n.$$.fragment,s),c=!0)},o(s){y(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function aU(q){let n,c,s,d,$,k,A,T,j,O,D,se,De,V,J,et,Ll,Ra,Oe,gy,K$,zl,my,V$,tt,Mx,X$,st,Fx,Q$,xe,at,ph,Na,$y,fh,qy,Z$,Jl,_y,eq,nt,tq,Pa,vy,Sa,Ey,sq,Wl,wy,aq,rt,nq,Yl,yy,rq,ot,hh,Ia,Kl,by,jy,dh,Ty,M,Ha,Ba,gh,ky,Ay,Dy,Vl,Oy,xy,Ga,Ca,mh,Ry,Ny,Py,Xl,Sy,Iy,Ma,Ql,Hy,By,pe,Gy,$h,Cy,My,Zl,Fy,Uy,Ly,Fa,ei,zy,Jy,lt,Wy,qh,Yy,Ky,Vy,Ua,ti,_h,Xy,Qy,si,Zy,eb,La,ai,tb,sb,it,ab,vh,nb,rb,ob,za,ni,lb,ib,ut,ub,Eh,cb,pb,fb,Ja,ri,hb,db,ct,gb,wh,mb,$b,oq,oi,qb,lq,li,_b,iq,pt,uq,ft,yh,Wa,ii,vb,Eb,bh,wb,Re,Ya,ui,jh,yb,bb,ci,jb,Tb,Ka,pi,Th,kb,Ab,fi,Db,Ob,Va,hi,kh,xb,Rb,ht,Nb,Ah,Pb,Sb,cq,Ne,dt,Dh,Xa,Ib,Oh,Hb,pq,di,Bb,fq,gt,hq,Qa,Gb,Za,Cb,dq,gi,Mb,gq,mt,mq,mi,Fb,$q,$t,xh,en,$i,Ub,Lb,Rh,zb,X,tn,sn,Nh,Jb,Wb,Yb,qi,Kb,Vb,an,_i,Ph,Xb,Qb,vi,Zb,e0,nn,Ei,t0,s0,qt,a0,Sh,n0,r0,o0,rn,wi,l0,i0,_t,u0,Ih,c0,p0,f0,on,yi,h0,d0,vt,g0,Hh,m0,$0,qq,bi,q0,_q,Et,Bh,ln,ji,_0,v0,Gh,E0,Ch,un,Ti,Mh,w0,y0,ki,b0,vq,Pe,wt,Fh,cn,j0,Uh,T0,Eq,yt,k0,Ai,A0,D0,wq,bt,yq,pn,O0,fn,x0,bq,Di,R0,jq,jt,Tq,Oi,N0,kq,Tt,Lh,hn,xi,P0,S0,zh,I0,B,dn,gn,Jh,H0,B0,G0,Ri,C0,M0,mn,Ni,Wh,F0,U0,Pi,L0,z0,$n,Si,J0,W0,fe,Y0,Yh,K0,V0,Kh,X0,Q0,Z0,qn,Ii,ej,tj,he,sj,Vh,aj,nj,Xh,rj,oj,lj,_n,Hi,ij,uj,de,cj,Qh,pj,fj,Bi,hj,dj,gj,vn,Gi,mj,$j,ge,qj,Zh,_j,vj,Ci,Ej,wj,yj,En,Mi,bj,jj,ae,Tj,ed,kj,Aj,Fi,Dj,Oj,Ui,xj,Rj,Nj,wn,Li,Pj,Sj,kt,Ij,td,Hj,Bj,Gj,yn,zi,Cj,Mj,At,Fj,sd,Uj,Lj,zj,bn,Ji,ad,Jj,Wj,Wi,Yj,Kj,jn,Yi,Vj,Xj,Dt,Qj,nd,Zj,e3,t3,Tn,Ki,s3,a3,Ot,n3,rd,r3,o3,l3,kn,Vi,i3,u3,xt,c3,od,p3,f3,Aq,Xi,h3,Dq,Rt,ld,An,Qi,d3,g3,id,m3,ud,Dn,Zi,cd,$3,q3,eu,_3,Oq,Se,Nt,pd,On,v3,fd,E3,xq,tu,w3,Rq,Pt,Nq,xn,y3,Rn,b3,Pq,su,j3,Sq,St,Iq,au,T3,Hq,It,hd,Nn,nu,k3,A3,dd,D3,P,Pn,Sn,gd,O3,x3,R3,md,N3,In,ru,P3,S3,ou,I3,H3,Hn,lu,B3,G3,iu,C3,M3,Bn,uu,F3,U3,Ht,L3,$d,z3,J3,W3,Gn,cu,qd,Y3,K3,pu,V3,X3,Cn,fu,Q3,Z3,me,eT,_d,tT,sT,vd,aT,nT,rT,Mn,hu,oT,lT,$e,iT,Ed,uT,cT,wd,pT,fT,hT,Fn,du,dT,gT,qe,mT,yd,$T,qT,gu,_T,vT,ET,Un,mu,wT,yT,_e,bT,bd,jT,TT,$u,kT,AT,DT,Ln,qu,OT,xT,ne,RT,jd,NT,PT,_u,ST,IT,vu,HT,BT,GT,zn,Eu,CT,MT,Bt,FT,Td,UT,LT,zT,Jn,wu,JT,WT,Gt,YT,kd,KT,VT,XT,Wn,yu,Ad,QT,ZT,bu,e5,t5,Yn,ju,s5,a5,Ct,n5,Dd,r5,o5,l5,Kn,Tu,i5,u5,Mt,c5,Od,p5,f5,h5,Vn,ku,d5,g5,Ft,m5,xd,$5,q5,Bq,Au,_5,Gq,Ut,Rd,Xn,Du,v5,E5,Nd,w5,oe,Qn,Ou,Pd,y5,b5,xu,j5,T5,Zn,Ru,Sd,k5,A5,Nu,D5,O5,er,Pu,x5,R5,Su,N5,P5,tr,Iu,S5,I5,Hu,H5,Cq,Ie,Lt,Id,sr,B5,Hd,G5,Mq,Bu,C5,Fq,zt,Uq,ar,M5,nr,F5,Lq,Gu,U5,zq,Jt,Jq,Cu,L5,Wq,Wt,Bd,rr,Mu,z5,J5,Gd,W5,L,or,lr,Cd,Y5,K5,V5,Md,X5,ir,Fu,Q5,Z5,Uu,e4,t4,ur,Lu,s4,a4,zu,n4,r4,cr,Ju,Fd,o4,l4,Wu,i4,u4,pr,Yu,c4,p4,Yt,f4,Ud,h4,d4,g4,fr,Ku,m4,$4,Kt,q4,Ld,_4,v4,E4,hr,Vu,w4,y4,Vt,b4,zd,j4,T4,Yq,Xu,k4,Kq,Xt,Vq,Qt,Jd,dr,Qu,A4,D4,Wd,O4,le,gr,Zu,Yd,x4,R4,ec,N4,P4,mr,tc,Kd,S4,I4,sc,H4,B4,$r,ac,Vd,G4,C4,nc,M4,F4,qr,rc,Xd,U4,L4,oc,z4,Xq,He,Zt,Qd,_r,J4,Zd,W4,Qq,lc,Y4,Zq,es,e_,Be,K4,vr,V4,X4,Er,Q4,t_,ic,Z4,s_,ts,a_,uc,ek,n_,cc,tk,r_,ss,o_,as,eg,wr,pc,sk,ak,tg,nk,ie,yr,fc,sg,rk,ok,hc,lk,ik,br,dc,ag,uk,ck,gc,pk,fk,jr,mc,ng,hk,dk,ns,gk,rg,mk,$k,qk,Tr,$c,og,_k,vk,rs,Ek,lg,wk,yk,l_,Ge,os,ig,kr,bk,ug,jk,i_,qc,Tk,u_,ls,c_,Ar,kk,Dr,Ak,p_,_c,Dk,f_,is,h_,vc,Ok,d_,us,cg,Or,Ec,xk,Rk,pg,Nk,Q,xr,Rr,fg,Pk,Sk,Ik,wc,Hk,Bk,Nr,yc,hg,Gk,Ck,bc,Mk,Fk,Pr,jc,Uk,Lk,cs,zk,dg,Jk,Wk,Yk,Sr,Tc,Kk,Vk,ps,Xk,gg,Qk,Zk,e7,Ir,kc,t7,s7,fs,a7,mg,n7,r7,g_,Ac,o7,m_,hs,$_,ds,$g,Hr,Dc,l7,i7,qg,u7,Br,Gr,Oc,_g,c7,p7,xc,f7,h7,Cr,Rc,vg,d7,g7,Nc,m7,q_,Ce,gs,Eg,Mr,$7,wg,q7,__,Fr,_7,Pc,v7,v_,Me,ms,yg,Ur,E7,bg,w7,E_,Sc,y7,w_,$s,y_,Fe,b7,Lr,j7,T7,zr,k7,b_,Ic,A7,j_,qs,T_,Hc,D7,k_,_s,jg,Jr,Bc,O7,x7,Tg,R7,z,Wr,Yr,kg,N7,P7,S7,Gc,I7,H7,Kr,Cc,Ag,B7,G7,Mc,C7,M7,Vr,Fc,F7,U7,vs,L7,Dg,z7,J7,W7,Xr,Uc,Og,Y7,K7,Lc,V7,X7,Qr,zc,Q7,Z7,Es,e9,xg,t9,s9,a9,Zr,Jc,n9,r9,ws,o9,Rg,l9,i9,u9,eo,Wc,c9,p9,ys,f9,Ng,h9,d9,A_,Yc,g9,D_,bs,O_,js,Pg,to,Kc,m9,$9,Sg,q9,Z,so,Vc,Ig,_9,v9,Xc,E9,w9,ao,Qc,Hg,y9,b9,Zc,j9,T9,no,ep,Bg,k9,A9,tp,D9,O9,ro,sp,Gg,x9,R9,Ts,N9,Cg,P9,S9,I9,oo,ap,Mg,H9,B9,ks,G9,Fg,C9,M9,x_,Ue,As,Ug,lo,F9,Lg,U9,R_,np,L9,N_,Ds,P_,io,z9,uo,J9,S_,rp,W9,I_,Os,H_,op,Y9,B_,xs,zg,co,lp,K9,V9,Jg,X9,S,po,fo,Wg,Q9,Z9,e6,ip,t6,s6,ho,up,Yg,a6,n6,cp,r6,o6,go,pp,l6,i6,ve,u6,Kg,c6,p6,fp,f6,h6,d6,mo,hp,g6,m6,Ee,$6,Vg,q6,_6,dp,v6,E6,w6,$o,gp,y6,b6,re,j6,Xg,T6,k6,mp,A6,D6,$p,O6,x6,R6,qo,qp,N6,P6,Rs,S6,Qg,I6,H6,B6,_o,_p,G6,C6,we,M6,Zg,F6,U6,em,L6,z6,J6,vo,vp,W6,Y6,Ns,K6,tm,V6,X6,Q6,Eo,Ep,Z6,e8,ye,t8,sm,s8,a8,am,n8,r8,o8,wo,wp,l8,i8,Ps,u8,nm,c8,p8,f8,yo,yp,h8,d8,Ss,g8,rm,m8,$8,q8,bo,bp,om,_8,v8,jp,E8,w8,jo,Tp,y8,b8,Is,j8,lm,T8,k8,A8,To,kp,D8,O8,Hs,x8,im,R8,N8,P8,ko,Ap,S8,I8,Bs,H8,um,B8,G8,G_,Dp,C8,C_,Gs,M_,Cs,cm,Ao,Op,M8,F8,pm,U8,fm,Do,xp,hm,L8,z8,Rp,J8,F_,Le,Ms,dm,Oo,W8,gm,Y8,U_,Fs,K8,Np,V8,X8,L_,ze,Us,mm,xo,Q8,$m,Z8,z_,Pp,eA,J_,Ls,W_,Ro,tA,No,sA,Y_,Sp,aA,K_,zs,V_,Ip,nA,X_,Js,qm,Po,Hp,rA,oA,_m,lA,ee,So,Io,vm,iA,uA,cA,Bp,pA,fA,Ho,Gp,Em,hA,dA,Cp,gA,mA,Bo,Mp,$A,qA,Ws,_A,wm,vA,EA,wA,Go,Fp,yA,bA,Ys,jA,ym,TA,kA,AA,Co,Up,DA,OA,Ks,xA,bm,RA,NA,Q_,Lp,PA,Z_,Vs,e1,Xs,jm,Mo,zp,SA,IA,Tm,HA,ue,Fo,Jp,km,BA,GA,Wp,CA,MA,Uo,Yp,Am,FA,UA,Kp,LA,zA,Lo,Vp,Dm,JA,WA,Xp,YA,KA,zo,Qp,Om,VA,XA,Zp,QA,t1,Je,Qs,xm,Jo,ZA,Rm,eD,s1,ef,tD,a1,Zs,n1,ea,r1,ce,sD,Wo,aD,nD,Yo,rD,oD,Ko,lD,o1,tf,iD,l1,ta,i1,sf,uD,u1,sa,Nm,Vo,af,cD,pD,Pm,fD,Sm,Xo,Qo,Im,hD,dD,gD,nf,mD,c1,rf,$D,p1,of,qD,f1,aa,h1,na,Hm,Zo,lf,_D,vD,Bm,ED,Gm,el,uf,Cm,wD,yD,cf,bD,d1,We,ra,Mm,tl,jD,Fm,TD,g1,pf,kD,m1,oa,$1,Ye,AD,sl,DD,OD,al,xD,q1,ff,RD,_1,la,Um,nl,hf,ND,PD,Lm,SD,te,rl,ol,zm,ID,HD,BD,df,GD,CD,ll,gf,Jm,MD,FD,mf,UD,LD,il,$f,zD,JD,ia,WD,Wm,YD,KD,VD,ul,qf,XD,QD,ua,ZD,Ym,eO,tO,sO,cl,_f,aO,nO,ca,rO,Km,oO,lO,v1,vf,iO,E1,pa,Vm,pl,Ef,uO,cO,Xm,pO,Qm,fl,wf,Zm,fO,hO,yf,dO,w1,bf,gO,y1,Ke,fa,e$,hl,mO,t$,$O,b1,jf,qO,j1,ha,T1,Ve,_O,dl,vO,EO,gl,wO,k1,Tf,yO,A1,da,D1,kf,bO,O1,ga,s$,ml,Af,jO,TO,a$,kO,n$,$l,ql,r$,AO,DO,OO,Df,xO,x1,Of,RO,R1,ma,N1,$a,o$,_l,xf,NO,PO,l$,SO,vl,El,Rf,i$,IO,HO,Nf,BO,GO,wl,Pf,u$,CO,MO,Sf,FO,P1,Xe,qa,c$,yl,UO,p$,LO,S1,If,zO,I1,_a,H1,bl,JO,jl,WO,B1,Hf,YO,G1,va,C1,Ea,KO,Tl,VO,XO,M1,wa,f$,kl,Bf,QO,ZO,h$,ex,d$,Al,Dl,g$,tx,sx,ax,Gf,nx,F1,Cf,rx,U1,ya,L1,ba,m$,Ol,Mf,ox,lx,$$,ix,Qe,xl,Ff,q$,ux,cx,Uf,px,fx,Rl,Lf,_$,hx,dx,zf,gx,mx,Nl,Jf,v$,$x,qx,Wf,_x,z1;return k=new U({}),V=new U({}),Na=new U({}),nt=new W({props:{$$slots:{default:[QC]},$$scope:{ctx:q}}}),rt=new C({props:{python:!0,js:!0,curl:!0,$$slots:{curl:[nM],js:[sM],python:[eM]},$$scope:{ctx:q}}}),pt=new C({props:{python:!0,js:!0,curl:!0,$$slots:{python:[oM]},$$scope:{ctx:q}}}),Xa=new U({}),gt=new W({props:{$$slots:{default:[lM]},$$scope:{ctx:q}}}),mt=new C({props:{python:!0,js:!0,curl:!0,$$slots:{curl:[hM],js:[pM],python:[uM]},$$scope:{ctx:q}}}),cn=new U({}),bt=new W({props:{$$slots:{default:[dM]},$$scope:{ctx:q}}}),jt=new C({props:{python:!0,js:!0,curl:!0,$$slots:{curl:[vM],js:[qM],python:[mM]},$$scope:{ctx:q}}}),On=new U({}),Pt=new W({props:{$$slots:{default:[EM]},$$scope:{ctx:q}}}),St=new C({props:{python:!0,js:!0,curl:!0,$$slots:{curl:[kM],js:[jM],python:[yM]},$$scope:{ctx:q}}}),sr=new U({}),zt=new W({props:{$$slots:{default:[AM]},$$scope:{ctx:q}}}),Jt=new C({props:{python:!0,js:!0,curl:!0,$$slots:{curl:[PM],js:[RM],python:[OM]},$$scope:{ctx:q}}}),Xt=new C({props:{python:!0,js:!0,curl:!0,$$slots:{python:[IM]},$$scope:{ctx:q}}}),_r=new U({}),es=new W({props:{$$slots:{default:[HM]},$$scope:{ctx:q}}}),ts=new C({props:{python:!0,js:!0,curl:!0,$$slots:{curl:[UM],js:[MM],python:[GM]},$$scope:{ctx:q}}}),ss=new C({props:{python:!0,js:!0,curl:!0,$$slots:{python:[zM]},$$scope:{ctx:q}}}),kr=new U({}),ls=new W({props:{$$slots:{default:[JM]},$$scope:{ctx:q}}}),is=new C({props:{python:!0,js:!0,curl:!0,$$slots:{curl:[QM],js:[VM],python:[YM]},$$scope:{ctx:q}}}),hs=new C({props:{python:!0,js:!0,curl:!0,$$slots:{python:[eF]},$$scope:{ctx:q}}}),Mr=new U({}),Ur=new U({}),$s=new W({props:{$$slots:{default:[tF]},$$scope:{ctx:q}}}),qs=new C({props:{python:!0,js:!0,curl:!0,$$slots:{curl:[lF],js:[rF],python:[aF]},$$scope:{ctx:q}}}),bs=new C({props:{python:!0,js:!0,curl:!0,$$slots:{python:[uF]},$$scope:{ctx:q}}}),lo=new U({}),Ds=new W({props:{$$slots:{default:[cF]},$$scope:{ctx:q}}}),Os=new C({props:{python:!0,js:!0,curl:!0,$$slots:{curl:[mF],js:[dF],python:[fF]},$$scope:{ctx:q}}}),Gs=new C({props:{python:!0,js:!0,curl:!0,$$slots:{python:[qF]},$$scope:{ctx:q}}}),Oo=new U({}),xo=new U({}),Ls=new W({props:{$$slots:{default:[_F]},$$scope:{ctx:q}}}),zs=new C({props:{python:!0,js:!0,curl:!0,$$slots:{curl:[jF],js:[yF],python:[EF]},$$scope:{ctx:q}}}),Vs=new C({props:{python:!0,js:!0,curl:!0,$$slots:{python:[kF]},$$scope:{ctx:q}}}),Jo=new U({}),Zs=new W({props:{$$slots:{default:[AF]},$$scope:{ctx:q}}}),ea=new W({props:{$$slots:{default:[DF]},$$scope:{ctx:q}}}),ta=new C({props:{python:!0,js:!0,curl:!0,$$slots:{curl:[SF],js:[NF],python:[xF]},$$scope:{ctx:q}}}),aa=new C({props:{python:!0,js:!0,curl:!0,$$slots:{python:[HF]},$$scope:{ctx:q}}}),tl=new U({}),oa=new W({props:{$$slots:{default:[BF]},$$scope:{ctx:q}}}),hl=new U({}),ha=new W({props:{$$slots:{default:[GF]},$$scope:{ctx:q}}}),da=new C({props:{python:!0,js:!0,curl:!0,$$slots:{curl:[zF],js:[UF],python:[MF]},$$scope:{ctx:q}}}),ma=new C({props:{python:!0,js:!0,curl:!0,$$slots:{python:[WF]},$$scope:{ctx:q}}}),yl=new U({}),_a=new W({props:{$$slots:{default:[YF]},$$scope:{ctx:q}}}),va=new C({props:{python:!0,js:!0,curl:!0,$$slots:{curl:[eU],js:[QF],python:[VF]},$$scope:{ctx:q}}}),ya=new C({props:{python:!0,js:!0,curl:!0,$$slots:{python:[sU]},$$scope:{ctx:q}}}),{c(){n=r("meta"),c=f(),s=r("h1"),d=r("a"),$=r("span"),_(k.$$.fragment),A=f(),T=r("span"),j=i("Detailed parameters"),O=f(),D=r("h2"),se=r("a"),De=r("span"),_(V.$$.fragment),J=f(),et=r("span"),Ll=i("Which task is used by this model ?"),Ra=f(),Oe=r("p"),gy=i(`In general the \u{1F917} Hosted API Inference accepts a simple string as an
input. However, more advanced usage depends on the \u201Ctask\u201D that the
model solves.`),K$=f(),zl=r("p"),my=i("The \u201Ctask\u201D of a model is defined here on it\u2019s model page:"),V$=f(),tt=r("img"),X$=f(),st=r("img"),Q$=f(),xe=r("h2"),at=r("a"),ph=r("span"),_(Na.$$.fragment),$y=f(),fh=r("span"),qy=i("Zero-shot classification task"),Z$=f(),Jl=r("p"),_y=i(`This task is a super useful to try it out classification with zero code,
you simply pass a sentence/paragraph and the possible labels for that
sentence and you get a result.`),eq=f(),_(nt.$$.fragment),tq=f(),Pa=r("p"),vy=i("Available with: "),Sa=r("a"),Ey=i("\u{1F917} Transformers"),sq=f(),Wl=r("p"),wy=i("Request:"),aq=f(),_(rt.$$.fragment),nq=f(),Yl=r("p"),yy=i(`When sending your request, you should send a JSON encoded payload. Here
are all the options`),rq=f(),ot=r("table"),hh=r("thead"),Ia=r("tr"),Kl=r("th"),by=i("All parameters"),jy=f(),dh=r("th"),Ty=f(),M=r("tbody"),Ha=r("tr"),Ba=r("td"),gh=r("strong"),ky=i("inputs"),Ay=i(" (required)"),Dy=f(),Vl=r("td"),Oy=i("a string or list of strings"),xy=f(),Ga=r("tr"),Ca=r("td"),mh=r("strong"),Ry=i("parameters"),Ny=i(" (required)"),Py=f(),Xl=r("td"),Sy=i("a dict containing the following keys:"),Iy=f(),Ma=r("tr"),Ql=r("td"),Hy=i("candidate_labels (required)"),By=f(),pe=r("td"),Gy=i("a list of strings that are potential classes for "),$h=r("code"),Cy=i("inputs"),My=i(". (max 10 candidate_labels, for more, simply run multiple requests, results are going to be misleading if using too many candidate_labels anyway. If you want to keep the exact same, you can simply run "),Zl=r("span"),Fy=i("multi_label=True"),Uy=i(" and do the scaling on your end. )"),Ly=f(),Fa=r("tr"),ei=r("td"),zy=i("multi_label"),Jy=f(),lt=r("td"),Wy=i("(Default: "),qh=r("code"),Yy=i("false"),Ky=i(") Boolean that is set to True if classes can overlap"),Vy=f(),Ua=r("tr"),ti=r("td"),_h=r("strong"),Xy=i("options"),Qy=f(),si=r("td"),Zy=i("a dict containing the following keys:"),eb=f(),La=r("tr"),ai=r("td"),tb=i("use_gpu"),sb=f(),it=r("td"),ab=i("(Default: "),vh=r("code"),nb=i("false"),rb=i("). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),ob=f(),za=r("tr"),ni=r("td"),lb=i("use_cache"),ib=f(),ut=r("td"),ub=i("(Default: "),Eh=r("code"),cb=i("true"),pb=i("). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),fb=f(),Ja=r("tr"),ri=r("td"),hb=i("wait_for_model"),db=f(),ct=r("td"),gb=i("(Default: "),wh=r("code"),mb=i("false"),$b=i(") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),oq=f(),oi=r("p"),qb=i("Return value is either a dict or a list of dicts if you sent a list of inputs"),lq=f(),li=r("p"),_b=i("Response:"),iq=f(),_(pt.$$.fragment),uq=f(),ft=r("table"),yh=r("thead"),Wa=r("tr"),ii=r("th"),vb=i("Returned values"),Eb=f(),bh=r("th"),wb=f(),Re=r("tbody"),Ya=r("tr"),ui=r("td"),jh=r("strong"),yb=i("sequence"),bb=f(),ci=r("td"),jb=i("The string sent as an input"),Tb=f(),Ka=r("tr"),pi=r("td"),Th=r("strong"),kb=i("labels"),Ab=f(),fi=r("td"),Db=i("The list of strings for labels that you sent (in order)"),Ob=f(),Va=r("tr"),hi=r("td"),kh=r("strong"),xb=i("scores"),Rb=f(),ht=r("td"),Nb=i("a list of floats that correspond the the probability of label, in the same order as "),Ah=r("code"),Pb=i("labels"),Sb=i("."),cq=f(),Ne=r("h2"),dt=r("a"),Dh=r("span"),_(Xa.$$.fragment),Ib=f(),Oh=r("span"),Hb=i("Translation task"),pq=f(),di=r("p"),Bb=i("This task is well known to translate text from one language to another"),fq=f(),_(gt.$$.fragment),hq=f(),Qa=r("p"),Gb=i("Available with: "),Za=r("a"),Cb=i("\u{1F917} Transformers"),dq=f(),gi=r("p"),Mb=i("Example:"),gq=f(),_(mt.$$.fragment),mq=f(),mi=r("p"),Fb=i(`When sending your request, you should send a JSON encoded payload. Here
are all the options`),$q=f(),$t=r("table"),xh=r("thead"),en=r("tr"),$i=r("th"),Ub=i("All parameters"),Lb=f(),Rh=r("th"),zb=f(),X=r("tbody"),tn=r("tr"),sn=r("td"),Nh=r("strong"),Jb=i("inputs"),Wb=i(" (required)"),Yb=f(),qi=r("td"),Kb=i("a string to be translated in the original languages"),Vb=f(),an=r("tr"),_i=r("td"),Ph=r("strong"),Xb=i("options"),Qb=f(),vi=r("td"),Zb=i("a dict containing the following keys:"),e0=f(),nn=r("tr"),Ei=r("td"),t0=i("use_gpu"),s0=f(),qt=r("td"),a0=i("(Default: "),Sh=r("code"),n0=i("false"),r0=i("). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),o0=f(),rn=r("tr"),wi=r("td"),l0=i("use_cache"),i0=f(),_t=r("td"),u0=i("(Default: "),Ih=r("code"),c0=i("true"),p0=i("). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),f0=f(),on=r("tr"),yi=r("td"),h0=i("wait_for_model"),d0=f(),vt=r("td"),g0=i("(Default: "),Hh=r("code"),m0=i("false"),$0=i(") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),qq=f(),bi=r("p"),q0=i("Return value is either a dict or a list of dicts if you sent a list of inputs"),_q=f(),Et=r("table"),Bh=r("thead"),ln=r("tr"),ji=r("th"),_0=i("Returned values"),v0=f(),Gh=r("th"),E0=f(),Ch=r("tbody"),un=r("tr"),Ti=r("td"),Mh=r("strong"),w0=i("translation_text"),y0=f(),ki=r("td"),b0=i("The string after translation"),vq=f(),Pe=r("h2"),wt=r("a"),Fh=r("span"),_(cn.$$.fragment),j0=f(),Uh=r("span"),T0=i("Summarization task"),Eq=f(),yt=r("p"),k0=i(`This task is well known to summarize text a big text into a small text.
Be careful, some models have a maximum length of input. That means that
the summary cannot handle full books for instance. Be careful when
choosing your model. If you want to discuss you summarization needs,
please get in touch <`),Ai=r("a"),A0=i("api-enterprise@huggingface.co"),D0=i(">"),wq=f(),_(bt.$$.fragment),yq=f(),pn=r("p"),O0=i("Available with: "),fn=r("a"),x0=i("\u{1F917} Transformers"),bq=f(),Di=r("p"),R0=i("Example:"),jq=f(),_(jt.$$.fragment),Tq=f(),Oi=r("p"),N0=i(`When sending your request, you should send a JSON encoded payload. Here
are all the options`),kq=f(),Tt=r("table"),Lh=r("thead"),hn=r("tr"),xi=r("th"),P0=i("All parameters"),S0=f(),zh=r("th"),I0=f(),B=r("tbody"),dn=r("tr"),gn=r("td"),Jh=r("strong"),H0=i("inputs"),B0=i(" (required)"),G0=f(),Ri=r("td"),C0=i("a string to be summarized"),M0=f(),mn=r("tr"),Ni=r("td"),Wh=r("strong"),F0=i("parameters"),U0=f(),Pi=r("td"),L0=i("a dict containing the following keys:"),z0=f(),$n=r("tr"),Si=r("td"),J0=i("min_length"),W0=f(),fe=r("td"),Y0=i("(Default: "),Yh=r("code"),K0=i("None"),V0=i("). Integer to define the minimum length "),Kh=r("strong"),X0=i("in tokens"),Q0=i(" of the output summary."),Z0=f(),qn=r("tr"),Ii=r("td"),ej=i("max_length"),tj=f(),he=r("td"),sj=i("(Default: "),Vh=r("code"),aj=i("None"),nj=i("). Integer to define the maximum length "),Xh=r("strong"),rj=i("in tokens"),oj=i(" of the output summary."),lj=f(),_n=r("tr"),Hi=r("td"),ij=i("top_k"),uj=f(),de=r("td"),cj=i("(Default: "),Qh=r("code"),pj=i("None"),fj=i("). Integer to define the top tokens considered within the "),Bi=r("span"),hj=i("sample"),dj=i(" operation to create new text."),gj=f(),vn=r("tr"),Gi=r("td"),mj=i("top_p"),$j=f(),ge=r("td"),qj=i("(Default: "),Zh=r("code"),_j=i("None"),vj=i("). Float to define the tokens that are within the sample` operation of text generation. Add tokens in the sample for more probable to least probable until the sum of the probabilities is greater than "),Ci=r("span"),Ej=i("top_p"),wj=i("."),yj=f(),En=r("tr"),Mi=r("td"),bj=i("temperature"),jj=f(),ae=r("td"),Tj=i("(Default: "),ed=r("code"),kj=i("1.0"),Aj=i("). Float (0.0-100.0). The temperature of the sampling operation. 1 means regular sampling, 0 means "),Fi=r("span"),Dj=i("top_k=1"),Oj=i(", "),Ui=r("span"),xj=i("100.0"),Rj=i(" is getting closer to uniform probability."),Nj=f(),wn=r("tr"),Li=r("td"),Pj=i("repetition_penalty"),Sj=f(),kt=r("td"),Ij=i("(Default: "),td=r("code"),Hj=i("None"),Bj=i("). Float (0.0-100.0). The more a token is used within generation the more it is penalized to not be picked in successive generation passes."),Gj=f(),yn=r("tr"),zi=r("td"),Cj=i("max_time"),Mj=f(),At=r("td"),Fj=i("(Default: "),sd=r("code"),Uj=i("None"),Lj=i("). Float (0-120.0). The amount of time in seconds that the query should take maximum. Network can cause some overhead so it will be a soft limit."),zj=f(),bn=r("tr"),Ji=r("td"),ad=r("strong"),Jj=i("options"),Wj=f(),Wi=r("td"),Yj=i("a dict containing the following keys:"),Kj=f(),jn=r("tr"),Yi=r("td"),Vj=i("use_gpu"),Xj=f(),Dt=r("td"),Qj=i("(Default: "),nd=r("code"),Zj=i("false"),e3=i("). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),t3=f(),Tn=r("tr"),Ki=r("td"),s3=i("use_cache"),a3=f(),Ot=r("td"),n3=i("(Default: "),rd=r("code"),r3=i("true"),o3=i("). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),l3=f(),kn=r("tr"),Vi=r("td"),i3=i("wait_for_model"),u3=f(),xt=r("td"),c3=i("(Default: "),od=r("code"),p3=i("false"),f3=i(") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),Aq=f(),Xi=r("p"),h3=i("Return value is either a dict or a list of dicts if you sent a list of inputs"),Dq=f(),Rt=r("table"),ld=r("thead"),An=r("tr"),Qi=r("th"),d3=i("Returned values"),g3=f(),id=r("th"),m3=f(),ud=r("tbody"),Dn=r("tr"),Zi=r("td"),cd=r("strong"),$3=i("summarization_text"),q3=f(),eu=r("td"),_3=i("The string after translation"),Oq=f(),Se=r("h2"),Nt=r("a"),pd=r("span"),_(On.$$.fragment),v3=f(),fd=r("span"),E3=i("Conversational task"),xq=f(),tu=r("p"),w3=i(`This task corresponds to any chatbot like structure. Models tend to have
shorted max_length, so please check with caution when using a given
model if you need long range dependency or not.`),Rq=f(),_(Pt.$$.fragment),Nq=f(),xn=r("p"),y3=i("Available with: "),Rn=r("a"),b3=i("\u{1F917} Transformers"),Pq=f(),su=r("p"),j3=i("Example:"),Sq=f(),_(St.$$.fragment),Iq=f(),au=r("p"),T3=i(`When sending your request, you should send a JSON encoded payload. Here
are all the options`),Hq=f(),It=r("table"),hd=r("thead"),Nn=r("tr"),nu=r("th"),k3=i("All parameters"),A3=f(),dd=r("th"),D3=f(),P=r("tbody"),Pn=r("tr"),Sn=r("td"),gd=r("strong"),O3=i("inputs"),x3=i(" (required)"),R3=f(),md=r("td"),N3=f(),In=r("tr"),ru=r("td"),P3=i("text (required)"),S3=f(),ou=r("td"),I3=i("The last input from the user in the conversation."),H3=f(),Hn=r("tr"),lu=r("td"),B3=i("generated_responses"),G3=f(),iu=r("td"),C3=i("A list of strings corresponding to the earlier replies from the model."),M3=f(),Bn=r("tr"),uu=r("td"),F3=i("past_user_inputs"),U3=f(),Ht=r("td"),L3=i("A list of strings corresponding to the earlier replies from the user. Should be of the same length of "),$d=r("code"),z3=i("generated_responses"),J3=i("."),W3=f(),Gn=r("tr"),cu=r("td"),qd=r("strong"),Y3=i("parameters"),K3=f(),pu=r("td"),V3=i("a dict containing the following keys:"),X3=f(),Cn=r("tr"),fu=r("td"),Q3=i("min_length"),Z3=f(),me=r("td"),eT=i("(Default: "),_d=r("code"),tT=i("None"),sT=i("). Integer to define the minimum length "),vd=r("strong"),aT=i("in tokens"),nT=i(" of the output summary."),rT=f(),Mn=r("tr"),hu=r("td"),oT=i("max_length"),lT=f(),$e=r("td"),iT=i("(Default: "),Ed=r("code"),uT=i("None"),cT=i("). Integer to define the maximum length "),wd=r("strong"),pT=i("in tokens"),fT=i(" of the output summary."),hT=f(),Fn=r("tr"),du=r("td"),dT=i("top_k"),gT=f(),qe=r("td"),mT=i("(Default: "),yd=r("code"),$T=i("None"),qT=i("). Integer to define the top tokens considered within the "),gu=r("span"),_T=i("sample"),vT=i(" operation to create new text."),ET=f(),Un=r("tr"),mu=r("td"),wT=i("top_p"),yT=f(),_e=r("td"),bT=i("(Default: "),bd=r("code"),jT=i("None"),TT=i("). Float to define the tokens that are within the sample` operation of text generation. Add tokens in the sample for more probable to least probable until the sum of the probabilities is greater than "),$u=r("span"),kT=i("top_p"),AT=i("."),DT=f(),Ln=r("tr"),qu=r("td"),OT=i("temperature"),xT=f(),ne=r("td"),RT=i("(Default: "),jd=r("code"),NT=i("1.0"),PT=i("). Float (0.0-100.0). The temperature of the sampling operation. 1 means regular sampling, 0 means "),_u=r("span"),ST=i("top_k=1"),IT=i(", "),vu=r("span"),HT=i("100.0"),BT=i(" is getting closer to uniform probability."),GT=f(),zn=r("tr"),Eu=r("td"),CT=i("repetition_penalty"),MT=f(),Bt=r("td"),FT=i("(Default: "),Td=r("code"),UT=i("None"),LT=i("). Float (0.0-100.0). The more a token is used within generation the more it is penalized to not be picked in successive generation passes."),zT=f(),Jn=r("tr"),wu=r("td"),JT=i("max_time"),WT=f(),Gt=r("td"),YT=i("(Default: "),kd=r("code"),KT=i("None"),VT=i("). Float (0-120.0). The amount of time in seconds that the query should take maximum. Network can cause some overhead so it will be a soft limit."),XT=f(),Wn=r("tr"),yu=r("td"),Ad=r("strong"),QT=i("options"),ZT=f(),bu=r("td"),e5=i("a dict containing the following keys:"),t5=f(),Yn=r("tr"),ju=r("td"),s5=i("use_gpu"),a5=f(),Ct=r("td"),n5=i("(Default: "),Dd=r("code"),r5=i("false"),o5=i("). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),l5=f(),Kn=r("tr"),Tu=r("td"),i5=i("use_cache"),u5=f(),Mt=r("td"),c5=i("(Default: "),Od=r("code"),p5=i("true"),f5=i("). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),h5=f(),Vn=r("tr"),ku=r("td"),d5=i("wait_for_model"),g5=f(),Ft=r("td"),m5=i("(Default: "),xd=r("code"),$5=i("false"),q5=i(") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),Bq=f(),Au=r("p"),_5=i("Return value is either a dict or a list of dicts if you sent a list of inputs"),Gq=f(),Ut=r("table"),Rd=r("thead"),Xn=r("tr"),Du=r("th"),v5=i("Returned values"),E5=f(),Nd=r("th"),w5=f(),oe=r("tbody"),Qn=r("tr"),Ou=r("td"),Pd=r("strong"),y5=i("generated_text"),b5=f(),xu=r("td"),j5=i("The answer of the bot"),T5=f(),Zn=r("tr"),Ru=r("td"),Sd=r("strong"),k5=i("conversation"),A5=f(),Nu=r("td"),D5=i("A facility dictionnary to send back for the next input (with the new user input addition)."),O5=f(),er=r("tr"),Pu=r("td"),x5=i("past_user_inputs"),R5=f(),Su=r("td"),N5=i("List of strings. The last inputs from the user in the conversation, <em>after the model has run."),P5=f(),tr=r("tr"),Iu=r("td"),S5=i("generated_responses"),I5=f(),Hu=r("td"),H5=i("List of strings. The last outputs from the model in the conversation, <em>after the model has run."),Cq=f(),Ie=r("h2"),Lt=r("a"),Id=r("span"),_(sr.$$.fragment),B5=f(),Hd=r("span"),G5=i("Table question answering task"),Mq=f(),Bu=r("p"),C5=i(`Don\u2019t know SQL? Don\u2019t want to dive into a large spreadsheet? Ask
questions in plain english!`),Fq=f(),_(zt.$$.fragment),Uq=f(),ar=r("p"),M5=i("Available with: "),nr=r("a"),F5=i("\u{1F917} Transformers"),Lq=f(),Gu=r("p"),U5=i("Example:"),zq=f(),_(Jt.$$.fragment),Jq=f(),Cu=r("p"),L5=i(`When sending your request, you should send a JSON encoded payload. Here
are all the options`),Wq=f(),Wt=r("table"),Bd=r("thead"),rr=r("tr"),Mu=r("th"),z5=i("All parameters"),J5=f(),Gd=r("th"),W5=f(),L=r("tbody"),or=r("tr"),lr=r("td"),Cd=r("strong"),Y5=i("inputs"),K5=i(" (required)"),V5=f(),Md=r("td"),X5=f(),ir=r("tr"),Fu=r("td"),Q5=i("query (required)"),Z5=f(),Uu=r("td"),e4=i("The query in plain text that you want to ask the table"),t4=f(),ur=r("tr"),Lu=r("td"),s4=i("table (required)"),a4=f(),zu=r("td"),n4=i("A table of data represented as a dict of list where entries are headers and the lists are all the values, all lists must have the same size."),r4=f(),cr=r("tr"),Ju=r("td"),Fd=r("strong"),o4=i("options"),l4=f(),Wu=r("td"),i4=i("a dict containing the following keys:"),u4=f(),pr=r("tr"),Yu=r("td"),c4=i("use_gpu"),p4=f(),Yt=r("td"),f4=i("(Default: "),Ud=r("code"),h4=i("false"),d4=i("). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),g4=f(),fr=r("tr"),Ku=r("td"),m4=i("use_cache"),$4=f(),Kt=r("td"),q4=i("(Default: "),Ld=r("code"),_4=i("true"),v4=i("). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),E4=f(),hr=r("tr"),Vu=r("td"),w4=i("wait_for_model"),y4=f(),Vt=r("td"),b4=i("(Default: "),zd=r("code"),j4=i("false"),T4=i(") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),Yq=f(),Xu=r("p"),k4=i("Return value is either a dict or a list of dicts if you sent a list of inputs"),Kq=f(),_(Xt.$$.fragment),Vq=f(),Qt=r("table"),Jd=r("thead"),dr=r("tr"),Qu=r("th"),A4=i("Returned values"),D4=f(),Wd=r("th"),O4=f(),le=r("tbody"),gr=r("tr"),Zu=r("td"),Yd=r("strong"),x4=i("answer"),R4=f(),ec=r("td"),N4=i("The plaintext answer"),P4=f(),mr=r("tr"),tc=r("td"),Kd=r("strong"),S4=i("coordinates"),I4=f(),sc=r("td"),H4=i("a list of coordinates of the cells references in the answer"),B4=f(),$r=r("tr"),ac=r("td"),Vd=r("strong"),G4=i("cells"),C4=f(),nc=r("td"),M4=i("a list of coordinates of the cells contents"),F4=f(),qr=r("tr"),rc=r("td"),Xd=r("strong"),U4=i("aggregator"),L4=f(),oc=r("td"),z4=i("The aggregator used to get the answer"),Xq=f(),He=r("h2"),Zt=r("a"),Qd=r("span"),_(_r.$$.fragment),J4=f(),Zd=r("span"),W4=i("Question answering task"),Qq=f(),lc=r("p"),Y4=i("Want to have a nice know-it-all bot that can answer any questions ?"),Zq=f(),_(es.$$.fragment),e_=f(),Be=r("p"),K4=i("Available with: "),vr=r("a"),V4=i("\u{1F917}Transformers"),X4=i(` and
`),Er=r("a"),Q4=i("AllenNLP"),t_=f(),ic=r("p"),Z4=i("Example:"),s_=f(),_(ts.$$.fragment),a_=f(),uc=r("p"),ek=i(`When sending your request, you should send a JSON encoded payload. Here
are all the options`),n_=f(),cc=r("p"),tk=i("Return value is either a dict or a list of dicts if you sent a list of inputs"),r_=f(),_(ss.$$.fragment),o_=f(),as=r("table"),eg=r("thead"),wr=r("tr"),pc=r("th"),sk=i("Returned values"),ak=f(),tg=r("th"),nk=f(),ie=r("tbody"),yr=r("tr"),fc=r("td"),sg=r("strong"),rk=i("answer"),ok=f(),hc=r("td"),lk=i("A string that\u2019s the answer within the text."),ik=f(),br=r("tr"),dc=r("td"),ag=r("strong"),uk=i("score"),ck=f(),gc=r("td"),pk=i("A floats that represents how likely that the answer is correct"),fk=f(),jr=r("tr"),mc=r("td"),ng=r("strong"),hk=i("start"),dk=f(),ns=r("td"),gk=i("The index (string wise) of the start of the answer within "),rg=r("code"),mk=i("context"),$k=i("."),qk=f(),Tr=r("tr"),$c=r("td"),og=r("strong"),_k=i("stop"),vk=f(),rs=r("td"),Ek=i("The index (string wise) of the stop of the answer within "),lg=r("code"),wk=i("context"),yk=i("."),l_=f(),Ge=r("h2"),os=r("a"),ig=r("span"),_(kr.$$.fragment),bk=f(),ug=r("span"),jk=i("Text-classification task"),i_=f(),qc=r("p"),Tk=i(`Usually used for sentiment-analysis this will output the likelihood of
classes of an input.`),u_=f(),_(ls.$$.fragment),c_=f(),Ar=r("p"),kk=i("Available with: "),Dr=r("a"),Ak=i("\u{1F917} Transformers"),p_=f(),_c=r("p"),Dk=i("Example:"),f_=f(),_(is.$$.fragment),h_=f(),vc=r("p"),Ok=i(`When sending your request, you should send a JSON encoded payload. Here
are all the options`),d_=f(),us=r("table"),cg=r("thead"),Or=r("tr"),Ec=r("th"),xk=i("All parameters"),Rk=f(),pg=r("th"),Nk=f(),Q=r("tbody"),xr=r("tr"),Rr=r("td"),fg=r("strong"),Pk=i("inputs"),Sk=i(" (required)"),Ik=f(),wc=r("td"),Hk=i("a string to be classified"),Bk=f(),Nr=r("tr"),yc=r("td"),hg=r("strong"),Gk=i("options"),Ck=f(),bc=r("td"),Mk=i("a dict containing the following keys:"),Fk=f(),Pr=r("tr"),jc=r("td"),Uk=i("use_gpu"),Lk=f(),cs=r("td"),zk=i("(Default: "),dg=r("code"),Jk=i("false"),Wk=i("). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),Yk=f(),Sr=r("tr"),Tc=r("td"),Kk=i("use_cache"),Vk=f(),ps=r("td"),Xk=i("(Default: "),gg=r("code"),Qk=i("true"),Zk=i("). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),e7=f(),Ir=r("tr"),kc=r("td"),t7=i("wait_for_model"),s7=f(),fs=r("td"),a7=i("(Default: "),mg=r("code"),n7=i("false"),r7=i(") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),g_=f(),Ac=r("p"),o7=i("Return value is either a dict or a list of dicts if you sent a list of inputs"),m_=f(),_(hs.$$.fragment),$_=f(),ds=r("table"),$g=r("thead"),Hr=r("tr"),Dc=r("th"),l7=i("Returned values"),i7=f(),qg=r("th"),u7=f(),Br=r("tbody"),Gr=r("tr"),Oc=r("td"),_g=r("strong"),c7=i("label"),p7=f(),xc=r("td"),f7=i("The label for the class (model specific)"),h7=f(),Cr=r("tr"),Rc=r("td"),vg=r("strong"),d7=i("score"),g7=f(),Nc=r("td"),m7=i("A floats that represents how likely is that the text belongs the this class."),q_=f(),Ce=r("h2"),gs=r("a"),Eg=r("span"),_(Mr.$$.fragment),$7=f(),wg=r("span"),q7=i("Named Entity Recognition (NER) task"),__=f(),Fr=r("p"),_7=i("See "),Pc=r("a"),v7=i("Token-classification task"),v_=f(),Me=r("h2"),ms=r("a"),yg=r("span"),_(Ur.$$.fragment),E7=f(),bg=r("span"),w7=i("Token-classification task"),E_=f(),Sc=r("p"),y7=i(`Usually used for sentence parsing, either grammatical, or Named Entity
Recognition (NER) to understand keywords contained within text.`),w_=f(),_($s.$$.fragment),y_=f(),Fe=r("p"),b7=i("Available with: "),Lr=r("a"),j7=i("\u{1F917} Transformers"),T7=i(`,
`),zr=r("a"),k7=i("Flair"),b_=f(),Ic=r("p"),A7=i("Example:"),j_=f(),_(qs.$$.fragment),T_=f(),Hc=r("p"),D7=i(`When sending your request, you should send a JSON encoded payload. Here
are all the options`),k_=f(),_s=r("table"),jg=r("thead"),Jr=r("tr"),Bc=r("th"),O7=i("All parameters"),x7=f(),Tg=r("th"),R7=f(),z=r("tbody"),Wr=r("tr"),Yr=r("td"),kg=r("strong"),N7=i("inputs"),P7=i(" (required)"),S7=f(),Gc=r("td"),I7=i("a string to be classified"),H7=f(),Kr=r("tr"),Cc=r("td"),Ag=r("strong"),B7=i("parameters"),G7=f(),Mc=r("td"),C7=i("a dict containing the following key:"),M7=f(),Vr=r("tr"),Fc=r("td"),F7=i("aggregation_strategy"),U7=f(),vs=r("td"),L7=i("(Default: "),Dg=r("code"),z7=i('simple</span></code>). There are several aggregation strategies:<br>* <code class="docutils literal notranslate"><span class="pre">none</span></code>: Every token gets classified without further aggregation.<br>* <code class="docutils literal notranslate"><span class="pre">simple</span></code>: Entities are grouped according to the default schema (B-, I- tags get merged when the tag is similar).<br>* <code class="docutils literal notranslate"><span class="pre">first</span></code>: Same as the <code class="docutils literal notranslate"><span class="pre">simple</span></code> strategy except words cannot end up with different tags. Words will use the tag of the first token when there is ambiguity.<br>* <code class="docutils literal notranslate"><span class="pre">average</span></code>: Same as the <code class="docutils literal notranslate"><span class="pre">simple</span></code> strategy except words cannot end up with different tags. Scores are averaged across tokens and then the maximum label is applied.<br>* <code class="docutils literal notranslate"><span class="pre">max</span></code>: Same as the <code class="docutils literal notranslate"><span class="pre">simple'),J7=i(" strategy except words cannot end up with different tags. Word entity will be the token with the maximum score."),W7=f(),Xr=r("tr"),Uc=r("td"),Og=r("strong"),Y7=i("options"),K7=f(),Lc=r("td"),V7=i("a dict containing the following keys:"),X7=f(),Qr=r("tr"),zc=r("td"),Q7=i("use_gpu"),Z7=f(),Es=r("td"),e9=i("(Default: "),xg=r("code"),t9=i("false"),s9=i("). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),a9=f(),Zr=r("tr"),Jc=r("td"),n9=i("use_cache"),r9=f(),ws=r("td"),o9=i("(Default: "),Rg=r("code"),l9=i("true"),i9=i("). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),u9=f(),eo=r("tr"),Wc=r("td"),c9=i("wait_for_model"),p9=f(),ys=r("td"),f9=i("(Default: "),Ng=r("code"),h9=i("false"),d9=i(") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),A_=f(),Yc=r("p"),g9=i("Return value is either a dict or a list of dicts if you sent a list of inputs"),D_=f(),_(bs.$$.fragment),O_=f(),js=r("table"),Pg=r("thead"),to=r("tr"),Kc=r("th"),m9=i("Returned values"),$9=f(),Sg=r("th"),q9=f(),Z=r("tbody"),so=r("tr"),Vc=r("td"),Ig=r("strong"),_9=i("entity_group"),v9=f(),Xc=r("td"),E9=i("The type for the entity being recognized (model specific)."),w9=f(),ao=r("tr"),Qc=r("td"),Hg=r("strong"),y9=i("score"),b9=f(),Zc=r("td"),j9=i("How likely the entity was recognized."),T9=f(),no=r("tr"),ep=r("td"),Bg=r("strong"),k9=i("word"),A9=f(),tp=r("td"),D9=i("The string that was captured"),O9=f(),ro=r("tr"),sp=r("td"),Gg=r("strong"),x9=i("start"),R9=f(),Ts=r("td"),N9=i("The offset stringwise where the answer is located. Useful to disambiguate if "),Cg=r("code"),P9=i("word"),S9=i(" occurs multiple times."),I9=f(),oo=r("tr"),ap=r("td"),Mg=r("strong"),H9=i("end"),B9=f(),ks=r("td"),G9=i("The offset stringwise where the answer is located. Useful to disambiguate if "),Fg=r("code"),C9=i("word"),M9=i(" occurs multiple times."),x_=f(),Ue=r("h2"),As=r("a"),Ug=r("span"),_(lo.$$.fragment),F9=f(),Lg=r("span"),U9=i("Text-generation task"),R_=f(),np=r("p"),L9=i("Use to continue text from a prompt. This is a very generic task."),N_=f(),_(Ds.$$.fragment),P_=f(),io=r("p"),z9=i("Available with: "),uo=r("a"),J9=i("\u{1F917} Transformers"),S_=f(),rp=r("p"),W9=i("Example:"),I_=f(),_(Os.$$.fragment),H_=f(),op=r("p"),Y9=i(`When sending your request, you should send a JSON encoded payload. Here
are all the options`),B_=f(),xs=r("table"),zg=r("thead"),co=r("tr"),lp=r("th"),K9=i("All parameters"),V9=f(),Jg=r("th"),X9=f(),S=r("tbody"),po=r("tr"),fo=r("td"),Wg=r("strong"),Q9=i("inputs"),Z9=i(" (required):"),e6=f(),ip=r("td"),t6=i("a string to be generated from"),s6=f(),ho=r("tr"),up=r("td"),Yg=r("strong"),a6=i("parameters"),n6=f(),cp=r("td"),r6=i("dict containing the following keys:"),o6=f(),go=r("tr"),pp=r("td"),l6=i("top_k"),i6=f(),ve=r("td"),u6=i("(Default: "),Kg=r("code"),c6=i("None"),p6=i("). Integer to define the top tokens considered within the "),fp=r("span"),f6=i("sample"),h6=i(" operation to create new text."),d6=f(),mo=r("tr"),hp=r("td"),g6=i("top_p"),m6=f(),Ee=r("td"),$6=i("(Default: "),Vg=r("code"),q6=i("None"),_6=i("). Float to define the tokens that are within the  sample` operation of text generation. Add tokens in the sample for more probable to least probable until the sum of the probabilities is greater than "),dp=r("span"),v6=i("top_p"),E6=i("."),w6=f(),$o=r("tr"),gp=r("td"),y6=i("temperature"),b6=f(),re=r("td"),j6=i("(Default: "),Xg=r("code"),T6=i("1.0"),k6=i("). Float (0.0-100.0). The temperature of the sampling operation. 1 means regular sampling, 0 means "),mp=r("span"),A6=i("top_k=1"),D6=i(", "),$p=r("span"),O6=i("100.0"),x6=i(" is getting closer to uniform probability."),R6=f(),qo=r("tr"),qp=r("td"),N6=i("repetition_penalty"),P6=f(),Rs=r("td"),S6=i("(Default: "),Qg=r("code"),I6=i("None"),H6=i("). Float (0.0-100.0). The more a token is used within generation the more it is penalized to not be picked in successive generation passes."),B6=f(),_o=r("tr"),_p=r("td"),G6=i("max_new_tokens"),C6=f(),we=r("td"),M6=i("(Default: "),Zg=r("code"),F6=i("None"),U6=i("). Int (0-250). The amount of new tokens to be generated, this does "),em=r("strong"),L6=i("not"),z6=i(" include the input length it is a estimate of the size of generated text you want. Each new tokens slows down the request, so look for balance between response times and length of text generated."),J6=f(),vo=r("tr"),vp=r("td"),W6=i("max_time"),Y6=f(),Ns=r("td"),K6=i("(Default: "),tm=r("code"),V6=i('None</span></code>). Float (0-120.0). The amount of time in seconds that the query should take maximum. Network can cause some overhead so it will be a soft limit. Use that in combination with <code class="xref py py-obj docutils literal notranslate"><span class="pre">max_new_tokens'),X6=i(" for best results."),Q6=f(),Eo=r("tr"),Ep=r("td"),Z6=i("return_full_text"),e8=f(),ye=r("td"),t8=i("(Default: "),sm=r("code"),s8=i("True"),a8=i("). Bool. If set to False, the return results will "),am=r("strong"),n8=i("not"),r8=i(" contain the original query making it easier for prompting."),o8=f(),wo=r("tr"),wp=r("td"),l8=i("num_return_sequences"),i8=f(),Ps=r("td"),u8=i("(Default: "),nm=r("code"),c8=i("1"),p8=i("). Integer. The number of proposition you want to be returned."),f8=f(),yo=r("tr"),yp=r("td"),h8=i("do_sample"),d8=f(),Ss=r("td"),g8=i("(Optional: "),rm=r("code"),m8=i("True"),$8=i("). Bool. Whether or not to use sampling, use greedy decoding otherwise."),q8=f(),bo=r("tr"),bp=r("td"),om=r("strong"),_8=i("options"),v8=f(),jp=r("td"),E8=i("a dict containing the following keys:"),w8=f(),jo=r("tr"),Tp=r("td"),y8=i("use_gpu"),b8=f(),Is=r("td"),j8=i("(Default: "),lm=r("code"),T8=i("false"),k8=i("). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),A8=f(),To=r("tr"),kp=r("td"),D8=i("use_cache"),O8=f(),Hs=r("td"),x8=i("(Default: "),im=r("code"),R8=i("true"),N8=i("). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),P8=f(),ko=r("tr"),Ap=r("td"),S8=i("wait_for_model"),I8=f(),Bs=r("td"),H8=i("(Default: "),um=r("code"),B8=i("false"),G8=i(") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),G_=f(),Dp=r("p"),C8=i("Return value is either a dict or a list of dicts if you sent a list of inputs"),C_=f(),_(Gs.$$.fragment),M_=f(),Cs=r("table"),cm=r("thead"),Ao=r("tr"),Op=r("th"),M8=i("Returned values"),F8=f(),pm=r("th"),U8=f(),fm=r("tbody"),Do=r("tr"),xp=r("td"),hm=r("strong"),L8=i("generated_text"),z8=f(),Rp=r("td"),J8=i("The continuated string"),F_=f(),Le=r("h2"),Ms=r("a"),dm=r("span"),_(Oo.$$.fragment),W8=f(),gm=r("span"),Y8=i("Text2text-generation task"),U_=f(),Fs=r("p"),K8=i("Essentially "),Np=r("a"),V8=i("Text-generation task"),X8=i(`. But uses
Encoder-Decoder architecture, so might change in the future for more
options.`),L_=f(),ze=r("h2"),Us=r("a"),mm=r("span"),_(xo.$$.fragment),Q8=f(),$m=r("span"),Z8=i("Fill mask task"),z_=f(),Pp=r("p"),eA=i(`Tries to fill in a hole with a missing word (token to be precise).
That\u2019s the base task for BERT models.`),J_=f(),_(Ls.$$.fragment),W_=f(),Ro=r("p"),tA=i("Available with: "),No=r("a"),sA=i("\u{1F917} Transformers"),Y_=f(),Sp=r("p"),aA=i("Example:"),K_=f(),_(zs.$$.fragment),V_=f(),Ip=r("p"),nA=i(`When sending your request, you should send a JSON encoded payload. Here
are all the options`),X_=f(),Js=r("table"),qm=r("thead"),Po=r("tr"),Hp=r("th"),rA=i("All parameters"),oA=f(),_m=r("th"),lA=f(),ee=r("tbody"),So=r("tr"),Io=r("td"),vm=r("strong"),iA=i("inputs"),uA=i(" (required):"),cA=f(),Bp=r("td"),pA=i("a string to be filled from, must contain the [MASK] token (check model card for exact name of the mask)"),fA=f(),Ho=r("tr"),Gp=r("td"),Em=r("strong"),hA=i("options"),dA=f(),Cp=r("td"),gA=i("a dict containing the following keys:"),mA=f(),Bo=r("tr"),Mp=r("td"),$A=i("use_gpu"),qA=f(),Ws=r("td"),_A=i("(Default: "),wm=r("code"),vA=i("false"),EA=i("). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),wA=f(),Go=r("tr"),Fp=r("td"),yA=i("use_cache"),bA=f(),Ys=r("td"),jA=i("(Default: "),ym=r("code"),TA=i("true"),kA=i("). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),AA=f(),Co=r("tr"),Up=r("td"),DA=i("wait_for_model"),OA=f(),Ks=r("td"),xA=i("(Default: "),bm=r("code"),RA=i("false"),NA=i(") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),Q_=f(),Lp=r("p"),PA=i("Return value is either a dict or a list of dicts if you sent a list of inputs"),Z_=f(),_(Vs.$$.fragment),e1=f(),Xs=r("table"),jm=r("thead"),Mo=r("tr"),zp=r("th"),SA=i("Returned values"),IA=f(),Tm=r("th"),HA=f(),ue=r("tbody"),Fo=r("tr"),Jp=r("td"),km=r("strong"),BA=i("sequence"),GA=f(),Wp=r("td"),CA=i("The actual sequence of tokens that ran against the model (may contain special tokens)"),MA=f(),Uo=r("tr"),Yp=r("td"),Am=r("strong"),FA=i("score"),UA=f(),Kp=r("td"),LA=i("The probability for this token."),zA=f(),Lo=r("tr"),Vp=r("td"),Dm=r("strong"),JA=i("token"),WA=f(),Xp=r("td"),YA=i("The id of the token"),KA=f(),zo=r("tr"),Qp=r("td"),Om=r("strong"),VA=i("token_str"),XA=f(),Zp=r("td"),QA=i("The string representation of the token"),t1=f(),Je=r("h2"),Qs=r("a"),xm=r("span"),_(Jo.$$.fragment),ZA=f(),Rm=r("span"),eD=i("Automatic speech recognition task"),s1=f(),ef=r("p"),tD=i(`This task reads some audio input and outputs the said words within the
audio files.`),a1=f(),_(Zs.$$.fragment),n1=f(),_(ea.$$.fragment),r1=f(),ce=r("p"),sD=i("Available with: "),Wo=r("a"),aD=i("\u{1F917} Transformers"),nD=f(),Yo=r("a"),rD=i("ESPnet"),oD=i(` and
`),Ko=r("a"),lD=i("SpeechBrain"),o1=f(),tf=r("p"),iD=i("Request:"),l1=f(),_(ta.$$.fragment),i1=f(),sf=r("p"),uD=i(`When sending your request, you should send a binary payload that simply
contains your audio file. We try to support most formats (Flac, Wav,
Mp3, Ogg etc...). And we automatically rescale the sampling rate to the
appropriate rate for the given model (usually 16KHz).`),u1=f(),sa=r("table"),Nm=r("thead"),Vo=r("tr"),af=r("th"),cD=i("All parameters"),pD=f(),Pm=r("th"),fD=f(),Sm=r("tbody"),Xo=r("tr"),Qo=r("td"),Im=r("strong"),hD=i("no parameter"),dD=i(" (required)"),gD=f(),nf=r("td"),mD=i("a binary representation of the audio file. No other parameters are currently allowed."),c1=f(),rf=r("p"),$D=i("Return value is either a dict or a list of dicts if you sent a list of inputs"),p1=f(),of=r("p"),qD=i("Response:"),f1=f(),_(aa.$$.fragment),h1=f(),na=r("table"),Hm=r("thead"),Zo=r("tr"),lf=r("th"),_D=i("Returned values"),vD=f(),Bm=r("th"),ED=f(),Gm=r("tbody"),el=r("tr"),uf=r("td"),Cm=r("strong"),wD=i("text"),yD=f(),cf=r("td"),bD=i("The string that was recognized within the audio file."),d1=f(),We=r("h2"),ra=r("a"),Mm=r("span"),_(tl.$$.fragment),jD=f(),Fm=r("span"),TD=i("Feature-extraction task"),g1=f(),pf=r("p"),kD=i(`This task reads some text and outputs raw float values, that usually
consumed as part of a semantic database/semantic search.`),m1=f(),_(oa.$$.fragment),$1=f(),Ye=r("p"),AD=i("Available with: "),sl=r("a"),DD=i("\u{1F917} Transformers"),OD=f(),al=r("a"),xD=i("Sentence-transformers"),q1=f(),ff=r("p"),RD=i("Request:"),_1=f(),la=r("table"),Um=r("thead"),nl=r("tr"),hf=r("th"),ND=i("All parameters"),PD=f(),Lm=r("th"),SD=f(),te=r("tbody"),rl=r("tr"),ol=r("td"),zm=r("strong"),ID=i("inputs"),HD=i(" (required):"),BD=f(),df=r("td"),GD=i("a string or a list of strings to get the features from."),CD=f(),ll=r("tr"),gf=r("td"),Jm=r("strong"),MD=i("options"),FD=f(),mf=r("td"),UD=i("a dict containing the following keys:"),LD=f(),il=r("tr"),$f=r("td"),zD=i("use_gpu"),JD=f(),ia=r("td"),WD=i("(Default: "),Wm=r("code"),YD=i("false"),KD=i("). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),VD=f(),ul=r("tr"),qf=r("td"),XD=i("use_cache"),QD=f(),ua=r("td"),ZD=i("(Default: "),Ym=r("code"),eO=i("true"),tO=i("). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),sO=f(),cl=r("tr"),_f=r("td"),aO=i("wait_for_model"),nO=f(),ca=r("td"),rO=i("(Default: "),Km=r("code"),oO=i("false"),lO=i(") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),v1=f(),vf=r("p"),iO=i("Return value is either a dict or a list of dicts if you sent a list of inputs"),E1=f(),pa=r("table"),Vm=r("thead"),pl=r("tr"),Ef=r("th"),uO=i("Returned values"),cO=f(),Xm=r("th"),pO=f(),Qm=r("tbody"),fl=r("tr"),wf=r("td"),Zm=r("strong"),fO=i("A list of float (or list of list of floats)"),hO=f(),yf=r("td"),dO=i("The numbers that are the representation features of the input."),w1=f(),bf=r("small"),gO=i(`Returned values are a list of floats, or a list of list of floats
(depending on if you sent a string or a list of string, and if the
automatic reduction, usually mean_pooling for instance was applied for
you or not. This should be explained on the model's README.`),y1=f(),Ke=r("h2"),fa=r("a"),e$=r("span"),_(hl.$$.fragment),mO=f(),t$=r("span"),$O=i("Audio-classification task"),b1=f(),jf=r("p"),qO=i("This task reads some audio input and outputs the likelihood of classes."),j1=f(),_(ha.$$.fragment),T1=f(),Ve=r("p"),_O=i("Available with: "),dl=r("a"),vO=i("\u{1F917} Transformers"),EO=f(),gl=r("a"),wO=i("SpeechBrain"),k1=f(),Tf=r("p"),yO=i("Request:"),A1=f(),_(da.$$.fragment),D1=f(),kf=r("p"),bO=i(`When sending your request, you should send a binary payload that simply
contains your audio file. We try to support most formats (Flac, Wav,
Mp3, Ogg etc...). And we automatically rescale the sampling rate to the
appropriate rate for the given model (usually 16KHz).`),O1=f(),ga=r("table"),s$=r("thead"),ml=r("tr"),Af=r("th"),jO=i("All parameters"),TO=f(),a$=r("th"),kO=f(),n$=r("tbody"),$l=r("tr"),ql=r("td"),r$=r("strong"),AO=i("no parameter"),DO=i(" (required)"),OO=f(),Df=r("td"),xO=i("a binary representation of the audio file. No other parameters are currently allowed."),x1=f(),Of=r("p"),RO=i("Return value is a dict"),R1=f(),_(ma.$$.fragment),N1=f(),$a=r("table"),o$=r("thead"),_l=r("tr"),xf=r("th"),NO=i("Returned values"),PO=f(),l$=r("th"),SO=f(),vl=r("tbody"),El=r("tr"),Rf=r("td"),i$=r("strong"),IO=i("label"),HO=f(),Nf=r("td"),BO=i("The label for the class (model specific)"),GO=f(),wl=r("tr"),Pf=r("td"),u$=r("strong"),CO=i("score"),MO=f(),Sf=r("td"),FO=i("A floats that represents how likely is that the audio file belongs the this class."),P1=f(),Xe=r("h2"),qa=r("a"),c$=r("span"),_(yl.$$.fragment),UO=f(),p$=r("span"),LO=i("Object-detection task"),S1=f(),If=r("p"),zO=i(`This task reads some image input and outputs the likelihood of classes &
bounding boxes of detected objects.`),I1=f(),_(_a.$$.fragment),H1=f(),bl=r("p"),JO=i("Available with: "),jl=r("a"),WO=i("\u{1F917} Transformers"),B1=f(),Hf=r("p"),YO=i("Request:"),G1=f(),_(va.$$.fragment),C1=f(),Ea=r("p"),KO=i(`When sending your request, you should send a binary payload that simply
contains your image file. We support all image formats `),Tl=r("a"),VO=i(`Pillow
supports`),XO=i("."),M1=f(),wa=r("table"),f$=r("thead"),kl=r("tr"),Bf=r("th"),QO=i("All parameters"),ZO=f(),h$=r("th"),ex=f(),d$=r("tbody"),Al=r("tr"),Dl=r("td"),g$=r("strong"),tx=i("no parameter"),sx=i(" (required)"),ax=f(),Gf=r("td"),nx=i("a binary representation of the image file. No other parameters are currently allowed."),F1=f(),Cf=r("p"),rx=i("Return value is a dict"),U1=f(),_(ya.$$.fragment),L1=f(),ba=r("table"),m$=r("thead"),Ol=r("tr"),Mf=r("th"),ox=i("Returned values"),lx=f(),$$=r("th"),ix=f(),Qe=r("tbody"),xl=r("tr"),Ff=r("td"),q$=r("strong"),ux=i("label"),cx=f(),Uf=r("td"),px=i("The label for the class (model specific) of a detected object."),fx=f(),Rl=r("tr"),Lf=r("td"),_$=r("strong"),hx=i("score"),dx=f(),zf=r("td"),gx=i("A float that represents how likely it is that the detected object belongs to the given class."),mx=f(),Nl=r("tr"),Jf=r("td"),v$=r("strong"),$x=i("box"),qx=f(),Wf=r("td"),_x=i("A dict (with keys [xmin,ymin,xmax,ymax]) representing the bounding box of a detected object."),this.h()},l(a){const g=VC('[data-svelte="svelte-1phssyn"]',document.head);n=o(g,"META",{name:!0,content:!0}),g.forEach(t),c=h(a),s=o(a,"H1",{class:!0});var Pl=l(s);d=o(Pl,"A",{id:!0,class:!0,href:!0});var E$=l(d);$=o(E$,"SPAN",{});var w$=l($);v(k.$$.fragment,w$),w$.forEach(t),E$.forEach(t),A=h(Pl),T=o(Pl,"SPAN",{});var y$=l(T);j=u(y$,"Detailed parameters"),y$.forEach(t),Pl.forEach(t),O=h(a),D=o(a,"H2",{class:!0});var Sl=l(D);se=o(Sl,"A",{id:!0,class:!0,href:!0});var b$=l(se);De=o(b$,"SPAN",{});var j$=l(De);v(V.$$.fragment,j$),j$.forEach(t),b$.forEach(t),J=h(Sl),et=o(Sl,"SPAN",{});var T$=l(et);Ll=u(T$,"Which task is used by this model ?"),T$.forEach(t),Sl.forEach(t),Ra=h(a),Oe=o(a,"P",{});var k$=l(Oe);gy=u(k$,`In general the \u{1F917} Hosted API Inference accepts a simple string as an
input. However, more advanced usage depends on the \u201Ctask\u201D that the
model solves.`),k$.forEach(t),K$=h(a),zl=o(a,"P",{});var A$=l(zl);my=u(A$,"The \u201Ctask\u201D of a model is defined here on it\u2019s model page:"),A$.forEach(t),V$=h(a),tt=o(a,"IMG",{class:!0,src:!0,width:!0}),X$=h(a),st=o(a,"IMG",{class:!0,src:!0,width:!0}),Q$=h(a),xe=o(a,"H2",{class:!0});var Il=l(xe);at=o(Il,"A",{id:!0,class:!0,href:!0});var D$=l(at);ph=o(D$,"SPAN",{});var O$=l(ph);v(Na.$$.fragment,O$),O$.forEach(t),D$.forEach(t),$y=h(Il),fh=o(Il,"SPAN",{});var x$=l(fh);qy=u(x$,"Zero-shot classification task"),x$.forEach(t),Il.forEach(t),Z$=h(a),Jl=o(a,"P",{});var R$=l(Jl);_y=u(R$,`This task is a super useful to try it out classification with zero code,
you simply pass a sentence/paragraph and the possible labels for that
sentence and you get a result.`),R$.forEach(t),eq=h(a),v(nt.$$.fragment,a),tq=h(a),Pa=o(a,"P",{});var Yf=l(Pa);vy=u(Yf,"Available with: "),Sa=o(Yf,"A",{href:!0,rel:!0});var N$=l(Sa);Ey=u(N$,"\u{1F917} Transformers"),N$.forEach(t),Yf.forEach(t),sq=h(a),Wl=o(a,"P",{});var P$=l(Wl);wy=u(P$,"Request:"),P$.forEach(t),aq=h(a),v(rt.$$.fragment,a),nq=h(a),Yl=o(a,"P",{});var S$=l(Yl);yy=u(S$,`When sending your request, you should send a JSON encoded payload. Here
are all the options`),S$.forEach(t),rq=h(a),ot=o(a,"TABLE",{});var Hl=l(ot);hh=o(Hl,"THEAD",{});var I$=l(hh);Ia=o(I$,"TR",{});var Bl=l(Ia);Kl=o(Bl,"TH",{align:!0});var H$=l(Kl);by=u(H$,"All parameters"),H$.forEach(t),jy=h(Bl),dh=o(Bl,"TH",{align:!0}),l(dh).forEach(t),Bl.forEach(t),I$.forEach(t),Ty=h(Hl),M=o(Hl,"TBODY",{});var F=l(M);Ha=o(F,"TR",{});var Gl=l(Ha);Ba=o(Gl,"TD",{align:!0});var Kf=l(Ba);gh=o(Kf,"STRONG",{});var B$=l(gh);ky=u(B$,"inputs"),B$.forEach(t),Ay=u(Kf," (required)"),Kf.forEach(t),Dy=h(Gl),Vl=o(Gl,"TD",{align:!0});var G$=l(Vl);Oy=u(G$,"a string or list of strings"),G$.forEach(t),Gl.forEach(t),xy=h(F),Ga=o(F,"TR",{});var Cl=l(Ga);Ca=o(Cl,"TD",{align:!0});var Vf=l(Ca);mh=o(Vf,"STRONG",{});var C$=l(mh);Ry=u(C$,"parameters"),C$.forEach(t),Ny=u(Vf," (required)"),Vf.forEach(t),Py=h(Cl),Xl=o(Cl,"TD",{align:!0});var M$=l(Xl);Sy=u(M$,"a dict containing the following keys:"),M$.forEach(t),Cl.forEach(t),Iy=h(F),Ma=o(F,"TR",{});var Ml=l(Ma);Ql=o(Ml,"TD",{align:!0});var F$=l(Ql);Hy=u(F$,"candidate_labels (required)"),F$.forEach(t),By=h(Ml),pe=o(Ml,"TD",{align:!0});var Ze=l(pe);Gy=u(Ze,"a list of strings that are potential classes for "),$h=o(Ze,"CODE",{});var U$=l($h);Cy=u(U$,"inputs"),U$.forEach(t),My=u(Ze,". (max 10 candidate_labels, for more, simply run multiple requests, results are going to be misleading if using too many candidate_labels anyway. If you want to keep the exact same, you can simply run "),Zl=o(Ze,"SPAN",{class:!0});var L$=l(Zl);Fy=u(L$,"multi_label=True"),L$.forEach(t),Uy=u(Ze," and do the scaling on your end. )"),Ze.forEach(t),Ml.forEach(t),Ly=h(F),Fa=o(F,"TR",{});var Fl=l(Fa);ei=o(Fl,"TD",{align:!0});var Ux=l(ei);zy=u(Ux,"multi_label"),Ux.forEach(t),Jy=h(Fl),lt=o(Fl,"TD",{align:!0});var J1=l(lt);Wy=u(J1,"(Default: "),qh=o(J1,"CODE",{});var Lx=l(qh);Yy=u(Lx,"false"),Lx.forEach(t),Ky=u(J1,") Boolean that is set to True if classes can overlap"),J1.forEach(t),Fl.forEach(t),Vy=h(F),Ua=o(F,"TR",{});var W1=l(Ua);ti=o(W1,"TD",{align:!0});var zx=l(ti);_h=o(zx,"STRONG",{});var Jx=l(_h);Xy=u(Jx,"options"),Jx.forEach(t),zx.forEach(t),Qy=h(W1),si=o(W1,"TD",{align:!0});var Wx=l(si);Zy=u(Wx,"a dict containing the following keys:"),Wx.forEach(t),W1.forEach(t),eb=h(F),La=o(F,"TR",{});var Y1=l(La);ai=o(Y1,"TD",{align:!0});var Yx=l(ai);tb=u(Yx,"use_gpu"),Yx.forEach(t),sb=h(Y1),it=o(Y1,"TD",{align:!0});var K1=l(it);ab=u(K1,"(Default: "),vh=o(K1,"CODE",{});var Kx=l(vh);nb=u(Kx,"false"),Kx.forEach(t),rb=u(K1,"). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),K1.forEach(t),Y1.forEach(t),ob=h(F),za=o(F,"TR",{});var V1=l(za);ni=o(V1,"TD",{align:!0});var Vx=l(ni);lb=u(Vx,"use_cache"),Vx.forEach(t),ib=h(V1),ut=o(V1,"TD",{align:!0});var X1=l(ut);ub=u(X1,"(Default: "),Eh=o(X1,"CODE",{});var Xx=l(Eh);cb=u(Xx,"true"),Xx.forEach(t),pb=u(X1,"). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),X1.forEach(t),V1.forEach(t),fb=h(F),Ja=o(F,"TR",{});var Q1=l(Ja);ri=o(Q1,"TD",{align:!0});var Qx=l(ri);hb=u(Qx,"wait_for_model"),Qx.forEach(t),db=h(Q1),ct=o(Q1,"TD",{align:!0});var Z1=l(ct);gb=u(Z1,"(Default: "),wh=o(Z1,"CODE",{});var Zx=l(wh);mb=u(Zx,"false"),Zx.forEach(t),$b=u(Z1,") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),Z1.forEach(t),Q1.forEach(t),F.forEach(t),Hl.forEach(t),oq=h(a),oi=o(a,"P",{});var eR=l(oi);qb=u(eR,"Return value is either a dict or a list of dicts if you sent a list of inputs"),eR.forEach(t),lq=h(a),li=o(a,"P",{});var tR=l(li);_b=u(tR,"Response:"),tR.forEach(t),iq=h(a),v(pt.$$.fragment,a),uq=h(a),ft=o(a,"TABLE",{});var e2=l(ft);yh=o(e2,"THEAD",{});var sR=l(yh);Wa=o(sR,"TR",{});var t2=l(Wa);ii=o(t2,"TH",{align:!0});var aR=l(ii);vb=u(aR,"Returned values"),aR.forEach(t),Eb=h(t2),bh=o(t2,"TH",{align:!0}),l(bh).forEach(t),t2.forEach(t),sR.forEach(t),wb=h(e2),Re=o(e2,"TBODY",{});var Xf=l(Re);Ya=o(Xf,"TR",{});var s2=l(Ya);ui=o(s2,"TD",{align:!0});var nR=l(ui);jh=o(nR,"STRONG",{});var rR=l(jh);yb=u(rR,"sequence"),rR.forEach(t),nR.forEach(t),bb=h(s2),ci=o(s2,"TD",{align:!0});var oR=l(ci);jb=u(oR,"The string sent as an input"),oR.forEach(t),s2.forEach(t),Tb=h(Xf),Ka=o(Xf,"TR",{});var a2=l(Ka);pi=o(a2,"TD",{align:!0});var lR=l(pi);Th=o(lR,"STRONG",{});var iR=l(Th);kb=u(iR,"labels"),iR.forEach(t),lR.forEach(t),Ab=h(a2),fi=o(a2,"TD",{align:!0});var uR=l(fi);Db=u(uR,"The list of strings for labels that you sent (in order)"),uR.forEach(t),a2.forEach(t),Ob=h(Xf),Va=o(Xf,"TR",{});var n2=l(Va);hi=o(n2,"TD",{align:!0});var cR=l(hi);kh=o(cR,"STRONG",{});var pR=l(kh);xb=u(pR,"scores"),pR.forEach(t),cR.forEach(t),Rb=h(n2),ht=o(n2,"TD",{align:!0});var r2=l(ht);Nb=u(r2,"a list of floats that correspond the the probability of label, in the same order as "),Ah=o(r2,"CODE",{});var fR=l(Ah);Pb=u(fR,"labels"),fR.forEach(t),Sb=u(r2,"."),r2.forEach(t),n2.forEach(t),Xf.forEach(t),e2.forEach(t),cq=h(a),Ne=o(a,"H2",{class:!0});var o2=l(Ne);dt=o(o2,"A",{id:!0,class:!0,href:!0});var hR=l(dt);Dh=o(hR,"SPAN",{});var dR=l(Dh);v(Xa.$$.fragment,dR),dR.forEach(t),hR.forEach(t),Ib=h(o2),Oh=o(o2,"SPAN",{});var gR=l(Oh);Hb=u(gR,"Translation task"),gR.forEach(t),o2.forEach(t),pq=h(a),di=o(a,"P",{});var mR=l(di);Bb=u(mR,"This task is well known to translate text from one language to another"),mR.forEach(t),fq=h(a),v(gt.$$.fragment,a),hq=h(a),Qa=o(a,"P",{});var vx=l(Qa);Gb=u(vx,"Available with: "),Za=o(vx,"A",{href:!0,rel:!0});var $R=l(Za);Cb=u($R,"\u{1F917} Transformers"),$R.forEach(t),vx.forEach(t),dq=h(a),gi=o(a,"P",{});var qR=l(gi);Mb=u(qR,"Example:"),qR.forEach(t),gq=h(a),v(mt.$$.fragment,a),mq=h(a),mi=o(a,"P",{});var _R=l(mi);Fb=u(_R,`When sending your request, you should send a JSON encoded payload. Here
are all the options`),_R.forEach(t),$q=h(a),$t=o(a,"TABLE",{});var l2=l($t);xh=o(l2,"THEAD",{});var vR=l(xh);en=o(vR,"TR",{});var i2=l(en);$i=o(i2,"TH",{align:!0});var ER=l($i);Ub=u(ER,"All parameters"),ER.forEach(t),Lb=h(i2),Rh=o(i2,"TH",{align:!0}),l(Rh).forEach(t),i2.forEach(t),vR.forEach(t),zb=h(l2),X=o(l2,"TBODY",{});var be=l(X);tn=o(be,"TR",{});var u2=l(tn);sn=o(u2,"TD",{align:!0});var Ex=l(sn);Nh=o(Ex,"STRONG",{});var wR=l(Nh);Jb=u(wR,"inputs"),wR.forEach(t),Wb=u(Ex," (required)"),Ex.forEach(t),Yb=h(u2),qi=o(u2,"TD",{align:!0});var yR=l(qi);Kb=u(yR,"a string to be translated in the original languages"),yR.forEach(t),u2.forEach(t),Vb=h(be),an=o(be,"TR",{});var c2=l(an);_i=o(c2,"TD",{align:!0});var bR=l(_i);Ph=o(bR,"STRONG",{});var jR=l(Ph);Xb=u(jR,"options"),jR.forEach(t),bR.forEach(t),Qb=h(c2),vi=o(c2,"TD",{align:!0});var TR=l(vi);Zb=u(TR,"a dict containing the following keys:"),TR.forEach(t),c2.forEach(t),e0=h(be),nn=o(be,"TR",{});var p2=l(nn);Ei=o(p2,"TD",{align:!0});var kR=l(Ei);t0=u(kR,"use_gpu"),kR.forEach(t),s0=h(p2),qt=o(p2,"TD",{align:!0});var f2=l(qt);a0=u(f2,"(Default: "),Sh=o(f2,"CODE",{});var AR=l(Sh);n0=u(AR,"false"),AR.forEach(t),r0=u(f2,"). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),f2.forEach(t),p2.forEach(t),o0=h(be),rn=o(be,"TR",{});var h2=l(rn);wi=o(h2,"TD",{align:!0});var DR=l(wi);l0=u(DR,"use_cache"),DR.forEach(t),i0=h(h2),_t=o(h2,"TD",{align:!0});var d2=l(_t);u0=u(d2,"(Default: "),Ih=o(d2,"CODE",{});var OR=l(Ih);c0=u(OR,"true"),OR.forEach(t),p0=u(d2,"). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),d2.forEach(t),h2.forEach(t),f0=h(be),on=o(be,"TR",{});var g2=l(on);yi=o(g2,"TD",{align:!0});var xR=l(yi);h0=u(xR,"wait_for_model"),xR.forEach(t),d0=h(g2),vt=o(g2,"TD",{align:!0});var m2=l(vt);g0=u(m2,"(Default: "),Hh=o(m2,"CODE",{});var RR=l(Hh);m0=u(RR,"false"),RR.forEach(t),$0=u(m2,") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),m2.forEach(t),g2.forEach(t),be.forEach(t),l2.forEach(t),qq=h(a),bi=o(a,"P",{});var NR=l(bi);q0=u(NR,"Return value is either a dict or a list of dicts if you sent a list of inputs"),NR.forEach(t),_q=h(a),Et=o(a,"TABLE",{});var $2=l(Et);Bh=o($2,"THEAD",{});var PR=l(Bh);ln=o(PR,"TR",{});var q2=l(ln);ji=o(q2,"TH",{align:!0});var SR=l(ji);_0=u(SR,"Returned values"),SR.forEach(t),v0=h(q2),Gh=o(q2,"TH",{align:!0}),l(Gh).forEach(t),q2.forEach(t),PR.forEach(t),E0=h($2),Ch=o($2,"TBODY",{});var IR=l(Ch);un=o(IR,"TR",{});var _2=l(un);Ti=o(_2,"TD",{align:!0});var HR=l(Ti);Mh=o(HR,"STRONG",{});var BR=l(Mh);w0=u(BR,"translation_text"),BR.forEach(t),HR.forEach(t),y0=h(_2),ki=o(_2,"TD",{align:!0});var GR=l(ki);b0=u(GR,"The string after translation"),GR.forEach(t),_2.forEach(t),IR.forEach(t),$2.forEach(t),vq=h(a),Pe=o(a,"H2",{class:!0});var v2=l(Pe);wt=o(v2,"A",{id:!0,class:!0,href:!0});var CR=l(wt);Fh=o(CR,"SPAN",{});var MR=l(Fh);v(cn.$$.fragment,MR),MR.forEach(t),CR.forEach(t),j0=h(v2),Uh=o(v2,"SPAN",{});var FR=l(Uh);T0=u(FR,"Summarization task"),FR.forEach(t),v2.forEach(t),Eq=h(a),yt=o(a,"P",{});var E2=l(yt);k0=u(E2,`This task is well known to summarize text a big text into a small text.
Be careful, some models have a maximum length of input. That means that
the summary cannot handle full books for instance. Be careful when
choosing your model. If you want to discuss you summarization needs,
please get in touch <`),Ai=o(E2,"A",{href:!0});var UR=l(Ai);A0=u(UR,"api-enterprise@huggingface.co"),UR.forEach(t),D0=u(E2,">"),E2.forEach(t),wq=h(a),v(bt.$$.fragment,a),yq=h(a),pn=o(a,"P",{});var wx=l(pn);O0=u(wx,"Available with: "),fn=o(wx,"A",{href:!0,rel:!0});var LR=l(fn);x0=u(LR,"\u{1F917} Transformers"),LR.forEach(t),wx.forEach(t),bq=h(a),Di=o(a,"P",{});var zR=l(Di);R0=u(zR,"Example:"),zR.forEach(t),jq=h(a),v(jt.$$.fragment,a),Tq=h(a),Oi=o(a,"P",{});var JR=l(Oi);N0=u(JR,`When sending your request, you should send a JSON encoded payload. Here
are all the options`),JR.forEach(t),kq=h(a),Tt=o(a,"TABLE",{});var w2=l(Tt);Lh=o(w2,"THEAD",{});var WR=l(Lh);hn=o(WR,"TR",{});var y2=l(hn);xi=o(y2,"TH",{align:!0});var YR=l(xi);P0=u(YR,"All parameters"),YR.forEach(t),S0=h(y2),zh=o(y2,"TH",{align:!0}),l(zh).forEach(t),y2.forEach(t),WR.forEach(t),I0=h(w2),B=o(w2,"TBODY",{});var G=l(B);dn=o(G,"TR",{});var b2=l(dn);gn=o(b2,"TD",{align:!0});var yx=l(gn);Jh=o(yx,"STRONG",{});var KR=l(Jh);H0=u(KR,"inputs"),KR.forEach(t),B0=u(yx," (required)"),yx.forEach(t),G0=h(b2),Ri=o(b2,"TD",{align:!0});var VR=l(Ri);C0=u(VR,"a string to be summarized"),VR.forEach(t),b2.forEach(t),M0=h(G),mn=o(G,"TR",{});var j2=l(mn);Ni=o(j2,"TD",{align:!0});var XR=l(Ni);Wh=o(XR,"STRONG",{});var QR=l(Wh);F0=u(QR,"parameters"),QR.forEach(t),XR.forEach(t),U0=h(j2),Pi=o(j2,"TD",{align:!0});var ZR=l(Pi);L0=u(ZR,"a dict containing the following keys:"),ZR.forEach(t),j2.forEach(t),z0=h(G),$n=o(G,"TR",{});var T2=l($n);Si=o(T2,"TD",{align:!0});var eN=l(Si);J0=u(eN,"min_length"),eN.forEach(t),W0=h(T2),fe=o(T2,"TD",{align:!0});var Qf=l(fe);Y0=u(Qf,"(Default: "),Yh=o(Qf,"CODE",{});var tN=l(Yh);K0=u(tN,"None"),tN.forEach(t),V0=u(Qf,"). Integer to define the minimum length "),Kh=o(Qf,"STRONG",{});var sN=l(Kh);X0=u(sN,"in tokens"),sN.forEach(t),Q0=u(Qf," of the output summary."),Qf.forEach(t),T2.forEach(t),Z0=h(G),qn=o(G,"TR",{});var k2=l(qn);Ii=o(k2,"TD",{align:!0});var aN=l(Ii);ej=u(aN,"max_length"),aN.forEach(t),tj=h(k2),he=o(k2,"TD",{align:!0});var Zf=l(he);sj=u(Zf,"(Default: "),Vh=o(Zf,"CODE",{});var nN=l(Vh);aj=u(nN,"None"),nN.forEach(t),nj=u(Zf,"). Integer to define the maximum length "),Xh=o(Zf,"STRONG",{});var rN=l(Xh);rj=u(rN,"in tokens"),rN.forEach(t),oj=u(Zf," of the output summary."),Zf.forEach(t),k2.forEach(t),lj=h(G),_n=o(G,"TR",{});var A2=l(_n);Hi=o(A2,"TD",{align:!0});var oN=l(Hi);ij=u(oN,"top_k"),oN.forEach(t),uj=h(A2),de=o(A2,"TD",{align:!0});var eh=l(de);cj=u(eh,"(Default: "),Qh=o(eh,"CODE",{});var lN=l(Qh);pj=u(lN,"None"),lN.forEach(t),fj=u(eh,"). Integer to define the top tokens considered within the "),Bi=o(eh,"SPAN",{class:!0});var iN=l(Bi);hj=u(iN,"sample"),iN.forEach(t),dj=u(eh," operation to create new text."),eh.forEach(t),A2.forEach(t),gj=h(G),vn=o(G,"TR",{});var D2=l(vn);Gi=o(D2,"TD",{align:!0});var uN=l(Gi);mj=u(uN,"top_p"),uN.forEach(t),$j=h(D2),ge=o(D2,"TD",{align:!0});var th=l(ge);qj=u(th,"(Default: "),Zh=o(th,"CODE",{});var cN=l(Zh);_j=u(cN,"None"),cN.forEach(t),vj=u(th,"). Float to define the tokens that are within the sample` operation of text generation. Add tokens in the sample for more probable to least probable until the sum of the probabilities is greater than "),Ci=o(th,"SPAN",{class:!0});var pN=l(Ci);Ej=u(pN,"top_p"),pN.forEach(t),wj=u(th,"."),th.forEach(t),D2.forEach(t),yj=h(G),En=o(G,"TR",{});var O2=l(En);Mi=o(O2,"TD",{align:!0});var fN=l(Mi);bj=u(fN,"temperature"),fN.forEach(t),jj=h(O2),ae=o(O2,"TD",{align:!0});var ja=l(ae);Tj=u(ja,"(Default: "),ed=o(ja,"CODE",{});var hN=l(ed);kj=u(hN,"1.0"),hN.forEach(t),Aj=u(ja,"). Float (0.0-100.0). The temperature of the sampling operation. 1 means regular sampling, 0 means "),Fi=o(ja,"SPAN",{class:!0});var dN=l(Fi);Dj=u(dN,"top_k=1"),dN.forEach(t),Oj=u(ja,", "),Ui=o(ja,"SPAN",{class:!0});var gN=l(Ui);xj=u(gN,"100.0"),gN.forEach(t),Rj=u(ja," is getting closer to uniform probability."),ja.forEach(t),O2.forEach(t),Nj=h(G),wn=o(G,"TR",{});var x2=l(wn);Li=o(x2,"TD",{align:!0});var mN=l(Li);Pj=u(mN,"repetition_penalty"),mN.forEach(t),Sj=h(x2),kt=o(x2,"TD",{align:!0});var R2=l(kt);Ij=u(R2,"(Default: "),td=o(R2,"CODE",{});var $N=l(td);Hj=u($N,"None"),$N.forEach(t),Bj=u(R2,"). Float (0.0-100.0). The more a token is used within generation the more it is penalized to not be picked in successive generation passes."),R2.forEach(t),x2.forEach(t),Gj=h(G),yn=o(G,"TR",{});var N2=l(yn);zi=o(N2,"TD",{align:!0});var qN=l(zi);Cj=u(qN,"max_time"),qN.forEach(t),Mj=h(N2),At=o(N2,"TD",{align:!0});var P2=l(At);Fj=u(P2,"(Default: "),sd=o(P2,"CODE",{});var _N=l(sd);Uj=u(_N,"None"),_N.forEach(t),Lj=u(P2,"). Float (0-120.0). The amount of time in seconds that the query should take maximum. Network can cause some overhead so it will be a soft limit."),P2.forEach(t),N2.forEach(t),zj=h(G),bn=o(G,"TR",{});var S2=l(bn);Ji=o(S2,"TD",{align:!0});var vN=l(Ji);ad=o(vN,"STRONG",{});var EN=l(ad);Jj=u(EN,"options"),EN.forEach(t),vN.forEach(t),Wj=h(S2),Wi=o(S2,"TD",{align:!0});var wN=l(Wi);Yj=u(wN,"a dict containing the following keys:"),wN.forEach(t),S2.forEach(t),Kj=h(G),jn=o(G,"TR",{});var I2=l(jn);Yi=o(I2,"TD",{align:!0});var yN=l(Yi);Vj=u(yN,"use_gpu"),yN.forEach(t),Xj=h(I2),Dt=o(I2,"TD",{align:!0});var H2=l(Dt);Qj=u(H2,"(Default: "),nd=o(H2,"CODE",{});var bN=l(nd);Zj=u(bN,"false"),bN.forEach(t),e3=u(H2,"). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),H2.forEach(t),I2.forEach(t),t3=h(G),Tn=o(G,"TR",{});var B2=l(Tn);Ki=o(B2,"TD",{align:!0});var jN=l(Ki);s3=u(jN,"use_cache"),jN.forEach(t),a3=h(B2),Ot=o(B2,"TD",{align:!0});var G2=l(Ot);n3=u(G2,"(Default: "),rd=o(G2,"CODE",{});var TN=l(rd);r3=u(TN,"true"),TN.forEach(t),o3=u(G2,"). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),G2.forEach(t),B2.forEach(t),l3=h(G),kn=o(G,"TR",{});var C2=l(kn);Vi=o(C2,"TD",{align:!0});var kN=l(Vi);i3=u(kN,"wait_for_model"),kN.forEach(t),u3=h(C2),xt=o(C2,"TD",{align:!0});var M2=l(xt);c3=u(M2,"(Default: "),od=o(M2,"CODE",{});var AN=l(od);p3=u(AN,"false"),AN.forEach(t),f3=u(M2,") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),M2.forEach(t),C2.forEach(t),G.forEach(t),w2.forEach(t),Aq=h(a),Xi=o(a,"P",{});var DN=l(Xi);h3=u(DN,"Return value is either a dict or a list of dicts if you sent a list of inputs"),DN.forEach(t),Dq=h(a),Rt=o(a,"TABLE",{});var F2=l(Rt);ld=o(F2,"THEAD",{});var ON=l(ld);An=o(ON,"TR",{});var U2=l(An);Qi=o(U2,"TH",{align:!0});var xN=l(Qi);d3=u(xN,"Returned values"),xN.forEach(t),g3=h(U2),id=o(U2,"TH",{align:!0}),l(id).forEach(t),U2.forEach(t),ON.forEach(t),m3=h(F2),ud=o(F2,"TBODY",{});var RN=l(ud);Dn=o(RN,"TR",{});var L2=l(Dn);Zi=o(L2,"TD",{align:!0});var NN=l(Zi);cd=o(NN,"STRONG",{});var PN=l(cd);$3=u(PN,"summarization_text"),PN.forEach(t),NN.forEach(t),q3=h(L2),eu=o(L2,"TD",{align:!0});var SN=l(eu);_3=u(SN,"The string after translation"),SN.forEach(t),L2.forEach(t),RN.forEach(t),F2.forEach(t),Oq=h(a),Se=o(a,"H2",{class:!0});var z2=l(Se);Nt=o(z2,"A",{id:!0,class:!0,href:!0});var IN=l(Nt);pd=o(IN,"SPAN",{});var HN=l(pd);v(On.$$.fragment,HN),HN.forEach(t),IN.forEach(t),v3=h(z2),fd=o(z2,"SPAN",{});var BN=l(fd);E3=u(BN,"Conversational task"),BN.forEach(t),z2.forEach(t),xq=h(a),tu=o(a,"P",{});var GN=l(tu);w3=u(GN,`This task corresponds to any chatbot like structure. Models tend to have
shorted max_length, so please check with caution when using a given
model if you need long range dependency or not.`),GN.forEach(t),Rq=h(a),v(Pt.$$.fragment,a),Nq=h(a),xn=o(a,"P",{});var bx=l(xn);y3=u(bx,"Available with: "),Rn=o(bx,"A",{href:!0,rel:!0});var CN=l(Rn);b3=u(CN,"\u{1F917} Transformers"),CN.forEach(t),bx.forEach(t),Pq=h(a),su=o(a,"P",{});var MN=l(su);j3=u(MN,"Example:"),MN.forEach(t),Sq=h(a),v(St.$$.fragment,a),Iq=h(a),au=o(a,"P",{});var FN=l(au);T3=u(FN,`When sending your request, you should send a JSON encoded payload. Here
are all the options`),FN.forEach(t),Hq=h(a),It=o(a,"TABLE",{});var J2=l(It);hd=o(J2,"THEAD",{});var UN=l(hd);Nn=o(UN,"TR",{});var W2=l(Nn);nu=o(W2,"TH",{align:!0});var LN=l(nu);k3=u(LN,"All parameters"),LN.forEach(t),A3=h(W2),dd=o(W2,"TH",{align:!0}),l(dd).forEach(t),W2.forEach(t),UN.forEach(t),D3=h(J2),P=o(J2,"TBODY",{});var I=l(P);Pn=o(I,"TR",{});var Y2=l(Pn);Sn=o(Y2,"TD",{align:!0});var jx=l(Sn);gd=o(jx,"STRONG",{});var zN=l(gd);O3=u(zN,"inputs"),zN.forEach(t),x3=u(jx," (required)"),jx.forEach(t),R3=h(Y2),md=o(Y2,"TD",{align:!0}),l(md).forEach(t),Y2.forEach(t),N3=h(I),In=o(I,"TR",{});var K2=l(In);ru=o(K2,"TD",{align:!0});var JN=l(ru);P3=u(JN,"text (required)"),JN.forEach(t),S3=h(K2),ou=o(K2,"TD",{align:!0});var WN=l(ou);I3=u(WN,"The last input from the user in the conversation."),WN.forEach(t),K2.forEach(t),H3=h(I),Hn=o(I,"TR",{});var V2=l(Hn);lu=o(V2,"TD",{align:!0});var YN=l(lu);B3=u(YN,"generated_responses"),YN.forEach(t),G3=h(V2),iu=o(V2,"TD",{align:!0});var KN=l(iu);C3=u(KN,"A list of strings corresponding to the earlier replies from the model."),KN.forEach(t),V2.forEach(t),M3=h(I),Bn=o(I,"TR",{});var X2=l(Bn);uu=o(X2,"TD",{align:!0});var VN=l(uu);F3=u(VN,"past_user_inputs"),VN.forEach(t),U3=h(X2),Ht=o(X2,"TD",{align:!0});var Q2=l(Ht);L3=u(Q2,"A list of strings corresponding to the earlier replies from the user. Should be of the same length of "),$d=o(Q2,"CODE",{});var XN=l($d);z3=u(XN,"generated_responses"),XN.forEach(t),J3=u(Q2,"."),Q2.forEach(t),X2.forEach(t),W3=h(I),Gn=o(I,"TR",{});var Z2=l(Gn);cu=o(Z2,"TD",{align:!0});var QN=l(cu);qd=o(QN,"STRONG",{});var ZN=l(qd);Y3=u(ZN,"parameters"),ZN.forEach(t),QN.forEach(t),K3=h(Z2),pu=o(Z2,"TD",{align:!0});var eP=l(pu);V3=u(eP,"a dict containing the following keys:"),eP.forEach(t),Z2.forEach(t),X3=h(I),Cn=o(I,"TR",{});var ev=l(Cn);fu=o(ev,"TD",{align:!0});var tP=l(fu);Q3=u(tP,"min_length"),tP.forEach(t),Z3=h(ev),me=o(ev,"TD",{align:!0});var sh=l(me);eT=u(sh,"(Default: "),_d=o(sh,"CODE",{});var sP=l(_d);tT=u(sP,"None"),sP.forEach(t),sT=u(sh,"). Integer to define the minimum length "),vd=o(sh,"STRONG",{});var aP=l(vd);aT=u(aP,"in tokens"),aP.forEach(t),nT=u(sh," of the output summary."),sh.forEach(t),ev.forEach(t),rT=h(I),Mn=o(I,"TR",{});var tv=l(Mn);hu=o(tv,"TD",{align:!0});var nP=l(hu);oT=u(nP,"max_length"),nP.forEach(t),lT=h(tv),$e=o(tv,"TD",{align:!0});var ah=l($e);iT=u(ah,"(Default: "),Ed=o(ah,"CODE",{});var rP=l(Ed);uT=u(rP,"None"),rP.forEach(t),cT=u(ah,"). Integer to define the maximum length "),wd=o(ah,"STRONG",{});var oP=l(wd);pT=u(oP,"in tokens"),oP.forEach(t),fT=u(ah," of the output summary."),ah.forEach(t),tv.forEach(t),hT=h(I),Fn=o(I,"TR",{});var sv=l(Fn);du=o(sv,"TD",{align:!0});var lP=l(du);dT=u(lP,"top_k"),lP.forEach(t),gT=h(sv),qe=o(sv,"TD",{align:!0});var nh=l(qe);mT=u(nh,"(Default: "),yd=o(nh,"CODE",{});var iP=l(yd);$T=u(iP,"None"),iP.forEach(t),qT=u(nh,"). Integer to define the top tokens considered within the "),gu=o(nh,"SPAN",{class:!0});var uP=l(gu);_T=u(uP,"sample"),uP.forEach(t),vT=u(nh," operation to create new text."),nh.forEach(t),sv.forEach(t),ET=h(I),Un=o(I,"TR",{});var av=l(Un);mu=o(av,"TD",{align:!0});var cP=l(mu);wT=u(cP,"top_p"),cP.forEach(t),yT=h(av),_e=o(av,"TD",{align:!0});var rh=l(_e);bT=u(rh,"(Default: "),bd=o(rh,"CODE",{});var pP=l(bd);jT=u(pP,"None"),pP.forEach(t),TT=u(rh,"). Float to define the tokens that are within the sample` operation of text generation. Add tokens in the sample for more probable to least probable until the sum of the probabilities is greater than "),$u=o(rh,"SPAN",{class:!0});var fP=l($u);kT=u(fP,"top_p"),fP.forEach(t),AT=u(rh,"."),rh.forEach(t),av.forEach(t),DT=h(I),Ln=o(I,"TR",{});var nv=l(Ln);qu=o(nv,"TD",{align:!0});var hP=l(qu);OT=u(hP,"temperature"),hP.forEach(t),xT=h(nv),ne=o(nv,"TD",{align:!0});var Ta=l(ne);RT=u(Ta,"(Default: "),jd=o(Ta,"CODE",{});var dP=l(jd);NT=u(dP,"1.0"),dP.forEach(t),PT=u(Ta,"). Float (0.0-100.0). The temperature of the sampling operation. 1 means regular sampling, 0 means "),_u=o(Ta,"SPAN",{class:!0});var gP=l(_u);ST=u(gP,"top_k=1"),gP.forEach(t),IT=u(Ta,", "),vu=o(Ta,"SPAN",{class:!0});var mP=l(vu);HT=u(mP,"100.0"),mP.forEach(t),BT=u(Ta," is getting closer to uniform probability."),Ta.forEach(t),nv.forEach(t),GT=h(I),zn=o(I,"TR",{});var rv=l(zn);Eu=o(rv,"TD",{align:!0});var $P=l(Eu);CT=u($P,"repetition_penalty"),$P.forEach(t),MT=h(rv),Bt=o(rv,"TD",{align:!0});var ov=l(Bt);FT=u(ov,"(Default: "),Td=o(ov,"CODE",{});var qP=l(Td);UT=u(qP,"None"),qP.forEach(t),LT=u(ov,"). Float (0.0-100.0). The more a token is used within generation the more it is penalized to not be picked in successive generation passes."),ov.forEach(t),rv.forEach(t),zT=h(I),Jn=o(I,"TR",{});var lv=l(Jn);wu=o(lv,"TD",{align:!0});var _P=l(wu);JT=u(_P,"max_time"),_P.forEach(t),WT=h(lv),Gt=o(lv,"TD",{align:!0});var iv=l(Gt);YT=u(iv,"(Default: "),kd=o(iv,"CODE",{});var vP=l(kd);KT=u(vP,"None"),vP.forEach(t),VT=u(iv,"). Float (0-120.0). The amount of time in seconds that the query should take maximum. Network can cause some overhead so it will be a soft limit."),iv.forEach(t),lv.forEach(t),XT=h(I),Wn=o(I,"TR",{});var uv=l(Wn);yu=o(uv,"TD",{align:!0});var EP=l(yu);Ad=o(EP,"STRONG",{});var wP=l(Ad);QT=u(wP,"options"),wP.forEach(t),EP.forEach(t),ZT=h(uv),bu=o(uv,"TD",{align:!0});var yP=l(bu);e5=u(yP,"a dict containing the following keys:"),yP.forEach(t),uv.forEach(t),t5=h(I),Yn=o(I,"TR",{});var cv=l(Yn);ju=o(cv,"TD",{align:!0});var bP=l(ju);s5=u(bP,"use_gpu"),bP.forEach(t),a5=h(cv),Ct=o(cv,"TD",{align:!0});var pv=l(Ct);n5=u(pv,"(Default: "),Dd=o(pv,"CODE",{});var jP=l(Dd);r5=u(jP,"false"),jP.forEach(t),o5=u(pv,"). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),pv.forEach(t),cv.forEach(t),l5=h(I),Kn=o(I,"TR",{});var fv=l(Kn);Tu=o(fv,"TD",{align:!0});var TP=l(Tu);i5=u(TP,"use_cache"),TP.forEach(t),u5=h(fv),Mt=o(fv,"TD",{align:!0});var hv=l(Mt);c5=u(hv,"(Default: "),Od=o(hv,"CODE",{});var kP=l(Od);p5=u(kP,"true"),kP.forEach(t),f5=u(hv,"). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),hv.forEach(t),fv.forEach(t),h5=h(I),Vn=o(I,"TR",{});var dv=l(Vn);ku=o(dv,"TD",{align:!0});var AP=l(ku);d5=u(AP,"wait_for_model"),AP.forEach(t),g5=h(dv),Ft=o(dv,"TD",{align:!0});var gv=l(Ft);m5=u(gv,"(Default: "),xd=o(gv,"CODE",{});var DP=l(xd);$5=u(DP,"false"),DP.forEach(t),q5=u(gv,") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),gv.forEach(t),dv.forEach(t),I.forEach(t),J2.forEach(t),Bq=h(a),Au=o(a,"P",{});var OP=l(Au);_5=u(OP,"Return value is either a dict or a list of dicts if you sent a list of inputs"),OP.forEach(t),Gq=h(a),Ut=o(a,"TABLE",{});var mv=l(Ut);Rd=o(mv,"THEAD",{});var xP=l(Rd);Xn=o(xP,"TR",{});var $v=l(Xn);Du=o($v,"TH",{align:!0});var RP=l(Du);v5=u(RP,"Returned values"),RP.forEach(t),E5=h($v),Nd=o($v,"TH",{align:!0}),l(Nd).forEach(t),$v.forEach(t),xP.forEach(t),w5=h(mv),oe=o(mv,"TBODY",{});var ka=l(oe);Qn=o(ka,"TR",{});var qv=l(Qn);Ou=o(qv,"TD",{align:!0});var NP=l(Ou);Pd=o(NP,"STRONG",{});var PP=l(Pd);y5=u(PP,"generated_text"),PP.forEach(t),NP.forEach(t),b5=h(qv),xu=o(qv,"TD",{align:!0});var SP=l(xu);j5=u(SP,"The answer of the bot"),SP.forEach(t),qv.forEach(t),T5=h(ka),Zn=o(ka,"TR",{});var _v=l(Zn);Ru=o(_v,"TD",{align:!0});var IP=l(Ru);Sd=o(IP,"STRONG",{});var HP=l(Sd);k5=u(HP,"conversation"),HP.forEach(t),IP.forEach(t),A5=h(_v),Nu=o(_v,"TD",{align:!0});var BP=l(Nu);D5=u(BP,"A facility dictionnary to send back for the next input (with the new user input addition)."),BP.forEach(t),_v.forEach(t),O5=h(ka),er=o(ka,"TR",{});var vv=l(er);Pu=o(vv,"TD",{align:!0});var GP=l(Pu);x5=u(GP,"past_user_inputs"),GP.forEach(t),R5=h(vv),Su=o(vv,"TD",{align:!0});var CP=l(Su);N5=u(CP,"List of strings. The last inputs from the user in the conversation, <em>after the model has run."),CP.forEach(t),vv.forEach(t),P5=h(ka),tr=o(ka,"TR",{});var Ev=l(tr);Iu=o(Ev,"TD",{align:!0});var MP=l(Iu);S5=u(MP,"generated_responses"),MP.forEach(t),I5=h(Ev),Hu=o(Ev,"TD",{align:!0});var FP=l(Hu);H5=u(FP,"List of strings. The last outputs from the model in the conversation, <em>after the model has run."),FP.forEach(t),Ev.forEach(t),ka.forEach(t),mv.forEach(t),Cq=h(a),Ie=o(a,"H2",{class:!0});var wv=l(Ie);Lt=o(wv,"A",{id:!0,class:!0,href:!0});var UP=l(Lt);Id=o(UP,"SPAN",{});var LP=l(Id);v(sr.$$.fragment,LP),LP.forEach(t),UP.forEach(t),B5=h(wv),Hd=o(wv,"SPAN",{});var zP=l(Hd);G5=u(zP,"Table question answering task"),zP.forEach(t),wv.forEach(t),Mq=h(a),Bu=o(a,"P",{});var JP=l(Bu);C5=u(JP,`Don\u2019t know SQL? Don\u2019t want to dive into a large spreadsheet? Ask
questions in plain english!`),JP.forEach(t),Fq=h(a),v(zt.$$.fragment,a),Uq=h(a),ar=o(a,"P",{});var Tx=l(ar);M5=u(Tx,"Available with: "),nr=o(Tx,"A",{href:!0,rel:!0});var WP=l(nr);F5=u(WP,"\u{1F917} Transformers"),WP.forEach(t),Tx.forEach(t),Lq=h(a),Gu=o(a,"P",{});var YP=l(Gu);U5=u(YP,"Example:"),YP.forEach(t),zq=h(a),v(Jt.$$.fragment,a),Jq=h(a),Cu=o(a,"P",{});var KP=l(Cu);L5=u(KP,`When sending your request, you should send a JSON encoded payload. Here
are all the options`),KP.forEach(t),Wq=h(a),Wt=o(a,"TABLE",{});var yv=l(Wt);Bd=o(yv,"THEAD",{});var VP=l(Bd);rr=o(VP,"TR",{});var bv=l(rr);Mu=o(bv,"TH",{align:!0});var XP=l(Mu);z5=u(XP,"All parameters"),XP.forEach(t),J5=h(bv),Gd=o(bv,"TH",{align:!0}),l(Gd).forEach(t),bv.forEach(t),VP.forEach(t),W5=h(yv),L=o(yv,"TBODY",{});var Y=l(L);or=o(Y,"TR",{});var jv=l(or);lr=o(jv,"TD",{align:!0});var kx=l(lr);Cd=o(kx,"STRONG",{});var QP=l(Cd);Y5=u(QP,"inputs"),QP.forEach(t),K5=u(kx," (required)"),kx.forEach(t),V5=h(jv),Md=o(jv,"TD",{align:!0}),l(Md).forEach(t),jv.forEach(t),X5=h(Y),ir=o(Y,"TR",{});var Tv=l(ir);Fu=o(Tv,"TD",{align:!0});var ZP=l(Fu);Q5=u(ZP,"query (required)"),ZP.forEach(t),Z5=h(Tv),Uu=o(Tv,"TD",{align:!0});var eS=l(Uu);e4=u(eS,"The query in plain text that you want to ask the table"),eS.forEach(t),Tv.forEach(t),t4=h(Y),ur=o(Y,"TR",{});var kv=l(ur);Lu=o(kv,"TD",{align:!0});var tS=l(Lu);s4=u(tS,"table (required)"),tS.forEach(t),a4=h(kv),zu=o(kv,"TD",{align:!0});var sS=l(zu);n4=u(sS,"A table of data represented as a dict of list where entries are headers and the lists are all the values, all lists must have the same size."),sS.forEach(t),kv.forEach(t),r4=h(Y),cr=o(Y,"TR",{});var Av=l(cr);Ju=o(Av,"TD",{align:!0});var aS=l(Ju);Fd=o(aS,"STRONG",{});var nS=l(Fd);o4=u(nS,"options"),nS.forEach(t),aS.forEach(t),l4=h(Av),Wu=o(Av,"TD",{align:!0});var rS=l(Wu);i4=u(rS,"a dict containing the following keys:"),rS.forEach(t),Av.forEach(t),u4=h(Y),pr=o(Y,"TR",{});var Dv=l(pr);Yu=o(Dv,"TD",{align:!0});var oS=l(Yu);c4=u(oS,"use_gpu"),oS.forEach(t),p4=h(Dv),Yt=o(Dv,"TD",{align:!0});var Ov=l(Yt);f4=u(Ov,"(Default: "),Ud=o(Ov,"CODE",{});var lS=l(Ud);h4=u(lS,"false"),lS.forEach(t),d4=u(Ov,"). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),Ov.forEach(t),Dv.forEach(t),g4=h(Y),fr=o(Y,"TR",{});var xv=l(fr);Ku=o(xv,"TD",{align:!0});var iS=l(Ku);m4=u(iS,"use_cache"),iS.forEach(t),$4=h(xv),Kt=o(xv,"TD",{align:!0});var Rv=l(Kt);q4=u(Rv,"(Default: "),Ld=o(Rv,"CODE",{});var uS=l(Ld);_4=u(uS,"true"),uS.forEach(t),v4=u(Rv,"). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),Rv.forEach(t),xv.forEach(t),E4=h(Y),hr=o(Y,"TR",{});var Nv=l(hr);Vu=o(Nv,"TD",{align:!0});var cS=l(Vu);w4=u(cS,"wait_for_model"),cS.forEach(t),y4=h(Nv),Vt=o(Nv,"TD",{align:!0});var Pv=l(Vt);b4=u(Pv,"(Default: "),zd=o(Pv,"CODE",{});var pS=l(zd);j4=u(pS,"false"),pS.forEach(t),T4=u(Pv,") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),Pv.forEach(t),Nv.forEach(t),Y.forEach(t),yv.forEach(t),Yq=h(a),Xu=o(a,"P",{});var fS=l(Xu);k4=u(fS,"Return value is either a dict or a list of dicts if you sent a list of inputs"),fS.forEach(t),Kq=h(a),v(Xt.$$.fragment,a),Vq=h(a),Qt=o(a,"TABLE",{});var Sv=l(Qt);Jd=o(Sv,"THEAD",{});var hS=l(Jd);dr=o(hS,"TR",{});var Iv=l(dr);Qu=o(Iv,"TH",{align:!0});var dS=l(Qu);A4=u(dS,"Returned values"),dS.forEach(t),D4=h(Iv),Wd=o(Iv,"TH",{align:!0}),l(Wd).forEach(t),Iv.forEach(t),hS.forEach(t),O4=h(Sv),le=o(Sv,"TBODY",{});var Aa=l(le);gr=o(Aa,"TR",{});var Hv=l(gr);Zu=o(Hv,"TD",{align:!0});var gS=l(Zu);Yd=o(gS,"STRONG",{});var mS=l(Yd);x4=u(mS,"answer"),mS.forEach(t),gS.forEach(t),R4=h(Hv),ec=o(Hv,"TD",{align:!0});var $S=l(ec);N4=u($S,"The plaintext answer"),$S.forEach(t),Hv.forEach(t),P4=h(Aa),mr=o(Aa,"TR",{});var Bv=l(mr);tc=o(Bv,"TD",{align:!0});var qS=l(tc);Kd=o(qS,"STRONG",{});var _S=l(Kd);S4=u(_S,"coordinates"),_S.forEach(t),qS.forEach(t),I4=h(Bv),sc=o(Bv,"TD",{align:!0});var vS=l(sc);H4=u(vS,"a list of coordinates of the cells references in the answer"),vS.forEach(t),Bv.forEach(t),B4=h(Aa),$r=o(Aa,"TR",{});var Gv=l($r);ac=o(Gv,"TD",{align:!0});var ES=l(ac);Vd=o(ES,"STRONG",{});var wS=l(Vd);G4=u(wS,"cells"),wS.forEach(t),ES.forEach(t),C4=h(Gv),nc=o(Gv,"TD",{align:!0});var yS=l(nc);M4=u(yS,"a list of coordinates of the cells contents"),yS.forEach(t),Gv.forEach(t),F4=h(Aa),qr=o(Aa,"TR",{});var Cv=l(qr);rc=o(Cv,"TD",{align:!0});var bS=l(rc);Xd=o(bS,"STRONG",{});var jS=l(Xd);U4=u(jS,"aggregator"),jS.forEach(t),bS.forEach(t),L4=h(Cv),oc=o(Cv,"TD",{align:!0});var TS=l(oc);z4=u(TS,"The aggregator used to get the answer"),TS.forEach(t),Cv.forEach(t),Aa.forEach(t),Sv.forEach(t),Xq=h(a),He=o(a,"H2",{class:!0});var Mv=l(He);Zt=o(Mv,"A",{id:!0,class:!0,href:!0});var kS=l(Zt);Qd=o(kS,"SPAN",{});var AS=l(Qd);v(_r.$$.fragment,AS),AS.forEach(t),kS.forEach(t),J4=h(Mv),Zd=o(Mv,"SPAN",{});var DS=l(Zd);W4=u(DS,"Question answering task"),DS.forEach(t),Mv.forEach(t),Qq=h(a),lc=o(a,"P",{});var OS=l(lc);Y4=u(OS,"Want to have a nice know-it-all bot that can answer any questions ?"),OS.forEach(t),Zq=h(a),v(es.$$.fragment,a),e_=h(a),Be=o(a,"P",{});var z$=l(Be);K4=u(z$,"Available with: "),vr=o(z$,"A",{href:!0,rel:!0});var xS=l(vr);V4=u(xS,"\u{1F917}Transformers"),xS.forEach(t),X4=u(z$,` and
`),Er=o(z$,"A",{href:!0,rel:!0});var RS=l(Er);Q4=u(RS,"AllenNLP"),RS.forEach(t),z$.forEach(t),t_=h(a),ic=o(a,"P",{});var NS=l(ic);Z4=u(NS,"Example:"),NS.forEach(t),s_=h(a),v(ts.$$.fragment,a),a_=h(a),uc=o(a,"P",{});var PS=l(uc);ek=u(PS,`When sending your request, you should send a JSON encoded payload. Here
are all the options`),PS.forEach(t),n_=h(a),cc=o(a,"P",{});var SS=l(cc);tk=u(SS,"Return value is either a dict or a list of dicts if you sent a list of inputs"),SS.forEach(t),r_=h(a),v(ss.$$.fragment,a),o_=h(a),as=o(a,"TABLE",{});var Fv=l(as);eg=o(Fv,"THEAD",{});var IS=l(eg);wr=o(IS,"TR",{});var Uv=l(wr);pc=o(Uv,"TH",{align:!0});var HS=l(pc);sk=u(HS,"Returned values"),HS.forEach(t),ak=h(Uv),tg=o(Uv,"TH",{align:!0}),l(tg).forEach(t),Uv.forEach(t),IS.forEach(t),nk=h(Fv),ie=o(Fv,"TBODY",{});var Da=l(ie);yr=o(Da,"TR",{});var Lv=l(yr);fc=o(Lv,"TD",{align:!0});var BS=l(fc);sg=o(BS,"STRONG",{});var GS=l(sg);rk=u(GS,"answer"),GS.forEach(t),BS.forEach(t),ok=h(Lv),hc=o(Lv,"TD",{align:!0});var CS=l(hc);lk=u(CS,"A string that\u2019s the answer within the text."),CS.forEach(t),Lv.forEach(t),ik=h(Da),br=o(Da,"TR",{});var zv=l(br);dc=o(zv,"TD",{align:!0});var MS=l(dc);ag=o(MS,"STRONG",{});var FS=l(ag);uk=u(FS,"score"),FS.forEach(t),MS.forEach(t),ck=h(zv),gc=o(zv,"TD",{align:!0});var US=l(gc);pk=u(US,"A floats that represents how likely that the answer is correct"),US.forEach(t),zv.forEach(t),fk=h(Da),jr=o(Da,"TR",{});var Jv=l(jr);mc=o(Jv,"TD",{align:!0});var LS=l(mc);ng=o(LS,"STRONG",{});var zS=l(ng);hk=u(zS,"start"),zS.forEach(t),LS.forEach(t),dk=h(Jv),ns=o(Jv,"TD",{align:!0});var Wv=l(ns);gk=u(Wv,"The index (string wise) of the start of the answer within "),rg=o(Wv,"CODE",{});var JS=l(rg);mk=u(JS,"context"),JS.forEach(t),$k=u(Wv,"."),Wv.forEach(t),Jv.forEach(t),qk=h(Da),Tr=o(Da,"TR",{});var Yv=l(Tr);$c=o(Yv,"TD",{align:!0});var WS=l($c);og=o(WS,"STRONG",{});var YS=l(og);_k=u(YS,"stop"),YS.forEach(t),WS.forEach(t),vk=h(Yv),rs=o(Yv,"TD",{align:!0});var Kv=l(rs);Ek=u(Kv,"The index (string wise) of the stop of the answer within "),lg=o(Kv,"CODE",{});var KS=l(lg);wk=u(KS,"context"),KS.forEach(t),yk=u(Kv,"."),Kv.forEach(t),Yv.forEach(t),Da.forEach(t),Fv.forEach(t),l_=h(a),Ge=o(a,"H2",{class:!0});var Vv=l(Ge);os=o(Vv,"A",{id:!0,class:!0,href:!0});var VS=l(os);ig=o(VS,"SPAN",{});var XS=l(ig);v(kr.$$.fragment,XS),XS.forEach(t),VS.forEach(t),bk=h(Vv),ug=o(Vv,"SPAN",{});var QS=l(ug);jk=u(QS,"Text-classification task"),QS.forEach(t),Vv.forEach(t),i_=h(a),qc=o(a,"P",{});var ZS=l(qc);Tk=u(ZS,`Usually used for sentiment-analysis this will output the likelihood of
classes of an input.`),ZS.forEach(t),u_=h(a),v(ls.$$.fragment,a),c_=h(a),Ar=o(a,"P",{});var Ax=l(Ar);kk=u(Ax,"Available with: "),Dr=o(Ax,"A",{href:!0,rel:!0});var eI=l(Dr);Ak=u(eI,"\u{1F917} Transformers"),eI.forEach(t),Ax.forEach(t),p_=h(a),_c=o(a,"P",{});var tI=l(_c);Dk=u(tI,"Example:"),tI.forEach(t),f_=h(a),v(is.$$.fragment,a),h_=h(a),vc=o(a,"P",{});var sI=l(vc);Ok=u(sI,`When sending your request, you should send a JSON encoded payload. Here
are all the options`),sI.forEach(t),d_=h(a),us=o(a,"TABLE",{});var Xv=l(us);cg=o(Xv,"THEAD",{});var aI=l(cg);Or=o(aI,"TR",{});var Qv=l(Or);Ec=o(Qv,"TH",{align:!0});var nI=l(Ec);xk=u(nI,"All parameters"),nI.forEach(t),Rk=h(Qv),pg=o(Qv,"TH",{align:!0}),l(pg).forEach(t),Qv.forEach(t),aI.forEach(t),Nk=h(Xv),Q=o(Xv,"TBODY",{});var je=l(Q);xr=o(je,"TR",{});var Zv=l(xr);Rr=o(Zv,"TD",{align:!0});var Dx=l(Rr);fg=o(Dx,"STRONG",{});var rI=l(fg);Pk=u(rI,"inputs"),rI.forEach(t),Sk=u(Dx," (required)"),Dx.forEach(t),Ik=h(Zv),wc=o(Zv,"TD",{align:!0});var oI=l(wc);Hk=u(oI,"a string to be classified"),oI.forEach(t),Zv.forEach(t),Bk=h(je),Nr=o(je,"TR",{});var eE=l(Nr);yc=o(eE,"TD",{align:!0});var lI=l(yc);hg=o(lI,"STRONG",{});var iI=l(hg);Gk=u(iI,"options"),iI.forEach(t),lI.forEach(t),Ck=h(eE),bc=o(eE,"TD",{align:!0});var uI=l(bc);Mk=u(uI,"a dict containing the following keys:"),uI.forEach(t),eE.forEach(t),Fk=h(je),Pr=o(je,"TR",{});var tE=l(Pr);jc=o(tE,"TD",{align:!0});var cI=l(jc);Uk=u(cI,"use_gpu"),cI.forEach(t),Lk=h(tE),cs=o(tE,"TD",{align:!0});var sE=l(cs);zk=u(sE,"(Default: "),dg=o(sE,"CODE",{});var pI=l(dg);Jk=u(pI,"false"),pI.forEach(t),Wk=u(sE,"). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),sE.forEach(t),tE.forEach(t),Yk=h(je),Sr=o(je,"TR",{});var aE=l(Sr);Tc=o(aE,"TD",{align:!0});var fI=l(Tc);Kk=u(fI,"use_cache"),fI.forEach(t),Vk=h(aE),ps=o(aE,"TD",{align:!0});var nE=l(ps);Xk=u(nE,"(Default: "),gg=o(nE,"CODE",{});var hI=l(gg);Qk=u(hI,"true"),hI.forEach(t),Zk=u(nE,"). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),nE.forEach(t),aE.forEach(t),e7=h(je),Ir=o(je,"TR",{});var rE=l(Ir);kc=o(rE,"TD",{align:!0});var dI=l(kc);t7=u(dI,"wait_for_model"),dI.forEach(t),s7=h(rE),fs=o(rE,"TD",{align:!0});var oE=l(fs);a7=u(oE,"(Default: "),mg=o(oE,"CODE",{});var gI=l(mg);n7=u(gI,"false"),gI.forEach(t),r7=u(oE,") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),oE.forEach(t),rE.forEach(t),je.forEach(t),Xv.forEach(t),g_=h(a),Ac=o(a,"P",{});var mI=l(Ac);o7=u(mI,"Return value is either a dict or a list of dicts if you sent a list of inputs"),mI.forEach(t),m_=h(a),v(hs.$$.fragment,a),$_=h(a),ds=o(a,"TABLE",{});var lE=l(ds);$g=o(lE,"THEAD",{});var $I=l($g);Hr=o($I,"TR",{});var iE=l(Hr);Dc=o(iE,"TH",{align:!0});var qI=l(Dc);l7=u(qI,"Returned values"),qI.forEach(t),i7=h(iE),qg=o(iE,"TH",{align:!0}),l(qg).forEach(t),iE.forEach(t),$I.forEach(t),u7=h(lE),Br=o(lE,"TBODY",{});var uE=l(Br);Gr=o(uE,"TR",{});var cE=l(Gr);Oc=o(cE,"TD",{align:!0});var _I=l(Oc);_g=o(_I,"STRONG",{});var vI=l(_g);c7=u(vI,"label"),vI.forEach(t),_I.forEach(t),p7=h(cE),xc=o(cE,"TD",{align:!0});var EI=l(xc);f7=u(EI,"The label for the class (model specific)"),EI.forEach(t),cE.forEach(t),h7=h(uE),Cr=o(uE,"TR",{});var pE=l(Cr);Rc=o(pE,"TD",{align:!0});var wI=l(Rc);vg=o(wI,"STRONG",{});var yI=l(vg);d7=u(yI,"score"),yI.forEach(t),wI.forEach(t),g7=h(pE),Nc=o(pE,"TD",{align:!0});var bI=l(Nc);m7=u(bI,"A floats that represents how likely is that the text belongs the this class."),bI.forEach(t),pE.forEach(t),uE.forEach(t),lE.forEach(t),q_=h(a),Ce=o(a,"H2",{class:!0});var fE=l(Ce);gs=o(fE,"A",{id:!0,class:!0,href:!0});var jI=l(gs);Eg=o(jI,"SPAN",{});var TI=l(Eg);v(Mr.$$.fragment,TI),TI.forEach(t),jI.forEach(t),$7=h(fE),wg=o(fE,"SPAN",{});var kI=l(wg);q7=u(kI,"Named Entity Recognition (NER) task"),kI.forEach(t),fE.forEach(t),__=h(a),Fr=o(a,"P",{});var Ox=l(Fr);_7=u(Ox,"See "),Pc=o(Ox,"A",{href:!0});var AI=l(Pc);v7=u(AI,"Token-classification task"),AI.forEach(t),Ox.forEach(t),v_=h(a),Me=o(a,"H2",{class:!0});var hE=l(Me);ms=o(hE,"A",{id:!0,class:!0,href:!0});var DI=l(ms);yg=o(DI,"SPAN",{});var OI=l(yg);v(Ur.$$.fragment,OI),OI.forEach(t),DI.forEach(t),E7=h(hE),bg=o(hE,"SPAN",{});var xI=l(bg);w7=u(xI,"Token-classification task"),xI.forEach(t),hE.forEach(t),E_=h(a),Sc=o(a,"P",{});var RI=l(Sc);y7=u(RI,`Usually used for sentence parsing, either grammatical, or Named Entity
Recognition (NER) to understand keywords contained within text.`),RI.forEach(t),w_=h(a),v($s.$$.fragment,a),y_=h(a),Fe=o(a,"P",{});var J$=l(Fe);b7=u(J$,"Available with: "),Lr=o(J$,"A",{href:!0,rel:!0});var NI=l(Lr);j7=u(NI,"\u{1F917} Transformers"),NI.forEach(t),T7=u(J$,`,
`),zr=o(J$,"A",{href:!0,rel:!0});var PI=l(zr);k7=u(PI,"Flair"),PI.forEach(t),J$.forEach(t),b_=h(a),Ic=o(a,"P",{});var SI=l(Ic);A7=u(SI,"Example:"),SI.forEach(t),j_=h(a),v(qs.$$.fragment,a),T_=h(a),Hc=o(a,"P",{});var II=l(Hc);D7=u(II,`When sending your request, you should send a JSON encoded payload. Here
are all the options`),II.forEach(t),k_=h(a),_s=o(a,"TABLE",{});var dE=l(_s);jg=o(dE,"THEAD",{});var HI=l(jg);Jr=o(HI,"TR",{});var gE=l(Jr);Bc=o(gE,"TH",{align:!0});var BI=l(Bc);O7=u(BI,"All parameters"),BI.forEach(t),x7=h(gE),Tg=o(gE,"TH",{align:!0}),l(Tg).forEach(t),gE.forEach(t),HI.forEach(t),R7=h(dE),z=o(dE,"TBODY",{});var K=l(z);Wr=o(K,"TR",{});var mE=l(Wr);Yr=o(mE,"TD",{align:!0});var xx=l(Yr);kg=o(xx,"STRONG",{});var GI=l(kg);N7=u(GI,"inputs"),GI.forEach(t),P7=u(xx," (required)"),xx.forEach(t),S7=h(mE),Gc=o(mE,"TD",{align:!0});var CI=l(Gc);I7=u(CI,"a string to be classified"),CI.forEach(t),mE.forEach(t),H7=h(K),Kr=o(K,"TR",{});var $E=l(Kr);Cc=o($E,"TD",{align:!0});var MI=l(Cc);Ag=o(MI,"STRONG",{});var FI=l(Ag);B7=u(FI,"parameters"),FI.forEach(t),MI.forEach(t),G7=h($E),Mc=o($E,"TD",{align:!0});var UI=l(Mc);C7=u(UI,"a dict containing the following key:"),UI.forEach(t),$E.forEach(t),M7=h(K),Vr=o(K,"TR",{});var qE=l(Vr);Fc=o(qE,"TD",{align:!0});var LI=l(Fc);F7=u(LI,"aggregation_strategy"),LI.forEach(t),U7=h(qE),vs=o(qE,"TD",{align:!0});var _E=l(vs);L7=u(_E,"(Default: "),Dg=o(_E,"CODE",{});var zI=l(Dg);z7=u(zI,'simple</span></code>). There are several aggregation strategies:<br>* <code class="docutils literal notranslate"><span class="pre">none</span></code>: Every token gets classified without further aggregation.<br>* <code class="docutils literal notranslate"><span class="pre">simple</span></code>: Entities are grouped according to the default schema (B-, I- tags get merged when the tag is similar).<br>* <code class="docutils literal notranslate"><span class="pre">first</span></code>: Same as the <code class="docutils literal notranslate"><span class="pre">simple</span></code> strategy except words cannot end up with different tags. Words will use the tag of the first token when there is ambiguity.<br>* <code class="docutils literal notranslate"><span class="pre">average</span></code>: Same as the <code class="docutils literal notranslate"><span class="pre">simple</span></code> strategy except words cannot end up with different tags. Scores are averaged across tokens and then the maximum label is applied.<br>* <code class="docutils literal notranslate"><span class="pre">max</span></code>: Same as the <code class="docutils literal notranslate"><span class="pre">simple'),zI.forEach(t),J7=u(_E," strategy except words cannot end up with different tags. Word entity will be the token with the maximum score."),_E.forEach(t),qE.forEach(t),W7=h(K),Xr=o(K,"TR",{});var vE=l(Xr);Uc=o(vE,"TD",{align:!0});var JI=l(Uc);Og=o(JI,"STRONG",{});var WI=l(Og);Y7=u(WI,"options"),WI.forEach(t),JI.forEach(t),K7=h(vE),Lc=o(vE,"TD",{align:!0});var YI=l(Lc);V7=u(YI,"a dict containing the following keys:"),YI.forEach(t),vE.forEach(t),X7=h(K),Qr=o(K,"TR",{});var EE=l(Qr);zc=o(EE,"TD",{align:!0});var KI=l(zc);Q7=u(KI,"use_gpu"),KI.forEach(t),Z7=h(EE),Es=o(EE,"TD",{align:!0});var wE=l(Es);e9=u(wE,"(Default: "),xg=o(wE,"CODE",{});var VI=l(xg);t9=u(VI,"false"),VI.forEach(t),s9=u(wE,"). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),wE.forEach(t),EE.forEach(t),a9=h(K),Zr=o(K,"TR",{});var yE=l(Zr);Jc=o(yE,"TD",{align:!0});var XI=l(Jc);n9=u(XI,"use_cache"),XI.forEach(t),r9=h(yE),ws=o(yE,"TD",{align:!0});var bE=l(ws);o9=u(bE,"(Default: "),Rg=o(bE,"CODE",{});var QI=l(Rg);l9=u(QI,"true"),QI.forEach(t),i9=u(bE,"). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),bE.forEach(t),yE.forEach(t),u9=h(K),eo=o(K,"TR",{});var jE=l(eo);Wc=o(jE,"TD",{align:!0});var ZI=l(Wc);c9=u(ZI,"wait_for_model"),ZI.forEach(t),p9=h(jE),ys=o(jE,"TD",{align:!0});var TE=l(ys);f9=u(TE,"(Default: "),Ng=o(TE,"CODE",{});var eH=l(Ng);h9=u(eH,"false"),eH.forEach(t),d9=u(TE,") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),TE.forEach(t),jE.forEach(t),K.forEach(t),dE.forEach(t),A_=h(a),Yc=o(a,"P",{});var tH=l(Yc);g9=u(tH,"Return value is either a dict or a list of dicts if you sent a list of inputs"),tH.forEach(t),D_=h(a),v(bs.$$.fragment,a),O_=h(a),js=o(a,"TABLE",{});var kE=l(js);Pg=o(kE,"THEAD",{});var sH=l(Pg);to=o(sH,"TR",{});var AE=l(to);Kc=o(AE,"TH",{align:!0});var aH=l(Kc);m9=u(aH,"Returned values"),aH.forEach(t),$9=h(AE),Sg=o(AE,"TH",{align:!0}),l(Sg).forEach(t),AE.forEach(t),sH.forEach(t),q9=h(kE),Z=o(kE,"TBODY",{});var Te=l(Z);so=o(Te,"TR",{});var DE=l(so);Vc=o(DE,"TD",{align:!0});var nH=l(Vc);Ig=o(nH,"STRONG",{});var rH=l(Ig);_9=u(rH,"entity_group"),rH.forEach(t),nH.forEach(t),v9=h(DE),Xc=o(DE,"TD",{align:!0});var oH=l(Xc);E9=u(oH,"The type for the entity being recognized (model specific)."),oH.forEach(t),DE.forEach(t),w9=h(Te),ao=o(Te,"TR",{});var OE=l(ao);Qc=o(OE,"TD",{align:!0});var lH=l(Qc);Hg=o(lH,"STRONG",{});var iH=l(Hg);y9=u(iH,"score"),iH.forEach(t),lH.forEach(t),b9=h(OE),Zc=o(OE,"TD",{align:!0});var uH=l(Zc);j9=u(uH,"How likely the entity was recognized."),uH.forEach(t),OE.forEach(t),T9=h(Te),no=o(Te,"TR",{});var xE=l(no);ep=o(xE,"TD",{align:!0});var cH=l(ep);Bg=o(cH,"STRONG",{});var pH=l(Bg);k9=u(pH,"word"),pH.forEach(t),cH.forEach(t),A9=h(xE),tp=o(xE,"TD",{align:!0});var fH=l(tp);D9=u(fH,"The string that was captured"),fH.forEach(t),xE.forEach(t),O9=h(Te),ro=o(Te,"TR",{});var RE=l(ro);sp=o(RE,"TD",{align:!0});var hH=l(sp);Gg=o(hH,"STRONG",{});var dH=l(Gg);x9=u(dH,"start"),dH.forEach(t),hH.forEach(t),R9=h(RE),Ts=o(RE,"TD",{align:!0});var NE=l(Ts);N9=u(NE,"The offset stringwise where the answer is located. Useful to disambiguate if "),Cg=o(NE,"CODE",{});var gH=l(Cg);P9=u(gH,"word"),gH.forEach(t),S9=u(NE," occurs multiple times."),NE.forEach(t),RE.forEach(t),I9=h(Te),oo=o(Te,"TR",{});var PE=l(oo);ap=o(PE,"TD",{align:!0});var mH=l(ap);Mg=o(mH,"STRONG",{});var $H=l(Mg);H9=u($H,"end"),$H.forEach(t),mH.forEach(t),B9=h(PE),ks=o(PE,"TD",{align:!0});var SE=l(ks);G9=u(SE,"The offset stringwise where the answer is located. Useful to disambiguate if "),Fg=o(SE,"CODE",{});var qH=l(Fg);C9=u(qH,"word"),qH.forEach(t),M9=u(SE," occurs multiple times."),SE.forEach(t),PE.forEach(t),Te.forEach(t),kE.forEach(t),x_=h(a),Ue=o(a,"H2",{class:!0});var IE=l(Ue);As=o(IE,"A",{id:!0,class:!0,href:!0});var _H=l(As);Ug=o(_H,"SPAN",{});var vH=l(Ug);v(lo.$$.fragment,vH),vH.forEach(t),_H.forEach(t),F9=h(IE),Lg=o(IE,"SPAN",{});var EH=l(Lg);U9=u(EH,"Text-generation task"),EH.forEach(t),IE.forEach(t),R_=h(a),np=o(a,"P",{});var wH=l(np);L9=u(wH,"Use to continue text from a prompt. This is a very generic task."),wH.forEach(t),N_=h(a),v(Ds.$$.fragment,a),P_=h(a),io=o(a,"P",{});var Rx=l(io);z9=u(Rx,"Available with: "),uo=o(Rx,"A",{href:!0,rel:!0});var yH=l(uo);J9=u(yH,"\u{1F917} Transformers"),yH.forEach(t),Rx.forEach(t),S_=h(a),rp=o(a,"P",{});var bH=l(rp);W9=u(bH,"Example:"),bH.forEach(t),I_=h(a),v(Os.$$.fragment,a),H_=h(a),op=o(a,"P",{});var jH=l(op);Y9=u(jH,`When sending your request, you should send a JSON encoded payload. Here
are all the options`),jH.forEach(t),B_=h(a),xs=o(a,"TABLE",{});var HE=l(xs);zg=o(HE,"THEAD",{});var TH=l(zg);co=o(TH,"TR",{});var BE=l(co);lp=o(BE,"TH",{align:!0});var kH=l(lp);K9=u(kH,"All parameters"),kH.forEach(t),V9=h(BE),Jg=o(BE,"TH",{align:!0}),l(Jg).forEach(t),BE.forEach(t),TH.forEach(t),X9=h(HE),S=o(HE,"TBODY",{});var H=l(S);po=o(H,"TR",{});var GE=l(po);fo=o(GE,"TD",{align:!0});var Nx=l(fo);Wg=o(Nx,"STRONG",{});var AH=l(Wg);Q9=u(AH,"inputs"),AH.forEach(t),Z9=u(Nx," (required):"),Nx.forEach(t),e6=h(GE),ip=o(GE,"TD",{align:!0});var DH=l(ip);t6=u(DH,"a string to be generated from"),DH.forEach(t),GE.forEach(t),s6=h(H),ho=o(H,"TR",{});var CE=l(ho);up=o(CE,"TD",{align:!0});var OH=l(up);Yg=o(OH,"STRONG",{});var xH=l(Yg);a6=u(xH,"parameters"),xH.forEach(t),OH.forEach(t),n6=h(CE),cp=o(CE,"TD",{align:!0});var RH=l(cp);r6=u(RH,"dict containing the following keys:"),RH.forEach(t),CE.forEach(t),o6=h(H),go=o(H,"TR",{});var ME=l(go);pp=o(ME,"TD",{align:!0});var NH=l(pp);l6=u(NH,"top_k"),NH.forEach(t),i6=h(ME),ve=o(ME,"TD",{align:!0});var oh=l(ve);u6=u(oh,"(Default: "),Kg=o(oh,"CODE",{});var PH=l(Kg);c6=u(PH,"None"),PH.forEach(t),p6=u(oh,"). Integer to define the top tokens considered within the "),fp=o(oh,"SPAN",{class:!0});var SH=l(fp);f6=u(SH,"sample"),SH.forEach(t),h6=u(oh," operation to create new text."),oh.forEach(t),ME.forEach(t),d6=h(H),mo=o(H,"TR",{});var FE=l(mo);hp=o(FE,"TD",{align:!0});var IH=l(hp);g6=u(IH,"top_p"),IH.forEach(t),m6=h(FE),Ee=o(FE,"TD",{align:!0});var lh=l(Ee);$6=u(lh,"(Default: "),Vg=o(lh,"CODE",{});var HH=l(Vg);q6=u(HH,"None"),HH.forEach(t),_6=u(lh,"). Float to define the tokens that are within the  sample` operation of text generation. Add tokens in the sample for more probable to least probable until the sum of the probabilities is greater than "),dp=o(lh,"SPAN",{class:!0});var BH=l(dp);v6=u(BH,"top_p"),BH.forEach(t),E6=u(lh,"."),lh.forEach(t),FE.forEach(t),w6=h(H),$o=o(H,"TR",{});var UE=l($o);gp=o(UE,"TD",{align:!0});var GH=l(gp);y6=u(GH,"temperature"),GH.forEach(t),b6=h(UE),re=o(UE,"TD",{align:!0});var Oa=l(re);j6=u(Oa,"(Default: "),Xg=o(Oa,"CODE",{});var CH=l(Xg);T6=u(CH,"1.0"),CH.forEach(t),k6=u(Oa,"). Float (0.0-100.0). The temperature of the sampling operation. 1 means regular sampling, 0 means "),mp=o(Oa,"SPAN",{class:!0});var MH=l(mp);A6=u(MH,"top_k=1"),MH.forEach(t),D6=u(Oa,", "),$p=o(Oa,"SPAN",{class:!0});var FH=l($p);O6=u(FH,"100.0"),FH.forEach(t),x6=u(Oa," is getting closer to uniform probability."),Oa.forEach(t),UE.forEach(t),R6=h(H),qo=o(H,"TR",{});var LE=l(qo);qp=o(LE,"TD",{align:!0});var UH=l(qp);N6=u(UH,"repetition_penalty"),UH.forEach(t),P6=h(LE),Rs=o(LE,"TD",{align:!0});var zE=l(Rs);S6=u(zE,"(Default: "),Qg=o(zE,"CODE",{});var LH=l(Qg);I6=u(LH,"None"),LH.forEach(t),H6=u(zE,"). Float (0.0-100.0). The more a token is used within generation the more it is penalized to not be picked in successive generation passes."),zE.forEach(t),LE.forEach(t),B6=h(H),_o=o(H,"TR",{});var JE=l(_o);_p=o(JE,"TD",{align:!0});var zH=l(_p);G6=u(zH,"max_new_tokens"),zH.forEach(t),C6=h(JE),we=o(JE,"TD",{align:!0});var ih=l(we);M6=u(ih,"(Default: "),Zg=o(ih,"CODE",{});var JH=l(Zg);F6=u(JH,"None"),JH.forEach(t),U6=u(ih,"). Int (0-250). The amount of new tokens to be generated, this does "),em=o(ih,"STRONG",{});var WH=l(em);L6=u(WH,"not"),WH.forEach(t),z6=u(ih," include the input length it is a estimate of the size of generated text you want. Each new tokens slows down the request, so look for balance between response times and length of text generated."),ih.forEach(t),JE.forEach(t),J6=h(H),vo=o(H,"TR",{});var WE=l(vo);vp=o(WE,"TD",{align:!0});var YH=l(vp);W6=u(YH,"max_time"),YH.forEach(t),Y6=h(WE),Ns=o(WE,"TD",{align:!0});var YE=l(Ns);K6=u(YE,"(Default: "),tm=o(YE,"CODE",{});var KH=l(tm);V6=u(KH,'None</span></code>). Float (0-120.0). The amount of time in seconds that the query should take maximum. Network can cause some overhead so it will be a soft limit. Use that in combination with <code class="xref py py-obj docutils literal notranslate"><span class="pre">max_new_tokens'),KH.forEach(t),X6=u(YE," for best results."),YE.forEach(t),WE.forEach(t),Q6=h(H),Eo=o(H,"TR",{});var KE=l(Eo);Ep=o(KE,"TD",{align:!0});var VH=l(Ep);Z6=u(VH,"return_full_text"),VH.forEach(t),e8=h(KE),ye=o(KE,"TD",{align:!0});var uh=l(ye);t8=u(uh,"(Default: "),sm=o(uh,"CODE",{});var XH=l(sm);s8=u(XH,"True"),XH.forEach(t),a8=u(uh,"). Bool. If set to False, the return results will "),am=o(uh,"STRONG",{});var QH=l(am);n8=u(QH,"not"),QH.forEach(t),r8=u(uh," contain the original query making it easier for prompting."),uh.forEach(t),KE.forEach(t),o8=h(H),wo=o(H,"TR",{});var VE=l(wo);wp=o(VE,"TD",{align:!0});var ZH=l(wp);l8=u(ZH,"num_return_sequences"),ZH.forEach(t),i8=h(VE),Ps=o(VE,"TD",{align:!0});var XE=l(Ps);u8=u(XE,"(Default: "),nm=o(XE,"CODE",{});var eB=l(nm);c8=u(eB,"1"),eB.forEach(t),p8=u(XE,"). Integer. The number of proposition you want to be returned."),XE.forEach(t),VE.forEach(t),f8=h(H),yo=o(H,"TR",{});var QE=l(yo);yp=o(QE,"TD",{align:!0});var tB=l(yp);h8=u(tB,"do_sample"),tB.forEach(t),d8=h(QE),Ss=o(QE,"TD",{align:!0});var ZE=l(Ss);g8=u(ZE,"(Optional: "),rm=o(ZE,"CODE",{});var sB=l(rm);m8=u(sB,"True"),sB.forEach(t),$8=u(ZE,"). Bool. Whether or not to use sampling, use greedy decoding otherwise."),ZE.forEach(t),QE.forEach(t),q8=h(H),bo=o(H,"TR",{});var ew=l(bo);bp=o(ew,"TD",{align:!0});var aB=l(bp);om=o(aB,"STRONG",{});var nB=l(om);_8=u(nB,"options"),nB.forEach(t),aB.forEach(t),v8=h(ew),jp=o(ew,"TD",{align:!0});var rB=l(jp);E8=u(rB,"a dict containing the following keys:"),rB.forEach(t),ew.forEach(t),w8=h(H),jo=o(H,"TR",{});var tw=l(jo);Tp=o(tw,"TD",{align:!0});var oB=l(Tp);y8=u(oB,"use_gpu"),oB.forEach(t),b8=h(tw),Is=o(tw,"TD",{align:!0});var sw=l(Is);j8=u(sw,"(Default: "),lm=o(sw,"CODE",{});var lB=l(lm);T8=u(lB,"false"),lB.forEach(t),k8=u(sw,"). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),sw.forEach(t),tw.forEach(t),A8=h(H),To=o(H,"TR",{});var aw=l(To);kp=o(aw,"TD",{align:!0});var iB=l(kp);D8=u(iB,"use_cache"),iB.forEach(t),O8=h(aw),Hs=o(aw,"TD",{align:!0});var nw=l(Hs);x8=u(nw,"(Default: "),im=o(nw,"CODE",{});var uB=l(im);R8=u(uB,"true"),uB.forEach(t),N8=u(nw,"). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),nw.forEach(t),aw.forEach(t),P8=h(H),ko=o(H,"TR",{});var rw=l(ko);Ap=o(rw,"TD",{align:!0});var cB=l(Ap);S8=u(cB,"wait_for_model"),cB.forEach(t),I8=h(rw),Bs=o(rw,"TD",{align:!0});var ow=l(Bs);H8=u(ow,"(Default: "),um=o(ow,"CODE",{});var pB=l(um);B8=u(pB,"false"),pB.forEach(t),G8=u(ow,") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),ow.forEach(t),rw.forEach(t),H.forEach(t),HE.forEach(t),G_=h(a),Dp=o(a,"P",{});var fB=l(Dp);C8=u(fB,"Return value is either a dict or a list of dicts if you sent a list of inputs"),fB.forEach(t),C_=h(a),v(Gs.$$.fragment,a),M_=h(a),Cs=o(a,"TABLE",{});var lw=l(Cs);cm=o(lw,"THEAD",{});var hB=l(cm);Ao=o(hB,"TR",{});var iw=l(Ao);Op=o(iw,"TH",{align:!0});var dB=l(Op);M8=u(dB,"Returned values"),dB.forEach(t),F8=h(iw),pm=o(iw,"TH",{align:!0}),l(pm).forEach(t),iw.forEach(t),hB.forEach(t),U8=h(lw),fm=o(lw,"TBODY",{});var gB=l(fm);Do=o(gB,"TR",{});var uw=l(Do);xp=o(uw,"TD",{align:!0});var mB=l(xp);hm=o(mB,"STRONG",{});var $B=l(hm);L8=u($B,"generated_text"),$B.forEach(t),mB.forEach(t),z8=h(uw),Rp=o(uw,"TD",{align:!0});var qB=l(Rp);J8=u(qB,"The continuated string"),qB.forEach(t),uw.forEach(t),gB.forEach(t),lw.forEach(t),F_=h(a),Le=o(a,"H2",{class:!0});var cw=l(Le);Ms=o(cw,"A",{id:!0,class:!0,href:!0});var _B=l(Ms);dm=o(_B,"SPAN",{});var vB=l(dm);v(Oo.$$.fragment,vB),vB.forEach(t),_B.forEach(t),W8=h(cw),gm=o(cw,"SPAN",{});var EB=l(gm);Y8=u(EB,"Text2text-generation task"),EB.forEach(t),cw.forEach(t),U_=h(a),Fs=o(a,"P",{});var pw=l(Fs);K8=u(pw,"Essentially "),Np=o(pw,"A",{href:!0});var wB=l(Np);V8=u(wB,"Text-generation task"),wB.forEach(t),X8=u(pw,`. But uses
Encoder-Decoder architecture, so might change in the future for more
options.`),pw.forEach(t),L_=h(a),ze=o(a,"H2",{class:!0});var fw=l(ze);Us=o(fw,"A",{id:!0,class:!0,href:!0});var yB=l(Us);mm=o(yB,"SPAN",{});var bB=l(mm);v(xo.$$.fragment,bB),bB.forEach(t),yB.forEach(t),Q8=h(fw),$m=o(fw,"SPAN",{});var jB=l($m);Z8=u(jB,"Fill mask task"),jB.forEach(t),fw.forEach(t),z_=h(a),Pp=o(a,"P",{});var TB=l(Pp);eA=u(TB,`Tries to fill in a hole with a missing word (token to be precise).
That\u2019s the base task for BERT models.`),TB.forEach(t),J_=h(a),v(Ls.$$.fragment,a),W_=h(a),Ro=o(a,"P",{});var Px=l(Ro);tA=u(Px,"Available with: "),No=o(Px,"A",{href:!0,rel:!0});var kB=l(No);sA=u(kB,"\u{1F917} Transformers"),kB.forEach(t),Px.forEach(t),Y_=h(a),Sp=o(a,"P",{});var AB=l(Sp);aA=u(AB,"Example:"),AB.forEach(t),K_=h(a),v(zs.$$.fragment,a),V_=h(a),Ip=o(a,"P",{});var DB=l(Ip);nA=u(DB,`When sending your request, you should send a JSON encoded payload. Here
are all the options`),DB.forEach(t),X_=h(a),Js=o(a,"TABLE",{});var hw=l(Js);qm=o(hw,"THEAD",{});var OB=l(qm);Po=o(OB,"TR",{});var dw=l(Po);Hp=o(dw,"TH",{align:!0});var xB=l(Hp);rA=u(xB,"All parameters"),xB.forEach(t),oA=h(dw),_m=o(dw,"TH",{align:!0}),l(_m).forEach(t),dw.forEach(t),OB.forEach(t),lA=h(hw),ee=o(hw,"TBODY",{});var ke=l(ee);So=o(ke,"TR",{});var gw=l(So);Io=o(gw,"TD",{align:!0});var Sx=l(Io);vm=o(Sx,"STRONG",{});var RB=l(vm);iA=u(RB,"inputs"),RB.forEach(t),uA=u(Sx," (required):"),Sx.forEach(t),cA=h(gw),Bp=o(gw,"TD",{align:!0});var NB=l(Bp);pA=u(NB,"a string to be filled from, must contain the [MASK] token (check model card for exact name of the mask)"),NB.forEach(t),gw.forEach(t),fA=h(ke),Ho=o(ke,"TR",{});var mw=l(Ho);Gp=o(mw,"TD",{align:!0});var PB=l(Gp);Em=o(PB,"STRONG",{});var SB=l(Em);hA=u(SB,"options"),SB.forEach(t),PB.forEach(t),dA=h(mw),Cp=o(mw,"TD",{align:!0});var IB=l(Cp);gA=u(IB,"a dict containing the following keys:"),IB.forEach(t),mw.forEach(t),mA=h(ke),Bo=o(ke,"TR",{});var $w=l(Bo);Mp=o($w,"TD",{align:!0});var HB=l(Mp);$A=u(HB,"use_gpu"),HB.forEach(t),qA=h($w),Ws=o($w,"TD",{align:!0});var qw=l(Ws);_A=u(qw,"(Default: "),wm=o(qw,"CODE",{});var BB=l(wm);vA=u(BB,"false"),BB.forEach(t),EA=u(qw,"). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),qw.forEach(t),$w.forEach(t),wA=h(ke),Go=o(ke,"TR",{});var _w=l(Go);Fp=o(_w,"TD",{align:!0});var GB=l(Fp);yA=u(GB,"use_cache"),GB.forEach(t),bA=h(_w),Ys=o(_w,"TD",{align:!0});var vw=l(Ys);jA=u(vw,"(Default: "),ym=o(vw,"CODE",{});var CB=l(ym);TA=u(CB,"true"),CB.forEach(t),kA=u(vw,"). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),vw.forEach(t),_w.forEach(t),AA=h(ke),Co=o(ke,"TR",{});var Ew=l(Co);Up=o(Ew,"TD",{align:!0});var MB=l(Up);DA=u(MB,"wait_for_model"),MB.forEach(t),OA=h(Ew),Ks=o(Ew,"TD",{align:!0});var ww=l(Ks);xA=u(ww,"(Default: "),bm=o(ww,"CODE",{});var FB=l(bm);RA=u(FB,"false"),FB.forEach(t),NA=u(ww,") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),ww.forEach(t),Ew.forEach(t),ke.forEach(t),hw.forEach(t),Q_=h(a),Lp=o(a,"P",{});var UB=l(Lp);PA=u(UB,"Return value is either a dict or a list of dicts if you sent a list of inputs"),UB.forEach(t),Z_=h(a),v(Vs.$$.fragment,a),e1=h(a),Xs=o(a,"TABLE",{});var yw=l(Xs);jm=o(yw,"THEAD",{});var LB=l(jm);Mo=o(LB,"TR",{});var bw=l(Mo);zp=o(bw,"TH",{align:!0});var zB=l(zp);SA=u(zB,"Returned values"),zB.forEach(t),IA=h(bw),Tm=o(bw,"TH",{align:!0}),l(Tm).forEach(t),bw.forEach(t),LB.forEach(t),HA=h(yw),ue=o(yw,"TBODY",{});var xa=l(ue);Fo=o(xa,"TR",{});var jw=l(Fo);Jp=o(jw,"TD",{align:!0});var JB=l(Jp);km=o(JB,"STRONG",{});var WB=l(km);BA=u(WB,"sequence"),WB.forEach(t),JB.forEach(t),GA=h(jw),Wp=o(jw,"TD",{align:!0});var YB=l(Wp);CA=u(YB,"The actual sequence of tokens that ran against the model (may contain special tokens)"),YB.forEach(t),jw.forEach(t),MA=h(xa),Uo=o(xa,"TR",{});var Tw=l(Uo);Yp=o(Tw,"TD",{align:!0});var KB=l(Yp);Am=o(KB,"STRONG",{});var VB=l(Am);FA=u(VB,"score"),VB.forEach(t),KB.forEach(t),UA=h(Tw),Kp=o(Tw,"TD",{align:!0});var XB=l(Kp);LA=u(XB,"The probability for this token."),XB.forEach(t),Tw.forEach(t),zA=h(xa),Lo=o(xa,"TR",{});var kw=l(Lo);Vp=o(kw,"TD",{align:!0});var QB=l(Vp);Dm=o(QB,"STRONG",{});var ZB=l(Dm);JA=u(ZB,"token"),ZB.forEach(t),QB.forEach(t),WA=h(kw),Xp=o(kw,"TD",{align:!0});var eG=l(Xp);YA=u(eG,"The id of the token"),eG.forEach(t),kw.forEach(t),KA=h(xa),zo=o(xa,"TR",{});var Aw=l(zo);Qp=o(Aw,"TD",{align:!0});var tG=l(Qp);Om=o(tG,"STRONG",{});var sG=l(Om);VA=u(sG,"token_str"),sG.forEach(t),tG.forEach(t),XA=h(Aw),Zp=o(Aw,"TD",{align:!0});var aG=l(Zp);QA=u(aG,"The string representation of the token"),aG.forEach(t),Aw.forEach(t),xa.forEach(t),yw.forEach(t),t1=h(a),Je=o(a,"H2",{class:!0});var Dw=l(Je);Qs=o(Dw,"A",{id:!0,class:!0,href:!0});var nG=l(Qs);xm=o(nG,"SPAN",{});var rG=l(xm);v(Jo.$$.fragment,rG),rG.forEach(t),nG.forEach(t),ZA=h(Dw),Rm=o(Dw,"SPAN",{});var oG=l(Rm);eD=u(oG,"Automatic speech recognition task"),oG.forEach(t),Dw.forEach(t),s1=h(a),ef=o(a,"P",{});var lG=l(ef);tD=u(lG,`This task reads some audio input and outputs the said words within the
audio files.`),lG.forEach(t),a1=h(a),v(Zs.$$.fragment,a),n1=h(a),v(ea.$$.fragment,a),r1=h(a),ce=o(a,"P",{});var Ul=l(ce);sD=u(Ul,"Available with: "),Wo=o(Ul,"A",{href:!0,rel:!0});var iG=l(Wo);aD=u(iG,"\u{1F917} Transformers"),iG.forEach(t),nD=h(Ul),Yo=o(Ul,"A",{href:!0,rel:!0});var uG=l(Yo);rD=u(uG,"ESPnet"),uG.forEach(t),oD=u(Ul,` and
`),Ko=o(Ul,"A",{href:!0,rel:!0});var cG=l(Ko);lD=u(cG,"SpeechBrain"),cG.forEach(t),Ul.forEach(t),o1=h(a),tf=o(a,"P",{});var pG=l(tf);iD=u(pG,"Request:"),pG.forEach(t),l1=h(a),v(ta.$$.fragment,a),i1=h(a),sf=o(a,"P",{});var fG=l(sf);uD=u(fG,`When sending your request, you should send a binary payload that simply
contains your audio file. We try to support most formats (Flac, Wav,
Mp3, Ogg etc...). And we automatically rescale the sampling rate to the
appropriate rate for the given model (usually 16KHz).`),fG.forEach(t),u1=h(a),sa=o(a,"TABLE",{});var Ow=l(sa);Nm=o(Ow,"THEAD",{});var hG=l(Nm);Vo=o(hG,"TR",{});var xw=l(Vo);af=o(xw,"TH",{align:!0});var dG=l(af);cD=u(dG,"All parameters"),dG.forEach(t),pD=h(xw),Pm=o(xw,"TH",{align:!0}),l(Pm).forEach(t),xw.forEach(t),hG.forEach(t),fD=h(Ow),Sm=o(Ow,"TBODY",{});var gG=l(Sm);Xo=o(gG,"TR",{});var Rw=l(Xo);Qo=o(Rw,"TD",{align:!0});var Ix=l(Qo);Im=o(Ix,"STRONG",{});var mG=l(Im);hD=u(mG,"no parameter"),mG.forEach(t),dD=u(Ix," (required)"),Ix.forEach(t),gD=h(Rw),nf=o(Rw,"TD",{align:!0});var $G=l(nf);mD=u($G,"a binary representation of the audio file. No other parameters are currently allowed."),$G.forEach(t),Rw.forEach(t),gG.forEach(t),Ow.forEach(t),c1=h(a),rf=o(a,"P",{});var qG=l(rf);$D=u(qG,"Return value is either a dict or a list of dicts if you sent a list of inputs"),qG.forEach(t),p1=h(a),of=o(a,"P",{});var _G=l(of);qD=u(_G,"Response:"),_G.forEach(t),f1=h(a),v(aa.$$.fragment,a),h1=h(a),na=o(a,"TABLE",{});var Nw=l(na);Hm=o(Nw,"THEAD",{});var vG=l(Hm);Zo=o(vG,"TR",{});var Pw=l(Zo);lf=o(Pw,"TH",{align:!0});var EG=l(lf);_D=u(EG,"Returned values"),EG.forEach(t),vD=h(Pw),Bm=o(Pw,"TH",{align:!0}),l(Bm).forEach(t),Pw.forEach(t),vG.forEach(t),ED=h(Nw),Gm=o(Nw,"TBODY",{});var wG=l(Gm);el=o(wG,"TR",{});var Sw=l(el);uf=o(Sw,"TD",{align:!0});var yG=l(uf);Cm=o(yG,"STRONG",{});var bG=l(Cm);wD=u(bG,"text"),bG.forEach(t),yG.forEach(t),yD=h(Sw),cf=o(Sw,"TD",{align:!0});var jG=l(cf);bD=u(jG,"The string that was recognized within the audio file."),jG.forEach(t),Sw.forEach(t),wG.forEach(t),Nw.forEach(t),d1=h(a),We=o(a,"H2",{class:!0});var Iw=l(We);ra=o(Iw,"A",{id:!0,class:!0,href:!0});var TG=l(ra);Mm=o(TG,"SPAN",{});var kG=l(Mm);v(tl.$$.fragment,kG),kG.forEach(t),TG.forEach(t),jD=h(Iw),Fm=o(Iw,"SPAN",{});var AG=l(Fm);TD=u(AG,"Feature-extraction task"),AG.forEach(t),Iw.forEach(t),g1=h(a),pf=o(a,"P",{});var DG=l(pf);kD=u(DG,`This task reads some text and outputs raw float values, that usually
consumed as part of a semantic database/semantic search.`),DG.forEach(t),m1=h(a),v(oa.$$.fragment,a),$1=h(a),Ye=o(a,"P",{});var W$=l(Ye);AD=u(W$,"Available with: "),sl=o(W$,"A",{href:!0,rel:!0});var OG=l(sl);DD=u(OG,"\u{1F917} Transformers"),OG.forEach(t),OD=h(W$),al=o(W$,"A",{href:!0,rel:!0});var xG=l(al);xD=u(xG,"Sentence-transformers"),xG.forEach(t),W$.forEach(t),q1=h(a),ff=o(a,"P",{});var RG=l(ff);RD=u(RG,"Request:"),RG.forEach(t),_1=h(a),la=o(a,"TABLE",{});var Hw=l(la);Um=o(Hw,"THEAD",{});var NG=l(Um);nl=o(NG,"TR",{});var Bw=l(nl);hf=o(Bw,"TH",{align:!0});var PG=l(hf);ND=u(PG,"All parameters"),PG.forEach(t),PD=h(Bw),Lm=o(Bw,"TH",{align:!0}),l(Lm).forEach(t),Bw.forEach(t),NG.forEach(t),SD=h(Hw),te=o(Hw,"TBODY",{});var Ae=l(te);rl=o(Ae,"TR",{});var Gw=l(rl);ol=o(Gw,"TD",{align:!0});var Hx=l(ol);zm=o(Hx,"STRONG",{});var SG=l(zm);ID=u(SG,"inputs"),SG.forEach(t),HD=u(Hx," (required):"),Hx.forEach(t),BD=h(Gw),df=o(Gw,"TD",{align:!0});var IG=l(df);GD=u(IG,"a string or a list of strings to get the features from."),IG.forEach(t),Gw.forEach(t),CD=h(Ae),ll=o(Ae,"TR",{});var Cw=l(ll);gf=o(Cw,"TD",{align:!0});var HG=l(gf);Jm=o(HG,"STRONG",{});var BG=l(Jm);MD=u(BG,"options"),BG.forEach(t),HG.forEach(t),FD=h(Cw),mf=o(Cw,"TD",{align:!0});var GG=l(mf);UD=u(GG,"a dict containing the following keys:"),GG.forEach(t),Cw.forEach(t),LD=h(Ae),il=o(Ae,"TR",{});var Mw=l(il);$f=o(Mw,"TD",{align:!0});var CG=l($f);zD=u(CG,"use_gpu"),CG.forEach(t),JD=h(Mw),ia=o(Mw,"TD",{align:!0});var Fw=l(ia);WD=u(Fw,"(Default: "),Wm=o(Fw,"CODE",{});var MG=l(Wm);YD=u(MG,"false"),MG.forEach(t),KD=u(Fw,"). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),Fw.forEach(t),Mw.forEach(t),VD=h(Ae),ul=o(Ae,"TR",{});var Uw=l(ul);qf=o(Uw,"TD",{align:!0});var FG=l(qf);XD=u(FG,"use_cache"),FG.forEach(t),QD=h(Uw),ua=o(Uw,"TD",{align:!0});var Lw=l(ua);ZD=u(Lw,"(Default: "),Ym=o(Lw,"CODE",{});var UG=l(Ym);eO=u(UG,"true"),UG.forEach(t),tO=u(Lw,"). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),Lw.forEach(t),Uw.forEach(t),sO=h(Ae),cl=o(Ae,"TR",{});var zw=l(cl);_f=o(zw,"TD",{align:!0});var LG=l(_f);aO=u(LG,"wait_for_model"),LG.forEach(t),nO=h(zw),ca=o(zw,"TD",{align:!0});var Jw=l(ca);rO=u(Jw,"(Default: "),Km=o(Jw,"CODE",{});var zG=l(Km);oO=u(zG,"false"),zG.forEach(t),lO=u(Jw,") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),Jw.forEach(t),zw.forEach(t),Ae.forEach(t),Hw.forEach(t),v1=h(a),vf=o(a,"P",{});var JG=l(vf);iO=u(JG,"Return value is either a dict or a list of dicts if you sent a list of inputs"),JG.forEach(t),E1=h(a),pa=o(a,"TABLE",{});var Ww=l(pa);Vm=o(Ww,"THEAD",{});var WG=l(Vm);pl=o(WG,"TR",{});var Yw=l(pl);Ef=o(Yw,"TH",{align:!0});var YG=l(Ef);uO=u(YG,"Returned values"),YG.forEach(t),cO=h(Yw),Xm=o(Yw,"TH",{align:!0}),l(Xm).forEach(t),Yw.forEach(t),WG.forEach(t),pO=h(Ww),Qm=o(Ww,"TBODY",{});var KG=l(Qm);fl=o(KG,"TR",{});var Kw=l(fl);wf=o(Kw,"TD",{align:!0});var VG=l(wf);Zm=o(VG,"STRONG",{});var XG=l(Zm);fO=u(XG,"A list of float (or list of list of floats)"),XG.forEach(t),VG.forEach(t),hO=h(Kw),yf=o(Kw,"TD",{align:!0});var QG=l(yf);dO=u(QG,"The numbers that are the representation features of the input."),QG.forEach(t),Kw.forEach(t),KG.forEach(t),Ww.forEach(t),w1=h(a),bf=o(a,"SMALL",{});var ZG=l(bf);gO=u(ZG,`Returned values are a list of floats, or a list of list of floats
(depending on if you sent a string or a list of string, and if the
automatic reduction, usually mean_pooling for instance was applied for
you or not. This should be explained on the model's README.`),ZG.forEach(t),y1=h(a),Ke=o(a,"H2",{class:!0});var Vw=l(Ke);fa=o(Vw,"A",{id:!0,class:!0,href:!0});var eC=l(fa);e$=o(eC,"SPAN",{});var tC=l(e$);v(hl.$$.fragment,tC),tC.forEach(t),eC.forEach(t),mO=h(Vw),t$=o(Vw,"SPAN",{});var sC=l(t$);$O=u(sC,"Audio-classification task"),sC.forEach(t),Vw.forEach(t),b1=h(a),jf=o(a,"P",{});var aC=l(jf);qO=u(aC,"This task reads some audio input and outputs the likelihood of classes."),aC.forEach(t),j1=h(a),v(ha.$$.fragment,a),T1=h(a),Ve=o(a,"P",{});var Y$=l(Ve);_O=u(Y$,"Available with: "),dl=o(Y$,"A",{href:!0,rel:!0});var nC=l(dl);vO=u(nC,"\u{1F917} Transformers"),nC.forEach(t),EO=h(Y$),gl=o(Y$,"A",{href:!0,rel:!0});var rC=l(gl);wO=u(rC,"SpeechBrain"),rC.forEach(t),Y$.forEach(t),k1=h(a),Tf=o(a,"P",{});var oC=l(Tf);yO=u(oC,"Request:"),oC.forEach(t),A1=h(a),v(da.$$.fragment,a),D1=h(a),kf=o(a,"P",{});var lC=l(kf);bO=u(lC,`When sending your request, you should send a binary payload that simply
contains your audio file. We try to support most formats (Flac, Wav,
Mp3, Ogg etc...). And we automatically rescale the sampling rate to the
appropriate rate for the given model (usually 16KHz).`),lC.forEach(t),O1=h(a),ga=o(a,"TABLE",{});var Xw=l(ga);s$=o(Xw,"THEAD",{});var iC=l(s$);ml=o(iC,"TR",{});var Qw=l(ml);Af=o(Qw,"TH",{align:!0});var uC=l(Af);jO=u(uC,"All parameters"),uC.forEach(t),TO=h(Qw),a$=o(Qw,"TH",{align:!0}),l(a$).forEach(t),Qw.forEach(t),iC.forEach(t),kO=h(Xw),n$=o(Xw,"TBODY",{});var cC=l(n$);$l=o(cC,"TR",{});var Zw=l($l);ql=o(Zw,"TD",{align:!0});var Bx=l(ql);r$=o(Bx,"STRONG",{});var pC=l(r$);AO=u(pC,"no parameter"),pC.forEach(t),DO=u(Bx," (required)"),Bx.forEach(t),OO=h(Zw),Df=o(Zw,"TD",{align:!0});var fC=l(Df);xO=u(fC,"a binary representation of the audio file. No other parameters are currently allowed."),fC.forEach(t),Zw.forEach(t),cC.forEach(t),Xw.forEach(t),x1=h(a),Of=o(a,"P",{});var hC=l(Of);RO=u(hC,"Return value is a dict"),hC.forEach(t),R1=h(a),v(ma.$$.fragment,a),N1=h(a),$a=o(a,"TABLE",{});var ey=l($a);o$=o(ey,"THEAD",{});var dC=l(o$);_l=o(dC,"TR",{});var ty=l(_l);xf=o(ty,"TH",{align:!0});var gC=l(xf);NO=u(gC,"Returned values"),gC.forEach(t),PO=h(ty),l$=o(ty,"TH",{align:!0}),l(l$).forEach(t),ty.forEach(t),dC.forEach(t),SO=h(ey),vl=o(ey,"TBODY",{});var sy=l(vl);El=o(sy,"TR",{});var ay=l(El);Rf=o(ay,"TD",{align:!0});var mC=l(Rf);i$=o(mC,"STRONG",{});var $C=l(i$);IO=u($C,"label"),$C.forEach(t),mC.forEach(t),HO=h(ay),Nf=o(ay,"TD",{align:!0});var qC=l(Nf);BO=u(qC,"The label for the class (model specific)"),qC.forEach(t),ay.forEach(t),GO=h(sy),wl=o(sy,"TR",{});var ny=l(wl);Pf=o(ny,"TD",{align:!0});var _C=l(Pf);u$=o(_C,"STRONG",{});var vC=l(u$);CO=u(vC,"score"),vC.forEach(t),_C.forEach(t),MO=h(ny),Sf=o(ny,"TD",{align:!0});var EC=l(Sf);FO=u(EC,"A floats that represents how likely is that the audio file belongs the this class."),EC.forEach(t),ny.forEach(t),sy.forEach(t),ey.forEach(t),P1=h(a),Xe=o(a,"H2",{class:!0});var ry=l(Xe);qa=o(ry,"A",{id:!0,class:!0,href:!0});var wC=l(qa);c$=o(wC,"SPAN",{});var yC=l(c$);v(yl.$$.fragment,yC),yC.forEach(t),wC.forEach(t),UO=h(ry),p$=o(ry,"SPAN",{});var bC=l(p$);LO=u(bC,"Object-detection task"),bC.forEach(t),ry.forEach(t),S1=h(a),If=o(a,"P",{});var jC=l(If);zO=u(jC,`This task reads some image input and outputs the likelihood of classes &
bounding boxes of detected objects.`),jC.forEach(t),I1=h(a),v(_a.$$.fragment,a),H1=h(a),bl=o(a,"P",{});var Gx=l(bl);JO=u(Gx,"Available with: "),jl=o(Gx,"A",{href:!0,rel:!0});var TC=l(jl);WO=u(TC,"\u{1F917} Transformers"),TC.forEach(t),Gx.forEach(t),B1=h(a),Hf=o(a,"P",{});var kC=l(Hf);YO=u(kC,"Request:"),kC.forEach(t),G1=h(a),v(va.$$.fragment,a),C1=h(a),Ea=o(a,"P",{});var oy=l(Ea);KO=u(oy,`When sending your request, you should send a binary payload that simply
contains your image file. We support all image formats `),Tl=o(oy,"A",{href:!0,rel:!0});var AC=l(Tl);VO=u(AC,`Pillow
supports`),AC.forEach(t),XO=u(oy,"."),oy.forEach(t),M1=h(a),wa=o(a,"TABLE",{});var ly=l(wa);f$=o(ly,"THEAD",{});var DC=l(f$);kl=o(DC,"TR",{});var iy=l(kl);Bf=o(iy,"TH",{align:!0});var OC=l(Bf);QO=u(OC,"All parameters"),OC.forEach(t),ZO=h(iy),h$=o(iy,"TH",{align:!0}),l(h$).forEach(t),iy.forEach(t),DC.forEach(t),ex=h(ly),d$=o(ly,"TBODY",{});var xC=l(d$);Al=o(xC,"TR",{});var uy=l(Al);Dl=o(uy,"TD",{align:!0});var Cx=l(Dl);g$=o(Cx,"STRONG",{});var RC=l(g$);tx=u(RC,"no parameter"),RC.forEach(t),sx=u(Cx," (required)"),Cx.forEach(t),ax=h(uy),Gf=o(uy,"TD",{align:!0});var NC=l(Gf);nx=u(NC,"a binary representation of the image file. No other parameters are currently allowed."),NC.forEach(t),uy.forEach(t),xC.forEach(t),ly.forEach(t),F1=h(a),Cf=o(a,"P",{});var PC=l(Cf);rx=u(PC,"Return value is a dict"),PC.forEach(t),U1=h(a),v(ya.$$.fragment,a),L1=h(a),ba=o(a,"TABLE",{});var cy=l(ba);m$=o(cy,"THEAD",{});var SC=l(m$);Ol=o(SC,"TR",{});var py=l(Ol);Mf=o(py,"TH",{align:!0});var IC=l(Mf);ox=u(IC,"Returned values"),IC.forEach(t),lx=h(py),$$=o(py,"TH",{align:!0}),l($$).forEach(t),py.forEach(t),SC.forEach(t),ix=h(cy),Qe=o(cy,"TBODY",{});var ch=l(Qe);xl=o(ch,"TR",{});var fy=l(xl);Ff=o(fy,"TD",{align:!0});var HC=l(Ff);q$=o(HC,"STRONG",{});var BC=l(q$);ux=u(BC,"label"),BC.forEach(t),HC.forEach(t),cx=h(fy),Uf=o(fy,"TD",{align:!0});var GC=l(Uf);px=u(GC,"The label for the class (model specific) of a detected object."),GC.forEach(t),fy.forEach(t),fx=h(ch),Rl=o(ch,"TR",{});var hy=l(Rl);Lf=o(hy,"TD",{align:!0});var CC=l(Lf);_$=o(CC,"STRONG",{});var MC=l(_$);hx=u(MC,"score"),MC.forEach(t),CC.forEach(t),dx=h(hy),zf=o(hy,"TD",{align:!0});var FC=l(zf);gx=u(FC,"A float that represents how likely it is that the detected object belongs to the given class."),FC.forEach(t),hy.forEach(t),mx=h(ch),Nl=o(ch,"TR",{});var dy=l(Nl);Jf=o(dy,"TD",{align:!0});var UC=l(Jf);v$=o(UC,"STRONG",{});var LC=l(v$);$x=u(LC,"box"),LC.forEach(t),UC.forEach(t),qx=h(dy),Wf=o(dy,"TD",{align:!0});var zC=l(Wf);_x=u(zC,"A dict (with keys [xmin,ymin,xmax,ymax]) representing the bounding box of a detected object."),zC.forEach(t),dy.forEach(t),ch.forEach(t),cy.forEach(t),this.h()},h(){p(n,"name","hf:doc:metadata"),p(n,"content",JSON.stringify(nU)),p(d,"id","detailed-parameters"),p(d,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(d,"href","#detailed-parameters"),p(s,"class","relative group"),p(se,"id","which-task-is-used-by-this-model"),p(se,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(se,"href","#which-task-is-used-by-this-model"),p(D,"class","relative group"),p(tt,"class","block dark:hidden"),JC(tt.src,Mx="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/inference-api/task.png")||p(tt,"src",Mx),p(tt,"width","300"),p(st,"class","hidden dark:block invert"),JC(st.src,Fx="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/inference-api/task-dark.png")||p(st,"src",Fx),p(st,"width","300"),p(at,"id","zeroshot-classification-task"),p(at,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(at,"href","#zeroshot-classification-task"),p(xe,"class","relative group"),p(Sa,"href","https://github.com/huggingface/transformers"),p(Sa,"rel","nofollow"),p(Kl,"align","left"),p(dh,"align","left"),p(Ba,"align","left"),p(Vl,"align","left"),p(Ca,"align","left"),p(Xl,"align","left"),p(Ql,"align","left"),p(Zl,"class","incremental"),p(pe,"align","left"),p(ei,"align","left"),p(lt,"align","left"),p(ti,"align","left"),p(si,"align","left"),p(ai,"align","left"),p(it,"align","left"),p(ni,"align","left"),p(ut,"align","left"),p(ri,"align","left"),p(ct,"align","left"),p(ii,"align","left"),p(bh,"align","left"),p(ui,"align","left"),p(ci,"align","left"),p(pi,"align","left"),p(fi,"align","left"),p(hi,"align","left"),p(ht,"align","left"),p(dt,"id","translation-task"),p(dt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(dt,"href","#translation-task"),p(Ne,"class","relative group"),p(Za,"href","https://github.com/huggingface/transformers"),p(Za,"rel","nofollow"),p($i,"align","left"),p(Rh,"align","left"),p(sn,"align","left"),p(qi,"align","left"),p(_i,"align","left"),p(vi,"align","left"),p(Ei,"align","left"),p(qt,"align","left"),p(wi,"align","left"),p(_t,"align","left"),p(yi,"align","left"),p(vt,"align","left"),p(ji,"align","left"),p(Gh,"align","left"),p(Ti,"align","left"),p(ki,"align","left"),p(wt,"id","summarization-task"),p(wt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(wt,"href","#summarization-task"),p(Pe,"class","relative group"),p(Ai,"href","mailto:api-enterprise@huggingface.co"),p(fn,"href","https://github.com/huggingface/transformers"),p(fn,"rel","nofollow"),p(xi,"align","left"),p(zh,"align","left"),p(gn,"align","left"),p(Ri,"align","left"),p(Ni,"align","left"),p(Pi,"align","left"),p(Si,"align","left"),p(fe,"align","left"),p(Ii,"align","left"),p(he,"align","left"),p(Hi,"align","left"),p(Bi,"class","incremental"),p(de,"align","left"),p(Gi,"align","left"),p(Ci,"class","incremental"),p(ge,"align","left"),p(Mi,"align","left"),p(Fi,"class","incremental"),p(Ui,"class","incremental"),p(ae,"align","left"),p(Li,"align","left"),p(kt,"align","left"),p(zi,"align","left"),p(At,"align","left"),p(Ji,"align","left"),p(Wi,"align","left"),p(Yi,"align","left"),p(Dt,"align","left"),p(Ki,"align","left"),p(Ot,"align","left"),p(Vi,"align","left"),p(xt,"align","left"),p(Qi,"align","left"),p(id,"align","left"),p(Zi,"align","left"),p(eu,"align","left"),p(Nt,"id","conversational-task"),p(Nt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(Nt,"href","#conversational-task"),p(Se,"class","relative group"),p(Rn,"href","https://github.com/huggingface/transformers"),p(Rn,"rel","nofollow"),p(nu,"align","left"),p(dd,"align","left"),p(Sn,"align","left"),p(md,"align","left"),p(ru,"align","left"),p(ou,"align","left"),p(lu,"align","left"),p(iu,"align","left"),p(uu,"align","left"),p(Ht,"align","left"),p(cu,"align","left"),p(pu,"align","left"),p(fu,"align","left"),p(me,"align","left"),p(hu,"align","left"),p($e,"align","left"),p(du,"align","left"),p(gu,"class","incremental"),p(qe,"align","left"),p(mu,"align","left"),p($u,"class","incremental"),p(_e,"align","left"),p(qu,"align","left"),p(_u,"class","incremental"),p(vu,"class","incremental"),p(ne,"align","left"),p(Eu,"align","left"),p(Bt,"align","left"),p(wu,"align","left"),p(Gt,"align","left"),p(yu,"align","left"),p(bu,"align","left"),p(ju,"align","left"),p(Ct,"align","left"),p(Tu,"align","left"),p(Mt,"align","left"),p(ku,"align","left"),p(Ft,"align","left"),p(Du,"align","left"),p(Nd,"align","left"),p(Ou,"align","left"),p(xu,"align","left"),p(Ru,"align","left"),p(Nu,"align","left"),p(Pu,"align","left"),p(Su,"align","left"),p(Iu,"align","left"),p(Hu,"align","left"),p(Lt,"id","table-question-answering-task"),p(Lt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(Lt,"href","#table-question-answering-task"),p(Ie,"class","relative group"),p(nr,"href","https://github.com/huggingface/transformers"),p(nr,"rel","nofollow"),p(Mu,"align","left"),p(Gd,"align","left"),p(lr,"align","left"),p(Md,"align","left"),p(Fu,"align","left"),p(Uu,"align","left"),p(Lu,"align","left"),p(zu,"align","left"),p(Ju,"align","left"),p(Wu,"align","left"),p(Yu,"align","left"),p(Yt,"align","left"),p(Ku,"align","left"),p(Kt,"align","left"),p(Vu,"align","left"),p(Vt,"align","left"),p(Qu,"align","left"),p(Wd,"align","left"),p(Zu,"align","left"),p(ec,"align","left"),p(tc,"align","left"),p(sc,"align","left"),p(ac,"align","left"),p(nc,"align","left"),p(rc,"align","left"),p(oc,"align","left"),p(Zt,"id","question-answering-task"),p(Zt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(Zt,"href","#question-answering-task"),p(He,"class","relative group"),p(vr,"href","https://github.com/huggingface/transformers"),p(vr,"rel","nofollow"),p(Er,"href","https://github.com/allenai/allennlp"),p(Er,"rel","nofollow"),p(pc,"align","left"),p(tg,"align","left"),p(fc,"align","left"),p(hc,"align","left"),p(dc,"align","left"),p(gc,"align","left"),p(mc,"align","left"),p(ns,"align","left"),p($c,"align","left"),p(rs,"align","left"),p(os,"id","textclassification-task"),p(os,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(os,"href","#textclassification-task"),p(Ge,"class","relative group"),p(Dr,"href","https://github.com/huggingface/transformers"),p(Dr,"rel","nofollow"),p(Ec,"align","left"),p(pg,"align","left"),p(Rr,"align","left"),p(wc,"align","left"),p(yc,"align","left"),p(bc,"align","left"),p(jc,"align","left"),p(cs,"align","left"),p(Tc,"align","left"),p(ps,"align","left"),p(kc,"align","left"),p(fs,"align","left"),p(Dc,"align","left"),p(qg,"align","left"),p(Oc,"align","left"),p(xc,"align","left"),p(Rc,"align","left"),p(Nc,"align","left"),p(gs,"id","named-entity-recognition-ner-task"),p(gs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(gs,"href","#named-entity-recognition-ner-task"),p(Ce,"class","relative group"),p(Pc,"href","#token-classification-task"),p(ms,"id","tokenclassification-task"),p(ms,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(ms,"href","#tokenclassification-task"),p(Me,"class","relative group"),p(Lr,"href","https://github.com/huggingface/transformers"),p(Lr,"rel","nofollow"),p(zr,"href","https://github.com/flairNLP/flair"),p(zr,"rel","nofollow"),p(Bc,"align","left"),p(Tg,"align","left"),p(Yr,"align","left"),p(Gc,"align","left"),p(Cc,"align","left"),p(Mc,"align","left"),p(Fc,"align","left"),p(vs,"align","left"),p(Uc,"align","left"),p(Lc,"align","left"),p(zc,"align","left"),p(Es,"align","left"),p(Jc,"align","left"),p(ws,"align","left"),p(Wc,"align","left"),p(ys,"align","left"),p(Kc,"align","left"),p(Sg,"align","left"),p(Vc,"align","left"),p(Xc,"align","left"),p(Qc,"align","left"),p(Zc,"align","left"),p(ep,"align","left"),p(tp,"align","left"),p(sp,"align","left"),p(Ts,"align","left"),p(ap,"align","left"),p(ks,"align","left"),p(As,"id","textgeneration-task"),p(As,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(As,"href","#textgeneration-task"),p(Ue,"class","relative group"),p(uo,"href","https://github.com/huggingface/transformers"),p(uo,"rel","nofollow"),p(lp,"align","left"),p(Jg,"align","left"),p(fo,"align","left"),p(ip,"align","left"),p(up,"align","left"),p(cp,"align","left"),p(pp,"align","left"),p(fp,"class","incremental"),p(ve,"align","left"),p(hp,"align","left"),p(dp,"class","incremental"),p(Ee,"align","left"),p(gp,"align","left"),p(mp,"class","incremental"),p($p,"class","incremental"),p(re,"align","left"),p(qp,"align","left"),p(Rs,"align","left"),p(_p,"align","left"),p(we,"align","left"),p(vp,"align","left"),p(Ns,"align","left"),p(Ep,"align","left"),p(ye,"align","left"),p(wp,"align","left"),p(Ps,"align","left"),p(yp,"align","left"),p(Ss,"align","left"),p(bp,"align","left"),p(jp,"align","left"),p(Tp,"align","left"),p(Is,"align","left"),p(kp,"align","left"),p(Hs,"align","left"),p(Ap,"align","left"),p(Bs,"align","left"),p(Op,"align","left"),p(pm,"align","left"),p(xp,"align","left"),p(Rp,"align","left"),p(Ms,"id","text2textgeneration-task"),p(Ms,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(Ms,"href","#text2textgeneration-task"),p(Le,"class","relative group"),p(Np,"href","#text-generation-task"),p(Us,"id","fill-mask-task"),p(Us,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(Us,"href","#fill-mask-task"),p(ze,"class","relative group"),p(No,"href","https://github.com/huggingface/transformers"),p(No,"rel","nofollow"),p(Hp,"align","left"),p(_m,"align","left"),p(Io,"align","left"),p(Bp,"align","left"),p(Gp,"align","left"),p(Cp,"align","left"),p(Mp,"align","left"),p(Ws,"align","left"),p(Fp,"align","left"),p(Ys,"align","left"),p(Up,"align","left"),p(Ks,"align","left"),p(zp,"align","left"),p(Tm,"align","left"),p(Jp,"align","left"),p(Wp,"align","left"),p(Yp,"align","left"),p(Kp,"align","left"),p(Vp,"align","left"),p(Xp,"align","left"),p(Qp,"align","left"),p(Zp,"align","left"),p(Qs,"id","automatic-speech-recognition-task"),p(Qs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(Qs,"href","#automatic-speech-recognition-task"),p(Je,"class","relative group"),p(Wo,"href","https://github.com/huggingface/transformers"),p(Wo,"rel","nofollow"),p(Yo,"href","https://github.com/espnet/espnet"),p(Yo,"rel","nofollow"),p(Ko,"href","https://github.com/speechbrain/speechbrain"),p(Ko,"rel","nofollow"),p(af,"align","left"),p(Pm,"align","left"),p(Qo,"align","left"),p(nf,"align","left"),p(lf,"align","left"),p(Bm,"align","left"),p(uf,"align","left"),p(cf,"align","left"),p(ra,"id","featureextraction-task"),p(ra,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(ra,"href","#featureextraction-task"),p(We,"class","relative group"),p(sl,"href","https://github.com/huggingface/transformers"),p(sl,"rel","nofollow"),p(al,"href","https://github.com/UKPLab/sentence-transformers"),p(al,"rel","nofollow"),p(hf,"align","left"),p(Lm,"align","left"),p(ol,"align","left"),p(df,"align","left"),p(gf,"align","left"),p(mf,"align","left"),p($f,"align","left"),p(ia,"align","left"),p(qf,"align","left"),p(ua,"align","left"),p(_f,"align","left"),p(ca,"align","left"),p(Ef,"align","left"),p(Xm,"align","left"),p(wf,"align","left"),p(yf,"align","left"),p(fa,"id","audioclassification-task"),p(fa,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(fa,"href","#audioclassification-task"),p(Ke,"class","relative group"),p(dl,"href","https://github.com/huggingface/transformers"),p(dl,"rel","nofollow"),p(gl,"href","https://github.com/speechbrain/speechbrain"),p(gl,"rel","nofollow"),p(Af,"align","left"),p(a$,"align","left"),p(ql,"align","left"),p(Df,"align","left"),p(xf,"align","left"),p(l$,"align","left"),p(Rf,"align","left"),p(Nf,"align","left"),p(Pf,"align","left"),p(Sf,"align","left"),p(qa,"id","objectdetection-task"),p(qa,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(qa,"href","#objectdetection-task"),p(Xe,"class","relative group"),p(jl,"href","https://github.com/huggingface/transformers"),p(jl,"rel","nofollow"),p(Tl,"href","https://pillow.readthedocs.io/en/stable/handbook/image-file-formats.html"),p(Tl,"rel","nofollow"),p(Bf,"align","left"),p(h$,"align","left"),p(Dl,"align","left"),p(Gf,"align","left"),p(Mf,"align","left"),p($$,"align","left"),p(Ff,"align","left"),p(Uf,"align","left"),p(Lf,"align","left"),p(zf,"align","left"),p(Jf,"align","left"),p(Wf,"align","left")},m(a,g){e(document.head,n),m(a,c,g),m(a,s,g),e(s,d),e(d,$),E(k,$,null),e(s,A),e(s,T),e(T,j),m(a,O,g),m(a,D,g),e(D,se),e(se,De),E(V,De,null),e(D,J),e(D,et),e(et,Ll),m(a,Ra,g),m(a,Oe,g),e(Oe,gy),m(a,K$,g),m(a,zl,g),e(zl,my),m(a,V$,g),m(a,tt,g),m(a,X$,g),m(a,st,g),m(a,Q$,g),m(a,xe,g),e(xe,at),e(at,ph),E(Na,ph,null),e(xe,$y),e(xe,fh),e(fh,qy),m(a,Z$,g),m(a,Jl,g),e(Jl,_y),m(a,eq,g),E(nt,a,g),m(a,tq,g),m(a,Pa,g),e(Pa,vy),e(Pa,Sa),e(Sa,Ey),m(a,sq,g),m(a,Wl,g),e(Wl,wy),m(a,aq,g),E(rt,a,g),m(a,nq,g),m(a,Yl,g),e(Yl,yy),m(a,rq,g),m(a,ot,g),e(ot,hh),e(hh,Ia),e(Ia,Kl),e(Kl,by),e(Ia,jy),e(Ia,dh),e(ot,Ty),e(ot,M),e(M,Ha),e(Ha,Ba),e(Ba,gh),e(gh,ky),e(Ba,Ay),e(Ha,Dy),e(Ha,Vl),e(Vl,Oy),e(M,xy),e(M,Ga),e(Ga,Ca),e(Ca,mh),e(mh,Ry),e(Ca,Ny),e(Ga,Py),e(Ga,Xl),e(Xl,Sy),e(M,Iy),e(M,Ma),e(Ma,Ql),e(Ql,Hy),e(Ma,By),e(Ma,pe),e(pe,Gy),e(pe,$h),e($h,Cy),e(pe,My),e(pe,Zl),e(Zl,Fy),e(pe,Uy),e(M,Ly),e(M,Fa),e(Fa,ei),e(ei,zy),e(Fa,Jy),e(Fa,lt),e(lt,Wy),e(lt,qh),e(qh,Yy),e(lt,Ky),e(M,Vy),e(M,Ua),e(Ua,ti),e(ti,_h),e(_h,Xy),e(Ua,Qy),e(Ua,si),e(si,Zy),e(M,eb),e(M,La),e(La,ai),e(ai,tb),e(La,sb),e(La,it),e(it,ab),e(it,vh),e(vh,nb),e(it,rb),e(M,ob),e(M,za),e(za,ni),e(ni,lb),e(za,ib),e(za,ut),e(ut,ub),e(ut,Eh),e(Eh,cb),e(ut,pb),e(M,fb),e(M,Ja),e(Ja,ri),e(ri,hb),e(Ja,db),e(Ja,ct),e(ct,gb),e(ct,wh),e(wh,mb),e(ct,$b),m(a,oq,g),m(a,oi,g),e(oi,qb),m(a,lq,g),m(a,li,g),e(li,_b),m(a,iq,g),E(pt,a,g),m(a,uq,g),m(a,ft,g),e(ft,yh),e(yh,Wa),e(Wa,ii),e(ii,vb),e(Wa,Eb),e(Wa,bh),e(ft,wb),e(ft,Re),e(Re,Ya),e(Ya,ui),e(ui,jh),e(jh,yb),e(Ya,bb),e(Ya,ci),e(ci,jb),e(Re,Tb),e(Re,Ka),e(Ka,pi),e(pi,Th),e(Th,kb),e(Ka,Ab),e(Ka,fi),e(fi,Db),e(Re,Ob),e(Re,Va),e(Va,hi),e(hi,kh),e(kh,xb),e(Va,Rb),e(Va,ht),e(ht,Nb),e(ht,Ah),e(Ah,Pb),e(ht,Sb),m(a,cq,g),m(a,Ne,g),e(Ne,dt),e(dt,Dh),E(Xa,Dh,null),e(Ne,Ib),e(Ne,Oh),e(Oh,Hb),m(a,pq,g),m(a,di,g),e(di,Bb),m(a,fq,g),E(gt,a,g),m(a,hq,g),m(a,Qa,g),e(Qa,Gb),e(Qa,Za),e(Za,Cb),m(a,dq,g),m(a,gi,g),e(gi,Mb),m(a,gq,g),E(mt,a,g),m(a,mq,g),m(a,mi,g),e(mi,Fb),m(a,$q,g),m(a,$t,g),e($t,xh),e(xh,en),e(en,$i),e($i,Ub),e(en,Lb),e(en,Rh),e($t,zb),e($t,X),e(X,tn),e(tn,sn),e(sn,Nh),e(Nh,Jb),e(sn,Wb),e(tn,Yb),e(tn,qi),e(qi,Kb),e(X,Vb),e(X,an),e(an,_i),e(_i,Ph),e(Ph,Xb),e(an,Qb),e(an,vi),e(vi,Zb),e(X,e0),e(X,nn),e(nn,Ei),e(Ei,t0),e(nn,s0),e(nn,qt),e(qt,a0),e(qt,Sh),e(Sh,n0),e(qt,r0),e(X,o0),e(X,rn),e(rn,wi),e(wi,l0),e(rn,i0),e(rn,_t),e(_t,u0),e(_t,Ih),e(Ih,c0),e(_t,p0),e(X,f0),e(X,on),e(on,yi),e(yi,h0),e(on,d0),e(on,vt),e(vt,g0),e(vt,Hh),e(Hh,m0),e(vt,$0),m(a,qq,g),m(a,bi,g),e(bi,q0),m(a,_q,g),m(a,Et,g),e(Et,Bh),e(Bh,ln),e(ln,ji),e(ji,_0),e(ln,v0),e(ln,Gh),e(Et,E0),e(Et,Ch),e(Ch,un),e(un,Ti),e(Ti,Mh),e(Mh,w0),e(un,y0),e(un,ki),e(ki,b0),m(a,vq,g),m(a,Pe,g),e(Pe,wt),e(wt,Fh),E(cn,Fh,null),e(Pe,j0),e(Pe,Uh),e(Uh,T0),m(a,Eq,g),m(a,yt,g),e(yt,k0),e(yt,Ai),e(Ai,A0),e(yt,D0),m(a,wq,g),E(bt,a,g),m(a,yq,g),m(a,pn,g),e(pn,O0),e(pn,fn),e(fn,x0),m(a,bq,g),m(a,Di,g),e(Di,R0),m(a,jq,g),E(jt,a,g),m(a,Tq,g),m(a,Oi,g),e(Oi,N0),m(a,kq,g),m(a,Tt,g),e(Tt,Lh),e(Lh,hn),e(hn,xi),e(xi,P0),e(hn,S0),e(hn,zh),e(Tt,I0),e(Tt,B),e(B,dn),e(dn,gn),e(gn,Jh),e(Jh,H0),e(gn,B0),e(dn,G0),e(dn,Ri),e(Ri,C0),e(B,M0),e(B,mn),e(mn,Ni),e(Ni,Wh),e(Wh,F0),e(mn,U0),e(mn,Pi),e(Pi,L0),e(B,z0),e(B,$n),e($n,Si),e(Si,J0),e($n,W0),e($n,fe),e(fe,Y0),e(fe,Yh),e(Yh,K0),e(fe,V0),e(fe,Kh),e(Kh,X0),e(fe,Q0),e(B,Z0),e(B,qn),e(qn,Ii),e(Ii,ej),e(qn,tj),e(qn,he),e(he,sj),e(he,Vh),e(Vh,aj),e(he,nj),e(he,Xh),e(Xh,rj),e(he,oj),e(B,lj),e(B,_n),e(_n,Hi),e(Hi,ij),e(_n,uj),e(_n,de),e(de,cj),e(de,Qh),e(Qh,pj),e(de,fj),e(de,Bi),e(Bi,hj),e(de,dj),e(B,gj),e(B,vn),e(vn,Gi),e(Gi,mj),e(vn,$j),e(vn,ge),e(ge,qj),e(ge,Zh),e(Zh,_j),e(ge,vj),e(ge,Ci),e(Ci,Ej),e(ge,wj),e(B,yj),e(B,En),e(En,Mi),e(Mi,bj),e(En,jj),e(En,ae),e(ae,Tj),e(ae,ed),e(ed,kj),e(ae,Aj),e(ae,Fi),e(Fi,Dj),e(ae,Oj),e(ae,Ui),e(Ui,xj),e(ae,Rj),e(B,Nj),e(B,wn),e(wn,Li),e(Li,Pj),e(wn,Sj),e(wn,kt),e(kt,Ij),e(kt,td),e(td,Hj),e(kt,Bj),e(B,Gj),e(B,yn),e(yn,zi),e(zi,Cj),e(yn,Mj),e(yn,At),e(At,Fj),e(At,sd),e(sd,Uj),e(At,Lj),e(B,zj),e(B,bn),e(bn,Ji),e(Ji,ad),e(ad,Jj),e(bn,Wj),e(bn,Wi),e(Wi,Yj),e(B,Kj),e(B,jn),e(jn,Yi),e(Yi,Vj),e(jn,Xj),e(jn,Dt),e(Dt,Qj),e(Dt,nd),e(nd,Zj),e(Dt,e3),e(B,t3),e(B,Tn),e(Tn,Ki),e(Ki,s3),e(Tn,a3),e(Tn,Ot),e(Ot,n3),e(Ot,rd),e(rd,r3),e(Ot,o3),e(B,l3),e(B,kn),e(kn,Vi),e(Vi,i3),e(kn,u3),e(kn,xt),e(xt,c3),e(xt,od),e(od,p3),e(xt,f3),m(a,Aq,g),m(a,Xi,g),e(Xi,h3),m(a,Dq,g),m(a,Rt,g),e(Rt,ld),e(ld,An),e(An,Qi),e(Qi,d3),e(An,g3),e(An,id),e(Rt,m3),e(Rt,ud),e(ud,Dn),e(Dn,Zi),e(Zi,cd),e(cd,$3),e(Dn,q3),e(Dn,eu),e(eu,_3),m(a,Oq,g),m(a,Se,g),e(Se,Nt),e(Nt,pd),E(On,pd,null),e(Se,v3),e(Se,fd),e(fd,E3),m(a,xq,g),m(a,tu,g),e(tu,w3),m(a,Rq,g),E(Pt,a,g),m(a,Nq,g),m(a,xn,g),e(xn,y3),e(xn,Rn),e(Rn,b3),m(a,Pq,g),m(a,su,g),e(su,j3),m(a,Sq,g),E(St,a,g),m(a,Iq,g),m(a,au,g),e(au,T3),m(a,Hq,g),m(a,It,g),e(It,hd),e(hd,Nn),e(Nn,nu),e(nu,k3),e(Nn,A3),e(Nn,dd),e(It,D3),e(It,P),e(P,Pn),e(Pn,Sn),e(Sn,gd),e(gd,O3),e(Sn,x3),e(Pn,R3),e(Pn,md),e(P,N3),e(P,In),e(In,ru),e(ru,P3),e(In,S3),e(In,ou),e(ou,I3),e(P,H3),e(P,Hn),e(Hn,lu),e(lu,B3),e(Hn,G3),e(Hn,iu),e(iu,C3),e(P,M3),e(P,Bn),e(Bn,uu),e(uu,F3),e(Bn,U3),e(Bn,Ht),e(Ht,L3),e(Ht,$d),e($d,z3),e(Ht,J3),e(P,W3),e(P,Gn),e(Gn,cu),e(cu,qd),e(qd,Y3),e(Gn,K3),e(Gn,pu),e(pu,V3),e(P,X3),e(P,Cn),e(Cn,fu),e(fu,Q3),e(Cn,Z3),e(Cn,me),e(me,eT),e(me,_d),e(_d,tT),e(me,sT),e(me,vd),e(vd,aT),e(me,nT),e(P,rT),e(P,Mn),e(Mn,hu),e(hu,oT),e(Mn,lT),e(Mn,$e),e($e,iT),e($e,Ed),e(Ed,uT),e($e,cT),e($e,wd),e(wd,pT),e($e,fT),e(P,hT),e(P,Fn),e(Fn,du),e(du,dT),e(Fn,gT),e(Fn,qe),e(qe,mT),e(qe,yd),e(yd,$T),e(qe,qT),e(qe,gu),e(gu,_T),e(qe,vT),e(P,ET),e(P,Un),e(Un,mu),e(mu,wT),e(Un,yT),e(Un,_e),e(_e,bT),e(_e,bd),e(bd,jT),e(_e,TT),e(_e,$u),e($u,kT),e(_e,AT),e(P,DT),e(P,Ln),e(Ln,qu),e(qu,OT),e(Ln,xT),e(Ln,ne),e(ne,RT),e(ne,jd),e(jd,NT),e(ne,PT),e(ne,_u),e(_u,ST),e(ne,IT),e(ne,vu),e(vu,HT),e(ne,BT),e(P,GT),e(P,zn),e(zn,Eu),e(Eu,CT),e(zn,MT),e(zn,Bt),e(Bt,FT),e(Bt,Td),e(Td,UT),e(Bt,LT),e(P,zT),e(P,Jn),e(Jn,wu),e(wu,JT),e(Jn,WT),e(Jn,Gt),e(Gt,YT),e(Gt,kd),e(kd,KT),e(Gt,VT),e(P,XT),e(P,Wn),e(Wn,yu),e(yu,Ad),e(Ad,QT),e(Wn,ZT),e(Wn,bu),e(bu,e5),e(P,t5),e(P,Yn),e(Yn,ju),e(ju,s5),e(Yn,a5),e(Yn,Ct),e(Ct,n5),e(Ct,Dd),e(Dd,r5),e(Ct,o5),e(P,l5),e(P,Kn),e(Kn,Tu),e(Tu,i5),e(Kn,u5),e(Kn,Mt),e(Mt,c5),e(Mt,Od),e(Od,p5),e(Mt,f5),e(P,h5),e(P,Vn),e(Vn,ku),e(ku,d5),e(Vn,g5),e(Vn,Ft),e(Ft,m5),e(Ft,xd),e(xd,$5),e(Ft,q5),m(a,Bq,g),m(a,Au,g),e(Au,_5),m(a,Gq,g),m(a,Ut,g),e(Ut,Rd),e(Rd,Xn),e(Xn,Du),e(Du,v5),e(Xn,E5),e(Xn,Nd),e(Ut,w5),e(Ut,oe),e(oe,Qn),e(Qn,Ou),e(Ou,Pd),e(Pd,y5),e(Qn,b5),e(Qn,xu),e(xu,j5),e(oe,T5),e(oe,Zn),e(Zn,Ru),e(Ru,Sd),e(Sd,k5),e(Zn,A5),e(Zn,Nu),e(Nu,D5),e(oe,O5),e(oe,er),e(er,Pu),e(Pu,x5),e(er,R5),e(er,Su),e(Su,N5),e(oe,P5),e(oe,tr),e(tr,Iu),e(Iu,S5),e(tr,I5),e(tr,Hu),e(Hu,H5),m(a,Cq,g),m(a,Ie,g),e(Ie,Lt),e(Lt,Id),E(sr,Id,null),e(Ie,B5),e(Ie,Hd),e(Hd,G5),m(a,Mq,g),m(a,Bu,g),e(Bu,C5),m(a,Fq,g),E(zt,a,g),m(a,Uq,g),m(a,ar,g),e(ar,M5),e(ar,nr),e(nr,F5),m(a,Lq,g),m(a,Gu,g),e(Gu,U5),m(a,zq,g),E(Jt,a,g),m(a,Jq,g),m(a,Cu,g),e(Cu,L5),m(a,Wq,g),m(a,Wt,g),e(Wt,Bd),e(Bd,rr),e(rr,Mu),e(Mu,z5),e(rr,J5),e(rr,Gd),e(Wt,W5),e(Wt,L),e(L,or),e(or,lr),e(lr,Cd),e(Cd,Y5),e(lr,K5),e(or,V5),e(or,Md),e(L,X5),e(L,ir),e(ir,Fu),e(Fu,Q5),e(ir,Z5),e(ir,Uu),e(Uu,e4),e(L,t4),e(L,ur),e(ur,Lu),e(Lu,s4),e(ur,a4),e(ur,zu),e(zu,n4),e(L,r4),e(L,cr),e(cr,Ju),e(Ju,Fd),e(Fd,o4),e(cr,l4),e(cr,Wu),e(Wu,i4),e(L,u4),e(L,pr),e(pr,Yu),e(Yu,c4),e(pr,p4),e(pr,Yt),e(Yt,f4),e(Yt,Ud),e(Ud,h4),e(Yt,d4),e(L,g4),e(L,fr),e(fr,Ku),e(Ku,m4),e(fr,$4),e(fr,Kt),e(Kt,q4),e(Kt,Ld),e(Ld,_4),e(Kt,v4),e(L,E4),e(L,hr),e(hr,Vu),e(Vu,w4),e(hr,y4),e(hr,Vt),e(Vt,b4),e(Vt,zd),e(zd,j4),e(Vt,T4),m(a,Yq,g),m(a,Xu,g),e(Xu,k4),m(a,Kq,g),E(Xt,a,g),m(a,Vq,g),m(a,Qt,g),e(Qt,Jd),e(Jd,dr),e(dr,Qu),e(Qu,A4),e(dr,D4),e(dr,Wd),e(Qt,O4),e(Qt,le),e(le,gr),e(gr,Zu),e(Zu,Yd),e(Yd,x4),e(gr,R4),e(gr,ec),e(ec,N4),e(le,P4),e(le,mr),e(mr,tc),e(tc,Kd),e(Kd,S4),e(mr,I4),e(mr,sc),e(sc,H4),e(le,B4),e(le,$r),e($r,ac),e(ac,Vd),e(Vd,G4),e($r,C4),e($r,nc),e(nc,M4),e(le,F4),e(le,qr),e(qr,rc),e(rc,Xd),e(Xd,U4),e(qr,L4),e(qr,oc),e(oc,z4),m(a,Xq,g),m(a,He,g),e(He,Zt),e(Zt,Qd),E(_r,Qd,null),e(He,J4),e(He,Zd),e(Zd,W4),m(a,Qq,g),m(a,lc,g),e(lc,Y4),m(a,Zq,g),E(es,a,g),m(a,e_,g),m(a,Be,g),e(Be,K4),e(Be,vr),e(vr,V4),e(Be,X4),e(Be,Er),e(Er,Q4),m(a,t_,g),m(a,ic,g),e(ic,Z4),m(a,s_,g),E(ts,a,g),m(a,a_,g),m(a,uc,g),e(uc,ek),m(a,n_,g),m(a,cc,g),e(cc,tk),m(a,r_,g),E(ss,a,g),m(a,o_,g),m(a,as,g),e(as,eg),e(eg,wr),e(wr,pc),e(pc,sk),e(wr,ak),e(wr,tg),e(as,nk),e(as,ie),e(ie,yr),e(yr,fc),e(fc,sg),e(sg,rk),e(yr,ok),e(yr,hc),e(hc,lk),e(ie,ik),e(ie,br),e(br,dc),e(dc,ag),e(ag,uk),e(br,ck),e(br,gc),e(gc,pk),e(ie,fk),e(ie,jr),e(jr,mc),e(mc,ng),e(ng,hk),e(jr,dk),e(jr,ns),e(ns,gk),e(ns,rg),e(rg,mk),e(ns,$k),e(ie,qk),e(ie,Tr),e(Tr,$c),e($c,og),e(og,_k),e(Tr,vk),e(Tr,rs),e(rs,Ek),e(rs,lg),e(lg,wk),e(rs,yk),m(a,l_,g),m(a,Ge,g),e(Ge,os),e(os,ig),E(kr,ig,null),e(Ge,bk),e(Ge,ug),e(ug,jk),m(a,i_,g),m(a,qc,g),e(qc,Tk),m(a,u_,g),E(ls,a,g),m(a,c_,g),m(a,Ar,g),e(Ar,kk),e(Ar,Dr),e(Dr,Ak),m(a,p_,g),m(a,_c,g),e(_c,Dk),m(a,f_,g),E(is,a,g),m(a,h_,g),m(a,vc,g),e(vc,Ok),m(a,d_,g),m(a,us,g),e(us,cg),e(cg,Or),e(Or,Ec),e(Ec,xk),e(Or,Rk),e(Or,pg),e(us,Nk),e(us,Q),e(Q,xr),e(xr,Rr),e(Rr,fg),e(fg,Pk),e(Rr,Sk),e(xr,Ik),e(xr,wc),e(wc,Hk),e(Q,Bk),e(Q,Nr),e(Nr,yc),e(yc,hg),e(hg,Gk),e(Nr,Ck),e(Nr,bc),e(bc,Mk),e(Q,Fk),e(Q,Pr),e(Pr,jc),e(jc,Uk),e(Pr,Lk),e(Pr,cs),e(cs,zk),e(cs,dg),e(dg,Jk),e(cs,Wk),e(Q,Yk),e(Q,Sr),e(Sr,Tc),e(Tc,Kk),e(Sr,Vk),e(Sr,ps),e(ps,Xk),e(ps,gg),e(gg,Qk),e(ps,Zk),e(Q,e7),e(Q,Ir),e(Ir,kc),e(kc,t7),e(Ir,s7),e(Ir,fs),e(fs,a7),e(fs,mg),e(mg,n7),e(fs,r7),m(a,g_,g),m(a,Ac,g),e(Ac,o7),m(a,m_,g),E(hs,a,g),m(a,$_,g),m(a,ds,g),e(ds,$g),e($g,Hr),e(Hr,Dc),e(Dc,l7),e(Hr,i7),e(Hr,qg),e(ds,u7),e(ds,Br),e(Br,Gr),e(Gr,Oc),e(Oc,_g),e(_g,c7),e(Gr,p7),e(Gr,xc),e(xc,f7),e(Br,h7),e(Br,Cr),e(Cr,Rc),e(Rc,vg),e(vg,d7),e(Cr,g7),e(Cr,Nc),e(Nc,m7),m(a,q_,g),m(a,Ce,g),e(Ce,gs),e(gs,Eg),E(Mr,Eg,null),e(Ce,$7),e(Ce,wg),e(wg,q7),m(a,__,g),m(a,Fr,g),e(Fr,_7),e(Fr,Pc),e(Pc,v7),m(a,v_,g),m(a,Me,g),e(Me,ms),e(ms,yg),E(Ur,yg,null),e(Me,E7),e(Me,bg),e(bg,w7),m(a,E_,g),m(a,Sc,g),e(Sc,y7),m(a,w_,g),E($s,a,g),m(a,y_,g),m(a,Fe,g),e(Fe,b7),e(Fe,Lr),e(Lr,j7),e(Fe,T7),e(Fe,zr),e(zr,k7),m(a,b_,g),m(a,Ic,g),e(Ic,A7),m(a,j_,g),E(qs,a,g),m(a,T_,g),m(a,Hc,g),e(Hc,D7),m(a,k_,g),m(a,_s,g),e(_s,jg),e(jg,Jr),e(Jr,Bc),e(Bc,O7),e(Jr,x7),e(Jr,Tg),e(_s,R7),e(_s,z),e(z,Wr),e(Wr,Yr),e(Yr,kg),e(kg,N7),e(Yr,P7),e(Wr,S7),e(Wr,Gc),e(Gc,I7),e(z,H7),e(z,Kr),e(Kr,Cc),e(Cc,Ag),e(Ag,B7),e(Kr,G7),e(Kr,Mc),e(Mc,C7),e(z,M7),e(z,Vr),e(Vr,Fc),e(Fc,F7),e(Vr,U7),e(Vr,vs),e(vs,L7),e(vs,Dg),e(Dg,z7),e(vs,J7),e(z,W7),e(z,Xr),e(Xr,Uc),e(Uc,Og),e(Og,Y7),e(Xr,K7),e(Xr,Lc),e(Lc,V7),e(z,X7),e(z,Qr),e(Qr,zc),e(zc,Q7),e(Qr,Z7),e(Qr,Es),e(Es,e9),e(Es,xg),e(xg,t9),e(Es,s9),e(z,a9),e(z,Zr),e(Zr,Jc),e(Jc,n9),e(Zr,r9),e(Zr,ws),e(ws,o9),e(ws,Rg),e(Rg,l9),e(ws,i9),e(z,u9),e(z,eo),e(eo,Wc),e(Wc,c9),e(eo,p9),e(eo,ys),e(ys,f9),e(ys,Ng),e(Ng,h9),e(ys,d9),m(a,A_,g),m(a,Yc,g),e(Yc,g9),m(a,D_,g),E(bs,a,g),m(a,O_,g),m(a,js,g),e(js,Pg),e(Pg,to),e(to,Kc),e(Kc,m9),e(to,$9),e(to,Sg),e(js,q9),e(js,Z),e(Z,so),e(so,Vc),e(Vc,Ig),e(Ig,_9),e(so,v9),e(so,Xc),e(Xc,E9),e(Z,w9),e(Z,ao),e(ao,Qc),e(Qc,Hg),e(Hg,y9),e(ao,b9),e(ao,Zc),e(Zc,j9),e(Z,T9),e(Z,no),e(no,ep),e(ep,Bg),e(Bg,k9),e(no,A9),e(no,tp),e(tp,D9),e(Z,O9),e(Z,ro),e(ro,sp),e(sp,Gg),e(Gg,x9),e(ro,R9),e(ro,Ts),e(Ts,N9),e(Ts,Cg),e(Cg,P9),e(Ts,S9),e(Z,I9),e(Z,oo),e(oo,ap),e(ap,Mg),e(Mg,H9),e(oo,B9),e(oo,ks),e(ks,G9),e(ks,Fg),e(Fg,C9),e(ks,M9),m(a,x_,g),m(a,Ue,g),e(Ue,As),e(As,Ug),E(lo,Ug,null),e(Ue,F9),e(Ue,Lg),e(Lg,U9),m(a,R_,g),m(a,np,g),e(np,L9),m(a,N_,g),E(Ds,a,g),m(a,P_,g),m(a,io,g),e(io,z9),e(io,uo),e(uo,J9),m(a,S_,g),m(a,rp,g),e(rp,W9),m(a,I_,g),E(Os,a,g),m(a,H_,g),m(a,op,g),e(op,Y9),m(a,B_,g),m(a,xs,g),e(xs,zg),e(zg,co),e(co,lp),e(lp,K9),e(co,V9),e(co,Jg),e(xs,X9),e(xs,S),e(S,po),e(po,fo),e(fo,Wg),e(Wg,Q9),e(fo,Z9),e(po,e6),e(po,ip),e(ip,t6),e(S,s6),e(S,ho),e(ho,up),e(up,Yg),e(Yg,a6),e(ho,n6),e(ho,cp),e(cp,r6),e(S,o6),e(S,go),e(go,pp),e(pp,l6),e(go,i6),e(go,ve),e(ve,u6),e(ve,Kg),e(Kg,c6),e(ve,p6),e(ve,fp),e(fp,f6),e(ve,h6),e(S,d6),e(S,mo),e(mo,hp),e(hp,g6),e(mo,m6),e(mo,Ee),e(Ee,$6),e(Ee,Vg),e(Vg,q6),e(Ee,_6),e(Ee,dp),e(dp,v6),e(Ee,E6),e(S,w6),e(S,$o),e($o,gp),e(gp,y6),e($o,b6),e($o,re),e(re,j6),e(re,Xg),e(Xg,T6),e(re,k6),e(re,mp),e(mp,A6),e(re,D6),e(re,$p),e($p,O6),e(re,x6),e(S,R6),e(S,qo),e(qo,qp),e(qp,N6),e(qo,P6),e(qo,Rs),e(Rs,S6),e(Rs,Qg),e(Qg,I6),e(Rs,H6),e(S,B6),e(S,_o),e(_o,_p),e(_p,G6),e(_o,C6),e(_o,we),e(we,M6),e(we,Zg),e(Zg,F6),e(we,U6),e(we,em),e(em,L6),e(we,z6),e(S,J6),e(S,vo),e(vo,vp),e(vp,W6),e(vo,Y6),e(vo,Ns),e(Ns,K6),e(Ns,tm),e(tm,V6),e(Ns,X6),e(S,Q6),e(S,Eo),e(Eo,Ep),e(Ep,Z6),e(Eo,e8),e(Eo,ye),e(ye,t8),e(ye,sm),e(sm,s8),e(ye,a8),e(ye,am),e(am,n8),e(ye,r8),e(S,o8),e(S,wo),e(wo,wp),e(wp,l8),e(wo,i8),e(wo,Ps),e(Ps,u8),e(Ps,nm),e(nm,c8),e(Ps,p8),e(S,f8),e(S,yo),e(yo,yp),e(yp,h8),e(yo,d8),e(yo,Ss),e(Ss,g8),e(Ss,rm),e(rm,m8),e(Ss,$8),e(S,q8),e(S,bo),e(bo,bp),e(bp,om),e(om,_8),e(bo,v8),e(bo,jp),e(jp,E8),e(S,w8),e(S,jo),e(jo,Tp),e(Tp,y8),e(jo,b8),e(jo,Is),e(Is,j8),e(Is,lm),e(lm,T8),e(Is,k8),e(S,A8),e(S,To),e(To,kp),e(kp,D8),e(To,O8),e(To,Hs),e(Hs,x8),e(Hs,im),e(im,R8),e(Hs,N8),e(S,P8),e(S,ko),e(ko,Ap),e(Ap,S8),e(ko,I8),e(ko,Bs),e(Bs,H8),e(Bs,um),e(um,B8),e(Bs,G8),m(a,G_,g),m(a,Dp,g),e(Dp,C8),m(a,C_,g),E(Gs,a,g),m(a,M_,g),m(a,Cs,g),e(Cs,cm),e(cm,Ao),e(Ao,Op),e(Op,M8),e(Ao,F8),e(Ao,pm),e(Cs,U8),e(Cs,fm),e(fm,Do),e(Do,xp),e(xp,hm),e(hm,L8),e(Do,z8),e(Do,Rp),e(Rp,J8),m(a,F_,g),m(a,Le,g),e(Le,Ms),e(Ms,dm),E(Oo,dm,null),e(Le,W8),e(Le,gm),e(gm,Y8),m(a,U_,g),m(a,Fs,g),e(Fs,K8),e(Fs,Np),e(Np,V8),e(Fs,X8),m(a,L_,g),m(a,ze,g),e(ze,Us),e(Us,mm),E(xo,mm,null),e(ze,Q8),e(ze,$m),e($m,Z8),m(a,z_,g),m(a,Pp,g),e(Pp,eA),m(a,J_,g),E(Ls,a,g),m(a,W_,g),m(a,Ro,g),e(Ro,tA),e(Ro,No),e(No,sA),m(a,Y_,g),m(a,Sp,g),e(Sp,aA),m(a,K_,g),E(zs,a,g),m(a,V_,g),m(a,Ip,g),e(Ip,nA),m(a,X_,g),m(a,Js,g),e(Js,qm),e(qm,Po),e(Po,Hp),e(Hp,rA),e(Po,oA),e(Po,_m),e(Js,lA),e(Js,ee),e(ee,So),e(So,Io),e(Io,vm),e(vm,iA),e(Io,uA),e(So,cA),e(So,Bp),e(Bp,pA),e(ee,fA),e(ee,Ho),e(Ho,Gp),e(Gp,Em),e(Em,hA),e(Ho,dA),e(Ho,Cp),e(Cp,gA),e(ee,mA),e(ee,Bo),e(Bo,Mp),e(Mp,$A),e(Bo,qA),e(Bo,Ws),e(Ws,_A),e(Ws,wm),e(wm,vA),e(Ws,EA),e(ee,wA),e(ee,Go),e(Go,Fp),e(Fp,yA),e(Go,bA),e(Go,Ys),e(Ys,jA),e(Ys,ym),e(ym,TA),e(Ys,kA),e(ee,AA),e(ee,Co),e(Co,Up),e(Up,DA),e(Co,OA),e(Co,Ks),e(Ks,xA),e(Ks,bm),e(bm,RA),e(Ks,NA),m(a,Q_,g),m(a,Lp,g),e(Lp,PA),m(a,Z_,g),E(Vs,a,g),m(a,e1,g),m(a,Xs,g),e(Xs,jm),e(jm,Mo),e(Mo,zp),e(zp,SA),e(Mo,IA),e(Mo,Tm),e(Xs,HA),e(Xs,ue),e(ue,Fo),e(Fo,Jp),e(Jp,km),e(km,BA),e(Fo,GA),e(Fo,Wp),e(Wp,CA),e(ue,MA),e(ue,Uo),e(Uo,Yp),e(Yp,Am),e(Am,FA),e(Uo,UA),e(Uo,Kp),e(Kp,LA),e(ue,zA),e(ue,Lo),e(Lo,Vp),e(Vp,Dm),e(Dm,JA),e(Lo,WA),e(Lo,Xp),e(Xp,YA),e(ue,KA),e(ue,zo),e(zo,Qp),e(Qp,Om),e(Om,VA),e(zo,XA),e(zo,Zp),e(Zp,QA),m(a,t1,g),m(a,Je,g),e(Je,Qs),e(Qs,xm),E(Jo,xm,null),e(Je,ZA),e(Je,Rm),e(Rm,eD),m(a,s1,g),m(a,ef,g),e(ef,tD),m(a,a1,g),E(Zs,a,g),m(a,n1,g),E(ea,a,g),m(a,r1,g),m(a,ce,g),e(ce,sD),e(ce,Wo),e(Wo,aD),e(ce,nD),e(ce,Yo),e(Yo,rD),e(ce,oD),e(ce,Ko),e(Ko,lD),m(a,o1,g),m(a,tf,g),e(tf,iD),m(a,l1,g),E(ta,a,g),m(a,i1,g),m(a,sf,g),e(sf,uD),m(a,u1,g),m(a,sa,g),e(sa,Nm),e(Nm,Vo),e(Vo,af),e(af,cD),e(Vo,pD),e(Vo,Pm),e(sa,fD),e(sa,Sm),e(Sm,Xo),e(Xo,Qo),e(Qo,Im),e(Im,hD),e(Qo,dD),e(Xo,gD),e(Xo,nf),e(nf,mD),m(a,c1,g),m(a,rf,g),e(rf,$D),m(a,p1,g),m(a,of,g),e(of,qD),m(a,f1,g),E(aa,a,g),m(a,h1,g),m(a,na,g),e(na,Hm),e(Hm,Zo),e(Zo,lf),e(lf,_D),e(Zo,vD),e(Zo,Bm),e(na,ED),e(na,Gm),e(Gm,el),e(el,uf),e(uf,Cm),e(Cm,wD),e(el,yD),e(el,cf),e(cf,bD),m(a,d1,g),m(a,We,g),e(We,ra),e(ra,Mm),E(tl,Mm,null),e(We,jD),e(We,Fm),e(Fm,TD),m(a,g1,g),m(a,pf,g),e(pf,kD),m(a,m1,g),E(oa,a,g),m(a,$1,g),m(a,Ye,g),e(Ye,AD),e(Ye,sl),e(sl,DD),e(Ye,OD),e(Ye,al),e(al,xD),m(a,q1,g),m(a,ff,g),e(ff,RD),m(a,_1,g),m(a,la,g),e(la,Um),e(Um,nl),e(nl,hf),e(hf,ND),e(nl,PD),e(nl,Lm),e(la,SD),e(la,te),e(te,rl),e(rl,ol),e(ol,zm),e(zm,ID),e(ol,HD),e(rl,BD),e(rl,df),e(df,GD),e(te,CD),e(te,ll),e(ll,gf),e(gf,Jm),e(Jm,MD),e(ll,FD),e(ll,mf),e(mf,UD),e(te,LD),e(te,il),e(il,$f),e($f,zD),e(il,JD),e(il,ia),e(ia,WD),e(ia,Wm),e(Wm,YD),e(ia,KD),e(te,VD),e(te,ul),e(ul,qf),e(qf,XD),e(ul,QD),e(ul,ua),e(ua,ZD),e(ua,Ym),e(Ym,eO),e(ua,tO),e(te,sO),e(te,cl),e(cl,_f),e(_f,aO),e(cl,nO),e(cl,ca),e(ca,rO),e(ca,Km),e(Km,oO),e(ca,lO),m(a,v1,g),m(a,vf,g),e(vf,iO),m(a,E1,g),m(a,pa,g),e(pa,Vm),e(Vm,pl),e(pl,Ef),e(Ef,uO),e(pl,cO),e(pl,Xm),e(pa,pO),e(pa,Qm),e(Qm,fl),e(fl,wf),e(wf,Zm),e(Zm,fO),e(fl,hO),e(fl,yf),e(yf,dO),m(a,w1,g),m(a,bf,g),e(bf,gO),m(a,y1,g),m(a,Ke,g),e(Ke,fa),e(fa,e$),E(hl,e$,null),e(Ke,mO),e(Ke,t$),e(t$,$O),m(a,b1,g),m(a,jf,g),e(jf,qO),m(a,j1,g),E(ha,a,g),m(a,T1,g),m(a,Ve,g),e(Ve,_O),e(Ve,dl),e(dl,vO),e(Ve,EO),e(Ve,gl),e(gl,wO),m(a,k1,g),m(a,Tf,g),e(Tf,yO),m(a,A1,g),E(da,a,g),m(a,D1,g),m(a,kf,g),e(kf,bO),m(a,O1,g),m(a,ga,g),e(ga,s$),e(s$,ml),e(ml,Af),e(Af,jO),e(ml,TO),e(ml,a$),e(ga,kO),e(ga,n$),e(n$,$l),e($l,ql),e(ql,r$),e(r$,AO),e(ql,DO),e($l,OO),e($l,Df),e(Df,xO),m(a,x1,g),m(a,Of,g),e(Of,RO),m(a,R1,g),E(ma,a,g),m(a,N1,g),m(a,$a,g),e($a,o$),e(o$,_l),e(_l,xf),e(xf,NO),e(_l,PO),e(_l,l$),e($a,SO),e($a,vl),e(vl,El),e(El,Rf),e(Rf,i$),e(i$,IO),e(El,HO),e(El,Nf),e(Nf,BO),e(vl,GO),e(vl,wl),e(wl,Pf),e(Pf,u$),e(u$,CO),e(wl,MO),e(wl,Sf),e(Sf,FO),m(a,P1,g),m(a,Xe,g),e(Xe,qa),e(qa,c$),E(yl,c$,null),e(Xe,UO),e(Xe,p$),e(p$,LO),m(a,S1,g),m(a,If,g),e(If,zO),m(a,I1,g),E(_a,a,g),m(a,H1,g),m(a,bl,g),e(bl,JO),e(bl,jl),e(jl,WO),m(a,B1,g),m(a,Hf,g),e(Hf,YO),m(a,G1,g),E(va,a,g),m(a,C1,g),m(a,Ea,g),e(Ea,KO),e(Ea,Tl),e(Tl,VO),e(Ea,XO),m(a,M1,g),m(a,wa,g),e(wa,f$),e(f$,kl),e(kl,Bf),e(Bf,QO),e(kl,ZO),e(kl,h$),e(wa,ex),e(wa,d$),e(d$,Al),e(Al,Dl),e(Dl,g$),e(g$,tx),e(Dl,sx),e(Al,ax),e(Al,Gf),e(Gf,nx),m(a,F1,g),m(a,Cf,g),e(Cf,rx),m(a,U1,g),E(ya,a,g),m(a,L1,g),m(a,ba,g),e(ba,m$),e(m$,Ol),e(Ol,Mf),e(Mf,ox),e(Ol,lx),e(Ol,$$),e(ba,ix),e(ba,Qe),e(Qe,xl),e(xl,Ff),e(Ff,q$),e(q$,ux),e(xl,cx),e(xl,Uf),e(Uf,px),e(Qe,fx),e(Qe,Rl),e(Rl,Lf),e(Lf,_$),e(_$,hx),e(Rl,dx),e(Rl,zf),e(zf,gx),e(Qe,mx),e(Qe,Nl),e(Nl,Jf),e(Jf,v$),e(v$,$x),e(Nl,qx),e(Nl,Wf),e(Wf,_x),z1=!0},p(a,[g]){const Pl={};g&2&&(Pl.$$scope={dirty:g,ctx:a}),nt.$set(Pl);const E$={};g&2&&(E$.$$scope={dirty:g,ctx:a}),rt.$set(E$);const w$={};g&2&&(w$.$$scope={dirty:g,ctx:a}),pt.$set(w$);const y$={};g&2&&(y$.$$scope={dirty:g,ctx:a}),gt.$set(y$);const Sl={};g&2&&(Sl.$$scope={dirty:g,ctx:a}),mt.$set(Sl);const b$={};g&2&&(b$.$$scope={dirty:g,ctx:a}),bt.$set(b$);const j$={};g&2&&(j$.$$scope={dirty:g,ctx:a}),jt.$set(j$);const T$={};g&2&&(T$.$$scope={dirty:g,ctx:a}),Pt.$set(T$);const k$={};g&2&&(k$.$$scope={dirty:g,ctx:a}),St.$set(k$);const A$={};g&2&&(A$.$$scope={dirty:g,ctx:a}),zt.$set(A$);const Il={};g&2&&(Il.$$scope={dirty:g,ctx:a}),Jt.$set(Il);const D$={};g&2&&(D$.$$scope={dirty:g,ctx:a}),Xt.$set(D$);const O$={};g&2&&(O$.$$scope={dirty:g,ctx:a}),es.$set(O$);const x$={};g&2&&(x$.$$scope={dirty:g,ctx:a}),ts.$set(x$);const R$={};g&2&&(R$.$$scope={dirty:g,ctx:a}),ss.$set(R$);const Yf={};g&2&&(Yf.$$scope={dirty:g,ctx:a}),ls.$set(Yf);const N$={};g&2&&(N$.$$scope={dirty:g,ctx:a}),is.$set(N$);const P$={};g&2&&(P$.$$scope={dirty:g,ctx:a}),hs.$set(P$);const S$={};g&2&&(S$.$$scope={dirty:g,ctx:a}),$s.$set(S$);const Hl={};g&2&&(Hl.$$scope={dirty:g,ctx:a}),qs.$set(Hl);const I$={};g&2&&(I$.$$scope={dirty:g,ctx:a}),bs.$set(I$);const Bl={};g&2&&(Bl.$$scope={dirty:g,ctx:a}),Ds.$set(Bl);const H$={};g&2&&(H$.$$scope={dirty:g,ctx:a}),Os.$set(H$);const F={};g&2&&(F.$$scope={dirty:g,ctx:a}),Gs.$set(F);const Gl={};g&2&&(Gl.$$scope={dirty:g,ctx:a}),Ls.$set(Gl);const Kf={};g&2&&(Kf.$$scope={dirty:g,ctx:a}),zs.$set(Kf);const B$={};g&2&&(B$.$$scope={dirty:g,ctx:a}),Vs.$set(B$);const G$={};g&2&&(G$.$$scope={dirty:g,ctx:a}),Zs.$set(G$);const Cl={};g&2&&(Cl.$$scope={dirty:g,ctx:a}),ea.$set(Cl);const Vf={};g&2&&(Vf.$$scope={dirty:g,ctx:a}),ta.$set(Vf);const C$={};g&2&&(C$.$$scope={dirty:g,ctx:a}),aa.$set(C$);const M$={};g&2&&(M$.$$scope={dirty:g,ctx:a}),oa.$set(M$);const Ml={};g&2&&(Ml.$$scope={dirty:g,ctx:a}),ha.$set(Ml);const F$={};g&2&&(F$.$$scope={dirty:g,ctx:a}),da.$set(F$);const Ze={};g&2&&(Ze.$$scope={dirty:g,ctx:a}),ma.$set(Ze);const U$={};g&2&&(U$.$$scope={dirty:g,ctx:a}),_a.$set(U$);const L$={};g&2&&(L$.$$scope={dirty:g,ctx:a}),va.$set(L$);const Fl={};g&2&&(Fl.$$scope={dirty:g,ctx:a}),ya.$set(Fl)},i(a){z1||(w(k.$$.fragment,a),w(V.$$.fragment,a),w(Na.$$.fragment,a),w(nt.$$.fragment,a),w(rt.$$.fragment,a),w(pt.$$.fragment,a),w(Xa.$$.fragment,a),w(gt.$$.fragment,a),w(mt.$$.fragment,a),w(cn.$$.fragment,a),w(bt.$$.fragment,a),w(jt.$$.fragment,a),w(On.$$.fragment,a),w(Pt.$$.fragment,a),w(St.$$.fragment,a),w(sr.$$.fragment,a),w(zt.$$.fragment,a),w(Jt.$$.fragment,a),w(Xt.$$.fragment,a),w(_r.$$.fragment,a),w(es.$$.fragment,a),w(ts.$$.fragment,a),w(ss.$$.fragment,a),w(kr.$$.fragment,a),w(ls.$$.fragment,a),w(is.$$.fragment,a),w(hs.$$.fragment,a),w(Mr.$$.fragment,a),w(Ur.$$.fragment,a),w($s.$$.fragment,a),w(qs.$$.fragment,a),w(bs.$$.fragment,a),w(lo.$$.fragment,a),w(Ds.$$.fragment,a),w(Os.$$.fragment,a),w(Gs.$$.fragment,a),w(Oo.$$.fragment,a),w(xo.$$.fragment,a),w(Ls.$$.fragment,a),w(zs.$$.fragment,a),w(Vs.$$.fragment,a),w(Jo.$$.fragment,a),w(Zs.$$.fragment,a),w(ea.$$.fragment,a),w(ta.$$.fragment,a),w(aa.$$.fragment,a),w(tl.$$.fragment,a),w(oa.$$.fragment,a),w(hl.$$.fragment,a),w(ha.$$.fragment,a),w(da.$$.fragment,a),w(ma.$$.fragment,a),w(yl.$$.fragment,a),w(_a.$$.fragment,a),w(va.$$.fragment,a),w(ya.$$.fragment,a),z1=!0)},o(a){y(k.$$.fragment,a),y(V.$$.fragment,a),y(Na.$$.fragment,a),y(nt.$$.fragment,a),y(rt.$$.fragment,a),y(pt.$$.fragment,a),y(Xa.$$.fragment,a),y(gt.$$.fragment,a),y(mt.$$.fragment,a),y(cn.$$.fragment,a),y(bt.$$.fragment,a),y(jt.$$.fragment,a),y(On.$$.fragment,a),y(Pt.$$.fragment,a),y(St.$$.fragment,a),y(sr.$$.fragment,a),y(zt.$$.fragment,a),y(Jt.$$.fragment,a),y(Xt.$$.fragment,a),y(_r.$$.fragment,a),y(es.$$.fragment,a),y(ts.$$.fragment,a),y(ss.$$.fragment,a),y(kr.$$.fragment,a),y(ls.$$.fragment,a),y(is.$$.fragment,a),y(hs.$$.fragment,a),y(Mr.$$.fragment,a),y(Ur.$$.fragment,a),y($s.$$.fragment,a),y(qs.$$.fragment,a),y(bs.$$.fragment,a),y(lo.$$.fragment,a),y(Ds.$$.fragment,a),y(Os.$$.fragment,a),y(Gs.$$.fragment,a),y(Oo.$$.fragment,a),y(xo.$$.fragment,a),y(Ls.$$.fragment,a),y(zs.$$.fragment,a),y(Vs.$$.fragment,a),y(Jo.$$.fragment,a),y(Zs.$$.fragment,a),y(ea.$$.fragment,a),y(ta.$$.fragment,a),y(aa.$$.fragment,a),y(tl.$$.fragment,a),y(oa.$$.fragment,a),y(hl.$$.fragment,a),y(ha.$$.fragment,a),y(da.$$.fragment,a),y(ma.$$.fragment,a),y(yl.$$.fragment,a),y(_a.$$.fragment,a),y(va.$$.fragment,a),y(ya.$$.fragment,a),z1=!1},d(a){t(n),a&&t(c),a&&t(s),b(k),a&&t(O),a&&t(D),b(V),a&&t(Ra),a&&t(Oe),a&&t(K$),a&&t(zl),a&&t(V$),a&&t(tt),a&&t(X$),a&&t(st),a&&t(Q$),a&&t(xe),b(Na),a&&t(Z$),a&&t(Jl),a&&t(eq),b(nt,a),a&&t(tq),a&&t(Pa),a&&t(sq),a&&t(Wl),a&&t(aq),b(rt,a),a&&t(nq),a&&t(Yl),a&&t(rq),a&&t(ot),a&&t(oq),a&&t(oi),a&&t(lq),a&&t(li),a&&t(iq),b(pt,a),a&&t(uq),a&&t(ft),a&&t(cq),a&&t(Ne),b(Xa),a&&t(pq),a&&t(di),a&&t(fq),b(gt,a),a&&t(hq),a&&t(Qa),a&&t(dq),a&&t(gi),a&&t(gq),b(mt,a),a&&t(mq),a&&t(mi),a&&t($q),a&&t($t),a&&t(qq),a&&t(bi),a&&t(_q),a&&t(Et),a&&t(vq),a&&t(Pe),b(cn),a&&t(Eq),a&&t(yt),a&&t(wq),b(bt,a),a&&t(yq),a&&t(pn),a&&t(bq),a&&t(Di),a&&t(jq),b(jt,a),a&&t(Tq),a&&t(Oi),a&&t(kq),a&&t(Tt),a&&t(Aq),a&&t(Xi),a&&t(Dq),a&&t(Rt),a&&t(Oq),a&&t(Se),b(On),a&&t(xq),a&&t(tu),a&&t(Rq),b(Pt,a),a&&t(Nq),a&&t(xn),a&&t(Pq),a&&t(su),a&&t(Sq),b(St,a),a&&t(Iq),a&&t(au),a&&t(Hq),a&&t(It),a&&t(Bq),a&&t(Au),a&&t(Gq),a&&t(Ut),a&&t(Cq),a&&t(Ie),b(sr),a&&t(Mq),a&&t(Bu),a&&t(Fq),b(zt,a),a&&t(Uq),a&&t(ar),a&&t(Lq),a&&t(Gu),a&&t(zq),b(Jt,a),a&&t(Jq),a&&t(Cu),a&&t(Wq),a&&t(Wt),a&&t(Yq),a&&t(Xu),a&&t(Kq),b(Xt,a),a&&t(Vq),a&&t(Qt),a&&t(Xq),a&&t(He),b(_r),a&&t(Qq),a&&t(lc),a&&t(Zq),b(es,a),a&&t(e_),a&&t(Be),a&&t(t_),a&&t(ic),a&&t(s_),b(ts,a),a&&t(a_),a&&t(uc),a&&t(n_),a&&t(cc),a&&t(r_),b(ss,a),a&&t(o_),a&&t(as),a&&t(l_),a&&t(Ge),b(kr),a&&t(i_),a&&t(qc),a&&t(u_),b(ls,a),a&&t(c_),a&&t(Ar),a&&t(p_),a&&t(_c),a&&t(f_),b(is,a),a&&t(h_),a&&t(vc),a&&t(d_),a&&t(us),a&&t(g_),a&&t(Ac),a&&t(m_),b(hs,a),a&&t($_),a&&t(ds),a&&t(q_),a&&t(Ce),b(Mr),a&&t(__),a&&t(Fr),a&&t(v_),a&&t(Me),b(Ur),a&&t(E_),a&&t(Sc),a&&t(w_),b($s,a),a&&t(y_),a&&t(Fe),a&&t(b_),a&&t(Ic),a&&t(j_),b(qs,a),a&&t(T_),a&&t(Hc),a&&t(k_),a&&t(_s),a&&t(A_),a&&t(Yc),a&&t(D_),b(bs,a),a&&t(O_),a&&t(js),a&&t(x_),a&&t(Ue),b(lo),a&&t(R_),a&&t(np),a&&t(N_),b(Ds,a),a&&t(P_),a&&t(io),a&&t(S_),a&&t(rp),a&&t(I_),b(Os,a),a&&t(H_),a&&t(op),a&&t(B_),a&&t(xs),a&&t(G_),a&&t(Dp),a&&t(C_),b(Gs,a),a&&t(M_),a&&t(Cs),a&&t(F_),a&&t(Le),b(Oo),a&&t(U_),a&&t(Fs),a&&t(L_),a&&t(ze),b(xo),a&&t(z_),a&&t(Pp),a&&t(J_),b(Ls,a),a&&t(W_),a&&t(Ro),a&&t(Y_),a&&t(Sp),a&&t(K_),b(zs,a),a&&t(V_),a&&t(Ip),a&&t(X_),a&&t(Js),a&&t(Q_),a&&t(Lp),a&&t(Z_),b(Vs,a),a&&t(e1),a&&t(Xs),a&&t(t1),a&&t(Je),b(Jo),a&&t(s1),a&&t(ef),a&&t(a1),b(Zs,a),a&&t(n1),b(ea,a),a&&t(r1),a&&t(ce),a&&t(o1),a&&t(tf),a&&t(l1),b(ta,a),a&&t(i1),a&&t(sf),a&&t(u1),a&&t(sa),a&&t(c1),a&&t(rf),a&&t(p1),a&&t(of),a&&t(f1),b(aa,a),a&&t(h1),a&&t(na),a&&t(d1),a&&t(We),b(tl),a&&t(g1),a&&t(pf),a&&t(m1),b(oa,a),a&&t($1),a&&t(Ye),a&&t(q1),a&&t(ff),a&&t(_1),a&&t(la),a&&t(v1),a&&t(vf),a&&t(E1),a&&t(pa),a&&t(w1),a&&t(bf),a&&t(y1),a&&t(Ke),b(hl),a&&t(b1),a&&t(jf),a&&t(j1),b(ha,a),a&&t(T1),a&&t(Ve),a&&t(k1),a&&t(Tf),a&&t(A1),b(da,a),a&&t(D1),a&&t(kf),a&&t(O1),a&&t(ga),a&&t(x1),a&&t(Of),a&&t(R1),b(ma,a),a&&t(N1),a&&t($a),a&&t(P1),a&&t(Xe),b(yl),a&&t(S1),a&&t(If),a&&t(I1),b(_a,a),a&&t(H1),a&&t(bl),a&&t(B1),a&&t(Hf),a&&t(G1),b(va,a),a&&t(C1),a&&t(Ea),a&&t(M1),a&&t(wa),a&&t(F1),a&&t(Cf),a&&t(U1),b(ya,a),a&&t(L1),a&&t(ba)}}}const nU={local:"detailed-parameters",sections:[{local:"which-task-is-used-by-this-model",title:"Which task is used by this model ?"},{local:"zeroshot-classification-task",title:"Zero-shot classification task"},{local:"translation-task",title:"Translation task"},{local:"summarization-task",title:"Summarization task"},{local:"conversational-task",title:"Conversational task"},{local:"table-question-answering-task",title:"Table question answering task"},{local:"question-answering-task",title:"Question answering task"},{local:"textclassification-task",title:"Text-classification task"},{local:"named-entity-recognition-ner-task",title:"Named Entity Recognition (NER) task"},{local:"tokenclassification-task",title:"Token-classification task"},{local:"textgeneration-task",title:"Text-generation task"},{local:"text2textgeneration-task",title:"Text2text-generation task"},{local:"fill-mask-task",title:"Fill mask task"},{local:"automatic-speech-recognition-task",title:"Automatic speech recognition task"},{local:"featureextraction-task",title:"Feature-extraction task"},{local:"audioclassification-task",title:"Audio-classification task"},{local:"objectdetection-task",title:"Object-detection task"}],title:"Detailed parameters"};function rU(q){return XC(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class cU extends WC{constructor(n){super();YC(this,n,rU,aU,KC,{})}}export{cU as default,nU as metadata};
