import{S as KG,i as WG,s as YG,e as r,k as f,w as q,t as i,M as VG,c as o,d as t,m as h,a as l,x as v,h as u,b as c,N as JG,F as e,g as m,y,q as w,o as E,B as b,v as XG,L as O}from"../chunks/vendor-7c454903.js";import{T as K}from"../chunks/Tip-735285fc.js";import{I as z}from"../chunks/IconCopyLink-5457534b.js";import{I as G,M as R,C as N}from"../chunks/InferenceApi-9b73136f.js";function QG(_){let n,p,s,d,$,k,A;return{c(){n=r("p"),p=r("strong"),s=i("Recommended model"),d=i(`:
`),$=r("a"),k=i("facebook/bart-large-mnli"),A=i("."),this.h()},l(T){n=o(T,"P",{});var j=l(n);p=o(j,"STRONG",{});var P=l(p);s=u(P,"Recommended model"),P.forEach(t),d=u(j,`:
`),$=o(j,"A",{href:!0,rel:!0});var D=l($);k=u(D,"facebook/bart-large-mnli"),D.forEach(t),A=u(j,"."),j.forEach(t),this.h()},h(){c($,"href","https://huggingface.co/facebook/bart-large-mnli"),c($,"rel","nofollow")},m(T,j){m(T,n,j),e(n,p),e(p,s),e(n,d),e(n,$),e($,k),e(n,A)},d(T){T&&t(n)}}}function ZG(_){let n,p;return n=new N({props:{code:`import json
import requests
headers = {"Authorization": f"Bearer {API_TOKEN}"}
API_URL = "https://api-inference.huggingface.co/models/facebook/bart-large-mnli"
def query(payload):
    data = json.dumps(payload)
    response = requests.request("POST", API_URL, headers=headers, data=data)
    return json.loads(response.content.decode("utf-8"))
data = query(
    {
        "inputs": "Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!",
        "parameters": {"candidate_labels": ["refund", "legal", "faq"]},
    }
)`,highlighted:`<span class="hljs-keyword">import</span> json
<span class="hljs-keyword">import</span> requests
headers = {<span class="hljs-string">&quot;Authorization&quot;</span>: <span class="hljs-string">f&quot;Bearer <span class="hljs-subst">{API_TOKEN}</span>&quot;</span>}
API_URL = <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/facebook/bart-large-mnli&quot;</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">payload</span>):
    data = json.dumps(payload)
    response = requests.request(<span class="hljs-string">&quot;POST&quot;</span>, API_URL, headers=headers, data=data)
    <span class="hljs-keyword">return</span> json.loads(response.content.decode(<span class="hljs-string">&quot;utf-8&quot;</span>))
data = query(
    {
        <span class="hljs-string">&quot;inputs&quot;</span>: <span class="hljs-string">&quot;Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!&quot;</span>,
        <span class="hljs-string">&quot;parameters&quot;</span>: {<span class="hljs-string">&quot;candidate_labels&quot;</span>: [<span class="hljs-string">&quot;refund&quot;</span>, <span class="hljs-string">&quot;legal&quot;</span>, <span class="hljs-string">&quot;faq&quot;</span>]},
    }
)`}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p:O,i(s){p||(w(n.$$.fragment,s),p=!0)},o(s){E(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function eU(_){let n,p;return n=new R({props:{$$slots:{default:[ZG]},$$scope:{ctx:_}}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){p||(w(n.$$.fragment,s),p=!0)},o(s){E(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function tU(_){let n,p;return n=new N({props:{code:`import fetch from "node-fetch";
async function query(data) {
    const response = await fetch(
        "https://api-inference.huggingface.co/models/facebook/bart-large-mnli",
        {
            headers: { Authorization: \`Bearer $}{API_TOKEN}\` },
            method: "POST",
            body: JSON.stringify(data),
        }
    );
    const result = await response.json();
    return result;
}
query({inputs: "Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!", parameters: {candidate_labels: ["refund", "legal", "faq"]}}).then((response) => {
    console.log(JSON.stringify(response));
});
// {"sequence":"Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!","labels":["refund","faq","legal"],"scores":[0.8778, 0.1052, 0.017]}`,highlighted:`<span class="hljs-keyword">import</span> fetch <span class="hljs-keyword">from</span> <span class="hljs-string">&quot;node-fetch&quot;</span>;
<span class="hljs-keyword">async</span> <span class="hljs-keyword">function</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">data</span>) {
    <span class="hljs-keyword">const</span> response = <span class="hljs-keyword">await</span> <span class="hljs-title function_">fetch</span>(
        <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/facebook/bart-large-mnli&quot;</span>,
        {
            <span class="hljs-attr">headers</span>: { <span class="hljs-title class_">Authorization</span>: <span class="hljs-string">\`Bearer <span class="hljs-subst">$}{API_TOKEN}</span>\`</span> },
            <span class="hljs-attr">method</span>: <span class="hljs-string">&quot;POST&quot;</span>,
            <span class="hljs-attr">body</span>: <span class="hljs-title class_">JSON</span>.<span class="hljs-title function_">stringify</span>(data),
        }
    );
    <span class="hljs-keyword">const</span> result = <span class="hljs-keyword">await</span> response.<span class="hljs-title function_">json</span>();
    <span class="hljs-keyword">return</span> result;
}
<span class="hljs-title function_">query</span>({<span class="hljs-attr">inputs</span>: <span class="hljs-string">&quot;Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!&quot;</span>, <span class="hljs-attr">parameters</span>: {<span class="hljs-attr">candidate_labels</span>: [<span class="hljs-string">&quot;refund&quot;</span>, <span class="hljs-string">&quot;legal&quot;</span>, <span class="hljs-string">&quot;faq&quot;</span>]}}).<span class="hljs-title function_">then</span>(<span class="hljs-function">(<span class="hljs-params">response</span>) =&gt;</span> {
    <span class="hljs-variable language_">console</span>.<span class="hljs-title function_">log</span>(<span class="hljs-title class_">JSON</span>.<span class="hljs-title function_">stringify</span>(response));
});
<span class="hljs-comment">// {&quot;sequence&quot;:&quot;Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!&quot;,&quot;labels&quot;:[&quot;refund&quot;,&quot;faq&quot;,&quot;legal&quot;],&quot;scores&quot;:[0.8778, 0.1052, 0.017]}</span>`}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p:O,i(s){p||(w(n.$$.fragment,s),p=!0)},o(s){E(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function sU(_){let n,p;return n=new R({props:{$$slots:{default:[tU]},$$scope:{ctx:_}}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){p||(w(n.$$.fragment,s),p=!0)},o(s){E(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function aU(_){let n,p;return n=new N({props:{code:`curl https://api-inference.huggingface.co/models/facebook/bart-large-mnli \\
        -X POST \\
        -d '{"inputs": "Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!", "parameters": {"candidate_labels": ["refund", "legal", "faq"]}}' \\
        -H "Authorization: Bearer $}{HF_API_TOKEN}"
# {"sequence":"Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!","labels":["refund","faq","legal"],"scores":[0.8778, 0.1052, 0.017]}`,highlighted:`curl https:<span class="hljs-comment">//api-inference.huggingface.co/models/facebook/bart-large-mnli \\</span>
        -X <span class="hljs-keyword">POST</span> \\
        -<span class="hljs-keyword">d</span> &#x27;{<span class="hljs-string">&quot;inputs&quot;</span>: <span class="hljs-string">&quot;Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!&quot;</span>, <span class="hljs-string">&quot;parameters&quot;</span>: {<span class="hljs-string">&quot;candidate_labels&quot;</span>: [<span class="hljs-string">&quot;refund&quot;</span>, <span class="hljs-string">&quot;legal&quot;</span>, <span class="hljs-string">&quot;faq&quot;</span>]}}&#x27; \\
        -<span class="hljs-keyword">H</span> <span class="hljs-string">&quot;Authorization: Bearer $}{HF_API_TOKEN}&quot;</span>
# {<span class="hljs-string">&quot;sequence&quot;</span>:<span class="hljs-string">&quot;Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!&quot;</span>,<span class="hljs-string">&quot;labels&quot;</span>:[<span class="hljs-string">&quot;refund&quot;</span>,<span class="hljs-string">&quot;faq&quot;</span>,<span class="hljs-string">&quot;legal&quot;</span>],<span class="hljs-string">&quot;scores&quot;</span>:[0.8778, 0.1052, 0.017]}`}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p:O,i(s){p||(w(n.$$.fragment,s),p=!0)},o(s){E(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function nU(_){let n,p;return n=new R({props:{$$slots:{default:[aU]},$$scope:{ctx:_}}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){p||(w(n.$$.fragment,s),p=!0)},o(s){E(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function rU(_){let n,p;return n=new N({props:{code:`self.assertEqual(
    deep_round(data),
    {
        "sequence": "Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!",
        "labels": ["refund", "faq", "legal"],
        "scores": [
            # 88% refund
            0.8778,
            0.1052,
            0.017,
        ],
    },
)`,highlighted:`self.assertEqual(
    deep_round(data),
    {
        <span class="hljs-string">&quot;sequence&quot;</span>: <span class="hljs-string">&quot;Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!&quot;</span>,
        <span class="hljs-string">&quot;labels&quot;</span>: [<span class="hljs-string">&quot;refund&quot;</span>, <span class="hljs-string">&quot;faq&quot;</span>, <span class="hljs-string">&quot;legal&quot;</span>],
        <span class="hljs-string">&quot;scores&quot;</span>: [
            <span class="hljs-comment"># 88% refund</span>
            <span class="hljs-number">0.8778</span>,
            <span class="hljs-number">0.1052</span>,
            <span class="hljs-number">0.017</span>,
        ],
    },
)`}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p:O,i(s){p||(w(n.$$.fragment,s),p=!0)},o(s){E(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function oU(_){let n,p;return n=new R({props:{$$slots:{default:[rU]},$$scope:{ctx:_}}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){p||(w(n.$$.fragment,s),p=!0)},o(s){E(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function lU(_){let n,p,s,d,$,k,A,T,j,P,D,se,De;return{c(){n=r("p"),p=r("strong"),s=i("Recommended model"),d=i(`:
`),$=r("a"),k=i("Helsinki-NLP/opus-mt-ru-en"),A=i(`.
Helsinki-NLP uploaded many models with many language pairs.
`),T=r("strong"),j=i("Recommended model"),P=i(": "),D=r("a"),se=i("t5-base"),De=i("."),this.h()},l(V){n=o(V,"P",{});var J=l(n);p=o(J,"STRONG",{});var et=l(p);s=u(et,"Recommended model"),et.forEach(t),d=u(J,`:
`),$=o(J,"A",{href:!0,rel:!0});var Ml=l($);k=u(Ml,"Helsinki-NLP/opus-mt-ru-en"),Ml.forEach(t),A=u(J,`.
Helsinki-NLP uploaded many models with many language pairs.
`),T=o(J,"STRONG",{});var Ra=l(T);j=u(Ra,"Recommended model"),Ra.forEach(t),P=u(J,": "),D=o(J,"A",{href:!0,rel:!0});var Pe=l(D);se=u(Pe,"t5-base"),Pe.forEach(t),De=u(J,"."),J.forEach(t),this.h()},h(){c($,"href","https://huggingface.co/Helsinki-NLP/opus-mt-ru-en"),c($,"rel","nofollow"),c(D,"href","https://huggingface.co/t5-base"),c(D,"rel","nofollow")},m(V,J){m(V,n,J),e(n,p),e(p,s),e(n,d),e(n,$),e($,k),e(n,A),e(n,T),e(T,j),e(n,P),e(n,D),e(D,se),e(n,De)},d(V){V&&t(n)}}}function iU(_){let n,p;return n=new N({props:{code:`import json
import requests
headers = {"Authorization": f"Bearer {API_TOKEN}"}
API_URL = "https://api-inference.huggingface.co/models/Helsinki-NLP/opus-mt-ru-en"
def query(payload):
    data = json.dumps(payload)
    response = requests.request("POST", API_URL, headers=headers, data=data)
    return json.loads(response.content.decode("utf-8"))
data = query(
    {
        "inputs": "\u041C\u0435\u043D\u044F \u0437\u043E\u0432\u0443\u0442 \u0412\u043E\u043B\u044C\u0444\u0433\u0430\u043D\u0433 \u0438 \u044F \u0436\u0438\u0432\u0443 \u0432 \u0411\u0435\u0440\u043B\u0438\u043D\u0435",
    }
)
# Response
self.assertEqual(
    data,
    [
        {
            "translation_text": "My name is Wolfgang and I live in Berlin.",
        },
    ],
)`,highlighted:`<span class="hljs-keyword">import</span> json
<span class="hljs-keyword">import</span> requests
headers = {<span class="hljs-string">&quot;Authorization&quot;</span>: <span class="hljs-string">f&quot;Bearer <span class="hljs-subst">{API_TOKEN}</span>&quot;</span>}
API_URL = <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/Helsinki-NLP/opus-mt-ru-en&quot;</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">payload</span>):
    data = json.dumps(payload)
    response = requests.request(<span class="hljs-string">&quot;POST&quot;</span>, API_URL, headers=headers, data=data)
    <span class="hljs-keyword">return</span> json.loads(response.content.decode(<span class="hljs-string">&quot;utf-8&quot;</span>))
data = query(
    {
        <span class="hljs-string">&quot;inputs&quot;</span>: <span class="hljs-string">&quot;\u041C\u0435\u043D\u044F \u0437\u043E\u0432\u0443\u0442 \u0412\u043E\u043B\u044C\u0444\u0433\u0430\u043D\u0433 \u0438 \u044F \u0436\u0438\u0432\u0443 \u0432 \u0411\u0435\u0440\u043B\u0438\u043D\u0435&quot;</span>,
    }
)
<span class="hljs-comment"># Response</span>
self.assertEqual(
    data,
    [
        {
            <span class="hljs-string">&quot;translation_text&quot;</span>: <span class="hljs-string">&quot;My name is Wolfgang and I live in Berlin.&quot;</span>,
        },
    ],
)`}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p:O,i(s){p||(w(n.$$.fragment,s),p=!0)},o(s){E(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function uU(_){let n,p;return n=new R({props:{$$slots:{default:[iU]},$$scope:{ctx:_}}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){p||(w(n.$$.fragment,s),p=!0)},o(s){E(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function pU(_){let n,p;return n=new N({props:{code:`import fetch from "node-fetch";
async function query(data) {
    const response = await fetch(
        "https://api-inference.huggingface.co/models/Helsinki-NLP/opus-mt-ru-en",
        {
            headers: { Authorization: \`Bearer $}{API_TOKEN}\` },
            method: "POST",
            body: JSON.stringify(data),
        }
    );
    const result = await response.json();
    return result;
}
query({inputs: "\u041C\u0435\u043D\u044F \u0437\u043E\u0432\u0443\u0442 \u0412\u043E\u043B\u044C\u0444\u0433\u0430\u043D\u0433 \u0438 \u044F \u0436\u0438\u0432\u0443 \u0432 \u0411\u0435\u0440\u043B\u0438\u043D\u0435"}).then((response) => {
    console.log(JSON.stringify(response));
});
// [{"translation_text":"My name is Wolfgang and I live in Berlin."}]`,highlighted:`<span class="hljs-keyword">import</span> fetch <span class="hljs-keyword">from</span> <span class="hljs-string">&quot;node-fetch&quot;</span>;
<span class="hljs-keyword">async</span> <span class="hljs-keyword">function</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">data</span>) {
    <span class="hljs-keyword">const</span> response = <span class="hljs-keyword">await</span> <span class="hljs-title function_">fetch</span>(
        <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/Helsinki-NLP/opus-mt-ru-en&quot;</span>,
        {
            <span class="hljs-attr">headers</span>: { <span class="hljs-title class_">Authorization</span>: <span class="hljs-string">\`Bearer <span class="hljs-subst">$}{API_TOKEN}</span>\`</span> },
            <span class="hljs-attr">method</span>: <span class="hljs-string">&quot;POST&quot;</span>,
            <span class="hljs-attr">body</span>: <span class="hljs-title class_">JSON</span>.<span class="hljs-title function_">stringify</span>(data),
        }
    );
    <span class="hljs-keyword">const</span> result = <span class="hljs-keyword">await</span> response.<span class="hljs-title function_">json</span>();
    <span class="hljs-keyword">return</span> result;
}
<span class="hljs-title function_">query</span>({<span class="hljs-attr">inputs</span>: <span class="hljs-string">&quot;\u041C\u0435\u043D\u044F \u0437\u043E\u0432\u0443\u0442 \u0412\u043E\u043B\u044C\u0444\u0433\u0430\u043D\u0433 \u0438 \u044F \u0436\u0438\u0432\u0443 \u0432 \u0411\u0435\u0440\u043B\u0438\u043D\u0435&quot;</span>}).<span class="hljs-title function_">then</span>(<span class="hljs-function">(<span class="hljs-params">response</span>) =&gt;</span> {
    <span class="hljs-variable language_">console</span>.<span class="hljs-title function_">log</span>(<span class="hljs-title class_">JSON</span>.<span class="hljs-title function_">stringify</span>(response));
});
<span class="hljs-comment">// [{&quot;translation_text&quot;:&quot;My name is Wolfgang and I live in Berlin.&quot;}]</span>`}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p:O,i(s){p||(w(n.$$.fragment,s),p=!0)},o(s){E(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function cU(_){let n,p;return n=new R({props:{$$slots:{default:[pU]},$$scope:{ctx:_}}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){p||(w(n.$$.fragment,s),p=!0)},o(s){E(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function fU(_){let n,p;return n=new N({props:{code:`curl https://api-inference.huggingface.co/models/Helsinki-NLP/opus-mt-ru-en \\
        -X POST \\
        -d '{"inputs": "\u041C\u0435\u043D\u044F \u0437\u043E\u0432\u0443\u0442 \u0412\u043E\u043B\u044C\u0444\u0433\u0430\u043D\u0433 \u0438 \u044F \u0436\u0438\u0432\u0443 \u0432 \u0411\u0435\u0440\u043B\u0438\u043D\u0435"}' \\
        -H "Authorization: Bearer $}{HF_API_TOKEN}"
# [{"translation_text":"My name is Wolfgang and I live in Berlin."}]`,highlighted:`curl https:<span class="hljs-regexp">//</span>api-inference.huggingface.co<span class="hljs-regexp">/models/</span>Helsinki-NLP/opus-mt-ru-en \\
        -X POST \\
        -d <span class="hljs-string">&#x27;{&quot;inputs&quot;: &quot;\u041C\u0435\u043D\u044F \u0437\u043E\u0432\u0443\u0442 \u0412\u043E\u043B\u044C\u0444\u0433\u0430\u043D\u0433 \u0438 \u044F \u0436\u0438\u0432\u0443 \u0432 \u0411\u0435\u0440\u043B\u0438\u043D\u0435&quot;}&#x27;</span> \\
        -H <span class="hljs-string">&quot;Authorization: Bearer $}{HF_API_TOKEN}&quot;</span>
<span class="hljs-comment"># [{&quot;translation_text&quot;:&quot;My name is Wolfgang and I live in Berlin.&quot;}]</span>`}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p:O,i(s){p||(w(n.$$.fragment,s),p=!0)},o(s){E(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function hU(_){let n,p;return n=new R({props:{$$slots:{default:[fU]},$$scope:{ctx:_}}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){p||(w(n.$$.fragment,s),p=!0)},o(s){E(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function dU(_){let n,p,s,d,$,k,A;return{c(){n=r("p"),p=r("strong"),s=i("Recommended model"),d=i(`:
`),$=r("a"),k=i("facebook/bart-large-cnn"),A=i("."),this.h()},l(T){n=o(T,"P",{});var j=l(n);p=o(j,"STRONG",{});var P=l(p);s=u(P,"Recommended model"),P.forEach(t),d=u(j,`:
`),$=o(j,"A",{href:!0,rel:!0});var D=l($);k=u(D,"facebook/bart-large-cnn"),D.forEach(t),A=u(j,"."),j.forEach(t),this.h()},h(){c($,"href","https://huggingface.co/facebook/bart-large-cnn"),c($,"rel","nofollow")},m(T,j){m(T,n,j),e(n,p),e(p,s),e(n,d),e(n,$),e($,k),e(n,A)},d(T){T&&t(n)}}}function gU(_){let n,p;return n=new N({props:{code:`import json
import requests
headers = {"Authorization": f"Bearer {API_TOKEN}"}
API_URL = "https://api-inference.huggingface.co/models/facebook/bart-large-cnn"
def query(payload):
    data = json.dumps(payload)
    response = requests.request("POST", API_URL, headers=headers, data=data)
    return json.loads(response.content.decode("utf-8"))
data = query(
    {
        "inputs": "The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world, a title it held for 41 years until the Chrysler Building in New York City was finished in 1930. It was the first structure to reach a height of 300 metres. Due to the addition of a broadcasting aerial at the top of the tower in 1957, it is now taller than the Chrysler Building by 5.2 metres (17 ft). Excluding transmitters, the Eiffel Tower is the second tallest free-standing structure in France after the Millau Viaduct.",
        "parameters": {"do_sample": False},
    }
)
# Response
self.assertEqual(
    data,
    [
        {
            "summary_text": "The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world.",
        },
    ],
)`,highlighted:`<span class="hljs-keyword">import</span> json
<span class="hljs-keyword">import</span> requests
headers = {<span class="hljs-string">&quot;Authorization&quot;</span>: <span class="hljs-string">f&quot;Bearer <span class="hljs-subst">{API_TOKEN}</span>&quot;</span>}
API_URL = <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/facebook/bart-large-cnn&quot;</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">payload</span>):
    data = json.dumps(payload)
    response = requests.request(<span class="hljs-string">&quot;POST&quot;</span>, API_URL, headers=headers, data=data)
    <span class="hljs-keyword">return</span> json.loads(response.content.decode(<span class="hljs-string">&quot;utf-8&quot;</span>))
data = query(
    {
        <span class="hljs-string">&quot;inputs&quot;</span>: <span class="hljs-string">&quot;The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world, a title it held for 41 years until the Chrysler Building in New York City was finished in 1930. It was the first structure to reach a height of 300 metres. Due to the addition of a broadcasting aerial at the top of the tower in 1957, it is now taller than the Chrysler Building by 5.2 metres (17 ft). Excluding transmitters, the Eiffel Tower is the second tallest free-standing structure in France after the Millau Viaduct.&quot;</span>,
        <span class="hljs-string">&quot;parameters&quot;</span>: {<span class="hljs-string">&quot;do_sample&quot;</span>: <span class="hljs-literal">False</span>},
    }
)
<span class="hljs-comment"># Response</span>
self.assertEqual(
    data,
    [
        {
            <span class="hljs-string">&quot;summary_text&quot;</span>: <span class="hljs-string">&quot;The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world.&quot;</span>,
        },
    ],
)`}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p:O,i(s){p||(w(n.$$.fragment,s),p=!0)},o(s){E(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function mU(_){let n,p;return n=new R({props:{$$slots:{default:[gU]},$$scope:{ctx:_}}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){p||(w(n.$$.fragment,s),p=!0)},o(s){E(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function $U(_){let n,p;return n=new N({props:{code:`import fetch from "node-fetch";
async function query(data) {
    const response = await fetch(
        "https://api-inference.huggingface.co/models/facebook/bart-large-cnn",
        {
            headers: { Authorization: \`Bearer $}{API_TOKEN}\` },
            method: "POST",
            body: JSON.stringify(data),
        }
    );
    const result = await response.json();
    return result;
}
query({inputs: "The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world, a title it held for 41 years until the Chrysler Building in New York City was finished in 1930. It was the first structure to reach a height of 300 metres. Due to the addition of a broadcasting aerial at the top of the tower in 1957, it is now taller than the Chrysler Building by 5.2 metres (17 ft). Excluding transmitters, the Eiffel Tower is the second tallest free-standing structure in France after the Millau Viaduct."}).then((response) => {
    console.log(JSON.stringify(response));
});
// [{"summary_text":"The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side. It was the first structure to reach a height of 300 metres."}]`,highlighted:`import fetch from <span class="hljs-comment">&quot;node-fetch&quot;</span>;
async function query(data) {
    const response = await fetch(
        <span class="hljs-comment">&quot;https://api-inference.huggingface.co/models/facebook/bart-large-cnn&quot;</span>,
        {
            headers: { <span class="hljs-type">Authorization</span>: \`<span class="hljs-type">Bearer</span> <span class="hljs-string">$}{</span><span class="hljs-type">API_TOKEN</span>}\` },
            method: <span class="hljs-comment">&quot;POST&quot;</span>,
            body: <span class="hljs-type">JSON</span>.stringify(data),
        }
    );
    const result = await response.json();
    return result;
}
query({inputs: <span class="hljs-comment">&quot;The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world, a title it held for 41 years until the Chrysler Building in New York City was finished in 1930. It was the first structure to reach a height of 300 metres. Due to the addition of a broadcasting aerial at the top of the tower in 1957, it is now taller than the Chrysler Building by 5.2 metres (17 ft). Excluding transmitters, the Eiffel Tower is the second tallest free-standing structure in France after the Millau Viaduct.&quot;</span>}).then((response) =&gt; {
    console.log(<span class="hljs-type">JSON</span>.stringify(response));
});
// [{<span class="hljs-comment">&quot;summary_text&quot;</span>:<span class="hljs-comment">&quot;The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side. It was the first structure to reach a height of 300 metres.&quot;</span>}]`}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p:O,i(s){p||(w(n.$$.fragment,s),p=!0)},o(s){E(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function _U(_){let n,p;return n=new R({props:{$$slots:{default:[$U]},$$scope:{ctx:_}}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){p||(w(n.$$.fragment,s),p=!0)},o(s){E(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function qU(_){let n,p;return n=new N({props:{code:`curl https://api-inference.huggingface.co/models/facebook/bart-large-cnn \\
        -X POST \\
        -d '{"inputs": "The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world, a title it held for 41 years until the Chrysler Building in New York City was finished in 1930. It was the first structure to reach a height of 300 metres. Due to the addition of a broadcasting aerial at the top of the tower in 1957, it is now taller than the Chrysler Building by 5.2 metres (17 ft). Excluding transmitters, the Eiffel Tower is the second tallest free-standing structure in France after the Millau Viaduct.", "parameters": {"do_sample": false}}' \\
        -H "Authorization: Bearer $}{HF_API_TOKEN}"
# [{"summary_text":"The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world."}]`,highlighted:`curl https:<span class="hljs-comment">//api-inference.huggingface.co/models/facebook/bart-large-cnn \\
        -X POST \\
        -d &#x27;{&quot;inputs&quot;: &quot;The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world, a title it held for 41 years until the Chrysler Building in New York City was finished in 1930. It was the first structure to reach a height of 300 metres. Due to the addition of a broadcasting aerial at the top of the tower in 1957, it is now taller than the Chrysler Building by 5.2 metres (17 ft). Excluding transmitters, the Eiffel Tower is the second tallest free-standing structure in France after the Millau Viaduct.&quot;, &quot;parameters&quot;: {&quot;do_sample&quot;: false}}&#x27; \\
        -H &quot;Authorization: Bearer $}{HF_API_TOKEN}&quot;</span>
# [{<span class="hljs-string">&quot;summary_text&quot;</span>:<span class="hljs-string">&quot;The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world.&quot;</span>}]`}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p:O,i(s){p||(w(n.$$.fragment,s),p=!0)},o(s){E(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function vU(_){let n,p;return n=new R({props:{$$slots:{default:[qU]},$$scope:{ctx:_}}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){p||(w(n.$$.fragment,s),p=!0)},o(s){E(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function yU(_){let n,p,s,d,$,k,A;return{c(){n=r("p"),p=r("strong"),s=i("Recommended model"),d=i(`:
`),$=r("a"),k=i("microsoft/DialoGPT-large"),A=i("."),this.h()},l(T){n=o(T,"P",{});var j=l(n);p=o(j,"STRONG",{});var P=l(p);s=u(P,"Recommended model"),P.forEach(t),d=u(j,`:
`),$=o(j,"A",{href:!0,rel:!0});var D=l($);k=u(D,"microsoft/DialoGPT-large"),D.forEach(t),A=u(j,"."),j.forEach(t),this.h()},h(){c($,"href","https://huggingface.co/microsoft/DialoGPT-large"),c($,"rel","nofollow")},m(T,j){m(T,n,j),e(n,p),e(p,s),e(n,d),e(n,$),e($,k),e(n,A)},d(T){T&&t(n)}}}function wU(_){let n,p;return n=new N({props:{code:`import json
import requests
headers = {"Authorization": f"Bearer {API_TOKEN}"}
API_URL = "https://api-inference.huggingface.co/models/microsoft/DialoGPT-large"
def query(payload):
    data = json.dumps(payload)
    response = requests.request("POST", API_URL, headers=headers, data=data)
    return json.loads(response.content.decode("utf-8"))
data = query(
    {
        "inputs": {
            "past_user_inputs": ["Which movie is the best ?"],
            "generated_responses": ["It's Die Hard for sure."],
            "text": "Can you explain why ?",
        },
    }
)
# Response
self.assertEqual(
    data,
    {
        "generated_text": "It's the best movie ever.",
        "conversation": {
            "past_user_inputs": [
                "Which movie is the best ?",
                "Can you explain why ?",
            ],
            "generated_responses": [
                "It's Die Hard for sure.",
                "It's the best movie ever.",
            ],
        },
        "warnings": ["Setting \`pad_token_id\` to \`eos_token_id\`:50256 for open-end generation."],
    },
)`,highlighted:`<span class="hljs-keyword">import</span> json
<span class="hljs-keyword">import</span> requests
headers = {<span class="hljs-string">&quot;Authorization&quot;</span>: <span class="hljs-string">f&quot;Bearer <span class="hljs-subst">{API_TOKEN}</span>&quot;</span>}
API_URL = <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/microsoft/DialoGPT-large&quot;</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">payload</span>):
    data = json.dumps(payload)
    response = requests.request(<span class="hljs-string">&quot;POST&quot;</span>, API_URL, headers=headers, data=data)
    <span class="hljs-keyword">return</span> json.loads(response.content.decode(<span class="hljs-string">&quot;utf-8&quot;</span>))
data = query(
    {
        <span class="hljs-string">&quot;inputs&quot;</span>: {
            <span class="hljs-string">&quot;past_user_inputs&quot;</span>: [<span class="hljs-string">&quot;Which movie is the best ?&quot;</span>],
            <span class="hljs-string">&quot;generated_responses&quot;</span>: [<span class="hljs-string">&quot;It&#x27;s Die Hard for sure.&quot;</span>],
            <span class="hljs-string">&quot;text&quot;</span>: <span class="hljs-string">&quot;Can you explain why ?&quot;</span>,
        },
    }
)
<span class="hljs-comment"># Response</span>
self.assertEqual(
    data,
    {
        <span class="hljs-string">&quot;generated_text&quot;</span>: <span class="hljs-string">&quot;It&#x27;s the best movie ever.&quot;</span>,
        <span class="hljs-string">&quot;conversation&quot;</span>: {
            <span class="hljs-string">&quot;past_user_inputs&quot;</span>: [
                <span class="hljs-string">&quot;Which movie is the best ?&quot;</span>,
                <span class="hljs-string">&quot;Can you explain why ?&quot;</span>,
            ],
            <span class="hljs-string">&quot;generated_responses&quot;</span>: [
                <span class="hljs-string">&quot;It&#x27;s Die Hard for sure.&quot;</span>,
                <span class="hljs-string">&quot;It&#x27;s the best movie ever.&quot;</span>,
            ],
        },
        <span class="hljs-string">&quot;warnings&quot;</span>: [<span class="hljs-string">&quot;Setting \`pad_token_id\` to \`eos_token_id\`:50256 for open-end generation.&quot;</span>],
    },
)`}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p:O,i(s){p||(w(n.$$.fragment,s),p=!0)},o(s){E(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function EU(_){let n,p;return n=new R({props:{$$slots:{default:[wU]},$$scope:{ctx:_}}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){p||(w(n.$$.fragment,s),p=!0)},o(s){E(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function bU(_){let n,p;return n=new N({props:{code:`import fetch from "node-fetch";
async function query(data) {
    const response = await fetch(
        "https://api-inference.huggingface.co/models/microsoft/DialoGPT-large",
        {
            headers: { Authorization: \`Bearer $}{API_TOKEN}\` },
            method: "POST",
            body: JSON.stringify(data),
        }
    );
    const result = await response.json();
    return result;
}
query({inputs: {past_user_inputs: ["Which movie is the best ?"], generated_responses: ["It is Die Hard for sure."], text:"Can you explain why ?"}}).then((response) => {
    console.log(JSON.stringify(response));
});
// {"generated_text":"It's the best movie ever.","conversation":{"past_user_inputs":["Which movie is the best ?","Can you explain why ?"],"generated_responses":["It is Die Hard for sure.","It's the best movie ever."]},"warnings":["Setting \`pad_token_id\` to \`eos_token_id\`:50256 for open-end generation."]}`,highlighted:`<span class="hljs-keyword">import</span> <span class="hljs-keyword">fetch</span> <span class="hljs-keyword">from</span> &quot;node-fetch&quot;;
async <span class="hljs-keyword">function</span> query(data) {
    const response = await <span class="hljs-keyword">fetch</span>(
        &quot;https://api-inference.huggingface.co/models/microsoft/DialoGPT-large&quot;,
        {
            headers: { <span class="hljs-keyword">Authorization</span>: \`Bearer $}{API_TOKEN}\` },
            <span class="hljs-keyword">method</span>: &quot;POST&quot;,
            body: <span class="hljs-type">JSON</span>.stringify(data),
        }
    );
    const result = await response.json();
    <span class="hljs-keyword">return</span> result;
}
query({inputs: {past_user_inputs: [&quot;Which movie is the best ?&quot;], generated_responses: [&quot;It is Die Hard for sure.&quot;], <span class="hljs-type">text</span>:&quot;Can you explain why ?&quot;}}).<span class="hljs-keyword">then</span>((response) =&gt; {
    console.log(<span class="hljs-type">JSON</span>.stringify(response));
});
// {&quot;generated_text&quot;:&quot;It&#x27;s the best movie ever.&quot;,&quot;conversation&quot;:{&quot;past_user_inputs&quot;:[&quot;Which movie is the best ?&quot;,&quot;Can you explain why ?&quot;],&quot;generated_responses&quot;:[&quot;It is Die Hard for sure.&quot;,&quot;It&#x27;s the best movie ever.&quot;]},&quot;warnings&quot;:[&quot;Setting \`pad_token_id\` to \`eos_token_id\`:50256 for open-end generation.&quot;]}`}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p:O,i(s){p||(w(n.$$.fragment,s),p=!0)},o(s){E(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function jU(_){let n,p;return n=new R({props:{$$slots:{default:[bU]},$$scope:{ctx:_}}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){p||(w(n.$$.fragment,s),p=!0)},o(s){E(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function TU(_){let n,p;return n=new N({props:{code:`curl https://api-inference.huggingface.co/models/microsoft/DialoGPT-large \\
        -X POST \\
        -d '{"inputs": {"past_user_inputs": ["Which movie is the best ?"], "generated_responses": ["It is Die Hard for sure."], "text":"Can you explain why ?"}}' \\
        -H "Authorization: Bearer $}{HF_API_TOKEN}"
# {"generated_text":"It's the best movie ever.","conversation":{"past_user_inputs":["Which movie is the best ?","Can you explain why ?"],"generated_responses":["It is Die Hard for sure.","It's the best movie ever."]},"warnings":["Setting \`pad_token_id\` to \`eos_token_id\`:50256 for open-end generation."]}`,highlighted:'<span class="hljs-title">curl https:</span>//api-inference.huggingface.co/models/microsoft/DialoGPT-large \\\n        -X POST \\\n        -d &#x27;{<span class="hljs-string">&quot;inputs&quot;</span>: {<span class="hljs-string">&quot;past_user_inputs&quot;</span>: [<span class="hljs-string">&quot;Which movie is the best ?&quot;</span>], <span class="hljs-string">&quot;generated_responses&quot;</span>: [<span class="hljs-string">&quot;It is Die Hard for sure.&quot;</span>], <span class="hljs-string">&quot;text&quot;</span>:<span class="hljs-string">&quot;Can you explain why ?&quot;</span>}}&#x27; \\\n        -H <span class="hljs-string">&quot;Authorization: Bearer $}{HF_API_TOKEN}&quot;</span>\n# {<span class="hljs-string">&quot;generated_text&quot;</span>:<span class="hljs-string">&quot;It&#x27;s the best movie ever.&quot;</span>,<span class="hljs-string">&quot;conversation&quot;</span>:{<span class="hljs-string">&quot;past_user_inputs&quot;</span>:[<span class="hljs-string">&quot;Which movie is the best ?&quot;</span>,<span class="hljs-string">&quot;Can you explain why ?&quot;</span>],<span class="hljs-string">&quot;generated_responses&quot;</span>:[<span class="hljs-string">&quot;It is Die Hard for sure.&quot;</span>,<span class="hljs-string">&quot;It&#x27;s the best movie ever.&quot;</span>]},<span class="hljs-string">&quot;warnings&quot;</span>:[<span class="hljs-string">&quot;Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.&quot;</span>]}'}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p:O,i(s){p||(w(n.$$.fragment,s),p=!0)},o(s){E(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function kU(_){let n,p;return n=new R({props:{$$slots:{default:[TU]},$$scope:{ctx:_}}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){p||(w(n.$$.fragment,s),p=!0)},o(s){E(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function AU(_){let n,p,s,d,$,k,A;return{c(){n=r("p"),p=r("strong"),s=i("Recommended model"),d=i(`:
`),$=r("a"),k=i("google/tapas-base-finetuned-wtq"),A=i("."),this.h()},l(T){n=o(T,"P",{});var j=l(n);p=o(j,"STRONG",{});var P=l(p);s=u(P,"Recommended model"),P.forEach(t),d=u(j,`:
`),$=o(j,"A",{href:!0,rel:!0});var D=l($);k=u(D,"google/tapas-base-finetuned-wtq"),D.forEach(t),A=u(j,"."),j.forEach(t),this.h()},h(){c($,"href","https://huggingface.co/google/tapas-base-finetuned-wtq"),c($,"rel","nofollow")},m(T,j){m(T,n,j),e(n,p),e(p,s),e(n,d),e(n,$),e($,k),e(n,A)},d(T){T&&t(n)}}}function DU(_){let n,p;return n=new N({props:{code:`import json
import requests
headers = {"Authorization": f"Bearer {API_TOKEN}"}
API_URL = "https://api-inference.huggingface.co/models/google/tapas-base-finetuned-wtq"
def query(payload):
    data = json.dumps(payload)
    response = requests.request("POST", API_URL, headers=headers, data=data)
    return json.loads(response.content.decode("utf-8"))
data = query(
    {
        "inputs": {
            "query": "How many stars does the transformers repository have?",
            "table": {
                "Repository": ["Transformers", "Datasets", "Tokenizers"],
                "Stars": ["36542", "4512", "3934"],
                "Contributors": ["651", "77", "34"],
                "Programming language": [
                    "Python",
                    "Python",
                    "Rust, Python and NodeJS",
                ],
            },
        }
    }
)`,highlighted:`<span class="hljs-keyword">import</span> json
<span class="hljs-keyword">import</span> requests
headers = {<span class="hljs-string">&quot;Authorization&quot;</span>: <span class="hljs-string">f&quot;Bearer <span class="hljs-subst">{API_TOKEN}</span>&quot;</span>}
API_URL = <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/google/tapas-base-finetuned-wtq&quot;</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">payload</span>):
    data = json.dumps(payload)
    response = requests.request(<span class="hljs-string">&quot;POST&quot;</span>, API_URL, headers=headers, data=data)
    <span class="hljs-keyword">return</span> json.loads(response.content.decode(<span class="hljs-string">&quot;utf-8&quot;</span>))
data = query(
    {
        <span class="hljs-string">&quot;inputs&quot;</span>: {
            <span class="hljs-string">&quot;query&quot;</span>: <span class="hljs-string">&quot;How many stars does the transformers repository have?&quot;</span>,
            <span class="hljs-string">&quot;table&quot;</span>: {
                <span class="hljs-string">&quot;Repository&quot;</span>: [<span class="hljs-string">&quot;Transformers&quot;</span>, <span class="hljs-string">&quot;Datasets&quot;</span>, <span class="hljs-string">&quot;Tokenizers&quot;</span>],
                <span class="hljs-string">&quot;Stars&quot;</span>: [<span class="hljs-string">&quot;36542&quot;</span>, <span class="hljs-string">&quot;4512&quot;</span>, <span class="hljs-string">&quot;3934&quot;</span>],
                <span class="hljs-string">&quot;Contributors&quot;</span>: [<span class="hljs-string">&quot;651&quot;</span>, <span class="hljs-string">&quot;77&quot;</span>, <span class="hljs-string">&quot;34&quot;</span>],
                <span class="hljs-string">&quot;Programming language&quot;</span>: [
                    <span class="hljs-string">&quot;Python&quot;</span>,
                    <span class="hljs-string">&quot;Python&quot;</span>,
                    <span class="hljs-string">&quot;Rust, Python and NodeJS&quot;</span>,
                ],
            },
        }
    }
)`}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p:O,i(s){p||(w(n.$$.fragment,s),p=!0)},o(s){E(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function PU(_){let n,p;return n=new R({props:{$$slots:{default:[DU]},$$scope:{ctx:_}}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){p||(w(n.$$.fragment,s),p=!0)},o(s){E(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function OU(_){let n,p;return n=new N({props:{code:`import fetch from "node-fetch";
async function query(data) {
    const response = await fetch(
        "https://api-inference.huggingface.co/models/google/tapas-base-finetuned-wtq",
        {
            headers: { Authorization: \`Bearer $}{API_TOKEN}\` },
            method: "POST",
            body: JSON.stringify(data),
        }
    );
    const result = await response.json();
    return result;
}
query({inputs:{query:"How many stars does the transformers repository have?",table:{Repository:["Transformers","Datasets","Tokenizers"],Stars:["36542","4512","3934"],Contributors:["651","77","34"],"Programming language":["Python","Python","Rust, Python and NodeJS"]}}}).then((response) => {
    console.log(JSON.stringify(response));
});
// {"answer":"AVERAGE > 36542","coordinates":[[0,1]],"cells":["36542"],"aggregator":"AVERAGE"}`,highlighted:`<span class="hljs-keyword">import</span> <span class="hljs-keyword">fetch</span> <span class="hljs-keyword">from</span> &quot;node-fetch&quot;;
async <span class="hljs-keyword">function</span> query(data) {
    const response = await <span class="hljs-keyword">fetch</span>(
        &quot;https://api-inference.huggingface.co/models/google/tapas-base-finetuned-wtq&quot;,
        {
            headers: { <span class="hljs-keyword">Authorization</span>: \`Bearer $}{API_TOKEN}\` },
            <span class="hljs-keyword">method</span>: &quot;POST&quot;,
            body: <span class="hljs-type">JSON</span>.stringify(data),
        }
    );
    const result = await response.json();
    <span class="hljs-keyword">return</span> result;
}
query({inputs:{query:&quot;How many stars does the transformers repository have?&quot;,<span class="hljs-keyword">table</span>:{Repository:[&quot;Transformers&quot;,&quot;Datasets&quot;,&quot;Tokenizers&quot;],Stars:[&quot;36542&quot;,&quot;4512&quot;,&quot;3934&quot;],Contributors:[&quot;651&quot;,&quot;77&quot;,&quot;34&quot;],&quot;Programming language&quot;:[&quot;Python&quot;,&quot;Python&quot;,&quot;Rust, Python and NodeJS&quot;]}}}).<span class="hljs-keyword">then</span>((response) =&gt; {
    console.log(<span class="hljs-type">JSON</span>.stringify(response));
});
// {&quot;answer&quot;:&quot;AVERAGE &gt; 36542&quot;,&quot;coordinates&quot;:[[<span class="hljs-number">0</span>,<span class="hljs-number">1</span>]],&quot;cells&quot;:[&quot;36542&quot;],&quot;aggregator&quot;:&quot;AVERAGE&quot;}`}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p:O,i(s){p||(w(n.$$.fragment,s),p=!0)},o(s){E(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function RU(_){let n,p;return n=new R({props:{$$slots:{default:[OU]},$$scope:{ctx:_}}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){p||(w(n.$$.fragment,s),p=!0)},o(s){E(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function NU(_){let n,p;return n=new N({props:{code:`curl https://api-inference.huggingface.co/models/google/tapas-base-finetuned-wtq \\
        -X POST \\
        -d '{"inputs":{"query":"How many stars does the transformers repository have?","table":{"Repository":["Transformers","Datasets","Tokenizers"],"Stars":["36542","4512","3934"],"Contributors":["651","77","34"],"Programming language":["Python","Python","Rust, Python and NodeJS"]}}}' \\
        -H "Authorization: Bearer $}{HF_API_TOKEN}"
# {"answer":"AVERAGE > 36542","coordinates":[[0,1]],"cells":["36542"],"aggregator":"AVERAGE"}`,highlighted:`curl https:<span class="hljs-comment">//api-inference.huggingface.co/models/google/tapas-base-finetuned-wtq \\</span>
        -X <span class="hljs-keyword">POST</span> \\
        -<span class="hljs-keyword">d</span> &#x27;{<span class="hljs-string">&quot;inputs&quot;</span>:{<span class="hljs-string">&quot;query&quot;</span>:<span class="hljs-string">&quot;How many stars does the transformers repository have?&quot;</span>,<span class="hljs-string">&quot;table&quot;</span>:{<span class="hljs-string">&quot;Repository&quot;</span>:[<span class="hljs-string">&quot;Transformers&quot;</span>,<span class="hljs-string">&quot;Datasets&quot;</span>,<span class="hljs-string">&quot;Tokenizers&quot;</span>],<span class="hljs-string">&quot;Stars&quot;</span>:[<span class="hljs-string">&quot;36542&quot;</span>,<span class="hljs-string">&quot;4512&quot;</span>,<span class="hljs-string">&quot;3934&quot;</span>],<span class="hljs-string">&quot;Contributors&quot;</span>:[<span class="hljs-string">&quot;651&quot;</span>,<span class="hljs-string">&quot;77&quot;</span>,<span class="hljs-string">&quot;34&quot;</span>],<span class="hljs-string">&quot;Programming language&quot;</span>:[<span class="hljs-string">&quot;Python&quot;</span>,<span class="hljs-string">&quot;Python&quot;</span>,<span class="hljs-string">&quot;Rust, Python and NodeJS&quot;</span>]}}}&#x27; \\
        -<span class="hljs-keyword">H</span> <span class="hljs-string">&quot;Authorization: Bearer $}{HF_API_TOKEN}&quot;</span>
# {<span class="hljs-string">&quot;answer&quot;</span>:<span class="hljs-string">&quot;AVERAGE &gt; 36542&quot;</span>,<span class="hljs-string">&quot;coordinates&quot;</span>:[[0,1]],<span class="hljs-string">&quot;cells&quot;</span>:[<span class="hljs-string">&quot;36542&quot;</span>],<span class="hljs-string">&quot;aggregator&quot;</span>:<span class="hljs-string">&quot;AVERAGE&quot;</span>}`}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p:O,i(s){p||(w(n.$$.fragment,s),p=!0)},o(s){E(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function SU(_){let n,p;return n=new R({props:{$$slots:{default:[NU]},$$scope:{ctx:_}}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){p||(w(n.$$.fragment,s),p=!0)},o(s){E(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function xU(_){let n,p;return n=new N({props:{code:`self.assertEqual(
    data,
    {
        "answer": "AVERAGE > 36542",
        "coordinates": [[0, 1]],
        "cells": ["36542"],
        "aggregator": "AVERAGE",
    },
)`,highlighted:`self.assertEqual(
    data,
    {
        <span class="hljs-string">&quot;answer&quot;</span>: <span class="hljs-string">&quot;AVERAGE &gt; 36542&quot;</span>,
        <span class="hljs-string">&quot;coordinates&quot;</span>: [[<span class="hljs-number">0</span>, <span class="hljs-number">1</span>]],
        <span class="hljs-string">&quot;cells&quot;</span>: [<span class="hljs-string">&quot;36542&quot;</span>],
        <span class="hljs-string">&quot;aggregator&quot;</span>: <span class="hljs-string">&quot;AVERAGE&quot;</span>,
    },
)`}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p:O,i(s){p||(w(n.$$.fragment,s),p=!0)},o(s){E(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function IU(_){let n,p;return n=new R({props:{$$slots:{default:[xU]},$$scope:{ctx:_}}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){p||(w(n.$$.fragment,s),p=!0)},o(s){E(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function HU(_){let n,p,s,d,$,k,A;return{c(){n=r("p"),p=r("strong"),s=i("Recommended model"),d=i(`:
`),$=r("a"),k=i("deepset/roberta-base-squad2"),A=i("."),this.h()},l(T){n=o(T,"P",{});var j=l(n);p=o(j,"STRONG",{});var P=l(p);s=u(P,"Recommended model"),P.forEach(t),d=u(j,`:
`),$=o(j,"A",{href:!0,rel:!0});var D=l($);k=u(D,"deepset/roberta-base-squad2"),D.forEach(t),A=u(j,"."),j.forEach(t),this.h()},h(){c($,"href","https://huggingface.co/deepset/roberta-base-squad2"),c($,"rel","nofollow")},m(T,j){m(T,n,j),e(n,p),e(p,s),e(n,d),e(n,$),e($,k),e(n,A)},d(T){T&&t(n)}}}function BU(_){let n,p;return n=new N({props:{code:`import json
import requests
headers = {"Authorization": f"Bearer {API_TOKEN}"}
API_URL = "https://api-inference.huggingface.co/models/deepset/roberta-base-squad2"
def query(payload):
    data = json.dumps(payload)
    response = requests.request("POST", API_URL, headers=headers, data=data)
    return json.loads(response.content.decode("utf-8"))
data = query(
    {
        "inputs": {
            "question": "What's my name?",
            "context": "My name is Clara and I live in Berkeley.",
        }
    }
)`,highlighted:`<span class="hljs-keyword">import</span> json
<span class="hljs-keyword">import</span> requests
headers = {<span class="hljs-string">&quot;Authorization&quot;</span>: <span class="hljs-string">f&quot;Bearer <span class="hljs-subst">{API_TOKEN}</span>&quot;</span>}
API_URL = <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/deepset/roberta-base-squad2&quot;</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">payload</span>):
    data = json.dumps(payload)
    response = requests.request(<span class="hljs-string">&quot;POST&quot;</span>, API_URL, headers=headers, data=data)
    <span class="hljs-keyword">return</span> json.loads(response.content.decode(<span class="hljs-string">&quot;utf-8&quot;</span>))
data = query(
    {
        <span class="hljs-string">&quot;inputs&quot;</span>: {
            <span class="hljs-string">&quot;question&quot;</span>: <span class="hljs-string">&quot;What&#x27;s my name?&quot;</span>,
            <span class="hljs-string">&quot;context&quot;</span>: <span class="hljs-string">&quot;My name is Clara and I live in Berkeley.&quot;</span>,
        }
    }
)`}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p:O,i(s){p||(w(n.$$.fragment,s),p=!0)},o(s){E(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function CU(_){let n,p;return n=new R({props:{$$slots:{default:[BU]},$$scope:{ctx:_}}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){p||(w(n.$$.fragment,s),p=!0)},o(s){E(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function GU(_){let n,p;return n=new N({props:{code:`import fetch from "node-fetch";
async function query(data) {
    const response = await fetch(
        "https://api-inference.huggingface.co/models/deepset/roberta-base-squad2",
        {
            headers: { Authorization: \`Bearer $}{API_TOKEN}\` },
            method: "POST",
            body: JSON.stringify(data),
        }
    );
    const result = await response.json();
    return result;
}
query({inputs:{question:"What is my name?",context:"My name is Clara and I live in Berkeley."}}).then((response) => {
    console.log(JSON.stringify(response));
});
// {"score":0.933128833770752,"start":11,"end":16,"answer":"Clara"}`,highlighted:`<span class="hljs-keyword">import</span> fetch <span class="hljs-keyword">from</span> <span class="hljs-string">&quot;node-fetch&quot;</span>;
<span class="hljs-keyword">async</span> <span class="hljs-keyword">function</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">data</span>) {
    <span class="hljs-keyword">const</span> response = <span class="hljs-keyword">await</span> <span class="hljs-title function_">fetch</span>(
        <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/deepset/roberta-base-squad2&quot;</span>,
        {
            <span class="hljs-attr">headers</span>: { <span class="hljs-title class_">Authorization</span>: <span class="hljs-string">\`Bearer <span class="hljs-subst">$}{API_TOKEN}</span>\`</span> },
            <span class="hljs-attr">method</span>: <span class="hljs-string">&quot;POST&quot;</span>,
            <span class="hljs-attr">body</span>: <span class="hljs-title class_">JSON</span>.<span class="hljs-title function_">stringify</span>(data),
        }
    );
    <span class="hljs-keyword">const</span> result = <span class="hljs-keyword">await</span> response.<span class="hljs-title function_">json</span>();
    <span class="hljs-keyword">return</span> result;
}
<span class="hljs-title function_">query</span>({<span class="hljs-attr">inputs</span>:{<span class="hljs-attr">question</span>:<span class="hljs-string">&quot;What is my name?&quot;</span>,<span class="hljs-attr">context</span>:<span class="hljs-string">&quot;My name is Clara and I live in Berkeley.&quot;</span>}}).<span class="hljs-title function_">then</span>(<span class="hljs-function">(<span class="hljs-params">response</span>) =&gt;</span> {
    <span class="hljs-variable language_">console</span>.<span class="hljs-title function_">log</span>(<span class="hljs-title class_">JSON</span>.<span class="hljs-title function_">stringify</span>(response));
});
<span class="hljs-comment">// {&quot;score&quot;:0.933128833770752,&quot;start&quot;:11,&quot;end&quot;:16,&quot;answer&quot;:&quot;Clara&quot;}</span>`}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p:O,i(s){p||(w(n.$$.fragment,s),p=!0)},o(s){E(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function UU(_){let n,p;return n=new R({props:{$$slots:{default:[GU]},$$scope:{ctx:_}}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){p||(w(n.$$.fragment,s),p=!0)},o(s){E(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function LU(_){let n,p;return n=new N({props:{code:`curl https://api-inference.huggingface.co/models/deepset/roberta-base-squad2 \\
        -X POST \\
        -d '{"inputs":{"question":"What is my name?","context":"My name is Clara and I live in Berkeley."}}' \\
        -H "Authorization: Bearer $}{HF_API_TOKEN}"
# {"score":0.933128833770752,"start":11,"end":16,"answer":"Clara"}`,highlighted:`curl https:<span class="hljs-comment">//api-inference.huggingface.co/models/deepset/roberta-base-squad2 \\</span>
        -X <span class="hljs-keyword">POST</span> \\
        -<span class="hljs-keyword">d</span> &#x27;{<span class="hljs-string">&quot;inputs&quot;</span>:{<span class="hljs-string">&quot;question&quot;</span>:<span class="hljs-string">&quot;What is my name?&quot;</span>,<span class="hljs-string">&quot;context&quot;</span>:<span class="hljs-string">&quot;My name is Clara and I live in Berkeley.&quot;</span>}}&#x27; \\
        -<span class="hljs-keyword">H</span> <span class="hljs-string">&quot;Authorization: Bearer $}{HF_API_TOKEN}&quot;</span>
# {<span class="hljs-string">&quot;score&quot;</span>:0.933128833770752,<span class="hljs-string">&quot;start&quot;</span>:11,<span class="hljs-string">&quot;end&quot;</span>:16,<span class="hljs-string">&quot;answer&quot;</span>:<span class="hljs-string">&quot;Clara&quot;</span>}`}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p:O,i(s){p||(w(n.$$.fragment,s),p=!0)},o(s){E(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function zU(_){let n,p;return n=new R({props:{$$slots:{default:[LU]},$$scope:{ctx:_}}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){p||(w(n.$$.fragment,s),p=!0)},o(s){E(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function MU(_){let n,p;return n=new N({props:{code:`self.assertEqual(
    deep_round(data),
    {"score": 0.9327, "start": 11, "end": 16, "answer": "Clara"},
)`,highlighted:`self.assertEqual(
    deep_round(data),
    {<span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.9327</span>, <span class="hljs-string">&quot;start&quot;</span>: <span class="hljs-number">11</span>, <span class="hljs-string">&quot;end&quot;</span>: <span class="hljs-number">16</span>, <span class="hljs-string">&quot;answer&quot;</span>: <span class="hljs-string">&quot;Clara&quot;</span>},
)`}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p:O,i(s){p||(w(n.$$.fragment,s),p=!0)},o(s){E(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function FU(_){let n,p;return n=new R({props:{$$slots:{default:[MU]},$$scope:{ctx:_}}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){p||(w(n.$$.fragment,s),p=!0)},o(s){E(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function JU(_){let n,p,s,d,$,k;return{c(){n=r("p"),p=r("strong"),s=i("Recommended model"),d=i(`:
`),$=r("a"),k=i("distilbert-base-uncased-finetuned-sst-2-english"),this.h()},l(A){n=o(A,"P",{});var T=l(n);p=o(T,"STRONG",{});var j=l(p);s=u(j,"Recommended model"),j.forEach(t),d=u(T,`:
`),$=o(T,"A",{href:!0,rel:!0});var P=l($);k=u(P,"distilbert-base-uncased-finetuned-sst-2-english"),P.forEach(t),T.forEach(t),this.h()},h(){c($,"href","https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english"),c($,"rel","nofollow")},m(A,T){m(A,n,T),e(n,p),e(p,s),e(n,d),e(n,$),e($,k)},d(A){A&&t(n)}}}function KU(_){let n,p;return n=new N({props:{code:`import json
import requests
headers = {"Authorization": f"Bearer {API_TOKEN}"}
API_URL = "https://api-inference.huggingface.co/models/distilbert-base-uncased-finetuned-sst-2-english"
def query(payload):
    data = json.dumps(payload)
    response = requests.request("POST", API_URL, headers=headers, data=data)
    return json.loads(response.content.decode("utf-8"))
data = query({"inputs": "I like you. I love you"})`,highlighted:`<span class="hljs-keyword">import</span> json
<span class="hljs-keyword">import</span> requests
headers = {<span class="hljs-string">&quot;Authorization&quot;</span>: <span class="hljs-string">f&quot;Bearer <span class="hljs-subst">{API_TOKEN}</span>&quot;</span>}
API_URL = <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/distilbert-base-uncased-finetuned-sst-2-english&quot;</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">payload</span>):
    data = json.dumps(payload)
    response = requests.request(<span class="hljs-string">&quot;POST&quot;</span>, API_URL, headers=headers, data=data)
    <span class="hljs-keyword">return</span> json.loads(response.content.decode(<span class="hljs-string">&quot;utf-8&quot;</span>))
data = query({<span class="hljs-string">&quot;inputs&quot;</span>: <span class="hljs-string">&quot;I like you. I love you&quot;</span>})`}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p:O,i(s){p||(w(n.$$.fragment,s),p=!0)},o(s){E(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function WU(_){let n,p;return n=new R({props:{$$slots:{default:[KU]},$$scope:{ctx:_}}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){p||(w(n.$$.fragment,s),p=!0)},o(s){E(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function YU(_){let n,p;return n=new N({props:{code:`import fetch from "node-fetch";
async function query(data) {
    const response = await fetch(
        "https://api-inference.huggingface.co/models/distilbert-base-uncased-finetuned-sst-2-english",
        {
            headers: { Authorization: \`Bearer $}{API_TOKEN}\` },
            method: "POST",
            body: JSON.stringify(data),
        }
    );
    const result = await response.json();
    return result;
}
query({inputs:"I like you. I love you"}).then((response) => {
    console.log(JSON.stringify(response));
});
// [[{"label":"NEGATIVE","score":0.0001261125144083053},{"label":"POSITIVE","score":0.9998738765716553}]]`,highlighted:`<span class="hljs-keyword">import</span> fetch <span class="hljs-keyword">from</span> <span class="hljs-string">&quot;node-fetch&quot;</span>;
<span class="hljs-keyword">async</span> <span class="hljs-keyword">function</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">data</span>) {
    <span class="hljs-keyword">const</span> response = <span class="hljs-keyword">await</span> <span class="hljs-title function_">fetch</span>(
        <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/distilbert-base-uncased-finetuned-sst-2-english&quot;</span>,
        {
            <span class="hljs-attr">headers</span>: { <span class="hljs-title class_">Authorization</span>: <span class="hljs-string">\`Bearer <span class="hljs-subst">$}{API_TOKEN}</span>\`</span> },
            <span class="hljs-attr">method</span>: <span class="hljs-string">&quot;POST&quot;</span>,
            <span class="hljs-attr">body</span>: <span class="hljs-title class_">JSON</span>.<span class="hljs-title function_">stringify</span>(data),
        }
    );
    <span class="hljs-keyword">const</span> result = <span class="hljs-keyword">await</span> response.<span class="hljs-title function_">json</span>();
    <span class="hljs-keyword">return</span> result;
}
<span class="hljs-title function_">query</span>({<span class="hljs-attr">inputs</span>:<span class="hljs-string">&quot;I like you. I love you&quot;</span>}).<span class="hljs-title function_">then</span>(<span class="hljs-function">(<span class="hljs-params">response</span>) =&gt;</span> {
    <span class="hljs-variable language_">console</span>.<span class="hljs-title function_">log</span>(<span class="hljs-title class_">JSON</span>.<span class="hljs-title function_">stringify</span>(response));
});
<span class="hljs-comment">// [[{&quot;label&quot;:&quot;NEGATIVE&quot;,&quot;score&quot;:0.0001261125144083053},{&quot;label&quot;:&quot;POSITIVE&quot;,&quot;score&quot;:0.9998738765716553}]]</span>`}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p:O,i(s){p||(w(n.$$.fragment,s),p=!0)},o(s){E(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function VU(_){let n,p;return n=new R({props:{$$slots:{default:[YU]},$$scope:{ctx:_}}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){p||(w(n.$$.fragment,s),p=!0)},o(s){E(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function XU(_){let n,p;return n=new N({props:{code:`curl https://api-inference.huggingface.co/models/distilbert-base-uncased-finetuned-sst-2-english \\
        -X POST \\
        -d '{"inputs":"I like you. I love you"}' \\
        -H "Authorization: Bearer $}{HF_API_TOKEN}"
# [[{"label":"NEGATIVE","score":0.0001261125144083053},{"label":"POSITIVE","score":0.9998738765716553}]]`,highlighted:`curl https:<span class="hljs-comment">//api-inference.huggingface.co/models/distilbert-base-uncased-finetuned-sst-2-english \\
        -X POST \\
        -d &#x27;{&quot;inputs&quot;:&quot;I like you. I love you&quot;}&#x27; \\
        -H &quot;Authorization: Bearer $}{HF_API_TOKEN}&quot;</span>
# [[{<span class="hljs-string">&quot;label&quot;</span>:<span class="hljs-string">&quot;NEGATIVE&quot;</span>,<span class="hljs-string">&quot;score&quot;</span>:<span class="hljs-number">0.0001261125144083053</span>},{<span class="hljs-string">&quot;label&quot;</span>:<span class="hljs-string">&quot;POSITIVE&quot;</span>,<span class="hljs-string">&quot;score&quot;</span>:<span class="hljs-number">0.9998738765716553</span>}]]`}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p:O,i(s){p||(w(n.$$.fragment,s),p=!0)},o(s){E(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function QU(_){let n,p;return n=new R({props:{$$slots:{default:[XU]},$$scope:{ctx:_}}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){p||(w(n.$$.fragment,s),p=!0)},o(s){E(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function ZU(_){let n,p;return n=new N({props:{code:`self.assertEqual(
    deep_round(data),
    [
        [
            {"label": "NEGATIVE", "score": 0.0001},
            {"label": "POSITIVE", "score": 0.9999},
        ]
    ],
)`,highlighted:`self.assertEqual(
    deep_round(data),
    [
        [
            {<span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;NEGATIVE&quot;</span>, <span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.0001</span>},
            {<span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;POSITIVE&quot;</span>, <span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.9999</span>},
        ]
    ],
)`}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p:O,i(s){p||(w(n.$$.fragment,s),p=!0)},o(s){E(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function eL(_){let n,p;return n=new R({props:{$$slots:{default:[ZU]},$$scope:{ctx:_}}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){p||(w(n.$$.fragment,s),p=!0)},o(s){E(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function tL(_){let n,p,s,d,$,k;return{c(){n=r("p"),p=r("strong"),s=i("Recommended model"),d=i(`:
`),$=r("a"),k=i("dbmdz/bert-large-cased-finetuned-conll03-english"),this.h()},l(A){n=o(A,"P",{});var T=l(n);p=o(T,"STRONG",{});var j=l(p);s=u(j,"Recommended model"),j.forEach(t),d=u(T,`:
`),$=o(T,"A",{href:!0,rel:!0});var P=l($);k=u(P,"dbmdz/bert-large-cased-finetuned-conll03-english"),P.forEach(t),T.forEach(t),this.h()},h(){c($,"href","https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english"),c($,"rel","nofollow")},m(A,T){m(A,n,T),e(n,p),e(p,s),e(n,d),e(n,$),e($,k)},d(A){A&&t(n)}}}function sL(_){let n,p;return n=new N({props:{code:`import json
import requests
headers = {"Authorization": f"Bearer {API_TOKEN}"}
API_URL = "https://api-inference.huggingface.co/models/dbmdz/bert-large-cased-finetuned-conll03-english"
def query(payload):
    data = json.dumps(payload)
    response = requests.request("POST", API_URL, headers=headers, data=data)
    return json.loads(response.content.decode("utf-8"))
data = query({"inputs": "My name is Sarah Jessica Parker but you can call me Jessica"})`,highlighted:`<span class="hljs-keyword">import</span> json
<span class="hljs-keyword">import</span> requests
headers = {<span class="hljs-string">&quot;Authorization&quot;</span>: <span class="hljs-string">f&quot;Bearer <span class="hljs-subst">{API_TOKEN}</span>&quot;</span>}
API_URL = <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/dbmdz/bert-large-cased-finetuned-conll03-english&quot;</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">payload</span>):
    data = json.dumps(payload)
    response = requests.request(<span class="hljs-string">&quot;POST&quot;</span>, API_URL, headers=headers, data=data)
    <span class="hljs-keyword">return</span> json.loads(response.content.decode(<span class="hljs-string">&quot;utf-8&quot;</span>))
data = query({<span class="hljs-string">&quot;inputs&quot;</span>: <span class="hljs-string">&quot;My name is Sarah Jessica Parker but you can call me Jessica&quot;</span>})`}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p:O,i(s){p||(w(n.$$.fragment,s),p=!0)},o(s){E(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function aL(_){let n,p;return n=new R({props:{$$slots:{default:[sL]},$$scope:{ctx:_}}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){p||(w(n.$$.fragment,s),p=!0)},o(s){E(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function nL(_){let n,p;return n=new N({props:{code:`import fetch from "node-fetch";
async function query(data) {
    const response = await fetch(
        "https://api-inference.huggingface.co/models/dbmdz/bert-large-cased-finetuned-conll03-english",
        {
            headers: { Authorization: \`Bearer $}{API_TOKEN}\` },
            method: "POST",
            body: JSON.stringify(data),
        }
    );
    const result = await response.json();
    return result;
}
query({inputs:"My name is Sarah Jessica Parker but you can call me Jessica"}).then((response) => {
    console.log(JSON.stringify(response));
});
// [{"entity_group":"PER","score":0.9991337060928345,"word":"Sarah Jessica Parker","start":11,"end":31},{"entity_group":"PER","score":0.9979912042617798,"word":"Jessica","start":52,"end":59}]`,highlighted:`<span class="hljs-keyword">import</span> <span class="hljs-keyword">fetch</span> <span class="hljs-keyword">from</span> &quot;node-fetch&quot;;
async <span class="hljs-keyword">function</span> query(data) {
    const response = await <span class="hljs-keyword">fetch</span>(
        &quot;https://api-inference.huggingface.co/models/dbmdz/bert-large-cased-finetuned-conll03-english&quot;,
        {
            headers: { <span class="hljs-keyword">Authorization</span>: \`Bearer $}{API_TOKEN}\` },
            <span class="hljs-keyword">method</span>: &quot;POST&quot;,
            body: <span class="hljs-type">JSON</span>.stringify(data),
        }
    );
    const result = await response.json();
    <span class="hljs-keyword">return</span> result;
}
query({inputs:&quot;My name is Sarah Jessica Parker but you can call me Jessica&quot;}).<span class="hljs-keyword">then</span>((response) =&gt; {
    console.log(<span class="hljs-type">JSON</span>.stringify(response));
});
// [{&quot;entity_group&quot;:&quot;PER&quot;,&quot;score&quot;:<span class="hljs-number">0.9991337060928345</span>,&quot;word&quot;:&quot;Sarah Jessica Parker&quot;,&quot;start&quot;:<span class="hljs-number">11</span>,&quot;end&quot;:<span class="hljs-number">31</span>},{&quot;entity_group&quot;:&quot;PER&quot;,&quot;score&quot;:<span class="hljs-number">0.9979912042617798</span>,&quot;word&quot;:&quot;Jessica&quot;,&quot;start&quot;:<span class="hljs-number">52</span>,&quot;end&quot;:<span class="hljs-number">59</span>}]`}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p:O,i(s){p||(w(n.$$.fragment,s),p=!0)},o(s){E(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function rL(_){let n,p;return n=new R({props:{$$slots:{default:[nL]},$$scope:{ctx:_}}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){p||(w(n.$$.fragment,s),p=!0)},o(s){E(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function oL(_){let n,p;return n=new N({props:{code:`curl https://api-inference.huggingface.co/models/dbmdz/bert-large-cased-finetuned-conll03-english \\
        -X POST \\
        -d '{"inputs":"My name is Sarah Jessica Parker but you can call me Jessica"}' \\
        -H "Authorization: Bearer $}{HF_API_TOKEN}"
# [{"entity_group":"PER","score":0.9991337060928345,"word":"Sarah Jessica Parker","start":11,"end":31},{"entity_group":"PER","score":0.9979912042617798,"word":"Jessica","start":52,"end":59}]`,highlighted:`curl https:<span class="hljs-comment">//api-inference.huggingface.co/models/dbmdz/bert-large-cased-finetuned-conll03-english \\
        -X POST \\
        -d &#x27;{&quot;inputs&quot;:&quot;My name is Sarah Jessica Parker but you can call me Jessica&quot;}&#x27; \\
        -H &quot;Authorization: Bearer $}{HF_API_TOKEN}&quot;</span>
# [{<span class="hljs-string">&quot;entity_group&quot;</span>:<span class="hljs-string">&quot;PER&quot;</span>,<span class="hljs-string">&quot;score&quot;</span>:<span class="hljs-number">0.9991337060928345</span>,<span class="hljs-string">&quot;word&quot;</span>:<span class="hljs-string">&quot;Sarah Jessica Parker&quot;</span>,<span class="hljs-string">&quot;start&quot;</span>:<span class="hljs-number">11</span>,<span class="hljs-string">&quot;end&quot;</span>:<span class="hljs-number">31</span>},{<span class="hljs-string">&quot;entity_group&quot;</span>:<span class="hljs-string">&quot;PER&quot;</span>,<span class="hljs-string">&quot;score&quot;</span>:<span class="hljs-number">0.9979912042617798</span>,<span class="hljs-string">&quot;word&quot;</span>:<span class="hljs-string">&quot;Jessica&quot;</span>,<span class="hljs-string">&quot;start&quot;</span>:<span class="hljs-number">52</span>,<span class="hljs-string">&quot;end&quot;</span>:<span class="hljs-number">59</span>}]`}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p:O,i(s){p||(w(n.$$.fragment,s),p=!0)},o(s){E(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function lL(_){let n,p;return n=new R({props:{$$slots:{default:[oL]},$$scope:{ctx:_}}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){p||(w(n.$$.fragment,s),p=!0)},o(s){E(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function iL(_){let n,p;return n=new N({props:{code:`self.assertEqual(
    deep_round(data),
    [
        {
            "entity_group": "PER",
            "score": 0.9991,
            "word": "Sarah Jessica Parker",
            "start": 11,
            "end": 31,
        },
        {
            "entity_group": "PER",
            "score": 0.998,
            "word": "Jessica",
            "start": 52,
            "end": 59,
        },
    ],
)`,highlighted:`self.assertEqual(
    deep_round(data),
    [
        {
            <span class="hljs-string">&quot;entity_group&quot;</span>: <span class="hljs-string">&quot;PER&quot;</span>,
            <span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.9991</span>,
            <span class="hljs-string">&quot;word&quot;</span>: <span class="hljs-string">&quot;Sarah Jessica Parker&quot;</span>,
            <span class="hljs-string">&quot;start&quot;</span>: <span class="hljs-number">11</span>,
            <span class="hljs-string">&quot;end&quot;</span>: <span class="hljs-number">31</span>,
        },
        {
            <span class="hljs-string">&quot;entity_group&quot;</span>: <span class="hljs-string">&quot;PER&quot;</span>,
            <span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.998</span>,
            <span class="hljs-string">&quot;word&quot;</span>: <span class="hljs-string">&quot;Jessica&quot;</span>,
            <span class="hljs-string">&quot;start&quot;</span>: <span class="hljs-number">52</span>,
            <span class="hljs-string">&quot;end&quot;</span>: <span class="hljs-number">59</span>,
        },
    ],
)`}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p:O,i(s){p||(w(n.$$.fragment,s),p=!0)},o(s){E(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function uL(_){let n,p;return n=new R({props:{$$slots:{default:[iL]},$$scope:{ctx:_}}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){p||(w(n.$$.fragment,s),p=!0)},o(s){E(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function pL(_){let n,p,s,d,$,k,A;return{c(){n=r("p"),p=r("strong"),s=i("Recommended model"),d=i(": "),$=r("a"),k=i("gpt2"),A=i(" (it\u2019s a simple model, but fun to play with)."),this.h()},l(T){n=o(T,"P",{});var j=l(n);p=o(j,"STRONG",{});var P=l(p);s=u(P,"Recommended model"),P.forEach(t),d=u(j,": "),$=o(j,"A",{href:!0,rel:!0});var D=l($);k=u(D,"gpt2"),D.forEach(t),A=u(j," (it\u2019s a simple model, but fun to play with)."),j.forEach(t),this.h()},h(){c($,"href","https://huggingface.co/gpt2"),c($,"rel","nofollow")},m(T,j){m(T,n,j),e(n,p),e(p,s),e(n,d),e(n,$),e($,k),e(n,A)},d(T){T&&t(n)}}}function cL(_){let n,p;return n=new N({props:{code:`import json
import requests
headers = {"Authorization": f"Bearer {API_TOKEN}"}
API_URL = "https://api-inference.huggingface.co/models/gpt2"
def query(payload):
    data = json.dumps(payload)
    response = requests.request("POST", API_URL, headers=headers, data=data)
    return json.loads(response.content.decode("utf-8"))
data = query({"inputs": "The answer to the universe is"})`,highlighted:`<span class="hljs-keyword">import</span> json
<span class="hljs-keyword">import</span> requests
headers = {<span class="hljs-string">&quot;Authorization&quot;</span>: <span class="hljs-string">f&quot;Bearer <span class="hljs-subst">{API_TOKEN}</span>&quot;</span>}
API_URL = <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/gpt2&quot;</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">payload</span>):
    data = json.dumps(payload)
    response = requests.request(<span class="hljs-string">&quot;POST&quot;</span>, API_URL, headers=headers, data=data)
    <span class="hljs-keyword">return</span> json.loads(response.content.decode(<span class="hljs-string">&quot;utf-8&quot;</span>))
data = query({<span class="hljs-string">&quot;inputs&quot;</span>: <span class="hljs-string">&quot;The answer to the universe is&quot;</span>})`}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p:O,i(s){p||(w(n.$$.fragment,s),p=!0)},o(s){E(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function fL(_){let n,p;return n=new R({props:{$$slots:{default:[cL]},$$scope:{ctx:_}}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){p||(w(n.$$.fragment,s),p=!0)},o(s){E(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function hL(_){let n,p;return n=new N({props:{code:`import fetch from "node-fetch";
async function query(data) {
    const response = await fetch(
        "https://api-inference.huggingface.co/models/gpt2",
        {
            headers: { Authorization: \`Bearer $}{API_TOKEN}\` },
            method: "POST",
            body: JSON.stringify(data),
        }
    );
    const result = await response.json();
    return result;
}
query({inputs:"The answer to the universe is"}).then((response) => {
    console.log(JSON.stringify(response));
});
// [{"generated_text":"The answer to the universe is in a different shape (or shapeless) than"}]`,highlighted:`<span class="hljs-keyword">import</span> fetch <span class="hljs-keyword">from</span> <span class="hljs-string">&quot;node-fetch&quot;</span>;
<span class="hljs-keyword">async</span> <span class="hljs-keyword">function</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">data</span>) {
    <span class="hljs-keyword">const</span> response = <span class="hljs-keyword">await</span> <span class="hljs-title function_">fetch</span>(
        <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/gpt2&quot;</span>,
        {
            <span class="hljs-attr">headers</span>: { <span class="hljs-title class_">Authorization</span>: <span class="hljs-string">\`Bearer <span class="hljs-subst">$}{API_TOKEN}</span>\`</span> },
            <span class="hljs-attr">method</span>: <span class="hljs-string">&quot;POST&quot;</span>,
            <span class="hljs-attr">body</span>: <span class="hljs-title class_">JSON</span>.<span class="hljs-title function_">stringify</span>(data),
        }
    );
    <span class="hljs-keyword">const</span> result = <span class="hljs-keyword">await</span> response.<span class="hljs-title function_">json</span>();
    <span class="hljs-keyword">return</span> result;
}
<span class="hljs-title function_">query</span>({<span class="hljs-attr">inputs</span>:<span class="hljs-string">&quot;The answer to the universe is&quot;</span>}).<span class="hljs-title function_">then</span>(<span class="hljs-function">(<span class="hljs-params">response</span>) =&gt;</span> {
    <span class="hljs-variable language_">console</span>.<span class="hljs-title function_">log</span>(<span class="hljs-title class_">JSON</span>.<span class="hljs-title function_">stringify</span>(response));
});
<span class="hljs-comment">// [{&quot;generated_text&quot;:&quot;The answer to the universe is in a different shape (or shapeless) than&quot;}]</span>`}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p:O,i(s){p||(w(n.$$.fragment,s),p=!0)},o(s){E(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function dL(_){let n,p;return n=new R({props:{$$slots:{default:[hL]},$$scope:{ctx:_}}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){p||(w(n.$$.fragment,s),p=!0)},o(s){E(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function gL(_){let n,p;return n=new N({props:{code:`curl https://api-inference.huggingface.co/models/gpt2 \\
        -X POST \\
        -d '{"inputs":"The answer to the universe is"}' \\
        -H "Authorization: Bearer $}{HF_API_TOKEN}"
# [{"generated_text":"The answer to the universe is in a different shape (or shapeless) than"}]`,highlighted:`curl https:<span class="hljs-regexp">//</span>api-inference.huggingface.co<span class="hljs-regexp">/models/g</span>pt2 \\
        -X POST \\
        -d <span class="hljs-string">&#x27;{&quot;inputs&quot;:&quot;The answer to the universe is&quot;}&#x27;</span> \\
        -H <span class="hljs-string">&quot;Authorization: Bearer $}{HF_API_TOKEN}&quot;</span>
<span class="hljs-comment"># [{&quot;generated_text&quot;:&quot;The answer to the universe is in a different shape (or shapeless) than&quot;}]</span>`}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p:O,i(s){p||(w(n.$$.fragment,s),p=!0)},o(s){E(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function mL(_){let n,p;return n=new R({props:{$$slots:{default:[gL]},$$scope:{ctx:_}}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){p||(w(n.$$.fragment,s),p=!0)},o(s){E(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function $L(_){let n,p;return n=new N({props:{code:`data == [
    {
        "generated_text": 'The answer to the universe is that we are the creation of the entire universe," says Fitch.\\n\\nAs of the 1960s, six times as many Americans still make fewer than six bucks ($}17) per year on their way to retirement.'
    }
]`,highlighted:`data == [
    {
        <span class="hljs-string">&quot;generated_text&quot;</span>: <span class="hljs-string">&#x27;The answer to the universe is that we are the creation of the entire universe,&quot; says Fitch.\\n\\nAs of the 1960s, six times as many Americans still make fewer than six bucks ($}17) per year on their way to retirement.&#x27;</span>
    }
]`}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p:O,i(s){p||(w(n.$$.fragment,s),p=!0)},o(s){E(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function _L(_){let n,p;return n=new R({props:{$$slots:{default:[$L]},$$scope:{ctx:_}}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){p||(w(n.$$.fragment,s),p=!0)},o(s){E(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function qL(_){let n,p,s,d,$,k,A;return{c(){n=r("p"),p=r("strong"),s=i("Recommended model"),d=i(`:
`),$=r("a"),k=i("bert-base-uncased"),A=i(" (it\u2019s a simple model, but fun to play with)."),this.h()},l(T){n=o(T,"P",{});var j=l(n);p=o(j,"STRONG",{});var P=l(p);s=u(P,"Recommended model"),P.forEach(t),d=u(j,`:
`),$=o(j,"A",{href:!0,rel:!0});var D=l($);k=u(D,"bert-base-uncased"),D.forEach(t),A=u(j," (it\u2019s a simple model, but fun to play with)."),j.forEach(t),this.h()},h(){c($,"href","https://huggingface.co/bert-base-uncased"),c($,"rel","nofollow")},m(T,j){m(T,n,j),e(n,p),e(p,s),e(n,d),e(n,$),e($,k),e(n,A)},d(T){T&&t(n)}}}function vL(_){let n,p;return n=new N({props:{code:`import json
import requests
headers = {"Authorization": f"Bearer {API_TOKEN}"}
API_URL = "https://api-inference.huggingface.co/models/bert-base-uncased"
def query(payload):
    data = json.dumps(payload)
    response = requests.request("POST", API_URL, headers=headers, data=data)
    return json.loads(response.content.decode("utf-8"))
data = query({"inputs": "The answer to the universe is [MASK]."})`,highlighted:`<span class="hljs-keyword">import</span> json
<span class="hljs-keyword">import</span> requests
headers = {<span class="hljs-string">&quot;Authorization&quot;</span>: <span class="hljs-string">f&quot;Bearer <span class="hljs-subst">{API_TOKEN}</span>&quot;</span>}
API_URL = <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/bert-base-uncased&quot;</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">payload</span>):
    data = json.dumps(payload)
    response = requests.request(<span class="hljs-string">&quot;POST&quot;</span>, API_URL, headers=headers, data=data)
    <span class="hljs-keyword">return</span> json.loads(response.content.decode(<span class="hljs-string">&quot;utf-8&quot;</span>))
data = query({<span class="hljs-string">&quot;inputs&quot;</span>: <span class="hljs-string">&quot;The answer to the universe is [MASK].&quot;</span>})`}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p:O,i(s){p||(w(n.$$.fragment,s),p=!0)},o(s){E(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function yL(_){let n,p;return n=new R({props:{$$slots:{default:[vL]},$$scope:{ctx:_}}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){p||(w(n.$$.fragment,s),p=!0)},o(s){E(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function wL(_){let n,p;return n=new N({props:{code:`import fetch from "node-fetch";
async function query(data) {
    const response = await fetch(
        "https://api-inference.huggingface.co/models/bert-base-uncased",
        {
            headers: { Authorization: \`Bearer $}{API_TOKEN}\` },
            method: "POST",
            body: JSON.stringify(data),
        }
    );
    const result = await response.json();
    return result;
}
query({inputs:"The answer to the universe is [MASK]."}).then((response) => {
    console.log(JSON.stringify(response));
});
// [{"sequence":"the answer to the universe is no.","score":0.16963955760002136,"token":2053,"token_str":"no"},{"sequence":"the answer to the universe is nothing.","score":0.07344776391983032,"token":2498,"token_str":"nothing"},{"sequence":"the answer to the universe is yes.","score":0.05803241208195686,"token":2748,"token_str":"yes"},{"sequence":"the answer to the universe is unknown.","score":0.043957844376564026,"token":4242,"token_str":"unknown"},{"sequence":"the answer to the universe is simple.","score":0.04015745222568512,"token":3722,"token_str":"simple"}]`,highlighted:`<span class="hljs-keyword">import</span> <span class="hljs-keyword">fetch</span> <span class="hljs-keyword">from</span> &quot;node-fetch&quot;;
async <span class="hljs-keyword">function</span> query(data) {
    const response = await <span class="hljs-keyword">fetch</span>(
        &quot;https://api-inference.huggingface.co/models/bert-base-uncased&quot;,
        {
            headers: { <span class="hljs-keyword">Authorization</span>: \`Bearer $}{API_TOKEN}\` },
            <span class="hljs-keyword">method</span>: &quot;POST&quot;,
            body: <span class="hljs-type">JSON</span>.stringify(data),
        }
    );
    const result = await response.json();
    <span class="hljs-keyword">return</span> result;
}
query({inputs:&quot;The answer to the universe is [MASK].&quot;}).<span class="hljs-keyword">then</span>((response) =&gt; {
    console.log(<span class="hljs-type">JSON</span>.stringify(response));
});
// [{&quot;sequence&quot;:&quot;the answer to the universe is no.&quot;,&quot;score&quot;:<span class="hljs-number">0.16963955760002136</span>,&quot;token&quot;:<span class="hljs-number">2053</span>,&quot;token_str&quot;:&quot;no&quot;},{&quot;sequence&quot;:&quot;the answer to the universe is nothing.&quot;,&quot;score&quot;:<span class="hljs-number">0.07344776391983032</span>,&quot;token&quot;:<span class="hljs-number">2498</span>,&quot;token_str&quot;:&quot;nothing&quot;},{&quot;sequence&quot;:&quot;the answer to the universe is yes.&quot;,&quot;score&quot;:<span class="hljs-number">0.05803241208195686</span>,&quot;token&quot;:<span class="hljs-number">2748</span>,&quot;token_str&quot;:&quot;yes&quot;},{&quot;sequence&quot;:&quot;the answer to the universe is unknown.&quot;,&quot;score&quot;:<span class="hljs-number">0.043957844376564026</span>,&quot;token&quot;:<span class="hljs-number">4242</span>,&quot;token_str&quot;:&quot;unknown&quot;},{&quot;sequence&quot;:&quot;the answer to the universe is simple.&quot;,&quot;score&quot;:<span class="hljs-number">0.04015745222568512</span>,&quot;token&quot;:<span class="hljs-number">3722</span>,&quot;token_str&quot;:&quot;simple&quot;}]`}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p:O,i(s){p||(w(n.$$.fragment,s),p=!0)},o(s){E(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function EL(_){let n,p;return n=new R({props:{$$slots:{default:[wL]},$$scope:{ctx:_}}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){p||(w(n.$$.fragment,s),p=!0)},o(s){E(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function bL(_){let n,p;return n=new N({props:{code:`curl https://api-inference.huggingface.co/models/bert-base-uncased \\
        -X POST \\
        -d '{"inputs":"The answer to the universe is [MASK]."}' \\
        -H "Authorization: Bearer $}{HF_API_TOKEN}"
# [{"sequence":"the answer to the universe is no.","score":0.16963955760002136,"token":2053,"token_str":"no"},{"sequence":"the answer to the universe is nothing.","score":0.07344776391983032,"token":2498,"token_str":"nothing"},{"sequence":"the answer to the universe is yes.","score":0.05803241208195686,"token":2748,"token_str":"yes"},{"sequence":"the answer to the universe is unknown.","score":0.043957844376564026,"token":4242,"token_str":"unknown"},{"sequence":"the answer to the universe is simple.","score":0.04015745222568512,"token":3722,"token_str":"simple"}]`,highlighted:`curl https://api-inference.huggingface.co/models/bert-base-uncased \\
        -<span class="hljs-type">X</span> <span class="hljs-type">POST</span> \\
        -d <span class="hljs-string">&#x27;{&quot;inputs&quot;:&quot;The answer to the universe is [MASK].&quot;}&#x27;</span> \\
        -<span class="hljs-type">H</span> <span class="hljs-comment">&quot;Authorization: Bearer $}{HF_API_TOKEN}&quot;</span>
# [{<span class="hljs-comment">&quot;sequence&quot;</span>:<span class="hljs-comment">&quot;the answer to the universe is no.&quot;</span>,<span class="hljs-comment">&quot;score&quot;</span>:<span class="hljs-number">0.16963955760002136</span>,<span class="hljs-comment">&quot;token&quot;</span>:<span class="hljs-number">2053</span>,<span class="hljs-comment">&quot;token_str&quot;</span>:<span class="hljs-comment">&quot;no&quot;</span>},{<span class="hljs-comment">&quot;sequence&quot;</span>:<span class="hljs-comment">&quot;the answer to the universe is nothing.&quot;</span>,<span class="hljs-comment">&quot;score&quot;</span>:<span class="hljs-number">0.07344776391983032</span>,<span class="hljs-comment">&quot;token&quot;</span>:<span class="hljs-number">2498</span>,<span class="hljs-comment">&quot;token_str&quot;</span>:<span class="hljs-comment">&quot;nothing&quot;</span>},{<span class="hljs-comment">&quot;sequence&quot;</span>:<span class="hljs-comment">&quot;the answer to the universe is yes.&quot;</span>,<span class="hljs-comment">&quot;score&quot;</span>:<span class="hljs-number">0.05803241208195686</span>,<span class="hljs-comment">&quot;token&quot;</span>:<span class="hljs-number">2748</span>,<span class="hljs-comment">&quot;token_str&quot;</span>:<span class="hljs-comment">&quot;yes&quot;</span>},{<span class="hljs-comment">&quot;sequence&quot;</span>:<span class="hljs-comment">&quot;the answer to the universe is unknown.&quot;</span>,<span class="hljs-comment">&quot;score&quot;</span>:<span class="hljs-number">0.043957844376564026</span>,<span class="hljs-comment">&quot;token&quot;</span>:<span class="hljs-number">4242</span>,<span class="hljs-comment">&quot;token_str&quot;</span>:<span class="hljs-comment">&quot;unknown&quot;</span>},{<span class="hljs-comment">&quot;sequence&quot;</span>:<span class="hljs-comment">&quot;the answer to the universe is simple.&quot;</span>,<span class="hljs-comment">&quot;score&quot;</span>:<span class="hljs-number">0.04015745222568512</span>,<span class="hljs-comment">&quot;token&quot;</span>:<span class="hljs-number">3722</span>,<span class="hljs-comment">&quot;token_str&quot;</span>:<span class="hljs-comment">&quot;simple&quot;</span>}]`}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p:O,i(s){p||(w(n.$$.fragment,s),p=!0)},o(s){E(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function jL(_){let n,p;return n=new R({props:{$$slots:{default:[bL]},$$scope:{ctx:_}}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){p||(w(n.$$.fragment,s),p=!0)},o(s){E(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function TL(_){let n,p;return n=new N({props:{code:`self.assertEqual(
    deep_round(data),
    [
        {
            "sequence": "the answer to the universe is no.",
            "score": 0.1696,
            "token": 2053,
            "token_str": "no",
        },
        {
            "sequence": "the answer to the universe is nothing.",
            "score": 0.0734,
            "token": 2498,
            "token_str": "nothing",
        },
        {
            "sequence": "the answer to the universe is yes.",
            "score": 0.0580,
            "token": 2748,
            "token_str": "yes",
        },
        {
            "sequence": "the answer to the universe is unknown.",
            "score": 0.044,
            "token": 4242,
            "token_str": "unknown",
        },
        {
            "sequence": "the answer to the universe is simple.",
            "score": 0.0402,
            "token": 3722,
            "token_str": "simple",
        },
    ],
)`,highlighted:`self.assertEqual(
    deep_round(data),
    [
        {
            <span class="hljs-string">&quot;sequence&quot;</span>: <span class="hljs-string">&quot;the answer to the universe is no.&quot;</span>,
            <span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.1696</span>,
            <span class="hljs-string">&quot;token&quot;</span>: <span class="hljs-number">2053</span>,
            <span class="hljs-string">&quot;token_str&quot;</span>: <span class="hljs-string">&quot;no&quot;</span>,
        },
        {
            <span class="hljs-string">&quot;sequence&quot;</span>: <span class="hljs-string">&quot;the answer to the universe is nothing.&quot;</span>,
            <span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.0734</span>,
            <span class="hljs-string">&quot;token&quot;</span>: <span class="hljs-number">2498</span>,
            <span class="hljs-string">&quot;token_str&quot;</span>: <span class="hljs-string">&quot;nothing&quot;</span>,
        },
        {
            <span class="hljs-string">&quot;sequence&quot;</span>: <span class="hljs-string">&quot;the answer to the universe is yes.&quot;</span>,
            <span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.0580</span>,
            <span class="hljs-string">&quot;token&quot;</span>: <span class="hljs-number">2748</span>,
            <span class="hljs-string">&quot;token_str&quot;</span>: <span class="hljs-string">&quot;yes&quot;</span>,
        },
        {
            <span class="hljs-string">&quot;sequence&quot;</span>: <span class="hljs-string">&quot;the answer to the universe is unknown.&quot;</span>,
            <span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.044</span>,
            <span class="hljs-string">&quot;token&quot;</span>: <span class="hljs-number">4242</span>,
            <span class="hljs-string">&quot;token_str&quot;</span>: <span class="hljs-string">&quot;unknown&quot;</span>,
        },
        {
            <span class="hljs-string">&quot;sequence&quot;</span>: <span class="hljs-string">&quot;the answer to the universe is simple.&quot;</span>,
            <span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.0402</span>,
            <span class="hljs-string">&quot;token&quot;</span>: <span class="hljs-number">3722</span>,
            <span class="hljs-string">&quot;token_str&quot;</span>: <span class="hljs-string">&quot;simple&quot;</span>,
        },
    ],
)`}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p:O,i(s){p||(w(n.$$.fragment,s),p=!0)},o(s){E(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function kL(_){let n,p;return n=new R({props:{$$slots:{default:[TL]},$$scope:{ctx:_}}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){p||(w(n.$$.fragment,s),p=!0)},o(s){E(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function AL(_){let n,p,s,d,$,k,A;return{c(){n=r("p"),p=r("strong"),s=i("Recommended model"),d=i(": "),$=r("a"),k=i(`Check your
langage`),A=i("."),this.h()},l(T){n=o(T,"P",{});var j=l(n);p=o(j,"STRONG",{});var P=l(p);s=u(P,"Recommended model"),P.forEach(t),d=u(j,": "),$=o(j,"A",{href:!0,rel:!0});var D=l($);k=u(D,`Check your
langage`),D.forEach(t),A=u(j,"."),j.forEach(t),this.h()},h(){c($,"href","https://huggingface.co/models?pipeline_tag=automatic-speech-recognition"),c($,"rel","nofollow")},m(T,j){m(T,n,j),e(n,p),e(p,s),e(n,d),e(n,$),e($,k),e(n,A)},d(T){T&&t(n)}}}function DL(_){let n,p,s,d,$,k,A;return{c(){n=r("p"),p=r("strong"),s=i("English"),d=i(`:
`),$=r("a"),k=i("facebook/wav2vec2-large-960h-lv60-self"),A=i("."),this.h()},l(T){n=o(T,"P",{});var j=l(n);p=o(j,"STRONG",{});var P=l(p);s=u(P,"English"),P.forEach(t),d=u(j,`:
`),$=o(j,"A",{href:!0,rel:!0});var D=l($);k=u(D,"facebook/wav2vec2-large-960h-lv60-self"),D.forEach(t),A=u(j,"."),j.forEach(t),this.h()},h(){c($,"href","https://huggingface.co/facebook/wav2vec2-large-960h-lv60-self"),c($,"rel","nofollow")},m(T,j){m(T,n,j),e(n,p),e(p,s),e(n,d),e(n,$),e($,k),e(n,A)},d(T){T&&t(n)}}}function PL(_){let n,p;return n=new N({props:{code:`    import json
    import requests
    headers = {"Authorization": f"Bearer {API_TOKEN}"}
    API_URL = "https://api-inference.huggingface.co/models/facebook/wav2vec2-base-960h"
    def query(filename):
        with open(filename, "rb") as f:
            data = f.read()
        response = requests.request("POST", API_URL, headers=headers, data=data)
        return json.loads(response.content.decode("utf-8"))
    data = query("sample1.flac")`,highlighted:`    <span class="hljs-keyword">import</span> json
    <span class="hljs-keyword">import</span> requests
    headers = {<span class="hljs-string">&quot;Authorization&quot;</span>: <span class="hljs-string">f&quot;Bearer <span class="hljs-subst">{API_TOKEN}</span>&quot;</span>}
    API_URL = <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/facebook/wav2vec2-base-960h&quot;</span>
    <span class="hljs-keyword">def</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">filename</span>):
        <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(filename, <span class="hljs-string">&quot;rb&quot;</span>) <span class="hljs-keyword">as</span> f:
            data = f.read()
        response = requests.request(<span class="hljs-string">&quot;POST&quot;</span>, API_URL, headers=headers, data=data)
        <span class="hljs-keyword">return</span> json.loads(response.content.decode(<span class="hljs-string">&quot;utf-8&quot;</span>))
    data = query(<span class="hljs-string">&quot;sample1.flac&quot;</span>)`}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p:O,i(s){p||(w(n.$$.fragment,s),p=!0)},o(s){E(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function OL(_){let n,p;return n=new R({props:{$$slots:{default:[PL]},$$scope:{ctx:_}}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){p||(w(n.$$.fragment,s),p=!0)},o(s){E(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function RL(_){let n,p;return n=new N({props:{code:`    import fetch from "node-fetch";
    import fs from "fs";
    async function query(filename) {
        const data = fs.readFileSync(filename);
        const response = await fetch(
            "https://api-inference.huggingface.co/models/facebook/wav2vec2-base-960h",
            {
                headers: { Authorization: \`Bearer $}{API_TOKEN}\` },
                method: "POST",
                body: data,
            }
        );
        const result = await response.json();
        return result;
    }
    query("sample1.flac").then((response) => {
        console.log(JSON.stringify(response));
    });
    // {"text":"GOING ALONG SLUSHY COUNTRY ROADS AND SPEAKING TO DAMP AUDIENCES IN DRAUGHTY SCHOOL ROOMS DAY AFTER DAY FOR A FORTNIGHT HE'LL HAVE TO PUT IN AN APPEARANCE AT SOME PLACE OF WORSHIP ON SUNDAY MORNING AND HE CAN COME TO US IMMEDIATELY AFTERWARDS"}`,highlighted:`    <span class="hljs-keyword">import</span> fetch <span class="hljs-keyword">from</span> <span class="hljs-string">&quot;node-fetch&quot;</span>;
    <span class="hljs-keyword">import</span> fs <span class="hljs-keyword">from</span> <span class="hljs-string">&quot;fs&quot;</span>;
    <span class="hljs-keyword">async</span> <span class="hljs-keyword">function</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">filename</span>) {
        <span class="hljs-keyword">const</span> data = fs.<span class="hljs-title function_">readFileSync</span>(filename);
        <span class="hljs-keyword">const</span> response = <span class="hljs-keyword">await</span> <span class="hljs-title function_">fetch</span>(
            <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/facebook/wav2vec2-base-960h&quot;</span>,
            {
                <span class="hljs-attr">headers</span>: { <span class="hljs-title class_">Authorization</span>: <span class="hljs-string">\`Bearer <span class="hljs-subst">$}{API_TOKEN}</span>\`</span> },
                <span class="hljs-attr">method</span>: <span class="hljs-string">&quot;POST&quot;</span>,
                <span class="hljs-attr">body</span>: data,
            }
        );
        <span class="hljs-keyword">const</span> result = <span class="hljs-keyword">await</span> response.<span class="hljs-title function_">json</span>();
        <span class="hljs-keyword">return</span> result;
    }
    <span class="hljs-title function_">query</span>(<span class="hljs-string">&quot;sample1.flac&quot;</span>).<span class="hljs-title function_">then</span>(<span class="hljs-function">(<span class="hljs-params">response</span>) =&gt;</span> {
        <span class="hljs-variable language_">console</span>.<span class="hljs-title function_">log</span>(<span class="hljs-title class_">JSON</span>.<span class="hljs-title function_">stringify</span>(response));
    });
    <span class="hljs-comment">// {&quot;text&quot;:&quot;GOING ALONG SLUSHY COUNTRY ROADS AND SPEAKING TO DAMP AUDIENCES IN DRAUGHTY SCHOOL ROOMS DAY AFTER DAY FOR A FORTNIGHT HE&#x27;LL HAVE TO PUT IN AN APPEARANCE AT SOME PLACE OF WORSHIP ON SUNDAY MORNING AND HE CAN COME TO US IMMEDIATELY AFTERWARDS&quot;}</span>`}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p:O,i(s){p||(w(n.$$.fragment,s),p=!0)},o(s){E(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function NL(_){let n,p;return n=new R({props:{$$slots:{default:[RL]},$$scope:{ctx:_}}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){p||(w(n.$$.fragment,s),p=!0)},o(s){E(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function SL(_){let n,p;return n=new N({props:{code:`    curl https://api-inference.huggingface.co/models/facebook/wav2vec2-base-960h \\
            -X POST \\
            --data-binary '@sample1.flac' \\
            -H "Authorization: Bearer $}{HF_API_TOKEN}"
    # {"text":"GOING ALONG SLUSHY COUNTRY ROADS AND SPEAKING TO DAMP AUDIENCES IN DRAUGHTY SCHOOL ROOMS DAY AFTER DAY FOR A FORTNIGHT HE'LL HAVE TO PUT IN AN APPEARANCE AT SOME PLACE OF WORSHIP ON SUNDAY MORNING AND HE CAN COME TO US IMMEDIATELY AFTERWARDS"}`,highlighted:`    curl https://api-inference.huggingface.co/models/facebook/wav2vec2-base-<span class="hljs-number">960</span>h \\
            -X POST \\
            --data-binary <span class="hljs-string">&#x27;@sample1.flac&#x27;</span> \\
            -H <span class="hljs-string">&quot;Authorization: Bearer <span class="hljs-variable">$}{HF_API_TOKEN}</span>&quot;</span>
    <span class="hljs-comment"># {&quot;text&quot;:&quot;GOING ALONG SLUSHY COUNTRY ROADS AND SPEAKING TO DAMP AUDIENCES IN DRAUGHTY SCHOOL ROOMS DAY AFTER DAY FOR A FORTNIGHT HE&#x27;LL HAVE TO PUT IN AN APPEARANCE AT SOME PLACE OF WORSHIP ON SUNDAY MORNING AND HE CAN COME TO US IMMEDIATELY AFTERWARDS&quot;}</span>`}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p:O,i(s){p||(w(n.$$.fragment,s),p=!0)},o(s){E(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function xL(_){let n,p;return n=new R({props:{$$slots:{default:[SL]},$$scope:{ctx:_}}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){p||(w(n.$$.fragment,s),p=!0)},o(s){E(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function IL(_){let n,p;return n=new N({props:{code:`    self.assertEqual(
        data,
        {
            "text": "GOING ALONG SLUSHY COUNTRY ROADS AND SPEAKING TO DAMP AUDIENCES IN DRAUGHTY SCHOOL ROOMS DAY AFTER DAY FOR A FORTNIGHT HE'LL HAVE TO PUT IN AN APPEARANCE AT SOME PLACE OF WORSHIP ON SUNDAY MORNING AND HE CAN COME TO US IMMEDIATELY AFTERWARDS"
        },
    )`,highlighted:`    self.assertEqual(
        data,
        {
            <span class="hljs-string">&quot;text&quot;</span>: <span class="hljs-string">&quot;GOING ALONG SLUSHY COUNTRY ROADS AND SPEAKING TO DAMP AUDIENCES IN DRAUGHTY SCHOOL ROOMS DAY AFTER DAY FOR A FORTNIGHT HE&#x27;LL HAVE TO PUT IN AN APPEARANCE AT SOME PLACE OF WORSHIP ON SUNDAY MORNING AND HE CAN COME TO US IMMEDIATELY AFTERWARDS&quot;</span>
        },
    )`}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p:O,i(s){p||(w(n.$$.fragment,s),p=!0)},o(s){E(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function HL(_){let n,p;return n=new R({props:{$$slots:{default:[IL]},$$scope:{ctx:_}}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){p||(w(n.$$.fragment,s),p=!0)},o(s){E(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function BL(_){let n,p,s,d,$,k,A;return{c(){n=r("p"),p=r("strong"),s=i("Recommended model"),d=i(`:
`),$=r("a"),k=i("Sentence-transformers"),A=i("."),this.h()},l(T){n=o(T,"P",{});var j=l(n);p=o(j,"STRONG",{});var P=l(p);s=u(P,"Recommended model"),P.forEach(t),d=u(j,`:
`),$=o(j,"A",{href:!0,rel:!0});var D=l($);k=u(D,"Sentence-transformers"),D.forEach(t),A=u(j,"."),j.forEach(t),this.h()},h(){c($,"href","https://huggingface.co/sentence-transformers/paraphrase-xlm-r-multilingual-v1"),c($,"rel","nofollow")},m(T,j){m(T,n,j),e(n,p),e(p,s),e(n,d),e(n,$),e($,k),e(n,A)},d(T){T&&t(n)}}}function CL(_){let n,p,s,d,$,k,A;return{c(){n=r("p"),p=r("strong"),s=i("Recommended model"),d=i(`:
`),$=r("a"),k=i("superb/hubert-large-superb-er"),A=i("."),this.h()},l(T){n=o(T,"P",{});var j=l(n);p=o(j,"STRONG",{});var P=l(p);s=u(P,"Recommended model"),P.forEach(t),d=u(j,`:
`),$=o(j,"A",{href:!0,rel:!0});var D=l($);k=u(D,"superb/hubert-large-superb-er"),D.forEach(t),A=u(j,"."),j.forEach(t),this.h()},h(){c($,"href","https://huggingface.co/superb/hubert-large-superb-er"),c($,"rel","nofollow")},m(T,j){m(T,n,j),e(n,p),e(p,s),e(n,d),e(n,$),e($,k),e(n,A)},d(T){T&&t(n)}}}function GL(_){let n,p;return n=new N({props:{code:`    import json
    import requests
    headers = {"Authorization": f"Bearer {API_TOKEN}"}
    API_URL = "https://api-inference.huggingface.co/models/superb/hubert-large-superb-er"
    def query(filename):
        with open(filename, "rb") as f:
            data = f.read()
        response = requests.request("POST", API_URL, headers=headers, data=data)
        return json.loads(response.content.decode("utf-8"))
    data = query("sample1.flac")`,highlighted:`    <span class="hljs-keyword">import</span> json
    <span class="hljs-keyword">import</span> requests
    headers = {<span class="hljs-string">&quot;Authorization&quot;</span>: <span class="hljs-string">f&quot;Bearer <span class="hljs-subst">{API_TOKEN}</span>&quot;</span>}
    API_URL = <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/superb/hubert-large-superb-er&quot;</span>
    <span class="hljs-keyword">def</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">filename</span>):
        <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(filename, <span class="hljs-string">&quot;rb&quot;</span>) <span class="hljs-keyword">as</span> f:
            data = f.read()
        response = requests.request(<span class="hljs-string">&quot;POST&quot;</span>, API_URL, headers=headers, data=data)
        <span class="hljs-keyword">return</span> json.loads(response.content.decode(<span class="hljs-string">&quot;utf-8&quot;</span>))
    data = query(<span class="hljs-string">&quot;sample1.flac&quot;</span>)`}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p:O,i(s){p||(w(n.$$.fragment,s),p=!0)},o(s){E(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function UL(_){let n,p;return n=new R({props:{$$slots:{default:[GL]},$$scope:{ctx:_}}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){p||(w(n.$$.fragment,s),p=!0)},o(s){E(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function LL(_){let n,p;return n=new N({props:{code:`    import fetch from "node-fetch";
    import fs from "fs";
    async function query(filename) {
        const data = fs.readFileSync(filename);
        const response = await fetch(
            "https://api-inference.huggingface.co/models/superb/hubert-large-superb-er",
            {
                headers: { Authorization: \`Bearer $}{API_TOKEN}\` },
                method: "POST",
                body: data,
            }
        );
        const result = await response.json();
        return result;
    }
    query("sample1.flac").then((response) => {
        console.log(JSON.stringify(response));
    });
    // [{"score":0.5927661657333374,"label":"neu"},{"score":0.2002529799938202,"label":"hap"},{"score":0.12795612215995789,"label":"ang"},{"score":0.07902472466230392,"label":"sad"}]`,highlighted:`    <span class="hljs-keyword">import</span> <span class="hljs-keyword">fetch</span> <span class="hljs-keyword">from</span> &quot;node-fetch&quot;;
    <span class="hljs-keyword">import</span> fs <span class="hljs-keyword">from</span> &quot;fs&quot;;
    async <span class="hljs-keyword">function</span> query(filename) {
        const data = fs.readFileSync(filename);
        const response = await <span class="hljs-keyword">fetch</span>(
            &quot;https://api-inference.huggingface.co/models/superb/hubert-large-superb-er&quot;,
            {
                headers: { <span class="hljs-keyword">Authorization</span>: \`Bearer $}{API_TOKEN}\` },
                <span class="hljs-keyword">method</span>: &quot;POST&quot;,
                body: data,
            }
        );
        const result = await response.json();
        <span class="hljs-keyword">return</span> result;
    }
    query(&quot;sample1.flac&quot;).<span class="hljs-keyword">then</span>((response) =&gt; {
        console.log(<span class="hljs-type">JSON</span>.stringify(response));
    });
    // [{&quot;score&quot;:<span class="hljs-number">0.5927661657333374</span>,&quot;label&quot;:&quot;neu&quot;},{&quot;score&quot;:<span class="hljs-number">0.2002529799938202</span>,&quot;label&quot;:&quot;hap&quot;},{&quot;score&quot;:<span class="hljs-number">0.12795612215995789</span>,&quot;label&quot;:&quot;ang&quot;},{&quot;score&quot;:<span class="hljs-number">0.07902472466230392</span>,&quot;label&quot;:&quot;sad&quot;}]`}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p:O,i(s){p||(w(n.$$.fragment,s),p=!0)},o(s){E(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function zL(_){let n,p;return n=new R({props:{$$slots:{default:[LL]},$$scope:{ctx:_}}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){p||(w(n.$$.fragment,s),p=!0)},o(s){E(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function ML(_){let n,p;return n=new N({props:{code:`    curl https://api-inference.huggingface.co/models/superb/hubert-large-superb-er \\
            -X POST \\
            --data-binary '@sample1.flac' \\
            -H "Authorization: Bearer $}{HF_API_TOKEN}"
    # [{"score":0.5927661657333374,"label":"neu"},{"score":0.2002529799938202,"label":"hap"},{"score":0.12795612215995789,"label":"ang"},{"score":0.07902472466230392,"label":"sad"}]`,highlighted:`    curl https:<span class="hljs-comment">//api-inference.huggingface.co/models/superb/hubert-large-superb-er \\</span>
            -X POST \\
            --<span class="hljs-keyword">data</span>-binary <span class="hljs-string">&#x27;@sample1.flac&#x27;</span> \\
            -H <span class="hljs-string">&quot;Authorization: Bearer <span class="hljs-subst">$}{HF_API_TOKEN}</span>&quot;</span>
    # [{<span class="hljs-string">&quot;score&quot;</span>:<span class="hljs-number">0.5927661657333374</span>,<span class="hljs-string">&quot;label&quot;</span>:<span class="hljs-string">&quot;neu&quot;</span>},{<span class="hljs-string">&quot;score&quot;</span>:<span class="hljs-number">0.2002529799938202</span>,<span class="hljs-string">&quot;label&quot;</span>:<span class="hljs-string">&quot;hap&quot;</span>},{<span class="hljs-string">&quot;score&quot;</span>:<span class="hljs-number">0.12795612215995789</span>,<span class="hljs-string">&quot;label&quot;</span>:<span class="hljs-string">&quot;ang&quot;</span>},{<span class="hljs-string">&quot;score&quot;</span>:<span class="hljs-number">0.07902472466230392</span>,<span class="hljs-string">&quot;label&quot;</span>:<span class="hljs-string">&quot;sad&quot;</span>}]`}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p:O,i(s){p||(w(n.$$.fragment,s),p=!0)},o(s){E(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function FL(_){let n,p;return n=new R({props:{$$slots:{default:[ML]},$$scope:{ctx:_}}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){p||(w(n.$$.fragment,s),p=!0)},o(s){E(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function JL(_){let n,p;return n=new N({props:{code:`    self.assertEqual(
        deep_round(data, 4),
        [
            {"score": 0.5928, "label": "neu"},
            {"score": 0.2003, "label": "hap"},
            {"score": 0.128, "label": "ang"},
            {"score": 0.079, "label": "sad"},
        ],
    )`,highlighted:`    self.assertEqual(
        deep_round(data, <span class="hljs-number">4</span>),
        [
            {<span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.5928</span>, <span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;neu&quot;</span>},
            {<span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.2003</span>, <span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;hap&quot;</span>},
            {<span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.128</span>, <span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;ang&quot;</span>},
            {<span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.079</span>, <span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;sad&quot;</span>},
        ],
    )`}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p:O,i(s){p||(w(n.$$.fragment,s),p=!0)},o(s){E(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function KL(_){let n,p;return n=new R({props:{$$slots:{default:[JL]},$$scope:{ctx:_}}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){p||(w(n.$$.fragment,s),p=!0)},o(s){E(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function WL(_){let n,p,s,d,$,k,A;return{c(){n=r("p"),p=r("strong"),s=i("Recommended model"),d=i(`:
`),$=r("a"),k=i("facebook/detr-resnet-50"),A=i("."),this.h()},l(T){n=o(T,"P",{});var j=l(n);p=o(j,"STRONG",{});var P=l(p);s=u(P,"Recommended model"),P.forEach(t),d=u(j,`:
`),$=o(j,"A",{href:!0,rel:!0});var D=l($);k=u(D,"facebook/detr-resnet-50"),D.forEach(t),A=u(j,"."),j.forEach(t),this.h()},h(){c($,"href","https://huggingface.co/facebook/detr-resnet-50"),c($,"rel","nofollow")},m(T,j){m(T,n,j),e(n,p),e(p,s),e(n,d),e(n,$),e($,k),e(n,A)},d(T){T&&t(n)}}}function YL(_){let n,p;return n=new N({props:{code:`    import json
    import requests
    headers = {"Authorization": f"Bearer {API_TOKEN}"}
    API_URL = "https://api-inference.huggingface.co/models/facebook/detr-resnet-50"
    def query(filename):
        with open(filename, "rb") as f:
            data = f.read()
        response = requests.request("POST", API_URL, headers=headers, data=data)
        return json.loads(response.content.decode("utf-8"))
    data = query("cats.jpg")`,highlighted:`    <span class="hljs-keyword">import</span> json
    <span class="hljs-keyword">import</span> requests
    headers = {<span class="hljs-string">&quot;Authorization&quot;</span>: <span class="hljs-string">f&quot;Bearer <span class="hljs-subst">{API_TOKEN}</span>&quot;</span>}
    API_URL = <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/facebook/detr-resnet-50&quot;</span>
    <span class="hljs-keyword">def</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">filename</span>):
        <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(filename, <span class="hljs-string">&quot;rb&quot;</span>) <span class="hljs-keyword">as</span> f:
            data = f.read()
        response = requests.request(<span class="hljs-string">&quot;POST&quot;</span>, API_URL, headers=headers, data=data)
        <span class="hljs-keyword">return</span> json.loads(response.content.decode(<span class="hljs-string">&quot;utf-8&quot;</span>))
    data = query(<span class="hljs-string">&quot;cats.jpg&quot;</span>)`}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p:O,i(s){p||(w(n.$$.fragment,s),p=!0)},o(s){E(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function VL(_){let n,p;return n=new R({props:{$$slots:{default:[YL]},$$scope:{ctx:_}}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){p||(w(n.$$.fragment,s),p=!0)},o(s){E(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function XL(_){let n,p;return n=new N({props:{code:`    import fetch from "node-fetch";
    import fs from "fs";
    async function query(filename) {
        const data = fs.readFileSync(filename);
        const response = await fetch(
            "https://api-inference.huggingface.co/models/facebook/detr-resnet-50",
            {
                headers: { Authorization: \`Bearer $}{API_TOKEN}\` },
                method: "POST",
                body: data,
            }
        );
        const result = await response.json();
        return result;
    }
    query("cats.jpg").then((response) => {
        console.log(JSON.stringify(response));
    });
    // [{"score":0.9982201457023621,"label":"remote","box":{"xmin":40,"ymin":70,"xmax":175,"ymax":117}},{"score":0.9960021376609802,"label":"remote","box":{"xmin":333,"ymin":72,"xmax":368,"ymax":187}},{"score":0.9954745173454285,"label":"couch","box":{"xmin":0,"ymin":1,"xmax":639,"ymax":473}},{"score":0.9988006353378296,"label":"cat","box":{"xmin":13,"ymin":52,"xmax":314,"ymax":470}},{"score":0.9986783862113953,"label":"cat","box":{"xmin":345,"ymin":23,"xmax":640,"ymax":368}}]`,highlighted:`    <span class="hljs-keyword">import</span> <span class="hljs-keyword">fetch</span> <span class="hljs-keyword">from</span> &quot;node-fetch&quot;;
    <span class="hljs-keyword">import</span> fs <span class="hljs-keyword">from</span> &quot;fs&quot;;
    async <span class="hljs-keyword">function</span> query(filename) {
        const data = fs.readFileSync(filename);
        const response = await <span class="hljs-keyword">fetch</span>(
            &quot;https://api-inference.huggingface.co/models/facebook/detr-resnet-50&quot;,
            {
                headers: { <span class="hljs-keyword">Authorization</span>: \`Bearer $}{API_TOKEN}\` },
                <span class="hljs-keyword">method</span>: &quot;POST&quot;,
                body: data,
            }
        );
        const result = await response.json();
        <span class="hljs-keyword">return</span> result;
    }
    query(&quot;cats.jpg&quot;).<span class="hljs-keyword">then</span>((response) =&gt; {
        console.log(<span class="hljs-type">JSON</span>.stringify(response));
    });
    // [{&quot;score&quot;:<span class="hljs-number">0.9982201457023621</span>,&quot;label&quot;:&quot;remote&quot;,&quot;box&quot;:{&quot;xmin&quot;:<span class="hljs-number">40</span>,&quot;ymin&quot;:<span class="hljs-number">70</span>,&quot;xmax&quot;:<span class="hljs-number">175</span>,&quot;ymax&quot;:<span class="hljs-number">117</span>}},{&quot;score&quot;:<span class="hljs-number">0.9960021376609802</span>,&quot;label&quot;:&quot;remote&quot;,&quot;box&quot;:{&quot;xmin&quot;:<span class="hljs-number">333</span>,&quot;ymin&quot;:<span class="hljs-number">72</span>,&quot;xmax&quot;:<span class="hljs-number">368</span>,&quot;ymax&quot;:<span class="hljs-number">187</span>}},{&quot;score&quot;:<span class="hljs-number">0.9954745173454285</span>,&quot;label&quot;:&quot;couch&quot;,&quot;box&quot;:{&quot;xmin&quot;:<span class="hljs-number">0</span>,&quot;ymin&quot;:<span class="hljs-number">1</span>,&quot;xmax&quot;:<span class="hljs-number">639</span>,&quot;ymax&quot;:<span class="hljs-number">473</span>}},{&quot;score&quot;:<span class="hljs-number">0.9988006353378296</span>,&quot;label&quot;:&quot;cat&quot;,&quot;box&quot;:{&quot;xmin&quot;:<span class="hljs-number">13</span>,&quot;ymin&quot;:<span class="hljs-number">52</span>,&quot;xmax&quot;:<span class="hljs-number">314</span>,&quot;ymax&quot;:<span class="hljs-number">470</span>}},{&quot;score&quot;:<span class="hljs-number">0.9986783862113953</span>,&quot;label&quot;:&quot;cat&quot;,&quot;box&quot;:{&quot;xmin&quot;:<span class="hljs-number">345</span>,&quot;ymin&quot;:<span class="hljs-number">23</span>,&quot;xmax&quot;:<span class="hljs-number">640</span>,&quot;ymax&quot;:<span class="hljs-number">368</span>}}]`}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p:O,i(s){p||(w(n.$$.fragment,s),p=!0)},o(s){E(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function QL(_){let n,p;return n=new R({props:{$$slots:{default:[XL]},$$scope:{ctx:_}}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){p||(w(n.$$.fragment,s),p=!0)},o(s){E(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function ZL(_){let n,p;return n=new N({props:{code:`    curl https://api-inference.huggingface.co/models/facebook/detr-resnet-50 \\
            -X POST \\
            --data-binary '@cats.jpg' \\
            -H "Authorization: Bearer $}{HF_API_TOKEN}"
    # [{"score":0.9982201457023621,"label":"remote","box":{"xmin":40,"ymin":70,"xmax":175,"ymax":117}},{"score":0.9960021376609802,"label":"remote","box":{"xmin":333,"ymin":72,"xmax":368,"ymax":187}},{"score":0.9954745173454285,"label":"couch","box":{"xmin":0,"ymin":1,"xmax":639,"ymax":473}},{"score":0.9988006353378296,"label":"cat","box":{"xmin":13,"ymin":52,"xmax":314,"ymax":470}},{"score":0.9986783862113953,"label":"cat","box":{"xmin":345,"ymin":23,"xmax":640,"ymax":368}}]`,highlighted:`    curl https:<span class="hljs-comment">//api-inference.huggingface.co/models/facebook/detr-resnet-50 \\</span>
            -X POST \\
            --<span class="hljs-keyword">data</span>-binary <span class="hljs-string">&#x27;@cats.jpg&#x27;</span> \\
            -H <span class="hljs-string">&quot;Authorization: Bearer <span class="hljs-subst">$}{HF_API_TOKEN}</span>&quot;</span>
    # [{<span class="hljs-string">&quot;score&quot;</span>:<span class="hljs-number">0.9982201457023621</span>,<span class="hljs-string">&quot;label&quot;</span>:<span class="hljs-string">&quot;remote&quot;</span>,<span class="hljs-string">&quot;box&quot;</span>:{<span class="hljs-string">&quot;xmin&quot;</span>:<span class="hljs-number">40</span>,<span class="hljs-string">&quot;ymin&quot;</span>:<span class="hljs-number">70</span>,<span class="hljs-string">&quot;xmax&quot;</span>:<span class="hljs-number">175</span>,<span class="hljs-string">&quot;ymax&quot;</span>:<span class="hljs-number">117</span>}},{<span class="hljs-string">&quot;score&quot;</span>:<span class="hljs-number">0.9960021376609802</span>,<span class="hljs-string">&quot;label&quot;</span>:<span class="hljs-string">&quot;remote&quot;</span>,<span class="hljs-string">&quot;box&quot;</span>:{<span class="hljs-string">&quot;xmin&quot;</span>:<span class="hljs-number">333</span>,<span class="hljs-string">&quot;ymin&quot;</span>:<span class="hljs-number">72</span>,<span class="hljs-string">&quot;xmax&quot;</span>:<span class="hljs-number">368</span>,<span class="hljs-string">&quot;ymax&quot;</span>:<span class="hljs-number">187</span>}},{<span class="hljs-string">&quot;score&quot;</span>:<span class="hljs-number">0.9954745173454285</span>,<span class="hljs-string">&quot;label&quot;</span>:<span class="hljs-string">&quot;couch&quot;</span>,<span class="hljs-string">&quot;box&quot;</span>:{<span class="hljs-string">&quot;xmin&quot;</span>:<span class="hljs-number">0</span>,<span class="hljs-string">&quot;ymin&quot;</span>:<span class="hljs-number">1</span>,<span class="hljs-string">&quot;xmax&quot;</span>:<span class="hljs-number">639</span>,<span class="hljs-string">&quot;ymax&quot;</span>:<span class="hljs-number">473</span>}},{<span class="hljs-string">&quot;score&quot;</span>:<span class="hljs-number">0.9988006353378296</span>,<span class="hljs-string">&quot;label&quot;</span>:<span class="hljs-string">&quot;cat&quot;</span>,<span class="hljs-string">&quot;box&quot;</span>:{<span class="hljs-string">&quot;xmin&quot;</span>:<span class="hljs-number">13</span>,<span class="hljs-string">&quot;ymin&quot;</span>:<span class="hljs-number">52</span>,<span class="hljs-string">&quot;xmax&quot;</span>:<span class="hljs-number">314</span>,<span class="hljs-string">&quot;ymax&quot;</span>:<span class="hljs-number">470</span>}},{<span class="hljs-string">&quot;score&quot;</span>:<span class="hljs-number">0.9986783862113953</span>,<span class="hljs-string">&quot;label&quot;</span>:<span class="hljs-string">&quot;cat&quot;</span>,<span class="hljs-string">&quot;box&quot;</span>:{<span class="hljs-string">&quot;xmin&quot;</span>:<span class="hljs-number">345</span>,<span class="hljs-string">&quot;ymin&quot;</span>:<span class="hljs-number">23</span>,<span class="hljs-string">&quot;xmax&quot;</span>:<span class="hljs-number">640</span>,<span class="hljs-string">&quot;ymax&quot;</span>:<span class="hljs-number">368</span>}}]`}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p:O,i(s){p||(w(n.$$.fragment,s),p=!0)},o(s){E(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function ez(_){let n,p;return n=new R({props:{$$slots:{default:[ZL]},$$scope:{ctx:_}}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){p||(w(n.$$.fragment,s),p=!0)},o(s){E(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function tz(_){let n,p;return n=new N({props:{code:`    self.assertEqual(
        deep_round(data, 4),
        [
            {
                "score": 0.9982,
                "label": "remote",
                "box": {"xmin": 40, "ymin": 70, "xmax": 175, "ymax": 117},
            },
            {
                "score": 0.9960,
                "label": "remote",
                "box": {"xmin": 333, "ymin": 72, "xmax": 368, "ymax": 187},
            },
            {
                "score": 0.9955,
                "label": "couch",
                "box": {"xmin": 0, "ymin": 1, "xmax": 639, "ymax": 473},
            },
            {
                "score": 0.9988,
                "label": "cat",
                "box": {"xmin": 13, "ymin": 52, "xmax": 314, "ymax": 470},
            },
            {
                "score": 0.9987,
                "label": "cat",
                "box": {"xmin": 345, "ymin": 23, "xmax": 640, "ymax": 368},
            },
        ],
    )`,highlighted:`    self.assertEqual(
        deep_round(data, <span class="hljs-number">4</span>),
        [
            {
                <span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.9982</span>,
                <span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;remote&quot;</span>,
                <span class="hljs-string">&quot;box&quot;</span>: {<span class="hljs-string">&quot;xmin&quot;</span>: <span class="hljs-number">40</span>, <span class="hljs-string">&quot;ymin&quot;</span>: <span class="hljs-number">70</span>, <span class="hljs-string">&quot;xmax&quot;</span>: <span class="hljs-number">175</span>, <span class="hljs-string">&quot;ymax&quot;</span>: <span class="hljs-number">117</span>},
            },
            {
                <span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.9960</span>,
                <span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;remote&quot;</span>,
                <span class="hljs-string">&quot;box&quot;</span>: {<span class="hljs-string">&quot;xmin&quot;</span>: <span class="hljs-number">333</span>, <span class="hljs-string">&quot;ymin&quot;</span>: <span class="hljs-number">72</span>, <span class="hljs-string">&quot;xmax&quot;</span>: <span class="hljs-number">368</span>, <span class="hljs-string">&quot;ymax&quot;</span>: <span class="hljs-number">187</span>},
            },
            {
                <span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.9955</span>,
                <span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;couch&quot;</span>,
                <span class="hljs-string">&quot;box&quot;</span>: {<span class="hljs-string">&quot;xmin&quot;</span>: <span class="hljs-number">0</span>, <span class="hljs-string">&quot;ymin&quot;</span>: <span class="hljs-number">1</span>, <span class="hljs-string">&quot;xmax&quot;</span>: <span class="hljs-number">639</span>, <span class="hljs-string">&quot;ymax&quot;</span>: <span class="hljs-number">473</span>},
            },
            {
                <span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.9988</span>,
                <span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;cat&quot;</span>,
                <span class="hljs-string">&quot;box&quot;</span>: {<span class="hljs-string">&quot;xmin&quot;</span>: <span class="hljs-number">13</span>, <span class="hljs-string">&quot;ymin&quot;</span>: <span class="hljs-number">52</span>, <span class="hljs-string">&quot;xmax&quot;</span>: <span class="hljs-number">314</span>, <span class="hljs-string">&quot;ymax&quot;</span>: <span class="hljs-number">470</span>},
            },
            {
                <span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.9987</span>,
                <span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;cat&quot;</span>,
                <span class="hljs-string">&quot;box&quot;</span>: {<span class="hljs-string">&quot;xmin&quot;</span>: <span class="hljs-number">345</span>, <span class="hljs-string">&quot;ymin&quot;</span>: <span class="hljs-number">23</span>, <span class="hljs-string">&quot;xmax&quot;</span>: <span class="hljs-number">640</span>, <span class="hljs-string">&quot;ymax&quot;</span>: <span class="hljs-number">368</span>},
            },
        ],
    )`}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p:O,i(s){p||(w(n.$$.fragment,s),p=!0)},o(s){E(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function sz(_){let n,p;return n=new R({props:{$$slots:{default:[tz]},$$scope:{ctx:_}}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){p||(w(n.$$.fragment,s),p=!0)},o(s){E(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function az(_){let n,p,s,d,$,k,A,T,j,P,D,se,De,V,J,et,Ml,Ra,Pe,gE,Y$,Fl,mE,V$,tt,UO,X$,st,LO,Q$,Oe,at,ch,Na,$E,fh,_E,Z$,Jl,qE,e_,nt,t_,Sa,vE,xa,yE,s_,Kl,wE,a_,rt,n_,Wl,EE,r_,ot,hh,Ia,Yl,bE,jE,dh,TE,U,Ha,Ba,gh,kE,AE,DE,Vl,PE,OE,Ca,Ga,mh,RE,NE,SE,Xl,xE,IE,Ua,Ql,HE,BE,ce,CE,$h,GE,UE,Zl,LE,zE,ME,La,ei,FE,JE,lt,KE,_h,WE,YE,VE,za,ti,qh,XE,QE,si,ZE,eb,Ma,ai,tb,sb,it,ab,vh,nb,rb,ob,Fa,ni,lb,ib,ut,ub,yh,pb,cb,fb,Ja,ri,hb,db,pt,gb,wh,mb,$b,o_,oi,_b,l_,li,qb,i_,ct,u_,ft,Eh,Ka,ii,vb,yb,bh,wb,Re,Wa,ui,jh,Eb,bb,pi,jb,Tb,Ya,ci,Th,kb,Ab,fi,Db,Pb,Va,hi,kh,Ob,Rb,ht,Nb,Ah,Sb,xb,p_,Ne,dt,Dh,Xa,Ib,Ph,Hb,c_,di,Bb,f_,gt,h_,Qa,Cb,Za,Gb,d_,gi,Ub,g_,mt,m_,mi,Lb,$_,$t,Oh,en,$i,zb,Mb,Rh,Fb,X,tn,sn,Nh,Jb,Kb,Wb,_i,Yb,Vb,an,qi,Sh,Xb,Qb,vi,Zb,ej,nn,yi,tj,sj,_t,aj,xh,nj,rj,oj,rn,wi,lj,ij,qt,uj,Ih,pj,cj,fj,on,Ei,hj,dj,vt,gj,Hh,mj,$j,__,bi,_j,q_,yt,Bh,ln,ji,qj,vj,Ch,yj,Gh,un,Ti,Uh,wj,Ej,ki,bj,v_,Se,wt,Lh,pn,jj,zh,Tj,y_,Et,kj,Ai,Aj,Dj,w_,bt,E_,cn,Pj,fn,Oj,b_,Di,Rj,j_,jt,T_,Pi,Nj,k_,Tt,Mh,hn,Oi,Sj,xj,Fh,Ij,B,dn,gn,Jh,Hj,Bj,Cj,Ri,Gj,Uj,mn,Ni,Kh,Lj,zj,Si,Mj,Fj,$n,xi,Jj,Kj,fe,Wj,Wh,Yj,Vj,Yh,Xj,Qj,Zj,_n,Ii,e0,t0,he,s0,Vh,a0,n0,Xh,r0,o0,l0,qn,Hi,i0,u0,de,p0,Qh,c0,f0,Bi,h0,d0,g0,vn,Ci,m0,$0,ge,_0,Zh,q0,v0,Gi,y0,w0,E0,yn,Ui,b0,j0,ae,T0,ed,k0,A0,Li,D0,P0,zi,O0,R0,N0,wn,Mi,S0,x0,kt,I0,td,H0,B0,C0,En,Fi,G0,U0,At,L0,sd,z0,M0,F0,bn,Ji,ad,J0,K0,Ki,W0,Y0,jn,Wi,V0,X0,Dt,Q0,nd,Z0,eT,tT,Tn,Yi,sT,aT,Pt,nT,rd,rT,oT,lT,kn,Vi,iT,uT,Ot,pT,od,cT,fT,A_,Xi,hT,D_,Rt,ld,An,Qi,dT,gT,id,mT,ud,Dn,Zi,pd,$T,_T,eu,qT,P_,xe,Nt,cd,Pn,vT,fd,yT,O_,tu,wT,R_,St,N_,On,ET,Rn,bT,S_,su,jT,x_,xt,I_,au,TT,H_,It,hd,Nn,nu,kT,AT,dd,DT,S,Sn,xn,gd,PT,OT,RT,md,NT,In,ru,ST,xT,ou,IT,HT,Hn,lu,BT,CT,iu,GT,UT,Bn,uu,LT,zT,Ht,MT,$d,FT,JT,KT,Cn,pu,_d,WT,YT,cu,VT,XT,Gn,fu,QT,ZT,me,e3,qd,t3,s3,vd,a3,n3,r3,Un,hu,o3,l3,$e,i3,yd,u3,p3,wd,c3,f3,h3,Ln,du,d3,g3,_e,m3,Ed,$3,_3,gu,q3,v3,y3,zn,mu,w3,E3,qe,b3,bd,j3,T3,$u,k3,A3,D3,Mn,_u,P3,O3,ne,R3,jd,N3,S3,qu,x3,I3,vu,H3,B3,C3,Fn,yu,G3,U3,Bt,L3,Td,z3,M3,F3,Jn,wu,J3,K3,Ct,W3,kd,Y3,V3,X3,Kn,Eu,Ad,Q3,Z3,bu,e5,t5,Wn,ju,s5,a5,Gt,n5,Dd,r5,o5,l5,Yn,Tu,i5,u5,Ut,p5,Pd,c5,f5,h5,Vn,ku,d5,g5,Lt,m5,Od,$5,_5,B_,Au,q5,C_,zt,Rd,Xn,Du,v5,y5,Nd,w5,oe,Qn,Pu,Sd,E5,b5,Ou,j5,T5,Zn,Ru,xd,k5,A5,Nu,D5,P5,er,Su,O5,R5,xu,N5,S5,tr,Iu,x5,I5,Hu,H5,G_,Ie,Mt,Id,sr,B5,Hd,C5,U_,Bu,G5,L_,Ft,z_,ar,U5,nr,L5,M_,Cu,z5,F_,Jt,J_,Gu,M5,K_,Kt,Bd,rr,Uu,F5,J5,Cd,K5,M,or,lr,Gd,W5,Y5,V5,Ud,X5,ir,Lu,Q5,Z5,zu,ek,tk,ur,Mu,sk,ak,Fu,nk,rk,pr,Ju,Ld,ok,lk,Ku,ik,uk,cr,Wu,pk,ck,Wt,fk,zd,hk,dk,gk,fr,Yu,mk,$k,Yt,_k,Md,qk,vk,yk,hr,Vu,wk,Ek,Vt,bk,Fd,jk,Tk,W_,Xu,kk,Y_,Xt,V_,Qt,Jd,dr,Qu,Ak,Dk,Kd,Pk,le,gr,Zu,Wd,Ok,Rk,ep,Nk,Sk,mr,tp,Yd,xk,Ik,sp,Hk,Bk,$r,ap,Vd,Ck,Gk,np,Uk,Lk,_r,rp,Xd,zk,Mk,op,Fk,X_,He,Zt,Qd,qr,Jk,Zd,Kk,Q_,lp,Wk,Z_,es,eq,Be,Yk,vr,Vk,Xk,yr,Qk,tq,ip,Zk,sq,ts,aq,up,e4,nq,pp,t4,rq,ss,oq,as,eg,wr,cp,s4,a4,tg,n4,ie,Er,fp,sg,r4,o4,hp,l4,i4,br,dp,ag,u4,p4,gp,c4,f4,jr,mp,ng,h4,d4,ns,g4,rg,m4,$4,_4,Tr,$p,og,q4,v4,rs,y4,lg,w4,E4,lq,Ce,os,ig,kr,b4,ug,j4,iq,_p,T4,uq,ls,pq,Ar,k4,Dr,A4,cq,qp,D4,fq,is,hq,vp,P4,dq,us,pg,Pr,yp,O4,R4,cg,N4,Q,Or,Rr,fg,S4,x4,I4,wp,H4,B4,Nr,Ep,hg,C4,G4,bp,U4,L4,Sr,jp,z4,M4,ps,F4,dg,J4,K4,W4,xr,Tp,Y4,V4,cs,X4,gg,Q4,Z4,e7,Ir,kp,t7,s7,fs,a7,mg,n7,r7,gq,Ap,o7,mq,hs,$q,ds,$g,Hr,Dp,l7,i7,_g,u7,Br,Cr,Pp,qg,p7,c7,Op,f7,h7,Gr,Rp,vg,d7,g7,Np,m7,_q,Ge,gs,yg,Ur,$7,wg,_7,qq,Lr,q7,Sp,v7,vq,Ue,ms,Eg,zr,y7,bg,w7,yq,xp,E7,wq,$s,Eq,Le,b7,Mr,j7,T7,Fr,k7,bq,Ip,A7,jq,_s,Tq,Hp,D7,kq,qs,jg,Jr,Bp,P7,O7,Tg,R7,F,Kr,Wr,kg,N7,S7,x7,Cp,I7,H7,Yr,Gp,Ag,B7,C7,Up,G7,U7,Vr,Lp,L7,z7,vs,M7,Dg,F7,J7,K7,Xr,zp,Pg,W7,Y7,Mp,V7,X7,Qr,Fp,Q7,Z7,ys,e6,Og,t6,s6,a6,Zr,Jp,n6,r6,ws,o6,Rg,l6,i6,u6,eo,Kp,p6,c6,Es,f6,Ng,h6,d6,Aq,Wp,g6,Dq,bs,Pq,js,Sg,to,Yp,m6,$6,xg,_6,Z,so,Vp,Ig,q6,v6,Xp,y6,w6,ao,Qp,Hg,E6,b6,Zp,j6,T6,no,ec,Bg,k6,A6,tc,D6,P6,ro,sc,Cg,O6,R6,Ts,N6,Gg,S6,x6,I6,oo,ac,Ug,H6,B6,ks,C6,Lg,G6,U6,Oq,ze,As,zg,lo,L6,Mg,z6,Rq,nc,M6,Nq,Ds,Sq,io,F6,uo,J6,xq,rc,K6,Iq,Ps,Hq,oc,W6,Bq,Os,Fg,po,lc,Y6,V6,Jg,X6,x,co,fo,Kg,Q6,Z6,e9,ic,t9,s9,ho,uc,Wg,a9,n9,pc,r9,o9,go,cc,l9,i9,ve,u9,Yg,p9,c9,fc,f9,h9,d9,mo,hc,g9,m9,ye,$9,Vg,_9,q9,dc,v9,y9,w9,$o,gc,E9,b9,re,j9,Xg,T9,k9,mc,A9,D9,$c,P9,O9,R9,_o,_c,N9,S9,Rs,x9,Qg,I9,H9,B9,qo,qc,C9,G9,we,U9,Zg,L9,z9,em,M9,F9,J9,vo,vc,K9,W9,Ns,Y9,tm,V9,X9,Q9,yo,yc,Z9,e8,Ee,t8,sm,s8,a8,am,n8,r8,o8,wo,wc,l8,i8,Ss,u8,nm,p8,c8,f8,Eo,Ec,h8,d8,xs,g8,rm,m8,$8,_8,bo,bc,om,q8,v8,jc,y8,w8,jo,Tc,E8,b8,Is,j8,lm,T8,k8,A8,To,kc,D8,P8,Hs,O8,im,R8,N8,S8,ko,Ac,x8,I8,Bs,H8,um,B8,C8,Cq,Dc,G8,Gq,Cs,Uq,Gs,pm,Ao,Pc,U8,L8,cm,z8,fm,Do,Oc,hm,M8,F8,Rc,J8,Lq,Me,Us,dm,Po,K8,gm,W8,zq,Ls,Y8,Nc,V8,X8,Mq,Fe,zs,mm,Oo,Q8,$m,Z8,Fq,Sc,eA,Jq,Ms,Kq,Ro,tA,No,sA,Wq,xc,aA,Yq,Fs,Vq,Ic,nA,Xq,Js,_m,So,Hc,rA,oA,qm,lA,ee,xo,Io,vm,iA,uA,pA,Bc,cA,fA,Ho,Cc,ym,hA,dA,Gc,gA,mA,Bo,Uc,$A,_A,Ks,qA,wm,vA,yA,wA,Co,Lc,EA,bA,Ws,jA,Em,TA,kA,AA,Go,zc,DA,PA,Ys,OA,bm,RA,NA,Qq,Mc,SA,Zq,Vs,e1,Xs,jm,Uo,Fc,xA,IA,Tm,HA,ue,Lo,Jc,km,BA,CA,Kc,GA,UA,zo,Wc,Am,LA,zA,Yc,MA,FA,Mo,Vc,Dm,JA,KA,Xc,WA,YA,Fo,Qc,Pm,VA,XA,Zc,QA,t1,Je,Qs,Om,Jo,ZA,Rm,eD,s1,ef,tD,a1,Zs,n1,ea,r1,pe,sD,Ko,aD,nD,Wo,rD,oD,Yo,lD,o1,tf,iD,l1,ta,i1,sf,uD,u1,sa,Nm,Vo,af,pD,cD,Sm,fD,xm,Xo,Qo,Im,hD,dD,gD,nf,mD,p1,rf,$D,c1,of,_D,f1,aa,h1,na,Hm,Zo,lf,qD,vD,Bm,yD,Cm,el,uf,Gm,wD,ED,pf,bD,d1,Ke,ra,Um,tl,jD,Lm,TD,g1,cf,kD,m1,oa,$1,We,AD,sl,DD,PD,al,OD,_1,ff,RD,q1,la,zm,nl,hf,ND,SD,Mm,xD,te,rl,ol,Fm,ID,HD,BD,df,CD,GD,ll,gf,Jm,UD,LD,mf,zD,MD,il,$f,FD,JD,ia,KD,Km,WD,YD,VD,ul,_f,XD,QD,ua,ZD,Wm,eP,tP,sP,pl,qf,aP,nP,pa,rP,Ym,oP,lP,v1,vf,iP,y1,ca,Vm,cl,yf,uP,pP,Xm,cP,Qm,fl,wf,Zm,fP,hP,Ef,dP,w1,bf,gP,E1,Ye,fa,e$,hl,mP,t$,$P,b1,jf,_P,j1,ha,T1,Ve,qP,dl,vP,yP,gl,wP,k1,Tf,EP,A1,da,D1,kf,bP,P1,ga,s$,ml,Af,jP,TP,a$,kP,n$,$l,_l,r$,AP,DP,PP,Df,OP,O1,Pf,RP,R1,ma,N1,$a,o$,ql,Of,NP,SP,l$,xP,vl,yl,Rf,i$,IP,HP,Nf,BP,CP,wl,Sf,u$,GP,UP,xf,LP,S1,Xe,_a,p$,El,zP,c$,MP,x1,If,FP,I1,qa,H1,bl,JP,jl,KP,B1,Hf,WP,C1,va,G1,ya,YP,Tl,VP,XP,U1,wa,f$,kl,Bf,QP,ZP,h$,eO,d$,Al,Dl,g$,tO,sO,aO,Cf,nO,L1,Gf,rO,z1,Ea,M1,ba,m$,Pl,Uf,oO,lO,$$,iO,Qe,Ol,Lf,_$,uO,pO,zf,cO,fO,Rl,Mf,q$,hO,dO,Ff,gO,mO,Nl,Jf,v$,$O,_O,Kf,qO,F1;return k=new z({}),V=new z({}),Na=new z({}),nt=new K({props:{$$slots:{default:[QG]},$$scope:{ctx:_}}}),rt=new G({props:{python:!0,js:!0,curl:!0,$$slots:{curl:[nU],js:[sU],python:[eU]},$$scope:{ctx:_}}}),ct=new G({props:{python:!0,js:!0,curl:!0,$$slots:{python:[oU]},$$scope:{ctx:_}}}),Xa=new z({}),gt=new K({props:{$$slots:{default:[lU]},$$scope:{ctx:_}}}),mt=new G({props:{python:!0,js:!0,curl:!0,$$slots:{curl:[hU],js:[cU],python:[uU]},$$scope:{ctx:_}}}),pn=new z({}),bt=new K({props:{$$slots:{default:[dU]},$$scope:{ctx:_}}}),jt=new G({props:{python:!0,js:!0,curl:!0,$$slots:{curl:[vU],js:[_U],python:[mU]},$$scope:{ctx:_}}}),Pn=new z({}),St=new K({props:{$$slots:{default:[yU]},$$scope:{ctx:_}}}),xt=new G({props:{python:!0,js:!0,curl:!0,$$slots:{curl:[kU],js:[jU],python:[EU]},$$scope:{ctx:_}}}),sr=new z({}),Ft=new K({props:{$$slots:{default:[AU]},$$scope:{ctx:_}}}),Jt=new G({props:{python:!0,js:!0,curl:!0,$$slots:{curl:[SU],js:[RU],python:[PU]},$$scope:{ctx:_}}}),Xt=new G({props:{python:!0,js:!0,curl:!0,$$slots:{python:[IU]},$$scope:{ctx:_}}}),qr=new z({}),es=new K({props:{$$slots:{default:[HU]},$$scope:{ctx:_}}}),ts=new G({props:{python:!0,js:!0,curl:!0,$$slots:{curl:[zU],js:[UU],python:[CU]},$$scope:{ctx:_}}}),ss=new G({props:{python:!0,js:!0,curl:!0,$$slots:{python:[FU]},$$scope:{ctx:_}}}),kr=new z({}),ls=new K({props:{$$slots:{default:[JU]},$$scope:{ctx:_}}}),is=new G({props:{python:!0,js:!0,curl:!0,$$slots:{curl:[QU],js:[VU],python:[WU]},$$scope:{ctx:_}}}),hs=new G({props:{python:!0,js:!0,curl:!0,$$slots:{python:[eL]},$$scope:{ctx:_}}}),Ur=new z({}),zr=new z({}),$s=new K({props:{$$slots:{default:[tL]},$$scope:{ctx:_}}}),_s=new G({props:{python:!0,js:!0,curl:!0,$$slots:{curl:[lL],js:[rL],python:[aL]},$$scope:{ctx:_}}}),bs=new G({props:{python:!0,js:!0,curl:!0,$$slots:{python:[uL]},$$scope:{ctx:_}}}),lo=new z({}),Ds=new K({props:{$$slots:{default:[pL]},$$scope:{ctx:_}}}),Ps=new G({props:{python:!0,js:!0,curl:!0,$$slots:{curl:[mL],js:[dL],python:[fL]},$$scope:{ctx:_}}}),Cs=new G({props:{python:!0,js:!0,curl:!0,$$slots:{python:[_L]},$$scope:{ctx:_}}}),Po=new z({}),Oo=new z({}),Ms=new K({props:{$$slots:{default:[qL]},$$scope:{ctx:_}}}),Fs=new G({props:{python:!0,js:!0,curl:!0,$$slots:{curl:[jL],js:[EL],python:[yL]},$$scope:{ctx:_}}}),Vs=new G({props:{python:!0,js:!0,curl:!0,$$slots:{python:[kL]},$$scope:{ctx:_}}}),Jo=new z({}),Zs=new K({props:{$$slots:{default:[AL]},$$scope:{ctx:_}}}),ea=new K({props:{$$slots:{default:[DL]},$$scope:{ctx:_}}}),ta=new G({props:{python:!0,js:!0,curl:!0,$$slots:{curl:[xL],js:[NL],python:[OL]},$$scope:{ctx:_}}}),aa=new G({props:{python:!0,js:!0,curl:!0,$$slots:{python:[HL]},$$scope:{ctx:_}}}),tl=new z({}),oa=new K({props:{$$slots:{default:[BL]},$$scope:{ctx:_}}}),hl=new z({}),ha=new K({props:{$$slots:{default:[CL]},$$scope:{ctx:_}}}),da=new G({props:{python:!0,js:!0,curl:!0,$$slots:{curl:[FL],js:[zL],python:[UL]},$$scope:{ctx:_}}}),ma=new G({props:{python:!0,js:!0,curl:!0,$$slots:{python:[KL]},$$scope:{ctx:_}}}),El=new z({}),qa=new K({props:{$$slots:{default:[WL]},$$scope:{ctx:_}}}),va=new G({props:{python:!0,js:!0,curl:!0,$$slots:{curl:[ez],js:[QL],python:[VL]},$$scope:{ctx:_}}}),Ea=new G({props:{python:!0,js:!0,curl:!0,$$slots:{python:[sz]},$$scope:{ctx:_}}}),{c(){n=r("meta"),p=f(),s=r("h1"),d=r("a"),$=r("span"),q(k.$$.fragment),A=f(),T=r("span"),j=i("Detailed parameters"),P=f(),D=r("h2"),se=r("a"),De=r("span"),q(V.$$.fragment),J=f(),et=r("span"),Ml=i("Which task is used by this model ?"),Ra=f(),Pe=r("p"),gE=i(`In general the \u{1F917} Hosted API Inference accepts a simple string as an
input. However, more advanced usage depends on the \u201Ctask\u201D that the
model solves.`),Y$=f(),Fl=r("p"),mE=i("The \u201Ctask\u201D of a model is defined here on it\u2019s model page:"),V$=f(),tt=r("img"),X$=f(),st=r("img"),Q$=f(),Oe=r("h2"),at=r("a"),ch=r("span"),q(Na.$$.fragment),$E=f(),fh=r("span"),_E=i("Zero-shot classification task"),Z$=f(),Jl=r("p"),qE=i(`This task is a super useful to try it out classification with zero code,
you simply pass a sentence/paragraph and the possible labels for that
sentence and you get a result.`),e_=f(),q(nt.$$.fragment),t_=f(),Sa=r("p"),vE=i("Available with: "),xa=r("a"),yE=i("\u{1F917} Transformers"),s_=f(),Kl=r("p"),wE=i("Request:"),a_=f(),q(rt.$$.fragment),n_=f(),Wl=r("p"),EE=i(`When sending your request, you should send a JSON encoded payload. Here
are all the options`),r_=f(),ot=r("table"),hh=r("thead"),Ia=r("tr"),Yl=r("th"),bE=i("All parameters"),jE=f(),dh=r("th"),TE=f(),U=r("tbody"),Ha=r("tr"),Ba=r("td"),gh=r("strong"),kE=i("inputs"),AE=i(" (required)"),DE=f(),Vl=r("td"),PE=i("a string or list of strings"),OE=f(),Ca=r("tr"),Ga=r("td"),mh=r("strong"),RE=i("parameters"),NE=i(" (required)"),SE=f(),Xl=r("td"),xE=i("a dict containing the following keys:"),IE=f(),Ua=r("tr"),Ql=r("td"),HE=i("candidate_labels (required)"),BE=f(),ce=r("td"),CE=i("a list of strings that are potential classes for "),$h=r("code"),GE=i("inputs"),UE=i(". (max 10 candidate_labels, for more, simply run multiple requests, results are going to be misleading if using too many candidate_labels anyway. If you want to keep the exact same, you can simply run "),Zl=r("span"),LE=i("multi_label=True"),zE=i(" and do the scaling on your end. )"),ME=f(),La=r("tr"),ei=r("td"),FE=i("multi_label"),JE=f(),lt=r("td"),KE=i("(Default: "),_h=r("code"),WE=i("false"),YE=i(") Boolean that is set to True if classes can overlap"),VE=f(),za=r("tr"),ti=r("td"),qh=r("strong"),XE=i("options"),QE=f(),si=r("td"),ZE=i("a dict containing the following keys:"),eb=f(),Ma=r("tr"),ai=r("td"),tb=i("use_gpu"),sb=f(),it=r("td"),ab=i("(Default: "),vh=r("code"),nb=i("false"),rb=i("). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),ob=f(),Fa=r("tr"),ni=r("td"),lb=i("use_cache"),ib=f(),ut=r("td"),ub=i("(Default: "),yh=r("code"),pb=i("true"),cb=i("). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),fb=f(),Ja=r("tr"),ri=r("td"),hb=i("wait_for_model"),db=f(),pt=r("td"),gb=i("(Default: "),wh=r("code"),mb=i("false"),$b=i(") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),o_=f(),oi=r("p"),_b=i("Return value is either a dict or a list of dicts if you sent a list of inputs"),l_=f(),li=r("p"),qb=i("Response:"),i_=f(),q(ct.$$.fragment),u_=f(),ft=r("table"),Eh=r("thead"),Ka=r("tr"),ii=r("th"),vb=i("Returned values"),yb=f(),bh=r("th"),wb=f(),Re=r("tbody"),Wa=r("tr"),ui=r("td"),jh=r("strong"),Eb=i("sequence"),bb=f(),pi=r("td"),jb=i("The string sent as an input"),Tb=f(),Ya=r("tr"),ci=r("td"),Th=r("strong"),kb=i("labels"),Ab=f(),fi=r("td"),Db=i("The list of strings for labels that you sent (in order)"),Pb=f(),Va=r("tr"),hi=r("td"),kh=r("strong"),Ob=i("scores"),Rb=f(),ht=r("td"),Nb=i("a list of floats that correspond the the probability of label, in the same order as "),Ah=r("code"),Sb=i("labels"),xb=i("."),p_=f(),Ne=r("h2"),dt=r("a"),Dh=r("span"),q(Xa.$$.fragment),Ib=f(),Ph=r("span"),Hb=i("Translation task"),c_=f(),di=r("p"),Bb=i("This task is well known to translate text from one language to another"),f_=f(),q(gt.$$.fragment),h_=f(),Qa=r("p"),Cb=i("Available with: "),Za=r("a"),Gb=i("\u{1F917} Transformers"),d_=f(),gi=r("p"),Ub=i("Example:"),g_=f(),q(mt.$$.fragment),m_=f(),mi=r("p"),Lb=i(`When sending your request, you should send a JSON encoded payload. Here
are all the options`),$_=f(),$t=r("table"),Oh=r("thead"),en=r("tr"),$i=r("th"),zb=i("All parameters"),Mb=f(),Rh=r("th"),Fb=f(),X=r("tbody"),tn=r("tr"),sn=r("td"),Nh=r("strong"),Jb=i("inputs"),Kb=i(" (required)"),Wb=f(),_i=r("td"),Yb=i("a string to be translated in the original languages"),Vb=f(),an=r("tr"),qi=r("td"),Sh=r("strong"),Xb=i("options"),Qb=f(),vi=r("td"),Zb=i("a dict containing the following keys:"),ej=f(),nn=r("tr"),yi=r("td"),tj=i("use_gpu"),sj=f(),_t=r("td"),aj=i("(Default: "),xh=r("code"),nj=i("false"),rj=i("). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),oj=f(),rn=r("tr"),wi=r("td"),lj=i("use_cache"),ij=f(),qt=r("td"),uj=i("(Default: "),Ih=r("code"),pj=i("true"),cj=i("). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),fj=f(),on=r("tr"),Ei=r("td"),hj=i("wait_for_model"),dj=f(),vt=r("td"),gj=i("(Default: "),Hh=r("code"),mj=i("false"),$j=i(") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),__=f(),bi=r("p"),_j=i("Return value is either a dict or a list of dicts if you sent a list of inputs"),q_=f(),yt=r("table"),Bh=r("thead"),ln=r("tr"),ji=r("th"),qj=i("Returned values"),vj=f(),Ch=r("th"),yj=f(),Gh=r("tbody"),un=r("tr"),Ti=r("td"),Uh=r("strong"),wj=i("translation_text"),Ej=f(),ki=r("td"),bj=i("The string after translation"),v_=f(),Se=r("h2"),wt=r("a"),Lh=r("span"),q(pn.$$.fragment),jj=f(),zh=r("span"),Tj=i("Summarization task"),y_=f(),Et=r("p"),kj=i(`This task is well known to summarize text a big text into a small text.
Be careful, some models have a maximum length of input. That means that
the summary cannot handle full books for instance. Be careful when
choosing your model. If you want to discuss you summarization needs,
please get in touch <`),Ai=r("a"),Aj=i("api-enterprise@huggingface.co"),Dj=i(">"),w_=f(),q(bt.$$.fragment),E_=f(),cn=r("p"),Pj=i("Available with: "),fn=r("a"),Oj=i("\u{1F917} Transformers"),b_=f(),Di=r("p"),Rj=i("Example:"),j_=f(),q(jt.$$.fragment),T_=f(),Pi=r("p"),Nj=i(`When sending your request, you should send a JSON encoded payload. Here
are all the options`),k_=f(),Tt=r("table"),Mh=r("thead"),hn=r("tr"),Oi=r("th"),Sj=i("All parameters"),xj=f(),Fh=r("th"),Ij=f(),B=r("tbody"),dn=r("tr"),gn=r("td"),Jh=r("strong"),Hj=i("inputs"),Bj=i(" (required)"),Cj=f(),Ri=r("td"),Gj=i("a string to be summarized"),Uj=f(),mn=r("tr"),Ni=r("td"),Kh=r("strong"),Lj=i("parameters"),zj=f(),Si=r("td"),Mj=i("a dict containing the following keys:"),Fj=f(),$n=r("tr"),xi=r("td"),Jj=i("min_length"),Kj=f(),fe=r("td"),Wj=i("(Default: "),Wh=r("code"),Yj=i("None"),Vj=i("). Integer to define the minimum length "),Yh=r("strong"),Xj=i("in tokens"),Qj=i(" of the output summary."),Zj=f(),_n=r("tr"),Ii=r("td"),e0=i("max_length"),t0=f(),he=r("td"),s0=i("(Default: "),Vh=r("code"),a0=i("None"),n0=i("). Integer to define the maximum length "),Xh=r("strong"),r0=i("in tokens"),o0=i(" of the output summary."),l0=f(),qn=r("tr"),Hi=r("td"),i0=i("top_k"),u0=f(),de=r("td"),p0=i("(Default: "),Qh=r("code"),c0=i("None"),f0=i("). Integer to define the top tokens considered within the "),Bi=r("span"),h0=i("sample"),d0=i(" operation to create new text."),g0=f(),vn=r("tr"),Ci=r("td"),m0=i("top_p"),$0=f(),ge=r("td"),_0=i("(Default: "),Zh=r("code"),q0=i("None"),v0=i("). Float to define the tokens that are within the sample` operation of text generation. Add tokens in the sample for more probable to least probable until the sum of the probabilities is greater than "),Gi=r("span"),y0=i("top_p"),w0=i("."),E0=f(),yn=r("tr"),Ui=r("td"),b0=i("temperature"),j0=f(),ae=r("td"),T0=i("(Default: "),ed=r("code"),k0=i("1.0"),A0=i("). Float (0.0-100.0). The temperature of the sampling operation. 1 means regular sampling, 0 means "),Li=r("span"),D0=i("top_k=1"),P0=i(", "),zi=r("span"),O0=i("100.0"),R0=i(" is getting closer to uniform probability."),N0=f(),wn=r("tr"),Mi=r("td"),S0=i("repetition_penalty"),x0=f(),kt=r("td"),I0=i("(Default: "),td=r("code"),H0=i("None"),B0=i("). Float (0.0-100.0). The more a token is used within generation the more it is penalized to not be picked in successive generation passes."),C0=f(),En=r("tr"),Fi=r("td"),G0=i("max_time"),U0=f(),At=r("td"),L0=i("(Default: "),sd=r("code"),z0=i("None"),M0=i("). Float (0-120.0). The amount of time in seconds that the query should take maximum. Network can cause some overhead so it will be a soft limit."),F0=f(),bn=r("tr"),Ji=r("td"),ad=r("strong"),J0=i("options"),K0=f(),Ki=r("td"),W0=i("a dict containing the following keys:"),Y0=f(),jn=r("tr"),Wi=r("td"),V0=i("use_gpu"),X0=f(),Dt=r("td"),Q0=i("(Default: "),nd=r("code"),Z0=i("false"),eT=i("). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),tT=f(),Tn=r("tr"),Yi=r("td"),sT=i("use_cache"),aT=f(),Pt=r("td"),nT=i("(Default: "),rd=r("code"),rT=i("true"),oT=i("). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),lT=f(),kn=r("tr"),Vi=r("td"),iT=i("wait_for_model"),uT=f(),Ot=r("td"),pT=i("(Default: "),od=r("code"),cT=i("false"),fT=i(") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),A_=f(),Xi=r("p"),hT=i("Return value is either a dict or a list of dicts if you sent a list of inputs"),D_=f(),Rt=r("table"),ld=r("thead"),An=r("tr"),Qi=r("th"),dT=i("Returned values"),gT=f(),id=r("th"),mT=f(),ud=r("tbody"),Dn=r("tr"),Zi=r("td"),pd=r("strong"),$T=i("summarization_text"),_T=f(),eu=r("td"),qT=i("The string after translation"),P_=f(),xe=r("h2"),Nt=r("a"),cd=r("span"),q(Pn.$$.fragment),vT=f(),fd=r("span"),yT=i("Conversational task"),O_=f(),tu=r("p"),wT=i(`This task corresponds to any chatbot like structure. Models tend to have
shorted max_length, so please check with caution when using a given
model if you need long range dependency or not.`),R_=f(),q(St.$$.fragment),N_=f(),On=r("p"),ET=i("Available with: "),Rn=r("a"),bT=i("\u{1F917} Transformers"),S_=f(),su=r("p"),jT=i("Example:"),x_=f(),q(xt.$$.fragment),I_=f(),au=r("p"),TT=i(`When sending your request, you should send a JSON encoded payload. Here
are all the options`),H_=f(),It=r("table"),hd=r("thead"),Nn=r("tr"),nu=r("th"),kT=i("All parameters"),AT=f(),dd=r("th"),DT=f(),S=r("tbody"),Sn=r("tr"),xn=r("td"),gd=r("strong"),PT=i("inputs"),OT=i(" (required)"),RT=f(),md=r("td"),NT=f(),In=r("tr"),ru=r("td"),ST=i("text (required)"),xT=f(),ou=r("td"),IT=i("The last input from the user in the conversation."),HT=f(),Hn=r("tr"),lu=r("td"),BT=i("generated_responses"),CT=f(),iu=r("td"),GT=i("A list of strings corresponding to the earlier replies from the model."),UT=f(),Bn=r("tr"),uu=r("td"),LT=i("past_user_inputs"),zT=f(),Ht=r("td"),MT=i("A list of strings corresponding to the earlier replies from the user. Should be of the same length of "),$d=r("code"),FT=i("generated_responses"),JT=i("."),KT=f(),Cn=r("tr"),pu=r("td"),_d=r("strong"),WT=i("parameters"),YT=f(),cu=r("td"),VT=i("a dict containing the following keys:"),XT=f(),Gn=r("tr"),fu=r("td"),QT=i("min_length"),ZT=f(),me=r("td"),e3=i("(Default: "),qd=r("code"),t3=i("None"),s3=i("). Integer to define the minimum length "),vd=r("strong"),a3=i("in tokens"),n3=i(" of the output summary."),r3=f(),Un=r("tr"),hu=r("td"),o3=i("max_length"),l3=f(),$e=r("td"),i3=i("(Default: "),yd=r("code"),u3=i("None"),p3=i("). Integer to define the maximum length "),wd=r("strong"),c3=i("in tokens"),f3=i(" of the output summary."),h3=f(),Ln=r("tr"),du=r("td"),d3=i("top_k"),g3=f(),_e=r("td"),m3=i("(Default: "),Ed=r("code"),$3=i("None"),_3=i("). Integer to define the top tokens considered within the "),gu=r("span"),q3=i("sample"),v3=i(" operation to create new text."),y3=f(),zn=r("tr"),mu=r("td"),w3=i("top_p"),E3=f(),qe=r("td"),b3=i("(Default: "),bd=r("code"),j3=i("None"),T3=i("). Float to define the tokens that are within the sample` operation of text generation. Add tokens in the sample for more probable to least probable until the sum of the probabilities is greater than "),$u=r("span"),k3=i("top_p"),A3=i("."),D3=f(),Mn=r("tr"),_u=r("td"),P3=i("temperature"),O3=f(),ne=r("td"),R3=i("(Default: "),jd=r("code"),N3=i("1.0"),S3=i("). Float (0.0-100.0). The temperature of the sampling operation. 1 means regular sampling, 0 means "),qu=r("span"),x3=i("top_k=1"),I3=i(", "),vu=r("span"),H3=i("100.0"),B3=i(" is getting closer to uniform probability."),C3=f(),Fn=r("tr"),yu=r("td"),G3=i("repetition_penalty"),U3=f(),Bt=r("td"),L3=i("(Default: "),Td=r("code"),z3=i("None"),M3=i("). Float (0.0-100.0). The more a token is used within generation the more it is penalized to not be picked in successive generation passes."),F3=f(),Jn=r("tr"),wu=r("td"),J3=i("max_time"),K3=f(),Ct=r("td"),W3=i("(Default: "),kd=r("code"),Y3=i("None"),V3=i("). Float (0-120.0). The amount of time in seconds that the query should take maximum. Network can cause some overhead so it will be a soft limit."),X3=f(),Kn=r("tr"),Eu=r("td"),Ad=r("strong"),Q3=i("options"),Z3=f(),bu=r("td"),e5=i("a dict containing the following keys:"),t5=f(),Wn=r("tr"),ju=r("td"),s5=i("use_gpu"),a5=f(),Gt=r("td"),n5=i("(Default: "),Dd=r("code"),r5=i("false"),o5=i("). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),l5=f(),Yn=r("tr"),Tu=r("td"),i5=i("use_cache"),u5=f(),Ut=r("td"),p5=i("(Default: "),Pd=r("code"),c5=i("true"),f5=i("). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),h5=f(),Vn=r("tr"),ku=r("td"),d5=i("wait_for_model"),g5=f(),Lt=r("td"),m5=i("(Default: "),Od=r("code"),$5=i("false"),_5=i(") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),B_=f(),Au=r("p"),q5=i("Return value is either a dict or a list of dicts if you sent a list of inputs"),C_=f(),zt=r("table"),Rd=r("thead"),Xn=r("tr"),Du=r("th"),v5=i("Returned values"),y5=f(),Nd=r("th"),w5=f(),oe=r("tbody"),Qn=r("tr"),Pu=r("td"),Sd=r("strong"),E5=i("generated_text"),b5=f(),Ou=r("td"),j5=i("The answer of the bot"),T5=f(),Zn=r("tr"),Ru=r("td"),xd=r("strong"),k5=i("conversation"),A5=f(),Nu=r("td"),D5=i("A facility dictionnary to send back for the next input (with the new user input addition)."),P5=f(),er=r("tr"),Su=r("td"),O5=i("past_user_inputs"),R5=f(),xu=r("td"),N5=i("List of strings. The last inputs from the user in the conversation, <em>after the model has run."),S5=f(),tr=r("tr"),Iu=r("td"),x5=i("generated_responses"),I5=f(),Hu=r("td"),H5=i("List of strings. The last outputs from the model in the conversation, <em>after the model has run."),G_=f(),Ie=r("h2"),Mt=r("a"),Id=r("span"),q(sr.$$.fragment),B5=f(),Hd=r("span"),C5=i("Table question answering task"),U_=f(),Bu=r("p"),G5=i(`Don\u2019t know SQL? Don\u2019t want to dive into a large spreadsheet? Ask
questions in plain english!`),L_=f(),q(Ft.$$.fragment),z_=f(),ar=r("p"),U5=i("Available with: "),nr=r("a"),L5=i("\u{1F917} Transformers"),M_=f(),Cu=r("p"),z5=i("Example:"),F_=f(),q(Jt.$$.fragment),J_=f(),Gu=r("p"),M5=i(`When sending your request, you should send a JSON encoded payload. Here
are all the options`),K_=f(),Kt=r("table"),Bd=r("thead"),rr=r("tr"),Uu=r("th"),F5=i("All parameters"),J5=f(),Cd=r("th"),K5=f(),M=r("tbody"),or=r("tr"),lr=r("td"),Gd=r("strong"),W5=i("inputs"),Y5=i(" (required)"),V5=f(),Ud=r("td"),X5=f(),ir=r("tr"),Lu=r("td"),Q5=i("query (required)"),Z5=f(),zu=r("td"),ek=i("The query in plain text that you want to ask the table"),tk=f(),ur=r("tr"),Mu=r("td"),sk=i("table (required)"),ak=f(),Fu=r("td"),nk=i("A table of data represented as a dict of list where entries are headers and the lists are all the values, all lists must have the same size."),rk=f(),pr=r("tr"),Ju=r("td"),Ld=r("strong"),ok=i("options"),lk=f(),Ku=r("td"),ik=i("a dict containing the following keys:"),uk=f(),cr=r("tr"),Wu=r("td"),pk=i("use_gpu"),ck=f(),Wt=r("td"),fk=i("(Default: "),zd=r("code"),hk=i("false"),dk=i("). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),gk=f(),fr=r("tr"),Yu=r("td"),mk=i("use_cache"),$k=f(),Yt=r("td"),_k=i("(Default: "),Md=r("code"),qk=i("true"),vk=i("). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),yk=f(),hr=r("tr"),Vu=r("td"),wk=i("wait_for_model"),Ek=f(),Vt=r("td"),bk=i("(Default: "),Fd=r("code"),jk=i("false"),Tk=i(") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),W_=f(),Xu=r("p"),kk=i("Return value is either a dict or a list of dicts if you sent a list of inputs"),Y_=f(),q(Xt.$$.fragment),V_=f(),Qt=r("table"),Jd=r("thead"),dr=r("tr"),Qu=r("th"),Ak=i("Returned values"),Dk=f(),Kd=r("th"),Pk=f(),le=r("tbody"),gr=r("tr"),Zu=r("td"),Wd=r("strong"),Ok=i("answer"),Rk=f(),ep=r("td"),Nk=i("The plaintext answer"),Sk=f(),mr=r("tr"),tp=r("td"),Yd=r("strong"),xk=i("coordinates"),Ik=f(),sp=r("td"),Hk=i("a list of coordinates of the cells references in the answer"),Bk=f(),$r=r("tr"),ap=r("td"),Vd=r("strong"),Ck=i("cells"),Gk=f(),np=r("td"),Uk=i("a list of coordinates of the cells contents"),Lk=f(),_r=r("tr"),rp=r("td"),Xd=r("strong"),zk=i("aggregator"),Mk=f(),op=r("td"),Fk=i("The aggregator used to get the answer"),X_=f(),He=r("h2"),Zt=r("a"),Qd=r("span"),q(qr.$$.fragment),Jk=f(),Zd=r("span"),Kk=i("Question answering task"),Q_=f(),lp=r("p"),Wk=i("Want to have a nice know-it-all bot that can answer any questions ?"),Z_=f(),q(es.$$.fragment),eq=f(),Be=r("p"),Yk=i("Available with: "),vr=r("a"),Vk=i("\u{1F917}Transformers"),Xk=i(` and
`),yr=r("a"),Qk=i("AllenNLP"),tq=f(),ip=r("p"),Zk=i("Example:"),sq=f(),q(ts.$$.fragment),aq=f(),up=r("p"),e4=i(`When sending your request, you should send a JSON encoded payload. Here
are all the options`),nq=f(),pp=r("p"),t4=i("Return value is either a dict or a list of dicts if you sent a list of inputs"),rq=f(),q(ss.$$.fragment),oq=f(),as=r("table"),eg=r("thead"),wr=r("tr"),cp=r("th"),s4=i("Returned values"),a4=f(),tg=r("th"),n4=f(),ie=r("tbody"),Er=r("tr"),fp=r("td"),sg=r("strong"),r4=i("answer"),o4=f(),hp=r("td"),l4=i("A string that\u2019s the answer within the text."),i4=f(),br=r("tr"),dp=r("td"),ag=r("strong"),u4=i("score"),p4=f(),gp=r("td"),c4=i("A floats that represents how likely that the answer is correct"),f4=f(),jr=r("tr"),mp=r("td"),ng=r("strong"),h4=i("start"),d4=f(),ns=r("td"),g4=i("The index (string wise) of the start of the answer within "),rg=r("code"),m4=i("context"),$4=i("."),_4=f(),Tr=r("tr"),$p=r("td"),og=r("strong"),q4=i("stop"),v4=f(),rs=r("td"),y4=i("The index (string wise) of the stop of the answer within "),lg=r("code"),w4=i("context"),E4=i("."),lq=f(),Ce=r("h2"),os=r("a"),ig=r("span"),q(kr.$$.fragment),b4=f(),ug=r("span"),j4=i("Text-classification task"),iq=f(),_p=r("p"),T4=i(`Usually used for sentiment-analysis this will output the likelihood of
classes of an input.`),uq=f(),q(ls.$$.fragment),pq=f(),Ar=r("p"),k4=i("Available with: "),Dr=r("a"),A4=i("\u{1F917} Transformers"),cq=f(),qp=r("p"),D4=i("Example:"),fq=f(),q(is.$$.fragment),hq=f(),vp=r("p"),P4=i(`When sending your request, you should send a JSON encoded payload. Here
are all the options`),dq=f(),us=r("table"),pg=r("thead"),Pr=r("tr"),yp=r("th"),O4=i("All parameters"),R4=f(),cg=r("th"),N4=f(),Q=r("tbody"),Or=r("tr"),Rr=r("td"),fg=r("strong"),S4=i("inputs"),x4=i(" (required)"),I4=f(),wp=r("td"),H4=i("a string to be classified"),B4=f(),Nr=r("tr"),Ep=r("td"),hg=r("strong"),C4=i("options"),G4=f(),bp=r("td"),U4=i("a dict containing the following keys:"),L4=f(),Sr=r("tr"),jp=r("td"),z4=i("use_gpu"),M4=f(),ps=r("td"),F4=i("(Default: "),dg=r("code"),J4=i("false"),K4=i("). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),W4=f(),xr=r("tr"),Tp=r("td"),Y4=i("use_cache"),V4=f(),cs=r("td"),X4=i("(Default: "),gg=r("code"),Q4=i("true"),Z4=i("). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),e7=f(),Ir=r("tr"),kp=r("td"),t7=i("wait_for_model"),s7=f(),fs=r("td"),a7=i("(Default: "),mg=r("code"),n7=i("false"),r7=i(") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),gq=f(),Ap=r("p"),o7=i("Return value is either a dict or a list of dicts if you sent a list of inputs"),mq=f(),q(hs.$$.fragment),$q=f(),ds=r("table"),$g=r("thead"),Hr=r("tr"),Dp=r("th"),l7=i("Returned values"),i7=f(),_g=r("th"),u7=f(),Br=r("tbody"),Cr=r("tr"),Pp=r("td"),qg=r("strong"),p7=i("label"),c7=f(),Op=r("td"),f7=i("The label for the class (model specific)"),h7=f(),Gr=r("tr"),Rp=r("td"),vg=r("strong"),d7=i("score"),g7=f(),Np=r("td"),m7=i("A floats that represents how likely is that the text belongs the this class."),_q=f(),Ge=r("h2"),gs=r("a"),yg=r("span"),q(Ur.$$.fragment),$7=f(),wg=r("span"),_7=i("Named Entity Recognition (NER) task"),qq=f(),Lr=r("p"),q7=i("See "),Sp=r("a"),v7=i("Token-classification task"),vq=f(),Ue=r("h2"),ms=r("a"),Eg=r("span"),q(zr.$$.fragment),y7=f(),bg=r("span"),w7=i("Token-classification task"),yq=f(),xp=r("p"),E7=i(`Usually used for sentence parsing, either grammatical, or Named Entity
Recognition (NER) to understand keywords contained within text.`),wq=f(),q($s.$$.fragment),Eq=f(),Le=r("p"),b7=i("Available with: "),Mr=r("a"),j7=i("\u{1F917} Transformers"),T7=i(`,
`),Fr=r("a"),k7=i("Flair"),bq=f(),Ip=r("p"),A7=i("Example:"),jq=f(),q(_s.$$.fragment),Tq=f(),Hp=r("p"),D7=i(`When sending your request, you should send a JSON encoded payload. Here
are all the options`),kq=f(),qs=r("table"),jg=r("thead"),Jr=r("tr"),Bp=r("th"),P7=i("All parameters"),O7=f(),Tg=r("th"),R7=f(),F=r("tbody"),Kr=r("tr"),Wr=r("td"),kg=r("strong"),N7=i("inputs"),S7=i(" (required)"),x7=f(),Cp=r("td"),I7=i("a string to be classified"),H7=f(),Yr=r("tr"),Gp=r("td"),Ag=r("strong"),B7=i("parameters"),C7=f(),Up=r("td"),G7=i("a dict containing the following key:"),U7=f(),Vr=r("tr"),Lp=r("td"),L7=i("aggregation_strategy"),z7=f(),vs=r("td"),M7=i("(Default: "),Dg=r("code"),F7=i('simple</span></code>). There are several aggregation strategies:<br>* <code class="docutils literal notranslate"><span class="pre">none</span></code>: Every token gets classified without further aggregation.<br>* <code class="docutils literal notranslate"><span class="pre">simple</span></code>: Entities are grouped according to the default schema (B-, I- tags get merged when the tag is similar).<br>* <code class="docutils literal notranslate"><span class="pre">first</span></code>: Same as the <code class="docutils literal notranslate"><span class="pre">simple</span></code> strategy except words cannot end up with different tags. Words will use the tag of the first token when there is ambiguity.<br>* <code class="docutils literal notranslate"><span class="pre">average</span></code>: Same as the <code class="docutils literal notranslate"><span class="pre">simple</span></code> strategy except words cannot end up with different tags. Scores are averaged across tokens and then the maximum label is applied.<br>* <code class="docutils literal notranslate"><span class="pre">max</span></code>: Same as the <code class="docutils literal notranslate"><span class="pre">simple'),J7=i(" strategy except words cannot end up with different tags. Word entity will be the token with the maximum score."),K7=f(),Xr=r("tr"),zp=r("td"),Pg=r("strong"),W7=i("options"),Y7=f(),Mp=r("td"),V7=i("a dict containing the following keys:"),X7=f(),Qr=r("tr"),Fp=r("td"),Q7=i("use_gpu"),Z7=f(),ys=r("td"),e6=i("(Default: "),Og=r("code"),t6=i("false"),s6=i("). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),a6=f(),Zr=r("tr"),Jp=r("td"),n6=i("use_cache"),r6=f(),ws=r("td"),o6=i("(Default: "),Rg=r("code"),l6=i("true"),i6=i("). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),u6=f(),eo=r("tr"),Kp=r("td"),p6=i("wait_for_model"),c6=f(),Es=r("td"),f6=i("(Default: "),Ng=r("code"),h6=i("false"),d6=i(") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),Aq=f(),Wp=r("p"),g6=i("Return value is either a dict or a list of dicts if you sent a list of inputs"),Dq=f(),q(bs.$$.fragment),Pq=f(),js=r("table"),Sg=r("thead"),to=r("tr"),Yp=r("th"),m6=i("Returned values"),$6=f(),xg=r("th"),_6=f(),Z=r("tbody"),so=r("tr"),Vp=r("td"),Ig=r("strong"),q6=i("entity_group"),v6=f(),Xp=r("td"),y6=i("The type for the entity being recognized (model specific)."),w6=f(),ao=r("tr"),Qp=r("td"),Hg=r("strong"),E6=i("score"),b6=f(),Zp=r("td"),j6=i("How likely the entity was recognized."),T6=f(),no=r("tr"),ec=r("td"),Bg=r("strong"),k6=i("word"),A6=f(),tc=r("td"),D6=i("The string that was captured"),P6=f(),ro=r("tr"),sc=r("td"),Cg=r("strong"),O6=i("start"),R6=f(),Ts=r("td"),N6=i("The offset stringwise where the answer is located. Useful to disambiguate if "),Gg=r("code"),S6=i("word"),x6=i(" occurs multiple times."),I6=f(),oo=r("tr"),ac=r("td"),Ug=r("strong"),H6=i("end"),B6=f(),ks=r("td"),C6=i("The offset stringwise where the answer is located. Useful to disambiguate if "),Lg=r("code"),G6=i("word"),U6=i(" occurs multiple times."),Oq=f(),ze=r("h2"),As=r("a"),zg=r("span"),q(lo.$$.fragment),L6=f(),Mg=r("span"),z6=i("Text-generation task"),Rq=f(),nc=r("p"),M6=i("Use to continue text from a prompt. This is a very generic task."),Nq=f(),q(Ds.$$.fragment),Sq=f(),io=r("p"),F6=i("Available with: "),uo=r("a"),J6=i("\u{1F917} Transformers"),xq=f(),rc=r("p"),K6=i("Example:"),Iq=f(),q(Ps.$$.fragment),Hq=f(),oc=r("p"),W6=i(`When sending your request, you should send a JSON encoded payload. Here
are all the options`),Bq=f(),Os=r("table"),Fg=r("thead"),po=r("tr"),lc=r("th"),Y6=i("All parameters"),V6=f(),Jg=r("th"),X6=f(),x=r("tbody"),co=r("tr"),fo=r("td"),Kg=r("strong"),Q6=i("inputs"),Z6=i(" (required):"),e9=f(),ic=r("td"),t9=i("a string to be generated from"),s9=f(),ho=r("tr"),uc=r("td"),Wg=r("strong"),a9=i("parameters"),n9=f(),pc=r("td"),r9=i("dict containing the following keys:"),o9=f(),go=r("tr"),cc=r("td"),l9=i("top_k"),i9=f(),ve=r("td"),u9=i("(Default: "),Yg=r("code"),p9=i("None"),c9=i("). Integer to define the top tokens considered within the "),fc=r("span"),f9=i("sample"),h9=i(" operation to create new text."),d9=f(),mo=r("tr"),hc=r("td"),g9=i("top_p"),m9=f(),ye=r("td"),$9=i("(Default: "),Vg=r("code"),_9=i("None"),q9=i("). Float to define the tokens that are within the  sample` operation of text generation. Add tokens in the sample for more probable to least probable until the sum of the probabilities is greater than "),dc=r("span"),v9=i("top_p"),y9=i("."),w9=f(),$o=r("tr"),gc=r("td"),E9=i("temperature"),b9=f(),re=r("td"),j9=i("(Default: "),Xg=r("code"),T9=i("1.0"),k9=i("). Float (0.0-100.0). The temperature of the sampling operation. 1 means regular sampling, 0 means "),mc=r("span"),A9=i("top_k=1"),D9=i(", "),$c=r("span"),P9=i("100.0"),O9=i(" is getting closer to uniform probability."),R9=f(),_o=r("tr"),_c=r("td"),N9=i("repetition_penalty"),S9=f(),Rs=r("td"),x9=i("(Default: "),Qg=r("code"),I9=i("None"),H9=i("). Float (0.0-100.0). The more a token is used within generation the more it is penalized to not be picked in successive generation passes."),B9=f(),qo=r("tr"),qc=r("td"),C9=i("max_new_tokens"),G9=f(),we=r("td"),U9=i("(Default: "),Zg=r("code"),L9=i("None"),z9=i("). Int (0-250). The amount of new tokens to be generated, this does "),em=r("strong"),M9=i("not"),F9=i(" include the input length it is a estimate of the size of generated text you want. Each new tokens slows down the request, so look for balance between response times and length of text generated."),J9=f(),vo=r("tr"),vc=r("td"),K9=i("max_time"),W9=f(),Ns=r("td"),Y9=i("(Default: "),tm=r("code"),V9=i('None</span></code>). Float (0-120.0). The amount of time in seconds that the query should take maximum. Network can cause some overhead so it will be a soft limit. Use that in combination with <code class="xref py py-obj docutils literal notranslate"><span class="pre">max_new_tokens'),X9=i(" for best results."),Q9=f(),yo=r("tr"),yc=r("td"),Z9=i("return_full_text"),e8=f(),Ee=r("td"),t8=i("(Default: "),sm=r("code"),s8=i("True"),a8=i("). Bool. If set to False, the return results will "),am=r("strong"),n8=i("not"),r8=i(" contain the original query making it easier for prompting."),o8=f(),wo=r("tr"),wc=r("td"),l8=i("num_return_sequences"),i8=f(),Ss=r("td"),u8=i("(Default: "),nm=r("code"),p8=i("1"),c8=i("). Integer. The number of proposition you want to be returned."),f8=f(),Eo=r("tr"),Ec=r("td"),h8=i("do_sample"),d8=f(),xs=r("td"),g8=i("(Optional: "),rm=r("code"),m8=i("True"),$8=i("). Bool. Whether or not to use sampling, use greedy decoding otherwise."),_8=f(),bo=r("tr"),bc=r("td"),om=r("strong"),q8=i("options"),v8=f(),jc=r("td"),y8=i("a dict containing the following keys:"),w8=f(),jo=r("tr"),Tc=r("td"),E8=i("use_gpu"),b8=f(),Is=r("td"),j8=i("(Default: "),lm=r("code"),T8=i("false"),k8=i("). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),A8=f(),To=r("tr"),kc=r("td"),D8=i("use_cache"),P8=f(),Hs=r("td"),O8=i("(Default: "),im=r("code"),R8=i("true"),N8=i("). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),S8=f(),ko=r("tr"),Ac=r("td"),x8=i("wait_for_model"),I8=f(),Bs=r("td"),H8=i("(Default: "),um=r("code"),B8=i("false"),C8=i(") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),Cq=f(),Dc=r("p"),G8=i("Return value is either a dict or a list of dicts if you sent a list of inputs"),Gq=f(),q(Cs.$$.fragment),Uq=f(),Gs=r("table"),pm=r("thead"),Ao=r("tr"),Pc=r("th"),U8=i("Returned values"),L8=f(),cm=r("th"),z8=f(),fm=r("tbody"),Do=r("tr"),Oc=r("td"),hm=r("strong"),M8=i("generated_text"),F8=f(),Rc=r("td"),J8=i("The continuated string"),Lq=f(),Me=r("h2"),Us=r("a"),dm=r("span"),q(Po.$$.fragment),K8=f(),gm=r("span"),W8=i("Text2text-generation task"),zq=f(),Ls=r("p"),Y8=i("Essentially "),Nc=r("a"),V8=i("Text-generation task"),X8=i(`. But uses
Encoder-Decoder architecture, so might change in the future for more
options.`),Mq=f(),Fe=r("h2"),zs=r("a"),mm=r("span"),q(Oo.$$.fragment),Q8=f(),$m=r("span"),Z8=i("Fill mask task"),Fq=f(),Sc=r("p"),eA=i(`Tries to fill in a hole with a missing word (token to be precise).
That\u2019s the base task for BERT models.`),Jq=f(),q(Ms.$$.fragment),Kq=f(),Ro=r("p"),tA=i("Available with: "),No=r("a"),sA=i("\u{1F917} Transformers"),Wq=f(),xc=r("p"),aA=i("Example:"),Yq=f(),q(Fs.$$.fragment),Vq=f(),Ic=r("p"),nA=i(`When sending your request, you should send a JSON encoded payload. Here
are all the options`),Xq=f(),Js=r("table"),_m=r("thead"),So=r("tr"),Hc=r("th"),rA=i("All parameters"),oA=f(),qm=r("th"),lA=f(),ee=r("tbody"),xo=r("tr"),Io=r("td"),vm=r("strong"),iA=i("inputs"),uA=i(" (required):"),pA=f(),Bc=r("td"),cA=i("a string to be filled from, must contain the [MASK] token (check model card for exact name of the mask)"),fA=f(),Ho=r("tr"),Cc=r("td"),ym=r("strong"),hA=i("options"),dA=f(),Gc=r("td"),gA=i("a dict containing the following keys:"),mA=f(),Bo=r("tr"),Uc=r("td"),$A=i("use_gpu"),_A=f(),Ks=r("td"),qA=i("(Default: "),wm=r("code"),vA=i("false"),yA=i("). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),wA=f(),Co=r("tr"),Lc=r("td"),EA=i("use_cache"),bA=f(),Ws=r("td"),jA=i("(Default: "),Em=r("code"),TA=i("true"),kA=i("). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),AA=f(),Go=r("tr"),zc=r("td"),DA=i("wait_for_model"),PA=f(),Ys=r("td"),OA=i("(Default: "),bm=r("code"),RA=i("false"),NA=i(") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),Qq=f(),Mc=r("p"),SA=i("Return value is either a dict or a list of dicts if you sent a list of inputs"),Zq=f(),q(Vs.$$.fragment),e1=f(),Xs=r("table"),jm=r("thead"),Uo=r("tr"),Fc=r("th"),xA=i("Returned values"),IA=f(),Tm=r("th"),HA=f(),ue=r("tbody"),Lo=r("tr"),Jc=r("td"),km=r("strong"),BA=i("sequence"),CA=f(),Kc=r("td"),GA=i("The actual sequence of tokens that ran against the model (may contain special tokens)"),UA=f(),zo=r("tr"),Wc=r("td"),Am=r("strong"),LA=i("score"),zA=f(),Yc=r("td"),MA=i("The probability for this token."),FA=f(),Mo=r("tr"),Vc=r("td"),Dm=r("strong"),JA=i("token"),KA=f(),Xc=r("td"),WA=i("The id of the token"),YA=f(),Fo=r("tr"),Qc=r("td"),Pm=r("strong"),VA=i("token_str"),XA=f(),Zc=r("td"),QA=i("The string representation of the token"),t1=f(),Je=r("h2"),Qs=r("a"),Om=r("span"),q(Jo.$$.fragment),ZA=f(),Rm=r("span"),eD=i("Automatic speech recognition task"),s1=f(),ef=r("p"),tD=i(`This task reads some audio input and outputs the said words within the
audio files.`),a1=f(),q(Zs.$$.fragment),n1=f(),q(ea.$$.fragment),r1=f(),pe=r("p"),sD=i("Available with: "),Ko=r("a"),aD=i("\u{1F917} Transformers"),nD=f(),Wo=r("a"),rD=i("ESPnet"),oD=i(` and
`),Yo=r("a"),lD=i("SpeechBrain"),o1=f(),tf=r("p"),iD=i("Request:"),l1=f(),q(ta.$$.fragment),i1=f(),sf=r("p"),uD=i(`When sending your request, you should send a binary payload that simply
contains your audio file. We try to support most formats (Flac, Wav,
Mp3, Ogg etc...). And we automatically rescale the sampling rate to the
appropriate rate for the given model (usually 16KHz).`),u1=f(),sa=r("table"),Nm=r("thead"),Vo=r("tr"),af=r("th"),pD=i("All parameters"),cD=f(),Sm=r("th"),fD=f(),xm=r("tbody"),Xo=r("tr"),Qo=r("td"),Im=r("strong"),hD=i("no parameter"),dD=i(" (required)"),gD=f(),nf=r("td"),mD=i("a binary representation of the audio file. No other parameters are currently allowed."),p1=f(),rf=r("p"),$D=i("Return value is either a dict or a list of dicts if you sent a list of inputs"),c1=f(),of=r("p"),_D=i("Response:"),f1=f(),q(aa.$$.fragment),h1=f(),na=r("table"),Hm=r("thead"),Zo=r("tr"),lf=r("th"),qD=i("Returned values"),vD=f(),Bm=r("th"),yD=f(),Cm=r("tbody"),el=r("tr"),uf=r("td"),Gm=r("strong"),wD=i("text"),ED=f(),pf=r("td"),bD=i("The string that was recognized within the audio file."),d1=f(),Ke=r("h2"),ra=r("a"),Um=r("span"),q(tl.$$.fragment),jD=f(),Lm=r("span"),TD=i("Feature-extraction task"),g1=f(),cf=r("p"),kD=i(`This task reads some text and outputs raw float values, that usually
consumed as part of a semantic database/semantic search.`),m1=f(),q(oa.$$.fragment),$1=f(),We=r("p"),AD=i("Available with: "),sl=r("a"),DD=i("\u{1F917} Transformers"),PD=f(),al=r("a"),OD=i("Sentence-transformers"),_1=f(),ff=r("p"),RD=i("Request:"),q1=f(),la=r("table"),zm=r("thead"),nl=r("tr"),hf=r("th"),ND=i("All parameters"),SD=f(),Mm=r("th"),xD=f(),te=r("tbody"),rl=r("tr"),ol=r("td"),Fm=r("strong"),ID=i("inputs"),HD=i(" (required):"),BD=f(),df=r("td"),CD=i("a string or a list of strings to get the features from."),GD=f(),ll=r("tr"),gf=r("td"),Jm=r("strong"),UD=i("options"),LD=f(),mf=r("td"),zD=i("a dict containing the following keys:"),MD=f(),il=r("tr"),$f=r("td"),FD=i("use_gpu"),JD=f(),ia=r("td"),KD=i("(Default: "),Km=r("code"),WD=i("false"),YD=i("). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),VD=f(),ul=r("tr"),_f=r("td"),XD=i("use_cache"),QD=f(),ua=r("td"),ZD=i("(Default: "),Wm=r("code"),eP=i("true"),tP=i("). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),sP=f(),pl=r("tr"),qf=r("td"),aP=i("wait_for_model"),nP=f(),pa=r("td"),rP=i("(Default: "),Ym=r("code"),oP=i("false"),lP=i(") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),v1=f(),vf=r("p"),iP=i("Return value is either a dict or a list of dicts if you sent a list of inputs"),y1=f(),ca=r("table"),Vm=r("thead"),cl=r("tr"),yf=r("th"),uP=i("Returned values"),pP=f(),Xm=r("th"),cP=f(),Qm=r("tbody"),fl=r("tr"),wf=r("td"),Zm=r("strong"),fP=i("A list of float (or list of list of floats)"),hP=f(),Ef=r("td"),dP=i("The numbers that are the representation features of the input."),w1=f(),bf=r("small"),gP=i(`Returned values are a list of floats, or a list of list of floats
(depending on if you sent a string or a list of string, and if the
automatic reduction, usually mean_pooling for instance was applied for
you or not. This should be explained on the model's README.`),E1=f(),Ye=r("h2"),fa=r("a"),e$=r("span"),q(hl.$$.fragment),mP=f(),t$=r("span"),$P=i("Audio-classification task"),b1=f(),jf=r("p"),_P=i("This task reads some audio input and outputs the likelihood of classes."),j1=f(),q(ha.$$.fragment),T1=f(),Ve=r("p"),qP=i("Available with: "),dl=r("a"),vP=i("\u{1F917} Transformers"),yP=f(),gl=r("a"),wP=i("SpeechBrain"),k1=f(),Tf=r("p"),EP=i("Request:"),A1=f(),q(da.$$.fragment),D1=f(),kf=r("p"),bP=i(`When sending your request, you should send a binary payload that simply
contains your audio file. We try to support most formats (Flac, Wav,
Mp3, Ogg etc...). And we automatically rescale the sampling rate to the
appropriate rate for the given model (usually 16KHz).`),P1=f(),ga=r("table"),s$=r("thead"),ml=r("tr"),Af=r("th"),jP=i("All parameters"),TP=f(),a$=r("th"),kP=f(),n$=r("tbody"),$l=r("tr"),_l=r("td"),r$=r("strong"),AP=i("no parameter"),DP=i(" (required)"),PP=f(),Df=r("td"),OP=i("a binary representation of the audio file. No other parameters are currently allowed."),O1=f(),Pf=r("p"),RP=i("Return value is a dict"),R1=f(),q(ma.$$.fragment),N1=f(),$a=r("table"),o$=r("thead"),ql=r("tr"),Of=r("th"),NP=i("Returned values"),SP=f(),l$=r("th"),xP=f(),vl=r("tbody"),yl=r("tr"),Rf=r("td"),i$=r("strong"),IP=i("label"),HP=f(),Nf=r("td"),BP=i("The label for the class (model specific)"),CP=f(),wl=r("tr"),Sf=r("td"),u$=r("strong"),GP=i("score"),UP=f(),xf=r("td"),LP=i("A floats that represents how likely is that the audio file belongs the this class."),S1=f(),Xe=r("h2"),_a=r("a"),p$=r("span"),q(El.$$.fragment),zP=f(),c$=r("span"),MP=i("Object-detection task"),x1=f(),If=r("p"),FP=i(`This task reads some image input and outputs the likelihood of classes &
bounding boxes of detected objects.`),I1=f(),q(qa.$$.fragment),H1=f(),bl=r("p"),JP=i("Available with: "),jl=r("a"),KP=i("\u{1F917} Transformers"),B1=f(),Hf=r("p"),WP=i("Request:"),C1=f(),q(va.$$.fragment),G1=f(),ya=r("p"),YP=i(`When sending your request, you should send a binary payload that simply
contains your image file. We support all image formats `),Tl=r("a"),VP=i(`Pillow
supports`),XP=i("."),U1=f(),wa=r("table"),f$=r("thead"),kl=r("tr"),Bf=r("th"),QP=i("All parameters"),ZP=f(),h$=r("th"),eO=f(),d$=r("tbody"),Al=r("tr"),Dl=r("td"),g$=r("strong"),tO=i("no parameter"),sO=i(" (required)"),aO=f(),Cf=r("td"),nO=i("a binary representation of the image file. No other parameters are currently allowed."),L1=f(),Gf=r("p"),rO=i("Return value is a dict"),z1=f(),q(Ea.$$.fragment),M1=f(),ba=r("table"),m$=r("thead"),Pl=r("tr"),Uf=r("th"),oO=i("Returned values"),lO=f(),$$=r("th"),iO=f(),Qe=r("tbody"),Ol=r("tr"),Lf=r("td"),_$=r("strong"),uO=i("label"),pO=f(),zf=r("td"),cO=i("The label for the class (model specific) of a detected object."),fO=f(),Rl=r("tr"),Mf=r("td"),q$=r("strong"),hO=i("score"),dO=f(),Ff=r("td"),gO=i("A float that represents how likely it is that the detected object belongs to the given class."),mO=f(),Nl=r("tr"),Jf=r("td"),v$=r("strong"),$O=i("box"),_O=f(),Kf=r("td"),qO=i("A dict (with keys [xmin,ymin,xmax,ymax]) representing the bounding box of a detected object."),this.h()},l(a){const g=VG('[data-svelte="svelte-1phssyn"]',document.head);n=o(g,"META",{name:!0,content:!0}),g.forEach(t),p=h(a),s=o(a,"H1",{class:!0});var Sl=l(s);d=o(Sl,"A",{id:!0,class:!0,href:!0});var y$=l(d);$=o(y$,"SPAN",{});var w$=l($);v(k.$$.fragment,w$),w$.forEach(t),y$.forEach(t),A=h(Sl),T=o(Sl,"SPAN",{});var E$=l(T);j=u(E$,"Detailed parameters"),E$.forEach(t),Sl.forEach(t),P=h(a),D=o(a,"H2",{class:!0});var xl=l(D);se=o(xl,"A",{id:!0,class:!0,href:!0});var b$=l(se);De=o(b$,"SPAN",{});var j$=l(De);v(V.$$.fragment,j$),j$.forEach(t),b$.forEach(t),J=h(xl),et=o(xl,"SPAN",{});var T$=l(et);Ml=u(T$,"Which task is used by this model ?"),T$.forEach(t),xl.forEach(t),Ra=h(a),Pe=o(a,"P",{});var k$=l(Pe);gE=u(k$,`In general the \u{1F917} Hosted API Inference accepts a simple string as an
input. However, more advanced usage depends on the \u201Ctask\u201D that the
model solves.`),k$.forEach(t),Y$=h(a),Fl=o(a,"P",{});var A$=l(Fl);mE=u(A$,"The \u201Ctask\u201D of a model is defined here on it\u2019s model page:"),A$.forEach(t),V$=h(a),tt=o(a,"IMG",{class:!0,src:!0,width:!0}),X$=h(a),st=o(a,"IMG",{class:!0,src:!0,width:!0}),Q$=h(a),Oe=o(a,"H2",{class:!0});var Il=l(Oe);at=o(Il,"A",{id:!0,class:!0,href:!0});var D$=l(at);ch=o(D$,"SPAN",{});var P$=l(ch);v(Na.$$.fragment,P$),P$.forEach(t),D$.forEach(t),$E=h(Il),fh=o(Il,"SPAN",{});var O$=l(fh);_E=u(O$,"Zero-shot classification task"),O$.forEach(t),Il.forEach(t),Z$=h(a),Jl=o(a,"P",{});var R$=l(Jl);qE=u(R$,`This task is a super useful to try it out classification with zero code,
you simply pass a sentence/paragraph and the possible labels for that
sentence and you get a result.`),R$.forEach(t),e_=h(a),v(nt.$$.fragment,a),t_=h(a),Sa=o(a,"P",{});var Wf=l(Sa);vE=u(Wf,"Available with: "),xa=o(Wf,"A",{href:!0,rel:!0});var N$=l(xa);yE=u(N$,"\u{1F917} Transformers"),N$.forEach(t),Wf.forEach(t),s_=h(a),Kl=o(a,"P",{});var S$=l(Kl);wE=u(S$,"Request:"),S$.forEach(t),a_=h(a),v(rt.$$.fragment,a),n_=h(a),Wl=o(a,"P",{});var x$=l(Wl);EE=u(x$,`When sending your request, you should send a JSON encoded payload. Here
are all the options`),x$.forEach(t),r_=h(a),ot=o(a,"TABLE",{});var Hl=l(ot);hh=o(Hl,"THEAD",{});var I$=l(hh);Ia=o(I$,"TR",{});var Bl=l(Ia);Yl=o(Bl,"TH",{align:!0});var H$=l(Yl);bE=u(H$,"All parameters"),H$.forEach(t),jE=h(Bl),dh=o(Bl,"TH",{align:!0}),l(dh).forEach(t),Bl.forEach(t),I$.forEach(t),TE=h(Hl),U=o(Hl,"TBODY",{});var L=l(U);Ha=o(L,"TR",{});var Cl=l(Ha);Ba=o(Cl,"TD",{align:!0});var Yf=l(Ba);gh=o(Yf,"STRONG",{});var B$=l(gh);kE=u(B$,"inputs"),B$.forEach(t),AE=u(Yf," (required)"),Yf.forEach(t),DE=h(Cl),Vl=o(Cl,"TD",{align:!0});var C$=l(Vl);PE=u(C$,"a string or list of strings"),C$.forEach(t),Cl.forEach(t),OE=h(L),Ca=o(L,"TR",{});var Gl=l(Ca);Ga=o(Gl,"TD",{align:!0});var Vf=l(Ga);mh=o(Vf,"STRONG",{});var G$=l(mh);RE=u(G$,"parameters"),G$.forEach(t),NE=u(Vf," (required)"),Vf.forEach(t),SE=h(Gl),Xl=o(Gl,"TD",{align:!0});var U$=l(Xl);xE=u(U$,"a dict containing the following keys:"),U$.forEach(t),Gl.forEach(t),IE=h(L),Ua=o(L,"TR",{});var Ul=l(Ua);Ql=o(Ul,"TD",{align:!0});var L$=l(Ql);HE=u(L$,"candidate_labels (required)"),L$.forEach(t),BE=h(Ul),ce=o(Ul,"TD",{align:!0});var Ze=l(ce);CE=u(Ze,"a list of strings that are potential classes for "),$h=o(Ze,"CODE",{});var z$=l($h);GE=u(z$,"inputs"),z$.forEach(t),UE=u(Ze,". (max 10 candidate_labels, for more, simply run multiple requests, results are going to be misleading if using too many candidate_labels anyway. If you want to keep the exact same, you can simply run "),Zl=o(Ze,"SPAN",{class:!0});var M$=l(Zl);LE=u(M$,"multi_label=True"),M$.forEach(t),zE=u(Ze," and do the scaling on your end. )"),Ze.forEach(t),Ul.forEach(t),ME=h(L),La=o(L,"TR",{});var Ll=l(La);ei=o(Ll,"TD",{align:!0});var zO=l(ei);FE=u(zO,"multi_label"),zO.forEach(t),JE=h(Ll),lt=o(Ll,"TD",{align:!0});var J1=l(lt);KE=u(J1,"(Default: "),_h=o(J1,"CODE",{});var MO=l(_h);WE=u(MO,"false"),MO.forEach(t),YE=u(J1,") Boolean that is set to True if classes can overlap"),J1.forEach(t),Ll.forEach(t),VE=h(L),za=o(L,"TR",{});var K1=l(za);ti=o(K1,"TD",{align:!0});var FO=l(ti);qh=o(FO,"STRONG",{});var JO=l(qh);XE=u(JO,"options"),JO.forEach(t),FO.forEach(t),QE=h(K1),si=o(K1,"TD",{align:!0});var KO=l(si);ZE=u(KO,"a dict containing the following keys:"),KO.forEach(t),K1.forEach(t),eb=h(L),Ma=o(L,"TR",{});var W1=l(Ma);ai=o(W1,"TD",{align:!0});var WO=l(ai);tb=u(WO,"use_gpu"),WO.forEach(t),sb=h(W1),it=o(W1,"TD",{align:!0});var Y1=l(it);ab=u(Y1,"(Default: "),vh=o(Y1,"CODE",{});var YO=l(vh);nb=u(YO,"false"),YO.forEach(t),rb=u(Y1,"). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),Y1.forEach(t),W1.forEach(t),ob=h(L),Fa=o(L,"TR",{});var V1=l(Fa);ni=o(V1,"TD",{align:!0});var VO=l(ni);lb=u(VO,"use_cache"),VO.forEach(t),ib=h(V1),ut=o(V1,"TD",{align:!0});var X1=l(ut);ub=u(X1,"(Default: "),yh=o(X1,"CODE",{});var XO=l(yh);pb=u(XO,"true"),XO.forEach(t),cb=u(X1,"). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),X1.forEach(t),V1.forEach(t),fb=h(L),Ja=o(L,"TR",{});var Q1=l(Ja);ri=o(Q1,"TD",{align:!0});var QO=l(ri);hb=u(QO,"wait_for_model"),QO.forEach(t),db=h(Q1),pt=o(Q1,"TD",{align:!0});var Z1=l(pt);gb=u(Z1,"(Default: "),wh=o(Z1,"CODE",{});var ZO=l(wh);mb=u(ZO,"false"),ZO.forEach(t),$b=u(Z1,") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),Z1.forEach(t),Q1.forEach(t),L.forEach(t),Hl.forEach(t),o_=h(a),oi=o(a,"P",{});var eR=l(oi);_b=u(eR,"Return value is either a dict or a list of dicts if you sent a list of inputs"),eR.forEach(t),l_=h(a),li=o(a,"P",{});var tR=l(li);qb=u(tR,"Response:"),tR.forEach(t),i_=h(a),v(ct.$$.fragment,a),u_=h(a),ft=o(a,"TABLE",{});var ev=l(ft);Eh=o(ev,"THEAD",{});var sR=l(Eh);Ka=o(sR,"TR",{});var tv=l(Ka);ii=o(tv,"TH",{align:!0});var aR=l(ii);vb=u(aR,"Returned values"),aR.forEach(t),yb=h(tv),bh=o(tv,"TH",{align:!0}),l(bh).forEach(t),tv.forEach(t),sR.forEach(t),wb=h(ev),Re=o(ev,"TBODY",{});var Xf=l(Re);Wa=o(Xf,"TR",{});var sv=l(Wa);ui=o(sv,"TD",{align:!0});var nR=l(ui);jh=o(nR,"STRONG",{});var rR=l(jh);Eb=u(rR,"sequence"),rR.forEach(t),nR.forEach(t),bb=h(sv),pi=o(sv,"TD",{align:!0});var oR=l(pi);jb=u(oR,"The string sent as an input"),oR.forEach(t),sv.forEach(t),Tb=h(Xf),Ya=o(Xf,"TR",{});var av=l(Ya);ci=o(av,"TD",{align:!0});var lR=l(ci);Th=o(lR,"STRONG",{});var iR=l(Th);kb=u(iR,"labels"),iR.forEach(t),lR.forEach(t),Ab=h(av),fi=o(av,"TD",{align:!0});var uR=l(fi);Db=u(uR,"The list of strings for labels that you sent (in order)"),uR.forEach(t),av.forEach(t),Pb=h(Xf),Va=o(Xf,"TR",{});var nv=l(Va);hi=o(nv,"TD",{align:!0});var pR=l(hi);kh=o(pR,"STRONG",{});var cR=l(kh);Ob=u(cR,"scores"),cR.forEach(t),pR.forEach(t),Rb=h(nv),ht=o(nv,"TD",{align:!0});var rv=l(ht);Nb=u(rv,"a list of floats that correspond the the probability of label, in the same order as "),Ah=o(rv,"CODE",{});var fR=l(Ah);Sb=u(fR,"labels"),fR.forEach(t),xb=u(rv,"."),rv.forEach(t),nv.forEach(t),Xf.forEach(t),ev.forEach(t),p_=h(a),Ne=o(a,"H2",{class:!0});var ov=l(Ne);dt=o(ov,"A",{id:!0,class:!0,href:!0});var hR=l(dt);Dh=o(hR,"SPAN",{});var dR=l(Dh);v(Xa.$$.fragment,dR),dR.forEach(t),hR.forEach(t),Ib=h(ov),Ph=o(ov,"SPAN",{});var gR=l(Ph);Hb=u(gR,"Translation task"),gR.forEach(t),ov.forEach(t),c_=h(a),di=o(a,"P",{});var mR=l(di);Bb=u(mR,"This task is well known to translate text from one language to another"),mR.forEach(t),f_=h(a),v(gt.$$.fragment,a),h_=h(a),Qa=o(a,"P",{});var vO=l(Qa);Cb=u(vO,"Available with: "),Za=o(vO,"A",{href:!0,rel:!0});var $R=l(Za);Gb=u($R,"\u{1F917} Transformers"),$R.forEach(t),vO.forEach(t),d_=h(a),gi=o(a,"P",{});var _R=l(gi);Ub=u(_R,"Example:"),_R.forEach(t),g_=h(a),v(mt.$$.fragment,a),m_=h(a),mi=o(a,"P",{});var qR=l(mi);Lb=u(qR,`When sending your request, you should send a JSON encoded payload. Here
are all the options`),qR.forEach(t),$_=h(a),$t=o(a,"TABLE",{});var lv=l($t);Oh=o(lv,"THEAD",{});var vR=l(Oh);en=o(vR,"TR",{});var iv=l(en);$i=o(iv,"TH",{align:!0});var yR=l($i);zb=u(yR,"All parameters"),yR.forEach(t),Mb=h(iv),Rh=o(iv,"TH",{align:!0}),l(Rh).forEach(t),iv.forEach(t),vR.forEach(t),Fb=h(lv),X=o(lv,"TBODY",{});var be=l(X);tn=o(be,"TR",{});var uv=l(tn);sn=o(uv,"TD",{align:!0});var yO=l(sn);Nh=o(yO,"STRONG",{});var wR=l(Nh);Jb=u(wR,"inputs"),wR.forEach(t),Kb=u(yO," (required)"),yO.forEach(t),Wb=h(uv),_i=o(uv,"TD",{align:!0});var ER=l(_i);Yb=u(ER,"a string to be translated in the original languages"),ER.forEach(t),uv.forEach(t),Vb=h(be),an=o(be,"TR",{});var pv=l(an);qi=o(pv,"TD",{align:!0});var bR=l(qi);Sh=o(bR,"STRONG",{});var jR=l(Sh);Xb=u(jR,"options"),jR.forEach(t),bR.forEach(t),Qb=h(pv),vi=o(pv,"TD",{align:!0});var TR=l(vi);Zb=u(TR,"a dict containing the following keys:"),TR.forEach(t),pv.forEach(t),ej=h(be),nn=o(be,"TR",{});var cv=l(nn);yi=o(cv,"TD",{align:!0});var kR=l(yi);tj=u(kR,"use_gpu"),kR.forEach(t),sj=h(cv),_t=o(cv,"TD",{align:!0});var fv=l(_t);aj=u(fv,"(Default: "),xh=o(fv,"CODE",{});var AR=l(xh);nj=u(AR,"false"),AR.forEach(t),rj=u(fv,"). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),fv.forEach(t),cv.forEach(t),oj=h(be),rn=o(be,"TR",{});var hv=l(rn);wi=o(hv,"TD",{align:!0});var DR=l(wi);lj=u(DR,"use_cache"),DR.forEach(t),ij=h(hv),qt=o(hv,"TD",{align:!0});var dv=l(qt);uj=u(dv,"(Default: "),Ih=o(dv,"CODE",{});var PR=l(Ih);pj=u(PR,"true"),PR.forEach(t),cj=u(dv,"). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),dv.forEach(t),hv.forEach(t),fj=h(be),on=o(be,"TR",{});var gv=l(on);Ei=o(gv,"TD",{align:!0});var OR=l(Ei);hj=u(OR,"wait_for_model"),OR.forEach(t),dj=h(gv),vt=o(gv,"TD",{align:!0});var mv=l(vt);gj=u(mv,"(Default: "),Hh=o(mv,"CODE",{});var RR=l(Hh);mj=u(RR,"false"),RR.forEach(t),$j=u(mv,") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),mv.forEach(t),gv.forEach(t),be.forEach(t),lv.forEach(t),__=h(a),bi=o(a,"P",{});var NR=l(bi);_j=u(NR,"Return value is either a dict or a list of dicts if you sent a list of inputs"),NR.forEach(t),q_=h(a),yt=o(a,"TABLE",{});var $v=l(yt);Bh=o($v,"THEAD",{});var SR=l(Bh);ln=o(SR,"TR",{});var _v=l(ln);ji=o(_v,"TH",{align:!0});var xR=l(ji);qj=u(xR,"Returned values"),xR.forEach(t),vj=h(_v),Ch=o(_v,"TH",{align:!0}),l(Ch).forEach(t),_v.forEach(t),SR.forEach(t),yj=h($v),Gh=o($v,"TBODY",{});var IR=l(Gh);un=o(IR,"TR",{});var qv=l(un);Ti=o(qv,"TD",{align:!0});var HR=l(Ti);Uh=o(HR,"STRONG",{});var BR=l(Uh);wj=u(BR,"translation_text"),BR.forEach(t),HR.forEach(t),Ej=h(qv),ki=o(qv,"TD",{align:!0});var CR=l(ki);bj=u(CR,"The string after translation"),CR.forEach(t),qv.forEach(t),IR.forEach(t),$v.forEach(t),v_=h(a),Se=o(a,"H2",{class:!0});var vv=l(Se);wt=o(vv,"A",{id:!0,class:!0,href:!0});var GR=l(wt);Lh=o(GR,"SPAN",{});var UR=l(Lh);v(pn.$$.fragment,UR),UR.forEach(t),GR.forEach(t),jj=h(vv),zh=o(vv,"SPAN",{});var LR=l(zh);Tj=u(LR,"Summarization task"),LR.forEach(t),vv.forEach(t),y_=h(a),Et=o(a,"P",{});var yv=l(Et);kj=u(yv,`This task is well known to summarize text a big text into a small text.
Be careful, some models have a maximum length of input. That means that
the summary cannot handle full books for instance. Be careful when
choosing your model. If you want to discuss you summarization needs,
please get in touch <`),Ai=o(yv,"A",{href:!0});var zR=l(Ai);Aj=u(zR,"api-enterprise@huggingface.co"),zR.forEach(t),Dj=u(yv,">"),yv.forEach(t),w_=h(a),v(bt.$$.fragment,a),E_=h(a),cn=o(a,"P",{});var wO=l(cn);Pj=u(wO,"Available with: "),fn=o(wO,"A",{href:!0,rel:!0});var MR=l(fn);Oj=u(MR,"\u{1F917} Transformers"),MR.forEach(t),wO.forEach(t),b_=h(a),Di=o(a,"P",{});var FR=l(Di);Rj=u(FR,"Example:"),FR.forEach(t),j_=h(a),v(jt.$$.fragment,a),T_=h(a),Pi=o(a,"P",{});var JR=l(Pi);Nj=u(JR,`When sending your request, you should send a JSON encoded payload. Here
are all the options`),JR.forEach(t),k_=h(a),Tt=o(a,"TABLE",{});var wv=l(Tt);Mh=o(wv,"THEAD",{});var KR=l(Mh);hn=o(KR,"TR",{});var Ev=l(hn);Oi=o(Ev,"TH",{align:!0});var WR=l(Oi);Sj=u(WR,"All parameters"),WR.forEach(t),xj=h(Ev),Fh=o(Ev,"TH",{align:!0}),l(Fh).forEach(t),Ev.forEach(t),KR.forEach(t),Ij=h(wv),B=o(wv,"TBODY",{});var C=l(B);dn=o(C,"TR",{});var bv=l(dn);gn=o(bv,"TD",{align:!0});var EO=l(gn);Jh=o(EO,"STRONG",{});var YR=l(Jh);Hj=u(YR,"inputs"),YR.forEach(t),Bj=u(EO," (required)"),EO.forEach(t),Cj=h(bv),Ri=o(bv,"TD",{align:!0});var VR=l(Ri);Gj=u(VR,"a string to be summarized"),VR.forEach(t),bv.forEach(t),Uj=h(C),mn=o(C,"TR",{});var jv=l(mn);Ni=o(jv,"TD",{align:!0});var XR=l(Ni);Kh=o(XR,"STRONG",{});var QR=l(Kh);Lj=u(QR,"parameters"),QR.forEach(t),XR.forEach(t),zj=h(jv),Si=o(jv,"TD",{align:!0});var ZR=l(Si);Mj=u(ZR,"a dict containing the following keys:"),ZR.forEach(t),jv.forEach(t),Fj=h(C),$n=o(C,"TR",{});var Tv=l($n);xi=o(Tv,"TD",{align:!0});var eN=l(xi);Jj=u(eN,"min_length"),eN.forEach(t),Kj=h(Tv),fe=o(Tv,"TD",{align:!0});var Qf=l(fe);Wj=u(Qf,"(Default: "),Wh=o(Qf,"CODE",{});var tN=l(Wh);Yj=u(tN,"None"),tN.forEach(t),Vj=u(Qf,"). Integer to define the minimum length "),Yh=o(Qf,"STRONG",{});var sN=l(Yh);Xj=u(sN,"in tokens"),sN.forEach(t),Qj=u(Qf," of the output summary."),Qf.forEach(t),Tv.forEach(t),Zj=h(C),_n=o(C,"TR",{});var kv=l(_n);Ii=o(kv,"TD",{align:!0});var aN=l(Ii);e0=u(aN,"max_length"),aN.forEach(t),t0=h(kv),he=o(kv,"TD",{align:!0});var Zf=l(he);s0=u(Zf,"(Default: "),Vh=o(Zf,"CODE",{});var nN=l(Vh);a0=u(nN,"None"),nN.forEach(t),n0=u(Zf,"). Integer to define the maximum length "),Xh=o(Zf,"STRONG",{});var rN=l(Xh);r0=u(rN,"in tokens"),rN.forEach(t),o0=u(Zf," of the output summary."),Zf.forEach(t),kv.forEach(t),l0=h(C),qn=o(C,"TR",{});var Av=l(qn);Hi=o(Av,"TD",{align:!0});var oN=l(Hi);i0=u(oN,"top_k"),oN.forEach(t),u0=h(Av),de=o(Av,"TD",{align:!0});var eh=l(de);p0=u(eh,"(Default: "),Qh=o(eh,"CODE",{});var lN=l(Qh);c0=u(lN,"None"),lN.forEach(t),f0=u(eh,"). Integer to define the top tokens considered within the "),Bi=o(eh,"SPAN",{class:!0});var iN=l(Bi);h0=u(iN,"sample"),iN.forEach(t),d0=u(eh," operation to create new text."),eh.forEach(t),Av.forEach(t),g0=h(C),vn=o(C,"TR",{});var Dv=l(vn);Ci=o(Dv,"TD",{align:!0});var uN=l(Ci);m0=u(uN,"top_p"),uN.forEach(t),$0=h(Dv),ge=o(Dv,"TD",{align:!0});var th=l(ge);_0=u(th,"(Default: "),Zh=o(th,"CODE",{});var pN=l(Zh);q0=u(pN,"None"),pN.forEach(t),v0=u(th,"). Float to define the tokens that are within the sample` operation of text generation. Add tokens in the sample for more probable to least probable until the sum of the probabilities is greater than "),Gi=o(th,"SPAN",{class:!0});var cN=l(Gi);y0=u(cN,"top_p"),cN.forEach(t),w0=u(th,"."),th.forEach(t),Dv.forEach(t),E0=h(C),yn=o(C,"TR",{});var Pv=l(yn);Ui=o(Pv,"TD",{align:!0});var fN=l(Ui);b0=u(fN,"temperature"),fN.forEach(t),j0=h(Pv),ae=o(Pv,"TD",{align:!0});var ja=l(ae);T0=u(ja,"(Default: "),ed=o(ja,"CODE",{});var hN=l(ed);k0=u(hN,"1.0"),hN.forEach(t),A0=u(ja,"). Float (0.0-100.0). The temperature of the sampling operation. 1 means regular sampling, 0 means "),Li=o(ja,"SPAN",{class:!0});var dN=l(Li);D0=u(dN,"top_k=1"),dN.forEach(t),P0=u(ja,", "),zi=o(ja,"SPAN",{class:!0});var gN=l(zi);O0=u(gN,"100.0"),gN.forEach(t),R0=u(ja," is getting closer to uniform probability."),ja.forEach(t),Pv.forEach(t),N0=h(C),wn=o(C,"TR",{});var Ov=l(wn);Mi=o(Ov,"TD",{align:!0});var mN=l(Mi);S0=u(mN,"repetition_penalty"),mN.forEach(t),x0=h(Ov),kt=o(Ov,"TD",{align:!0});var Rv=l(kt);I0=u(Rv,"(Default: "),td=o(Rv,"CODE",{});var $N=l(td);H0=u($N,"None"),$N.forEach(t),B0=u(Rv,"). Float (0.0-100.0). The more a token is used within generation the more it is penalized to not be picked in successive generation passes."),Rv.forEach(t),Ov.forEach(t),C0=h(C),En=o(C,"TR",{});var Nv=l(En);Fi=o(Nv,"TD",{align:!0});var _N=l(Fi);G0=u(_N,"max_time"),_N.forEach(t),U0=h(Nv),At=o(Nv,"TD",{align:!0});var Sv=l(At);L0=u(Sv,"(Default: "),sd=o(Sv,"CODE",{});var qN=l(sd);z0=u(qN,"None"),qN.forEach(t),M0=u(Sv,"). Float (0-120.0). The amount of time in seconds that the query should take maximum. Network can cause some overhead so it will be a soft limit."),Sv.forEach(t),Nv.forEach(t),F0=h(C),bn=o(C,"TR",{});var xv=l(bn);Ji=o(xv,"TD",{align:!0});var vN=l(Ji);ad=o(vN,"STRONG",{});var yN=l(ad);J0=u(yN,"options"),yN.forEach(t),vN.forEach(t),K0=h(xv),Ki=o(xv,"TD",{align:!0});var wN=l(Ki);W0=u(wN,"a dict containing the following keys:"),wN.forEach(t),xv.forEach(t),Y0=h(C),jn=o(C,"TR",{});var Iv=l(jn);Wi=o(Iv,"TD",{align:!0});var EN=l(Wi);V0=u(EN,"use_gpu"),EN.forEach(t),X0=h(Iv),Dt=o(Iv,"TD",{align:!0});var Hv=l(Dt);Q0=u(Hv,"(Default: "),nd=o(Hv,"CODE",{});var bN=l(nd);Z0=u(bN,"false"),bN.forEach(t),eT=u(Hv,"). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),Hv.forEach(t),Iv.forEach(t),tT=h(C),Tn=o(C,"TR",{});var Bv=l(Tn);Yi=o(Bv,"TD",{align:!0});var jN=l(Yi);sT=u(jN,"use_cache"),jN.forEach(t),aT=h(Bv),Pt=o(Bv,"TD",{align:!0});var Cv=l(Pt);nT=u(Cv,"(Default: "),rd=o(Cv,"CODE",{});var TN=l(rd);rT=u(TN,"true"),TN.forEach(t),oT=u(Cv,"). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),Cv.forEach(t),Bv.forEach(t),lT=h(C),kn=o(C,"TR",{});var Gv=l(kn);Vi=o(Gv,"TD",{align:!0});var kN=l(Vi);iT=u(kN,"wait_for_model"),kN.forEach(t),uT=h(Gv),Ot=o(Gv,"TD",{align:!0});var Uv=l(Ot);pT=u(Uv,"(Default: "),od=o(Uv,"CODE",{});var AN=l(od);cT=u(AN,"false"),AN.forEach(t),fT=u(Uv,") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),Uv.forEach(t),Gv.forEach(t),C.forEach(t),wv.forEach(t),A_=h(a),Xi=o(a,"P",{});var DN=l(Xi);hT=u(DN,"Return value is either a dict or a list of dicts if you sent a list of inputs"),DN.forEach(t),D_=h(a),Rt=o(a,"TABLE",{});var Lv=l(Rt);ld=o(Lv,"THEAD",{});var PN=l(ld);An=o(PN,"TR",{});var zv=l(An);Qi=o(zv,"TH",{align:!0});var ON=l(Qi);dT=u(ON,"Returned values"),ON.forEach(t),gT=h(zv),id=o(zv,"TH",{align:!0}),l(id).forEach(t),zv.forEach(t),PN.forEach(t),mT=h(Lv),ud=o(Lv,"TBODY",{});var RN=l(ud);Dn=o(RN,"TR",{});var Mv=l(Dn);Zi=o(Mv,"TD",{align:!0});var NN=l(Zi);pd=o(NN,"STRONG",{});var SN=l(pd);$T=u(SN,"summarization_text"),SN.forEach(t),NN.forEach(t),_T=h(Mv),eu=o(Mv,"TD",{align:!0});var xN=l(eu);qT=u(xN,"The string after translation"),xN.forEach(t),Mv.forEach(t),RN.forEach(t),Lv.forEach(t),P_=h(a),xe=o(a,"H2",{class:!0});var Fv=l(xe);Nt=o(Fv,"A",{id:!0,class:!0,href:!0});var IN=l(Nt);cd=o(IN,"SPAN",{});var HN=l(cd);v(Pn.$$.fragment,HN),HN.forEach(t),IN.forEach(t),vT=h(Fv),fd=o(Fv,"SPAN",{});var BN=l(fd);yT=u(BN,"Conversational task"),BN.forEach(t),Fv.forEach(t),O_=h(a),tu=o(a,"P",{});var CN=l(tu);wT=u(CN,`This task corresponds to any chatbot like structure. Models tend to have
shorted max_length, so please check with caution when using a given
model if you need long range dependency or not.`),CN.forEach(t),R_=h(a),v(St.$$.fragment,a),N_=h(a),On=o(a,"P",{});var bO=l(On);ET=u(bO,"Available with: "),Rn=o(bO,"A",{href:!0,rel:!0});var GN=l(Rn);bT=u(GN,"\u{1F917} Transformers"),GN.forEach(t),bO.forEach(t),S_=h(a),su=o(a,"P",{});var UN=l(su);jT=u(UN,"Example:"),UN.forEach(t),x_=h(a),v(xt.$$.fragment,a),I_=h(a),au=o(a,"P",{});var LN=l(au);TT=u(LN,`When sending your request, you should send a JSON encoded payload. Here
are all the options`),LN.forEach(t),H_=h(a),It=o(a,"TABLE",{});var Jv=l(It);hd=o(Jv,"THEAD",{});var zN=l(hd);Nn=o(zN,"TR",{});var Kv=l(Nn);nu=o(Kv,"TH",{align:!0});var MN=l(nu);kT=u(MN,"All parameters"),MN.forEach(t),AT=h(Kv),dd=o(Kv,"TH",{align:!0}),l(dd).forEach(t),Kv.forEach(t),zN.forEach(t),DT=h(Jv),S=o(Jv,"TBODY",{});var I=l(S);Sn=o(I,"TR",{});var Wv=l(Sn);xn=o(Wv,"TD",{align:!0});var jO=l(xn);gd=o(jO,"STRONG",{});var FN=l(gd);PT=u(FN,"inputs"),FN.forEach(t),OT=u(jO," (required)"),jO.forEach(t),RT=h(Wv),md=o(Wv,"TD",{align:!0}),l(md).forEach(t),Wv.forEach(t),NT=h(I),In=o(I,"TR",{});var Yv=l(In);ru=o(Yv,"TD",{align:!0});var JN=l(ru);ST=u(JN,"text (required)"),JN.forEach(t),xT=h(Yv),ou=o(Yv,"TD",{align:!0});var KN=l(ou);IT=u(KN,"The last input from the user in the conversation."),KN.forEach(t),Yv.forEach(t),HT=h(I),Hn=o(I,"TR",{});var Vv=l(Hn);lu=o(Vv,"TD",{align:!0});var WN=l(lu);BT=u(WN,"generated_responses"),WN.forEach(t),CT=h(Vv),iu=o(Vv,"TD",{align:!0});var YN=l(iu);GT=u(YN,"A list of strings corresponding to the earlier replies from the model."),YN.forEach(t),Vv.forEach(t),UT=h(I),Bn=o(I,"TR",{});var Xv=l(Bn);uu=o(Xv,"TD",{align:!0});var VN=l(uu);LT=u(VN,"past_user_inputs"),VN.forEach(t),zT=h(Xv),Ht=o(Xv,"TD",{align:!0});var Qv=l(Ht);MT=u(Qv,"A list of strings corresponding to the earlier replies from the user. Should be of the same length of "),$d=o(Qv,"CODE",{});var XN=l($d);FT=u(XN,"generated_responses"),XN.forEach(t),JT=u(Qv,"."),Qv.forEach(t),Xv.forEach(t),KT=h(I),Cn=o(I,"TR",{});var Zv=l(Cn);pu=o(Zv,"TD",{align:!0});var QN=l(pu);_d=o(QN,"STRONG",{});var ZN=l(_d);WT=u(ZN,"parameters"),ZN.forEach(t),QN.forEach(t),YT=h(Zv),cu=o(Zv,"TD",{align:!0});var eS=l(cu);VT=u(eS,"a dict containing the following keys:"),eS.forEach(t),Zv.forEach(t),XT=h(I),Gn=o(I,"TR",{});var e2=l(Gn);fu=o(e2,"TD",{align:!0});var tS=l(fu);QT=u(tS,"min_length"),tS.forEach(t),ZT=h(e2),me=o(e2,"TD",{align:!0});var sh=l(me);e3=u(sh,"(Default: "),qd=o(sh,"CODE",{});var sS=l(qd);t3=u(sS,"None"),sS.forEach(t),s3=u(sh,"). Integer to define the minimum length "),vd=o(sh,"STRONG",{});var aS=l(vd);a3=u(aS,"in tokens"),aS.forEach(t),n3=u(sh," of the output summary."),sh.forEach(t),e2.forEach(t),r3=h(I),Un=o(I,"TR",{});var t2=l(Un);hu=o(t2,"TD",{align:!0});var nS=l(hu);o3=u(nS,"max_length"),nS.forEach(t),l3=h(t2),$e=o(t2,"TD",{align:!0});var ah=l($e);i3=u(ah,"(Default: "),yd=o(ah,"CODE",{});var rS=l(yd);u3=u(rS,"None"),rS.forEach(t),p3=u(ah,"). Integer to define the maximum length "),wd=o(ah,"STRONG",{});var oS=l(wd);c3=u(oS,"in tokens"),oS.forEach(t),f3=u(ah," of the output summary."),ah.forEach(t),t2.forEach(t),h3=h(I),Ln=o(I,"TR",{});var s2=l(Ln);du=o(s2,"TD",{align:!0});var lS=l(du);d3=u(lS,"top_k"),lS.forEach(t),g3=h(s2),_e=o(s2,"TD",{align:!0});var nh=l(_e);m3=u(nh,"(Default: "),Ed=o(nh,"CODE",{});var iS=l(Ed);$3=u(iS,"None"),iS.forEach(t),_3=u(nh,"). Integer to define the top tokens considered within the "),gu=o(nh,"SPAN",{class:!0});var uS=l(gu);q3=u(uS,"sample"),uS.forEach(t),v3=u(nh," operation to create new text."),nh.forEach(t),s2.forEach(t),y3=h(I),zn=o(I,"TR",{});var a2=l(zn);mu=o(a2,"TD",{align:!0});var pS=l(mu);w3=u(pS,"top_p"),pS.forEach(t),E3=h(a2),qe=o(a2,"TD",{align:!0});var rh=l(qe);b3=u(rh,"(Default: "),bd=o(rh,"CODE",{});var cS=l(bd);j3=u(cS,"None"),cS.forEach(t),T3=u(rh,"). Float to define the tokens that are within the sample` operation of text generation. Add tokens in the sample for more probable to least probable until the sum of the probabilities is greater than "),$u=o(rh,"SPAN",{class:!0});var fS=l($u);k3=u(fS,"top_p"),fS.forEach(t),A3=u(rh,"."),rh.forEach(t),a2.forEach(t),D3=h(I),Mn=o(I,"TR",{});var n2=l(Mn);_u=o(n2,"TD",{align:!0});var hS=l(_u);P3=u(hS,"temperature"),hS.forEach(t),O3=h(n2),ne=o(n2,"TD",{align:!0});var Ta=l(ne);R3=u(Ta,"(Default: "),jd=o(Ta,"CODE",{});var dS=l(jd);N3=u(dS,"1.0"),dS.forEach(t),S3=u(Ta,"). Float (0.0-100.0). The temperature of the sampling operation. 1 means regular sampling, 0 means "),qu=o(Ta,"SPAN",{class:!0});var gS=l(qu);x3=u(gS,"top_k=1"),gS.forEach(t),I3=u(Ta,", "),vu=o(Ta,"SPAN",{class:!0});var mS=l(vu);H3=u(mS,"100.0"),mS.forEach(t),B3=u(Ta," is getting closer to uniform probability."),Ta.forEach(t),n2.forEach(t),C3=h(I),Fn=o(I,"TR",{});var r2=l(Fn);yu=o(r2,"TD",{align:!0});var $S=l(yu);G3=u($S,"repetition_penalty"),$S.forEach(t),U3=h(r2),Bt=o(r2,"TD",{align:!0});var o2=l(Bt);L3=u(o2,"(Default: "),Td=o(o2,"CODE",{});var _S=l(Td);z3=u(_S,"None"),_S.forEach(t),M3=u(o2,"). Float (0.0-100.0). The more a token is used within generation the more it is penalized to not be picked in successive generation passes."),o2.forEach(t),r2.forEach(t),F3=h(I),Jn=o(I,"TR",{});var l2=l(Jn);wu=o(l2,"TD",{align:!0});var qS=l(wu);J3=u(qS,"max_time"),qS.forEach(t),K3=h(l2),Ct=o(l2,"TD",{align:!0});var i2=l(Ct);W3=u(i2,"(Default: "),kd=o(i2,"CODE",{});var vS=l(kd);Y3=u(vS,"None"),vS.forEach(t),V3=u(i2,"). Float (0-120.0). The amount of time in seconds that the query should take maximum. Network can cause some overhead so it will be a soft limit."),i2.forEach(t),l2.forEach(t),X3=h(I),Kn=o(I,"TR",{});var u2=l(Kn);Eu=o(u2,"TD",{align:!0});var yS=l(Eu);Ad=o(yS,"STRONG",{});var wS=l(Ad);Q3=u(wS,"options"),wS.forEach(t),yS.forEach(t),Z3=h(u2),bu=o(u2,"TD",{align:!0});var ES=l(bu);e5=u(ES,"a dict containing the following keys:"),ES.forEach(t),u2.forEach(t),t5=h(I),Wn=o(I,"TR",{});var p2=l(Wn);ju=o(p2,"TD",{align:!0});var bS=l(ju);s5=u(bS,"use_gpu"),bS.forEach(t),a5=h(p2),Gt=o(p2,"TD",{align:!0});var c2=l(Gt);n5=u(c2,"(Default: "),Dd=o(c2,"CODE",{});var jS=l(Dd);r5=u(jS,"false"),jS.forEach(t),o5=u(c2,"). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),c2.forEach(t),p2.forEach(t),l5=h(I),Yn=o(I,"TR",{});var f2=l(Yn);Tu=o(f2,"TD",{align:!0});var TS=l(Tu);i5=u(TS,"use_cache"),TS.forEach(t),u5=h(f2),Ut=o(f2,"TD",{align:!0});var h2=l(Ut);p5=u(h2,"(Default: "),Pd=o(h2,"CODE",{});var kS=l(Pd);c5=u(kS,"true"),kS.forEach(t),f5=u(h2,"). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),h2.forEach(t),f2.forEach(t),h5=h(I),Vn=o(I,"TR",{});var d2=l(Vn);ku=o(d2,"TD",{align:!0});var AS=l(ku);d5=u(AS,"wait_for_model"),AS.forEach(t),g5=h(d2),Lt=o(d2,"TD",{align:!0});var g2=l(Lt);m5=u(g2,"(Default: "),Od=o(g2,"CODE",{});var DS=l(Od);$5=u(DS,"false"),DS.forEach(t),_5=u(g2,") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),g2.forEach(t),d2.forEach(t),I.forEach(t),Jv.forEach(t),B_=h(a),Au=o(a,"P",{});var PS=l(Au);q5=u(PS,"Return value is either a dict or a list of dicts if you sent a list of inputs"),PS.forEach(t),C_=h(a),zt=o(a,"TABLE",{});var m2=l(zt);Rd=o(m2,"THEAD",{});var OS=l(Rd);Xn=o(OS,"TR",{});var $2=l(Xn);Du=o($2,"TH",{align:!0});var RS=l(Du);v5=u(RS,"Returned values"),RS.forEach(t),y5=h($2),Nd=o($2,"TH",{align:!0}),l(Nd).forEach(t),$2.forEach(t),OS.forEach(t),w5=h(m2),oe=o(m2,"TBODY",{});var ka=l(oe);Qn=o(ka,"TR",{});var _2=l(Qn);Pu=o(_2,"TD",{align:!0});var NS=l(Pu);Sd=o(NS,"STRONG",{});var SS=l(Sd);E5=u(SS,"generated_text"),SS.forEach(t),NS.forEach(t),b5=h(_2),Ou=o(_2,"TD",{align:!0});var xS=l(Ou);j5=u(xS,"The answer of the bot"),xS.forEach(t),_2.forEach(t),T5=h(ka),Zn=o(ka,"TR",{});var q2=l(Zn);Ru=o(q2,"TD",{align:!0});var IS=l(Ru);xd=o(IS,"STRONG",{});var HS=l(xd);k5=u(HS,"conversation"),HS.forEach(t),IS.forEach(t),A5=h(q2),Nu=o(q2,"TD",{align:!0});var BS=l(Nu);D5=u(BS,"A facility dictionnary to send back for the next input (with the new user input addition)."),BS.forEach(t),q2.forEach(t),P5=h(ka),er=o(ka,"TR",{});var v2=l(er);Su=o(v2,"TD",{align:!0});var CS=l(Su);O5=u(CS,"past_user_inputs"),CS.forEach(t),R5=h(v2),xu=o(v2,"TD",{align:!0});var GS=l(xu);N5=u(GS,"List of strings. The last inputs from the user in the conversation, <em>after the model has run."),GS.forEach(t),v2.forEach(t),S5=h(ka),tr=o(ka,"TR",{});var y2=l(tr);Iu=o(y2,"TD",{align:!0});var US=l(Iu);x5=u(US,"generated_responses"),US.forEach(t),I5=h(y2),Hu=o(y2,"TD",{align:!0});var LS=l(Hu);H5=u(LS,"List of strings. The last outputs from the model in the conversation, <em>after the model has run."),LS.forEach(t),y2.forEach(t),ka.forEach(t),m2.forEach(t),G_=h(a),Ie=o(a,"H2",{class:!0});var w2=l(Ie);Mt=o(w2,"A",{id:!0,class:!0,href:!0});var zS=l(Mt);Id=o(zS,"SPAN",{});var MS=l(Id);v(sr.$$.fragment,MS),MS.forEach(t),zS.forEach(t),B5=h(w2),Hd=o(w2,"SPAN",{});var FS=l(Hd);C5=u(FS,"Table question answering task"),FS.forEach(t),w2.forEach(t),U_=h(a),Bu=o(a,"P",{});var JS=l(Bu);G5=u(JS,`Don\u2019t know SQL? Don\u2019t want to dive into a large spreadsheet? Ask
questions in plain english!`),JS.forEach(t),L_=h(a),v(Ft.$$.fragment,a),z_=h(a),ar=o(a,"P",{});var TO=l(ar);U5=u(TO,"Available with: "),nr=o(TO,"A",{href:!0,rel:!0});var KS=l(nr);L5=u(KS,"\u{1F917} Transformers"),KS.forEach(t),TO.forEach(t),M_=h(a),Cu=o(a,"P",{});var WS=l(Cu);z5=u(WS,"Example:"),WS.forEach(t),F_=h(a),v(Jt.$$.fragment,a),J_=h(a),Gu=o(a,"P",{});var YS=l(Gu);M5=u(YS,`When sending your request, you should send a JSON encoded payload. Here
are all the options`),YS.forEach(t),K_=h(a),Kt=o(a,"TABLE",{});var E2=l(Kt);Bd=o(E2,"THEAD",{});var VS=l(Bd);rr=o(VS,"TR",{});var b2=l(rr);Uu=o(b2,"TH",{align:!0});var XS=l(Uu);F5=u(XS,"All parameters"),XS.forEach(t),J5=h(b2),Cd=o(b2,"TH",{align:!0}),l(Cd).forEach(t),b2.forEach(t),VS.forEach(t),K5=h(E2),M=o(E2,"TBODY",{});var W=l(M);or=o(W,"TR",{});var j2=l(or);lr=o(j2,"TD",{align:!0});var kO=l(lr);Gd=o(kO,"STRONG",{});var QS=l(Gd);W5=u(QS,"inputs"),QS.forEach(t),Y5=u(kO," (required)"),kO.forEach(t),V5=h(j2),Ud=o(j2,"TD",{align:!0}),l(Ud).forEach(t),j2.forEach(t),X5=h(W),ir=o(W,"TR",{});var T2=l(ir);Lu=o(T2,"TD",{align:!0});var ZS=l(Lu);Q5=u(ZS,"query (required)"),ZS.forEach(t),Z5=h(T2),zu=o(T2,"TD",{align:!0});var ex=l(zu);ek=u(ex,"The query in plain text that you want to ask the table"),ex.forEach(t),T2.forEach(t),tk=h(W),ur=o(W,"TR",{});var k2=l(ur);Mu=o(k2,"TD",{align:!0});var tx=l(Mu);sk=u(tx,"table (required)"),tx.forEach(t),ak=h(k2),Fu=o(k2,"TD",{align:!0});var sx=l(Fu);nk=u(sx,"A table of data represented as a dict of list where entries are headers and the lists are all the values, all lists must have the same size."),sx.forEach(t),k2.forEach(t),rk=h(W),pr=o(W,"TR",{});var A2=l(pr);Ju=o(A2,"TD",{align:!0});var ax=l(Ju);Ld=o(ax,"STRONG",{});var nx=l(Ld);ok=u(nx,"options"),nx.forEach(t),ax.forEach(t),lk=h(A2),Ku=o(A2,"TD",{align:!0});var rx=l(Ku);ik=u(rx,"a dict containing the following keys:"),rx.forEach(t),A2.forEach(t),uk=h(W),cr=o(W,"TR",{});var D2=l(cr);Wu=o(D2,"TD",{align:!0});var ox=l(Wu);pk=u(ox,"use_gpu"),ox.forEach(t),ck=h(D2),Wt=o(D2,"TD",{align:!0});var P2=l(Wt);fk=u(P2,"(Default: "),zd=o(P2,"CODE",{});var lx=l(zd);hk=u(lx,"false"),lx.forEach(t),dk=u(P2,"). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),P2.forEach(t),D2.forEach(t),gk=h(W),fr=o(W,"TR",{});var O2=l(fr);Yu=o(O2,"TD",{align:!0});var ix=l(Yu);mk=u(ix,"use_cache"),ix.forEach(t),$k=h(O2),Yt=o(O2,"TD",{align:!0});var R2=l(Yt);_k=u(R2,"(Default: "),Md=o(R2,"CODE",{});var ux=l(Md);qk=u(ux,"true"),ux.forEach(t),vk=u(R2,"). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),R2.forEach(t),O2.forEach(t),yk=h(W),hr=o(W,"TR",{});var N2=l(hr);Vu=o(N2,"TD",{align:!0});var px=l(Vu);wk=u(px,"wait_for_model"),px.forEach(t),Ek=h(N2),Vt=o(N2,"TD",{align:!0});var S2=l(Vt);bk=u(S2,"(Default: "),Fd=o(S2,"CODE",{});var cx=l(Fd);jk=u(cx,"false"),cx.forEach(t),Tk=u(S2,") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),S2.forEach(t),N2.forEach(t),W.forEach(t),E2.forEach(t),W_=h(a),Xu=o(a,"P",{});var fx=l(Xu);kk=u(fx,"Return value is either a dict or a list of dicts if you sent a list of inputs"),fx.forEach(t),Y_=h(a),v(Xt.$$.fragment,a),V_=h(a),Qt=o(a,"TABLE",{});var x2=l(Qt);Jd=o(x2,"THEAD",{});var hx=l(Jd);dr=o(hx,"TR",{});var I2=l(dr);Qu=o(I2,"TH",{align:!0});var dx=l(Qu);Ak=u(dx,"Returned values"),dx.forEach(t),Dk=h(I2),Kd=o(I2,"TH",{align:!0}),l(Kd).forEach(t),I2.forEach(t),hx.forEach(t),Pk=h(x2),le=o(x2,"TBODY",{});var Aa=l(le);gr=o(Aa,"TR",{});var H2=l(gr);Zu=o(H2,"TD",{align:!0});var gx=l(Zu);Wd=o(gx,"STRONG",{});var mx=l(Wd);Ok=u(mx,"answer"),mx.forEach(t),gx.forEach(t),Rk=h(H2),ep=o(H2,"TD",{align:!0});var $x=l(ep);Nk=u($x,"The plaintext answer"),$x.forEach(t),H2.forEach(t),Sk=h(Aa),mr=o(Aa,"TR",{});var B2=l(mr);tp=o(B2,"TD",{align:!0});var _x=l(tp);Yd=o(_x,"STRONG",{});var qx=l(Yd);xk=u(qx,"coordinates"),qx.forEach(t),_x.forEach(t),Ik=h(B2),sp=o(B2,"TD",{align:!0});var vx=l(sp);Hk=u(vx,"a list of coordinates of the cells references in the answer"),vx.forEach(t),B2.forEach(t),Bk=h(Aa),$r=o(Aa,"TR",{});var C2=l($r);ap=o(C2,"TD",{align:!0});var yx=l(ap);Vd=o(yx,"STRONG",{});var wx=l(Vd);Ck=u(wx,"cells"),wx.forEach(t),yx.forEach(t),Gk=h(C2),np=o(C2,"TD",{align:!0});var Ex=l(np);Uk=u(Ex,"a list of coordinates of the cells contents"),Ex.forEach(t),C2.forEach(t),Lk=h(Aa),_r=o(Aa,"TR",{});var G2=l(_r);rp=o(G2,"TD",{align:!0});var bx=l(rp);Xd=o(bx,"STRONG",{});var jx=l(Xd);zk=u(jx,"aggregator"),jx.forEach(t),bx.forEach(t),Mk=h(G2),op=o(G2,"TD",{align:!0});var Tx=l(op);Fk=u(Tx,"The aggregator used to get the answer"),Tx.forEach(t),G2.forEach(t),Aa.forEach(t),x2.forEach(t),X_=h(a),He=o(a,"H2",{class:!0});var U2=l(He);Zt=o(U2,"A",{id:!0,class:!0,href:!0});var kx=l(Zt);Qd=o(kx,"SPAN",{});var Ax=l(Qd);v(qr.$$.fragment,Ax),Ax.forEach(t),kx.forEach(t),Jk=h(U2),Zd=o(U2,"SPAN",{});var Dx=l(Zd);Kk=u(Dx,"Question answering task"),Dx.forEach(t),U2.forEach(t),Q_=h(a),lp=o(a,"P",{});var Px=l(lp);Wk=u(Px,"Want to have a nice know-it-all bot that can answer any questions ?"),Px.forEach(t),Z_=h(a),v(es.$$.fragment,a),eq=h(a),Be=o(a,"P",{});var F$=l(Be);Yk=u(F$,"Available with: "),vr=o(F$,"A",{href:!0,rel:!0});var Ox=l(vr);Vk=u(Ox,"\u{1F917}Transformers"),Ox.forEach(t),Xk=u(F$,` and
`),yr=o(F$,"A",{href:!0,rel:!0});var Rx=l(yr);Qk=u(Rx,"AllenNLP"),Rx.forEach(t),F$.forEach(t),tq=h(a),ip=o(a,"P",{});var Nx=l(ip);Zk=u(Nx,"Example:"),Nx.forEach(t),sq=h(a),v(ts.$$.fragment,a),aq=h(a),up=o(a,"P",{});var Sx=l(up);e4=u(Sx,`When sending your request, you should send a JSON encoded payload. Here
are all the options`),Sx.forEach(t),nq=h(a),pp=o(a,"P",{});var xx=l(pp);t4=u(xx,"Return value is either a dict or a list of dicts if you sent a list of inputs"),xx.forEach(t),rq=h(a),v(ss.$$.fragment,a),oq=h(a),as=o(a,"TABLE",{});var L2=l(as);eg=o(L2,"THEAD",{});var Ix=l(eg);wr=o(Ix,"TR",{});var z2=l(wr);cp=o(z2,"TH",{align:!0});var Hx=l(cp);s4=u(Hx,"Returned values"),Hx.forEach(t),a4=h(z2),tg=o(z2,"TH",{align:!0}),l(tg).forEach(t),z2.forEach(t),Ix.forEach(t),n4=h(L2),ie=o(L2,"TBODY",{});var Da=l(ie);Er=o(Da,"TR",{});var M2=l(Er);fp=o(M2,"TD",{align:!0});var Bx=l(fp);sg=o(Bx,"STRONG",{});var Cx=l(sg);r4=u(Cx,"answer"),Cx.forEach(t),Bx.forEach(t),o4=h(M2),hp=o(M2,"TD",{align:!0});var Gx=l(hp);l4=u(Gx,"A string that\u2019s the answer within the text."),Gx.forEach(t),M2.forEach(t),i4=h(Da),br=o(Da,"TR",{});var F2=l(br);dp=o(F2,"TD",{align:!0});var Ux=l(dp);ag=o(Ux,"STRONG",{});var Lx=l(ag);u4=u(Lx,"score"),Lx.forEach(t),Ux.forEach(t),p4=h(F2),gp=o(F2,"TD",{align:!0});var zx=l(gp);c4=u(zx,"A floats that represents how likely that the answer is correct"),zx.forEach(t),F2.forEach(t),f4=h(Da),jr=o(Da,"TR",{});var J2=l(jr);mp=o(J2,"TD",{align:!0});var Mx=l(mp);ng=o(Mx,"STRONG",{});var Fx=l(ng);h4=u(Fx,"start"),Fx.forEach(t),Mx.forEach(t),d4=h(J2),ns=o(J2,"TD",{align:!0});var K2=l(ns);g4=u(K2,"The index (string wise) of the start of the answer within "),rg=o(K2,"CODE",{});var Jx=l(rg);m4=u(Jx,"context"),Jx.forEach(t),$4=u(K2,"."),K2.forEach(t),J2.forEach(t),_4=h(Da),Tr=o(Da,"TR",{});var W2=l(Tr);$p=o(W2,"TD",{align:!0});var Kx=l($p);og=o(Kx,"STRONG",{});var Wx=l(og);q4=u(Wx,"stop"),Wx.forEach(t),Kx.forEach(t),v4=h(W2),rs=o(W2,"TD",{align:!0});var Y2=l(rs);y4=u(Y2,"The index (string wise) of the stop of the answer within "),lg=o(Y2,"CODE",{});var Yx=l(lg);w4=u(Yx,"context"),Yx.forEach(t),E4=u(Y2,"."),Y2.forEach(t),W2.forEach(t),Da.forEach(t),L2.forEach(t),lq=h(a),Ce=o(a,"H2",{class:!0});var V2=l(Ce);os=o(V2,"A",{id:!0,class:!0,href:!0});var Vx=l(os);ig=o(Vx,"SPAN",{});var Xx=l(ig);v(kr.$$.fragment,Xx),Xx.forEach(t),Vx.forEach(t),b4=h(V2),ug=o(V2,"SPAN",{});var Qx=l(ug);j4=u(Qx,"Text-classification task"),Qx.forEach(t),V2.forEach(t),iq=h(a),_p=o(a,"P",{});var Zx=l(_p);T4=u(Zx,`Usually used for sentiment-analysis this will output the likelihood of
classes of an input.`),Zx.forEach(t),uq=h(a),v(ls.$$.fragment,a),pq=h(a),Ar=o(a,"P",{});var AO=l(Ar);k4=u(AO,"Available with: "),Dr=o(AO,"A",{href:!0,rel:!0});var eI=l(Dr);A4=u(eI,"\u{1F917} Transformers"),eI.forEach(t),AO.forEach(t),cq=h(a),qp=o(a,"P",{});var tI=l(qp);D4=u(tI,"Example:"),tI.forEach(t),fq=h(a),v(is.$$.fragment,a),hq=h(a),vp=o(a,"P",{});var sI=l(vp);P4=u(sI,`When sending your request, you should send a JSON encoded payload. Here
are all the options`),sI.forEach(t),dq=h(a),us=o(a,"TABLE",{});var X2=l(us);pg=o(X2,"THEAD",{});var aI=l(pg);Pr=o(aI,"TR",{});var Q2=l(Pr);yp=o(Q2,"TH",{align:!0});var nI=l(yp);O4=u(nI,"All parameters"),nI.forEach(t),R4=h(Q2),cg=o(Q2,"TH",{align:!0}),l(cg).forEach(t),Q2.forEach(t),aI.forEach(t),N4=h(X2),Q=o(X2,"TBODY",{});var je=l(Q);Or=o(je,"TR",{});var Z2=l(Or);Rr=o(Z2,"TD",{align:!0});var DO=l(Rr);fg=o(DO,"STRONG",{});var rI=l(fg);S4=u(rI,"inputs"),rI.forEach(t),x4=u(DO," (required)"),DO.forEach(t),I4=h(Z2),wp=o(Z2,"TD",{align:!0});var oI=l(wp);H4=u(oI,"a string to be classified"),oI.forEach(t),Z2.forEach(t),B4=h(je),Nr=o(je,"TR",{});var ey=l(Nr);Ep=o(ey,"TD",{align:!0});var lI=l(Ep);hg=o(lI,"STRONG",{});var iI=l(hg);C4=u(iI,"options"),iI.forEach(t),lI.forEach(t),G4=h(ey),bp=o(ey,"TD",{align:!0});var uI=l(bp);U4=u(uI,"a dict containing the following keys:"),uI.forEach(t),ey.forEach(t),L4=h(je),Sr=o(je,"TR",{});var ty=l(Sr);jp=o(ty,"TD",{align:!0});var pI=l(jp);z4=u(pI,"use_gpu"),pI.forEach(t),M4=h(ty),ps=o(ty,"TD",{align:!0});var sy=l(ps);F4=u(sy,"(Default: "),dg=o(sy,"CODE",{});var cI=l(dg);J4=u(cI,"false"),cI.forEach(t),K4=u(sy,"). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),sy.forEach(t),ty.forEach(t),W4=h(je),xr=o(je,"TR",{});var ay=l(xr);Tp=o(ay,"TD",{align:!0});var fI=l(Tp);Y4=u(fI,"use_cache"),fI.forEach(t),V4=h(ay),cs=o(ay,"TD",{align:!0});var ny=l(cs);X4=u(ny,"(Default: "),gg=o(ny,"CODE",{});var hI=l(gg);Q4=u(hI,"true"),hI.forEach(t),Z4=u(ny,"). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),ny.forEach(t),ay.forEach(t),e7=h(je),Ir=o(je,"TR",{});var ry=l(Ir);kp=o(ry,"TD",{align:!0});var dI=l(kp);t7=u(dI,"wait_for_model"),dI.forEach(t),s7=h(ry),fs=o(ry,"TD",{align:!0});var oy=l(fs);a7=u(oy,"(Default: "),mg=o(oy,"CODE",{});var gI=l(mg);n7=u(gI,"false"),gI.forEach(t),r7=u(oy,") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),oy.forEach(t),ry.forEach(t),je.forEach(t),X2.forEach(t),gq=h(a),Ap=o(a,"P",{});var mI=l(Ap);o7=u(mI,"Return value is either a dict or a list of dicts if you sent a list of inputs"),mI.forEach(t),mq=h(a),v(hs.$$.fragment,a),$q=h(a),ds=o(a,"TABLE",{});var ly=l(ds);$g=o(ly,"THEAD",{});var $I=l($g);Hr=o($I,"TR",{});var iy=l(Hr);Dp=o(iy,"TH",{align:!0});var _I=l(Dp);l7=u(_I,"Returned values"),_I.forEach(t),i7=h(iy),_g=o(iy,"TH",{align:!0}),l(_g).forEach(t),iy.forEach(t),$I.forEach(t),u7=h(ly),Br=o(ly,"TBODY",{});var uy=l(Br);Cr=o(uy,"TR",{});var py=l(Cr);Pp=o(py,"TD",{align:!0});var qI=l(Pp);qg=o(qI,"STRONG",{});var vI=l(qg);p7=u(vI,"label"),vI.forEach(t),qI.forEach(t),c7=h(py),Op=o(py,"TD",{align:!0});var yI=l(Op);f7=u(yI,"The label for the class (model specific)"),yI.forEach(t),py.forEach(t),h7=h(uy),Gr=o(uy,"TR",{});var cy=l(Gr);Rp=o(cy,"TD",{align:!0});var wI=l(Rp);vg=o(wI,"STRONG",{});var EI=l(vg);d7=u(EI,"score"),EI.forEach(t),wI.forEach(t),g7=h(cy),Np=o(cy,"TD",{align:!0});var bI=l(Np);m7=u(bI,"A floats that represents how likely is that the text belongs the this class."),bI.forEach(t),cy.forEach(t),uy.forEach(t),ly.forEach(t),_q=h(a),Ge=o(a,"H2",{class:!0});var fy=l(Ge);gs=o(fy,"A",{id:!0,class:!0,href:!0});var jI=l(gs);yg=o(jI,"SPAN",{});var TI=l(yg);v(Ur.$$.fragment,TI),TI.forEach(t),jI.forEach(t),$7=h(fy),wg=o(fy,"SPAN",{});var kI=l(wg);_7=u(kI,"Named Entity Recognition (NER) task"),kI.forEach(t),fy.forEach(t),qq=h(a),Lr=o(a,"P",{});var PO=l(Lr);q7=u(PO,"See "),Sp=o(PO,"A",{href:!0});var AI=l(Sp);v7=u(AI,"Token-classification task"),AI.forEach(t),PO.forEach(t),vq=h(a),Ue=o(a,"H2",{class:!0});var hy=l(Ue);ms=o(hy,"A",{id:!0,class:!0,href:!0});var DI=l(ms);Eg=o(DI,"SPAN",{});var PI=l(Eg);v(zr.$$.fragment,PI),PI.forEach(t),DI.forEach(t),y7=h(hy),bg=o(hy,"SPAN",{});var OI=l(bg);w7=u(OI,"Token-classification task"),OI.forEach(t),hy.forEach(t),yq=h(a),xp=o(a,"P",{});var RI=l(xp);E7=u(RI,`Usually used for sentence parsing, either grammatical, or Named Entity
Recognition (NER) to understand keywords contained within text.`),RI.forEach(t),wq=h(a),v($s.$$.fragment,a),Eq=h(a),Le=o(a,"P",{});var J$=l(Le);b7=u(J$,"Available with: "),Mr=o(J$,"A",{href:!0,rel:!0});var NI=l(Mr);j7=u(NI,"\u{1F917} Transformers"),NI.forEach(t),T7=u(J$,`,
`),Fr=o(J$,"A",{href:!0,rel:!0});var SI=l(Fr);k7=u(SI,"Flair"),SI.forEach(t),J$.forEach(t),bq=h(a),Ip=o(a,"P",{});var xI=l(Ip);A7=u(xI,"Example:"),xI.forEach(t),jq=h(a),v(_s.$$.fragment,a),Tq=h(a),Hp=o(a,"P",{});var II=l(Hp);D7=u(II,`When sending your request, you should send a JSON encoded payload. Here
are all the options`),II.forEach(t),kq=h(a),qs=o(a,"TABLE",{});var dy=l(qs);jg=o(dy,"THEAD",{});var HI=l(jg);Jr=o(HI,"TR",{});var gy=l(Jr);Bp=o(gy,"TH",{align:!0});var BI=l(Bp);P7=u(BI,"All parameters"),BI.forEach(t),O7=h(gy),Tg=o(gy,"TH",{align:!0}),l(Tg).forEach(t),gy.forEach(t),HI.forEach(t),R7=h(dy),F=o(dy,"TBODY",{});var Y=l(F);Kr=o(Y,"TR",{});var my=l(Kr);Wr=o(my,"TD",{align:!0});var OO=l(Wr);kg=o(OO,"STRONG",{});var CI=l(kg);N7=u(CI,"inputs"),CI.forEach(t),S7=u(OO," (required)"),OO.forEach(t),x7=h(my),Cp=o(my,"TD",{align:!0});var GI=l(Cp);I7=u(GI,"a string to be classified"),GI.forEach(t),my.forEach(t),H7=h(Y),Yr=o(Y,"TR",{});var $y=l(Yr);Gp=o($y,"TD",{align:!0});var UI=l(Gp);Ag=o(UI,"STRONG",{});var LI=l(Ag);B7=u(LI,"parameters"),LI.forEach(t),UI.forEach(t),C7=h($y),Up=o($y,"TD",{align:!0});var zI=l(Up);G7=u(zI,"a dict containing the following key:"),zI.forEach(t),$y.forEach(t),U7=h(Y),Vr=o(Y,"TR",{});var _y=l(Vr);Lp=o(_y,"TD",{align:!0});var MI=l(Lp);L7=u(MI,"aggregation_strategy"),MI.forEach(t),z7=h(_y),vs=o(_y,"TD",{align:!0});var qy=l(vs);M7=u(qy,"(Default: "),Dg=o(qy,"CODE",{});var FI=l(Dg);F7=u(FI,'simple</span></code>). There are several aggregation strategies:<br>* <code class="docutils literal notranslate"><span class="pre">none</span></code>: Every token gets classified without further aggregation.<br>* <code class="docutils literal notranslate"><span class="pre">simple</span></code>: Entities are grouped according to the default schema (B-, I- tags get merged when the tag is similar).<br>* <code class="docutils literal notranslate"><span class="pre">first</span></code>: Same as the <code class="docutils literal notranslate"><span class="pre">simple</span></code> strategy except words cannot end up with different tags. Words will use the tag of the first token when there is ambiguity.<br>* <code class="docutils literal notranslate"><span class="pre">average</span></code>: Same as the <code class="docutils literal notranslate"><span class="pre">simple</span></code> strategy except words cannot end up with different tags. Scores are averaged across tokens and then the maximum label is applied.<br>* <code class="docutils literal notranslate"><span class="pre">max</span></code>: Same as the <code class="docutils literal notranslate"><span class="pre">simple'),FI.forEach(t),J7=u(qy," strategy except words cannot end up with different tags. Word entity will be the token with the maximum score."),qy.forEach(t),_y.forEach(t),K7=h(Y),Xr=o(Y,"TR",{});var vy=l(Xr);zp=o(vy,"TD",{align:!0});var JI=l(zp);Pg=o(JI,"STRONG",{});var KI=l(Pg);W7=u(KI,"options"),KI.forEach(t),JI.forEach(t),Y7=h(vy),Mp=o(vy,"TD",{align:!0});var WI=l(Mp);V7=u(WI,"a dict containing the following keys:"),WI.forEach(t),vy.forEach(t),X7=h(Y),Qr=o(Y,"TR",{});var yy=l(Qr);Fp=o(yy,"TD",{align:!0});var YI=l(Fp);Q7=u(YI,"use_gpu"),YI.forEach(t),Z7=h(yy),ys=o(yy,"TD",{align:!0});var wy=l(ys);e6=u(wy,"(Default: "),Og=o(wy,"CODE",{});var VI=l(Og);t6=u(VI,"false"),VI.forEach(t),s6=u(wy,"). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),wy.forEach(t),yy.forEach(t),a6=h(Y),Zr=o(Y,"TR",{});var Ey=l(Zr);Jp=o(Ey,"TD",{align:!0});var XI=l(Jp);n6=u(XI,"use_cache"),XI.forEach(t),r6=h(Ey),ws=o(Ey,"TD",{align:!0});var by=l(ws);o6=u(by,"(Default: "),Rg=o(by,"CODE",{});var QI=l(Rg);l6=u(QI,"true"),QI.forEach(t),i6=u(by,"). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),by.forEach(t),Ey.forEach(t),u6=h(Y),eo=o(Y,"TR",{});var jy=l(eo);Kp=o(jy,"TD",{align:!0});var ZI=l(Kp);p6=u(ZI,"wait_for_model"),ZI.forEach(t),c6=h(jy),Es=o(jy,"TD",{align:!0});var Ty=l(Es);f6=u(Ty,"(Default: "),Ng=o(Ty,"CODE",{});var eH=l(Ng);h6=u(eH,"false"),eH.forEach(t),d6=u(Ty,") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),Ty.forEach(t),jy.forEach(t),Y.forEach(t),dy.forEach(t),Aq=h(a),Wp=o(a,"P",{});var tH=l(Wp);g6=u(tH,"Return value is either a dict or a list of dicts if you sent a list of inputs"),tH.forEach(t),Dq=h(a),v(bs.$$.fragment,a),Pq=h(a),js=o(a,"TABLE",{});var ky=l(js);Sg=o(ky,"THEAD",{});var sH=l(Sg);to=o(sH,"TR",{});var Ay=l(to);Yp=o(Ay,"TH",{align:!0});var aH=l(Yp);m6=u(aH,"Returned values"),aH.forEach(t),$6=h(Ay),xg=o(Ay,"TH",{align:!0}),l(xg).forEach(t),Ay.forEach(t),sH.forEach(t),_6=h(ky),Z=o(ky,"TBODY",{});var Te=l(Z);so=o(Te,"TR",{});var Dy=l(so);Vp=o(Dy,"TD",{align:!0});var nH=l(Vp);Ig=o(nH,"STRONG",{});var rH=l(Ig);q6=u(rH,"entity_group"),rH.forEach(t),nH.forEach(t),v6=h(Dy),Xp=o(Dy,"TD",{align:!0});var oH=l(Xp);y6=u(oH,"The type for the entity being recognized (model specific)."),oH.forEach(t),Dy.forEach(t),w6=h(Te),ao=o(Te,"TR",{});var Py=l(ao);Qp=o(Py,"TD",{align:!0});var lH=l(Qp);Hg=o(lH,"STRONG",{});var iH=l(Hg);E6=u(iH,"score"),iH.forEach(t),lH.forEach(t),b6=h(Py),Zp=o(Py,"TD",{align:!0});var uH=l(Zp);j6=u(uH,"How likely the entity was recognized."),uH.forEach(t),Py.forEach(t),T6=h(Te),no=o(Te,"TR",{});var Oy=l(no);ec=o(Oy,"TD",{align:!0});var pH=l(ec);Bg=o(pH,"STRONG",{});var cH=l(Bg);k6=u(cH,"word"),cH.forEach(t),pH.forEach(t),A6=h(Oy),tc=o(Oy,"TD",{align:!0});var fH=l(tc);D6=u(fH,"The string that was captured"),fH.forEach(t),Oy.forEach(t),P6=h(Te),ro=o(Te,"TR",{});var Ry=l(ro);sc=o(Ry,"TD",{align:!0});var hH=l(sc);Cg=o(hH,"STRONG",{});var dH=l(Cg);O6=u(dH,"start"),dH.forEach(t),hH.forEach(t),R6=h(Ry),Ts=o(Ry,"TD",{align:!0});var Ny=l(Ts);N6=u(Ny,"The offset stringwise where the answer is located. Useful to disambiguate if "),Gg=o(Ny,"CODE",{});var gH=l(Gg);S6=u(gH,"word"),gH.forEach(t),x6=u(Ny," occurs multiple times."),Ny.forEach(t),Ry.forEach(t),I6=h(Te),oo=o(Te,"TR",{});var Sy=l(oo);ac=o(Sy,"TD",{align:!0});var mH=l(ac);Ug=o(mH,"STRONG",{});var $H=l(Ug);H6=u($H,"end"),$H.forEach(t),mH.forEach(t),B6=h(Sy),ks=o(Sy,"TD",{align:!0});var xy=l(ks);C6=u(xy,"The offset stringwise where the answer is located. Useful to disambiguate if "),Lg=o(xy,"CODE",{});var _H=l(Lg);G6=u(_H,"word"),_H.forEach(t),U6=u(xy," occurs multiple times."),xy.forEach(t),Sy.forEach(t),Te.forEach(t),ky.forEach(t),Oq=h(a),ze=o(a,"H2",{class:!0});var Iy=l(ze);As=o(Iy,"A",{id:!0,class:!0,href:!0});var qH=l(As);zg=o(qH,"SPAN",{});var vH=l(zg);v(lo.$$.fragment,vH),vH.forEach(t),qH.forEach(t),L6=h(Iy),Mg=o(Iy,"SPAN",{});var yH=l(Mg);z6=u(yH,"Text-generation task"),yH.forEach(t),Iy.forEach(t),Rq=h(a),nc=o(a,"P",{});var wH=l(nc);M6=u(wH,"Use to continue text from a prompt. This is a very generic task."),wH.forEach(t),Nq=h(a),v(Ds.$$.fragment,a),Sq=h(a),io=o(a,"P",{});var RO=l(io);F6=u(RO,"Available with: "),uo=o(RO,"A",{href:!0,rel:!0});var EH=l(uo);J6=u(EH,"\u{1F917} Transformers"),EH.forEach(t),RO.forEach(t),xq=h(a),rc=o(a,"P",{});var bH=l(rc);K6=u(bH,"Example:"),bH.forEach(t),Iq=h(a),v(Ps.$$.fragment,a),Hq=h(a),oc=o(a,"P",{});var jH=l(oc);W6=u(jH,`When sending your request, you should send a JSON encoded payload. Here
are all the options`),jH.forEach(t),Bq=h(a),Os=o(a,"TABLE",{});var Hy=l(Os);Fg=o(Hy,"THEAD",{});var TH=l(Fg);po=o(TH,"TR",{});var By=l(po);lc=o(By,"TH",{align:!0});var kH=l(lc);Y6=u(kH,"All parameters"),kH.forEach(t),V6=h(By),Jg=o(By,"TH",{align:!0}),l(Jg).forEach(t),By.forEach(t),TH.forEach(t),X6=h(Hy),x=o(Hy,"TBODY",{});var H=l(x);co=o(H,"TR",{});var Cy=l(co);fo=o(Cy,"TD",{align:!0});var NO=l(fo);Kg=o(NO,"STRONG",{});var AH=l(Kg);Q6=u(AH,"inputs"),AH.forEach(t),Z6=u(NO," (required):"),NO.forEach(t),e9=h(Cy),ic=o(Cy,"TD",{align:!0});var DH=l(ic);t9=u(DH,"a string to be generated from"),DH.forEach(t),Cy.forEach(t),s9=h(H),ho=o(H,"TR",{});var Gy=l(ho);uc=o(Gy,"TD",{align:!0});var PH=l(uc);Wg=o(PH,"STRONG",{});var OH=l(Wg);a9=u(OH,"parameters"),OH.forEach(t),PH.forEach(t),n9=h(Gy),pc=o(Gy,"TD",{align:!0});var RH=l(pc);r9=u(RH,"dict containing the following keys:"),RH.forEach(t),Gy.forEach(t),o9=h(H),go=o(H,"TR",{});var Uy=l(go);cc=o(Uy,"TD",{align:!0});var NH=l(cc);l9=u(NH,"top_k"),NH.forEach(t),i9=h(Uy),ve=o(Uy,"TD",{align:!0});var oh=l(ve);u9=u(oh,"(Default: "),Yg=o(oh,"CODE",{});var SH=l(Yg);p9=u(SH,"None"),SH.forEach(t),c9=u(oh,"). Integer to define the top tokens considered within the "),fc=o(oh,"SPAN",{class:!0});var xH=l(fc);f9=u(xH,"sample"),xH.forEach(t),h9=u(oh," operation to create new text."),oh.forEach(t),Uy.forEach(t),d9=h(H),mo=o(H,"TR",{});var Ly=l(mo);hc=o(Ly,"TD",{align:!0});var IH=l(hc);g9=u(IH,"top_p"),IH.forEach(t),m9=h(Ly),ye=o(Ly,"TD",{align:!0});var lh=l(ye);$9=u(lh,"(Default: "),Vg=o(lh,"CODE",{});var HH=l(Vg);_9=u(HH,"None"),HH.forEach(t),q9=u(lh,"). Float to define the tokens that are within the  sample` operation of text generation. Add tokens in the sample for more probable to least probable until the sum of the probabilities is greater than "),dc=o(lh,"SPAN",{class:!0});var BH=l(dc);v9=u(BH,"top_p"),BH.forEach(t),y9=u(lh,"."),lh.forEach(t),Ly.forEach(t),w9=h(H),$o=o(H,"TR",{});var zy=l($o);gc=o(zy,"TD",{align:!0});var CH=l(gc);E9=u(CH,"temperature"),CH.forEach(t),b9=h(zy),re=o(zy,"TD",{align:!0});var Pa=l(re);j9=u(Pa,"(Default: "),Xg=o(Pa,"CODE",{});var GH=l(Xg);T9=u(GH,"1.0"),GH.forEach(t),k9=u(Pa,"). Float (0.0-100.0). The temperature of the sampling operation. 1 means regular sampling, 0 means "),mc=o(Pa,"SPAN",{class:!0});var UH=l(mc);A9=u(UH,"top_k=1"),UH.forEach(t),D9=u(Pa,", "),$c=o(Pa,"SPAN",{class:!0});var LH=l($c);P9=u(LH,"100.0"),LH.forEach(t),O9=u(Pa," is getting closer to uniform probability."),Pa.forEach(t),zy.forEach(t),R9=h(H),_o=o(H,"TR",{});var My=l(_o);_c=o(My,"TD",{align:!0});var zH=l(_c);N9=u(zH,"repetition_penalty"),zH.forEach(t),S9=h(My),Rs=o(My,"TD",{align:!0});var Fy=l(Rs);x9=u(Fy,"(Default: "),Qg=o(Fy,"CODE",{});var MH=l(Qg);I9=u(MH,"None"),MH.forEach(t),H9=u(Fy,"). Float (0.0-100.0). The more a token is used within generation the more it is penalized to not be picked in successive generation passes."),Fy.forEach(t),My.forEach(t),B9=h(H),qo=o(H,"TR",{});var Jy=l(qo);qc=o(Jy,"TD",{align:!0});var FH=l(qc);C9=u(FH,"max_new_tokens"),FH.forEach(t),G9=h(Jy),we=o(Jy,"TD",{align:!0});var ih=l(we);U9=u(ih,"(Default: "),Zg=o(ih,"CODE",{});var JH=l(Zg);L9=u(JH,"None"),JH.forEach(t),z9=u(ih,"). Int (0-250). The amount of new tokens to be generated, this does "),em=o(ih,"STRONG",{});var KH=l(em);M9=u(KH,"not"),KH.forEach(t),F9=u(ih," include the input length it is a estimate of the size of generated text you want. Each new tokens slows down the request, so look for balance between response times and length of text generated."),ih.forEach(t),Jy.forEach(t),J9=h(H),vo=o(H,"TR",{});var Ky=l(vo);vc=o(Ky,"TD",{align:!0});var WH=l(vc);K9=u(WH,"max_time"),WH.forEach(t),W9=h(Ky),Ns=o(Ky,"TD",{align:!0});var Wy=l(Ns);Y9=u(Wy,"(Default: "),tm=o(Wy,"CODE",{});var YH=l(tm);V9=u(YH,'None</span></code>). Float (0-120.0). The amount of time in seconds that the query should take maximum. Network can cause some overhead so it will be a soft limit. Use that in combination with <code class="xref py py-obj docutils literal notranslate"><span class="pre">max_new_tokens'),YH.forEach(t),X9=u(Wy," for best results."),Wy.forEach(t),Ky.forEach(t),Q9=h(H),yo=o(H,"TR",{});var Yy=l(yo);yc=o(Yy,"TD",{align:!0});var VH=l(yc);Z9=u(VH,"return_full_text"),VH.forEach(t),e8=h(Yy),Ee=o(Yy,"TD",{align:!0});var uh=l(Ee);t8=u(uh,"(Default: "),sm=o(uh,"CODE",{});var XH=l(sm);s8=u(XH,"True"),XH.forEach(t),a8=u(uh,"). Bool. If set to False, the return results will "),am=o(uh,"STRONG",{});var QH=l(am);n8=u(QH,"not"),QH.forEach(t),r8=u(uh," contain the original query making it easier for prompting."),uh.forEach(t),Yy.forEach(t),o8=h(H),wo=o(H,"TR",{});var Vy=l(wo);wc=o(Vy,"TD",{align:!0});var ZH=l(wc);l8=u(ZH,"num_return_sequences"),ZH.forEach(t),i8=h(Vy),Ss=o(Vy,"TD",{align:!0});var Xy=l(Ss);u8=u(Xy,"(Default: "),nm=o(Xy,"CODE",{});var eB=l(nm);p8=u(eB,"1"),eB.forEach(t),c8=u(Xy,"). Integer. The number of proposition you want to be returned."),Xy.forEach(t),Vy.forEach(t),f8=h(H),Eo=o(H,"TR",{});var Qy=l(Eo);Ec=o(Qy,"TD",{align:!0});var tB=l(Ec);h8=u(tB,"do_sample"),tB.forEach(t),d8=h(Qy),xs=o(Qy,"TD",{align:!0});var Zy=l(xs);g8=u(Zy,"(Optional: "),rm=o(Zy,"CODE",{});var sB=l(rm);m8=u(sB,"True"),sB.forEach(t),$8=u(Zy,"). Bool. Whether or not to use sampling, use greedy decoding otherwise."),Zy.forEach(t),Qy.forEach(t),_8=h(H),bo=o(H,"TR",{});var ew=l(bo);bc=o(ew,"TD",{align:!0});var aB=l(bc);om=o(aB,"STRONG",{});var nB=l(om);q8=u(nB,"options"),nB.forEach(t),aB.forEach(t),v8=h(ew),jc=o(ew,"TD",{align:!0});var rB=l(jc);y8=u(rB,"a dict containing the following keys:"),rB.forEach(t),ew.forEach(t),w8=h(H),jo=o(H,"TR",{});var tw=l(jo);Tc=o(tw,"TD",{align:!0});var oB=l(Tc);E8=u(oB,"use_gpu"),oB.forEach(t),b8=h(tw),Is=o(tw,"TD",{align:!0});var sw=l(Is);j8=u(sw,"(Default: "),lm=o(sw,"CODE",{});var lB=l(lm);T8=u(lB,"false"),lB.forEach(t),k8=u(sw,"). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),sw.forEach(t),tw.forEach(t),A8=h(H),To=o(H,"TR",{});var aw=l(To);kc=o(aw,"TD",{align:!0});var iB=l(kc);D8=u(iB,"use_cache"),iB.forEach(t),P8=h(aw),Hs=o(aw,"TD",{align:!0});var nw=l(Hs);O8=u(nw,"(Default: "),im=o(nw,"CODE",{});var uB=l(im);R8=u(uB,"true"),uB.forEach(t),N8=u(nw,"). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),nw.forEach(t),aw.forEach(t),S8=h(H),ko=o(H,"TR",{});var rw=l(ko);Ac=o(rw,"TD",{align:!0});var pB=l(Ac);x8=u(pB,"wait_for_model"),pB.forEach(t),I8=h(rw),Bs=o(rw,"TD",{align:!0});var ow=l(Bs);H8=u(ow,"(Default: "),um=o(ow,"CODE",{});var cB=l(um);B8=u(cB,"false"),cB.forEach(t),C8=u(ow,") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),ow.forEach(t),rw.forEach(t),H.forEach(t),Hy.forEach(t),Cq=h(a),Dc=o(a,"P",{});var fB=l(Dc);G8=u(fB,"Return value is either a dict or a list of dicts if you sent a list of inputs"),fB.forEach(t),Gq=h(a),v(Cs.$$.fragment,a),Uq=h(a),Gs=o(a,"TABLE",{});var lw=l(Gs);pm=o(lw,"THEAD",{});var hB=l(pm);Ao=o(hB,"TR",{});var iw=l(Ao);Pc=o(iw,"TH",{align:!0});var dB=l(Pc);U8=u(dB,"Returned values"),dB.forEach(t),L8=h(iw),cm=o(iw,"TH",{align:!0}),l(cm).forEach(t),iw.forEach(t),hB.forEach(t),z8=h(lw),fm=o(lw,"TBODY",{});var gB=l(fm);Do=o(gB,"TR",{});var uw=l(Do);Oc=o(uw,"TD",{align:!0});var mB=l(Oc);hm=o(mB,"STRONG",{});var $B=l(hm);M8=u($B,"generated_text"),$B.forEach(t),mB.forEach(t),F8=h(uw),Rc=o(uw,"TD",{align:!0});var _B=l(Rc);J8=u(_B,"The continuated string"),_B.forEach(t),uw.forEach(t),gB.forEach(t),lw.forEach(t),Lq=h(a),Me=o(a,"H2",{class:!0});var pw=l(Me);Us=o(pw,"A",{id:!0,class:!0,href:!0});var qB=l(Us);dm=o(qB,"SPAN",{});var vB=l(dm);v(Po.$$.fragment,vB),vB.forEach(t),qB.forEach(t),K8=h(pw),gm=o(pw,"SPAN",{});var yB=l(gm);W8=u(yB,"Text2text-generation task"),yB.forEach(t),pw.forEach(t),zq=h(a),Ls=o(a,"P",{});var cw=l(Ls);Y8=u(cw,"Essentially "),Nc=o(cw,"A",{href:!0});var wB=l(Nc);V8=u(wB,"Text-generation task"),wB.forEach(t),X8=u(cw,`. But uses
Encoder-Decoder architecture, so might change in the future for more
options.`),cw.forEach(t),Mq=h(a),Fe=o(a,"H2",{class:!0});var fw=l(Fe);zs=o(fw,"A",{id:!0,class:!0,href:!0});var EB=l(zs);mm=o(EB,"SPAN",{});var bB=l(mm);v(Oo.$$.fragment,bB),bB.forEach(t),EB.forEach(t),Q8=h(fw),$m=o(fw,"SPAN",{});var jB=l($m);Z8=u(jB,"Fill mask task"),jB.forEach(t),fw.forEach(t),Fq=h(a),Sc=o(a,"P",{});var TB=l(Sc);eA=u(TB,`Tries to fill in a hole with a missing word (token to be precise).
That\u2019s the base task for BERT models.`),TB.forEach(t),Jq=h(a),v(Ms.$$.fragment,a),Kq=h(a),Ro=o(a,"P",{});var SO=l(Ro);tA=u(SO,"Available with: "),No=o(SO,"A",{href:!0,rel:!0});var kB=l(No);sA=u(kB,"\u{1F917} Transformers"),kB.forEach(t),SO.forEach(t),Wq=h(a),xc=o(a,"P",{});var AB=l(xc);aA=u(AB,"Example:"),AB.forEach(t),Yq=h(a),v(Fs.$$.fragment,a),Vq=h(a),Ic=o(a,"P",{});var DB=l(Ic);nA=u(DB,`When sending your request, you should send a JSON encoded payload. Here
are all the options`),DB.forEach(t),Xq=h(a),Js=o(a,"TABLE",{});var hw=l(Js);_m=o(hw,"THEAD",{});var PB=l(_m);So=o(PB,"TR",{});var dw=l(So);Hc=o(dw,"TH",{align:!0});var OB=l(Hc);rA=u(OB,"All parameters"),OB.forEach(t),oA=h(dw),qm=o(dw,"TH",{align:!0}),l(qm).forEach(t),dw.forEach(t),PB.forEach(t),lA=h(hw),ee=o(hw,"TBODY",{});var ke=l(ee);xo=o(ke,"TR",{});var gw=l(xo);Io=o(gw,"TD",{align:!0});var xO=l(Io);vm=o(xO,"STRONG",{});var RB=l(vm);iA=u(RB,"inputs"),RB.forEach(t),uA=u(xO," (required):"),xO.forEach(t),pA=h(gw),Bc=o(gw,"TD",{align:!0});var NB=l(Bc);cA=u(NB,"a string to be filled from, must contain the [MASK] token (check model card for exact name of the mask)"),NB.forEach(t),gw.forEach(t),fA=h(ke),Ho=o(ke,"TR",{});var mw=l(Ho);Cc=o(mw,"TD",{align:!0});var SB=l(Cc);ym=o(SB,"STRONG",{});var xB=l(ym);hA=u(xB,"options"),xB.forEach(t),SB.forEach(t),dA=h(mw),Gc=o(mw,"TD",{align:!0});var IB=l(Gc);gA=u(IB,"a dict containing the following keys:"),IB.forEach(t),mw.forEach(t),mA=h(ke),Bo=o(ke,"TR",{});var $w=l(Bo);Uc=o($w,"TD",{align:!0});var HB=l(Uc);$A=u(HB,"use_gpu"),HB.forEach(t),_A=h($w),Ks=o($w,"TD",{align:!0});var _w=l(Ks);qA=u(_w,"(Default: "),wm=o(_w,"CODE",{});var BB=l(wm);vA=u(BB,"false"),BB.forEach(t),yA=u(_w,"). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),_w.forEach(t),$w.forEach(t),wA=h(ke),Co=o(ke,"TR",{});var qw=l(Co);Lc=o(qw,"TD",{align:!0});var CB=l(Lc);EA=u(CB,"use_cache"),CB.forEach(t),bA=h(qw),Ws=o(qw,"TD",{align:!0});var vw=l(Ws);jA=u(vw,"(Default: "),Em=o(vw,"CODE",{});var GB=l(Em);TA=u(GB,"true"),GB.forEach(t),kA=u(vw,"). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),vw.forEach(t),qw.forEach(t),AA=h(ke),Go=o(ke,"TR",{});var yw=l(Go);zc=o(yw,"TD",{align:!0});var UB=l(zc);DA=u(UB,"wait_for_model"),UB.forEach(t),PA=h(yw),Ys=o(yw,"TD",{align:!0});var ww=l(Ys);OA=u(ww,"(Default: "),bm=o(ww,"CODE",{});var LB=l(bm);RA=u(LB,"false"),LB.forEach(t),NA=u(ww,") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),ww.forEach(t),yw.forEach(t),ke.forEach(t),hw.forEach(t),Qq=h(a),Mc=o(a,"P",{});var zB=l(Mc);SA=u(zB,"Return value is either a dict or a list of dicts if you sent a list of inputs"),zB.forEach(t),Zq=h(a),v(Vs.$$.fragment,a),e1=h(a),Xs=o(a,"TABLE",{});var Ew=l(Xs);jm=o(Ew,"THEAD",{});var MB=l(jm);Uo=o(MB,"TR",{});var bw=l(Uo);Fc=o(bw,"TH",{align:!0});var FB=l(Fc);xA=u(FB,"Returned values"),FB.forEach(t),IA=h(bw),Tm=o(bw,"TH",{align:!0}),l(Tm).forEach(t),bw.forEach(t),MB.forEach(t),HA=h(Ew),ue=o(Ew,"TBODY",{});var Oa=l(ue);Lo=o(Oa,"TR",{});var jw=l(Lo);Jc=o(jw,"TD",{align:!0});var JB=l(Jc);km=o(JB,"STRONG",{});var KB=l(km);BA=u(KB,"sequence"),KB.forEach(t),JB.forEach(t),CA=h(jw),Kc=o(jw,"TD",{align:!0});var WB=l(Kc);GA=u(WB,"The actual sequence of tokens that ran against the model (may contain special tokens)"),WB.forEach(t),jw.forEach(t),UA=h(Oa),zo=o(Oa,"TR",{});var Tw=l(zo);Wc=o(Tw,"TD",{align:!0});var YB=l(Wc);Am=o(YB,"STRONG",{});var VB=l(Am);LA=u(VB,"score"),VB.forEach(t),YB.forEach(t),zA=h(Tw),Yc=o(Tw,"TD",{align:!0});var XB=l(Yc);MA=u(XB,"The probability for this token."),XB.forEach(t),Tw.forEach(t),FA=h(Oa),Mo=o(Oa,"TR",{});var kw=l(Mo);Vc=o(kw,"TD",{align:!0});var QB=l(Vc);Dm=o(QB,"STRONG",{});var ZB=l(Dm);JA=u(ZB,"token"),ZB.forEach(t),QB.forEach(t),KA=h(kw),Xc=o(kw,"TD",{align:!0});var eC=l(Xc);WA=u(eC,"The id of the token"),eC.forEach(t),kw.forEach(t),YA=h(Oa),Fo=o(Oa,"TR",{});var Aw=l(Fo);Qc=o(Aw,"TD",{align:!0});var tC=l(Qc);Pm=o(tC,"STRONG",{});var sC=l(Pm);VA=u(sC,"token_str"),sC.forEach(t),tC.forEach(t),XA=h(Aw),Zc=o(Aw,"TD",{align:!0});var aC=l(Zc);QA=u(aC,"The string representation of the token"),aC.forEach(t),Aw.forEach(t),Oa.forEach(t),Ew.forEach(t),t1=h(a),Je=o(a,"H2",{class:!0});var Dw=l(Je);Qs=o(Dw,"A",{id:!0,class:!0,href:!0});var nC=l(Qs);Om=o(nC,"SPAN",{});var rC=l(Om);v(Jo.$$.fragment,rC),rC.forEach(t),nC.forEach(t),ZA=h(Dw),Rm=o(Dw,"SPAN",{});var oC=l(Rm);eD=u(oC,"Automatic speech recognition task"),oC.forEach(t),Dw.forEach(t),s1=h(a),ef=o(a,"P",{});var lC=l(ef);tD=u(lC,`This task reads some audio input and outputs the said words within the
audio files.`),lC.forEach(t),a1=h(a),v(Zs.$$.fragment,a),n1=h(a),v(ea.$$.fragment,a),r1=h(a),pe=o(a,"P",{});var zl=l(pe);sD=u(zl,"Available with: "),Ko=o(zl,"A",{href:!0,rel:!0});var iC=l(Ko);aD=u(iC,"\u{1F917} Transformers"),iC.forEach(t),nD=h(zl),Wo=o(zl,"A",{href:!0,rel:!0});var uC=l(Wo);rD=u(uC,"ESPnet"),uC.forEach(t),oD=u(zl,` and
`),Yo=o(zl,"A",{href:!0,rel:!0});var pC=l(Yo);lD=u(pC,"SpeechBrain"),pC.forEach(t),zl.forEach(t),o1=h(a),tf=o(a,"P",{});var cC=l(tf);iD=u(cC,"Request:"),cC.forEach(t),l1=h(a),v(ta.$$.fragment,a),i1=h(a),sf=o(a,"P",{});var fC=l(sf);uD=u(fC,`When sending your request, you should send a binary payload that simply
contains your audio file. We try to support most formats (Flac, Wav,
Mp3, Ogg etc...). And we automatically rescale the sampling rate to the
appropriate rate for the given model (usually 16KHz).`),fC.forEach(t),u1=h(a),sa=o(a,"TABLE",{});var Pw=l(sa);Nm=o(Pw,"THEAD",{});var hC=l(Nm);Vo=o(hC,"TR",{});var Ow=l(Vo);af=o(Ow,"TH",{align:!0});var dC=l(af);pD=u(dC,"All parameters"),dC.forEach(t),cD=h(Ow),Sm=o(Ow,"TH",{align:!0}),l(Sm).forEach(t),Ow.forEach(t),hC.forEach(t),fD=h(Pw),xm=o(Pw,"TBODY",{});var gC=l(xm);Xo=o(gC,"TR",{});var Rw=l(Xo);Qo=o(Rw,"TD",{align:!0});var IO=l(Qo);Im=o(IO,"STRONG",{});var mC=l(Im);hD=u(mC,"no parameter"),mC.forEach(t),dD=u(IO," (required)"),IO.forEach(t),gD=h(Rw),nf=o(Rw,"TD",{align:!0});var $C=l(nf);mD=u($C,"a binary representation of the audio file. No other parameters are currently allowed."),$C.forEach(t),Rw.forEach(t),gC.forEach(t),Pw.forEach(t),p1=h(a),rf=o(a,"P",{});var _C=l(rf);$D=u(_C,"Return value is either a dict or a list of dicts if you sent a list of inputs"),_C.forEach(t),c1=h(a),of=o(a,"P",{});var qC=l(of);_D=u(qC,"Response:"),qC.forEach(t),f1=h(a),v(aa.$$.fragment,a),h1=h(a),na=o(a,"TABLE",{});var Nw=l(na);Hm=o(Nw,"THEAD",{});var vC=l(Hm);Zo=o(vC,"TR",{});var Sw=l(Zo);lf=o(Sw,"TH",{align:!0});var yC=l(lf);qD=u(yC,"Returned values"),yC.forEach(t),vD=h(Sw),Bm=o(Sw,"TH",{align:!0}),l(Bm).forEach(t),Sw.forEach(t),vC.forEach(t),yD=h(Nw),Cm=o(Nw,"TBODY",{});var wC=l(Cm);el=o(wC,"TR",{});var xw=l(el);uf=o(xw,"TD",{align:!0});var EC=l(uf);Gm=o(EC,"STRONG",{});var bC=l(Gm);wD=u(bC,"text"),bC.forEach(t),EC.forEach(t),ED=h(xw),pf=o(xw,"TD",{align:!0});var jC=l(pf);bD=u(jC,"The string that was recognized within the audio file."),jC.forEach(t),xw.forEach(t),wC.forEach(t),Nw.forEach(t),d1=h(a),Ke=o(a,"H2",{class:!0});var Iw=l(Ke);ra=o(Iw,"A",{id:!0,class:!0,href:!0});var TC=l(ra);Um=o(TC,"SPAN",{});var kC=l(Um);v(tl.$$.fragment,kC),kC.forEach(t),TC.forEach(t),jD=h(Iw),Lm=o(Iw,"SPAN",{});var AC=l(Lm);TD=u(AC,"Feature-extraction task"),AC.forEach(t),Iw.forEach(t),g1=h(a),cf=o(a,"P",{});var DC=l(cf);kD=u(DC,`This task reads some text and outputs raw float values, that usually
consumed as part of a semantic database/semantic search.`),DC.forEach(t),m1=h(a),v(oa.$$.fragment,a),$1=h(a),We=o(a,"P",{});var K$=l(We);AD=u(K$,"Available with: "),sl=o(K$,"A",{href:!0,rel:!0});var PC=l(sl);DD=u(PC,"\u{1F917} Transformers"),PC.forEach(t),PD=h(K$),al=o(K$,"A",{href:!0,rel:!0});var OC=l(al);OD=u(OC,"Sentence-transformers"),OC.forEach(t),K$.forEach(t),_1=h(a),ff=o(a,"P",{});var RC=l(ff);RD=u(RC,"Request:"),RC.forEach(t),q1=h(a),la=o(a,"TABLE",{});var Hw=l(la);zm=o(Hw,"THEAD",{});var NC=l(zm);nl=o(NC,"TR",{});var Bw=l(nl);hf=o(Bw,"TH",{align:!0});var SC=l(hf);ND=u(SC,"All parameters"),SC.forEach(t),SD=h(Bw),Mm=o(Bw,"TH",{align:!0}),l(Mm).forEach(t),Bw.forEach(t),NC.forEach(t),xD=h(Hw),te=o(Hw,"TBODY",{});var Ae=l(te);rl=o(Ae,"TR",{});var Cw=l(rl);ol=o(Cw,"TD",{align:!0});var HO=l(ol);Fm=o(HO,"STRONG",{});var xC=l(Fm);ID=u(xC,"inputs"),xC.forEach(t),HD=u(HO," (required):"),HO.forEach(t),BD=h(Cw),df=o(Cw,"TD",{align:!0});var IC=l(df);CD=u(IC,"a string or a list of strings to get the features from."),IC.forEach(t),Cw.forEach(t),GD=h(Ae),ll=o(Ae,"TR",{});var Gw=l(ll);gf=o(Gw,"TD",{align:!0});var HC=l(gf);Jm=o(HC,"STRONG",{});var BC=l(Jm);UD=u(BC,"options"),BC.forEach(t),HC.forEach(t),LD=h(Gw),mf=o(Gw,"TD",{align:!0});var CC=l(mf);zD=u(CC,"a dict containing the following keys:"),CC.forEach(t),Gw.forEach(t),MD=h(Ae),il=o(Ae,"TR",{});var Uw=l(il);$f=o(Uw,"TD",{align:!0});var GC=l($f);FD=u(GC,"use_gpu"),GC.forEach(t),JD=h(Uw),ia=o(Uw,"TD",{align:!0});var Lw=l(ia);KD=u(Lw,"(Default: "),Km=o(Lw,"CODE",{});var UC=l(Km);WD=u(UC,"false"),UC.forEach(t),YD=u(Lw,"). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),Lw.forEach(t),Uw.forEach(t),VD=h(Ae),ul=o(Ae,"TR",{});var zw=l(ul);_f=o(zw,"TD",{align:!0});var LC=l(_f);XD=u(LC,"use_cache"),LC.forEach(t),QD=h(zw),ua=o(zw,"TD",{align:!0});var Mw=l(ua);ZD=u(Mw,"(Default: "),Wm=o(Mw,"CODE",{});var zC=l(Wm);eP=u(zC,"true"),zC.forEach(t),tP=u(Mw,"). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),Mw.forEach(t),zw.forEach(t),sP=h(Ae),pl=o(Ae,"TR",{});var Fw=l(pl);qf=o(Fw,"TD",{align:!0});var MC=l(qf);aP=u(MC,"wait_for_model"),MC.forEach(t),nP=h(Fw),pa=o(Fw,"TD",{align:!0});var Jw=l(pa);rP=u(Jw,"(Default: "),Ym=o(Jw,"CODE",{});var FC=l(Ym);oP=u(FC,"false"),FC.forEach(t),lP=u(Jw,") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),Jw.forEach(t),Fw.forEach(t),Ae.forEach(t),Hw.forEach(t),v1=h(a),vf=o(a,"P",{});var JC=l(vf);iP=u(JC,"Return value is either a dict or a list of dicts if you sent a list of inputs"),JC.forEach(t),y1=h(a),ca=o(a,"TABLE",{});var Kw=l(ca);Vm=o(Kw,"THEAD",{});var KC=l(Vm);cl=o(KC,"TR",{});var Ww=l(cl);yf=o(Ww,"TH",{align:!0});var WC=l(yf);uP=u(WC,"Returned values"),WC.forEach(t),pP=h(Ww),Xm=o(Ww,"TH",{align:!0}),l(Xm).forEach(t),Ww.forEach(t),KC.forEach(t),cP=h(Kw),Qm=o(Kw,"TBODY",{});var YC=l(Qm);fl=o(YC,"TR",{});var Yw=l(fl);wf=o(Yw,"TD",{align:!0});var VC=l(wf);Zm=o(VC,"STRONG",{});var XC=l(Zm);fP=u(XC,"A list of float (or list of list of floats)"),XC.forEach(t),VC.forEach(t),hP=h(Yw),Ef=o(Yw,"TD",{align:!0});var QC=l(Ef);dP=u(QC,"The numbers that are the representation features of the input."),QC.forEach(t),Yw.forEach(t),YC.forEach(t),Kw.forEach(t),w1=h(a),bf=o(a,"SMALL",{});var ZC=l(bf);gP=u(ZC,`Returned values are a list of floats, or a list of list of floats
(depending on if you sent a string or a list of string, and if the
automatic reduction, usually mean_pooling for instance was applied for
you or not. This should be explained on the model's README.`),ZC.forEach(t),E1=h(a),Ye=o(a,"H2",{class:!0});var Vw=l(Ye);fa=o(Vw,"A",{id:!0,class:!0,href:!0});var eG=l(fa);e$=o(eG,"SPAN",{});var tG=l(e$);v(hl.$$.fragment,tG),tG.forEach(t),eG.forEach(t),mP=h(Vw),t$=o(Vw,"SPAN",{});var sG=l(t$);$P=u(sG,"Audio-classification task"),sG.forEach(t),Vw.forEach(t),b1=h(a),jf=o(a,"P",{});var aG=l(jf);_P=u(aG,"This task reads some audio input and outputs the likelihood of classes."),aG.forEach(t),j1=h(a),v(ha.$$.fragment,a),T1=h(a),Ve=o(a,"P",{});var W$=l(Ve);qP=u(W$,"Available with: "),dl=o(W$,"A",{href:!0,rel:!0});var nG=l(dl);vP=u(nG,"\u{1F917} Transformers"),nG.forEach(t),yP=h(W$),gl=o(W$,"A",{href:!0,rel:!0});var rG=l(gl);wP=u(rG,"SpeechBrain"),rG.forEach(t),W$.forEach(t),k1=h(a),Tf=o(a,"P",{});var oG=l(Tf);EP=u(oG,"Request:"),oG.forEach(t),A1=h(a),v(da.$$.fragment,a),D1=h(a),kf=o(a,"P",{});var lG=l(kf);bP=u(lG,`When sending your request, you should send a binary payload that simply
contains your audio file. We try to support most formats (Flac, Wav,
Mp3, Ogg etc...). And we automatically rescale the sampling rate to the
appropriate rate for the given model (usually 16KHz).`),lG.forEach(t),P1=h(a),ga=o(a,"TABLE",{});var Xw=l(ga);s$=o(Xw,"THEAD",{});var iG=l(s$);ml=o(iG,"TR",{});var Qw=l(ml);Af=o(Qw,"TH",{align:!0});var uG=l(Af);jP=u(uG,"All parameters"),uG.forEach(t),TP=h(Qw),a$=o(Qw,"TH",{align:!0}),l(a$).forEach(t),Qw.forEach(t),iG.forEach(t),kP=h(Xw),n$=o(Xw,"TBODY",{});var pG=l(n$);$l=o(pG,"TR",{});var Zw=l($l);_l=o(Zw,"TD",{align:!0});var BO=l(_l);r$=o(BO,"STRONG",{});var cG=l(r$);AP=u(cG,"no parameter"),cG.forEach(t),DP=u(BO," (required)"),BO.forEach(t),PP=h(Zw),Df=o(Zw,"TD",{align:!0});var fG=l(Df);OP=u(fG,"a binary representation of the audio file. No other parameters are currently allowed."),fG.forEach(t),Zw.forEach(t),pG.forEach(t),Xw.forEach(t),O1=h(a),Pf=o(a,"P",{});var hG=l(Pf);RP=u(hG,"Return value is a dict"),hG.forEach(t),R1=h(a),v(ma.$$.fragment,a),N1=h(a),$a=o(a,"TABLE",{});var eE=l($a);o$=o(eE,"THEAD",{});var dG=l(o$);ql=o(dG,"TR",{});var tE=l(ql);Of=o(tE,"TH",{align:!0});var gG=l(Of);NP=u(gG,"Returned values"),gG.forEach(t),SP=h(tE),l$=o(tE,"TH",{align:!0}),l(l$).forEach(t),tE.forEach(t),dG.forEach(t),xP=h(eE),vl=o(eE,"TBODY",{});var sE=l(vl);yl=o(sE,"TR",{});var aE=l(yl);Rf=o(aE,"TD",{align:!0});var mG=l(Rf);i$=o(mG,"STRONG",{});var $G=l(i$);IP=u($G,"label"),$G.forEach(t),mG.forEach(t),HP=h(aE),Nf=o(aE,"TD",{align:!0});var _G=l(Nf);BP=u(_G,"The label for the class (model specific)"),_G.forEach(t),aE.forEach(t),CP=h(sE),wl=o(sE,"TR",{});var nE=l(wl);Sf=o(nE,"TD",{align:!0});var qG=l(Sf);u$=o(qG,"STRONG",{});var vG=l(u$);GP=u(vG,"score"),vG.forEach(t),qG.forEach(t),UP=h(nE),xf=o(nE,"TD",{align:!0});var yG=l(xf);LP=u(yG,"A floats that represents how likely is that the audio file belongs the this class."),yG.forEach(t),nE.forEach(t),sE.forEach(t),eE.forEach(t),S1=h(a),Xe=o(a,"H2",{class:!0});var rE=l(Xe);_a=o(rE,"A",{id:!0,class:!0,href:!0});var wG=l(_a);p$=o(wG,"SPAN",{});var EG=l(p$);v(El.$$.fragment,EG),EG.forEach(t),wG.forEach(t),zP=h(rE),c$=o(rE,"SPAN",{});var bG=l(c$);MP=u(bG,"Object-detection task"),bG.forEach(t),rE.forEach(t),x1=h(a),If=o(a,"P",{});var jG=l(If);FP=u(jG,`This task reads some image input and outputs the likelihood of classes &
bounding boxes of detected objects.`),jG.forEach(t),I1=h(a),v(qa.$$.fragment,a),H1=h(a),bl=o(a,"P",{});var CO=l(bl);JP=u(CO,"Available with: "),jl=o(CO,"A",{href:!0,rel:!0});var TG=l(jl);KP=u(TG,"\u{1F917} Transformers"),TG.forEach(t),CO.forEach(t),B1=h(a),Hf=o(a,"P",{});var kG=l(Hf);WP=u(kG,"Request:"),kG.forEach(t),C1=h(a),v(va.$$.fragment,a),G1=h(a),ya=o(a,"P",{});var oE=l(ya);YP=u(oE,`When sending your request, you should send a binary payload that simply
contains your image file. We support all image formats `),Tl=o(oE,"A",{href:!0,rel:!0});var AG=l(Tl);VP=u(AG,`Pillow
supports`),AG.forEach(t),XP=u(oE,"."),oE.forEach(t),U1=h(a),wa=o(a,"TABLE",{});var lE=l(wa);f$=o(lE,"THEAD",{});var DG=l(f$);kl=o(DG,"TR",{});var iE=l(kl);Bf=o(iE,"TH",{align:!0});var PG=l(Bf);QP=u(PG,"All parameters"),PG.forEach(t),ZP=h(iE),h$=o(iE,"TH",{align:!0}),l(h$).forEach(t),iE.forEach(t),DG.forEach(t),eO=h(lE),d$=o(lE,"TBODY",{});var OG=l(d$);Al=o(OG,"TR",{});var uE=l(Al);Dl=o(uE,"TD",{align:!0});var GO=l(Dl);g$=o(GO,"STRONG",{});var RG=l(g$);tO=u(RG,"no parameter"),RG.forEach(t),sO=u(GO," (required)"),GO.forEach(t),aO=h(uE),Cf=o(uE,"TD",{align:!0});var NG=l(Cf);nO=u(NG,"a binary representation of the image file. No other parameters are currently allowed."),NG.forEach(t),uE.forEach(t),OG.forEach(t),lE.forEach(t),L1=h(a),Gf=o(a,"P",{});var SG=l(Gf);rO=u(SG,"Return value is a dict"),SG.forEach(t),z1=h(a),v(Ea.$$.fragment,a),M1=h(a),ba=o(a,"TABLE",{});var pE=l(ba);m$=o(pE,"THEAD",{});var xG=l(m$);Pl=o(xG,"TR",{});var cE=l(Pl);Uf=o(cE,"TH",{align:!0});var IG=l(Uf);oO=u(IG,"Returned values"),IG.forEach(t),lO=h(cE),$$=o(cE,"TH",{align:!0}),l($$).forEach(t),cE.forEach(t),xG.forEach(t),iO=h(pE),Qe=o(pE,"TBODY",{});var ph=l(Qe);Ol=o(ph,"TR",{});var fE=l(Ol);Lf=o(fE,"TD",{align:!0});var HG=l(Lf);_$=o(HG,"STRONG",{});var BG=l(_$);uO=u(BG,"label"),BG.forEach(t),HG.forEach(t),pO=h(fE),zf=o(fE,"TD",{align:!0});var CG=l(zf);cO=u(CG,"The label for the class (model specific) of a detected object."),CG.forEach(t),fE.forEach(t),fO=h(ph),Rl=o(ph,"TR",{});var hE=l(Rl);Mf=o(hE,"TD",{align:!0});var GG=l(Mf);q$=o(GG,"STRONG",{});var UG=l(q$);hO=u(UG,"score"),UG.forEach(t),GG.forEach(t),dO=h(hE),Ff=o(hE,"TD",{align:!0});var LG=l(Ff);gO=u(LG,"A float that represents how likely it is that the detected object belongs to the given class."),LG.forEach(t),hE.forEach(t),mO=h(ph),Nl=o(ph,"TR",{});var dE=l(Nl);Jf=o(dE,"TD",{align:!0});var zG=l(Jf);v$=o(zG,"STRONG",{});var MG=l(v$);$O=u(MG,"box"),MG.forEach(t),zG.forEach(t),_O=h(dE),Kf=o(dE,"TD",{align:!0});var FG=l(Kf);qO=u(FG,"A dict (with keys [xmin,ymin,xmax,ymax]) representing the bounding box of a detected object."),FG.forEach(t),dE.forEach(t),ph.forEach(t),pE.forEach(t),this.h()},h(){c(n,"name","hf:doc:metadata"),c(n,"content",JSON.stringify(nz)),c(d,"id","detailed-parameters"),c(d,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(d,"href","#detailed-parameters"),c(s,"class","relative group"),c(se,"id","which-task-is-used-by-this-model"),c(se,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(se,"href","#which-task-is-used-by-this-model"),c(D,"class","relative group"),c(tt,"class","block dark:hidden"),JG(tt.src,UO="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/inference-api/task.png")||c(tt,"src",UO),c(tt,"width","300"),c(st,"class","hidden dark:block invert"),JG(st.src,LO="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/inference-api/task-dark.png")||c(st,"src",LO),c(st,"width","300"),c(at,"id","zeroshot-classification-task"),c(at,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(at,"href","#zeroshot-classification-task"),c(Oe,"class","relative group"),c(xa,"href","https://github.com/huggingface/transformers"),c(xa,"rel","nofollow"),c(Yl,"align","left"),c(dh,"align","left"),c(Ba,"align","left"),c(Vl,"align","left"),c(Ga,"align","left"),c(Xl,"align","left"),c(Ql,"align","left"),c(Zl,"class","incremental"),c(ce,"align","left"),c(ei,"align","left"),c(lt,"align","left"),c(ti,"align","left"),c(si,"align","left"),c(ai,"align","left"),c(it,"align","left"),c(ni,"align","left"),c(ut,"align","left"),c(ri,"align","left"),c(pt,"align","left"),c(ii,"align","left"),c(bh,"align","left"),c(ui,"align","left"),c(pi,"align","left"),c(ci,"align","left"),c(fi,"align","left"),c(hi,"align","left"),c(ht,"align","left"),c(dt,"id","translation-task"),c(dt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(dt,"href","#translation-task"),c(Ne,"class","relative group"),c(Za,"href","https://github.com/huggingface/transformers"),c(Za,"rel","nofollow"),c($i,"align","left"),c(Rh,"align","left"),c(sn,"align","left"),c(_i,"align","left"),c(qi,"align","left"),c(vi,"align","left"),c(yi,"align","left"),c(_t,"align","left"),c(wi,"align","left"),c(qt,"align","left"),c(Ei,"align","left"),c(vt,"align","left"),c(ji,"align","left"),c(Ch,"align","left"),c(Ti,"align","left"),c(ki,"align","left"),c(wt,"id","summarization-task"),c(wt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(wt,"href","#summarization-task"),c(Se,"class","relative group"),c(Ai,"href","mailto:api-enterprise@huggingface.co"),c(fn,"href","https://github.com/huggingface/transformers"),c(fn,"rel","nofollow"),c(Oi,"align","left"),c(Fh,"align","left"),c(gn,"align","left"),c(Ri,"align","left"),c(Ni,"align","left"),c(Si,"align","left"),c(xi,"align","left"),c(fe,"align","left"),c(Ii,"align","left"),c(he,"align","left"),c(Hi,"align","left"),c(Bi,"class","incremental"),c(de,"align","left"),c(Ci,"align","left"),c(Gi,"class","incremental"),c(ge,"align","left"),c(Ui,"align","left"),c(Li,"class","incremental"),c(zi,"class","incremental"),c(ae,"align","left"),c(Mi,"align","left"),c(kt,"align","left"),c(Fi,"align","left"),c(At,"align","left"),c(Ji,"align","left"),c(Ki,"align","left"),c(Wi,"align","left"),c(Dt,"align","left"),c(Yi,"align","left"),c(Pt,"align","left"),c(Vi,"align","left"),c(Ot,"align","left"),c(Qi,"align","left"),c(id,"align","left"),c(Zi,"align","left"),c(eu,"align","left"),c(Nt,"id","conversational-task"),c(Nt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Nt,"href","#conversational-task"),c(xe,"class","relative group"),c(Rn,"href","https://github.com/huggingface/transformers"),c(Rn,"rel","nofollow"),c(nu,"align","left"),c(dd,"align","left"),c(xn,"align","left"),c(md,"align","left"),c(ru,"align","left"),c(ou,"align","left"),c(lu,"align","left"),c(iu,"align","left"),c(uu,"align","left"),c(Ht,"align","left"),c(pu,"align","left"),c(cu,"align","left"),c(fu,"align","left"),c(me,"align","left"),c(hu,"align","left"),c($e,"align","left"),c(du,"align","left"),c(gu,"class","incremental"),c(_e,"align","left"),c(mu,"align","left"),c($u,"class","incremental"),c(qe,"align","left"),c(_u,"align","left"),c(qu,"class","incremental"),c(vu,"class","incremental"),c(ne,"align","left"),c(yu,"align","left"),c(Bt,"align","left"),c(wu,"align","left"),c(Ct,"align","left"),c(Eu,"align","left"),c(bu,"align","left"),c(ju,"align","left"),c(Gt,"align","left"),c(Tu,"align","left"),c(Ut,"align","left"),c(ku,"align","left"),c(Lt,"align","left"),c(Du,"align","left"),c(Nd,"align","left"),c(Pu,"align","left"),c(Ou,"align","left"),c(Ru,"align","left"),c(Nu,"align","left"),c(Su,"align","left"),c(xu,"align","left"),c(Iu,"align","left"),c(Hu,"align","left"),c(Mt,"id","table-question-answering-task"),c(Mt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Mt,"href","#table-question-answering-task"),c(Ie,"class","relative group"),c(nr,"href","https://github.com/huggingface/transformers"),c(nr,"rel","nofollow"),c(Uu,"align","left"),c(Cd,"align","left"),c(lr,"align","left"),c(Ud,"align","left"),c(Lu,"align","left"),c(zu,"align","left"),c(Mu,"align","left"),c(Fu,"align","left"),c(Ju,"align","left"),c(Ku,"align","left"),c(Wu,"align","left"),c(Wt,"align","left"),c(Yu,"align","left"),c(Yt,"align","left"),c(Vu,"align","left"),c(Vt,"align","left"),c(Qu,"align","left"),c(Kd,"align","left"),c(Zu,"align","left"),c(ep,"align","left"),c(tp,"align","left"),c(sp,"align","left"),c(ap,"align","left"),c(np,"align","left"),c(rp,"align","left"),c(op,"align","left"),c(Zt,"id","question-answering-task"),c(Zt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Zt,"href","#question-answering-task"),c(He,"class","relative group"),c(vr,"href","https://github.com/huggingface/transformers"),c(vr,"rel","nofollow"),c(yr,"href","https://github.com/allenai/allennlp"),c(yr,"rel","nofollow"),c(cp,"align","left"),c(tg,"align","left"),c(fp,"align","left"),c(hp,"align","left"),c(dp,"align","left"),c(gp,"align","left"),c(mp,"align","left"),c(ns,"align","left"),c($p,"align","left"),c(rs,"align","left"),c(os,"id","textclassification-task"),c(os,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(os,"href","#textclassification-task"),c(Ce,"class","relative group"),c(Dr,"href","https://github.com/huggingface/transformers"),c(Dr,"rel","nofollow"),c(yp,"align","left"),c(cg,"align","left"),c(Rr,"align","left"),c(wp,"align","left"),c(Ep,"align","left"),c(bp,"align","left"),c(jp,"align","left"),c(ps,"align","left"),c(Tp,"align","left"),c(cs,"align","left"),c(kp,"align","left"),c(fs,"align","left"),c(Dp,"align","left"),c(_g,"align","left"),c(Pp,"align","left"),c(Op,"align","left"),c(Rp,"align","left"),c(Np,"align","left"),c(gs,"id","named-entity-recognition-ner-task"),c(gs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(gs,"href","#named-entity-recognition-ner-task"),c(Ge,"class","relative group"),c(Sp,"href","#token-classification-task"),c(ms,"id","tokenclassification-task"),c(ms,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(ms,"href","#tokenclassification-task"),c(Ue,"class","relative group"),c(Mr,"href","https://github.com/huggingface/transformers"),c(Mr,"rel","nofollow"),c(Fr,"href","https://github.com/flairNLP/flair"),c(Fr,"rel","nofollow"),c(Bp,"align","left"),c(Tg,"align","left"),c(Wr,"align","left"),c(Cp,"align","left"),c(Gp,"align","left"),c(Up,"align","left"),c(Lp,"align","left"),c(vs,"align","left"),c(zp,"align","left"),c(Mp,"align","left"),c(Fp,"align","left"),c(ys,"align","left"),c(Jp,"align","left"),c(ws,"align","left"),c(Kp,"align","left"),c(Es,"align","left"),c(Yp,"align","left"),c(xg,"align","left"),c(Vp,"align","left"),c(Xp,"align","left"),c(Qp,"align","left"),c(Zp,"align","left"),c(ec,"align","left"),c(tc,"align","left"),c(sc,"align","left"),c(Ts,"align","left"),c(ac,"align","left"),c(ks,"align","left"),c(As,"id","textgeneration-task"),c(As,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(As,"href","#textgeneration-task"),c(ze,"class","relative group"),c(uo,"href","https://github.com/huggingface/transformers"),c(uo,"rel","nofollow"),c(lc,"align","left"),c(Jg,"align","left"),c(fo,"align","left"),c(ic,"align","left"),c(uc,"align","left"),c(pc,"align","left"),c(cc,"align","left"),c(fc,"class","incremental"),c(ve,"align","left"),c(hc,"align","left"),c(dc,"class","incremental"),c(ye,"align","left"),c(gc,"align","left"),c(mc,"class","incremental"),c($c,"class","incremental"),c(re,"align","left"),c(_c,"align","left"),c(Rs,"align","left"),c(qc,"align","left"),c(we,"align","left"),c(vc,"align","left"),c(Ns,"align","left"),c(yc,"align","left"),c(Ee,"align","left"),c(wc,"align","left"),c(Ss,"align","left"),c(Ec,"align","left"),c(xs,"align","left"),c(bc,"align","left"),c(jc,"align","left"),c(Tc,"align","left"),c(Is,"align","left"),c(kc,"align","left"),c(Hs,"align","left"),c(Ac,"align","left"),c(Bs,"align","left"),c(Pc,"align","left"),c(cm,"align","left"),c(Oc,"align","left"),c(Rc,"align","left"),c(Us,"id","text2textgeneration-task"),c(Us,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Us,"href","#text2textgeneration-task"),c(Me,"class","relative group"),c(Nc,"href","#text-generation-task"),c(zs,"id","fill-mask-task"),c(zs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(zs,"href","#fill-mask-task"),c(Fe,"class","relative group"),c(No,"href","https://github.com/huggingface/transformers"),c(No,"rel","nofollow"),c(Hc,"align","left"),c(qm,"align","left"),c(Io,"align","left"),c(Bc,"align","left"),c(Cc,"align","left"),c(Gc,"align","left"),c(Uc,"align","left"),c(Ks,"align","left"),c(Lc,"align","left"),c(Ws,"align","left"),c(zc,"align","left"),c(Ys,"align","left"),c(Fc,"align","left"),c(Tm,"align","left"),c(Jc,"align","left"),c(Kc,"align","left"),c(Wc,"align","left"),c(Yc,"align","left"),c(Vc,"align","left"),c(Xc,"align","left"),c(Qc,"align","left"),c(Zc,"align","left"),c(Qs,"id","automatic-speech-recognition-task"),c(Qs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Qs,"href","#automatic-speech-recognition-task"),c(Je,"class","relative group"),c(Ko,"href","https://github.com/huggingface/transformers"),c(Ko,"rel","nofollow"),c(Wo,"href","https://github.com/espnet/espnet"),c(Wo,"rel","nofollow"),c(Yo,"href","https://github.com/speechbrain/speechbrain"),c(Yo,"rel","nofollow"),c(af,"align","left"),c(Sm,"align","left"),c(Qo,"align","left"),c(nf,"align","left"),c(lf,"align","left"),c(Bm,"align","left"),c(uf,"align","left"),c(pf,"align","left"),c(ra,"id","featureextraction-task"),c(ra,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(ra,"href","#featureextraction-task"),c(Ke,"class","relative group"),c(sl,"href","https://github.com/huggingface/transformers"),c(sl,"rel","nofollow"),c(al,"href","https://github.com/UKPLab/sentence-transformers"),c(al,"rel","nofollow"),c(hf,"align","left"),c(Mm,"align","left"),c(ol,"align","left"),c(df,"align","left"),c(gf,"align","left"),c(mf,"align","left"),c($f,"align","left"),c(ia,"align","left"),c(_f,"align","left"),c(ua,"align","left"),c(qf,"align","left"),c(pa,"align","left"),c(yf,"align","left"),c(Xm,"align","left"),c(wf,"align","left"),c(Ef,"align","left"),c(fa,"id","audioclassification-task"),c(fa,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(fa,"href","#audioclassification-task"),c(Ye,"class","relative group"),c(dl,"href","https://github.com/huggingface/transformers"),c(dl,"rel","nofollow"),c(gl,"href","https://github.com/speechbrain/speechbrain"),c(gl,"rel","nofollow"),c(Af,"align","left"),c(a$,"align","left"),c(_l,"align","left"),c(Df,"align","left"),c(Of,"align","left"),c(l$,"align","left"),c(Rf,"align","left"),c(Nf,"align","left"),c(Sf,"align","left"),c(xf,"align","left"),c(_a,"id","objectdetection-task"),c(_a,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(_a,"href","#objectdetection-task"),c(Xe,"class","relative group"),c(jl,"href","https://github.com/huggingface/transformers"),c(jl,"rel","nofollow"),c(Tl,"href","https://pillow.readthedocs.io/en/stable/handbook/image-file-formats.html"),c(Tl,"rel","nofollow"),c(Bf,"align","left"),c(h$,"align","left"),c(Dl,"align","left"),c(Cf,"align","left"),c(Uf,"align","left"),c($$,"align","left"),c(Lf,"align","left"),c(zf,"align","left"),c(Mf,"align","left"),c(Ff,"align","left"),c(Jf,"align","left"),c(Kf,"align","left")},m(a,g){e(document.head,n),m(a,p,g),m(a,s,g),e(s,d),e(d,$),y(k,$,null),e(s,A),e(s,T),e(T,j),m(a,P,g),m(a,D,g),e(D,se),e(se,De),y(V,De,null),e(D,J),e(D,et),e(et,Ml),m(a,Ra,g),m(a,Pe,g),e(Pe,gE),m(a,Y$,g),m(a,Fl,g),e(Fl,mE),m(a,V$,g),m(a,tt,g),m(a,X$,g),m(a,st,g),m(a,Q$,g),m(a,Oe,g),e(Oe,at),e(at,ch),y(Na,ch,null),e(Oe,$E),e(Oe,fh),e(fh,_E),m(a,Z$,g),m(a,Jl,g),e(Jl,qE),m(a,e_,g),y(nt,a,g),m(a,t_,g),m(a,Sa,g),e(Sa,vE),e(Sa,xa),e(xa,yE),m(a,s_,g),m(a,Kl,g),e(Kl,wE),m(a,a_,g),y(rt,a,g),m(a,n_,g),m(a,Wl,g),e(Wl,EE),m(a,r_,g),m(a,ot,g),e(ot,hh),e(hh,Ia),e(Ia,Yl),e(Yl,bE),e(Ia,jE),e(Ia,dh),e(ot,TE),e(ot,U),e(U,Ha),e(Ha,Ba),e(Ba,gh),e(gh,kE),e(Ba,AE),e(Ha,DE),e(Ha,Vl),e(Vl,PE),e(U,OE),e(U,Ca),e(Ca,Ga),e(Ga,mh),e(mh,RE),e(Ga,NE),e(Ca,SE),e(Ca,Xl),e(Xl,xE),e(U,IE),e(U,Ua),e(Ua,Ql),e(Ql,HE),e(Ua,BE),e(Ua,ce),e(ce,CE),e(ce,$h),e($h,GE),e(ce,UE),e(ce,Zl),e(Zl,LE),e(ce,zE),e(U,ME),e(U,La),e(La,ei),e(ei,FE),e(La,JE),e(La,lt),e(lt,KE),e(lt,_h),e(_h,WE),e(lt,YE),e(U,VE),e(U,za),e(za,ti),e(ti,qh),e(qh,XE),e(za,QE),e(za,si),e(si,ZE),e(U,eb),e(U,Ma),e(Ma,ai),e(ai,tb),e(Ma,sb),e(Ma,it),e(it,ab),e(it,vh),e(vh,nb),e(it,rb),e(U,ob),e(U,Fa),e(Fa,ni),e(ni,lb),e(Fa,ib),e(Fa,ut),e(ut,ub),e(ut,yh),e(yh,pb),e(ut,cb),e(U,fb),e(U,Ja),e(Ja,ri),e(ri,hb),e(Ja,db),e(Ja,pt),e(pt,gb),e(pt,wh),e(wh,mb),e(pt,$b),m(a,o_,g),m(a,oi,g),e(oi,_b),m(a,l_,g),m(a,li,g),e(li,qb),m(a,i_,g),y(ct,a,g),m(a,u_,g),m(a,ft,g),e(ft,Eh),e(Eh,Ka),e(Ka,ii),e(ii,vb),e(Ka,yb),e(Ka,bh),e(ft,wb),e(ft,Re),e(Re,Wa),e(Wa,ui),e(ui,jh),e(jh,Eb),e(Wa,bb),e(Wa,pi),e(pi,jb),e(Re,Tb),e(Re,Ya),e(Ya,ci),e(ci,Th),e(Th,kb),e(Ya,Ab),e(Ya,fi),e(fi,Db),e(Re,Pb),e(Re,Va),e(Va,hi),e(hi,kh),e(kh,Ob),e(Va,Rb),e(Va,ht),e(ht,Nb),e(ht,Ah),e(Ah,Sb),e(ht,xb),m(a,p_,g),m(a,Ne,g),e(Ne,dt),e(dt,Dh),y(Xa,Dh,null),e(Ne,Ib),e(Ne,Ph),e(Ph,Hb),m(a,c_,g),m(a,di,g),e(di,Bb),m(a,f_,g),y(gt,a,g),m(a,h_,g),m(a,Qa,g),e(Qa,Cb),e(Qa,Za),e(Za,Gb),m(a,d_,g),m(a,gi,g),e(gi,Ub),m(a,g_,g),y(mt,a,g),m(a,m_,g),m(a,mi,g),e(mi,Lb),m(a,$_,g),m(a,$t,g),e($t,Oh),e(Oh,en),e(en,$i),e($i,zb),e(en,Mb),e(en,Rh),e($t,Fb),e($t,X),e(X,tn),e(tn,sn),e(sn,Nh),e(Nh,Jb),e(sn,Kb),e(tn,Wb),e(tn,_i),e(_i,Yb),e(X,Vb),e(X,an),e(an,qi),e(qi,Sh),e(Sh,Xb),e(an,Qb),e(an,vi),e(vi,Zb),e(X,ej),e(X,nn),e(nn,yi),e(yi,tj),e(nn,sj),e(nn,_t),e(_t,aj),e(_t,xh),e(xh,nj),e(_t,rj),e(X,oj),e(X,rn),e(rn,wi),e(wi,lj),e(rn,ij),e(rn,qt),e(qt,uj),e(qt,Ih),e(Ih,pj),e(qt,cj),e(X,fj),e(X,on),e(on,Ei),e(Ei,hj),e(on,dj),e(on,vt),e(vt,gj),e(vt,Hh),e(Hh,mj),e(vt,$j),m(a,__,g),m(a,bi,g),e(bi,_j),m(a,q_,g),m(a,yt,g),e(yt,Bh),e(Bh,ln),e(ln,ji),e(ji,qj),e(ln,vj),e(ln,Ch),e(yt,yj),e(yt,Gh),e(Gh,un),e(un,Ti),e(Ti,Uh),e(Uh,wj),e(un,Ej),e(un,ki),e(ki,bj),m(a,v_,g),m(a,Se,g),e(Se,wt),e(wt,Lh),y(pn,Lh,null),e(Se,jj),e(Se,zh),e(zh,Tj),m(a,y_,g),m(a,Et,g),e(Et,kj),e(Et,Ai),e(Ai,Aj),e(Et,Dj),m(a,w_,g),y(bt,a,g),m(a,E_,g),m(a,cn,g),e(cn,Pj),e(cn,fn),e(fn,Oj),m(a,b_,g),m(a,Di,g),e(Di,Rj),m(a,j_,g),y(jt,a,g),m(a,T_,g),m(a,Pi,g),e(Pi,Nj),m(a,k_,g),m(a,Tt,g),e(Tt,Mh),e(Mh,hn),e(hn,Oi),e(Oi,Sj),e(hn,xj),e(hn,Fh),e(Tt,Ij),e(Tt,B),e(B,dn),e(dn,gn),e(gn,Jh),e(Jh,Hj),e(gn,Bj),e(dn,Cj),e(dn,Ri),e(Ri,Gj),e(B,Uj),e(B,mn),e(mn,Ni),e(Ni,Kh),e(Kh,Lj),e(mn,zj),e(mn,Si),e(Si,Mj),e(B,Fj),e(B,$n),e($n,xi),e(xi,Jj),e($n,Kj),e($n,fe),e(fe,Wj),e(fe,Wh),e(Wh,Yj),e(fe,Vj),e(fe,Yh),e(Yh,Xj),e(fe,Qj),e(B,Zj),e(B,_n),e(_n,Ii),e(Ii,e0),e(_n,t0),e(_n,he),e(he,s0),e(he,Vh),e(Vh,a0),e(he,n0),e(he,Xh),e(Xh,r0),e(he,o0),e(B,l0),e(B,qn),e(qn,Hi),e(Hi,i0),e(qn,u0),e(qn,de),e(de,p0),e(de,Qh),e(Qh,c0),e(de,f0),e(de,Bi),e(Bi,h0),e(de,d0),e(B,g0),e(B,vn),e(vn,Ci),e(Ci,m0),e(vn,$0),e(vn,ge),e(ge,_0),e(ge,Zh),e(Zh,q0),e(ge,v0),e(ge,Gi),e(Gi,y0),e(ge,w0),e(B,E0),e(B,yn),e(yn,Ui),e(Ui,b0),e(yn,j0),e(yn,ae),e(ae,T0),e(ae,ed),e(ed,k0),e(ae,A0),e(ae,Li),e(Li,D0),e(ae,P0),e(ae,zi),e(zi,O0),e(ae,R0),e(B,N0),e(B,wn),e(wn,Mi),e(Mi,S0),e(wn,x0),e(wn,kt),e(kt,I0),e(kt,td),e(td,H0),e(kt,B0),e(B,C0),e(B,En),e(En,Fi),e(Fi,G0),e(En,U0),e(En,At),e(At,L0),e(At,sd),e(sd,z0),e(At,M0),e(B,F0),e(B,bn),e(bn,Ji),e(Ji,ad),e(ad,J0),e(bn,K0),e(bn,Ki),e(Ki,W0),e(B,Y0),e(B,jn),e(jn,Wi),e(Wi,V0),e(jn,X0),e(jn,Dt),e(Dt,Q0),e(Dt,nd),e(nd,Z0),e(Dt,eT),e(B,tT),e(B,Tn),e(Tn,Yi),e(Yi,sT),e(Tn,aT),e(Tn,Pt),e(Pt,nT),e(Pt,rd),e(rd,rT),e(Pt,oT),e(B,lT),e(B,kn),e(kn,Vi),e(Vi,iT),e(kn,uT),e(kn,Ot),e(Ot,pT),e(Ot,od),e(od,cT),e(Ot,fT),m(a,A_,g),m(a,Xi,g),e(Xi,hT),m(a,D_,g),m(a,Rt,g),e(Rt,ld),e(ld,An),e(An,Qi),e(Qi,dT),e(An,gT),e(An,id),e(Rt,mT),e(Rt,ud),e(ud,Dn),e(Dn,Zi),e(Zi,pd),e(pd,$T),e(Dn,_T),e(Dn,eu),e(eu,qT),m(a,P_,g),m(a,xe,g),e(xe,Nt),e(Nt,cd),y(Pn,cd,null),e(xe,vT),e(xe,fd),e(fd,yT),m(a,O_,g),m(a,tu,g),e(tu,wT),m(a,R_,g),y(St,a,g),m(a,N_,g),m(a,On,g),e(On,ET),e(On,Rn),e(Rn,bT),m(a,S_,g),m(a,su,g),e(su,jT),m(a,x_,g),y(xt,a,g),m(a,I_,g),m(a,au,g),e(au,TT),m(a,H_,g),m(a,It,g),e(It,hd),e(hd,Nn),e(Nn,nu),e(nu,kT),e(Nn,AT),e(Nn,dd),e(It,DT),e(It,S),e(S,Sn),e(Sn,xn),e(xn,gd),e(gd,PT),e(xn,OT),e(Sn,RT),e(Sn,md),e(S,NT),e(S,In),e(In,ru),e(ru,ST),e(In,xT),e(In,ou),e(ou,IT),e(S,HT),e(S,Hn),e(Hn,lu),e(lu,BT),e(Hn,CT),e(Hn,iu),e(iu,GT),e(S,UT),e(S,Bn),e(Bn,uu),e(uu,LT),e(Bn,zT),e(Bn,Ht),e(Ht,MT),e(Ht,$d),e($d,FT),e(Ht,JT),e(S,KT),e(S,Cn),e(Cn,pu),e(pu,_d),e(_d,WT),e(Cn,YT),e(Cn,cu),e(cu,VT),e(S,XT),e(S,Gn),e(Gn,fu),e(fu,QT),e(Gn,ZT),e(Gn,me),e(me,e3),e(me,qd),e(qd,t3),e(me,s3),e(me,vd),e(vd,a3),e(me,n3),e(S,r3),e(S,Un),e(Un,hu),e(hu,o3),e(Un,l3),e(Un,$e),e($e,i3),e($e,yd),e(yd,u3),e($e,p3),e($e,wd),e(wd,c3),e($e,f3),e(S,h3),e(S,Ln),e(Ln,du),e(du,d3),e(Ln,g3),e(Ln,_e),e(_e,m3),e(_e,Ed),e(Ed,$3),e(_e,_3),e(_e,gu),e(gu,q3),e(_e,v3),e(S,y3),e(S,zn),e(zn,mu),e(mu,w3),e(zn,E3),e(zn,qe),e(qe,b3),e(qe,bd),e(bd,j3),e(qe,T3),e(qe,$u),e($u,k3),e(qe,A3),e(S,D3),e(S,Mn),e(Mn,_u),e(_u,P3),e(Mn,O3),e(Mn,ne),e(ne,R3),e(ne,jd),e(jd,N3),e(ne,S3),e(ne,qu),e(qu,x3),e(ne,I3),e(ne,vu),e(vu,H3),e(ne,B3),e(S,C3),e(S,Fn),e(Fn,yu),e(yu,G3),e(Fn,U3),e(Fn,Bt),e(Bt,L3),e(Bt,Td),e(Td,z3),e(Bt,M3),e(S,F3),e(S,Jn),e(Jn,wu),e(wu,J3),e(Jn,K3),e(Jn,Ct),e(Ct,W3),e(Ct,kd),e(kd,Y3),e(Ct,V3),e(S,X3),e(S,Kn),e(Kn,Eu),e(Eu,Ad),e(Ad,Q3),e(Kn,Z3),e(Kn,bu),e(bu,e5),e(S,t5),e(S,Wn),e(Wn,ju),e(ju,s5),e(Wn,a5),e(Wn,Gt),e(Gt,n5),e(Gt,Dd),e(Dd,r5),e(Gt,o5),e(S,l5),e(S,Yn),e(Yn,Tu),e(Tu,i5),e(Yn,u5),e(Yn,Ut),e(Ut,p5),e(Ut,Pd),e(Pd,c5),e(Ut,f5),e(S,h5),e(S,Vn),e(Vn,ku),e(ku,d5),e(Vn,g5),e(Vn,Lt),e(Lt,m5),e(Lt,Od),e(Od,$5),e(Lt,_5),m(a,B_,g),m(a,Au,g),e(Au,q5),m(a,C_,g),m(a,zt,g),e(zt,Rd),e(Rd,Xn),e(Xn,Du),e(Du,v5),e(Xn,y5),e(Xn,Nd),e(zt,w5),e(zt,oe),e(oe,Qn),e(Qn,Pu),e(Pu,Sd),e(Sd,E5),e(Qn,b5),e(Qn,Ou),e(Ou,j5),e(oe,T5),e(oe,Zn),e(Zn,Ru),e(Ru,xd),e(xd,k5),e(Zn,A5),e(Zn,Nu),e(Nu,D5),e(oe,P5),e(oe,er),e(er,Su),e(Su,O5),e(er,R5),e(er,xu),e(xu,N5),e(oe,S5),e(oe,tr),e(tr,Iu),e(Iu,x5),e(tr,I5),e(tr,Hu),e(Hu,H5),m(a,G_,g),m(a,Ie,g),e(Ie,Mt),e(Mt,Id),y(sr,Id,null),e(Ie,B5),e(Ie,Hd),e(Hd,C5),m(a,U_,g),m(a,Bu,g),e(Bu,G5),m(a,L_,g),y(Ft,a,g),m(a,z_,g),m(a,ar,g),e(ar,U5),e(ar,nr),e(nr,L5),m(a,M_,g),m(a,Cu,g),e(Cu,z5),m(a,F_,g),y(Jt,a,g),m(a,J_,g),m(a,Gu,g),e(Gu,M5),m(a,K_,g),m(a,Kt,g),e(Kt,Bd),e(Bd,rr),e(rr,Uu),e(Uu,F5),e(rr,J5),e(rr,Cd),e(Kt,K5),e(Kt,M),e(M,or),e(or,lr),e(lr,Gd),e(Gd,W5),e(lr,Y5),e(or,V5),e(or,Ud),e(M,X5),e(M,ir),e(ir,Lu),e(Lu,Q5),e(ir,Z5),e(ir,zu),e(zu,ek),e(M,tk),e(M,ur),e(ur,Mu),e(Mu,sk),e(ur,ak),e(ur,Fu),e(Fu,nk),e(M,rk),e(M,pr),e(pr,Ju),e(Ju,Ld),e(Ld,ok),e(pr,lk),e(pr,Ku),e(Ku,ik),e(M,uk),e(M,cr),e(cr,Wu),e(Wu,pk),e(cr,ck),e(cr,Wt),e(Wt,fk),e(Wt,zd),e(zd,hk),e(Wt,dk),e(M,gk),e(M,fr),e(fr,Yu),e(Yu,mk),e(fr,$k),e(fr,Yt),e(Yt,_k),e(Yt,Md),e(Md,qk),e(Yt,vk),e(M,yk),e(M,hr),e(hr,Vu),e(Vu,wk),e(hr,Ek),e(hr,Vt),e(Vt,bk),e(Vt,Fd),e(Fd,jk),e(Vt,Tk),m(a,W_,g),m(a,Xu,g),e(Xu,kk),m(a,Y_,g),y(Xt,a,g),m(a,V_,g),m(a,Qt,g),e(Qt,Jd),e(Jd,dr),e(dr,Qu),e(Qu,Ak),e(dr,Dk),e(dr,Kd),e(Qt,Pk),e(Qt,le),e(le,gr),e(gr,Zu),e(Zu,Wd),e(Wd,Ok),e(gr,Rk),e(gr,ep),e(ep,Nk),e(le,Sk),e(le,mr),e(mr,tp),e(tp,Yd),e(Yd,xk),e(mr,Ik),e(mr,sp),e(sp,Hk),e(le,Bk),e(le,$r),e($r,ap),e(ap,Vd),e(Vd,Ck),e($r,Gk),e($r,np),e(np,Uk),e(le,Lk),e(le,_r),e(_r,rp),e(rp,Xd),e(Xd,zk),e(_r,Mk),e(_r,op),e(op,Fk),m(a,X_,g),m(a,He,g),e(He,Zt),e(Zt,Qd),y(qr,Qd,null),e(He,Jk),e(He,Zd),e(Zd,Kk),m(a,Q_,g),m(a,lp,g),e(lp,Wk),m(a,Z_,g),y(es,a,g),m(a,eq,g),m(a,Be,g),e(Be,Yk),e(Be,vr),e(vr,Vk),e(Be,Xk),e(Be,yr),e(yr,Qk),m(a,tq,g),m(a,ip,g),e(ip,Zk),m(a,sq,g),y(ts,a,g),m(a,aq,g),m(a,up,g),e(up,e4),m(a,nq,g),m(a,pp,g),e(pp,t4),m(a,rq,g),y(ss,a,g),m(a,oq,g),m(a,as,g),e(as,eg),e(eg,wr),e(wr,cp),e(cp,s4),e(wr,a4),e(wr,tg),e(as,n4),e(as,ie),e(ie,Er),e(Er,fp),e(fp,sg),e(sg,r4),e(Er,o4),e(Er,hp),e(hp,l4),e(ie,i4),e(ie,br),e(br,dp),e(dp,ag),e(ag,u4),e(br,p4),e(br,gp),e(gp,c4),e(ie,f4),e(ie,jr),e(jr,mp),e(mp,ng),e(ng,h4),e(jr,d4),e(jr,ns),e(ns,g4),e(ns,rg),e(rg,m4),e(ns,$4),e(ie,_4),e(ie,Tr),e(Tr,$p),e($p,og),e(og,q4),e(Tr,v4),e(Tr,rs),e(rs,y4),e(rs,lg),e(lg,w4),e(rs,E4),m(a,lq,g),m(a,Ce,g),e(Ce,os),e(os,ig),y(kr,ig,null),e(Ce,b4),e(Ce,ug),e(ug,j4),m(a,iq,g),m(a,_p,g),e(_p,T4),m(a,uq,g),y(ls,a,g),m(a,pq,g),m(a,Ar,g),e(Ar,k4),e(Ar,Dr),e(Dr,A4),m(a,cq,g),m(a,qp,g),e(qp,D4),m(a,fq,g),y(is,a,g),m(a,hq,g),m(a,vp,g),e(vp,P4),m(a,dq,g),m(a,us,g),e(us,pg),e(pg,Pr),e(Pr,yp),e(yp,O4),e(Pr,R4),e(Pr,cg),e(us,N4),e(us,Q),e(Q,Or),e(Or,Rr),e(Rr,fg),e(fg,S4),e(Rr,x4),e(Or,I4),e(Or,wp),e(wp,H4),e(Q,B4),e(Q,Nr),e(Nr,Ep),e(Ep,hg),e(hg,C4),e(Nr,G4),e(Nr,bp),e(bp,U4),e(Q,L4),e(Q,Sr),e(Sr,jp),e(jp,z4),e(Sr,M4),e(Sr,ps),e(ps,F4),e(ps,dg),e(dg,J4),e(ps,K4),e(Q,W4),e(Q,xr),e(xr,Tp),e(Tp,Y4),e(xr,V4),e(xr,cs),e(cs,X4),e(cs,gg),e(gg,Q4),e(cs,Z4),e(Q,e7),e(Q,Ir),e(Ir,kp),e(kp,t7),e(Ir,s7),e(Ir,fs),e(fs,a7),e(fs,mg),e(mg,n7),e(fs,r7),m(a,gq,g),m(a,Ap,g),e(Ap,o7),m(a,mq,g),y(hs,a,g),m(a,$q,g),m(a,ds,g),e(ds,$g),e($g,Hr),e(Hr,Dp),e(Dp,l7),e(Hr,i7),e(Hr,_g),e(ds,u7),e(ds,Br),e(Br,Cr),e(Cr,Pp),e(Pp,qg),e(qg,p7),e(Cr,c7),e(Cr,Op),e(Op,f7),e(Br,h7),e(Br,Gr),e(Gr,Rp),e(Rp,vg),e(vg,d7),e(Gr,g7),e(Gr,Np),e(Np,m7),m(a,_q,g),m(a,Ge,g),e(Ge,gs),e(gs,yg),y(Ur,yg,null),e(Ge,$7),e(Ge,wg),e(wg,_7),m(a,qq,g),m(a,Lr,g),e(Lr,q7),e(Lr,Sp),e(Sp,v7),m(a,vq,g),m(a,Ue,g),e(Ue,ms),e(ms,Eg),y(zr,Eg,null),e(Ue,y7),e(Ue,bg),e(bg,w7),m(a,yq,g),m(a,xp,g),e(xp,E7),m(a,wq,g),y($s,a,g),m(a,Eq,g),m(a,Le,g),e(Le,b7),e(Le,Mr),e(Mr,j7),e(Le,T7),e(Le,Fr),e(Fr,k7),m(a,bq,g),m(a,Ip,g),e(Ip,A7),m(a,jq,g),y(_s,a,g),m(a,Tq,g),m(a,Hp,g),e(Hp,D7),m(a,kq,g),m(a,qs,g),e(qs,jg),e(jg,Jr),e(Jr,Bp),e(Bp,P7),e(Jr,O7),e(Jr,Tg),e(qs,R7),e(qs,F),e(F,Kr),e(Kr,Wr),e(Wr,kg),e(kg,N7),e(Wr,S7),e(Kr,x7),e(Kr,Cp),e(Cp,I7),e(F,H7),e(F,Yr),e(Yr,Gp),e(Gp,Ag),e(Ag,B7),e(Yr,C7),e(Yr,Up),e(Up,G7),e(F,U7),e(F,Vr),e(Vr,Lp),e(Lp,L7),e(Vr,z7),e(Vr,vs),e(vs,M7),e(vs,Dg),e(Dg,F7),e(vs,J7),e(F,K7),e(F,Xr),e(Xr,zp),e(zp,Pg),e(Pg,W7),e(Xr,Y7),e(Xr,Mp),e(Mp,V7),e(F,X7),e(F,Qr),e(Qr,Fp),e(Fp,Q7),e(Qr,Z7),e(Qr,ys),e(ys,e6),e(ys,Og),e(Og,t6),e(ys,s6),e(F,a6),e(F,Zr),e(Zr,Jp),e(Jp,n6),e(Zr,r6),e(Zr,ws),e(ws,o6),e(ws,Rg),e(Rg,l6),e(ws,i6),e(F,u6),e(F,eo),e(eo,Kp),e(Kp,p6),e(eo,c6),e(eo,Es),e(Es,f6),e(Es,Ng),e(Ng,h6),e(Es,d6),m(a,Aq,g),m(a,Wp,g),e(Wp,g6),m(a,Dq,g),y(bs,a,g),m(a,Pq,g),m(a,js,g),e(js,Sg),e(Sg,to),e(to,Yp),e(Yp,m6),e(to,$6),e(to,xg),e(js,_6),e(js,Z),e(Z,so),e(so,Vp),e(Vp,Ig),e(Ig,q6),e(so,v6),e(so,Xp),e(Xp,y6),e(Z,w6),e(Z,ao),e(ao,Qp),e(Qp,Hg),e(Hg,E6),e(ao,b6),e(ao,Zp),e(Zp,j6),e(Z,T6),e(Z,no),e(no,ec),e(ec,Bg),e(Bg,k6),e(no,A6),e(no,tc),e(tc,D6),e(Z,P6),e(Z,ro),e(ro,sc),e(sc,Cg),e(Cg,O6),e(ro,R6),e(ro,Ts),e(Ts,N6),e(Ts,Gg),e(Gg,S6),e(Ts,x6),e(Z,I6),e(Z,oo),e(oo,ac),e(ac,Ug),e(Ug,H6),e(oo,B6),e(oo,ks),e(ks,C6),e(ks,Lg),e(Lg,G6),e(ks,U6),m(a,Oq,g),m(a,ze,g),e(ze,As),e(As,zg),y(lo,zg,null),e(ze,L6),e(ze,Mg),e(Mg,z6),m(a,Rq,g),m(a,nc,g),e(nc,M6),m(a,Nq,g),y(Ds,a,g),m(a,Sq,g),m(a,io,g),e(io,F6),e(io,uo),e(uo,J6),m(a,xq,g),m(a,rc,g),e(rc,K6),m(a,Iq,g),y(Ps,a,g),m(a,Hq,g),m(a,oc,g),e(oc,W6),m(a,Bq,g),m(a,Os,g),e(Os,Fg),e(Fg,po),e(po,lc),e(lc,Y6),e(po,V6),e(po,Jg),e(Os,X6),e(Os,x),e(x,co),e(co,fo),e(fo,Kg),e(Kg,Q6),e(fo,Z6),e(co,e9),e(co,ic),e(ic,t9),e(x,s9),e(x,ho),e(ho,uc),e(uc,Wg),e(Wg,a9),e(ho,n9),e(ho,pc),e(pc,r9),e(x,o9),e(x,go),e(go,cc),e(cc,l9),e(go,i9),e(go,ve),e(ve,u9),e(ve,Yg),e(Yg,p9),e(ve,c9),e(ve,fc),e(fc,f9),e(ve,h9),e(x,d9),e(x,mo),e(mo,hc),e(hc,g9),e(mo,m9),e(mo,ye),e(ye,$9),e(ye,Vg),e(Vg,_9),e(ye,q9),e(ye,dc),e(dc,v9),e(ye,y9),e(x,w9),e(x,$o),e($o,gc),e(gc,E9),e($o,b9),e($o,re),e(re,j9),e(re,Xg),e(Xg,T9),e(re,k9),e(re,mc),e(mc,A9),e(re,D9),e(re,$c),e($c,P9),e(re,O9),e(x,R9),e(x,_o),e(_o,_c),e(_c,N9),e(_o,S9),e(_o,Rs),e(Rs,x9),e(Rs,Qg),e(Qg,I9),e(Rs,H9),e(x,B9),e(x,qo),e(qo,qc),e(qc,C9),e(qo,G9),e(qo,we),e(we,U9),e(we,Zg),e(Zg,L9),e(we,z9),e(we,em),e(em,M9),e(we,F9),e(x,J9),e(x,vo),e(vo,vc),e(vc,K9),e(vo,W9),e(vo,Ns),e(Ns,Y9),e(Ns,tm),e(tm,V9),e(Ns,X9),e(x,Q9),e(x,yo),e(yo,yc),e(yc,Z9),e(yo,e8),e(yo,Ee),e(Ee,t8),e(Ee,sm),e(sm,s8),e(Ee,a8),e(Ee,am),e(am,n8),e(Ee,r8),e(x,o8),e(x,wo),e(wo,wc),e(wc,l8),e(wo,i8),e(wo,Ss),e(Ss,u8),e(Ss,nm),e(nm,p8),e(Ss,c8),e(x,f8),e(x,Eo),e(Eo,Ec),e(Ec,h8),e(Eo,d8),e(Eo,xs),e(xs,g8),e(xs,rm),e(rm,m8),e(xs,$8),e(x,_8),e(x,bo),e(bo,bc),e(bc,om),e(om,q8),e(bo,v8),e(bo,jc),e(jc,y8),e(x,w8),e(x,jo),e(jo,Tc),e(Tc,E8),e(jo,b8),e(jo,Is),e(Is,j8),e(Is,lm),e(lm,T8),e(Is,k8),e(x,A8),e(x,To),e(To,kc),e(kc,D8),e(To,P8),e(To,Hs),e(Hs,O8),e(Hs,im),e(im,R8),e(Hs,N8),e(x,S8),e(x,ko),e(ko,Ac),e(Ac,x8),e(ko,I8),e(ko,Bs),e(Bs,H8),e(Bs,um),e(um,B8),e(Bs,C8),m(a,Cq,g),m(a,Dc,g),e(Dc,G8),m(a,Gq,g),y(Cs,a,g),m(a,Uq,g),m(a,Gs,g),e(Gs,pm),e(pm,Ao),e(Ao,Pc),e(Pc,U8),e(Ao,L8),e(Ao,cm),e(Gs,z8),e(Gs,fm),e(fm,Do),e(Do,Oc),e(Oc,hm),e(hm,M8),e(Do,F8),e(Do,Rc),e(Rc,J8),m(a,Lq,g),m(a,Me,g),e(Me,Us),e(Us,dm),y(Po,dm,null),e(Me,K8),e(Me,gm),e(gm,W8),m(a,zq,g),m(a,Ls,g),e(Ls,Y8),e(Ls,Nc),e(Nc,V8),e(Ls,X8),m(a,Mq,g),m(a,Fe,g),e(Fe,zs),e(zs,mm),y(Oo,mm,null),e(Fe,Q8),e(Fe,$m),e($m,Z8),m(a,Fq,g),m(a,Sc,g),e(Sc,eA),m(a,Jq,g),y(Ms,a,g),m(a,Kq,g),m(a,Ro,g),e(Ro,tA),e(Ro,No),e(No,sA),m(a,Wq,g),m(a,xc,g),e(xc,aA),m(a,Yq,g),y(Fs,a,g),m(a,Vq,g),m(a,Ic,g),e(Ic,nA),m(a,Xq,g),m(a,Js,g),e(Js,_m),e(_m,So),e(So,Hc),e(Hc,rA),e(So,oA),e(So,qm),e(Js,lA),e(Js,ee),e(ee,xo),e(xo,Io),e(Io,vm),e(vm,iA),e(Io,uA),e(xo,pA),e(xo,Bc),e(Bc,cA),e(ee,fA),e(ee,Ho),e(Ho,Cc),e(Cc,ym),e(ym,hA),e(Ho,dA),e(Ho,Gc),e(Gc,gA),e(ee,mA),e(ee,Bo),e(Bo,Uc),e(Uc,$A),e(Bo,_A),e(Bo,Ks),e(Ks,qA),e(Ks,wm),e(wm,vA),e(Ks,yA),e(ee,wA),e(ee,Co),e(Co,Lc),e(Lc,EA),e(Co,bA),e(Co,Ws),e(Ws,jA),e(Ws,Em),e(Em,TA),e(Ws,kA),e(ee,AA),e(ee,Go),e(Go,zc),e(zc,DA),e(Go,PA),e(Go,Ys),e(Ys,OA),e(Ys,bm),e(bm,RA),e(Ys,NA),m(a,Qq,g),m(a,Mc,g),e(Mc,SA),m(a,Zq,g),y(Vs,a,g),m(a,e1,g),m(a,Xs,g),e(Xs,jm),e(jm,Uo),e(Uo,Fc),e(Fc,xA),e(Uo,IA),e(Uo,Tm),e(Xs,HA),e(Xs,ue),e(ue,Lo),e(Lo,Jc),e(Jc,km),e(km,BA),e(Lo,CA),e(Lo,Kc),e(Kc,GA),e(ue,UA),e(ue,zo),e(zo,Wc),e(Wc,Am),e(Am,LA),e(zo,zA),e(zo,Yc),e(Yc,MA),e(ue,FA),e(ue,Mo),e(Mo,Vc),e(Vc,Dm),e(Dm,JA),e(Mo,KA),e(Mo,Xc),e(Xc,WA),e(ue,YA),e(ue,Fo),e(Fo,Qc),e(Qc,Pm),e(Pm,VA),e(Fo,XA),e(Fo,Zc),e(Zc,QA),m(a,t1,g),m(a,Je,g),e(Je,Qs),e(Qs,Om),y(Jo,Om,null),e(Je,ZA),e(Je,Rm),e(Rm,eD),m(a,s1,g),m(a,ef,g),e(ef,tD),m(a,a1,g),y(Zs,a,g),m(a,n1,g),y(ea,a,g),m(a,r1,g),m(a,pe,g),e(pe,sD),e(pe,Ko),e(Ko,aD),e(pe,nD),e(pe,Wo),e(Wo,rD),e(pe,oD),e(pe,Yo),e(Yo,lD),m(a,o1,g),m(a,tf,g),e(tf,iD),m(a,l1,g),y(ta,a,g),m(a,i1,g),m(a,sf,g),e(sf,uD),m(a,u1,g),m(a,sa,g),e(sa,Nm),e(Nm,Vo),e(Vo,af),e(af,pD),e(Vo,cD),e(Vo,Sm),e(sa,fD),e(sa,xm),e(xm,Xo),e(Xo,Qo),e(Qo,Im),e(Im,hD),e(Qo,dD),e(Xo,gD),e(Xo,nf),e(nf,mD),m(a,p1,g),m(a,rf,g),e(rf,$D),m(a,c1,g),m(a,of,g),e(of,_D),m(a,f1,g),y(aa,a,g),m(a,h1,g),m(a,na,g),e(na,Hm),e(Hm,Zo),e(Zo,lf),e(lf,qD),e(Zo,vD),e(Zo,Bm),e(na,yD),e(na,Cm),e(Cm,el),e(el,uf),e(uf,Gm),e(Gm,wD),e(el,ED),e(el,pf),e(pf,bD),m(a,d1,g),m(a,Ke,g),e(Ke,ra),e(ra,Um),y(tl,Um,null),e(Ke,jD),e(Ke,Lm),e(Lm,TD),m(a,g1,g),m(a,cf,g),e(cf,kD),m(a,m1,g),y(oa,a,g),m(a,$1,g),m(a,We,g),e(We,AD),e(We,sl),e(sl,DD),e(We,PD),e(We,al),e(al,OD),m(a,_1,g),m(a,ff,g),e(ff,RD),m(a,q1,g),m(a,la,g),e(la,zm),e(zm,nl),e(nl,hf),e(hf,ND),e(nl,SD),e(nl,Mm),e(la,xD),e(la,te),e(te,rl),e(rl,ol),e(ol,Fm),e(Fm,ID),e(ol,HD),e(rl,BD),e(rl,df),e(df,CD),e(te,GD),e(te,ll),e(ll,gf),e(gf,Jm),e(Jm,UD),e(ll,LD),e(ll,mf),e(mf,zD),e(te,MD),e(te,il),e(il,$f),e($f,FD),e(il,JD),e(il,ia),e(ia,KD),e(ia,Km),e(Km,WD),e(ia,YD),e(te,VD),e(te,ul),e(ul,_f),e(_f,XD),e(ul,QD),e(ul,ua),e(ua,ZD),e(ua,Wm),e(Wm,eP),e(ua,tP),e(te,sP),e(te,pl),e(pl,qf),e(qf,aP),e(pl,nP),e(pl,pa),e(pa,rP),e(pa,Ym),e(Ym,oP),e(pa,lP),m(a,v1,g),m(a,vf,g),e(vf,iP),m(a,y1,g),m(a,ca,g),e(ca,Vm),e(Vm,cl),e(cl,yf),e(yf,uP),e(cl,pP),e(cl,Xm),e(ca,cP),e(ca,Qm),e(Qm,fl),e(fl,wf),e(wf,Zm),e(Zm,fP),e(fl,hP),e(fl,Ef),e(Ef,dP),m(a,w1,g),m(a,bf,g),e(bf,gP),m(a,E1,g),m(a,Ye,g),e(Ye,fa),e(fa,e$),y(hl,e$,null),e(Ye,mP),e(Ye,t$),e(t$,$P),m(a,b1,g),m(a,jf,g),e(jf,_P),m(a,j1,g),y(ha,a,g),m(a,T1,g),m(a,Ve,g),e(Ve,qP),e(Ve,dl),e(dl,vP),e(Ve,yP),e(Ve,gl),e(gl,wP),m(a,k1,g),m(a,Tf,g),e(Tf,EP),m(a,A1,g),y(da,a,g),m(a,D1,g),m(a,kf,g),e(kf,bP),m(a,P1,g),m(a,ga,g),e(ga,s$),e(s$,ml),e(ml,Af),e(Af,jP),e(ml,TP),e(ml,a$),e(ga,kP),e(ga,n$),e(n$,$l),e($l,_l),e(_l,r$),e(r$,AP),e(_l,DP),e($l,PP),e($l,Df),e(Df,OP),m(a,O1,g),m(a,Pf,g),e(Pf,RP),m(a,R1,g),y(ma,a,g),m(a,N1,g),m(a,$a,g),e($a,o$),e(o$,ql),e(ql,Of),e(Of,NP),e(ql,SP),e(ql,l$),e($a,xP),e($a,vl),e(vl,yl),e(yl,Rf),e(Rf,i$),e(i$,IP),e(yl,HP),e(yl,Nf),e(Nf,BP),e(vl,CP),e(vl,wl),e(wl,Sf),e(Sf,u$),e(u$,GP),e(wl,UP),e(wl,xf),e(xf,LP),m(a,S1,g),m(a,Xe,g),e(Xe,_a),e(_a,p$),y(El,p$,null),e(Xe,zP),e(Xe,c$),e(c$,MP),m(a,x1,g),m(a,If,g),e(If,FP),m(a,I1,g),y(qa,a,g),m(a,H1,g),m(a,bl,g),e(bl,JP),e(bl,jl),e(jl,KP),m(a,B1,g),m(a,Hf,g),e(Hf,WP),m(a,C1,g),y(va,a,g),m(a,G1,g),m(a,ya,g),e(ya,YP),e(ya,Tl),e(Tl,VP),e(ya,XP),m(a,U1,g),m(a,wa,g),e(wa,f$),e(f$,kl),e(kl,Bf),e(Bf,QP),e(kl,ZP),e(kl,h$),e(wa,eO),e(wa,d$),e(d$,Al),e(Al,Dl),e(Dl,g$),e(g$,tO),e(Dl,sO),e(Al,aO),e(Al,Cf),e(Cf,nO),m(a,L1,g),m(a,Gf,g),e(Gf,rO),m(a,z1,g),y(Ea,a,g),m(a,M1,g),m(a,ba,g),e(ba,m$),e(m$,Pl),e(Pl,Uf),e(Uf,oO),e(Pl,lO),e(Pl,$$),e(ba,iO),e(ba,Qe),e(Qe,Ol),e(Ol,Lf),e(Lf,_$),e(_$,uO),e(Ol,pO),e(Ol,zf),e(zf,cO),e(Qe,fO),e(Qe,Rl),e(Rl,Mf),e(Mf,q$),e(q$,hO),e(Rl,dO),e(Rl,Ff),e(Ff,gO),e(Qe,mO),e(Qe,Nl),e(Nl,Jf),e(Jf,v$),e(v$,$O),e(Nl,_O),e(Nl,Kf),e(Kf,qO),F1=!0},p(a,[g]){const Sl={};g&2&&(Sl.$$scope={dirty:g,ctx:a}),nt.$set(Sl);const y$={};g&2&&(y$.$$scope={dirty:g,ctx:a}),rt.$set(y$);const w$={};g&2&&(w$.$$scope={dirty:g,ctx:a}),ct.$set(w$);const E$={};g&2&&(E$.$$scope={dirty:g,ctx:a}),gt.$set(E$);const xl={};g&2&&(xl.$$scope={dirty:g,ctx:a}),mt.$set(xl);const b$={};g&2&&(b$.$$scope={dirty:g,ctx:a}),bt.$set(b$);const j$={};g&2&&(j$.$$scope={dirty:g,ctx:a}),jt.$set(j$);const T$={};g&2&&(T$.$$scope={dirty:g,ctx:a}),St.$set(T$);const k$={};g&2&&(k$.$$scope={dirty:g,ctx:a}),xt.$set(k$);const A$={};g&2&&(A$.$$scope={dirty:g,ctx:a}),Ft.$set(A$);const Il={};g&2&&(Il.$$scope={dirty:g,ctx:a}),Jt.$set(Il);const D$={};g&2&&(D$.$$scope={dirty:g,ctx:a}),Xt.$set(D$);const P$={};g&2&&(P$.$$scope={dirty:g,ctx:a}),es.$set(P$);const O$={};g&2&&(O$.$$scope={dirty:g,ctx:a}),ts.$set(O$);const R$={};g&2&&(R$.$$scope={dirty:g,ctx:a}),ss.$set(R$);const Wf={};g&2&&(Wf.$$scope={dirty:g,ctx:a}),ls.$set(Wf);const N$={};g&2&&(N$.$$scope={dirty:g,ctx:a}),is.$set(N$);const S$={};g&2&&(S$.$$scope={dirty:g,ctx:a}),hs.$set(S$);const x$={};g&2&&(x$.$$scope={dirty:g,ctx:a}),$s.$set(x$);const Hl={};g&2&&(Hl.$$scope={dirty:g,ctx:a}),_s.$set(Hl);const I$={};g&2&&(I$.$$scope={dirty:g,ctx:a}),bs.$set(I$);const Bl={};g&2&&(Bl.$$scope={dirty:g,ctx:a}),Ds.$set(Bl);const H$={};g&2&&(H$.$$scope={dirty:g,ctx:a}),Ps.$set(H$);const L={};g&2&&(L.$$scope={dirty:g,ctx:a}),Cs.$set(L);const Cl={};g&2&&(Cl.$$scope={dirty:g,ctx:a}),Ms.$set(Cl);const Yf={};g&2&&(Yf.$$scope={dirty:g,ctx:a}),Fs.$set(Yf);const B$={};g&2&&(B$.$$scope={dirty:g,ctx:a}),Vs.$set(B$);const C$={};g&2&&(C$.$$scope={dirty:g,ctx:a}),Zs.$set(C$);const Gl={};g&2&&(Gl.$$scope={dirty:g,ctx:a}),ea.$set(Gl);const Vf={};g&2&&(Vf.$$scope={dirty:g,ctx:a}),ta.$set(Vf);const G$={};g&2&&(G$.$$scope={dirty:g,ctx:a}),aa.$set(G$);const U$={};g&2&&(U$.$$scope={dirty:g,ctx:a}),oa.$set(U$);const Ul={};g&2&&(Ul.$$scope={dirty:g,ctx:a}),ha.$set(Ul);const L$={};g&2&&(L$.$$scope={dirty:g,ctx:a}),da.$set(L$);const Ze={};g&2&&(Ze.$$scope={dirty:g,ctx:a}),ma.$set(Ze);const z$={};g&2&&(z$.$$scope={dirty:g,ctx:a}),qa.$set(z$);const M$={};g&2&&(M$.$$scope={dirty:g,ctx:a}),va.$set(M$);const Ll={};g&2&&(Ll.$$scope={dirty:g,ctx:a}),Ea.$set(Ll)},i(a){F1||(w(k.$$.fragment,a),w(V.$$.fragment,a),w(Na.$$.fragment,a),w(nt.$$.fragment,a),w(rt.$$.fragment,a),w(ct.$$.fragment,a),w(Xa.$$.fragment,a),w(gt.$$.fragment,a),w(mt.$$.fragment,a),w(pn.$$.fragment,a),w(bt.$$.fragment,a),w(jt.$$.fragment,a),w(Pn.$$.fragment,a),w(St.$$.fragment,a),w(xt.$$.fragment,a),w(sr.$$.fragment,a),w(Ft.$$.fragment,a),w(Jt.$$.fragment,a),w(Xt.$$.fragment,a),w(qr.$$.fragment,a),w(es.$$.fragment,a),w(ts.$$.fragment,a),w(ss.$$.fragment,a),w(kr.$$.fragment,a),w(ls.$$.fragment,a),w(is.$$.fragment,a),w(hs.$$.fragment,a),w(Ur.$$.fragment,a),w(zr.$$.fragment,a),w($s.$$.fragment,a),w(_s.$$.fragment,a),w(bs.$$.fragment,a),w(lo.$$.fragment,a),w(Ds.$$.fragment,a),w(Ps.$$.fragment,a),w(Cs.$$.fragment,a),w(Po.$$.fragment,a),w(Oo.$$.fragment,a),w(Ms.$$.fragment,a),w(Fs.$$.fragment,a),w(Vs.$$.fragment,a),w(Jo.$$.fragment,a),w(Zs.$$.fragment,a),w(ea.$$.fragment,a),w(ta.$$.fragment,a),w(aa.$$.fragment,a),w(tl.$$.fragment,a),w(oa.$$.fragment,a),w(hl.$$.fragment,a),w(ha.$$.fragment,a),w(da.$$.fragment,a),w(ma.$$.fragment,a),w(El.$$.fragment,a),w(qa.$$.fragment,a),w(va.$$.fragment,a),w(Ea.$$.fragment,a),F1=!0)},o(a){E(k.$$.fragment,a),E(V.$$.fragment,a),E(Na.$$.fragment,a),E(nt.$$.fragment,a),E(rt.$$.fragment,a),E(ct.$$.fragment,a),E(Xa.$$.fragment,a),E(gt.$$.fragment,a),E(mt.$$.fragment,a),E(pn.$$.fragment,a),E(bt.$$.fragment,a),E(jt.$$.fragment,a),E(Pn.$$.fragment,a),E(St.$$.fragment,a),E(xt.$$.fragment,a),E(sr.$$.fragment,a),E(Ft.$$.fragment,a),E(Jt.$$.fragment,a),E(Xt.$$.fragment,a),E(qr.$$.fragment,a),E(es.$$.fragment,a),E(ts.$$.fragment,a),E(ss.$$.fragment,a),E(kr.$$.fragment,a),E(ls.$$.fragment,a),E(is.$$.fragment,a),E(hs.$$.fragment,a),E(Ur.$$.fragment,a),E(zr.$$.fragment,a),E($s.$$.fragment,a),E(_s.$$.fragment,a),E(bs.$$.fragment,a),E(lo.$$.fragment,a),E(Ds.$$.fragment,a),E(Ps.$$.fragment,a),E(Cs.$$.fragment,a),E(Po.$$.fragment,a),E(Oo.$$.fragment,a),E(Ms.$$.fragment,a),E(Fs.$$.fragment,a),E(Vs.$$.fragment,a),E(Jo.$$.fragment,a),E(Zs.$$.fragment,a),E(ea.$$.fragment,a),E(ta.$$.fragment,a),E(aa.$$.fragment,a),E(tl.$$.fragment,a),E(oa.$$.fragment,a),E(hl.$$.fragment,a),E(ha.$$.fragment,a),E(da.$$.fragment,a),E(ma.$$.fragment,a),E(El.$$.fragment,a),E(qa.$$.fragment,a),E(va.$$.fragment,a),E(Ea.$$.fragment,a),F1=!1},d(a){t(n),a&&t(p),a&&t(s),b(k),a&&t(P),a&&t(D),b(V),a&&t(Ra),a&&t(Pe),a&&t(Y$),a&&t(Fl),a&&t(V$),a&&t(tt),a&&t(X$),a&&t(st),a&&t(Q$),a&&t(Oe),b(Na),a&&t(Z$),a&&t(Jl),a&&t(e_),b(nt,a),a&&t(t_),a&&t(Sa),a&&t(s_),a&&t(Kl),a&&t(a_),b(rt,a),a&&t(n_),a&&t(Wl),a&&t(r_),a&&t(ot),a&&t(o_),a&&t(oi),a&&t(l_),a&&t(li),a&&t(i_),b(ct,a),a&&t(u_),a&&t(ft),a&&t(p_),a&&t(Ne),b(Xa),a&&t(c_),a&&t(di),a&&t(f_),b(gt,a),a&&t(h_),a&&t(Qa),a&&t(d_),a&&t(gi),a&&t(g_),b(mt,a),a&&t(m_),a&&t(mi),a&&t($_),a&&t($t),a&&t(__),a&&t(bi),a&&t(q_),a&&t(yt),a&&t(v_),a&&t(Se),b(pn),a&&t(y_),a&&t(Et),a&&t(w_),b(bt,a),a&&t(E_),a&&t(cn),a&&t(b_),a&&t(Di),a&&t(j_),b(jt,a),a&&t(T_),a&&t(Pi),a&&t(k_),a&&t(Tt),a&&t(A_),a&&t(Xi),a&&t(D_),a&&t(Rt),a&&t(P_),a&&t(xe),b(Pn),a&&t(O_),a&&t(tu),a&&t(R_),b(St,a),a&&t(N_),a&&t(On),a&&t(S_),a&&t(su),a&&t(x_),b(xt,a),a&&t(I_),a&&t(au),a&&t(H_),a&&t(It),a&&t(B_),a&&t(Au),a&&t(C_),a&&t(zt),a&&t(G_),a&&t(Ie),b(sr),a&&t(U_),a&&t(Bu),a&&t(L_),b(Ft,a),a&&t(z_),a&&t(ar),a&&t(M_),a&&t(Cu),a&&t(F_),b(Jt,a),a&&t(J_),a&&t(Gu),a&&t(K_),a&&t(Kt),a&&t(W_),a&&t(Xu),a&&t(Y_),b(Xt,a),a&&t(V_),a&&t(Qt),a&&t(X_),a&&t(He),b(qr),a&&t(Q_),a&&t(lp),a&&t(Z_),b(es,a),a&&t(eq),a&&t(Be),a&&t(tq),a&&t(ip),a&&t(sq),b(ts,a),a&&t(aq),a&&t(up),a&&t(nq),a&&t(pp),a&&t(rq),b(ss,a),a&&t(oq),a&&t(as),a&&t(lq),a&&t(Ce),b(kr),a&&t(iq),a&&t(_p),a&&t(uq),b(ls,a),a&&t(pq),a&&t(Ar),a&&t(cq),a&&t(qp),a&&t(fq),b(is,a),a&&t(hq),a&&t(vp),a&&t(dq),a&&t(us),a&&t(gq),a&&t(Ap),a&&t(mq),b(hs,a),a&&t($q),a&&t(ds),a&&t(_q),a&&t(Ge),b(Ur),a&&t(qq),a&&t(Lr),a&&t(vq),a&&t(Ue),b(zr),a&&t(yq),a&&t(xp),a&&t(wq),b($s,a),a&&t(Eq),a&&t(Le),a&&t(bq),a&&t(Ip),a&&t(jq),b(_s,a),a&&t(Tq),a&&t(Hp),a&&t(kq),a&&t(qs),a&&t(Aq),a&&t(Wp),a&&t(Dq),b(bs,a),a&&t(Pq),a&&t(js),a&&t(Oq),a&&t(ze),b(lo),a&&t(Rq),a&&t(nc),a&&t(Nq),b(Ds,a),a&&t(Sq),a&&t(io),a&&t(xq),a&&t(rc),a&&t(Iq),b(Ps,a),a&&t(Hq),a&&t(oc),a&&t(Bq),a&&t(Os),a&&t(Cq),a&&t(Dc),a&&t(Gq),b(Cs,a),a&&t(Uq),a&&t(Gs),a&&t(Lq),a&&t(Me),b(Po),a&&t(zq),a&&t(Ls),a&&t(Mq),a&&t(Fe),b(Oo),a&&t(Fq),a&&t(Sc),a&&t(Jq),b(Ms,a),a&&t(Kq),a&&t(Ro),a&&t(Wq),a&&t(xc),a&&t(Yq),b(Fs,a),a&&t(Vq),a&&t(Ic),a&&t(Xq),a&&t(Js),a&&t(Qq),a&&t(Mc),a&&t(Zq),b(Vs,a),a&&t(e1),a&&t(Xs),a&&t(t1),a&&t(Je),b(Jo),a&&t(s1),a&&t(ef),a&&t(a1),b(Zs,a),a&&t(n1),b(ea,a),a&&t(r1),a&&t(pe),a&&t(o1),a&&t(tf),a&&t(l1),b(ta,a),a&&t(i1),a&&t(sf),a&&t(u1),a&&t(sa),a&&t(p1),a&&t(rf),a&&t(c1),a&&t(of),a&&t(f1),b(aa,a),a&&t(h1),a&&t(na),a&&t(d1),a&&t(Ke),b(tl),a&&t(g1),a&&t(cf),a&&t(m1),b(oa,a),a&&t($1),a&&t(We),a&&t(_1),a&&t(ff),a&&t(q1),a&&t(la),a&&t(v1),a&&t(vf),a&&t(y1),a&&t(ca),a&&t(w1),a&&t(bf),a&&t(E1),a&&t(Ye),b(hl),a&&t(b1),a&&t(jf),a&&t(j1),b(ha,a),a&&t(T1),a&&t(Ve),a&&t(k1),a&&t(Tf),a&&t(A1),b(da,a),a&&t(D1),a&&t(kf),a&&t(P1),a&&t(ga),a&&t(O1),a&&t(Pf),a&&t(R1),b(ma,a),a&&t(N1),a&&t($a),a&&t(S1),a&&t(Xe),b(El),a&&t(x1),a&&t(If),a&&t(I1),b(qa,a),a&&t(H1),a&&t(bl),a&&t(B1),a&&t(Hf),a&&t(C1),b(va,a),a&&t(G1),a&&t(ya),a&&t(U1),a&&t(wa),a&&t(L1),a&&t(Gf),a&&t(z1),b(Ea,a),a&&t(M1),a&&t(ba)}}}const nz={local:"detailed-parameters",sections:[{local:"which-task-is-used-by-this-model",title:"Which task is used by this model ?"},{local:"zeroshot-classification-task",title:"Zero-shot classification task"},{local:"translation-task",title:"Translation task"},{local:"summarization-task",title:"Summarization task"},{local:"conversational-task",title:"Conversational task"},{local:"table-question-answering-task",title:"Table question answering task"},{local:"question-answering-task",title:"Question answering task"},{local:"textclassification-task",title:"Text-classification task"},{local:"named-entity-recognition-ner-task",title:"Named Entity Recognition (NER) task"},{local:"tokenclassification-task",title:"Token-classification task"},{local:"textgeneration-task",title:"Text-generation task"},{local:"text2textgeneration-task",title:"Text2text-generation task"},{local:"fill-mask-task",title:"Fill mask task"},{local:"automatic-speech-recognition-task",title:"Automatic speech recognition task"},{local:"featureextraction-task",title:"Feature-extraction task"},{local:"audioclassification-task",title:"Audio-classification task"},{local:"objectdetection-task",title:"Object-detection task"}],title:"Detailed parameters"};function rz(_){return XG(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class pz extends KG{constructor(n){super();WG(this,n,rz,az,YG,{})}}export{pz as default,nz as metadata};
