import{S as wU,i as bU,s as jU,e as r,k as c,w as q,t as i,M as TU,c as o,d as t,m as f,a as l,x as v,h as u,b as h,N as EU,F as e,g as m,y,q as E,o as w,B as b,v as kU,L as P}from"../chunks/vendor-7c454903.js";import{T as Y}from"../chunks/Tip-735285fc.js";import{I as F}from"../chunks/IconCopyLink-5457534b.js";import{I as L,M as R,C as N}from"../chunks/InferenceApi-9b73136f.js";function AU(_){let n,p,s,d,$,k,A;return{c(){n=r("p"),p=r("strong"),s=i("Recommended model"),d=i(`:
`),$=r("a"),k=i("facebook/bart-large-mnli"),A=i("."),this.h()},l(T){n=o(T,"P",{});var j=l(n);p=o(j,"STRONG",{});var O=l(p);s=u(O,"Recommended model"),O.forEach(t),d=u(j,`:
`),$=o(j,"A",{href:!0,rel:!0});var D=l($);k=u(D,"facebook/bart-large-mnli"),D.forEach(t),A=u(j,"."),j.forEach(t),this.h()},h(){h($,"href","https://huggingface.co/facebook/bart-large-mnli"),h($,"rel","nofollow")},m(T,j){m(T,n,j),e(n,p),e(p,s),e(n,d),e(n,$),e($,k),e(n,A)},d(T){T&&t(n)}}}function DU(_){let n,p;return n=new N({props:{code:`import json
import requests
headers = {"Authorization": f"Bearer {API_TOKEN}"}
API_URL = "https://api-inference.huggingface.co/models/facebook/bart-large-mnli"
def query(payload):
    data = json.dumps(payload)
    response = requests.request("POST", API_URL, headers=headers, data=data)
    return json.loads(response.content.decode("utf-8"))
data = query(
    {
        "inputs": "Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!",
        "parameters": {"candidate_labels": ["refund", "legal", "faq"]},
    }
)`,highlighted:`<span class="hljs-keyword">import</span> json
<span class="hljs-keyword">import</span> requests
headers = {<span class="hljs-string">&quot;Authorization&quot;</span>: <span class="hljs-string">f&quot;Bearer <span class="hljs-subst">{API_TOKEN}</span>&quot;</span>}
API_URL = <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/facebook/bart-large-mnli&quot;</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">payload</span>):
    data = json.dumps(payload)
    response = requests.request(<span class="hljs-string">&quot;POST&quot;</span>, API_URL, headers=headers, data=data)
    <span class="hljs-keyword">return</span> json.loads(response.content.decode(<span class="hljs-string">&quot;utf-8&quot;</span>))
data = query(
    {
        <span class="hljs-string">&quot;inputs&quot;</span>: <span class="hljs-string">&quot;Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!&quot;</span>,
        <span class="hljs-string">&quot;parameters&quot;</span>: {<span class="hljs-string">&quot;candidate_labels&quot;</span>: [<span class="hljs-string">&quot;refund&quot;</span>, <span class="hljs-string">&quot;legal&quot;</span>, <span class="hljs-string">&quot;faq&quot;</span>]},
    }
)`}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p:P,i(s){p||(E(n.$$.fragment,s),p=!0)},o(s){w(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function OU(_){let n,p;return n=new R({props:{$$slots:{default:[DU]},$$scope:{ctx:_}}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){p||(E(n.$$.fragment,s),p=!0)},o(s){w(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function PU(_){let n,p;return n=new N({props:{code:`import fetch from "node-fetch";
async function query(data) {
    const response = await fetch(
        "https://api-inference.huggingface.co/models/facebook/bart-large-mnli",
        {
            headers: { Authorization: \`Bearer $}{API_TOKEN}\` },
            method: "POST",
            body: JSON.stringify(data),
        }
    );
    const result = await response.json();
    return result;
}
query({inputs: "Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!", parameters: {candidate_labels: ["refund", "legal", "faq"]}}).then((response) => {
    console.log(JSON.stringify(response));
});
// {"sequence":"Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!","labels":["refund","faq","legal"],"scores":[0.8778, 0.1052, 0.017]}`,highlighted:`<span class="hljs-keyword">import</span> fetch <span class="hljs-keyword">from</span> <span class="hljs-string">&quot;node-fetch&quot;</span>;
<span class="hljs-keyword">async</span> <span class="hljs-keyword">function</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">data</span>) {
    <span class="hljs-keyword">const</span> response = <span class="hljs-keyword">await</span> <span class="hljs-title function_">fetch</span>(
        <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/facebook/bart-large-mnli&quot;</span>,
        {
            <span class="hljs-attr">headers</span>: { <span class="hljs-title class_">Authorization</span>: <span class="hljs-string">\`Bearer <span class="hljs-subst">$}{API_TOKEN}</span>\`</span> },
            <span class="hljs-attr">method</span>: <span class="hljs-string">&quot;POST&quot;</span>,
            <span class="hljs-attr">body</span>: <span class="hljs-title class_">JSON</span>.<span class="hljs-title function_">stringify</span>(data),
        }
    );
    <span class="hljs-keyword">const</span> result = <span class="hljs-keyword">await</span> response.<span class="hljs-title function_">json</span>();
    <span class="hljs-keyword">return</span> result;
}
<span class="hljs-title function_">query</span>({<span class="hljs-attr">inputs</span>: <span class="hljs-string">&quot;Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!&quot;</span>, <span class="hljs-attr">parameters</span>: {<span class="hljs-attr">candidate_labels</span>: [<span class="hljs-string">&quot;refund&quot;</span>, <span class="hljs-string">&quot;legal&quot;</span>, <span class="hljs-string">&quot;faq&quot;</span>]}}).<span class="hljs-title function_">then</span>(<span class="hljs-function">(<span class="hljs-params">response</span>) =&gt;</span> {
    <span class="hljs-variable language_">console</span>.<span class="hljs-title function_">log</span>(<span class="hljs-title class_">JSON</span>.<span class="hljs-title function_">stringify</span>(response));
});
<span class="hljs-comment">// {&quot;sequence&quot;:&quot;Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!&quot;,&quot;labels&quot;:[&quot;refund&quot;,&quot;faq&quot;,&quot;legal&quot;],&quot;scores&quot;:[0.8778, 0.1052, 0.017]}</span>`}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p:P,i(s){p||(E(n.$$.fragment,s),p=!0)},o(s){w(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function RU(_){let n,p;return n=new R({props:{$$slots:{default:[PU]},$$scope:{ctx:_}}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){p||(E(n.$$.fragment,s),p=!0)},o(s){w(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function NU(_){let n,p;return n=new N({props:{code:`curl https://api-inference.huggingface.co/models/facebook/bart-large-mnli \\
        -X POST \\
        -d '{"inputs": "Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!", "parameters": {"candidate_labels": ["refund", "legal", "faq"]}}' \\
        -H "Authorization: Bearer $}{HF_API_TOKEN}"
# {"sequence":"Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!","labels":["refund","faq","legal"],"scores":[0.8778, 0.1052, 0.017]}`,highlighted:`curl https:<span class="hljs-comment">//api-inference.huggingface.co/models/facebook/bart-large-mnli \\</span>
        -X <span class="hljs-keyword">POST</span> \\
        -<span class="hljs-keyword">d</span> &#x27;{<span class="hljs-string">&quot;inputs&quot;</span>: <span class="hljs-string">&quot;Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!&quot;</span>, <span class="hljs-string">&quot;parameters&quot;</span>: {<span class="hljs-string">&quot;candidate_labels&quot;</span>: [<span class="hljs-string">&quot;refund&quot;</span>, <span class="hljs-string">&quot;legal&quot;</span>, <span class="hljs-string">&quot;faq&quot;</span>]}}&#x27; \\
        -<span class="hljs-keyword">H</span> <span class="hljs-string">&quot;Authorization: Bearer $}{HF_API_TOKEN}&quot;</span>
# {<span class="hljs-string">&quot;sequence&quot;</span>:<span class="hljs-string">&quot;Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!&quot;</span>,<span class="hljs-string">&quot;labels&quot;</span>:[<span class="hljs-string">&quot;refund&quot;</span>,<span class="hljs-string">&quot;faq&quot;</span>,<span class="hljs-string">&quot;legal&quot;</span>],<span class="hljs-string">&quot;scores&quot;</span>:[0.8778, 0.1052, 0.017]}`}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p:P,i(s){p||(E(n.$$.fragment,s),p=!0)},o(s){w(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function xU(_){let n,p;return n=new R({props:{$$slots:{default:[NU]},$$scope:{ctx:_}}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){p||(E(n.$$.fragment,s),p=!0)},o(s){w(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function SU(_){let n,p;return n=new N({props:{code:`self.assertEqual(
    deep_round(data),
    {
        "sequence": "Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!",
        "labels": ["refund", "faq", "legal"],
        "scores": [
            # 88% refund
            0.8778,
            0.1052,
            0.017,
        ],
    },
)`,highlighted:`self.assertEqual(
    deep_round(data),
    {
        <span class="hljs-string">&quot;sequence&quot;</span>: <span class="hljs-string">&quot;Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!&quot;</span>,
        <span class="hljs-string">&quot;labels&quot;</span>: [<span class="hljs-string">&quot;refund&quot;</span>, <span class="hljs-string">&quot;faq&quot;</span>, <span class="hljs-string">&quot;legal&quot;</span>],
        <span class="hljs-string">&quot;scores&quot;</span>: [
            <span class="hljs-comment"># 88% refund</span>
            <span class="hljs-number">0.8778</span>,
            <span class="hljs-number">0.1052</span>,
            <span class="hljs-number">0.017</span>,
        ],
    },
)`}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p:P,i(s){p||(E(n.$$.fragment,s),p=!0)},o(s){w(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function IU(_){let n,p;return n=new R({props:{$$slots:{default:[SU]},$$scope:{ctx:_}}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){p||(E(n.$$.fragment,s),p=!0)},o(s){w(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function HU(_){let n,p,s,d,$,k,A,T,j,O,D,ne,Pe;return{c(){n=r("p"),p=r("strong"),s=i("Recommended model"),d=i(`:
`),$=r("a"),k=i("Helsinki-NLP/opus-mt-ru-en"),A=i(`.
Helsinki-NLP uploaded many models with many language pairs.
`),T=r("strong"),j=i("Recommended model"),O=i(": "),D=r("a"),ne=i("t5-base"),Pe=i("."),this.h()},l(Q){n=o(Q,"P",{});var W=l(n);p=o(W,"STRONG",{});var st=l(p);s=u(st,"Recommended model"),st.forEach(t),d=u(W,`:
`),$=o(W,"A",{href:!0,rel:!0});var Ll=l($);k=u(Ll,"Helsinki-NLP/opus-mt-ru-en"),Ll.forEach(t),A=u(W,`.
Helsinki-NLP uploaded many models with many language pairs.
`),T=o(W,"STRONG",{});var Oa=l(T);j=u(Oa,"Recommended model"),Oa.forEach(t),O=u(W,": "),D=o(W,"A",{href:!0,rel:!0});var Re=l(D);ne=u(Re,"t5-base"),Re.forEach(t),Pe=u(W,"."),W.forEach(t),this.h()},h(){h($,"href","https://huggingface.co/Helsinki-NLP/opus-mt-ru-en"),h($,"rel","nofollow"),h(D,"href","https://huggingface.co/t5-base"),h(D,"rel","nofollow")},m(Q,W){m(Q,n,W),e(n,p),e(p,s),e(n,d),e(n,$),e($,k),e(n,A),e(n,T),e(T,j),e(n,O),e(n,D),e(D,ne),e(n,Pe)},d(Q){Q&&t(n)}}}function BU(_){let n,p;return n=new N({props:{code:`import json
import requests
headers = {"Authorization": f"Bearer {API_TOKEN}"}
API_URL = "https://api-inference.huggingface.co/models/Helsinki-NLP/opus-mt-ru-en"
def query(payload):
    data = json.dumps(payload)
    response = requests.request("POST", API_URL, headers=headers, data=data)
    return json.loads(response.content.decode("utf-8"))
data = query(
    {
        "inputs": "\u041C\u0435\u043D\u044F \u0437\u043E\u0432\u0443\u0442 \u0412\u043E\u043B\u044C\u0444\u0433\u0430\u043D\u0433 \u0438 \u044F \u0436\u0438\u0432\u0443 \u0432 \u0411\u0435\u0440\u043B\u0438\u043D\u0435",
    }
)
# Response
self.assertEqual(
    data,
    [
        {
            "translation_text": "My name is Wolfgang and I live in Berlin.",
        },
    ],
)`,highlighted:`<span class="hljs-keyword">import</span> json
<span class="hljs-keyword">import</span> requests
headers = {<span class="hljs-string">&quot;Authorization&quot;</span>: <span class="hljs-string">f&quot;Bearer <span class="hljs-subst">{API_TOKEN}</span>&quot;</span>}
API_URL = <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/Helsinki-NLP/opus-mt-ru-en&quot;</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">payload</span>):
    data = json.dumps(payload)
    response = requests.request(<span class="hljs-string">&quot;POST&quot;</span>, API_URL, headers=headers, data=data)
    <span class="hljs-keyword">return</span> json.loads(response.content.decode(<span class="hljs-string">&quot;utf-8&quot;</span>))
data = query(
    {
        <span class="hljs-string">&quot;inputs&quot;</span>: <span class="hljs-string">&quot;\u041C\u0435\u043D\u044F \u0437\u043E\u0432\u0443\u0442 \u0412\u043E\u043B\u044C\u0444\u0433\u0430\u043D\u0433 \u0438 \u044F \u0436\u0438\u0432\u0443 \u0432 \u0411\u0435\u0440\u043B\u0438\u043D\u0435&quot;</span>,
    }
)
<span class="hljs-comment"># Response</span>
self.assertEqual(
    data,
    [
        {
            <span class="hljs-string">&quot;translation_text&quot;</span>: <span class="hljs-string">&quot;My name is Wolfgang and I live in Berlin.&quot;</span>,
        },
    ],
)`}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p:P,i(s){p||(E(n.$$.fragment,s),p=!0)},o(s){w(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function CU(_){let n,p;return n=new R({props:{$$slots:{default:[BU]},$$scope:{ctx:_}}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){p||(E(n.$$.fragment,s),p=!0)},o(s){w(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function GU(_){let n,p;return n=new N({props:{code:`import fetch from "node-fetch";
async function query(data) {
    const response = await fetch(
        "https://api-inference.huggingface.co/models/Helsinki-NLP/opus-mt-ru-en",
        {
            headers: { Authorization: \`Bearer $}{API_TOKEN}\` },
            method: "POST",
            body: JSON.stringify(data),
        }
    );
    const result = await response.json();
    return result;
}
query({inputs: "\u041C\u0435\u043D\u044F \u0437\u043E\u0432\u0443\u0442 \u0412\u043E\u043B\u044C\u0444\u0433\u0430\u043D\u0433 \u0438 \u044F \u0436\u0438\u0432\u0443 \u0432 \u0411\u0435\u0440\u043B\u0438\u043D\u0435"}).then((response) => {
    console.log(JSON.stringify(response));
});
// [{"translation_text":"My name is Wolfgang and I live in Berlin."}]`,highlighted:`<span class="hljs-keyword">import</span> fetch <span class="hljs-keyword">from</span> <span class="hljs-string">&quot;node-fetch&quot;</span>;
<span class="hljs-keyword">async</span> <span class="hljs-keyword">function</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">data</span>) {
    <span class="hljs-keyword">const</span> response = <span class="hljs-keyword">await</span> <span class="hljs-title function_">fetch</span>(
        <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/Helsinki-NLP/opus-mt-ru-en&quot;</span>,
        {
            <span class="hljs-attr">headers</span>: { <span class="hljs-title class_">Authorization</span>: <span class="hljs-string">\`Bearer <span class="hljs-subst">$}{API_TOKEN}</span>\`</span> },
            <span class="hljs-attr">method</span>: <span class="hljs-string">&quot;POST&quot;</span>,
            <span class="hljs-attr">body</span>: <span class="hljs-title class_">JSON</span>.<span class="hljs-title function_">stringify</span>(data),
        }
    );
    <span class="hljs-keyword">const</span> result = <span class="hljs-keyword">await</span> response.<span class="hljs-title function_">json</span>();
    <span class="hljs-keyword">return</span> result;
}
<span class="hljs-title function_">query</span>({<span class="hljs-attr">inputs</span>: <span class="hljs-string">&quot;\u041C\u0435\u043D\u044F \u0437\u043E\u0432\u0443\u0442 \u0412\u043E\u043B\u044C\u0444\u0433\u0430\u043D\u0433 \u0438 \u044F \u0436\u0438\u0432\u0443 \u0432 \u0411\u0435\u0440\u043B\u0438\u043D\u0435&quot;</span>}).<span class="hljs-title function_">then</span>(<span class="hljs-function">(<span class="hljs-params">response</span>) =&gt;</span> {
    <span class="hljs-variable language_">console</span>.<span class="hljs-title function_">log</span>(<span class="hljs-title class_">JSON</span>.<span class="hljs-title function_">stringify</span>(response));
});
<span class="hljs-comment">// [{&quot;translation_text&quot;:&quot;My name is Wolfgang and I live in Berlin.&quot;}]</span>`}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p:P,i(s){p||(E(n.$$.fragment,s),p=!0)},o(s){w(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function UU(_){let n,p;return n=new R({props:{$$slots:{default:[GU]},$$scope:{ctx:_}}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){p||(E(n.$$.fragment,s),p=!0)},o(s){w(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function LU(_){let n,p;return n=new N({props:{code:`curl https://api-inference.huggingface.co/models/Helsinki-NLP/opus-mt-ru-en \\
        -X POST \\
        -d '{"inputs": "\u041C\u0435\u043D\u044F \u0437\u043E\u0432\u0443\u0442 \u0412\u043E\u043B\u044C\u0444\u0433\u0430\u043D\u0433 \u0438 \u044F \u0436\u0438\u0432\u0443 \u0432 \u0411\u0435\u0440\u043B\u0438\u043D\u0435"}' \\
        -H "Authorization: Bearer $}{HF_API_TOKEN}"
# [{"translation_text":"My name is Wolfgang and I live in Berlin."}]`,highlighted:`curl https:<span class="hljs-regexp">//</span>api-inference.huggingface.co<span class="hljs-regexp">/models/</span>Helsinki-NLP/opus-mt-ru-en \\
        -X POST \\
        -d <span class="hljs-string">&#x27;{&quot;inputs&quot;: &quot;\u041C\u0435\u043D\u044F \u0437\u043E\u0432\u0443\u0442 \u0412\u043E\u043B\u044C\u0444\u0433\u0430\u043D\u0433 \u0438 \u044F \u0436\u0438\u0432\u0443 \u0432 \u0411\u0435\u0440\u043B\u0438\u043D\u0435&quot;}&#x27;</span> \\
        -H <span class="hljs-string">&quot;Authorization: Bearer $}{HF_API_TOKEN}&quot;</span>
<span class="hljs-comment"># [{&quot;translation_text&quot;:&quot;My name is Wolfgang and I live in Berlin.&quot;}]</span>`}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p:P,i(s){p||(E(n.$$.fragment,s),p=!0)},o(s){w(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function zU(_){let n,p;return n=new R({props:{$$slots:{default:[LU]},$$scope:{ctx:_}}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){p||(E(n.$$.fragment,s),p=!0)},o(s){w(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function MU(_){let n,p,s,d,$,k,A;return{c(){n=r("p"),p=r("strong"),s=i("Recommended model"),d=i(`:
`),$=r("a"),k=i("facebook/bart-large-cnn"),A=i("."),this.h()},l(T){n=o(T,"P",{});var j=l(n);p=o(j,"STRONG",{});var O=l(p);s=u(O,"Recommended model"),O.forEach(t),d=u(j,`:
`),$=o(j,"A",{href:!0,rel:!0});var D=l($);k=u(D,"facebook/bart-large-cnn"),D.forEach(t),A=u(j,"."),j.forEach(t),this.h()},h(){h($,"href","https://huggingface.co/facebook/bart-large-cnn"),h($,"rel","nofollow")},m(T,j){m(T,n,j),e(n,p),e(p,s),e(n,d),e(n,$),e($,k),e(n,A)},d(T){T&&t(n)}}}function FU(_){let n,p;return n=new N({props:{code:`import json
import requests
headers = {"Authorization": f"Bearer {API_TOKEN}"}
API_URL = "https://api-inference.huggingface.co/models/facebook/bart-large-cnn"
def query(payload):
    data = json.dumps(payload)
    response = requests.request("POST", API_URL, headers=headers, data=data)
    return json.loads(response.content.decode("utf-8"))
data = query(
    {
        "inputs": "The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world, a title it held for 41 years until the Chrysler Building in New York City was finished in 1930. It was the first structure to reach a height of 300 metres. Due to the addition of a broadcasting aerial at the top of the tower in 1957, it is now taller than the Chrysler Building by 5.2 metres (17 ft). Excluding transmitters, the Eiffel Tower is the second tallest free-standing structure in France after the Millau Viaduct.",
        "parameters": {"do_sample": False},
    }
)
# Response
self.assertEqual(
    data,
    [
        {
            "summary_text": "The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world.",
        },
    ],
)`,highlighted:`<span class="hljs-keyword">import</span> json
<span class="hljs-keyword">import</span> requests
headers = {<span class="hljs-string">&quot;Authorization&quot;</span>: <span class="hljs-string">f&quot;Bearer <span class="hljs-subst">{API_TOKEN}</span>&quot;</span>}
API_URL = <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/facebook/bart-large-cnn&quot;</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">payload</span>):
    data = json.dumps(payload)
    response = requests.request(<span class="hljs-string">&quot;POST&quot;</span>, API_URL, headers=headers, data=data)
    <span class="hljs-keyword">return</span> json.loads(response.content.decode(<span class="hljs-string">&quot;utf-8&quot;</span>))
data = query(
    {
        <span class="hljs-string">&quot;inputs&quot;</span>: <span class="hljs-string">&quot;The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world, a title it held for 41 years until the Chrysler Building in New York City was finished in 1930. It was the first structure to reach a height of 300 metres. Due to the addition of a broadcasting aerial at the top of the tower in 1957, it is now taller than the Chrysler Building by 5.2 metres (17 ft). Excluding transmitters, the Eiffel Tower is the second tallest free-standing structure in France after the Millau Viaduct.&quot;</span>,
        <span class="hljs-string">&quot;parameters&quot;</span>: {<span class="hljs-string">&quot;do_sample&quot;</span>: <span class="hljs-literal">False</span>},
    }
)
<span class="hljs-comment"># Response</span>
self.assertEqual(
    data,
    [
        {
            <span class="hljs-string">&quot;summary_text&quot;</span>: <span class="hljs-string">&quot;The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world.&quot;</span>,
        },
    ],
)`}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p:P,i(s){p||(E(n.$$.fragment,s),p=!0)},o(s){w(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function JU(_){let n,p;return n=new R({props:{$$slots:{default:[FU]},$$scope:{ctx:_}}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){p||(E(n.$$.fragment,s),p=!0)},o(s){w(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function KU(_){let n,p;return n=new N({props:{code:`import fetch from "node-fetch";
async function query(data) {
    const response = await fetch(
        "https://api-inference.huggingface.co/models/facebook/bart-large-cnn",
        {
            headers: { Authorization: \`Bearer $}{API_TOKEN}\` },
            method: "POST",
            body: JSON.stringify(data),
        }
    );
    const result = await response.json();
    return result;
}
query({inputs: "The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world, a title it held for 41 years until the Chrysler Building in New York City was finished in 1930. It was the first structure to reach a height of 300 metres. Due to the addition of a broadcasting aerial at the top of the tower in 1957, it is now taller than the Chrysler Building by 5.2 metres (17 ft). Excluding transmitters, the Eiffel Tower is the second tallest free-standing structure in France after the Millau Viaduct."}).then((response) => {
    console.log(JSON.stringify(response));
});
// [{"summary_text":"The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side. It was the first structure to reach a height of 300 metres."}]`,highlighted:`import fetch from <span class="hljs-comment">&quot;node-fetch&quot;</span>;
async function query(data) {
    const response = await fetch(
        <span class="hljs-comment">&quot;https://api-inference.huggingface.co/models/facebook/bart-large-cnn&quot;</span>,
        {
            headers: { <span class="hljs-type">Authorization</span>: \`<span class="hljs-type">Bearer</span> <span class="hljs-string">$}{</span><span class="hljs-type">API_TOKEN</span>}\` },
            method: <span class="hljs-comment">&quot;POST&quot;</span>,
            body: <span class="hljs-type">JSON</span>.stringify(data),
        }
    );
    const result = await response.json();
    return result;
}
query({inputs: <span class="hljs-comment">&quot;The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world, a title it held for 41 years until the Chrysler Building in New York City was finished in 1930. It was the first structure to reach a height of 300 metres. Due to the addition of a broadcasting aerial at the top of the tower in 1957, it is now taller than the Chrysler Building by 5.2 metres (17 ft). Excluding transmitters, the Eiffel Tower is the second tallest free-standing structure in France after the Millau Viaduct.&quot;</span>}).then((response) =&gt; {
    console.log(<span class="hljs-type">JSON</span>.stringify(response));
});
// [{<span class="hljs-comment">&quot;summary_text&quot;</span>:<span class="hljs-comment">&quot;The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side. It was the first structure to reach a height of 300 metres.&quot;</span>}]`}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p:P,i(s){p||(E(n.$$.fragment,s),p=!0)},o(s){w(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function WU(_){let n,p;return n=new R({props:{$$slots:{default:[KU]},$$scope:{ctx:_}}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){p||(E(n.$$.fragment,s),p=!0)},o(s){w(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function YU(_){let n,p;return n=new N({props:{code:`curl https://api-inference.huggingface.co/models/facebook/bart-large-cnn \\
        -X POST \\
        -d '{"inputs": "The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world, a title it held for 41 years until the Chrysler Building in New York City was finished in 1930. It was the first structure to reach a height of 300 metres. Due to the addition of a broadcasting aerial at the top of the tower in 1957, it is now taller than the Chrysler Building by 5.2 metres (17 ft). Excluding transmitters, the Eiffel Tower is the second tallest free-standing structure in France after the Millau Viaduct.", "parameters": {"do_sample": false}}' \\
        -H "Authorization: Bearer $}{HF_API_TOKEN}"
# [{"summary_text":"The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world."}]`,highlighted:`curl https:<span class="hljs-comment">//api-inference.huggingface.co/models/facebook/bart-large-cnn \\
        -X POST \\
        -d &#x27;{&quot;inputs&quot;: &quot;The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world, a title it held for 41 years until the Chrysler Building in New York City was finished in 1930. It was the first structure to reach a height of 300 metres. Due to the addition of a broadcasting aerial at the top of the tower in 1957, it is now taller than the Chrysler Building by 5.2 metres (17 ft). Excluding transmitters, the Eiffel Tower is the second tallest free-standing structure in France after the Millau Viaduct.&quot;, &quot;parameters&quot;: {&quot;do_sample&quot;: false}}&#x27; \\
        -H &quot;Authorization: Bearer $}{HF_API_TOKEN}&quot;</span>
# [{<span class="hljs-string">&quot;summary_text&quot;</span>:<span class="hljs-string">&quot;The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world.&quot;</span>}]`}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p:P,i(s){p||(E(n.$$.fragment,s),p=!0)},o(s){w(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function VU(_){let n,p;return n=new R({props:{$$slots:{default:[YU]},$$scope:{ctx:_}}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){p||(E(n.$$.fragment,s),p=!0)},o(s){w(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function XU(_){let n,p,s,d,$,k,A;return{c(){n=r("p"),p=r("strong"),s=i("Recommended model"),d=i(`:
`),$=r("a"),k=i("microsoft/DialoGPT-large"),A=i("."),this.h()},l(T){n=o(T,"P",{});var j=l(n);p=o(j,"STRONG",{});var O=l(p);s=u(O,"Recommended model"),O.forEach(t),d=u(j,`:
`),$=o(j,"A",{href:!0,rel:!0});var D=l($);k=u(D,"microsoft/DialoGPT-large"),D.forEach(t),A=u(j,"."),j.forEach(t),this.h()},h(){h($,"href","https://huggingface.co/microsoft/DialoGPT-large"),h($,"rel","nofollow")},m(T,j){m(T,n,j),e(n,p),e(p,s),e(n,d),e(n,$),e($,k),e(n,A)},d(T){T&&t(n)}}}function QU(_){let n,p;return n=new N({props:{code:`import json
import requests
headers = {"Authorization": f"Bearer {API_TOKEN}"}
API_URL = "https://api-inference.huggingface.co/models/microsoft/DialoGPT-large"
def query(payload):
    data = json.dumps(payload)
    response = requests.request("POST", API_URL, headers=headers, data=data)
    return json.loads(response.content.decode("utf-8"))
data = query(
    {
        "inputs": {
            "past_user_inputs": ["Which movie is the best ?"],
            "generated_responses": ["It's Die Hard for sure."],
            "text": "Can you explain why ?",
        },
    }
)
# Response
self.assertEqual(
    data,
    {
        "generated_text": "It's the best movie ever.",
        "conversation": {
            "past_user_inputs": [
                "Which movie is the best ?",
                "Can you explain why ?",
            ],
            "generated_responses": [
                "It's Die Hard for sure.",
                "It's the best movie ever.",
            ],
        },
        "warnings": ["Setting \`pad_token_id\` to \`eos_token_id\`:50256 for open-end generation."],
    },
)`,highlighted:`<span class="hljs-keyword">import</span> json
<span class="hljs-keyword">import</span> requests
headers = {<span class="hljs-string">&quot;Authorization&quot;</span>: <span class="hljs-string">f&quot;Bearer <span class="hljs-subst">{API_TOKEN}</span>&quot;</span>}
API_URL = <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/microsoft/DialoGPT-large&quot;</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">payload</span>):
    data = json.dumps(payload)
    response = requests.request(<span class="hljs-string">&quot;POST&quot;</span>, API_URL, headers=headers, data=data)
    <span class="hljs-keyword">return</span> json.loads(response.content.decode(<span class="hljs-string">&quot;utf-8&quot;</span>))
data = query(
    {
        <span class="hljs-string">&quot;inputs&quot;</span>: {
            <span class="hljs-string">&quot;past_user_inputs&quot;</span>: [<span class="hljs-string">&quot;Which movie is the best ?&quot;</span>],
            <span class="hljs-string">&quot;generated_responses&quot;</span>: [<span class="hljs-string">&quot;It&#x27;s Die Hard for sure.&quot;</span>],
            <span class="hljs-string">&quot;text&quot;</span>: <span class="hljs-string">&quot;Can you explain why ?&quot;</span>,
        },
    }
)
<span class="hljs-comment"># Response</span>
self.assertEqual(
    data,
    {
        <span class="hljs-string">&quot;generated_text&quot;</span>: <span class="hljs-string">&quot;It&#x27;s the best movie ever.&quot;</span>,
        <span class="hljs-string">&quot;conversation&quot;</span>: {
            <span class="hljs-string">&quot;past_user_inputs&quot;</span>: [
                <span class="hljs-string">&quot;Which movie is the best ?&quot;</span>,
                <span class="hljs-string">&quot;Can you explain why ?&quot;</span>,
            ],
            <span class="hljs-string">&quot;generated_responses&quot;</span>: [
                <span class="hljs-string">&quot;It&#x27;s Die Hard for sure.&quot;</span>,
                <span class="hljs-string">&quot;It&#x27;s the best movie ever.&quot;</span>,
            ],
        },
        <span class="hljs-string">&quot;warnings&quot;</span>: [<span class="hljs-string">&quot;Setting \`pad_token_id\` to \`eos_token_id\`:50256 for open-end generation.&quot;</span>],
    },
)`}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p:P,i(s){p||(E(n.$$.fragment,s),p=!0)},o(s){w(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function ZU(_){let n,p;return n=new R({props:{$$slots:{default:[QU]},$$scope:{ctx:_}}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){p||(E(n.$$.fragment,s),p=!0)},o(s){w(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function eL(_){let n,p;return n=new N({props:{code:`import fetch from "node-fetch";
async function query(data) {
    const response = await fetch(
        "https://api-inference.huggingface.co/models/microsoft/DialoGPT-large",
        {
            headers: { Authorization: \`Bearer $}{API_TOKEN}\` },
            method: "POST",
            body: JSON.stringify(data),
        }
    );
    const result = await response.json();
    return result;
}
query({inputs: {past_user_inputs: ["Which movie is the best ?"], generated_responses: ["It is Die Hard for sure."], text:"Can you explain why ?"}}).then((response) => {
    console.log(JSON.stringify(response));
});
// {"generated_text":"It's the best movie ever.","conversation":{"past_user_inputs":["Which movie is the best ?","Can you explain why ?"],"generated_responses":["It is Die Hard for sure.","It's the best movie ever."]},"warnings":["Setting \`pad_token_id\` to \`eos_token_id\`:50256 for open-end generation."]}`,highlighted:`<span class="hljs-keyword">import</span> <span class="hljs-keyword">fetch</span> <span class="hljs-keyword">from</span> &quot;node-fetch&quot;;
async <span class="hljs-keyword">function</span> query(data) {
    const response = await <span class="hljs-keyword">fetch</span>(
        &quot;https://api-inference.huggingface.co/models/microsoft/DialoGPT-large&quot;,
        {
            headers: { <span class="hljs-keyword">Authorization</span>: \`Bearer $}{API_TOKEN}\` },
            <span class="hljs-keyword">method</span>: &quot;POST&quot;,
            body: <span class="hljs-type">JSON</span>.stringify(data),
        }
    );
    const result = await response.json();
    <span class="hljs-keyword">return</span> result;
}
query({inputs: {past_user_inputs: [&quot;Which movie is the best ?&quot;], generated_responses: [&quot;It is Die Hard for sure.&quot;], <span class="hljs-type">text</span>:&quot;Can you explain why ?&quot;}}).<span class="hljs-keyword">then</span>((response) =&gt; {
    console.log(<span class="hljs-type">JSON</span>.stringify(response));
});
// {&quot;generated_text&quot;:&quot;It&#x27;s the best movie ever.&quot;,&quot;conversation&quot;:{&quot;past_user_inputs&quot;:[&quot;Which movie is the best ?&quot;,&quot;Can you explain why ?&quot;],&quot;generated_responses&quot;:[&quot;It is Die Hard for sure.&quot;,&quot;It&#x27;s the best movie ever.&quot;]},&quot;warnings&quot;:[&quot;Setting \`pad_token_id\` to \`eos_token_id\`:50256 for open-end generation.&quot;]}`}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p:P,i(s){p||(E(n.$$.fragment,s),p=!0)},o(s){w(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function tL(_){let n,p;return n=new R({props:{$$slots:{default:[eL]},$$scope:{ctx:_}}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){p||(E(n.$$.fragment,s),p=!0)},o(s){w(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function sL(_){let n,p;return n=new N({props:{code:`curl https://api-inference.huggingface.co/models/microsoft/DialoGPT-large \\
        -X POST \\
        -d '{"inputs": {"past_user_inputs": ["Which movie is the best ?"], "generated_responses": ["It is Die Hard for sure."], "text":"Can you explain why ?"}}' \\
        -H "Authorization: Bearer $}{HF_API_TOKEN}"
# {"generated_text":"It's the best movie ever.","conversation":{"past_user_inputs":["Which movie is the best ?","Can you explain why ?"],"generated_responses":["It is Die Hard for sure.","It's the best movie ever."]},"warnings":["Setting \`pad_token_id\` to \`eos_token_id\`:50256 for open-end generation."]}`,highlighted:'<span class="hljs-title">curl https:</span>//api-inference.huggingface.co/models/microsoft/DialoGPT-large \\\n        -X POST \\\n        -d &#x27;{<span class="hljs-string">&quot;inputs&quot;</span>: {<span class="hljs-string">&quot;past_user_inputs&quot;</span>: [<span class="hljs-string">&quot;Which movie is the best ?&quot;</span>], <span class="hljs-string">&quot;generated_responses&quot;</span>: [<span class="hljs-string">&quot;It is Die Hard for sure.&quot;</span>], <span class="hljs-string">&quot;text&quot;</span>:<span class="hljs-string">&quot;Can you explain why ?&quot;</span>}}&#x27; \\\n        -H <span class="hljs-string">&quot;Authorization: Bearer $}{HF_API_TOKEN}&quot;</span>\n# {<span class="hljs-string">&quot;generated_text&quot;</span>:<span class="hljs-string">&quot;It&#x27;s the best movie ever.&quot;</span>,<span class="hljs-string">&quot;conversation&quot;</span>:{<span class="hljs-string">&quot;past_user_inputs&quot;</span>:[<span class="hljs-string">&quot;Which movie is the best ?&quot;</span>,<span class="hljs-string">&quot;Can you explain why ?&quot;</span>],<span class="hljs-string">&quot;generated_responses&quot;</span>:[<span class="hljs-string">&quot;It is Die Hard for sure.&quot;</span>,<span class="hljs-string">&quot;It&#x27;s the best movie ever.&quot;</span>]},<span class="hljs-string">&quot;warnings&quot;</span>:[<span class="hljs-string">&quot;Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.&quot;</span>]}'}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p:P,i(s){p||(E(n.$$.fragment,s),p=!0)},o(s){w(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function aL(_){let n,p;return n=new R({props:{$$slots:{default:[sL]},$$scope:{ctx:_}}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){p||(E(n.$$.fragment,s),p=!0)},o(s){w(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function nL(_){let n,p,s,d,$,k,A;return{c(){n=r("p"),p=r("strong"),s=i("Recommended model"),d=i(`:
`),$=r("a"),k=i("google/tapas-base-finetuned-wtq"),A=i("."),this.h()},l(T){n=o(T,"P",{});var j=l(n);p=o(j,"STRONG",{});var O=l(p);s=u(O,"Recommended model"),O.forEach(t),d=u(j,`:
`),$=o(j,"A",{href:!0,rel:!0});var D=l($);k=u(D,"google/tapas-base-finetuned-wtq"),D.forEach(t),A=u(j,"."),j.forEach(t),this.h()},h(){h($,"href","https://huggingface.co/google/tapas-base-finetuned-wtq"),h($,"rel","nofollow")},m(T,j){m(T,n,j),e(n,p),e(p,s),e(n,d),e(n,$),e($,k),e(n,A)},d(T){T&&t(n)}}}function rL(_){let n,p;return n=new N({props:{code:`import json
import requests
headers = {"Authorization": f"Bearer {API_TOKEN}"}
API_URL = "https://api-inference.huggingface.co/models/google/tapas-base-finetuned-wtq"
def query(payload):
    data = json.dumps(payload)
    response = requests.request("POST", API_URL, headers=headers, data=data)
    return json.loads(response.content.decode("utf-8"))
data = query(
    {
        "inputs": {
            "query": "How many stars does the transformers repository have?",
            "table": {
                "Repository": ["Transformers", "Datasets", "Tokenizers"],
                "Stars": ["36542", "4512", "3934"],
                "Contributors": ["651", "77", "34"],
                "Programming language": [
                    "Python",
                    "Python",
                    "Rust, Python and NodeJS",
                ],
            },
        }
    }
)`,highlighted:`<span class="hljs-keyword">import</span> json
<span class="hljs-keyword">import</span> requests
headers = {<span class="hljs-string">&quot;Authorization&quot;</span>: <span class="hljs-string">f&quot;Bearer <span class="hljs-subst">{API_TOKEN}</span>&quot;</span>}
API_URL = <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/google/tapas-base-finetuned-wtq&quot;</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">payload</span>):
    data = json.dumps(payload)
    response = requests.request(<span class="hljs-string">&quot;POST&quot;</span>, API_URL, headers=headers, data=data)
    <span class="hljs-keyword">return</span> json.loads(response.content.decode(<span class="hljs-string">&quot;utf-8&quot;</span>))
data = query(
    {
        <span class="hljs-string">&quot;inputs&quot;</span>: {
            <span class="hljs-string">&quot;query&quot;</span>: <span class="hljs-string">&quot;How many stars does the transformers repository have?&quot;</span>,
            <span class="hljs-string">&quot;table&quot;</span>: {
                <span class="hljs-string">&quot;Repository&quot;</span>: [<span class="hljs-string">&quot;Transformers&quot;</span>, <span class="hljs-string">&quot;Datasets&quot;</span>, <span class="hljs-string">&quot;Tokenizers&quot;</span>],
                <span class="hljs-string">&quot;Stars&quot;</span>: [<span class="hljs-string">&quot;36542&quot;</span>, <span class="hljs-string">&quot;4512&quot;</span>, <span class="hljs-string">&quot;3934&quot;</span>],
                <span class="hljs-string">&quot;Contributors&quot;</span>: [<span class="hljs-string">&quot;651&quot;</span>, <span class="hljs-string">&quot;77&quot;</span>, <span class="hljs-string">&quot;34&quot;</span>],
                <span class="hljs-string">&quot;Programming language&quot;</span>: [
                    <span class="hljs-string">&quot;Python&quot;</span>,
                    <span class="hljs-string">&quot;Python&quot;</span>,
                    <span class="hljs-string">&quot;Rust, Python and NodeJS&quot;</span>,
                ],
            },
        }
    }
)`}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p:P,i(s){p||(E(n.$$.fragment,s),p=!0)},o(s){w(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function oL(_){let n,p;return n=new R({props:{$$slots:{default:[rL]},$$scope:{ctx:_}}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){p||(E(n.$$.fragment,s),p=!0)},o(s){w(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function lL(_){let n,p;return n=new N({props:{code:`import fetch from "node-fetch";
async function query(data) {
    const response = await fetch(
        "https://api-inference.huggingface.co/models/google/tapas-base-finetuned-wtq",
        {
            headers: { Authorization: \`Bearer $}{API_TOKEN}\` },
            method: "POST",
            body: JSON.stringify(data),
        }
    );
    const result = await response.json();
    return result;
}
query({inputs:{query:"How many stars does the transformers repository have?",table:{Repository:["Transformers","Datasets","Tokenizers"],Stars:["36542","4512","3934"],Contributors:["651","77","34"],"Programming language":["Python","Python","Rust, Python and NodeJS"]}}}).then((response) => {
    console.log(JSON.stringify(response));
});
// {"answer":"AVERAGE > 36542","coordinates":[[0,1]],"cells":["36542"],"aggregator":"AVERAGE"}`,highlighted:`<span class="hljs-keyword">import</span> <span class="hljs-keyword">fetch</span> <span class="hljs-keyword">from</span> &quot;node-fetch&quot;;
async <span class="hljs-keyword">function</span> query(data) {
    const response = await <span class="hljs-keyword">fetch</span>(
        &quot;https://api-inference.huggingface.co/models/google/tapas-base-finetuned-wtq&quot;,
        {
            headers: { <span class="hljs-keyword">Authorization</span>: \`Bearer $}{API_TOKEN}\` },
            <span class="hljs-keyword">method</span>: &quot;POST&quot;,
            body: <span class="hljs-type">JSON</span>.stringify(data),
        }
    );
    const result = await response.json();
    <span class="hljs-keyword">return</span> result;
}
query({inputs:{query:&quot;How many stars does the transformers repository have?&quot;,<span class="hljs-keyword">table</span>:{Repository:[&quot;Transformers&quot;,&quot;Datasets&quot;,&quot;Tokenizers&quot;],Stars:[&quot;36542&quot;,&quot;4512&quot;,&quot;3934&quot;],Contributors:[&quot;651&quot;,&quot;77&quot;,&quot;34&quot;],&quot;Programming language&quot;:[&quot;Python&quot;,&quot;Python&quot;,&quot;Rust, Python and NodeJS&quot;]}}}).<span class="hljs-keyword">then</span>((response) =&gt; {
    console.log(<span class="hljs-type">JSON</span>.stringify(response));
});
// {&quot;answer&quot;:&quot;AVERAGE &gt; 36542&quot;,&quot;coordinates&quot;:[[<span class="hljs-number">0</span>,<span class="hljs-number">1</span>]],&quot;cells&quot;:[&quot;36542&quot;],&quot;aggregator&quot;:&quot;AVERAGE&quot;}`}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p:P,i(s){p||(E(n.$$.fragment,s),p=!0)},o(s){w(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function iL(_){let n,p;return n=new R({props:{$$slots:{default:[lL]},$$scope:{ctx:_}}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){p||(E(n.$$.fragment,s),p=!0)},o(s){w(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function uL(_){let n,p;return n=new N({props:{code:`curl https://api-inference.huggingface.co/models/google/tapas-base-finetuned-wtq \\
        -X POST \\
        -d '{"inputs":{"query":"How many stars does the transformers repository have?","table":{"Repository":["Transformers","Datasets","Tokenizers"],"Stars":["36542","4512","3934"],"Contributors":["651","77","34"],"Programming language":["Python","Python","Rust, Python and NodeJS"]}}}' \\
        -H "Authorization: Bearer $}{HF_API_TOKEN}"
# {"answer":"AVERAGE > 36542","coordinates":[[0,1]],"cells":["36542"],"aggregator":"AVERAGE"}`,highlighted:`curl https:<span class="hljs-comment">//api-inference.huggingface.co/models/google/tapas-base-finetuned-wtq \\</span>
        -X <span class="hljs-keyword">POST</span> \\
        -<span class="hljs-keyword">d</span> &#x27;{<span class="hljs-string">&quot;inputs&quot;</span>:{<span class="hljs-string">&quot;query&quot;</span>:<span class="hljs-string">&quot;How many stars does the transformers repository have?&quot;</span>,<span class="hljs-string">&quot;table&quot;</span>:{<span class="hljs-string">&quot;Repository&quot;</span>:[<span class="hljs-string">&quot;Transformers&quot;</span>,<span class="hljs-string">&quot;Datasets&quot;</span>,<span class="hljs-string">&quot;Tokenizers&quot;</span>],<span class="hljs-string">&quot;Stars&quot;</span>:[<span class="hljs-string">&quot;36542&quot;</span>,<span class="hljs-string">&quot;4512&quot;</span>,<span class="hljs-string">&quot;3934&quot;</span>],<span class="hljs-string">&quot;Contributors&quot;</span>:[<span class="hljs-string">&quot;651&quot;</span>,<span class="hljs-string">&quot;77&quot;</span>,<span class="hljs-string">&quot;34&quot;</span>],<span class="hljs-string">&quot;Programming language&quot;</span>:[<span class="hljs-string">&quot;Python&quot;</span>,<span class="hljs-string">&quot;Python&quot;</span>,<span class="hljs-string">&quot;Rust, Python and NodeJS&quot;</span>]}}}&#x27; \\
        -<span class="hljs-keyword">H</span> <span class="hljs-string">&quot;Authorization: Bearer $}{HF_API_TOKEN}&quot;</span>
# {<span class="hljs-string">&quot;answer&quot;</span>:<span class="hljs-string">&quot;AVERAGE &gt; 36542&quot;</span>,<span class="hljs-string">&quot;coordinates&quot;</span>:[[0,1]],<span class="hljs-string">&quot;cells&quot;</span>:[<span class="hljs-string">&quot;36542&quot;</span>],<span class="hljs-string">&quot;aggregator&quot;</span>:<span class="hljs-string">&quot;AVERAGE&quot;</span>}`}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p:P,i(s){p||(E(n.$$.fragment,s),p=!0)},o(s){w(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function pL(_){let n,p;return n=new R({props:{$$slots:{default:[uL]},$$scope:{ctx:_}}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){p||(E(n.$$.fragment,s),p=!0)},o(s){w(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function cL(_){let n,p;return n=new N({props:{code:`self.assertEqual(
    data,
    {
        "answer": "AVERAGE > 36542",
        "coordinates": [[0, 1]],
        "cells": ["36542"],
        "aggregator": "AVERAGE",
    },
)`,highlighted:`self.assertEqual(
    data,
    {
        <span class="hljs-string">&quot;answer&quot;</span>: <span class="hljs-string">&quot;AVERAGE &gt; 36542&quot;</span>,
        <span class="hljs-string">&quot;coordinates&quot;</span>: [[<span class="hljs-number">0</span>, <span class="hljs-number">1</span>]],
        <span class="hljs-string">&quot;cells&quot;</span>: [<span class="hljs-string">&quot;36542&quot;</span>],
        <span class="hljs-string">&quot;aggregator&quot;</span>: <span class="hljs-string">&quot;AVERAGE&quot;</span>,
    },
)`}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p:P,i(s){p||(E(n.$$.fragment,s),p=!0)},o(s){w(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function fL(_){let n,p;return n=new R({props:{$$slots:{default:[cL]},$$scope:{ctx:_}}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){p||(E(n.$$.fragment,s),p=!0)},o(s){w(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function hL(_){let n,p,s,d,$,k,A;return{c(){n=r("p"),p=r("strong"),s=i("Recommended model"),d=i(`:
`),$=r("a"),k=i("deepset/roberta-base-squad2"),A=i("."),this.h()},l(T){n=o(T,"P",{});var j=l(n);p=o(j,"STRONG",{});var O=l(p);s=u(O,"Recommended model"),O.forEach(t),d=u(j,`:
`),$=o(j,"A",{href:!0,rel:!0});var D=l($);k=u(D,"deepset/roberta-base-squad2"),D.forEach(t),A=u(j,"."),j.forEach(t),this.h()},h(){h($,"href","https://huggingface.co/deepset/roberta-base-squad2"),h($,"rel","nofollow")},m(T,j){m(T,n,j),e(n,p),e(p,s),e(n,d),e(n,$),e($,k),e(n,A)},d(T){T&&t(n)}}}function dL(_){let n,p;return n=new N({props:{code:`import json
import requests
headers = {"Authorization": f"Bearer {API_TOKEN}"}
API_URL = "https://api-inference.huggingface.co/models/deepset/roberta-base-squad2"
def query(payload):
    data = json.dumps(payload)
    response = requests.request("POST", API_URL, headers=headers, data=data)
    return json.loads(response.content.decode("utf-8"))
data = query(
    {
        "inputs": {
            "question": "What's my name?",
            "context": "My name is Clara and I live in Berkeley.",
        }
    }
)`,highlighted:`<span class="hljs-keyword">import</span> json
<span class="hljs-keyword">import</span> requests
headers = {<span class="hljs-string">&quot;Authorization&quot;</span>: <span class="hljs-string">f&quot;Bearer <span class="hljs-subst">{API_TOKEN}</span>&quot;</span>}
API_URL = <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/deepset/roberta-base-squad2&quot;</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">payload</span>):
    data = json.dumps(payload)
    response = requests.request(<span class="hljs-string">&quot;POST&quot;</span>, API_URL, headers=headers, data=data)
    <span class="hljs-keyword">return</span> json.loads(response.content.decode(<span class="hljs-string">&quot;utf-8&quot;</span>))
data = query(
    {
        <span class="hljs-string">&quot;inputs&quot;</span>: {
            <span class="hljs-string">&quot;question&quot;</span>: <span class="hljs-string">&quot;What&#x27;s my name?&quot;</span>,
            <span class="hljs-string">&quot;context&quot;</span>: <span class="hljs-string">&quot;My name is Clara and I live in Berkeley.&quot;</span>,
        }
    }
)`}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p:P,i(s){p||(E(n.$$.fragment,s),p=!0)},o(s){w(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function gL(_){let n,p;return n=new R({props:{$$slots:{default:[dL]},$$scope:{ctx:_}}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){p||(E(n.$$.fragment,s),p=!0)},o(s){w(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function mL(_){let n,p;return n=new N({props:{code:`import fetch from "node-fetch";
async function query(data) {
    const response = await fetch(
        "https://api-inference.huggingface.co/models/deepset/roberta-base-squad2",
        {
            headers: { Authorization: \`Bearer $}{API_TOKEN}\` },
            method: "POST",
            body: JSON.stringify(data),
        }
    );
    const result = await response.json();
    return result;
}
query({inputs:{question:"What is my name?",context:"My name is Clara and I live in Berkeley."}}).then((response) => {
    console.log(JSON.stringify(response));
});
// {"score":0.933128833770752,"start":11,"end":16,"answer":"Clara"}`,highlighted:`<span class="hljs-keyword">import</span> fetch <span class="hljs-keyword">from</span> <span class="hljs-string">&quot;node-fetch&quot;</span>;
<span class="hljs-keyword">async</span> <span class="hljs-keyword">function</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">data</span>) {
    <span class="hljs-keyword">const</span> response = <span class="hljs-keyword">await</span> <span class="hljs-title function_">fetch</span>(
        <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/deepset/roberta-base-squad2&quot;</span>,
        {
            <span class="hljs-attr">headers</span>: { <span class="hljs-title class_">Authorization</span>: <span class="hljs-string">\`Bearer <span class="hljs-subst">$}{API_TOKEN}</span>\`</span> },
            <span class="hljs-attr">method</span>: <span class="hljs-string">&quot;POST&quot;</span>,
            <span class="hljs-attr">body</span>: <span class="hljs-title class_">JSON</span>.<span class="hljs-title function_">stringify</span>(data),
        }
    );
    <span class="hljs-keyword">const</span> result = <span class="hljs-keyword">await</span> response.<span class="hljs-title function_">json</span>();
    <span class="hljs-keyword">return</span> result;
}
<span class="hljs-title function_">query</span>({<span class="hljs-attr">inputs</span>:{<span class="hljs-attr">question</span>:<span class="hljs-string">&quot;What is my name?&quot;</span>,<span class="hljs-attr">context</span>:<span class="hljs-string">&quot;My name is Clara and I live in Berkeley.&quot;</span>}}).<span class="hljs-title function_">then</span>(<span class="hljs-function">(<span class="hljs-params">response</span>) =&gt;</span> {
    <span class="hljs-variable language_">console</span>.<span class="hljs-title function_">log</span>(<span class="hljs-title class_">JSON</span>.<span class="hljs-title function_">stringify</span>(response));
});
<span class="hljs-comment">// {&quot;score&quot;:0.933128833770752,&quot;start&quot;:11,&quot;end&quot;:16,&quot;answer&quot;:&quot;Clara&quot;}</span>`}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p:P,i(s){p||(E(n.$$.fragment,s),p=!0)},o(s){w(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function $L(_){let n,p;return n=new R({props:{$$slots:{default:[mL]},$$scope:{ctx:_}}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){p||(E(n.$$.fragment,s),p=!0)},o(s){w(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function _L(_){let n,p;return n=new N({props:{code:`curl https://api-inference.huggingface.co/models/deepset/roberta-base-squad2 \\
        -X POST \\
        -d '{"inputs":{"question":"What is my name?","context":"My name is Clara and I live in Berkeley."}}' \\
        -H "Authorization: Bearer $}{HF_API_TOKEN}"
# {"score":0.933128833770752,"start":11,"end":16,"answer":"Clara"}`,highlighted:`curl https:<span class="hljs-comment">//api-inference.huggingface.co/models/deepset/roberta-base-squad2 \\</span>
        -X <span class="hljs-keyword">POST</span> \\
        -<span class="hljs-keyword">d</span> &#x27;{<span class="hljs-string">&quot;inputs&quot;</span>:{<span class="hljs-string">&quot;question&quot;</span>:<span class="hljs-string">&quot;What is my name?&quot;</span>,<span class="hljs-string">&quot;context&quot;</span>:<span class="hljs-string">&quot;My name is Clara and I live in Berkeley.&quot;</span>}}&#x27; \\
        -<span class="hljs-keyword">H</span> <span class="hljs-string">&quot;Authorization: Bearer $}{HF_API_TOKEN}&quot;</span>
# {<span class="hljs-string">&quot;score&quot;</span>:0.933128833770752,<span class="hljs-string">&quot;start&quot;</span>:11,<span class="hljs-string">&quot;end&quot;</span>:16,<span class="hljs-string">&quot;answer&quot;</span>:<span class="hljs-string">&quot;Clara&quot;</span>}`}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p:P,i(s){p||(E(n.$$.fragment,s),p=!0)},o(s){w(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function qL(_){let n,p;return n=new R({props:{$$slots:{default:[_L]},$$scope:{ctx:_}}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){p||(E(n.$$.fragment,s),p=!0)},o(s){w(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function vL(_){let n,p;return n=new N({props:{code:`self.assertEqual(
    deep_round(data),
    {"score": 0.9327, "start": 11, "end": 16, "answer": "Clara"},
)`,highlighted:`self.assertEqual(
    deep_round(data),
    {<span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.9327</span>, <span class="hljs-string">&quot;start&quot;</span>: <span class="hljs-number">11</span>, <span class="hljs-string">&quot;end&quot;</span>: <span class="hljs-number">16</span>, <span class="hljs-string">&quot;answer&quot;</span>: <span class="hljs-string">&quot;Clara&quot;</span>},
)`}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p:P,i(s){p||(E(n.$$.fragment,s),p=!0)},o(s){w(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function yL(_){let n,p;return n=new R({props:{$$slots:{default:[vL]},$$scope:{ctx:_}}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){p||(E(n.$$.fragment,s),p=!0)},o(s){w(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function EL(_){let n,p,s,d,$,k;return{c(){n=r("p"),p=r("strong"),s=i("Recommended model"),d=i(`:
`),$=r("a"),k=i("distilbert-base-uncased-finetuned-sst-2-english"),this.h()},l(A){n=o(A,"P",{});var T=l(n);p=o(T,"STRONG",{});var j=l(p);s=u(j,"Recommended model"),j.forEach(t),d=u(T,`:
`),$=o(T,"A",{href:!0,rel:!0});var O=l($);k=u(O,"distilbert-base-uncased-finetuned-sst-2-english"),O.forEach(t),T.forEach(t),this.h()},h(){h($,"href","https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english"),h($,"rel","nofollow")},m(A,T){m(A,n,T),e(n,p),e(p,s),e(n,d),e(n,$),e($,k)},d(A){A&&t(n)}}}function wL(_){let n,p;return n=new N({props:{code:`import json
import requests
headers = {"Authorization": f"Bearer {API_TOKEN}"}
API_URL = "https://api-inference.huggingface.co/models/distilbert-base-uncased-finetuned-sst-2-english"
def query(payload):
    data = json.dumps(payload)
    response = requests.request("POST", API_URL, headers=headers, data=data)
    return json.loads(response.content.decode("utf-8"))
data = query({"inputs": "I like you. I love you"})`,highlighted:`<span class="hljs-keyword">import</span> json
<span class="hljs-keyword">import</span> requests
headers = {<span class="hljs-string">&quot;Authorization&quot;</span>: <span class="hljs-string">f&quot;Bearer <span class="hljs-subst">{API_TOKEN}</span>&quot;</span>}
API_URL = <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/distilbert-base-uncased-finetuned-sst-2-english&quot;</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">payload</span>):
    data = json.dumps(payload)
    response = requests.request(<span class="hljs-string">&quot;POST&quot;</span>, API_URL, headers=headers, data=data)
    <span class="hljs-keyword">return</span> json.loads(response.content.decode(<span class="hljs-string">&quot;utf-8&quot;</span>))
data = query({<span class="hljs-string">&quot;inputs&quot;</span>: <span class="hljs-string">&quot;I like you. I love you&quot;</span>})`}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p:P,i(s){p||(E(n.$$.fragment,s),p=!0)},o(s){w(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function bL(_){let n,p;return n=new R({props:{$$slots:{default:[wL]},$$scope:{ctx:_}}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){p||(E(n.$$.fragment,s),p=!0)},o(s){w(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function jL(_){let n,p;return n=new N({props:{code:`import fetch from "node-fetch";
async function query(data) {
    const response = await fetch(
        "https://api-inference.huggingface.co/models/distilbert-base-uncased-finetuned-sst-2-english",
        {
            headers: { Authorization: \`Bearer $}{API_TOKEN}\` },
            method: "POST",
            body: JSON.stringify(data),
        }
    );
    const result = await response.json();
    return result;
}
query({inputs:"I like you. I love you"}).then((response) => {
    console.log(JSON.stringify(response));
});
// [[{"label":"NEGATIVE","score":0.0001261125144083053},{"label":"POSITIVE","score":0.9998738765716553}]]`,highlighted:`<span class="hljs-keyword">import</span> fetch <span class="hljs-keyword">from</span> <span class="hljs-string">&quot;node-fetch&quot;</span>;
<span class="hljs-keyword">async</span> <span class="hljs-keyword">function</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">data</span>) {
    <span class="hljs-keyword">const</span> response = <span class="hljs-keyword">await</span> <span class="hljs-title function_">fetch</span>(
        <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/distilbert-base-uncased-finetuned-sst-2-english&quot;</span>,
        {
            <span class="hljs-attr">headers</span>: { <span class="hljs-title class_">Authorization</span>: <span class="hljs-string">\`Bearer <span class="hljs-subst">$}{API_TOKEN}</span>\`</span> },
            <span class="hljs-attr">method</span>: <span class="hljs-string">&quot;POST&quot;</span>,
            <span class="hljs-attr">body</span>: <span class="hljs-title class_">JSON</span>.<span class="hljs-title function_">stringify</span>(data),
        }
    );
    <span class="hljs-keyword">const</span> result = <span class="hljs-keyword">await</span> response.<span class="hljs-title function_">json</span>();
    <span class="hljs-keyword">return</span> result;
}
<span class="hljs-title function_">query</span>({<span class="hljs-attr">inputs</span>:<span class="hljs-string">&quot;I like you. I love you&quot;</span>}).<span class="hljs-title function_">then</span>(<span class="hljs-function">(<span class="hljs-params">response</span>) =&gt;</span> {
    <span class="hljs-variable language_">console</span>.<span class="hljs-title function_">log</span>(<span class="hljs-title class_">JSON</span>.<span class="hljs-title function_">stringify</span>(response));
});
<span class="hljs-comment">// [[{&quot;label&quot;:&quot;NEGATIVE&quot;,&quot;score&quot;:0.0001261125144083053},{&quot;label&quot;:&quot;POSITIVE&quot;,&quot;score&quot;:0.9998738765716553}]]</span>`}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p:P,i(s){p||(E(n.$$.fragment,s),p=!0)},o(s){w(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function TL(_){let n,p;return n=new R({props:{$$slots:{default:[jL]},$$scope:{ctx:_}}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){p||(E(n.$$.fragment,s),p=!0)},o(s){w(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function kL(_){let n,p;return n=new N({props:{code:`curl https://api-inference.huggingface.co/models/distilbert-base-uncased-finetuned-sst-2-english \\
        -X POST \\
        -d '{"inputs":"I like you. I love you"}' \\
        -H "Authorization: Bearer $}{HF_API_TOKEN}"
# [[{"label":"NEGATIVE","score":0.0001261125144083053},{"label":"POSITIVE","score":0.9998738765716553}]]`,highlighted:`curl https:<span class="hljs-comment">//api-inference.huggingface.co/models/distilbert-base-uncased-finetuned-sst-2-english \\
        -X POST \\
        -d &#x27;{&quot;inputs&quot;:&quot;I like you. I love you&quot;}&#x27; \\
        -H &quot;Authorization: Bearer $}{HF_API_TOKEN}&quot;</span>
# [[{<span class="hljs-string">&quot;label&quot;</span>:<span class="hljs-string">&quot;NEGATIVE&quot;</span>,<span class="hljs-string">&quot;score&quot;</span>:<span class="hljs-number">0.0001261125144083053</span>},{<span class="hljs-string">&quot;label&quot;</span>:<span class="hljs-string">&quot;POSITIVE&quot;</span>,<span class="hljs-string">&quot;score&quot;</span>:<span class="hljs-number">0.9998738765716553</span>}]]`}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p:P,i(s){p||(E(n.$$.fragment,s),p=!0)},o(s){w(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function AL(_){let n,p;return n=new R({props:{$$slots:{default:[kL]},$$scope:{ctx:_}}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){p||(E(n.$$.fragment,s),p=!0)},o(s){w(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function DL(_){let n,p;return n=new N({props:{code:`self.assertEqual(
    deep_round(data),
    [
        [
            {"label": "NEGATIVE", "score": 0.0001},
            {"label": "POSITIVE", "score": 0.9999},
        ]
    ],
)`,highlighted:`self.assertEqual(
    deep_round(data),
    [
        [
            {<span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;NEGATIVE&quot;</span>, <span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.0001</span>},
            {<span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;POSITIVE&quot;</span>, <span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.9999</span>},
        ]
    ],
)`}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p:P,i(s){p||(E(n.$$.fragment,s),p=!0)},o(s){w(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function OL(_){let n,p;return n=new R({props:{$$slots:{default:[DL]},$$scope:{ctx:_}}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){p||(E(n.$$.fragment,s),p=!0)},o(s){w(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function PL(_){let n,p,s,d,$,k;return{c(){n=r("p"),p=r("strong"),s=i("Recommended model"),d=i(`:
`),$=r("a"),k=i("dbmdz/bert-large-cased-finetuned-conll03-english"),this.h()},l(A){n=o(A,"P",{});var T=l(n);p=o(T,"STRONG",{});var j=l(p);s=u(j,"Recommended model"),j.forEach(t),d=u(T,`:
`),$=o(T,"A",{href:!0,rel:!0});var O=l($);k=u(O,"dbmdz/bert-large-cased-finetuned-conll03-english"),O.forEach(t),T.forEach(t),this.h()},h(){h($,"href","https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english"),h($,"rel","nofollow")},m(A,T){m(A,n,T),e(n,p),e(p,s),e(n,d),e(n,$),e($,k)},d(A){A&&t(n)}}}function RL(_){let n,p;return n=new N({props:{code:`import json
import requests
headers = {"Authorization": f"Bearer {API_TOKEN}"}
API_URL = "https://api-inference.huggingface.co/models/dbmdz/bert-large-cased-finetuned-conll03-english"
def query(payload):
    data = json.dumps(payload)
    response = requests.request("POST", API_URL, headers=headers, data=data)
    return json.loads(response.content.decode("utf-8"))
data = query({"inputs": "My name is Sarah Jessica Parker but you can call me Jessica"})`,highlighted:`<span class="hljs-keyword">import</span> json
<span class="hljs-keyword">import</span> requests
headers = {<span class="hljs-string">&quot;Authorization&quot;</span>: <span class="hljs-string">f&quot;Bearer <span class="hljs-subst">{API_TOKEN}</span>&quot;</span>}
API_URL = <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/dbmdz/bert-large-cased-finetuned-conll03-english&quot;</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">payload</span>):
    data = json.dumps(payload)
    response = requests.request(<span class="hljs-string">&quot;POST&quot;</span>, API_URL, headers=headers, data=data)
    <span class="hljs-keyword">return</span> json.loads(response.content.decode(<span class="hljs-string">&quot;utf-8&quot;</span>))
data = query({<span class="hljs-string">&quot;inputs&quot;</span>: <span class="hljs-string">&quot;My name is Sarah Jessica Parker but you can call me Jessica&quot;</span>})`}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p:P,i(s){p||(E(n.$$.fragment,s),p=!0)},o(s){w(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function NL(_){let n,p;return n=new R({props:{$$slots:{default:[RL]},$$scope:{ctx:_}}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){p||(E(n.$$.fragment,s),p=!0)},o(s){w(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function xL(_){let n,p;return n=new N({props:{code:`import fetch from "node-fetch";
async function query(data) {
    const response = await fetch(
        "https://api-inference.huggingface.co/models/dbmdz/bert-large-cased-finetuned-conll03-english",
        {
            headers: { Authorization: \`Bearer $}{API_TOKEN}\` },
            method: "POST",
            body: JSON.stringify(data),
        }
    );
    const result = await response.json();
    return result;
}
query({inputs:"My name is Sarah Jessica Parker but you can call me Jessica"}).then((response) => {
    console.log(JSON.stringify(response));
});
// [{"entity_group":"PER","score":0.9991337060928345,"word":"Sarah Jessica Parker","start":11,"end":31},{"entity_group":"PER","score":0.9979912042617798,"word":"Jessica","start":52,"end":59}]`,highlighted:`<span class="hljs-keyword">import</span> <span class="hljs-keyword">fetch</span> <span class="hljs-keyword">from</span> &quot;node-fetch&quot;;
async <span class="hljs-keyword">function</span> query(data) {
    const response = await <span class="hljs-keyword">fetch</span>(
        &quot;https://api-inference.huggingface.co/models/dbmdz/bert-large-cased-finetuned-conll03-english&quot;,
        {
            headers: { <span class="hljs-keyword">Authorization</span>: \`Bearer $}{API_TOKEN}\` },
            <span class="hljs-keyword">method</span>: &quot;POST&quot;,
            body: <span class="hljs-type">JSON</span>.stringify(data),
        }
    );
    const result = await response.json();
    <span class="hljs-keyword">return</span> result;
}
query({inputs:&quot;My name is Sarah Jessica Parker but you can call me Jessica&quot;}).<span class="hljs-keyword">then</span>((response) =&gt; {
    console.log(<span class="hljs-type">JSON</span>.stringify(response));
});
// [{&quot;entity_group&quot;:&quot;PER&quot;,&quot;score&quot;:<span class="hljs-number">0.9991337060928345</span>,&quot;word&quot;:&quot;Sarah Jessica Parker&quot;,&quot;start&quot;:<span class="hljs-number">11</span>,&quot;end&quot;:<span class="hljs-number">31</span>},{&quot;entity_group&quot;:&quot;PER&quot;,&quot;score&quot;:<span class="hljs-number">0.9979912042617798</span>,&quot;word&quot;:&quot;Jessica&quot;,&quot;start&quot;:<span class="hljs-number">52</span>,&quot;end&quot;:<span class="hljs-number">59</span>}]`}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p:P,i(s){p||(E(n.$$.fragment,s),p=!0)},o(s){w(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function SL(_){let n,p;return n=new R({props:{$$slots:{default:[xL]},$$scope:{ctx:_}}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){p||(E(n.$$.fragment,s),p=!0)},o(s){w(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function IL(_){let n,p;return n=new N({props:{code:`curl https://api-inference.huggingface.co/models/dbmdz/bert-large-cased-finetuned-conll03-english \\
        -X POST \\
        -d '{"inputs":"My name is Sarah Jessica Parker but you can call me Jessica"}' \\
        -H "Authorization: Bearer $}{HF_API_TOKEN}"
# [{"entity_group":"PER","score":0.9991337060928345,"word":"Sarah Jessica Parker","start":11,"end":31},{"entity_group":"PER","score":0.9979912042617798,"word":"Jessica","start":52,"end":59}]`,highlighted:`curl https:<span class="hljs-comment">//api-inference.huggingface.co/models/dbmdz/bert-large-cased-finetuned-conll03-english \\
        -X POST \\
        -d &#x27;{&quot;inputs&quot;:&quot;My name is Sarah Jessica Parker but you can call me Jessica&quot;}&#x27; \\
        -H &quot;Authorization: Bearer $}{HF_API_TOKEN}&quot;</span>
# [{<span class="hljs-string">&quot;entity_group&quot;</span>:<span class="hljs-string">&quot;PER&quot;</span>,<span class="hljs-string">&quot;score&quot;</span>:<span class="hljs-number">0.9991337060928345</span>,<span class="hljs-string">&quot;word&quot;</span>:<span class="hljs-string">&quot;Sarah Jessica Parker&quot;</span>,<span class="hljs-string">&quot;start&quot;</span>:<span class="hljs-number">11</span>,<span class="hljs-string">&quot;end&quot;</span>:<span class="hljs-number">31</span>},{<span class="hljs-string">&quot;entity_group&quot;</span>:<span class="hljs-string">&quot;PER&quot;</span>,<span class="hljs-string">&quot;score&quot;</span>:<span class="hljs-number">0.9979912042617798</span>,<span class="hljs-string">&quot;word&quot;</span>:<span class="hljs-string">&quot;Jessica&quot;</span>,<span class="hljs-string">&quot;start&quot;</span>:<span class="hljs-number">52</span>,<span class="hljs-string">&quot;end&quot;</span>:<span class="hljs-number">59</span>}]`}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p:P,i(s){p||(E(n.$$.fragment,s),p=!0)},o(s){w(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function HL(_){let n,p;return n=new R({props:{$$slots:{default:[IL]},$$scope:{ctx:_}}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){p||(E(n.$$.fragment,s),p=!0)},o(s){w(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function BL(_){let n,p;return n=new N({props:{code:`self.assertEqual(
    deep_round(data),
    [
        {
            "entity_group": "PER",
            "score": 0.9991,
            "word": "Sarah Jessica Parker",
            "start": 11,
            "end": 31,
        },
        {
            "entity_group": "PER",
            "score": 0.998,
            "word": "Jessica",
            "start": 52,
            "end": 59,
        },
    ],
)`,highlighted:`self.assertEqual(
    deep_round(data),
    [
        {
            <span class="hljs-string">&quot;entity_group&quot;</span>: <span class="hljs-string">&quot;PER&quot;</span>,
            <span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.9991</span>,
            <span class="hljs-string">&quot;word&quot;</span>: <span class="hljs-string">&quot;Sarah Jessica Parker&quot;</span>,
            <span class="hljs-string">&quot;start&quot;</span>: <span class="hljs-number">11</span>,
            <span class="hljs-string">&quot;end&quot;</span>: <span class="hljs-number">31</span>,
        },
        {
            <span class="hljs-string">&quot;entity_group&quot;</span>: <span class="hljs-string">&quot;PER&quot;</span>,
            <span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.998</span>,
            <span class="hljs-string">&quot;word&quot;</span>: <span class="hljs-string">&quot;Jessica&quot;</span>,
            <span class="hljs-string">&quot;start&quot;</span>: <span class="hljs-number">52</span>,
            <span class="hljs-string">&quot;end&quot;</span>: <span class="hljs-number">59</span>,
        },
    ],
)`}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p:P,i(s){p||(E(n.$$.fragment,s),p=!0)},o(s){w(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function CL(_){let n,p;return n=new R({props:{$$slots:{default:[BL]},$$scope:{ctx:_}}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){p||(E(n.$$.fragment,s),p=!0)},o(s){w(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function GL(_){let n,p,s,d,$,k,A;return{c(){n=r("p"),p=r("strong"),s=i("Recommended model"),d=i(": "),$=r("a"),k=i("gpt2"),A=i(" (it\u2019s a simple model, but fun to play with)."),this.h()},l(T){n=o(T,"P",{});var j=l(n);p=o(j,"STRONG",{});var O=l(p);s=u(O,"Recommended model"),O.forEach(t),d=u(j,": "),$=o(j,"A",{href:!0,rel:!0});var D=l($);k=u(D,"gpt2"),D.forEach(t),A=u(j," (it\u2019s a simple model, but fun to play with)."),j.forEach(t),this.h()},h(){h($,"href","https://huggingface.co/gpt2"),h($,"rel","nofollow")},m(T,j){m(T,n,j),e(n,p),e(p,s),e(n,d),e(n,$),e($,k),e(n,A)},d(T){T&&t(n)}}}function UL(_){let n,p;return n=new N({props:{code:`import json
import requests
headers = {"Authorization": f"Bearer {API_TOKEN}"}
API_URL = "https://api-inference.huggingface.co/models/gpt2"
def query(payload):
    data = json.dumps(payload)
    response = requests.request("POST", API_URL, headers=headers, data=data)
    return json.loads(response.content.decode("utf-8"))
data = query({"inputs": "The answer to the universe is"})`,highlighted:`<span class="hljs-keyword">import</span> json
<span class="hljs-keyword">import</span> requests
headers = {<span class="hljs-string">&quot;Authorization&quot;</span>: <span class="hljs-string">f&quot;Bearer <span class="hljs-subst">{API_TOKEN}</span>&quot;</span>}
API_URL = <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/gpt2&quot;</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">payload</span>):
    data = json.dumps(payload)
    response = requests.request(<span class="hljs-string">&quot;POST&quot;</span>, API_URL, headers=headers, data=data)
    <span class="hljs-keyword">return</span> json.loads(response.content.decode(<span class="hljs-string">&quot;utf-8&quot;</span>))
data = query({<span class="hljs-string">&quot;inputs&quot;</span>: <span class="hljs-string">&quot;The answer to the universe is&quot;</span>})`}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p:P,i(s){p||(E(n.$$.fragment,s),p=!0)},o(s){w(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function LL(_){let n,p;return n=new R({props:{$$slots:{default:[UL]},$$scope:{ctx:_}}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){p||(E(n.$$.fragment,s),p=!0)},o(s){w(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function zL(_){let n,p;return n=new N({props:{code:`import fetch from "node-fetch";
async function query(data) {
    const response = await fetch(
        "https://api-inference.huggingface.co/models/gpt2",
        {
            headers: { Authorization: \`Bearer $}{API_TOKEN}\` },
            method: "POST",
            body: JSON.stringify(data),
        }
    );
    const result = await response.json();
    return result;
}
query({inputs:"The answer to the universe is"}).then((response) => {
    console.log(JSON.stringify(response));
});
// [{"generated_text":"The answer to the universe is in a different shape (or shapeless) than"}]`,highlighted:`<span class="hljs-keyword">import</span> fetch <span class="hljs-keyword">from</span> <span class="hljs-string">&quot;node-fetch&quot;</span>;
<span class="hljs-keyword">async</span> <span class="hljs-keyword">function</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">data</span>) {
    <span class="hljs-keyword">const</span> response = <span class="hljs-keyword">await</span> <span class="hljs-title function_">fetch</span>(
        <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/gpt2&quot;</span>,
        {
            <span class="hljs-attr">headers</span>: { <span class="hljs-title class_">Authorization</span>: <span class="hljs-string">\`Bearer <span class="hljs-subst">$}{API_TOKEN}</span>\`</span> },
            <span class="hljs-attr">method</span>: <span class="hljs-string">&quot;POST&quot;</span>,
            <span class="hljs-attr">body</span>: <span class="hljs-title class_">JSON</span>.<span class="hljs-title function_">stringify</span>(data),
        }
    );
    <span class="hljs-keyword">const</span> result = <span class="hljs-keyword">await</span> response.<span class="hljs-title function_">json</span>();
    <span class="hljs-keyword">return</span> result;
}
<span class="hljs-title function_">query</span>({<span class="hljs-attr">inputs</span>:<span class="hljs-string">&quot;The answer to the universe is&quot;</span>}).<span class="hljs-title function_">then</span>(<span class="hljs-function">(<span class="hljs-params">response</span>) =&gt;</span> {
    <span class="hljs-variable language_">console</span>.<span class="hljs-title function_">log</span>(<span class="hljs-title class_">JSON</span>.<span class="hljs-title function_">stringify</span>(response));
});
<span class="hljs-comment">// [{&quot;generated_text&quot;:&quot;The answer to the universe is in a different shape (or shapeless) than&quot;}]</span>`}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p:P,i(s){p||(E(n.$$.fragment,s),p=!0)},o(s){w(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function ML(_){let n,p;return n=new R({props:{$$slots:{default:[zL]},$$scope:{ctx:_}}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){p||(E(n.$$.fragment,s),p=!0)},o(s){w(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function FL(_){let n,p;return n=new N({props:{code:`curl https://api-inference.huggingface.co/models/gpt2 \\
        -X POST \\
        -d '{"inputs":"The answer to the universe is"}' \\
        -H "Authorization: Bearer $}{HF_API_TOKEN}"
# [{"generated_text":"The answer to the universe is in a different shape (or shapeless) than"}]`,highlighted:`curl https:<span class="hljs-regexp">//</span>api-inference.huggingface.co<span class="hljs-regexp">/models/g</span>pt2 \\
        -X POST \\
        -d <span class="hljs-string">&#x27;{&quot;inputs&quot;:&quot;The answer to the universe is&quot;}&#x27;</span> \\
        -H <span class="hljs-string">&quot;Authorization: Bearer $}{HF_API_TOKEN}&quot;</span>
<span class="hljs-comment"># [{&quot;generated_text&quot;:&quot;The answer to the universe is in a different shape (or shapeless) than&quot;}]</span>`}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p:P,i(s){p||(E(n.$$.fragment,s),p=!0)},o(s){w(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function JL(_){let n,p;return n=new R({props:{$$slots:{default:[FL]},$$scope:{ctx:_}}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){p||(E(n.$$.fragment,s),p=!0)},o(s){w(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function KL(_){let n,p;return n=new N({props:{code:`data == [
    {
        "generated_text": 'The answer to the universe is that we are the creation of the entire universe," says Fitch.\\n\\nAs of the 1960s, six times as many Americans still make fewer than six bucks ($}17) per year on their way to retirement.'
    }
]`,highlighted:`data == [
    {
        <span class="hljs-string">&quot;generated_text&quot;</span>: <span class="hljs-string">&#x27;The answer to the universe is that we are the creation of the entire universe,&quot; says Fitch.\\n\\nAs of the 1960s, six times as many Americans still make fewer than six bucks ($}17) per year on their way to retirement.&#x27;</span>
    }
]`}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p:P,i(s){p||(E(n.$$.fragment,s),p=!0)},o(s){w(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function WL(_){let n,p;return n=new R({props:{$$slots:{default:[KL]},$$scope:{ctx:_}}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){p||(E(n.$$.fragment,s),p=!0)},o(s){w(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function YL(_){let n,p,s,d,$,k,A;return{c(){n=r("p"),p=r("strong"),s=i("Recommended model"),d=i(`:
`),$=r("a"),k=i("bert-base-uncased"),A=i(" (it\u2019s a simple model, but fun to play with)."),this.h()},l(T){n=o(T,"P",{});var j=l(n);p=o(j,"STRONG",{});var O=l(p);s=u(O,"Recommended model"),O.forEach(t),d=u(j,`:
`),$=o(j,"A",{href:!0,rel:!0});var D=l($);k=u(D,"bert-base-uncased"),D.forEach(t),A=u(j," (it\u2019s a simple model, but fun to play with)."),j.forEach(t),this.h()},h(){h($,"href","https://huggingface.co/bert-base-uncased"),h($,"rel","nofollow")},m(T,j){m(T,n,j),e(n,p),e(p,s),e(n,d),e(n,$),e($,k),e(n,A)},d(T){T&&t(n)}}}function VL(_){let n,p;return n=new N({props:{code:`import json
import requests
headers = {"Authorization": f"Bearer {API_TOKEN}"}
API_URL = "https://api-inference.huggingface.co/models/bert-base-uncased"
def query(payload):
    data = json.dumps(payload)
    response = requests.request("POST", API_URL, headers=headers, data=data)
    return json.loads(response.content.decode("utf-8"))
data = query({"inputs": "The answer to the universe is [MASK]."})`,highlighted:`<span class="hljs-keyword">import</span> json
<span class="hljs-keyword">import</span> requests
headers = {<span class="hljs-string">&quot;Authorization&quot;</span>: <span class="hljs-string">f&quot;Bearer <span class="hljs-subst">{API_TOKEN}</span>&quot;</span>}
API_URL = <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/bert-base-uncased&quot;</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">payload</span>):
    data = json.dumps(payload)
    response = requests.request(<span class="hljs-string">&quot;POST&quot;</span>, API_URL, headers=headers, data=data)
    <span class="hljs-keyword">return</span> json.loads(response.content.decode(<span class="hljs-string">&quot;utf-8&quot;</span>))
data = query({<span class="hljs-string">&quot;inputs&quot;</span>: <span class="hljs-string">&quot;The answer to the universe is [MASK].&quot;</span>})`}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p:P,i(s){p||(E(n.$$.fragment,s),p=!0)},o(s){w(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function XL(_){let n,p;return n=new R({props:{$$slots:{default:[VL]},$$scope:{ctx:_}}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){p||(E(n.$$.fragment,s),p=!0)},o(s){w(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function QL(_){let n,p;return n=new N({props:{code:`import fetch from "node-fetch";
async function query(data) {
    const response = await fetch(
        "https://api-inference.huggingface.co/models/bert-base-uncased",
        {
            headers: { Authorization: \`Bearer $}{API_TOKEN}\` },
            method: "POST",
            body: JSON.stringify(data),
        }
    );
    const result = await response.json();
    return result;
}
query({inputs:"The answer to the universe is [MASK]."}).then((response) => {
    console.log(JSON.stringify(response));
});
// [{"sequence":"the answer to the universe is no.","score":0.16963955760002136,"token":2053,"token_str":"no"},{"sequence":"the answer to the universe is nothing.","score":0.07344776391983032,"token":2498,"token_str":"nothing"},{"sequence":"the answer to the universe is yes.","score":0.05803241208195686,"token":2748,"token_str":"yes"},{"sequence":"the answer to the universe is unknown.","score":0.043957844376564026,"token":4242,"token_str":"unknown"},{"sequence":"the answer to the universe is simple.","score":0.04015745222568512,"token":3722,"token_str":"simple"}]`,highlighted:`<span class="hljs-keyword">import</span> <span class="hljs-keyword">fetch</span> <span class="hljs-keyword">from</span> &quot;node-fetch&quot;;
async <span class="hljs-keyword">function</span> query(data) {
    const response = await <span class="hljs-keyword">fetch</span>(
        &quot;https://api-inference.huggingface.co/models/bert-base-uncased&quot;,
        {
            headers: { <span class="hljs-keyword">Authorization</span>: \`Bearer $}{API_TOKEN}\` },
            <span class="hljs-keyword">method</span>: &quot;POST&quot;,
            body: <span class="hljs-type">JSON</span>.stringify(data),
        }
    );
    const result = await response.json();
    <span class="hljs-keyword">return</span> result;
}
query({inputs:&quot;The answer to the universe is [MASK].&quot;}).<span class="hljs-keyword">then</span>((response) =&gt; {
    console.log(<span class="hljs-type">JSON</span>.stringify(response));
});
// [{&quot;sequence&quot;:&quot;the answer to the universe is no.&quot;,&quot;score&quot;:<span class="hljs-number">0.16963955760002136</span>,&quot;token&quot;:<span class="hljs-number">2053</span>,&quot;token_str&quot;:&quot;no&quot;},{&quot;sequence&quot;:&quot;the answer to the universe is nothing.&quot;,&quot;score&quot;:<span class="hljs-number">0.07344776391983032</span>,&quot;token&quot;:<span class="hljs-number">2498</span>,&quot;token_str&quot;:&quot;nothing&quot;},{&quot;sequence&quot;:&quot;the answer to the universe is yes.&quot;,&quot;score&quot;:<span class="hljs-number">0.05803241208195686</span>,&quot;token&quot;:<span class="hljs-number">2748</span>,&quot;token_str&quot;:&quot;yes&quot;},{&quot;sequence&quot;:&quot;the answer to the universe is unknown.&quot;,&quot;score&quot;:<span class="hljs-number">0.043957844376564026</span>,&quot;token&quot;:<span class="hljs-number">4242</span>,&quot;token_str&quot;:&quot;unknown&quot;},{&quot;sequence&quot;:&quot;the answer to the universe is simple.&quot;,&quot;score&quot;:<span class="hljs-number">0.04015745222568512</span>,&quot;token&quot;:<span class="hljs-number">3722</span>,&quot;token_str&quot;:&quot;simple&quot;}]`}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p:P,i(s){p||(E(n.$$.fragment,s),p=!0)},o(s){w(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function ZL(_){let n,p;return n=new R({props:{$$slots:{default:[QL]},$$scope:{ctx:_}}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){p||(E(n.$$.fragment,s),p=!0)},o(s){w(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function ez(_){let n,p;return n=new N({props:{code:`curl https://api-inference.huggingface.co/models/bert-base-uncased \\
        -X POST \\
        -d '{"inputs":"The answer to the universe is [MASK]."}' \\
        -H "Authorization: Bearer $}{HF_API_TOKEN}"
# [{"sequence":"the answer to the universe is no.","score":0.16963955760002136,"token":2053,"token_str":"no"},{"sequence":"the answer to the universe is nothing.","score":0.07344776391983032,"token":2498,"token_str":"nothing"},{"sequence":"the answer to the universe is yes.","score":0.05803241208195686,"token":2748,"token_str":"yes"},{"sequence":"the answer to the universe is unknown.","score":0.043957844376564026,"token":4242,"token_str":"unknown"},{"sequence":"the answer to the universe is simple.","score":0.04015745222568512,"token":3722,"token_str":"simple"}]`,highlighted:`curl https://api-inference.huggingface.co/models/bert-base-uncased \\
        -<span class="hljs-type">X</span> <span class="hljs-type">POST</span> \\
        -d <span class="hljs-string">&#x27;{&quot;inputs&quot;:&quot;The answer to the universe is [MASK].&quot;}&#x27;</span> \\
        -<span class="hljs-type">H</span> <span class="hljs-comment">&quot;Authorization: Bearer $}{HF_API_TOKEN}&quot;</span>
# [{<span class="hljs-comment">&quot;sequence&quot;</span>:<span class="hljs-comment">&quot;the answer to the universe is no.&quot;</span>,<span class="hljs-comment">&quot;score&quot;</span>:<span class="hljs-number">0.16963955760002136</span>,<span class="hljs-comment">&quot;token&quot;</span>:<span class="hljs-number">2053</span>,<span class="hljs-comment">&quot;token_str&quot;</span>:<span class="hljs-comment">&quot;no&quot;</span>},{<span class="hljs-comment">&quot;sequence&quot;</span>:<span class="hljs-comment">&quot;the answer to the universe is nothing.&quot;</span>,<span class="hljs-comment">&quot;score&quot;</span>:<span class="hljs-number">0.07344776391983032</span>,<span class="hljs-comment">&quot;token&quot;</span>:<span class="hljs-number">2498</span>,<span class="hljs-comment">&quot;token_str&quot;</span>:<span class="hljs-comment">&quot;nothing&quot;</span>},{<span class="hljs-comment">&quot;sequence&quot;</span>:<span class="hljs-comment">&quot;the answer to the universe is yes.&quot;</span>,<span class="hljs-comment">&quot;score&quot;</span>:<span class="hljs-number">0.05803241208195686</span>,<span class="hljs-comment">&quot;token&quot;</span>:<span class="hljs-number">2748</span>,<span class="hljs-comment">&quot;token_str&quot;</span>:<span class="hljs-comment">&quot;yes&quot;</span>},{<span class="hljs-comment">&quot;sequence&quot;</span>:<span class="hljs-comment">&quot;the answer to the universe is unknown.&quot;</span>,<span class="hljs-comment">&quot;score&quot;</span>:<span class="hljs-number">0.043957844376564026</span>,<span class="hljs-comment">&quot;token&quot;</span>:<span class="hljs-number">4242</span>,<span class="hljs-comment">&quot;token_str&quot;</span>:<span class="hljs-comment">&quot;unknown&quot;</span>},{<span class="hljs-comment">&quot;sequence&quot;</span>:<span class="hljs-comment">&quot;the answer to the universe is simple.&quot;</span>,<span class="hljs-comment">&quot;score&quot;</span>:<span class="hljs-number">0.04015745222568512</span>,<span class="hljs-comment">&quot;token&quot;</span>:<span class="hljs-number">3722</span>,<span class="hljs-comment">&quot;token_str&quot;</span>:<span class="hljs-comment">&quot;simple&quot;</span>}]`}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p:P,i(s){p||(E(n.$$.fragment,s),p=!0)},o(s){w(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function tz(_){let n,p;return n=new R({props:{$$slots:{default:[ez]},$$scope:{ctx:_}}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){p||(E(n.$$.fragment,s),p=!0)},o(s){w(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function sz(_){let n,p;return n=new N({props:{code:`self.assertEqual(
    deep_round(data),
    [
        {
            "sequence": "the answer to the universe is no.",
            "score": 0.1696,
            "token": 2053,
            "token_str": "no",
        },
        {
            "sequence": "the answer to the universe is nothing.",
            "score": 0.0734,
            "token": 2498,
            "token_str": "nothing",
        },
        {
            "sequence": "the answer to the universe is yes.",
            "score": 0.0580,
            "token": 2748,
            "token_str": "yes",
        },
        {
            "sequence": "the answer to the universe is unknown.",
            "score": 0.044,
            "token": 4242,
            "token_str": "unknown",
        },
        {
            "sequence": "the answer to the universe is simple.",
            "score": 0.0402,
            "token": 3722,
            "token_str": "simple",
        },
    ],
)`,highlighted:`self.assertEqual(
    deep_round(data),
    [
        {
            <span class="hljs-string">&quot;sequence&quot;</span>: <span class="hljs-string">&quot;the answer to the universe is no.&quot;</span>,
            <span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.1696</span>,
            <span class="hljs-string">&quot;token&quot;</span>: <span class="hljs-number">2053</span>,
            <span class="hljs-string">&quot;token_str&quot;</span>: <span class="hljs-string">&quot;no&quot;</span>,
        },
        {
            <span class="hljs-string">&quot;sequence&quot;</span>: <span class="hljs-string">&quot;the answer to the universe is nothing.&quot;</span>,
            <span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.0734</span>,
            <span class="hljs-string">&quot;token&quot;</span>: <span class="hljs-number">2498</span>,
            <span class="hljs-string">&quot;token_str&quot;</span>: <span class="hljs-string">&quot;nothing&quot;</span>,
        },
        {
            <span class="hljs-string">&quot;sequence&quot;</span>: <span class="hljs-string">&quot;the answer to the universe is yes.&quot;</span>,
            <span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.0580</span>,
            <span class="hljs-string">&quot;token&quot;</span>: <span class="hljs-number">2748</span>,
            <span class="hljs-string">&quot;token_str&quot;</span>: <span class="hljs-string">&quot;yes&quot;</span>,
        },
        {
            <span class="hljs-string">&quot;sequence&quot;</span>: <span class="hljs-string">&quot;the answer to the universe is unknown.&quot;</span>,
            <span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.044</span>,
            <span class="hljs-string">&quot;token&quot;</span>: <span class="hljs-number">4242</span>,
            <span class="hljs-string">&quot;token_str&quot;</span>: <span class="hljs-string">&quot;unknown&quot;</span>,
        },
        {
            <span class="hljs-string">&quot;sequence&quot;</span>: <span class="hljs-string">&quot;the answer to the universe is simple.&quot;</span>,
            <span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.0402</span>,
            <span class="hljs-string">&quot;token&quot;</span>: <span class="hljs-number">3722</span>,
            <span class="hljs-string">&quot;token_str&quot;</span>: <span class="hljs-string">&quot;simple&quot;</span>,
        },
    ],
)`}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p:P,i(s){p||(E(n.$$.fragment,s),p=!0)},o(s){w(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function az(_){let n,p;return n=new R({props:{$$slots:{default:[sz]},$$scope:{ctx:_}}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){p||(E(n.$$.fragment,s),p=!0)},o(s){w(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function nz(_){let n,p,s,d,$,k,A;return{c(){n=r("p"),p=r("strong"),s=i("Recommended model"),d=i(": "),$=r("a"),k=i(`Check your
langage`),A=i("."),this.h()},l(T){n=o(T,"P",{});var j=l(n);p=o(j,"STRONG",{});var O=l(p);s=u(O,"Recommended model"),O.forEach(t),d=u(j,": "),$=o(j,"A",{href:!0,rel:!0});var D=l($);k=u(D,`Check your
langage`),D.forEach(t),A=u(j,"."),j.forEach(t),this.h()},h(){h($,"href","https://huggingface.co/models?pipeline_tag=automatic-speech-recognition"),h($,"rel","nofollow")},m(T,j){m(T,n,j),e(n,p),e(p,s),e(n,d),e(n,$),e($,k),e(n,A)},d(T){T&&t(n)}}}function rz(_){let n,p,s,d,$,k,A;return{c(){n=r("p"),p=r("strong"),s=i("English"),d=i(`:
`),$=r("a"),k=i("facebook/wav2vec2-large-960h-lv60-self"),A=i("."),this.h()},l(T){n=o(T,"P",{});var j=l(n);p=o(j,"STRONG",{});var O=l(p);s=u(O,"English"),O.forEach(t),d=u(j,`:
`),$=o(j,"A",{href:!0,rel:!0});var D=l($);k=u(D,"facebook/wav2vec2-large-960h-lv60-self"),D.forEach(t),A=u(j,"."),j.forEach(t),this.h()},h(){h($,"href","https://huggingface.co/facebook/wav2vec2-large-960h-lv60-self"),h($,"rel","nofollow")},m(T,j){m(T,n,j),e(n,p),e(p,s),e(n,d),e(n,$),e($,k),e(n,A)},d(T){T&&t(n)}}}function oz(_){let n,p;return n=new N({props:{code:`    import json
    import requests
    headers = {"Authorization": f"Bearer {API_TOKEN}"}
    API_URL = "https://api-inference.huggingface.co/models/facebook/wav2vec2-base-960h"
    def query(filename):
        with open(filename, "rb") as f:
            data = f.read()
        response = requests.request("POST", API_URL, headers=headers, data=data)
        return json.loads(response.content.decode("utf-8"))
    data = query("sample1.flac")`,highlighted:`    <span class="hljs-keyword">import</span> json
    <span class="hljs-keyword">import</span> requests
    headers = {<span class="hljs-string">&quot;Authorization&quot;</span>: <span class="hljs-string">f&quot;Bearer <span class="hljs-subst">{API_TOKEN}</span>&quot;</span>}
    API_URL = <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/facebook/wav2vec2-base-960h&quot;</span>
    <span class="hljs-keyword">def</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">filename</span>):
        <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(filename, <span class="hljs-string">&quot;rb&quot;</span>) <span class="hljs-keyword">as</span> f:
            data = f.read()
        response = requests.request(<span class="hljs-string">&quot;POST&quot;</span>, API_URL, headers=headers, data=data)
        <span class="hljs-keyword">return</span> json.loads(response.content.decode(<span class="hljs-string">&quot;utf-8&quot;</span>))
    data = query(<span class="hljs-string">&quot;sample1.flac&quot;</span>)`}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p:P,i(s){p||(E(n.$$.fragment,s),p=!0)},o(s){w(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function lz(_){let n,p;return n=new R({props:{$$slots:{default:[oz]},$$scope:{ctx:_}}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){p||(E(n.$$.fragment,s),p=!0)},o(s){w(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function iz(_){let n,p;return n=new N({props:{code:`    import fetch from "node-fetch";
    import fs from "fs";
    async function query(filename) {
        const data = fs.readFileSync(filename);
        const response = await fetch(
            "https://api-inference.huggingface.co/models/facebook/wav2vec2-base-960h",
            {
                headers: { Authorization: \`Bearer $}{API_TOKEN}\` },
                method: "POST",
                body: data,
            }
        );
        const result = await response.json();
        return result;
    }
    query("sample1.flac").then((response) => {
        console.log(JSON.stringify(response));
    });
    // {"text":"GOING ALONG SLUSHY COUNTRY ROADS AND SPEAKING TO DAMP AUDIENCES IN DRAUGHTY SCHOOL ROOMS DAY AFTER DAY FOR A FORTNIGHT HE'LL HAVE TO PUT IN AN APPEARANCE AT SOME PLACE OF WORSHIP ON SUNDAY MORNING AND HE CAN COME TO US IMMEDIATELY AFTERWARDS"}`,highlighted:`    <span class="hljs-keyword">import</span> fetch <span class="hljs-keyword">from</span> <span class="hljs-string">&quot;node-fetch&quot;</span>;
    <span class="hljs-keyword">import</span> fs <span class="hljs-keyword">from</span> <span class="hljs-string">&quot;fs&quot;</span>;
    <span class="hljs-keyword">async</span> <span class="hljs-keyword">function</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">filename</span>) {
        <span class="hljs-keyword">const</span> data = fs.<span class="hljs-title function_">readFileSync</span>(filename);
        <span class="hljs-keyword">const</span> response = <span class="hljs-keyword">await</span> <span class="hljs-title function_">fetch</span>(
            <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/facebook/wav2vec2-base-960h&quot;</span>,
            {
                <span class="hljs-attr">headers</span>: { <span class="hljs-title class_">Authorization</span>: <span class="hljs-string">\`Bearer <span class="hljs-subst">$}{API_TOKEN}</span>\`</span> },
                <span class="hljs-attr">method</span>: <span class="hljs-string">&quot;POST&quot;</span>,
                <span class="hljs-attr">body</span>: data,
            }
        );
        <span class="hljs-keyword">const</span> result = <span class="hljs-keyword">await</span> response.<span class="hljs-title function_">json</span>();
        <span class="hljs-keyword">return</span> result;
    }
    <span class="hljs-title function_">query</span>(<span class="hljs-string">&quot;sample1.flac&quot;</span>).<span class="hljs-title function_">then</span>(<span class="hljs-function">(<span class="hljs-params">response</span>) =&gt;</span> {
        <span class="hljs-variable language_">console</span>.<span class="hljs-title function_">log</span>(<span class="hljs-title class_">JSON</span>.<span class="hljs-title function_">stringify</span>(response));
    });
    <span class="hljs-comment">// {&quot;text&quot;:&quot;GOING ALONG SLUSHY COUNTRY ROADS AND SPEAKING TO DAMP AUDIENCES IN DRAUGHTY SCHOOL ROOMS DAY AFTER DAY FOR A FORTNIGHT HE&#x27;LL HAVE TO PUT IN AN APPEARANCE AT SOME PLACE OF WORSHIP ON SUNDAY MORNING AND HE CAN COME TO US IMMEDIATELY AFTERWARDS&quot;}</span>`}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p:P,i(s){p||(E(n.$$.fragment,s),p=!0)},o(s){w(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function uz(_){let n,p;return n=new R({props:{$$slots:{default:[iz]},$$scope:{ctx:_}}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){p||(E(n.$$.fragment,s),p=!0)},o(s){w(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function pz(_){let n,p;return n=new N({props:{code:`    curl https://api-inference.huggingface.co/models/facebook/wav2vec2-base-960h \\
            -X POST \\
            --data-binary '@sample1.flac' \\
            -H "Authorization: Bearer $}{HF_API_TOKEN}"
    # {"text":"GOING ALONG SLUSHY COUNTRY ROADS AND SPEAKING TO DAMP AUDIENCES IN DRAUGHTY SCHOOL ROOMS DAY AFTER DAY FOR A FORTNIGHT HE'LL HAVE TO PUT IN AN APPEARANCE AT SOME PLACE OF WORSHIP ON SUNDAY MORNING AND HE CAN COME TO US IMMEDIATELY AFTERWARDS"}`,highlighted:`    curl https://api-inference.huggingface.co/models/facebook/wav2vec2-base-<span class="hljs-number">960</span>h \\
            -X POST \\
            --data-binary <span class="hljs-string">&#x27;@sample1.flac&#x27;</span> \\
            -H <span class="hljs-string">&quot;Authorization: Bearer <span class="hljs-variable">$}{HF_API_TOKEN}</span>&quot;</span>
    <span class="hljs-comment"># {&quot;text&quot;:&quot;GOING ALONG SLUSHY COUNTRY ROADS AND SPEAKING TO DAMP AUDIENCES IN DRAUGHTY SCHOOL ROOMS DAY AFTER DAY FOR A FORTNIGHT HE&#x27;LL HAVE TO PUT IN AN APPEARANCE AT SOME PLACE OF WORSHIP ON SUNDAY MORNING AND HE CAN COME TO US IMMEDIATELY AFTERWARDS&quot;}</span>`}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p:P,i(s){p||(E(n.$$.fragment,s),p=!0)},o(s){w(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function cz(_){let n,p;return n=new R({props:{$$slots:{default:[pz]},$$scope:{ctx:_}}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){p||(E(n.$$.fragment,s),p=!0)},o(s){w(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function fz(_){let n,p;return n=new N({props:{code:`    self.assertEqual(
        data,
        {
            "text": "GOING ALONG SLUSHY COUNTRY ROADS AND SPEAKING TO DAMP AUDIENCES IN DRAUGHTY SCHOOL ROOMS DAY AFTER DAY FOR A FORTNIGHT HE'LL HAVE TO PUT IN AN APPEARANCE AT SOME PLACE OF WORSHIP ON SUNDAY MORNING AND HE CAN COME TO US IMMEDIATELY AFTERWARDS"
        },
    )`,highlighted:`    self.assertEqual(
        data,
        {
            <span class="hljs-string">&quot;text&quot;</span>: <span class="hljs-string">&quot;GOING ALONG SLUSHY COUNTRY ROADS AND SPEAKING TO DAMP AUDIENCES IN DRAUGHTY SCHOOL ROOMS DAY AFTER DAY FOR A FORTNIGHT HE&#x27;LL HAVE TO PUT IN AN APPEARANCE AT SOME PLACE OF WORSHIP ON SUNDAY MORNING AND HE CAN COME TO US IMMEDIATELY AFTERWARDS&quot;</span>
        },
    )`}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p:P,i(s){p||(E(n.$$.fragment,s),p=!0)},o(s){w(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function hz(_){let n,p;return n=new R({props:{$$slots:{default:[fz]},$$scope:{ctx:_}}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){p||(E(n.$$.fragment,s),p=!0)},o(s){w(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function dz(_){let n,p,s,d,$,k,A;return{c(){n=r("p"),p=r("strong"),s=i("Recommended model"),d=i(`:
`),$=r("a"),k=i("Sentence-transformers"),A=i("."),this.h()},l(T){n=o(T,"P",{});var j=l(n);p=o(j,"STRONG",{});var O=l(p);s=u(O,"Recommended model"),O.forEach(t),d=u(j,`:
`),$=o(j,"A",{href:!0,rel:!0});var D=l($);k=u(D,"Sentence-transformers"),D.forEach(t),A=u(j,"."),j.forEach(t),this.h()},h(){h($,"href","https://huggingface.co/sentence-transformers/paraphrase-xlm-r-multilingual-v1"),h($,"rel","nofollow")},m(T,j){m(T,n,j),e(n,p),e(p,s),e(n,d),e(n,$),e($,k),e(n,A)},d(T){T&&t(n)}}}function gz(_){let n,p,s,d,$,k,A;return{c(){n=r("p"),p=r("strong"),s=i("Recommended model"),d=i(`:
`),$=r("a"),k=i("superb/hubert-large-superb-er"),A=i("."),this.h()},l(T){n=o(T,"P",{});var j=l(n);p=o(j,"STRONG",{});var O=l(p);s=u(O,"Recommended model"),O.forEach(t),d=u(j,`:
`),$=o(j,"A",{href:!0,rel:!0});var D=l($);k=u(D,"superb/hubert-large-superb-er"),D.forEach(t),A=u(j,"."),j.forEach(t),this.h()},h(){h($,"href","https://huggingface.co/superb/hubert-large-superb-er"),h($,"rel","nofollow")},m(T,j){m(T,n,j),e(n,p),e(p,s),e(n,d),e(n,$),e($,k),e(n,A)},d(T){T&&t(n)}}}function mz(_){let n,p;return n=new N({props:{code:`    import json
    import requests
    headers = {"Authorization": f"Bearer {API_TOKEN}"}
    API_URL = "https://api-inference.huggingface.co/models/superb/hubert-large-superb-er"
    def query(filename):
        with open(filename, "rb") as f:
            data = f.read()
        response = requests.request("POST", API_URL, headers=headers, data=data)
        return json.loads(response.content.decode("utf-8"))
    data = query("sample1.flac")`,highlighted:`    <span class="hljs-keyword">import</span> json
    <span class="hljs-keyword">import</span> requests
    headers = {<span class="hljs-string">&quot;Authorization&quot;</span>: <span class="hljs-string">f&quot;Bearer <span class="hljs-subst">{API_TOKEN}</span>&quot;</span>}
    API_URL = <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/superb/hubert-large-superb-er&quot;</span>
    <span class="hljs-keyword">def</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">filename</span>):
        <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(filename, <span class="hljs-string">&quot;rb&quot;</span>) <span class="hljs-keyword">as</span> f:
            data = f.read()
        response = requests.request(<span class="hljs-string">&quot;POST&quot;</span>, API_URL, headers=headers, data=data)
        <span class="hljs-keyword">return</span> json.loads(response.content.decode(<span class="hljs-string">&quot;utf-8&quot;</span>))
    data = query(<span class="hljs-string">&quot;sample1.flac&quot;</span>)`}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p:P,i(s){p||(E(n.$$.fragment,s),p=!0)},o(s){w(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function $z(_){let n,p;return n=new R({props:{$$slots:{default:[mz]},$$scope:{ctx:_}}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){p||(E(n.$$.fragment,s),p=!0)},o(s){w(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function _z(_){let n,p;return n=new N({props:{code:`    import fetch from "node-fetch";
    import fs from "fs";
    async function query(filename) {
        const data = fs.readFileSync(filename);
        const response = await fetch(
            "https://api-inference.huggingface.co/models/superb/hubert-large-superb-er",
            {
                headers: { Authorization: \`Bearer $}{API_TOKEN}\` },
                method: "POST",
                body: data,
            }
        );
        const result = await response.json();
        return result;
    }
    query("sample1.flac").then((response) => {
        console.log(JSON.stringify(response));
    });
    // [{"score":0.5927661657333374,"label":"neu"},{"score":0.2002529799938202,"label":"hap"},{"score":0.12795612215995789,"label":"ang"},{"score":0.07902472466230392,"label":"sad"}]`,highlighted:`    <span class="hljs-keyword">import</span> <span class="hljs-keyword">fetch</span> <span class="hljs-keyword">from</span> &quot;node-fetch&quot;;
    <span class="hljs-keyword">import</span> fs <span class="hljs-keyword">from</span> &quot;fs&quot;;
    async <span class="hljs-keyword">function</span> query(filename) {
        const data = fs.readFileSync(filename);
        const response = await <span class="hljs-keyword">fetch</span>(
            &quot;https://api-inference.huggingface.co/models/superb/hubert-large-superb-er&quot;,
            {
                headers: { <span class="hljs-keyword">Authorization</span>: \`Bearer $}{API_TOKEN}\` },
                <span class="hljs-keyword">method</span>: &quot;POST&quot;,
                body: data,
            }
        );
        const result = await response.json();
        <span class="hljs-keyword">return</span> result;
    }
    query(&quot;sample1.flac&quot;).<span class="hljs-keyword">then</span>((response) =&gt; {
        console.log(<span class="hljs-type">JSON</span>.stringify(response));
    });
    // [{&quot;score&quot;:<span class="hljs-number">0.5927661657333374</span>,&quot;label&quot;:&quot;neu&quot;},{&quot;score&quot;:<span class="hljs-number">0.2002529799938202</span>,&quot;label&quot;:&quot;hap&quot;},{&quot;score&quot;:<span class="hljs-number">0.12795612215995789</span>,&quot;label&quot;:&quot;ang&quot;},{&quot;score&quot;:<span class="hljs-number">0.07902472466230392</span>,&quot;label&quot;:&quot;sad&quot;}]`}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p:P,i(s){p||(E(n.$$.fragment,s),p=!0)},o(s){w(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function qz(_){let n,p;return n=new R({props:{$$slots:{default:[_z]},$$scope:{ctx:_}}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){p||(E(n.$$.fragment,s),p=!0)},o(s){w(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function vz(_){let n,p;return n=new N({props:{code:`    curl https://api-inference.huggingface.co/models/superb/hubert-large-superb-er \\
            -X POST \\
            --data-binary '@sample1.flac' \\
            -H "Authorization: Bearer $}{HF_API_TOKEN}"
    # [{"score":0.5927661657333374,"label":"neu"},{"score":0.2002529799938202,"label":"hap"},{"score":0.12795612215995789,"label":"ang"},{"score":0.07902472466230392,"label":"sad"}]`,highlighted:`    curl https:<span class="hljs-comment">//api-inference.huggingface.co/models/superb/hubert-large-superb-er \\</span>
            -X POST \\
            --<span class="hljs-keyword">data</span>-binary <span class="hljs-string">&#x27;@sample1.flac&#x27;</span> \\
            -H <span class="hljs-string">&quot;Authorization: Bearer <span class="hljs-subst">$}{HF_API_TOKEN}</span>&quot;</span>
    # [{<span class="hljs-string">&quot;score&quot;</span>:<span class="hljs-number">0.5927661657333374</span>,<span class="hljs-string">&quot;label&quot;</span>:<span class="hljs-string">&quot;neu&quot;</span>},{<span class="hljs-string">&quot;score&quot;</span>:<span class="hljs-number">0.2002529799938202</span>,<span class="hljs-string">&quot;label&quot;</span>:<span class="hljs-string">&quot;hap&quot;</span>},{<span class="hljs-string">&quot;score&quot;</span>:<span class="hljs-number">0.12795612215995789</span>,<span class="hljs-string">&quot;label&quot;</span>:<span class="hljs-string">&quot;ang&quot;</span>},{<span class="hljs-string">&quot;score&quot;</span>:<span class="hljs-number">0.07902472466230392</span>,<span class="hljs-string">&quot;label&quot;</span>:<span class="hljs-string">&quot;sad&quot;</span>}]`}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p:P,i(s){p||(E(n.$$.fragment,s),p=!0)},o(s){w(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function yz(_){let n,p;return n=new R({props:{$$slots:{default:[vz]},$$scope:{ctx:_}}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){p||(E(n.$$.fragment,s),p=!0)},o(s){w(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function Ez(_){let n,p;return n=new N({props:{code:`    self.assertEqual(
        deep_round(data, 4),
        [
            {"score": 0.5928, "label": "neu"},
            {"score": 0.2003, "label": "hap"},
            {"score": 0.128, "label": "ang"},
            {"score": 0.079, "label": "sad"},
        ],
    )`,highlighted:`    self.assertEqual(
        deep_round(data, <span class="hljs-number">4</span>),
        [
            {<span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.5928</span>, <span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;neu&quot;</span>},
            {<span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.2003</span>, <span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;hap&quot;</span>},
            {<span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.128</span>, <span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;ang&quot;</span>},
            {<span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.079</span>, <span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;sad&quot;</span>},
        ],
    )`}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p:P,i(s){p||(E(n.$$.fragment,s),p=!0)},o(s){w(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function wz(_){let n,p;return n=new R({props:{$$slots:{default:[Ez]},$$scope:{ctx:_}}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){p||(E(n.$$.fragment,s),p=!0)},o(s){w(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function bz(_){let n,p,s,d,$,k,A;return{c(){n=r("p"),p=r("strong"),s=i("Recommended model"),d=i(`:
`),$=r("a"),k=i("facebook/detr-resnet-50"),A=i("."),this.h()},l(T){n=o(T,"P",{});var j=l(n);p=o(j,"STRONG",{});var O=l(p);s=u(O,"Recommended model"),O.forEach(t),d=u(j,`:
`),$=o(j,"A",{href:!0,rel:!0});var D=l($);k=u(D,"facebook/detr-resnet-50"),D.forEach(t),A=u(j,"."),j.forEach(t),this.h()},h(){h($,"href","https://huggingface.co/facebook/detr-resnet-50"),h($,"rel","nofollow")},m(T,j){m(T,n,j),e(n,p),e(p,s),e(n,d),e(n,$),e($,k),e(n,A)},d(T){T&&t(n)}}}function jz(_){let n,p;return n=new N({props:{code:`    import json
    import requests
    headers = {"Authorization": f"Bearer {API_TOKEN}"}
    API_URL = "https://api-inference.huggingface.co/models/facebook/detr-resnet-50"
    def query(filename):
        with open(filename, "rb") as f:
            data = f.read()
        response = requests.request("POST", API_URL, headers=headers, data=data)
        return json.loads(response.content.decode("utf-8"))
    data = query("cats.jpg")`,highlighted:`    <span class="hljs-keyword">import</span> json
    <span class="hljs-keyword">import</span> requests
    headers = {<span class="hljs-string">&quot;Authorization&quot;</span>: <span class="hljs-string">f&quot;Bearer <span class="hljs-subst">{API_TOKEN}</span>&quot;</span>}
    API_URL = <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/facebook/detr-resnet-50&quot;</span>
    <span class="hljs-keyword">def</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">filename</span>):
        <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(filename, <span class="hljs-string">&quot;rb&quot;</span>) <span class="hljs-keyword">as</span> f:
            data = f.read()
        response = requests.request(<span class="hljs-string">&quot;POST&quot;</span>, API_URL, headers=headers, data=data)
        <span class="hljs-keyword">return</span> json.loads(response.content.decode(<span class="hljs-string">&quot;utf-8&quot;</span>))
    data = query(<span class="hljs-string">&quot;cats.jpg&quot;</span>)`}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p:P,i(s){p||(E(n.$$.fragment,s),p=!0)},o(s){w(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function Tz(_){let n,p;return n=new R({props:{$$slots:{default:[jz]},$$scope:{ctx:_}}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){p||(E(n.$$.fragment,s),p=!0)},o(s){w(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function kz(_){let n,p;return n=new N({props:{code:`    import fetch from "node-fetch";
    import fs from "fs";
    async function query(filename) {
        const data = fs.readFileSync(filename);
        const response = await fetch(
            "https://api-inference.huggingface.co/models/facebook/detr-resnet-50",
            {
                headers: { Authorization: \`Bearer $}{API_TOKEN}\` },
                method: "POST",
                body: data,
            }
        );
        const result = await response.json();
        return result;
    }
    query("cats.jpg").then((response) => {
        console.log(JSON.stringify(response));
    });
    // [{"score":0.9982201457023621,"label":"remote","box":{"xmin":40,"ymin":70,"xmax":175,"ymax":117}},{"score":0.9960021376609802,"label":"remote","box":{"xmin":333,"ymin":72,"xmax":368,"ymax":187}},{"score":0.9954745173454285,"label":"couch","box":{"xmin":0,"ymin":1,"xmax":639,"ymax":473}},{"score":0.9988006353378296,"label":"cat","box":{"xmin":13,"ymin":52,"xmax":314,"ymax":470}},{"score":0.9986783862113953,"label":"cat","box":{"xmin":345,"ymin":23,"xmax":640,"ymax":368}}]`,highlighted:`    <span class="hljs-keyword">import</span> <span class="hljs-keyword">fetch</span> <span class="hljs-keyword">from</span> &quot;node-fetch&quot;;
    <span class="hljs-keyword">import</span> fs <span class="hljs-keyword">from</span> &quot;fs&quot;;
    async <span class="hljs-keyword">function</span> query(filename) {
        const data = fs.readFileSync(filename);
        const response = await <span class="hljs-keyword">fetch</span>(
            &quot;https://api-inference.huggingface.co/models/facebook/detr-resnet-50&quot;,
            {
                headers: { <span class="hljs-keyword">Authorization</span>: \`Bearer $}{API_TOKEN}\` },
                <span class="hljs-keyword">method</span>: &quot;POST&quot;,
                body: data,
            }
        );
        const result = await response.json();
        <span class="hljs-keyword">return</span> result;
    }
    query(&quot;cats.jpg&quot;).<span class="hljs-keyword">then</span>((response) =&gt; {
        console.log(<span class="hljs-type">JSON</span>.stringify(response));
    });
    // [{&quot;score&quot;:<span class="hljs-number">0.9982201457023621</span>,&quot;label&quot;:&quot;remote&quot;,&quot;box&quot;:{&quot;xmin&quot;:<span class="hljs-number">40</span>,&quot;ymin&quot;:<span class="hljs-number">70</span>,&quot;xmax&quot;:<span class="hljs-number">175</span>,&quot;ymax&quot;:<span class="hljs-number">117</span>}},{&quot;score&quot;:<span class="hljs-number">0.9960021376609802</span>,&quot;label&quot;:&quot;remote&quot;,&quot;box&quot;:{&quot;xmin&quot;:<span class="hljs-number">333</span>,&quot;ymin&quot;:<span class="hljs-number">72</span>,&quot;xmax&quot;:<span class="hljs-number">368</span>,&quot;ymax&quot;:<span class="hljs-number">187</span>}},{&quot;score&quot;:<span class="hljs-number">0.9954745173454285</span>,&quot;label&quot;:&quot;couch&quot;,&quot;box&quot;:{&quot;xmin&quot;:<span class="hljs-number">0</span>,&quot;ymin&quot;:<span class="hljs-number">1</span>,&quot;xmax&quot;:<span class="hljs-number">639</span>,&quot;ymax&quot;:<span class="hljs-number">473</span>}},{&quot;score&quot;:<span class="hljs-number">0.9988006353378296</span>,&quot;label&quot;:&quot;cat&quot;,&quot;box&quot;:{&quot;xmin&quot;:<span class="hljs-number">13</span>,&quot;ymin&quot;:<span class="hljs-number">52</span>,&quot;xmax&quot;:<span class="hljs-number">314</span>,&quot;ymax&quot;:<span class="hljs-number">470</span>}},{&quot;score&quot;:<span class="hljs-number">0.9986783862113953</span>,&quot;label&quot;:&quot;cat&quot;,&quot;box&quot;:{&quot;xmin&quot;:<span class="hljs-number">345</span>,&quot;ymin&quot;:<span class="hljs-number">23</span>,&quot;xmax&quot;:<span class="hljs-number">640</span>,&quot;ymax&quot;:<span class="hljs-number">368</span>}}]`}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p:P,i(s){p||(E(n.$$.fragment,s),p=!0)},o(s){w(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function Az(_){let n,p;return n=new R({props:{$$slots:{default:[kz]},$$scope:{ctx:_}}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){p||(E(n.$$.fragment,s),p=!0)},o(s){w(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function Dz(_){let n,p;return n=new N({props:{code:`    curl https://api-inference.huggingface.co/models/facebook/detr-resnet-50 \\
            -X POST \\
            --data-binary '@cats.jpg' \\
            -H "Authorization: Bearer $}{HF_API_TOKEN}"
    # [{"score":0.9982201457023621,"label":"remote","box":{"xmin":40,"ymin":70,"xmax":175,"ymax":117}},{"score":0.9960021376609802,"label":"remote","box":{"xmin":333,"ymin":72,"xmax":368,"ymax":187}},{"score":0.9954745173454285,"label":"couch","box":{"xmin":0,"ymin":1,"xmax":639,"ymax":473}},{"score":0.9988006353378296,"label":"cat","box":{"xmin":13,"ymin":52,"xmax":314,"ymax":470}},{"score":0.9986783862113953,"label":"cat","box":{"xmin":345,"ymin":23,"xmax":640,"ymax":368}}]`,highlighted:`    curl https:<span class="hljs-comment">//api-inference.huggingface.co/models/facebook/detr-resnet-50 \\</span>
            -X POST \\
            --<span class="hljs-keyword">data</span>-binary <span class="hljs-string">&#x27;@cats.jpg&#x27;</span> \\
            -H <span class="hljs-string">&quot;Authorization: Bearer <span class="hljs-subst">$}{HF_API_TOKEN}</span>&quot;</span>
    # [{<span class="hljs-string">&quot;score&quot;</span>:<span class="hljs-number">0.9982201457023621</span>,<span class="hljs-string">&quot;label&quot;</span>:<span class="hljs-string">&quot;remote&quot;</span>,<span class="hljs-string">&quot;box&quot;</span>:{<span class="hljs-string">&quot;xmin&quot;</span>:<span class="hljs-number">40</span>,<span class="hljs-string">&quot;ymin&quot;</span>:<span class="hljs-number">70</span>,<span class="hljs-string">&quot;xmax&quot;</span>:<span class="hljs-number">175</span>,<span class="hljs-string">&quot;ymax&quot;</span>:<span class="hljs-number">117</span>}},{<span class="hljs-string">&quot;score&quot;</span>:<span class="hljs-number">0.9960021376609802</span>,<span class="hljs-string">&quot;label&quot;</span>:<span class="hljs-string">&quot;remote&quot;</span>,<span class="hljs-string">&quot;box&quot;</span>:{<span class="hljs-string">&quot;xmin&quot;</span>:<span class="hljs-number">333</span>,<span class="hljs-string">&quot;ymin&quot;</span>:<span class="hljs-number">72</span>,<span class="hljs-string">&quot;xmax&quot;</span>:<span class="hljs-number">368</span>,<span class="hljs-string">&quot;ymax&quot;</span>:<span class="hljs-number">187</span>}},{<span class="hljs-string">&quot;score&quot;</span>:<span class="hljs-number">0.9954745173454285</span>,<span class="hljs-string">&quot;label&quot;</span>:<span class="hljs-string">&quot;couch&quot;</span>,<span class="hljs-string">&quot;box&quot;</span>:{<span class="hljs-string">&quot;xmin&quot;</span>:<span class="hljs-number">0</span>,<span class="hljs-string">&quot;ymin&quot;</span>:<span class="hljs-number">1</span>,<span class="hljs-string">&quot;xmax&quot;</span>:<span class="hljs-number">639</span>,<span class="hljs-string">&quot;ymax&quot;</span>:<span class="hljs-number">473</span>}},{<span class="hljs-string">&quot;score&quot;</span>:<span class="hljs-number">0.9988006353378296</span>,<span class="hljs-string">&quot;label&quot;</span>:<span class="hljs-string">&quot;cat&quot;</span>,<span class="hljs-string">&quot;box&quot;</span>:{<span class="hljs-string">&quot;xmin&quot;</span>:<span class="hljs-number">13</span>,<span class="hljs-string">&quot;ymin&quot;</span>:<span class="hljs-number">52</span>,<span class="hljs-string">&quot;xmax&quot;</span>:<span class="hljs-number">314</span>,<span class="hljs-string">&quot;ymax&quot;</span>:<span class="hljs-number">470</span>}},{<span class="hljs-string">&quot;score&quot;</span>:<span class="hljs-number">0.9986783862113953</span>,<span class="hljs-string">&quot;label&quot;</span>:<span class="hljs-string">&quot;cat&quot;</span>,<span class="hljs-string">&quot;box&quot;</span>:{<span class="hljs-string">&quot;xmin&quot;</span>:<span class="hljs-number">345</span>,<span class="hljs-string">&quot;ymin&quot;</span>:<span class="hljs-number">23</span>,<span class="hljs-string">&quot;xmax&quot;</span>:<span class="hljs-number">640</span>,<span class="hljs-string">&quot;ymax&quot;</span>:<span class="hljs-number">368</span>}}]`}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p:P,i(s){p||(E(n.$$.fragment,s),p=!0)},o(s){w(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function Oz(_){let n,p;return n=new R({props:{$$slots:{default:[Dz]},$$scope:{ctx:_}}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){p||(E(n.$$.fragment,s),p=!0)},o(s){w(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function Pz(_){let n,p;return n=new N({props:{code:`    self.assertEqual(
        deep_round(data, 4),
        [
            {
                "score": 0.9982,
                "label": "remote",
                "box": {"xmin": 40, "ymin": 70, "xmax": 175, "ymax": 117},
            },
            {
                "score": 0.9960,
                "label": "remote",
                "box": {"xmin": 333, "ymin": 72, "xmax": 368, "ymax": 187},
            },
            {
                "score": 0.9955,
                "label": "couch",
                "box": {"xmin": 0, "ymin": 1, "xmax": 639, "ymax": 473},
            },
            {
                "score": 0.9988,
                "label": "cat",
                "box": {"xmin": 13, "ymin": 52, "xmax": 314, "ymax": 470},
            },
            {
                "score": 0.9987,
                "label": "cat",
                "box": {"xmin": 345, "ymin": 23, "xmax": 640, "ymax": 368},
            },
        ],
    )`,highlighted:`    self.assertEqual(
        deep_round(data, <span class="hljs-number">4</span>),
        [
            {
                <span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.9982</span>,
                <span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;remote&quot;</span>,
                <span class="hljs-string">&quot;box&quot;</span>: {<span class="hljs-string">&quot;xmin&quot;</span>: <span class="hljs-number">40</span>, <span class="hljs-string">&quot;ymin&quot;</span>: <span class="hljs-number">70</span>, <span class="hljs-string">&quot;xmax&quot;</span>: <span class="hljs-number">175</span>, <span class="hljs-string">&quot;ymax&quot;</span>: <span class="hljs-number">117</span>},
            },
            {
                <span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.9960</span>,
                <span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;remote&quot;</span>,
                <span class="hljs-string">&quot;box&quot;</span>: {<span class="hljs-string">&quot;xmin&quot;</span>: <span class="hljs-number">333</span>, <span class="hljs-string">&quot;ymin&quot;</span>: <span class="hljs-number">72</span>, <span class="hljs-string">&quot;xmax&quot;</span>: <span class="hljs-number">368</span>, <span class="hljs-string">&quot;ymax&quot;</span>: <span class="hljs-number">187</span>},
            },
            {
                <span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.9955</span>,
                <span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;couch&quot;</span>,
                <span class="hljs-string">&quot;box&quot;</span>: {<span class="hljs-string">&quot;xmin&quot;</span>: <span class="hljs-number">0</span>, <span class="hljs-string">&quot;ymin&quot;</span>: <span class="hljs-number">1</span>, <span class="hljs-string">&quot;xmax&quot;</span>: <span class="hljs-number">639</span>, <span class="hljs-string">&quot;ymax&quot;</span>: <span class="hljs-number">473</span>},
            },
            {
                <span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.9988</span>,
                <span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;cat&quot;</span>,
                <span class="hljs-string">&quot;box&quot;</span>: {<span class="hljs-string">&quot;xmin&quot;</span>: <span class="hljs-number">13</span>, <span class="hljs-string">&quot;ymin&quot;</span>: <span class="hljs-number">52</span>, <span class="hljs-string">&quot;xmax&quot;</span>: <span class="hljs-number">314</span>, <span class="hljs-string">&quot;ymax&quot;</span>: <span class="hljs-number">470</span>},
            },
            {
                <span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.9987</span>,
                <span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;cat&quot;</span>,
                <span class="hljs-string">&quot;box&quot;</span>: {<span class="hljs-string">&quot;xmin&quot;</span>: <span class="hljs-number">345</span>, <span class="hljs-string">&quot;ymin&quot;</span>: <span class="hljs-number">23</span>, <span class="hljs-string">&quot;xmax&quot;</span>: <span class="hljs-number">640</span>, <span class="hljs-string">&quot;ymax&quot;</span>: <span class="hljs-number">368</span>},
            },
        ],
    )`}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p:P,i(s){p||(E(n.$$.fragment,s),p=!0)},o(s){w(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function Rz(_){let n,p;return n=new R({props:{$$slots:{default:[Pz]},$$scope:{ctx:_}}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),p=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){p||(E(n.$$.fragment,s),p=!0)},o(s){w(n.$$.fragment,s),p=!1},d(s){b(n,s)}}}function Nz(_){let n,p,s,d,$,k,A,T,j,O,D,ne,Pe,Q,W,st,Ll,Oa,Re,vw,t_,zl,yw,s_,at,fR,a_,nt,hR,n_,Ne,rt,Zf,Pa,Ew,eh,ww,r_,Ml,bw,o_,ot,l_,Ra,jw,Na,Tw,i_,Fl,kw,u_,lt,p_,Jl,Aw,c_,it,th,xa,Kl,Dw,Ow,sh,Pw,z,Sa,Ia,ah,Rw,Nw,xw,Wl,Sw,Iw,Ha,Ba,nh,Hw,Bw,Cw,Yl,Gw,Uw,Ca,Vl,Lw,zw,pe,Mw,rh,Fw,Jw,oh,Kw,Ww,Yw,Ga,Xl,Vw,Xw,ut,Qw,lh,Zw,eb,tb,Ua,Ql,ih,sb,ab,Zl,nb,rb,La,ei,ob,lb,pt,ib,uh,ub,pb,cb,za,ti,fb,hb,ct,db,ph,gb,mb,$b,Ma,si,_b,qb,ft,vb,ch,yb,Eb,f_,ai,wb,h_,ni,bb,d_,ht,g_,dt,fh,Fa,ri,jb,Tb,hh,kb,xe,Ja,oi,dh,Ab,Db,li,Ob,Pb,Ka,ii,gh,Rb,Nb,ui,xb,Sb,Wa,pi,mh,Ib,Hb,gt,Bb,$h,Cb,Gb,m_,Se,mt,_h,Ya,Ub,qh,Lb,$_,ci,zb,__,$t,q_,Va,Mb,Xa,Fb,v_,fi,Jb,y_,_t,E_,hi,Kb,w_,qt,vh,Qa,di,Wb,Yb,yh,Vb,Z,Za,en,Eh,Xb,Qb,Zb,gi,ej,tj,tn,mi,wh,sj,aj,$i,nj,rj,sn,_i,oj,lj,vt,ij,bh,uj,pj,cj,an,qi,fj,hj,yt,dj,jh,gj,mj,$j,nn,vi,_j,qj,Et,vj,Th,yj,Ej,b_,yi,wj,j_,wt,kh,rn,Ei,bj,jj,Ah,Tj,Dh,on,wi,Oh,kj,Aj,bi,Dj,T_,Ie,bt,Ph,ln,Oj,Rh,Pj,k_,jt,Rj,ji,Nj,xj,A_,Tt,D_,un,Sj,pn,Ij,O_,Ti,Hj,P_,kt,R_,ki,Bj,N_,At,Nh,cn,Ai,Cj,Gj,xh,Uj,G,fn,hn,Sh,Lj,zj,Mj,Di,Fj,Jj,dn,Oi,Ih,Kj,Wj,Pi,Yj,Vj,gn,Ri,Xj,Qj,ce,Zj,Hh,e0,t0,Bh,s0,a0,n0,mn,Ni,r0,o0,fe,l0,Ch,i0,u0,Gh,p0,c0,f0,$n,xi,h0,d0,he,g0,Uh,m0,$0,Lh,_0,q0,v0,_n,Si,y0,E0,de,w0,zh,b0,j0,Mh,T0,k0,A0,qn,Ii,D0,O0,ge,P0,Fh,R0,N0,Jh,x0,S0,I0,vn,Hi,H0,B0,Dt,C0,Kh,G0,U0,L0,yn,Bi,z0,M0,Ot,F0,Wh,J0,K0,W0,En,Ci,Yh,Y0,V0,Gi,X0,Q0,wn,Ui,Z0,eT,Pt,tT,Vh,sT,aT,nT,bn,Li,rT,oT,Rt,lT,Xh,iT,uT,pT,jn,zi,cT,fT,Nt,hT,Qh,dT,gT,x_,Mi,mT,S_,xt,Zh,Tn,Fi,$T,_T,ed,qT,td,kn,Ji,sd,vT,yT,Ki,ET,I_,He,St,ad,An,wT,nd,bT,H_,Wi,jT,B_,It,C_,Dn,TT,On,kT,G_,Yi,AT,U_,Ht,L_,Vi,DT,z_,Bt,rd,Pn,Xi,OT,PT,od,RT,x,Rn,Nn,ld,NT,xT,ST,id,IT,xn,Qi,HT,BT,Zi,CT,GT,Sn,eu,UT,LT,tu,zT,MT,In,su,FT,JT,Ct,KT,ud,WT,YT,VT,Hn,au,pd,XT,QT,nu,ZT,e3,Bn,ru,t3,s3,me,a3,cd,n3,r3,fd,o3,l3,i3,Cn,ou,u3,p3,$e,c3,hd,f3,h3,dd,d3,g3,m3,Gn,lu,$3,_3,_e,q3,gd,v3,y3,md,E3,w3,b3,Un,iu,j3,T3,qe,k3,$d,A3,D3,_d,O3,P3,R3,Ln,uu,N3,x3,ve,S3,qd,I3,H3,vd,B3,C3,G3,zn,pu,U3,L3,Gt,z3,yd,M3,F3,J3,Mn,cu,K3,W3,Ut,Y3,Ed,V3,X3,Q3,Fn,fu,wd,Z3,e5,hu,t5,s5,Jn,du,a5,n5,Lt,r5,bd,o5,l5,i5,Kn,gu,u5,p5,zt,c5,jd,f5,h5,d5,Wn,mu,g5,m5,Mt,$5,Td,_5,q5,M_,$u,v5,F_,Ft,kd,Yn,_u,y5,E5,Ad,w5,re,Vn,qu,Dd,b5,j5,vu,T5,k5,Xn,yu,Od,A5,D5,Eu,O5,P5,Qn,wu,R5,N5,bu,x5,S5,Zn,ju,I5,H5,Tu,B5,J_,Be,Jt,Pd,er,C5,Rd,G5,K_,ku,U5,W_,Kt,Y_,tr,L5,sr,z5,V_,Au,M5,X_,Wt,Q_,Du,F5,Z_,Yt,Nd,ar,Ou,J5,K5,xd,W5,J,nr,rr,Sd,Y5,V5,X5,Id,Q5,or,Pu,Z5,ek,Ru,tk,sk,lr,Nu,ak,nk,xu,rk,ok,ir,Su,Hd,lk,ik,Iu,uk,pk,ur,Hu,ck,fk,Vt,hk,Bd,dk,gk,mk,pr,Bu,$k,_k,Xt,qk,Cd,vk,yk,Ek,cr,Cu,wk,bk,Qt,jk,Gd,Tk,kk,eq,Gu,Ak,tq,Zt,sq,es,Ud,fr,Uu,Dk,Ok,Ld,Pk,oe,hr,Lu,zd,Rk,Nk,zu,xk,Sk,dr,Mu,Md,Ik,Hk,Fu,Bk,Ck,gr,Ju,Fd,Gk,Uk,Ku,Lk,zk,mr,Wu,Jd,Mk,Fk,Yu,Jk,aq,Ce,ts,Kd,$r,Kk,Wd,Wk,nq,Vu,Yk,rq,ss,oq,Ge,Vk,_r,Xk,Qk,qr,Zk,lq,Xu,e4,iq,as,uq,Qu,t4,pq,Zu,s4,cq,ns,fq,rs,Yd,vr,ep,a4,n4,Vd,r4,le,yr,tp,Xd,o4,l4,sp,i4,u4,Er,ap,Qd,p4,c4,np,f4,h4,wr,rp,Zd,d4,g4,os,m4,eg,$4,_4,q4,br,op,tg,v4,y4,ls,E4,sg,w4,b4,hq,Ue,is,ag,jr,j4,ng,T4,dq,lp,k4,gq,us,mq,Tr,A4,kr,D4,$q,ip,O4,_q,ps,qq,up,P4,vq,cs,rg,Ar,pp,R4,N4,og,x4,ee,Dr,Or,lg,S4,I4,H4,cp,B4,C4,Pr,fp,ig,G4,U4,hp,L4,z4,Rr,dp,M4,F4,fs,J4,ug,K4,W4,Y4,Nr,gp,V4,X4,hs,Q4,pg,Z4,e7,t7,xr,mp,s7,a7,ds,n7,cg,r7,o7,yq,$p,l7,Eq,gs,wq,ms,fg,Sr,_p,i7,u7,hg,p7,Ir,Hr,qp,dg,c7,f7,vp,h7,d7,Br,yp,gg,g7,m7,Ep,$7,bq,Le,$s,mg,Cr,_7,$g,q7,jq,Gr,v7,wp,y7,Tq,ze,_s,_g,Ur,E7,qg,w7,kq,bp,b7,Aq,qs,Dq,Me,j7,Lr,T7,k7,zr,A7,Oq,jp,D7,Pq,vs,Rq,Tp,O7,Nq,ys,vg,Mr,kp,P7,R7,yg,N7,K,Fr,Jr,Eg,x7,S7,I7,Ap,H7,B7,Kr,Dp,wg,C7,G7,Op,U7,L7,Wr,Pp,z7,M7,S,F7,bg,J7,K7,W7,Y7,jg,V7,X7,Q7,Z7,Tg,e6,t6,s6,a6,kg,n6,r6,Ag,o6,l6,i6,u6,Dg,p6,c6,Og,f6,h6,d6,g6,Pg,m6,$6,Rg,_6,q6,v6,Yr,Rp,Ng,y6,E6,Np,w6,b6,Vr,xp,j6,T6,Es,k6,xg,A6,D6,O6,Xr,Sp,P6,R6,ws,N6,Sg,x6,S6,I6,Qr,Ip,H6,B6,bs,C6,Ig,G6,U6,xq,Hp,L6,Sq,js,Iq,Ts,Hg,Zr,Bp,z6,M6,Bg,F6,te,eo,Cp,Cg,J6,K6,Gp,W6,Y6,to,Up,Gg,V6,X6,Lp,Q6,Z6,so,zp,Ug,e9,t9,Mp,s9,a9,ao,Fp,Lg,n9,r9,ks,o9,zg,l9,i9,u9,no,Jp,Mg,p9,c9,As,f9,Fg,h9,d9,Hq,Fe,Ds,Jg,ro,g9,Kg,m9,Bq,Kp,$9,Cq,Os,Gq,oo,_9,lo,q9,Uq,Wp,v9,Lq,Ps,zq,Yp,y9,Mq,Rs,Wg,io,Vp,E9,w9,Yg,b9,I,uo,po,Vg,j9,T9,k9,Xp,A9,D9,co,Qp,Xg,O9,P9,Zp,R9,N9,fo,ec,x9,S9,ye,I9,Qg,H9,B9,Zg,C9,G9,U9,ho,tc,L9,z9,Ee,M9,em,F9,J9,tm,K9,W9,Y9,go,sc,V9,X9,we,Q9,sm,Z9,e8,am,t8,s8,a8,mo,ac,n8,r8,Ns,o8,nm,l8,i8,u8,$o,nc,p8,c8,be,f8,rm,h8,d8,om,g8,m8,$8,_o,rc,_8,q8,xs,v8,lm,y8,E8,w8,qo,oc,b8,j8,je,T8,im,k8,A8,um,D8,O8,P8,vo,lc,R8,N8,Ss,x8,pm,S8,I8,H8,yo,ic,B8,C8,Is,G8,cm,U8,L8,z8,Eo,uc,fm,M8,F8,pc,J8,K8,wo,cc,W8,Y8,Hs,V8,hm,X8,Q8,Z8,bo,fc,eA,tA,Bs,sA,dm,aA,nA,rA,jo,hc,oA,lA,Cs,iA,gm,uA,pA,Fq,dc,cA,Jq,Gs,Kq,Us,mm,To,gc,fA,hA,$m,dA,_m,ko,mc,qm,gA,mA,$c,$A,Wq,Je,Ls,vm,Ao,_A,ym,qA,Yq,zs,vA,_c,yA,EA,Vq,Ke,Ms,Em,Do,wA,wm,bA,Xq,qc,jA,Qq,Fs,Zq,Oo,TA,Po,kA,e1,vc,AA,t1,Js,s1,yc,DA,a1,Ks,bm,Ro,Ec,OA,PA,jm,RA,se,No,xo,Tm,NA,xA,SA,wc,IA,HA,So,bc,km,BA,CA,jc,GA,UA,Io,Tc,LA,zA,Ws,MA,Am,FA,JA,KA,Ho,kc,WA,YA,Ys,VA,Dm,XA,QA,ZA,Bo,Ac,eD,tD,Vs,sD,Om,aD,nD,n1,Dc,rD,r1,Xs,o1,Qs,Pm,Co,Oc,oD,lD,Rm,iD,ie,Go,Pc,Nm,uD,pD,Rc,cD,fD,Uo,Nc,xm,hD,dD,xc,gD,mD,Lo,Sc,Sm,$D,_D,Ic,qD,vD,zo,Hc,Im,yD,ED,Bc,wD,l1,We,Zs,Hm,Mo,bD,Bm,jD,i1,Cc,TD,u1,ea,p1,ta,c1,ue,kD,Fo,AD,DD,Jo,OD,PD,Ko,RD,f1,Gc,ND,h1,sa,d1,Uc,xD,g1,aa,Cm,Wo,Lc,SD,ID,Gm,HD,Um,Yo,Vo,Lm,BD,CD,GD,zc,UD,m1,Mc,LD,$1,Fc,zD,_1,na,q1,ra,zm,Xo,Jc,MD,FD,Mm,JD,Fm,Qo,Kc,Jm,KD,WD,Wc,YD,v1,Ye,oa,Km,Zo,VD,Wm,XD,y1,Yc,QD,E1,la,w1,Ve,ZD,el,eO,tO,tl,sO,b1,Vc,aO,j1,ia,Ym,sl,Xc,nO,rO,Vm,oO,ae,al,nl,Xm,lO,iO,uO,Qc,pO,cO,rl,Zc,Qm,fO,hO,ef,dO,gO,ol,tf,mO,$O,ua,_O,Zm,qO,vO,yO,ll,sf,EO,wO,pa,bO,e$,jO,TO,kO,il,af,AO,DO,ca,OO,t$,PO,RO,T1,nf,NO,k1,fa,s$,ul,rf,xO,SO,a$,IO,n$,pl,of,r$,HO,BO,lf,CO,A1,uf,GO,D1,Xe,ha,o$,cl,UO,l$,LO,O1,pf,zO,P1,da,R1,Qe,MO,fl,FO,JO,hl,KO,N1,cf,WO,x1,ga,S1,ff,YO,I1,ma,i$,dl,hf,VO,XO,u$,QO,p$,gl,ml,c$,ZO,eP,tP,df,sP,H1,gf,aP,B1,$a,C1,_a,f$,$l,mf,nP,rP,h$,oP,_l,ql,$f,d$,lP,iP,_f,uP,pP,vl,qf,g$,cP,fP,vf,hP,G1,Ze,qa,m$,yl,dP,$$,gP,U1,yf,mP,L1,va,z1,El,$P,wl,_P,M1,Ef,qP,F1,ya,J1,Ea,vP,bl,yP,EP,K1,wa,_$,jl,wf,wP,bP,q$,jP,v$,Tl,kl,y$,TP,kP,AP,bf,DP,W1,jf,OP,Y1,ba,V1,ja,E$,Al,Tf,PP,RP,w$,NP,et,Dl,kf,b$,xP,SP,Af,IP,HP,Ol,Df,j$,BP,CP,Of,GP,UP,Pl,Pf,T$,LP,zP,Rf,MP,X1;return k=new F({}),Q=new F({}),Pa=new F({}),ot=new Y({props:{$$slots:{default:[AU]},$$scope:{ctx:_}}}),lt=new L({props:{python:!0,js:!0,curl:!0,$$slots:{curl:[xU],js:[RU],python:[OU]},$$scope:{ctx:_}}}),ht=new L({props:{python:!0,js:!0,curl:!0,$$slots:{python:[IU]},$$scope:{ctx:_}}}),Ya=new F({}),$t=new Y({props:{$$slots:{default:[HU]},$$scope:{ctx:_}}}),_t=new L({props:{python:!0,js:!0,curl:!0,$$slots:{curl:[zU],js:[UU],python:[CU]},$$scope:{ctx:_}}}),ln=new F({}),Tt=new Y({props:{$$slots:{default:[MU]},$$scope:{ctx:_}}}),kt=new L({props:{python:!0,js:!0,curl:!0,$$slots:{curl:[VU],js:[WU],python:[JU]},$$scope:{ctx:_}}}),An=new F({}),It=new Y({props:{$$slots:{default:[XU]},$$scope:{ctx:_}}}),Ht=new L({props:{python:!0,js:!0,curl:!0,$$slots:{curl:[aL],js:[tL],python:[ZU]},$$scope:{ctx:_}}}),er=new F({}),Kt=new Y({props:{$$slots:{default:[nL]},$$scope:{ctx:_}}}),Wt=new L({props:{python:!0,js:!0,curl:!0,$$slots:{curl:[pL],js:[iL],python:[oL]},$$scope:{ctx:_}}}),Zt=new L({props:{python:!0,js:!0,curl:!0,$$slots:{python:[fL]},$$scope:{ctx:_}}}),$r=new F({}),ss=new Y({props:{$$slots:{default:[hL]},$$scope:{ctx:_}}}),as=new L({props:{python:!0,js:!0,curl:!0,$$slots:{curl:[qL],js:[$L],python:[gL]},$$scope:{ctx:_}}}),ns=new L({props:{python:!0,js:!0,curl:!0,$$slots:{python:[yL]},$$scope:{ctx:_}}}),jr=new F({}),us=new Y({props:{$$slots:{default:[EL]},$$scope:{ctx:_}}}),ps=new L({props:{python:!0,js:!0,curl:!0,$$slots:{curl:[AL],js:[TL],python:[bL]},$$scope:{ctx:_}}}),gs=new L({props:{python:!0,js:!0,curl:!0,$$slots:{python:[OL]},$$scope:{ctx:_}}}),Cr=new F({}),Ur=new F({}),qs=new Y({props:{$$slots:{default:[PL]},$$scope:{ctx:_}}}),vs=new L({props:{python:!0,js:!0,curl:!0,$$slots:{curl:[HL],js:[SL],python:[NL]},$$scope:{ctx:_}}}),js=new L({props:{python:!0,js:!0,curl:!0,$$slots:{python:[CL]},$$scope:{ctx:_}}}),ro=new F({}),Os=new Y({props:{$$slots:{default:[GL]},$$scope:{ctx:_}}}),Ps=new L({props:{python:!0,js:!0,curl:!0,$$slots:{curl:[JL],js:[ML],python:[LL]},$$scope:{ctx:_}}}),Gs=new L({props:{python:!0,js:!0,curl:!0,$$slots:{python:[WL]},$$scope:{ctx:_}}}),Ao=new F({}),Do=new F({}),Fs=new Y({props:{$$slots:{default:[YL]},$$scope:{ctx:_}}}),Js=new L({props:{python:!0,js:!0,curl:!0,$$slots:{curl:[tz],js:[ZL],python:[XL]},$$scope:{ctx:_}}}),Xs=new L({props:{python:!0,js:!0,curl:!0,$$slots:{python:[az]},$$scope:{ctx:_}}}),Mo=new F({}),ea=new Y({props:{$$slots:{default:[nz]},$$scope:{ctx:_}}}),ta=new Y({props:{$$slots:{default:[rz]},$$scope:{ctx:_}}}),sa=new L({props:{python:!0,js:!0,curl:!0,$$slots:{curl:[cz],js:[uz],python:[lz]},$$scope:{ctx:_}}}),na=new L({props:{python:!0,js:!0,curl:!0,$$slots:{python:[hz]},$$scope:{ctx:_}}}),Zo=new F({}),la=new Y({props:{$$slots:{default:[dz]},$$scope:{ctx:_}}}),cl=new F({}),da=new Y({props:{$$slots:{default:[gz]},$$scope:{ctx:_}}}),ga=new L({props:{python:!0,js:!0,curl:!0,$$slots:{curl:[yz],js:[qz],python:[$z]},$$scope:{ctx:_}}}),$a=new L({props:{python:!0,js:!0,curl:!0,$$slots:{python:[wz]},$$scope:{ctx:_}}}),yl=new F({}),va=new Y({props:{$$slots:{default:[bz]},$$scope:{ctx:_}}}),ya=new L({props:{python:!0,js:!0,curl:!0,$$slots:{curl:[Oz],js:[Az],python:[Tz]},$$scope:{ctx:_}}}),ba=new L({props:{python:!0,js:!0,curl:!0,$$slots:{python:[Rz]},$$scope:{ctx:_}}}),{c(){n=r("meta"),p=c(),s=r("h1"),d=r("a"),$=r("span"),q(k.$$.fragment),A=c(),T=r("span"),j=i("Detailed parameters"),O=c(),D=r("h2"),ne=r("a"),Pe=r("span"),q(Q.$$.fragment),W=c(),st=r("span"),Ll=i("Which task is used by this model ?"),Oa=c(),Re=r("p"),vw=i(`In general the \u{1F917} Hosted API Inference accepts a simple string as an
input. However, more advanced usage depends on the \u201Ctask\u201D that the
model solves.`),t_=c(),zl=r("p"),yw=i("The \u201Ctask\u201D of a model is defined here on it\u2019s model page:"),s_=c(),at=r("img"),a_=c(),nt=r("img"),n_=c(),Ne=r("h2"),rt=r("a"),Zf=r("span"),q(Pa.$$.fragment),Ew=c(),eh=r("span"),ww=i("Zero-shot classification task"),r_=c(),Ml=r("p"),bw=i(`This task is a super useful to try it out classification with zero code,
you simply pass a sentence/paragraph and the possible labels for that
sentence and you get a result.`),o_=c(),q(ot.$$.fragment),l_=c(),Ra=r("p"),jw=i("Available with: "),Na=r("a"),Tw=i("\u{1F917} Transformers"),i_=c(),Fl=r("p"),kw=i("Request:"),u_=c(),q(lt.$$.fragment),p_=c(),Jl=r("p"),Aw=i(`When sending your request, you should send a JSON encoded payload. Here
are all the options`),c_=c(),it=r("table"),th=r("thead"),xa=r("tr"),Kl=r("th"),Dw=i("All parameters"),Ow=c(),sh=r("th"),Pw=c(),z=r("tbody"),Sa=r("tr"),Ia=r("td"),ah=r("strong"),Rw=i("inputs"),Nw=i(" (required)"),xw=c(),Wl=r("td"),Sw=i("a string or list of strings"),Iw=c(),Ha=r("tr"),Ba=r("td"),nh=r("strong"),Hw=i("parameters"),Bw=i(" (required)"),Cw=c(),Yl=r("td"),Gw=i("a dict containing the following keys:"),Uw=c(),Ca=r("tr"),Vl=r("td"),Lw=i("candidate_labels (required)"),zw=c(),pe=r("td"),Mw=i("a list of strings that are potential classes for "),rh=r("code"),Fw=i("inputs"),Jw=i(". (max 10 candidate_labels, for more, simply run multiple requests, results are going to be misleading if using too many candidate_labels anyway. If you want to keep the exact same, you can simply run "),oh=r("code"),Kw=i("multi_label=True"),Ww=i(" and do the scaling on your end. )"),Yw=c(),Ga=r("tr"),Xl=r("td"),Vw=i("multi_label"),Xw=c(),ut=r("td"),Qw=i("(Default: "),lh=r("code"),Zw=i("false"),eb=i(") Boolean that is set to True if classes can overlap"),tb=c(),Ua=r("tr"),Ql=r("td"),ih=r("strong"),sb=i("options"),ab=c(),Zl=r("td"),nb=i("a dict containing the following keys:"),rb=c(),La=r("tr"),ei=r("td"),ob=i("use_gpu"),lb=c(),pt=r("td"),ib=i("(Default: "),uh=r("code"),ub=i("false"),pb=i("). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),cb=c(),za=r("tr"),ti=r("td"),fb=i("use_cache"),hb=c(),ct=r("td"),db=i("(Default: "),ph=r("code"),gb=i("true"),mb=i("). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),$b=c(),Ma=r("tr"),si=r("td"),_b=i("wait_for_model"),qb=c(),ft=r("td"),vb=i("(Default: "),ch=r("code"),yb=i("false"),Eb=i(") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),f_=c(),ai=r("p"),wb=i("Return value is either a dict or a list of dicts if you sent a list of inputs"),h_=c(),ni=r("p"),bb=i("Response:"),d_=c(),q(ht.$$.fragment),g_=c(),dt=r("table"),fh=r("thead"),Fa=r("tr"),ri=r("th"),jb=i("Returned values"),Tb=c(),hh=r("th"),kb=c(),xe=r("tbody"),Ja=r("tr"),oi=r("td"),dh=r("strong"),Ab=i("sequence"),Db=c(),li=r("td"),Ob=i("The string sent as an input"),Pb=c(),Ka=r("tr"),ii=r("td"),gh=r("strong"),Rb=i("labels"),Nb=c(),ui=r("td"),xb=i("The list of strings for labels that you sent (in order)"),Sb=c(),Wa=r("tr"),pi=r("td"),mh=r("strong"),Ib=i("scores"),Hb=c(),gt=r("td"),Bb=i("a list of floats that correspond the the probability of label, in the same order as "),$h=r("code"),Cb=i("labels"),Gb=i("."),m_=c(),Se=r("h2"),mt=r("a"),_h=r("span"),q(Ya.$$.fragment),Ub=c(),qh=r("span"),Lb=i("Translation task"),$_=c(),ci=r("p"),zb=i("This task is well known to translate text from one language to another"),__=c(),q($t.$$.fragment),q_=c(),Va=r("p"),Mb=i("Available with: "),Xa=r("a"),Fb=i("\u{1F917} Transformers"),v_=c(),fi=r("p"),Jb=i("Example:"),y_=c(),q(_t.$$.fragment),E_=c(),hi=r("p"),Kb=i(`When sending your request, you should send a JSON encoded payload. Here
are all the options`),w_=c(),qt=r("table"),vh=r("thead"),Qa=r("tr"),di=r("th"),Wb=i("All parameters"),Yb=c(),yh=r("th"),Vb=c(),Z=r("tbody"),Za=r("tr"),en=r("td"),Eh=r("strong"),Xb=i("inputs"),Qb=i(" (required)"),Zb=c(),gi=r("td"),ej=i("a string to be translated in the original languages"),tj=c(),tn=r("tr"),mi=r("td"),wh=r("strong"),sj=i("options"),aj=c(),$i=r("td"),nj=i("a dict containing the following keys:"),rj=c(),sn=r("tr"),_i=r("td"),oj=i("use_gpu"),lj=c(),vt=r("td"),ij=i("(Default: "),bh=r("code"),uj=i("false"),pj=i("). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),cj=c(),an=r("tr"),qi=r("td"),fj=i("use_cache"),hj=c(),yt=r("td"),dj=i("(Default: "),jh=r("code"),gj=i("true"),mj=i("). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),$j=c(),nn=r("tr"),vi=r("td"),_j=i("wait_for_model"),qj=c(),Et=r("td"),vj=i("(Default: "),Th=r("code"),yj=i("false"),Ej=i(") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),b_=c(),yi=r("p"),wj=i("Return value is either a dict or a list of dicts if you sent a list of inputs"),j_=c(),wt=r("table"),kh=r("thead"),rn=r("tr"),Ei=r("th"),bj=i("Returned values"),jj=c(),Ah=r("th"),Tj=c(),Dh=r("tbody"),on=r("tr"),wi=r("td"),Oh=r("strong"),kj=i("translation_text"),Aj=c(),bi=r("td"),Dj=i("The string after translation"),T_=c(),Ie=r("h2"),bt=r("a"),Ph=r("span"),q(ln.$$.fragment),Oj=c(),Rh=r("span"),Pj=i("Summarization task"),k_=c(),jt=r("p"),Rj=i(`This task is well known to summarize text a big text into a small text.
Be careful, some models have a maximum length of input. That means that
the summary cannot handle full books for instance. Be careful when
choosing your model. If you want to discuss you summarization needs,
please get in touch <`),ji=r("a"),Nj=i("api-enterprise@huggingface.co"),xj=i(">"),A_=c(),q(Tt.$$.fragment),D_=c(),un=r("p"),Sj=i("Available with: "),pn=r("a"),Ij=i("\u{1F917} Transformers"),O_=c(),Ti=r("p"),Hj=i("Example:"),P_=c(),q(kt.$$.fragment),R_=c(),ki=r("p"),Bj=i(`When sending your request, you should send a JSON encoded payload. Here
are all the options`),N_=c(),At=r("table"),Nh=r("thead"),cn=r("tr"),Ai=r("th"),Cj=i("All parameters"),Gj=c(),xh=r("th"),Uj=c(),G=r("tbody"),fn=r("tr"),hn=r("td"),Sh=r("strong"),Lj=i("inputs"),zj=i(" (required)"),Mj=c(),Di=r("td"),Fj=i("a string to be summarized"),Jj=c(),dn=r("tr"),Oi=r("td"),Ih=r("strong"),Kj=i("parameters"),Wj=c(),Pi=r("td"),Yj=i("a dict containing the following keys:"),Vj=c(),gn=r("tr"),Ri=r("td"),Xj=i("min_length"),Qj=c(),ce=r("td"),Zj=i("(Default: "),Hh=r("code"),e0=i("None"),t0=i("). Integer to define the minimum length "),Bh=r("strong"),s0=i("in tokens"),a0=i(" of the output summary."),n0=c(),mn=r("tr"),Ni=r("td"),r0=i("max_length"),o0=c(),fe=r("td"),l0=i("(Default: "),Ch=r("code"),i0=i("None"),u0=i("). Integer to define the maximum length "),Gh=r("strong"),p0=i("in tokens"),c0=i(" of the output summary."),f0=c(),$n=r("tr"),xi=r("td"),h0=i("top_k"),d0=c(),he=r("td"),g0=i("(Default: "),Uh=r("code"),m0=i("None"),$0=i("). Integer to define the top tokens considered within the "),Lh=r("code"),_0=i("sample"),q0=i(" operation to create new text."),v0=c(),_n=r("tr"),Si=r("td"),y0=i("top_p"),E0=c(),de=r("td"),w0=i("(Default: "),zh=r("code"),b0=i("None"),j0=i("). Float to define the tokens that are within the sample"),Mh=r("code"),T0=i("operation of text generation. Add tokens in the sample for more probable to least probable until the sum of the probabilities is greater than"),k0=i("top_p`."),A0=c(),qn=r("tr"),Ii=r("td"),D0=i("temperature"),O0=c(),ge=r("td"),P0=i("(Default: "),Fh=r("code"),R0=i("1.0"),N0=i("). Float (0.0-100.0). The temperature of the sampling operation. 1 means regular sampling, 0 means "),Jh=r("code"),x0=i("100.0"),S0=i(" is getting closer to uniform probability."),I0=c(),vn=r("tr"),Hi=r("td"),H0=i("repetition_penalty"),B0=c(),Dt=r("td"),C0=i("(Default: "),Kh=r("code"),G0=i("None"),U0=i("). Float (0.0-100.0). The more a token is used within generation the more it is penalized to not be picked in successive generation passes."),L0=c(),yn=r("tr"),Bi=r("td"),z0=i("max_time"),M0=c(),Ot=r("td"),F0=i("(Default: "),Wh=r("code"),J0=i("None"),K0=i("). Float (0-120.0). The amount of time in seconds that the query should take maximum. Network can cause some overhead so it will be a soft limit."),W0=c(),En=r("tr"),Ci=r("td"),Yh=r("strong"),Y0=i("options"),V0=c(),Gi=r("td"),X0=i("a dict containing the following keys:"),Q0=c(),wn=r("tr"),Ui=r("td"),Z0=i("use_gpu"),eT=c(),Pt=r("td"),tT=i("(Default: "),Vh=r("code"),sT=i("false"),aT=i("). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),nT=c(),bn=r("tr"),Li=r("td"),rT=i("use_cache"),oT=c(),Rt=r("td"),lT=i("(Default: "),Xh=r("code"),iT=i("true"),uT=i("). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),pT=c(),jn=r("tr"),zi=r("td"),cT=i("wait_for_model"),fT=c(),Nt=r("td"),hT=i("(Default: "),Qh=r("code"),dT=i("false"),gT=i(") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),x_=c(),Mi=r("p"),mT=i("Return value is either a dict or a list of dicts if you sent a list of inputs"),S_=c(),xt=r("table"),Zh=r("thead"),Tn=r("tr"),Fi=r("th"),$T=i("Returned values"),_T=c(),ed=r("th"),qT=c(),td=r("tbody"),kn=r("tr"),Ji=r("td"),sd=r("strong"),vT=i("summarization_text"),yT=c(),Ki=r("td"),ET=i("The string after translation"),I_=c(),He=r("h2"),St=r("a"),ad=r("span"),q(An.$$.fragment),wT=c(),nd=r("span"),bT=i("Conversational task"),H_=c(),Wi=r("p"),jT=i(`This task corresponds to any chatbot like structure. Models tend to have
shorted max_length, so please check with caution when using a given
model if you need long range dependency or not.`),B_=c(),q(It.$$.fragment),C_=c(),Dn=r("p"),TT=i("Available with: "),On=r("a"),kT=i("\u{1F917} Transformers"),G_=c(),Yi=r("p"),AT=i("Example:"),U_=c(),q(Ht.$$.fragment),L_=c(),Vi=r("p"),DT=i(`When sending your request, you should send a JSON encoded payload. Here
are all the options`),z_=c(),Bt=r("table"),rd=r("thead"),Pn=r("tr"),Xi=r("th"),OT=i("All parameters"),PT=c(),od=r("th"),RT=c(),x=r("tbody"),Rn=r("tr"),Nn=r("td"),ld=r("strong"),NT=i("inputs"),xT=i(" (required)"),ST=c(),id=r("td"),IT=c(),xn=r("tr"),Qi=r("td"),HT=i("text (required)"),BT=c(),Zi=r("td"),CT=i("The last input from the user in the conversation."),GT=c(),Sn=r("tr"),eu=r("td"),UT=i("generated_responses"),LT=c(),tu=r("td"),zT=i("A list of strings corresponding to the earlier replies from the model."),MT=c(),In=r("tr"),su=r("td"),FT=i("past_user_inputs"),JT=c(),Ct=r("td"),KT=i("A list of strings corresponding to the earlier replies from the user. Should be of the same length of "),ud=r("code"),WT=i("generated_responses"),YT=i("."),VT=c(),Hn=r("tr"),au=r("td"),pd=r("strong"),XT=i("parameters"),QT=c(),nu=r("td"),ZT=i("a dict containing the following keys:"),e3=c(),Bn=r("tr"),ru=r("td"),t3=i("min_length"),s3=c(),me=r("td"),a3=i("(Default: "),cd=r("code"),n3=i("None"),r3=i("). Integer to define the minimum length "),fd=r("strong"),o3=i("in tokens"),l3=i(" of the output summary."),i3=c(),Cn=r("tr"),ou=r("td"),u3=i("max_length"),p3=c(),$e=r("td"),c3=i("(Default: "),hd=r("code"),f3=i("None"),h3=i("). Integer to define the maximum length "),dd=r("strong"),d3=i("in tokens"),g3=i(" of the output summary."),m3=c(),Gn=r("tr"),lu=r("td"),$3=i("top_k"),_3=c(),_e=r("td"),q3=i("(Default: "),gd=r("code"),v3=i("None"),y3=i("). Integer to define the top tokens considered within the "),md=r("code"),E3=i("sample"),w3=i(" operation to create new text."),b3=c(),Un=r("tr"),iu=r("td"),j3=i("top_p"),T3=c(),qe=r("td"),k3=i("(Default: "),$d=r("code"),A3=i("None"),D3=i("). Float to define the tokens that are within the sample"),_d=r("code"),O3=i("operation of text generation. Add tokens in the sample for more probable to least probable until the sum of the probabilities is greater than"),P3=i("top_p`."),R3=c(),Ln=r("tr"),uu=r("td"),N3=i("temperature"),x3=c(),ve=r("td"),S3=i("(Default: "),qd=r("code"),I3=i("1.0"),H3=i("). Float (0.0-100.0). The temperature of the sampling operation. 1 means regular sampling, 0 means "),vd=r("code"),B3=i("100.0"),C3=i(" is getting closer to uniform probability."),G3=c(),zn=r("tr"),pu=r("td"),U3=i("repetition_penalty"),L3=c(),Gt=r("td"),z3=i("(Default: "),yd=r("code"),M3=i("None"),F3=i("). Float (0.0-100.0). The more a token is used within generation the more it is penalized to not be picked in successive generation passes."),J3=c(),Mn=r("tr"),cu=r("td"),K3=i("max_time"),W3=c(),Ut=r("td"),Y3=i("(Default: "),Ed=r("code"),V3=i("None"),X3=i("). Float (0-120.0). The amount of time in seconds that the query should take maximum. Network can cause some overhead so it will be a soft limit."),Q3=c(),Fn=r("tr"),fu=r("td"),wd=r("strong"),Z3=i("options"),e5=c(),hu=r("td"),t5=i("a dict containing the following keys:"),s5=c(),Jn=r("tr"),du=r("td"),a5=i("use_gpu"),n5=c(),Lt=r("td"),r5=i("(Default: "),bd=r("code"),o5=i("false"),l5=i("). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),i5=c(),Kn=r("tr"),gu=r("td"),u5=i("use_cache"),p5=c(),zt=r("td"),c5=i("(Default: "),jd=r("code"),f5=i("true"),h5=i("). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),d5=c(),Wn=r("tr"),mu=r("td"),g5=i("wait_for_model"),m5=c(),Mt=r("td"),$5=i("(Default: "),Td=r("code"),_5=i("false"),q5=i(") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),M_=c(),$u=r("p"),v5=i("Return value is either a dict or a list of dicts if you sent a list of inputs"),F_=c(),Ft=r("table"),kd=r("thead"),Yn=r("tr"),_u=r("th"),y5=i("Returned values"),E5=c(),Ad=r("th"),w5=c(),re=r("tbody"),Vn=r("tr"),qu=r("td"),Dd=r("strong"),b5=i("generated_text"),j5=c(),vu=r("td"),T5=i("The answer of the bot"),k5=c(),Xn=r("tr"),yu=r("td"),Od=r("strong"),A5=i("conversation"),D5=c(),Eu=r("td"),O5=i("A facility dictionnary to send back for the next input (with the new user input addition)."),P5=c(),Qn=r("tr"),wu=r("td"),R5=i("past_user_inputs"),N5=c(),bu=r("td"),x5=i("List of strings. The last inputs from the user in the conversation, <em>after the model has run."),S5=c(),Zn=r("tr"),ju=r("td"),I5=i("generated_responses"),H5=c(),Tu=r("td"),B5=i("List of strings. The last outputs from the model in the conversation, <em>after the model has run."),J_=c(),Be=r("h2"),Jt=r("a"),Pd=r("span"),q(er.$$.fragment),C5=c(),Rd=r("span"),G5=i("Table question answering task"),K_=c(),ku=r("p"),U5=i(`Don\u2019t know SQL? Don\u2019t want to dive into a large spreadsheet? Ask
questions in plain english!`),W_=c(),q(Kt.$$.fragment),Y_=c(),tr=r("p"),L5=i("Available with: "),sr=r("a"),z5=i("\u{1F917} Transformers"),V_=c(),Au=r("p"),M5=i("Example:"),X_=c(),q(Wt.$$.fragment),Q_=c(),Du=r("p"),F5=i(`When sending your request, you should send a JSON encoded payload. Here
are all the options`),Z_=c(),Yt=r("table"),Nd=r("thead"),ar=r("tr"),Ou=r("th"),J5=i("All parameters"),K5=c(),xd=r("th"),W5=c(),J=r("tbody"),nr=r("tr"),rr=r("td"),Sd=r("strong"),Y5=i("inputs"),V5=i(" (required)"),X5=c(),Id=r("td"),Q5=c(),or=r("tr"),Pu=r("td"),Z5=i("query (required)"),ek=c(),Ru=r("td"),tk=i("The query in plain text that you want to ask the table"),sk=c(),lr=r("tr"),Nu=r("td"),ak=i("table (required)"),nk=c(),xu=r("td"),rk=i("A table of data represented as a dict of list where entries are headers and the lists are all the values, all lists must have the same size."),ok=c(),ir=r("tr"),Su=r("td"),Hd=r("strong"),lk=i("options"),ik=c(),Iu=r("td"),uk=i("a dict containing the following keys:"),pk=c(),ur=r("tr"),Hu=r("td"),ck=i("use_gpu"),fk=c(),Vt=r("td"),hk=i("(Default: "),Bd=r("code"),dk=i("false"),gk=i("). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),mk=c(),pr=r("tr"),Bu=r("td"),$k=i("use_cache"),_k=c(),Xt=r("td"),qk=i("(Default: "),Cd=r("code"),vk=i("true"),yk=i("). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),Ek=c(),cr=r("tr"),Cu=r("td"),wk=i("wait_for_model"),bk=c(),Qt=r("td"),jk=i("(Default: "),Gd=r("code"),Tk=i("false"),kk=i(") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),eq=c(),Gu=r("p"),Ak=i("Return value is either a dict or a list of dicts if you sent a list of inputs"),tq=c(),q(Zt.$$.fragment),sq=c(),es=r("table"),Ud=r("thead"),fr=r("tr"),Uu=r("th"),Dk=i("Returned values"),Ok=c(),Ld=r("th"),Pk=c(),oe=r("tbody"),hr=r("tr"),Lu=r("td"),zd=r("strong"),Rk=i("answer"),Nk=c(),zu=r("td"),xk=i("The plaintext answer"),Sk=c(),dr=r("tr"),Mu=r("td"),Md=r("strong"),Ik=i("coordinates"),Hk=c(),Fu=r("td"),Bk=i("a list of coordinates of the cells references in the answer"),Ck=c(),gr=r("tr"),Ju=r("td"),Fd=r("strong"),Gk=i("cells"),Uk=c(),Ku=r("td"),Lk=i("a list of coordinates of the cells contents"),zk=c(),mr=r("tr"),Wu=r("td"),Jd=r("strong"),Mk=i("aggregator"),Fk=c(),Yu=r("td"),Jk=i("The aggregator used to get the answer"),aq=c(),Ce=r("h2"),ts=r("a"),Kd=r("span"),q($r.$$.fragment),Kk=c(),Wd=r("span"),Wk=i("Question answering task"),nq=c(),Vu=r("p"),Yk=i("Want to have a nice know-it-all bot that can answer any questions ?"),rq=c(),q(ss.$$.fragment),oq=c(),Ge=r("p"),Vk=i("Available with: "),_r=r("a"),Xk=i("\u{1F917}Transformers"),Qk=i(` and
`),qr=r("a"),Zk=i("AllenNLP"),lq=c(),Xu=r("p"),e4=i("Example:"),iq=c(),q(as.$$.fragment),uq=c(),Qu=r("p"),t4=i(`When sending your request, you should send a JSON encoded payload. Here
are all the options`),pq=c(),Zu=r("p"),s4=i("Return value is either a dict or a list of dicts if you sent a list of inputs"),cq=c(),q(ns.$$.fragment),fq=c(),rs=r("table"),Yd=r("thead"),vr=r("tr"),ep=r("th"),a4=i("Returned values"),n4=c(),Vd=r("th"),r4=c(),le=r("tbody"),yr=r("tr"),tp=r("td"),Xd=r("strong"),o4=i("answer"),l4=c(),sp=r("td"),i4=i("A string that\u2019s the answer within the text."),u4=c(),Er=r("tr"),ap=r("td"),Qd=r("strong"),p4=i("score"),c4=c(),np=r("td"),f4=i("A floats that represents how likely that the answer is correct"),h4=c(),wr=r("tr"),rp=r("td"),Zd=r("strong"),d4=i("start"),g4=c(),os=r("td"),m4=i("The index (string wise) of the start of the answer within "),eg=r("code"),$4=i("context"),_4=i("."),q4=c(),br=r("tr"),op=r("td"),tg=r("strong"),v4=i("stop"),y4=c(),ls=r("td"),E4=i("The index (string wise) of the stop of the answer within "),sg=r("code"),w4=i("context"),b4=i("."),hq=c(),Ue=r("h2"),is=r("a"),ag=r("span"),q(jr.$$.fragment),j4=c(),ng=r("span"),T4=i("Text-classification task"),dq=c(),lp=r("p"),k4=i(`Usually used for sentiment-analysis this will output the likelihood of
classes of an input.`),gq=c(),q(us.$$.fragment),mq=c(),Tr=r("p"),A4=i("Available with: "),kr=r("a"),D4=i("\u{1F917} Transformers"),$q=c(),ip=r("p"),O4=i("Example:"),_q=c(),q(ps.$$.fragment),qq=c(),up=r("p"),P4=i(`When sending your request, you should send a JSON encoded payload. Here
are all the options`),vq=c(),cs=r("table"),rg=r("thead"),Ar=r("tr"),pp=r("th"),R4=i("All parameters"),N4=c(),og=r("th"),x4=c(),ee=r("tbody"),Dr=r("tr"),Or=r("td"),lg=r("strong"),S4=i("inputs"),I4=i(" (required)"),H4=c(),cp=r("td"),B4=i("a string to be classified"),C4=c(),Pr=r("tr"),fp=r("td"),ig=r("strong"),G4=i("options"),U4=c(),hp=r("td"),L4=i("a dict containing the following keys:"),z4=c(),Rr=r("tr"),dp=r("td"),M4=i("use_gpu"),F4=c(),fs=r("td"),J4=i("(Default: "),ug=r("code"),K4=i("false"),W4=i("). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),Y4=c(),Nr=r("tr"),gp=r("td"),V4=i("use_cache"),X4=c(),hs=r("td"),Q4=i("(Default: "),pg=r("code"),Z4=i("true"),e7=i("). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),t7=c(),xr=r("tr"),mp=r("td"),s7=i("wait_for_model"),a7=c(),ds=r("td"),n7=i("(Default: "),cg=r("code"),r7=i("false"),o7=i(") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),yq=c(),$p=r("p"),l7=i("Return value is either a dict or a list of dicts if you sent a list of inputs"),Eq=c(),q(gs.$$.fragment),wq=c(),ms=r("table"),fg=r("thead"),Sr=r("tr"),_p=r("th"),i7=i("Returned values"),u7=c(),hg=r("th"),p7=c(),Ir=r("tbody"),Hr=r("tr"),qp=r("td"),dg=r("strong"),c7=i("label"),f7=c(),vp=r("td"),h7=i("The label for the class (model specific)"),d7=c(),Br=r("tr"),yp=r("td"),gg=r("strong"),g7=i("score"),m7=c(),Ep=r("td"),$7=i("A floats that represents how likely is that the text belongs the this class."),bq=c(),Le=r("h2"),$s=r("a"),mg=r("span"),q(Cr.$$.fragment),_7=c(),$g=r("span"),q7=i("Named Entity Recognition (NER) task"),jq=c(),Gr=r("p"),v7=i("See "),wp=r("a"),y7=i("Token-classification task"),Tq=c(),ze=r("h2"),_s=r("a"),_g=r("span"),q(Ur.$$.fragment),E7=c(),qg=r("span"),w7=i("Token-classification task"),kq=c(),bp=r("p"),b7=i(`Usually used for sentence parsing, either grammatical, or Named Entity
Recognition (NER) to understand keywords contained within text.`),Aq=c(),q(qs.$$.fragment),Dq=c(),Me=r("p"),j7=i("Available with: "),Lr=r("a"),T7=i("\u{1F917} Transformers"),k7=i(`,
`),zr=r("a"),A7=i("Flair"),Oq=c(),jp=r("p"),D7=i("Example:"),Pq=c(),q(vs.$$.fragment),Rq=c(),Tp=r("p"),O7=i(`When sending your request, you should send a JSON encoded payload. Here
are all the options`),Nq=c(),ys=r("table"),vg=r("thead"),Mr=r("tr"),kp=r("th"),P7=i("All parameters"),R7=c(),yg=r("th"),N7=c(),K=r("tbody"),Fr=r("tr"),Jr=r("td"),Eg=r("strong"),x7=i("inputs"),S7=i(" (required)"),I7=c(),Ap=r("td"),H7=i("a string to be classified"),B7=c(),Kr=r("tr"),Dp=r("td"),wg=r("strong"),C7=i("parameters"),G7=c(),Op=r("td"),U7=i("a dict containing the following key:"),L7=c(),Wr=r("tr"),Pp=r("td"),z7=i("aggregation_strategy"),M7=c(),S=r("td"),F7=i("(Default: "),bg=r("code"),J7=i("simple"),K7=i("). There are several aggregation strategies: "),W7=r("br"),Y7=c(),jg=r("code"),V7=i("none"),X7=i(": Every token gets classified without further aggregation. "),Q7=r("br"),Z7=c(),Tg=r("code"),e6=i("simple"),t6=i(": Entities are grouped according to the default schema (B-, I- tags get merged when the tag is similar). "),s6=r("br"),a6=c(),kg=r("code"),n6=i("first"),r6=i(": Same as the "),Ag=r("code"),o6=i("simple"),l6=i(" strategy except words cannot end up with different tags. Words will use the tag of the first token when there is ambiguity. "),i6=r("br"),u6=c(),Dg=r("code"),p6=i("average"),c6=i(": Same as the "),Og=r("code"),f6=i("simple"),h6=i(" strategy except words cannot end up with different tags. Scores are averaged across tokens and then the maximum label is applied. "),d6=r("br"),g6=c(),Pg=r("code"),m6=i("max"),$6=i(": Same as the "),Rg=r("code"),_6=i("simple"),q6=i(" strategy except words cannot end up with different tags. Word entity will be the token with the maximum score."),v6=c(),Yr=r("tr"),Rp=r("td"),Ng=r("strong"),y6=i("options"),E6=c(),Np=r("td"),w6=i("a dict containing the following keys:"),b6=c(),Vr=r("tr"),xp=r("td"),j6=i("use_gpu"),T6=c(),Es=r("td"),k6=i("(Default: "),xg=r("code"),A6=i("false"),D6=i("). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),O6=c(),Xr=r("tr"),Sp=r("td"),P6=i("use_cache"),R6=c(),ws=r("td"),N6=i("(Default: "),Sg=r("code"),x6=i("true"),S6=i("). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),I6=c(),Qr=r("tr"),Ip=r("td"),H6=i("wait_for_model"),B6=c(),bs=r("td"),C6=i("(Default: "),Ig=r("code"),G6=i("false"),U6=i(") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),xq=c(),Hp=r("p"),L6=i("Return value is either a dict or a list of dicts if you sent a list of inputs"),Sq=c(),q(js.$$.fragment),Iq=c(),Ts=r("table"),Hg=r("thead"),Zr=r("tr"),Bp=r("th"),z6=i("Returned values"),M6=c(),Bg=r("th"),F6=c(),te=r("tbody"),eo=r("tr"),Cp=r("td"),Cg=r("strong"),J6=i("entity_group"),K6=c(),Gp=r("td"),W6=i("The type for the entity being recognized (model specific)."),Y6=c(),to=r("tr"),Up=r("td"),Gg=r("strong"),V6=i("score"),X6=c(),Lp=r("td"),Q6=i("How likely the entity was recognized."),Z6=c(),so=r("tr"),zp=r("td"),Ug=r("strong"),e9=i("word"),t9=c(),Mp=r("td"),s9=i("The string that was captured"),a9=c(),ao=r("tr"),Fp=r("td"),Lg=r("strong"),n9=i("start"),r9=c(),ks=r("td"),o9=i("The offset stringwise where the answer is located. Useful to disambiguate if "),zg=r("code"),l9=i("word"),i9=i(" occurs multiple times."),u9=c(),no=r("tr"),Jp=r("td"),Mg=r("strong"),p9=i("end"),c9=c(),As=r("td"),f9=i("The offset stringwise where the answer is located. Useful to disambiguate if "),Fg=r("code"),h9=i("word"),d9=i(" occurs multiple times."),Hq=c(),Fe=r("h2"),Ds=r("a"),Jg=r("span"),q(ro.$$.fragment),g9=c(),Kg=r("span"),m9=i("Text-generation task"),Bq=c(),Kp=r("p"),$9=i("Use to continue text from a prompt. This is a very generic task."),Cq=c(),q(Os.$$.fragment),Gq=c(),oo=r("p"),_9=i("Available with: "),lo=r("a"),q9=i("\u{1F917} Transformers"),Uq=c(),Wp=r("p"),v9=i("Example:"),Lq=c(),q(Ps.$$.fragment),zq=c(),Yp=r("p"),y9=i(`When sending your request, you should send a JSON encoded payload. Here
are all the options`),Mq=c(),Rs=r("table"),Wg=r("thead"),io=r("tr"),Vp=r("th"),E9=i("All parameters"),w9=c(),Yg=r("th"),b9=c(),I=r("tbody"),uo=r("tr"),po=r("td"),Vg=r("strong"),j9=i("inputs"),T9=i(" (required):"),k9=c(),Xp=r("td"),A9=i("a string to be generated from"),D9=c(),co=r("tr"),Qp=r("td"),Xg=r("strong"),O9=i("parameters"),P9=c(),Zp=r("td"),R9=i("dict containing the following keys:"),N9=c(),fo=r("tr"),ec=r("td"),x9=i("top_k"),S9=c(),ye=r("td"),I9=i("(Default: "),Qg=r("code"),H9=i("None"),B9=i("). Integer to define the top tokens considered within the "),Zg=r("code"),C9=i("sample"),G9=i(" operation to create new text."),U9=c(),ho=r("tr"),tc=r("td"),L9=i("top_p"),z9=c(),Ee=r("td"),M9=i("(Default: "),em=r("code"),F9=i("None"),J9=i("). Float to define the tokens that are within the  sample"),tm=r("code"),K9=i("operation of text generation. Add tokens in the sample for more probable to least probable until the sum of the probabilities is greater than"),W9=i("top_p`."),Y9=c(),go=r("tr"),sc=r("td"),V9=i("temperature"),X9=c(),we=r("td"),Q9=i("(Default: "),sm=r("code"),Z9=i("1.0"),e8=i("). Float (0.0-100.0). The temperature of the sampling operation. 1 means regular sampling, 0 means "),am=r("code"),t8=i("100.0"),s8=i(" is getting closer to uniform probability."),a8=c(),mo=r("tr"),ac=r("td"),n8=i("repetition_penalty"),r8=c(),Ns=r("td"),o8=i("(Default: "),nm=r("code"),l8=i("None"),i8=i("). Float (0.0-100.0). The more a token is used within generation the more it is penalized to not be picked in successive generation passes."),u8=c(),$o=r("tr"),nc=r("td"),p8=i("max_new_tokens"),c8=c(),be=r("td"),f8=i("(Default: "),rm=r("code"),h8=i("None"),d8=i("). Int (0-250). The amount of new tokens to be generated, this does "),om=r("strong"),g8=i("not"),m8=i(" include the input length it is a estimate of the size of generated text you want. Each new tokens slows down the request, so look for balance between response times and length of text generated."),$8=c(),_o=r("tr"),rc=r("td"),_8=i("max_time"),q8=c(),xs=r("td"),v8=i("(Default: "),lm=r("code"),y8=i("None"),E8=i("). Float (0-120.0). The amount of time in seconds that the query should take maximum. Network can cause some overhead so it will be a soft limit. Use that in combination with \u2018max_new_tokens` for best results."),w8=c(),qo=r("tr"),oc=r("td"),b8=i("return_full_text"),j8=c(),je=r("td"),T8=i("(Default: "),im=r("code"),k8=i("True"),A8=i("). Bool. If set to False, the return results will "),um=r("strong"),D8=i("not"),O8=i(" contain the original query making it easier for prompting."),P8=c(),vo=r("tr"),lc=r("td"),R8=i("num_return_sequences"),N8=c(),Ss=r("td"),x8=i("(Default: "),pm=r("code"),S8=i("1"),I8=i("). Integer. The number of proposition you want to be returned."),H8=c(),yo=r("tr"),ic=r("td"),B8=i("do_sample"),C8=c(),Is=r("td"),G8=i("(Optional: "),cm=r("code"),U8=i("True"),L8=i("). Bool. Whether or not to use sampling, use greedy decoding otherwise."),z8=c(),Eo=r("tr"),uc=r("td"),fm=r("strong"),M8=i("options"),F8=c(),pc=r("td"),J8=i("a dict containing the following keys:"),K8=c(),wo=r("tr"),cc=r("td"),W8=i("use_gpu"),Y8=c(),Hs=r("td"),V8=i("(Default: "),hm=r("code"),X8=i("false"),Q8=i("). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),Z8=c(),bo=r("tr"),fc=r("td"),eA=i("use_cache"),tA=c(),Bs=r("td"),sA=i("(Default: "),dm=r("code"),aA=i("true"),nA=i("). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),rA=c(),jo=r("tr"),hc=r("td"),oA=i("wait_for_model"),lA=c(),Cs=r("td"),iA=i("(Default: "),gm=r("code"),uA=i("false"),pA=i(") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),Fq=c(),dc=r("p"),cA=i("Return value is either a dict or a list of dicts if you sent a list of inputs"),Jq=c(),q(Gs.$$.fragment),Kq=c(),Us=r("table"),mm=r("thead"),To=r("tr"),gc=r("th"),fA=i("Returned values"),hA=c(),$m=r("th"),dA=c(),_m=r("tbody"),ko=r("tr"),mc=r("td"),qm=r("strong"),gA=i("generated_text"),mA=c(),$c=r("td"),$A=i("The continuated string"),Wq=c(),Je=r("h2"),Ls=r("a"),vm=r("span"),q(Ao.$$.fragment),_A=c(),ym=r("span"),qA=i("Text2text-generation task"),Yq=c(),zs=r("p"),vA=i("Essentially "),_c=r("a"),yA=i("Text-generation task"),EA=i(`. But uses
Encoder-Decoder architecture, so might change in the future for more
options.`),Vq=c(),Ke=r("h2"),Ms=r("a"),Em=r("span"),q(Do.$$.fragment),wA=c(),wm=r("span"),bA=i("Fill mask task"),Xq=c(),qc=r("p"),jA=i(`Tries to fill in a hole with a missing word (token to be precise).
That\u2019s the base task for BERT models.`),Qq=c(),q(Fs.$$.fragment),Zq=c(),Oo=r("p"),TA=i("Available with: "),Po=r("a"),kA=i("\u{1F917} Transformers"),e1=c(),vc=r("p"),AA=i("Example:"),t1=c(),q(Js.$$.fragment),s1=c(),yc=r("p"),DA=i(`When sending your request, you should send a JSON encoded payload. Here
are all the options`),a1=c(),Ks=r("table"),bm=r("thead"),Ro=r("tr"),Ec=r("th"),OA=i("All parameters"),PA=c(),jm=r("th"),RA=c(),se=r("tbody"),No=r("tr"),xo=r("td"),Tm=r("strong"),NA=i("inputs"),xA=i(" (required):"),SA=c(),wc=r("td"),IA=i("a string to be filled from, must contain the [MASK] token (check model card for exact name of the mask)"),HA=c(),So=r("tr"),bc=r("td"),km=r("strong"),BA=i("options"),CA=c(),jc=r("td"),GA=i("a dict containing the following keys:"),UA=c(),Io=r("tr"),Tc=r("td"),LA=i("use_gpu"),zA=c(),Ws=r("td"),MA=i("(Default: "),Am=r("code"),FA=i("false"),JA=i("). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),KA=c(),Ho=r("tr"),kc=r("td"),WA=i("use_cache"),YA=c(),Ys=r("td"),VA=i("(Default: "),Dm=r("code"),XA=i("true"),QA=i("). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),ZA=c(),Bo=r("tr"),Ac=r("td"),eD=i("wait_for_model"),tD=c(),Vs=r("td"),sD=i("(Default: "),Om=r("code"),aD=i("false"),nD=i(") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),n1=c(),Dc=r("p"),rD=i("Return value is either a dict or a list of dicts if you sent a list of inputs"),r1=c(),q(Xs.$$.fragment),o1=c(),Qs=r("table"),Pm=r("thead"),Co=r("tr"),Oc=r("th"),oD=i("Returned values"),lD=c(),Rm=r("th"),iD=c(),ie=r("tbody"),Go=r("tr"),Pc=r("td"),Nm=r("strong"),uD=i("sequence"),pD=c(),Rc=r("td"),cD=i("The actual sequence of tokens that ran against the model (may contain special tokens)"),fD=c(),Uo=r("tr"),Nc=r("td"),xm=r("strong"),hD=i("score"),dD=c(),xc=r("td"),gD=i("The probability for this token."),mD=c(),Lo=r("tr"),Sc=r("td"),Sm=r("strong"),$D=i("token"),_D=c(),Ic=r("td"),qD=i("The id of the token"),vD=c(),zo=r("tr"),Hc=r("td"),Im=r("strong"),yD=i("token_str"),ED=c(),Bc=r("td"),wD=i("The string representation of the token"),l1=c(),We=r("h2"),Zs=r("a"),Hm=r("span"),q(Mo.$$.fragment),bD=c(),Bm=r("span"),jD=i("Automatic speech recognition task"),i1=c(),Cc=r("p"),TD=i(`This task reads some audio input and outputs the said words within the
audio files.`),u1=c(),q(ea.$$.fragment),p1=c(),q(ta.$$.fragment),c1=c(),ue=r("p"),kD=i("Available with: "),Fo=r("a"),AD=i("\u{1F917} Transformers"),DD=c(),Jo=r("a"),OD=i("ESPnet"),PD=i(` and
`),Ko=r("a"),RD=i("SpeechBrain"),f1=c(),Gc=r("p"),ND=i("Request:"),h1=c(),q(sa.$$.fragment),d1=c(),Uc=r("p"),xD=i(`When sending your request, you should send a binary payload that simply
contains your audio file. We try to support most formats (Flac, Wav,
Mp3, Ogg etc...). And we automatically rescale the sampling rate to the
appropriate rate for the given model (usually 16KHz).`),g1=c(),aa=r("table"),Cm=r("thead"),Wo=r("tr"),Lc=r("th"),SD=i("All parameters"),ID=c(),Gm=r("th"),HD=c(),Um=r("tbody"),Yo=r("tr"),Vo=r("td"),Lm=r("strong"),BD=i("no parameter"),CD=i(" (required)"),GD=c(),zc=r("td"),UD=i("a binary representation of the audio file. No other parameters are currently allowed."),m1=c(),Mc=r("p"),LD=i("Return value is either a dict or a list of dicts if you sent a list of inputs"),$1=c(),Fc=r("p"),zD=i("Response:"),_1=c(),q(na.$$.fragment),q1=c(),ra=r("table"),zm=r("thead"),Xo=r("tr"),Jc=r("th"),MD=i("Returned values"),FD=c(),Mm=r("th"),JD=c(),Fm=r("tbody"),Qo=r("tr"),Kc=r("td"),Jm=r("strong"),KD=i("text"),WD=c(),Wc=r("td"),YD=i("The string that was recognized within the audio file."),v1=c(),Ye=r("h2"),oa=r("a"),Km=r("span"),q(Zo.$$.fragment),VD=c(),Wm=r("span"),XD=i("Feature-extraction task"),y1=c(),Yc=r("p"),QD=i(`This task reads some text and outputs raw float values, that usually
consumed as part of a semantic database/semantic search.`),E1=c(),q(la.$$.fragment),w1=c(),Ve=r("p"),ZD=i("Available with: "),el=r("a"),eO=i("\u{1F917} Transformers"),tO=c(),tl=r("a"),sO=i("Sentence-transformers"),b1=c(),Vc=r("p"),aO=i("Request:"),j1=c(),ia=r("table"),Ym=r("thead"),sl=r("tr"),Xc=r("th"),nO=i("All parameters"),rO=c(),Vm=r("th"),oO=c(),ae=r("tbody"),al=r("tr"),nl=r("td"),Xm=r("strong"),lO=i("inputs"),iO=i(" (required):"),uO=c(),Qc=r("td"),pO=i("a string or a list of strings to get the features from."),cO=c(),rl=r("tr"),Zc=r("td"),Qm=r("strong"),fO=i("options"),hO=c(),ef=r("td"),dO=i("a dict containing the following keys:"),gO=c(),ol=r("tr"),tf=r("td"),mO=i("use_gpu"),$O=c(),ua=r("td"),_O=i("(Default: "),Zm=r("code"),qO=i("false"),vO=i("). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),yO=c(),ll=r("tr"),sf=r("td"),EO=i("use_cache"),wO=c(),pa=r("td"),bO=i("(Default: "),e$=r("code"),jO=i("true"),TO=i("). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),kO=c(),il=r("tr"),af=r("td"),AO=i("wait_for_model"),DO=c(),ca=r("td"),OO=i("(Default: "),t$=r("code"),PO=i("false"),RO=i(") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),T1=c(),nf=r("p"),NO=i("Return value is either a dict or a list of dicts if you sent a list of inputs"),k1=c(),fa=r("table"),s$=r("thead"),ul=r("tr"),rf=r("th"),xO=i("Returned values"),SO=c(),a$=r("th"),IO=c(),n$=r("tbody"),pl=r("tr"),of=r("td"),r$=r("strong"),HO=i("A list of float (or list of list of floats)"),BO=c(),lf=r("td"),CO=i("The numbers that are the representation features of the input."),A1=c(),uf=r("small"),GO=i(`Returned values are a list of floats, or a list of list of floats
(depending on if you sent a string or a list of string, and if the
automatic reduction, usually mean_pooling for instance was applied for
you or not. This should be explained on the model's README.`),D1=c(),Xe=r("h2"),ha=r("a"),o$=r("span"),q(cl.$$.fragment),UO=c(),l$=r("span"),LO=i("Audio-classification task"),O1=c(),pf=r("p"),zO=i("This task reads some audio input and outputs the likelihood of classes."),P1=c(),q(da.$$.fragment),R1=c(),Qe=r("p"),MO=i("Available with: "),fl=r("a"),FO=i("\u{1F917} Transformers"),JO=c(),hl=r("a"),KO=i("SpeechBrain"),N1=c(),cf=r("p"),WO=i("Request:"),x1=c(),q(ga.$$.fragment),S1=c(),ff=r("p"),YO=i(`When sending your request, you should send a binary payload that simply
contains your audio file. We try to support most formats (Flac, Wav,
Mp3, Ogg etc...). And we automatically rescale the sampling rate to the
appropriate rate for the given model (usually 16KHz).`),I1=c(),ma=r("table"),i$=r("thead"),dl=r("tr"),hf=r("th"),VO=i("All parameters"),XO=c(),u$=r("th"),QO=c(),p$=r("tbody"),gl=r("tr"),ml=r("td"),c$=r("strong"),ZO=i("no parameter"),eP=i(" (required)"),tP=c(),df=r("td"),sP=i("a binary representation of the audio file. No other parameters are currently allowed."),H1=c(),gf=r("p"),aP=i("Return value is a dict"),B1=c(),q($a.$$.fragment),C1=c(),_a=r("table"),f$=r("thead"),$l=r("tr"),mf=r("th"),nP=i("Returned values"),rP=c(),h$=r("th"),oP=c(),_l=r("tbody"),ql=r("tr"),$f=r("td"),d$=r("strong"),lP=i("label"),iP=c(),_f=r("td"),uP=i("The label for the class (model specific)"),pP=c(),vl=r("tr"),qf=r("td"),g$=r("strong"),cP=i("score"),fP=c(),vf=r("td"),hP=i("A floats that represents how likely is that the audio file belongs the this class."),G1=c(),Ze=r("h2"),qa=r("a"),m$=r("span"),q(yl.$$.fragment),dP=c(),$$=r("span"),gP=i("Object-detection task"),U1=c(),yf=r("p"),mP=i(`This task reads some image input and outputs the likelihood of classes &
bounding boxes of detected objects.`),L1=c(),q(va.$$.fragment),z1=c(),El=r("p"),$P=i("Available with: "),wl=r("a"),_P=i("\u{1F917} Transformers"),M1=c(),Ef=r("p"),qP=i("Request:"),F1=c(),q(ya.$$.fragment),J1=c(),Ea=r("p"),vP=i(`When sending your request, you should send a binary payload that simply
contains your image file. We support all image formats `),bl=r("a"),yP=i(`Pillow
supports`),EP=i("."),K1=c(),wa=r("table"),_$=r("thead"),jl=r("tr"),wf=r("th"),wP=i("All parameters"),bP=c(),q$=r("th"),jP=c(),v$=r("tbody"),Tl=r("tr"),kl=r("td"),y$=r("strong"),TP=i("no parameter"),kP=i(" (required)"),AP=c(),bf=r("td"),DP=i("a binary representation of the image file. No other parameters are currently allowed."),W1=c(),jf=r("p"),OP=i("Return value is a dict"),Y1=c(),q(ba.$$.fragment),V1=c(),ja=r("table"),E$=r("thead"),Al=r("tr"),Tf=r("th"),PP=i("Returned values"),RP=c(),w$=r("th"),NP=c(),et=r("tbody"),Dl=r("tr"),kf=r("td"),b$=r("strong"),xP=i("label"),SP=c(),Af=r("td"),IP=i("The label for the class (model specific) of a detected object."),HP=c(),Ol=r("tr"),Df=r("td"),j$=r("strong"),BP=i("score"),CP=c(),Of=r("td"),GP=i("A float that represents how likely it is that the detected object belongs to the given class."),UP=c(),Pl=r("tr"),Pf=r("td"),T$=r("strong"),LP=i("box"),zP=c(),Rf=r("td"),MP=i("A dict (with keys [xmin,ymin,xmax,ymax]) representing the bounding box of a detected object."),this.h()},l(a){const g=TU('[data-svelte="svelte-1phssyn"]',document.head);n=o(g,"META",{name:!0,content:!0}),g.forEach(t),p=f(a),s=o(a,"H1",{class:!0});var Rl=l(s);d=o(Rl,"A",{id:!0,class:!0,href:!0});var k$=l(d);$=o(k$,"SPAN",{});var A$=l($);v(k.$$.fragment,A$),A$.forEach(t),k$.forEach(t),A=f(Rl),T=o(Rl,"SPAN",{});var D$=l(T);j=u(D$,"Detailed parameters"),D$.forEach(t),Rl.forEach(t),O=f(a),D=o(a,"H2",{class:!0});var Nl=l(D);ne=o(Nl,"A",{id:!0,class:!0,href:!0});var O$=l(ne);Pe=o(O$,"SPAN",{});var P$=l(Pe);v(Q.$$.fragment,P$),P$.forEach(t),O$.forEach(t),W=f(Nl),st=o(Nl,"SPAN",{});var R$=l(st);Ll=u(R$,"Which task is used by this model ?"),R$.forEach(t),Nl.forEach(t),Oa=f(a),Re=o(a,"P",{});var N$=l(Re);vw=u(N$,`In general the \u{1F917} Hosted API Inference accepts a simple string as an
input. However, more advanced usage depends on the \u201Ctask\u201D that the
model solves.`),N$.forEach(t),t_=f(a),zl=o(a,"P",{});var x$=l(zl);yw=u(x$,"The \u201Ctask\u201D of a model is defined here on it\u2019s model page:"),x$.forEach(t),s_=f(a),at=o(a,"IMG",{class:!0,src:!0,width:!0}),a_=f(a),nt=o(a,"IMG",{class:!0,src:!0,width:!0}),n_=f(a),Ne=o(a,"H2",{class:!0});var xl=l(Ne);rt=o(xl,"A",{id:!0,class:!0,href:!0});var S$=l(rt);Zf=o(S$,"SPAN",{});var I$=l(Zf);v(Pa.$$.fragment,I$),I$.forEach(t),S$.forEach(t),Ew=f(xl),eh=o(xl,"SPAN",{});var H$=l(eh);ww=u(H$,"Zero-shot classification task"),H$.forEach(t),xl.forEach(t),r_=f(a),Ml=o(a,"P",{});var B$=l(Ml);bw=u(B$,`This task is a super useful to try it out classification with zero code,
you simply pass a sentence/paragraph and the possible labels for that
sentence and you get a result.`),B$.forEach(t),o_=f(a),v(ot.$$.fragment,a),l_=f(a),Ra=o(a,"P",{});var Nf=l(Ra);jw=u(Nf,"Available with: "),Na=o(Nf,"A",{href:!0,rel:!0});var C$=l(Na);Tw=u(C$,"\u{1F917} Transformers"),C$.forEach(t),Nf.forEach(t),i_=f(a),Fl=o(a,"P",{});var G$=l(Fl);kw=u(G$,"Request:"),G$.forEach(t),u_=f(a),v(lt.$$.fragment,a),p_=f(a),Jl=o(a,"P",{});var U$=l(Jl);Aw=u(U$,`When sending your request, you should send a JSON encoded payload. Here
are all the options`),U$.forEach(t),c_=f(a),it=o(a,"TABLE",{});var Sl=l(it);th=o(Sl,"THEAD",{});var L$=l(th);xa=o(L$,"TR",{});var Il=l(xa);Kl=o(Il,"TH",{align:!0});var z$=l(Kl);Dw=u(z$,"All parameters"),z$.forEach(t),Ow=f(Il),sh=o(Il,"TH",{align:!0}),l(sh).forEach(t),Il.forEach(t),L$.forEach(t),Pw=f(Sl),z=o(Sl,"TBODY",{});var M=l(z);Sa=o(M,"TR",{});var Hl=l(Sa);Ia=o(Hl,"TD",{align:!0});var xf=l(Ia);ah=o(xf,"STRONG",{});var M$=l(ah);Rw=u(M$,"inputs"),M$.forEach(t),Nw=u(xf," (required)"),xf.forEach(t),xw=f(Hl),Wl=o(Hl,"TD",{align:!0});var F$=l(Wl);Sw=u(F$,"a string or list of strings"),F$.forEach(t),Hl.forEach(t),Iw=f(M),Ha=o(M,"TR",{});var Bl=l(Ha);Ba=o(Bl,"TD",{align:!0});var Sf=l(Ba);nh=o(Sf,"STRONG",{});var J$=l(nh);Hw=u(J$,"parameters"),J$.forEach(t),Bw=u(Sf," (required)"),Sf.forEach(t),Cw=f(Bl),Yl=o(Bl,"TD",{align:!0});var K$=l(Yl);Gw=u(K$,"a dict containing the following keys:"),K$.forEach(t),Bl.forEach(t),Uw=f(M),Ca=o(M,"TR",{});var Cl=l(Ca);Vl=o(Cl,"TD",{align:!0});var W$=l(Vl);Lw=u(W$,"candidate_labels (required)"),W$.forEach(t),zw=f(Cl),pe=o(Cl,"TD",{align:!0});var tt=l(pe);Mw=u(tt,"a list of strings that are potential classes for "),rh=o(tt,"CODE",{});var Y$=l(rh);Fw=u(Y$,"inputs"),Y$.forEach(t),Jw=u(tt,". (max 10 candidate_labels, for more, simply run multiple requests, results are going to be misleading if using too many candidate_labels anyway. If you want to keep the exact same, you can simply run "),oh=o(tt,"CODE",{});var V$=l(oh);Kw=u(V$,"multi_label=True"),V$.forEach(t),Ww=u(tt," and do the scaling on your end. )"),tt.forEach(t),Cl.forEach(t),Yw=f(M),Ga=o(M,"TR",{});var Gl=l(Ga);Xl=o(Gl,"TD",{align:!0});var dR=l(Xl);Vw=u(dR,"multi_label"),dR.forEach(t),Xw=f(Gl),ut=o(Gl,"TD",{align:!0});var Q1=l(ut);Qw=u(Q1,"(Default: "),lh=o(Q1,"CODE",{});var gR=l(lh);Zw=u(gR,"false"),gR.forEach(t),eb=u(Q1,") Boolean that is set to True if classes can overlap"),Q1.forEach(t),Gl.forEach(t),tb=f(M),Ua=o(M,"TR",{});var Z1=l(Ua);Ql=o(Z1,"TD",{align:!0});var mR=l(Ql);ih=o(mR,"STRONG",{});var $R=l(ih);sb=u($R,"options"),$R.forEach(t),mR.forEach(t),ab=f(Z1),Zl=o(Z1,"TD",{align:!0});var _R=l(Zl);nb=u(_R,"a dict containing the following keys:"),_R.forEach(t),Z1.forEach(t),rb=f(M),La=o(M,"TR",{});var ev=l(La);ei=o(ev,"TD",{align:!0});var qR=l(ei);ob=u(qR,"use_gpu"),qR.forEach(t),lb=f(ev),pt=o(ev,"TD",{align:!0});var tv=l(pt);ib=u(tv,"(Default: "),uh=o(tv,"CODE",{});var vR=l(uh);ub=u(vR,"false"),vR.forEach(t),pb=u(tv,"). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),tv.forEach(t),ev.forEach(t),cb=f(M),za=o(M,"TR",{});var sv=l(za);ti=o(sv,"TD",{align:!0});var yR=l(ti);fb=u(yR,"use_cache"),yR.forEach(t),hb=f(sv),ct=o(sv,"TD",{align:!0});var av=l(ct);db=u(av,"(Default: "),ph=o(av,"CODE",{});var ER=l(ph);gb=u(ER,"true"),ER.forEach(t),mb=u(av,"). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),av.forEach(t),sv.forEach(t),$b=f(M),Ma=o(M,"TR",{});var nv=l(Ma);si=o(nv,"TD",{align:!0});var wR=l(si);_b=u(wR,"wait_for_model"),wR.forEach(t),qb=f(nv),ft=o(nv,"TD",{align:!0});var rv=l(ft);vb=u(rv,"(Default: "),ch=o(rv,"CODE",{});var bR=l(ch);yb=u(bR,"false"),bR.forEach(t),Eb=u(rv,") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),rv.forEach(t),nv.forEach(t),M.forEach(t),Sl.forEach(t),f_=f(a),ai=o(a,"P",{});var jR=l(ai);wb=u(jR,"Return value is either a dict or a list of dicts if you sent a list of inputs"),jR.forEach(t),h_=f(a),ni=o(a,"P",{});var TR=l(ni);bb=u(TR,"Response:"),TR.forEach(t),d_=f(a),v(ht.$$.fragment,a),g_=f(a),dt=o(a,"TABLE",{});var ov=l(dt);fh=o(ov,"THEAD",{});var kR=l(fh);Fa=o(kR,"TR",{});var lv=l(Fa);ri=o(lv,"TH",{align:!0});var AR=l(ri);jb=u(AR,"Returned values"),AR.forEach(t),Tb=f(lv),hh=o(lv,"TH",{align:!0}),l(hh).forEach(t),lv.forEach(t),kR.forEach(t),kb=f(ov),xe=o(ov,"TBODY",{});var If=l(xe);Ja=o(If,"TR",{});var iv=l(Ja);oi=o(iv,"TD",{align:!0});var DR=l(oi);dh=o(DR,"STRONG",{});var OR=l(dh);Ab=u(OR,"sequence"),OR.forEach(t),DR.forEach(t),Db=f(iv),li=o(iv,"TD",{align:!0});var PR=l(li);Ob=u(PR,"The string sent as an input"),PR.forEach(t),iv.forEach(t),Pb=f(If),Ka=o(If,"TR",{});var uv=l(Ka);ii=o(uv,"TD",{align:!0});var RR=l(ii);gh=o(RR,"STRONG",{});var NR=l(gh);Rb=u(NR,"labels"),NR.forEach(t),RR.forEach(t),Nb=f(uv),ui=o(uv,"TD",{align:!0});var xR=l(ui);xb=u(xR,"The list of strings for labels that you sent (in order)"),xR.forEach(t),uv.forEach(t),Sb=f(If),Wa=o(If,"TR",{});var pv=l(Wa);pi=o(pv,"TD",{align:!0});var SR=l(pi);mh=o(SR,"STRONG",{});var IR=l(mh);Ib=u(IR,"scores"),IR.forEach(t),SR.forEach(t),Hb=f(pv),gt=o(pv,"TD",{align:!0});var cv=l(gt);Bb=u(cv,"a list of floats that correspond the the probability of label, in the same order as "),$h=o(cv,"CODE",{});var HR=l($h);Cb=u(HR,"labels"),HR.forEach(t),Gb=u(cv,"."),cv.forEach(t),pv.forEach(t),If.forEach(t),ov.forEach(t),m_=f(a),Se=o(a,"H2",{class:!0});var fv=l(Se);mt=o(fv,"A",{id:!0,class:!0,href:!0});var BR=l(mt);_h=o(BR,"SPAN",{});var CR=l(_h);v(Ya.$$.fragment,CR),CR.forEach(t),BR.forEach(t),Ub=f(fv),qh=o(fv,"SPAN",{});var GR=l(qh);Lb=u(GR,"Translation task"),GR.forEach(t),fv.forEach(t),$_=f(a),ci=o(a,"P",{});var UR=l(ci);zb=u(UR,"This task is well known to translate text from one language to another"),UR.forEach(t),__=f(a),v($t.$$.fragment,a),q_=f(a),Va=o(a,"P",{});var FP=l(Va);Mb=u(FP,"Available with: "),Xa=o(FP,"A",{href:!0,rel:!0});var LR=l(Xa);Fb=u(LR,"\u{1F917} Transformers"),LR.forEach(t),FP.forEach(t),v_=f(a),fi=o(a,"P",{});var zR=l(fi);Jb=u(zR,"Example:"),zR.forEach(t),y_=f(a),v(_t.$$.fragment,a),E_=f(a),hi=o(a,"P",{});var MR=l(hi);Kb=u(MR,`When sending your request, you should send a JSON encoded payload. Here
are all the options`),MR.forEach(t),w_=f(a),qt=o(a,"TABLE",{});var hv=l(qt);vh=o(hv,"THEAD",{});var FR=l(vh);Qa=o(FR,"TR",{});var dv=l(Qa);di=o(dv,"TH",{align:!0});var JR=l(di);Wb=u(JR,"All parameters"),JR.forEach(t),Yb=f(dv),yh=o(dv,"TH",{align:!0}),l(yh).forEach(t),dv.forEach(t),FR.forEach(t),Vb=f(hv),Z=o(hv,"TBODY",{});var Te=l(Z);Za=o(Te,"TR",{});var gv=l(Za);en=o(gv,"TD",{align:!0});var JP=l(en);Eh=o(JP,"STRONG",{});var KR=l(Eh);Xb=u(KR,"inputs"),KR.forEach(t),Qb=u(JP," (required)"),JP.forEach(t),Zb=f(gv),gi=o(gv,"TD",{align:!0});var WR=l(gi);ej=u(WR,"a string to be translated in the original languages"),WR.forEach(t),gv.forEach(t),tj=f(Te),tn=o(Te,"TR",{});var mv=l(tn);mi=o(mv,"TD",{align:!0});var YR=l(mi);wh=o(YR,"STRONG",{});var VR=l(wh);sj=u(VR,"options"),VR.forEach(t),YR.forEach(t),aj=f(mv),$i=o(mv,"TD",{align:!0});var XR=l($i);nj=u(XR,"a dict containing the following keys:"),XR.forEach(t),mv.forEach(t),rj=f(Te),sn=o(Te,"TR",{});var $v=l(sn);_i=o($v,"TD",{align:!0});var QR=l(_i);oj=u(QR,"use_gpu"),QR.forEach(t),lj=f($v),vt=o($v,"TD",{align:!0});var _v=l(vt);ij=u(_v,"(Default: "),bh=o(_v,"CODE",{});var ZR=l(bh);uj=u(ZR,"false"),ZR.forEach(t),pj=u(_v,"). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),_v.forEach(t),$v.forEach(t),cj=f(Te),an=o(Te,"TR",{});var qv=l(an);qi=o(qv,"TD",{align:!0});var eN=l(qi);fj=u(eN,"use_cache"),eN.forEach(t),hj=f(qv),yt=o(qv,"TD",{align:!0});var vv=l(yt);dj=u(vv,"(Default: "),jh=o(vv,"CODE",{});var tN=l(jh);gj=u(tN,"true"),tN.forEach(t),mj=u(vv,"). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),vv.forEach(t),qv.forEach(t),$j=f(Te),nn=o(Te,"TR",{});var yv=l(nn);vi=o(yv,"TD",{align:!0});var sN=l(vi);_j=u(sN,"wait_for_model"),sN.forEach(t),qj=f(yv),Et=o(yv,"TD",{align:!0});var Ev=l(Et);vj=u(Ev,"(Default: "),Th=o(Ev,"CODE",{});var aN=l(Th);yj=u(aN,"false"),aN.forEach(t),Ej=u(Ev,") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),Ev.forEach(t),yv.forEach(t),Te.forEach(t),hv.forEach(t),b_=f(a),yi=o(a,"P",{});var nN=l(yi);wj=u(nN,"Return value is either a dict or a list of dicts if you sent a list of inputs"),nN.forEach(t),j_=f(a),wt=o(a,"TABLE",{});var wv=l(wt);kh=o(wv,"THEAD",{});var rN=l(kh);rn=o(rN,"TR",{});var bv=l(rn);Ei=o(bv,"TH",{align:!0});var oN=l(Ei);bj=u(oN,"Returned values"),oN.forEach(t),jj=f(bv),Ah=o(bv,"TH",{align:!0}),l(Ah).forEach(t),bv.forEach(t),rN.forEach(t),Tj=f(wv),Dh=o(wv,"TBODY",{});var lN=l(Dh);on=o(lN,"TR",{});var jv=l(on);wi=o(jv,"TD",{align:!0});var iN=l(wi);Oh=o(iN,"STRONG",{});var uN=l(Oh);kj=u(uN,"translation_text"),uN.forEach(t),iN.forEach(t),Aj=f(jv),bi=o(jv,"TD",{align:!0});var pN=l(bi);Dj=u(pN,"The string after translation"),pN.forEach(t),jv.forEach(t),lN.forEach(t),wv.forEach(t),T_=f(a),Ie=o(a,"H2",{class:!0});var Tv=l(Ie);bt=o(Tv,"A",{id:!0,class:!0,href:!0});var cN=l(bt);Ph=o(cN,"SPAN",{});var fN=l(Ph);v(ln.$$.fragment,fN),fN.forEach(t),cN.forEach(t),Oj=f(Tv),Rh=o(Tv,"SPAN",{});var hN=l(Rh);Pj=u(hN,"Summarization task"),hN.forEach(t),Tv.forEach(t),k_=f(a),jt=o(a,"P",{});var kv=l(jt);Rj=u(kv,`This task is well known to summarize text a big text into a small text.
Be careful, some models have a maximum length of input. That means that
the summary cannot handle full books for instance. Be careful when
choosing your model. If you want to discuss you summarization needs,
please get in touch <`),ji=o(kv,"A",{href:!0});var dN=l(ji);Nj=u(dN,"api-enterprise@huggingface.co"),dN.forEach(t),xj=u(kv,">"),kv.forEach(t),A_=f(a),v(Tt.$$.fragment,a),D_=f(a),un=o(a,"P",{});var KP=l(un);Sj=u(KP,"Available with: "),pn=o(KP,"A",{href:!0,rel:!0});var gN=l(pn);Ij=u(gN,"\u{1F917} Transformers"),gN.forEach(t),KP.forEach(t),O_=f(a),Ti=o(a,"P",{});var mN=l(Ti);Hj=u(mN,"Example:"),mN.forEach(t),P_=f(a),v(kt.$$.fragment,a),R_=f(a),ki=o(a,"P",{});var $N=l(ki);Bj=u($N,`When sending your request, you should send a JSON encoded payload. Here
are all the options`),$N.forEach(t),N_=f(a),At=o(a,"TABLE",{});var Av=l(At);Nh=o(Av,"THEAD",{});var _N=l(Nh);cn=o(_N,"TR",{});var Dv=l(cn);Ai=o(Dv,"TH",{align:!0});var qN=l(Ai);Cj=u(qN,"All parameters"),qN.forEach(t),Gj=f(Dv),xh=o(Dv,"TH",{align:!0}),l(xh).forEach(t),Dv.forEach(t),_N.forEach(t),Uj=f(Av),G=o(Av,"TBODY",{});var U=l(G);fn=o(U,"TR",{});var Ov=l(fn);hn=o(Ov,"TD",{align:!0});var WP=l(hn);Sh=o(WP,"STRONG",{});var vN=l(Sh);Lj=u(vN,"inputs"),vN.forEach(t),zj=u(WP," (required)"),WP.forEach(t),Mj=f(Ov),Di=o(Ov,"TD",{align:!0});var yN=l(Di);Fj=u(yN,"a string to be summarized"),yN.forEach(t),Ov.forEach(t),Jj=f(U),dn=o(U,"TR",{});var Pv=l(dn);Oi=o(Pv,"TD",{align:!0});var EN=l(Oi);Ih=o(EN,"STRONG",{});var wN=l(Ih);Kj=u(wN,"parameters"),wN.forEach(t),EN.forEach(t),Wj=f(Pv),Pi=o(Pv,"TD",{align:!0});var bN=l(Pi);Yj=u(bN,"a dict containing the following keys:"),bN.forEach(t),Pv.forEach(t),Vj=f(U),gn=o(U,"TR",{});var Rv=l(gn);Ri=o(Rv,"TD",{align:!0});var jN=l(Ri);Xj=u(jN,"min_length"),jN.forEach(t),Qj=f(Rv),ce=o(Rv,"TD",{align:!0});var Hf=l(ce);Zj=u(Hf,"(Default: "),Hh=o(Hf,"CODE",{});var TN=l(Hh);e0=u(TN,"None"),TN.forEach(t),t0=u(Hf,"). Integer to define the minimum length "),Bh=o(Hf,"STRONG",{});var kN=l(Bh);s0=u(kN,"in tokens"),kN.forEach(t),a0=u(Hf," of the output summary."),Hf.forEach(t),Rv.forEach(t),n0=f(U),mn=o(U,"TR",{});var Nv=l(mn);Ni=o(Nv,"TD",{align:!0});var AN=l(Ni);r0=u(AN,"max_length"),AN.forEach(t),o0=f(Nv),fe=o(Nv,"TD",{align:!0});var Bf=l(fe);l0=u(Bf,"(Default: "),Ch=o(Bf,"CODE",{});var DN=l(Ch);i0=u(DN,"None"),DN.forEach(t),u0=u(Bf,"). Integer to define the maximum length "),Gh=o(Bf,"STRONG",{});var ON=l(Gh);p0=u(ON,"in tokens"),ON.forEach(t),c0=u(Bf," of the output summary."),Bf.forEach(t),Nv.forEach(t),f0=f(U),$n=o(U,"TR",{});var xv=l($n);xi=o(xv,"TD",{align:!0});var PN=l(xi);h0=u(PN,"top_k"),PN.forEach(t),d0=f(xv),he=o(xv,"TD",{align:!0});var Cf=l(he);g0=u(Cf,"(Default: "),Uh=o(Cf,"CODE",{});var RN=l(Uh);m0=u(RN,"None"),RN.forEach(t),$0=u(Cf,"). Integer to define the top tokens considered within the "),Lh=o(Cf,"CODE",{});var NN=l(Lh);_0=u(NN,"sample"),NN.forEach(t),q0=u(Cf," operation to create new text."),Cf.forEach(t),xv.forEach(t),v0=f(U),_n=o(U,"TR",{});var Sv=l(_n);Si=o(Sv,"TD",{align:!0});var xN=l(Si);y0=u(xN,"top_p"),xN.forEach(t),E0=f(Sv),de=o(Sv,"TD",{align:!0});var Gf=l(de);w0=u(Gf,"(Default: "),zh=o(Gf,"CODE",{});var SN=l(zh);b0=u(SN,"None"),SN.forEach(t),j0=u(Gf,"). Float to define the tokens that are within the sample"),Mh=o(Gf,"CODE",{});var IN=l(Mh);T0=u(IN,"operation of text generation. Add tokens in the sample for more probable to least probable until the sum of the probabilities is greater than"),IN.forEach(t),k0=u(Gf,"top_p`."),Gf.forEach(t),Sv.forEach(t),A0=f(U),qn=o(U,"TR",{});var Iv=l(qn);Ii=o(Iv,"TD",{align:!0});var HN=l(Ii);D0=u(HN,"temperature"),HN.forEach(t),O0=f(Iv),ge=o(Iv,"TD",{align:!0});var Uf=l(ge);P0=u(Uf,"(Default: "),Fh=o(Uf,"CODE",{});var BN=l(Fh);R0=u(BN,"1.0"),BN.forEach(t),N0=u(Uf,"). Float (0.0-100.0). The temperature of the sampling operation. 1 means regular sampling, 0 means "),Jh=o(Uf,"CODE",{});var CN=l(Jh);x0=u(CN,"100.0"),CN.forEach(t),S0=u(Uf," is getting closer to uniform probability."),Uf.forEach(t),Iv.forEach(t),I0=f(U),vn=o(U,"TR",{});var Hv=l(vn);Hi=o(Hv,"TD",{align:!0});var GN=l(Hi);H0=u(GN,"repetition_penalty"),GN.forEach(t),B0=f(Hv),Dt=o(Hv,"TD",{align:!0});var Bv=l(Dt);C0=u(Bv,"(Default: "),Kh=o(Bv,"CODE",{});var UN=l(Kh);G0=u(UN,"None"),UN.forEach(t),U0=u(Bv,"). Float (0.0-100.0). The more a token is used within generation the more it is penalized to not be picked in successive generation passes."),Bv.forEach(t),Hv.forEach(t),L0=f(U),yn=o(U,"TR",{});var Cv=l(yn);Bi=o(Cv,"TD",{align:!0});var LN=l(Bi);z0=u(LN,"max_time"),LN.forEach(t),M0=f(Cv),Ot=o(Cv,"TD",{align:!0});var Gv=l(Ot);F0=u(Gv,"(Default: "),Wh=o(Gv,"CODE",{});var zN=l(Wh);J0=u(zN,"None"),zN.forEach(t),K0=u(Gv,"). Float (0-120.0). The amount of time in seconds that the query should take maximum. Network can cause some overhead so it will be a soft limit."),Gv.forEach(t),Cv.forEach(t),W0=f(U),En=o(U,"TR",{});var Uv=l(En);Ci=o(Uv,"TD",{align:!0});var MN=l(Ci);Yh=o(MN,"STRONG",{});var FN=l(Yh);Y0=u(FN,"options"),FN.forEach(t),MN.forEach(t),V0=f(Uv),Gi=o(Uv,"TD",{align:!0});var JN=l(Gi);X0=u(JN,"a dict containing the following keys:"),JN.forEach(t),Uv.forEach(t),Q0=f(U),wn=o(U,"TR",{});var Lv=l(wn);Ui=o(Lv,"TD",{align:!0});var KN=l(Ui);Z0=u(KN,"use_gpu"),KN.forEach(t),eT=f(Lv),Pt=o(Lv,"TD",{align:!0});var zv=l(Pt);tT=u(zv,"(Default: "),Vh=o(zv,"CODE",{});var WN=l(Vh);sT=u(WN,"false"),WN.forEach(t),aT=u(zv,"). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),zv.forEach(t),Lv.forEach(t),nT=f(U),bn=o(U,"TR",{});var Mv=l(bn);Li=o(Mv,"TD",{align:!0});var YN=l(Li);rT=u(YN,"use_cache"),YN.forEach(t),oT=f(Mv),Rt=o(Mv,"TD",{align:!0});var Fv=l(Rt);lT=u(Fv,"(Default: "),Xh=o(Fv,"CODE",{});var VN=l(Xh);iT=u(VN,"true"),VN.forEach(t),uT=u(Fv,"). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),Fv.forEach(t),Mv.forEach(t),pT=f(U),jn=o(U,"TR",{});var Jv=l(jn);zi=o(Jv,"TD",{align:!0});var XN=l(zi);cT=u(XN,"wait_for_model"),XN.forEach(t),fT=f(Jv),Nt=o(Jv,"TD",{align:!0});var Kv=l(Nt);hT=u(Kv,"(Default: "),Qh=o(Kv,"CODE",{});var QN=l(Qh);dT=u(QN,"false"),QN.forEach(t),gT=u(Kv,") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),Kv.forEach(t),Jv.forEach(t),U.forEach(t),Av.forEach(t),x_=f(a),Mi=o(a,"P",{});var ZN=l(Mi);mT=u(ZN,"Return value is either a dict or a list of dicts if you sent a list of inputs"),ZN.forEach(t),S_=f(a),xt=o(a,"TABLE",{});var Wv=l(xt);Zh=o(Wv,"THEAD",{});var ex=l(Zh);Tn=o(ex,"TR",{});var Yv=l(Tn);Fi=o(Yv,"TH",{align:!0});var tx=l(Fi);$T=u(tx,"Returned values"),tx.forEach(t),_T=f(Yv),ed=o(Yv,"TH",{align:!0}),l(ed).forEach(t),Yv.forEach(t),ex.forEach(t),qT=f(Wv),td=o(Wv,"TBODY",{});var sx=l(td);kn=o(sx,"TR",{});var Vv=l(kn);Ji=o(Vv,"TD",{align:!0});var ax=l(Ji);sd=o(ax,"STRONG",{});var nx=l(sd);vT=u(nx,"summarization_text"),nx.forEach(t),ax.forEach(t),yT=f(Vv),Ki=o(Vv,"TD",{align:!0});var rx=l(Ki);ET=u(rx,"The string after translation"),rx.forEach(t),Vv.forEach(t),sx.forEach(t),Wv.forEach(t),I_=f(a),He=o(a,"H2",{class:!0});var Xv=l(He);St=o(Xv,"A",{id:!0,class:!0,href:!0});var ox=l(St);ad=o(ox,"SPAN",{});var lx=l(ad);v(An.$$.fragment,lx),lx.forEach(t),ox.forEach(t),wT=f(Xv),nd=o(Xv,"SPAN",{});var ix=l(nd);bT=u(ix,"Conversational task"),ix.forEach(t),Xv.forEach(t),H_=f(a),Wi=o(a,"P",{});var ux=l(Wi);jT=u(ux,`This task corresponds to any chatbot like structure. Models tend to have
shorted max_length, so please check with caution when using a given
model if you need long range dependency or not.`),ux.forEach(t),B_=f(a),v(It.$$.fragment,a),C_=f(a),Dn=o(a,"P",{});var YP=l(Dn);TT=u(YP,"Available with: "),On=o(YP,"A",{href:!0,rel:!0});var px=l(On);kT=u(px,"\u{1F917} Transformers"),px.forEach(t),YP.forEach(t),G_=f(a),Yi=o(a,"P",{});var cx=l(Yi);AT=u(cx,"Example:"),cx.forEach(t),U_=f(a),v(Ht.$$.fragment,a),L_=f(a),Vi=o(a,"P",{});var fx=l(Vi);DT=u(fx,`When sending your request, you should send a JSON encoded payload. Here
are all the options`),fx.forEach(t),z_=f(a),Bt=o(a,"TABLE",{});var Qv=l(Bt);rd=o(Qv,"THEAD",{});var hx=l(rd);Pn=o(hx,"TR",{});var Zv=l(Pn);Xi=o(Zv,"TH",{align:!0});var dx=l(Xi);OT=u(dx,"All parameters"),dx.forEach(t),PT=f(Zv),od=o(Zv,"TH",{align:!0}),l(od).forEach(t),Zv.forEach(t),hx.forEach(t),RT=f(Qv),x=o(Qv,"TBODY",{});var H=l(x);Rn=o(H,"TR",{});var e2=l(Rn);Nn=o(e2,"TD",{align:!0});var VP=l(Nn);ld=o(VP,"STRONG",{});var gx=l(ld);NT=u(gx,"inputs"),gx.forEach(t),xT=u(VP," (required)"),VP.forEach(t),ST=f(e2),id=o(e2,"TD",{align:!0}),l(id).forEach(t),e2.forEach(t),IT=f(H),xn=o(H,"TR",{});var t2=l(xn);Qi=o(t2,"TD",{align:!0});var mx=l(Qi);HT=u(mx,"text (required)"),mx.forEach(t),BT=f(t2),Zi=o(t2,"TD",{align:!0});var $x=l(Zi);CT=u($x,"The last input from the user in the conversation."),$x.forEach(t),t2.forEach(t),GT=f(H),Sn=o(H,"TR",{});var s2=l(Sn);eu=o(s2,"TD",{align:!0});var _x=l(eu);UT=u(_x,"generated_responses"),_x.forEach(t),LT=f(s2),tu=o(s2,"TD",{align:!0});var qx=l(tu);zT=u(qx,"A list of strings corresponding to the earlier replies from the model."),qx.forEach(t),s2.forEach(t),MT=f(H),In=o(H,"TR",{});var a2=l(In);su=o(a2,"TD",{align:!0});var vx=l(su);FT=u(vx,"past_user_inputs"),vx.forEach(t),JT=f(a2),Ct=o(a2,"TD",{align:!0});var n2=l(Ct);KT=u(n2,"A list of strings corresponding to the earlier replies from the user. Should be of the same length of "),ud=o(n2,"CODE",{});var yx=l(ud);WT=u(yx,"generated_responses"),yx.forEach(t),YT=u(n2,"."),n2.forEach(t),a2.forEach(t),VT=f(H),Hn=o(H,"TR",{});var r2=l(Hn);au=o(r2,"TD",{align:!0});var Ex=l(au);pd=o(Ex,"STRONG",{});var wx=l(pd);XT=u(wx,"parameters"),wx.forEach(t),Ex.forEach(t),QT=f(r2),nu=o(r2,"TD",{align:!0});var bx=l(nu);ZT=u(bx,"a dict containing the following keys:"),bx.forEach(t),r2.forEach(t),e3=f(H),Bn=o(H,"TR",{});var o2=l(Bn);ru=o(o2,"TD",{align:!0});var jx=l(ru);t3=u(jx,"min_length"),jx.forEach(t),s3=f(o2),me=o(o2,"TD",{align:!0});var Lf=l(me);a3=u(Lf,"(Default: "),cd=o(Lf,"CODE",{});var Tx=l(cd);n3=u(Tx,"None"),Tx.forEach(t),r3=u(Lf,"). Integer to define the minimum length "),fd=o(Lf,"STRONG",{});var kx=l(fd);o3=u(kx,"in tokens"),kx.forEach(t),l3=u(Lf," of the output summary."),Lf.forEach(t),o2.forEach(t),i3=f(H),Cn=o(H,"TR",{});var l2=l(Cn);ou=o(l2,"TD",{align:!0});var Ax=l(ou);u3=u(Ax,"max_length"),Ax.forEach(t),p3=f(l2),$e=o(l2,"TD",{align:!0});var zf=l($e);c3=u(zf,"(Default: "),hd=o(zf,"CODE",{});var Dx=l(hd);f3=u(Dx,"None"),Dx.forEach(t),h3=u(zf,"). Integer to define the maximum length "),dd=o(zf,"STRONG",{});var Ox=l(dd);d3=u(Ox,"in tokens"),Ox.forEach(t),g3=u(zf," of the output summary."),zf.forEach(t),l2.forEach(t),m3=f(H),Gn=o(H,"TR",{});var i2=l(Gn);lu=o(i2,"TD",{align:!0});var Px=l(lu);$3=u(Px,"top_k"),Px.forEach(t),_3=f(i2),_e=o(i2,"TD",{align:!0});var Mf=l(_e);q3=u(Mf,"(Default: "),gd=o(Mf,"CODE",{});var Rx=l(gd);v3=u(Rx,"None"),Rx.forEach(t),y3=u(Mf,"). Integer to define the top tokens considered within the "),md=o(Mf,"CODE",{});var Nx=l(md);E3=u(Nx,"sample"),Nx.forEach(t),w3=u(Mf," operation to create new text."),Mf.forEach(t),i2.forEach(t),b3=f(H),Un=o(H,"TR",{});var u2=l(Un);iu=o(u2,"TD",{align:!0});var xx=l(iu);j3=u(xx,"top_p"),xx.forEach(t),T3=f(u2),qe=o(u2,"TD",{align:!0});var Ff=l(qe);k3=u(Ff,"(Default: "),$d=o(Ff,"CODE",{});var Sx=l($d);A3=u(Sx,"None"),Sx.forEach(t),D3=u(Ff,"). Float to define the tokens that are within the sample"),_d=o(Ff,"CODE",{});var Ix=l(_d);O3=u(Ix,"operation of text generation. Add tokens in the sample for more probable to least probable until the sum of the probabilities is greater than"),Ix.forEach(t),P3=u(Ff,"top_p`."),Ff.forEach(t),u2.forEach(t),R3=f(H),Ln=o(H,"TR",{});var p2=l(Ln);uu=o(p2,"TD",{align:!0});var Hx=l(uu);N3=u(Hx,"temperature"),Hx.forEach(t),x3=f(p2),ve=o(p2,"TD",{align:!0});var Jf=l(ve);S3=u(Jf,"(Default: "),qd=o(Jf,"CODE",{});var Bx=l(qd);I3=u(Bx,"1.0"),Bx.forEach(t),H3=u(Jf,"). Float (0.0-100.0). The temperature of the sampling operation. 1 means regular sampling, 0 means "),vd=o(Jf,"CODE",{});var Cx=l(vd);B3=u(Cx,"100.0"),Cx.forEach(t),C3=u(Jf," is getting closer to uniform probability."),Jf.forEach(t),p2.forEach(t),G3=f(H),zn=o(H,"TR",{});var c2=l(zn);pu=o(c2,"TD",{align:!0});var Gx=l(pu);U3=u(Gx,"repetition_penalty"),Gx.forEach(t),L3=f(c2),Gt=o(c2,"TD",{align:!0});var f2=l(Gt);z3=u(f2,"(Default: "),yd=o(f2,"CODE",{});var Ux=l(yd);M3=u(Ux,"None"),Ux.forEach(t),F3=u(f2,"). Float (0.0-100.0). The more a token is used within generation the more it is penalized to not be picked in successive generation passes."),f2.forEach(t),c2.forEach(t),J3=f(H),Mn=o(H,"TR",{});var h2=l(Mn);cu=o(h2,"TD",{align:!0});var Lx=l(cu);K3=u(Lx,"max_time"),Lx.forEach(t),W3=f(h2),Ut=o(h2,"TD",{align:!0});var d2=l(Ut);Y3=u(d2,"(Default: "),Ed=o(d2,"CODE",{});var zx=l(Ed);V3=u(zx,"None"),zx.forEach(t),X3=u(d2,"). Float (0-120.0). The amount of time in seconds that the query should take maximum. Network can cause some overhead so it will be a soft limit."),d2.forEach(t),h2.forEach(t),Q3=f(H),Fn=o(H,"TR",{});var g2=l(Fn);fu=o(g2,"TD",{align:!0});var Mx=l(fu);wd=o(Mx,"STRONG",{});var Fx=l(wd);Z3=u(Fx,"options"),Fx.forEach(t),Mx.forEach(t),e5=f(g2),hu=o(g2,"TD",{align:!0});var Jx=l(hu);t5=u(Jx,"a dict containing the following keys:"),Jx.forEach(t),g2.forEach(t),s5=f(H),Jn=o(H,"TR",{});var m2=l(Jn);du=o(m2,"TD",{align:!0});var Kx=l(du);a5=u(Kx,"use_gpu"),Kx.forEach(t),n5=f(m2),Lt=o(m2,"TD",{align:!0});var $2=l(Lt);r5=u($2,"(Default: "),bd=o($2,"CODE",{});var Wx=l(bd);o5=u(Wx,"false"),Wx.forEach(t),l5=u($2,"). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),$2.forEach(t),m2.forEach(t),i5=f(H),Kn=o(H,"TR",{});var _2=l(Kn);gu=o(_2,"TD",{align:!0});var Yx=l(gu);u5=u(Yx,"use_cache"),Yx.forEach(t),p5=f(_2),zt=o(_2,"TD",{align:!0});var q2=l(zt);c5=u(q2,"(Default: "),jd=o(q2,"CODE",{});var Vx=l(jd);f5=u(Vx,"true"),Vx.forEach(t),h5=u(q2,"). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),q2.forEach(t),_2.forEach(t),d5=f(H),Wn=o(H,"TR",{});var v2=l(Wn);mu=o(v2,"TD",{align:!0});var Xx=l(mu);g5=u(Xx,"wait_for_model"),Xx.forEach(t),m5=f(v2),Mt=o(v2,"TD",{align:!0});var y2=l(Mt);$5=u(y2,"(Default: "),Td=o(y2,"CODE",{});var Qx=l(Td);_5=u(Qx,"false"),Qx.forEach(t),q5=u(y2,") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),y2.forEach(t),v2.forEach(t),H.forEach(t),Qv.forEach(t),M_=f(a),$u=o(a,"P",{});var Zx=l($u);v5=u(Zx,"Return value is either a dict or a list of dicts if you sent a list of inputs"),Zx.forEach(t),F_=f(a),Ft=o(a,"TABLE",{});var E2=l(Ft);kd=o(E2,"THEAD",{});var eS=l(kd);Yn=o(eS,"TR",{});var w2=l(Yn);_u=o(w2,"TH",{align:!0});var tS=l(_u);y5=u(tS,"Returned values"),tS.forEach(t),E5=f(w2),Ad=o(w2,"TH",{align:!0}),l(Ad).forEach(t),w2.forEach(t),eS.forEach(t),w5=f(E2),re=o(E2,"TBODY",{});var Ta=l(re);Vn=o(Ta,"TR",{});var b2=l(Vn);qu=o(b2,"TD",{align:!0});var sS=l(qu);Dd=o(sS,"STRONG",{});var aS=l(Dd);b5=u(aS,"generated_text"),aS.forEach(t),sS.forEach(t),j5=f(b2),vu=o(b2,"TD",{align:!0});var nS=l(vu);T5=u(nS,"The answer of the bot"),nS.forEach(t),b2.forEach(t),k5=f(Ta),Xn=o(Ta,"TR",{});var j2=l(Xn);yu=o(j2,"TD",{align:!0});var rS=l(yu);Od=o(rS,"STRONG",{});var oS=l(Od);A5=u(oS,"conversation"),oS.forEach(t),rS.forEach(t),D5=f(j2),Eu=o(j2,"TD",{align:!0});var lS=l(Eu);O5=u(lS,"A facility dictionnary to send back for the next input (with the new user input addition)."),lS.forEach(t),j2.forEach(t),P5=f(Ta),Qn=o(Ta,"TR",{});var T2=l(Qn);wu=o(T2,"TD",{align:!0});var iS=l(wu);R5=u(iS,"past_user_inputs"),iS.forEach(t),N5=f(T2),bu=o(T2,"TD",{align:!0});var uS=l(bu);x5=u(uS,"List of strings. The last inputs from the user in the conversation, <em>after the model has run."),uS.forEach(t),T2.forEach(t),S5=f(Ta),Zn=o(Ta,"TR",{});var k2=l(Zn);ju=o(k2,"TD",{align:!0});var pS=l(ju);I5=u(pS,"generated_responses"),pS.forEach(t),H5=f(k2),Tu=o(k2,"TD",{align:!0});var cS=l(Tu);B5=u(cS,"List of strings. The last outputs from the model in the conversation, <em>after the model has run."),cS.forEach(t),k2.forEach(t),Ta.forEach(t),E2.forEach(t),J_=f(a),Be=o(a,"H2",{class:!0});var A2=l(Be);Jt=o(A2,"A",{id:!0,class:!0,href:!0});var fS=l(Jt);Pd=o(fS,"SPAN",{});var hS=l(Pd);v(er.$$.fragment,hS),hS.forEach(t),fS.forEach(t),C5=f(A2),Rd=o(A2,"SPAN",{});var dS=l(Rd);G5=u(dS,"Table question answering task"),dS.forEach(t),A2.forEach(t),K_=f(a),ku=o(a,"P",{});var gS=l(ku);U5=u(gS,`Don\u2019t know SQL? Don\u2019t want to dive into a large spreadsheet? Ask
questions in plain english!`),gS.forEach(t),W_=f(a),v(Kt.$$.fragment,a),Y_=f(a),tr=o(a,"P",{});var XP=l(tr);L5=u(XP,"Available with: "),sr=o(XP,"A",{href:!0,rel:!0});var mS=l(sr);z5=u(mS,"\u{1F917} Transformers"),mS.forEach(t),XP.forEach(t),V_=f(a),Au=o(a,"P",{});var $S=l(Au);M5=u($S,"Example:"),$S.forEach(t),X_=f(a),v(Wt.$$.fragment,a),Q_=f(a),Du=o(a,"P",{});var _S=l(Du);F5=u(_S,`When sending your request, you should send a JSON encoded payload. Here
are all the options`),_S.forEach(t),Z_=f(a),Yt=o(a,"TABLE",{});var D2=l(Yt);Nd=o(D2,"THEAD",{});var qS=l(Nd);ar=o(qS,"TR",{});var O2=l(ar);Ou=o(O2,"TH",{align:!0});var vS=l(Ou);J5=u(vS,"All parameters"),vS.forEach(t),K5=f(O2),xd=o(O2,"TH",{align:!0}),l(xd).forEach(t),O2.forEach(t),qS.forEach(t),W5=f(D2),J=o(D2,"TBODY",{});var V=l(J);nr=o(V,"TR",{});var P2=l(nr);rr=o(P2,"TD",{align:!0});var QP=l(rr);Sd=o(QP,"STRONG",{});var yS=l(Sd);Y5=u(yS,"inputs"),yS.forEach(t),V5=u(QP," (required)"),QP.forEach(t),X5=f(P2),Id=o(P2,"TD",{align:!0}),l(Id).forEach(t),P2.forEach(t),Q5=f(V),or=o(V,"TR",{});var R2=l(or);Pu=o(R2,"TD",{align:!0});var ES=l(Pu);Z5=u(ES,"query (required)"),ES.forEach(t),ek=f(R2),Ru=o(R2,"TD",{align:!0});var wS=l(Ru);tk=u(wS,"The query in plain text that you want to ask the table"),wS.forEach(t),R2.forEach(t),sk=f(V),lr=o(V,"TR",{});var N2=l(lr);Nu=o(N2,"TD",{align:!0});var bS=l(Nu);ak=u(bS,"table (required)"),bS.forEach(t),nk=f(N2),xu=o(N2,"TD",{align:!0});var jS=l(xu);rk=u(jS,"A table of data represented as a dict of list where entries are headers and the lists are all the values, all lists must have the same size."),jS.forEach(t),N2.forEach(t),ok=f(V),ir=o(V,"TR",{});var x2=l(ir);Su=o(x2,"TD",{align:!0});var TS=l(Su);Hd=o(TS,"STRONG",{});var kS=l(Hd);lk=u(kS,"options"),kS.forEach(t),TS.forEach(t),ik=f(x2),Iu=o(x2,"TD",{align:!0});var AS=l(Iu);uk=u(AS,"a dict containing the following keys:"),AS.forEach(t),x2.forEach(t),pk=f(V),ur=o(V,"TR",{});var S2=l(ur);Hu=o(S2,"TD",{align:!0});var DS=l(Hu);ck=u(DS,"use_gpu"),DS.forEach(t),fk=f(S2),Vt=o(S2,"TD",{align:!0});var I2=l(Vt);hk=u(I2,"(Default: "),Bd=o(I2,"CODE",{});var OS=l(Bd);dk=u(OS,"false"),OS.forEach(t),gk=u(I2,"). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),I2.forEach(t),S2.forEach(t),mk=f(V),pr=o(V,"TR",{});var H2=l(pr);Bu=o(H2,"TD",{align:!0});var PS=l(Bu);$k=u(PS,"use_cache"),PS.forEach(t),_k=f(H2),Xt=o(H2,"TD",{align:!0});var B2=l(Xt);qk=u(B2,"(Default: "),Cd=o(B2,"CODE",{});var RS=l(Cd);vk=u(RS,"true"),RS.forEach(t),yk=u(B2,"). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),B2.forEach(t),H2.forEach(t),Ek=f(V),cr=o(V,"TR",{});var C2=l(cr);Cu=o(C2,"TD",{align:!0});var NS=l(Cu);wk=u(NS,"wait_for_model"),NS.forEach(t),bk=f(C2),Qt=o(C2,"TD",{align:!0});var G2=l(Qt);jk=u(G2,"(Default: "),Gd=o(G2,"CODE",{});var xS=l(Gd);Tk=u(xS,"false"),xS.forEach(t),kk=u(G2,") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),G2.forEach(t),C2.forEach(t),V.forEach(t),D2.forEach(t),eq=f(a),Gu=o(a,"P",{});var SS=l(Gu);Ak=u(SS,"Return value is either a dict or a list of dicts if you sent a list of inputs"),SS.forEach(t),tq=f(a),v(Zt.$$.fragment,a),sq=f(a),es=o(a,"TABLE",{});var U2=l(es);Ud=o(U2,"THEAD",{});var IS=l(Ud);fr=o(IS,"TR",{});var L2=l(fr);Uu=o(L2,"TH",{align:!0});var HS=l(Uu);Dk=u(HS,"Returned values"),HS.forEach(t),Ok=f(L2),Ld=o(L2,"TH",{align:!0}),l(Ld).forEach(t),L2.forEach(t),IS.forEach(t),Pk=f(U2),oe=o(U2,"TBODY",{});var ka=l(oe);hr=o(ka,"TR",{});var z2=l(hr);Lu=o(z2,"TD",{align:!0});var BS=l(Lu);zd=o(BS,"STRONG",{});var CS=l(zd);Rk=u(CS,"answer"),CS.forEach(t),BS.forEach(t),Nk=f(z2),zu=o(z2,"TD",{align:!0});var GS=l(zu);xk=u(GS,"The plaintext answer"),GS.forEach(t),z2.forEach(t),Sk=f(ka),dr=o(ka,"TR",{});var M2=l(dr);Mu=o(M2,"TD",{align:!0});var US=l(Mu);Md=o(US,"STRONG",{});var LS=l(Md);Ik=u(LS,"coordinates"),LS.forEach(t),US.forEach(t),Hk=f(M2),Fu=o(M2,"TD",{align:!0});var zS=l(Fu);Bk=u(zS,"a list of coordinates of the cells references in the answer"),zS.forEach(t),M2.forEach(t),Ck=f(ka),gr=o(ka,"TR",{});var F2=l(gr);Ju=o(F2,"TD",{align:!0});var MS=l(Ju);Fd=o(MS,"STRONG",{});var FS=l(Fd);Gk=u(FS,"cells"),FS.forEach(t),MS.forEach(t),Uk=f(F2),Ku=o(F2,"TD",{align:!0});var JS=l(Ku);Lk=u(JS,"a list of coordinates of the cells contents"),JS.forEach(t),F2.forEach(t),zk=f(ka),mr=o(ka,"TR",{});var J2=l(mr);Wu=o(J2,"TD",{align:!0});var KS=l(Wu);Jd=o(KS,"STRONG",{});var WS=l(Jd);Mk=u(WS,"aggregator"),WS.forEach(t),KS.forEach(t),Fk=f(J2),Yu=o(J2,"TD",{align:!0});var YS=l(Yu);Jk=u(YS,"The aggregator used to get the answer"),YS.forEach(t),J2.forEach(t),ka.forEach(t),U2.forEach(t),aq=f(a),Ce=o(a,"H2",{class:!0});var K2=l(Ce);ts=o(K2,"A",{id:!0,class:!0,href:!0});var VS=l(ts);Kd=o(VS,"SPAN",{});var XS=l(Kd);v($r.$$.fragment,XS),XS.forEach(t),VS.forEach(t),Kk=f(K2),Wd=o(K2,"SPAN",{});var QS=l(Wd);Wk=u(QS,"Question answering task"),QS.forEach(t),K2.forEach(t),nq=f(a),Vu=o(a,"P",{});var ZS=l(Vu);Yk=u(ZS,"Want to have a nice know-it-all bot that can answer any questions ?"),ZS.forEach(t),rq=f(a),v(ss.$$.fragment,a),oq=f(a),Ge=o(a,"P",{});var X$=l(Ge);Vk=u(X$,"Available with: "),_r=o(X$,"A",{href:!0,rel:!0});var eI=l(_r);Xk=u(eI,"\u{1F917}Transformers"),eI.forEach(t),Qk=u(X$,` and
`),qr=o(X$,"A",{href:!0,rel:!0});var tI=l(qr);Zk=u(tI,"AllenNLP"),tI.forEach(t),X$.forEach(t),lq=f(a),Xu=o(a,"P",{});var sI=l(Xu);e4=u(sI,"Example:"),sI.forEach(t),iq=f(a),v(as.$$.fragment,a),uq=f(a),Qu=o(a,"P",{});var aI=l(Qu);t4=u(aI,`When sending your request, you should send a JSON encoded payload. Here
are all the options`),aI.forEach(t),pq=f(a),Zu=o(a,"P",{});var nI=l(Zu);s4=u(nI,"Return value is either a dict or a list of dicts if you sent a list of inputs"),nI.forEach(t),cq=f(a),v(ns.$$.fragment,a),fq=f(a),rs=o(a,"TABLE",{});var W2=l(rs);Yd=o(W2,"THEAD",{});var rI=l(Yd);vr=o(rI,"TR",{});var Y2=l(vr);ep=o(Y2,"TH",{align:!0});var oI=l(ep);a4=u(oI,"Returned values"),oI.forEach(t),n4=f(Y2),Vd=o(Y2,"TH",{align:!0}),l(Vd).forEach(t),Y2.forEach(t),rI.forEach(t),r4=f(W2),le=o(W2,"TBODY",{});var Aa=l(le);yr=o(Aa,"TR",{});var V2=l(yr);tp=o(V2,"TD",{align:!0});var lI=l(tp);Xd=o(lI,"STRONG",{});var iI=l(Xd);o4=u(iI,"answer"),iI.forEach(t),lI.forEach(t),l4=f(V2),sp=o(V2,"TD",{align:!0});var uI=l(sp);i4=u(uI,"A string that\u2019s the answer within the text."),uI.forEach(t),V2.forEach(t),u4=f(Aa),Er=o(Aa,"TR",{});var X2=l(Er);ap=o(X2,"TD",{align:!0});var pI=l(ap);Qd=o(pI,"STRONG",{});var cI=l(Qd);p4=u(cI,"score"),cI.forEach(t),pI.forEach(t),c4=f(X2),np=o(X2,"TD",{align:!0});var fI=l(np);f4=u(fI,"A floats that represents how likely that the answer is correct"),fI.forEach(t),X2.forEach(t),h4=f(Aa),wr=o(Aa,"TR",{});var Q2=l(wr);rp=o(Q2,"TD",{align:!0});var hI=l(rp);Zd=o(hI,"STRONG",{});var dI=l(Zd);d4=u(dI,"start"),dI.forEach(t),hI.forEach(t),g4=f(Q2),os=o(Q2,"TD",{align:!0});var Z2=l(os);m4=u(Z2,"The index (string wise) of the start of the answer within "),eg=o(Z2,"CODE",{});var gI=l(eg);$4=u(gI,"context"),gI.forEach(t),_4=u(Z2,"."),Z2.forEach(t),Q2.forEach(t),q4=f(Aa),br=o(Aa,"TR",{});var ey=l(br);op=o(ey,"TD",{align:!0});var mI=l(op);tg=o(mI,"STRONG",{});var $I=l(tg);v4=u($I,"stop"),$I.forEach(t),mI.forEach(t),y4=f(ey),ls=o(ey,"TD",{align:!0});var ty=l(ls);E4=u(ty,"The index (string wise) of the stop of the answer within "),sg=o(ty,"CODE",{});var _I=l(sg);w4=u(_I,"context"),_I.forEach(t),b4=u(ty,"."),ty.forEach(t),ey.forEach(t),Aa.forEach(t),W2.forEach(t),hq=f(a),Ue=o(a,"H2",{class:!0});var sy=l(Ue);is=o(sy,"A",{id:!0,class:!0,href:!0});var qI=l(is);ag=o(qI,"SPAN",{});var vI=l(ag);v(jr.$$.fragment,vI),vI.forEach(t),qI.forEach(t),j4=f(sy),ng=o(sy,"SPAN",{});var yI=l(ng);T4=u(yI,"Text-classification task"),yI.forEach(t),sy.forEach(t),dq=f(a),lp=o(a,"P",{});var EI=l(lp);k4=u(EI,`Usually used for sentiment-analysis this will output the likelihood of
classes of an input.`),EI.forEach(t),gq=f(a),v(us.$$.fragment,a),mq=f(a),Tr=o(a,"P",{});var ZP=l(Tr);A4=u(ZP,"Available with: "),kr=o(ZP,"A",{href:!0,rel:!0});var wI=l(kr);D4=u(wI,"\u{1F917} Transformers"),wI.forEach(t),ZP.forEach(t),$q=f(a),ip=o(a,"P",{});var bI=l(ip);O4=u(bI,"Example:"),bI.forEach(t),_q=f(a),v(ps.$$.fragment,a),qq=f(a),up=o(a,"P",{});var jI=l(up);P4=u(jI,`When sending your request, you should send a JSON encoded payload. Here
are all the options`),jI.forEach(t),vq=f(a),cs=o(a,"TABLE",{});var ay=l(cs);rg=o(ay,"THEAD",{});var TI=l(rg);Ar=o(TI,"TR",{});var ny=l(Ar);pp=o(ny,"TH",{align:!0});var kI=l(pp);R4=u(kI,"All parameters"),kI.forEach(t),N4=f(ny),og=o(ny,"TH",{align:!0}),l(og).forEach(t),ny.forEach(t),TI.forEach(t),x4=f(ay),ee=o(ay,"TBODY",{});var ke=l(ee);Dr=o(ke,"TR",{});var ry=l(Dr);Or=o(ry,"TD",{align:!0});var eR=l(Or);lg=o(eR,"STRONG",{});var AI=l(lg);S4=u(AI,"inputs"),AI.forEach(t),I4=u(eR," (required)"),eR.forEach(t),H4=f(ry),cp=o(ry,"TD",{align:!0});var DI=l(cp);B4=u(DI,"a string to be classified"),DI.forEach(t),ry.forEach(t),C4=f(ke),Pr=o(ke,"TR",{});var oy=l(Pr);fp=o(oy,"TD",{align:!0});var OI=l(fp);ig=o(OI,"STRONG",{});var PI=l(ig);G4=u(PI,"options"),PI.forEach(t),OI.forEach(t),U4=f(oy),hp=o(oy,"TD",{align:!0});var RI=l(hp);L4=u(RI,"a dict containing the following keys:"),RI.forEach(t),oy.forEach(t),z4=f(ke),Rr=o(ke,"TR",{});var ly=l(Rr);dp=o(ly,"TD",{align:!0});var NI=l(dp);M4=u(NI,"use_gpu"),NI.forEach(t),F4=f(ly),fs=o(ly,"TD",{align:!0});var iy=l(fs);J4=u(iy,"(Default: "),ug=o(iy,"CODE",{});var xI=l(ug);K4=u(xI,"false"),xI.forEach(t),W4=u(iy,"). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),iy.forEach(t),ly.forEach(t),Y4=f(ke),Nr=o(ke,"TR",{});var uy=l(Nr);gp=o(uy,"TD",{align:!0});var SI=l(gp);V4=u(SI,"use_cache"),SI.forEach(t),X4=f(uy),hs=o(uy,"TD",{align:!0});var py=l(hs);Q4=u(py,"(Default: "),pg=o(py,"CODE",{});var II=l(pg);Z4=u(II,"true"),II.forEach(t),e7=u(py,"). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),py.forEach(t),uy.forEach(t),t7=f(ke),xr=o(ke,"TR",{});var cy=l(xr);mp=o(cy,"TD",{align:!0});var HI=l(mp);s7=u(HI,"wait_for_model"),HI.forEach(t),a7=f(cy),ds=o(cy,"TD",{align:!0});var fy=l(ds);n7=u(fy,"(Default: "),cg=o(fy,"CODE",{});var BI=l(cg);r7=u(BI,"false"),BI.forEach(t),o7=u(fy,") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),fy.forEach(t),cy.forEach(t),ke.forEach(t),ay.forEach(t),yq=f(a),$p=o(a,"P",{});var CI=l($p);l7=u(CI,"Return value is either a dict or a list of dicts if you sent a list of inputs"),CI.forEach(t),Eq=f(a),v(gs.$$.fragment,a),wq=f(a),ms=o(a,"TABLE",{});var hy=l(ms);fg=o(hy,"THEAD",{});var GI=l(fg);Sr=o(GI,"TR",{});var dy=l(Sr);_p=o(dy,"TH",{align:!0});var UI=l(_p);i7=u(UI,"Returned values"),UI.forEach(t),u7=f(dy),hg=o(dy,"TH",{align:!0}),l(hg).forEach(t),dy.forEach(t),GI.forEach(t),p7=f(hy),Ir=o(hy,"TBODY",{});var gy=l(Ir);Hr=o(gy,"TR",{});var my=l(Hr);qp=o(my,"TD",{align:!0});var LI=l(qp);dg=o(LI,"STRONG",{});var zI=l(dg);c7=u(zI,"label"),zI.forEach(t),LI.forEach(t),f7=f(my),vp=o(my,"TD",{align:!0});var MI=l(vp);h7=u(MI,"The label for the class (model specific)"),MI.forEach(t),my.forEach(t),d7=f(gy),Br=o(gy,"TR",{});var $y=l(Br);yp=o($y,"TD",{align:!0});var FI=l(yp);gg=o(FI,"STRONG",{});var JI=l(gg);g7=u(JI,"score"),JI.forEach(t),FI.forEach(t),m7=f($y),Ep=o($y,"TD",{align:!0});var KI=l(Ep);$7=u(KI,"A floats that represents how likely is that the text belongs the this class."),KI.forEach(t),$y.forEach(t),gy.forEach(t),hy.forEach(t),bq=f(a),Le=o(a,"H2",{class:!0});var _y=l(Le);$s=o(_y,"A",{id:!0,class:!0,href:!0});var WI=l($s);mg=o(WI,"SPAN",{});var YI=l(mg);v(Cr.$$.fragment,YI),YI.forEach(t),WI.forEach(t),_7=f(_y),$g=o(_y,"SPAN",{});var VI=l($g);q7=u(VI,"Named Entity Recognition (NER) task"),VI.forEach(t),_y.forEach(t),jq=f(a),Gr=o(a,"P",{});var tR=l(Gr);v7=u(tR,"See "),wp=o(tR,"A",{href:!0});var XI=l(wp);y7=u(XI,"Token-classification task"),XI.forEach(t),tR.forEach(t),Tq=f(a),ze=o(a,"H2",{class:!0});var qy=l(ze);_s=o(qy,"A",{id:!0,class:!0,href:!0});var QI=l(_s);_g=o(QI,"SPAN",{});var ZI=l(_g);v(Ur.$$.fragment,ZI),ZI.forEach(t),QI.forEach(t),E7=f(qy),qg=o(qy,"SPAN",{});var eH=l(qg);w7=u(eH,"Token-classification task"),eH.forEach(t),qy.forEach(t),kq=f(a),bp=o(a,"P",{});var tH=l(bp);b7=u(tH,`Usually used for sentence parsing, either grammatical, or Named Entity
Recognition (NER) to understand keywords contained within text.`),tH.forEach(t),Aq=f(a),v(qs.$$.fragment,a),Dq=f(a),Me=o(a,"P",{});var Q$=l(Me);j7=u(Q$,"Available with: "),Lr=o(Q$,"A",{href:!0,rel:!0});var sH=l(Lr);T7=u(sH,"\u{1F917} Transformers"),sH.forEach(t),k7=u(Q$,`,
`),zr=o(Q$,"A",{href:!0,rel:!0});var aH=l(zr);A7=u(aH,"Flair"),aH.forEach(t),Q$.forEach(t),Oq=f(a),jp=o(a,"P",{});var nH=l(jp);D7=u(nH,"Example:"),nH.forEach(t),Pq=f(a),v(vs.$$.fragment,a),Rq=f(a),Tp=o(a,"P",{});var rH=l(Tp);O7=u(rH,`When sending your request, you should send a JSON encoded payload. Here
are all the options`),rH.forEach(t),Nq=f(a),ys=o(a,"TABLE",{});var vy=l(ys);vg=o(vy,"THEAD",{});var oH=l(vg);Mr=o(oH,"TR",{});var yy=l(Mr);kp=o(yy,"TH",{align:!0});var lH=l(kp);P7=u(lH,"All parameters"),lH.forEach(t),R7=f(yy),yg=o(yy,"TH",{align:!0}),l(yg).forEach(t),yy.forEach(t),oH.forEach(t),N7=f(vy),K=o(vy,"TBODY",{});var X=l(K);Fr=o(X,"TR",{});var Ey=l(Fr);Jr=o(Ey,"TD",{align:!0});var sR=l(Jr);Eg=o(sR,"STRONG",{});var iH=l(Eg);x7=u(iH,"inputs"),iH.forEach(t),S7=u(sR," (required)"),sR.forEach(t),I7=f(Ey),Ap=o(Ey,"TD",{align:!0});var uH=l(Ap);H7=u(uH,"a string to be classified"),uH.forEach(t),Ey.forEach(t),B7=f(X),Kr=o(X,"TR",{});var wy=l(Kr);Dp=o(wy,"TD",{align:!0});var pH=l(Dp);wg=o(pH,"STRONG",{});var cH=l(wg);C7=u(cH,"parameters"),cH.forEach(t),pH.forEach(t),G7=f(wy),Op=o(wy,"TD",{align:!0});var fH=l(Op);U7=u(fH,"a dict containing the following key:"),fH.forEach(t),wy.forEach(t),L7=f(X),Wr=o(X,"TR",{});var by=l(Wr);Pp=o(by,"TD",{align:!0});var hH=l(Pp);z7=u(hH,"aggregation_strategy"),hH.forEach(t),M7=f(by),S=o(by,"TD",{align:!0});var B=l(S);F7=u(B,"(Default: "),bg=o(B,"CODE",{});var dH=l(bg);J7=u(dH,"simple"),dH.forEach(t),K7=u(B,"). There are several aggregation strategies: "),W7=o(B,"BR",{}),Y7=f(B),jg=o(B,"CODE",{});var gH=l(jg);V7=u(gH,"none"),gH.forEach(t),X7=u(B,": Every token gets classified without further aggregation. "),Q7=o(B,"BR",{}),Z7=f(B),Tg=o(B,"CODE",{});var mH=l(Tg);e6=u(mH,"simple"),mH.forEach(t),t6=u(B,": Entities are grouped according to the default schema (B-, I- tags get merged when the tag is similar). "),s6=o(B,"BR",{}),a6=f(B),kg=o(B,"CODE",{});var $H=l(kg);n6=u($H,"first"),$H.forEach(t),r6=u(B,": Same as the "),Ag=o(B,"CODE",{});var _H=l(Ag);o6=u(_H,"simple"),_H.forEach(t),l6=u(B," strategy except words cannot end up with different tags. Words will use the tag of the first token when there is ambiguity. "),i6=o(B,"BR",{}),u6=f(B),Dg=o(B,"CODE",{});var qH=l(Dg);p6=u(qH,"average"),qH.forEach(t),c6=u(B,": Same as the "),Og=o(B,"CODE",{});var vH=l(Og);f6=u(vH,"simple"),vH.forEach(t),h6=u(B," strategy except words cannot end up with different tags. Scores are averaged across tokens and then the maximum label is applied. "),d6=o(B,"BR",{}),g6=f(B),Pg=o(B,"CODE",{});var yH=l(Pg);m6=u(yH,"max"),yH.forEach(t),$6=u(B,": Same as the "),Rg=o(B,"CODE",{});var EH=l(Rg);_6=u(EH,"simple"),EH.forEach(t),q6=u(B," strategy except words cannot end up with different tags. Word entity will be the token with the maximum score."),B.forEach(t),by.forEach(t),v6=f(X),Yr=o(X,"TR",{});var jy=l(Yr);Rp=o(jy,"TD",{align:!0});var wH=l(Rp);Ng=o(wH,"STRONG",{});var bH=l(Ng);y6=u(bH,"options"),bH.forEach(t),wH.forEach(t),E6=f(jy),Np=o(jy,"TD",{align:!0});var jH=l(Np);w6=u(jH,"a dict containing the following keys:"),jH.forEach(t),jy.forEach(t),b6=f(X),Vr=o(X,"TR",{});var Ty=l(Vr);xp=o(Ty,"TD",{align:!0});var TH=l(xp);j6=u(TH,"use_gpu"),TH.forEach(t),T6=f(Ty),Es=o(Ty,"TD",{align:!0});var ky=l(Es);k6=u(ky,"(Default: "),xg=o(ky,"CODE",{});var kH=l(xg);A6=u(kH,"false"),kH.forEach(t),D6=u(ky,"). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),ky.forEach(t),Ty.forEach(t),O6=f(X),Xr=o(X,"TR",{});var Ay=l(Xr);Sp=o(Ay,"TD",{align:!0});var AH=l(Sp);P6=u(AH,"use_cache"),AH.forEach(t),R6=f(Ay),ws=o(Ay,"TD",{align:!0});var Dy=l(ws);N6=u(Dy,"(Default: "),Sg=o(Dy,"CODE",{});var DH=l(Sg);x6=u(DH,"true"),DH.forEach(t),S6=u(Dy,"). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),Dy.forEach(t),Ay.forEach(t),I6=f(X),Qr=o(X,"TR",{});var Oy=l(Qr);Ip=o(Oy,"TD",{align:!0});var OH=l(Ip);H6=u(OH,"wait_for_model"),OH.forEach(t),B6=f(Oy),bs=o(Oy,"TD",{align:!0});var Py=l(bs);C6=u(Py,"(Default: "),Ig=o(Py,"CODE",{});var PH=l(Ig);G6=u(PH,"false"),PH.forEach(t),U6=u(Py,") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),Py.forEach(t),Oy.forEach(t),X.forEach(t),vy.forEach(t),xq=f(a),Hp=o(a,"P",{});var RH=l(Hp);L6=u(RH,"Return value is either a dict or a list of dicts if you sent a list of inputs"),RH.forEach(t),Sq=f(a),v(js.$$.fragment,a),Iq=f(a),Ts=o(a,"TABLE",{});var Ry=l(Ts);Hg=o(Ry,"THEAD",{});var NH=l(Hg);Zr=o(NH,"TR",{});var Ny=l(Zr);Bp=o(Ny,"TH",{align:!0});var xH=l(Bp);z6=u(xH,"Returned values"),xH.forEach(t),M6=f(Ny),Bg=o(Ny,"TH",{align:!0}),l(Bg).forEach(t),Ny.forEach(t),NH.forEach(t),F6=f(Ry),te=o(Ry,"TBODY",{});var Ae=l(te);eo=o(Ae,"TR",{});var xy=l(eo);Cp=o(xy,"TD",{align:!0});var SH=l(Cp);Cg=o(SH,"STRONG",{});var IH=l(Cg);J6=u(IH,"entity_group"),IH.forEach(t),SH.forEach(t),K6=f(xy),Gp=o(xy,"TD",{align:!0});var HH=l(Gp);W6=u(HH,"The type for the entity being recognized (model specific)."),HH.forEach(t),xy.forEach(t),Y6=f(Ae),to=o(Ae,"TR",{});var Sy=l(to);Up=o(Sy,"TD",{align:!0});var BH=l(Up);Gg=o(BH,"STRONG",{});var CH=l(Gg);V6=u(CH,"score"),CH.forEach(t),BH.forEach(t),X6=f(Sy),Lp=o(Sy,"TD",{align:!0});var GH=l(Lp);Q6=u(GH,"How likely the entity was recognized."),GH.forEach(t),Sy.forEach(t),Z6=f(Ae),so=o(Ae,"TR",{});var Iy=l(so);zp=o(Iy,"TD",{align:!0});var UH=l(zp);Ug=o(UH,"STRONG",{});var LH=l(Ug);e9=u(LH,"word"),LH.forEach(t),UH.forEach(t),t9=f(Iy),Mp=o(Iy,"TD",{align:!0});var zH=l(Mp);s9=u(zH,"The string that was captured"),zH.forEach(t),Iy.forEach(t),a9=f(Ae),ao=o(Ae,"TR",{});var Hy=l(ao);Fp=o(Hy,"TD",{align:!0});var MH=l(Fp);Lg=o(MH,"STRONG",{});var FH=l(Lg);n9=u(FH,"start"),FH.forEach(t),MH.forEach(t),r9=f(Hy),ks=o(Hy,"TD",{align:!0});var By=l(ks);o9=u(By,"The offset stringwise where the answer is located. Useful to disambiguate if "),zg=o(By,"CODE",{});var JH=l(zg);l9=u(JH,"word"),JH.forEach(t),i9=u(By," occurs multiple times."),By.forEach(t),Hy.forEach(t),u9=f(Ae),no=o(Ae,"TR",{});var Cy=l(no);Jp=o(Cy,"TD",{align:!0});var KH=l(Jp);Mg=o(KH,"STRONG",{});var WH=l(Mg);p9=u(WH,"end"),WH.forEach(t),KH.forEach(t),c9=f(Cy),As=o(Cy,"TD",{align:!0});var Gy=l(As);f9=u(Gy,"The offset stringwise where the answer is located. Useful to disambiguate if "),Fg=o(Gy,"CODE",{});var YH=l(Fg);h9=u(YH,"word"),YH.forEach(t),d9=u(Gy," occurs multiple times."),Gy.forEach(t),Cy.forEach(t),Ae.forEach(t),Ry.forEach(t),Hq=f(a),Fe=o(a,"H2",{class:!0});var Uy=l(Fe);Ds=o(Uy,"A",{id:!0,class:!0,href:!0});var VH=l(Ds);Jg=o(VH,"SPAN",{});var XH=l(Jg);v(ro.$$.fragment,XH),XH.forEach(t),VH.forEach(t),g9=f(Uy),Kg=o(Uy,"SPAN",{});var QH=l(Kg);m9=u(QH,"Text-generation task"),QH.forEach(t),Uy.forEach(t),Bq=f(a),Kp=o(a,"P",{});var ZH=l(Kp);$9=u(ZH,"Use to continue text from a prompt. This is a very generic task."),ZH.forEach(t),Cq=f(a),v(Os.$$.fragment,a),Gq=f(a),oo=o(a,"P",{});var aR=l(oo);_9=u(aR,"Available with: "),lo=o(aR,"A",{href:!0,rel:!0});var eB=l(lo);q9=u(eB,"\u{1F917} Transformers"),eB.forEach(t),aR.forEach(t),Uq=f(a),Wp=o(a,"P",{});var tB=l(Wp);v9=u(tB,"Example:"),tB.forEach(t),Lq=f(a),v(Ps.$$.fragment,a),zq=f(a),Yp=o(a,"P",{});var sB=l(Yp);y9=u(sB,`When sending your request, you should send a JSON encoded payload. Here
are all the options`),sB.forEach(t),Mq=f(a),Rs=o(a,"TABLE",{});var Ly=l(Rs);Wg=o(Ly,"THEAD",{});var aB=l(Wg);io=o(aB,"TR",{});var zy=l(io);Vp=o(zy,"TH",{align:!0});var nB=l(Vp);E9=u(nB,"All parameters"),nB.forEach(t),w9=f(zy),Yg=o(zy,"TH",{align:!0}),l(Yg).forEach(t),zy.forEach(t),aB.forEach(t),b9=f(Ly),I=o(Ly,"TBODY",{});var C=l(I);uo=o(C,"TR",{});var My=l(uo);po=o(My,"TD",{align:!0});var nR=l(po);Vg=o(nR,"STRONG",{});var rB=l(Vg);j9=u(rB,"inputs"),rB.forEach(t),T9=u(nR," (required):"),nR.forEach(t),k9=f(My),Xp=o(My,"TD",{align:!0});var oB=l(Xp);A9=u(oB,"a string to be generated from"),oB.forEach(t),My.forEach(t),D9=f(C),co=o(C,"TR",{});var Fy=l(co);Qp=o(Fy,"TD",{align:!0});var lB=l(Qp);Xg=o(lB,"STRONG",{});var iB=l(Xg);O9=u(iB,"parameters"),iB.forEach(t),lB.forEach(t),P9=f(Fy),Zp=o(Fy,"TD",{align:!0});var uB=l(Zp);R9=u(uB,"dict containing the following keys:"),uB.forEach(t),Fy.forEach(t),N9=f(C),fo=o(C,"TR",{});var Jy=l(fo);ec=o(Jy,"TD",{align:!0});var pB=l(ec);x9=u(pB,"top_k"),pB.forEach(t),S9=f(Jy),ye=o(Jy,"TD",{align:!0});var Kf=l(ye);I9=u(Kf,"(Default: "),Qg=o(Kf,"CODE",{});var cB=l(Qg);H9=u(cB,"None"),cB.forEach(t),B9=u(Kf,"). Integer to define the top tokens considered within the "),Zg=o(Kf,"CODE",{});var fB=l(Zg);C9=u(fB,"sample"),fB.forEach(t),G9=u(Kf," operation to create new text."),Kf.forEach(t),Jy.forEach(t),U9=f(C),ho=o(C,"TR",{});var Ky=l(ho);tc=o(Ky,"TD",{align:!0});var hB=l(tc);L9=u(hB,"top_p"),hB.forEach(t),z9=f(Ky),Ee=o(Ky,"TD",{align:!0});var Wf=l(Ee);M9=u(Wf,"(Default: "),em=o(Wf,"CODE",{});var dB=l(em);F9=u(dB,"None"),dB.forEach(t),J9=u(Wf,"). Float to define the tokens that are within the  sample"),tm=o(Wf,"CODE",{});var gB=l(tm);K9=u(gB,"operation of text generation. Add tokens in the sample for more probable to least probable until the sum of the probabilities is greater than"),gB.forEach(t),W9=u(Wf,"top_p`."),Wf.forEach(t),Ky.forEach(t),Y9=f(C),go=o(C,"TR",{});var Wy=l(go);sc=o(Wy,"TD",{align:!0});var mB=l(sc);V9=u(mB,"temperature"),mB.forEach(t),X9=f(Wy),we=o(Wy,"TD",{align:!0});var Yf=l(we);Q9=u(Yf,"(Default: "),sm=o(Yf,"CODE",{});var $B=l(sm);Z9=u($B,"1.0"),$B.forEach(t),e8=u(Yf,"). Float (0.0-100.0). The temperature of the sampling operation. 1 means regular sampling, 0 means "),am=o(Yf,"CODE",{});var _B=l(am);t8=u(_B,"100.0"),_B.forEach(t),s8=u(Yf," is getting closer to uniform probability."),Yf.forEach(t),Wy.forEach(t),a8=f(C),mo=o(C,"TR",{});var Yy=l(mo);ac=o(Yy,"TD",{align:!0});var qB=l(ac);n8=u(qB,"repetition_penalty"),qB.forEach(t),r8=f(Yy),Ns=o(Yy,"TD",{align:!0});var Vy=l(Ns);o8=u(Vy,"(Default: "),nm=o(Vy,"CODE",{});var vB=l(nm);l8=u(vB,"None"),vB.forEach(t),i8=u(Vy,"). Float (0.0-100.0). The more a token is used within generation the more it is penalized to not be picked in successive generation passes."),Vy.forEach(t),Yy.forEach(t),u8=f(C),$o=o(C,"TR",{});var Xy=l($o);nc=o(Xy,"TD",{align:!0});var yB=l(nc);p8=u(yB,"max_new_tokens"),yB.forEach(t),c8=f(Xy),be=o(Xy,"TD",{align:!0});var Vf=l(be);f8=u(Vf,"(Default: "),rm=o(Vf,"CODE",{});var EB=l(rm);h8=u(EB,"None"),EB.forEach(t),d8=u(Vf,"). Int (0-250). The amount of new tokens to be generated, this does "),om=o(Vf,"STRONG",{});var wB=l(om);g8=u(wB,"not"),wB.forEach(t),m8=u(Vf," include the input length it is a estimate of the size of generated text you want. Each new tokens slows down the request, so look for balance between response times and length of text generated."),Vf.forEach(t),Xy.forEach(t),$8=f(C),_o=o(C,"TR",{});var Qy=l(_o);rc=o(Qy,"TD",{align:!0});var bB=l(rc);_8=u(bB,"max_time"),bB.forEach(t),q8=f(Qy),xs=o(Qy,"TD",{align:!0});var Zy=l(xs);v8=u(Zy,"(Default: "),lm=o(Zy,"CODE",{});var jB=l(lm);y8=u(jB,"None"),jB.forEach(t),E8=u(Zy,"). Float (0-120.0). The amount of time in seconds that the query should take maximum. Network can cause some overhead so it will be a soft limit. Use that in combination with \u2018max_new_tokens` for best results."),Zy.forEach(t),Qy.forEach(t),w8=f(C),qo=o(C,"TR",{});var eE=l(qo);oc=o(eE,"TD",{align:!0});var TB=l(oc);b8=u(TB,"return_full_text"),TB.forEach(t),j8=f(eE),je=o(eE,"TD",{align:!0});var Xf=l(je);T8=u(Xf,"(Default: "),im=o(Xf,"CODE",{});var kB=l(im);k8=u(kB,"True"),kB.forEach(t),A8=u(Xf,"). Bool. If set to False, the return results will "),um=o(Xf,"STRONG",{});var AB=l(um);D8=u(AB,"not"),AB.forEach(t),O8=u(Xf," contain the original query making it easier for prompting."),Xf.forEach(t),eE.forEach(t),P8=f(C),vo=o(C,"TR",{});var tE=l(vo);lc=o(tE,"TD",{align:!0});var DB=l(lc);R8=u(DB,"num_return_sequences"),DB.forEach(t),N8=f(tE),Ss=o(tE,"TD",{align:!0});var sE=l(Ss);x8=u(sE,"(Default: "),pm=o(sE,"CODE",{});var OB=l(pm);S8=u(OB,"1"),OB.forEach(t),I8=u(sE,"). Integer. The number of proposition you want to be returned."),sE.forEach(t),tE.forEach(t),H8=f(C),yo=o(C,"TR",{});var aE=l(yo);ic=o(aE,"TD",{align:!0});var PB=l(ic);B8=u(PB,"do_sample"),PB.forEach(t),C8=f(aE),Is=o(aE,"TD",{align:!0});var nE=l(Is);G8=u(nE,"(Optional: "),cm=o(nE,"CODE",{});var RB=l(cm);U8=u(RB,"True"),RB.forEach(t),L8=u(nE,"). Bool. Whether or not to use sampling, use greedy decoding otherwise."),nE.forEach(t),aE.forEach(t),z8=f(C),Eo=o(C,"TR",{});var rE=l(Eo);uc=o(rE,"TD",{align:!0});var NB=l(uc);fm=o(NB,"STRONG",{});var xB=l(fm);M8=u(xB,"options"),xB.forEach(t),NB.forEach(t),F8=f(rE),pc=o(rE,"TD",{align:!0});var SB=l(pc);J8=u(SB,"a dict containing the following keys:"),SB.forEach(t),rE.forEach(t),K8=f(C),wo=o(C,"TR",{});var oE=l(wo);cc=o(oE,"TD",{align:!0});var IB=l(cc);W8=u(IB,"use_gpu"),IB.forEach(t),Y8=f(oE),Hs=o(oE,"TD",{align:!0});var lE=l(Hs);V8=u(lE,"(Default: "),hm=o(lE,"CODE",{});var HB=l(hm);X8=u(HB,"false"),HB.forEach(t),Q8=u(lE,"). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),lE.forEach(t),oE.forEach(t),Z8=f(C),bo=o(C,"TR",{});var iE=l(bo);fc=o(iE,"TD",{align:!0});var BB=l(fc);eA=u(BB,"use_cache"),BB.forEach(t),tA=f(iE),Bs=o(iE,"TD",{align:!0});var uE=l(Bs);sA=u(uE,"(Default: "),dm=o(uE,"CODE",{});var CB=l(dm);aA=u(CB,"true"),CB.forEach(t),nA=u(uE,"). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),uE.forEach(t),iE.forEach(t),rA=f(C),jo=o(C,"TR",{});var pE=l(jo);hc=o(pE,"TD",{align:!0});var GB=l(hc);oA=u(GB,"wait_for_model"),GB.forEach(t),lA=f(pE),Cs=o(pE,"TD",{align:!0});var cE=l(Cs);iA=u(cE,"(Default: "),gm=o(cE,"CODE",{});var UB=l(gm);uA=u(UB,"false"),UB.forEach(t),pA=u(cE,") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),cE.forEach(t),pE.forEach(t),C.forEach(t),Ly.forEach(t),Fq=f(a),dc=o(a,"P",{});var LB=l(dc);cA=u(LB,"Return value is either a dict or a list of dicts if you sent a list of inputs"),LB.forEach(t),Jq=f(a),v(Gs.$$.fragment,a),Kq=f(a),Us=o(a,"TABLE",{});var fE=l(Us);mm=o(fE,"THEAD",{});var zB=l(mm);To=o(zB,"TR",{});var hE=l(To);gc=o(hE,"TH",{align:!0});var MB=l(gc);fA=u(MB,"Returned values"),MB.forEach(t),hA=f(hE),$m=o(hE,"TH",{align:!0}),l($m).forEach(t),hE.forEach(t),zB.forEach(t),dA=f(fE),_m=o(fE,"TBODY",{});var FB=l(_m);ko=o(FB,"TR",{});var dE=l(ko);mc=o(dE,"TD",{align:!0});var JB=l(mc);qm=o(JB,"STRONG",{});var KB=l(qm);gA=u(KB,"generated_text"),KB.forEach(t),JB.forEach(t),mA=f(dE),$c=o(dE,"TD",{align:!0});var WB=l($c);$A=u(WB,"The continuated string"),WB.forEach(t),dE.forEach(t),FB.forEach(t),fE.forEach(t),Wq=f(a),Je=o(a,"H2",{class:!0});var gE=l(Je);Ls=o(gE,"A",{id:!0,class:!0,href:!0});var YB=l(Ls);vm=o(YB,"SPAN",{});var VB=l(vm);v(Ao.$$.fragment,VB),VB.forEach(t),YB.forEach(t),_A=f(gE),ym=o(gE,"SPAN",{});var XB=l(ym);qA=u(XB,"Text2text-generation task"),XB.forEach(t),gE.forEach(t),Yq=f(a),zs=o(a,"P",{});var mE=l(zs);vA=u(mE,"Essentially "),_c=o(mE,"A",{href:!0});var QB=l(_c);yA=u(QB,"Text-generation task"),QB.forEach(t),EA=u(mE,`. But uses
Encoder-Decoder architecture, so might change in the future for more
options.`),mE.forEach(t),Vq=f(a),Ke=o(a,"H2",{class:!0});var $E=l(Ke);Ms=o($E,"A",{id:!0,class:!0,href:!0});var ZB=l(Ms);Em=o(ZB,"SPAN",{});var eC=l(Em);v(Do.$$.fragment,eC),eC.forEach(t),ZB.forEach(t),wA=f($E),wm=o($E,"SPAN",{});var tC=l(wm);bA=u(tC,"Fill mask task"),tC.forEach(t),$E.forEach(t),Xq=f(a),qc=o(a,"P",{});var sC=l(qc);jA=u(sC,`Tries to fill in a hole with a missing word (token to be precise).
That\u2019s the base task for BERT models.`),sC.forEach(t),Qq=f(a),v(Fs.$$.fragment,a),Zq=f(a),Oo=o(a,"P",{});var rR=l(Oo);TA=u(rR,"Available with: "),Po=o(rR,"A",{href:!0,rel:!0});var aC=l(Po);kA=u(aC,"\u{1F917} Transformers"),aC.forEach(t),rR.forEach(t),e1=f(a),vc=o(a,"P",{});var nC=l(vc);AA=u(nC,"Example:"),nC.forEach(t),t1=f(a),v(Js.$$.fragment,a),s1=f(a),yc=o(a,"P",{});var rC=l(yc);DA=u(rC,`When sending your request, you should send a JSON encoded payload. Here
are all the options`),rC.forEach(t),a1=f(a),Ks=o(a,"TABLE",{});var _E=l(Ks);bm=o(_E,"THEAD",{});var oC=l(bm);Ro=o(oC,"TR",{});var qE=l(Ro);Ec=o(qE,"TH",{align:!0});var lC=l(Ec);OA=u(lC,"All parameters"),lC.forEach(t),PA=f(qE),jm=o(qE,"TH",{align:!0}),l(jm).forEach(t),qE.forEach(t),oC.forEach(t),RA=f(_E),se=o(_E,"TBODY",{});var De=l(se);No=o(De,"TR",{});var vE=l(No);xo=o(vE,"TD",{align:!0});var oR=l(xo);Tm=o(oR,"STRONG",{});var iC=l(Tm);NA=u(iC,"inputs"),iC.forEach(t),xA=u(oR," (required):"),oR.forEach(t),SA=f(vE),wc=o(vE,"TD",{align:!0});var uC=l(wc);IA=u(uC,"a string to be filled from, must contain the [MASK] token (check model card for exact name of the mask)"),uC.forEach(t),vE.forEach(t),HA=f(De),So=o(De,"TR",{});var yE=l(So);bc=o(yE,"TD",{align:!0});var pC=l(bc);km=o(pC,"STRONG",{});var cC=l(km);BA=u(cC,"options"),cC.forEach(t),pC.forEach(t),CA=f(yE),jc=o(yE,"TD",{align:!0});var fC=l(jc);GA=u(fC,"a dict containing the following keys:"),fC.forEach(t),yE.forEach(t),UA=f(De),Io=o(De,"TR",{});var EE=l(Io);Tc=o(EE,"TD",{align:!0});var hC=l(Tc);LA=u(hC,"use_gpu"),hC.forEach(t),zA=f(EE),Ws=o(EE,"TD",{align:!0});var wE=l(Ws);MA=u(wE,"(Default: "),Am=o(wE,"CODE",{});var dC=l(Am);FA=u(dC,"false"),dC.forEach(t),JA=u(wE,"). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),wE.forEach(t),EE.forEach(t),KA=f(De),Ho=o(De,"TR",{});var bE=l(Ho);kc=o(bE,"TD",{align:!0});var gC=l(kc);WA=u(gC,"use_cache"),gC.forEach(t),YA=f(bE),Ys=o(bE,"TD",{align:!0});var jE=l(Ys);VA=u(jE,"(Default: "),Dm=o(jE,"CODE",{});var mC=l(Dm);XA=u(mC,"true"),mC.forEach(t),QA=u(jE,"). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),jE.forEach(t),bE.forEach(t),ZA=f(De),Bo=o(De,"TR",{});var TE=l(Bo);Ac=o(TE,"TD",{align:!0});var $C=l(Ac);eD=u($C,"wait_for_model"),$C.forEach(t),tD=f(TE),Vs=o(TE,"TD",{align:!0});var kE=l(Vs);sD=u(kE,"(Default: "),Om=o(kE,"CODE",{});var _C=l(Om);aD=u(_C,"false"),_C.forEach(t),nD=u(kE,") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),kE.forEach(t),TE.forEach(t),De.forEach(t),_E.forEach(t),n1=f(a),Dc=o(a,"P",{});var qC=l(Dc);rD=u(qC,"Return value is either a dict or a list of dicts if you sent a list of inputs"),qC.forEach(t),r1=f(a),v(Xs.$$.fragment,a),o1=f(a),Qs=o(a,"TABLE",{});var AE=l(Qs);Pm=o(AE,"THEAD",{});var vC=l(Pm);Co=o(vC,"TR",{});var DE=l(Co);Oc=o(DE,"TH",{align:!0});var yC=l(Oc);oD=u(yC,"Returned values"),yC.forEach(t),lD=f(DE),Rm=o(DE,"TH",{align:!0}),l(Rm).forEach(t),DE.forEach(t),vC.forEach(t),iD=f(AE),ie=o(AE,"TBODY",{});var Da=l(ie);Go=o(Da,"TR",{});var OE=l(Go);Pc=o(OE,"TD",{align:!0});var EC=l(Pc);Nm=o(EC,"STRONG",{});var wC=l(Nm);uD=u(wC,"sequence"),wC.forEach(t),EC.forEach(t),pD=f(OE),Rc=o(OE,"TD",{align:!0});var bC=l(Rc);cD=u(bC,"The actual sequence of tokens that ran against the model (may contain special tokens)"),bC.forEach(t),OE.forEach(t),fD=f(Da),Uo=o(Da,"TR",{});var PE=l(Uo);Nc=o(PE,"TD",{align:!0});var jC=l(Nc);xm=o(jC,"STRONG",{});var TC=l(xm);hD=u(TC,"score"),TC.forEach(t),jC.forEach(t),dD=f(PE),xc=o(PE,"TD",{align:!0});var kC=l(xc);gD=u(kC,"The probability for this token."),kC.forEach(t),PE.forEach(t),mD=f(Da),Lo=o(Da,"TR",{});var RE=l(Lo);Sc=o(RE,"TD",{align:!0});var AC=l(Sc);Sm=o(AC,"STRONG",{});var DC=l(Sm);$D=u(DC,"token"),DC.forEach(t),AC.forEach(t),_D=f(RE),Ic=o(RE,"TD",{align:!0});var OC=l(Ic);qD=u(OC,"The id of the token"),OC.forEach(t),RE.forEach(t),vD=f(Da),zo=o(Da,"TR",{});var NE=l(zo);Hc=o(NE,"TD",{align:!0});var PC=l(Hc);Im=o(PC,"STRONG",{});var RC=l(Im);yD=u(RC,"token_str"),RC.forEach(t),PC.forEach(t),ED=f(NE),Bc=o(NE,"TD",{align:!0});var NC=l(Bc);wD=u(NC,"The string representation of the token"),NC.forEach(t),NE.forEach(t),Da.forEach(t),AE.forEach(t),l1=f(a),We=o(a,"H2",{class:!0});var xE=l(We);Zs=o(xE,"A",{id:!0,class:!0,href:!0});var xC=l(Zs);Hm=o(xC,"SPAN",{});var SC=l(Hm);v(Mo.$$.fragment,SC),SC.forEach(t),xC.forEach(t),bD=f(xE),Bm=o(xE,"SPAN",{});var IC=l(Bm);jD=u(IC,"Automatic speech recognition task"),IC.forEach(t),xE.forEach(t),i1=f(a),Cc=o(a,"P",{});var HC=l(Cc);TD=u(HC,`This task reads some audio input and outputs the said words within the
audio files.`),HC.forEach(t),u1=f(a),v(ea.$$.fragment,a),p1=f(a),v(ta.$$.fragment,a),c1=f(a),ue=o(a,"P",{});var Ul=l(ue);kD=u(Ul,"Available with: "),Fo=o(Ul,"A",{href:!0,rel:!0});var BC=l(Fo);AD=u(BC,"\u{1F917} Transformers"),BC.forEach(t),DD=f(Ul),Jo=o(Ul,"A",{href:!0,rel:!0});var CC=l(Jo);OD=u(CC,"ESPnet"),CC.forEach(t),PD=u(Ul,` and
`),Ko=o(Ul,"A",{href:!0,rel:!0});var GC=l(Ko);RD=u(GC,"SpeechBrain"),GC.forEach(t),Ul.forEach(t),f1=f(a),Gc=o(a,"P",{});var UC=l(Gc);ND=u(UC,"Request:"),UC.forEach(t),h1=f(a),v(sa.$$.fragment,a),d1=f(a),Uc=o(a,"P",{});var LC=l(Uc);xD=u(LC,`When sending your request, you should send a binary payload that simply
contains your audio file. We try to support most formats (Flac, Wav,
Mp3, Ogg etc...). And we automatically rescale the sampling rate to the
appropriate rate for the given model (usually 16KHz).`),LC.forEach(t),g1=f(a),aa=o(a,"TABLE",{});var SE=l(aa);Cm=o(SE,"THEAD",{});var zC=l(Cm);Wo=o(zC,"TR",{});var IE=l(Wo);Lc=o(IE,"TH",{align:!0});var MC=l(Lc);SD=u(MC,"All parameters"),MC.forEach(t),ID=f(IE),Gm=o(IE,"TH",{align:!0}),l(Gm).forEach(t),IE.forEach(t),zC.forEach(t),HD=f(SE),Um=o(SE,"TBODY",{});var FC=l(Um);Yo=o(FC,"TR",{});var HE=l(Yo);Vo=o(HE,"TD",{align:!0});var lR=l(Vo);Lm=o(lR,"STRONG",{});var JC=l(Lm);BD=u(JC,"no parameter"),JC.forEach(t),CD=u(lR," (required)"),lR.forEach(t),GD=f(HE),zc=o(HE,"TD",{align:!0});var KC=l(zc);UD=u(KC,"a binary representation of the audio file. No other parameters are currently allowed."),KC.forEach(t),HE.forEach(t),FC.forEach(t),SE.forEach(t),m1=f(a),Mc=o(a,"P",{});var WC=l(Mc);LD=u(WC,"Return value is either a dict or a list of dicts if you sent a list of inputs"),WC.forEach(t),$1=f(a),Fc=o(a,"P",{});var YC=l(Fc);zD=u(YC,"Response:"),YC.forEach(t),_1=f(a),v(na.$$.fragment,a),q1=f(a),ra=o(a,"TABLE",{});var BE=l(ra);zm=o(BE,"THEAD",{});var VC=l(zm);Xo=o(VC,"TR",{});var CE=l(Xo);Jc=o(CE,"TH",{align:!0});var XC=l(Jc);MD=u(XC,"Returned values"),XC.forEach(t),FD=f(CE),Mm=o(CE,"TH",{align:!0}),l(Mm).forEach(t),CE.forEach(t),VC.forEach(t),JD=f(BE),Fm=o(BE,"TBODY",{});var QC=l(Fm);Qo=o(QC,"TR",{});var GE=l(Qo);Kc=o(GE,"TD",{align:!0});var ZC=l(Kc);Jm=o(ZC,"STRONG",{});var eG=l(Jm);KD=u(eG,"text"),eG.forEach(t),ZC.forEach(t),WD=f(GE),Wc=o(GE,"TD",{align:!0});var tG=l(Wc);YD=u(tG,"The string that was recognized within the audio file."),tG.forEach(t),GE.forEach(t),QC.forEach(t),BE.forEach(t),v1=f(a),Ye=o(a,"H2",{class:!0});var UE=l(Ye);oa=o(UE,"A",{id:!0,class:!0,href:!0});var sG=l(oa);Km=o(sG,"SPAN",{});var aG=l(Km);v(Zo.$$.fragment,aG),aG.forEach(t),sG.forEach(t),VD=f(UE),Wm=o(UE,"SPAN",{});var nG=l(Wm);XD=u(nG,"Feature-extraction task"),nG.forEach(t),UE.forEach(t),y1=f(a),Yc=o(a,"P",{});var rG=l(Yc);QD=u(rG,`This task reads some text and outputs raw float values, that usually
consumed as part of a semantic database/semantic search.`),rG.forEach(t),E1=f(a),v(la.$$.fragment,a),w1=f(a),Ve=o(a,"P",{});var Z$=l(Ve);ZD=u(Z$,"Available with: "),el=o(Z$,"A",{href:!0,rel:!0});var oG=l(el);eO=u(oG,"\u{1F917} Transformers"),oG.forEach(t),tO=f(Z$),tl=o(Z$,"A",{href:!0,rel:!0});var lG=l(tl);sO=u(lG,"Sentence-transformers"),lG.forEach(t),Z$.forEach(t),b1=f(a),Vc=o(a,"P",{});var iG=l(Vc);aO=u(iG,"Request:"),iG.forEach(t),j1=f(a),ia=o(a,"TABLE",{});var LE=l(ia);Ym=o(LE,"THEAD",{});var uG=l(Ym);sl=o(uG,"TR",{});var zE=l(sl);Xc=o(zE,"TH",{align:!0});var pG=l(Xc);nO=u(pG,"All parameters"),pG.forEach(t),rO=f(zE),Vm=o(zE,"TH",{align:!0}),l(Vm).forEach(t),zE.forEach(t),uG.forEach(t),oO=f(LE),ae=o(LE,"TBODY",{});var Oe=l(ae);al=o(Oe,"TR",{});var ME=l(al);nl=o(ME,"TD",{align:!0});var iR=l(nl);Xm=o(iR,"STRONG",{});var cG=l(Xm);lO=u(cG,"inputs"),cG.forEach(t),iO=u(iR," (required):"),iR.forEach(t),uO=f(ME),Qc=o(ME,"TD",{align:!0});var fG=l(Qc);pO=u(fG,"a string or a list of strings to get the features from."),fG.forEach(t),ME.forEach(t),cO=f(Oe),rl=o(Oe,"TR",{});var FE=l(rl);Zc=o(FE,"TD",{align:!0});var hG=l(Zc);Qm=o(hG,"STRONG",{});var dG=l(Qm);fO=u(dG,"options"),dG.forEach(t),hG.forEach(t),hO=f(FE),ef=o(FE,"TD",{align:!0});var gG=l(ef);dO=u(gG,"a dict containing the following keys:"),gG.forEach(t),FE.forEach(t),gO=f(Oe),ol=o(Oe,"TR",{});var JE=l(ol);tf=o(JE,"TD",{align:!0});var mG=l(tf);mO=u(mG,"use_gpu"),mG.forEach(t),$O=f(JE),ua=o(JE,"TD",{align:!0});var KE=l(ua);_O=u(KE,"(Default: "),Zm=o(KE,"CODE",{});var $G=l(Zm);qO=u($G,"false"),$G.forEach(t),vO=u(KE,"). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),KE.forEach(t),JE.forEach(t),yO=f(Oe),ll=o(Oe,"TR",{});var WE=l(ll);sf=o(WE,"TD",{align:!0});var _G=l(sf);EO=u(_G,"use_cache"),_G.forEach(t),wO=f(WE),pa=o(WE,"TD",{align:!0});var YE=l(pa);bO=u(YE,"(Default: "),e$=o(YE,"CODE",{});var qG=l(e$);jO=u(qG,"true"),qG.forEach(t),TO=u(YE,"). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),YE.forEach(t),WE.forEach(t),kO=f(Oe),il=o(Oe,"TR",{});var VE=l(il);af=o(VE,"TD",{align:!0});var vG=l(af);AO=u(vG,"wait_for_model"),vG.forEach(t),DO=f(VE),ca=o(VE,"TD",{align:!0});var XE=l(ca);OO=u(XE,"(Default: "),t$=o(XE,"CODE",{});var yG=l(t$);PO=u(yG,"false"),yG.forEach(t),RO=u(XE,") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),XE.forEach(t),VE.forEach(t),Oe.forEach(t),LE.forEach(t),T1=f(a),nf=o(a,"P",{});var EG=l(nf);NO=u(EG,"Return value is either a dict or a list of dicts if you sent a list of inputs"),EG.forEach(t),k1=f(a),fa=o(a,"TABLE",{});var QE=l(fa);s$=o(QE,"THEAD",{});var wG=l(s$);ul=o(wG,"TR",{});var ZE=l(ul);rf=o(ZE,"TH",{align:!0});var bG=l(rf);xO=u(bG,"Returned values"),bG.forEach(t),SO=f(ZE),a$=o(ZE,"TH",{align:!0}),l(a$).forEach(t),ZE.forEach(t),wG.forEach(t),IO=f(QE),n$=o(QE,"TBODY",{});var jG=l(n$);pl=o(jG,"TR",{});var ew=l(pl);of=o(ew,"TD",{align:!0});var TG=l(of);r$=o(TG,"STRONG",{});var kG=l(r$);HO=u(kG,"A list of float (or list of list of floats)"),kG.forEach(t),TG.forEach(t),BO=f(ew),lf=o(ew,"TD",{align:!0});var AG=l(lf);CO=u(AG,"The numbers that are the representation features of the input."),AG.forEach(t),ew.forEach(t),jG.forEach(t),QE.forEach(t),A1=f(a),uf=o(a,"SMALL",{});var DG=l(uf);GO=u(DG,`Returned values are a list of floats, or a list of list of floats
(depending on if you sent a string or a list of string, and if the
automatic reduction, usually mean_pooling for instance was applied for
you or not. This should be explained on the model's README.`),DG.forEach(t),D1=f(a),Xe=o(a,"H2",{class:!0});var tw=l(Xe);ha=o(tw,"A",{id:!0,class:!0,href:!0});var OG=l(ha);o$=o(OG,"SPAN",{});var PG=l(o$);v(cl.$$.fragment,PG),PG.forEach(t),OG.forEach(t),UO=f(tw),l$=o(tw,"SPAN",{});var RG=l(l$);LO=u(RG,"Audio-classification task"),RG.forEach(t),tw.forEach(t),O1=f(a),pf=o(a,"P",{});var NG=l(pf);zO=u(NG,"This task reads some audio input and outputs the likelihood of classes."),NG.forEach(t),P1=f(a),v(da.$$.fragment,a),R1=f(a),Qe=o(a,"P",{});var e_=l(Qe);MO=u(e_,"Available with: "),fl=o(e_,"A",{href:!0,rel:!0});var xG=l(fl);FO=u(xG,"\u{1F917} Transformers"),xG.forEach(t),JO=f(e_),hl=o(e_,"A",{href:!0,rel:!0});var SG=l(hl);KO=u(SG,"SpeechBrain"),SG.forEach(t),e_.forEach(t),N1=f(a),cf=o(a,"P",{});var IG=l(cf);WO=u(IG,"Request:"),IG.forEach(t),x1=f(a),v(ga.$$.fragment,a),S1=f(a),ff=o(a,"P",{});var HG=l(ff);YO=u(HG,`When sending your request, you should send a binary payload that simply
contains your audio file. We try to support most formats (Flac, Wav,
Mp3, Ogg etc...). And we automatically rescale the sampling rate to the
appropriate rate for the given model (usually 16KHz).`),HG.forEach(t),I1=f(a),ma=o(a,"TABLE",{});var sw=l(ma);i$=o(sw,"THEAD",{});var BG=l(i$);dl=o(BG,"TR",{});var aw=l(dl);hf=o(aw,"TH",{align:!0});var CG=l(hf);VO=u(CG,"All parameters"),CG.forEach(t),XO=f(aw),u$=o(aw,"TH",{align:!0}),l(u$).forEach(t),aw.forEach(t),BG.forEach(t),QO=f(sw),p$=o(sw,"TBODY",{});var GG=l(p$);gl=o(GG,"TR",{});var nw=l(gl);ml=o(nw,"TD",{align:!0});var uR=l(ml);c$=o(uR,"STRONG",{});var UG=l(c$);ZO=u(UG,"no parameter"),UG.forEach(t),eP=u(uR," (required)"),uR.forEach(t),tP=f(nw),df=o(nw,"TD",{align:!0});var LG=l(df);sP=u(LG,"a binary representation of the audio file. No other parameters are currently allowed."),LG.forEach(t),nw.forEach(t),GG.forEach(t),sw.forEach(t),H1=f(a),gf=o(a,"P",{});var zG=l(gf);aP=u(zG,"Return value is a dict"),zG.forEach(t),B1=f(a),v($a.$$.fragment,a),C1=f(a),_a=o(a,"TABLE",{});var rw=l(_a);f$=o(rw,"THEAD",{});var MG=l(f$);$l=o(MG,"TR",{});var ow=l($l);mf=o(ow,"TH",{align:!0});var FG=l(mf);nP=u(FG,"Returned values"),FG.forEach(t),rP=f(ow),h$=o(ow,"TH",{align:!0}),l(h$).forEach(t),ow.forEach(t),MG.forEach(t),oP=f(rw),_l=o(rw,"TBODY",{});var lw=l(_l);ql=o(lw,"TR",{});var iw=l(ql);$f=o(iw,"TD",{align:!0});var JG=l($f);d$=o(JG,"STRONG",{});var KG=l(d$);lP=u(KG,"label"),KG.forEach(t),JG.forEach(t),iP=f(iw),_f=o(iw,"TD",{align:!0});var WG=l(_f);uP=u(WG,"The label for the class (model specific)"),WG.forEach(t),iw.forEach(t),pP=f(lw),vl=o(lw,"TR",{});var uw=l(vl);qf=o(uw,"TD",{align:!0});var YG=l(qf);g$=o(YG,"STRONG",{});var VG=l(g$);cP=u(VG,"score"),VG.forEach(t),YG.forEach(t),fP=f(uw),vf=o(uw,"TD",{align:!0});var XG=l(vf);hP=u(XG,"A floats that represents how likely is that the audio file belongs the this class."),XG.forEach(t),uw.forEach(t),lw.forEach(t),rw.forEach(t),G1=f(a),Ze=o(a,"H2",{class:!0});var pw=l(Ze);qa=o(pw,"A",{id:!0,class:!0,href:!0});var QG=l(qa);m$=o(QG,"SPAN",{});var ZG=l(m$);v(yl.$$.fragment,ZG),ZG.forEach(t),QG.forEach(t),dP=f(pw),$$=o(pw,"SPAN",{});var eU=l($$);gP=u(eU,"Object-detection task"),eU.forEach(t),pw.forEach(t),U1=f(a),yf=o(a,"P",{});var tU=l(yf);mP=u(tU,`This task reads some image input and outputs the likelihood of classes &
bounding boxes of detected objects.`),tU.forEach(t),L1=f(a),v(va.$$.fragment,a),z1=f(a),El=o(a,"P",{});var pR=l(El);$P=u(pR,"Available with: "),wl=o(pR,"A",{href:!0,rel:!0});var sU=l(wl);_P=u(sU,"\u{1F917} Transformers"),sU.forEach(t),pR.forEach(t),M1=f(a),Ef=o(a,"P",{});var aU=l(Ef);qP=u(aU,"Request:"),aU.forEach(t),F1=f(a),v(ya.$$.fragment,a),J1=f(a),Ea=o(a,"P",{});var cw=l(Ea);vP=u(cw,`When sending your request, you should send a binary payload that simply
contains your image file. We support all image formats `),bl=o(cw,"A",{href:!0,rel:!0});var nU=l(bl);yP=u(nU,`Pillow
supports`),nU.forEach(t),EP=u(cw,"."),cw.forEach(t),K1=f(a),wa=o(a,"TABLE",{});var fw=l(wa);_$=o(fw,"THEAD",{});var rU=l(_$);jl=o(rU,"TR",{});var hw=l(jl);wf=o(hw,"TH",{align:!0});var oU=l(wf);wP=u(oU,"All parameters"),oU.forEach(t),bP=f(hw),q$=o(hw,"TH",{align:!0}),l(q$).forEach(t),hw.forEach(t),rU.forEach(t),jP=f(fw),v$=o(fw,"TBODY",{});var lU=l(v$);Tl=o(lU,"TR",{});var dw=l(Tl);kl=o(dw,"TD",{align:!0});var cR=l(kl);y$=o(cR,"STRONG",{});var iU=l(y$);TP=u(iU,"no parameter"),iU.forEach(t),kP=u(cR," (required)"),cR.forEach(t),AP=f(dw),bf=o(dw,"TD",{align:!0});var uU=l(bf);DP=u(uU,"a binary representation of the image file. No other parameters are currently allowed."),uU.forEach(t),dw.forEach(t),lU.forEach(t),fw.forEach(t),W1=f(a),jf=o(a,"P",{});var pU=l(jf);OP=u(pU,"Return value is a dict"),pU.forEach(t),Y1=f(a),v(ba.$$.fragment,a),V1=f(a),ja=o(a,"TABLE",{});var gw=l(ja);E$=o(gw,"THEAD",{});var cU=l(E$);Al=o(cU,"TR",{});var mw=l(Al);Tf=o(mw,"TH",{align:!0});var fU=l(Tf);PP=u(fU,"Returned values"),fU.forEach(t),RP=f(mw),w$=o(mw,"TH",{align:!0}),l(w$).forEach(t),mw.forEach(t),cU.forEach(t),NP=f(gw),et=o(gw,"TBODY",{});var Qf=l(et);Dl=o(Qf,"TR",{});var $w=l(Dl);kf=o($w,"TD",{align:!0});var hU=l(kf);b$=o(hU,"STRONG",{});var dU=l(b$);xP=u(dU,"label"),dU.forEach(t),hU.forEach(t),SP=f($w),Af=o($w,"TD",{align:!0});var gU=l(Af);IP=u(gU,"The label for the class (model specific) of a detected object."),gU.forEach(t),$w.forEach(t),HP=f(Qf),Ol=o(Qf,"TR",{});var _w=l(Ol);Df=o(_w,"TD",{align:!0});var mU=l(Df);j$=o(mU,"STRONG",{});var $U=l(j$);BP=u($U,"score"),$U.forEach(t),mU.forEach(t),CP=f(_w),Of=o(_w,"TD",{align:!0});var _U=l(Of);GP=u(_U,"A float that represents how likely it is that the detected object belongs to the given class."),_U.forEach(t),_w.forEach(t),UP=f(Qf),Pl=o(Qf,"TR",{});var qw=l(Pl);Pf=o(qw,"TD",{align:!0});var qU=l(Pf);T$=o(qU,"STRONG",{});var vU=l(T$);LP=u(vU,"box"),vU.forEach(t),qU.forEach(t),zP=f(qw),Rf=o(qw,"TD",{align:!0});var yU=l(Rf);MP=u(yU,"A dict (with keys [xmin,ymin,xmax,ymax]) representing the bounding box of a detected object."),yU.forEach(t),qw.forEach(t),Qf.forEach(t),gw.forEach(t),this.h()},h(){h(n,"name","hf:doc:metadata"),h(n,"content",JSON.stringify(xz)),h(d,"id","detailed-parameters"),h(d,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(d,"href","#detailed-parameters"),h(s,"class","relative group"),h(ne,"id","which-task-is-used-by-this-model"),h(ne,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(ne,"href","#which-task-is-used-by-this-model"),h(D,"class","relative group"),h(at,"class","block dark:hidden"),EU(at.src,fR="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/inference-api/task.png")||h(at,"src",fR),h(at,"width","300"),h(nt,"class","hidden dark:block invert"),EU(nt.src,hR="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/inference-api/task-dark.png")||h(nt,"src",hR),h(nt,"width","300"),h(rt,"id","zeroshot-classification-task"),h(rt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(rt,"href","#zeroshot-classification-task"),h(Ne,"class","relative group"),h(Na,"href","https://github.com/huggingface/transformers"),h(Na,"rel","nofollow"),h(Kl,"align","left"),h(sh,"align","left"),h(Ia,"align","left"),h(Wl,"align","left"),h(Ba,"align","left"),h(Yl,"align","left"),h(Vl,"align","left"),h(pe,"align","left"),h(Xl,"align","left"),h(ut,"align","left"),h(Ql,"align","left"),h(Zl,"align","left"),h(ei,"align","left"),h(pt,"align","left"),h(ti,"align","left"),h(ct,"align","left"),h(si,"align","left"),h(ft,"align","left"),h(ri,"align","left"),h(hh,"align","left"),h(oi,"align","left"),h(li,"align","left"),h(ii,"align","left"),h(ui,"align","left"),h(pi,"align","left"),h(gt,"align","left"),h(mt,"id","translation-task"),h(mt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(mt,"href","#translation-task"),h(Se,"class","relative group"),h(Xa,"href","https://github.com/huggingface/transformers"),h(Xa,"rel","nofollow"),h(di,"align","left"),h(yh,"align","left"),h(en,"align","left"),h(gi,"align","left"),h(mi,"align","left"),h($i,"align","left"),h(_i,"align","left"),h(vt,"align","left"),h(qi,"align","left"),h(yt,"align","left"),h(vi,"align","left"),h(Et,"align","left"),h(Ei,"align","left"),h(Ah,"align","left"),h(wi,"align","left"),h(bi,"align","left"),h(bt,"id","summarization-task"),h(bt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(bt,"href","#summarization-task"),h(Ie,"class","relative group"),h(ji,"href","mailto:api-enterprise@huggingface.co"),h(pn,"href","https://github.com/huggingface/transformers"),h(pn,"rel","nofollow"),h(Ai,"align","left"),h(xh,"align","left"),h(hn,"align","left"),h(Di,"align","left"),h(Oi,"align","left"),h(Pi,"align","left"),h(Ri,"align","left"),h(ce,"align","left"),h(Ni,"align","left"),h(fe,"align","left"),h(xi,"align","left"),h(he,"align","left"),h(Si,"align","left"),h(de,"align","left"),h(Ii,"align","left"),h(ge,"align","left"),h(Hi,"align","left"),h(Dt,"align","left"),h(Bi,"align","left"),h(Ot,"align","left"),h(Ci,"align","left"),h(Gi,"align","left"),h(Ui,"align","left"),h(Pt,"align","left"),h(Li,"align","left"),h(Rt,"align","left"),h(zi,"align","left"),h(Nt,"align","left"),h(Fi,"align","left"),h(ed,"align","left"),h(Ji,"align","left"),h(Ki,"align","left"),h(St,"id","conversational-task"),h(St,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(St,"href","#conversational-task"),h(He,"class","relative group"),h(On,"href","https://github.com/huggingface/transformers"),h(On,"rel","nofollow"),h(Xi,"align","left"),h(od,"align","left"),h(Nn,"align","left"),h(id,"align","left"),h(Qi,"align","left"),h(Zi,"align","left"),h(eu,"align","left"),h(tu,"align","left"),h(su,"align","left"),h(Ct,"align","left"),h(au,"align","left"),h(nu,"align","left"),h(ru,"align","left"),h(me,"align","left"),h(ou,"align","left"),h($e,"align","left"),h(lu,"align","left"),h(_e,"align","left"),h(iu,"align","left"),h(qe,"align","left"),h(uu,"align","left"),h(ve,"align","left"),h(pu,"align","left"),h(Gt,"align","left"),h(cu,"align","left"),h(Ut,"align","left"),h(fu,"align","left"),h(hu,"align","left"),h(du,"align","left"),h(Lt,"align","left"),h(gu,"align","left"),h(zt,"align","left"),h(mu,"align","left"),h(Mt,"align","left"),h(_u,"align","left"),h(Ad,"align","left"),h(qu,"align","left"),h(vu,"align","left"),h(yu,"align","left"),h(Eu,"align","left"),h(wu,"align","left"),h(bu,"align","left"),h(ju,"align","left"),h(Tu,"align","left"),h(Jt,"id","table-question-answering-task"),h(Jt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(Jt,"href","#table-question-answering-task"),h(Be,"class","relative group"),h(sr,"href","https://github.com/huggingface/transformers"),h(sr,"rel","nofollow"),h(Ou,"align","left"),h(xd,"align","left"),h(rr,"align","left"),h(Id,"align","left"),h(Pu,"align","left"),h(Ru,"align","left"),h(Nu,"align","left"),h(xu,"align","left"),h(Su,"align","left"),h(Iu,"align","left"),h(Hu,"align","left"),h(Vt,"align","left"),h(Bu,"align","left"),h(Xt,"align","left"),h(Cu,"align","left"),h(Qt,"align","left"),h(Uu,"align","left"),h(Ld,"align","left"),h(Lu,"align","left"),h(zu,"align","left"),h(Mu,"align","left"),h(Fu,"align","left"),h(Ju,"align","left"),h(Ku,"align","left"),h(Wu,"align","left"),h(Yu,"align","left"),h(ts,"id","question-answering-task"),h(ts,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(ts,"href","#question-answering-task"),h(Ce,"class","relative group"),h(_r,"href","https://github.com/huggingface/transformers"),h(_r,"rel","nofollow"),h(qr,"href","https://github.com/allenai/allennlp"),h(qr,"rel","nofollow"),h(ep,"align","left"),h(Vd,"align","left"),h(tp,"align","left"),h(sp,"align","left"),h(ap,"align","left"),h(np,"align","left"),h(rp,"align","left"),h(os,"align","left"),h(op,"align","left"),h(ls,"align","left"),h(is,"id","textclassification-task"),h(is,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(is,"href","#textclassification-task"),h(Ue,"class","relative group"),h(kr,"href","https://github.com/huggingface/transformers"),h(kr,"rel","nofollow"),h(pp,"align","left"),h(og,"align","left"),h(Or,"align","left"),h(cp,"align","left"),h(fp,"align","left"),h(hp,"align","left"),h(dp,"align","left"),h(fs,"align","left"),h(gp,"align","left"),h(hs,"align","left"),h(mp,"align","left"),h(ds,"align","left"),h(_p,"align","left"),h(hg,"align","left"),h(qp,"align","left"),h(vp,"align","left"),h(yp,"align","left"),h(Ep,"align","left"),h($s,"id","named-entity-recognition-ner-task"),h($s,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h($s,"href","#named-entity-recognition-ner-task"),h(Le,"class","relative group"),h(wp,"href","#token-classification-task"),h(_s,"id","tokenclassification-task"),h(_s,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(_s,"href","#tokenclassification-task"),h(ze,"class","relative group"),h(Lr,"href","https://github.com/huggingface/transformers"),h(Lr,"rel","nofollow"),h(zr,"href","https://github.com/flairNLP/flair"),h(zr,"rel","nofollow"),h(kp,"align","left"),h(yg,"align","left"),h(Jr,"align","left"),h(Ap,"align","left"),h(Dp,"align","left"),h(Op,"align","left"),h(Pp,"align","left"),h(S,"align","left"),h(Rp,"align","left"),h(Np,"align","left"),h(xp,"align","left"),h(Es,"align","left"),h(Sp,"align","left"),h(ws,"align","left"),h(Ip,"align","left"),h(bs,"align","left"),h(Bp,"align","left"),h(Bg,"align","left"),h(Cp,"align","left"),h(Gp,"align","left"),h(Up,"align","left"),h(Lp,"align","left"),h(zp,"align","left"),h(Mp,"align","left"),h(Fp,"align","left"),h(ks,"align","left"),h(Jp,"align","left"),h(As,"align","left"),h(Ds,"id","textgeneration-task"),h(Ds,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(Ds,"href","#textgeneration-task"),h(Fe,"class","relative group"),h(lo,"href","https://github.com/huggingface/transformers"),h(lo,"rel","nofollow"),h(Vp,"align","left"),h(Yg,"align","left"),h(po,"align","left"),h(Xp,"align","left"),h(Qp,"align","left"),h(Zp,"align","left"),h(ec,"align","left"),h(ye,"align","left"),h(tc,"align","left"),h(Ee,"align","left"),h(sc,"align","left"),h(we,"align","left"),h(ac,"align","left"),h(Ns,"align","left"),h(nc,"align","left"),h(be,"align","left"),h(rc,"align","left"),h(xs,"align","left"),h(oc,"align","left"),h(je,"align","left"),h(lc,"align","left"),h(Ss,"align","left"),h(ic,"align","left"),h(Is,"align","left"),h(uc,"align","left"),h(pc,"align","left"),h(cc,"align","left"),h(Hs,"align","left"),h(fc,"align","left"),h(Bs,"align","left"),h(hc,"align","left"),h(Cs,"align","left"),h(gc,"align","left"),h($m,"align","left"),h(mc,"align","left"),h($c,"align","left"),h(Ls,"id","text2textgeneration-task"),h(Ls,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(Ls,"href","#text2textgeneration-task"),h(Je,"class","relative group"),h(_c,"href","#text-generation-task"),h(Ms,"id","fill-mask-task"),h(Ms,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(Ms,"href","#fill-mask-task"),h(Ke,"class","relative group"),h(Po,"href","https://github.com/huggingface/transformers"),h(Po,"rel","nofollow"),h(Ec,"align","left"),h(jm,"align","left"),h(xo,"align","left"),h(wc,"align","left"),h(bc,"align","left"),h(jc,"align","left"),h(Tc,"align","left"),h(Ws,"align","left"),h(kc,"align","left"),h(Ys,"align","left"),h(Ac,"align","left"),h(Vs,"align","left"),h(Oc,"align","left"),h(Rm,"align","left"),h(Pc,"align","left"),h(Rc,"align","left"),h(Nc,"align","left"),h(xc,"align","left"),h(Sc,"align","left"),h(Ic,"align","left"),h(Hc,"align","left"),h(Bc,"align","left"),h(Zs,"id","automatic-speech-recognition-task"),h(Zs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(Zs,"href","#automatic-speech-recognition-task"),h(We,"class","relative group"),h(Fo,"href","https://github.com/huggingface/transformers"),h(Fo,"rel","nofollow"),h(Jo,"href","https://github.com/espnet/espnet"),h(Jo,"rel","nofollow"),h(Ko,"href","https://github.com/speechbrain/speechbrain"),h(Ko,"rel","nofollow"),h(Lc,"align","left"),h(Gm,"align","left"),h(Vo,"align","left"),h(zc,"align","left"),h(Jc,"align","left"),h(Mm,"align","left"),h(Kc,"align","left"),h(Wc,"align","left"),h(oa,"id","featureextraction-task"),h(oa,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(oa,"href","#featureextraction-task"),h(Ye,"class","relative group"),h(el,"href","https://github.com/huggingface/transformers"),h(el,"rel","nofollow"),h(tl,"href","https://github.com/UKPLab/sentence-transformers"),h(tl,"rel","nofollow"),h(Xc,"align","left"),h(Vm,"align","left"),h(nl,"align","left"),h(Qc,"align","left"),h(Zc,"align","left"),h(ef,"align","left"),h(tf,"align","left"),h(ua,"align","left"),h(sf,"align","left"),h(pa,"align","left"),h(af,"align","left"),h(ca,"align","left"),h(rf,"align","left"),h(a$,"align","left"),h(of,"align","left"),h(lf,"align","left"),h(ha,"id","audioclassification-task"),h(ha,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(ha,"href","#audioclassification-task"),h(Xe,"class","relative group"),h(fl,"href","https://github.com/huggingface/transformers"),h(fl,"rel","nofollow"),h(hl,"href","https://github.com/speechbrain/speechbrain"),h(hl,"rel","nofollow"),h(hf,"align","left"),h(u$,"align","left"),h(ml,"align","left"),h(df,"align","left"),h(mf,"align","left"),h(h$,"align","left"),h($f,"align","left"),h(_f,"align","left"),h(qf,"align","left"),h(vf,"align","left"),h(qa,"id","objectdetection-task"),h(qa,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(qa,"href","#objectdetection-task"),h(Ze,"class","relative group"),h(wl,"href","https://github.com/huggingface/transformers"),h(wl,"rel","nofollow"),h(bl,"href","https://pillow.readthedocs.io/en/stable/handbook/image-file-formats.html"),h(bl,"rel","nofollow"),h(wf,"align","left"),h(q$,"align","left"),h(kl,"align","left"),h(bf,"align","left"),h(Tf,"align","left"),h(w$,"align","left"),h(kf,"align","left"),h(Af,"align","left"),h(Df,"align","left"),h(Of,"align","left"),h(Pf,"align","left"),h(Rf,"align","left")},m(a,g){e(document.head,n),m(a,p,g),m(a,s,g),e(s,d),e(d,$),y(k,$,null),e(s,A),e(s,T),e(T,j),m(a,O,g),m(a,D,g),e(D,ne),e(ne,Pe),y(Q,Pe,null),e(D,W),e(D,st),e(st,Ll),m(a,Oa,g),m(a,Re,g),e(Re,vw),m(a,t_,g),m(a,zl,g),e(zl,yw),m(a,s_,g),m(a,at,g),m(a,a_,g),m(a,nt,g),m(a,n_,g),m(a,Ne,g),e(Ne,rt),e(rt,Zf),y(Pa,Zf,null),e(Ne,Ew),e(Ne,eh),e(eh,ww),m(a,r_,g),m(a,Ml,g),e(Ml,bw),m(a,o_,g),y(ot,a,g),m(a,l_,g),m(a,Ra,g),e(Ra,jw),e(Ra,Na),e(Na,Tw),m(a,i_,g),m(a,Fl,g),e(Fl,kw),m(a,u_,g),y(lt,a,g),m(a,p_,g),m(a,Jl,g),e(Jl,Aw),m(a,c_,g),m(a,it,g),e(it,th),e(th,xa),e(xa,Kl),e(Kl,Dw),e(xa,Ow),e(xa,sh),e(it,Pw),e(it,z),e(z,Sa),e(Sa,Ia),e(Ia,ah),e(ah,Rw),e(Ia,Nw),e(Sa,xw),e(Sa,Wl),e(Wl,Sw),e(z,Iw),e(z,Ha),e(Ha,Ba),e(Ba,nh),e(nh,Hw),e(Ba,Bw),e(Ha,Cw),e(Ha,Yl),e(Yl,Gw),e(z,Uw),e(z,Ca),e(Ca,Vl),e(Vl,Lw),e(Ca,zw),e(Ca,pe),e(pe,Mw),e(pe,rh),e(rh,Fw),e(pe,Jw),e(pe,oh),e(oh,Kw),e(pe,Ww),e(z,Yw),e(z,Ga),e(Ga,Xl),e(Xl,Vw),e(Ga,Xw),e(Ga,ut),e(ut,Qw),e(ut,lh),e(lh,Zw),e(ut,eb),e(z,tb),e(z,Ua),e(Ua,Ql),e(Ql,ih),e(ih,sb),e(Ua,ab),e(Ua,Zl),e(Zl,nb),e(z,rb),e(z,La),e(La,ei),e(ei,ob),e(La,lb),e(La,pt),e(pt,ib),e(pt,uh),e(uh,ub),e(pt,pb),e(z,cb),e(z,za),e(za,ti),e(ti,fb),e(za,hb),e(za,ct),e(ct,db),e(ct,ph),e(ph,gb),e(ct,mb),e(z,$b),e(z,Ma),e(Ma,si),e(si,_b),e(Ma,qb),e(Ma,ft),e(ft,vb),e(ft,ch),e(ch,yb),e(ft,Eb),m(a,f_,g),m(a,ai,g),e(ai,wb),m(a,h_,g),m(a,ni,g),e(ni,bb),m(a,d_,g),y(ht,a,g),m(a,g_,g),m(a,dt,g),e(dt,fh),e(fh,Fa),e(Fa,ri),e(ri,jb),e(Fa,Tb),e(Fa,hh),e(dt,kb),e(dt,xe),e(xe,Ja),e(Ja,oi),e(oi,dh),e(dh,Ab),e(Ja,Db),e(Ja,li),e(li,Ob),e(xe,Pb),e(xe,Ka),e(Ka,ii),e(ii,gh),e(gh,Rb),e(Ka,Nb),e(Ka,ui),e(ui,xb),e(xe,Sb),e(xe,Wa),e(Wa,pi),e(pi,mh),e(mh,Ib),e(Wa,Hb),e(Wa,gt),e(gt,Bb),e(gt,$h),e($h,Cb),e(gt,Gb),m(a,m_,g),m(a,Se,g),e(Se,mt),e(mt,_h),y(Ya,_h,null),e(Se,Ub),e(Se,qh),e(qh,Lb),m(a,$_,g),m(a,ci,g),e(ci,zb),m(a,__,g),y($t,a,g),m(a,q_,g),m(a,Va,g),e(Va,Mb),e(Va,Xa),e(Xa,Fb),m(a,v_,g),m(a,fi,g),e(fi,Jb),m(a,y_,g),y(_t,a,g),m(a,E_,g),m(a,hi,g),e(hi,Kb),m(a,w_,g),m(a,qt,g),e(qt,vh),e(vh,Qa),e(Qa,di),e(di,Wb),e(Qa,Yb),e(Qa,yh),e(qt,Vb),e(qt,Z),e(Z,Za),e(Za,en),e(en,Eh),e(Eh,Xb),e(en,Qb),e(Za,Zb),e(Za,gi),e(gi,ej),e(Z,tj),e(Z,tn),e(tn,mi),e(mi,wh),e(wh,sj),e(tn,aj),e(tn,$i),e($i,nj),e(Z,rj),e(Z,sn),e(sn,_i),e(_i,oj),e(sn,lj),e(sn,vt),e(vt,ij),e(vt,bh),e(bh,uj),e(vt,pj),e(Z,cj),e(Z,an),e(an,qi),e(qi,fj),e(an,hj),e(an,yt),e(yt,dj),e(yt,jh),e(jh,gj),e(yt,mj),e(Z,$j),e(Z,nn),e(nn,vi),e(vi,_j),e(nn,qj),e(nn,Et),e(Et,vj),e(Et,Th),e(Th,yj),e(Et,Ej),m(a,b_,g),m(a,yi,g),e(yi,wj),m(a,j_,g),m(a,wt,g),e(wt,kh),e(kh,rn),e(rn,Ei),e(Ei,bj),e(rn,jj),e(rn,Ah),e(wt,Tj),e(wt,Dh),e(Dh,on),e(on,wi),e(wi,Oh),e(Oh,kj),e(on,Aj),e(on,bi),e(bi,Dj),m(a,T_,g),m(a,Ie,g),e(Ie,bt),e(bt,Ph),y(ln,Ph,null),e(Ie,Oj),e(Ie,Rh),e(Rh,Pj),m(a,k_,g),m(a,jt,g),e(jt,Rj),e(jt,ji),e(ji,Nj),e(jt,xj),m(a,A_,g),y(Tt,a,g),m(a,D_,g),m(a,un,g),e(un,Sj),e(un,pn),e(pn,Ij),m(a,O_,g),m(a,Ti,g),e(Ti,Hj),m(a,P_,g),y(kt,a,g),m(a,R_,g),m(a,ki,g),e(ki,Bj),m(a,N_,g),m(a,At,g),e(At,Nh),e(Nh,cn),e(cn,Ai),e(Ai,Cj),e(cn,Gj),e(cn,xh),e(At,Uj),e(At,G),e(G,fn),e(fn,hn),e(hn,Sh),e(Sh,Lj),e(hn,zj),e(fn,Mj),e(fn,Di),e(Di,Fj),e(G,Jj),e(G,dn),e(dn,Oi),e(Oi,Ih),e(Ih,Kj),e(dn,Wj),e(dn,Pi),e(Pi,Yj),e(G,Vj),e(G,gn),e(gn,Ri),e(Ri,Xj),e(gn,Qj),e(gn,ce),e(ce,Zj),e(ce,Hh),e(Hh,e0),e(ce,t0),e(ce,Bh),e(Bh,s0),e(ce,a0),e(G,n0),e(G,mn),e(mn,Ni),e(Ni,r0),e(mn,o0),e(mn,fe),e(fe,l0),e(fe,Ch),e(Ch,i0),e(fe,u0),e(fe,Gh),e(Gh,p0),e(fe,c0),e(G,f0),e(G,$n),e($n,xi),e(xi,h0),e($n,d0),e($n,he),e(he,g0),e(he,Uh),e(Uh,m0),e(he,$0),e(he,Lh),e(Lh,_0),e(he,q0),e(G,v0),e(G,_n),e(_n,Si),e(Si,y0),e(_n,E0),e(_n,de),e(de,w0),e(de,zh),e(zh,b0),e(de,j0),e(de,Mh),e(Mh,T0),e(de,k0),e(G,A0),e(G,qn),e(qn,Ii),e(Ii,D0),e(qn,O0),e(qn,ge),e(ge,P0),e(ge,Fh),e(Fh,R0),e(ge,N0),e(ge,Jh),e(Jh,x0),e(ge,S0),e(G,I0),e(G,vn),e(vn,Hi),e(Hi,H0),e(vn,B0),e(vn,Dt),e(Dt,C0),e(Dt,Kh),e(Kh,G0),e(Dt,U0),e(G,L0),e(G,yn),e(yn,Bi),e(Bi,z0),e(yn,M0),e(yn,Ot),e(Ot,F0),e(Ot,Wh),e(Wh,J0),e(Ot,K0),e(G,W0),e(G,En),e(En,Ci),e(Ci,Yh),e(Yh,Y0),e(En,V0),e(En,Gi),e(Gi,X0),e(G,Q0),e(G,wn),e(wn,Ui),e(Ui,Z0),e(wn,eT),e(wn,Pt),e(Pt,tT),e(Pt,Vh),e(Vh,sT),e(Pt,aT),e(G,nT),e(G,bn),e(bn,Li),e(Li,rT),e(bn,oT),e(bn,Rt),e(Rt,lT),e(Rt,Xh),e(Xh,iT),e(Rt,uT),e(G,pT),e(G,jn),e(jn,zi),e(zi,cT),e(jn,fT),e(jn,Nt),e(Nt,hT),e(Nt,Qh),e(Qh,dT),e(Nt,gT),m(a,x_,g),m(a,Mi,g),e(Mi,mT),m(a,S_,g),m(a,xt,g),e(xt,Zh),e(Zh,Tn),e(Tn,Fi),e(Fi,$T),e(Tn,_T),e(Tn,ed),e(xt,qT),e(xt,td),e(td,kn),e(kn,Ji),e(Ji,sd),e(sd,vT),e(kn,yT),e(kn,Ki),e(Ki,ET),m(a,I_,g),m(a,He,g),e(He,St),e(St,ad),y(An,ad,null),e(He,wT),e(He,nd),e(nd,bT),m(a,H_,g),m(a,Wi,g),e(Wi,jT),m(a,B_,g),y(It,a,g),m(a,C_,g),m(a,Dn,g),e(Dn,TT),e(Dn,On),e(On,kT),m(a,G_,g),m(a,Yi,g),e(Yi,AT),m(a,U_,g),y(Ht,a,g),m(a,L_,g),m(a,Vi,g),e(Vi,DT),m(a,z_,g),m(a,Bt,g),e(Bt,rd),e(rd,Pn),e(Pn,Xi),e(Xi,OT),e(Pn,PT),e(Pn,od),e(Bt,RT),e(Bt,x),e(x,Rn),e(Rn,Nn),e(Nn,ld),e(ld,NT),e(Nn,xT),e(Rn,ST),e(Rn,id),e(x,IT),e(x,xn),e(xn,Qi),e(Qi,HT),e(xn,BT),e(xn,Zi),e(Zi,CT),e(x,GT),e(x,Sn),e(Sn,eu),e(eu,UT),e(Sn,LT),e(Sn,tu),e(tu,zT),e(x,MT),e(x,In),e(In,su),e(su,FT),e(In,JT),e(In,Ct),e(Ct,KT),e(Ct,ud),e(ud,WT),e(Ct,YT),e(x,VT),e(x,Hn),e(Hn,au),e(au,pd),e(pd,XT),e(Hn,QT),e(Hn,nu),e(nu,ZT),e(x,e3),e(x,Bn),e(Bn,ru),e(ru,t3),e(Bn,s3),e(Bn,me),e(me,a3),e(me,cd),e(cd,n3),e(me,r3),e(me,fd),e(fd,o3),e(me,l3),e(x,i3),e(x,Cn),e(Cn,ou),e(ou,u3),e(Cn,p3),e(Cn,$e),e($e,c3),e($e,hd),e(hd,f3),e($e,h3),e($e,dd),e(dd,d3),e($e,g3),e(x,m3),e(x,Gn),e(Gn,lu),e(lu,$3),e(Gn,_3),e(Gn,_e),e(_e,q3),e(_e,gd),e(gd,v3),e(_e,y3),e(_e,md),e(md,E3),e(_e,w3),e(x,b3),e(x,Un),e(Un,iu),e(iu,j3),e(Un,T3),e(Un,qe),e(qe,k3),e(qe,$d),e($d,A3),e(qe,D3),e(qe,_d),e(_d,O3),e(qe,P3),e(x,R3),e(x,Ln),e(Ln,uu),e(uu,N3),e(Ln,x3),e(Ln,ve),e(ve,S3),e(ve,qd),e(qd,I3),e(ve,H3),e(ve,vd),e(vd,B3),e(ve,C3),e(x,G3),e(x,zn),e(zn,pu),e(pu,U3),e(zn,L3),e(zn,Gt),e(Gt,z3),e(Gt,yd),e(yd,M3),e(Gt,F3),e(x,J3),e(x,Mn),e(Mn,cu),e(cu,K3),e(Mn,W3),e(Mn,Ut),e(Ut,Y3),e(Ut,Ed),e(Ed,V3),e(Ut,X3),e(x,Q3),e(x,Fn),e(Fn,fu),e(fu,wd),e(wd,Z3),e(Fn,e5),e(Fn,hu),e(hu,t5),e(x,s5),e(x,Jn),e(Jn,du),e(du,a5),e(Jn,n5),e(Jn,Lt),e(Lt,r5),e(Lt,bd),e(bd,o5),e(Lt,l5),e(x,i5),e(x,Kn),e(Kn,gu),e(gu,u5),e(Kn,p5),e(Kn,zt),e(zt,c5),e(zt,jd),e(jd,f5),e(zt,h5),e(x,d5),e(x,Wn),e(Wn,mu),e(mu,g5),e(Wn,m5),e(Wn,Mt),e(Mt,$5),e(Mt,Td),e(Td,_5),e(Mt,q5),m(a,M_,g),m(a,$u,g),e($u,v5),m(a,F_,g),m(a,Ft,g),e(Ft,kd),e(kd,Yn),e(Yn,_u),e(_u,y5),e(Yn,E5),e(Yn,Ad),e(Ft,w5),e(Ft,re),e(re,Vn),e(Vn,qu),e(qu,Dd),e(Dd,b5),e(Vn,j5),e(Vn,vu),e(vu,T5),e(re,k5),e(re,Xn),e(Xn,yu),e(yu,Od),e(Od,A5),e(Xn,D5),e(Xn,Eu),e(Eu,O5),e(re,P5),e(re,Qn),e(Qn,wu),e(wu,R5),e(Qn,N5),e(Qn,bu),e(bu,x5),e(re,S5),e(re,Zn),e(Zn,ju),e(ju,I5),e(Zn,H5),e(Zn,Tu),e(Tu,B5),m(a,J_,g),m(a,Be,g),e(Be,Jt),e(Jt,Pd),y(er,Pd,null),e(Be,C5),e(Be,Rd),e(Rd,G5),m(a,K_,g),m(a,ku,g),e(ku,U5),m(a,W_,g),y(Kt,a,g),m(a,Y_,g),m(a,tr,g),e(tr,L5),e(tr,sr),e(sr,z5),m(a,V_,g),m(a,Au,g),e(Au,M5),m(a,X_,g),y(Wt,a,g),m(a,Q_,g),m(a,Du,g),e(Du,F5),m(a,Z_,g),m(a,Yt,g),e(Yt,Nd),e(Nd,ar),e(ar,Ou),e(Ou,J5),e(ar,K5),e(ar,xd),e(Yt,W5),e(Yt,J),e(J,nr),e(nr,rr),e(rr,Sd),e(Sd,Y5),e(rr,V5),e(nr,X5),e(nr,Id),e(J,Q5),e(J,or),e(or,Pu),e(Pu,Z5),e(or,ek),e(or,Ru),e(Ru,tk),e(J,sk),e(J,lr),e(lr,Nu),e(Nu,ak),e(lr,nk),e(lr,xu),e(xu,rk),e(J,ok),e(J,ir),e(ir,Su),e(Su,Hd),e(Hd,lk),e(ir,ik),e(ir,Iu),e(Iu,uk),e(J,pk),e(J,ur),e(ur,Hu),e(Hu,ck),e(ur,fk),e(ur,Vt),e(Vt,hk),e(Vt,Bd),e(Bd,dk),e(Vt,gk),e(J,mk),e(J,pr),e(pr,Bu),e(Bu,$k),e(pr,_k),e(pr,Xt),e(Xt,qk),e(Xt,Cd),e(Cd,vk),e(Xt,yk),e(J,Ek),e(J,cr),e(cr,Cu),e(Cu,wk),e(cr,bk),e(cr,Qt),e(Qt,jk),e(Qt,Gd),e(Gd,Tk),e(Qt,kk),m(a,eq,g),m(a,Gu,g),e(Gu,Ak),m(a,tq,g),y(Zt,a,g),m(a,sq,g),m(a,es,g),e(es,Ud),e(Ud,fr),e(fr,Uu),e(Uu,Dk),e(fr,Ok),e(fr,Ld),e(es,Pk),e(es,oe),e(oe,hr),e(hr,Lu),e(Lu,zd),e(zd,Rk),e(hr,Nk),e(hr,zu),e(zu,xk),e(oe,Sk),e(oe,dr),e(dr,Mu),e(Mu,Md),e(Md,Ik),e(dr,Hk),e(dr,Fu),e(Fu,Bk),e(oe,Ck),e(oe,gr),e(gr,Ju),e(Ju,Fd),e(Fd,Gk),e(gr,Uk),e(gr,Ku),e(Ku,Lk),e(oe,zk),e(oe,mr),e(mr,Wu),e(Wu,Jd),e(Jd,Mk),e(mr,Fk),e(mr,Yu),e(Yu,Jk),m(a,aq,g),m(a,Ce,g),e(Ce,ts),e(ts,Kd),y($r,Kd,null),e(Ce,Kk),e(Ce,Wd),e(Wd,Wk),m(a,nq,g),m(a,Vu,g),e(Vu,Yk),m(a,rq,g),y(ss,a,g),m(a,oq,g),m(a,Ge,g),e(Ge,Vk),e(Ge,_r),e(_r,Xk),e(Ge,Qk),e(Ge,qr),e(qr,Zk),m(a,lq,g),m(a,Xu,g),e(Xu,e4),m(a,iq,g),y(as,a,g),m(a,uq,g),m(a,Qu,g),e(Qu,t4),m(a,pq,g),m(a,Zu,g),e(Zu,s4),m(a,cq,g),y(ns,a,g),m(a,fq,g),m(a,rs,g),e(rs,Yd),e(Yd,vr),e(vr,ep),e(ep,a4),e(vr,n4),e(vr,Vd),e(rs,r4),e(rs,le),e(le,yr),e(yr,tp),e(tp,Xd),e(Xd,o4),e(yr,l4),e(yr,sp),e(sp,i4),e(le,u4),e(le,Er),e(Er,ap),e(ap,Qd),e(Qd,p4),e(Er,c4),e(Er,np),e(np,f4),e(le,h4),e(le,wr),e(wr,rp),e(rp,Zd),e(Zd,d4),e(wr,g4),e(wr,os),e(os,m4),e(os,eg),e(eg,$4),e(os,_4),e(le,q4),e(le,br),e(br,op),e(op,tg),e(tg,v4),e(br,y4),e(br,ls),e(ls,E4),e(ls,sg),e(sg,w4),e(ls,b4),m(a,hq,g),m(a,Ue,g),e(Ue,is),e(is,ag),y(jr,ag,null),e(Ue,j4),e(Ue,ng),e(ng,T4),m(a,dq,g),m(a,lp,g),e(lp,k4),m(a,gq,g),y(us,a,g),m(a,mq,g),m(a,Tr,g),e(Tr,A4),e(Tr,kr),e(kr,D4),m(a,$q,g),m(a,ip,g),e(ip,O4),m(a,_q,g),y(ps,a,g),m(a,qq,g),m(a,up,g),e(up,P4),m(a,vq,g),m(a,cs,g),e(cs,rg),e(rg,Ar),e(Ar,pp),e(pp,R4),e(Ar,N4),e(Ar,og),e(cs,x4),e(cs,ee),e(ee,Dr),e(Dr,Or),e(Or,lg),e(lg,S4),e(Or,I4),e(Dr,H4),e(Dr,cp),e(cp,B4),e(ee,C4),e(ee,Pr),e(Pr,fp),e(fp,ig),e(ig,G4),e(Pr,U4),e(Pr,hp),e(hp,L4),e(ee,z4),e(ee,Rr),e(Rr,dp),e(dp,M4),e(Rr,F4),e(Rr,fs),e(fs,J4),e(fs,ug),e(ug,K4),e(fs,W4),e(ee,Y4),e(ee,Nr),e(Nr,gp),e(gp,V4),e(Nr,X4),e(Nr,hs),e(hs,Q4),e(hs,pg),e(pg,Z4),e(hs,e7),e(ee,t7),e(ee,xr),e(xr,mp),e(mp,s7),e(xr,a7),e(xr,ds),e(ds,n7),e(ds,cg),e(cg,r7),e(ds,o7),m(a,yq,g),m(a,$p,g),e($p,l7),m(a,Eq,g),y(gs,a,g),m(a,wq,g),m(a,ms,g),e(ms,fg),e(fg,Sr),e(Sr,_p),e(_p,i7),e(Sr,u7),e(Sr,hg),e(ms,p7),e(ms,Ir),e(Ir,Hr),e(Hr,qp),e(qp,dg),e(dg,c7),e(Hr,f7),e(Hr,vp),e(vp,h7),e(Ir,d7),e(Ir,Br),e(Br,yp),e(yp,gg),e(gg,g7),e(Br,m7),e(Br,Ep),e(Ep,$7),m(a,bq,g),m(a,Le,g),e(Le,$s),e($s,mg),y(Cr,mg,null),e(Le,_7),e(Le,$g),e($g,q7),m(a,jq,g),m(a,Gr,g),e(Gr,v7),e(Gr,wp),e(wp,y7),m(a,Tq,g),m(a,ze,g),e(ze,_s),e(_s,_g),y(Ur,_g,null),e(ze,E7),e(ze,qg),e(qg,w7),m(a,kq,g),m(a,bp,g),e(bp,b7),m(a,Aq,g),y(qs,a,g),m(a,Dq,g),m(a,Me,g),e(Me,j7),e(Me,Lr),e(Lr,T7),e(Me,k7),e(Me,zr),e(zr,A7),m(a,Oq,g),m(a,jp,g),e(jp,D7),m(a,Pq,g),y(vs,a,g),m(a,Rq,g),m(a,Tp,g),e(Tp,O7),m(a,Nq,g),m(a,ys,g),e(ys,vg),e(vg,Mr),e(Mr,kp),e(kp,P7),e(Mr,R7),e(Mr,yg),e(ys,N7),e(ys,K),e(K,Fr),e(Fr,Jr),e(Jr,Eg),e(Eg,x7),e(Jr,S7),e(Fr,I7),e(Fr,Ap),e(Ap,H7),e(K,B7),e(K,Kr),e(Kr,Dp),e(Dp,wg),e(wg,C7),e(Kr,G7),e(Kr,Op),e(Op,U7),e(K,L7),e(K,Wr),e(Wr,Pp),e(Pp,z7),e(Wr,M7),e(Wr,S),e(S,F7),e(S,bg),e(bg,J7),e(S,K7),e(S,W7),e(S,Y7),e(S,jg),e(jg,V7),e(S,X7),e(S,Q7),e(S,Z7),e(S,Tg),e(Tg,e6),e(S,t6),e(S,s6),e(S,a6),e(S,kg),e(kg,n6),e(S,r6),e(S,Ag),e(Ag,o6),e(S,l6),e(S,i6),e(S,u6),e(S,Dg),e(Dg,p6),e(S,c6),e(S,Og),e(Og,f6),e(S,h6),e(S,d6),e(S,g6),e(S,Pg),e(Pg,m6),e(S,$6),e(S,Rg),e(Rg,_6),e(S,q6),e(K,v6),e(K,Yr),e(Yr,Rp),e(Rp,Ng),e(Ng,y6),e(Yr,E6),e(Yr,Np),e(Np,w6),e(K,b6),e(K,Vr),e(Vr,xp),e(xp,j6),e(Vr,T6),e(Vr,Es),e(Es,k6),e(Es,xg),e(xg,A6),e(Es,D6),e(K,O6),e(K,Xr),e(Xr,Sp),e(Sp,P6),e(Xr,R6),e(Xr,ws),e(ws,N6),e(ws,Sg),e(Sg,x6),e(ws,S6),e(K,I6),e(K,Qr),e(Qr,Ip),e(Ip,H6),e(Qr,B6),e(Qr,bs),e(bs,C6),e(bs,Ig),e(Ig,G6),e(bs,U6),m(a,xq,g),m(a,Hp,g),e(Hp,L6),m(a,Sq,g),y(js,a,g),m(a,Iq,g),m(a,Ts,g),e(Ts,Hg),e(Hg,Zr),e(Zr,Bp),e(Bp,z6),e(Zr,M6),e(Zr,Bg),e(Ts,F6),e(Ts,te),e(te,eo),e(eo,Cp),e(Cp,Cg),e(Cg,J6),e(eo,K6),e(eo,Gp),e(Gp,W6),e(te,Y6),e(te,to),e(to,Up),e(Up,Gg),e(Gg,V6),e(to,X6),e(to,Lp),e(Lp,Q6),e(te,Z6),e(te,so),e(so,zp),e(zp,Ug),e(Ug,e9),e(so,t9),e(so,Mp),e(Mp,s9),e(te,a9),e(te,ao),e(ao,Fp),e(Fp,Lg),e(Lg,n9),e(ao,r9),e(ao,ks),e(ks,o9),e(ks,zg),e(zg,l9),e(ks,i9),e(te,u9),e(te,no),e(no,Jp),e(Jp,Mg),e(Mg,p9),e(no,c9),e(no,As),e(As,f9),e(As,Fg),e(Fg,h9),e(As,d9),m(a,Hq,g),m(a,Fe,g),e(Fe,Ds),e(Ds,Jg),y(ro,Jg,null),e(Fe,g9),e(Fe,Kg),e(Kg,m9),m(a,Bq,g),m(a,Kp,g),e(Kp,$9),m(a,Cq,g),y(Os,a,g),m(a,Gq,g),m(a,oo,g),e(oo,_9),e(oo,lo),e(lo,q9),m(a,Uq,g),m(a,Wp,g),e(Wp,v9),m(a,Lq,g),y(Ps,a,g),m(a,zq,g),m(a,Yp,g),e(Yp,y9),m(a,Mq,g),m(a,Rs,g),e(Rs,Wg),e(Wg,io),e(io,Vp),e(Vp,E9),e(io,w9),e(io,Yg),e(Rs,b9),e(Rs,I),e(I,uo),e(uo,po),e(po,Vg),e(Vg,j9),e(po,T9),e(uo,k9),e(uo,Xp),e(Xp,A9),e(I,D9),e(I,co),e(co,Qp),e(Qp,Xg),e(Xg,O9),e(co,P9),e(co,Zp),e(Zp,R9),e(I,N9),e(I,fo),e(fo,ec),e(ec,x9),e(fo,S9),e(fo,ye),e(ye,I9),e(ye,Qg),e(Qg,H9),e(ye,B9),e(ye,Zg),e(Zg,C9),e(ye,G9),e(I,U9),e(I,ho),e(ho,tc),e(tc,L9),e(ho,z9),e(ho,Ee),e(Ee,M9),e(Ee,em),e(em,F9),e(Ee,J9),e(Ee,tm),e(tm,K9),e(Ee,W9),e(I,Y9),e(I,go),e(go,sc),e(sc,V9),e(go,X9),e(go,we),e(we,Q9),e(we,sm),e(sm,Z9),e(we,e8),e(we,am),e(am,t8),e(we,s8),e(I,a8),e(I,mo),e(mo,ac),e(ac,n8),e(mo,r8),e(mo,Ns),e(Ns,o8),e(Ns,nm),e(nm,l8),e(Ns,i8),e(I,u8),e(I,$o),e($o,nc),e(nc,p8),e($o,c8),e($o,be),e(be,f8),e(be,rm),e(rm,h8),e(be,d8),e(be,om),e(om,g8),e(be,m8),e(I,$8),e(I,_o),e(_o,rc),e(rc,_8),e(_o,q8),e(_o,xs),e(xs,v8),e(xs,lm),e(lm,y8),e(xs,E8),e(I,w8),e(I,qo),e(qo,oc),e(oc,b8),e(qo,j8),e(qo,je),e(je,T8),e(je,im),e(im,k8),e(je,A8),e(je,um),e(um,D8),e(je,O8),e(I,P8),e(I,vo),e(vo,lc),e(lc,R8),e(vo,N8),e(vo,Ss),e(Ss,x8),e(Ss,pm),e(pm,S8),e(Ss,I8),e(I,H8),e(I,yo),e(yo,ic),e(ic,B8),e(yo,C8),e(yo,Is),e(Is,G8),e(Is,cm),e(cm,U8),e(Is,L8),e(I,z8),e(I,Eo),e(Eo,uc),e(uc,fm),e(fm,M8),e(Eo,F8),e(Eo,pc),e(pc,J8),e(I,K8),e(I,wo),e(wo,cc),e(cc,W8),e(wo,Y8),e(wo,Hs),e(Hs,V8),e(Hs,hm),e(hm,X8),e(Hs,Q8),e(I,Z8),e(I,bo),e(bo,fc),e(fc,eA),e(bo,tA),e(bo,Bs),e(Bs,sA),e(Bs,dm),e(dm,aA),e(Bs,nA),e(I,rA),e(I,jo),e(jo,hc),e(hc,oA),e(jo,lA),e(jo,Cs),e(Cs,iA),e(Cs,gm),e(gm,uA),e(Cs,pA),m(a,Fq,g),m(a,dc,g),e(dc,cA),m(a,Jq,g),y(Gs,a,g),m(a,Kq,g),m(a,Us,g),e(Us,mm),e(mm,To),e(To,gc),e(gc,fA),e(To,hA),e(To,$m),e(Us,dA),e(Us,_m),e(_m,ko),e(ko,mc),e(mc,qm),e(qm,gA),e(ko,mA),e(ko,$c),e($c,$A),m(a,Wq,g),m(a,Je,g),e(Je,Ls),e(Ls,vm),y(Ao,vm,null),e(Je,_A),e(Je,ym),e(ym,qA),m(a,Yq,g),m(a,zs,g),e(zs,vA),e(zs,_c),e(_c,yA),e(zs,EA),m(a,Vq,g),m(a,Ke,g),e(Ke,Ms),e(Ms,Em),y(Do,Em,null),e(Ke,wA),e(Ke,wm),e(wm,bA),m(a,Xq,g),m(a,qc,g),e(qc,jA),m(a,Qq,g),y(Fs,a,g),m(a,Zq,g),m(a,Oo,g),e(Oo,TA),e(Oo,Po),e(Po,kA),m(a,e1,g),m(a,vc,g),e(vc,AA),m(a,t1,g),y(Js,a,g),m(a,s1,g),m(a,yc,g),e(yc,DA),m(a,a1,g),m(a,Ks,g),e(Ks,bm),e(bm,Ro),e(Ro,Ec),e(Ec,OA),e(Ro,PA),e(Ro,jm),e(Ks,RA),e(Ks,se),e(se,No),e(No,xo),e(xo,Tm),e(Tm,NA),e(xo,xA),e(No,SA),e(No,wc),e(wc,IA),e(se,HA),e(se,So),e(So,bc),e(bc,km),e(km,BA),e(So,CA),e(So,jc),e(jc,GA),e(se,UA),e(se,Io),e(Io,Tc),e(Tc,LA),e(Io,zA),e(Io,Ws),e(Ws,MA),e(Ws,Am),e(Am,FA),e(Ws,JA),e(se,KA),e(se,Ho),e(Ho,kc),e(kc,WA),e(Ho,YA),e(Ho,Ys),e(Ys,VA),e(Ys,Dm),e(Dm,XA),e(Ys,QA),e(se,ZA),e(se,Bo),e(Bo,Ac),e(Ac,eD),e(Bo,tD),e(Bo,Vs),e(Vs,sD),e(Vs,Om),e(Om,aD),e(Vs,nD),m(a,n1,g),m(a,Dc,g),e(Dc,rD),m(a,r1,g),y(Xs,a,g),m(a,o1,g),m(a,Qs,g),e(Qs,Pm),e(Pm,Co),e(Co,Oc),e(Oc,oD),e(Co,lD),e(Co,Rm),e(Qs,iD),e(Qs,ie),e(ie,Go),e(Go,Pc),e(Pc,Nm),e(Nm,uD),e(Go,pD),e(Go,Rc),e(Rc,cD),e(ie,fD),e(ie,Uo),e(Uo,Nc),e(Nc,xm),e(xm,hD),e(Uo,dD),e(Uo,xc),e(xc,gD),e(ie,mD),e(ie,Lo),e(Lo,Sc),e(Sc,Sm),e(Sm,$D),e(Lo,_D),e(Lo,Ic),e(Ic,qD),e(ie,vD),e(ie,zo),e(zo,Hc),e(Hc,Im),e(Im,yD),e(zo,ED),e(zo,Bc),e(Bc,wD),m(a,l1,g),m(a,We,g),e(We,Zs),e(Zs,Hm),y(Mo,Hm,null),e(We,bD),e(We,Bm),e(Bm,jD),m(a,i1,g),m(a,Cc,g),e(Cc,TD),m(a,u1,g),y(ea,a,g),m(a,p1,g),y(ta,a,g),m(a,c1,g),m(a,ue,g),e(ue,kD),e(ue,Fo),e(Fo,AD),e(ue,DD),e(ue,Jo),e(Jo,OD),e(ue,PD),e(ue,Ko),e(Ko,RD),m(a,f1,g),m(a,Gc,g),e(Gc,ND),m(a,h1,g),y(sa,a,g),m(a,d1,g),m(a,Uc,g),e(Uc,xD),m(a,g1,g),m(a,aa,g),e(aa,Cm),e(Cm,Wo),e(Wo,Lc),e(Lc,SD),e(Wo,ID),e(Wo,Gm),e(aa,HD),e(aa,Um),e(Um,Yo),e(Yo,Vo),e(Vo,Lm),e(Lm,BD),e(Vo,CD),e(Yo,GD),e(Yo,zc),e(zc,UD),m(a,m1,g),m(a,Mc,g),e(Mc,LD),m(a,$1,g),m(a,Fc,g),e(Fc,zD),m(a,_1,g),y(na,a,g),m(a,q1,g),m(a,ra,g),e(ra,zm),e(zm,Xo),e(Xo,Jc),e(Jc,MD),e(Xo,FD),e(Xo,Mm),e(ra,JD),e(ra,Fm),e(Fm,Qo),e(Qo,Kc),e(Kc,Jm),e(Jm,KD),e(Qo,WD),e(Qo,Wc),e(Wc,YD),m(a,v1,g),m(a,Ye,g),e(Ye,oa),e(oa,Km),y(Zo,Km,null),e(Ye,VD),e(Ye,Wm),e(Wm,XD),m(a,y1,g),m(a,Yc,g),e(Yc,QD),m(a,E1,g),y(la,a,g),m(a,w1,g),m(a,Ve,g),e(Ve,ZD),e(Ve,el),e(el,eO),e(Ve,tO),e(Ve,tl),e(tl,sO),m(a,b1,g),m(a,Vc,g),e(Vc,aO),m(a,j1,g),m(a,ia,g),e(ia,Ym),e(Ym,sl),e(sl,Xc),e(Xc,nO),e(sl,rO),e(sl,Vm),e(ia,oO),e(ia,ae),e(ae,al),e(al,nl),e(nl,Xm),e(Xm,lO),e(nl,iO),e(al,uO),e(al,Qc),e(Qc,pO),e(ae,cO),e(ae,rl),e(rl,Zc),e(Zc,Qm),e(Qm,fO),e(rl,hO),e(rl,ef),e(ef,dO),e(ae,gO),e(ae,ol),e(ol,tf),e(tf,mO),e(ol,$O),e(ol,ua),e(ua,_O),e(ua,Zm),e(Zm,qO),e(ua,vO),e(ae,yO),e(ae,ll),e(ll,sf),e(sf,EO),e(ll,wO),e(ll,pa),e(pa,bO),e(pa,e$),e(e$,jO),e(pa,TO),e(ae,kO),e(ae,il),e(il,af),e(af,AO),e(il,DO),e(il,ca),e(ca,OO),e(ca,t$),e(t$,PO),e(ca,RO),m(a,T1,g),m(a,nf,g),e(nf,NO),m(a,k1,g),m(a,fa,g),e(fa,s$),e(s$,ul),e(ul,rf),e(rf,xO),e(ul,SO),e(ul,a$),e(fa,IO),e(fa,n$),e(n$,pl),e(pl,of),e(of,r$),e(r$,HO),e(pl,BO),e(pl,lf),e(lf,CO),m(a,A1,g),m(a,uf,g),e(uf,GO),m(a,D1,g),m(a,Xe,g),e(Xe,ha),e(ha,o$),y(cl,o$,null),e(Xe,UO),e(Xe,l$),e(l$,LO),m(a,O1,g),m(a,pf,g),e(pf,zO),m(a,P1,g),y(da,a,g),m(a,R1,g),m(a,Qe,g),e(Qe,MO),e(Qe,fl),e(fl,FO),e(Qe,JO),e(Qe,hl),e(hl,KO),m(a,N1,g),m(a,cf,g),e(cf,WO),m(a,x1,g),y(ga,a,g),m(a,S1,g),m(a,ff,g),e(ff,YO),m(a,I1,g),m(a,ma,g),e(ma,i$),e(i$,dl),e(dl,hf),e(hf,VO),e(dl,XO),e(dl,u$),e(ma,QO),e(ma,p$),e(p$,gl),e(gl,ml),e(ml,c$),e(c$,ZO),e(ml,eP),e(gl,tP),e(gl,df),e(df,sP),m(a,H1,g),m(a,gf,g),e(gf,aP),m(a,B1,g),y($a,a,g),m(a,C1,g),m(a,_a,g),e(_a,f$),e(f$,$l),e($l,mf),e(mf,nP),e($l,rP),e($l,h$),e(_a,oP),e(_a,_l),e(_l,ql),e(ql,$f),e($f,d$),e(d$,lP),e(ql,iP),e(ql,_f),e(_f,uP),e(_l,pP),e(_l,vl),e(vl,qf),e(qf,g$),e(g$,cP),e(vl,fP),e(vl,vf),e(vf,hP),m(a,G1,g),m(a,Ze,g),e(Ze,qa),e(qa,m$),y(yl,m$,null),e(Ze,dP),e(Ze,$$),e($$,gP),m(a,U1,g),m(a,yf,g),e(yf,mP),m(a,L1,g),y(va,a,g),m(a,z1,g),m(a,El,g),e(El,$P),e(El,wl),e(wl,_P),m(a,M1,g),m(a,Ef,g),e(Ef,qP),m(a,F1,g),y(ya,a,g),m(a,J1,g),m(a,Ea,g),e(Ea,vP),e(Ea,bl),e(bl,yP),e(Ea,EP),m(a,K1,g),m(a,wa,g),e(wa,_$),e(_$,jl),e(jl,wf),e(wf,wP),e(jl,bP),e(jl,q$),e(wa,jP),e(wa,v$),e(v$,Tl),e(Tl,kl),e(kl,y$),e(y$,TP),e(kl,kP),e(Tl,AP),e(Tl,bf),e(bf,DP),m(a,W1,g),m(a,jf,g),e(jf,OP),m(a,Y1,g),y(ba,a,g),m(a,V1,g),m(a,ja,g),e(ja,E$),e(E$,Al),e(Al,Tf),e(Tf,PP),e(Al,RP),e(Al,w$),e(ja,NP),e(ja,et),e(et,Dl),e(Dl,kf),e(kf,b$),e(b$,xP),e(Dl,SP),e(Dl,Af),e(Af,IP),e(et,HP),e(et,Ol),e(Ol,Df),e(Df,j$),e(j$,BP),e(Ol,CP),e(Ol,Of),e(Of,GP),e(et,UP),e(et,Pl),e(Pl,Pf),e(Pf,T$),e(T$,LP),e(Pl,zP),e(Pl,Rf),e(Rf,MP),X1=!0},p(a,[g]){const Rl={};g&2&&(Rl.$$scope={dirty:g,ctx:a}),ot.$set(Rl);const k$={};g&2&&(k$.$$scope={dirty:g,ctx:a}),lt.$set(k$);const A$={};g&2&&(A$.$$scope={dirty:g,ctx:a}),ht.$set(A$);const D$={};g&2&&(D$.$$scope={dirty:g,ctx:a}),$t.$set(D$);const Nl={};g&2&&(Nl.$$scope={dirty:g,ctx:a}),_t.$set(Nl);const O$={};g&2&&(O$.$$scope={dirty:g,ctx:a}),Tt.$set(O$);const P$={};g&2&&(P$.$$scope={dirty:g,ctx:a}),kt.$set(P$);const R$={};g&2&&(R$.$$scope={dirty:g,ctx:a}),It.$set(R$);const N$={};g&2&&(N$.$$scope={dirty:g,ctx:a}),Ht.$set(N$);const x$={};g&2&&(x$.$$scope={dirty:g,ctx:a}),Kt.$set(x$);const xl={};g&2&&(xl.$$scope={dirty:g,ctx:a}),Wt.$set(xl);const S$={};g&2&&(S$.$$scope={dirty:g,ctx:a}),Zt.$set(S$);const I$={};g&2&&(I$.$$scope={dirty:g,ctx:a}),ss.$set(I$);const H$={};g&2&&(H$.$$scope={dirty:g,ctx:a}),as.$set(H$);const B$={};g&2&&(B$.$$scope={dirty:g,ctx:a}),ns.$set(B$);const Nf={};g&2&&(Nf.$$scope={dirty:g,ctx:a}),us.$set(Nf);const C$={};g&2&&(C$.$$scope={dirty:g,ctx:a}),ps.$set(C$);const G$={};g&2&&(G$.$$scope={dirty:g,ctx:a}),gs.$set(G$);const U$={};g&2&&(U$.$$scope={dirty:g,ctx:a}),qs.$set(U$);const Sl={};g&2&&(Sl.$$scope={dirty:g,ctx:a}),vs.$set(Sl);const L$={};g&2&&(L$.$$scope={dirty:g,ctx:a}),js.$set(L$);const Il={};g&2&&(Il.$$scope={dirty:g,ctx:a}),Os.$set(Il);const z$={};g&2&&(z$.$$scope={dirty:g,ctx:a}),Ps.$set(z$);const M={};g&2&&(M.$$scope={dirty:g,ctx:a}),Gs.$set(M);const Hl={};g&2&&(Hl.$$scope={dirty:g,ctx:a}),Fs.$set(Hl);const xf={};g&2&&(xf.$$scope={dirty:g,ctx:a}),Js.$set(xf);const M$={};g&2&&(M$.$$scope={dirty:g,ctx:a}),Xs.$set(M$);const F$={};g&2&&(F$.$$scope={dirty:g,ctx:a}),ea.$set(F$);const Bl={};g&2&&(Bl.$$scope={dirty:g,ctx:a}),ta.$set(Bl);const Sf={};g&2&&(Sf.$$scope={dirty:g,ctx:a}),sa.$set(Sf);const J$={};g&2&&(J$.$$scope={dirty:g,ctx:a}),na.$set(J$);const K$={};g&2&&(K$.$$scope={dirty:g,ctx:a}),la.$set(K$);const Cl={};g&2&&(Cl.$$scope={dirty:g,ctx:a}),da.$set(Cl);const W$={};g&2&&(W$.$$scope={dirty:g,ctx:a}),ga.$set(W$);const tt={};g&2&&(tt.$$scope={dirty:g,ctx:a}),$a.$set(tt);const Y$={};g&2&&(Y$.$$scope={dirty:g,ctx:a}),va.$set(Y$);const V$={};g&2&&(V$.$$scope={dirty:g,ctx:a}),ya.$set(V$);const Gl={};g&2&&(Gl.$$scope={dirty:g,ctx:a}),ba.$set(Gl)},i(a){X1||(E(k.$$.fragment,a),E(Q.$$.fragment,a),E(Pa.$$.fragment,a),E(ot.$$.fragment,a),E(lt.$$.fragment,a),E(ht.$$.fragment,a),E(Ya.$$.fragment,a),E($t.$$.fragment,a),E(_t.$$.fragment,a),E(ln.$$.fragment,a),E(Tt.$$.fragment,a),E(kt.$$.fragment,a),E(An.$$.fragment,a),E(It.$$.fragment,a),E(Ht.$$.fragment,a),E(er.$$.fragment,a),E(Kt.$$.fragment,a),E(Wt.$$.fragment,a),E(Zt.$$.fragment,a),E($r.$$.fragment,a),E(ss.$$.fragment,a),E(as.$$.fragment,a),E(ns.$$.fragment,a),E(jr.$$.fragment,a),E(us.$$.fragment,a),E(ps.$$.fragment,a),E(gs.$$.fragment,a),E(Cr.$$.fragment,a),E(Ur.$$.fragment,a),E(qs.$$.fragment,a),E(vs.$$.fragment,a),E(js.$$.fragment,a),E(ro.$$.fragment,a),E(Os.$$.fragment,a),E(Ps.$$.fragment,a),E(Gs.$$.fragment,a),E(Ao.$$.fragment,a),E(Do.$$.fragment,a),E(Fs.$$.fragment,a),E(Js.$$.fragment,a),E(Xs.$$.fragment,a),E(Mo.$$.fragment,a),E(ea.$$.fragment,a),E(ta.$$.fragment,a),E(sa.$$.fragment,a),E(na.$$.fragment,a),E(Zo.$$.fragment,a),E(la.$$.fragment,a),E(cl.$$.fragment,a),E(da.$$.fragment,a),E(ga.$$.fragment,a),E($a.$$.fragment,a),E(yl.$$.fragment,a),E(va.$$.fragment,a),E(ya.$$.fragment,a),E(ba.$$.fragment,a),X1=!0)},o(a){w(k.$$.fragment,a),w(Q.$$.fragment,a),w(Pa.$$.fragment,a),w(ot.$$.fragment,a),w(lt.$$.fragment,a),w(ht.$$.fragment,a),w(Ya.$$.fragment,a),w($t.$$.fragment,a),w(_t.$$.fragment,a),w(ln.$$.fragment,a),w(Tt.$$.fragment,a),w(kt.$$.fragment,a),w(An.$$.fragment,a),w(It.$$.fragment,a),w(Ht.$$.fragment,a),w(er.$$.fragment,a),w(Kt.$$.fragment,a),w(Wt.$$.fragment,a),w(Zt.$$.fragment,a),w($r.$$.fragment,a),w(ss.$$.fragment,a),w(as.$$.fragment,a),w(ns.$$.fragment,a),w(jr.$$.fragment,a),w(us.$$.fragment,a),w(ps.$$.fragment,a),w(gs.$$.fragment,a),w(Cr.$$.fragment,a),w(Ur.$$.fragment,a),w(qs.$$.fragment,a),w(vs.$$.fragment,a),w(js.$$.fragment,a),w(ro.$$.fragment,a),w(Os.$$.fragment,a),w(Ps.$$.fragment,a),w(Gs.$$.fragment,a),w(Ao.$$.fragment,a),w(Do.$$.fragment,a),w(Fs.$$.fragment,a),w(Js.$$.fragment,a),w(Xs.$$.fragment,a),w(Mo.$$.fragment,a),w(ea.$$.fragment,a),w(ta.$$.fragment,a),w(sa.$$.fragment,a),w(na.$$.fragment,a),w(Zo.$$.fragment,a),w(la.$$.fragment,a),w(cl.$$.fragment,a),w(da.$$.fragment,a),w(ga.$$.fragment,a),w($a.$$.fragment,a),w(yl.$$.fragment,a),w(va.$$.fragment,a),w(ya.$$.fragment,a),w(ba.$$.fragment,a),X1=!1},d(a){t(n),a&&t(p),a&&t(s),b(k),a&&t(O),a&&t(D),b(Q),a&&t(Oa),a&&t(Re),a&&t(t_),a&&t(zl),a&&t(s_),a&&t(at),a&&t(a_),a&&t(nt),a&&t(n_),a&&t(Ne),b(Pa),a&&t(r_),a&&t(Ml),a&&t(o_),b(ot,a),a&&t(l_),a&&t(Ra),a&&t(i_),a&&t(Fl),a&&t(u_),b(lt,a),a&&t(p_),a&&t(Jl),a&&t(c_),a&&t(it),a&&t(f_),a&&t(ai),a&&t(h_),a&&t(ni),a&&t(d_),b(ht,a),a&&t(g_),a&&t(dt),a&&t(m_),a&&t(Se),b(Ya),a&&t($_),a&&t(ci),a&&t(__),b($t,a),a&&t(q_),a&&t(Va),a&&t(v_),a&&t(fi),a&&t(y_),b(_t,a),a&&t(E_),a&&t(hi),a&&t(w_),a&&t(qt),a&&t(b_),a&&t(yi),a&&t(j_),a&&t(wt),a&&t(T_),a&&t(Ie),b(ln),a&&t(k_),a&&t(jt),a&&t(A_),b(Tt,a),a&&t(D_),a&&t(un),a&&t(O_),a&&t(Ti),a&&t(P_),b(kt,a),a&&t(R_),a&&t(ki),a&&t(N_),a&&t(At),a&&t(x_),a&&t(Mi),a&&t(S_),a&&t(xt),a&&t(I_),a&&t(He),b(An),a&&t(H_),a&&t(Wi),a&&t(B_),b(It,a),a&&t(C_),a&&t(Dn),a&&t(G_),a&&t(Yi),a&&t(U_),b(Ht,a),a&&t(L_),a&&t(Vi),a&&t(z_),a&&t(Bt),a&&t(M_),a&&t($u),a&&t(F_),a&&t(Ft),a&&t(J_),a&&t(Be),b(er),a&&t(K_),a&&t(ku),a&&t(W_),b(Kt,a),a&&t(Y_),a&&t(tr),a&&t(V_),a&&t(Au),a&&t(X_),b(Wt,a),a&&t(Q_),a&&t(Du),a&&t(Z_),a&&t(Yt),a&&t(eq),a&&t(Gu),a&&t(tq),b(Zt,a),a&&t(sq),a&&t(es),a&&t(aq),a&&t(Ce),b($r),a&&t(nq),a&&t(Vu),a&&t(rq),b(ss,a),a&&t(oq),a&&t(Ge),a&&t(lq),a&&t(Xu),a&&t(iq),b(as,a),a&&t(uq),a&&t(Qu),a&&t(pq),a&&t(Zu),a&&t(cq),b(ns,a),a&&t(fq),a&&t(rs),a&&t(hq),a&&t(Ue),b(jr),a&&t(dq),a&&t(lp),a&&t(gq),b(us,a),a&&t(mq),a&&t(Tr),a&&t($q),a&&t(ip),a&&t(_q),b(ps,a),a&&t(qq),a&&t(up),a&&t(vq),a&&t(cs),a&&t(yq),a&&t($p),a&&t(Eq),b(gs,a),a&&t(wq),a&&t(ms),a&&t(bq),a&&t(Le),b(Cr),a&&t(jq),a&&t(Gr),a&&t(Tq),a&&t(ze),b(Ur),a&&t(kq),a&&t(bp),a&&t(Aq),b(qs,a),a&&t(Dq),a&&t(Me),a&&t(Oq),a&&t(jp),a&&t(Pq),b(vs,a),a&&t(Rq),a&&t(Tp),a&&t(Nq),a&&t(ys),a&&t(xq),a&&t(Hp),a&&t(Sq),b(js,a),a&&t(Iq),a&&t(Ts),a&&t(Hq),a&&t(Fe),b(ro),a&&t(Bq),a&&t(Kp),a&&t(Cq),b(Os,a),a&&t(Gq),a&&t(oo),a&&t(Uq),a&&t(Wp),a&&t(Lq),b(Ps,a),a&&t(zq),a&&t(Yp),a&&t(Mq),a&&t(Rs),a&&t(Fq),a&&t(dc),a&&t(Jq),b(Gs,a),a&&t(Kq),a&&t(Us),a&&t(Wq),a&&t(Je),b(Ao),a&&t(Yq),a&&t(zs),a&&t(Vq),a&&t(Ke),b(Do),a&&t(Xq),a&&t(qc),a&&t(Qq),b(Fs,a),a&&t(Zq),a&&t(Oo),a&&t(e1),a&&t(vc),a&&t(t1),b(Js,a),a&&t(s1),a&&t(yc),a&&t(a1),a&&t(Ks),a&&t(n1),a&&t(Dc),a&&t(r1),b(Xs,a),a&&t(o1),a&&t(Qs),a&&t(l1),a&&t(We),b(Mo),a&&t(i1),a&&t(Cc),a&&t(u1),b(ea,a),a&&t(p1),b(ta,a),a&&t(c1),a&&t(ue),a&&t(f1),a&&t(Gc),a&&t(h1),b(sa,a),a&&t(d1),a&&t(Uc),a&&t(g1),a&&t(aa),a&&t(m1),a&&t(Mc),a&&t($1),a&&t(Fc),a&&t(_1),b(na,a),a&&t(q1),a&&t(ra),a&&t(v1),a&&t(Ye),b(Zo),a&&t(y1),a&&t(Yc),a&&t(E1),b(la,a),a&&t(w1),a&&t(Ve),a&&t(b1),a&&t(Vc),a&&t(j1),a&&t(ia),a&&t(T1),a&&t(nf),a&&t(k1),a&&t(fa),a&&t(A1),a&&t(uf),a&&t(D1),a&&t(Xe),b(cl),a&&t(O1),a&&t(pf),a&&t(P1),b(da,a),a&&t(R1),a&&t(Qe),a&&t(N1),a&&t(cf),a&&t(x1),b(ga,a),a&&t(S1),a&&t(ff),a&&t(I1),a&&t(ma),a&&t(H1),a&&t(gf),a&&t(B1),b($a,a),a&&t(C1),a&&t(_a),a&&t(G1),a&&t(Ze),b(yl),a&&t(U1),a&&t(yf),a&&t(L1),b(va,a),a&&t(z1),a&&t(El),a&&t(M1),a&&t(Ef),a&&t(F1),b(ya,a),a&&t(J1),a&&t(Ea),a&&t(K1),a&&t(wa),a&&t(W1),a&&t(jf),a&&t(Y1),b(ba,a),a&&t(V1),a&&t(ja)}}}const xz={local:"detailed-parameters",sections:[{local:"which-task-is-used-by-this-model",title:"Which task is used by this model ?"},{local:"zeroshot-classification-task",title:"Zero-shot classification task"},{local:"translation-task",title:"Translation task"},{local:"summarization-task",title:"Summarization task"},{local:"conversational-task",title:"Conversational task"},{local:"table-question-answering-task",title:"Table question answering task"},{local:"question-answering-task",title:"Question answering task"},{local:"textclassification-task",title:"Text-classification task"},{local:"named-entity-recognition-ner-task",title:"Named Entity Recognition (NER) task"},{local:"tokenclassification-task",title:"Token-classification task"},{local:"textgeneration-task",title:"Text-generation task"},{local:"text2textgeneration-task",title:"Text2text-generation task"},{local:"fill-mask-task",title:"Fill mask task"},{local:"automatic-speech-recognition-task",title:"Automatic speech recognition task"},{local:"featureextraction-task",title:"Feature-extraction task"},{local:"audioclassification-task",title:"Audio-classification task"},{local:"objectdetection-task",title:"Object-detection task"}],title:"Detailed parameters"};function Sz(_){return kU(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Gz extends wU{constructor(n){super();bU(this,n,Sz,Nz,jU,{})}}export{Gz as default,xz as metadata};
