import{S as Se,i as ke,s as Me,e as a,k as u,w as be,t as m,M as Ne,c as l,d as t,m as d,a as o,x as xe,h as f,b as n,F as s,g as c,y as Pe,L as Ue,q as Be,o as Ce,B as Ie,v as De}from"../../chunks/vendor-1e8b365d.js";import{Y as Je}from"../../chunks/Youtube-c2a8cc39.js";import{I as Ye}from"../../chunks/IconCopyLink-483c28ba.js";function je(ue){let h,N,v,_,x,L,z,P,G,U,q,D,E,K,B,V,W,J,T,X,Y,g,Z,j,b,ee,F,i,C,w,te,se,I,$,ae,le,S,y,oe,re,k,A,ne,ie,M,R,ce,H;return L=new Ye({}),q=new Je({props:{id:"MUqNwgPjJvQ"}}),{c(){h=a("meta"),N=u(),v=a("h1"),_=a("a"),x=a("span"),be(L.$$.fragment),z=u(),P=a("span"),G=m("Les mod\xE8les encodeurs"),U=u(),be(q.$$.fragment),D=u(),E=a("p"),K=m("Les mod\xE8les encodeurs utilisent uniquement l\u2019encodeur d\u2019un mod\xE8le Transformer. \xC0 chaque \xE9tape, les couches d\u2019attention peuvent acc\xE9der \xE0 tous les mots de la phrase initiale. Ces mod\xE8les sont souvent caract\xE9ris\xE9s comme ayant une attention \u201Cbi-directionnelle\u201D et sont souvent appel\xE9s "),B=a("em"),V=m("mod\xE8les d\u2019auto-encodage"),W=m("."),J=u(),T=a("p"),X=m("Le pr\xE9-entra\xEEnement de ces mod\xE8les se concentre g\xE9n\xE9ralement sur la modification d\u2019une phrase donn\xE9e (par exemple, en masquant des mots al\xE9atoires dans celle-ci) et en demandant au mod\xE8le de trouver ou de reconstruire la phrase initiale."),Y=u(),g=a("p"),Z=m("Ces mod\xE8les encodeurs sont les plus adapt\xE9s pour des t\xE2ches qui requi\xE8rent une compr\xE9hension compl\xE8te de la phrase, telles que la classification de phrases, la reconnaissance des entit\xE9s nomm\xE9es (et plus g\xE9n\xE9ralement la classification de mots) et les questions-r\xE9ponses extractives."),j=u(),b=a("p"),ee=m("Les mod\xE8les les plus repr\xE9sentatifs de cette famille sont:"),F=u(),i=a("ul"),C=a("li"),w=a("a"),te=m("ALBERT"),se=u(),I=a("li"),$=a("a"),ae=m("BERT"),le=u(),S=a("li"),y=a("a"),oe=m("DistilBERT"),re=u(),k=a("li"),A=a("a"),ne=m("ELECTRA"),ie=u(),M=a("li"),R=a("a"),ce=m("RoBERTa"),this.h()},l(e){const r=Ne('[data-svelte="svelte-1phssyn"]',document.head);h=l(r,"META",{name:!0,content:!0}),r.forEach(t),N=d(e),v=l(e,"H1",{class:!0});var O=o(v);_=l(O,"A",{id:!0,class:!0,href:!0});var me=o(_);x=l(me,"SPAN",{});var de=o(x);xe(L.$$.fragment,de),de.forEach(t),me.forEach(t),z=d(O),P=l(O,"SPAN",{});var fe=o(P);G=f(fe,"Les mod\xE8les encodeurs"),fe.forEach(t),O.forEach(t),U=d(e),xe(q.$$.fragment,e),D=d(e),E=l(e,"P",{});var Q=o(E);K=f(Q,"Les mod\xE8les encodeurs utilisent uniquement l\u2019encodeur d\u2019un mod\xE8le Transformer. \xC0 chaque \xE9tape, les couches d\u2019attention peuvent acc\xE9der \xE0 tous les mots de la phrase initiale. Ces mod\xE8les sont souvent caract\xE9ris\xE9s comme ayant une attention \u201Cbi-directionnelle\u201D et sont souvent appel\xE9s "),B=l(Q,"EM",{});var pe=o(B);V=f(pe,"mod\xE8les d\u2019auto-encodage"),pe.forEach(t),W=f(Q,"."),Q.forEach(t),J=d(e),T=l(e,"P",{});var he=o(T);X=f(he,"Le pr\xE9-entra\xEEnement de ces mod\xE8les se concentre g\xE9n\xE9ralement sur la modification d\u2019une phrase donn\xE9e (par exemple, en masquant des mots al\xE9atoires dans celle-ci) et en demandant au mod\xE8le de trouver ou de reconstruire la phrase initiale."),he.forEach(t),Y=d(e),g=l(e,"P",{});var ve=o(g);Z=f(ve,"Ces mod\xE8les encodeurs sont les plus adapt\xE9s pour des t\xE2ches qui requi\xE8rent une compr\xE9hension compl\xE8te de la phrase, telles que la classification de phrases, la reconnaissance des entit\xE9s nomm\xE9es (et plus g\xE9n\xE9ralement la classification de mots) et les questions-r\xE9ponses extractives."),ve.forEach(t),j=d(e),b=l(e,"P",{});var _e=o(b);ee=f(_e,"Les mod\xE8les les plus repr\xE9sentatifs de cette famille sont:"),_e.forEach(t),F=d(e),i=l(e,"UL",{});var p=o(i);C=l(p,"LI",{});var Ee=o(C);w=l(Ee,"A",{href:!0,rel:!0});var Le=o(w);te=f(Le,"ALBERT"),Le.forEach(t),Ee.forEach(t),se=d(p),I=l(p,"LI",{});var qe=o(I);$=l(qe,"A",{href:!0,rel:!0});var we=o($);ae=f(we,"BERT"),we.forEach(t),qe.forEach(t),le=d(p),S=l(p,"LI",{});var $e=o(S);y=l($e,"A",{href:!0,rel:!0});var ye=o(y);oe=f(ye,"DistilBERT"),ye.forEach(t),$e.forEach(t),re=d(p),k=l(p,"LI",{});var Ae=o(k);A=l(Ae,"A",{href:!0,rel:!0});var Re=o(A);ne=f(Re,"ELECTRA"),Re.forEach(t),Ae.forEach(t),ie=d(p),M=l(p,"LI",{});var Te=o(M);R=l(Te,"A",{href:!0,rel:!0});var ge=o(R);ce=f(ge,"RoBERTa"),ge.forEach(t),Te.forEach(t),p.forEach(t),this.h()},h(){n(h,"name","hf:doc:metadata"),n(h,"content",JSON.stringify(Fe)),n(_,"id","les-modles-encodeurs"),n(_,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),n(_,"href","#les-modles-encodeurs"),n(v,"class","relative group"),n(w,"href","https://huggingface.co/transformers/model_doc/albert.html"),n(w,"rel","nofollow"),n($,"href","https://huggingface.co/transformers/model_doc/bert.html"),n($,"rel","nofollow"),n(y,"href","https://huggingface.co/transformers/model_doc/distilbert.html"),n(y,"rel","nofollow"),n(A,"href","https://huggingface.co/transformers/model_doc/electra.html"),n(A,"rel","nofollow"),n(R,"href","https://huggingface.co/transformers/model_doc/roberta.html"),n(R,"rel","nofollow")},m(e,r){s(document.head,h),c(e,N,r),c(e,v,r),s(v,_),s(_,x),Pe(L,x,null),s(v,z),s(v,P),s(P,G),c(e,U,r),Pe(q,e,r),c(e,D,r),c(e,E,r),s(E,K),s(E,B),s(B,V),s(E,W),c(e,J,r),c(e,T,r),s(T,X),c(e,Y,r),c(e,g,r),s(g,Z),c(e,j,r),c(e,b,r),s(b,ee),c(e,F,r),c(e,i,r),s(i,C),s(C,w),s(w,te),s(i,se),s(i,I),s(I,$),s($,ae),s(i,le),s(i,S),s(S,y),s(y,oe),s(i,re),s(i,k),s(k,A),s(A,ne),s(i,ie),s(i,M),s(M,R),s(R,ce),H=!0},p:Ue,i(e){H||(Be(L.$$.fragment,e),Be(q.$$.fragment,e),H=!0)},o(e){Ce(L.$$.fragment,e),Ce(q.$$.fragment,e),H=!1},d(e){t(h),e&&t(N),e&&t(v),Ie(L),e&&t(U),Ie(q,e),e&&t(D),e&&t(E),e&&t(J),e&&t(T),e&&t(Y),e&&t(g),e&&t(j),e&&t(b),e&&t(F),e&&t(i)}}}const Fe={local:"les-modles-encodeurs",title:"Les mod\xE8les encodeurs"};function He(ue){return De(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Ge extends Se{constructor(h){super();ke(this,h,He,je,Me,{})}}export{Ge as default,Fe as metadata};
