<meta charset="utf-8" /><meta http-equiv="content-security-policy" content=""><meta name="hf:doc:metadata" content="{&quot;local&quot;:&quot;introduction&quot;,&quot;sections&quot;:[{&quot;local&quot;:&quot;bienvenue-sur-le-cours&quot;,&quot;title&quot;:&quot;Bienvenue sur le cours ğŸ¤— !&quot;},{&quot;local&quot;:&quot;quoi-sattendre&quot;,&quot;title&quot;:&quot;Ã€ quoi s&#39;attendre ?&quot;},{&quot;local&quot;:&quot;qui-sommesnous&quot;,&quot;title&quot;:&quot;Qui sommes-nous ?&quot;}],&quot;title&quot;:&quot;Introduction&quot;}" data-svelte="svelte-1phssyn">
	<link rel="stylesheet" href="/docs/course/pr_139/fr/_app/assets/pages/__layout.svelte-4e40f570.css">
	<link rel="modulepreload" href="/docs/course/pr_139/fr/_app/start-bd2fae5d.js">
	<link rel="modulepreload" href="/docs/course/pr_139/fr/_app/chunks/vendor-1e8b365d.js">
	<link rel="modulepreload" href="/docs/course/pr_139/fr/_app/chunks/paths-4b3c6e7e.js">
	<link rel="modulepreload" href="/docs/course/pr_139/fr/_app/pages/__layout.svelte-05c2fb0c.js">
	<link rel="modulepreload" href="/docs/course/pr_139/fr/_app/pages/chapter1/1.mdx-51b3b4dc.js">
	<link rel="modulepreload" href="/docs/course/pr_139/fr/_app/chunks/Youtube-c2a8cc39.js">
	<link rel="modulepreload" href="/docs/course/pr_139/fr/_app/chunks/IconCopyLink-483c28ba.js"> 





<h1 class="relative group"><a id="introduction" class="header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full" href="#introduction"><span><svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 256 256"><path d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z" fill="currentColor"></path></svg></span></a>
	<span>Introduction
	</span></h1>

<h2 class="relative group"><a id="bienvenue-sur-le-cours" class="header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full" href="#bienvenue-sur-le-cours"><span><svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 256 256"><path d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z" fill="currentColor"></path></svg></span></a>
	<span>Bienvenue sur le cours ğŸ¤— !
	</span></h2>

<iframe class="w-full xl:w-4/6 h-80" src="https://www.youtube-nocookie.com/embed/00GKzGyWFEs" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
<p>Ce cours vous apprendra Ã  utiliser les librairies de NLP de lâ€™Ã©cosystÃ¨me <a href="https://huggingface.co/" rel="nofollow">Hugging Face</a> â€” <a href="https://github.com/huggingface/transformers" rel="nofollow">ğŸ¤— Transformers</a>, <a href="https://github.com/huggingface/datasets" rel="nofollow">ğŸ¤— Datasets</a>, <a href="https://github.com/huggingface/tokenizers" rel="nofollow">ğŸ¤— Tokenizers</a>, et <a href="https://github.com/huggingface/accelerate" rel="nofollow">ğŸ¤— Accelerate</a> â€” ainsi que le <a href="https://huggingface.co/models" rel="nofollow">Hub de Hugging Face</a>. Câ€™est totalement gratuit et sans publicitÃ©.</p>
<h2 class="relative group"><a id="quoi-sattendre" class="header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full" href="#quoi-sattendre"><span><svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 256 256"><path d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z" fill="currentColor"></path></svg></span></a>
	<span>Ã€ quoi s&#39;attendre ?
	</span></h2>

<p>Voici un bref aperÃ§u du cours:</p>
<div class="flex justify-center"><img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/summary.svg" alt="Bref aperÃ§u du contenu du cours.">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/summary-dark.svg" alt="Bref aperÃ§u des diffÃ©rents chapitres du cours."></div>
<ul><li>Les chapitres 1 Ã  4 prÃ©sentent les principaux concepts de la librairie ğŸ¤— Transformers. Ã€ la fin de ce chapitre, vous serez familier du fonctionnement des modÃ¨les Transformers et vous saurez comment utiliser un modÃ¨le du <a href="https://huggingface.co/models" rel="nofollow">Hub de Hugging Face</a>, le finetuner sur un jeu de donnÃ©es, et partager vos rÃ©sultats sur le Hub!</li>
<li>Les chapitres 5 Ã  8 prÃ©sentent les bases de ğŸ¤— Datasets et ğŸ¤— Tokenizers ainsi quâ€™une dÃ©couverte des problÃ¨mes classiques de NLP. Ã€ la fin de ce chapitre, vous serez capable de rÃ©soudre les problÃ¨mes de NLP les plus communs par vous-mÃªme.</li>
<li>Les chapitres 9 Ã  12 proposent dâ€™aller plus loin et dâ€™explorer comment les modÃ¨les Transformers peuvent Ãªtre utilisÃ©s pour rÃ©soudre des problÃ¨mes de traitement de la parole et de la vision par ordinateur. En suivant ces chapitres, vous apprendrez Ã  construire et Ã  partager vos modÃ¨les via des dÃ©monstrateurs, et vous serez capable dâ€™optimiser ces modÃ¨les pour les environnements de production. Enfin, vous serez prÃªt Ã  appliquer ğŸ¤— Transformers Ã  (presque) nâ€™importe quel problÃ¨me de machine learning!</li></ul>
<p>Ce cours:</p>
<ul><li>Requiert un bon niveau en Python</li>
<li>Se comprend mieux si vous avez dÃ©jÃ  suivi un cours dâ€™introduction au deep learning, comme <a href="https://www.fast.ai/" rel="nofollow">fast.aiâ€™s</a> <a href="https://course.fast.ai/" rel="nofollow">Practical Deep Learning for Coders</a> ou un des cours dÃ©veloppÃ©s par <a href="https://www.deeplearning.ai/" rel="nofollow">DeepLearning.AI</a></li>
<li>Nâ€™attend pas une connaissance appronfondie de <a href="https://pytorch.org/" rel="nofollow">PyTorch</a> ou de <a href="https://www.tensorflow.org/" rel="nofollow">TensorFlow</a>, bien quâ€™Ãªtre familiarisÃ© avec lâ€™un dâ€™entre eux peut aider</li></ul>
<p>AprÃ¨s avoir terminÃ© ce cours, nous vous recommandons de suivre la <a href="https://www.coursera.org/specializations/natural-language-processing?utm_source=deeplearning-ai&utm_medium=institutions&utm_campaign=20211011-nlp-2-hugging_face-page-nlp-refresh" rel="nofollow">SpÃ©cialisation en NLP</a> dispensÃ©e par DeepLearning.AI, qui couvre une grande gamme de modÃ¨les traditionnels de NLP comme les NaÃ¯ves Bayes et les LSTMs qui sont importants Ã  connaÃ®tre!</p>
<h2 class="relative group"><a id="qui-sommesnous" class="header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full" href="#qui-sommesnous"><span><svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 256 256"><path d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z" fill="currentColor"></path></svg></span></a>
	<span>Qui sommes-nous ?
	</span></h2>

<p>Ã€ propos des auteurs de ce cours:</p>
<p><strong>Matthew Carrigan</strong> est Machine Learning Engineer chez Hugging Face. Il vit Ã  Dublin, en Irlande et Ã  travaillÃ© auparavant comme ingÃ©nieur en ML chez Parse.ly et avant cela comme chercheur postdoctoral Ã  Trinity College Dublin. Il ne croit pas que nous arrivions Ã  AGI en mettant Ã  lâ€™Ã©chelle les architectures existantes, mais il a tout de mÃªme beaucoup dâ€™espoir dans lâ€™immortalitÃ© des robots.</p>
<p><strong>Lysandre Debut</strong> est Machine Learning Engineer chez Hugging Face et a travaillÃ© sur la librairie ğŸ¤— Transformers depuis les premiÃ¨res phases de dÃ©veloppement. Son but est de rendre NLP accessible pour tout le monde en dÃ©veloppant des outils disposant dâ€™une API trÃ¨s simple.</p>
<p><strong>Sylvain Gugger</strong> est Research Engineer chez Hugging Face et un des principaux responsable de la librairie ğŸ¤— Transformers. PrÃ©cÃ©demment, Il a Ã©tÃ© chercheur en ML chez fast.ai et a Ã©crit <em><a href="https://learning.oreilly.com/library/view/deep-learning-for/9781492045519/" rel="nofollow">Deep Learning for Coders with fastai and PyTorch</a></em> avec Jeremy Howard. Son but ultime est de rendre le deep learning plus accessible, en dÃ©veloppant et en amÃ©liorant des techniques permettant aux modÃ¨les dâ€™apprendre rapidement sur des ressources limitÃ©es.</p>
<p><strong>Merve Noyan</strong> est Developer Advocate chez Hugging Face et travaille Ã  la crÃ©ation dâ€™outils et de contenu visant Ã  dÃ©mocratiser le machine learning pour tous.</p>
<p><strong>Lucile Saulnier</strong> est Machine Learning Engineer chez Hugging Face qui travaille au dÃ©veloppement et Ã  lâ€™implÃ©mentation de nombreux outils open source. Elle est Ã©galement activement impliquÃ©e dans de nombreux projets de recherche dans le domaine de NLP comme lâ€™entraÃ®nement collaboratif de modÃ¨les et le projet BigScience.</p>
<p><strong>Lewis Tunstall</strong> est Machine Learning Engineer chez Hugging Face dÃ©vouÃ© au dÃ©veloppement dâ€™outils open source avec la volontÃ© de les rendre accessibles Ã  une communautÃ© plus large. Il est Ã©galement co-auteur dâ€™un livre qui va bientÃ´t paraÃ®tre, <a href="https://www.oreilly.com/library/view/natural-language-processing/9781098103231/" rel="nofollow">Natural Language Processing with Transformers</a>.</p>
<p><strong>Leandro von Werra</strong> est Machine Learning Engineer dans lâ€™Ã©quipe open-source chez Hugging Face et Ã©galement co-auteur du livre qui va bientÃ´t paraÃ®tre, <a href="https://www.oreilly.com/library/view/natural-language-processing/9781098103231/" rel="nofollow">Natural Language Processing with Transformers</a>. Il a plusieurs annÃ©es dâ€™expÃ©rience dans lâ€™industrie du machine learning, oÃ¹ il a pu dÃ©ployer des projets de NLP en production en travaillant sur toutes les Ã©tapes clefs du dÃ©ploiement.</p>
<p>ÃŠtes-vous prÃªt Ã  commencer ? Dans ce chapitre, vous apprendrez:</p>
<ul><li>Ã€ utiliser la fonction <code>pipeline()</code> pour rÃ©soudre des problÃ¨mes de NLP comme la gÃ©nÃ©ration de texte et la classification</li>
<li>Quelle est lâ€™architecture dâ€™un modÃ¨le Transformer</li>
<li>Comment faire la distinction entre les diffÃ©rentes architectures dâ€™encodeur, de dÃ©codeur et dâ€™encodeur-dÃ©codeur et leurs condition dâ€™utilisation</li></ul>


		<script type="module" data-hydrate="m99xyv">
		import { start } from "/docs/course/pr_139/fr/_app/start-bd2fae5d.js";
		start({
			target: document.querySelector('[data-hydrate="m99xyv"]').parentNode,
			paths: {"base":"/docs/course/pr_139/fr","assets":"/docs/course/pr_139/fr"},
			session: {},
			route: false,
			spa: false,
			trailing_slash: "never",
			hydrate: {
				status: 200,
				error: null,
				nodes: [
					import("/docs/course/pr_139/fr/_app/pages/__layout.svelte-05c2fb0c.js"),
						import("/docs/course/pr_139/fr/_app/pages/chapter1/1.mdx-51b3b4dc.js")
				],
				params: {}
			}
		});
	</script>
