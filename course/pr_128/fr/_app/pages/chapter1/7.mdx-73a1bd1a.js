import{S as Pe,i as Te,s as Ie,e as l,k as i,w as $e,t as d,M as Se,c as o,d as t,m,a,x as we,h as p,b as r,F as s,g as u,y as be,L as ke,q as ge,o as Ae,B as ye,v as Be}from"../../chunks/vendor-1e8b365d.js";import{Y as Me}from"../../chunks/Youtube-c2a8cc39.js";import{I as Re}from"../../chunks/IconCopyLink-483c28ba.js";function je(ne){let f,R,h,q,T,x,K,I,O,j,E,N,A,D,z,_,G,L,Q,V,C,y,W,U,P,X,Y,c,S,$,Z,ee,k,w,te,se,B,b,le,oe,M,g,ae,F;return x=new Re({}),E=new Me({props:{id:"0_4KEb08xrE"}}),{c(){f=l("meta"),R=i(),h=l("h1"),q=l("a"),T=l("span"),$e(x.$$.fragment),K=i(),I=l("span"),O=d("Les mod\xE8les de s\xE9quence-\xE0-s\xE9quence"),j=i(),$e(E.$$.fragment),N=i(),A=l("p"),D=d("Les mod\xE8les encodeur-d\xE9codeur (\xE9galement appel\xE9s mod\xE8les de s\xE9quence-\xE0-s\xE9quence) utilisent les deux parties de l\u2019architecture Transformer. \xC0 chaque \xE9tape, les couches d\u2019attention de l\u2019encodeur peuvent acc\xE9der \xE0 tous les mots de la phrase initiale, tandis que les couches d\u2019attention du d\xE9codeur n\u2019ont acc\xE8s qu\u2019aux mots positionn\xE9s avant un mot donn\xE9 en entr\xE9e de ces couches d\u2019attention."),z=i(),_=l("p"),G=d("Le pr\xE9-entra\xEEnement de ces mod\xE8les peut \xEAtre fait en utilisant les objectifs des mod\xE8les d\u2019encodeur ou de d\xE9codeur, mais en g\xE9n\xE9ral cela implique quelque chose de plus complexe. Par exemple, le mod\xE8le "),L=l("a"),Q=d("T5"),V=d(" est pr\xE9-entra\xEEn\xE9 en rempla\xE7ant des zones al\xE9atoires de texte (qui peuvent contenir plusieurs mots) par un mot-masque sp\xE9cial, et l\u2019objectif est alors de pr\xE9dire le texte que ce mot-masque remplace."),C=i(),y=l("p"),W=d("Les mod\xE8les de s\xE9quence-\xE0-s\xE9quence sont les plus adapt\xE9s pour les t\xE2ches li\xE9es \xE0 la g\xE9n\xE9ration de nouvelles phrases en fonction d\u2019une entr\xE9e donn\xE9e, comme le r\xE9sum\xE9 de texte, la traduction ou la g\xE9n\xE9ration de question-r\xE9ponse."),U=i(),P=l("p"),X=d("Les mod\xE8les qui repr\xE9sentent le mieux cette famille sont:"),Y=i(),c=l("ul"),S=l("li"),$=l("a"),Z=d("BART"),ee=i(),k=l("li"),w=l("a"),te=d("mBART"),se=i(),B=l("li"),b=l("a"),le=d("Marian"),oe=i(),M=l("li"),g=l("a"),ae=d("T5"),this.h()},l(e){const n=Se('[data-svelte="svelte-1phssyn"]',document.head);f=o(n,"META",{name:!0,content:!0}),n.forEach(t),R=m(e),h=o(e,"H1",{class:!0});var H=a(h);q=o(H,"A",{id:!0,class:!0,href:!0});var re=a(q);T=o(re,"SPAN",{});var ue=a(T);we(x.$$.fragment,ue),ue.forEach(t),re.forEach(t),K=m(H),I=o(H,"SPAN",{});var ce=a(I);O=p(ce,"Les mod\xE8les de s\xE9quence-\xE0-s\xE9quence"),ce.forEach(t),H.forEach(t),j=m(e),we(E.$$.fragment,e),N=m(e),A=o(e,"P",{});var ie=a(A);D=p(ie,"Les mod\xE8les encodeur-d\xE9codeur (\xE9galement appel\xE9s mod\xE8les de s\xE9quence-\xE0-s\xE9quence) utilisent les deux parties de l\u2019architecture Transformer. \xC0 chaque \xE9tape, les couches d\u2019attention de l\u2019encodeur peuvent acc\xE9der \xE0 tous les mots de la phrase initiale, tandis que les couches d\u2019attention du d\xE9codeur n\u2019ont acc\xE8s qu\u2019aux mots positionn\xE9s avant un mot donn\xE9 en entr\xE9e de ces couches d\u2019attention."),ie.forEach(t),z=m(e),_=o(e,"P",{});var J=a(_);G=p(J,"Le pr\xE9-entra\xEEnement de ces mod\xE8les peut \xEAtre fait en utilisant les objectifs des mod\xE8les d\u2019encodeur ou de d\xE9codeur, mais en g\xE9n\xE9ral cela implique quelque chose de plus complexe. Par exemple, le mod\xE8le "),L=o(J,"A",{href:!0,rel:!0});var de=a(L);Q=p(de,"T5"),de.forEach(t),V=p(J," est pr\xE9-entra\xEEn\xE9 en rempla\xE7ant des zones al\xE9atoires de texte (qui peuvent contenir plusieurs mots) par un mot-masque sp\xE9cial, et l\u2019objectif est alors de pr\xE9dire le texte que ce mot-masque remplace."),J.forEach(t),C=m(e),y=o(e,"P",{});var me=a(y);W=p(me,"Les mod\xE8les de s\xE9quence-\xE0-s\xE9quence sont les plus adapt\xE9s pour les t\xE2ches li\xE9es \xE0 la g\xE9n\xE9ration de nouvelles phrases en fonction d\u2019une entr\xE9e donn\xE9e, comme le r\xE9sum\xE9 de texte, la traduction ou la g\xE9n\xE9ration de question-r\xE9ponse."),me.forEach(t),U=m(e),P=o(e,"P",{});var pe=a(P);X=p(pe,"Les mod\xE8les qui repr\xE9sentent le mieux cette famille sont:"),pe.forEach(t),Y=m(e),c=o(e,"UL",{});var v=a(c);S=o(v,"LI",{});var fe=a(S);$=o(fe,"A",{href:!0,rel:!0});var he=a($);Z=p(he,"BART"),he.forEach(t),fe.forEach(t),ee=m(v),k=o(v,"LI",{});var qe=a(k);w=o(qe,"A",{href:!0,rel:!0});var _e=a(w);te=p(_e,"mBART"),_e.forEach(t),qe.forEach(t),se=m(v),B=o(v,"LI",{});var ve=a(B);b=o(ve,"A",{href:!0,rel:!0});var xe=a(b);le=p(xe,"Marian"),xe.forEach(t),ve.forEach(t),oe=m(v),M=o(v,"LI",{});var Ee=a(M);g=o(Ee,"A",{href:!0,rel:!0});var Le=a(g);ae=p(Le,"T5"),Le.forEach(t),Ee.forEach(t),v.forEach(t),this.h()},h(){r(f,"name","hf:doc:metadata"),r(f,"content",JSON.stringify(Ne)),r(q,"id","les-modles-de-squencesquence"),r(q,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(q,"href","#les-modles-de-squencesquence"),r(h,"class","relative group"),r(L,"href","https://huggingface.co/t5-base"),r(L,"rel","nofollow"),r($,"href","https://huggingface.co/transformers/model_doc/bart.html"),r($,"rel","nofollow"),r(w,"href","https://huggingface.co/transformers/model_doc/mbart.html"),r(w,"rel","nofollow"),r(b,"href","https://huggingface.co/transformers/model_doc/marian.html"),r(b,"rel","nofollow"),r(g,"href","https://huggingface.co/transformers/model_doc/t5.html"),r(g,"rel","nofollow")},m(e,n){s(document.head,f),u(e,R,n),u(e,h,n),s(h,q),s(q,T),be(x,T,null),s(h,K),s(h,I),s(I,O),u(e,j,n),be(E,e,n),u(e,N,n),u(e,A,n),s(A,D),u(e,z,n),u(e,_,n),s(_,G),s(_,L),s(L,Q),s(_,V),u(e,C,n),u(e,y,n),s(y,W),u(e,U,n),u(e,P,n),s(P,X),u(e,Y,n),u(e,c,n),s(c,S),s(S,$),s($,Z),s(c,ee),s(c,k),s(k,w),s(w,te),s(c,se),s(c,B),s(B,b),s(b,le),s(c,oe),s(c,M),s(M,g),s(g,ae),F=!0},p:ke,i(e){F||(ge(x.$$.fragment,e),ge(E.$$.fragment,e),F=!0)},o(e){Ae(x.$$.fragment,e),Ae(E.$$.fragment,e),F=!1},d(e){t(f),e&&t(R),e&&t(h),ye(x),e&&t(j),ye(E,e),e&&t(N),e&&t(A),e&&t(z),e&&t(_),e&&t(C),e&&t(y),e&&t(U),e&&t(P),e&&t(Y),e&&t(c)}}}const Ne={local:"les-modles-de-squencesquence",title:"Les mod\xE8les de s\xE9quence-\xE0-s\xE9quence"};function ze(ne){return Be(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Fe extends Pe{constructor(f){super();Te(this,f,ze,je,Ie,{})}}export{Fe as default,Ne as metadata};
