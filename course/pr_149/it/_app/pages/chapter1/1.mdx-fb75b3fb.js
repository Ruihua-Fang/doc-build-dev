import{S as zt,i as wt,s as Et,e as o,c as l,a as n,d as i,b as s,T as Fa,g as u,L as wr,k as p,w as Xe,t as r,R as bt,m as d,x as Ze,h as t,F as a,y as ea,q as aa,o as ia,B as ra,v as Lt}from"../../chunks/vendor-f4a867ed.js";import{I as ka}from"../../chunks/IconCopyLink-d27af064.js";function yt(D){let f,z;return{c(){f=o("iframe"),this.h()},l(m){f=l(m,"IFRAME",{class:!0,src:!0,title:!0,frameborder:!0,allow:!0}),n(f).forEach(i),this.h()},h(){s(f,"class","w-full xl:w-4/6 h-80"),Fa(f.src,z="https://www.youtube-nocookie.com/embed/"+D[0])||s(f,"src",z),s(f,"title","YouTube video player"),s(f,"frameborder","0"),s(f,"allow","accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"),f.allowFullscreen=!0},m(m,v){u(m,f,v)},p(m,[v]){v&1&&!Fa(f.src,z="https://www.youtube-nocookie.com/embed/"+m[0])&&s(f,"src",z)},i:wr,o:wr,d(m){m&&i(f)}}}function Pt(D,f,z){let{id:m}=f;return D.$$set=v=>{"id"in v&&z(0,m=v.id)},[m]}class At extends zt{constructor(f){super();wt(this,f,Pt,yt,Et,{id:0})}}function Tt(D){let f,z,m,v,ye,R,Ma,Pe,qa,ta,P,F,Ae,C,Da,Te,Ra,oa,x,la,h,Ca,Ne,xa,Ga,G,Oa,Ba,O,Qa,Ua,B,Ja,Va,Q,Wa,Ya,U,ja,Ka,J,Xa,Za,na,A,M,$e,V,ei,Ie,ai,sa,me,ii,ca,T,W,Er,ri,Y,br,ua,E,j,ti,K,oi,li,ni,Se,si,ci,N,ui,He,pi,di,ke,fi,mi,pa,he,hi,da,b,Fe,gi,vi,_,_i,Me,zi,wi,X,Ei,bi,Z,Li,yi,ee,Pi,Ai,$,Ti,ae,Ni,$i,ie,Ii,Si,fa,L,Hi,re,ki,Fi,qe,Mi,qi,ma,I,q,De,te,Di,Re,Ri,ha,ge,Ci,ga,oe,Ce,xi,Gi,va,le,xe,Oi,Bi,_a,w,Ge,Qi,Ui,ne,Ji,Vi,Oe,Wi,Yi,za,se,Be,ji,Ki,wa,ce,Qe,Xi,Zi,Ea,S,Ue,er,ar,ue,ir,rr,ba,H,Je,tr,or,pe,lr,nr,La,ve,sr,ya,y,de,cr,Ve,ur,pr,dr,We,fr,mr,Ye,hr,Pa;return R=new ka({}),C=new ka({}),x=new At({props:{id:"00GKzGyWFEs"}}),V=new ka({}),te=new ka({}),{c(){f=o("meta"),z=p(),m=o("h1"),v=o("a"),ye=o("span"),Xe(R.$$.fragment),Ma=p(),Pe=o("span"),qa=r("Introduzione"),ta=p(),P=o("h2"),F=o("a"),Ae=o("span"),Xe(C.$$.fragment),Da=p(),Te=o("span"),Ra=r("Benvenuto/a al corso di \u{1F917}!"),oa=p(),Xe(x.$$.fragment),la=p(),h=o("p"),Ca=r("Questo corso ti insegner\xE0 a eseguire compiti di Natural Language Processing (NLP, "),Ne=o("em"),xa=r("elaborazione del linguaggio naturale"),Ga=r(") utilizzando le librerie dell\u2019ecosistema di "),G=o("a"),Oa=r("Hugging Face"),Ba=r(": "),O=o("a"),Qa=r("\u{1F917} Transformers"),Ua=r(", "),B=o("a"),Ja=r("\u{1F917} Datasets"),Va=r(", "),Q=o("a"),Wa=r("\u{1F917} Tokenizers"),Ya=r(", e "),U=o("a"),ja=r("\u{1F917} Accelerate"),Ka=r(". Ti insegneremo anche ad usare il nostro "),J=o("a"),Xa=r("Hugging Face Hub"),Za=r(", che \xE8 completamente gratuito e senza pubblicit\xE0."),na=p(),A=o("h2"),M=o("a"),$e=o("span"),Xe(V.$$.fragment),ei=p(),Ie=o("span"),ai=r("Contenuti"),sa=p(),me=o("p"),ii=r("Eccoti un breve riassunto dei contenuti del corso:"),ca=p(),T=o("div"),W=o("img"),ri=p(),Y=o("img"),ua=p(),E=o("ul"),j=o("li"),ti=r("I capitoli da 1 a 4 forniscono un\u2019introduzione ai concetti principali della libreria \u{1F917} Transformers. Alla fine di questa parte del corso, conoscerai come funzionano i modelli Transformers e saprai come utilizzare un modello dell\u2019"),K=o("a"),oi=r("Hugging Face Hub"),li=r(", affinarlo in un dataset, e condividere i tuoi risultati nell\u2019Hub!"),ni=p(),Se=o("li"),si=r("I capitoli da 5 a 8 insegnano le basi degli \u{1F917} Dataset e degli \u{1F917} Tokenizer, per poi esplorare alcuni compiti classici di NLP. Alla fine di questa parte, saprai far fronte ai problemi di NLP pi\xF9 comuni in maniera autonoma."),ci=p(),N=o("li"),ui=r("I capitoli da 9 a 12 vanno oltre il Natural Language Processing, ed esplorano come i modelli Transformer possano essere utilizzati per affrontare compiti di elaborazione vocale o visione artificiale. Strada facendo, imparerai a costruire e condividere demo ("),He=o("em"),pi=r("dimostrazioni"),di=r(") dei tuoi modelli, e ad ottimizzarli per la produzione. Alla fine di questa parte, sarai pronto ad utilizzare gli \u{1F917} Transformer per qualsiasi problema di machine learning ("),ke=o("em"),fi=r("apprendimento automatico"),mi=r("), o quasi!"),pa=p(),he=o("p"),hi=r("Questo corso:"),da=p(),b=o("ul"),Fe=o("li"),gi=r("Richiede una buona conoscenza di Python"),vi=p(),_=o("li"),_i=r("Andrebbe seguito di preferenza a seguito di un corso introduttivo di deep learning ("),Me=o("em"),zi=r("apprendimento profondo"),wi=r("), come ad esempio il "),X=o("a"),Ei=r("Practical Deep Learning for Coders"),bi=r(" di "),Z=o("a"),Li=r("fast.ai"),yi=r(", oppure uno dei programmi sviluppati da "),ee=o("a"),Pi=r("DeepLearning.AI"),Ai=p(),$=o("li"),Ti=r("Non richiede conoscenze pregresse di "),ae=o("a"),Ni=r("PyTorch"),$i=r(" o "),ie=o("a"),Ii=r("TensorFlow"),Si=r(", nonostante sia gradita una conoscenza anche superficiale dell\u2019uno o dell\u2019altro"),fa=p(),L=o("p"),Hi=r("Quando avrai completato questo corso, ti raccomandiamo di passare al "),re=o("a"),ki=r("Natural Language Processing Specialization"),Fi=r(" di DeepLearning.AI, un corso che copre un ampio spettro di modelli tradizionali di NLP che vale davvero la pena di conoscere, come Naive Bayes e LSTM ("),qe=o("em"),Mi=r("Memoria a breve termine a lungo termine"),qi=r(")!"),ma=p(),I=o("h2"),q=o("a"),De=o("span"),Xe(te.$$.fragment),Di=p(),Re=o("span"),Ri=r("Chi siamo?"),ha=p(),ge=o("p"),Ci=r("A proposito degli autori:"),ga=p(),oe=o("p"),Ce=o("strong"),xi=r("Matthew Carrigan"),Gi=r(" \xE8 Machine Learning Engineer da Hugging Face. Vive a Dublino, in Irlanda, ed in passato \xE8 stato ML engineer da Parse.ly, e prima ancora ricercatore postdottorale al Trinity College di Dublin. Nonostante non creda che otterremo l\u2019Intelligenza artificiale forte semplicemente ingrandendo le architetture a nostra disposizione, spera comunque nell\u2019immortalit\xE0 cibernetica."),va=p(),le=o("p"),xe=o("strong"),Oi=r("Lysandre Debut"),Bi=r(" \xE8 Machine Learning Engineer da Hugging Face e ha lavorato agli \u{1F917} Transformer fin dalle primissime tappe del loro sviluppo. Il suo obiettivo \xE8 di rendere il NLP accessibile a tutti sviluppando strumenti con un semplice API."),_a=p(),w=o("p"),Ge=o("strong"),Qi=r("Sylvain Gugger"),Ui=r(" \xE8 Research Engineer da Hugging Face e uno dei principali manutentori della libreria \u{1F917} Transformers. In passato, \xE8 stato Research Scientist da fast.ai, e ha scritto "),ne=o("a"),Ji=r("Deep Learning for Coders with fastai and PyTorch"),Vi=r(" con Jeremy Howard. Il centro principale della sua ricerca consiste nel rendere il deep learning ("),Oe=o("em"),Wi=r("apprendimento profondo"),Yi=r(") pi\xF9 accessibile, concependo e migliorando tecniche che permettano di allenare modelli velocemente con risorse limitate."),za=p(),se=o("p"),Be=o("strong"),ji=r("Merve Noyan"),Ki=r(" \xE8 developer advocate da Hugging Face, e lavora allo sviluppo di strumenti e alla creazione di contenuti ad essi legati per democratizzare l\u2019accesso al deep learning."),wa=p(),ce=o("p"),Qe=o("strong"),Xi=r("Lucile Saulnier"),Zi=r(" \xE8 machine learning engineer da Hugging Face, e sviluppa e supporta l\u2019utilizzo di strumenti open source. \xC8 anche attivamente coinvolta in numerosi progetti di ricerca nell\u2019ambito del NLP, come ad esempio collaborative training e BigScience."),Ea=p(),S=o("p"),Ue=o("strong"),er=r("Lewis Tunstall"),ar=r(" \xE8 machine learning engineer da Hugging Face che si specializza nello sviluppo di strumenti open-source e la loro distribuzione alla comunit\xE0 pi\xF9 ampia. \xC8 anche co-autore dell\u2019imminente "),ue=o("a"),ir=r("O\u2019Reilly book on Transformers"),rr=r("."),ba=p(),H=o("p"),Je=o("strong"),tr=r("Leandro von Werra"),or=r(" \xE8 machine learning engineer nel team open-source di Hugging Face, nonch\xE9 co-autore dell\u2019imminente "),pe=o("a"),lr=r("O\u2019Reilly book on Transformers"),nr=r(". Ha tanti anni di esperienza nel portare progetti di NLP in produzione, lavorando a tutti i livelli di esecuzione di compiti di machine learning."),La=p(),ve=o("p"),sr=r("Sei pronto/a a iniziare? In questo capitolo, imparerai:"),ya=p(),y=o("ul"),de=o("li"),cr=r("Ad utilizzare la funzione "),Ve=o("code"),ur=r("pipeline()"),pr=r(" per eseguire compiti di NLP come la generazione e classificazione di testi"),dr=p(),We=o("li"),fr=r("L\u2019architettura dei Transformer"),mr=p(),Ye=o("li"),hr=r("Come fare la distinzione tra architetture encoder, decoder, encoder-decoder, e casi d\u2019uso"),this.h()},l(e){const c=bt('[data-svelte="svelte-1phssyn"]',document.head);f=l(c,"META",{name:!0,content:!0}),c.forEach(i),z=d(e),m=l(e,"H1",{class:!0});var Aa=n(m);v=l(Aa,"A",{id:!0,class:!0,href:!0});var Lr=n(v);ye=l(Lr,"SPAN",{});var yr=n(ye);Ze(R.$$.fragment,yr),yr.forEach(i),Lr.forEach(i),Ma=d(Aa),Pe=l(Aa,"SPAN",{});var Pr=n(Pe);qa=t(Pr,"Introduzione"),Pr.forEach(i),Aa.forEach(i),ta=d(e),P=l(e,"H2",{class:!0});var Ta=n(P);F=l(Ta,"A",{id:!0,class:!0,href:!0});var Ar=n(F);Ae=l(Ar,"SPAN",{});var Tr=n(Ae);Ze(C.$$.fragment,Tr),Tr.forEach(i),Ar.forEach(i),Da=d(Ta),Te=l(Ta,"SPAN",{});var Nr=n(Te);Ra=t(Nr,"Benvenuto/a al corso di \u{1F917}!"),Nr.forEach(i),Ta.forEach(i),oa=d(e),Ze(x.$$.fragment,e),la=d(e),h=l(e,"P",{});var g=n(h);Ca=t(g,"Questo corso ti insegner\xE0 a eseguire compiti di Natural Language Processing (NLP, "),Ne=l(g,"EM",{});var $r=n(Ne);xa=t($r,"elaborazione del linguaggio naturale"),$r.forEach(i),Ga=t(g,") utilizzando le librerie dell\u2019ecosistema di "),G=l(g,"A",{href:!0,rel:!0});var Ir=n(G);Oa=t(Ir,"Hugging Face"),Ir.forEach(i),Ba=t(g,": "),O=l(g,"A",{href:!0,rel:!0});var Sr=n(O);Qa=t(Sr,"\u{1F917} Transformers"),Sr.forEach(i),Ua=t(g,", "),B=l(g,"A",{href:!0,rel:!0});var Hr=n(B);Ja=t(Hr,"\u{1F917} Datasets"),Hr.forEach(i),Va=t(g,", "),Q=l(g,"A",{href:!0,rel:!0});var kr=n(Q);Wa=t(kr,"\u{1F917} Tokenizers"),kr.forEach(i),Ya=t(g,", e "),U=l(g,"A",{href:!0,rel:!0});var Fr=n(U);ja=t(Fr,"\u{1F917} Accelerate"),Fr.forEach(i),Ka=t(g,". Ti insegneremo anche ad usare il nostro "),J=l(g,"A",{href:!0,rel:!0});var Mr=n(J);Xa=t(Mr,"Hugging Face Hub"),Mr.forEach(i),Za=t(g,", che \xE8 completamente gratuito e senza pubblicit\xE0."),g.forEach(i),na=d(e),A=l(e,"H2",{class:!0});var Na=n(A);M=l(Na,"A",{id:!0,class:!0,href:!0});var qr=n(M);$e=l(qr,"SPAN",{});var Dr=n($e);Ze(V.$$.fragment,Dr),Dr.forEach(i),qr.forEach(i),ei=d(Na),Ie=l(Na,"SPAN",{});var Rr=n(Ie);ai=t(Rr,"Contenuti"),Rr.forEach(i),Na.forEach(i),sa=d(e),me=l(e,"P",{});var Cr=n(me);ii=t(Cr,"Eccoti un breve riassunto dei contenuti del corso:"),Cr.forEach(i),ca=d(e),T=l(e,"DIV",{class:!0});var $a=n(T);W=l($a,"IMG",{class:!0,src:!0,alt:!0}),ri=d($a),Y=l($a,"IMG",{class:!0,src:!0,alt:!0}),$a.forEach(i),ua=d(e),E=l(e,"UL",{});var _e=n(E);j=l(_e,"LI",{});var Ia=n(j);ti=t(Ia,"I capitoli da 1 a 4 forniscono un\u2019introduzione ai concetti principali della libreria \u{1F917} Transformers. Alla fine di questa parte del corso, conoscerai come funzionano i modelli Transformers e saprai come utilizzare un modello dell\u2019"),K=l(Ia,"A",{href:!0,rel:!0});var xr=n(K);oi=t(xr,"Hugging Face Hub"),xr.forEach(i),li=t(Ia,", affinarlo in un dataset, e condividere i tuoi risultati nell\u2019Hub!"),Ia.forEach(i),ni=d(_e),Se=l(_e,"LI",{});var Gr=n(Se);si=t(Gr,"I capitoli da 5 a 8 insegnano le basi degli \u{1F917} Dataset e degli \u{1F917} Tokenizer, per poi esplorare alcuni compiti classici di NLP. Alla fine di questa parte, saprai far fronte ai problemi di NLP pi\xF9 comuni in maniera autonoma."),Gr.forEach(i),ci=d(_e),N=l(_e,"LI",{});var ze=n(N);ui=t(ze,"I capitoli da 9 a 12 vanno oltre il Natural Language Processing, ed esplorano come i modelli Transformer possano essere utilizzati per affrontare compiti di elaborazione vocale o visione artificiale. Strada facendo, imparerai a costruire e condividere demo ("),He=l(ze,"EM",{});var Or=n(He);pi=t(Or,"dimostrazioni"),Or.forEach(i),di=t(ze,") dei tuoi modelli, e ad ottimizzarli per la produzione. Alla fine di questa parte, sarai pronto ad utilizzare gli \u{1F917} Transformer per qualsiasi problema di machine learning ("),ke=l(ze,"EM",{});var Br=n(ke);fi=t(Br,"apprendimento automatico"),Br.forEach(i),mi=t(ze,"), o quasi!"),ze.forEach(i),_e.forEach(i),pa=d(e),he=l(e,"P",{});var Qr=n(he);hi=t(Qr,"Questo corso:"),Qr.forEach(i),da=d(e),b=l(e,"UL",{});var we=n(b);Fe=l(we,"LI",{});var Ur=n(Fe);gi=t(Ur,"Richiede una buona conoscenza di Python"),Ur.forEach(i),vi=d(we),_=l(we,"LI",{});var k=n(_);_i=t(k,"Andrebbe seguito di preferenza a seguito di un corso introduttivo di deep learning ("),Me=l(k,"EM",{});var Jr=n(Me);zi=t(Jr,"apprendimento profondo"),Jr.forEach(i),wi=t(k,"), come ad esempio il "),X=l(k,"A",{href:!0,rel:!0});var Vr=n(X);Ei=t(Vr,"Practical Deep Learning for Coders"),Vr.forEach(i),bi=t(k," di "),Z=l(k,"A",{href:!0,rel:!0});var Wr=n(Z);Li=t(Wr,"fast.ai"),Wr.forEach(i),yi=t(k,", oppure uno dei programmi sviluppati da "),ee=l(k,"A",{href:!0,rel:!0});var Yr=n(ee);Pi=t(Yr,"DeepLearning.AI"),Yr.forEach(i),k.forEach(i),Ai=d(we),$=l(we,"LI",{});var Ee=n($);Ti=t(Ee,"Non richiede conoscenze pregresse di "),ae=l(Ee,"A",{href:!0,rel:!0});var jr=n(ae);Ni=t(jr,"PyTorch"),jr.forEach(i),$i=t(Ee," o "),ie=l(Ee,"A",{href:!0,rel:!0});var Kr=n(ie);Ii=t(Kr,"TensorFlow"),Kr.forEach(i),Si=t(Ee,", nonostante sia gradita una conoscenza anche superficiale dell\u2019uno o dell\u2019altro"),Ee.forEach(i),we.forEach(i),fa=d(e),L=l(e,"P",{});var be=n(L);Hi=t(be,"Quando avrai completato questo corso, ti raccomandiamo di passare al "),re=l(be,"A",{href:!0,rel:!0});var Xr=n(re);ki=t(Xr,"Natural Language Processing Specialization"),Xr.forEach(i),Fi=t(be," di DeepLearning.AI, un corso che copre un ampio spettro di modelli tradizionali di NLP che vale davvero la pena di conoscere, come Naive Bayes e LSTM ("),qe=l(be,"EM",{});var Zr=n(qe);Mi=t(Zr,"Memoria a breve termine a lungo termine"),Zr.forEach(i),qi=t(be,")!"),be.forEach(i),ma=d(e),I=l(e,"H2",{class:!0});var Sa=n(I);q=l(Sa,"A",{id:!0,class:!0,href:!0});var et=n(q);De=l(et,"SPAN",{});var at=n(De);Ze(te.$$.fragment,at),at.forEach(i),et.forEach(i),Di=d(Sa),Re=l(Sa,"SPAN",{});var it=n(Re);Ri=t(it,"Chi siamo?"),it.forEach(i),Sa.forEach(i),ha=d(e),ge=l(e,"P",{});var rt=n(ge);Ci=t(rt,"A proposito degli autori:"),rt.forEach(i),ga=d(e),oe=l(e,"P",{});var gr=n(oe);Ce=l(gr,"STRONG",{});var tt=n(Ce);xi=t(tt,"Matthew Carrigan"),tt.forEach(i),Gi=t(gr," \xE8 Machine Learning Engineer da Hugging Face. Vive a Dublino, in Irlanda, ed in passato \xE8 stato ML engineer da Parse.ly, e prima ancora ricercatore postdottorale al Trinity College di Dublin. Nonostante non creda che otterremo l\u2019Intelligenza artificiale forte semplicemente ingrandendo le architetture a nostra disposizione, spera comunque nell\u2019immortalit\xE0 cibernetica."),gr.forEach(i),va=d(e),le=l(e,"P",{});var vr=n(le);xe=l(vr,"STRONG",{});var ot=n(xe);Oi=t(ot,"Lysandre Debut"),ot.forEach(i),Bi=t(vr," \xE8 Machine Learning Engineer da Hugging Face e ha lavorato agli \u{1F917} Transformer fin dalle primissime tappe del loro sviluppo. Il suo obiettivo \xE8 di rendere il NLP accessibile a tutti sviluppando strumenti con un semplice API."),vr.forEach(i),_a=d(e),w=l(e,"P",{});var fe=n(w);Ge=l(fe,"STRONG",{});var lt=n(Ge);Qi=t(lt,"Sylvain Gugger"),lt.forEach(i),Ui=t(fe," \xE8 Research Engineer da Hugging Face e uno dei principali manutentori della libreria \u{1F917} Transformers. In passato, \xE8 stato Research Scientist da fast.ai, e ha scritto "),ne=l(fe,"A",{href:!0,rel:!0});var nt=n(ne);Ji=t(nt,"Deep Learning for Coders with fastai and PyTorch"),nt.forEach(i),Vi=t(fe," con Jeremy Howard. Il centro principale della sua ricerca consiste nel rendere il deep learning ("),Oe=l(fe,"EM",{});var st=n(Oe);Wi=t(st,"apprendimento profondo"),st.forEach(i),Yi=t(fe,") pi\xF9 accessibile, concependo e migliorando tecniche che permettano di allenare modelli velocemente con risorse limitate."),fe.forEach(i),za=d(e),se=l(e,"P",{});var _r=n(se);Be=l(_r,"STRONG",{});var ct=n(Be);ji=t(ct,"Merve Noyan"),ct.forEach(i),Ki=t(_r," \xE8 developer advocate da Hugging Face, e lavora allo sviluppo di strumenti e alla creazione di contenuti ad essi legati per democratizzare l\u2019accesso al deep learning."),_r.forEach(i),wa=d(e),ce=l(e,"P",{});var zr=n(ce);Qe=l(zr,"STRONG",{});var ut=n(Qe);Xi=t(ut,"Lucile Saulnier"),ut.forEach(i),Zi=t(zr," \xE8 machine learning engineer da Hugging Face, e sviluppa e supporta l\u2019utilizzo di strumenti open source. \xC8 anche attivamente coinvolta in numerosi progetti di ricerca nell\u2019ambito del NLP, come ad esempio collaborative training e BigScience."),zr.forEach(i),Ea=d(e),S=l(e,"P",{});var je=n(S);Ue=l(je,"STRONG",{});var pt=n(Ue);er=t(pt,"Lewis Tunstall"),pt.forEach(i),ar=t(je," \xE8 machine learning engineer da Hugging Face che si specializza nello sviluppo di strumenti open-source e la loro distribuzione alla comunit\xE0 pi\xF9 ampia. \xC8 anche co-autore dell\u2019imminente "),ue=l(je,"A",{href:!0,rel:!0});var dt=n(ue);ir=t(dt,"O\u2019Reilly book on Transformers"),dt.forEach(i),rr=t(je,"."),je.forEach(i),ba=d(e),H=l(e,"P",{});var Ke=n(H);Je=l(Ke,"STRONG",{});var ft=n(Je);tr=t(ft,"Leandro von Werra"),ft.forEach(i),or=t(Ke," \xE8 machine learning engineer nel team open-source di Hugging Face, nonch\xE9 co-autore dell\u2019imminente "),pe=l(Ke,"A",{href:!0,rel:!0});var mt=n(pe);lr=t(mt,"O\u2019Reilly book on Transformers"),mt.forEach(i),nr=t(Ke,". Ha tanti anni di esperienza nel portare progetti di NLP in produzione, lavorando a tutti i livelli di esecuzione di compiti di machine learning."),Ke.forEach(i),La=d(e),ve=l(e,"P",{});var ht=n(ve);sr=t(ht,"Sei pronto/a a iniziare? In questo capitolo, imparerai:"),ht.forEach(i),ya=d(e),y=l(e,"UL",{});var Le=n(y);de=l(Le,"LI",{});var Ha=n(de);cr=t(Ha,"Ad utilizzare la funzione "),Ve=l(Ha,"CODE",{});var gt=n(Ve);ur=t(gt,"pipeline()"),gt.forEach(i),pr=t(Ha," per eseguire compiti di NLP come la generazione e classificazione di testi"),Ha.forEach(i),dr=d(Le),We=l(Le,"LI",{});var vt=n(We);fr=t(vt,"L\u2019architettura dei Transformer"),vt.forEach(i),mr=d(Le),Ye=l(Le,"LI",{});var _t=n(Ye);hr=t(_t,"Come fare la distinzione tra architetture encoder, decoder, encoder-decoder, e casi d\u2019uso"),_t.forEach(i),Le.forEach(i),this.h()},h(){s(f,"name","hf:doc:metadata"),s(f,"content",JSON.stringify(Nt)),s(v,"id","introduzione"),s(v,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),s(v,"href","#introduzione"),s(m,"class","relative group"),s(F,"id","benvenutoa-al-corso-di"),s(F,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),s(F,"href","#benvenutoa-al-corso-di"),s(P,"class","relative group"),s(G,"href","https://huggingface.co/"),s(G,"rel","nofollow"),s(O,"href","https://github.com/huggingface/transformers"),s(O,"rel","nofollow"),s(B,"href","https://github.com/huggingface/datasets"),s(B,"rel","nofollow"),s(Q,"href","https://github.com/huggingface/tokenizers"),s(Q,"rel","nofollow"),s(U,"href","https://github.com/huggingface/accelerate"),s(U,"rel","nofollow"),s(J,"href","https://huggingface.co/models"),s(J,"rel","nofollow"),s(M,"id","contenuti"),s(M,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),s(M,"href","#contenuti"),s(A,"class","relative group"),s(W,"class","block dark:hidden"),Fa(W.src,Er="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/summary.svg")||s(W,"src",Er),s(W,"alt","Brief overview of the chapters of the course."),s(Y,"class","hidden dark:block"),Fa(Y.src,br="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/summary-dark.svg")||s(Y,"src",br),s(Y,"alt","Brief overview of the chapters of the course."),s(T,"class","flex justify-center"),s(K,"href","https://huggingface.co/models"),s(K,"rel","nofollow"),s(X,"href","https://course.fast.ai/"),s(X,"rel","nofollow"),s(Z,"href","https://www.fast.ai/"),s(Z,"rel","nofollow"),s(ee,"href","https://www.deeplearning.ai/"),s(ee,"rel","nofollow"),s(ae,"href","https://pytorch.org/"),s(ae,"rel","nofollow"),s(ie,"href","https://www.tensorflow.org/"),s(ie,"rel","nofollow"),s(re,"href","https://www.coursera.org/specializations/natural-language-processing?utm_source=deeplearning-ai&utm_medium=institutions&utm_campaign=20211011-nlp-2-hugging_face-page-nlp-refresh"),s(re,"rel","nofollow"),s(q,"id","chi-siamo"),s(q,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),s(q,"href","#chi-siamo"),s(I,"class","relative group"),s(ne,"href","https://learning.oreilly.com/library/view/deep-learning-for/9781492045519/"),s(ne,"rel","nofollow"),s(ue,"href","https://www.oreilly.com/library/view/natural-language-processing/9781098103231/"),s(ue,"rel","nofollow"),s(pe,"href","https://www.oreilly.com/library/view/natural-language-processing/9781098103231/"),s(pe,"rel","nofollow")},m(e,c){a(document.head,f),u(e,z,c),u(e,m,c),a(m,v),a(v,ye),ea(R,ye,null),a(m,Ma),a(m,Pe),a(Pe,qa),u(e,ta,c),u(e,P,c),a(P,F),a(F,Ae),ea(C,Ae,null),a(P,Da),a(P,Te),a(Te,Ra),u(e,oa,c),ea(x,e,c),u(e,la,c),u(e,h,c),a(h,Ca),a(h,Ne),a(Ne,xa),a(h,Ga),a(h,G),a(G,Oa),a(h,Ba),a(h,O),a(O,Qa),a(h,Ua),a(h,B),a(B,Ja),a(h,Va),a(h,Q),a(Q,Wa),a(h,Ya),a(h,U),a(U,ja),a(h,Ka),a(h,J),a(J,Xa),a(h,Za),u(e,na,c),u(e,A,c),a(A,M),a(M,$e),ea(V,$e,null),a(A,ei),a(A,Ie),a(Ie,ai),u(e,sa,c),u(e,me,c),a(me,ii),u(e,ca,c),u(e,T,c),a(T,W),a(T,ri),a(T,Y),u(e,ua,c),u(e,E,c),a(E,j),a(j,ti),a(j,K),a(K,oi),a(j,li),a(E,ni),a(E,Se),a(Se,si),a(E,ci),a(E,N),a(N,ui),a(N,He),a(He,pi),a(N,di),a(N,ke),a(ke,fi),a(N,mi),u(e,pa,c),u(e,he,c),a(he,hi),u(e,da,c),u(e,b,c),a(b,Fe),a(Fe,gi),a(b,vi),a(b,_),a(_,_i),a(_,Me),a(Me,zi),a(_,wi),a(_,X),a(X,Ei),a(_,bi),a(_,Z),a(Z,Li),a(_,yi),a(_,ee),a(ee,Pi),a(b,Ai),a(b,$),a($,Ti),a($,ae),a(ae,Ni),a($,$i),a($,ie),a(ie,Ii),a($,Si),u(e,fa,c),u(e,L,c),a(L,Hi),a(L,re),a(re,ki),a(L,Fi),a(L,qe),a(qe,Mi),a(L,qi),u(e,ma,c),u(e,I,c),a(I,q),a(q,De),ea(te,De,null),a(I,Di),a(I,Re),a(Re,Ri),u(e,ha,c),u(e,ge,c),a(ge,Ci),u(e,ga,c),u(e,oe,c),a(oe,Ce),a(Ce,xi),a(oe,Gi),u(e,va,c),u(e,le,c),a(le,xe),a(xe,Oi),a(le,Bi),u(e,_a,c),u(e,w,c),a(w,Ge),a(Ge,Qi),a(w,Ui),a(w,ne),a(ne,Ji),a(w,Vi),a(w,Oe),a(Oe,Wi),a(w,Yi),u(e,za,c),u(e,se,c),a(se,Be),a(Be,ji),a(se,Ki),u(e,wa,c),u(e,ce,c),a(ce,Qe),a(Qe,Xi),a(ce,Zi),u(e,Ea,c),u(e,S,c),a(S,Ue),a(Ue,er),a(S,ar),a(S,ue),a(ue,ir),a(S,rr),u(e,ba,c),u(e,H,c),a(H,Je),a(Je,tr),a(H,or),a(H,pe),a(pe,lr),a(H,nr),u(e,La,c),u(e,ve,c),a(ve,sr),u(e,ya,c),u(e,y,c),a(y,de),a(de,cr),a(de,Ve),a(Ve,ur),a(de,pr),a(y,dr),a(y,We),a(We,fr),a(y,mr),a(y,Ye),a(Ye,hr),Pa=!0},p:wr,i(e){Pa||(aa(R.$$.fragment,e),aa(C.$$.fragment,e),aa(x.$$.fragment,e),aa(V.$$.fragment,e),aa(te.$$.fragment,e),Pa=!0)},o(e){ia(R.$$.fragment,e),ia(C.$$.fragment,e),ia(x.$$.fragment,e),ia(V.$$.fragment,e),ia(te.$$.fragment,e),Pa=!1},d(e){i(f),e&&i(z),e&&i(m),ra(R),e&&i(ta),e&&i(P),ra(C),e&&i(oa),ra(x,e),e&&i(la),e&&i(h),e&&i(na),e&&i(A),ra(V),e&&i(sa),e&&i(me),e&&i(ca),e&&i(T),e&&i(ua),e&&i(E),e&&i(pa),e&&i(he),e&&i(da),e&&i(b),e&&i(fa),e&&i(L),e&&i(ma),e&&i(I),ra(te),e&&i(ha),e&&i(ge),e&&i(ga),e&&i(oe),e&&i(va),e&&i(le),e&&i(_a),e&&i(w),e&&i(za),e&&i(se),e&&i(wa),e&&i(ce),e&&i(Ea),e&&i(S),e&&i(ba),e&&i(H),e&&i(La),e&&i(ve),e&&i(ya),e&&i(y)}}}const Nt={local:"introduzione",sections:[{local:"benvenutoa-al-corso-di",title:"Benvenuto/a al corso di \u{1F917}!"},{local:"contenuti",title:"Contenuti"},{local:"chi-siamo",title:"Chi siamo?"}],title:"Introduzione"};function $t(D){return Lt(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Ht extends zt{constructor(f){super();wt(this,f,$t,Tt,Et,{})}}export{Ht as default,Nt as metadata};
