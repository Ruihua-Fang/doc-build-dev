import{S as xn,i as Cn,s as yn,e as r,k as d,w as $,t as a,M as Pn,c as l,d as t,m as c,x as q,a as i,h as n,b as j,F as s,g as u,y as b,q as g,o as E,B as k,v as An}from"../../chunks/vendor-1e8b365d.js";import{T as mt}from"../../chunks/Tip-62b14c6e.js";import{Y as ft}from"../../chunks/Youtube-c2a8cc39.js";import{I as vt}from"../../chunks/IconCopyLink-483c28ba.js";import{C as I}from"../../chunks/CodeBlock-e5764662.js";import{D as Tn}from"../../chunks/DocNotebookDropdown-37d928d3.js";import{F as Sn}from"../../chunks/FrameworkSwitchCourse-7f8f0f31.js";function Dn(U){let p,x,m,C,h,f,P,w;return{c(){p=r("p"),x=a("Notez que les mod\xE8les \u{1F917} "),m=r("em"),C=a("Transformers"),h=a(" ont une capacit\xE9 sp\xE9ciale que la plupart des mod\xE8les Keras n\u2019ont pas. Ils peuvent automatiquement utiliser une perte appropri\xE9e qu\u2019ils calculent en interne. Ils utiliseront cette perte par d\xE9faut si vous ne d\xE9finissez pas un argument de perte dans "),f=r("code"),P=a("compile()"),w=a(". Notez que pour utiliser la perte interne, vous devrez passer vos labels comme faisant partie de l\u2019entr\xE9e, et non pas comme un label s\xE9par\xE9, ce qui est la fa\xE7on normale d\u2019utiliser les labels avec les mod\xE8les Keras. Vous verrez des exemples de cela dans la partie 2 du cours, o\xF9 la d\xE9finition de la fonction de perte correcte peut \xEAtre d\xE9licate. Pour la classification des s\xE9quences, cependant, une fonction de perte standard de Keras fonctionne bien, et c\u2019est donc ce que nous utiliserons ici.")},l(_){p=l(_,"P",{});var v=i(p);x=n(v,"Notez que les mod\xE8les \u{1F917} "),m=l(v,"EM",{});var S=i(m);C=n(S,"Transformers"),S.forEach(t),h=n(v," ont une capacit\xE9 sp\xE9ciale que la plupart des mod\xE8les Keras n\u2019ont pas. Ils peuvent automatiquement utiliser une perte appropri\xE9e qu\u2019ils calculent en interne. Ils utiliseront cette perte par d\xE9faut si vous ne d\xE9finissez pas un argument de perte dans "),f=l(v,"CODE",{});var z=i(f);P=n(z,"compile()"),z.forEach(t),w=n(v,". Notez que pour utiliser la perte interne, vous devrez passer vos labels comme faisant partie de l\u2019entr\xE9e, et non pas comme un label s\xE9par\xE9, ce qui est la fa\xE7on normale d\u2019utiliser les labels avec les mod\xE8les Keras. Vous verrez des exemples de cela dans la partie 2 du cours, o\xF9 la d\xE9finition de la fonction de perte correcte peut \xEAtre d\xE9licate. Pour la classification des s\xE9quences, cependant, une fonction de perte standard de Keras fonctionne bien, et c\u2019est donc ce que nous utiliserons ici."),v.forEach(t)},m(_,v){u(_,p,v),s(p,x),s(p,m),s(m,C),s(p,h),s(p,f),s(f,P),s(p,w)},d(_){_&&t(p)}}}function Mn(U){let p,x,m,C,h,f,P,w;return{c(){p=r("p"),x=a("Notez un pi\xE8ge tr\xE8s commun ici. Vous "),m=r("em"),C=a("pouvez"),h=a(" simplement passer le nom de la perte comme une cha\xEEne \xE0 Keras, mais par d\xE9faut Keras supposera que vous avez d\xE9j\xE0 appliqu\xE9 une fonction softmax \xE0 vos sorties. Cependant, de nombreux mod\xE8les produisent les valeurs juste avant l\u2019application de la softmax, que l\u2019on appelle aussi les "),f=r("em"),P=a("logits"),w=a(". Nous devons indiquer \xE0 la fonction de perte que c\u2019est ce que fait notre mod\xE8le, et la seule fa\xE7on de le faire est de l\u2019appeler directement, plut\xF4t que par son nom avec une cha\xEEne.")},l(_){p=l(_,"P",{});var v=i(p);x=n(v,"Notez un pi\xE8ge tr\xE8s commun ici. Vous "),m=l(v,"EM",{});var S=i(m);C=n(S,"pouvez"),S.forEach(t),h=n(v," simplement passer le nom de la perte comme une cha\xEEne \xE0 Keras, mais par d\xE9faut Keras supposera que vous avez d\xE9j\xE0 appliqu\xE9 une fonction softmax \xE0 vos sorties. Cependant, de nombreux mod\xE8les produisent les valeurs juste avant l\u2019application de la softmax, que l\u2019on appelle aussi les "),f=l(v,"EM",{});var z=i(f);P=n(z,"logits"),z.forEach(t),w=n(v,". Nous devons indiquer \xE0 la fonction de perte que c\u2019est ce que fait notre mod\xE8le, et la seule fa\xE7on de le faire est de l\u2019appeler directement, plut\xF4t que par son nom avec une cha\xEEne."),v.forEach(t)},m(_,v){u(_,p,v),s(p,x),s(p,m),s(m,C),s(p,h),s(p,f),s(f,P),s(p,w)},d(_){_&&t(p)}}}function Nn(U){let p,x,m,C,h,f,P,w,_,v,S;return{c(){p=r("p"),x=a("La biblioth\xE8que \u{1F917} "),m=r("em"),C=a("Transformers"),h=a(" poss\xE8de \xE9galement une fonction "),f=r("code"),P=a("create_optimizer()"),w=a(" qui cr\xE9era un optimiseur "),_=r("code"),v=a("AdamW"),S=a(" avec un taux d\u2019apprentissage d\xE9croissant. Il s\u2019agit d\u2019un raccourci pratique que vous verrez en d\xE9tail dans les prochaines sections du cours.")},l(z){p=l(z,"P",{});var y=i(p);x=n(y,"La biblioth\xE8que \u{1F917} "),m=l(y,"EM",{});var O=i(m);C=n(O,"Transformers"),O.forEach(t),h=n(y," poss\xE8de \xE9galement une fonction "),f=l(y,"CODE",{});var A=i(f);P=n(A,"create_optimizer()"),A.forEach(t),w=n(y," qui cr\xE9era un optimiseur "),_=l(y,"CODE",{});var T=i(_);v=n(T,"AdamW"),T.forEach(t),S=n(y," avec un taux d\u2019apprentissage d\xE9croissant. Il s\u2019agit d\u2019un raccourci pratique que vous verrez en d\xE9tail dans les prochaines sections du cours."),y.forEach(t)},m(z,y){u(z,p,y),s(p,x),s(p,m),s(m,C),s(p,h),s(p,f),s(f,P),s(p,w),s(p,_),s(_,v),s(p,S)},d(z){z&&t(p)}}}function Fn(U){let p,x,m,C,h,f,P,w,_,v,S,z,y,O;return{c(){p=r("p"),x=a("\u{1F4A1} Si vous voulez t\xE9l\xE9charger automatiquement votre mod\xE8le sur le "),m=r("em"),C=a("Hub"),h=a(" pendant l\u2019entra\xEEnement, vous pouvez passer un "),f=r("code"),P=a("PushToHubCallback"),w=a(" dans la m\xE9thode "),_=r("code"),v=a("model.fit()"),S=a(". Nous en apprendrons davantage \xE0 ce sujet au "),z=r("a"),y=a("Chapitre 4"),O=a("."),this.h()},l(A){p=l(A,"P",{});var T=i(p);x=n(T,"\u{1F4A1} Si vous voulez t\xE9l\xE9charger automatiquement votre mod\xE8le sur le "),m=l(T,"EM",{});var Q=i(m);C=n(Q,"Hub"),Q.forEach(t),h=n(T," pendant l\u2019entra\xEEnement, vous pouvez passer un "),f=l(T,"CODE",{});var Se=i(f);P=n(Se,"PushToHubCallback"),Se.forEach(t),w=n(T," dans la m\xE9thode "),_=l(T,"CODE",{});var De=i(_);v=n(De,"model.fit()"),De.forEach(t),S=n(T,". Nous en apprendrons davantage \xE0 ce sujet au "),z=l(T,"A",{href:!0});var G=i(z);y=n(G,"Chapitre 4"),G.forEach(t),O=n(T,"."),T.forEach(t),this.h()},h(){j(z,"href","/course/fr/chapter4/3")},m(A,T){u(A,p,T),s(p,x),s(p,m),s(m,C),s(p,h),s(p,f),s(f,P),s(p,w),s(p,_),s(_,v),s(p,S),s(p,z),s(z,y),s(p,O)},d(A){A&&t(p)}}}function Kn(U){let p,x,m,C,h,f,P,w,_,v,S,z,y,O,A,T,Q,Se,De,G,_t,ht,gs,Me,$t,Es,me,ks,B,X,Ie,fe,qt,Ge,bt,js,Z,gt,Ve,Et,kt,ws,ve,zs,Ne,jt,xs,_e,Cs,ee,wt,He,zt,xt,ys,he,Ps,se,Ct,Fe,yt,Pt,As,M,At,Re,Tt,St,Be,Dt,Mt,We,Nt,Ft,Ye,Kt,Lt,Ts,te,Ss,$e,Ds,ae,Ms,W,ne,Je,qe,Ut,Qe,Ot,Ns,be,Fs,V,It,Xe,Gt,Vt,Ze,Ht,Rt,Ks,D,Bt,es,Wt,Yt,ss,Jt,Qt,ts,Xt,Zt,as,ea,sa,ns,ta,aa,Ls,ge,Us,oe,Os,Ke,na,Is,Ee,Gs,re,oa,os,ra,la,Vs,ke,Hs,le,Rs,Y,ie,rs,je,ia,ls,ua,Bs,we,Ws,H,pa,is,da,ca,us,ma,fa,Ys,ze,Js,ue,va,ps,_a,ha,Qs,xe,Xs,Ce,Zs,N,$a,ds,qa,ba,cs,ga,Ea,ms,ka,ja,et,ye,st,Pe,tt,F,wa,Ae,za,xa,fs,Ca,ya,vs,Pa,Aa,at,K,Ta,_s,Sa,Da,Le,Ma,Na,hs,Fa,Ka,nt;return m=new Sn({props:{fw:U[0]}}),w=new vt({}),y=new Tn({props:{classNames:"absolute z-10 right-0 top-0",options:[{label:"Google Colab",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter3/section3_tf.ipynb"},{label:"Aws Studio",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter3/section3_tf.ipynb"}]}}),me=new I({props:{code:`from datasets import load_dataset
from transformers import AutoTokenizer, DataCollatorWithPadding
import numpy as np

raw_datasets = load_dataset("glue", "mrpc")
checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)


def tokenize_function(example):
    return tokenizer(example["sentence1"], example["sentence2"], truncation=True)


tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)

data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors="tf")

tf_train_dataset = tokenized_datasets["train"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "token_type_ids"],
    label_cols=["labels"],
    shuffle=True,
    collate_fn=data_collator,
    batch_size=8,
)

tf_validation_dataset = tokenized_datasets["validation"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "token_type_ids"],
    label_cols=["labels"],
    shuffle=False,
    collate_fn=data_collator,
    batch_size=8,
)`,highlighted:`<span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, DataCollatorWithPadding
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np

raw_datasets = load_dataset(<span class="hljs-string">&quot;glue&quot;</span>, <span class="hljs-string">&quot;mrpc&quot;</span>)
checkpoint = <span class="hljs-string">&quot;bert-base-uncased&quot;</span>
tokenizer = AutoTokenizer.from_pretrained(checkpoint)


<span class="hljs-keyword">def</span> <span class="hljs-title function_">tokenize_function</span>(<span class="hljs-params">example</span>):
    <span class="hljs-keyword">return</span> tokenizer(example[<span class="hljs-string">&quot;sentence1&quot;</span>], example[<span class="hljs-string">&quot;sentence2&quot;</span>], truncation=<span class="hljs-literal">True</span>)


tokenized_datasets = raw_datasets.<span class="hljs-built_in">map</span>(tokenize_function, batched=<span class="hljs-literal">True</span>)

data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)

tf_train_dataset = tokenized_datasets[<span class="hljs-string">&quot;train&quot;</span>].to_tf_dataset(
    columns=[<span class="hljs-string">&quot;attention_mask&quot;</span>, <span class="hljs-string">&quot;input_ids&quot;</span>, <span class="hljs-string">&quot;token_type_ids&quot;</span>],
    label_cols=[<span class="hljs-string">&quot;labels&quot;</span>],
    shuffle=<span class="hljs-literal">True</span>,
    collate_fn=data_collator,
    batch_size=<span class="hljs-number">8</span>,
)

tf_validation_dataset = tokenized_datasets[<span class="hljs-string">&quot;validation&quot;</span>].to_tf_dataset(
    columns=[<span class="hljs-string">&quot;attention_mask&quot;</span>, <span class="hljs-string">&quot;input_ids&quot;</span>, <span class="hljs-string">&quot;token_type_ids&quot;</span>],
    label_cols=[<span class="hljs-string">&quot;labels&quot;</span>],
    shuffle=<span class="hljs-literal">False</span>,
    collate_fn=data_collator,
    batch_size=<span class="hljs-number">8</span>,
)`}}),fe=new vt({}),ve=new ft({props:{id:"rnTGBy2ax1c"}}),_e=new ft({props:{id:"AUozVp78dhk"}}),he=new I({props:{code:`from transformers import TFAutoModelForSequenceClassification

model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFAutoModelForSequenceClassification

model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=<span class="hljs-number">2</span>)`}}),te=new mt({props:{$$slots:{default:[Dn]},$$scope:{ctx:U}}}),$e=new I({props:{code:`from tensorflow.keras.losses import SparseCategoricalCrossentropy

model.compile(
    optimizer="adam",
    loss=SparseCategoricalCrossentropy(from_logits=True),
    metrics=["accuracy"],
)
model.fit(
    tf_train_dataset,
    validation_data=tf_validation_dataset,
)`,highlighted:`<span class="hljs-keyword">from</span> tensorflow.keras.losses <span class="hljs-keyword">import</span> SparseCategoricalCrossentropy

model.<span class="hljs-built_in">compile</span>(
    optimizer=<span class="hljs-string">&quot;adam&quot;</span>,
    loss=SparseCategoricalCrossentropy(from_logits=<span class="hljs-literal">True</span>),
    metrics=[<span class="hljs-string">&quot;accuracy&quot;</span>],
)
model.fit(
    tf_train_dataset,
    validation_data=tf_validation_dataset,
)`}}),ae=new mt({props:{warning:!0,$$slots:{default:[Mn]},$$scope:{ctx:U}}}),qe=new vt({}),be=new ft({props:{id:"cpzq6ESSM5c"}}),ge=new I({props:{code:`from tensorflow.keras.optimizers.schedules import PolynomialDecay

batch_size = 8
num_epochs = 3
# Le nombre d'\xE9tapes d'entra\xEEnement est le nombre d'\xE9chantillons dans l'ensemble de donn\xE9es, divis\xE9 par la taille du batch puis multipli\xE9
# par le nombre total d'\xE9poques. Notez que le jeu de donn\xE9es tf_train_dataset est ici un lot tf.data.Dataset
# et non le jeu de donn\xE9es original Hugging Face Dataset, donc son len() est d\xE9j\xE0 num_samples // batch_size.
num_train_steps = len(tf_train_dataset) * num_epochs
lr_scheduler = PolynomialDecay(
    initial_learning_rate=5e-5, end_learning_rate=0.0, decay_steps=num_train_steps
)
from tensorflow.keras.optimizers import Adam

opt = Adam(learning_rate=lr_scheduler)`,highlighted:`<span class="hljs-keyword">from</span> tensorflow.keras.optimizers.schedules <span class="hljs-keyword">import</span> PolynomialDecay

batch_size = <span class="hljs-number">8</span>
num_epochs = <span class="hljs-number">3</span>
<span class="hljs-comment"># Le nombre d&#x27;\xE9tapes d&#x27;entra\xEEnement est le nombre d&#x27;\xE9chantillons dans l&#x27;ensemble de donn\xE9es, divis\xE9 par la taille du batch puis multipli\xE9</span>
<span class="hljs-comment"># par le nombre total d&#x27;\xE9poques. Notez que le jeu de donn\xE9es tf_train_dataset est ici un lot tf.data.Dataset</span>
<span class="hljs-comment"># et non le jeu de donn\xE9es original Hugging Face Dataset, donc son len() est d\xE9j\xE0 num_samples // batch_size.</span>
num_train_steps = <span class="hljs-built_in">len</span>(tf_train_dataset) * num_epochs
lr_scheduler = PolynomialDecay(
    initial_learning_rate=<span class="hljs-number">5e-5</span>, end_learning_rate=<span class="hljs-number">0.0</span>, decay_steps=num_train_steps
)
<span class="hljs-keyword">from</span> tensorflow.keras.optimizers <span class="hljs-keyword">import</span> Adam

opt = Adam(learning_rate=lr_scheduler)`}}),oe=new mt({props:{$$slots:{default:[Nn]},$$scope:{ctx:U}}}),Ee=new I({props:{code:`import tensorflow as tf

model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)
loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
model.compile(optimizer=opt, loss=loss, metrics=["accuracy"])`,highlighted:`<span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=<span class="hljs-number">2</span>)
loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=<span class="hljs-literal">True</span>)
model.<span class="hljs-built_in">compile</span>(optimizer=opt, loss=loss, metrics=[<span class="hljs-string">&quot;accuracy&quot;</span>])`}}),ke=new I({props:{code:"model.fit(tf_train_dataset, validation_data=tf_validation_dataset, epochs=3)",highlighted:'model.fit(tf_train_dataset, validation_data=tf_validation_dataset, epochs=<span class="hljs-number">3</span>)'}}),le=new mt({props:{$$slots:{default:[Fn]},$$scope:{ctx:U}}}),je=new vt({}),we=new ft({props:{id:"nx10eh4CoOs"}}),ze=new I({props:{code:'preds = model.predict(tf_validation_dataset)["logits"]',highlighted:'preds = model.predict(tf_validation_dataset)[<span class="hljs-string">&quot;logits&quot;</span>]'}}),xe=new I({props:{code:`class_preds = np.argmax(preds, axis=1)
print(preds.shape, class_preds.shape)`,highlighted:`class_preds = np.argmax(preds, axis=<span class="hljs-number">1</span>)
<span class="hljs-built_in">print</span>(preds.shape, class_preds.shape)`}}),Ce=new I({props:{code:"(408, 2) (408,)",highlighted:'(<span class="hljs-number">408</span>, <span class="hljs-number">2</span>) (<span class="hljs-number">408</span>,)'}}),ye=new I({props:{code:`from datasets import load_metric

metric = load_metric("glue", "mrpc")
metric.compute(predictions=class_preds, references=raw_datasets["validation"]["label"])`,highlighted:`<span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_metric

metric = load_metric(<span class="hljs-string">&quot;glue&quot;</span>, <span class="hljs-string">&quot;mrpc&quot;</span>)
metric.compute(predictions=class_preds, references=raw_datasets[<span class="hljs-string">&quot;validation&quot;</span>][<span class="hljs-string">&quot;label&quot;</span>])`}}),Pe=new I({props:{code:"{'accuracy': 0.8578431372549019, 'f1': 0.8996539792387542}",highlighted:'{<span class="hljs-string">&#x27;accuracy&#x27;</span>: <span class="hljs-number">0.8578431372549019</span>, <span class="hljs-string">&#x27;f1&#x27;</span>: <span class="hljs-number">0.8996539792387542</span>}'}}),{c(){p=r("meta"),x=d(),$(m.$$.fragment),C=d(),h=r("h1"),f=r("a"),P=r("span"),$(w.$$.fragment),_=d(),v=r("span"),S=a("*Finetuner* un mod\xE8le avec Keras"),z=d(),$(y.$$.fragment),O=d(),A=r("p"),T=a("Une fois que vous avez fait tout le travail de pr\xE9traitement des donn\xE9es dans la derni\xE8re section, il ne vous reste que quelques \xE9tapes pour entra\xEEner le mod\xE8le. Notez, cependant, que la commande "),Q=r("code"),Se=a("model.fit()"),De=a(" s\u2019ex\xE9cutera tr\xE8s lentement sur un CPU. Si vous n\u2019avez pas de GPU, vous pouvez avoir acc\xE8s \xE0 des GPUs ou TPUs gratuits sur [Google Colab] ("),G=r("a"),_t=a("https://colab.research.google.com/"),ht=a(")."),gs=d(),Me=r("p"),$t=a("Les exemples de code ci-dessous supposent que vous avez d\xE9j\xE0 ex\xE9cut\xE9 les exemples de la section pr\xE9c\xE9dente. Voici un bref r\xE9sum\xE9 de ce dont vous avez besoin :"),Es=d(),$(me.$$.fragment),ks=d(),B=r("h3"),X=r("a"),Ie=r("span"),$(fe.$$.fragment),qt=d(),Ge=r("span"),bt=a("Entra\xEEnement"),js=d(),Z=r("p"),gt=a("Les mod\xE8les TensorFlow import\xE9s depuis \u{1F917} "),Ve=r("em"),Et=a("Transformers"),kt=a(" sont d\xE9j\xE0 des mod\xE8les Keras. Voici une courte introduction \xE0 Keras."),ws=d(),$(ve.$$.fragment),zs=d(),Ne=r("p"),jt=a("Cela signifie qu\u2019une fois que nous disposons de nos donn\xE9es, tr\xE8s peu de travail est n\xE9cessaire pour commencer \xE0 entra\xEEner sur celles-ci."),xs=d(),$(_e.$$.fragment),Cs=d(),ee=r("p"),wt=a("Comme dans le [chapitre pr\xE9c\xE9dent] (/course/fr/chapter2), nous allons utiliser la classe "),He=r("code"),zt=a("TFAutoModelForSequenceClassification"),xt=a(", avec deux \xE9tiquettes :"),ys=d(),$(he.$$.fragment),Ps=d(),se=r("p"),Ct=a("Vous remarquerez que, contrairement au "),Fe=r("a"),yt=a("Chapitre 2"),Pt=a(", vous obtenez un message d\u2019avertissement apr\xE8s l\u2019instanciation de ce mod\xE8le pr\xE9-entra\xEEn\xE9. Ceci est d\xFB au fait que BERT n\u2019a pas \xE9t\xE9 pr\xE9-entra\xEEn\xE9 \xE0 la classification de paires de phrases, donc la t\xEAte du mod\xE8le pr\xE9-entra\xEEn\xE9 a \xE9t\xE9 supprim\xE9e et une nouvelle t\xEAte adapt\xE9e \xE0 la classification de s\xE9quences a \xE9t\xE9 ins\xE9r\xE9e \xE0 la place. Les messages d\u2019avertissement indiquent que certains poids n\u2019ont pas \xE9t\xE9 utilis\xE9s (ceux correspondant \xE0 la t\xEAte de pr\xE9-entra\xEEnement abandonn\xE9e) et que d\u2019autres ont \xE9t\xE9 initialis\xE9s de mani\xE8re al\xE9atoire (ceux pour la nouvelle t\xEAte). Il conclut en vous encourageant \xE0 entra\xEEner le mod\xE8le, ce qui est exactement ce que nous allons faire maintenant."),As=d(),M=r("p"),At=a("Pour "),Re=r("em"),Tt=a("finetuner"),St=a(" le mod\xE8le sur notre jeu de donn\xE9es, nous devons simplement "),Be=r("code"),Dt=a("compiler()"),Mt=a(" notre mod\xE8le et ensuite passer nos donn\xE9es \xE0 la m\xE9thode "),We=r("code"),Nt=a("fit()"),Ft=a(". Cela va d\xE9marrer le processus de "),Ye=r("em"),Kt=a("finetuning"),Lt=a(" (qui devrait prendre quelques minutes sur un GPU) et rapporter la perte d\u2019entra\xEEnement au fur et \xE0 mesure, plus la perte de validation \xE0 la fin de chaque \xE9poque."),Ts=d(),$(te.$$.fragment),Ss=d(),$($e.$$.fragment),Ds=d(),$(ae.$$.fragment),Ms=d(),W=r("h3"),ne=r("a"),Je=r("span"),$(qe.$$.fragment),Ut=d(),Qe=r("span"),Ot=a("Am\xE9liorer les performances d'entra\xEEnement"),Ns=d(),$(be.$$.fragment),Fs=d(),V=r("p"),It=a("Si vous essayez le code ci-dessus, il fonctionne certainement, mais vous constaterez que la perte ne diminue que lentement ou sporadiquement. La cause principale est le "),Xe=r("em"),Gt=a("taux d\u2019apprentissage"),Vt=a(". Comme pour la perte, lorsque nous transmettons \xE0 Keras le nom d\u2019un optimiseur sous forme de cha\xEEne de caract\xE8res, Keras initialise cet optimiseur avec des valeurs par d\xE9faut pour tous les param\xE8tres, y compris le taux d\u2019apprentissage. Cependant, nous savons depuis longtemps que les "),Ze=r("em"),Ht=a("transformers"),Rt=a(" b\xE9n\xE9ficient d\u2019un taux d\u2019apprentissage beaucoup plus faible que celui par d\xE9faut d\u2019Adam, qui est de 1e-3, \xE9galement \xE9crit comme 10 \xE0 la puissance -3, ou 0,001. 5e-5 (0,00005), qui est environ vingt fois inf\xE9rieur, est un bien meilleur point de d\xE9part."),Ks=d(),D=r("p"),Bt=a("En plus de r\xE9duire le taux d\u2019apprentissage, nous avons une deuxi\xE8me astuce dans notre manche : nous pouvons r\xE9duire lentement le taux d\u2019apprentissage au cours de l\u2019entra\xEEnement. Dans la litt\xE9rature, on parle parfois de "),es=r("em"),Wt=a("d\xE9croissance"),Yt=a(" ou d\u2019 "),ss=r("em"),Jt=a("annulation"),Qt=a(" du taux d\u2019apprentissage.le taux d\u2019apprentissage. Dans Keras, la meilleure fa\xE7on de le faire est d\u2019utiliser un "),ts=r("em"),Xt=a("planificateur du taux d\u2019apprentissage"),Zt=a(". Un bon planificateur \xE0 utiliser est "),as=r("code"),ea=a("PolynomialDecay"),sa=a(". Malgr\xE9 son nom, avec les param\xE8tres par d\xE9faut, il diminue simplement de fa\xE7on lin\xE9aire le taux d\u2019apprentissage de la valeur initiale \xE0 la valeur finale au cours de l\u2019entra\xEEnement, ce qui est exactement ce que nous voulons. Afin d\u2019utiliser correctement un planificateur, nous devons lui dire combien de temps l\u2019entra\xEEnement va durer. Nous calculons cela comme "),ns=r("code"),ta=a("num_train_steps"),aa=a(" ci-dessous."),Ls=d(),$(ge.$$.fragment),Us=d(),$(oe.$$.fragment),Os=d(),Ke=r("p"),na=a("Nous avons maintenant notre tout nouvel optimiseur et nous pouvons essayer de nous entra\xEEner avec lui. Tout d\u2019abord, rechargeons le mod\xE8le pour r\xE9initialiser les modifications apport\xE9es aux poids lors de l\u2019entra\xEEnement que nous venons d\u2019effectuer, puis nous pouvons le compiler avec le nouvel optimiseur :"),Is=d(),$(Ee.$$.fragment),Gs=d(),re=r("p"),oa=a("Maintenant, on "),os=r("em"),ra=a("fit"),la=a(" :"),Vs=d(),$(ke.$$.fragment),Hs=d(),$(le.$$.fragment),Rs=d(),Y=r("h3"),ie=r("a"),rs=r("span"),$(je.$$.fragment),ia=d(),ls=r("span"),ua=a("Pr\xE9dictions du mod\xE8le"),Bs=d(),$(we.$$.fragment),Ws=d(),H=r("p"),pa=a("Entra\xEEner et regarder la perte diminuer, c\u2019est tr\xE8s bien, mais que faire si l\u2019on veut r\xE9ellement obtenir des r\xE9sultats du mod\xE8le entra\xEEn\xE9, soit pour calculer des m\xE9triques, soit pour utiliser le mod\xE8le en production ? Pour ce faire, nous pouvons simplement utiliser la m\xE9thode "),is=r("code"),da=a("predict()"),ca=a(". Ceci retournera les "),us=r("em"),ma=a("logits"),fa=a(" de la t\xEAte de sortie du mod\xE8le, un par classe."),Ys=d(),$(ze.$$.fragment),Js=d(),ue=r("p"),va=a("Nous pouvons convertir ces logits en pr\xE9dictions de classe du mod\xE8le en utilisant "),ps=r("code"),_a=a("argmax"),ha=a(" pour trouver le logit le plus \xE9lev\xE9, qui correspond \xE0 la classe la plus probable :"),Qs=d(),$(xe.$$.fragment),Xs=d(),$(Ce.$$.fragment),Zs=d(),N=r("p"),$a=a("Maintenant, utilisons ces "),ds=r("code"),qa=a("preds"),ba=a(" pour calculer des m\xE9triques ! Nous pouvons charger les m\xE9triques associ\xE9es au jeu de donn\xE9es MRPC aussi facilement que nous avons charg\xE9 le jeu de donn\xE9es, cette fois avec la fonction "),cs=r("code"),ga=a("load_metric()"),Ea=a(". L\u2019objet retourn\xE9 a une m\xE9thode "),ms=r("code"),ka=a("compute()"),ja=a(" que nous pouvons utiliser pour faire le calcul de la m\xE9trique :"),et=d(),$(ye.$$.fragment),st=d(),$(Pe.$$.fragment),tt=d(),F=r("p"),wa=a("Les r\xE9sultats exacts que vous obtiendrez peuvent varier, car l\u2019initialisation al\xE9atoire de la t\xEAte du mod\xE8le peut modifier les m\xE9triques obtenues. Ici, nous pouvons voir que notre mod\xE8le a une pr\xE9cision de 85,78% sur l\u2019ensemble de validation et un score F1 de 89,97. Ce sont les deux m\xE9triques utilis\xE9es pour \xE9valuer les r\xE9sultats sur le jeu de donn\xE9es MRPC pour le benchmark GLUE. Le tableau du papier de "),Ae=r("a"),za=a("BERT"),xa=a(" indique un score F1 de 88,9 pour le mod\xE8le de base. Il s\u2019agissait du mod\xE8le "),fs=r("code"),Ca=a("uncased"),ya=a(" alors que nous utilisons actuellement le mod\xE8le "),vs=r("code"),Pa=a("cased"),Aa=a(", ce qui explique le meilleur r\xE9sultat."),at=d(),K=r("p"),Ta=a("Ceci conclut l\u2019introduction \xE0 le "),_s=r("em"),Sa=a("finetuning"),Da=a(" en utilisant l\u2019API Keras. Un exemple d\u2019application de cette m\xE9thode aux t\xE2ches les plus courantes du traitement automatique des langues sera pr\xE9sent\xE9 au "),Le=r("a"),Ma=a("Chapitre 7"),Na=a(". Si vous souhaitez affiner vos connaissances de l\u2019API Keras, essayez "),hs=r("em"),Fa=a("finetuner"),Ka=a(" un mod\xE8le sur le jeu de donn\xE9es GLUE SST-2, en utilisant le traitement des donn\xE9es que vous avez effectu\xE9 dans la section 2."),this.h()},l(e){const o=Pn('[data-svelte="svelte-1phssyn"]',document.head);p=l(o,"META",{name:!0,content:!0}),o.forEach(t),x=c(e),q(m.$$.fragment,e),C=c(e),h=l(e,"H1",{class:!0});var Te=i(h);f=l(Te,"A",{id:!0,class:!0,href:!0});var $s=i(f);P=l($s,"SPAN",{});var qs=i(P);q(w.$$.fragment,qs),qs.forEach(t),$s.forEach(t),_=c(Te),v=l(Te,"SPAN",{});var bs=i(v);S=n(bs,"*Finetuner* un mod\xE8le avec Keras"),bs.forEach(t),Te.forEach(t),z=c(e),q(y.$$.fragment,e),O=c(e),A=l(e,"P",{});var J=i(A);T=n(J,"Une fois que vous avez fait tout le travail de pr\xE9traitement des donn\xE9es dans la derni\xE8re section, il ne vous reste que quelques \xE9tapes pour entra\xEEner le mod\xE8le. Notez, cependant, que la commande "),Q=l(J,"CODE",{});var La=i(Q);Se=n(La,"model.fit()"),La.forEach(t),De=n(J," s\u2019ex\xE9cutera tr\xE8s lentement sur un CPU. Si vous n\u2019avez pas de GPU, vous pouvez avoir acc\xE8s \xE0 des GPUs ou TPUs gratuits sur [Google Colab] ("),G=l(J,"A",{href:!0,rel:!0});var Ua=i(G);_t=n(Ua,"https://colab.research.google.com/"),Ua.forEach(t),ht=n(J,")."),J.forEach(t),gs=c(e),Me=l(e,"P",{});var Oa=i(Me);$t=n(Oa,"Les exemples de code ci-dessous supposent que vous avez d\xE9j\xE0 ex\xE9cut\xE9 les exemples de la section pr\xE9c\xE9dente. Voici un bref r\xE9sum\xE9 de ce dont vous avez besoin :"),Oa.forEach(t),Es=c(e),q(me.$$.fragment,e),ks=c(e),B=l(e,"H3",{class:!0});var ot=i(B);X=l(ot,"A",{id:!0,class:!0,href:!0});var Ia=i(X);Ie=l(Ia,"SPAN",{});var Ga=i(Ie);q(fe.$$.fragment,Ga),Ga.forEach(t),Ia.forEach(t),qt=c(ot),Ge=l(ot,"SPAN",{});var Va=i(Ge);bt=n(Va,"Entra\xEEnement"),Va.forEach(t),ot.forEach(t),js=c(e),Z=l(e,"P",{});var rt=i(Z);gt=n(rt,"Les mod\xE8les TensorFlow import\xE9s depuis \u{1F917} "),Ve=l(rt,"EM",{});var Ha=i(Ve);Et=n(Ha,"Transformers"),Ha.forEach(t),kt=n(rt," sont d\xE9j\xE0 des mod\xE8les Keras. Voici une courte introduction \xE0 Keras."),rt.forEach(t),ws=c(e),q(ve.$$.fragment,e),zs=c(e),Ne=l(e,"P",{});var Ra=i(Ne);jt=n(Ra,"Cela signifie qu\u2019une fois que nous disposons de nos donn\xE9es, tr\xE8s peu de travail est n\xE9cessaire pour commencer \xE0 entra\xEEner sur celles-ci."),Ra.forEach(t),xs=c(e),q(_e.$$.fragment,e),Cs=c(e),ee=l(e,"P",{});var lt=i(ee);wt=n(lt,"Comme dans le [chapitre pr\xE9c\xE9dent] (/course/fr/chapter2), nous allons utiliser la classe "),He=l(lt,"CODE",{});var Ba=i(He);zt=n(Ba,"TFAutoModelForSequenceClassification"),Ba.forEach(t),xt=n(lt,", avec deux \xE9tiquettes :"),lt.forEach(t),ys=c(e),q(he.$$.fragment,e),Ps=c(e),se=l(e,"P",{});var it=i(se);Ct=n(it,"Vous remarquerez que, contrairement au "),Fe=l(it,"A",{href:!0});var Wa=i(Fe);yt=n(Wa,"Chapitre 2"),Wa.forEach(t),Pt=n(it,", vous obtenez un message d\u2019avertissement apr\xE8s l\u2019instanciation de ce mod\xE8le pr\xE9-entra\xEEn\xE9. Ceci est d\xFB au fait que BERT n\u2019a pas \xE9t\xE9 pr\xE9-entra\xEEn\xE9 \xE0 la classification de paires de phrases, donc la t\xEAte du mod\xE8le pr\xE9-entra\xEEn\xE9 a \xE9t\xE9 supprim\xE9e et une nouvelle t\xEAte adapt\xE9e \xE0 la classification de s\xE9quences a \xE9t\xE9 ins\xE9r\xE9e \xE0 la place. Les messages d\u2019avertissement indiquent que certains poids n\u2019ont pas \xE9t\xE9 utilis\xE9s (ceux correspondant \xE0 la t\xEAte de pr\xE9-entra\xEEnement abandonn\xE9e) et que d\u2019autres ont \xE9t\xE9 initialis\xE9s de mani\xE8re al\xE9atoire (ceux pour la nouvelle t\xEAte). Il conclut en vous encourageant \xE0 entra\xEEner le mod\xE8le, ce qui est exactement ce que nous allons faire maintenant."),it.forEach(t),As=c(e),M=l(e,"P",{});var R=i(M);At=n(R,"Pour "),Re=l(R,"EM",{});var Ya=i(Re);Tt=n(Ya,"finetuner"),Ya.forEach(t),St=n(R," le mod\xE8le sur notre jeu de donn\xE9es, nous devons simplement "),Be=l(R,"CODE",{});var Ja=i(Be);Dt=n(Ja,"compiler()"),Ja.forEach(t),Mt=n(R," notre mod\xE8le et ensuite passer nos donn\xE9es \xE0 la m\xE9thode "),We=l(R,"CODE",{});var Qa=i(We);Nt=n(Qa,"fit()"),Qa.forEach(t),Ft=n(R,". Cela va d\xE9marrer le processus de "),Ye=l(R,"EM",{});var Xa=i(Ye);Kt=n(Xa,"finetuning"),Xa.forEach(t),Lt=n(R," (qui devrait prendre quelques minutes sur un GPU) et rapporter la perte d\u2019entra\xEEnement au fur et \xE0 mesure, plus la perte de validation \xE0 la fin de chaque \xE9poque."),R.forEach(t),Ts=c(e),q(te.$$.fragment,e),Ss=c(e),q($e.$$.fragment,e),Ds=c(e),q(ae.$$.fragment,e),Ms=c(e),W=l(e,"H3",{class:!0});var ut=i(W);ne=l(ut,"A",{id:!0,class:!0,href:!0});var Za=i(ne);Je=l(Za,"SPAN",{});var en=i(Je);q(qe.$$.fragment,en),en.forEach(t),Za.forEach(t),Ut=c(ut),Qe=l(ut,"SPAN",{});var sn=i(Qe);Ot=n(sn,"Am\xE9liorer les performances d'entra\xEEnement"),sn.forEach(t),ut.forEach(t),Ns=c(e),q(be.$$.fragment,e),Fs=c(e),V=l(e,"P",{});var Ue=i(V);It=n(Ue,"Si vous essayez le code ci-dessus, il fonctionne certainement, mais vous constaterez que la perte ne diminue que lentement ou sporadiquement. La cause principale est le "),Xe=l(Ue,"EM",{});var tn=i(Xe);Gt=n(tn,"taux d\u2019apprentissage"),tn.forEach(t),Vt=n(Ue,". Comme pour la perte, lorsque nous transmettons \xE0 Keras le nom d\u2019un optimiseur sous forme de cha\xEEne de caract\xE8res, Keras initialise cet optimiseur avec des valeurs par d\xE9faut pour tous les param\xE8tres, y compris le taux d\u2019apprentissage. Cependant, nous savons depuis longtemps que les "),Ze=l(Ue,"EM",{});var an=i(Ze);Ht=n(an,"transformers"),an.forEach(t),Rt=n(Ue," b\xE9n\xE9ficient d\u2019un taux d\u2019apprentissage beaucoup plus faible que celui par d\xE9faut d\u2019Adam, qui est de 1e-3, \xE9galement \xE9crit comme 10 \xE0 la puissance -3, ou 0,001. 5e-5 (0,00005), qui est environ vingt fois inf\xE9rieur, est un bien meilleur point de d\xE9part."),Ue.forEach(t),Ks=c(e),D=l(e,"P",{});var L=i(D);Bt=n(L,"En plus de r\xE9duire le taux d\u2019apprentissage, nous avons une deuxi\xE8me astuce dans notre manche : nous pouvons r\xE9duire lentement le taux d\u2019apprentissage au cours de l\u2019entra\xEEnement. Dans la litt\xE9rature, on parle parfois de "),es=l(L,"EM",{});var nn=i(es);Wt=n(nn,"d\xE9croissance"),nn.forEach(t),Yt=n(L," ou d\u2019 "),ss=l(L,"EM",{});var on=i(ss);Jt=n(on,"annulation"),on.forEach(t),Qt=n(L," du taux d\u2019apprentissage.le taux d\u2019apprentissage. Dans Keras, la meilleure fa\xE7on de le faire est d\u2019utiliser un "),ts=l(L,"EM",{});var rn=i(ts);Xt=n(rn,"planificateur du taux d\u2019apprentissage"),rn.forEach(t),Zt=n(L,". Un bon planificateur \xE0 utiliser est "),as=l(L,"CODE",{});var ln=i(as);ea=n(ln,"PolynomialDecay"),ln.forEach(t),sa=n(L,". Malgr\xE9 son nom, avec les param\xE8tres par d\xE9faut, il diminue simplement de fa\xE7on lin\xE9aire le taux d\u2019apprentissage de la valeur initiale \xE0 la valeur finale au cours de l\u2019entra\xEEnement, ce qui est exactement ce que nous voulons. Afin d\u2019utiliser correctement un planificateur, nous devons lui dire combien de temps l\u2019entra\xEEnement va durer. Nous calculons cela comme "),ns=l(L,"CODE",{});var un=i(ns);ta=n(un,"num_train_steps"),un.forEach(t),aa=n(L," ci-dessous."),L.forEach(t),Ls=c(e),q(ge.$$.fragment,e),Us=c(e),q(oe.$$.fragment,e),Os=c(e),Ke=l(e,"P",{});var pn=i(Ke);na=n(pn,"Nous avons maintenant notre tout nouvel optimiseur et nous pouvons essayer de nous entra\xEEner avec lui. Tout d\u2019abord, rechargeons le mod\xE8le pour r\xE9initialiser les modifications apport\xE9es aux poids lors de l\u2019entra\xEEnement que nous venons d\u2019effectuer, puis nous pouvons le compiler avec le nouvel optimiseur :"),pn.forEach(t),Is=c(e),q(Ee.$$.fragment,e),Gs=c(e),re=l(e,"P",{});var pt=i(re);oa=n(pt,"Maintenant, on "),os=l(pt,"EM",{});var dn=i(os);ra=n(dn,"fit"),dn.forEach(t),la=n(pt," :"),pt.forEach(t),Vs=c(e),q(ke.$$.fragment,e),Hs=c(e),q(le.$$.fragment,e),Rs=c(e),Y=l(e,"H3",{class:!0});var dt=i(Y);ie=l(dt,"A",{id:!0,class:!0,href:!0});var cn=i(ie);rs=l(cn,"SPAN",{});var mn=i(rs);q(je.$$.fragment,mn),mn.forEach(t),cn.forEach(t),ia=c(dt),ls=l(dt,"SPAN",{});var fn=i(ls);ua=n(fn,"Pr\xE9dictions du mod\xE8le"),fn.forEach(t),dt.forEach(t),Bs=c(e),q(we.$$.fragment,e),Ws=c(e),H=l(e,"P",{});var Oe=i(H);pa=n(Oe,"Entra\xEEner et regarder la perte diminuer, c\u2019est tr\xE8s bien, mais que faire si l\u2019on veut r\xE9ellement obtenir des r\xE9sultats du mod\xE8le entra\xEEn\xE9, soit pour calculer des m\xE9triques, soit pour utiliser le mod\xE8le en production ? Pour ce faire, nous pouvons simplement utiliser la m\xE9thode "),is=l(Oe,"CODE",{});var vn=i(is);da=n(vn,"predict()"),vn.forEach(t),ca=n(Oe,". Ceci retournera les "),us=l(Oe,"EM",{});var _n=i(us);ma=n(_n,"logits"),_n.forEach(t),fa=n(Oe," de la t\xEAte de sortie du mod\xE8le, un par classe."),Oe.forEach(t),Ys=c(e),q(ze.$$.fragment,e),Js=c(e),ue=l(e,"P",{});var ct=i(ue);va=n(ct,"Nous pouvons convertir ces logits en pr\xE9dictions de classe du mod\xE8le en utilisant "),ps=l(ct,"CODE",{});var hn=i(ps);_a=n(hn,"argmax"),hn.forEach(t),ha=n(ct," pour trouver le logit le plus \xE9lev\xE9, qui correspond \xE0 la classe la plus probable :"),ct.forEach(t),Qs=c(e),q(xe.$$.fragment,e),Xs=c(e),q(Ce.$$.fragment,e),Zs=c(e),N=l(e,"P",{});var pe=i(N);$a=n(pe,"Maintenant, utilisons ces "),ds=l(pe,"CODE",{});var $n=i(ds);qa=n($n,"preds"),$n.forEach(t),ba=n(pe," pour calculer des m\xE9triques ! Nous pouvons charger les m\xE9triques associ\xE9es au jeu de donn\xE9es MRPC aussi facilement que nous avons charg\xE9 le jeu de donn\xE9es, cette fois avec la fonction "),cs=l(pe,"CODE",{});var qn=i(cs);ga=n(qn,"load_metric()"),qn.forEach(t),Ea=n(pe,". L\u2019objet retourn\xE9 a une m\xE9thode "),ms=l(pe,"CODE",{});var bn=i(ms);ka=n(bn,"compute()"),bn.forEach(t),ja=n(pe," que nous pouvons utiliser pour faire le calcul de la m\xE9trique :"),pe.forEach(t),et=c(e),q(ye.$$.fragment,e),st=c(e),q(Pe.$$.fragment,e),tt=c(e),F=l(e,"P",{});var de=i(F);wa=n(de,"Les r\xE9sultats exacts que vous obtiendrez peuvent varier, car l\u2019initialisation al\xE9atoire de la t\xEAte du mod\xE8le peut modifier les m\xE9triques obtenues. Ici, nous pouvons voir que notre mod\xE8le a une pr\xE9cision de 85,78% sur l\u2019ensemble de validation et un score F1 de 89,97. Ce sont les deux m\xE9triques utilis\xE9es pour \xE9valuer les r\xE9sultats sur le jeu de donn\xE9es MRPC pour le benchmark GLUE. Le tableau du papier de "),Ae=l(de,"A",{href:!0,rel:!0});var gn=i(Ae);za=n(gn,"BERT"),gn.forEach(t),xa=n(de," indique un score F1 de 88,9 pour le mod\xE8le de base. Il s\u2019agissait du mod\xE8le "),fs=l(de,"CODE",{});var En=i(fs);Ca=n(En,"uncased"),En.forEach(t),ya=n(de," alors que nous utilisons actuellement le mod\xE8le "),vs=l(de,"CODE",{});var kn=i(vs);Pa=n(kn,"cased"),kn.forEach(t),Aa=n(de,", ce qui explique le meilleur r\xE9sultat."),de.forEach(t),at=c(e),K=l(e,"P",{});var ce=i(K);Ta=n(ce,"Ceci conclut l\u2019introduction \xE0 le "),_s=l(ce,"EM",{});var jn=i(_s);Sa=n(jn,"finetuning"),jn.forEach(t),Da=n(ce," en utilisant l\u2019API Keras. Un exemple d\u2019application de cette m\xE9thode aux t\xE2ches les plus courantes du traitement automatique des langues sera pr\xE9sent\xE9 au "),Le=l(ce,"A",{href:!0});var wn=i(Le);Ma=n(wn,"Chapitre 7"),wn.forEach(t),Na=n(ce,". Si vous souhaitez affiner vos connaissances de l\u2019API Keras, essayez "),hs=l(ce,"EM",{});var zn=i(hs);Fa=n(zn,"finetuner"),zn.forEach(t),Ka=n(ce," un mod\xE8le sur le jeu de donn\xE9es GLUE SST-2, en utilisant le traitement des donn\xE9es que vous avez effectu\xE9 dans la section 2."),ce.forEach(t),this.h()},h(){j(p,"name","hf:doc:metadata"),j(p,"content",JSON.stringify(Ln)),j(f,"id","finetuner-un-modle-avec-keras"),j(f,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),j(f,"href","#finetuner-un-modle-avec-keras"),j(h,"class","relative group"),j(G,"href","https://colab.research.google.com/"),j(G,"rel","nofollow"),j(X,"id","entranement"),j(X,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),j(X,"href","#entranement"),j(B,"class","relative group"),j(Fe,"href","/course/fr/chapter2"),j(ne,"id","amliorer-les-performances-dentranement"),j(ne,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),j(ne,"href","#amliorer-les-performances-dentranement"),j(W,"class","relative group"),j(ie,"id","prdictions-du-modle"),j(ie,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),j(ie,"href","#prdictions-du-modle"),j(Y,"class","relative group"),j(Ae,"href","https://arxiv.org/pdf/1810.04805.pdf"),j(Ae,"rel","nofollow"),j(Le,"href","/course/fr/chapter7")},m(e,o){s(document.head,p),u(e,x,o),b(m,e,o),u(e,C,o),u(e,h,o),s(h,f),s(f,P),b(w,P,null),s(h,_),s(h,v),s(v,S),u(e,z,o),b(y,e,o),u(e,O,o),u(e,A,o),s(A,T),s(A,Q),s(Q,Se),s(A,De),s(A,G),s(G,_t),s(A,ht),u(e,gs,o),u(e,Me,o),s(Me,$t),u(e,Es,o),b(me,e,o),u(e,ks,o),u(e,B,o),s(B,X),s(X,Ie),b(fe,Ie,null),s(B,qt),s(B,Ge),s(Ge,bt),u(e,js,o),u(e,Z,o),s(Z,gt),s(Z,Ve),s(Ve,Et),s(Z,kt),u(e,ws,o),b(ve,e,o),u(e,zs,o),u(e,Ne,o),s(Ne,jt),u(e,xs,o),b(_e,e,o),u(e,Cs,o),u(e,ee,o),s(ee,wt),s(ee,He),s(He,zt),s(ee,xt),u(e,ys,o),b(he,e,o),u(e,Ps,o),u(e,se,o),s(se,Ct),s(se,Fe),s(Fe,yt),s(se,Pt),u(e,As,o),u(e,M,o),s(M,At),s(M,Re),s(Re,Tt),s(M,St),s(M,Be),s(Be,Dt),s(M,Mt),s(M,We),s(We,Nt),s(M,Ft),s(M,Ye),s(Ye,Kt),s(M,Lt),u(e,Ts,o),b(te,e,o),u(e,Ss,o),b($e,e,o),u(e,Ds,o),b(ae,e,o),u(e,Ms,o),u(e,W,o),s(W,ne),s(ne,Je),b(qe,Je,null),s(W,Ut),s(W,Qe),s(Qe,Ot),u(e,Ns,o),b(be,e,o),u(e,Fs,o),u(e,V,o),s(V,It),s(V,Xe),s(Xe,Gt),s(V,Vt),s(V,Ze),s(Ze,Ht),s(V,Rt),u(e,Ks,o),u(e,D,o),s(D,Bt),s(D,es),s(es,Wt),s(D,Yt),s(D,ss),s(ss,Jt),s(D,Qt),s(D,ts),s(ts,Xt),s(D,Zt),s(D,as),s(as,ea),s(D,sa),s(D,ns),s(ns,ta),s(D,aa),u(e,Ls,o),b(ge,e,o),u(e,Us,o),b(oe,e,o),u(e,Os,o),u(e,Ke,o),s(Ke,na),u(e,Is,o),b(Ee,e,o),u(e,Gs,o),u(e,re,o),s(re,oa),s(re,os),s(os,ra),s(re,la),u(e,Vs,o),b(ke,e,o),u(e,Hs,o),b(le,e,o),u(e,Rs,o),u(e,Y,o),s(Y,ie),s(ie,rs),b(je,rs,null),s(Y,ia),s(Y,ls),s(ls,ua),u(e,Bs,o),b(we,e,o),u(e,Ws,o),u(e,H,o),s(H,pa),s(H,is),s(is,da),s(H,ca),s(H,us),s(us,ma),s(H,fa),u(e,Ys,o),b(ze,e,o),u(e,Js,o),u(e,ue,o),s(ue,va),s(ue,ps),s(ps,_a),s(ue,ha),u(e,Qs,o),b(xe,e,o),u(e,Xs,o),b(Ce,e,o),u(e,Zs,o),u(e,N,o),s(N,$a),s(N,ds),s(ds,qa),s(N,ba),s(N,cs),s(cs,ga),s(N,Ea),s(N,ms),s(ms,ka),s(N,ja),u(e,et,o),b(ye,e,o),u(e,st,o),b(Pe,e,o),u(e,tt,o),u(e,F,o),s(F,wa),s(F,Ae),s(Ae,za),s(F,xa),s(F,fs),s(fs,Ca),s(F,ya),s(F,vs),s(vs,Pa),s(F,Aa),u(e,at,o),u(e,K,o),s(K,Ta),s(K,_s),s(_s,Sa),s(K,Da),s(K,Le),s(Le,Ma),s(K,Na),s(K,hs),s(hs,Fa),s(K,Ka),nt=!0},p(e,[o]){const Te={};o&1&&(Te.fw=e[0]),m.$set(Te);const $s={};o&2&&($s.$$scope={dirty:o,ctx:e}),te.$set($s);const qs={};o&2&&(qs.$$scope={dirty:o,ctx:e}),ae.$set(qs);const bs={};o&2&&(bs.$$scope={dirty:o,ctx:e}),oe.$set(bs);const J={};o&2&&(J.$$scope={dirty:o,ctx:e}),le.$set(J)},i(e){nt||(g(m.$$.fragment,e),g(w.$$.fragment,e),g(y.$$.fragment,e),g(me.$$.fragment,e),g(fe.$$.fragment,e),g(ve.$$.fragment,e),g(_e.$$.fragment,e),g(he.$$.fragment,e),g(te.$$.fragment,e),g($e.$$.fragment,e),g(ae.$$.fragment,e),g(qe.$$.fragment,e),g(be.$$.fragment,e),g(ge.$$.fragment,e),g(oe.$$.fragment,e),g(Ee.$$.fragment,e),g(ke.$$.fragment,e),g(le.$$.fragment,e),g(je.$$.fragment,e),g(we.$$.fragment,e),g(ze.$$.fragment,e),g(xe.$$.fragment,e),g(Ce.$$.fragment,e),g(ye.$$.fragment,e),g(Pe.$$.fragment,e),nt=!0)},o(e){E(m.$$.fragment,e),E(w.$$.fragment,e),E(y.$$.fragment,e),E(me.$$.fragment,e),E(fe.$$.fragment,e),E(ve.$$.fragment,e),E(_e.$$.fragment,e),E(he.$$.fragment,e),E(te.$$.fragment,e),E($e.$$.fragment,e),E(ae.$$.fragment,e),E(qe.$$.fragment,e),E(be.$$.fragment,e),E(ge.$$.fragment,e),E(oe.$$.fragment,e),E(Ee.$$.fragment,e),E(ke.$$.fragment,e),E(le.$$.fragment,e),E(je.$$.fragment,e),E(we.$$.fragment,e),E(ze.$$.fragment,e),E(xe.$$.fragment,e),E(Ce.$$.fragment,e),E(ye.$$.fragment,e),E(Pe.$$.fragment,e),nt=!1},d(e){t(p),e&&t(x),k(m,e),e&&t(C),e&&t(h),k(w),e&&t(z),k(y,e),e&&t(O),e&&t(A),e&&t(gs),e&&t(Me),e&&t(Es),k(me,e),e&&t(ks),e&&t(B),k(fe),e&&t(js),e&&t(Z),e&&t(ws),k(ve,e),e&&t(zs),e&&t(Ne),e&&t(xs),k(_e,e),e&&t(Cs),e&&t(ee),e&&t(ys),k(he,e),e&&t(Ps),e&&t(se),e&&t(As),e&&t(M),e&&t(Ts),k(te,e),e&&t(Ss),k($e,e),e&&t(Ds),k(ae,e),e&&t(Ms),e&&t(W),k(qe),e&&t(Ns),k(be,e),e&&t(Fs),e&&t(V),e&&t(Ks),e&&t(D),e&&t(Ls),k(ge,e),e&&t(Us),k(oe,e),e&&t(Os),e&&t(Ke),e&&t(Is),k(Ee,e),e&&t(Gs),e&&t(re),e&&t(Vs),k(ke,e),e&&t(Hs),k(le,e),e&&t(Rs),e&&t(Y),k(je),e&&t(Bs),k(we,e),e&&t(Ws),e&&t(H),e&&t(Ys),k(ze,e),e&&t(Js),e&&t(ue),e&&t(Qs),k(xe,e),e&&t(Xs),k(Ce,e),e&&t(Zs),e&&t(N),e&&t(et),k(ye,e),e&&t(st),k(Pe,e),e&&t(tt),e&&t(F),e&&t(at),e&&t(K)}}}const Ln={local:"finetuner-un-modle-avec-keras",sections:[{local:"entranement",title:"Entra\xEEnement"},{local:"amliorer-les-performances-dentranement",title:"Am\xE9liorer les performances d'entra\xEEnement"},{local:"prdictions-du-modle",title:"Pr\xE9dictions du mod\xE8le"}],title:"*Finetuner* un mod\xE8le avec Keras"};function Un(U,p,x){let m="pt";return An(()=>{const C=new URLSearchParams(window.location.search);x(0,m=C.get("fw")||"pt")}),[m]}class Wn extends xn{constructor(p){super();Cn(this,p,Un,Kn,yn,{})}}export{Wn as default,Ln as metadata};
