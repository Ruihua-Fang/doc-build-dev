import{S as xd,i as $d,s as wd,e as r,k as c,w as h,t as l,M as yd,c as n,d as s,m as p,a,x as v,h as o,b as d,N as m,F as t,g as u,y as g,L as Ld,q,o as _,B as E,v as Ad}from"../../chunks/vendor-1e8b365d.js";import{Y as ii}from"../../chunks/Youtube-c2a8cc39.js";import{I as N}from"../../chunks/IconCopyLink-483c28ba.js";function Id(ui){let M,cr,G,K,Wt,be,Xn,Zt,Qn,pr,ft,Wn,mr,R,ee,Kt,Te,Zn,es,Kn,fr,ht,ea,hr,j,Pe,di,ta,ke,ci,vr,xe,$e,sa,ra,gr,f,ts,te,ss,na,aa,we,la,oa,ia,rs,se,ns,ua,da,ye,ca,pa,ma,as,re,ls,fa,ha,Le,va,ga,qa,os,ne,is,_a,Ea,Ae,ba,Ta,Pa,us,T,ds,ka,xa,Ie,$a,wa,Ce,ya,La,Aa,cs,P,ps,Ia,Ca,Ne,Na,Ma,ms,Ga,Ra,qr,vt,ja,_r,k,Me,Sa,fs,Ba,Da,Oa,Ge,za,hs,Ua,Va,Ha,Re,Ya,vs,Fa,Ja,Er,gt,Xa,br,S,ae,gs,je,Qa,qs,Wa,Tr,le,Za,_s,Ka,el,Pr,oe,tl,Es,sl,rl,kr,x,nl,bs,al,ll,Ts,ol,il,xr,B,Se,pi,ul,Be,mi,$r,ie,dl,Ps,cl,pl,wr,D,De,fi,ml,Oe,hi,yr,O,ue,ks,ze,fl,xs,hl,Lr,qt,vl,Ar,Ue,Ve,vi,Ir,_t,gl,Cr,z,He,gi,ql,Ye,qi,Nr,Fe,Mr,Et,_l,Gr,bt,El,Rr,Tt,bl,jr,U,de,$s,Je,Tl,ws,Pl,Sr,Xe,Br,ce,kl,ys,xl,$l,Dr,V,Qe,_i,wl,We,Ei,Or,Pt,yl,zr,$,Ll,Ls,Al,Il,As,Cl,Nl,Ur,w,Is,Ml,Gl,Cs,Rl,jl,Ns,Sl,Vr,pe,Bl,Ms,Dl,Ol,Hr,H,Ze,bi,zl,Ke,Ti,Yr,kt,Ul,Fr,xt,Vl,Jr,Y,me,Gs,et,Hl,Rs,Yl,Xr,$t,Fl,Qr,tt,Wr,F,fe,js,st,Jl,Ss,Xl,Zr,wt,Ql,Kr,he,yt,Bs,Wl,Zl,Kl,Lt,Ds,eo,to,en,J,rt,Pi,so,nt,ki,tn,At,ro,sn,y,It,Os,no,ao,lo,Ct,zs,oo,io,uo,ve,Us,co,po,Vs,mo,fo,rn,Nt,ho,nn,X,ge,Hs,at,vo,Ys,go,an,L,qo,Fs,_o,Eo,lt,bo,To,ln,Mt,Po,on,Gt,ko,un,Rt,xo,dn,Q,qe,Js,ot,$o,Xs,wo,cn,jt,yo,pn,St,Lo,mn,Bt,Ao,fn,W,it,xi,Io,ut,$i,hn,Dt,Co,vn,_e,No,Qs,Mo,Go,gn,Z,Ee,Ws,dt,Ro,Zs,jo,qn,A,Ot,Ks,So,Bo,Do,zt,er,Oo,zo,Uo,I,tr,Vo,Ho,sr,Yo,Fo,rr,Jo,Xo,_n,C,Qo,nr,Wo,Zo,ar,Ko,ei,En;return be=new N({}),Te=new N({}),je=new N({}),ze=new N({}),Fe=new ii({props:{id:"ftWlj4FBHTg"}}),Je=new N({}),Xe=new ii({props:{id:"BqqfQnyjmgg"}}),et=new N({}),tt=new ii({props:{id:"H39Z_720T5s"}}),st=new N({}),at=new N({}),ot=new N({}),dt=new N({}),{c(){M=r("meta"),cr=c(),G=r("h1"),K=r("a"),Wt=r("span"),h(be.$$.fragment),Xn=c(),Zt=r("span"),Qn=l("Comment fonctionnent les mod\xE8les Transformers ?"),pr=c(),ft=r("p"),Wn=l("Dans cette partie, nous allons jeter un coup d\u2019oeil \xE0 l\u2019architecture des mod\xE8les Transformers."),mr=c(),R=r("h2"),ee=r("a"),Kt=r("span"),h(Te.$$.fragment),Zn=c(),es=r("span"),Kn=l("Court historique des Transformers"),fr=c(),ht=r("p"),ea=l("Voici quelques dates clefs dans la courte histoire des mod\xE8les Transformers :"),hr=c(),j=r("div"),Pe=r("img"),ta=c(),ke=r("img"),vr=c(),xe=r("p"),$e=r("a"),sa=l("L\u2019architecture Transformer"),ra=l(" a \xE9t\xE9 pr\xE9sent\xE9e en Juin 2017. La recherche initiale portait sur les t\xE2ches de traduction. Cela s\u2019est suivi par la pr\xE9sentation de plusieurs mod\xE8les influents, dont :"),gr=c(),f=r("ul"),ts=r("li"),te=r("p"),ss=r("strong"),na=l("Juin 2018"),aa=l(": "),we=r("a"),la=l("GPT"),oa=l(", le premier mod\xE8le Transformer pr\xE9-entra\xEEn\xE9, utilis\xE9 pour fine-tuning sur diff\xE9rentes t\xE2ches de NLP et ayant obtenu des r\xE9sultats \xE0 l\u2019\xE9tat de l\u2019art"),ia=c(),rs=r("li"),se=r("p"),ns=r("strong"),ua=l("Octobre 2018"),da=l(": "),ye=r("a"),ca=l("BERT"),pa=l(", autre mod\xE8le large pr\xE9-entra\xEEn\xE9, ayant \xE9t\xE9 construit pour produire de meilleurs r\xE9sum\xE9s de texte (plus de d\xE9tails dans le chapitre suivant !)"),ma=c(),as=r("li"),re=r("p"),ls=r("strong"),fa=l("F\xE9vrier 2019"),ha=l(": "),Le=r("a"),va=l("GPT-2"),ga=l(", une version am\xE9lior\xE9e (et plus grande) de GPT qui n\u2019a pas \xE9t\xE9 directement rendu publique pour cause de raisons \xE9thiques"),qa=c(),os=r("li"),ne=r("p"),is=r("strong"),_a=l("Octobre 2019"),Ea=l(": "),Ae=r("a"),ba=l("DistilBERT"),Ta=l(", une version distill\xE9e de BERT \xE9tant 60% plus rapide, 40% plus l\xE9g\xE8re en m\xE9moire, et conservant tout de m\xEAme 97% performances initiales de BERT"),Pa=c(),us=r("li"),T=r("p"),ds=r("strong"),ka=l("Octobre 2019"),xa=l(": "),Ie=r("a"),$a=l("BART"),wa=l(" et "),Ce=r("a"),ya=l("T5"),La=l(", deux mod\xE8les larges pr\xE9-entra\xEEn\xE9s utilisant la m\xEAme architecture que le mod\xE8le Transformer originel (les premiers \xE0 faire cela)"),Aa=c(),cs=r("li"),P=r("p"),ps=r("strong"),Ia=l("Mai 2020"),Ca=l(", "),Ne=r("a"),Na=l("GPT-3"),Ma=l(", une version encore plus grande que GPT-2 ayant des performances tr\xE8s bonnes sur une vari\xE9t\xE9 de t\xE2ches ne n\xE9cessitant pas de fine-tuning (appell\xE9 "),ms=r("em"),Ga=l("zero-shot learning"),Ra=l(")"),qr=c(),vt=r("p"),ja=l("Cette liste est loin d\u2019\xEAtre exhaustive, et permet juste de mettre en lumi\xE8re certains mod\xE8les Transformers. Plus largement, ces mod\xE8les peuvent \xEAtre regroup\xE9s en trois cat\xE9gories :"),_r=c(),k=r("ul"),Me=r("li"),Sa=l("GPT-like (aussi appel\xE9 mod\xE8les Transformers "),fs=r("em"),Ba=l("auto-regressif"),Da=l(")"),Oa=c(),Ge=r("li"),za=l("BERT-like (aussi appel\xE9 mod\xE8les Transformers "),hs=r("em"),Ua=l("auto-encodeur"),Va=l(")"),Ha=c(),Re=r("li"),Ya=l("BART/T5-like (aussi appel\xE9 mod\xE8les Transformers "),vs=r("em"),Fa=l("s\xE9quence-\xE0-s\xE9quence"),Ja=l(")"),Er=c(),gt=r("p"),Xa=l("Nous verrons plus en profondeur ces familles de mod\xE8les plus tard."),br=c(),S=r("h2"),ae=r("a"),gs=r("span"),h(je.$$.fragment),Qa=c(),qs=r("span"),Wa=l("Transformers, des mod\xE8les du langage"),Tr=c(),le=r("p"),Za=l("Tous les mod\xE8les Transformers mentionn\xE9 ci-dessus (GPT, BERT, BART, T5, etc.) ont \xE9t\xE9 entra\xEEn\xE9s comme des "),_s=r("em"),Ka=l("mod\xE8les du langage"),el=l(". Cela signifie qu\u2019ils ont \xE9t\xE9 entra\xEEn\xE9s sur une large quantit\xE9 de textes bruts en mode auto-supervis\xE9. L\u2019apprentissage auto-supervis\xE9 est un type d\u2019entra\xEEnement o\xF9 l\u2019objectif est automatiquement d\xE9termin\xE9 par les entr\xE9es du mod\xE8le. Cela veut dire qu\u2019une labellisation des donn\xE9es par des humains n\u2019est pas n\xE9cessaire !"),Pr=c(),oe=r("p"),tl=l("Ce type de mod\xE8le d\xE9veloppe une compr\xE9hension statistique du langage sur lequel il est entra\xEEn\xE9, mais cela n\u2019est pas adapt\xE9 \xE0 une t\xE2che sp\xE9cifique. A cause de cela, le mod\xE8le pr\xE9-entra\xEEn\xE9 doit ensuite suivre une proc\xE9dure de "),Es=r("em"),sl=l("transfer learning"),rl=l(". Durant cette proc\xE9dure, le mod\xE8le est fine-tun\xE9 en mode supervis\xE9 \u2014 en utilisant des donn\xE9es labellis\xE9es par un human \u2014 sur une t\xE2che donn\xE9e."),kr=c(),x=r("p"),nl=l("Un exemple de t\xE2che est la pr\xE9diction du prochain mot de la phrase en ayant lu les "),bs=r("em"),al=l("n"),ll=l(" mots pr\xE9c\xE9dents. Ce proc\xE9d\xE9 s\u2019appelle "),Ts=r("em"),ol=l("mod\xE9lisation du langage causal"),il=l(" car les pr\xE9dictions d\xE9pendent des entr\xE9es pr\xE9c\xE9dentes et actuelles, mais pas des suivantes."),xr=c(),B=r("div"),Se=r("img"),ul=c(),Be=r("img"),$r=c(),ie=r("p"),dl=l("Un autre exemple est la "),Ps=r("em"),cl=l("mod\xE9lisation du langage contextuel"),pl=l(", o\xF9 me mod\xE8le doit pr\xE9dire un mot masqu\xE9 dans une phrase."),wr=c(),D=r("div"),De=r("img"),ml=c(),Oe=r("img"),yr=c(),O=r("h2"),ue=r("a"),ks=r("span"),h(ze.$$.fragment),fl=c(),xs=r("span"),hl=l("Les mod\xE8les Transformers sont \xE9normes"),Lr=c(),qt=r("p"),vl=l("En dehors de quelques exceptions (comme DistilBERT), la strat\xE9gie g\xE9n\xE9rale pour obtenir de meilleure performance consiste \xE0 augmenter la taille des mod\xE8les ainsi que la quantit\xE9 de donn\xE9es utilis\xE9es pour l\u2019entra\xEEnement de ces derniers."),Ar=c(),Ue=r("div"),Ve=r("img"),Ir=c(),_t=r("p"),gl=l("Malheureusement, entra\xEEner un mod\xE8le, et particuli\xE8rement un mod\xE8le large, n\xE9cessite une importante quantit\xE9 de donn\xE9es. Cela devient tr\xE8s co\xFBteux en terme de temps et de ressources de calcul. Cela se traduit m\xEAme par un impact environnemental, comme le montre le graphique suivant."),Cr=c(),z=r("div"),He=r("img"),ql=c(),Ye=r("img"),Nr=c(),h(Fe.$$.fragment),Mr=c(),Et=r("p"),_l=l("Et cela pr\xE9sente un projet pour un (gigantesque) mod\xE8le men\xE9 par une \xE9quipe qui essaye consciemment de r\xE9duire l\u2019impact environnemental du pr\xE9-entra\xEEnement. L\u2019empreinte environnemental serait encore plus cons\xE9quente s\u2019il fallait lancer beaucoup d\u2019entra\xEEnements pour obtenir les meilleurs hyper-param\xE8tres."),Gr=c(),bt=r("p"),El=l("Imaginez si \xE0 chaque fois qu\u2019une \xE9quipe de recherche, qu\u2019une organisation \xE9tudiante ou qu\u2019une entreprise voulait entra\xEEner un mod\xE8le, ils devaient repartir de z\xE9ro. Ceci menerait \xE0 des \xE9normes et non-n\xE9cessaires co\xFBts environnementaux."),Rr=c(),Tt=r("p"),bl=l("C\u2019est pourquoi le partage des mod\xE8les du langage est primordial : partager les poids d\u2019entra\xEEnement et construire \xE0 partir de ces poids permet de r\xE9duire les co\xFBts de calcul globaux ainsi que l\u2019empreinte carbone de toute la communaut\xE9."),jr=c(),U=r("h2"),de=r("a"),$s=r("span"),h(Je.$$.fragment),Tl=c(),ws=r("span"),Pl=l("Le Transfer Learning"),Sr=c(),h(Xe.$$.fragment),Br=c(),ce=r("p"),kl=l("Le "),ys=r("em"),xl=l("Pr\xE9-entra\xEEnement"),$l=l(" est le fait d\u2019entra\xEEner un mod\xE8le de z\xE9ro : les poids sont initialis\xE9s de mani\xE8re al\xE9atoire, et l\u2019entra\xEEnement d\xE9bute sans aucune connaissance pr\xE9alable (ou heuristique)."),Dr=c(),V=r("div"),Qe=r("img"),wl=c(),We=r("img"),Or=c(),Pt=r("p"),yl=l("Ce pr\xE9-entra\xEEnement est habituellement r\xE9alis\xE9 sur de grande quantit\xE9 de donn\xE9es. Il n\xE9cessite donc un tr\xE8s grand corpus de donn\xE9es, et l\u2019entra\xEEnement peut prendre jusqu\u2019\xE0 plusieurs semaines."),zr=c(),$=r("p"),Ll=l("Le "),Ls=r("em"),Al=l("Fine-tuning"),Il=l(", d\u2019un autre c\xF4t\xE9, est un entra\xEEnement effectu\xE9 "),As=r("strong"),Cl=l("apr\xE8s"),Nl=l(" qu\u2019un mod\xE8le ait \xE9t\xE9 pr\xE9-entra\xEEn\xE9. Pour r\xE9aliser le fine-tuning, il faut d\u2019abord r\xE9cup\xE8rer un mod\xE8le du langage pr\xE9-entra\xEEn\xE9, puis lanc\xE9 un entra\xEEnement sur un jeu de donn\xE9es adapt\xE9 \xE0 la t\xE2che cible. Mais attendez \u2014 pourquoi ne pas simplement entra\xEEn\xE9 un mod\xE8le sur la t\xE2che cible ? Voici plusieurs raisons :"),Ur=c(),w=r("ul"),Is=r("li"),Ml=l("Le mod\xE8le pr\xE9-entra\xEEn\xE9 a d\xE9j\xE0 \xE9t\xE9 entra\xEEn\xE9 sur un jeu de donn\xE9es qui a des similarit\xE9s avec le jeu de donn\xE9es utilis\xE9 pour le fine-tuning. Ainsi le processus de fine-tuning peut b\xE9n\xE9ficier du savoir acquis par le mod\xE8le initial lors du pr\xE9-entra\xEEnement (par exemple, avec des probl\xE8mes de NLP, le mod\xE8le pr\xE9-entra\xEEn\xE9 aura une sorte de compr\xE9hension statistique de la langue associ\xE9e \xE0 la t\xE2che cible)."),Gl=c(),Cs=r("li"),Rl=l("Puisque le mod\xE8le pr\xE9-entra\xEEn\xE9 a d\xE9j\xE0 \xE9t\xE9 entra\xEEn\xE9 sur beaucoup de donn\xE9es, le fine-tuning n\xE9cessite beaucoup moins de donn\xE9es pour obtenir des r\xE9sultats d\xE9cents."),jl=c(),Ns=r("li"),Sl=l("Pour la m\xEAme raison, la quantit\xE9 de temps et de ressources n\xE9cessaires pour obtenir de bons r\xE9sultats est beaucoup moins grande."),Vr=c(),pe=r("p"),Bl=l("Par exemple, il serait possible de tirer prodit d\u2019un mod\xE8le pr\xE9-entra\xEEn\xE9 sur la langue anglaise and ensuite fine-tun\xE9 sur un corpus arXiv, permettant d\u2019obtenir un mod\xE8le bas\xE9 sur la recherche et la science. Le fine-tuning ne n\xE9cessiterait qu\u2019une quantit\xE9 r\xE9duite de donn\xE9es : le savoir acquis par le mod\xE8le pr\xE9-entra\xEEn\xE9 est transf\xE9r\xE9 au nouveau mod\xE8le, d\u2019o\xF9 le terme de "),Ms=r("em"),Dl=l("transfer learning"),Ol=l("."),Hr=c(),H=r("div"),Ze=r("img"),zl=c(),Ke=r("img"),Yr=c(),kt=r("p"),Ul=l("Le fine-tuning d\u2019un mod\xE8le n\xE9cessite ainsi moins de temps, de donn\xE9es, d\u2019argent et a un impact environnemental r\xE9duit. Il est \xE9galement possible d\u2019it\xE9rer plus rapidement sur diff\xE9rentes strat\xE9gies de fine-tuning, puisque l\u2019entra\xEEnement est moins contraignant qu\u2019un entra\xEEnement complet."),Fr=c(),xt=r("p"),Vl=l("Ce proc\xE9d\xE9 permet \xE9galement d\u2019obtenir de meilleurs r\xE9sultats qu\u2019un entra\xEEnement de z\xE9ro (sauf si on dispose de beaucoup de donn\xE9es), ce qui explique pourquoi il faut toujours essayer de tirer profit d\u2019un mod\xE8le pr\xE9-entra\xEEn\xE9 \u2014 un qui est le plus proche de la t\xE2che cible \u2014 et de le fine-tuner."),Jr=c(),Y=r("h2"),me=r("a"),Gs=r("span"),h(et.$$.fragment),Hl=c(),Rs=r("span"),Yl=l("Architecture g\xE9n\xE9rale"),Xr=c(),$t=r("p"),Fl=l("Dans cette section, nous allons voir l\u2019architecture g\xE9n\xE9rale des mod\xE8les Transformers. Pas d\u2019inqui\xE9tudes si vous ne comprenez pas tous les concepts; il y a des sections d\xE9taill\xE9es qui couvrent chaque composants plus tard."),Qr=c(),h(tt.$$.fragment),Wr=c(),F=r("h2"),fe=r("a"),js=r("span"),h(st.$$.fragment),Jl=c(),Ss=r("span"),Xl=l("Introduction"),Zr=c(),wt=r("p"),Ql=l("Le mod\xE8le est principalement compos\xE9 de deux blocs :"),Kr=c(),he=r("ul"),yt=r("li"),Bs=r("strong"),Wl=l("Encodeur (gauche)"),Zl=l(": L\u2019encodeur re\xE7oit une entr\xE9e et contruit une r\xE9pr\xE9sentation de celle-ci (ses caract\xE9ristiques). Cela signifie que le mod\xE8le est optimis\xE9 pour acqu\xE9rir une compr\xE9hension venant de ces entr\xE9es."),Kl=c(),Lt=r("li"),Ds=r("strong"),eo=l("D\xE9codeur (droite)"),to=l(": Le d\xE9codeur utilise la r\xE9pr\xE9sentation de l\u2019encodeur (caract\xE9ristiques) en plus des autres entr\xE9es pour g\xE9n\xE9rer une s\xE9quence cible. Cela signifie que le mod\xE8le est optimis\xE9 pour g\xE9n\xE9rer des sorties."),en=c(),J=r("div"),rt=r("img"),so=c(),nt=r("img"),tn=c(),At=r("p"),ro=l("Chacune de ces parties peuvent \xEAtre utilis\xE9es ind\xE9pendamment, en fonction de la t\xE2che :"),sn=c(),y=r("ul"),It=r("li"),Os=r("strong"),no=l("Mod\xE8les uniquement encodeurs"),ao=l(" : Adapt\xE9s pour des t\xE2ches qui n\xE9cessitent une compr\xE9hension de l\u2019entr\xE9e, comme la classification de phrases et la reconnaissance d\u2019entit\xE9s nomm\xE9es."),lo=c(),Ct=r("li"),zs=r("strong"),oo=l("Mod\xE8les uniquement d\xE9codeurs"),io=l(" : Adapt\xE9s pour les t\xE2ches g\xE9n\xE9ratives telles que la g\xE9n\xE9ration de texte."),uo=c(),ve=r("li"),Us=r("strong"),co=l("Mod\xE8les encodeurs-d\xE9codeurs"),po=l(" ou "),Vs=r("strong"),mo=l("mod\xE8les de s\xE9quence-\xE0-s\xE9quence"),fo=l(" : Adapt\xE9s aux t\xE2ches g\xE9n\xE9ratives qui n\xE9cessitent une entr\xE9e, telles que la traduction ou le r\xE9sum\xE9 de texte."),rn=c(),Nt=r("p"),ho=l("Nous verrons plus en d\xE9tails chacune de ces architectures plus tard."),nn=c(),X=r("h2"),ge=r("a"),Hs=r("span"),h(at.$$.fragment),vo=c(),Ys=r("span"),go=l("Les couches d'attention"),an=c(),L=r("p"),qo=l("Une caract\xE9ristique clef des mod\xE8les Transformers est qu\u2019ils sont construits avec des couches sp\xE9ciales appell\xE9es "),Fs=r("em"),_o=l("couches d\u2019attention"),Eo=l(". En fait, le titre de la publication qui a pr\xE9sent\xE9 l\u2019architecture Transformer est "),lt=r("a"),bo=l("\u201CAttention Is All You Need\u201D"),To=l(" ! Nous allons explorer les d\xE9tails des couches d\u2019attention plus tard dans ce cours; pour le moment, ce qu\u2019il faut retenir est que la couche d\u2019attention va indiquer au mod\xE8le les mots, de la phrase d\u2019entr\xE9e, sur lesquels porter son attention (et plus ou moins ignorer les autres) lorsqu\u2019il s\u2019agit de construire une repr\xE9sentation de chaque mot."),ln=c(),Mt=r("p"),Po=l("Pour mettre ceci en contexte, consid\xE9rons la t\xE2che de traduction de l\u2019anglais vers le fran\xE7ais. \xC9tant donn\xE9 l\u2019entr\xE9e \u201CYou like this course\u201D, un mod\xE8le de traduction devra \xE9galement s\u2019occuper du mot adjacent \u201CYou\u201D pour obtenir la traduction correcte du mot \u201Clike\u201D, car en fran\xE7ais le verbe \u201Caimer\u201D est conjugu\xE9 diff\xE9remment selon l\u2019objet. Le reste de la phrase, cependant, n\u2019est pas utile pour la traduction de ce mot. De m\xEAme, lors de la traduction de \u201Cthis\u201D, le mod\xE8le devra \xE9galement faire attention au mot \u201Ccourse\u201D, car \u201Cthis\u201D se traduit diff\xE9remment selon que le nom associ\xE9 est masculin ou f\xE9minin. Encore une fois, les autres mots de la phrase n\u2019auront pas d\u2019importance pour la traduction de \u201Cthis\u201D. Avec des phrases plus complexes (et des r\xE8gles de grammaire plus complexes), le mod\xE8le devrait accorder une attention particuli\xE8re aux mots qui pourraient appara\xEEtre plus loin dans la phrase pour traduire correctement chaque mot."),on=c(),Gt=r("p"),ko=l("Le m\xEAme concept s\u2019applique \xE0 n\u2019importe quelle t\xE2che associ\xE9e au langage naturel : un mot seul est porteur de sens, mais ce sens est profond\xE9ment affect\xE9 par le contexte, qui peut \xEAtre un ou plusieurs mots se situant avant ou apr\xE8s le mot \xE9tudi\xE9."),un=c(),Rt=r("p"),xo=l("Maintenant que vous avez une id\xE9e plus pr\xE9cise des couches d\u2019attentions, nous allons regarder de plus pr\xE8s l\u2019architecture des mod\xE8les Transformers."),dn=c(),Q=r("h2"),qe=r("a"),Js=r("span"),h(ot.$$.fragment),$o=c(),Xs=r("span"),wo=l("L'architecture originelle"),cn=c(),jt=r("p"),yo=l("L\u2019architecture Transformer a initialement \xE9t\xE9 construite pour des t\xE2ches de traduction. Pendant l\u2019entra\xEEnement, l\u2019encodeur re\xE7oit des entr\xE9es (des phrases) dans une certaine langue, tandis que le d\xE9codeur re\xE7oit la m\xEAme phrase traduite dans la langue cible. Pour l\u2019encodeur, les couches d\u2019attention peuvent utiliser tous les mots d\u2019une phrase (puisque, comme nous venons de le voir, la traduction d\u2019un mot donn\xE9 peut d\xE9pendre de ce qui le suit ou le pr\xE9c\xE8de dans la phrase). Le d\xE9codeur, quant \xE0 lui, fonctionne de fa\xE7on s\xE9quentielle et ne peut porter son attention qu\u2019aux mots d\xE9j\xE0 traduits dans la phrase (donc, uniquement les mots g\xE9n\xE9r\xE9s avant le mot en cours). Par exemple, lorsqu\u2019on a pr\xE9dit les trois premiers mots de la phrase cible, on les donne au d\xE9codeur qui utilise alors toutes les entr\xE9es de l\u2019encodeur pour essayer de pr\xE9dire le quatri\xE8me mot."),pn=c(),St=r("p"),Lo=l("Pour acc\xE9l\xE9rer les choses pendant l\u2019apprentissage (lorsque le mod\xE8le a acc\xE8s aux phrases cibles), le d\xE9codeur est aliment\xE9 avec la cible enti\xE8re, mais il n\u2019est pas autoris\xE9 \xE0 utiliser les mots futurs (s\u2019il avait acc\xE8s au mot en position 2 lorsqu\u2019il essayait de pr\xE9dire le mot en position 2, le probl\xE8me ne serait pas tr\xE8s difficile !). Par exemple, en essayant de pr\xE9dire le quatri\xE8me mot, la couche d\u2019attention n\u2019aura acc\xE8s qu\u2019aux mots des positions 1 \xE0 3."),mn=c(),Bt=r("p"),Ao=l("L\u2019architecture originale des mod\xE8les Transformers ressemblait \xE0 ceci, avec l\u2019encodeur \xE0 gauche et le d\xE9codeur \xE0 droite :"),fn=c(),W=r("div"),it=r("img"),Io=c(),ut=r("img"),hn=c(),Dt=r("p"),Co=l("Notez que la premi\xE8re couche d\u2019attention dans un bloc d\xE9codeur porte sont attention \xE0 toutes les entr\xE9es (pass\xE9es) du d\xE9codeur, mais la seconde couche d\u2019attention utilise la sortie de l\u2019encodeur. Il peut ainsi acc\xE9der \xE0 toute la phrase d\u2019entr\xE9e pour pr\xE9dire au mieux le mot actuel. Ceci est tr\xE8s utile puisque les diff\xE9rentes langues peuvent avoir des r\xE8gles grammaticales qui placent les mots dans des ordres diff\xE9rents, ou un contexte fourni plus tard dans la phrase peut \xEAtre utile pour d\xE9terminer la meilleure traduction d\u2019un mot donn\xE9."),vn=c(),_e=r("p"),No=l("Le "),Qs=r("em"),Mo=l("masque d\u2019attention"),Go=l(" peut \xE9galement \xEAtre utilis\xE9 dans un mod\xE8le encodeur/d\xE9codeur pour l\u2019emp\xEAcher de porter son attention sur certains mots sp\xE9ciaux \u2014 par exemple, le motde remplissage sp\xE9cial utilis\xE9 pour que toutes les entr\xE9es aient la m\xEAme longueur lors du regroupement de phrases."),gn=c(),Z=r("h2"),Ee=r("a"),Ws=r("span"),h(dt.$$.fragment),Ro=c(),Zs=r("span"),jo=l("Architectures contre checkpoints"),qn=l(`

 
En approfondissant l'\xE9tude des mod\xE8les Transformers dans ce cours, vous verrez des mentions d'*architectures* et de *checkpoints* ainsi que de *mod\xE8les*. Ces termes ont tous des significations l\xE9g\xE8rement diff\xE9rentes :
`),A=r("ul"),Ot=r("li"),Ks=r("strong"),So=l("Architecture"),Bo=l(": C\u2019est le squelette du mod\xE8le \u2014 la d\xE9finition de chaque couche et chaque op\xE9ration qui se produit au sein du mod\xE8le."),Do=c(),zt=r("li"),er=r("strong"),Oo=l("Checkpoints"),zo=l(": Ce sont les poids qui seront charg\xE9s dans une architecture donn\xE9e."),Uo=c(),I=r("li"),tr=r("strong"),Vo=l("Model"),Ho=l(": C\u2019est un mot valise n\u2019\xE9tant pas aussi pr\xE9cis que les mots \u201Carchitecture\u201D ou \u201Ccheckpoint\u201D: il peut d\xE9signer l\u2019un comme l\u2019autre. Dans ce cours, il sera sp\xE9cifi\xE9 "),sr=r("em"),Yo=l("architecture"),Fo=l(" ou "),rr=r("em"),Jo=l("checkpoint"),Xo=l(" lorsqu\u2019il est n\xE9cessaire de r\xE9duire l\u2019ambiguit\xE9."),_n=c(),C=r("p"),Qo=l("Par exemple, BERT est une architecture alors que "),nr=r("code"),Wo=l("bert-base-cased"),Zo=l(", un ensemble de poids entra\xEEn\xE9 par l\u2019\xE9quipe de Google lors de la premi\xE8re sortie de BERT, est un checkpoint. Cependant, il est possible de dire \u201Cle mod\xE8le BERT\u201D et \u201Cle mod\xE8le "),ar=r("code"),Ko=l("bert-base-cased"),ei=l("\u201C."),this.h()},l(e){const i=yd('[data-svelte="svelte-1phssyn"]',document.head);M=n(i,"META",{name:!0,content:!0}),i.forEach(s),cr=p(e),G=n(e,"H1",{class:!0});var bn=a(G);K=n(bn,"A",{id:!0,class:!0,href:!0});var wi=a(K);Wt=n(wi,"SPAN",{});var yi=a(Wt);v(be.$$.fragment,yi),yi.forEach(s),wi.forEach(s),Xn=p(bn),Zt=n(bn,"SPAN",{});var Li=a(Zt);Qn=o(Li,"Comment fonctionnent les mod\xE8les Transformers ?"),Li.forEach(s),bn.forEach(s),pr=p(e),ft=n(e,"P",{});var Ai=a(ft);Wn=o(Ai,"Dans cette partie, nous allons jeter un coup d\u2019oeil \xE0 l\u2019architecture des mod\xE8les Transformers."),Ai.forEach(s),mr=p(e),R=n(e,"H2",{class:!0});var Tn=a(R);ee=n(Tn,"A",{id:!0,class:!0,href:!0});var Ii=a(ee);Kt=n(Ii,"SPAN",{});var Ci=a(Kt);v(Te.$$.fragment,Ci),Ci.forEach(s),Ii.forEach(s),Zn=p(Tn),es=n(Tn,"SPAN",{});var Ni=a(es);Kn=o(Ni,"Court historique des Transformers"),Ni.forEach(s),Tn.forEach(s),fr=p(e),ht=n(e,"P",{});var Mi=a(ht);ea=o(Mi,"Voici quelques dates clefs dans la courte histoire des mod\xE8les Transformers :"),Mi.forEach(s),hr=p(e),j=n(e,"DIV",{class:!0});var Pn=a(j);Pe=n(Pn,"IMG",{class:!0,src:!0,alt:!0}),ta=p(Pn),ke=n(Pn,"IMG",{class:!0,src:!0,alt:!0}),Pn.forEach(s),vr=p(e),xe=n(e,"P",{});var ti=a(xe);$e=n(ti,"A",{href:!0,rel:!0});var Gi=a($e);sa=o(Gi,"L\u2019architecture Transformer"),Gi.forEach(s),ra=o(ti," a \xE9t\xE9 pr\xE9sent\xE9e en Juin 2017. La recherche initiale portait sur les t\xE2ches de traduction. Cela s\u2019est suivi par la pr\xE9sentation de plusieurs mod\xE8les influents, dont :"),ti.forEach(s),gr=p(e),f=n(e,"UL",{});var b=a(f);ts=n(b,"LI",{});var Ri=a(ts);te=n(Ri,"P",{});var lr=a(te);ss=n(lr,"STRONG",{});var ji=a(ss);na=o(ji,"Juin 2018"),ji.forEach(s),aa=o(lr,": "),we=n(lr,"A",{href:!0,rel:!0});var Si=a(we);la=o(Si,"GPT"),Si.forEach(s),oa=o(lr,", le premier mod\xE8le Transformer pr\xE9-entra\xEEn\xE9, utilis\xE9 pour fine-tuning sur diff\xE9rentes t\xE2ches de NLP et ayant obtenu des r\xE9sultats \xE0 l\u2019\xE9tat de l\u2019art"),lr.forEach(s),Ri.forEach(s),ia=p(b),rs=n(b,"LI",{});var Bi=a(rs);se=n(Bi,"P",{});var or=a(se);ns=n(or,"STRONG",{});var Di=a(ns);ua=o(Di,"Octobre 2018"),Di.forEach(s),da=o(or,": "),ye=n(or,"A",{href:!0,rel:!0});var Oi=a(ye);ca=o(Oi,"BERT"),Oi.forEach(s),pa=o(or,", autre mod\xE8le large pr\xE9-entra\xEEn\xE9, ayant \xE9t\xE9 construit pour produire de meilleurs r\xE9sum\xE9s de texte (plus de d\xE9tails dans le chapitre suivant !)"),or.forEach(s),Bi.forEach(s),ma=p(b),as=n(b,"LI",{});var zi=a(as);re=n(zi,"P",{});var ir=a(re);ls=n(ir,"STRONG",{});var Ui=a(ls);fa=o(Ui,"F\xE9vrier 2019"),Ui.forEach(s),ha=o(ir,": "),Le=n(ir,"A",{href:!0,rel:!0});var Vi=a(Le);va=o(Vi,"GPT-2"),Vi.forEach(s),ga=o(ir,", une version am\xE9lior\xE9e (et plus grande) de GPT qui n\u2019a pas \xE9t\xE9 directement rendu publique pour cause de raisons \xE9thiques"),ir.forEach(s),zi.forEach(s),qa=p(b),os=n(b,"LI",{});var Hi=a(os);ne=n(Hi,"P",{});var ur=a(ne);is=n(ur,"STRONG",{});var Yi=a(is);_a=o(Yi,"Octobre 2019"),Yi.forEach(s),Ea=o(ur,": "),Ae=n(ur,"A",{href:!0,rel:!0});var Fi=a(Ae);ba=o(Fi,"DistilBERT"),Fi.forEach(s),Ta=o(ur,", une version distill\xE9e de BERT \xE9tant 60% plus rapide, 40% plus l\xE9g\xE8re en m\xE9moire, et conservant tout de m\xEAme 97% performances initiales de BERT"),ur.forEach(s),Hi.forEach(s),Pa=p(b),us=n(b,"LI",{});var Ji=a(us);T=n(Ji,"P",{});var ct=a(T);ds=n(ct,"STRONG",{});var Xi=a(ds);ka=o(Xi,"Octobre 2019"),Xi.forEach(s),xa=o(ct,": "),Ie=n(ct,"A",{href:!0,rel:!0});var Qi=a(Ie);$a=o(Qi,"BART"),Qi.forEach(s),wa=o(ct," et "),Ce=n(ct,"A",{href:!0,rel:!0});var Wi=a(Ce);ya=o(Wi,"T5"),Wi.forEach(s),La=o(ct,", deux mod\xE8les larges pr\xE9-entra\xEEn\xE9s utilisant la m\xEAme architecture que le mod\xE8le Transformer originel (les premiers \xE0 faire cela)"),ct.forEach(s),Ji.forEach(s),Aa=p(b),cs=n(b,"LI",{});var Zi=a(cs);P=n(Zi,"P",{});var pt=a(P);ps=n(pt,"STRONG",{});var Ki=a(ps);Ia=o(Ki,"Mai 2020"),Ki.forEach(s),Ca=o(pt,", "),Ne=n(pt,"A",{href:!0,rel:!0});var eu=a(Ne);Na=o(eu,"GPT-3"),eu.forEach(s),Ma=o(pt,", une version encore plus grande que GPT-2 ayant des performances tr\xE8s bonnes sur une vari\xE9t\xE9 de t\xE2ches ne n\xE9cessitant pas de fine-tuning (appell\xE9 "),ms=n(pt,"EM",{});var tu=a(ms);Ga=o(tu,"zero-shot learning"),tu.forEach(s),Ra=o(pt,")"),pt.forEach(s),Zi.forEach(s),b.forEach(s),qr=p(e),vt=n(e,"P",{});var su=a(vt);ja=o(su,"Cette liste est loin d\u2019\xEAtre exhaustive, et permet juste de mettre en lumi\xE8re certains mod\xE8les Transformers. Plus largement, ces mod\xE8les peuvent \xEAtre regroup\xE9s en trois cat\xE9gories :"),su.forEach(s),_r=p(e),k=n(e,"UL",{});var Ut=a(k);Me=n(Ut,"LI",{});var kn=a(Me);Sa=o(kn,"GPT-like (aussi appel\xE9 mod\xE8les Transformers "),fs=n(kn,"EM",{});var ru=a(fs);Ba=o(ru,"auto-regressif"),ru.forEach(s),Da=o(kn,")"),kn.forEach(s),Oa=p(Ut),Ge=n(Ut,"LI",{});var xn=a(Ge);za=o(xn,"BERT-like (aussi appel\xE9 mod\xE8les Transformers "),hs=n(xn,"EM",{});var nu=a(hs);Ua=o(nu,"auto-encodeur"),nu.forEach(s),Va=o(xn,")"),xn.forEach(s),Ha=p(Ut),Re=n(Ut,"LI",{});var $n=a(Re);Ya=o($n,"BART/T5-like (aussi appel\xE9 mod\xE8les Transformers "),vs=n($n,"EM",{});var au=a(vs);Fa=o(au,"s\xE9quence-\xE0-s\xE9quence"),au.forEach(s),Ja=o($n,")"),$n.forEach(s),Ut.forEach(s),Er=p(e),gt=n(e,"P",{});var lu=a(gt);Xa=o(lu,"Nous verrons plus en profondeur ces familles de mod\xE8les plus tard."),lu.forEach(s),br=p(e),S=n(e,"H2",{class:!0});var wn=a(S);ae=n(wn,"A",{id:!0,class:!0,href:!0});var ou=a(ae);gs=n(ou,"SPAN",{});var iu=a(gs);v(je.$$.fragment,iu),iu.forEach(s),ou.forEach(s),Qa=p(wn),qs=n(wn,"SPAN",{});var uu=a(qs);Wa=o(uu,"Transformers, des mod\xE8les du langage"),uu.forEach(s),wn.forEach(s),Tr=p(e),le=n(e,"P",{});var yn=a(le);Za=o(yn,"Tous les mod\xE8les Transformers mentionn\xE9 ci-dessus (GPT, BERT, BART, T5, etc.) ont \xE9t\xE9 entra\xEEn\xE9s comme des "),_s=n(yn,"EM",{});var du=a(_s);Ka=o(du,"mod\xE8les du langage"),du.forEach(s),el=o(yn,". Cela signifie qu\u2019ils ont \xE9t\xE9 entra\xEEn\xE9s sur une large quantit\xE9 de textes bruts en mode auto-supervis\xE9. L\u2019apprentissage auto-supervis\xE9 est un type d\u2019entra\xEEnement o\xF9 l\u2019objectif est automatiquement d\xE9termin\xE9 par les entr\xE9es du mod\xE8le. Cela veut dire qu\u2019une labellisation des donn\xE9es par des humains n\u2019est pas n\xE9cessaire !"),yn.forEach(s),Pr=p(e),oe=n(e,"P",{});var Ln=a(oe);tl=o(Ln,"Ce type de mod\xE8le d\xE9veloppe une compr\xE9hension statistique du langage sur lequel il est entra\xEEn\xE9, mais cela n\u2019est pas adapt\xE9 \xE0 une t\xE2che sp\xE9cifique. A cause de cela, le mod\xE8le pr\xE9-entra\xEEn\xE9 doit ensuite suivre une proc\xE9dure de "),Es=n(Ln,"EM",{});var cu=a(Es);sl=o(cu,"transfer learning"),cu.forEach(s),rl=o(Ln,". Durant cette proc\xE9dure, le mod\xE8le est fine-tun\xE9 en mode supervis\xE9 \u2014 en utilisant des donn\xE9es labellis\xE9es par un human \u2014 sur une t\xE2che donn\xE9e."),Ln.forEach(s),kr=p(e),x=n(e,"P",{});var Vt=a(x);nl=o(Vt,"Un exemple de t\xE2che est la pr\xE9diction du prochain mot de la phrase en ayant lu les "),bs=n(Vt,"EM",{});var pu=a(bs);al=o(pu,"n"),pu.forEach(s),ll=o(Vt," mots pr\xE9c\xE9dents. Ce proc\xE9d\xE9 s\u2019appelle "),Ts=n(Vt,"EM",{});var mu=a(Ts);ol=o(mu,"mod\xE9lisation du langage causal"),mu.forEach(s),il=o(Vt," car les pr\xE9dictions d\xE9pendent des entr\xE9es pr\xE9c\xE9dentes et actuelles, mais pas des suivantes."),Vt.forEach(s),xr=p(e),B=n(e,"DIV",{class:!0});var An=a(B);Se=n(An,"IMG",{class:!0,src:!0,alt:!0}),ul=p(An),Be=n(An,"IMG",{class:!0,src:!0,alt:!0}),An.forEach(s),$r=p(e),ie=n(e,"P",{});var In=a(ie);dl=o(In,"Un autre exemple est la "),Ps=n(In,"EM",{});var fu=a(Ps);cl=o(fu,"mod\xE9lisation du langage contextuel"),fu.forEach(s),pl=o(In,", o\xF9 me mod\xE8le doit pr\xE9dire un mot masqu\xE9 dans une phrase."),In.forEach(s),wr=p(e),D=n(e,"DIV",{class:!0});var Cn=a(D);De=n(Cn,"IMG",{class:!0,src:!0,alt:!0}),ml=p(Cn),Oe=n(Cn,"IMG",{class:!0,src:!0,alt:!0}),Cn.forEach(s),yr=p(e),O=n(e,"H2",{class:!0});var Nn=a(O);ue=n(Nn,"A",{id:!0,class:!0,href:!0});var hu=a(ue);ks=n(hu,"SPAN",{});var vu=a(ks);v(ze.$$.fragment,vu),vu.forEach(s),hu.forEach(s),fl=p(Nn),xs=n(Nn,"SPAN",{});var gu=a(xs);hl=o(gu,"Les mod\xE8les Transformers sont \xE9normes"),gu.forEach(s),Nn.forEach(s),Lr=p(e),qt=n(e,"P",{});var qu=a(qt);vl=o(qu,"En dehors de quelques exceptions (comme DistilBERT), la strat\xE9gie g\xE9n\xE9rale pour obtenir de meilleure performance consiste \xE0 augmenter la taille des mod\xE8les ainsi que la quantit\xE9 de donn\xE9es utilis\xE9es pour l\u2019entra\xEEnement de ces derniers."),qu.forEach(s),Ar=p(e),Ue=n(e,"DIV",{class:!0});var _u=a(Ue);Ve=n(_u,"IMG",{src:!0,alt:!0,width:!0}),_u.forEach(s),Ir=p(e),_t=n(e,"P",{});var Eu=a(_t);gl=o(Eu,"Malheureusement, entra\xEEner un mod\xE8le, et particuli\xE8rement un mod\xE8le large, n\xE9cessite une importante quantit\xE9 de donn\xE9es. Cela devient tr\xE8s co\xFBteux en terme de temps et de ressources de calcul. Cela se traduit m\xEAme par un impact environnemental, comme le montre le graphique suivant."),Eu.forEach(s),Cr=p(e),z=n(e,"DIV",{class:!0});var Mn=a(z);He=n(Mn,"IMG",{class:!0,src:!0,alt:!0}),ql=p(Mn),Ye=n(Mn,"IMG",{class:!0,src:!0,alt:!0}),Mn.forEach(s),Nr=p(e),v(Fe.$$.fragment,e),Mr=p(e),Et=n(e,"P",{});var bu=a(Et);_l=o(bu,"Et cela pr\xE9sente un projet pour un (gigantesque) mod\xE8le men\xE9 par une \xE9quipe qui essaye consciemment de r\xE9duire l\u2019impact environnemental du pr\xE9-entra\xEEnement. L\u2019empreinte environnemental serait encore plus cons\xE9quente s\u2019il fallait lancer beaucoup d\u2019entra\xEEnements pour obtenir les meilleurs hyper-param\xE8tres."),bu.forEach(s),Gr=p(e),bt=n(e,"P",{});var Tu=a(bt);El=o(Tu,"Imaginez si \xE0 chaque fois qu\u2019une \xE9quipe de recherche, qu\u2019une organisation \xE9tudiante ou qu\u2019une entreprise voulait entra\xEEner un mod\xE8le, ils devaient repartir de z\xE9ro. Ceci menerait \xE0 des \xE9normes et non-n\xE9cessaires co\xFBts environnementaux."),Tu.forEach(s),Rr=p(e),Tt=n(e,"P",{});var Pu=a(Tt);bl=o(Pu,"C\u2019est pourquoi le partage des mod\xE8les du langage est primordial : partager les poids d\u2019entra\xEEnement et construire \xE0 partir de ces poids permet de r\xE9duire les co\xFBts de calcul globaux ainsi que l\u2019empreinte carbone de toute la communaut\xE9."),Pu.forEach(s),jr=p(e),U=n(e,"H2",{class:!0});var Gn=a(U);de=n(Gn,"A",{id:!0,class:!0,href:!0});var ku=a(de);$s=n(ku,"SPAN",{});var xu=a($s);v(Je.$$.fragment,xu),xu.forEach(s),ku.forEach(s),Tl=p(Gn),ws=n(Gn,"SPAN",{});var $u=a(ws);Pl=o($u,"Le Transfer Learning"),$u.forEach(s),Gn.forEach(s),Sr=p(e),v(Xe.$$.fragment,e),Br=p(e),ce=n(e,"P",{});var Rn=a(ce);kl=o(Rn,"Le "),ys=n(Rn,"EM",{});var wu=a(ys);xl=o(wu,"Pr\xE9-entra\xEEnement"),wu.forEach(s),$l=o(Rn," est le fait d\u2019entra\xEEner un mod\xE8le de z\xE9ro : les poids sont initialis\xE9s de mani\xE8re al\xE9atoire, et l\u2019entra\xEEnement d\xE9bute sans aucune connaissance pr\xE9alable (ou heuristique)."),Rn.forEach(s),Dr=p(e),V=n(e,"DIV",{class:!0});var jn=a(V);Qe=n(jn,"IMG",{class:!0,src:!0,alt:!0}),wl=p(jn),We=n(jn,"IMG",{class:!0,src:!0,alt:!0}),jn.forEach(s),Or=p(e),Pt=n(e,"P",{});var yu=a(Pt);yl=o(yu,"Ce pr\xE9-entra\xEEnement est habituellement r\xE9alis\xE9 sur de grande quantit\xE9 de donn\xE9es. Il n\xE9cessite donc un tr\xE8s grand corpus de donn\xE9es, et l\u2019entra\xEEnement peut prendre jusqu\u2019\xE0 plusieurs semaines."),yu.forEach(s),zr=p(e),$=n(e,"P",{});var Ht=a($);Ll=o(Ht,"Le "),Ls=n(Ht,"EM",{});var Lu=a(Ls);Al=o(Lu,"Fine-tuning"),Lu.forEach(s),Il=o(Ht,", d\u2019un autre c\xF4t\xE9, est un entra\xEEnement effectu\xE9 "),As=n(Ht,"STRONG",{});var Au=a(As);Cl=o(Au,"apr\xE8s"),Au.forEach(s),Nl=o(Ht," qu\u2019un mod\xE8le ait \xE9t\xE9 pr\xE9-entra\xEEn\xE9. Pour r\xE9aliser le fine-tuning, il faut d\u2019abord r\xE9cup\xE8rer un mod\xE8le du langage pr\xE9-entra\xEEn\xE9, puis lanc\xE9 un entra\xEEnement sur un jeu de donn\xE9es adapt\xE9 \xE0 la t\xE2che cible. Mais attendez \u2014 pourquoi ne pas simplement entra\xEEn\xE9 un mod\xE8le sur la t\xE2che cible ? Voici plusieurs raisons :"),Ht.forEach(s),Ur=p(e),w=n(e,"UL",{});var Yt=a(w);Is=n(Yt,"LI",{});var Iu=a(Is);Ml=o(Iu,"Le mod\xE8le pr\xE9-entra\xEEn\xE9 a d\xE9j\xE0 \xE9t\xE9 entra\xEEn\xE9 sur un jeu de donn\xE9es qui a des similarit\xE9s avec le jeu de donn\xE9es utilis\xE9 pour le fine-tuning. Ainsi le processus de fine-tuning peut b\xE9n\xE9ficier du savoir acquis par le mod\xE8le initial lors du pr\xE9-entra\xEEnement (par exemple, avec des probl\xE8mes de NLP, le mod\xE8le pr\xE9-entra\xEEn\xE9 aura une sorte de compr\xE9hension statistique de la langue associ\xE9e \xE0 la t\xE2che cible)."),Iu.forEach(s),Gl=p(Yt),Cs=n(Yt,"LI",{});var Cu=a(Cs);Rl=o(Cu,"Puisque le mod\xE8le pr\xE9-entra\xEEn\xE9 a d\xE9j\xE0 \xE9t\xE9 entra\xEEn\xE9 sur beaucoup de donn\xE9es, le fine-tuning n\xE9cessite beaucoup moins de donn\xE9es pour obtenir des r\xE9sultats d\xE9cents."),Cu.forEach(s),jl=p(Yt),Ns=n(Yt,"LI",{});var Nu=a(Ns);Sl=o(Nu,"Pour la m\xEAme raison, la quantit\xE9 de temps et de ressources n\xE9cessaires pour obtenir de bons r\xE9sultats est beaucoup moins grande."),Nu.forEach(s),Yt.forEach(s),Vr=p(e),pe=n(e,"P",{});var Sn=a(pe);Bl=o(Sn,"Par exemple, il serait possible de tirer prodit d\u2019un mod\xE8le pr\xE9-entra\xEEn\xE9 sur la langue anglaise and ensuite fine-tun\xE9 sur un corpus arXiv, permettant d\u2019obtenir un mod\xE8le bas\xE9 sur la recherche et la science. Le fine-tuning ne n\xE9cessiterait qu\u2019une quantit\xE9 r\xE9duite de donn\xE9es : le savoir acquis par le mod\xE8le pr\xE9-entra\xEEn\xE9 est transf\xE9r\xE9 au nouveau mod\xE8le, d\u2019o\xF9 le terme de "),Ms=n(Sn,"EM",{});var Mu=a(Ms);Dl=o(Mu,"transfer learning"),Mu.forEach(s),Ol=o(Sn,"."),Sn.forEach(s),Hr=p(e),H=n(e,"DIV",{class:!0});var Bn=a(H);Ze=n(Bn,"IMG",{class:!0,src:!0,alt:!0}),zl=p(Bn),Ke=n(Bn,"IMG",{class:!0,src:!0,alt:!0}),Bn.forEach(s),Yr=p(e),kt=n(e,"P",{});var Gu=a(kt);Ul=o(Gu,"Le fine-tuning d\u2019un mod\xE8le n\xE9cessite ainsi moins de temps, de donn\xE9es, d\u2019argent et a un impact environnemental r\xE9duit. Il est \xE9galement possible d\u2019it\xE9rer plus rapidement sur diff\xE9rentes strat\xE9gies de fine-tuning, puisque l\u2019entra\xEEnement est moins contraignant qu\u2019un entra\xEEnement complet."),Gu.forEach(s),Fr=p(e),xt=n(e,"P",{});var Ru=a(xt);Vl=o(Ru,"Ce proc\xE9d\xE9 permet \xE9galement d\u2019obtenir de meilleurs r\xE9sultats qu\u2019un entra\xEEnement de z\xE9ro (sauf si on dispose de beaucoup de donn\xE9es), ce qui explique pourquoi il faut toujours essayer de tirer profit d\u2019un mod\xE8le pr\xE9-entra\xEEn\xE9 \u2014 un qui est le plus proche de la t\xE2che cible \u2014 et de le fine-tuner."),Ru.forEach(s),Jr=p(e),Y=n(e,"H2",{class:!0});var Dn=a(Y);me=n(Dn,"A",{id:!0,class:!0,href:!0});var ju=a(me);Gs=n(ju,"SPAN",{});var Su=a(Gs);v(et.$$.fragment,Su),Su.forEach(s),ju.forEach(s),Hl=p(Dn),Rs=n(Dn,"SPAN",{});var Bu=a(Rs);Yl=o(Bu,"Architecture g\xE9n\xE9rale"),Bu.forEach(s),Dn.forEach(s),Xr=p(e),$t=n(e,"P",{});var Du=a($t);Fl=o(Du,"Dans cette section, nous allons voir l\u2019architecture g\xE9n\xE9rale des mod\xE8les Transformers. Pas d\u2019inqui\xE9tudes si vous ne comprenez pas tous les concepts; il y a des sections d\xE9taill\xE9es qui couvrent chaque composants plus tard."),Du.forEach(s),Qr=p(e),v(tt.$$.fragment,e),Wr=p(e),F=n(e,"H2",{class:!0});var On=a(F);fe=n(On,"A",{id:!0,class:!0,href:!0});var Ou=a(fe);js=n(Ou,"SPAN",{});var zu=a(js);v(st.$$.fragment,zu),zu.forEach(s),Ou.forEach(s),Jl=p(On),Ss=n(On,"SPAN",{});var Uu=a(Ss);Xl=o(Uu,"Introduction"),Uu.forEach(s),On.forEach(s),Zr=p(e),wt=n(e,"P",{});var Vu=a(wt);Ql=o(Vu,"Le mod\xE8le est principalement compos\xE9 de deux blocs :"),Vu.forEach(s),Kr=p(e),he=n(e,"UL",{});var zn=a(he);yt=n(zn,"LI",{});var si=a(yt);Bs=n(si,"STRONG",{});var Hu=a(Bs);Wl=o(Hu,"Encodeur (gauche)"),Hu.forEach(s),Zl=o(si,": L\u2019encodeur re\xE7oit une entr\xE9e et contruit une r\xE9pr\xE9sentation de celle-ci (ses caract\xE9ristiques). Cela signifie que le mod\xE8le est optimis\xE9 pour acqu\xE9rir une compr\xE9hension venant de ces entr\xE9es."),si.forEach(s),Kl=p(zn),Lt=n(zn,"LI",{});var ri=a(Lt);Ds=n(ri,"STRONG",{});var Yu=a(Ds);eo=o(Yu,"D\xE9codeur (droite)"),Yu.forEach(s),to=o(ri,": Le d\xE9codeur utilise la r\xE9pr\xE9sentation de l\u2019encodeur (caract\xE9ristiques) en plus des autres entr\xE9es pour g\xE9n\xE9rer une s\xE9quence cible. Cela signifie que le mod\xE8le est optimis\xE9 pour g\xE9n\xE9rer des sorties."),ri.forEach(s),zn.forEach(s),en=p(e),J=n(e,"DIV",{class:!0});var Un=a(J);rt=n(Un,"IMG",{class:!0,src:!0,alt:!0}),so=p(Un),nt=n(Un,"IMG",{class:!0,src:!0,alt:!0}),Un.forEach(s),tn=p(e),At=n(e,"P",{});var Fu=a(At);ro=o(Fu,"Chacune de ces parties peuvent \xEAtre utilis\xE9es ind\xE9pendamment, en fonction de la t\xE2che :"),Fu.forEach(s),sn=p(e),y=n(e,"UL",{});var Ft=a(y);It=n(Ft,"LI",{});var ni=a(It);Os=n(ni,"STRONG",{});var Ju=a(Os);no=o(Ju,"Mod\xE8les uniquement encodeurs"),Ju.forEach(s),ao=o(ni," : Adapt\xE9s pour des t\xE2ches qui n\xE9cessitent une compr\xE9hension de l\u2019entr\xE9e, comme la classification de phrases et la reconnaissance d\u2019entit\xE9s nomm\xE9es."),ni.forEach(s),lo=p(Ft),Ct=n(Ft,"LI",{});var ai=a(Ct);zs=n(ai,"STRONG",{});var Xu=a(zs);oo=o(Xu,"Mod\xE8les uniquement d\xE9codeurs"),Xu.forEach(s),io=o(ai," : Adapt\xE9s pour les t\xE2ches g\xE9n\xE9ratives telles que la g\xE9n\xE9ration de texte."),ai.forEach(s),uo=p(Ft),ve=n(Ft,"LI",{});var dr=a(ve);Us=n(dr,"STRONG",{});var Qu=a(Us);co=o(Qu,"Mod\xE8les encodeurs-d\xE9codeurs"),Qu.forEach(s),po=o(dr," ou "),Vs=n(dr,"STRONG",{});var Wu=a(Vs);mo=o(Wu,"mod\xE8les de s\xE9quence-\xE0-s\xE9quence"),Wu.forEach(s),fo=o(dr," : Adapt\xE9s aux t\xE2ches g\xE9n\xE9ratives qui n\xE9cessitent une entr\xE9e, telles que la traduction ou le r\xE9sum\xE9 de texte."),dr.forEach(s),Ft.forEach(s),rn=p(e),Nt=n(e,"P",{});var Zu=a(Nt);ho=o(Zu,"Nous verrons plus en d\xE9tails chacune de ces architectures plus tard."),Zu.forEach(s),nn=p(e),X=n(e,"H2",{class:!0});var Vn=a(X);ge=n(Vn,"A",{id:!0,class:!0,href:!0});var Ku=a(ge);Hs=n(Ku,"SPAN",{});var ed=a(Hs);v(at.$$.fragment,ed),ed.forEach(s),Ku.forEach(s),vo=p(Vn),Ys=n(Vn,"SPAN",{});var td=a(Ys);go=o(td,"Les couches d'attention"),td.forEach(s),Vn.forEach(s),an=p(e),L=n(e,"P",{});var Jt=a(L);qo=o(Jt,"Une caract\xE9ristique clef des mod\xE8les Transformers est qu\u2019ils sont construits avec des couches sp\xE9ciales appell\xE9es "),Fs=n(Jt,"EM",{});var sd=a(Fs);_o=o(sd,"couches d\u2019attention"),sd.forEach(s),Eo=o(Jt,". En fait, le titre de la publication qui a pr\xE9sent\xE9 l\u2019architecture Transformer est "),lt=n(Jt,"A",{href:!0,rel:!0});var rd=a(lt);bo=o(rd,"\u201CAttention Is All You Need\u201D"),rd.forEach(s),To=o(Jt," ! Nous allons explorer les d\xE9tails des couches d\u2019attention plus tard dans ce cours; pour le moment, ce qu\u2019il faut retenir est que la couche d\u2019attention va indiquer au mod\xE8le les mots, de la phrase d\u2019entr\xE9e, sur lesquels porter son attention (et plus ou moins ignorer les autres) lorsqu\u2019il s\u2019agit de construire une repr\xE9sentation de chaque mot."),Jt.forEach(s),ln=p(e),Mt=n(e,"P",{});var nd=a(Mt);Po=o(nd,"Pour mettre ceci en contexte, consid\xE9rons la t\xE2che de traduction de l\u2019anglais vers le fran\xE7ais. \xC9tant donn\xE9 l\u2019entr\xE9e \u201CYou like this course\u201D, un mod\xE8le de traduction devra \xE9galement s\u2019occuper du mot adjacent \u201CYou\u201D pour obtenir la traduction correcte du mot \u201Clike\u201D, car en fran\xE7ais le verbe \u201Caimer\u201D est conjugu\xE9 diff\xE9remment selon l\u2019objet. Le reste de la phrase, cependant, n\u2019est pas utile pour la traduction de ce mot. De m\xEAme, lors de la traduction de \u201Cthis\u201D, le mod\xE8le devra \xE9galement faire attention au mot \u201Ccourse\u201D, car \u201Cthis\u201D se traduit diff\xE9remment selon que le nom associ\xE9 est masculin ou f\xE9minin. Encore une fois, les autres mots de la phrase n\u2019auront pas d\u2019importance pour la traduction de \u201Cthis\u201D. Avec des phrases plus complexes (et des r\xE8gles de grammaire plus complexes), le mod\xE8le devrait accorder une attention particuli\xE8re aux mots qui pourraient appara\xEEtre plus loin dans la phrase pour traduire correctement chaque mot."),nd.forEach(s),on=p(e),Gt=n(e,"P",{});var ad=a(Gt);ko=o(ad,"Le m\xEAme concept s\u2019applique \xE0 n\u2019importe quelle t\xE2che associ\xE9e au langage naturel : un mot seul est porteur de sens, mais ce sens est profond\xE9ment affect\xE9 par le contexte, qui peut \xEAtre un ou plusieurs mots se situant avant ou apr\xE8s le mot \xE9tudi\xE9."),ad.forEach(s),un=p(e),Rt=n(e,"P",{});var ld=a(Rt);xo=o(ld,"Maintenant que vous avez une id\xE9e plus pr\xE9cise des couches d\u2019attentions, nous allons regarder de plus pr\xE8s l\u2019architecture des mod\xE8les Transformers."),ld.forEach(s),dn=p(e),Q=n(e,"H2",{class:!0});var Hn=a(Q);qe=n(Hn,"A",{id:!0,class:!0,href:!0});var od=a(qe);Js=n(od,"SPAN",{});var id=a(Js);v(ot.$$.fragment,id),id.forEach(s),od.forEach(s),$o=p(Hn),Xs=n(Hn,"SPAN",{});var ud=a(Xs);wo=o(ud,"L'architecture originelle"),ud.forEach(s),Hn.forEach(s),cn=p(e),jt=n(e,"P",{});var dd=a(jt);yo=o(dd,"L\u2019architecture Transformer a initialement \xE9t\xE9 construite pour des t\xE2ches de traduction. Pendant l\u2019entra\xEEnement, l\u2019encodeur re\xE7oit des entr\xE9es (des phrases) dans une certaine langue, tandis que le d\xE9codeur re\xE7oit la m\xEAme phrase traduite dans la langue cible. Pour l\u2019encodeur, les couches d\u2019attention peuvent utiliser tous les mots d\u2019une phrase (puisque, comme nous venons de le voir, la traduction d\u2019un mot donn\xE9 peut d\xE9pendre de ce qui le suit ou le pr\xE9c\xE8de dans la phrase). Le d\xE9codeur, quant \xE0 lui, fonctionne de fa\xE7on s\xE9quentielle et ne peut porter son attention qu\u2019aux mots d\xE9j\xE0 traduits dans la phrase (donc, uniquement les mots g\xE9n\xE9r\xE9s avant le mot en cours). Par exemple, lorsqu\u2019on a pr\xE9dit les trois premiers mots de la phrase cible, on les donne au d\xE9codeur qui utilise alors toutes les entr\xE9es de l\u2019encodeur pour essayer de pr\xE9dire le quatri\xE8me mot."),dd.forEach(s),pn=p(e),St=n(e,"P",{});var cd=a(St);Lo=o(cd,"Pour acc\xE9l\xE9rer les choses pendant l\u2019apprentissage (lorsque le mod\xE8le a acc\xE8s aux phrases cibles), le d\xE9codeur est aliment\xE9 avec la cible enti\xE8re, mais il n\u2019est pas autoris\xE9 \xE0 utiliser les mots futurs (s\u2019il avait acc\xE8s au mot en position 2 lorsqu\u2019il essayait de pr\xE9dire le mot en position 2, le probl\xE8me ne serait pas tr\xE8s difficile !). Par exemple, en essayant de pr\xE9dire le quatri\xE8me mot, la couche d\u2019attention n\u2019aura acc\xE8s qu\u2019aux mots des positions 1 \xE0 3."),cd.forEach(s),mn=p(e),Bt=n(e,"P",{});var pd=a(Bt);Ao=o(pd,"L\u2019architecture originale des mod\xE8les Transformers ressemblait \xE0 ceci, avec l\u2019encodeur \xE0 gauche et le d\xE9codeur \xE0 droite :"),pd.forEach(s),fn=p(e),W=n(e,"DIV",{class:!0});var Yn=a(W);it=n(Yn,"IMG",{class:!0,src:!0,alt:!0}),Io=p(Yn),ut=n(Yn,"IMG",{class:!0,src:!0,alt:!0}),Yn.forEach(s),hn=p(e),Dt=n(e,"P",{});var md=a(Dt);Co=o(md,"Notez que la premi\xE8re couche d\u2019attention dans un bloc d\xE9codeur porte sont attention \xE0 toutes les entr\xE9es (pass\xE9es) du d\xE9codeur, mais la seconde couche d\u2019attention utilise la sortie de l\u2019encodeur. Il peut ainsi acc\xE9der \xE0 toute la phrase d\u2019entr\xE9e pour pr\xE9dire au mieux le mot actuel. Ceci est tr\xE8s utile puisque les diff\xE9rentes langues peuvent avoir des r\xE8gles grammaticales qui placent les mots dans des ordres diff\xE9rents, ou un contexte fourni plus tard dans la phrase peut \xEAtre utile pour d\xE9terminer la meilleure traduction d\u2019un mot donn\xE9."),md.forEach(s),vn=p(e),_e=n(e,"P",{});var Fn=a(_e);No=o(Fn,"Le "),Qs=n(Fn,"EM",{});var fd=a(Qs);Mo=o(fd,"masque d\u2019attention"),fd.forEach(s),Go=o(Fn," peut \xE9galement \xEAtre utilis\xE9 dans un mod\xE8le encodeur/d\xE9codeur pour l\u2019emp\xEAcher de porter son attention sur certains mots sp\xE9ciaux \u2014 par exemple, le motde remplissage sp\xE9cial utilis\xE9 pour que toutes les entr\xE9es aient la m\xEAme longueur lors du regroupement de phrases."),Fn.forEach(s),gn=p(e),Z=n(e,"H2",{class:!0});var Jn=a(Z);Ee=n(Jn,"A",{id:!0,class:!0,href:!0});var hd=a(Ee);Ws=n(hd,"SPAN",{});var vd=a(Ws);v(dt.$$.fragment,vd),vd.forEach(s),hd.forEach(s),Ro=p(Jn),Zs=n(Jn,"SPAN",{});var gd=a(Zs);jo=o(gd,"Architectures contre checkpoints"),gd.forEach(s),Jn.forEach(s),qn=o(e,`

 
En approfondissant l'\xE9tude des mod\xE8les Transformers dans ce cours, vous verrez des mentions d'*architectures* et de *checkpoints* ainsi que de *mod\xE8les*. Ces termes ont tous des significations l\xE9g\xE8rement diff\xE9rentes :
`),A=n(e,"UL",{});var Xt=a(A);Ot=n(Xt,"LI",{});var li=a(Ot);Ks=n(li,"STRONG",{});var qd=a(Ks);So=o(qd,"Architecture"),qd.forEach(s),Bo=o(li,": C\u2019est le squelette du mod\xE8le \u2014 la d\xE9finition de chaque couche et chaque op\xE9ration qui se produit au sein du mod\xE8le."),li.forEach(s),Do=p(Xt),zt=n(Xt,"LI",{});var oi=a(zt);er=n(oi,"STRONG",{});var _d=a(er);Oo=o(_d,"Checkpoints"),_d.forEach(s),zo=o(oi,": Ce sont les poids qui seront charg\xE9s dans une architecture donn\xE9e."),oi.forEach(s),Uo=p(Xt),I=n(Xt,"LI",{});var mt=a(I);tr=n(mt,"STRONG",{});var Ed=a(tr);Vo=o(Ed,"Model"),Ed.forEach(s),Ho=o(mt,": C\u2019est un mot valise n\u2019\xE9tant pas aussi pr\xE9cis que les mots \u201Carchitecture\u201D ou \u201Ccheckpoint\u201D: il peut d\xE9signer l\u2019un comme l\u2019autre. Dans ce cours, il sera sp\xE9cifi\xE9 "),sr=n(mt,"EM",{});var bd=a(sr);Yo=o(bd,"architecture"),bd.forEach(s),Fo=o(mt," ou "),rr=n(mt,"EM",{});var Td=a(rr);Jo=o(Td,"checkpoint"),Td.forEach(s),Xo=o(mt," lorsqu\u2019il est n\xE9cessaire de r\xE9duire l\u2019ambiguit\xE9."),mt.forEach(s),Xt.forEach(s),_n=p(e),C=n(e,"P",{});var Qt=a(C);Qo=o(Qt,"Par exemple, BERT est une architecture alors que "),nr=n(Qt,"CODE",{});var Pd=a(nr);Wo=o(Pd,"bert-base-cased"),Pd.forEach(s),Zo=o(Qt,", un ensemble de poids entra\xEEn\xE9 par l\u2019\xE9quipe de Google lors de la premi\xE8re sortie de BERT, est un checkpoint. Cependant, il est possible de dire \u201Cle mod\xE8le BERT\u201D et \u201Cle mod\xE8le "),ar=n(Qt,"CODE",{});var kd=a(ar);Ko=o(kd,"bert-base-cased"),kd.forEach(s),ei=o(Qt,"\u201C."),Qt.forEach(s),this.h()},h(){d(M,"name","hf:doc:metadata"),d(M,"content",JSON.stringify(Cd)),d(K,"id","comment-fonctionnent-les-modles-transformers"),d(K,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(K,"href","#comment-fonctionnent-les-modles-transformers"),d(G,"class","relative group"),d(ee,"id","court-historique-des-transformers"),d(ee,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(ee,"href","#court-historique-des-transformers"),d(R,"class","relative group"),d(Pe,"class","block dark:hidden"),m(Pe.src,di="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers_chrono.svg")||d(Pe,"src",di),d(Pe,"alt","A brief chronology of Transformers models."),d(ke,"class","hidden dark:block"),m(ke.src,ci="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers_chrono-dark.svg")||d(ke,"src",ci),d(ke,"alt","A brief chronology of Transformers models."),d(j,"class","flex justify-center"),d($e,"href","https://arxiv.org/abs/1706.03762"),d($e,"rel","nofollow"),d(we,"href","https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf"),d(we,"rel","nofollow"),d(ye,"href","https://arxiv.org/abs/1810.04805"),d(ye,"rel","nofollow"),d(Le,"href","https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf"),d(Le,"rel","nofollow"),d(Ae,"href","https://arxiv.org/abs/1910.01108"),d(Ae,"rel","nofollow"),d(Ie,"href","https://arxiv.org/abs/1910.13461"),d(Ie,"rel","nofollow"),d(Ce,"href","https://arxiv.org/abs/1910.10683"),d(Ce,"rel","nofollow"),d(Ne,"href","https://arxiv.org/abs/2005.14165"),d(Ne,"rel","nofollow"),d(ae,"id","transformers-des-modles-du-langage"),d(ae,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(ae,"href","#transformers-des-modles-du-langage"),d(S,"class","relative group"),d(Se,"class","block dark:hidden"),m(Se.src,pi="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/causal_modeling.svg")||d(Se,"src",pi),d(Se,"alt","Example of causal language modeling in which the next word from a sentence is predicted."),d(Be,"class","hidden dark:block"),m(Be.src,mi="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/causal_modeling-dark.svg")||d(Be,"src",mi),d(Be,"alt","Example of causal language modeling in which the next word from a sentence is predicted."),d(B,"class","flex justify-center"),d(De,"class","block dark:hidden"),m(De.src,fi="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/masked_modeling.svg")||d(De,"src",fi),d(De,"alt","Example of masked language modeling in which a masked word from a sentence is predicted."),d(Oe,"class","hidden dark:block"),m(Oe.src,hi="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/masked_modeling-dark.svg")||d(Oe,"src",hi),d(Oe,"alt","Example of masked language modeling in which a masked word from a sentence is predicted."),d(D,"class","flex justify-center"),d(ue,"id","les-modles-transformers-sont-normes"),d(ue,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(ue,"href","#les-modles-transformers-sont-normes"),d(O,"class","relative group"),m(Ve.src,vi="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/model_parameters.png")||d(Ve,"src",vi),d(Ve,"alt","Number of parameters of recent Transformers models"),d(Ve,"width","90%"),d(Ue,"class","flex justify-center"),d(He,"class","block dark:hidden"),m(He.src,gi="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/carbon_footprint.svg")||d(He,"src",gi),d(He,"alt","The carbon footprint of a large language model."),d(Ye,"class","hidden dark:block"),m(Ye.src,qi="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/carbon_footprint-dark.svg")||d(Ye,"src",qi),d(Ye,"alt","The carbon footprint of a large language model."),d(z,"class","flex justify-center"),d(de,"id","le-transfer-learning"),d(de,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(de,"href","#le-transfer-learning"),d(U,"class","relative group"),d(Qe,"class","block dark:hidden"),m(Qe.src,_i="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/pretraining.svg")||d(Qe,"src",_i),d(Qe,"alt","The pretraining of a language model is costly in both time and money."),d(We,"class","hidden dark:block"),m(We.src,Ei="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/pretraining-dark.svg")||d(We,"src",Ei),d(We,"alt","The pretraining of a language model is costly in both time and money."),d(V,"class","flex justify-center"),d(Ze,"class","block dark:hidden"),m(Ze.src,bi="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/finetuning.svg")||d(Ze,"src",bi),d(Ze,"alt","The fine-tuning of a language model is cheaper than pretraining in both time and money."),d(Ke,"class","hidden dark:block"),m(Ke.src,Ti="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/finetuning-dark.svg")||d(Ke,"src",Ti),d(Ke,"alt","The fine-tuning of a language model is cheaper than pretraining in both time and money."),d(H,"class","flex justify-center"),d(me,"id","architecture-gnrale"),d(me,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(me,"href","#architecture-gnrale"),d(Y,"class","relative group"),d(fe,"id","introduction"),d(fe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(fe,"href","#introduction"),d(F,"class","relative group"),d(rt,"class","block dark:hidden"),m(rt.src,Pi="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers_blocks.svg")||d(rt,"src",Pi),d(rt,"alt","Architecture of a Transformers models"),d(nt,"class","hidden dark:block"),m(nt.src,ki="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers_blocks-dark.svg")||d(nt,"src",ki),d(nt,"alt","Architecture of a Transformers models"),d(J,"class","flex justify-center"),d(ge,"id","les-couches-dattention"),d(ge,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(ge,"href","#les-couches-dattention"),d(X,"class","relative group"),d(lt,"href","https://arxiv.org/abs/1706.03762"),d(lt,"rel","nofollow"),d(qe,"id","larchitecture-originelle"),d(qe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(qe,"href","#larchitecture-originelle"),d(Q,"class","relative group"),d(it,"class","block dark:hidden"),m(it.src,xi="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers.svg")||d(it,"src",xi),d(it,"alt","Architecture of a Transformers models"),d(ut,"class","hidden dark:block"),m(ut.src,$i="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers-dark.svg")||d(ut,"src",$i),d(ut,"alt","Architecture of a Transformers models"),d(W,"class","flex justify-center"),d(Ee,"id","architectures-contre-checkpoints"),d(Ee,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Ee,"href","#architectures-contre-checkpoints"),d(Z,"class","relative group")},m(e,i){t(document.head,M),u(e,cr,i),u(e,G,i),t(G,K),t(K,Wt),g(be,Wt,null),t(G,Xn),t(G,Zt),t(Zt,Qn),u(e,pr,i),u(e,ft,i),t(ft,Wn),u(e,mr,i),u(e,R,i),t(R,ee),t(ee,Kt),g(Te,Kt,null),t(R,Zn),t(R,es),t(es,Kn),u(e,fr,i),u(e,ht,i),t(ht,ea),u(e,hr,i),u(e,j,i),t(j,Pe),t(j,ta),t(j,ke),u(e,vr,i),u(e,xe,i),t(xe,$e),t($e,sa),t(xe,ra),u(e,gr,i),u(e,f,i),t(f,ts),t(ts,te),t(te,ss),t(ss,na),t(te,aa),t(te,we),t(we,la),t(te,oa),t(f,ia),t(f,rs),t(rs,se),t(se,ns),t(ns,ua),t(se,da),t(se,ye),t(ye,ca),t(se,pa),t(f,ma),t(f,as),t(as,re),t(re,ls),t(ls,fa),t(re,ha),t(re,Le),t(Le,va),t(re,ga),t(f,qa),t(f,os),t(os,ne),t(ne,is),t(is,_a),t(ne,Ea),t(ne,Ae),t(Ae,ba),t(ne,Ta),t(f,Pa),t(f,us),t(us,T),t(T,ds),t(ds,ka),t(T,xa),t(T,Ie),t(Ie,$a),t(T,wa),t(T,Ce),t(Ce,ya),t(T,La),t(f,Aa),t(f,cs),t(cs,P),t(P,ps),t(ps,Ia),t(P,Ca),t(P,Ne),t(Ne,Na),t(P,Ma),t(P,ms),t(ms,Ga),t(P,Ra),u(e,qr,i),u(e,vt,i),t(vt,ja),u(e,_r,i),u(e,k,i),t(k,Me),t(Me,Sa),t(Me,fs),t(fs,Ba),t(Me,Da),t(k,Oa),t(k,Ge),t(Ge,za),t(Ge,hs),t(hs,Ua),t(Ge,Va),t(k,Ha),t(k,Re),t(Re,Ya),t(Re,vs),t(vs,Fa),t(Re,Ja),u(e,Er,i),u(e,gt,i),t(gt,Xa),u(e,br,i),u(e,S,i),t(S,ae),t(ae,gs),g(je,gs,null),t(S,Qa),t(S,qs),t(qs,Wa),u(e,Tr,i),u(e,le,i),t(le,Za),t(le,_s),t(_s,Ka),t(le,el),u(e,Pr,i),u(e,oe,i),t(oe,tl),t(oe,Es),t(Es,sl),t(oe,rl),u(e,kr,i),u(e,x,i),t(x,nl),t(x,bs),t(bs,al),t(x,ll),t(x,Ts),t(Ts,ol),t(x,il),u(e,xr,i),u(e,B,i),t(B,Se),t(B,ul),t(B,Be),u(e,$r,i),u(e,ie,i),t(ie,dl),t(ie,Ps),t(Ps,cl),t(ie,pl),u(e,wr,i),u(e,D,i),t(D,De),t(D,ml),t(D,Oe),u(e,yr,i),u(e,O,i),t(O,ue),t(ue,ks),g(ze,ks,null),t(O,fl),t(O,xs),t(xs,hl),u(e,Lr,i),u(e,qt,i),t(qt,vl),u(e,Ar,i),u(e,Ue,i),t(Ue,Ve),u(e,Ir,i),u(e,_t,i),t(_t,gl),u(e,Cr,i),u(e,z,i),t(z,He),t(z,ql),t(z,Ye),u(e,Nr,i),g(Fe,e,i),u(e,Mr,i),u(e,Et,i),t(Et,_l),u(e,Gr,i),u(e,bt,i),t(bt,El),u(e,Rr,i),u(e,Tt,i),t(Tt,bl),u(e,jr,i),u(e,U,i),t(U,de),t(de,$s),g(Je,$s,null),t(U,Tl),t(U,ws),t(ws,Pl),u(e,Sr,i),g(Xe,e,i),u(e,Br,i),u(e,ce,i),t(ce,kl),t(ce,ys),t(ys,xl),t(ce,$l),u(e,Dr,i),u(e,V,i),t(V,Qe),t(V,wl),t(V,We),u(e,Or,i),u(e,Pt,i),t(Pt,yl),u(e,zr,i),u(e,$,i),t($,Ll),t($,Ls),t(Ls,Al),t($,Il),t($,As),t(As,Cl),t($,Nl),u(e,Ur,i),u(e,w,i),t(w,Is),t(Is,Ml),t(w,Gl),t(w,Cs),t(Cs,Rl),t(w,jl),t(w,Ns),t(Ns,Sl),u(e,Vr,i),u(e,pe,i),t(pe,Bl),t(pe,Ms),t(Ms,Dl),t(pe,Ol),u(e,Hr,i),u(e,H,i),t(H,Ze),t(H,zl),t(H,Ke),u(e,Yr,i),u(e,kt,i),t(kt,Ul),u(e,Fr,i),u(e,xt,i),t(xt,Vl),u(e,Jr,i),u(e,Y,i),t(Y,me),t(me,Gs),g(et,Gs,null),t(Y,Hl),t(Y,Rs),t(Rs,Yl),u(e,Xr,i),u(e,$t,i),t($t,Fl),u(e,Qr,i),g(tt,e,i),u(e,Wr,i),u(e,F,i),t(F,fe),t(fe,js),g(st,js,null),t(F,Jl),t(F,Ss),t(Ss,Xl),u(e,Zr,i),u(e,wt,i),t(wt,Ql),u(e,Kr,i),u(e,he,i),t(he,yt),t(yt,Bs),t(Bs,Wl),t(yt,Zl),t(he,Kl),t(he,Lt),t(Lt,Ds),t(Ds,eo),t(Lt,to),u(e,en,i),u(e,J,i),t(J,rt),t(J,so),t(J,nt),u(e,tn,i),u(e,At,i),t(At,ro),u(e,sn,i),u(e,y,i),t(y,It),t(It,Os),t(Os,no),t(It,ao),t(y,lo),t(y,Ct),t(Ct,zs),t(zs,oo),t(Ct,io),t(y,uo),t(y,ve),t(ve,Us),t(Us,co),t(ve,po),t(ve,Vs),t(Vs,mo),t(ve,fo),u(e,rn,i),u(e,Nt,i),t(Nt,ho),u(e,nn,i),u(e,X,i),t(X,ge),t(ge,Hs),g(at,Hs,null),t(X,vo),t(X,Ys),t(Ys,go),u(e,an,i),u(e,L,i),t(L,qo),t(L,Fs),t(Fs,_o),t(L,Eo),t(L,lt),t(lt,bo),t(L,To),u(e,ln,i),u(e,Mt,i),t(Mt,Po),u(e,on,i),u(e,Gt,i),t(Gt,ko),u(e,un,i),u(e,Rt,i),t(Rt,xo),u(e,dn,i),u(e,Q,i),t(Q,qe),t(qe,Js),g(ot,Js,null),t(Q,$o),t(Q,Xs),t(Xs,wo),u(e,cn,i),u(e,jt,i),t(jt,yo),u(e,pn,i),u(e,St,i),t(St,Lo),u(e,mn,i),u(e,Bt,i),t(Bt,Ao),u(e,fn,i),u(e,W,i),t(W,it),t(W,Io),t(W,ut),u(e,hn,i),u(e,Dt,i),t(Dt,Co),u(e,vn,i),u(e,_e,i),t(_e,No),t(_e,Qs),t(Qs,Mo),t(_e,Go),u(e,gn,i),u(e,Z,i),t(Z,Ee),t(Ee,Ws),g(dt,Ws,null),t(Z,Ro),t(Z,Zs),t(Zs,jo),u(e,qn,i),u(e,A,i),t(A,Ot),t(Ot,Ks),t(Ks,So),t(Ot,Bo),t(A,Do),t(A,zt),t(zt,er),t(er,Oo),t(zt,zo),t(A,Uo),t(A,I),t(I,tr),t(tr,Vo),t(I,Ho),t(I,sr),t(sr,Yo),t(I,Fo),t(I,rr),t(rr,Jo),t(I,Xo),u(e,_n,i),u(e,C,i),t(C,Qo),t(C,nr),t(nr,Wo),t(C,Zo),t(C,ar),t(ar,Ko),t(C,ei),En=!0},p:Ld,i(e){En||(q(be.$$.fragment,e),q(Te.$$.fragment,e),q(je.$$.fragment,e),q(ze.$$.fragment,e),q(Fe.$$.fragment,e),q(Je.$$.fragment,e),q(Xe.$$.fragment,e),q(et.$$.fragment,e),q(tt.$$.fragment,e),q(st.$$.fragment,e),q(at.$$.fragment,e),q(ot.$$.fragment,e),q(dt.$$.fragment,e),En=!0)},o(e){_(be.$$.fragment,e),_(Te.$$.fragment,e),_(je.$$.fragment,e),_(ze.$$.fragment,e),_(Fe.$$.fragment,e),_(Je.$$.fragment,e),_(Xe.$$.fragment,e),_(et.$$.fragment,e),_(tt.$$.fragment,e),_(st.$$.fragment,e),_(at.$$.fragment,e),_(ot.$$.fragment,e),_(dt.$$.fragment,e),En=!1},d(e){s(M),e&&s(cr),e&&s(G),E(be),e&&s(pr),e&&s(ft),e&&s(mr),e&&s(R),E(Te),e&&s(fr),e&&s(ht),e&&s(hr),e&&s(j),e&&s(vr),e&&s(xe),e&&s(gr),e&&s(f),e&&s(qr),e&&s(vt),e&&s(_r),e&&s(k),e&&s(Er),e&&s(gt),e&&s(br),e&&s(S),E(je),e&&s(Tr),e&&s(le),e&&s(Pr),e&&s(oe),e&&s(kr),e&&s(x),e&&s(xr),e&&s(B),e&&s($r),e&&s(ie),e&&s(wr),e&&s(D),e&&s(yr),e&&s(O),E(ze),e&&s(Lr),e&&s(qt),e&&s(Ar),e&&s(Ue),e&&s(Ir),e&&s(_t),e&&s(Cr),e&&s(z),e&&s(Nr),E(Fe,e),e&&s(Mr),e&&s(Et),e&&s(Gr),e&&s(bt),e&&s(Rr),e&&s(Tt),e&&s(jr),e&&s(U),E(Je),e&&s(Sr),E(Xe,e),e&&s(Br),e&&s(ce),e&&s(Dr),e&&s(V),e&&s(Or),e&&s(Pt),e&&s(zr),e&&s($),e&&s(Ur),e&&s(w),e&&s(Vr),e&&s(pe),e&&s(Hr),e&&s(H),e&&s(Yr),e&&s(kt),e&&s(Fr),e&&s(xt),e&&s(Jr),e&&s(Y),E(et),e&&s(Xr),e&&s($t),e&&s(Qr),E(tt,e),e&&s(Wr),e&&s(F),E(st),e&&s(Zr),e&&s(wt),e&&s(Kr),e&&s(he),e&&s(en),e&&s(J),e&&s(tn),e&&s(At),e&&s(sn),e&&s(y),e&&s(rn),e&&s(Nt),e&&s(nn),e&&s(X),E(at),e&&s(an),e&&s(L),e&&s(ln),e&&s(Mt),e&&s(on),e&&s(Gt),e&&s(un),e&&s(Rt),e&&s(dn),e&&s(Q),E(ot),e&&s(cn),e&&s(jt),e&&s(pn),e&&s(St),e&&s(mn),e&&s(Bt),e&&s(fn),e&&s(W),e&&s(hn),e&&s(Dt),e&&s(vn),e&&s(_e),e&&s(gn),e&&s(Z),E(dt),e&&s(qn),e&&s(A),e&&s(_n),e&&s(C)}}}const Cd={local:"comment-fonctionnent-les-modles-transformers",sections:[{local:"court-historique-des-transformers",title:"Court historique des Transformers"},{local:"transformers-des-modles-du-langage",title:"Transformers, des mod\xE8les du langage"},{local:"les-modles-transformers-sont-normes",title:"Les mod\xE8les Transformers sont \xE9normes"},{local:"le-transfer-learning",title:"Le Transfer Learning"},{local:"architecture-gnrale",title:"Architecture g\xE9n\xE9rale"},{local:"introduction",title:"Introduction"},{local:"les-couches-dattention",title:"Les couches d'attention"},{local:"larchitecture-originelle",title:"L'architecture originelle"},{local:"architectures-contre-checkpoints",title:"Architectures contre checkpoints"}],title:"Comment fonctionnent les mod\xE8les Transformers ?"};function Nd(ui){return Ad(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class jd extends xd{constructor(M){super();$d(this,M,Nd,Id,wd,{})}}export{jd as default,Cd as metadata};
