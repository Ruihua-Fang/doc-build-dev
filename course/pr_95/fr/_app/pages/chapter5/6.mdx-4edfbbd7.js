import{S as Pa,i as Ia,s as Oa,O as De,P as Ce,a as r,d as s,b as $,g as d,F as t,L as Na,e as l,w as k,k as p,t as n,c as i,x as C,m,h as a,y,n as ns,o as w,B as j,p as as,q,G as Tu,M as Au,N as ku,f as Sa,v as Su}from"../../chunks/vendor-e6c5d93e.js";import{Y as Nu,T as yu,D as Cu}from"../../chunks/DocNotebookDropdown-b3e38740.js";import{I as gn}from"../../chunks/IconCopyLink-7b8d27fe.js";import{C as O}from"../../chunks/CodeBlock-37867453.js";function Pu(T){let o,h,c,v,g,_;return{c(){o=De("svg"),h=De("defs"),c=De("clipPath"),v=De("rect"),g=De("g"),_=De("path"),this.h()},l(x){o=Ce(x,"svg",{class:!0,xmlns:!0,"xmlns:xlink":!0,"aria-hidden":!0,focusable:!0,role:!0,width:!0,height:!0,preserveAspectRatio:!0,viewBox:!0});var b=r(o);h=Ce(b,"defs",{});var D=r(h);c=Ce(D,"clipPath",{id:!0});var E=r(c);v=Ce(E,"rect",{x:!0,y:!0,width:!0,height:!0,fill:!0}),r(v).forEach(s),E.forEach(s),D.forEach(s),g=Ce(b,"g",{"clip-path":!0});var R=r(g);_=Ce(R,"path",{d:!0,fill:!0}),r(_).forEach(s),R.forEach(s),b.forEach(s),this.h()},h(){$(v,"x","3.05"),$(v,"y","0.5"),$(v,"width","25.73"),$(v,"height","31"),$(v,"fill","none"),$(c,"id","a"),$(_,"d","M24.94,9.51a12.81,12.81,0,0,1,0,18.16,12.68,12.68,0,0,1-18,0,12.81,12.81,0,0,1,0-18.16l9-9V5l-.84.83-6,6a9.58,9.58,0,1,0,13.55,0ZM20.44,9a1.68,1.68,0,1,1,1.67-1.67A1.68,1.68,0,0,1,20.44,9Z"),$(_,"fill","#ee4c2c"),$(g,"clip-path","url(#a)"),$(o,"class",T[0]),$(o,"xmlns","http://www.w3.org/2000/svg"),$(o,"xmlns:xlink","http://www.w3.org/1999/xlink"),$(o,"aria-hidden","true"),$(o,"focusable","false"),$(o,"role","img"),$(o,"width","1em"),$(o,"height","1em"),$(o,"preserveAspectRatio","xMidYMid meet"),$(o,"viewBox","0 0 32 32")},m(x,b){d(x,o,b),t(o,h),t(h,c),t(c,v),t(o,g),t(g,_)},p(x,[b]){b&1&&$(o,"class",x[0])},i:Na,o:Na,d(x){x&&s(o)}}}function Iu(T,o,h){let{classNames:c=""}=o;return T.$$set=v=>{"classNames"in v&&h(0,c=v.classNames)},[c]}class Ou extends Pa{constructor(o){super();Ia(this,o,Iu,Pu,Oa,{classNames:0})}}function Mu(T){let o,h,c,v;return{c(){o=De("svg"),h=De("path"),c=De("path"),v=De("path"),this.h()},l(g){o=Ce(g,"svg",{class:!0,xmlns:!0,"xmlns:xlink":!0,"aria-hidden":!0,focusable:!0,role:!0,width:!0,height:!0,preserveAspectRatio:!0,viewBox:!0});var _=r(o);h=Ce(_,"path",{d:!0,fill:!0}),r(h).forEach(s),c=Ce(_,"path",{d:!0,fill:!0}),r(c).forEach(s),v=Ce(_,"path",{d:!0,fill:!0}),r(v).forEach(s),_.forEach(s),this.h()},h(){$(h,"d","M145.726 42.065v42.07l72.861 42.07v-42.07l-72.86-42.07zM0 84.135v42.07l36.43 21.03V105.17L0 84.135zm109.291 21.035l-36.43 21.034v126.2l36.43 21.035v-84.135l36.435 21.035v-42.07l-36.435-21.034V105.17z"),$(h,"fill","#E55B2D"),$(c,"d","M145.726 42.065L36.43 105.17v42.065l72.861-42.065v42.065l36.435-21.03v-84.14zM255.022 63.1l-36.435 21.035v42.07l36.435-21.035V63.1zm-72.865 84.135l-36.43 21.035v42.07l36.43-21.036v-42.07zm-36.43 63.104l-36.436-21.035v84.135l36.435-21.035V210.34z"),$(c,"fill","#ED8E24"),$(v,"d","M145.726 0L0 84.135l36.43 21.035l109.296-63.105l72.861 42.07L255.022 63.1L145.726 0zm0 126.204l-36.435 21.03l36.435 21.036l36.43-21.035l-36.43-21.03z"),$(v,"fill","#F8BF3C"),$(o,"class",T[0]),$(o,"xmlns","http://www.w3.org/2000/svg"),$(o,"xmlns:xlink","http://www.w3.org/1999/xlink"),$(o,"aria-hidden","true"),$(o,"focusable","false"),$(o,"role","img"),$(o,"width","0.94em"),$(o,"height","1em"),$(o,"preserveAspectRatio","xMidYMid meet"),$(o,"viewBox","0 0 256 274")},m(g,_){d(g,o,_),t(o,h),t(o,c),t(o,v)},p(g,[_]){_&1&&$(o,"class",g[0])},i:Na,o:Na,d(g){g&&s(o)}}}function Ru(T,o,h){let{classNames:c=""}=o;return T.$$set=v=>{"classNames"in v&&h(0,c=v.classNames)},[c]}class zu extends Pa{constructor(o){super();Ia(this,o,Ru,Mu,Oa,{classNames:0})}}function ju(T,o,h){const c=T.slice();return c[2]=o[h],c[4]=h,c}function Du(T){let o,h,c,v=T[2].name+"",g,_,x,b,D;var E=T[2].icon;function R(A){return{props:{classNames:"mr-1.5"}}}return E&&(h=new E(R())),{c(){o=l("a"),h&&k(h.$$.fragment),c=p(),g=n(v),_=p(),this.h()},l(A){o=i(A,"A",{class:!0,href:!0});var S=r(o);h&&C(h.$$.fragment,S),c=m(S),g=a(S,v),_=m(S),S.forEach(s),this.h()},h(){$(o,"class",x="flex justify-center flex-1 py-1.5 px-2.5 focus:outline-none !no-underline rounded-"+(T[4]?"r":"l")+" "+(T[2].id===T[0]?T[2].classNames:"text-gray-500 filter grayscale")),$(o,"href",b="?fw="+T[2].id)},m(A,S){d(A,o,S),h&&y(h,o,null),t(o,c),t(o,g),t(o,_),D=!0},p(A,S){if(E!==(E=A[2].icon)){if(h){ns();const M=h;w(M.$$.fragment,1,0,()=>{j(M,1)}),as()}E?(h=new E(R()),k(h.$$.fragment),q(h.$$.fragment,1),y(h,o,c)):h=null}(!D||S&1&&x!==(x="flex justify-center flex-1 py-1.5 px-2.5 focus:outline-none !no-underline rounded-"+(A[4]?"r":"l")+" "+(A[2].id===A[0]?A[2].classNames:"text-gray-500 filter grayscale")))&&$(o,"class",x)},i(A){D||(h&&q(h.$$.fragment,A),D=!0)},o(A){h&&w(h.$$.fragment,A),D=!1},d(A){A&&s(o),h&&j(h)}}}function Lu(T){let o,h,c=T[1],v=[];for(let _=0;_<c.length;_+=1)v[_]=Du(ju(T,c,_));const g=_=>w(v[_],1,1,()=>{v[_]=null});return{c(){o=l("div");for(let _=0;_<v.length;_+=1)v[_].c();this.h()},l(_){o=i(_,"DIV",{class:!0});var x=r(o);for(let b=0;b<v.length;b+=1)v[b].l(x);x.forEach(s),this.h()},h(){$(o,"class","bg-white leading-none border border-gray-100 rounded-lg flex p-0.5 w-56 text-sm mb-4")},m(_,x){d(_,o,x);for(let b=0;b<v.length;b+=1)v[b].m(o,null);h=!0},p(_,[x]){if(x&3){c=_[1];let b;for(b=0;b<c.length;b+=1){const D=ju(_,c,b);v[b]?(v[b].p(D,x),q(v[b],1)):(v[b]=Du(D),v[b].c(),q(v[b],1),v[b].m(o,null))}for(ns(),b=c.length;b<v.length;b+=1)g(b);as()}},i(_){if(!h){for(let x=0;x<c.length;x+=1)q(v[x]);h=!0}},o(_){v=v.filter(Boolean);for(let x=0;x<v.length;x+=1)w(v[x]);h=!1},d(_){_&&s(o),Tu(v,_)}}}function Fu(T,o,h){let{fw:c}=o;const v=[{id:"pt",classNames:"bg-red-50 dark:bg-transparent text-red-600",icon:Ou,name:"Pytorch"},{id:"tf",classNames:"bg-orange-50 dark:bg-transparent text-yellow-600",icon:zu,name:"TensorFlow"}];return T.$$set=g=>{"fw"in g&&h(0,c=g.fw)},[c,v]}class Hu extends Pa{constructor(o){super();Ia(this,o,Fu,Lu,Oa,{fw:0})}}function Uu(T){let o,h;return o=new Cu({props:{classNames:"absolute z-10 right-0 top-0",options:[{label:"Google Colab",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter5/section6_tf.ipynb"},{label:"Aws Studio",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter5/section6_tf.ipynb"}]}}),{c(){k(o.$$.fragment)},l(c){C(o.$$.fragment,c)},m(c,v){y(o,c,v),h=!0},i(c){h||(q(o.$$.fragment,c),h=!0)},o(c){w(o.$$.fragment,c),h=!1},d(c){j(o,c)}}}function Gu(T){let o,h;return o=new Cu({props:{classNames:"absolute z-10 right-0 top-0",options:[{label:"Google Colab",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter5/section6_pt.ipynb"},{label:"Aws Studio",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter5/section6_pt.ipynb"}]}}),{c(){k(o.$$.fragment)},l(c){C(o.$$.fragment,c)},m(c,v){y(o,c,v),h=!0},i(c){h||(q(o.$$.fragment,c),h=!0)},o(c){w(o.$$.fragment,c),h=!1},d(c){j(o,c)}}}function Vu(T){let o,h,c,v,g,_,x,b,D,E,R,A,S,M,z,F,G,P,L,Y;return{c(){o=l("p"),h=n("\u270F\uFE0F "),c=l("strong"),v=n("Essayez-le !"),g=n(" Voyez si vous pouvez utiliser "),_=l("code"),x=n("Dataset.map()"),b=n(" pour exploser la colonne "),D=l("code"),E=n("comments"),R=n(" de "),A=l("code"),S=n("issues_dataset"),M=p(),z=l("em"),F=n("sans"),G=n(" recourir \xE0 l\u2019utilisation de Pandas. C\u2019est un peu d\xE9licat; vous pourriez trouver la section "),P=l("a"),L=n("\u201CBatch mapping\u201D"),Y=n(" de la documentation \u{1F917} Datasets utile pour cette t\xE2che."),this.h()},l(U){o=i(U,"P",{});var N=r(o);h=a(N,"\u270F\uFE0F "),c=i(N,"STRONG",{});var V=r(c);v=a(V,"Essayez-le !"),V.forEach(s),g=a(N," Voyez si vous pouvez utiliser "),_=i(N,"CODE",{});var f=r(_);x=a(f,"Dataset.map()"),f.forEach(s),b=a(N," pour exploser la colonne "),D=i(N,"CODE",{});var I=r(D);E=a(I,"comments"),I.forEach(s),R=a(N," de "),A=i(N,"CODE",{});var H=r(A);S=a(H,"issues_dataset"),H.forEach(s),M=m(N),z=i(N,"EM",{});var W=r(z);F=a(W,"sans"),W.forEach(s),G=a(N," recourir \xE0 l\u2019utilisation de Pandas. C\u2019est un peu d\xE9licat; vous pourriez trouver la section "),P=i(N,"A",{href:!0,rel:!0});var ae=r(P);L=a(ae,"\u201CBatch mapping\u201D"),ae.forEach(s),Y=a(N," de la documentation \u{1F917} Datasets utile pour cette t\xE2che."),N.forEach(s),this.h()},h(){$(P,"href","https://huggingface.co/docs/datasets/v1.12.1/about_map_batch.html?batch-mapping#batch-mapping"),$(P,"rel","nofollow")},m(U,N){d(U,o,N),t(o,h),t(o,c),t(c,v),t(o,g),t(o,_),t(_,x),t(o,b),t(o,D),t(D,E),t(o,R),t(o,A),t(A,S),t(o,M),t(o,z),t(z,F),t(o,G),t(o,P),t(P,L),t(o,Y)},d(U){U&&s(o)}}}function Bu(T){let o,h,c,v,g,_,x,b,D,E,R,A,S,M,z,F,G;return o=new O({props:{code:`from transformers import AutoTokenizer, TFAutoModel

model_ckpt = "sentence-transformers/multi-qa-mpnet-base-dot-v1"
tokenizer = AutoTokenizer.from_pretrained(model_ckpt)
model = TFAutoModel.from_pretrained(model_ckpt, from_pt=True)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, TFAutoModel

model_ckpt = <span class="hljs-string">&quot;sentence-transformers/multi-qa-mpnet-base-dot-v1&quot;</span>
tokenizer = AutoTokenizer.from_pretrained(model_ckpt)
model = TFAutoModel.from_pretrained(model_ckpt, from_pt=<span class="hljs-literal">True</span>)`}}),{c(){k(o.$$.fragment),h=p(),c=l("p"),v=n("Notez que nous avons d\xE9fini "),g=l("code"),_=n("from_pt=True"),x=n(" comme argument de la m\xE9thode "),b=l("code"),D=n("from_pretrained()"),E=n(". C\u2019est parce que le point de contr\xF4le "),R=l("code"),A=n("multi-qa-mpnet-base-dot-v1"),S=n(" n\u2019a que des poids PyTorch, donc d\xE9finir "),M=l("code"),z=n("from_pt=True"),F=n(" les convertira automatiquement au format TensorFlow pour nous. Comme vous pouvez le voir, il est tr\xE8s simple de passer d\u2019un framework \xE0 l\u2019autre dans \u{1F917} Transformers !")},l(P){C(o.$$.fragment,P),h=m(P),c=i(P,"P",{});var L=r(c);v=a(L,"Notez que nous avons d\xE9fini "),g=i(L,"CODE",{});var Y=r(g);_=a(Y,"from_pt=True"),Y.forEach(s),x=a(L," comme argument de la m\xE9thode "),b=i(L,"CODE",{});var U=r(b);D=a(U,"from_pretrained()"),U.forEach(s),E=a(L,". C\u2019est parce que le point de contr\xF4le "),R=i(L,"CODE",{});var N=r(R);A=a(N,"multi-qa-mpnet-base-dot-v1"),N.forEach(s),S=a(L," n\u2019a que des poids PyTorch, donc d\xE9finir "),M=i(L,"CODE",{});var V=r(M);z=a(V,"from_pt=True"),V.forEach(s),F=a(L," les convertira automatiquement au format TensorFlow pour nous. Comme vous pouvez le voir, il est tr\xE8s simple de passer d\u2019un framework \xE0 l\u2019autre dans \u{1F917} Transformers !"),L.forEach(s)},m(P,L){y(o,P,L),d(P,h,L),d(P,c,L),t(c,v),t(c,g),t(g,_),t(c,x),t(c,b),t(b,D),t(c,E),t(c,R),t(R,A),t(c,S),t(c,M),t(M,z),t(c,F),G=!0},i(P){G||(q(o.$$.fragment,P),G=!0)},o(P){w(o.$$.fragment,P),G=!1},d(P){j(o,P),P&&s(h),P&&s(c)}}}function Yu(T){let o,h,c,v,g,_,x;return o=new O({props:{code:`from transformers import AutoTokenizer, AutoModel

model_ckpt = "sentence-transformers/multi-qa-mpnet-base-dot-v1"
tokenizer = AutoTokenizer.from_pretrained(model_ckpt)
model = AutoModel.from_pretrained(model_ckpt)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, AutoModel

model_ckpt = <span class="hljs-string">&quot;sentence-transformers/multi-qa-mpnet-base-dot-v1&quot;</span>
tokenizer = AutoTokenizer.from_pretrained(model_ckpt)
model = AutoModel.from_pretrained(model_ckpt)`}}),_=new O({props:{code:`import torch

device = torch.device("cuda")
model.to(device)`,highlighted:`<span class="hljs-keyword">import</span> torch

device = torch.device(<span class="hljs-string">&quot;cuda&quot;</span>)
model.to(device)`}}),{c(){k(o.$$.fragment),h=p(),c=l("p"),v=n("Pour acc\xE9l\xE9rer le processus d\u2019int\xE9gration, il est utile de placer le mod\xE8le et les entr\xE9es sur un p\xE9riph\xE9rique GPU, alors faisons-le maintenant\xA0:"),g=p(),k(_.$$.fragment)},l(b){C(o.$$.fragment,b),h=m(b),c=i(b,"P",{});var D=r(c);v=a(D,"Pour acc\xE9l\xE9rer le processus d\u2019int\xE9gration, il est utile de placer le mod\xE8le et les entr\xE9es sur un p\xE9riph\xE9rique GPU, alors faisons-le maintenant\xA0:"),D.forEach(s),g=m(b),C(_.$$.fragment,b)},m(b,D){y(o,b,D),d(b,h,D),d(b,c,D),t(c,v),d(b,g,D),y(_,b,D),x=!0},i(b){x||(q(o.$$.fragment,b),q(_.$$.fragment,b),x=!0)},o(b){w(o.$$.fragment,b),w(_.$$.fragment,b),x=!1},d(b){j(o,b),b&&s(h),b&&s(c),b&&s(g),j(_,b)}}}function Wu(T){let o,h,c,v,g,_,x,b,D,E,R,A,S,M,z,F,G,P,L,Y,U,N,V;return o=new O({props:{code:`def get_embeddings(text_list):
    encoded_input = tokenizer(
        text_list, padding=True, truncation=True, return_tensors="tf"
    )
    encoded_input = {k: v for k, v in encoded_input.items()}
    model_output = model(**encoded_input)
    return cls_pooling(model_output)`,highlighted:`<span class="hljs-keyword">def</span> <span class="hljs-title function_">get_embeddings</span>(<span class="hljs-params">text_list</span>):
    encoded_input = tokenizer(
        text_list, padding=<span class="hljs-literal">True</span>, truncation=<span class="hljs-literal">True</span>, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>
    )
    encoded_input = {k: v <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> encoded_input.items()}
    model_output = model(**encoded_input)
    <span class="hljs-keyword">return</span> cls_pooling(model_output)`}}),_=new O({props:{code:`embedding = get_embeddings(comments_dataset["text"][0])
embedding.shape`,highlighted:`embedding = get_embeddings(comments_dataset[<span class="hljs-string">&quot;text&quot;</span>][<span class="hljs-number">0</span>])
embedding.shape`}}),b=new O({props:{code:"TensorShape([1, 768])",highlighted:'TensorShape([<span class="hljs-number">1</span>, <span class="hljs-number">768</span>])'}}),N=new O({props:{code:`embeddings_dataset = comments_dataset.map(
    lambda x: {"embeddings": get_embeddings(x["text"]).numpy()[0]}
)`,highlighted:`embeddings_dataset = comments_dataset.<span class="hljs-built_in">map</span>(
    <span class="hljs-keyword">lambda</span> x: {<span class="hljs-string">&quot;embeddings&quot;</span>: get_embeddings(x[<span class="hljs-string">&quot;text&quot;</span>]).numpy()[<span class="hljs-number">0</span>]}
)`}}),{c(){k(o.$$.fragment),h=p(),c=l("p"),v=n("Nous pouvons tester le fonctionnement de la fonction en lui fournissant la premi\xE8re entr\xE9e de texte dans notre corpus et en inspectant la forme de sortie\xA0:"),g=p(),k(_.$$.fragment),x=p(),k(b.$$.fragment),D=p(),E=l("p"),R=n("Super, nous avons converti la premi\xE8re entr\xE9e de notre corpus en un vecteur \xE0 768 dimensions ! Nous pouvons utiliser "),A=l("code"),S=n("Dataset.map()"),M=n(" pour appliquer notre fonction "),z=l("code"),F=n("get_embeddings()"),G=n(" \xE0 chaque ligne de notre corpus, cr\xE9ons donc une nouvelle colonne "),P=l("code"),L=n("embeddings"),Y=n(" comme suit\xA0:"),U=p(),k(N.$$.fragment)},l(f){C(o.$$.fragment,f),h=m(f),c=i(f,"P",{});var I=r(c);v=a(I,"Nous pouvons tester le fonctionnement de la fonction en lui fournissant la premi\xE8re entr\xE9e de texte dans notre corpus et en inspectant la forme de sortie\xA0:"),I.forEach(s),g=m(f),C(_.$$.fragment,f),x=m(f),C(b.$$.fragment,f),D=m(f),E=i(f,"P",{});var H=r(E);R=a(H,"Super, nous avons converti la premi\xE8re entr\xE9e de notre corpus en un vecteur \xE0 768 dimensions ! Nous pouvons utiliser "),A=i(H,"CODE",{});var W=r(A);S=a(W,"Dataset.map()"),W.forEach(s),M=a(H," pour appliquer notre fonction "),z=i(H,"CODE",{});var ae=r(z);F=a(ae,"get_embeddings()"),ae.forEach(s),G=a(H," \xE0 chaque ligne de notre corpus, cr\xE9ons donc une nouvelle colonne "),P=i(H,"CODE",{});var ie=r(P);L=a(ie,"embeddings"),ie.forEach(s),Y=a(H," comme suit\xA0:"),H.forEach(s),U=m(f),C(N.$$.fragment,f)},m(f,I){y(o,f,I),d(f,h,I),d(f,c,I),t(c,v),d(f,g,I),y(_,f,I),d(f,x,I),y(b,f,I),d(f,D,I),d(f,E,I),t(E,R),t(E,A),t(A,S),t(E,M),t(E,z),t(z,F),t(E,G),t(E,P),t(P,L),t(E,Y),d(f,U,I),y(N,f,I),V=!0},i(f){V||(q(o.$$.fragment,f),q(_.$$.fragment,f),q(b.$$.fragment,f),q(N.$$.fragment,f),V=!0)},o(f){w(o.$$.fragment,f),w(_.$$.fragment,f),w(b.$$.fragment,f),w(N.$$.fragment,f),V=!1},d(f){j(o,f),f&&s(h),f&&s(c),f&&s(g),j(_,f),f&&s(x),j(b,f),f&&s(D),f&&s(E),f&&s(U),j(N,f)}}}function Zu(T){let o,h,c,v,g,_,x,b,D,E,R,A,S,M,z,F,G,P,L,Y,U,N,V;return o=new O({props:{code:`def get_embeddings(text_list):
    encoded_input = tokenizer(
        text_list, padding=True, truncation=True, return_tensors="pt"
    )
    encoded_input = {k: v.to(device) for k, v in encoded_input.items()}
    model_output = model(**encoded_input)
    return cls_pooling(model_output)`,highlighted:`<span class="hljs-keyword">def</span> <span class="hljs-title function_">get_embeddings</span>(<span class="hljs-params">text_list</span>):
    encoded_input = tokenizer(
        text_list, padding=<span class="hljs-literal">True</span>, truncation=<span class="hljs-literal">True</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>
    )
    encoded_input = {k: v.to(device) <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> encoded_input.items()}
    model_output = model(**encoded_input)
    <span class="hljs-keyword">return</span> cls_pooling(model_output)`}}),_=new O({props:{code:`embedding = get_embeddings(comments_dataset["text"][0])
embedding.shape`,highlighted:`embedding = get_embeddings(comments_dataset[<span class="hljs-string">&quot;text&quot;</span>][<span class="hljs-number">0</span>])
embedding.shape`}}),b=new O({props:{code:"torch.Size([1, 768])",highlighted:'torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">768</span>])'}}),N=new O({props:{code:`embeddings_dataset = comments_dataset.map(
    lambda x: {"embeddings": get_embeddings(x["text"]).detach().cpu().numpy()[0]}
)`,highlighted:`embeddings_dataset = comments_dataset.<span class="hljs-built_in">map</span>(
    <span class="hljs-keyword">lambda</span> x: {<span class="hljs-string">&quot;embeddings&quot;</span>: get_embeddings(x[<span class="hljs-string">&quot;text&quot;</span>]).detach().cpu().numpy()[<span class="hljs-number">0</span>]}
)`}}),{c(){k(o.$$.fragment),h=p(),c=l("p"),v=n("Nous pouvons tester le fonctionnement de la fonction en lui fournissant la premi\xE8re entr\xE9e de texte dans notre corpus et en inspectant la forme de sortie\xA0:"),g=p(),k(_.$$.fragment),x=p(),k(b.$$.fragment),D=p(),E=l("p"),R=n("Super, nous avons converti la premi\xE8re entr\xE9e de notre corpus en un vecteur \xE0 768 dimensions ! Nous pouvons utiliser "),A=l("code"),S=n("Dataset.map()"),M=n(" pour appliquer notre fonction "),z=l("code"),F=n("get_embeddings()"),G=n(" \xE0 chaque ligne de notre corpus, cr\xE9ons donc une nouvelle colonne "),P=l("code"),L=n("embeddings"),Y=n(" comme suit\xA0:"),U=p(),k(N.$$.fragment)},l(f){C(o.$$.fragment,f),h=m(f),c=i(f,"P",{});var I=r(c);v=a(I,"Nous pouvons tester le fonctionnement de la fonction en lui fournissant la premi\xE8re entr\xE9e de texte dans notre corpus et en inspectant la forme de sortie\xA0:"),I.forEach(s),g=m(f),C(_.$$.fragment,f),x=m(f),C(b.$$.fragment,f),D=m(f),E=i(f,"P",{});var H=r(E);R=a(H,"Super, nous avons converti la premi\xE8re entr\xE9e de notre corpus en un vecteur \xE0 768 dimensions ! Nous pouvons utiliser "),A=i(H,"CODE",{});var W=r(A);S=a(W,"Dataset.map()"),W.forEach(s),M=a(H," pour appliquer notre fonction "),z=i(H,"CODE",{});var ae=r(z);F=a(ae,"get_embeddings()"),ae.forEach(s),G=a(H," \xE0 chaque ligne de notre corpus, cr\xE9ons donc une nouvelle colonne "),P=i(H,"CODE",{});var ie=r(P);L=a(ie,"embeddings"),ie.forEach(s),Y=a(H," comme suit\xA0:"),H.forEach(s),U=m(f),C(N.$$.fragment,f)},m(f,I){y(o,f,I),d(f,h,I),d(f,c,I),t(c,v),d(f,g,I),y(_,f,I),d(f,x,I),y(b,f,I),d(f,D,I),d(f,E,I),t(E,R),t(E,A),t(A,S),t(E,M),t(E,z),t(z,F),t(E,G),t(E,P),t(P,L),t(E,Y),d(f,U,I),y(N,f,I),V=!0},i(f){V||(q(o.$$.fragment,f),q(_.$$.fragment,f),q(b.$$.fragment,f),q(N.$$.fragment,f),V=!0)},o(f){w(o.$$.fragment,f),w(_.$$.fragment,f),w(b.$$.fragment,f),w(N.$$.fragment,f),V=!1},d(f){j(o,f),f&&s(h),f&&s(c),f&&s(g),j(_,f),f&&s(x),j(b,f),f&&s(D),f&&s(E),f&&s(U),j(N,f)}}}function Ju(T){let o,h,c,v;return o=new O({props:{code:`question = "How can I load a dataset offline?"
question_embedding = get_embeddings([question]).numpy()
question_embedding.shape`,highlighted:`question = <span class="hljs-string">&quot;How can I load a dataset offline?&quot;</span>
question_embedding = get_embeddings([question]).numpy()
question_embedding.shape`}}),c=new O({props:{code:"(1, 768)",highlighted:'(<span class="hljs-number">1</span>, <span class="hljs-number">768</span>)'}}),{c(){k(o.$$.fragment),h=p(),k(c.$$.fragment)},l(g){C(o.$$.fragment,g),h=m(g),C(c.$$.fragment,g)},m(g,_){y(o,g,_),d(g,h,_),y(c,g,_),v=!0},i(g){v||(q(o.$$.fragment,g),q(c.$$.fragment,g),v=!0)},o(g){w(o.$$.fragment,g),w(c.$$.fragment,g),v=!1},d(g){j(o,g),g&&s(h),j(c,g)}}}function Ku(T){let o,h,c,v;return o=new O({props:{code:`question = "How can I load a dataset offline?"
question_embedding = get_embeddings([question]).cpu().detach().numpy()
question_embedding.shape`,highlighted:`question = <span class="hljs-string">&quot;How can I load a dataset offline?&quot;</span>
question_embedding = get_embeddings([question]).cpu().detach().numpy()
question_embedding.shape`}}),c=new O({props:{code:"torch.Size([1, 768])",highlighted:'torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">768</span>])'}}),{c(){k(o.$$.fragment),h=p(),k(c.$$.fragment)},l(g){C(o.$$.fragment,g),h=m(g),C(c.$$.fragment,g)},m(g,_){y(o,g,_),d(g,h,_),y(c,g,_),v=!0},i(g){v||(q(o.$$.fragment,g),q(c.$$.fragment,g),v=!0)},o(g){w(o.$$.fragment,g),w(c.$$.fragment,g),v=!1},d(g){j(o,g),g&&s(h),j(c,g)}}}function Qu(T){let o,h,c,v,g,_,x,b,D,E,R;return{c(){o=l("p"),h=n("\u270F\uFE0F "),c=l("strong"),v=n("Essayez-le\xA0!"),g=n(" Cr\xE9ez votre propre requ\xEAte et voyez si vous pouvez trouver une r\xE9ponse dans les documents r\xE9cup\xE9r\xE9s. Vous devrez peut-\xEAtre augmenter le param\xE8tre "),_=l("code"),x=n("k"),b=n(" dans "),D=l("code"),E=n("Dataset.get_nearest_examples()"),R=n(" pour \xE9largir la recherche.")},l(A){o=i(A,"P",{});var S=r(o);h=a(S,"\u270F\uFE0F "),c=i(S,"STRONG",{});var M=r(c);v=a(M,"Essayez-le\xA0!"),M.forEach(s),g=a(S," Cr\xE9ez votre propre requ\xEAte et voyez si vous pouvez trouver une r\xE9ponse dans les documents r\xE9cup\xE9r\xE9s. Vous devrez peut-\xEAtre augmenter le param\xE8tre "),_=i(S,"CODE",{});var z=r(_);x=a(z,"k"),z.forEach(s),b=a(S," dans "),D=i(S,"CODE",{});var F=r(D);E=a(F,"Dataset.get_nearest_examples()"),F.forEach(s),R=a(S," pour \xE9largir la recherche."),S.forEach(s)},m(A,S){d(A,o,S),t(o,h),t(o,c),t(c,v),t(o,g),t(o,_),t(_,x),t(o,b),t(o,D),t(D,E),t(o,R)},d(A){A&&s(o)}}}function Xu(T){let o,h,c,v,g,_,x,b,D,E,R,A,S,M,z,F,G,P,L,Y,U,N,V,f,I,H,W,ae,ie,Ma,vn,_e,Ra,At,za,La,os,Fa,Ha,bn,St,Ua,$n,Te,Ye,Ol,Ga,We,Ml,qn,Ae,Pe,rs,Ze,Va,ls,Ba,xn,Nt,Ya,wn,Je,En,ge,Wa,is,Za,Ja,Pt,Ka,Qa,kn,Ke,yn,Qe,jn,Z,Xa,us,eo,to,cs,so,no,ds,ao,oo,ps,ro,lo,ms,io,uo,Dn,Xe,Cn,et,Tn,J,co,fs,po,mo,hs,fo,ho,_s,_o,go,gs,vo,bo,vs,$o,qo,An,tt,Sn,st,Nn,oe,xo,bs,wo,Eo,$s,ko,yo,nt,qs,jo,Do,Pn,at,In,Ie,Co,xs,To,Ao,On,ot,Mn,rt,Rn,Oe,So,ws,No,Po,zn,lt,Ln,X,Es,Q,Fn,Io,ks,Oo,Mo,ys,Ro,zo,js,Lo,Fo,Ds,Ho,Uo,ue,ee,Cs,Go,Vo,Ts,Bo,Yo,As,Wo,Zo,Ss,Jo,Ko,Ns,Qo,Xo,te,Ps,er,tr,Is,sr,nr,Os,ar,or,Ms,rr,lr,Rs,ir,ur,se,zs,cr,dr,Ls,pr,mr,Fs,fr,hr,Hs,_r,gr,Us,vr,br,ne,Gs,$r,qr,Vs,xr,wr,Bs,Er,kr,Ys,yr,jr,Ws,Dr,Hn,ve,Cr,Zs,Tr,Ar,Js,Sr,Nr,Un,it,Gn,ut,Vn,It,Pr,Bn,Me,Yn,Re,Ir,Ks,Or,Mr,Wn,ct,Zn,Ot,Rr,Jn,dt,Kn,pt,Qn,ze,zr,Qs,Lr,Fr,Xn,mt,ea,Mt,Hr,ta,Se,Le,Xs,ft,Ur,en,Gr,sa,B,Vr,Rt,Br,Yr,tn,Wr,Zr,sn,Jr,Kr,ht,Qr,Xr,nn,el,tl,_t,sl,nl,an,al,ol,na,ce,de,zt,be,rl,on,ll,il,rn,ul,cl,aa,gt,oa,Lt,dl,ra,pe,me,Ft,Ht,pl,la,Ne,Fe,ln,vt,ml,un,fl,ia,$e,hl,cn,_l,gl,bt,vl,bl,ua,qe,$l,dn,ql,xl,pn,wl,El,ca,$t,da,He,kl,mn,yl,jl,pa,fe,he,Ut,Gt,Dl,ma,qt,fa,xe,Cl,fn,Tl,Al,hn,Sl,Nl,ha,xt,_a,Vt,Pl,ga,wt,va,Et,ba,Bt,Il,$a,Ue,qa;c=new Hu({props:{fw:T[0]}}),b=new gn({});const Rl=[Gu,Uu],kt=[];function zl(e,u){return e[0]==="pt"?0:1}S=zl(T),M=kt[S]=Rl[S](T),N=new Nu({props:{id:"OATCgQtNX2o"}}),W=new gn({}),Ze=new gn({}),Je=new O({props:{code:`from huggingface_hub import hf_hub_url

data_files = hf_hub_url(
    repo_id="lewtun/github-issues",
    filename="datasets-issues-with-comments.jsonl",
    repo_type="dataset",
)`,highlighted:`<span class="hljs-keyword">from</span> huggingface_hub <span class="hljs-keyword">import</span> hf_hub_url

data_files = hf_hub_url(
    repo_id=<span class="hljs-string">&quot;lewtun/github-issues&quot;</span>,
    filename=<span class="hljs-string">&quot;datasets-issues-with-comments.jsonl&quot;</span>,
    repo_type=<span class="hljs-string">&quot;dataset&quot;</span>,
)`}}),Ke=new O({props:{code:`from datasets import load_dataset

issues_dataset = load_dataset("json", data_files=data_files, split="train")
issues_dataset`,highlighted:`<span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

issues_dataset = load_dataset(<span class="hljs-string">&quot;json&quot;</span>, data_files=data_files, split=<span class="hljs-string">&quot;train&quot;</span>)
issues_dataset`}}),Qe=new O({props:{code:`Dataset({
    features: ['url', 'repository_url', 'labels_url', 'comments_url', 'events_url', 'html_url', 'id', 'node_id', 'number', 'title', 'user', 'labels', 'state', 'locked', 'assignee', 'assignees', 'milestone', 'comments', 'created_at', 'updated_at', 'closed_at', 'author_association', 'active_lock_reason', 'pull_request', 'body', 'performed_via_github_app', 'is_pull_request'],
    num_rows: 2855
})`,highlighted:`Dataset({
    features: [<span class="hljs-string">&#x27;url&#x27;</span>, <span class="hljs-string">&#x27;repository_url&#x27;</span>, <span class="hljs-string">&#x27;labels_url&#x27;</span>, <span class="hljs-string">&#x27;comments_url&#x27;</span>, <span class="hljs-string">&#x27;events_url&#x27;</span>, <span class="hljs-string">&#x27;html_url&#x27;</span>, <span class="hljs-string">&#x27;id&#x27;</span>, <span class="hljs-string">&#x27;node_id&#x27;</span>, <span class="hljs-string">&#x27;number&#x27;</span>, <span class="hljs-string">&#x27;title&#x27;</span>, <span class="hljs-string">&#x27;user&#x27;</span>, <span class="hljs-string">&#x27;labels&#x27;</span>, <span class="hljs-string">&#x27;state&#x27;</span>, <span class="hljs-string">&#x27;locked&#x27;</span>, <span class="hljs-string">&#x27;assignee&#x27;</span>, <span class="hljs-string">&#x27;assignees&#x27;</span>, <span class="hljs-string">&#x27;milestone&#x27;</span>, <span class="hljs-string">&#x27;comments&#x27;</span>, <span class="hljs-string">&#x27;created_at&#x27;</span>, <span class="hljs-string">&#x27;updated_at&#x27;</span>, <span class="hljs-string">&#x27;closed_at&#x27;</span>, <span class="hljs-string">&#x27;author_association&#x27;</span>, <span class="hljs-string">&#x27;active_lock_reason&#x27;</span>, <span class="hljs-string">&#x27;pull_request&#x27;</span>, <span class="hljs-string">&#x27;body&#x27;</span>, <span class="hljs-string">&#x27;performed_via_github_app&#x27;</span>, <span class="hljs-string">&#x27;is_pull_request&#x27;</span>],
    num_rows: <span class="hljs-number">2855</span>
})`}}),Xe=new O({props:{code:`issues_dataset = issues_dataset.filter(
    lambda x: (x["is_pull_request"] == False and len(x["comments"]) > 0)
)
issues_dataset`,highlighted:`issues_dataset = issues_dataset.<span class="hljs-built_in">filter</span>(
    <span class="hljs-keyword">lambda</span> x: (x[<span class="hljs-string">&quot;is_pull_request&quot;</span>] == <span class="hljs-literal">False</span> <span class="hljs-keyword">and</span> <span class="hljs-built_in">len</span>(x[<span class="hljs-string">&quot;comments&quot;</span>]) &gt; <span class="hljs-number">0</span>)
)
issues_dataset`}}),et=new O({props:{code:`Dataset({
    features: ['url', 'repository_url', 'labels_url', 'comments_url', 'events_url', 'html_url', 'id', 'node_id', 'number', 'title', 'user', 'labels', 'state', 'locked', 'assignee', 'assignees', 'milestone', 'comments', 'created_at', 'updated_at', 'closed_at', 'author_association', 'active_lock_reason', 'pull_request', 'body', 'performed_via_github_app', 'is_pull_request'],
    num_rows: 771
})`,highlighted:`Dataset({
    features: [<span class="hljs-string">&#x27;url&#x27;</span>, <span class="hljs-string">&#x27;repository_url&#x27;</span>, <span class="hljs-string">&#x27;labels_url&#x27;</span>, <span class="hljs-string">&#x27;comments_url&#x27;</span>, <span class="hljs-string">&#x27;events_url&#x27;</span>, <span class="hljs-string">&#x27;html_url&#x27;</span>, <span class="hljs-string">&#x27;id&#x27;</span>, <span class="hljs-string">&#x27;node_id&#x27;</span>, <span class="hljs-string">&#x27;number&#x27;</span>, <span class="hljs-string">&#x27;title&#x27;</span>, <span class="hljs-string">&#x27;user&#x27;</span>, <span class="hljs-string">&#x27;labels&#x27;</span>, <span class="hljs-string">&#x27;state&#x27;</span>, <span class="hljs-string">&#x27;locked&#x27;</span>, <span class="hljs-string">&#x27;assignee&#x27;</span>, <span class="hljs-string">&#x27;assignees&#x27;</span>, <span class="hljs-string">&#x27;milestone&#x27;</span>, <span class="hljs-string">&#x27;comments&#x27;</span>, <span class="hljs-string">&#x27;created_at&#x27;</span>, <span class="hljs-string">&#x27;updated_at&#x27;</span>, <span class="hljs-string">&#x27;closed_at&#x27;</span>, <span class="hljs-string">&#x27;author_association&#x27;</span>, <span class="hljs-string">&#x27;active_lock_reason&#x27;</span>, <span class="hljs-string">&#x27;pull_request&#x27;</span>, <span class="hljs-string">&#x27;body&#x27;</span>, <span class="hljs-string">&#x27;performed_via_github_app&#x27;</span>, <span class="hljs-string">&#x27;is_pull_request&#x27;</span>],
    num_rows: <span class="hljs-number">771</span>
})`}}),tt=new O({props:{code:`columns = issues_dataset.column_names
columns_to_keep = ["title", "body", "html_url", "comments"]
columns_to_remove = set(columns_to_keep).symmetric_difference(columns)
issues_dataset = issues_dataset.remove_columns(columns_to_remove)
issues_dataset`,highlighted:`columns = issues_dataset.column_names
columns_to_keep = [<span class="hljs-string">&quot;title&quot;</span>, <span class="hljs-string">&quot;body&quot;</span>, <span class="hljs-string">&quot;html_url&quot;</span>, <span class="hljs-string">&quot;comments&quot;</span>]
columns_to_remove = <span class="hljs-built_in">set</span>(columns_to_keep).symmetric_difference(columns)
issues_dataset = issues_dataset.remove_columns(columns_to_remove)
issues_dataset`}}),st=new O({props:{code:`Dataset({
    features: ['html_url', 'title', 'comments', 'body'],
    num_rows: 771
})`,highlighted:`Dataset({
    features: [<span class="hljs-string">&#x27;html_url&#x27;</span>, <span class="hljs-string">&#x27;title&#x27;</span>, <span class="hljs-string">&#x27;comments&#x27;</span>, <span class="hljs-string">&#x27;body&#x27;</span>],
    num_rows: <span class="hljs-number">771</span>
})`}}),at=new O({props:{code:`issues_dataset.set_format("pandas")
df = issues_dataset[:]`,highlighted:`issues_dataset.set_format(<span class="hljs-string">&quot;pandas&quot;</span>)
df = issues_dataset[:]`}}),ot=new O({props:{code:'df["comments"][0].tolist()',highlighted:'df[<span class="hljs-string">&quot;comments&quot;</span>][<span class="hljs-number">0</span>].tolist()'}}),rt=new O({props:{code:`['the bug code locate in \uFF1A\\r\\n    if data_args.task_name is not None:\\r\\n        # Downloading and loading a dataset from the hub.\\r\\n        datasets = load_dataset("glue", data_args.task_name, cache_dir=model_args.cache_dir)',
 'Hi @jinec,\\r\\n\\r\\nFrom time to time we get this kind of \`ConnectionError\` coming from the github.com website: https://raw.githubusercontent.com\\r\\n\\r\\nNormally, it should work if you wait a little and then retry.\\r\\n\\r\\nCould you please confirm if the problem persists?',
 'cannot connect\uFF0Ceven by Web browser\uFF0Cplease check that  there is some  problems\u3002',
 'I can access https://raw.githubusercontent.com/huggingface/datasets/1.7.0/datasets/glue/glue.py without problem...']`,highlighted:`[<span class="hljs-string">&#x27;the bug code locate in \uFF1A\\r\\n    if data_args.task_name is not None:\\r\\n        # Downloading and loading a dataset from the hub.\\r\\n        datasets = load_dataset(&quot;glue&quot;, data_args.task_name, cache_dir=model_args.cache_dir)&#x27;</span>,
 <span class="hljs-string">&#x27;Hi @jinec,\\r\\n\\r\\nFrom time to time we get this kind of \`ConnectionError\` coming from the github.com website: https://raw.githubusercontent.com\\r\\n\\r\\nNormally, it should work if you wait a little and then retry.\\r\\n\\r\\nCould you please confirm if the problem persists?&#x27;</span>,
 <span class="hljs-string">&#x27;cannot connect\uFF0Ceven by Web browser\uFF0Cplease check that  there is some  problems\u3002&#x27;</span>,
 <span class="hljs-string">&#x27;I can access https://raw.githubusercontent.com/huggingface/datasets/1.7.0/datasets/glue/glue.py without problem...&#x27;</span>]`}}),lt=new O({props:{code:`comments_df = df.explode("comments", ignore_index=True)
comments_df.head(4)`,highlighted:`comments_df = df.explode(<span class="hljs-string">&quot;comments&quot;</span>, ignore_index=<span class="hljs-literal">True</span>)
comments_df.head(<span class="hljs-number">4</span>)`}}),it=new O({props:{code:`from datasets import Dataset

comments_dataset = Dataset.from_pandas(comments_df)
comments_dataset`,highlighted:`<span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> Dataset

comments_dataset = Dataset.from_pandas(comments_df)
comments_dataset`}}),ut=new O({props:{code:`Dataset({
    features: ['html_url', 'title', 'comments', 'body'],
    num_rows: 2842
})`,highlighted:`Dataset({
    features: [<span class="hljs-string">&#x27;html_url&#x27;</span>, <span class="hljs-string">&#x27;title&#x27;</span>, <span class="hljs-string">&#x27;comments&#x27;</span>, <span class="hljs-string">&#x27;body&#x27;</span>],
    num_rows: <span class="hljs-number">2842</span>
})`}}),Me=new yu({props:{$$slots:{default:[Vu]},$$scope:{ctx:T}}}),ct=new O({props:{code:`comments_dataset = comments_dataset.map(
    lambda x: {"comment_length": len(x["comments"].split())}
)`,highlighted:`comments_dataset = comments_dataset.<span class="hljs-built_in">map</span>(
    <span class="hljs-keyword">lambda</span> x: {<span class="hljs-string">&quot;comment_length&quot;</span>: <span class="hljs-built_in">len</span>(x[<span class="hljs-string">&quot;comments&quot;</span>].split())}
)`}}),dt=new O({props:{code:`comments_dataset = comments_dataset.filter(lambda x: x["comment_length"] > 15)
comments_dataset`,highlighted:`comments_dataset = comments_dataset.<span class="hljs-built_in">filter</span>(<span class="hljs-keyword">lambda</span> x: x[<span class="hljs-string">&quot;comment_length&quot;</span>] &gt; <span class="hljs-number">15</span>)
comments_dataset`}}),pt=new O({props:{code:`Dataset({
    features: ['html_url', 'title', 'comments', 'body', 'comment_length'],
    num_rows: 2098
})`,highlighted:`Dataset({
    features: [<span class="hljs-string">&#x27;html_url&#x27;</span>, <span class="hljs-string">&#x27;title&#x27;</span>, <span class="hljs-string">&#x27;comments&#x27;</span>, <span class="hljs-string">&#x27;body&#x27;</span>, <span class="hljs-string">&#x27;comment_length&#x27;</span>],
    num_rows: <span class="hljs-number">2098</span>
})`}}),mt=new O({props:{code:`def concatenate_text(examples):
    return {
        "text": examples["title"]
        + " \\n "
        + examples["body"]
        + " \\n "
        + examples["comments"]
    }


comments_dataset = comments_dataset.map(concatenate_text)`,highlighted:`<span class="hljs-keyword">def</span> <span class="hljs-title function_">concatenate_text</span>(<span class="hljs-params">examples</span>):
    <span class="hljs-keyword">return</span> {
        <span class="hljs-string">&quot;text&quot;</span>: examples[<span class="hljs-string">&quot;title&quot;</span>]
        + <span class="hljs-string">&quot; \\n &quot;</span>
        + examples[<span class="hljs-string">&quot;body&quot;</span>]
        + <span class="hljs-string">&quot; \\n &quot;</span>
        + examples[<span class="hljs-string">&quot;comments&quot;</span>]
    }


comments_dataset = comments_dataset.<span class="hljs-built_in">map</span>(concatenate_text)`}}),ft=new gn({});const Ll=[Yu,Bu],yt=[];function Fl(e,u){return e[0]==="pt"?0:1}ce=Fl(T),de=yt[ce]=Ll[ce](T),gt=new O({props:{code:`def cls_pooling(model_output):
    return model_output.last_hidden_state[:, 0]`,highlighted:`<span class="hljs-keyword">def</span> <span class="hljs-title function_">cls_pooling</span>(<span class="hljs-params">model_output</span>):
    <span class="hljs-keyword">return</span> model_output.last_hidden_state[:, <span class="hljs-number">0</span>]`}});const Hl=[Zu,Wu],jt=[];function Ul(e,u){return e[0]==="pt"?0:1}pe=Ul(T),me=jt[pe]=Hl[pe](T),vt=new gn({}),$t=new O({props:{code:'embeddings_dataset.add_faiss_index(column="embeddings")',highlighted:'embeddings_dataset.add_faiss_index(column=<span class="hljs-string">&quot;embeddings&quot;</span>)'}});const Gl=[Ku,Ju],Dt=[];function Vl(e,u){return e[0]==="pt"?0:1}return fe=Vl(T),he=Dt[fe]=Gl[fe](T),qt=new O({props:{code:`scores, samples = embeddings_dataset.get_nearest_examples(
    "embeddings", question_embedding, k=5
)`,highlighted:`scores, samples = embeddings_dataset.get_nearest_examples(
    <span class="hljs-string">&quot;embeddings&quot;</span>, question_embedding, k=<span class="hljs-number">5</span>
)`}}),xt=new O({props:{code:`import pandas as pd

samples_df = pd.DataFrame.from_dict(samples)
samples_df["scores"] = scores
samples_df.sort_values("scores", ascending=False, inplace=True)`,highlighted:`<span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd

samples_df = pd.DataFrame.from_dict(samples)
samples_df[<span class="hljs-string">&quot;scores&quot;</span>] = scores
samples_df.sort_values(<span class="hljs-string">&quot;scores&quot;</span>, ascending=<span class="hljs-literal">False</span>, inplace=<span class="hljs-literal">True</span>)`}}),wt=new O({props:{code:`for _, row in samples_df.iterrows():
    print(f"COMMENT: {row.comments}")
    print(f"SCORE: {row.scores}")
    print(f"TITLE: {row.title}")
    print(f"URL: {row.html_url}")
    print("=" * 50)
    print()`,highlighted:`<span class="hljs-keyword">for</span> _, row <span class="hljs-keyword">in</span> samples_df.iterrows():
    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;COMMENT: <span class="hljs-subst">{row.comments}</span>&quot;</span>)
    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;SCORE: <span class="hljs-subst">{row.scores}</span>&quot;</span>)
    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;TITLE: <span class="hljs-subst">{row.title}</span>&quot;</span>)
    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;URL: <span class="hljs-subst">{row.html_url}</span>&quot;</span>)
    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;=&quot;</span> * <span class="hljs-number">50</span>)
    <span class="hljs-built_in">print</span>()`}}),Et=new O({props:{code:`"""
COMMENT: Requiring online connection is a deal breaker in some cases unfortunately so it'd be great if offline mode is added similar to how \`transformers\` loads models offline fine.

@mandubian's second bullet point suggests that there's a workaround allowing you to use your offline (custom?) dataset with \`datasets\`. Could you please elaborate on how that should look like?
SCORE: 25.505046844482422
TITLE: Discussion using datasets in offline mode
URL: https://github.com/huggingface/datasets/issues/824
==================================================

COMMENT: The local dataset builders (csv, text , json and pandas) are now part of the \`datasets\` package since #1726 :)
You can now use them offline
\\\`\\\`\\\`python
datasets = load_dataset("text", data_files=data_files)
\\\`\\\`\\\`

We'll do a new release soon
SCORE: 24.555509567260742
TITLE: Discussion using datasets in offline mode
URL: https://github.com/huggingface/datasets/issues/824
==================================================

COMMENT: I opened a PR that allows to reload modules that have already been loaded once even if there's no internet.

Let me know if you know other ways that can make the offline mode experience better. I'd be happy to add them :)

I already note the "freeze" modules option, to prevent local modules updates. It would be a cool feature.

----------

> @mandubian's second bullet point suggests that there's a workaround allowing you to use your offline (custom?) dataset with \`datasets\`. Could you please elaborate on how that should look like?

Indeed \`load_dataset\` allows to load remote dataset script (squad, glue, etc.) but also you own local ones.
For example if you have a dataset script at \`./my_dataset/my_dataset.py\` then you can do
\\\`\\\`\\\`python
load_dataset("./my_dataset")
\\\`\\\`\\\`
and the dataset script will generate your dataset once and for all.

----------

About I'm looking into having \`csv\`, \`json\`, \`text\`, \`pandas\` dataset builders already included in the \`datasets\` package, so that they are available offline by default, as opposed to the other datasets that require the script to be downloaded.
cf #1724
SCORE: 24.14896583557129
TITLE: Discussion using datasets in offline mode
URL: https://github.com/huggingface/datasets/issues/824
==================================================

COMMENT: > here is my way to load a dataset offline, but it **requires** an online machine
>
> 1. (online machine)
>
> \`\`\`
>
> import datasets
>
> data = datasets.load_dataset(...)
>
> data.save_to_disk(/YOUR/DATASET/DIR)
>
> \`\`\`
>
> 2. copy the dir from online to the offline machine
>
> 3. (offline machine)
>
> \`\`\`
>
> import datasets
>
> data = datasets.load_from_disk(/SAVED/DATA/DIR)
>
> \`\`\`
>
>
>
> HTH.


SCORE: 22.893993377685547
TITLE: Discussion using datasets in offline mode
URL: https://github.com/huggingface/datasets/issues/824
==================================================

COMMENT: here is my way to load a dataset offline, but it **requires** an online machine
1. (online machine)
\\\`\\\`\\\`
import datasets
data = datasets.load_dataset(...)
data.save_to_disk(/YOUR/DATASET/DIR)
\\\`\\\`\\\`
2. copy the dir from online to the offline machine
3. (offline machine)
\\\`\\\`\\\`
import datasets
data = datasets.load_from_disk(/SAVED/DATA/DIR)
\\\`\\\`\\\`

HTH.
SCORE: 22.406635284423828
TITLE: Discussion using datasets in offline mode
URL: https://github.com/huggingface/datasets/issues/824
==================================================
"""`,highlighted:`<span class="hljs-string">&quot;&quot;&quot;
COMMENT: Requiring online connection is a deal breaker in some cases unfortunately so it&#x27;d be great if offline mode is added similar to how \`transformers\` loads models offline fine.

@mandubian&#x27;s second bullet point suggests that there&#x27;s a workaround allowing you to use your offline (custom?) dataset with \`datasets\`. Could you please elaborate on how that should look like?
SCORE: 25.505046844482422
TITLE: Discussion using datasets in offline mode
URL: https://github.com/huggingface/datasets/issues/824
==================================================

COMMENT: The local dataset builders (csv, text , json and pandas) are now part of the \`datasets\` package since #1726 :)
You can now use them offline
\\\`\\\`\\\`python
datasets = load_dataset(&quot;text&quot;, data_files=data_files)
\\\`\\\`\\\`

We&#x27;ll do a new release soon
SCORE: 24.555509567260742
TITLE: Discussion using datasets in offline mode
URL: https://github.com/huggingface/datasets/issues/824
==================================================

COMMENT: I opened a PR that allows to reload modules that have already been loaded once even if there&#x27;s no internet.

Let me know if you know other ways that can make the offline mode experience better. I&#x27;d be happy to add them :)

I already note the &quot;freeze&quot; modules option, to prevent local modules updates. It would be a cool feature.

----------

&gt; @mandubian&#x27;s second bullet point suggests that there&#x27;s a workaround allowing you to use your offline (custom?) dataset with \`datasets\`. Could you please elaborate on how that should look like?

Indeed \`load_dataset\` allows to load remote dataset script (squad, glue, etc.) but also you own local ones.
For example if you have a dataset script at \`./my_dataset/my_dataset.py\` then you can do
\\\`\\\`\\\`python
load_dataset(&quot;./my_dataset&quot;)
\\\`\\\`\\\`
and the dataset script will generate your dataset once and for all.

----------

About I&#x27;m looking into having \`csv\`, \`json\`, \`text\`, \`pandas\` dataset builders already included in the \`datasets\` package, so that they are available offline by default, as opposed to the other datasets that require the script to be downloaded.
cf #1724
SCORE: 24.14896583557129
TITLE: Discussion using datasets in offline mode
URL: https://github.com/huggingface/datasets/issues/824
==================================================

COMMENT: &gt; here is my way to load a dataset offline, but it **requires** an online machine
&gt;
&gt; 1. (online machine)
&gt;
&gt; \`\`\`
&gt;
&gt; import datasets
&gt;
&gt; data = datasets.load_dataset(...)
&gt;
&gt; data.save_to_disk(/YOUR/DATASET/DIR)
&gt;
&gt; \`\`\`
&gt;
&gt; 2. copy the dir from online to the offline machine
&gt;
&gt; 3. (offline machine)
&gt;
&gt; \`\`\`
&gt;
&gt; import datasets
&gt;
&gt; data = datasets.load_from_disk(/SAVED/DATA/DIR)
&gt;
&gt; \`\`\`
&gt;
&gt;
&gt;
&gt; HTH.


SCORE: 22.893993377685547
TITLE: Discussion using datasets in offline mode
URL: https://github.com/huggingface/datasets/issues/824
==================================================

COMMENT: here is my way to load a dataset offline, but it **requires** an online machine
1. (online machine)
\\\`\\\`\\\`
import datasets
data = datasets.load_dataset(...)
data.save_to_disk(/YOUR/DATASET/DIR)
\\\`\\\`\\\`
2. copy the dir from online to the offline machine
3. (offline machine)
\\\`\\\`\\\`
import datasets
data = datasets.load_from_disk(/SAVED/DATA/DIR)
\\\`\\\`\\\`

HTH.
SCORE: 22.406635284423828
TITLE: Discussion using datasets in offline mode
URL: https://github.com/huggingface/datasets/issues/824
==================================================
&quot;&quot;&quot;</span>`}}),Ue=new yu({props:{$$slots:{default:[Qu]},$$scope:{ctx:T}}}),{c(){o=l("meta"),h=p(),k(c.$$.fragment),v=p(),g=l("h1"),_=l("a"),x=l("span"),k(b.$$.fragment),D=p(),E=l("span"),R=n("Recherche s\xE9mantique avec FAISS"),A=p(),M.c(),z=p(),F=l("p"),G=n("Dans "),P=l("a"),L=n("section 5"),Y=n(", nous avons cr\xE9\xE9 un ensemble de donn\xE9es de probl\xE8mes et de commentaires GitHub \xE0 partir du r\xE9f\xE9rentiel \u{1F917} Datasets. Dans cette section, nous utiliserons ces informations pour cr\xE9er un moteur de recherche qui peut nous aider \xE0 trouver des r\xE9ponses \xE0 nos questions les plus urgentes sur la biblioth\xE8que\xA0!"),U=p(),k(N.$$.fragment),V=p(),f=l("h2"),I=l("a"),H=l("span"),k(W.$$.fragment),ae=p(),ie=l("span"),Ma=n("Utilisation des repr\xE9sentations vectorielles continues pour la recherche s\xE9mantique"),vn=p(),_e=l("p"),Ra=n("Comme nous l\u2019avons vu dans le "),At=l("a"),za=n("Chapitre 1"),La=n(", les mod\xE8les de langage bas\xE9s sur Transformer repr\xE9sentent chaque jeton dans une \xE9tendue de texte sous la forme d\u2019un "),os=l("em"),Fa=n("vecteur d\u2019int\xE9gration"),Ha=n(". Il s\u2019av\xE8re que l\u2019on peut \u201Cregrouper\u201D les incorporations individuelles pour cr\xE9er une repr\xE9sentation vectorielle pour des phrases enti\xE8res, des paragraphes ou (dans certains cas) des documents. Ces int\xE9grations peuvent ensuite \xEAtre utilis\xE9es pour trouver des documents similaires dans le corpus en calculant la similarit\xE9 du produit scalaire (ou une autre m\xE9trique de similarit\xE9) entre chaque int\xE9gration et en renvoyant les documents avec le plus grand chevauchement."),bn=p(),St=l("p"),Ua=n("Dans cette section, nous utiliserons les incorporations pour d\xE9velopper un moteur de recherche s\xE9mantique. Ces moteurs de recherche offrent plusieurs avantages par rapport aux approches conventionnelles bas\xE9es sur la correspondance des mots-cl\xE9s dans une requ\xEAte avec les documents."),$n=p(),Te=l("div"),Ye=l("img"),Ga=p(),We=l("img"),qn=p(),Ae=l("h2"),Pe=l("a"),rs=l("span"),k(Ze.$$.fragment),Va=p(),ls=l("span"),Ba=n("Chargement et pr\xE9paration du jeu de donn\xE9es"),xn=p(),Nt=l("p"),Ya=n("La premi\xE8re chose que nous devons faire est de t\xE9l\xE9charger notre ensemble de donn\xE9es de probl\xE8mes GitHub, alors utilisons la biblioth\xE8que \u{1F917} Hub pour r\xE9soudre l\u2019URL o\xF9 notre fichier est stock\xE9 sur le Hugging Face Hub :"),wn=p(),k(Je.$$.fragment),En=p(),ge=l("p"),Wa=n("Avec l\u2019URL stock\xE9e dans "),is=l("code"),Za=n("data_files"),Ja=n(", nous pouvons ensuite charger le jeu de donn\xE9es distant en utilisant la m\xE9thode introduite dans "),Pt=l("a"),Ka=n("section 2"),Qa=n(" :"),kn=p(),k(Ke.$$.fragment),yn=p(),k(Qe.$$.fragment),jn=p(),Z=l("p"),Xa=n("Ici, nous avons sp\xE9cifi\xE9 la division "),us=l("code"),eo=n("train"),to=n(" par d\xE9faut dans "),cs=l("code"),so=n("load_dataset()"),no=n(", de sorte qu\u2019elle renvoie un "),ds=l("code"),ao=n("Dataset"),oo=n(" au lieu d\u2019un "),ps=l("code"),ro=n("DatasetDict"),lo=n(". La premi\xE8re chose \xE0 faire est de filtrer les demandes d\u2019extraction, car celles-ci ont tendance \xE0 \xEAtre rarement utilis\xE9es pour r\xE9pondre aux requ\xEAtes des utilisateurs et introduiront du bruit dans notre moteur de recherche. Comme cela devrait \xEAtre familier maintenant, nous pouvons utiliser la fonction "),ms=l("code"),io=n("Dataset.filter()"),uo=n(" pour exclure ces lignes de notre ensemble de donn\xE9es. Pendant que nous y sommes, filtrons \xE9galement les lignes sans commentaires, car celles-ci ne fournissent aucune r\xE9ponse aux requ\xEAtes des utilisateurs\xA0:"),Dn=p(),k(Xe.$$.fragment),Cn=p(),k(et.$$.fragment),Tn=p(),J=l("p"),co=n("Nous pouvons voir qu\u2019il y a beaucoup de colonnes dans notre ensemble de donn\xE9es, dont la plupart n\u2019ont pas besoin de construire notre moteur de recherche. Du point de vue de la recherche, les colonnes les plus informatives sont "),fs=l("code"),po=n("title"),mo=n(", "),hs=l("code"),fo=n("body"),ho=n(" et "),_s=l("code"),_o=n("comments"),go=n(", tandis que "),gs=l("code"),vo=n("html_url"),bo=n(" nous fournit un lien vers le probl\xE8me source. Utilisons la fonction "),vs=l("code"),$o=n("Dataset.remove_columns()"),qo=n(" pour supprimer le reste\xA0:"),An=p(),k(tt.$$.fragment),Sn=p(),k(st.$$.fragment),Nn=p(),oe=l("p"),xo=n("Pour cr\xE9er nos int\xE9grations, nous ajouterons \xE0 chaque commentaire le titre et le corps du probl\xE8me, car ces champs contiennent souvent des informations contextuelles utiles. \xC9tant donn\xE9 que notre colonne "),bs=l("code"),wo=n("comments"),Eo=n(" est actuellement une liste de commentaires pour chaque probl\xE8me, nous devons \u201C\xE9clater\u201D la colonne afin que chaque ligne se compose d\u2019un tuple "),$s=l("code"),ko=n("(html_url, title, body, comment)"),yo=n(". Dans Pandas, nous pouvons le faire avec la fonction "),nt=l("a"),qs=l("code"),jo=n("DataFrame.explode()"),Do=n(", qui cr\xE9e une nouvelle ligne pour chaque \xE9l\xE9ment dans une colonne de type liste, tout en r\xE9pliquant toutes les autres valeurs de colonne. Pour voir cela en action, passons d\u2019abord au format \u201CDataFrame\u201D de Pandas\xA0:"),Pn=p(),k(at.$$.fragment),In=p(),Ie=l("p"),Co=n("Si nous inspectons la premi\xE8re ligne de ce "),xs=l("code"),To=n("DataFrame"),Ao=n(", nous pouvons voir qu\u2019il y a quatre commentaires associ\xE9s \xE0 ce probl\xE8me\xA0:"),On=p(),k(ot.$$.fragment),Mn=p(),k(rt.$$.fragment),Rn=p(),Oe=l("p"),So=n("Lorsque nous d\xE9composons "),ws=l("code"),No=n("df"),Po=n(", nous nous attendons \xE0 obtenir une ligne pour chacun de ces commentaires. V\xE9rifions si c\u2019est le cas :"),zn=p(),k(lt.$$.fragment),Ln=p(),X=l("table"),Es=l("thead"),Q=l("tr"),Fn=l("th"),Io=p(),ks=l("th"),Oo=n("html_url"),Mo=p(),ys=l("th"),Ro=n("title"),zo=p(),js=l("th"),Lo=n("comments"),Fo=p(),Ds=l("th"),Ho=n("body"),Uo=p(),ue=l("tbody"),ee=l("tr"),Cs=l("th"),Go=n("0"),Vo=p(),Ts=l("td"),Bo=n("https://github.com/huggingface/datasets/issues/2787"),Yo=p(),As=l("td"),Wo=n("ConnectionError: Couldn't reach https://raw.githubusercontent.com"),Zo=p(),Ss=l("td"),Jo=n("the bug code locate in \uFF1A\\r\\n    if data_args.task_name is not None..."),Ko=p(),Ns=l("td"),Qo=n("Hello,\\r\\nI am trying to run run_glue.py and it gives me this error..."),Xo=p(),te=l("tr"),Ps=l("th"),er=n("1"),tr=p(),Is=l("td"),sr=n("https://github.com/huggingface/datasets/issues/2787"),nr=p(),Os=l("td"),ar=n("ConnectionError: Couldn't reach https://raw.githubusercontent.com"),or=p(),Ms=l("td"),rr=n("Hi @jinec,\\r\\n\\r\\nFrom time to time we get this kind of `ConnectionError` coming from the github.com website: https://raw.githubusercontent.com..."),lr=p(),Rs=l("td"),ir=n("Hello,\\r\\nI am trying to run run_glue.py and it gives me this error..."),ur=p(),se=l("tr"),zs=l("th"),cr=n("2"),dr=p(),Ls=l("td"),pr=n("https://github.com/huggingface/datasets/issues/2787"),mr=p(),Fs=l("td"),fr=n("ConnectionError: Couldn't reach https://raw.githubusercontent.com"),hr=p(),Hs=l("td"),_r=n("cannot connect\uFF0Ceven by Web browser\uFF0Cplease check that  there is some  problems\u3002"),gr=p(),Us=l("td"),vr=n("Hello,\\r\\nI am trying to run run_glue.py and it gives me this error..."),br=p(),ne=l("tr"),Gs=l("th"),$r=n("3"),qr=p(),Vs=l("td"),xr=n("https://github.com/huggingface/datasets/issues/2787"),wr=p(),Bs=l("td"),Er=n("ConnectionError: Couldn't reach https://raw.githubusercontent.com"),kr=p(),Ys=l("td"),yr=n("I can access https://raw.githubusercontent.com/huggingface/datasets/1.7.0/datasets/glue/glue.py without problem..."),jr=p(),Ws=l("td"),Dr=n("Hello,\\r\\nI am trying to run run_glue.py and it gives me this error..."),Hn=p(),ve=l("p"),Cr=n("G\xE9nial, nous pouvons voir que les lignes ont \xE9t\xE9 r\xE9pliqu\xE9es, avec la colonne \u201Ccommentaires\u201D contenant les commentaires individuels\xA0! Maintenant que nous en avons fini avec Pandas, nous pouvons rapidement revenir \xE0 un "),Zs=l("code"),Tr=n("Dataset"),Ar=n(" en chargeant le "),Js=l("code"),Sr=n("DataFrame"),Nr=n(" en m\xE9moire\xA0:"),Un=p(),k(it.$$.fragment),Gn=p(),k(ut.$$.fragment),Vn=p(),It=l("p"),Pr=n("D\u2019accord, cela nous a donn\xE9 quelques milliers de commentaires avec lesquels travailler\xA0!"),Bn=p(),k(Me.$$.fragment),Yn=p(),Re=l("p"),Ir=n("Maintenant que nous avons un commentaire par ligne, cr\xE9ons une nouvelle colonne "),Ks=l("code"),Or=n("comments_length"),Mr=n(" contenant le nombre de mots par commentaire\xA0:"),Wn=p(),k(ct.$$.fragment),Zn=p(),Ot=l("p"),Rr=n("Nous pouvons utiliser cette nouvelle colonne pour filtrer les commentaires courts, qui incluent g\xE9n\xE9ralement des \xE9l\xE9ments tels que \u201Ccc @lewtun\u201D ou \u201CMerci\xA0!\u201D qui ne sont pas pertinents pour notre moteur de recherche. Il n\u2019y a pas de nombre pr\xE9cis \xE0 s\xE9lectionner pour le filtre, mais environ 15 mots semblent \xEAtre un bon d\xE9but :"),Jn=p(),k(dt.$$.fragment),Kn=p(),k(pt.$$.fragment),Qn=p(),ze=l("p"),zr=n("Apr\xE8s avoir un peu nettoy\xE9 notre ensemble de donn\xE9es, concat\xE9nons le titre, la description et les commentaires du probl\xE8me dans une nouvelle colonne \u201Ctexte\u201D. Comme d\u2019habitude, nous allons \xE9crire une fonction simple que nous pouvons passer \xE0 "),Qs=l("code"),Lr=n("Dataset.map()"),Fr=n("\xA0:"),Xn=p(),k(mt.$$.fragment),ea=p(),Mt=l("p"),Hr=n("Nous sommes enfin pr\xEAts \xE0 cr\xE9er des embeddings\xA0! Nous allons jeter un coup d\u2019oeil."),ta=p(),Se=l("h2"),Le=l("a"),Xs=l("span"),k(ft.$$.fragment),Ur=p(),en=l("span"),Gr=n("Cr\xE9ation d'incorporations de texte"),sa=p(),B=l("p"),Vr=n("Nous avons vu dans "),Rt=l("a"),Br=n("Chapitre 2"),Yr=n(" que nous pouvons obtenir des incorporations de jetons en utilisant la classe "),tn=l("code"),Wr=n("AutoModel"),Zr=n(". Tout ce que nous avons \xE0 faire est de choisir un point de contr\xF4le appropri\xE9 \xE0 partir duquel charger le mod\xE8le. Heureusement, il existe une biblioth\xE8que appel\xE9e "),sn=l("code"),Jr=n("sentence-transformers"),Kr=n(" d\xE9di\xE9e \xE0 la cr\xE9ation d\u2019incorporations. Comme d\xE9crit dans la [documentation] de la biblioth\xE8que ("),ht=l("a"),Qr=n("https://www.sbert.net/examples/applications/semantic-search/README.html#symmetric-vs-asymmetric-semantic-search"),Xr=n("), notre cas d\u2019utilisation est un exemple de "),nn=l("em"),el=n("asymmetric recherche s\xE9mantique"),tl=n(" car nous avons une requ\xEAte courte dont nous aimerions trouver la r\xE9ponse dans un document plus long, comme un commentaire sur un probl\xE8me. Le [tableau de pr\xE9sentation des mod\xE8les] ("),_t=l("a"),sl=n("https://www.sbert.net/docs/pretrained_models.html#model-overview"),nl=n(") pratique de la documentation indique que le point de contr\xF4le "),an=l("code"),al=n("multi-qa-mpnet-base-dot-v1"),ol=n(" a le meilleures performances pour la recherche s\xE9mantique, nous l\u2019utiliserons donc pour notre application. Nous allons \xE9galement charger le tokenizer en utilisant le m\xEAme point de contr\xF4le\xA0:"),na=p(),de.c(),zt=p(),be=l("p"),rl=n("Comme nous l\u2019avons mentionn\xE9 pr\xE9c\xE9demment, nous aimerions repr\xE9senter chaque entr\xE9e dans notre corpus de probl\xE8mes GitHub comme un vecteur unique, nous devons donc \u201Cregrouper\u201D ou faire la moyenne de nos incorporations de jetons d\u2019une mani\xE8re ou d\u2019une autre. Une approche populaire consiste \xE0 effectuer un "),on=l("em"),ll=n("regroupement CLS"),il=n(" sur les sorties de notre mod\xE8le, o\xF9 nous collectons simplement le dernier \xE9tat cach\xE9 pour le jeton sp\xE9cial "),rn=l("code"),ul=n("[CLS]"),cl=n(". La fonction suivante fait l\u2019affaire pour nous :"),aa=p(),k(gt.$$.fragment),oa=p(),Lt=l("p"),dl=n("Ensuite, nous allons cr\xE9er une fonction d\u2019assistance qui va tokeniser une liste de documents, placer les tenseurs sur le GPU, les alimenter au mod\xE8le et enfin appliquer le regroupement CLS aux sorties\xA0:"),ra=p(),me.c(),Ft=p(),Ht=l("p"),pl=n("Notez que nous avons converti les int\xE9grations en tableaux NumPy \u2014 c\u2019est parce que \u{1F917} Datasets n\xE9cessite ce format lorsque nous essayons de les indexer avec FAISS, ce que nous ferons ensuite."),la=p(),Ne=l("h2"),Fe=l("a"),ln=l("span"),k(vt.$$.fragment),ml=p(),un=l("span"),fl=n("Utilisation de FAISS pour une recherche de similarit\xE9 efficace"),ia=p(),$e=l("p"),hl=n("Maintenant que nous avons un ensemble de donn\xE9es d\u2019incorporations, nous avons besoin d\u2019un moyen de les rechercher. Pour ce faire, nous utiliserons une structure de donn\xE9es sp\xE9ciale dans \u{1F917} Datasets appel\xE9e "),cn=l("em"),_l=n("FAISS index"),gl=n(". "),bt=l("a"),vl=n("FAISS"),bl=n(" (abr\xE9viation de Facebook AI Similarity Search) est une biblioth\xE8que qui fournit des algorithmes efficaces pour rechercher et regrouper rapidement des vecteurs d\u2019int\xE9gration."),ua=p(),qe=l("p"),$l=n("L\u2019id\xE9e de base derri\xE8re FAISS est de cr\xE9er une structure de donn\xE9es sp\xE9ciale appel\xE9e un "),dn=l("em"),ql=n("index"),xl=n(" qui permet de trouver quels plongements sont similaires \xE0 un plongement d\u2019entr\xE9e. Cr\xE9er un index FAISS dans \u{1F917} Datasets est simple \u2014 nous utilisons la fonction "),pn=l("code"),wl=n("Dataset.add_faiss_index()"),El=n(" et sp\xE9cifions quelle colonne de notre jeu de donn\xE9es nous aimerions indexer\xA0:"),ca=p(),k($t.$$.fragment),da=p(),He=l("p"),kl=n("Nous pouvons maintenant effectuer des requ\xEAtes sur cet index en effectuant une recherche de voisin le plus proche avec la fonction "),mn=l("code"),yl=n("Dataset.get_nearest_examples()"),jl=n(". Testons cela en int\xE9grant d\u2019abord une question comme suit\xA0:"),pa=p(),he.c(),Ut=p(),Gt=l("p"),Dl=n("Tout comme avec les documents, nous avons maintenant un vecteur de 768 dimensions repr\xE9sentant la requ\xEAte, que nous pouvons comparer \xE0 l\u2019ensemble du corpus pour trouver les plongements les plus similaires\xA0:"),ma=p(),k(qt.$$.fragment),fa=p(),xe=l("p"),Cl=n("La fonction "),fn=l("code"),Tl=n("Dataset.get_nearest_examples()"),Al=n(" renvoie un tuple de scores qui classent le chevauchement entre la requ\xEAte et le document, et un ensemble correspondant d\u2019\xE9chantillons (ici, les 5 meilleures correspondances). Collectons-les dans un "),hn=l("code"),Sl=n("pandas.DataFrame"),Nl=n(" afin de pouvoir les trier facilement\xA0:"),ha=p(),k(xt.$$.fragment),_a=p(),Vt=l("p"),Pl=n("Nous pouvons maintenant parcourir les premi\xE8res lignes pour voir dans quelle mesure notre requ\xEAte correspond aux commentaires disponibles\xA0:"),ga=p(),k(wt.$$.fragment),va=p(),k(Et.$$.fragment),ba=p(),Bt=l("p"),Il=n("Pas mal! Notre deuxi\xE8me r\xE9sultat semble correspondre \xE0 la requ\xEAte."),$a=p(),k(Ue.$$.fragment),this.h()},l(e){const u=Au('[data-svelte="svelte-1phssyn"]',document.head);o=i(u,"META",{name:!0,content:!0}),u.forEach(s),h=m(e),C(c.$$.fragment,e),v=m(e),g=i(e,"H1",{class:!0});var Ct=r(g);_=i(Ct,"A",{id:!0,class:!0,href:!0});var Yt=r(_);x=i(Yt,"SPAN",{});var _n=r(x);C(b.$$.fragment,_n),_n.forEach(s),Yt.forEach(s),D=m(Ct),E=i(Ct,"SPAN",{});var Wt=r(E);R=a(Wt,"Recherche s\xE9mantique avec FAISS"),Wt.forEach(s),Ct.forEach(s),A=m(e),M.l(e),z=m(e),F=i(e,"P",{});var Ge=r(F);G=a(Ge,"Dans "),P=i(Ge,"A",{href:!0});var Zt=r(P);L=a(Zt,"section 5"),Zt.forEach(s),Y=a(Ge,", nous avons cr\xE9\xE9 un ensemble de donn\xE9es de probl\xE8mes et de commentaires GitHub \xE0 partir du r\xE9f\xE9rentiel \u{1F917} Datasets. Dans cette section, nous utiliserons ces informations pour cr\xE9er un moteur de recherche qui peut nous aider \xE0 trouver des r\xE9ponses \xE0 nos questions les plus urgentes sur la biblioth\xE8que\xA0!"),Ge.forEach(s),U=m(e),C(N.$$.fragment,e),V=m(e),f=i(e,"H2",{class:!0});var Tt=r(f);I=i(Tt,"A",{id:!0,class:!0,href:!0});var Bl=r(I);H=i(Bl,"SPAN",{});var Yl=r(H);C(W.$$.fragment,Yl),Yl.forEach(s),Bl.forEach(s),ae=m(Tt),ie=i(Tt,"SPAN",{});var Wl=r(ie);Ma=a(Wl,"Utilisation des repr\xE9sentations vectorielles continues pour la recherche s\xE9mantique"),Wl.forEach(s),Tt.forEach(s),vn=m(e),_e=i(e,"P",{});var Jt=r(_e);Ra=a(Jt,"Comme nous l\u2019avons vu dans le "),At=i(Jt,"A",{href:!0});var Zl=r(At);za=a(Zl,"Chapitre 1"),Zl.forEach(s),La=a(Jt,", les mod\xE8les de langage bas\xE9s sur Transformer repr\xE9sentent chaque jeton dans une \xE9tendue de texte sous la forme d\u2019un "),os=i(Jt,"EM",{});var Jl=r(os);Fa=a(Jl,"vecteur d\u2019int\xE9gration"),Jl.forEach(s),Ha=a(Jt,". Il s\u2019av\xE8re que l\u2019on peut \u201Cregrouper\u201D les incorporations individuelles pour cr\xE9er une repr\xE9sentation vectorielle pour des phrases enti\xE8res, des paragraphes ou (dans certains cas) des documents. Ces int\xE9grations peuvent ensuite \xEAtre utilis\xE9es pour trouver des documents similaires dans le corpus en calculant la similarit\xE9 du produit scalaire (ou une autre m\xE9trique de similarit\xE9) entre chaque int\xE9gration et en renvoyant les documents avec le plus grand chevauchement."),Jt.forEach(s),bn=m(e),St=i(e,"P",{});var Kl=r(St);Ua=a(Kl,"Dans cette section, nous utiliserons les incorporations pour d\xE9velopper un moteur de recherche s\xE9mantique. Ces moteurs de recherche offrent plusieurs avantages par rapport aux approches conventionnelles bas\xE9es sur la correspondance des mots-cl\xE9s dans une requ\xEAte avec les documents."),Kl.forEach(s),$n=m(e),Te=i(e,"DIV",{class:!0});var xa=r(Te);Ye=i(xa,"IMG",{class:!0,src:!0,alt:!0}),Ga=m(xa),We=i(xa,"IMG",{class:!0,src:!0,alt:!0}),xa.forEach(s),qn=m(e),Ae=i(e,"H2",{class:!0});var wa=r(Ae);Pe=i(wa,"A",{id:!0,class:!0,href:!0});var Ql=r(Pe);rs=i(Ql,"SPAN",{});var Xl=r(rs);C(Ze.$$.fragment,Xl),Xl.forEach(s),Ql.forEach(s),Va=m(wa),ls=i(wa,"SPAN",{});var ei=r(ls);Ba=a(ei,"Chargement et pr\xE9paration du jeu de donn\xE9es"),ei.forEach(s),wa.forEach(s),xn=m(e),Nt=i(e,"P",{});var ti=r(Nt);Ya=a(ti,"La premi\xE8re chose que nous devons faire est de t\xE9l\xE9charger notre ensemble de donn\xE9es de probl\xE8mes GitHub, alors utilisons la biblioth\xE8que \u{1F917} Hub pour r\xE9soudre l\u2019URL o\xF9 notre fichier est stock\xE9 sur le Hugging Face Hub :"),ti.forEach(s),wn=m(e),C(Je.$$.fragment,e),En=m(e),ge=i(e,"P",{});var Kt=r(ge);Wa=a(Kt,"Avec l\u2019URL stock\xE9e dans "),is=i(Kt,"CODE",{});var si=r(is);Za=a(si,"data_files"),si.forEach(s),Ja=a(Kt,", nous pouvons ensuite charger le jeu de donn\xE9es distant en utilisant la m\xE9thode introduite dans "),Pt=i(Kt,"A",{href:!0});var ni=r(Pt);Ka=a(ni,"section 2"),ni.forEach(s),Qa=a(Kt," :"),Kt.forEach(s),kn=m(e),C(Ke.$$.fragment,e),yn=m(e),C(Qe.$$.fragment,e),jn=m(e),Z=i(e,"P",{});var re=r(Z);Xa=a(re,"Ici, nous avons sp\xE9cifi\xE9 la division "),us=i(re,"CODE",{});var ai=r(us);eo=a(ai,"train"),ai.forEach(s),to=a(re," par d\xE9faut dans "),cs=i(re,"CODE",{});var oi=r(cs);so=a(oi,"load_dataset()"),oi.forEach(s),no=a(re,", de sorte qu\u2019elle renvoie un "),ds=i(re,"CODE",{});var ri=r(ds);ao=a(ri,"Dataset"),ri.forEach(s),oo=a(re," au lieu d\u2019un "),ps=i(re,"CODE",{});var li=r(ps);ro=a(li,"DatasetDict"),li.forEach(s),lo=a(re,". La premi\xE8re chose \xE0 faire est de filtrer les demandes d\u2019extraction, car celles-ci ont tendance \xE0 \xEAtre rarement utilis\xE9es pour r\xE9pondre aux requ\xEAtes des utilisateurs et introduiront du bruit dans notre moteur de recherche. Comme cela devrait \xEAtre familier maintenant, nous pouvons utiliser la fonction "),ms=i(re,"CODE",{});var ii=r(ms);io=a(ii,"Dataset.filter()"),ii.forEach(s),uo=a(re," pour exclure ces lignes de notre ensemble de donn\xE9es. Pendant que nous y sommes, filtrons \xE9galement les lignes sans commentaires, car celles-ci ne fournissent aucune r\xE9ponse aux requ\xEAtes des utilisateurs\xA0:"),re.forEach(s),Dn=m(e),C(Xe.$$.fragment,e),Cn=m(e),C(et.$$.fragment,e),Tn=m(e),J=i(e,"P",{});var le=r(J);co=a(le,"Nous pouvons voir qu\u2019il y a beaucoup de colonnes dans notre ensemble de donn\xE9es, dont la plupart n\u2019ont pas besoin de construire notre moteur de recherche. Du point de vue de la recherche, les colonnes les plus informatives sont "),fs=i(le,"CODE",{});var ui=r(fs);po=a(ui,"title"),ui.forEach(s),mo=a(le,", "),hs=i(le,"CODE",{});var ci=r(hs);fo=a(ci,"body"),ci.forEach(s),ho=a(le," et "),_s=i(le,"CODE",{});var di=r(_s);_o=a(di,"comments"),di.forEach(s),go=a(le,", tandis que "),gs=i(le,"CODE",{});var pi=r(gs);vo=a(pi,"html_url"),pi.forEach(s),bo=a(le," nous fournit un lien vers le probl\xE8me source. Utilisons la fonction "),vs=i(le,"CODE",{});var mi=r(vs);$o=a(mi,"Dataset.remove_columns()"),mi.forEach(s),qo=a(le," pour supprimer le reste\xA0:"),le.forEach(s),An=m(e),C(tt.$$.fragment,e),Sn=m(e),C(st.$$.fragment,e),Nn=m(e),oe=i(e,"P",{});var Ve=r(oe);xo=a(Ve,"Pour cr\xE9er nos int\xE9grations, nous ajouterons \xE0 chaque commentaire le titre et le corps du probl\xE8me, car ces champs contiennent souvent des informations contextuelles utiles. \xC9tant donn\xE9 que notre colonne "),bs=i(Ve,"CODE",{});var fi=r(bs);wo=a(fi,"comments"),fi.forEach(s),Eo=a(Ve," est actuellement une liste de commentaires pour chaque probl\xE8me, nous devons \u201C\xE9clater\u201D la colonne afin que chaque ligne se compose d\u2019un tuple "),$s=i(Ve,"CODE",{});var hi=r($s);ko=a(hi,"(html_url, title, body, comment)"),hi.forEach(s),yo=a(Ve,". Dans Pandas, nous pouvons le faire avec la fonction "),nt=i(Ve,"A",{href:!0,rel:!0});var _i=r(nt);qs=i(_i,"CODE",{});var gi=r(qs);jo=a(gi,"DataFrame.explode()"),gi.forEach(s),_i.forEach(s),Do=a(Ve,", qui cr\xE9e une nouvelle ligne pour chaque \xE9l\xE9ment dans une colonne de type liste, tout en r\xE9pliquant toutes les autres valeurs de colonne. Pour voir cela en action, passons d\u2019abord au format \u201CDataFrame\u201D de Pandas\xA0:"),Ve.forEach(s),Pn=m(e),C(at.$$.fragment,e),In=m(e),Ie=i(e,"P",{});var Ea=r(Ie);Co=a(Ea,"Si nous inspectons la premi\xE8re ligne de ce "),xs=i(Ea,"CODE",{});var vi=r(xs);To=a(vi,"DataFrame"),vi.forEach(s),Ao=a(Ea,", nous pouvons voir qu\u2019il y a quatre commentaires associ\xE9s \xE0 ce probl\xE8me\xA0:"),Ea.forEach(s),On=m(e),C(ot.$$.fragment,e),Mn=m(e),C(rt.$$.fragment,e),Rn=m(e),Oe=i(e,"P",{});var ka=r(Oe);So=a(ka,"Lorsque nous d\xE9composons "),ws=i(ka,"CODE",{});var bi=r(ws);No=a(bi,"df"),bi.forEach(s),Po=a(ka,", nous nous attendons \xE0 obtenir une ligne pour chacun de ces commentaires. V\xE9rifions si c\u2019est le cas :"),ka.forEach(s),zn=m(e),C(lt.$$.fragment,e),Ln=m(e),X=i(e,"TABLE",{border:!0,class:!0,style:!0});var ya=r(X);Es=i(ya,"THEAD",{});var $i=r(Es);Q=i($i,"TR",{style:!0});var we=r(Q);Fn=i(we,"TH",{}),r(Fn).forEach(s),Io=m(we),ks=i(we,"TH",{});var qi=r(ks);Oo=a(qi,"html_url"),qi.forEach(s),Mo=m(we),ys=i(we,"TH",{});var xi=r(ys);Ro=a(xi,"title"),xi.forEach(s),zo=m(we),js=i(we,"TH",{});var wi=r(js);Lo=a(wi,"comments"),wi.forEach(s),Fo=m(we),Ds=i(we,"TH",{});var Ei=r(Ds);Ho=a(Ei,"body"),Ei.forEach(s),we.forEach(s),$i.forEach(s),Uo=m(ya),ue=i(ya,"TBODY",{});var Be=r(ue);ee=i(Be,"TR",{});var Ee=r(ee);Cs=i(Ee,"TH",{});var ki=r(Cs);Go=a(ki,"0"),ki.forEach(s),Vo=m(Ee),Ts=i(Ee,"TD",{});var yi=r(Ts);Bo=a(yi,"https://github.com/huggingface/datasets/issues/2787"),yi.forEach(s),Yo=m(Ee),As=i(Ee,"TD",{});var ji=r(As);Wo=a(ji,"ConnectionError: Couldn't reach https://raw.githubusercontent.com"),ji.forEach(s),Zo=m(Ee),Ss=i(Ee,"TD",{});var Di=r(Ss);Jo=a(Di,"the bug code locate in \uFF1A\\r\\n    if data_args.task_name is not None..."),Di.forEach(s),Ko=m(Ee),Ns=i(Ee,"TD",{});var Ci=r(Ns);Qo=a(Ci,"Hello,\\r\\nI am trying to run run_glue.py and it gives me this error..."),Ci.forEach(s),Ee.forEach(s),Xo=m(Be),te=i(Be,"TR",{});var ke=r(te);Ps=i(ke,"TH",{});var Ti=r(Ps);er=a(Ti,"1"),Ti.forEach(s),tr=m(ke),Is=i(ke,"TD",{});var Ai=r(Is);sr=a(Ai,"https://github.com/huggingface/datasets/issues/2787"),Ai.forEach(s),nr=m(ke),Os=i(ke,"TD",{});var Si=r(Os);ar=a(Si,"ConnectionError: Couldn't reach https://raw.githubusercontent.com"),Si.forEach(s),or=m(ke),Ms=i(ke,"TD",{});var Ni=r(Ms);rr=a(Ni,"Hi @jinec,\\r\\n\\r\\nFrom time to time we get this kind of `ConnectionError` coming from the github.com website: https://raw.githubusercontent.com..."),Ni.forEach(s),lr=m(ke),Rs=i(ke,"TD",{});var Pi=r(Rs);ir=a(Pi,"Hello,\\r\\nI am trying to run run_glue.py and it gives me this error..."),Pi.forEach(s),ke.forEach(s),ur=m(Be),se=i(Be,"TR",{});var ye=r(se);zs=i(ye,"TH",{});var Ii=r(zs);cr=a(Ii,"2"),Ii.forEach(s),dr=m(ye),Ls=i(ye,"TD",{});var Oi=r(Ls);pr=a(Oi,"https://github.com/huggingface/datasets/issues/2787"),Oi.forEach(s),mr=m(ye),Fs=i(ye,"TD",{});var Mi=r(Fs);fr=a(Mi,"ConnectionError: Couldn't reach https://raw.githubusercontent.com"),Mi.forEach(s),hr=m(ye),Hs=i(ye,"TD",{});var Ri=r(Hs);_r=a(Ri,"cannot connect\uFF0Ceven by Web browser\uFF0Cplease check that  there is some  problems\u3002"),Ri.forEach(s),gr=m(ye),Us=i(ye,"TD",{});var zi=r(Us);vr=a(zi,"Hello,\\r\\nI am trying to run run_glue.py and it gives me this error..."),zi.forEach(s),ye.forEach(s),br=m(Be),ne=i(Be,"TR",{});var je=r(ne);Gs=i(je,"TH",{});var Li=r(Gs);$r=a(Li,"3"),Li.forEach(s),qr=m(je),Vs=i(je,"TD",{});var Fi=r(Vs);xr=a(Fi,"https://github.com/huggingface/datasets/issues/2787"),Fi.forEach(s),wr=m(je),Bs=i(je,"TD",{});var Hi=r(Bs);Er=a(Hi,"ConnectionError: Couldn't reach https://raw.githubusercontent.com"),Hi.forEach(s),kr=m(je),Ys=i(je,"TD",{});var Ui=r(Ys);yr=a(Ui,"I can access https://raw.githubusercontent.com/huggingface/datasets/1.7.0/datasets/glue/glue.py without problem..."),Ui.forEach(s),jr=m(je),Ws=i(je,"TD",{});var Gi=r(Ws);Dr=a(Gi,"Hello,\\r\\nI am trying to run run_glue.py and it gives me this error..."),Gi.forEach(s),je.forEach(s),Be.forEach(s),ya.forEach(s),Hn=m(e),ve=i(e,"P",{});var Qt=r(ve);Cr=a(Qt,"G\xE9nial, nous pouvons voir que les lignes ont \xE9t\xE9 r\xE9pliqu\xE9es, avec la colonne \u201Ccommentaires\u201D contenant les commentaires individuels\xA0! Maintenant que nous en avons fini avec Pandas, nous pouvons rapidement revenir \xE0 un "),Zs=i(Qt,"CODE",{});var Vi=r(Zs);Tr=a(Vi,"Dataset"),Vi.forEach(s),Ar=a(Qt," en chargeant le "),Js=i(Qt,"CODE",{});var Bi=r(Js);Sr=a(Bi,"DataFrame"),Bi.forEach(s),Nr=a(Qt," en m\xE9moire\xA0:"),Qt.forEach(s),Un=m(e),C(it.$$.fragment,e),Gn=m(e),C(ut.$$.fragment,e),Vn=m(e),It=i(e,"P",{});var Yi=r(It);Pr=a(Yi,"D\u2019accord, cela nous a donn\xE9 quelques milliers de commentaires avec lesquels travailler\xA0!"),Yi.forEach(s),Bn=m(e),C(Me.$$.fragment,e),Yn=m(e),Re=i(e,"P",{});var ja=r(Re);Ir=a(ja,"Maintenant que nous avons un commentaire par ligne, cr\xE9ons une nouvelle colonne "),Ks=i(ja,"CODE",{});var Wi=r(Ks);Or=a(Wi,"comments_length"),Wi.forEach(s),Mr=a(ja," contenant le nombre de mots par commentaire\xA0:"),ja.forEach(s),Wn=m(e),C(ct.$$.fragment,e),Zn=m(e),Ot=i(e,"P",{});var Zi=r(Ot);Rr=a(Zi,"Nous pouvons utiliser cette nouvelle colonne pour filtrer les commentaires courts, qui incluent g\xE9n\xE9ralement des \xE9l\xE9ments tels que \u201Ccc @lewtun\u201D ou \u201CMerci\xA0!\u201D qui ne sont pas pertinents pour notre moteur de recherche. Il n\u2019y a pas de nombre pr\xE9cis \xE0 s\xE9lectionner pour le filtre, mais environ 15 mots semblent \xEAtre un bon d\xE9but :"),Zi.forEach(s),Jn=m(e),C(dt.$$.fragment,e),Kn=m(e),C(pt.$$.fragment,e),Qn=m(e),ze=i(e,"P",{});var Da=r(ze);zr=a(Da,"Apr\xE8s avoir un peu nettoy\xE9 notre ensemble de donn\xE9es, concat\xE9nons le titre, la description et les commentaires du probl\xE8me dans une nouvelle colonne \u201Ctexte\u201D. Comme d\u2019habitude, nous allons \xE9crire une fonction simple que nous pouvons passer \xE0 "),Qs=i(Da,"CODE",{});var Ji=r(Qs);Lr=a(Ji,"Dataset.map()"),Ji.forEach(s),Fr=a(Da,"\xA0:"),Da.forEach(s),Xn=m(e),C(mt.$$.fragment,e),ea=m(e),Mt=i(e,"P",{});var Ki=r(Mt);Hr=a(Ki,"Nous sommes enfin pr\xEAts \xE0 cr\xE9er des embeddings\xA0! Nous allons jeter un coup d\u2019oeil."),Ki.forEach(s),ta=m(e),Se=i(e,"H2",{class:!0});var Ca=r(Se);Le=i(Ca,"A",{id:!0,class:!0,href:!0});var Qi=r(Le);Xs=i(Qi,"SPAN",{});var Xi=r(Xs);C(ft.$$.fragment,Xi),Xi.forEach(s),Qi.forEach(s),Ur=m(Ca),en=i(Ca,"SPAN",{});var eu=r(en);Gr=a(eu,"Cr\xE9ation d'incorporations de texte"),eu.forEach(s),Ca.forEach(s),sa=m(e),B=i(e,"P",{});var K=r(B);Vr=a(K,"Nous avons vu dans "),Rt=i(K,"A",{href:!0});var tu=r(Rt);Br=a(tu,"Chapitre 2"),tu.forEach(s),Yr=a(K," que nous pouvons obtenir des incorporations de jetons en utilisant la classe "),tn=i(K,"CODE",{});var su=r(tn);Wr=a(su,"AutoModel"),su.forEach(s),Zr=a(K,". Tout ce que nous avons \xE0 faire est de choisir un point de contr\xF4le appropri\xE9 \xE0 partir duquel charger le mod\xE8le. Heureusement, il existe une biblioth\xE8que appel\xE9e "),sn=i(K,"CODE",{});var nu=r(sn);Jr=a(nu,"sentence-transformers"),nu.forEach(s),Kr=a(K," d\xE9di\xE9e \xE0 la cr\xE9ation d\u2019incorporations. Comme d\xE9crit dans la [documentation] de la biblioth\xE8que ("),ht=i(K,"A",{href:!0,rel:!0});var au=r(ht);Qr=a(au,"https://www.sbert.net/examples/applications/semantic-search/README.html#symmetric-vs-asymmetric-semantic-search"),au.forEach(s),Xr=a(K,"), notre cas d\u2019utilisation est un exemple de "),nn=i(K,"EM",{});var ou=r(nn);el=a(ou,"asymmetric recherche s\xE9mantique"),ou.forEach(s),tl=a(K," car nous avons une requ\xEAte courte dont nous aimerions trouver la r\xE9ponse dans un document plus long, comme un commentaire sur un probl\xE8me. Le [tableau de pr\xE9sentation des mod\xE8les] ("),_t=i(K,"A",{href:!0,rel:!0});var ru=r(_t);sl=a(ru,"https://www.sbert.net/docs/pretrained_models.html#model-overview"),ru.forEach(s),nl=a(K,") pratique de la documentation indique que le point de contr\xF4le "),an=i(K,"CODE",{});var lu=r(an);al=a(lu,"multi-qa-mpnet-base-dot-v1"),lu.forEach(s),ol=a(K," a le meilleures performances pour la recherche s\xE9mantique, nous l\u2019utiliserons donc pour notre application. Nous allons \xE9galement charger le tokenizer en utilisant le m\xEAme point de contr\xF4le\xA0:"),K.forEach(s),na=m(e),de.l(e),zt=m(e),be=i(e,"P",{});var Xt=r(be);rl=a(Xt,"Comme nous l\u2019avons mentionn\xE9 pr\xE9c\xE9demment, nous aimerions repr\xE9senter chaque entr\xE9e dans notre corpus de probl\xE8mes GitHub comme un vecteur unique, nous devons donc \u201Cregrouper\u201D ou faire la moyenne de nos incorporations de jetons d\u2019une mani\xE8re ou d\u2019une autre. Une approche populaire consiste \xE0 effectuer un "),on=i(Xt,"EM",{});var iu=r(on);ll=a(iu,"regroupement CLS"),iu.forEach(s),il=a(Xt," sur les sorties de notre mod\xE8le, o\xF9 nous collectons simplement le dernier \xE9tat cach\xE9 pour le jeton sp\xE9cial "),rn=i(Xt,"CODE",{});var uu=r(rn);ul=a(uu,"[CLS]"),uu.forEach(s),cl=a(Xt,". La fonction suivante fait l\u2019affaire pour nous :"),Xt.forEach(s),aa=m(e),C(gt.$$.fragment,e),oa=m(e),Lt=i(e,"P",{});var cu=r(Lt);dl=a(cu,"Ensuite, nous allons cr\xE9er une fonction d\u2019assistance qui va tokeniser une liste de documents, placer les tenseurs sur le GPU, les alimenter au mod\xE8le et enfin appliquer le regroupement CLS aux sorties\xA0:"),cu.forEach(s),ra=m(e),me.l(e),Ft=m(e),Ht=i(e,"P",{});var du=r(Ht);pl=a(du,"Notez que nous avons converti les int\xE9grations en tableaux NumPy \u2014 c\u2019est parce que \u{1F917} Datasets n\xE9cessite ce format lorsque nous essayons de les indexer avec FAISS, ce que nous ferons ensuite."),du.forEach(s),la=m(e),Ne=i(e,"H2",{class:!0});var Ta=r(Ne);Fe=i(Ta,"A",{id:!0,class:!0,href:!0});var pu=r(Fe);ln=i(pu,"SPAN",{});var mu=r(ln);C(vt.$$.fragment,mu),mu.forEach(s),pu.forEach(s),ml=m(Ta),un=i(Ta,"SPAN",{});var fu=r(un);fl=a(fu,"Utilisation de FAISS pour une recherche de similarit\xE9 efficace"),fu.forEach(s),Ta.forEach(s),ia=m(e),$e=i(e,"P",{});var es=r($e);hl=a(es,"Maintenant que nous avons un ensemble de donn\xE9es d\u2019incorporations, nous avons besoin d\u2019un moyen de les rechercher. Pour ce faire, nous utiliserons une structure de donn\xE9es sp\xE9ciale dans \u{1F917} Datasets appel\xE9e "),cn=i(es,"EM",{});var hu=r(cn);_l=a(hu,"FAISS index"),hu.forEach(s),gl=a(es,". "),bt=i(es,"A",{href:!0,rel:!0});var _u=r(bt);vl=a(_u,"FAISS"),_u.forEach(s),bl=a(es," (abr\xE9viation de Facebook AI Similarity Search) est une biblioth\xE8que qui fournit des algorithmes efficaces pour rechercher et regrouper rapidement des vecteurs d\u2019int\xE9gration."),es.forEach(s),ua=m(e),qe=i(e,"P",{});var ts=r(qe);$l=a(ts,"L\u2019id\xE9e de base derri\xE8re FAISS est de cr\xE9er une structure de donn\xE9es sp\xE9ciale appel\xE9e un "),dn=i(ts,"EM",{});var gu=r(dn);ql=a(gu,"index"),gu.forEach(s),xl=a(ts," qui permet de trouver quels plongements sont similaires \xE0 un plongement d\u2019entr\xE9e. Cr\xE9er un index FAISS dans \u{1F917} Datasets est simple \u2014 nous utilisons la fonction "),pn=i(ts,"CODE",{});var vu=r(pn);wl=a(vu,"Dataset.add_faiss_index()"),vu.forEach(s),El=a(ts," et sp\xE9cifions quelle colonne de notre jeu de donn\xE9es nous aimerions indexer\xA0:"),ts.forEach(s),ca=m(e),C($t.$$.fragment,e),da=m(e),He=i(e,"P",{});var Aa=r(He);kl=a(Aa,"Nous pouvons maintenant effectuer des requ\xEAtes sur cet index en effectuant une recherche de voisin le plus proche avec la fonction "),mn=i(Aa,"CODE",{});var bu=r(mn);yl=a(bu,"Dataset.get_nearest_examples()"),bu.forEach(s),jl=a(Aa,". Testons cela en int\xE9grant d\u2019abord une question comme suit\xA0:"),Aa.forEach(s),pa=m(e),he.l(e),Ut=m(e),Gt=i(e,"P",{});var $u=r(Gt);Dl=a($u,"Tout comme avec les documents, nous avons maintenant un vecteur de 768 dimensions repr\xE9sentant la requ\xEAte, que nous pouvons comparer \xE0 l\u2019ensemble du corpus pour trouver les plongements les plus similaires\xA0:"),$u.forEach(s),ma=m(e),C(qt.$$.fragment,e),fa=m(e),xe=i(e,"P",{});var ss=r(xe);Cl=a(ss,"La fonction "),fn=i(ss,"CODE",{});var qu=r(fn);Tl=a(qu,"Dataset.get_nearest_examples()"),qu.forEach(s),Al=a(ss," renvoie un tuple de scores qui classent le chevauchement entre la requ\xEAte et le document, et un ensemble correspondant d\u2019\xE9chantillons (ici, les 5 meilleures correspondances). Collectons-les dans un "),hn=i(ss,"CODE",{});var xu=r(hn);Sl=a(xu,"pandas.DataFrame"),xu.forEach(s),Nl=a(ss," afin de pouvoir les trier facilement\xA0:"),ss.forEach(s),ha=m(e),C(xt.$$.fragment,e),_a=m(e),Vt=i(e,"P",{});var wu=r(Vt);Pl=a(wu,"Nous pouvons maintenant parcourir les premi\xE8res lignes pour voir dans quelle mesure notre requ\xEAte correspond aux commentaires disponibles\xA0:"),wu.forEach(s),ga=m(e),C(wt.$$.fragment,e),va=m(e),C(Et.$$.fragment,e),ba=m(e),Bt=i(e,"P",{});var Eu=r(Bt);Il=a(Eu,"Pas mal! Notre deuxi\xE8me r\xE9sultat semble correspondre \xE0 la requ\xEAte."),Eu.forEach(s),$a=m(e),C(Ue.$$.fragment,e),this.h()},h(){$(o,"name","hf:doc:metadata"),$(o,"content",JSON.stringify(ec)),$(_,"id","recherche-smantique-avec-faiss"),$(_,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),$(_,"href","#recherche-smantique-avec-faiss"),$(g,"class","relative group"),$(P,"href","/course/chapter5/5"),$(I,"id","utilisation-des-reprsentations-vectorielles-continues-pour-la-recherche-smantique"),$(I,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),$(I,"href","#utilisation-des-reprsentations-vectorielles-continues-pour-la-recherche-smantique"),$(f,"class","relative group"),$(At,"href","/course/chapter1"),$(Ye,"class","block dark:hidden"),ku(Ye.src,Ol="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter5/semantic-search.svg")||$(Ye,"src",Ol),$(Ye,"alt","Recherche s\xE9mantique."),$(We,"class","hidden dark:block"),ku(We.src,Ml="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter5/semantic-search-dark.svg")||$(We,"src",Ml),$(We,"alt","Recherche s\xE9mantique."),$(Te,"class","flex justify-center"),$(Pe,"id","chargement-et-prparation-du-jeu-de-donnes"),$(Pe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),$(Pe,"href","#chargement-et-prparation-du-jeu-de-donnes"),$(Ae,"class","relative group"),$(Pt,"href","/course/chapter5/2"),$(nt,"href","https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.explode.html"),$(nt,"rel","nofollow"),Sa(Q,"text-align","right"),$(X,"border","1"),$(X,"class","dataframe"),Sa(X,"table-layout","fixed"),Sa(X,"word-wrap","break-word"),Sa(X,"width","100%"),$(Le,"id","cration-dincorporations-de-texte"),$(Le,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),$(Le,"href","#cration-dincorporations-de-texte"),$(Se,"class","relative group"),$(Rt,"href","/course/chapter2"),$(ht,"href","https://www.sbert.net/examples/applications/semantic-search/README.html#symmetric-vs-asymmetric-semantic-search"),$(ht,"rel","nofollow"),$(_t,"href","https://www.sbert.net/docs/pretrained_models.html#model-overview"),$(_t,"rel","nofollow"),$(Fe,"id","utilisation-de-faiss-pour-une-recherche-de-similarit-efficace"),$(Fe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),$(Fe,"href","#utilisation-de-faiss-pour-une-recherche-de-similarit-efficace"),$(Ne,"class","relative group"),$(bt,"href","https://faiss.ai/"),$(bt,"rel","nofollow")},m(e,u){t(document.head,o),d(e,h,u),y(c,e,u),d(e,v,u),d(e,g,u),t(g,_),t(_,x),y(b,x,null),t(g,D),t(g,E),t(E,R),d(e,A,u),kt[S].m(e,u),d(e,z,u),d(e,F,u),t(F,G),t(F,P),t(P,L),t(F,Y),d(e,U,u),y(N,e,u),d(e,V,u),d(e,f,u),t(f,I),t(I,H),y(W,H,null),t(f,ae),t(f,ie),t(ie,Ma),d(e,vn,u),d(e,_e,u),t(_e,Ra),t(_e,At),t(At,za),t(_e,La),t(_e,os),t(os,Fa),t(_e,Ha),d(e,bn,u),d(e,St,u),t(St,Ua),d(e,$n,u),d(e,Te,u),t(Te,Ye),t(Te,Ga),t(Te,We),d(e,qn,u),d(e,Ae,u),t(Ae,Pe),t(Pe,rs),y(Ze,rs,null),t(Ae,Va),t(Ae,ls),t(ls,Ba),d(e,xn,u),d(e,Nt,u),t(Nt,Ya),d(e,wn,u),y(Je,e,u),d(e,En,u),d(e,ge,u),t(ge,Wa),t(ge,is),t(is,Za),t(ge,Ja),t(ge,Pt),t(Pt,Ka),t(ge,Qa),d(e,kn,u),y(Ke,e,u),d(e,yn,u),y(Qe,e,u),d(e,jn,u),d(e,Z,u),t(Z,Xa),t(Z,us),t(us,eo),t(Z,to),t(Z,cs),t(cs,so),t(Z,no),t(Z,ds),t(ds,ao),t(Z,oo),t(Z,ps),t(ps,ro),t(Z,lo),t(Z,ms),t(ms,io),t(Z,uo),d(e,Dn,u),y(Xe,e,u),d(e,Cn,u),y(et,e,u),d(e,Tn,u),d(e,J,u),t(J,co),t(J,fs),t(fs,po),t(J,mo),t(J,hs),t(hs,fo),t(J,ho),t(J,_s),t(_s,_o),t(J,go),t(J,gs),t(gs,vo),t(J,bo),t(J,vs),t(vs,$o),t(J,qo),d(e,An,u),y(tt,e,u),d(e,Sn,u),y(st,e,u),d(e,Nn,u),d(e,oe,u),t(oe,xo),t(oe,bs),t(bs,wo),t(oe,Eo),t(oe,$s),t($s,ko),t(oe,yo),t(oe,nt),t(nt,qs),t(qs,jo),t(oe,Do),d(e,Pn,u),y(at,e,u),d(e,In,u),d(e,Ie,u),t(Ie,Co),t(Ie,xs),t(xs,To),t(Ie,Ao),d(e,On,u),y(ot,e,u),d(e,Mn,u),y(rt,e,u),d(e,Rn,u),d(e,Oe,u),t(Oe,So),t(Oe,ws),t(ws,No),t(Oe,Po),d(e,zn,u),y(lt,e,u),d(e,Ln,u),d(e,X,u),t(X,Es),t(Es,Q),t(Q,Fn),t(Q,Io),t(Q,ks),t(ks,Oo),t(Q,Mo),t(Q,ys),t(ys,Ro),t(Q,zo),t(Q,js),t(js,Lo),t(Q,Fo),t(Q,Ds),t(Ds,Ho),t(X,Uo),t(X,ue),t(ue,ee),t(ee,Cs),t(Cs,Go),t(ee,Vo),t(ee,Ts),t(Ts,Bo),t(ee,Yo),t(ee,As),t(As,Wo),t(ee,Zo),t(ee,Ss),t(Ss,Jo),t(ee,Ko),t(ee,Ns),t(Ns,Qo),t(ue,Xo),t(ue,te),t(te,Ps),t(Ps,er),t(te,tr),t(te,Is),t(Is,sr),t(te,nr),t(te,Os),t(Os,ar),t(te,or),t(te,Ms),t(Ms,rr),t(te,lr),t(te,Rs),t(Rs,ir),t(ue,ur),t(ue,se),t(se,zs),t(zs,cr),t(se,dr),t(se,Ls),t(Ls,pr),t(se,mr),t(se,Fs),t(Fs,fr),t(se,hr),t(se,Hs),t(Hs,_r),t(se,gr),t(se,Us),t(Us,vr),t(ue,br),t(ue,ne),t(ne,Gs),t(Gs,$r),t(ne,qr),t(ne,Vs),t(Vs,xr),t(ne,wr),t(ne,Bs),t(Bs,Er),t(ne,kr),t(ne,Ys),t(Ys,yr),t(ne,jr),t(ne,Ws),t(Ws,Dr),d(e,Hn,u),d(e,ve,u),t(ve,Cr),t(ve,Zs),t(Zs,Tr),t(ve,Ar),t(ve,Js),t(Js,Sr),t(ve,Nr),d(e,Un,u),y(it,e,u),d(e,Gn,u),y(ut,e,u),d(e,Vn,u),d(e,It,u),t(It,Pr),d(e,Bn,u),y(Me,e,u),d(e,Yn,u),d(e,Re,u),t(Re,Ir),t(Re,Ks),t(Ks,Or),t(Re,Mr),d(e,Wn,u),y(ct,e,u),d(e,Zn,u),d(e,Ot,u),t(Ot,Rr),d(e,Jn,u),y(dt,e,u),d(e,Kn,u),y(pt,e,u),d(e,Qn,u),d(e,ze,u),t(ze,zr),t(ze,Qs),t(Qs,Lr),t(ze,Fr),d(e,Xn,u),y(mt,e,u),d(e,ea,u),d(e,Mt,u),t(Mt,Hr),d(e,ta,u),d(e,Se,u),t(Se,Le),t(Le,Xs),y(ft,Xs,null),t(Se,Ur),t(Se,en),t(en,Gr),d(e,sa,u),d(e,B,u),t(B,Vr),t(B,Rt),t(Rt,Br),t(B,Yr),t(B,tn),t(tn,Wr),t(B,Zr),t(B,sn),t(sn,Jr),t(B,Kr),t(B,ht),t(ht,Qr),t(B,Xr),t(B,nn),t(nn,el),t(B,tl),t(B,_t),t(_t,sl),t(B,nl),t(B,an),t(an,al),t(B,ol),d(e,na,u),yt[ce].m(e,u),d(e,zt,u),d(e,be,u),t(be,rl),t(be,on),t(on,ll),t(be,il),t(be,rn),t(rn,ul),t(be,cl),d(e,aa,u),y(gt,e,u),d(e,oa,u),d(e,Lt,u),t(Lt,dl),d(e,ra,u),jt[pe].m(e,u),d(e,Ft,u),d(e,Ht,u),t(Ht,pl),d(e,la,u),d(e,Ne,u),t(Ne,Fe),t(Fe,ln),y(vt,ln,null),t(Ne,ml),t(Ne,un),t(un,fl),d(e,ia,u),d(e,$e,u),t($e,hl),t($e,cn),t(cn,_l),t($e,gl),t($e,bt),t(bt,vl),t($e,bl),d(e,ua,u),d(e,qe,u),t(qe,$l),t(qe,dn),t(dn,ql),t(qe,xl),t(qe,pn),t(pn,wl),t(qe,El),d(e,ca,u),y($t,e,u),d(e,da,u),d(e,He,u),t(He,kl),t(He,mn),t(mn,yl),t(He,jl),d(e,pa,u),Dt[fe].m(e,u),d(e,Ut,u),d(e,Gt,u),t(Gt,Dl),d(e,ma,u),y(qt,e,u),d(e,fa,u),d(e,xe,u),t(xe,Cl),t(xe,fn),t(fn,Tl),t(xe,Al),t(xe,hn),t(hn,Sl),t(xe,Nl),d(e,ha,u),y(xt,e,u),d(e,_a,u),d(e,Vt,u),t(Vt,Pl),d(e,ga,u),y(wt,e,u),d(e,va,u),y(Et,e,u),d(e,ba,u),d(e,Bt,u),t(Bt,Il),d(e,$a,u),y(Ue,e,u),qa=!0},p(e,[u]){const Ct={};u&1&&(Ct.fw=e[0]),c.$set(Ct);let Yt=S;S=zl(e),S!==Yt&&(ns(),w(kt[Yt],1,1,()=>{kt[Yt]=null}),as(),M=kt[S],M||(M=kt[S]=Rl[S](e),M.c()),q(M,1),M.m(z.parentNode,z));const _n={};u&2&&(_n.$$scope={dirty:u,ctx:e}),Me.$set(_n);let Wt=ce;ce=Fl(e),ce!==Wt&&(ns(),w(yt[Wt],1,1,()=>{yt[Wt]=null}),as(),de=yt[ce],de||(de=yt[ce]=Ll[ce](e),de.c()),q(de,1),de.m(zt.parentNode,zt));let Ge=pe;pe=Ul(e),pe!==Ge&&(ns(),w(jt[Ge],1,1,()=>{jt[Ge]=null}),as(),me=jt[pe],me||(me=jt[pe]=Hl[pe](e),me.c()),q(me,1),me.m(Ft.parentNode,Ft));let Zt=fe;fe=Vl(e),fe!==Zt&&(ns(),w(Dt[Zt],1,1,()=>{Dt[Zt]=null}),as(),he=Dt[fe],he||(he=Dt[fe]=Gl[fe](e),he.c()),q(he,1),he.m(Ut.parentNode,Ut));const Tt={};u&2&&(Tt.$$scope={dirty:u,ctx:e}),Ue.$set(Tt)},i(e){qa||(q(c.$$.fragment,e),q(b.$$.fragment,e),q(M),q(N.$$.fragment,e),q(W.$$.fragment,e),q(Ze.$$.fragment,e),q(Je.$$.fragment,e),q(Ke.$$.fragment,e),q(Qe.$$.fragment,e),q(Xe.$$.fragment,e),q(et.$$.fragment,e),q(tt.$$.fragment,e),q(st.$$.fragment,e),q(at.$$.fragment,e),q(ot.$$.fragment,e),q(rt.$$.fragment,e),q(lt.$$.fragment,e),q(it.$$.fragment,e),q(ut.$$.fragment,e),q(Me.$$.fragment,e),q(ct.$$.fragment,e),q(dt.$$.fragment,e),q(pt.$$.fragment,e),q(mt.$$.fragment,e),q(ft.$$.fragment,e),q(de),q(gt.$$.fragment,e),q(me),q(vt.$$.fragment,e),q($t.$$.fragment,e),q(he),q(qt.$$.fragment,e),q(xt.$$.fragment,e),q(wt.$$.fragment,e),q(Et.$$.fragment,e),q(Ue.$$.fragment,e),qa=!0)},o(e){w(c.$$.fragment,e),w(b.$$.fragment,e),w(M),w(N.$$.fragment,e),w(W.$$.fragment,e),w(Ze.$$.fragment,e),w(Je.$$.fragment,e),w(Ke.$$.fragment,e),w(Qe.$$.fragment,e),w(Xe.$$.fragment,e),w(et.$$.fragment,e),w(tt.$$.fragment,e),w(st.$$.fragment,e),w(at.$$.fragment,e),w(ot.$$.fragment,e),w(rt.$$.fragment,e),w(lt.$$.fragment,e),w(it.$$.fragment,e),w(ut.$$.fragment,e),w(Me.$$.fragment,e),w(ct.$$.fragment,e),w(dt.$$.fragment,e),w(pt.$$.fragment,e),w(mt.$$.fragment,e),w(ft.$$.fragment,e),w(de),w(gt.$$.fragment,e),w(me),w(vt.$$.fragment,e),w($t.$$.fragment,e),w(he),w(qt.$$.fragment,e),w(xt.$$.fragment,e),w(wt.$$.fragment,e),w(Et.$$.fragment,e),w(Ue.$$.fragment,e),qa=!1},d(e){s(o),e&&s(h),j(c,e),e&&s(v),e&&s(g),j(b),e&&s(A),kt[S].d(e),e&&s(z),e&&s(F),e&&s(U),j(N,e),e&&s(V),e&&s(f),j(W),e&&s(vn),e&&s(_e),e&&s(bn),e&&s(St),e&&s($n),e&&s(Te),e&&s(qn),e&&s(Ae),j(Ze),e&&s(xn),e&&s(Nt),e&&s(wn),j(Je,e),e&&s(En),e&&s(ge),e&&s(kn),j(Ke,e),e&&s(yn),j(Qe,e),e&&s(jn),e&&s(Z),e&&s(Dn),j(Xe,e),e&&s(Cn),j(et,e),e&&s(Tn),e&&s(J),e&&s(An),j(tt,e),e&&s(Sn),j(st,e),e&&s(Nn),e&&s(oe),e&&s(Pn),j(at,e),e&&s(In),e&&s(Ie),e&&s(On),j(ot,e),e&&s(Mn),j(rt,e),e&&s(Rn),e&&s(Oe),e&&s(zn),j(lt,e),e&&s(Ln),e&&s(X),e&&s(Hn),e&&s(ve),e&&s(Un),j(it,e),e&&s(Gn),j(ut,e),e&&s(Vn),e&&s(It),e&&s(Bn),j(Me,e),e&&s(Yn),e&&s(Re),e&&s(Wn),j(ct,e),e&&s(Zn),e&&s(Ot),e&&s(Jn),j(dt,e),e&&s(Kn),j(pt,e),e&&s(Qn),e&&s(ze),e&&s(Xn),j(mt,e),e&&s(ea),e&&s(Mt),e&&s(ta),e&&s(Se),j(ft),e&&s(sa),e&&s(B),e&&s(na),yt[ce].d(e),e&&s(zt),e&&s(be),e&&s(aa),j(gt,e),e&&s(oa),e&&s(Lt),e&&s(ra),jt[pe].d(e),e&&s(Ft),e&&s(Ht),e&&s(la),e&&s(Ne),j(vt),e&&s(ia),e&&s($e),e&&s(ua),e&&s(qe),e&&s(ca),j($t,e),e&&s(da),e&&s(He),e&&s(pa),Dt[fe].d(e),e&&s(Ut),e&&s(Gt),e&&s(ma),j(qt,e),e&&s(fa),e&&s(xe),e&&s(ha),j(xt,e),e&&s(_a),e&&s(Vt),e&&s(ga),j(wt,e),e&&s(va),j(Et,e),e&&s(ba),e&&s(Bt),e&&s($a),j(Ue,e)}}}const ec={local:"recherche-smantique-avec-faiss",sections:[{local:"utilisation-des-reprsentations-vectorielles-continues-pour-la-recherche-smantique",title:"Utilisation des repr\xE9sentations vectorielles continues pour la recherche s\xE9mantique"},{local:"chargement-et-prparation-du-jeu-de-donnes",title:"Chargement et pr\xE9paration du jeu de donn\xE9es"},{local:"cration-dincorporations-de-texte",title:"Cr\xE9ation d'incorporations de texte"},{local:"utilisation-de-faiss-pour-une-recherche-de-similarit-efficace",title:"Utilisation de FAISS pour une recherche de similarit\xE9 efficace"}],title:"Recherche s\xE9mantique avec FAISS"};function tc(T,o,h){let c="pt";return Su(()=>{const v=new URLSearchParams(window.location.search);h(0,c=v.get("fw")||"pt")}),[c]}class rc extends Pa{constructor(o){super();Ia(this,o,tc,Xu,Oa,{})}}export{rc as default,ec as metadata};
