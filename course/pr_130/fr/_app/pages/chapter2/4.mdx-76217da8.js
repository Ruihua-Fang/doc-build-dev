import{S as Km,i as Ym,s as Wm,e as r,k as p,w as b,t as n,M as Qm,c as a,d as s,m,x as _,a as l,h as o,b as d,N as ls,F as t,g as u,y as E,o as v,p as Xm,q as h,B as $,v as Zm,n as ec}from"../../chunks/vendor-1e8b365d.js";import{T as tc}from"../../chunks/Tip-62b14c6e.js";import{Y as Rn}from"../../chunks/Youtube-c2a8cc39.js";import{I as ce}from"../../chunks/IconCopyLink-483c28ba.js";import{C as V}from"../../chunks/CodeBlock-e5764662.js";import{D as Fm}from"../../chunks/DocNotebookDropdown-37d928d3.js";import{F as sc}from"../../chunks/FrameworkSwitchCourse-7f8f0f31.js";function nc(J){let c,q;return c=new Fm({props:{classNames:"absolute z-10 right-0 top-0",options:[{label:"Google Colab",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter2/section4_tf.ipynb"},{label:"Aws Studio",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter2/section4_tf.ipynb"}]}}),{c(){b(c.$$.fragment)},l(f){_(c.$$.fragment,f)},m(f,x){E(c,f,x),q=!0},i(f){q||(h(c.$$.fragment,f),q=!0)},o(f){v(c.$$.fragment,f),q=!1},d(f){$(c,f)}}}function oc(J){let c,q;return c=new Fm({props:{classNames:"absolute z-10 right-0 top-0",options:[{label:"Google Colab",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter2/section4_pt.ipynb"},{label:"Aws Studio",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter2/section4_pt.ipynb"}]}}),{c(){b(c.$$.fragment)},l(f){_(c.$$.fragment,f)},m(f,x){E(c,f,x),q=!0},i(f){q||(h(c.$$.fragment,f),q=!0)},o(f){v(c.$$.fragment,f),q=!1},d(f){$(c,f)}}}function rc(J){let c,q,f,x,w,g,P,A,S,G,X,T,y,M,C,H,R;return{c(){c=r("p"),q=n("Similaire \xE0 "),f=r("code"),x=n("TFAutoModel"),w=n(", la classe "),g=r("code"),P=n("AutoTokenizer"),A=n(" r\xE9cup\xE8re la classe de "),S=r("em"),G=n("tokenizer"),X=n(" appropri\xE9e dans la biblioth\xE8que bas\xE9e sur le nom du "),T=r("em"),y=n("checkpoint"),M=n(". Elle peut \xEAtre utilis\xE9e directement avec n\u2019importe quel "),C=r("em"),H=n("checkpoint"),R=n(" :")},l(z){c=a(z,"P",{});var k=l(c);q=o(k,"Similaire \xE0 "),f=a(k,"CODE",{});var Z=l(f);x=o(Z,"TFAutoModel"),Z.forEach(s),w=o(k,", la classe "),g=a(k,"CODE",{});var de=l(g);P=o(de,"AutoTokenizer"),de.forEach(s),A=o(k," r\xE9cup\xE8re la classe de "),S=a(k,"EM",{});var fe=l(S);G=o(fe,"tokenizer"),fe.forEach(s),X=o(k," appropri\xE9e dans la biblioth\xE8que bas\xE9e sur le nom du "),T=a(k,"EM",{});var ee=l(T);y=o(ee,"checkpoint"),ee.forEach(s),M=o(k,". Elle peut \xEAtre utilis\xE9e directement avec n\u2019importe quel "),C=a(k,"EM",{});var ve=l(C);H=o(ve,"checkpoint"),ve.forEach(s),R=o(k," :"),k.forEach(s)},m(z,k){u(z,c,k),t(c,q),t(c,f),t(f,x),t(c,w),t(c,g),t(g,P),t(c,A),t(c,S),t(S,G),t(c,X),t(c,T),t(T,y),t(c,M),t(c,C),t(C,H),t(c,R)},d(z){z&&s(c)}}}function ac(J){let c,q,f,x,w,g,P,A,S,G,X,T,y,M,C,H,R;return{c(){c=r("p"),q=n("Similaire \xE0 "),f=r("code"),x=n("AutoModel"),w=n(", la classe "),g=r("code"),P=n("AutoTokenizer"),A=n(" r\xE9cup\xE8re la classe de "),S=r("em"),G=n("tokenizer"),X=n(" appropri\xE9e dans la biblioth\xE8que bas\xE9e sur le nom du "),T=r("em"),y=n("checkpoint"),M=n(". Elle peut \xEAtre utilis\xE9e directement avec n\u2019importe quel "),C=r("em"),H=n("checkpoint"),R=n(" :")},l(z){c=a(z,"P",{});var k=l(c);q=o(k,"Similaire \xE0 "),f=a(k,"CODE",{});var Z=l(f);x=o(Z,"AutoModel"),Z.forEach(s),w=o(k,", la classe "),g=a(k,"CODE",{});var de=l(g);P=o(de,"AutoTokenizer"),de.forEach(s),A=o(k," r\xE9cup\xE8re la classe de "),S=a(k,"EM",{});var fe=l(S);G=o(fe,"tokenizer"),fe.forEach(s),X=o(k," appropri\xE9e dans la biblioth\xE8que bas\xE9e sur le nom du "),T=a(k,"EM",{});var ee=l(T);y=o(ee,"checkpoint"),ee.forEach(s),M=o(k,". Elle peut \xEAtre utilis\xE9e directement avec n\u2019importe quel "),C=a(k,"EM",{});var ve=l(C);H=o(ve,"checkpoint"),ve.forEach(s),R=o(k," :"),k.forEach(s)},m(z,k){u(z,c,k),t(c,q),t(c,f),t(f,x),t(c,w),t(c,g),t(g,P),t(c,A),t(c,S),t(S,G),t(c,X),t(c,T),t(T,y),t(c,M),t(c,C),t(C,H),t(c,R)},d(z){z&&s(c)}}}function lc(J){let c,q,f,x,w;return{c(){c=r("p"),q=n("\u270F\uFE0F "),f=r("strong"),x=n("Essayez !"),w=n(" Reproduisez les deux derni\xE8res \xE9tapes (tok\xE9nisation et conversion en identifiants d\u2019entr\xE9e) sur les phrases des entr\xE9es que nous avons utilis\xE9es dans la section 2 (\xAB I\u2019ve been waiting for a HuggingFace course my whole life. \xBB et \xAB I hate this so much! \xBB). V\xE9rifiez que vous obtenez les m\xEAmes identifiants d\u2019entr\xE9e que nous avons obtenus pr\xE9c\xE9demment !")},l(g){c=a(g,"P",{});var P=l(c);q=o(P,"\u270F\uFE0F "),f=a(P,"STRONG",{});var A=l(f);x=o(A,"Essayez !"),A.forEach(s),w=o(P," Reproduisez les deux derni\xE8res \xE9tapes (tok\xE9nisation et conversion en identifiants d\u2019entr\xE9e) sur les phrases des entr\xE9es que nous avons utilis\xE9es dans la section 2 (\xAB I\u2019ve been waiting for a HuggingFace course my whole life. \xBB et \xAB I hate this so much! \xBB). V\xE9rifiez que vous obtenez les m\xEAmes identifiants d\u2019entr\xE9e que nous avons obtenus pr\xE9c\xE9demment !"),P.forEach(s)},m(g,P){u(g,c,P),t(c,q),t(c,f),t(f,x),t(c,w)},d(g){g&&s(c)}}}function ic(J){let c,q,f,x,w,g,P,A,S,G,X,T,y,M,C,H,R,z,k,Z,de,fe,ee,ve,Hr,Fn,St,Ur,Kn,tt,Yn,ye,Or,is,Br,Vr,Wn,Ht,Jr,Qn,he,Me,us,st,Gr,ps,Rr,Xn,nt,Zn,Ae,Fr,ms,Kr,Yr,eo,ke,ot,Au,Wr,rt,Cu,to,Ce,Qr,cs,Xr,Zr,so,at,no,lt,oo,F,ea,ds,ta,sa,fs,na,oa,vs,ra,aa,ro,Ut,la,ao,ne,ia,hs,ua,pa,ks,ma,ca,lo,te,da,bs,fa,va,_s,ha,ka,se,ba,Es,_a,Ea,$s,$a,qa,qs,ga,xa,io,oe,za,gs,wa,Pa,xs,ja,ya,uo,be,Le,zs,it,Ma,ws,Aa,po,ut,mo,Te,Ca,Ps,La,Ta,co,Ne,js,Na,Da,pt,Ia,ys,Sa,Ha,fo,Ot,Ua,vo,_e,mt,Lu,Oa,ct,Tu,ho,Bt,Ba,ko,N,Va,Ms,Ja,Ga,As,Ra,Fa,Cs,Ka,Ya,Ls,Wa,Qa,Ts,Xa,Za,bo,De,el,Ns,tl,sl,_o,Ee,Ie,Ds,dt,nl,Is,ol,Eo,ft,$o,Vt,rl,qo,Jt,al,go,Gt,ll,xo,$e,vt,Nu,il,ht,Du,zo,K,ul,Ss,pl,ml,Hs,cl,dl,Us,fl,vl,wo,Rt,hl,Po,qe,Se,Os,kt,kl,Bs,bl,jo,Ft,_l,yo,re,Vs,El,$l,Js,ql,gl,Gs,xl,Mo,Kt,zl,Ao,ge,He,Rs,bt,wl,Fs,Pl,Co,L,jl,Ks,yl,Ml,Ys,Al,Cl,Ws,Ll,Tl,Qs,Nl,Dl,Xs,Il,Sl,Zs,Hl,Ul,Lo,Y,Ol,en,Bl,Vl,tn,Jl,Gl,sn,Rl,Fl,To,_t,No,Yt,Et,Do,Ue,Kl,nn,Yl,Wl,Io,$t,So,qt,Ho,Wt,Ql,Uo,gt,Oo,D,Xl,on,Zl,ei,Qt,ti,si,rn,ni,oi,an,ri,ai,ln,li,ii,Bo,xe,Oe,un,xt,ui,pn,pi,Vo,zt,Jo,Be,mi,mn,ci,di,Go,ae,fi,cn,vi,hi,dn,ki,bi,Ro,U,_i,fn,Ei,$i,vn,qi,gi,hn,xi,zi,kn,wi,Pi,Fo,Ve,ji,bn,yi,Mi,Ko,ze,Je,_n,wt,Ai,En,Ci,Yo,le,Li,$n,Ti,Ni,qn,Di,Ii,Wo,Pt,Qo,Ge,Si,gn,Hi,Ui,Xo,jt,Zo,j,Oi,xn,Bi,Vi,zn,Ji,Gi,wn,Ri,Fi,Pn,Ki,Yi,jn,Wi,Qi,yn,Xi,Zi,Mn,eu,tu,er,we,Re,An,yt,su,Cn,nu,tr,ie,ou,Ln,ru,au,Tn,lu,iu,sr,Mt,nr,At,or,Fe,uu,Nn,pu,mu,rr,Ke,ar,Pe,Ye,Dn,Ct,cu,In,du,lr,ue,fu,Sn,vu,hu,Hn,ku,bu,ir,Lt,ur,Tt,pr,O,_u,Un,Eu,$u,On,qu,gu,Bn,xu,zu,Vn,wu,Pu,mr,We,ju,Jn,yu,Mu,cr;f=new sc({props:{fw:J[0]}}),A=new ce({});const Iu=[oc,nc],Nt=[];function Su(e,i){return e[0]==="pt"?0:1}y=Su(J),M=Nt[y]=Iu[y](J),H=new Rn({props:{id:"VFp38yj8h3A"}}),tt=new V({props:{code:"Jim Henson was a puppeteer # Jim Henson \xE9tait un marionnettiste.",highlighted:'Jim Henson was <span class="hljs-keyword">a</span> puppeteer <span class="hljs-comment"># Jim Henson \xE9tait un marionnettiste.</span>'}}),st=new ce({}),nt=new Rn({props:{id:"nhJxYji1aho"}}),at=new V({props:{code:`tokenized_text = "Jim Henson was a puppeteer".split()
print(tokenized_text)`,highlighted:`tokenized_text = <span class="hljs-string">&quot;Jim Henson was a puppeteer&quot;</span>.split()
<span class="hljs-built_in">print</span>(tokenized_text)`}}),lt=new V({props:{code:"['Jim', 'Henson', 'was', 'a', 'puppeteer'] # ['Jim', 'Henson', \xE9tait, 'un', 'marionnettiste']",highlighted:'[<span class="hljs-string">&#x27;Jim&#x27;</span>, <span class="hljs-string">&#x27;Henson&#x27;</span>, <span class="hljs-string">&#x27;was&#x27;</span>, <span class="hljs-string">&#x27;a&#x27;</span>, <span class="hljs-string">&#x27;puppeteer&#x27;</span>] <span class="hljs-comment"># [&#x27;Jim&#x27;, &#x27;Henson&#x27;, \xE9tait, &#x27;un&#x27;, &#x27;marionnettiste&#x27;]</span>'}}),it=new ce({}),ut=new Rn({props:{id:"ssLq_EK2jLE"}}),dt=new ce({}),ft=new Rn({props:{id:"zHvTiHr506c"}}),kt=new ce({}),bt=new ce({}),_t=new V({props:{code:`from transformers import BertTokenizer

tokenizer = BertTokenizer.from_pretrained("bert-base-cased")`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> BertTokenizer

tokenizer = BertTokenizer.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)`}});function Hu(e,i){return e[0]==="pt"?ac:rc}let dr=Hu(J),je=dr(J);return Et=new V({props:{code:`from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)`}}),$t=new V({props:{code:'tokenizer("Using a Transformer network is simple")',highlighted:'tokenizer(<span class="hljs-string">&quot;Using a Transformer network is simple&quot;</span>)'}}),qt=new V({props:{code:`{'input_ids': [101, 7993, 170, 11303, 1200, 2443, 1110, 3014, 102],
 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0],
 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}`,highlighted:`{<span class="hljs-string">&#x27;input_ids&#x27;</span>: [<span class="hljs-number">101</span>, <span class="hljs-number">7993</span>, <span class="hljs-number">170</span>, <span class="hljs-number">11303</span>, <span class="hljs-number">1200</span>, <span class="hljs-number">2443</span>, <span class="hljs-number">1110</span>, <span class="hljs-number">3014</span>, <span class="hljs-number">102</span>],
 <span class="hljs-string">&#x27;token_type_ids&#x27;</span>: [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
 <span class="hljs-string">&#x27;attention_mask&#x27;</span>: [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]}`}}),gt=new V({props:{code:'tokenizer.save_pretrained("directory_on_my_computer")',highlighted:'tokenizer.save_pretrained(<span class="hljs-string">&quot;directory_on_my_computer&quot;</span>)'}}),xt=new ce({}),zt=new Rn({props:{id:"Yffk5aydLzg"}}),wt=new ce({}),Pt=new V({props:{code:`from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")

sequence = "Using a Transformer network is simple"
tokens = tokenizer.tokenize(sequence)

print(tokens)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

sequence = <span class="hljs-string">&quot;Using a Transformer network is simple&quot;</span>
tokens = tokenizer.tokenize(sequence)

<span class="hljs-built_in">print</span>(tokens)`}}),jt=new V({props:{code:"['Using', 'a', 'transform', '##er', 'network', 'is', 'simple']",highlighted:'[<span class="hljs-string">&#x27;Using&#x27;</span>, <span class="hljs-string">&#x27;a&#x27;</span>, <span class="hljs-string">&#x27;transform&#x27;</span>, <span class="hljs-string">&#x27;##er&#x27;</span>, <span class="hljs-string">&#x27;network&#x27;</span>, <span class="hljs-string">&#x27;is&#x27;</span>, <span class="hljs-string">&#x27;simple&#x27;</span>]'}}),yt=new ce({}),Mt=new V({props:{code:`ids = tokenizer.convert_tokens_to_ids(tokens)

print(ids)`,highlighted:`ids = tokenizer.convert_tokens_to_ids(tokens)

<span class="hljs-built_in">print</span>(ids)`}}),At=new V({props:{code:"[7993, 170, 11303, 1200, 2443, 1110, 3014]",highlighted:'[<span class="hljs-number">7993</span>, <span class="hljs-number">170</span>, <span class="hljs-number">11303</span>, <span class="hljs-number">1200</span>, <span class="hljs-number">2443</span>, <span class="hljs-number">1110</span>, <span class="hljs-number">3014</span>]'}}),Ke=new tc({props:{$$slots:{default:[lc]},$$scope:{ctx:J}}}),Ct=new ce({}),Lt=new V({props:{code:`decoded_string = tokenizer.decode([7993, 170, 11303, 1200, 2443, 1110, 3014])
print(decoded_string)`,highlighted:`decoded_string = tokenizer.decode([<span class="hljs-number">7993</span>, <span class="hljs-number">170</span>, <span class="hljs-number">11303</span>, <span class="hljs-number">1200</span>, <span class="hljs-number">2443</span>, <span class="hljs-number">1110</span>, <span class="hljs-number">3014</span>])
<span class="hljs-built_in">print</span>(decoded_string)`}}),Tt=new V({props:{code:"'Using a Transformer network is simple'",highlighted:'<span class="hljs-string">&#x27;Using a Transformer network is simple&#x27;</span>'}}),{c(){c=r("meta"),q=p(),b(f.$$.fragment),x=p(),w=r("h1"),g=r("a"),P=r("span"),b(A.$$.fragment),S=p(),G=r("span"),X=n("Les *tokenizers*"),T=p(),M.c(),C=p(),b(H.$$.fragment),R=p(),z=r("p"),k=n("Les "),Z=r("em"),de=n("tokenizers"),fe=n(" sont l\u2019un des principaux composants du pipeline de NLP. Ils ont un seul objectif : traduire le texte en donn\xE9es pouvant \xEAtre trait\xE9es par le mod\xE8le. Les mod\xE8les ne pouvant traiter que des nombres, les "),ee=r("em"),ve=n("tokenizers"),Hr=n(" doivent convertir nos entr\xE9es textuelles en donn\xE9es num\xE9riques. Dans cette section, nous allons explorer ce qui se passe exactement dans le pipeline de tok\xE9nisation."),Fn=p(),St=r("p"),Ur=n("Dans les t\xE2ches de NLP, les donn\xE9es trait\xE9es sont g\xE9n\xE9ralement du texte brut. Voici un exemple de ce type de texte :"),Kn=p(),b(tt.$$.fragment),Yn=p(),ye=r("p"),Or=n("Les mod\xE8les ne pouvant traiter que des nombres, nous devons trouver un moyen de convertir le texte brut en nombres. C\u2019est ce que font les "),is=r("em"),Br=n("tokenizers"),Vr=n(" et il existe de nombreuses fa\xE7ons de proc\xE9der. L\u2019objectif est de trouver la repr\xE9sentation la plus significative, c\u2019est-\xE0-dire celle qui a le plus de sens pour le mod\xE8le, et si possible qui soit la plus petite."),Wn=p(),Ht=r("p"),Jr=n("Voyons quelques exemples d\u2019algorithmes de tok\xE9nisation et essayons de r\xE9pondre \xE0 certaines des questions que vous pouvez vous poser \xE0 ce sujet."),Qn=p(),he=r("h2"),Me=r("a"),us=r("span"),b(st.$$.fragment),Gr=p(),ps=r("span"),Rr=n("*Tokenizer* bas\xE9 sur les mots"),Xn=p(),b(nt.$$.fragment),Zn=p(),Ae=r("p"),Fr=n("Le premier type de "),ms=r("em"),Kr=n("tokenizer"),Yr=n(" qui vient \xE0 l\u2019esprit est celui bas\xE9 sur les mots. Il est g\xE9n\xE9ralement tr\xE8s facile \xE0 utiliser et configurable avec seulement quelques r\xE8gles. Il donne souvent des r\xE9sultats d\xE9cents. Par exemple, dans l\u2019image ci-dessous, l\u2019objectif est de diviser le texte brut en mots et de trouver une repr\xE9sentation num\xE9rique pour chacun d\u2019eux :"),eo=p(),ke=r("div"),ot=r("img"),Wr=p(),rt=r("img"),to=p(),Ce=r("p"),Qr=n("Il existe diff\xE9rentes fa\xE7ons de diviser le texte. Par exemple, nous pouvons utiliser les espaces pour segmenter le texte en mots en appliquant la fonction "),cs=r("code"),Xr=n("split()"),Zr=n(" de Python :"),so=p(),b(at.$$.fragment),no=p(),b(lt.$$.fragment),oo=p(),F=r("p"),ea=n("Il existe \xE9galement des variantes des "),ds=r("em"),ta=n("tokenizers"),sa=n(" bas\xE9s sur les mots qui ont des r\xE8gles suppl\xE9mentaires pour la ponctuation. Avec ce type de "),fs=r("em"),na=n("tokenizers"),oa=n(" nous pouvons nous retrouver avec des \xAB vocabulaires \xBB assez larges, o\xF9 un vocabulaire est d\xE9fini par le nombre total de "),vs=r("em"),ra=n("tokens"),aa=n(" ind\xE9pendants que nous avons dans notre corpus."),ro=p(),Ut=r("p"),la=n("Un identifiant est attribu\xE9 \xE0 chaque mot, en commen\xE7ant par 0 et en allant jusqu\u2019\xE0 la taille du vocabulaire. Le mod\xE8le utilise ces identifiants pour identifier chaque mot."),ao=p(),ne=r("p"),ia=n("Si nous voulons couvrir compl\xE8tement une langue avec un "),hs=r("em"),ua=n("tokenizer"),pa=n(" bas\xE9 sur les mots, nous devons avoir un identifiant pour chaque mot de la langue que nous traitons, ce qui g\xE9n\xE9re une \xE9norme quantit\xE9 de "),ks=r("em"),ma=n("tokens"),ca=n(". Par exemple, il y a plus de 500 000 mots dans la langue anglaise. Ainsi pour associer chaque mot \xE0 un identifiant, nous devrions garder la trace d\u2019autant d\u2019identifiants. De plus, des mots comme \xAB chien \xBB sont repr\xE9sent\xE9s diff\xE9remment de mots comme \xAB chiens \xBB. Le mod\xE8le n\u2019a initialement aucun moyen de savoir que \xAB chien \xBB et \xAB chiens \xBB sont similaires : il identifie les deux mots comme non apparent\xE9s. Il en va de m\xEAme pour d\u2019autres mots similaires, comme \xAB maison \xBB et \xAB maisonnette \xBB que le mod\xE8le ne consid\xE9rera pas comme similaires au d\xE9part."),lo=p(),te=r("p"),da=n("Enfin, nous avons besoin d\u2019un "),bs=r("em"),fa=n("token"),va=n(" personnalis\xE9 pour repr\xE9senter les mots qui ne font pas partie de notre vocabulaire. C\u2019est ce qu\u2019on appelle le "),_s=r("em"),ha=n("token"),ka=n(" \xAB inconnu \xBB souvent repr\xE9sent\xE9 par \xAB [UNK] \xBB (de l\u2019anglais \xAB unknown \xBB) ou \xAB "),se=r("unk"),ba=n("; \xBB. C\u2019est g\xE9n\xE9ralement un mauvais signe si vous constatez que le "),Es=r("em"),_a=n("tokenizer"),Ea=n(" produit un nombre important de ce jeton sp\xE9cial. Cela signifie qu\u2019il n\u2019a pas \xE9t\xE9 en mesure de r\xE9cup\xE9rer une repr\xE9sentation sens\xE9e d\u2019un mot et que vous perdez des informations en cours de route. L\u2019objectif de l\u2019\xE9laboration du vocabulaire est de faire en sorte que le "),$s=r("em"),$a=n("tokenizer"),qa=n(" transforme le moins de mots possible en "),qs=r("em"),ga=n("token"),xa=n(" inconnu."),io=p(),oe=r("p"),za=n("Une fa\xE7on de r\xE9duire la quantit\xE9 de "),gs=r("em"),wa=n("tokens"),Pa=n(" inconnus est d\u2019aller un niveau plus profond, en utilisant un "),xs=r("em"),ja=n("tokenizer"),ya=n(" bas\xE9 sur les caract\xE8res."),uo=p(),be=r("h2"),Le=r("a"),zs=r("span"),b(it.$$.fragment),Ma=p(),ws=r("span"),Aa=n("*Tokenizer* bas\xE9 sur les caract\xE8res"),po=p(),b(ut.$$.fragment),mo=p(),Te=r("p"),Ca=n("Les "),Ps=r("em"),La=n("tokenizers"),Ta=n(" bas\xE9s sur les caract\xE8res divisent le texte en caract\xE8res, plut\xF4t qu\u2019en mots. Cela pr\xE9sente deux avantages principaux :"),co=p(),Ne=r("ul"),js=r("li"),Na=n("le vocabulaire est beaucoup plus petit"),Da=p(),pt=r("li"),Ia=n("il y a beaucoup moins de "),ys=r("em"),Sa=n("tokens"),Ha=n(" hors vocabulaire (inconnus) puisque chaque mot peut \xEAtre construit \xE0 partir de caract\xE8res."),fo=p(),Ot=r("p"),Ua=n("Mais l\xE0 aussi, des questions se posent concernant les espaces et la ponctuation :"),vo=p(),_e=r("div"),mt=r("img"),Oa=p(),ct=r("img"),ho=p(),Bt=r("p"),Ba=n("Cette approche n\u2019est pas non plus parfaite. Puisque la repr\xE9sentation est maintenant bas\xE9e sur des caract\xE8res plut\xF4t que sur des mots, on pourrait dire intuitivement qu\u2019elle est moins significative : chaque caract\xE8re ne signifie pas grand-chose en soi, alors que c\u2019est le cas pour les mots. Toutefois, l\xE0 encore, cela diff\xE8re selon la langue. En chinois, par exemple, chaque caract\xE8re est porteur de plus d\u2019informations qu\u2019un caract\xE8re dans une langue latine."),ko=p(),N=r("p"),Va=n("Un autre \xE9l\xE9ment \xE0 prendre en compte est que nous nous retrouverons avec une tr\xE8s grande quantit\xE9 de "),Ms=r("em"),Ja=n("tokens"),Ga=n(" \xE0 traiter par notre mod\xE8le. Alors qu\u2019avec un "),As=r("em"),Ra=n("tokenizer"),Fa=n(" bas\xE9 sur les mots, pour un mot donn\xE9 on aurait qu\u2019un seul "),Cs=r("em"),Ka=n("token"),Ya=n(", avec un "),Ls=r("em"),Wa=n("tokenizer"),Qa=n(" bas\xE9 sur les caract\xE8res, cela peut facilement se transformer en 10 "),Ts=r("em"),Xa=n("tokens"),Za=n(" voire plus."),bo=p(),De=r("p"),el=n("Pour obtenir le meilleur des deux mondes, nous pouvons utiliser une troisi\xE8me technique qui combine les deux approches : la "),Ns=r("em"),tl=n("tok\xE9nisation en sous-mots"),sl=n("."),_o=p(),Ee=r("h2"),Ie=r("a"),Ds=r("span"),b(dt.$$.fragment),nl=p(),Is=r("span"),ol=n("Tok\xE9nisation en sous-mots"),Eo=p(),b(ft.$$.fragment),$o=p(),Vt=r("p"),rl=n("Les algorithmes de tokenisation en sous-mots reposent sur le principe selon lequel les mots fr\xE9quemment utilis\xE9s ne doivent pas \xEAtre divis\xE9s en sous-mots plus petits, mais les mots rares doivent \xEAtre d\xE9compos\xE9s en sous-mots significatifs."),qo=p(),Jt=r("p"),al=n("Par exemple, le mot \xAB maisonnette \xBB peut \xEAtre consid\xE9r\xE9 comme un mot rare et peut \xEAtre d\xE9compos\xE9 en \xAB maison \xBB et \xAB ette \xBB. Ces deux mots sont susceptibles d\u2019appara\xEEtre plus fr\xE9quemment en tant que sous-mots autonomes, alors qu\u2019en m\xEAme temps le sens de \xAB maison \xBB est conserv\xE9 par le sens composite de \xAB maison \xBB et \xAB ette \xBB."),go=p(),Gt=r("p"),ll=n("Voici un exemple montrant comment un algorithme de tokenisation en sous-mots tokeniserait la s\xE9quence \xAB Let\u2019s do tokenization ! \xBB :"),xo=p(),$e=r("div"),vt=r("img"),il=p(),ht=r("img"),zo=p(),K=r("p"),ul=n("Ces sous-mots finissent par fournir beaucoup de sens s\xE9mantique. Par exemple, ci-dessus, \xAB tokenization \xBB a \xE9t\xE9 divis\xE9 en \xAB token \xBB et \xAB ization \xBB : deux "),Ss=r("em"),pl=n("tokens"),ml=n(" qui ont un sens s\xE9mantique tout en \xE9tant peu encombrants (seuls deux "),Hs=r("em"),cl=n("tokens"),dl=n(" sont n\xE9cessaires pour repr\xE9senter un long mot). Cela nous permet d\u2019avoir une couverture relativement bonne avec de petits vocabulaires et presque aucun "),Us=r("em"),fl=n("token"),vl=n(" inconnu."),wo=p(),Rt=r("p"),hl=n("Cette approche est particuli\xE8rement utile dans les langues agglutinantes comme le turc, o\xF9 l\u2019on peut former des mots complexes (presque) arbitrairement longs en encha\xEEnant des sous-mots."),Po=p(),qe=r("h3"),Se=r("a"),Os=r("span"),b(kt.$$.fragment),kl=p(),Bs=r("span"),bl=n("Et plus encore !"),jo=p(),Ft=r("p"),_l=n("Il existe de nombreuses autres techniques. Pour n\u2019en citer que quelques-unes :"),yo=p(),re=r("ul"),Vs=r("li"),El=n("le Byte-level BPE utilis\xE9 par exemple dans le GPT-2"),$l=p(),Js=r("li"),ql=n("le WordPiece utilis\xE9 par exemple dans BERT"),gl=p(),Gs=r("li"),xl=n("SentencePiece ou Unigram, utilis\xE9s dans plusieurs mod\xE8les multilingues."),Mo=p(),Kt=r("p"),zl=n("Vous devriez maintenant avoir une connaissance suffisante du fonctionnement des tokenizers pour commencer \xE0 utiliser l\u2019API."),Ao=p(),ge=r("h2"),He=r("a"),Rs=r("span"),b(bt.$$.fragment),wl=p(),Fs=r("span"),Pl=n("Chargement et sauvegarde"),Co=p(),L=r("p"),jl=n("Le chargement et la sauvegarde des "),Ks=r("em"),yl=n("tokenizers"),Ml=n(" est aussi simple que pour les mod\xE8les. En fait, c\u2019est bas\xE9 sur les deux m\xEAmes m\xE9thodes : "),Ys=r("code"),Al=n("from_pretrained()"),Cl=n(" et "),Ws=r("code"),Ll=n("save_pretrained()"),Tl=n(". Ces m\xE9thodes vont charger ou sauvegarder l\u2019algorithme utilis\xE9 par le "),Qs=r("em"),Nl=n("tokenizer"),Dl=n(" (un peu comme l\u2019"),Xs=r("em"),Il=n("architecture"),Sl=n(" du mod\xE8le) ainsi que son vocabulaire (un peu comme les "),Zs=r("em"),Hl=n("poids"),Ul=n(" du mod\xE8le)."),Lo=p(),Y=r("p"),Ol=n("Le chargement du "),en=r("em"),Bl=n("tokenizer"),Vl=n(" de BERT entra\xEEn\xE9 avec le m\xEAme "),tn=r("em"),Jl=n("checkpoint"),Gl=n(" que BERT se fait de la m\xEAme mani\xE8re que le chargement du mod\xE8le, sauf que nous utilisons la classe "),sn=r("code"),Rl=n("BertTokenizer"),Fl=n(" :"),To=p(),b(_t.$$.fragment),No=p(),je.c(),Yt=p(),b(Et.$$.fragment),Do=p(),Ue=r("p"),Kl=n("Nous pouvons \xE0 pr\xE9sent utiliser le "),nn=r("em"),Yl=n("tokenizer"),Wl=n(" comme indiqu\xE9 dans la section pr\xE9c\xE9dente :"),Io=p(),b($t.$$.fragment),So=p(),b(qt.$$.fragment),Ho=p(),Wt=r("p"),Ql=n("La sauvegarde d\u2019un tokenizer est identique \xE0 celle d\u2019un mod\xE8le :"),Uo=p(),b(gt.$$.fragment),Oo=p(),D=r("p"),Xl=n("Nous parlerons plus en d\xE9tail des "),on=r("code"),Zl=n("token_type_ids"),ei=n(" au "),Qt=r("a"),ti=n("Chapitre 3"),si=n(" et nous expliquerons la cl\xE9 "),rn=r("code"),ni=n("attention_mask"),oi=n(" un peu plus tard. Tout d\u2019abord, voyons comment les "),an=r("code"),ri=n("input_ids"),ai=n(" sont g\xE9n\xE9r\xE9s. Pour ce faire, nous devons examiner les m\xE9thodes interm\xE9diaires du "),ln=r("em"),li=n("tokenizer"),ii=n("."),Bo=p(),xe=r("h2"),Oe=r("a"),un=r("span"),b(xt.$$.fragment),ui=p(),pn=r("span"),pi=n("Encodage"),Vo=p(),b(zt.$$.fragment),Jo=p(),Be=r("p"),mi=n("La traduction d\u2019un texte en chiffres est connue sous le nom d\u2019"),mn=r("em"),ci=n("encodage"),di=n(". L\u2019encodage se fait en deux \xE9tapes : la tokenisation, suivie de la conversion en identifiants d\u2019entr\xE9e."),Go=p(),ae=r("p"),fi=n("Comme nous l\u2019avons vu, la premi\xE8re \xE9tape consiste \xE0 diviser le texte en mots (ou parties de mots, symboles de ponctuation, etc.), g\xE9n\xE9ralement appel\xE9s "),cn=r("em"),vi=n("tokens"),hi=n(". De nombreuses r\xE8gles peuvent r\xE9gir ce processus. C\u2019est pourquoi nous devons instancier le "),dn=r("em"),ki=n("tokenizer"),bi=n(" en utilisant le nom du mod\xE8le afin de nous assurer que nous utilisons les m\xEAmes r\xE8gles que celles utilis\xE9es lors du pr\xE9-entra\xEEnement du mod\xE8le."),Ro=p(),U=r("p"),_i=n("La deuxi\xE8me \xE9tape consiste \xE0 convertir ces "),fn=r("em"),Ei=n("tokens"),$i=n(" en nombres afin de construire un tenseur \xE0 partir de ceux-ci ainsi que de les transmettre au mod\xE8le. Pour ce faire, le "),vn=r("em"),qi=n("tokenizer"),gi=n(" poss\xE8de un "),hn=r("em"),xi=n("vocabulaire"),zi=n(", qui est la partie que nous t\xE9l\xE9chargeons lorsque nous l\u2019instancions avec la m\xE9thode "),kn=r("code"),wi=n("from_pretrained()"),Pi=n(". Encore une fois, nous devons utiliser le m\xEAme vocabulaire que celui utilis\xE9 lors du pr\xE9-entra\xEEnement du mod\xE8le."),Fo=p(),Ve=r("p"),ji=n("Pour mieux comprendre les deux \xE9tapes, nous allons les explorer s\xE9par\xE9ment. A noter que nous utilisons des m\xE9thodes effectuant s\xE9par\xE9ment des parties du pipeline de tokenisation afin de montrer les r\xE9sultats interm\xE9diaires de ces \xE9tapes. N\xE9anmoins, en pratique, il faut appeler le "),bn=r("em"),yi=n("tokenizer"),Mi=n(" directement sur vos entr\xE9es (comme indiqu\xE9 dans la section 2)."),Ko=p(),ze=r("h3"),Je=r("a"),_n=r("span"),b(wt.$$.fragment),Ai=p(),En=r("span"),Ci=n("Tokenisation"),Yo=p(),le=r("p"),Li=n("Le processus de tokenisation est effectu\xE9 par la m\xE9thode "),$n=r("code"),Ti=n("tokenize()"),Ni=n(" du "),qn=r("em"),Di=n("tokenizer"),Ii=n(" :"),Wo=p(),b(Pt.$$.fragment),Qo=p(),Ge=r("p"),Si=n("La sortie de cette m\xE9thode est une liste de cha\xEEnes de caract\xE8res ou de "),gn=r("em"),Hi=n("tokens"),Ui=n(" :"),Xo=p(),b(jt.$$.fragment),Zo=p(),j=r("p"),Oi=n("Ce "),xn=r("em"),Bi=n("tokenizer"),Vi=n(" est un "),zn=r("em"),Ji=n("tokenizer"),Gi=n(" de sous-mots : il d\xE9coupe les mots jusqu\u2019\xE0 obtenir des "),wn=r("em"),Ri=n("tokens"),Fi=n(" qui peuvent \xEAtre repr\xE9sent\xE9s par son vocabulaire. C\u2019est le cas ici avec "),Pn=r("code"),Ki=n("transformer"),Yi=n(" qui est divis\xE9 en deux "),jn=r("em"),Wi=n("tokens"),Qi=n(" : "),yn=r("code"),Xi=n("transform"),Zi=n(" et "),Mn=r("code"),eu=n("##er"),tu=n("."),er=p(),we=r("h3"),Re=r("a"),An=r("span"),b(yt.$$.fragment),su=p(),Cn=r("span"),nu=n("De *tokens* aux identifiants d'entr\xE9e"),tr=p(),ie=r("p"),ou=n("La conversion en identifiants d\u2019entr\xE9e est g\xE9r\xE9e par la m\xE9thode "),Ln=r("code"),ru=n("convert_tokens_to_ids()"),au=n(" du "),Tn=r("em"),lu=n("tokenizer"),iu=n(" :"),sr=p(),b(Mt.$$.fragment),nr=p(),b(At.$$.fragment),or=p(),Fe=r("p"),uu=n("Une fois converties en tenseur dans le "),Nn=r("em"),pu=n("framework"),mu=n(" appropri\xE9, ces sorties peuvent ensuite \xEAtre utilis\xE9es comme entr\xE9es d\u2019un mod\xE8le, comme nous l\u2019avons vu pr\xE9c\xE9demment dans ce chapitre."),rr=p(),b(Ke.$$.fragment),ar=p(),Pe=r("h2"),Ye=r("a"),Dn=r("span"),b(Ct.$$.fragment),cu=p(),In=r("span"),du=n("D\xE9codage"),lr=p(),ue=r("p"),fu=n("Le "),Sn=r("em"),vu=n("d\xE9codage"),hu=n(" va dans l\u2019autre sens : \xE0 partir d\u2019indices du vocabulaire nous voulons obtenir une cha\xEEne de caract\xE8res. Cela peut \xEAtre fait avec la m\xE9thode "),Hn=r("code"),ku=n("decode()"),bu=n(" comme suit :"),ir=p(),b(Lt.$$.fragment),ur=p(),b(Tt.$$.fragment),pr=p(),O=r("p"),_u=n("Notez que la m\xE9thode "),Un=r("code"),Eu=n("decode"),$u=n(" non seulement reconvertit les indices en "),On=r("em"),qu=n("tokens"),gu=n(" mais regroupe \xE9galement les "),Bn=r("em"),xu=n("tokens"),zu=n(" faisant partie des m\xEAmes mots. Le but \xE9tant de produire une phrase lisible. Ce comportement sera extr\xEAmement utile lorsque dans la suite du cours nous utiliserons des mod\xE8les pouvant produire du nouveau texte (soit du texte g\xE9n\xE9r\xE9 \xE0 partir d\u2019un "),Vn=r("em"),wu=n("prompt"),Pu=n(", soit pour des probl\xE8mes de s\xE9quence \xE0 s\xE9quence comme la traduction ou le r\xE9sum\xE9 de texte)."),mr=p(),We=r("p"),ju=n("Vous devriez maintenant comprendre les op\xE9rations atomiques qu\u2019un "),Jn=r("em"),yu=n("tokenizer"),Mu=n(" peut g\xE9rer : tokenisation, conversion en identifiants, et reconversion des identifiants en cha\xEEne de caract\xE8res. Cependant, nous n\u2019avons fait qu\u2019effleurer la partie \xE9merg\xE9e de l\u2019iceberg. Dans la section suivante, nous allons pousser notre approche jusqu\u2019\xE0 ses limites et voir comment les surmonter."),this.h()},l(e){const i=Qm('[data-svelte="svelte-1phssyn"]',document.head);c=a(i,"META",{name:!0,content:!0}),i.forEach(s),q=m(e),_(f.$$.fragment,e),x=m(e),w=a(e,"H1",{class:!0});var Dt=l(w);g=a(Dt,"A",{id:!0,class:!0,href:!0});var Xt=l(g);P=a(Xt,"SPAN",{});var Gn=l(P);_(A.$$.fragment,Gn),Gn.forEach(s),Xt.forEach(s),S=m(Dt),G=a(Dt,"SPAN",{});var Uu=l(G);X=o(Uu,"Les *tokenizers*"),Uu.forEach(s),Dt.forEach(s),T=m(e),M.l(e),C=m(e),_(H.$$.fragment,e),R=m(e),z=a(e,"P",{});var Zt=l(z);k=o(Zt,"Les "),Z=a(Zt,"EM",{});var Ou=l(Z);de=o(Ou,"tokenizers"),Ou.forEach(s),fe=o(Zt," sont l\u2019un des principaux composants du pipeline de NLP. Ils ont un seul objectif : traduire le texte en donn\xE9es pouvant \xEAtre trait\xE9es par le mod\xE8le. Les mod\xE8les ne pouvant traiter que des nombres, les "),ee=a(Zt,"EM",{});var Bu=l(ee);ve=o(Bu,"tokenizers"),Bu.forEach(s),Hr=o(Zt," doivent convertir nos entr\xE9es textuelles en donn\xE9es num\xE9riques. Dans cette section, nous allons explorer ce qui se passe exactement dans le pipeline de tok\xE9nisation."),Zt.forEach(s),Fn=m(e),St=a(e,"P",{});var Vu=l(St);Ur=o(Vu,"Dans les t\xE2ches de NLP, les donn\xE9es trait\xE9es sont g\xE9n\xE9ralement du texte brut. Voici un exemple de ce type de texte :"),Vu.forEach(s),Kn=m(e),_(tt.$$.fragment,e),Yn=m(e),ye=a(e,"P",{});var fr=l(ye);Or=o(fr,"Les mod\xE8les ne pouvant traiter que des nombres, nous devons trouver un moyen de convertir le texte brut en nombres. C\u2019est ce que font les "),is=a(fr,"EM",{});var Ju=l(is);Br=o(Ju,"tokenizers"),Ju.forEach(s),Vr=o(fr," et il existe de nombreuses fa\xE7ons de proc\xE9der. L\u2019objectif est de trouver la repr\xE9sentation la plus significative, c\u2019est-\xE0-dire celle qui a le plus de sens pour le mod\xE8le, et si possible qui soit la plus petite."),fr.forEach(s),Wn=m(e),Ht=a(e,"P",{});var Gu=l(Ht);Jr=o(Gu,"Voyons quelques exemples d\u2019algorithmes de tok\xE9nisation et essayons de r\xE9pondre \xE0 certaines des questions que vous pouvez vous poser \xE0 ce sujet."),Gu.forEach(s),Qn=m(e),he=a(e,"H2",{class:!0});var vr=l(he);Me=a(vr,"A",{id:!0,class:!0,href:!0});var Ru=l(Me);us=a(Ru,"SPAN",{});var Fu=l(us);_(st.$$.fragment,Fu),Fu.forEach(s),Ru.forEach(s),Gr=m(vr),ps=a(vr,"SPAN",{});var Ku=l(ps);Rr=o(Ku,"*Tokenizer* bas\xE9 sur les mots"),Ku.forEach(s),vr.forEach(s),Xn=m(e),_(nt.$$.fragment,e),Zn=m(e),Ae=a(e,"P",{});var hr=l(Ae);Fr=o(hr,"Le premier type de "),ms=a(hr,"EM",{});var Yu=l(ms);Kr=o(Yu,"tokenizer"),Yu.forEach(s),Yr=o(hr," qui vient \xE0 l\u2019esprit est celui bas\xE9 sur les mots. Il est g\xE9n\xE9ralement tr\xE8s facile \xE0 utiliser et configurable avec seulement quelques r\xE8gles. Il donne souvent des r\xE9sultats d\xE9cents. Par exemple, dans l\u2019image ci-dessous, l\u2019objectif est de diviser le texte brut en mots et de trouver une repr\xE9sentation num\xE9rique pour chacun d\u2019eux :"),hr.forEach(s),eo=m(e),ke=a(e,"DIV",{class:!0});var kr=l(ke);ot=a(kr,"IMG",{class:!0,src:!0,alt:!0}),Wr=m(kr),rt=a(kr,"IMG",{class:!0,src:!0,alt:!0}),kr.forEach(s),to=m(e),Ce=a(e,"P",{});var br=l(Ce);Qr=o(br,"Il existe diff\xE9rentes fa\xE7ons de diviser le texte. Par exemple, nous pouvons utiliser les espaces pour segmenter le texte en mots en appliquant la fonction "),cs=a(br,"CODE",{});var Wu=l(cs);Xr=o(Wu,"split()"),Wu.forEach(s),Zr=o(br," de Python :"),br.forEach(s),so=m(e),_(at.$$.fragment,e),no=m(e),_(lt.$$.fragment,e),oo=m(e),F=a(e,"P",{});var Qe=l(F);ea=o(Qe,"Il existe \xE9galement des variantes des "),ds=a(Qe,"EM",{});var Qu=l(ds);ta=o(Qu,"tokenizers"),Qu.forEach(s),sa=o(Qe," bas\xE9s sur les mots qui ont des r\xE8gles suppl\xE9mentaires pour la ponctuation. Avec ce type de "),fs=a(Qe,"EM",{});var Xu=l(fs);na=o(Xu,"tokenizers"),Xu.forEach(s),oa=o(Qe," nous pouvons nous retrouver avec des \xAB vocabulaires \xBB assez larges, o\xF9 un vocabulaire est d\xE9fini par le nombre total de "),vs=a(Qe,"EM",{});var Zu=l(vs);ra=o(Zu,"tokens"),Zu.forEach(s),aa=o(Qe," ind\xE9pendants que nous avons dans notre corpus."),Qe.forEach(s),ro=m(e),Ut=a(e,"P",{});var ep=l(Ut);la=o(ep,"Un identifiant est attribu\xE9 \xE0 chaque mot, en commen\xE7ant par 0 et en allant jusqu\u2019\xE0 la taille du vocabulaire. Le mod\xE8le utilise ces identifiants pour identifier chaque mot."),ep.forEach(s),ao=m(e),ne=a(e,"P",{});var es=l(ne);ia=o(es,"Si nous voulons couvrir compl\xE8tement une langue avec un "),hs=a(es,"EM",{});var tp=l(hs);ua=o(tp,"tokenizer"),tp.forEach(s),pa=o(es," bas\xE9 sur les mots, nous devons avoir un identifiant pour chaque mot de la langue que nous traitons, ce qui g\xE9n\xE9re une \xE9norme quantit\xE9 de "),ks=a(es,"EM",{});var sp=l(ks);ma=o(sp,"tokens"),sp.forEach(s),ca=o(es,". Par exemple, il y a plus de 500 000 mots dans la langue anglaise. Ainsi pour associer chaque mot \xE0 un identifiant, nous devrions garder la trace d\u2019autant d\u2019identifiants. De plus, des mots comme \xAB chien \xBB sont repr\xE9sent\xE9s diff\xE9remment de mots comme \xAB chiens \xBB. Le mod\xE8le n\u2019a initialement aucun moyen de savoir que \xAB chien \xBB et \xAB chiens \xBB sont similaires : il identifie les deux mots comme non apparent\xE9s. Il en va de m\xEAme pour d\u2019autres mots similaires, comme \xAB maison \xBB et \xAB maisonnette \xBB que le mod\xE8le ne consid\xE9rera pas comme similaires au d\xE9part."),es.forEach(s),lo=m(e),te=a(e,"P",{});var It=l(te);da=o(It,"Enfin, nous avons besoin d\u2019un "),bs=a(It,"EM",{});var np=l(bs);fa=o(np,"token"),np.forEach(s),va=o(It," personnalis\xE9 pour repr\xE9senter les mots qui ne font pas partie de notre vocabulaire. C\u2019est ce qu\u2019on appelle le "),_s=a(It,"EM",{});var op=l(_s);ha=o(op,"token"),op.forEach(s),ka=o(It," \xAB inconnu \xBB souvent repr\xE9sent\xE9 par \xAB [UNK] \xBB (de l\u2019anglais \xAB unknown \xBB) ou \xAB "),se=a(It,"UNK",{});var Xe=l(se);ba=o(Xe,"; \xBB. C\u2019est g\xE9n\xE9ralement un mauvais signe si vous constatez que le "),Es=a(Xe,"EM",{});var rp=l(Es);_a=o(rp,"tokenizer"),rp.forEach(s),Ea=o(Xe," produit un nombre important de ce jeton sp\xE9cial. Cela signifie qu\u2019il n\u2019a pas \xE9t\xE9 en mesure de r\xE9cup\xE9rer une repr\xE9sentation sens\xE9e d\u2019un mot et que vous perdez des informations en cours de route. L\u2019objectif de l\u2019\xE9laboration du vocabulaire est de faire en sorte que le "),$s=a(Xe,"EM",{});var ap=l($s);$a=o(ap,"tokenizer"),ap.forEach(s),qa=o(Xe," transforme le moins de mots possible en "),qs=a(Xe,"EM",{});var lp=l(qs);ga=o(lp,"token"),lp.forEach(s),xa=o(Xe," inconnu."),Xe.forEach(s),It.forEach(s),io=m(e),oe=a(e,"P",{});var ts=l(oe);za=o(ts,"Une fa\xE7on de r\xE9duire la quantit\xE9 de "),gs=a(ts,"EM",{});var ip=l(gs);wa=o(ip,"tokens"),ip.forEach(s),Pa=o(ts," inconnus est d\u2019aller un niveau plus profond, en utilisant un "),xs=a(ts,"EM",{});var up=l(xs);ja=o(up,"tokenizer"),up.forEach(s),ya=o(ts," bas\xE9 sur les caract\xE8res."),ts.forEach(s),uo=m(e),be=a(e,"H2",{class:!0});var _r=l(be);Le=a(_r,"A",{id:!0,class:!0,href:!0});var pp=l(Le);zs=a(pp,"SPAN",{});var mp=l(zs);_(it.$$.fragment,mp),mp.forEach(s),pp.forEach(s),Ma=m(_r),ws=a(_r,"SPAN",{});var cp=l(ws);Aa=o(cp,"*Tokenizer* bas\xE9 sur les caract\xE8res"),cp.forEach(s),_r.forEach(s),po=m(e),_(ut.$$.fragment,e),mo=m(e),Te=a(e,"P",{});var Er=l(Te);Ca=o(Er,"Les "),Ps=a(Er,"EM",{});var dp=l(Ps);La=o(dp,"tokenizers"),dp.forEach(s),Ta=o(Er," bas\xE9s sur les caract\xE8res divisent le texte en caract\xE8res, plut\xF4t qu\u2019en mots. Cela pr\xE9sente deux avantages principaux :"),Er.forEach(s),co=m(e),Ne=a(e,"UL",{});var $r=l(Ne);js=a($r,"LI",{});var fp=l(js);Na=o(fp,"le vocabulaire est beaucoup plus petit"),fp.forEach(s),Da=m($r),pt=a($r,"LI",{});var qr=l(pt);Ia=o(qr,"il y a beaucoup moins de "),ys=a(qr,"EM",{});var vp=l(ys);Sa=o(vp,"tokens"),vp.forEach(s),Ha=o(qr," hors vocabulaire (inconnus) puisque chaque mot peut \xEAtre construit \xE0 partir de caract\xE8res."),qr.forEach(s),$r.forEach(s),fo=m(e),Ot=a(e,"P",{});var hp=l(Ot);Ua=o(hp,"Mais l\xE0 aussi, des questions se posent concernant les espaces et la ponctuation :"),hp.forEach(s),vo=m(e),_e=a(e,"DIV",{class:!0});var gr=l(_e);mt=a(gr,"IMG",{class:!0,src:!0,alt:!0}),Oa=m(gr),ct=a(gr,"IMG",{class:!0,src:!0,alt:!0}),gr.forEach(s),ho=m(e),Bt=a(e,"P",{});var kp=l(Bt);Ba=o(kp,"Cette approche n\u2019est pas non plus parfaite. Puisque la repr\xE9sentation est maintenant bas\xE9e sur des caract\xE8res plut\xF4t que sur des mots, on pourrait dire intuitivement qu\u2019elle est moins significative : chaque caract\xE8re ne signifie pas grand-chose en soi, alors que c\u2019est le cas pour les mots. Toutefois, l\xE0 encore, cela diff\xE8re selon la langue. En chinois, par exemple, chaque caract\xE8re est porteur de plus d\u2019informations qu\u2019un caract\xE8re dans une langue latine."),kp.forEach(s),ko=m(e),N=a(e,"P",{});var W=l(N);Va=o(W,"Un autre \xE9l\xE9ment \xE0 prendre en compte est que nous nous retrouverons avec une tr\xE8s grande quantit\xE9 de "),Ms=a(W,"EM",{});var bp=l(Ms);Ja=o(bp,"tokens"),bp.forEach(s),Ga=o(W," \xE0 traiter par notre mod\xE8le. Alors qu\u2019avec un "),As=a(W,"EM",{});var _p=l(As);Ra=o(_p,"tokenizer"),_p.forEach(s),Fa=o(W," bas\xE9 sur les mots, pour un mot donn\xE9 on aurait qu\u2019un seul "),Cs=a(W,"EM",{});var Ep=l(Cs);Ka=o(Ep,"token"),Ep.forEach(s),Ya=o(W,", avec un "),Ls=a(W,"EM",{});var $p=l(Ls);Wa=o($p,"tokenizer"),$p.forEach(s),Qa=o(W," bas\xE9 sur les caract\xE8res, cela peut facilement se transformer en 10 "),Ts=a(W,"EM",{});var qp=l(Ts);Xa=o(qp,"tokens"),qp.forEach(s),Za=o(W," voire plus."),W.forEach(s),bo=m(e),De=a(e,"P",{});var xr=l(De);el=o(xr,"Pour obtenir le meilleur des deux mondes, nous pouvons utiliser une troisi\xE8me technique qui combine les deux approches : la "),Ns=a(xr,"EM",{});var gp=l(Ns);tl=o(gp,"tok\xE9nisation en sous-mots"),gp.forEach(s),sl=o(xr,"."),xr.forEach(s),_o=m(e),Ee=a(e,"H2",{class:!0});var zr=l(Ee);Ie=a(zr,"A",{id:!0,class:!0,href:!0});var xp=l(Ie);Ds=a(xp,"SPAN",{});var zp=l(Ds);_(dt.$$.fragment,zp),zp.forEach(s),xp.forEach(s),nl=m(zr),Is=a(zr,"SPAN",{});var wp=l(Is);ol=o(wp,"Tok\xE9nisation en sous-mots"),wp.forEach(s),zr.forEach(s),Eo=m(e),_(ft.$$.fragment,e),$o=m(e),Vt=a(e,"P",{});var Pp=l(Vt);rl=o(Pp,"Les algorithmes de tokenisation en sous-mots reposent sur le principe selon lequel les mots fr\xE9quemment utilis\xE9s ne doivent pas \xEAtre divis\xE9s en sous-mots plus petits, mais les mots rares doivent \xEAtre d\xE9compos\xE9s en sous-mots significatifs."),Pp.forEach(s),qo=m(e),Jt=a(e,"P",{});var jp=l(Jt);al=o(jp,"Par exemple, le mot \xAB maisonnette \xBB peut \xEAtre consid\xE9r\xE9 comme un mot rare et peut \xEAtre d\xE9compos\xE9 en \xAB maison \xBB et \xAB ette \xBB. Ces deux mots sont susceptibles d\u2019appara\xEEtre plus fr\xE9quemment en tant que sous-mots autonomes, alors qu\u2019en m\xEAme temps le sens de \xAB maison \xBB est conserv\xE9 par le sens composite de \xAB maison \xBB et \xAB ette \xBB."),jp.forEach(s),go=m(e),Gt=a(e,"P",{});var yp=l(Gt);ll=o(yp,"Voici un exemple montrant comment un algorithme de tokenisation en sous-mots tokeniserait la s\xE9quence \xAB Let\u2019s do tokenization ! \xBB :"),yp.forEach(s),xo=m(e),$e=a(e,"DIV",{class:!0});var wr=l($e);vt=a(wr,"IMG",{class:!0,src:!0,alt:!0}),il=m(wr),ht=a(wr,"IMG",{class:!0,src:!0,alt:!0}),wr.forEach(s),zo=m(e),K=a(e,"P",{});var Ze=l(K);ul=o(Ze,"Ces sous-mots finissent par fournir beaucoup de sens s\xE9mantique. Par exemple, ci-dessus, \xAB tokenization \xBB a \xE9t\xE9 divis\xE9 en \xAB token \xBB et \xAB ization \xBB : deux "),Ss=a(Ze,"EM",{});var Mp=l(Ss);pl=o(Mp,"tokens"),Mp.forEach(s),ml=o(Ze," qui ont un sens s\xE9mantique tout en \xE9tant peu encombrants (seuls deux "),Hs=a(Ze,"EM",{});var Ap=l(Hs);cl=o(Ap,"tokens"),Ap.forEach(s),dl=o(Ze," sont n\xE9cessaires pour repr\xE9senter un long mot). Cela nous permet d\u2019avoir une couverture relativement bonne avec de petits vocabulaires et presque aucun "),Us=a(Ze,"EM",{});var Cp=l(Us);fl=o(Cp,"token"),Cp.forEach(s),vl=o(Ze," inconnu."),Ze.forEach(s),wo=m(e),Rt=a(e,"P",{});var Lp=l(Rt);hl=o(Lp,"Cette approche est particuli\xE8rement utile dans les langues agglutinantes comme le turc, o\xF9 l\u2019on peut former des mots complexes (presque) arbitrairement longs en encha\xEEnant des sous-mots."),Lp.forEach(s),Po=m(e),qe=a(e,"H3",{class:!0});var Pr=l(qe);Se=a(Pr,"A",{id:!0,class:!0,href:!0});var Tp=l(Se);Os=a(Tp,"SPAN",{});var Np=l(Os);_(kt.$$.fragment,Np),Np.forEach(s),Tp.forEach(s),kl=m(Pr),Bs=a(Pr,"SPAN",{});var Dp=l(Bs);bl=o(Dp,"Et plus encore !"),Dp.forEach(s),Pr.forEach(s),jo=m(e),Ft=a(e,"P",{});var Ip=l(Ft);_l=o(Ip,"Il existe de nombreuses autres techniques. Pour n\u2019en citer que quelques-unes :"),Ip.forEach(s),yo=m(e),re=a(e,"UL",{});var ss=l(re);Vs=a(ss,"LI",{});var Sp=l(Vs);El=o(Sp,"le Byte-level BPE utilis\xE9 par exemple dans le GPT-2"),Sp.forEach(s),$l=m(ss),Js=a(ss,"LI",{});var Hp=l(Js);ql=o(Hp,"le WordPiece utilis\xE9 par exemple dans BERT"),Hp.forEach(s),gl=m(ss),Gs=a(ss,"LI",{});var Up=l(Gs);xl=o(Up,"SentencePiece ou Unigram, utilis\xE9s dans plusieurs mod\xE8les multilingues."),Up.forEach(s),ss.forEach(s),Mo=m(e),Kt=a(e,"P",{});var Op=l(Kt);zl=o(Op,"Vous devriez maintenant avoir une connaissance suffisante du fonctionnement des tokenizers pour commencer \xE0 utiliser l\u2019API."),Op.forEach(s),Ao=m(e),ge=a(e,"H2",{class:!0});var jr=l(ge);He=a(jr,"A",{id:!0,class:!0,href:!0});var Bp=l(He);Rs=a(Bp,"SPAN",{});var Vp=l(Rs);_(bt.$$.fragment,Vp),Vp.forEach(s),Bp.forEach(s),wl=m(jr),Fs=a(jr,"SPAN",{});var Jp=l(Fs);Pl=o(Jp,"Chargement et sauvegarde"),Jp.forEach(s),jr.forEach(s),Co=m(e),L=a(e,"P",{});var B=l(L);jl=o(B,"Le chargement et la sauvegarde des "),Ks=a(B,"EM",{});var Gp=l(Ks);yl=o(Gp,"tokenizers"),Gp.forEach(s),Ml=o(B," est aussi simple que pour les mod\xE8les. En fait, c\u2019est bas\xE9 sur les deux m\xEAmes m\xE9thodes : "),Ys=a(B,"CODE",{});var Rp=l(Ys);Al=o(Rp,"from_pretrained()"),Rp.forEach(s),Cl=o(B," et "),Ws=a(B,"CODE",{});var Fp=l(Ws);Ll=o(Fp,"save_pretrained()"),Fp.forEach(s),Tl=o(B,". Ces m\xE9thodes vont charger ou sauvegarder l\u2019algorithme utilis\xE9 par le "),Qs=a(B,"EM",{});var Kp=l(Qs);Nl=o(Kp,"tokenizer"),Kp.forEach(s),Dl=o(B," (un peu comme l\u2019"),Xs=a(B,"EM",{});var Yp=l(Xs);Il=o(Yp,"architecture"),Yp.forEach(s),Sl=o(B," du mod\xE8le) ainsi que son vocabulaire (un peu comme les "),Zs=a(B,"EM",{});var Wp=l(Zs);Hl=o(Wp,"poids"),Wp.forEach(s),Ul=o(B," du mod\xE8le)."),B.forEach(s),Lo=m(e),Y=a(e,"P",{});var et=l(Y);Ol=o(et,"Le chargement du "),en=a(et,"EM",{});var Qp=l(en);Bl=o(Qp,"tokenizer"),Qp.forEach(s),Vl=o(et," de BERT entra\xEEn\xE9 avec le m\xEAme "),tn=a(et,"EM",{});var Xp=l(tn);Jl=o(Xp,"checkpoint"),Xp.forEach(s),Gl=o(et," que BERT se fait de la m\xEAme mani\xE8re que le chargement du mod\xE8le, sauf que nous utilisons la classe "),sn=a(et,"CODE",{});var Zp=l(sn);Rl=o(Zp,"BertTokenizer"),Zp.forEach(s),Fl=o(et," :"),et.forEach(s),To=m(e),_(_t.$$.fragment,e),No=m(e),je.l(e),Yt=m(e),_(Et.$$.fragment,e),Do=m(e),Ue=a(e,"P",{});var yr=l(Ue);Kl=o(yr,"Nous pouvons \xE0 pr\xE9sent utiliser le "),nn=a(yr,"EM",{});var em=l(nn);Yl=o(em,"tokenizer"),em.forEach(s),Wl=o(yr," comme indiqu\xE9 dans la section pr\xE9c\xE9dente :"),yr.forEach(s),Io=m(e),_($t.$$.fragment,e),So=m(e),_(qt.$$.fragment,e),Ho=m(e),Wt=a(e,"P",{});var tm=l(Wt);Ql=o(tm,"La sauvegarde d\u2019un tokenizer est identique \xE0 celle d\u2019un mod\xE8le :"),tm.forEach(s),Uo=m(e),_(gt.$$.fragment,e),Oo=m(e),D=a(e,"P",{});var Q=l(D);Xl=o(Q,"Nous parlerons plus en d\xE9tail des "),on=a(Q,"CODE",{});var sm=l(on);Zl=o(sm,"token_type_ids"),sm.forEach(s),ei=o(Q," au "),Qt=a(Q,"A",{href:!0});var nm=l(Qt);ti=o(nm,"Chapitre 3"),nm.forEach(s),si=o(Q," et nous expliquerons la cl\xE9 "),rn=a(Q,"CODE",{});var om=l(rn);ni=o(om,"attention_mask"),om.forEach(s),oi=o(Q," un peu plus tard. Tout d\u2019abord, voyons comment les "),an=a(Q,"CODE",{});var rm=l(an);ri=o(rm,"input_ids"),rm.forEach(s),ai=o(Q," sont g\xE9n\xE9r\xE9s. Pour ce faire, nous devons examiner les m\xE9thodes interm\xE9diaires du "),ln=a(Q,"EM",{});var am=l(ln);li=o(am,"tokenizer"),am.forEach(s),ii=o(Q,"."),Q.forEach(s),Bo=m(e),xe=a(e,"H2",{class:!0});var Mr=l(xe);Oe=a(Mr,"A",{id:!0,class:!0,href:!0});var lm=l(Oe);un=a(lm,"SPAN",{});var im=l(un);_(xt.$$.fragment,im),im.forEach(s),lm.forEach(s),ui=m(Mr),pn=a(Mr,"SPAN",{});var um=l(pn);pi=o(um,"Encodage"),um.forEach(s),Mr.forEach(s),Vo=m(e),_(zt.$$.fragment,e),Jo=m(e),Be=a(e,"P",{});var Ar=l(Be);mi=o(Ar,"La traduction d\u2019un texte en chiffres est connue sous le nom d\u2019"),mn=a(Ar,"EM",{});var pm=l(mn);ci=o(pm,"encodage"),pm.forEach(s),di=o(Ar,". L\u2019encodage se fait en deux \xE9tapes : la tokenisation, suivie de la conversion en identifiants d\u2019entr\xE9e."),Ar.forEach(s),Go=m(e),ae=a(e,"P",{});var ns=l(ae);fi=o(ns,"Comme nous l\u2019avons vu, la premi\xE8re \xE9tape consiste \xE0 diviser le texte en mots (ou parties de mots, symboles de ponctuation, etc.), g\xE9n\xE9ralement appel\xE9s "),cn=a(ns,"EM",{});var mm=l(cn);vi=o(mm,"tokens"),mm.forEach(s),hi=o(ns,". De nombreuses r\xE8gles peuvent r\xE9gir ce processus. C\u2019est pourquoi nous devons instancier le "),dn=a(ns,"EM",{});var cm=l(dn);ki=o(cm,"tokenizer"),cm.forEach(s),bi=o(ns," en utilisant le nom du mod\xE8le afin de nous assurer que nous utilisons les m\xEAmes r\xE8gles que celles utilis\xE9es lors du pr\xE9-entra\xEEnement du mod\xE8le."),ns.forEach(s),Ro=m(e),U=a(e,"P",{});var pe=l(U);_i=o(pe,"La deuxi\xE8me \xE9tape consiste \xE0 convertir ces "),fn=a(pe,"EM",{});var dm=l(fn);Ei=o(dm,"tokens"),dm.forEach(s),$i=o(pe," en nombres afin de construire un tenseur \xE0 partir de ceux-ci ainsi que de les transmettre au mod\xE8le. Pour ce faire, le "),vn=a(pe,"EM",{});var fm=l(vn);qi=o(fm,"tokenizer"),fm.forEach(s),gi=o(pe," poss\xE8de un "),hn=a(pe,"EM",{});var vm=l(hn);xi=o(vm,"vocabulaire"),vm.forEach(s),zi=o(pe,", qui est la partie que nous t\xE9l\xE9chargeons lorsque nous l\u2019instancions avec la m\xE9thode "),kn=a(pe,"CODE",{});var hm=l(kn);wi=o(hm,"from_pretrained()"),hm.forEach(s),Pi=o(pe,". Encore une fois, nous devons utiliser le m\xEAme vocabulaire que celui utilis\xE9 lors du pr\xE9-entra\xEEnement du mod\xE8le."),pe.forEach(s),Fo=m(e),Ve=a(e,"P",{});var Cr=l(Ve);ji=o(Cr,"Pour mieux comprendre les deux \xE9tapes, nous allons les explorer s\xE9par\xE9ment. A noter que nous utilisons des m\xE9thodes effectuant s\xE9par\xE9ment des parties du pipeline de tokenisation afin de montrer les r\xE9sultats interm\xE9diaires de ces \xE9tapes. N\xE9anmoins, en pratique, il faut appeler le "),bn=a(Cr,"EM",{});var km=l(bn);yi=o(km,"tokenizer"),km.forEach(s),Mi=o(Cr," directement sur vos entr\xE9es (comme indiqu\xE9 dans la section 2)."),Cr.forEach(s),Ko=m(e),ze=a(e,"H3",{class:!0});var Lr=l(ze);Je=a(Lr,"A",{id:!0,class:!0,href:!0});var bm=l(Je);_n=a(bm,"SPAN",{});var _m=l(_n);_(wt.$$.fragment,_m),_m.forEach(s),bm.forEach(s),Ai=m(Lr),En=a(Lr,"SPAN",{});var Em=l(En);Ci=o(Em,"Tokenisation"),Em.forEach(s),Lr.forEach(s),Yo=m(e),le=a(e,"P",{});var os=l(le);Li=o(os,"Le processus de tokenisation est effectu\xE9 par la m\xE9thode "),$n=a(os,"CODE",{});var $m=l($n);Ti=o($m,"tokenize()"),$m.forEach(s),Ni=o(os," du "),qn=a(os,"EM",{});var qm=l(qn);Di=o(qm,"tokenizer"),qm.forEach(s),Ii=o(os," :"),os.forEach(s),Wo=m(e),_(Pt.$$.fragment,e),Qo=m(e),Ge=a(e,"P",{});var Tr=l(Ge);Si=o(Tr,"La sortie de cette m\xE9thode est une liste de cha\xEEnes de caract\xE8res ou de "),gn=a(Tr,"EM",{});var gm=l(gn);Hi=o(gm,"tokens"),gm.forEach(s),Ui=o(Tr," :"),Tr.forEach(s),Xo=m(e),_(jt.$$.fragment,e),Zo=m(e),j=a(e,"P",{});var I=l(j);Oi=o(I,"Ce "),xn=a(I,"EM",{});var xm=l(xn);Bi=o(xm,"tokenizer"),xm.forEach(s),Vi=o(I," est un "),zn=a(I,"EM",{});var zm=l(zn);Ji=o(zm,"tokenizer"),zm.forEach(s),Gi=o(I," de sous-mots : il d\xE9coupe les mots jusqu\u2019\xE0 obtenir des "),wn=a(I,"EM",{});var wm=l(wn);Ri=o(wm,"tokens"),wm.forEach(s),Fi=o(I," qui peuvent \xEAtre repr\xE9sent\xE9s par son vocabulaire. C\u2019est le cas ici avec "),Pn=a(I,"CODE",{});var Pm=l(Pn);Ki=o(Pm,"transformer"),Pm.forEach(s),Yi=o(I," qui est divis\xE9 en deux "),jn=a(I,"EM",{});var jm=l(jn);Wi=o(jm,"tokens"),jm.forEach(s),Qi=o(I," : "),yn=a(I,"CODE",{});var ym=l(yn);Xi=o(ym,"transform"),ym.forEach(s),Zi=o(I," et "),Mn=a(I,"CODE",{});var Mm=l(Mn);eu=o(Mm,"##er"),Mm.forEach(s),tu=o(I,"."),I.forEach(s),er=m(e),we=a(e,"H3",{class:!0});var Nr=l(we);Re=a(Nr,"A",{id:!0,class:!0,href:!0});var Am=l(Re);An=a(Am,"SPAN",{});var Cm=l(An);_(yt.$$.fragment,Cm),Cm.forEach(s),Am.forEach(s),su=m(Nr),Cn=a(Nr,"SPAN",{});var Lm=l(Cn);nu=o(Lm,"De *tokens* aux identifiants d'entr\xE9e"),Lm.forEach(s),Nr.forEach(s),tr=m(e),ie=a(e,"P",{});var rs=l(ie);ou=o(rs,"La conversion en identifiants d\u2019entr\xE9e est g\xE9r\xE9e par la m\xE9thode "),Ln=a(rs,"CODE",{});var Tm=l(Ln);ru=o(Tm,"convert_tokens_to_ids()"),Tm.forEach(s),au=o(rs," du "),Tn=a(rs,"EM",{});var Nm=l(Tn);lu=o(Nm,"tokenizer"),Nm.forEach(s),iu=o(rs," :"),rs.forEach(s),sr=m(e),_(Mt.$$.fragment,e),nr=m(e),_(At.$$.fragment,e),or=m(e),Fe=a(e,"P",{});var Dr=l(Fe);uu=o(Dr,"Une fois converties en tenseur dans le "),Nn=a(Dr,"EM",{});var Dm=l(Nn);pu=o(Dm,"framework"),Dm.forEach(s),mu=o(Dr," appropri\xE9, ces sorties peuvent ensuite \xEAtre utilis\xE9es comme entr\xE9es d\u2019un mod\xE8le, comme nous l\u2019avons vu pr\xE9c\xE9demment dans ce chapitre."),Dr.forEach(s),rr=m(e),_(Ke.$$.fragment,e),ar=m(e),Pe=a(e,"H2",{class:!0});var Ir=l(Pe);Ye=a(Ir,"A",{id:!0,class:!0,href:!0});var Im=l(Ye);Dn=a(Im,"SPAN",{});var Sm=l(Dn);_(Ct.$$.fragment,Sm),Sm.forEach(s),Im.forEach(s),cu=m(Ir),In=a(Ir,"SPAN",{});var Hm=l(In);du=o(Hm,"D\xE9codage"),Hm.forEach(s),Ir.forEach(s),lr=m(e),ue=a(e,"P",{});var as=l(ue);fu=o(as,"Le "),Sn=a(as,"EM",{});var Um=l(Sn);vu=o(Um,"d\xE9codage"),Um.forEach(s),hu=o(as," va dans l\u2019autre sens : \xE0 partir d\u2019indices du vocabulaire nous voulons obtenir une cha\xEEne de caract\xE8res. Cela peut \xEAtre fait avec la m\xE9thode "),Hn=a(as,"CODE",{});var Om=l(Hn);ku=o(Om,"decode()"),Om.forEach(s),bu=o(as," comme suit :"),as.forEach(s),ir=m(e),_(Lt.$$.fragment,e),ur=m(e),_(Tt.$$.fragment,e),pr=m(e),O=a(e,"P",{});var me=l(O);_u=o(me,"Notez que la m\xE9thode "),Un=a(me,"CODE",{});var Bm=l(Un);Eu=o(Bm,"decode"),Bm.forEach(s),$u=o(me," non seulement reconvertit les indices en "),On=a(me,"EM",{});var Vm=l(On);qu=o(Vm,"tokens"),Vm.forEach(s),gu=o(me," mais regroupe \xE9galement les "),Bn=a(me,"EM",{});var Jm=l(Bn);xu=o(Jm,"tokens"),Jm.forEach(s),zu=o(me," faisant partie des m\xEAmes mots. Le but \xE9tant de produire une phrase lisible. Ce comportement sera extr\xEAmement utile lorsque dans la suite du cours nous utiliserons des mod\xE8les pouvant produire du nouveau texte (soit du texte g\xE9n\xE9r\xE9 \xE0 partir d\u2019un "),Vn=a(me,"EM",{});var Gm=l(Vn);wu=o(Gm,"prompt"),Gm.forEach(s),Pu=o(me,", soit pour des probl\xE8mes de s\xE9quence \xE0 s\xE9quence comme la traduction ou le r\xE9sum\xE9 de texte)."),me.forEach(s),mr=m(e),We=a(e,"P",{});var Sr=l(We);ju=o(Sr,"Vous devriez maintenant comprendre les op\xE9rations atomiques qu\u2019un "),Jn=a(Sr,"EM",{});var Rm=l(Jn);yu=o(Rm,"tokenizer"),Rm.forEach(s),Mu=o(Sr," peut g\xE9rer : tokenisation, conversion en identifiants, et reconversion des identifiants en cha\xEEne de caract\xE8res. Cependant, nous n\u2019avons fait qu\u2019effleurer la partie \xE9merg\xE9e de l\u2019iceberg. Dans la section suivante, nous allons pousser notre approche jusqu\u2019\xE0 ses limites et voir comment les surmonter."),Sr.forEach(s),this.h()},h(){d(c,"name","hf:doc:metadata"),d(c,"content",JSON.stringify(uc)),d(g,"id","les-tokenizers"),d(g,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(g,"href","#les-tokenizers"),d(w,"class","relative group"),d(Me,"id","tokenizer-bas-sur-les-mots"),d(Me,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Me,"href","#tokenizer-bas-sur-les-mots"),d(he,"class","relative group"),d(ot,"class","block dark:hidden"),ls(ot.src,Au="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/word_based_tokenization.svg")||d(ot,"src",Au),d(ot,"alt","An example of word-based tokenization."),d(rt,"class","hidden dark:block"),ls(rt.src,Cu="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/word_based_tokenization-dark.svg")||d(rt,"src",Cu),d(rt,"alt","An example of word-based tokenization."),d(ke,"class","flex justify-center"),d(Le,"id","tokenizer-bas-sur-les-caractres"),d(Le,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Le,"href","#tokenizer-bas-sur-les-caractres"),d(be,"class","relative group"),d(mt,"class","block dark:hidden"),ls(mt.src,Lu="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/character_based_tokenization.svg")||d(mt,"src",Lu),d(mt,"alt","An example of character-based tokenization."),d(ct,"class","hidden dark:block"),ls(ct.src,Tu="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/character_based_tokenization-dark.svg")||d(ct,"src",Tu),d(ct,"alt","An example of character-based tokenization."),d(_e,"class","flex justify-center"),d(Ie,"id","toknisation-en-sousmots"),d(Ie,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Ie,"href","#toknisation-en-sousmots"),d(Ee,"class","relative group"),d(vt,"class","block dark:hidden"),ls(vt.src,Nu="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/bpe_subword.svg")||d(vt,"src",Nu),d(vt,"alt","A subword tokenization algorithm."),d(ht,"class","hidden dark:block"),ls(ht.src,Du="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/bpe_subword-dark.svg")||d(ht,"src",Du),d(ht,"alt","A subword tokenization algorithm."),d($e,"class","flex justify-center"),d(Se,"id","et-plus-encore"),d(Se,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Se,"href","#et-plus-encore"),d(qe,"class","relative group"),d(He,"id","chargement-et-sauvegarde"),d(He,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(He,"href","#chargement-et-sauvegarde"),d(ge,"class","relative group"),d(Qt,"href","/course/fr/chapter3"),d(Oe,"id","encodage"),d(Oe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Oe,"href","#encodage"),d(xe,"class","relative group"),d(Je,"id","tokenisation"),d(Je,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Je,"href","#tokenisation"),d(ze,"class","relative group"),d(Re,"id","de-tokens-aux-identifiants-dentre"),d(Re,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Re,"href","#de-tokens-aux-identifiants-dentre"),d(we,"class","relative group"),d(Ye,"id","dcodage"),d(Ye,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Ye,"href","#dcodage"),d(Pe,"class","relative group")},m(e,i){t(document.head,c),u(e,q,i),E(f,e,i),u(e,x,i),u(e,w,i),t(w,g),t(g,P),E(A,P,null),t(w,S),t(w,G),t(G,X),u(e,T,i),Nt[y].m(e,i),u(e,C,i),E(H,e,i),u(e,R,i),u(e,z,i),t(z,k),t(z,Z),t(Z,de),t(z,fe),t(z,ee),t(ee,ve),t(z,Hr),u(e,Fn,i),u(e,St,i),t(St,Ur),u(e,Kn,i),E(tt,e,i),u(e,Yn,i),u(e,ye,i),t(ye,Or),t(ye,is),t(is,Br),t(ye,Vr),u(e,Wn,i),u(e,Ht,i),t(Ht,Jr),u(e,Qn,i),u(e,he,i),t(he,Me),t(Me,us),E(st,us,null),t(he,Gr),t(he,ps),t(ps,Rr),u(e,Xn,i),E(nt,e,i),u(e,Zn,i),u(e,Ae,i),t(Ae,Fr),t(Ae,ms),t(ms,Kr),t(Ae,Yr),u(e,eo,i),u(e,ke,i),t(ke,ot),t(ke,Wr),t(ke,rt),u(e,to,i),u(e,Ce,i),t(Ce,Qr),t(Ce,cs),t(cs,Xr),t(Ce,Zr),u(e,so,i),E(at,e,i),u(e,no,i),E(lt,e,i),u(e,oo,i),u(e,F,i),t(F,ea),t(F,ds),t(ds,ta),t(F,sa),t(F,fs),t(fs,na),t(F,oa),t(F,vs),t(vs,ra),t(F,aa),u(e,ro,i),u(e,Ut,i),t(Ut,la),u(e,ao,i),u(e,ne,i),t(ne,ia),t(ne,hs),t(hs,ua),t(ne,pa),t(ne,ks),t(ks,ma),t(ne,ca),u(e,lo,i),u(e,te,i),t(te,da),t(te,bs),t(bs,fa),t(te,va),t(te,_s),t(_s,ha),t(te,ka),t(te,se),t(se,ba),t(se,Es),t(Es,_a),t(se,Ea),t(se,$s),t($s,$a),t(se,qa),t(se,qs),t(qs,ga),t(se,xa),u(e,io,i),u(e,oe,i),t(oe,za),t(oe,gs),t(gs,wa),t(oe,Pa),t(oe,xs),t(xs,ja),t(oe,ya),u(e,uo,i),u(e,be,i),t(be,Le),t(Le,zs),E(it,zs,null),t(be,Ma),t(be,ws),t(ws,Aa),u(e,po,i),E(ut,e,i),u(e,mo,i),u(e,Te,i),t(Te,Ca),t(Te,Ps),t(Ps,La),t(Te,Ta),u(e,co,i),u(e,Ne,i),t(Ne,js),t(js,Na),t(Ne,Da),t(Ne,pt),t(pt,Ia),t(pt,ys),t(ys,Sa),t(pt,Ha),u(e,fo,i),u(e,Ot,i),t(Ot,Ua),u(e,vo,i),u(e,_e,i),t(_e,mt),t(_e,Oa),t(_e,ct),u(e,ho,i),u(e,Bt,i),t(Bt,Ba),u(e,ko,i),u(e,N,i),t(N,Va),t(N,Ms),t(Ms,Ja),t(N,Ga),t(N,As),t(As,Ra),t(N,Fa),t(N,Cs),t(Cs,Ka),t(N,Ya),t(N,Ls),t(Ls,Wa),t(N,Qa),t(N,Ts),t(Ts,Xa),t(N,Za),u(e,bo,i),u(e,De,i),t(De,el),t(De,Ns),t(Ns,tl),t(De,sl),u(e,_o,i),u(e,Ee,i),t(Ee,Ie),t(Ie,Ds),E(dt,Ds,null),t(Ee,nl),t(Ee,Is),t(Is,ol),u(e,Eo,i),E(ft,e,i),u(e,$o,i),u(e,Vt,i),t(Vt,rl),u(e,qo,i),u(e,Jt,i),t(Jt,al),u(e,go,i),u(e,Gt,i),t(Gt,ll),u(e,xo,i),u(e,$e,i),t($e,vt),t($e,il),t($e,ht),u(e,zo,i),u(e,K,i),t(K,ul),t(K,Ss),t(Ss,pl),t(K,ml),t(K,Hs),t(Hs,cl),t(K,dl),t(K,Us),t(Us,fl),t(K,vl),u(e,wo,i),u(e,Rt,i),t(Rt,hl),u(e,Po,i),u(e,qe,i),t(qe,Se),t(Se,Os),E(kt,Os,null),t(qe,kl),t(qe,Bs),t(Bs,bl),u(e,jo,i),u(e,Ft,i),t(Ft,_l),u(e,yo,i),u(e,re,i),t(re,Vs),t(Vs,El),t(re,$l),t(re,Js),t(Js,ql),t(re,gl),t(re,Gs),t(Gs,xl),u(e,Mo,i),u(e,Kt,i),t(Kt,zl),u(e,Ao,i),u(e,ge,i),t(ge,He),t(He,Rs),E(bt,Rs,null),t(ge,wl),t(ge,Fs),t(Fs,Pl),u(e,Co,i),u(e,L,i),t(L,jl),t(L,Ks),t(Ks,yl),t(L,Ml),t(L,Ys),t(Ys,Al),t(L,Cl),t(L,Ws),t(Ws,Ll),t(L,Tl),t(L,Qs),t(Qs,Nl),t(L,Dl),t(L,Xs),t(Xs,Il),t(L,Sl),t(L,Zs),t(Zs,Hl),t(L,Ul),u(e,Lo,i),u(e,Y,i),t(Y,Ol),t(Y,en),t(en,Bl),t(Y,Vl),t(Y,tn),t(tn,Jl),t(Y,Gl),t(Y,sn),t(sn,Rl),t(Y,Fl),u(e,To,i),E(_t,e,i),u(e,No,i),je.m(e,i),u(e,Yt,i),E(Et,e,i),u(e,Do,i),u(e,Ue,i),t(Ue,Kl),t(Ue,nn),t(nn,Yl),t(Ue,Wl),u(e,Io,i),E($t,e,i),u(e,So,i),E(qt,e,i),u(e,Ho,i),u(e,Wt,i),t(Wt,Ql),u(e,Uo,i),E(gt,e,i),u(e,Oo,i),u(e,D,i),t(D,Xl),t(D,on),t(on,Zl),t(D,ei),t(D,Qt),t(Qt,ti),t(D,si),t(D,rn),t(rn,ni),t(D,oi),t(D,an),t(an,ri),t(D,ai),t(D,ln),t(ln,li),t(D,ii),u(e,Bo,i),u(e,xe,i),t(xe,Oe),t(Oe,un),E(xt,un,null),t(xe,ui),t(xe,pn),t(pn,pi),u(e,Vo,i),E(zt,e,i),u(e,Jo,i),u(e,Be,i),t(Be,mi),t(Be,mn),t(mn,ci),t(Be,di),u(e,Go,i),u(e,ae,i),t(ae,fi),t(ae,cn),t(cn,vi),t(ae,hi),t(ae,dn),t(dn,ki),t(ae,bi),u(e,Ro,i),u(e,U,i),t(U,_i),t(U,fn),t(fn,Ei),t(U,$i),t(U,vn),t(vn,qi),t(U,gi),t(U,hn),t(hn,xi),t(U,zi),t(U,kn),t(kn,wi),t(U,Pi),u(e,Fo,i),u(e,Ve,i),t(Ve,ji),t(Ve,bn),t(bn,yi),t(Ve,Mi),u(e,Ko,i),u(e,ze,i),t(ze,Je),t(Je,_n),E(wt,_n,null),t(ze,Ai),t(ze,En),t(En,Ci),u(e,Yo,i),u(e,le,i),t(le,Li),t(le,$n),t($n,Ti),t(le,Ni),t(le,qn),t(qn,Di),t(le,Ii),u(e,Wo,i),E(Pt,e,i),u(e,Qo,i),u(e,Ge,i),t(Ge,Si),t(Ge,gn),t(gn,Hi),t(Ge,Ui),u(e,Xo,i),E(jt,e,i),u(e,Zo,i),u(e,j,i),t(j,Oi),t(j,xn),t(xn,Bi),t(j,Vi),t(j,zn),t(zn,Ji),t(j,Gi),t(j,wn),t(wn,Ri),t(j,Fi),t(j,Pn),t(Pn,Ki),t(j,Yi),t(j,jn),t(jn,Wi),t(j,Qi),t(j,yn),t(yn,Xi),t(j,Zi),t(j,Mn),t(Mn,eu),t(j,tu),u(e,er,i),u(e,we,i),t(we,Re),t(Re,An),E(yt,An,null),t(we,su),t(we,Cn),t(Cn,nu),u(e,tr,i),u(e,ie,i),t(ie,ou),t(ie,Ln),t(Ln,ru),t(ie,au),t(ie,Tn),t(Tn,lu),t(ie,iu),u(e,sr,i),E(Mt,e,i),u(e,nr,i),E(At,e,i),u(e,or,i),u(e,Fe,i),t(Fe,uu),t(Fe,Nn),t(Nn,pu),t(Fe,mu),u(e,rr,i),E(Ke,e,i),u(e,ar,i),u(e,Pe,i),t(Pe,Ye),t(Ye,Dn),E(Ct,Dn,null),t(Pe,cu),t(Pe,In),t(In,du),u(e,lr,i),u(e,ue,i),t(ue,fu),t(ue,Sn),t(Sn,vu),t(ue,hu),t(ue,Hn),t(Hn,ku),t(ue,bu),u(e,ir,i),E(Lt,e,i),u(e,ur,i),E(Tt,e,i),u(e,pr,i),u(e,O,i),t(O,_u),t(O,Un),t(Un,Eu),t(O,$u),t(O,On),t(On,qu),t(O,gu),t(O,Bn),t(Bn,xu),t(O,zu),t(O,Vn),t(Vn,wu),t(O,Pu),u(e,mr,i),u(e,We,i),t(We,ju),t(We,Jn),t(Jn,yu),t(We,Mu),cr=!0},p(e,[i]){const Dt={};i&1&&(Dt.fw=e[0]),f.$set(Dt);let Xt=y;y=Su(e),y!==Xt&&(ec(),v(Nt[Xt],1,1,()=>{Nt[Xt]=null}),Xm(),M=Nt[y],M||(M=Nt[y]=Iu[y](e),M.c()),h(M,1),M.m(C.parentNode,C)),dr!==(dr=Hu(e))&&(je.d(1),je=dr(e),je&&(je.c(),je.m(Yt.parentNode,Yt)));const Gn={};i&2&&(Gn.$$scope={dirty:i,ctx:e}),Ke.$set(Gn)},i(e){cr||(h(f.$$.fragment,e),h(A.$$.fragment,e),h(M),h(H.$$.fragment,e),h(tt.$$.fragment,e),h(st.$$.fragment,e),h(nt.$$.fragment,e),h(at.$$.fragment,e),h(lt.$$.fragment,e),h(it.$$.fragment,e),h(ut.$$.fragment,e),h(dt.$$.fragment,e),h(ft.$$.fragment,e),h(kt.$$.fragment,e),h(bt.$$.fragment,e),h(_t.$$.fragment,e),h(Et.$$.fragment,e),h($t.$$.fragment,e),h(qt.$$.fragment,e),h(gt.$$.fragment,e),h(xt.$$.fragment,e),h(zt.$$.fragment,e),h(wt.$$.fragment,e),h(Pt.$$.fragment,e),h(jt.$$.fragment,e),h(yt.$$.fragment,e),h(Mt.$$.fragment,e),h(At.$$.fragment,e),h(Ke.$$.fragment,e),h(Ct.$$.fragment,e),h(Lt.$$.fragment,e),h(Tt.$$.fragment,e),cr=!0)},o(e){v(f.$$.fragment,e),v(A.$$.fragment,e),v(M),v(H.$$.fragment,e),v(tt.$$.fragment,e),v(st.$$.fragment,e),v(nt.$$.fragment,e),v(at.$$.fragment,e),v(lt.$$.fragment,e),v(it.$$.fragment,e),v(ut.$$.fragment,e),v(dt.$$.fragment,e),v(ft.$$.fragment,e),v(kt.$$.fragment,e),v(bt.$$.fragment,e),v(_t.$$.fragment,e),v(Et.$$.fragment,e),v($t.$$.fragment,e),v(qt.$$.fragment,e),v(gt.$$.fragment,e),v(xt.$$.fragment,e),v(zt.$$.fragment,e),v(wt.$$.fragment,e),v(Pt.$$.fragment,e),v(jt.$$.fragment,e),v(yt.$$.fragment,e),v(Mt.$$.fragment,e),v(At.$$.fragment,e),v(Ke.$$.fragment,e),v(Ct.$$.fragment,e),v(Lt.$$.fragment,e),v(Tt.$$.fragment,e),cr=!1},d(e){s(c),e&&s(q),$(f,e),e&&s(x),e&&s(w),$(A),e&&s(T),Nt[y].d(e),e&&s(C),$(H,e),e&&s(R),e&&s(z),e&&s(Fn),e&&s(St),e&&s(Kn),$(tt,e),e&&s(Yn),e&&s(ye),e&&s(Wn),e&&s(Ht),e&&s(Qn),e&&s(he),$(st),e&&s(Xn),$(nt,e),e&&s(Zn),e&&s(Ae),e&&s(eo),e&&s(ke),e&&s(to),e&&s(Ce),e&&s(so),$(at,e),e&&s(no),$(lt,e),e&&s(oo),e&&s(F),e&&s(ro),e&&s(Ut),e&&s(ao),e&&s(ne),e&&s(lo),e&&s(te),e&&s(io),e&&s(oe),e&&s(uo),e&&s(be),$(it),e&&s(po),$(ut,e),e&&s(mo),e&&s(Te),e&&s(co),e&&s(Ne),e&&s(fo),e&&s(Ot),e&&s(vo),e&&s(_e),e&&s(ho),e&&s(Bt),e&&s(ko),e&&s(N),e&&s(bo),e&&s(De),e&&s(_o),e&&s(Ee),$(dt),e&&s(Eo),$(ft,e),e&&s($o),e&&s(Vt),e&&s(qo),e&&s(Jt),e&&s(go),e&&s(Gt),e&&s(xo),e&&s($e),e&&s(zo),e&&s(K),e&&s(wo),e&&s(Rt),e&&s(Po),e&&s(qe),$(kt),e&&s(jo),e&&s(Ft),e&&s(yo),e&&s(re),e&&s(Mo),e&&s(Kt),e&&s(Ao),e&&s(ge),$(bt),e&&s(Co),e&&s(L),e&&s(Lo),e&&s(Y),e&&s(To),$(_t,e),e&&s(No),je.d(e),e&&s(Yt),$(Et,e),e&&s(Do),e&&s(Ue),e&&s(Io),$($t,e),e&&s(So),$(qt,e),e&&s(Ho),e&&s(Wt),e&&s(Uo),$(gt,e),e&&s(Oo),e&&s(D),e&&s(Bo),e&&s(xe),$(xt),e&&s(Vo),$(zt,e),e&&s(Jo),e&&s(Be),e&&s(Go),e&&s(ae),e&&s(Ro),e&&s(U),e&&s(Fo),e&&s(Ve),e&&s(Ko),e&&s(ze),$(wt),e&&s(Yo),e&&s(le),e&&s(Wo),$(Pt,e),e&&s(Qo),e&&s(Ge),e&&s(Xo),$(jt,e),e&&s(Zo),e&&s(j),e&&s(er),e&&s(we),$(yt),e&&s(tr),e&&s(ie),e&&s(sr),$(Mt,e),e&&s(nr),$(At,e),e&&s(or),e&&s(Fe),e&&s(rr),$(Ke,e),e&&s(ar),e&&s(Pe),$(Ct),e&&s(lr),e&&s(ue),e&&s(ir),$(Lt,e),e&&s(ur),$(Tt,e),e&&s(pr),e&&s(O),e&&s(mr),e&&s(We)}}}const uc={local:"les-tokenizers",sections:[{local:"tokenizer-bas-sur-les-mots",title:"*Tokenizer* bas\xE9 sur les mots"},{local:"tokenizer-bas-sur-les-caractres",title:"*Tokenizer* bas\xE9 sur les caract\xE8res"},{local:"toknisation-en-sousmots",sections:[{local:"et-plus-encore",title:"Et plus encore !"}],title:"Tok\xE9nisation en sous-mots"},{local:"chargement-et-sauvegarde",title:"Chargement et sauvegarde"},{local:"encodage",sections:[{local:"tokenisation",title:"Tokenisation"},{local:"de-tokens-aux-identifiants-dentre",title:"De *tokens* aux identifiants d'entr\xE9e"}],title:"Encodage"},{local:"dcodage",title:"D\xE9codage"}],title:"Les *tokenizers*"};function pc(J,c,q){let f="pt";return Zm(()=>{const x=new URLSearchParams(window.location.search);q(0,f=x.get("fw")||"pt")}),[f]}class bc extends Km{constructor(c){super();Ym(this,c,pc,ic,Wm,{})}}export{bc as default,uc as metadata};
