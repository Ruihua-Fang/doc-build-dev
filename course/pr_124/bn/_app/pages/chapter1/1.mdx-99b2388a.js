import{S as ga,i as da,s as ma,e as o,c as a,a as i,d as r,b as n,T as Ft,g as c,L as mo,k as u,w as Ve,t as l,R as va,m as f,x as Qe,h as s,F as t,y as Xe,q as Ze,o as et,B as tt,v as wa}from"../../chunks/vendor-f4a867ed.js";import{I as xt}from"../../chunks/IconCopyLink-d27af064.js";function ya(R){let p,y;return{c(){p=o("iframe"),this.h()},l(g){p=a(g,"IFRAME",{class:!0,src:!0,title:!0,frameborder:!0,allow:!0}),i(p).forEach(r),this.h()},h(){n(p,"class","w-full xl:w-4/6 h-80"),Ft(p.src,y="https://www.youtube-nocookie.com/embed/"+R[0])||n(p,"src",y),n(p,"title","YouTube video player"),n(p,"frameborder","0"),n(p,"allow","accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"),p.allowFullscreen=!0},m(g,m){c(g,p,m)},p(g,[m]){m&1&&!Ft(p.src,y="https://www.youtube-nocookie.com/embed/"+g[0])&&n(p,"src",y)},i:mo,o:mo,d(g){g&&r(p)}}}function ba(R,p,y){let{id:g}=p;return R.$$set=m=>{"id"in m&&y(0,g=m.id)},[g]}class _a extends ga{constructor(p){super();da(this,p,ba,ya,ma,{id:0})}}function Ea(R){let p,y,g,m,Pe,M,Dt,Ae,Ct,rt,L,S,Te,G,Rt,He,Mt,ot,z,at,d,Gt,B,zt,Bt,O,Ot,Wt,W,qt,jt,q,Ut,Jt,j,Yt,Kt,U,Vt,Qt,lt,w,Xt,J,Zt,er,Y,tr,rr,K,or,ar,st,P,x,$e,V,lr,Ne,sr,it,ve,ir,nt,A,Q,vo,nr,X,wo,ht,b,Z,hr,ee,cr,ur,fr,Ie,pr,gr,Se,dr,ct,we,mr,ut,_,xe,vr,wr,E,yr,te,br,_r,re,Er,kr,oe,Lr,Pr,T,Ar,ae,Tr,Hr,le,$r,Nr,ft,F,Ir,se,Sr,xr,pt,H,D,Fe,ie,Fr,De,Dr,gt,ye,Cr,dt,ne,Ce,Rr,Mr,mt,he,Re,Gr,zr,vt,$,Me,Br,Or,Ge,ce,Wr,qr,wt,ue,ze,jr,Ur,yt,fe,Be,Jr,Yr,bt,N,Oe,Kr,Vr,pe,Qr,Xr,_t,I,We,Zr,eo,ge,to,ro,Et,be,oo,kt,k,de,ao,qe,lo,so,io,je,no,ho,Ue,co,Lt;return M=new xt({}),G=new xt({}),z=new _a({props:{id:"00GKzGyWFEs"}}),V=new xt({}),ie=new xt({}),{c(){p=o("meta"),y=u(),g=o("h1"),m=o("a"),Pe=o("span"),Ve(M.$$.fragment),Dt=u(),Ae=o("span"),Ct=l("Introduction"),rt=u(),L=o("h2"),S=o("a"),Te=o("span"),Ve(G.$$.fragment),Rt=u(),He=o("span"),Mt=l("Welcome to the \u{1F917} Course!"),ot=u(),Ve(z.$$.fragment),at=u(),d=o("p"),Gt=l("This course will teach you about natural language processing (NLP) using libraries from the "),B=o("a"),zt=l("Hugging Face"),Bt=l(" ecosystem \u2014 "),O=o("a"),Ot=l("\u{1F917} Transformers"),Wt=l(", "),W=o("a"),qt=l("\u{1F917} Datasets"),jt=l(", "),q=o("a"),Ut=l("\u{1F917} Tokenizers"),Jt=l(", and "),j=o("a"),Yt=l("\u{1F917} Accelerate"),Kt=l(" \u2014 as well as the "),U=o("a"),Vt=l("Hugging Face Hub"),Qt=l(". It\u2019s completely free and without ads."),lt=u(),w=o("p"),Xt=l("\u098F\u0987 \u0995\u09CB\u09B0\u09CD\u09B8\u099F\u09BF \u0986\u09AA\u09A8\u09BE\u0995\u09C7 "),J=o("a"),Zt=l("\u09B9\u09BE\u0997\u09BF\u0982 \u09AB\u09C7\u09B8"),er=l(` \u0987\u0995\u09CB\u09B8\u09BF\u09B8\u09CD\u099F\u09C7\u09AE \u09A5\u09C7\u0995\u09C7 \u2014 \u{1F917}
`),Y=o("a"),tr=l("\u{1F917} \u099F\u09CD\u09B0\u09BE\u09A8\u09CD\u09B8\u09AB\u09B0\u09AE\u09BE\u09B0"),rr=l(","),K=o("a"),or=l("\u{1F917} \u09A1\u09C7\u099F\u09BE\u09B8\u09C7\u099F"),ar=l(", \u{1F917} \u099F\u09CB\u0995\u09C7\u09A8\u09BE\u0987\u099C\u09BE\u09B0 \u098F\u09AC\u0982 \u{1F917} \u0985\u09CD\u09AF\u09BE\u0995\u09CD\u09B8\u09BF\u09B2\u09BE\u09B0\u09C7\u099F \u2014 \u09B8\u09C7\u0987\u09B8\u09BE\u09A5\u09C7 \u09B9\u09BE\u0997\u09BF\u0982 \u09AB\u09C7\u09B8 \u09B9\u09BE\u09AC \u09A5\u09C7\u0995\u09C7 \u09B2\u09BE\u0987\u09AC\u09CD\u09B0\u09C7\u09B0\u09BF \u09AC\u09CD\u09AF\u09AC\u09B9\u09BE\u09B0 \u0995\u09B0\u09C7 \u09AA\u09CD\u09B0\u09BE\u0995\u09C3\u09A4\u09BF\u0995 \u09AD\u09BE\u09B7\u09BE \u09AA\u09CD\u09B0\u0995\u09CD\u09B0\u09BF\u09AF\u09BC\u09BE\u0995\u09B0\u09A3 (NLP) \u09B8\u09AE\u09CD\u09AA\u09B0\u09CD\u0995\u09C7 \u09B6\u09C7\u0996\u09BE\u09AC\u09C7\u0964 \u098F\u099F\u09BF \u09B8\u09AE\u09CD\u09AA\u09C2\u09B0\u09CD\u09A3 \u09AC\u09BF\u09A8\u09BE\u09AE\u09C2\u09B2\u09CD\u09AF\u09C7 \u098F\u09AC\u0982 \u09AC\u09BF\u099C\u09CD\u099E\u09BE\u09AA\u09A8 \u099B\u09BE\u09A1\u09BC\u09BE\u0987\u0964"),st=u(),P=o("h2"),x=o("a"),$e=o("span"),Ve(V.$$.fragment),lr=u(),Ne=o("span"),sr=l("What to expect?"),it=u(),ve=o("p"),ir=l("Here is a brief overview of the course:"),nt=u(),A=o("div"),Q=o("img"),nr=u(),X=o("img"),ht=u(),b=o("ul"),Z=o("li"),hr=l("Chapters 1 to 4 provide an introduction to the main concepts of the \u{1F917} Transformers library. By the end of this part of the course, you will be familiar with how Transformer models work and will know how to use a model from the "),ee=o("a"),cr=l("Hugging Face Hub"),ur=l(", fine-tune it on a dataset, and share your results on the Hub!"),fr=u(),Ie=o("li"),pr=l("Chapters 5 to 8 teach the basics of \u{1F917} Datasets and \u{1F917} Tokenizers before diving into classic NLP tasks. By the end of this part, you will be able to tackle the most common NLP problems by yourself."),gr=u(),Se=o("li"),dr=l("Chapters 9 to 12 go beyond NLP, and explore how Transformer models can be used tackle tasks in speech processing and computer vision. Along the way, you\u2019ll learn how to build and share demos of your models, and optimize them for production environments. By the end of this part, you will be ready to apply \u{1F917} Transformers to (almost) any machine learning problem!"),ct=u(),we=o("p"),mr=l("This course:"),ut=u(),_=o("ul"),xe=o("li"),vr=l("Requires a good knowledge of Python"),wr=u(),E=o("li"),yr=l("Is better taken after an introductory deep learning course, such as "),te=o("a"),br=l("fast.ai\u2019s"),_r=u(),re=o("a"),Er=l("Practical Deep Learning for Coders"),kr=l(" or one of the programs developed by "),oe=o("a"),Lr=l("DeepLearning.AI"),Pr=u(),T=o("li"),Ar=l("Does not expect prior "),ae=o("a"),Tr=l("PyTorch"),Hr=l(" or "),le=o("a"),$r=l("TensorFlow"),Nr=l(" knowledge, though some familiarity with either of those will help"),ft=u(),F=o("p"),Ir=l("After you\u2019ve completed this course, we recommend checking out DeepLearning.AI\u2019s "),se=o("a"),Sr=l("Natural Language Processing Specialization"),xr=l(", which covers a wide range of traditional NLP models like naive Bayes and LSTMs that are well worth knowing about!"),pt=u(),H=o("h2"),D=o("a"),Fe=o("span"),Ve(ie.$$.fragment),Fr=u(),De=o("span"),Dr=l("Who are we?"),gt=u(),ye=o("p"),Cr=l("About the authors:"),dt=u(),ne=o("p"),Ce=o("strong"),Rr=l("Matthew Carrigan"),Mr=l(" is a Machine Learning Engineer at Hugging Face. He lives in Dublin, Ireland and previously worked as an ML engineer at Parse.ly and before that as a post-doctoral researcher at Trinity College Dublin. He does not believe we\u2019re going to get to AGI by scaling existing architectures, but has high hopes for robot immortality regardless."),mt=u(),he=o("p"),Re=o("strong"),Gr=l("Lysandre Debut"),zr=l(" is a Machine Learning Engineer at Hugging Face and has been working on the \u{1F917} Transformers library since the very early development stages. His aim is to make NLP accessible for everyone by developing tools with a very simple API."),vt=u(),$=o("p"),Me=o("strong"),Br=l("Sylvain Gugger"),Or=l(" is a Research Engineer at Hugging Face and one of the core maintainers of the \u{1F917} Transformers library. Previously he was a Research Scientist at fast.ai, and he co-wrote "),Ge=o("em"),ce=o("a"),Wr=l("Deep Learning for Coders with fastai and PyTorch"),qr=l(" with Jeremy Howard. The main focus of his research is on making deep learning more accessible, by designing and improving techniques that allow models to train fast on limited resources."),wt=u(),ue=o("p"),ze=o("strong"),jr=l("Merve Noyan"),Ur=l(" is a developer advocate at Hugging Face, working on developing tools and building content around them to democratize machine learning for everyone."),yt=u(),fe=o("p"),Be=o("strong"),Jr=l("Lucile Saulnier"),Yr=l(" is a machine learning engineer at Hugging Face, developing and supporting the use of open source tools. She is also actively involved in many research projects in the field of Natural Language Processing such as collaborative training and BigScience."),bt=u(),N=o("p"),Oe=o("strong"),Kr=l("Lewis Tunstall"),Vr=l("  is a machine learning engineer at Hugging Face, focused on developing open-source tools and making them accessible to the wider community. He is also a co-author of an upcoming "),pe=o("a"),Qr=l("O\u2019Reilly book on Transformers"),Xr=l("."),_t=u(),I=o("p"),We=o("strong"),Zr=l("Leandro von Werra"),eo=l("  is a machine learning engineer in the open-source team at Hugging Face and also a co-author of the an upcoming "),ge=o("a"),to=l("O\u2019Reilly book on Transformers"),ro=l(". He has several years of industry experience bringing NLP projects to production by working across the whole machine learning stack.."),Et=u(),be=o("p"),oo=l("Are you ready to roll? In this chapter, you will learn:"),kt=u(),k=o("ul"),de=o("li"),ao=l("How to use the "),qe=o("code"),lo=l("pipeline()"),so=l(" function to solve NLP tasks such as text generation and classification"),io=u(),je=o("li"),no=l("About the Transformer architecture"),ho=u(),Ue=o("li"),co=l("How to distinguish between encoder, decoder, and encoder-decoder architectures and use cases"),this.h()},l(e){const h=va('[data-svelte="svelte-1phssyn"]',document.head);p=a(h,"META",{name:!0,content:!0}),h.forEach(r),y=f(e),g=a(e,"H1",{class:!0});var Pt=i(g);m=a(Pt,"A",{id:!0,class:!0,href:!0});var yo=i(m);Pe=a(yo,"SPAN",{});var bo=i(Pe);Qe(M.$$.fragment,bo),bo.forEach(r),yo.forEach(r),Dt=f(Pt),Ae=a(Pt,"SPAN",{});var _o=i(Ae);Ct=s(_o,"Introduction"),_o.forEach(r),Pt.forEach(r),rt=f(e),L=a(e,"H2",{class:!0});var At=i(L);S=a(At,"A",{id:!0,class:!0,href:!0});var Eo=i(S);Te=a(Eo,"SPAN",{});var ko=i(Te);Qe(G.$$.fragment,ko),ko.forEach(r),Eo.forEach(r),Rt=f(At),He=a(At,"SPAN",{});var Lo=i(He);Mt=s(Lo,"Welcome to the \u{1F917} Course!"),Lo.forEach(r),At.forEach(r),ot=f(e),Qe(z.$$.fragment,e),at=f(e),d=a(e,"P",{});var v=i(d);Gt=s(v,"This course will teach you about natural language processing (NLP) using libraries from the "),B=a(v,"A",{href:!0,rel:!0});var Po=i(B);zt=s(Po,"Hugging Face"),Po.forEach(r),Bt=s(v," ecosystem \u2014 "),O=a(v,"A",{href:!0,rel:!0});var Ao=i(O);Ot=s(Ao,"\u{1F917} Transformers"),Ao.forEach(r),Wt=s(v,", "),W=a(v,"A",{href:!0,rel:!0});var To=i(W);qt=s(To,"\u{1F917} Datasets"),To.forEach(r),jt=s(v,", "),q=a(v,"A",{href:!0,rel:!0});var Ho=i(q);Ut=s(Ho,"\u{1F917} Tokenizers"),Ho.forEach(r),Jt=s(v,", and "),j=a(v,"A",{href:!0,rel:!0});var $o=i(j);Yt=s($o,"\u{1F917} Accelerate"),$o.forEach(r),Kt=s(v," \u2014 as well as the "),U=a(v,"A",{href:!0,rel:!0});var No=i(U);Vt=s(No,"Hugging Face Hub"),No.forEach(r),Qt=s(v,". It\u2019s completely free and without ads."),v.forEach(r),lt=f(e),w=a(e,"P",{});var C=i(w);Xt=s(C,"\u098F\u0987 \u0995\u09CB\u09B0\u09CD\u09B8\u099F\u09BF \u0986\u09AA\u09A8\u09BE\u0995\u09C7 "),J=a(C,"A",{href:!0,rel:!0});var Io=i(J);Zt=s(Io,"\u09B9\u09BE\u0997\u09BF\u0982 \u09AB\u09C7\u09B8"),Io.forEach(r),er=s(C,` \u0987\u0995\u09CB\u09B8\u09BF\u09B8\u09CD\u099F\u09C7\u09AE \u09A5\u09C7\u0995\u09C7 \u2014 \u{1F917}
`),Y=a(C,"A",{href:!0,rel:!0});var So=i(Y);tr=s(So,"\u{1F917} \u099F\u09CD\u09B0\u09BE\u09A8\u09CD\u09B8\u09AB\u09B0\u09AE\u09BE\u09B0"),So.forEach(r),rr=s(C,","),K=a(C,"A",{href:!0,rel:!0});var xo=i(K);or=s(xo,"\u{1F917} \u09A1\u09C7\u099F\u09BE\u09B8\u09C7\u099F"),xo.forEach(r),ar=s(C,", \u{1F917} \u099F\u09CB\u0995\u09C7\u09A8\u09BE\u0987\u099C\u09BE\u09B0 \u098F\u09AC\u0982 \u{1F917} \u0985\u09CD\u09AF\u09BE\u0995\u09CD\u09B8\u09BF\u09B2\u09BE\u09B0\u09C7\u099F \u2014 \u09B8\u09C7\u0987\u09B8\u09BE\u09A5\u09C7 \u09B9\u09BE\u0997\u09BF\u0982 \u09AB\u09C7\u09B8 \u09B9\u09BE\u09AC \u09A5\u09C7\u0995\u09C7 \u09B2\u09BE\u0987\u09AC\u09CD\u09B0\u09C7\u09B0\u09BF \u09AC\u09CD\u09AF\u09AC\u09B9\u09BE\u09B0 \u0995\u09B0\u09C7 \u09AA\u09CD\u09B0\u09BE\u0995\u09C3\u09A4\u09BF\u0995 \u09AD\u09BE\u09B7\u09BE \u09AA\u09CD\u09B0\u0995\u09CD\u09B0\u09BF\u09AF\u09BC\u09BE\u0995\u09B0\u09A3 (NLP) \u09B8\u09AE\u09CD\u09AA\u09B0\u09CD\u0995\u09C7 \u09B6\u09C7\u0996\u09BE\u09AC\u09C7\u0964 \u098F\u099F\u09BF \u09B8\u09AE\u09CD\u09AA\u09C2\u09B0\u09CD\u09A3 \u09AC\u09BF\u09A8\u09BE\u09AE\u09C2\u09B2\u09CD\u09AF\u09C7 \u098F\u09AC\u0982 \u09AC\u09BF\u099C\u09CD\u099E\u09BE\u09AA\u09A8 \u099B\u09BE\u09A1\u09BC\u09BE\u0987\u0964"),C.forEach(r),st=f(e),P=a(e,"H2",{class:!0});var Tt=i(P);x=a(Tt,"A",{id:!0,class:!0,href:!0});var Fo=i(x);$e=a(Fo,"SPAN",{});var Do=i($e);Qe(V.$$.fragment,Do),Do.forEach(r),Fo.forEach(r),lr=f(Tt),Ne=a(Tt,"SPAN",{});var Co=i(Ne);sr=s(Co,"What to expect?"),Co.forEach(r),Tt.forEach(r),it=f(e),ve=a(e,"P",{});var Ro=i(ve);ir=s(Ro,"Here is a brief overview of the course:"),Ro.forEach(r),nt=f(e),A=a(e,"DIV",{class:!0});var Ht=i(A);Q=a(Ht,"IMG",{class:!0,src:!0,alt:!0}),nr=f(Ht),X=a(Ht,"IMG",{class:!0,src:!0,alt:!0}),Ht.forEach(r),ht=f(e),b=a(e,"UL",{});var _e=i(b);Z=a(_e,"LI",{});var $t=i(Z);hr=s($t,"Chapters 1 to 4 provide an introduction to the main concepts of the \u{1F917} Transformers library. By the end of this part of the course, you will be familiar with how Transformer models work and will know how to use a model from the "),ee=a($t,"A",{href:!0,rel:!0});var Mo=i(ee);cr=s(Mo,"Hugging Face Hub"),Mo.forEach(r),ur=s($t,", fine-tune it on a dataset, and share your results on the Hub!"),$t.forEach(r),fr=f(_e),Ie=a(_e,"LI",{});var Go=i(Ie);pr=s(Go,"Chapters 5 to 8 teach the basics of \u{1F917} Datasets and \u{1F917} Tokenizers before diving into classic NLP tasks. By the end of this part, you will be able to tackle the most common NLP problems by yourself."),Go.forEach(r),gr=f(_e),Se=a(_e,"LI",{});var zo=i(Se);dr=s(zo,"Chapters 9 to 12 go beyond NLP, and explore how Transformer models can be used tackle tasks in speech processing and computer vision. Along the way, you\u2019ll learn how to build and share demos of your models, and optimize them for production environments. By the end of this part, you will be ready to apply \u{1F917} Transformers to (almost) any machine learning problem!"),zo.forEach(r),_e.forEach(r),ct=f(e),we=a(e,"P",{});var Bo=i(we);mr=s(Bo,"This course:"),Bo.forEach(r),ut=f(e),_=a(e,"UL",{});var Ee=i(_);xe=a(Ee,"LI",{});var Oo=i(xe);vr=s(Oo,"Requires a good knowledge of Python"),Oo.forEach(r),wr=f(Ee),E=a(Ee,"LI",{});var me=i(E);yr=s(me,"Is better taken after an introductory deep learning course, such as "),te=a(me,"A",{href:!0,rel:!0});var Wo=i(te);br=s(Wo,"fast.ai\u2019s"),Wo.forEach(r),_r=f(me),re=a(me,"A",{href:!0,rel:!0});var qo=i(re);Er=s(qo,"Practical Deep Learning for Coders"),qo.forEach(r),kr=s(me," or one of the programs developed by "),oe=a(me,"A",{href:!0,rel:!0});var jo=i(oe);Lr=s(jo,"DeepLearning.AI"),jo.forEach(r),me.forEach(r),Pr=f(Ee),T=a(Ee,"LI",{});var ke=i(T);Ar=s(ke,"Does not expect prior "),ae=a(ke,"A",{href:!0,rel:!0});var Uo=i(ae);Tr=s(Uo,"PyTorch"),Uo.forEach(r),Hr=s(ke," or "),le=a(ke,"A",{href:!0,rel:!0});var Jo=i(le);$r=s(Jo,"TensorFlow"),Jo.forEach(r),Nr=s(ke," knowledge, though some familiarity with either of those will help"),ke.forEach(r),Ee.forEach(r),ft=f(e),F=a(e,"P",{});var Nt=i(F);Ir=s(Nt,"After you\u2019ve completed this course, we recommend checking out DeepLearning.AI\u2019s "),se=a(Nt,"A",{href:!0,rel:!0});var Yo=i(se);Sr=s(Yo,"Natural Language Processing Specialization"),Yo.forEach(r),xr=s(Nt,", which covers a wide range of traditional NLP models like naive Bayes and LSTMs that are well worth knowing about!"),Nt.forEach(r),pt=f(e),H=a(e,"H2",{class:!0});var It=i(H);D=a(It,"A",{id:!0,class:!0,href:!0});var Ko=i(D);Fe=a(Ko,"SPAN",{});var Vo=i(Fe);Qe(ie.$$.fragment,Vo),Vo.forEach(r),Ko.forEach(r),Fr=f(It),De=a(It,"SPAN",{});var Qo=i(De);Dr=s(Qo,"Who are we?"),Qo.forEach(r),It.forEach(r),gt=f(e),ye=a(e,"P",{});var Xo=i(ye);Cr=s(Xo,"About the authors:"),Xo.forEach(r),dt=f(e),ne=a(e,"P",{});var uo=i(ne);Ce=a(uo,"STRONG",{});var Zo=i(Ce);Rr=s(Zo,"Matthew Carrigan"),Zo.forEach(r),Mr=s(uo," is a Machine Learning Engineer at Hugging Face. He lives in Dublin, Ireland and previously worked as an ML engineer at Parse.ly and before that as a post-doctoral researcher at Trinity College Dublin. He does not believe we\u2019re going to get to AGI by scaling existing architectures, but has high hopes for robot immortality regardless."),uo.forEach(r),mt=f(e),he=a(e,"P",{});var fo=i(he);Re=a(fo,"STRONG",{});var ea=i(Re);Gr=s(ea,"Lysandre Debut"),ea.forEach(r),zr=s(fo," is a Machine Learning Engineer at Hugging Face and has been working on the \u{1F917} Transformers library since the very early development stages. His aim is to make NLP accessible for everyone by developing tools with a very simple API."),fo.forEach(r),vt=f(e),$=a(e,"P",{});var Je=i($);Me=a(Je,"STRONG",{});var ta=i(Me);Br=s(ta,"Sylvain Gugger"),ta.forEach(r),Or=s(Je," is a Research Engineer at Hugging Face and one of the core maintainers of the \u{1F917} Transformers library. Previously he was a Research Scientist at fast.ai, and he co-wrote "),Ge=a(Je,"EM",{});var ra=i(Ge);ce=a(ra,"A",{href:!0,rel:!0});var oa=i(ce);Wr=s(oa,"Deep Learning for Coders with fastai and PyTorch"),oa.forEach(r),ra.forEach(r),qr=s(Je," with Jeremy Howard. The main focus of his research is on making deep learning more accessible, by designing and improving techniques that allow models to train fast on limited resources."),Je.forEach(r),wt=f(e),ue=a(e,"P",{});var po=i(ue);ze=a(po,"STRONG",{});var aa=i(ze);jr=s(aa,"Merve Noyan"),aa.forEach(r),Ur=s(po," is a developer advocate at Hugging Face, working on developing tools and building content around them to democratize machine learning for everyone."),po.forEach(r),yt=f(e),fe=a(e,"P",{});var go=i(fe);Be=a(go,"STRONG",{});var la=i(Be);Jr=s(la,"Lucile Saulnier"),la.forEach(r),Yr=s(go," is a machine learning engineer at Hugging Face, developing and supporting the use of open source tools. She is also actively involved in many research projects in the field of Natural Language Processing such as collaborative training and BigScience."),go.forEach(r),bt=f(e),N=a(e,"P",{});var Ye=i(N);Oe=a(Ye,"STRONG",{});var sa=i(Oe);Kr=s(sa,"Lewis Tunstall"),sa.forEach(r),Vr=s(Ye,"  is a machine learning engineer at Hugging Face, focused on developing open-source tools and making them accessible to the wider community. He is also a co-author of an upcoming "),pe=a(Ye,"A",{href:!0,rel:!0});var ia=i(pe);Qr=s(ia,"O\u2019Reilly book on Transformers"),ia.forEach(r),Xr=s(Ye,"."),Ye.forEach(r),_t=f(e),I=a(e,"P",{});var Ke=i(I);We=a(Ke,"STRONG",{});var na=i(We);Zr=s(na,"Leandro von Werra"),na.forEach(r),eo=s(Ke,"  is a machine learning engineer in the open-source team at Hugging Face and also a co-author of the an upcoming "),ge=a(Ke,"A",{href:!0,rel:!0});var ha=i(ge);to=s(ha,"O\u2019Reilly book on Transformers"),ha.forEach(r),ro=s(Ke,". He has several years of industry experience bringing NLP projects to production by working across the whole machine learning stack.."),Ke.forEach(r),Et=f(e),be=a(e,"P",{});var ca=i(be);oo=s(ca,"Are you ready to roll? In this chapter, you will learn:"),ca.forEach(r),kt=f(e),k=a(e,"UL",{});var Le=i(k);de=a(Le,"LI",{});var St=i(de);ao=s(St,"How to use the "),qe=a(St,"CODE",{});var ua=i(qe);lo=s(ua,"pipeline()"),ua.forEach(r),so=s(St," function to solve NLP tasks such as text generation and classification"),St.forEach(r),io=f(Le),je=a(Le,"LI",{});var fa=i(je);no=s(fa,"About the Transformer architecture"),fa.forEach(r),ho=f(Le),Ue=a(Le,"LI",{});var pa=i(Ue);co=s(pa,"How to distinguish between encoder, decoder, and encoder-decoder architectures and use cases"),pa.forEach(r),Le.forEach(r),this.h()},h(){n(p,"name","hf:doc:metadata"),n(p,"content",JSON.stringify(ka)),n(m,"id","introduction"),n(m,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),n(m,"href","#introduction"),n(g,"class","relative group"),n(S,"id","welcome-to-the-course"),n(S,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),n(S,"href","#welcome-to-the-course"),n(L,"class","relative group"),n(B,"href","https://huggingface.co/"),n(B,"rel","nofollow"),n(O,"href","https://github.com/huggingface/transformers"),n(O,"rel","nofollow"),n(W,"href","https://github.com/huggingface/datasets"),n(W,"rel","nofollow"),n(q,"href","https://github.com/huggingface/tokenizers"),n(q,"rel","nofollow"),n(j,"href","https://github.com/huggingface/accelerate"),n(j,"rel","nofollow"),n(U,"href","https://huggingface.co/models"),n(U,"rel","nofollow"),n(J,"href","https://huggingface.co/"),n(J,"rel","nofollow"),n(Y,"href","https://github.com/huggingface/transformers"),n(Y,"rel","nofollow"),n(K,"href","https://github.com/huggingface/datasets"),n(K,"rel","nofollow"),n(x,"id","what-to-expect"),n(x,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),n(x,"href","#what-to-expect"),n(P,"class","relative group"),n(Q,"class","block dark:hidden"),Ft(Q.src,vo="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/summary.svg")||n(Q,"src",vo),n(Q,"alt","Brief overview of the chapters of the course."),n(X,"class","hidden dark:block"),Ft(X.src,wo="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/summary-dark.svg")||n(X,"src",wo),n(X,"alt","Brief overview of the chapters of the course."),n(A,"class","flex justify-center"),n(ee,"href","https://huggingface.co/models"),n(ee,"rel","nofollow"),n(te,"href","https://www.fast.ai/"),n(te,"rel","nofollow"),n(re,"href","https://course.fast.ai/"),n(re,"rel","nofollow"),n(oe,"href","https://www.deeplearning.ai/"),n(oe,"rel","nofollow"),n(ae,"href","https://pytorch.org/"),n(ae,"rel","nofollow"),n(le,"href","https://www.tensorflow.org/"),n(le,"rel","nofollow"),n(se,"href","https://www.coursera.org/specializations/natural-language-processing?utm_source=deeplearning-ai&utm_medium=institutions&utm_campaign=20211011-nlp-2-hugging_face-page-nlp-refresh"),n(se,"rel","nofollow"),n(D,"id","who-are-we"),n(D,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),n(D,"href","#who-are-we"),n(H,"class","relative group"),n(ce,"href","https://learning.oreilly.com/library/view/deep-learning-for/9781492045519/"),n(ce,"rel","nofollow"),n(pe,"href","https://www.oreilly.com/library/view/natural-language-processing/9781098103231/"),n(pe,"rel","nofollow"),n(ge,"href","https://www.oreilly.com/library/view/natural-language-processing/9781098103231/"),n(ge,"rel","nofollow")},m(e,h){t(document.head,p),c(e,y,h),c(e,g,h),t(g,m),t(m,Pe),Xe(M,Pe,null),t(g,Dt),t(g,Ae),t(Ae,Ct),c(e,rt,h),c(e,L,h),t(L,S),t(S,Te),Xe(G,Te,null),t(L,Rt),t(L,He),t(He,Mt),c(e,ot,h),Xe(z,e,h),c(e,at,h),c(e,d,h),t(d,Gt),t(d,B),t(B,zt),t(d,Bt),t(d,O),t(O,Ot),t(d,Wt),t(d,W),t(W,qt),t(d,jt),t(d,q),t(q,Ut),t(d,Jt),t(d,j),t(j,Yt),t(d,Kt),t(d,U),t(U,Vt),t(d,Qt),c(e,lt,h),c(e,w,h),t(w,Xt),t(w,J),t(J,Zt),t(w,er),t(w,Y),t(Y,tr),t(w,rr),t(w,K),t(K,or),t(w,ar),c(e,st,h),c(e,P,h),t(P,x),t(x,$e),Xe(V,$e,null),t(P,lr),t(P,Ne),t(Ne,sr),c(e,it,h),c(e,ve,h),t(ve,ir),c(e,nt,h),c(e,A,h),t(A,Q),t(A,nr),t(A,X),c(e,ht,h),c(e,b,h),t(b,Z),t(Z,hr),t(Z,ee),t(ee,cr),t(Z,ur),t(b,fr),t(b,Ie),t(Ie,pr),t(b,gr),t(b,Se),t(Se,dr),c(e,ct,h),c(e,we,h),t(we,mr),c(e,ut,h),c(e,_,h),t(_,xe),t(xe,vr),t(_,wr),t(_,E),t(E,yr),t(E,te),t(te,br),t(E,_r),t(E,re),t(re,Er),t(E,kr),t(E,oe),t(oe,Lr),t(_,Pr),t(_,T),t(T,Ar),t(T,ae),t(ae,Tr),t(T,Hr),t(T,le),t(le,$r),t(T,Nr),c(e,ft,h),c(e,F,h),t(F,Ir),t(F,se),t(se,Sr),t(F,xr),c(e,pt,h),c(e,H,h),t(H,D),t(D,Fe),Xe(ie,Fe,null),t(H,Fr),t(H,De),t(De,Dr),c(e,gt,h),c(e,ye,h),t(ye,Cr),c(e,dt,h),c(e,ne,h),t(ne,Ce),t(Ce,Rr),t(ne,Mr),c(e,mt,h),c(e,he,h),t(he,Re),t(Re,Gr),t(he,zr),c(e,vt,h),c(e,$,h),t($,Me),t(Me,Br),t($,Or),t($,Ge),t(Ge,ce),t(ce,Wr),t($,qr),c(e,wt,h),c(e,ue,h),t(ue,ze),t(ze,jr),t(ue,Ur),c(e,yt,h),c(e,fe,h),t(fe,Be),t(Be,Jr),t(fe,Yr),c(e,bt,h),c(e,N,h),t(N,Oe),t(Oe,Kr),t(N,Vr),t(N,pe),t(pe,Qr),t(N,Xr),c(e,_t,h),c(e,I,h),t(I,We),t(We,Zr),t(I,eo),t(I,ge),t(ge,to),t(I,ro),c(e,Et,h),c(e,be,h),t(be,oo),c(e,kt,h),c(e,k,h),t(k,de),t(de,ao),t(de,qe),t(qe,lo),t(de,so),t(k,io),t(k,je),t(je,no),t(k,ho),t(k,Ue),t(Ue,co),Lt=!0},p:mo,i(e){Lt||(Ze(M.$$.fragment,e),Ze(G.$$.fragment,e),Ze(z.$$.fragment,e),Ze(V.$$.fragment,e),Ze(ie.$$.fragment,e),Lt=!0)},o(e){et(M.$$.fragment,e),et(G.$$.fragment,e),et(z.$$.fragment,e),et(V.$$.fragment,e),et(ie.$$.fragment,e),Lt=!1},d(e){r(p),e&&r(y),e&&r(g),tt(M),e&&r(rt),e&&r(L),tt(G),e&&r(ot),tt(z,e),e&&r(at),e&&r(d),e&&r(lt),e&&r(w),e&&r(st),e&&r(P),tt(V),e&&r(it),e&&r(ve),e&&r(nt),e&&r(A),e&&r(ht),e&&r(b),e&&r(ct),e&&r(we),e&&r(ut),e&&r(_),e&&r(ft),e&&r(F),e&&r(pt),e&&r(H),tt(ie),e&&r(gt),e&&r(ye),e&&r(dt),e&&r(ne),e&&r(mt),e&&r(he),e&&r(vt),e&&r($),e&&r(wt),e&&r(ue),e&&r(yt),e&&r(fe),e&&r(bt),e&&r(N),e&&r(_t),e&&r(I),e&&r(Et),e&&r(be),e&&r(kt),e&&r(k)}}}const ka={local:"introduction",sections:[{local:"welcome-to-the-course",title:"Welcome to the \u{1F917} Course!"},{local:"what-to-expect",title:"What to expect?"},{local:"who-are-we",title:"Who are we?"}],title:"Introduction"};function La(R){return wa(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Ta extends ga{constructor(p){super();da(this,p,La,Ea,ma,{})}}export{Ta as default,ka as metadata};
