import{S as Zz,i as eM,s as tM,e as r,k as f,w as _,t as i,M as sM,c as o,d as t,m as p,a as l,x as v,h as u,b as d,N as Qz,F as e,g as m,y,q as E,o as w,B as b,v as aM,L as P}from"../chunks/vendor-7c454903.js";import{T as W}from"../chunks/Tip-735285fc.js";import{I as F}from"../chunks/IconCopyLink-5457534b.js";import{I as U,M as R,C as N}from"../chunks/InferenceApi-041dc1b2.js";function nM($){let n,c,s,h,q,k,A;return{c(){n=r("p"),c=r("strong"),s=i("Recommended model"),h=i(`:
`),q=r("a"),k=i("facebook/bart-large-mnli"),A=i("."),this.h()},l(j){n=o(j,"P",{});var T=l(n);c=o(T,"STRONG",{});var O=l(c);s=u(O,"Recommended model"),O.forEach(t),h=u(T,`:
`),q=o(T,"A",{href:!0,rel:!0});var D=l(q);k=u(D,"facebook/bart-large-mnli"),D.forEach(t),A=u(T,"."),T.forEach(t),this.h()},h(){d(q,"href","https://huggingface.co/facebook/bart-large-mnli"),d(q,"rel","nofollow")},m(j,T){m(j,n,T),e(n,c),e(c,s),e(n,h),e(n,q),e(q,k),e(n,A)},d(j){j&&t(n)}}}function rM($){let n,c;return n=new N({props:{code:`import json
import requests
headers = {"Authorization": f"Bearer {API_TOKEN}"}
API_URL = "https://api-inference.huggingface.co/models/facebook/bart-large-mnli"
def query(payload):
    data = json.dumps(payload)
    response = requests.request("POST", API_URL, headers=headers, data=data)
    return json.loads(response.content.decode("utf-8"))
data = query(
    {
        "inputs": "Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!",
        "parameters": {"candidate_labels": ["refund", "legal", "faq"]},
    }
)`,highlighted:`<span class="hljs-keyword">import</span> json
<span class="hljs-keyword">import</span> requests
headers = {<span class="hljs-string">&quot;Authorization&quot;</span>: <span class="hljs-string">f&quot;Bearer <span class="hljs-subst">{API_TOKEN}</span>&quot;</span>}
API_URL = <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/facebook/bart-large-mnli&quot;</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">payload</span>):
    data = json.dumps(payload)
    response = requests.request(<span class="hljs-string">&quot;POST&quot;</span>, API_URL, headers=headers, data=data)
    <span class="hljs-keyword">return</span> json.loads(response.content.decode(<span class="hljs-string">&quot;utf-8&quot;</span>))
data = query(
    {
        <span class="hljs-string">&quot;inputs&quot;</span>: <span class="hljs-string">&quot;Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!&quot;</span>,
        <span class="hljs-string">&quot;parameters&quot;</span>: {<span class="hljs-string">&quot;candidate_labels&quot;</span>: [<span class="hljs-string">&quot;refund&quot;</span>, <span class="hljs-string">&quot;legal&quot;</span>, <span class="hljs-string">&quot;faq&quot;</span>]},
    }
)`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),c=!0},p:P,i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function oM($){let n,c;return n=new R({props:{$$slots:{default:[rM]},$$scope:{ctx:$}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),c=!0},p(s,h){const q={};h&2&&(q.$$scope={dirty:h,ctx:s}),n.$set(q)},i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function lM($){let n,c;return n=new N({props:{code:`import fetch from "node-fetch";
async function query(data) {
    const response = await fetch(
        "https://api-inference.huggingface.co/models/facebook/bart-large-mnli",
        {
            headers: { Authorization: \`Bearer \${API_TOKEN}\` },
            method: "POST",
            body: JSON.stringify(data),
        }
    );
    const result = await response.json();
    return result;
}
query({inputs: "Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!", parameters: {candidate_labels: ["refund", "legal", "faq"]}}).then((response) => {
    console.log(JSON.stringify(response));
});
// {"sequence":"Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!","labels":["refund","faq","legal"],"scores":[0.8778, 0.1052, 0.017]}`,highlighted:`<span class="hljs-keyword">import</span> fetch <span class="hljs-keyword">from</span> <span class="hljs-string">&quot;node-fetch&quot;</span>;
<span class="hljs-keyword">async</span> <span class="hljs-keyword">function</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">data</span>) {
    <span class="hljs-keyword">const</span> response = <span class="hljs-keyword">await</span> <span class="hljs-title function_">fetch</span>(
        <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/facebook/bart-large-mnli&quot;</span>,
        {
            <span class="hljs-attr">headers</span>: { <span class="hljs-title class_">Authorization</span>: <span class="hljs-string">\`Bearer <span class="hljs-subst">\${API_TOKEN}</span>\`</span> },
            <span class="hljs-attr">method</span>: <span class="hljs-string">&quot;POST&quot;</span>,
            <span class="hljs-attr">body</span>: <span class="hljs-title class_">JSON</span>.<span class="hljs-title function_">stringify</span>(data),
        }
    );
    <span class="hljs-keyword">const</span> result = <span class="hljs-keyword">await</span> response.<span class="hljs-title function_">json</span>();
    <span class="hljs-keyword">return</span> result;
}
<span class="hljs-title function_">query</span>({<span class="hljs-attr">inputs</span>: <span class="hljs-string">&quot;Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!&quot;</span>, <span class="hljs-attr">parameters</span>: {<span class="hljs-attr">candidate_labels</span>: [<span class="hljs-string">&quot;refund&quot;</span>, <span class="hljs-string">&quot;legal&quot;</span>, <span class="hljs-string">&quot;faq&quot;</span>]}}).<span class="hljs-title function_">then</span>(<span class="hljs-function">(<span class="hljs-params">response</span>) =&gt;</span> {
    <span class="hljs-variable language_">console</span>.<span class="hljs-title function_">log</span>(<span class="hljs-title class_">JSON</span>.<span class="hljs-title function_">stringify</span>(response));
});
<span class="hljs-comment">// {&quot;sequence&quot;:&quot;Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!&quot;,&quot;labels&quot;:[&quot;refund&quot;,&quot;faq&quot;,&quot;legal&quot;],&quot;scores&quot;:[0.8778, 0.1052, 0.017]}</span>`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),c=!0},p:P,i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function iM($){let n,c;return n=new R({props:{$$slots:{default:[lM]},$$scope:{ctx:$}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),c=!0},p(s,h){const q={};h&2&&(q.$$scope={dirty:h,ctx:s}),n.$set(q)},i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function uM($){let n,c;return n=new N({props:{code:`curl https://api-inference.huggingface.co/models/facebook/bart-large-mnli \\
        -X POST \\
        -d '{"inputs": "Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!", "parameters": {"candidate_labels": ["refund", "legal", "faq"]}}' \\
        -H "Authorization: Bearer \${HF_API_TOKEN}"
# {"sequence":"Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!","labels":["refund","faq","legal"],"scores":[0.8778, 0.1052, 0.017]}`,highlighted:`curl https://api-inference.huggingface.co/models/facebook/bart-large-mnli \\
        -X POST \\
        -d <span class="hljs-string">&#x27;{&quot;inputs&quot;: &quot;Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!&quot;, &quot;parameters&quot;: {&quot;candidate_labels&quot;: [&quot;refund&quot;, &quot;legal&quot;, &quot;faq&quot;]}}&#x27;</span> \\
        -H <span class="hljs-string">&quot;Authorization: Bearer <span class="hljs-variable">\${HF_API_TOKEN}</span>&quot;</span>
<span class="hljs-comment"># {&quot;sequence&quot;:&quot;Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!&quot;,&quot;labels&quot;:[&quot;refund&quot;,&quot;faq&quot;,&quot;legal&quot;],&quot;scores&quot;:[0.8778, 0.1052, 0.017]}</span>`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),c=!0},p:P,i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function cM($){let n,c;return n=new R({props:{$$slots:{default:[uM]},$$scope:{ctx:$}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),c=!0},p(s,h){const q={};h&2&&(q.$$scope={dirty:h,ctx:s}),n.$set(q)},i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function fM($){let n,c;return n=new N({props:{code:`self.assertEqual(
    deep_round(data),
    {
        "sequence": "Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!",
        "labels": ["refund", "faq", "legal"],
        "scores": [
            # 88% refund
            0.8778,
            0.1052,
            0.017,
        ],
    },
)`,highlighted:`self.assertEqual(
    deep_round(data),
    {
        <span class="hljs-string">&quot;sequence&quot;</span>: <span class="hljs-string">&quot;Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!&quot;</span>,
        <span class="hljs-string">&quot;labels&quot;</span>: [<span class="hljs-string">&quot;refund&quot;</span>, <span class="hljs-string">&quot;faq&quot;</span>, <span class="hljs-string">&quot;legal&quot;</span>],
        <span class="hljs-string">&quot;scores&quot;</span>: [
            <span class="hljs-comment"># 88% refund</span>
            <span class="hljs-number">0.8778</span>,
            <span class="hljs-number">0.1052</span>,
            <span class="hljs-number">0.017</span>,
        ],
    },
)`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),c=!0},p:P,i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function pM($){let n,c;return n=new R({props:{$$slots:{default:[fM]},$$scope:{ctx:$}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),c=!0},p(s,h){const q={};h&2&&(q.$$scope={dirty:h,ctx:s}),n.$set(q)},i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function dM($){let n,c,s,h,q,k,A,j,T,O,D,ne,Re;return{c(){n=r("p"),c=r("strong"),s=i("Recommended model"),h=i(`:
`),q=r("a"),k=i("Helsinki-NLP/opus-mt-ru-en"),A=i(`.
Helsinki-NLP uploaded many models with many language pairs.
`),j=r("strong"),T=i("Recommended model"),O=i(": "),D=r("a"),ne=i("t5-base"),Re=i("."),this.h()},l(Q){n=o(Q,"P",{});var Y=l(n);c=o(Y,"STRONG",{});var rt=l(c);s=u(rt,"Recommended model"),rt.forEach(t),h=u(Y,`:
`),q=o(Y,"A",{href:!0,rel:!0});var pi=l(q);k=u(pi,"Helsinki-NLP/opus-mt-ru-en"),pi.forEach(t),A=u(Y,`.
Helsinki-NLP uploaded many models with many language pairs.
`),j=o(Y,"STRONG",{});var Ua=l(j);T=u(Ua,"Recommended model"),Ua.forEach(t),O=u(Y,": "),D=o(Y,"A",{href:!0,rel:!0});var Ne=l(D);ne=u(Ne,"t5-base"),Ne.forEach(t),Re=u(Y,"."),Y.forEach(t),this.h()},h(){d(q,"href","https://huggingface.co/Helsinki-NLP/opus-mt-ru-en"),d(q,"rel","nofollow"),d(D,"href","https://huggingface.co/t5-base"),d(D,"rel","nofollow")},m(Q,Y){m(Q,n,Y),e(n,c),e(c,s),e(n,h),e(n,q),e(q,k),e(n,A),e(n,j),e(j,T),e(n,O),e(n,D),e(D,ne),e(n,Re)},d(Q){Q&&t(n)}}}function hM($){let n,c;return n=new N({props:{code:`import json
import requests
headers = {"Authorization": f"Bearer {API_TOKEN}"}
API_URL = "https://api-inference.huggingface.co/models/Helsinki-NLP/opus-mt-ru-en"
def query(payload):
    data = json.dumps(payload)
    response = requests.request("POST", API_URL, headers=headers, data=data)
    return json.loads(response.content.decode("utf-8"))
data = query(
    {
        "inputs": "\u041C\u0435\u043D\u044F \u0437\u043E\u0432\u0443\u0442 \u0412\u043E\u043B\u044C\u0444\u0433\u0430\u043D\u0433 \u0438 \u044F \u0436\u0438\u0432\u0443 \u0432 \u0411\u0435\u0440\u043B\u0438\u043D\u0435",
    }
)
# Response
self.assertEqual(
    data,
    [
        {
            "translation_text": "My name is Wolfgang and I live in Berlin.",
        },
    ],
)`,highlighted:`<span class="hljs-keyword">import</span> json
<span class="hljs-keyword">import</span> requests
headers = {<span class="hljs-string">&quot;Authorization&quot;</span>: <span class="hljs-string">f&quot;Bearer <span class="hljs-subst">{API_TOKEN}</span>&quot;</span>}
API_URL = <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/Helsinki-NLP/opus-mt-ru-en&quot;</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">payload</span>):
    data = json.dumps(payload)
    response = requests.request(<span class="hljs-string">&quot;POST&quot;</span>, API_URL, headers=headers, data=data)
    <span class="hljs-keyword">return</span> json.loads(response.content.decode(<span class="hljs-string">&quot;utf-8&quot;</span>))
data = query(
    {
        <span class="hljs-string">&quot;inputs&quot;</span>: <span class="hljs-string">&quot;\u041C\u0435\u043D\u044F \u0437\u043E\u0432\u0443\u0442 \u0412\u043E\u043B\u044C\u0444\u0433\u0430\u043D\u0433 \u0438 \u044F \u0436\u0438\u0432\u0443 \u0432 \u0411\u0435\u0440\u043B\u0438\u043D\u0435&quot;</span>,
    }
)
<span class="hljs-comment"># Response</span>
self.assertEqual(
    data,
    [
        {
            <span class="hljs-string">&quot;translation_text&quot;</span>: <span class="hljs-string">&quot;My name is Wolfgang and I live in Berlin.&quot;</span>,
        },
    ],
)`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),c=!0},p:P,i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function gM($){let n,c;return n=new R({props:{$$slots:{default:[hM]},$$scope:{ctx:$}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),c=!0},p(s,h){const q={};h&2&&(q.$$scope={dirty:h,ctx:s}),n.$set(q)},i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function mM($){let n,c;return n=new N({props:{code:`import fetch from "node-fetch";
async function query(data) {
    const response = await fetch(
        "https://api-inference.huggingface.co/models/Helsinki-NLP/opus-mt-ru-en",
        {
            headers: { Authorization: \`Bearer \${API_TOKEN}\` },
            method: "POST",
            body: JSON.stringify(data),
        }
    );
    const result = await response.json();
    return result;
}
query({inputs: "\u041C\u0435\u043D\u044F \u0437\u043E\u0432\u0443\u0442 \u0412\u043E\u043B\u044C\u0444\u0433\u0430\u043D\u0433 \u0438 \u044F \u0436\u0438\u0432\u0443 \u0432 \u0411\u0435\u0440\u043B\u0438\u043D\u0435"}).then((response) => {
    console.log(JSON.stringify(response));
});
// [{"translation_text":"My name is Wolfgang and I live in Berlin."}]`,highlighted:`<span class="hljs-keyword">import</span> fetch <span class="hljs-keyword">from</span> <span class="hljs-string">&quot;node-fetch&quot;</span>;
<span class="hljs-keyword">async</span> <span class="hljs-keyword">function</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">data</span>) {
    <span class="hljs-keyword">const</span> response = <span class="hljs-keyword">await</span> <span class="hljs-title function_">fetch</span>(
        <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/Helsinki-NLP/opus-mt-ru-en&quot;</span>,
        {
            <span class="hljs-attr">headers</span>: { <span class="hljs-title class_">Authorization</span>: <span class="hljs-string">\`Bearer <span class="hljs-subst">\${API_TOKEN}</span>\`</span> },
            <span class="hljs-attr">method</span>: <span class="hljs-string">&quot;POST&quot;</span>,
            <span class="hljs-attr">body</span>: <span class="hljs-title class_">JSON</span>.<span class="hljs-title function_">stringify</span>(data),
        }
    );
    <span class="hljs-keyword">const</span> result = <span class="hljs-keyword">await</span> response.<span class="hljs-title function_">json</span>();
    <span class="hljs-keyword">return</span> result;
}
<span class="hljs-title function_">query</span>({<span class="hljs-attr">inputs</span>: <span class="hljs-string">&quot;\u041C\u0435\u043D\u044F \u0437\u043E\u0432\u0443\u0442 \u0412\u043E\u043B\u044C\u0444\u0433\u0430\u043D\u0433 \u0438 \u044F \u0436\u0438\u0432\u0443 \u0432 \u0411\u0435\u0440\u043B\u0438\u043D\u0435&quot;</span>}).<span class="hljs-title function_">then</span>(<span class="hljs-function">(<span class="hljs-params">response</span>) =&gt;</span> {
    <span class="hljs-variable language_">console</span>.<span class="hljs-title function_">log</span>(<span class="hljs-title class_">JSON</span>.<span class="hljs-title function_">stringify</span>(response));
});
<span class="hljs-comment">// [{&quot;translation_text&quot;:&quot;My name is Wolfgang and I live in Berlin.&quot;}]</span>`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),c=!0},p:P,i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function qM($){let n,c;return n=new R({props:{$$slots:{default:[mM]},$$scope:{ctx:$}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),c=!0},p(s,h){const q={};h&2&&(q.$$scope={dirty:h,ctx:s}),n.$set(q)},i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function $M($){let n,c;return n=new N({props:{code:`curl https://api-inference.huggingface.co/models/Helsinki-NLP/opus-mt-ru-en \\
        -X POST \\
        -d '{"inputs": "\u041C\u0435\u043D\u044F \u0437\u043E\u0432\u0443\u0442 \u0412\u043E\u043B\u044C\u0444\u0433\u0430\u043D\u0433 \u0438 \u044F \u0436\u0438\u0432\u0443 \u0432 \u0411\u0435\u0440\u043B\u0438\u043D\u0435"}' \\
        -H "Authorization: Bearer \${HF_API_TOKEN}"
# [{"translation_text":"My name is Wolfgang and I live in Berlin."}]`,highlighted:`curl https://api-inference.huggingface.co/models/Helsinki-NLP/opus-mt-ru-en \\
        -X POST \\
        -d <span class="hljs-string">&#x27;{&quot;inputs&quot;: &quot;\u041C\u0435\u043D\u044F \u0437\u043E\u0432\u0443\u0442 \u0412\u043E\u043B\u044C\u0444\u0433\u0430\u043D\u0433 \u0438 \u044F \u0436\u0438\u0432\u0443 \u0432 \u0411\u0435\u0440\u043B\u0438\u043D\u0435&quot;}&#x27;</span> \\
        -H <span class="hljs-string">&quot;Authorization: Bearer <span class="hljs-variable">\${HF_API_TOKEN}</span>&quot;</span>
<span class="hljs-comment"># [{&quot;translation_text&quot;:&quot;My name is Wolfgang and I live in Berlin.&quot;}]</span>`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),c=!0},p:P,i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function _M($){let n,c;return n=new R({props:{$$slots:{default:[$M]},$$scope:{ctx:$}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),c=!0},p(s,h){const q={};h&2&&(q.$$scope={dirty:h,ctx:s}),n.$set(q)},i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function vM($){let n,c,s,h,q,k,A;return{c(){n=r("p"),c=r("strong"),s=i("Recommended model"),h=i(`:
`),q=r("a"),k=i("facebook/bart-large-cnn"),A=i("."),this.h()},l(j){n=o(j,"P",{});var T=l(n);c=o(T,"STRONG",{});var O=l(c);s=u(O,"Recommended model"),O.forEach(t),h=u(T,`:
`),q=o(T,"A",{href:!0,rel:!0});var D=l(q);k=u(D,"facebook/bart-large-cnn"),D.forEach(t),A=u(T,"."),T.forEach(t),this.h()},h(){d(q,"href","https://huggingface.co/facebook/bart-large-cnn"),d(q,"rel","nofollow")},m(j,T){m(j,n,T),e(n,c),e(c,s),e(n,h),e(n,q),e(q,k),e(n,A)},d(j){j&&t(n)}}}function yM($){let n,c;return n=new N({props:{code:`import json
import requests
headers = {"Authorization": f"Bearer {API_TOKEN}"}
API_URL = "https://api-inference.huggingface.co/models/facebook/bart-large-cnn"
def query(payload):
    data = json.dumps(payload)
    response = requests.request("POST", API_URL, headers=headers, data=data)
    return json.loads(response.content.decode("utf-8"))
data = query(
    {
        "inputs": "The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world, a title it held for 41 years until the Chrysler Building in New York City was finished in 1930. It was the first structure to reach a height of 300 metres. Due to the addition of a broadcasting aerial at the top of the tower in 1957, it is now taller than the Chrysler Building by 5.2 metres (17 ft). Excluding transmitters, the Eiffel Tower is the second tallest free-standing structure in France after the Millau Viaduct.",
        "parameters": {"do_sample": False},
    }
)
# Response
self.assertEqual(
    data,
    [
        {
            "summary_text": "The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world.",
        },
    ],
)`,highlighted:`<span class="hljs-keyword">import</span> json
<span class="hljs-keyword">import</span> requests
headers = {<span class="hljs-string">&quot;Authorization&quot;</span>: <span class="hljs-string">f&quot;Bearer <span class="hljs-subst">{API_TOKEN}</span>&quot;</span>}
API_URL = <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/facebook/bart-large-cnn&quot;</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">payload</span>):
    data = json.dumps(payload)
    response = requests.request(<span class="hljs-string">&quot;POST&quot;</span>, API_URL, headers=headers, data=data)
    <span class="hljs-keyword">return</span> json.loads(response.content.decode(<span class="hljs-string">&quot;utf-8&quot;</span>))
data = query(
    {
        <span class="hljs-string">&quot;inputs&quot;</span>: <span class="hljs-string">&quot;The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world, a title it held for 41 years until the Chrysler Building in New York City was finished in 1930. It was the first structure to reach a height of 300 metres. Due to the addition of a broadcasting aerial at the top of the tower in 1957, it is now taller than the Chrysler Building by 5.2 metres (17 ft). Excluding transmitters, the Eiffel Tower is the second tallest free-standing structure in France after the Millau Viaduct.&quot;</span>,
        <span class="hljs-string">&quot;parameters&quot;</span>: {<span class="hljs-string">&quot;do_sample&quot;</span>: <span class="hljs-literal">False</span>},
    }
)
<span class="hljs-comment"># Response</span>
self.assertEqual(
    data,
    [
        {
            <span class="hljs-string">&quot;summary_text&quot;</span>: <span class="hljs-string">&quot;The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world.&quot;</span>,
        },
    ],
)`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),c=!0},p:P,i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function EM($){let n,c;return n=new R({props:{$$slots:{default:[yM]},$$scope:{ctx:$}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),c=!0},p(s,h){const q={};h&2&&(q.$$scope={dirty:h,ctx:s}),n.$set(q)},i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function wM($){let n,c;return n=new N({props:{code:`import fetch from "node-fetch";
async function query(data) {
    const response = await fetch(
        "https://api-inference.huggingface.co/models/facebook/bart-large-cnn",
        {
            headers: { Authorization: \`Bearer \${API_TOKEN}\` },
            method: "POST",
            body: JSON.stringify(data),
        }
    );
    const result = await response.json();
    return result;
}
query({inputs: "The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world, a title it held for 41 years until the Chrysler Building in New York City was finished in 1930. It was the first structure to reach a height of 300 metres. Due to the addition of a broadcasting aerial at the top of the tower in 1957, it is now taller than the Chrysler Building by 5.2 metres (17 ft). Excluding transmitters, the Eiffel Tower is the second tallest free-standing structure in France after the Millau Viaduct."}).then((response) => {
    console.log(JSON.stringify(response));
});
// [{"summary_text":"The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side. It was the first structure to reach a height of 300 metres."}]`,highlighted:`import fetch from <span class="hljs-comment">&quot;node-fetch&quot;</span>;
async function query(data) {
    const response = await fetch(
        <span class="hljs-comment">&quot;https://api-inference.huggingface.co/models/facebook/bart-large-cnn&quot;</span>,
        {
            headers: { <span class="hljs-type">Authorization</span>: \`<span class="hljs-type">Bearer</span> <span class="hljs-string">\${</span><span class="hljs-type">API_TOKEN</span>}\` },
            method: <span class="hljs-comment">&quot;POST&quot;</span>,
            body: <span class="hljs-type">JSON</span>.stringify(data),
        }
    );
    const result = await response.json();
    return result;
}
query({inputs: <span class="hljs-comment">&quot;The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world, a title it held for 41 years until the Chrysler Building in New York City was finished in 1930. It was the first structure to reach a height of 300 metres. Due to the addition of a broadcasting aerial at the top of the tower in 1957, it is now taller than the Chrysler Building by 5.2 metres (17 ft). Excluding transmitters, the Eiffel Tower is the second tallest free-standing structure in France after the Millau Viaduct.&quot;</span>}).then((response) =&gt; {
    console.log(<span class="hljs-type">JSON</span>.stringify(response));
});
// [{<span class="hljs-comment">&quot;summary_text&quot;</span>:<span class="hljs-comment">&quot;The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side. It was the first structure to reach a height of 300 metres.&quot;</span>}]`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),c=!0},p:P,i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function bM($){let n,c;return n=new R({props:{$$slots:{default:[wM]},$$scope:{ctx:$}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),c=!0},p(s,h){const q={};h&2&&(q.$$scope={dirty:h,ctx:s}),n.$set(q)},i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function TM($){let n,c;return n=new N({props:{code:`curl https://api-inference.huggingface.co/models/facebook/bart-large-cnn \\
        -X POST \\
        -d '{"inputs": "The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world, a title it held for 41 years until the Chrysler Building in New York City was finished in 1930. It was the first structure to reach a height of 300 metres. Due to the addition of a broadcasting aerial at the top of the tower in 1957, it is now taller than the Chrysler Building by 5.2 metres (17 ft). Excluding transmitters, the Eiffel Tower is the second tallest free-standing structure in France after the Millau Viaduct.", "parameters": {"do_sample": false}}' \\
        -H "Authorization: Bearer \${HF_API_TOKEN}"
# [{"summary_text":"The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world."}]`,highlighted:`curl https://api-inference.huggingface.co/models/facebook/bart-large-cnn \\
        -X POST \\
        -d <span class="hljs-string">&#x27;{&quot;inputs&quot;: &quot;The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world, a title it held for 41 years until the Chrysler Building in New York City was finished in 1930. It was the first structure to reach a height of 300 metres. Due to the addition of a broadcasting aerial at the top of the tower in 1957, it is now taller than the Chrysler Building by 5.2 metres (17 ft). Excluding transmitters, the Eiffel Tower is the second tallest free-standing structure in France after the Millau Viaduct.&quot;, &quot;parameters&quot;: {&quot;do_sample&quot;: false}}&#x27;</span> \\
        -H <span class="hljs-string">&quot;Authorization: Bearer <span class="hljs-variable">\${HF_API_TOKEN}</span>&quot;</span>
<span class="hljs-comment"># [{&quot;summary_text&quot;:&quot;The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world.&quot;}]</span>`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),c=!0},p:P,i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function jM($){let n,c;return n=new R({props:{$$slots:{default:[TM]},$$scope:{ctx:$}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),c=!0},p(s,h){const q={};h&2&&(q.$$scope={dirty:h,ctx:s}),n.$set(q)},i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function kM($){let n,c,s,h,q,k,A;return{c(){n=r("p"),c=r("strong"),s=i("Recommended model"),h=i(`:
`),q=r("a"),k=i("microsoft/DialoGPT-large"),A=i("."),this.h()},l(j){n=o(j,"P",{});var T=l(n);c=o(T,"STRONG",{});var O=l(c);s=u(O,"Recommended model"),O.forEach(t),h=u(T,`:
`),q=o(T,"A",{href:!0,rel:!0});var D=l(q);k=u(D,"microsoft/DialoGPT-large"),D.forEach(t),A=u(T,"."),T.forEach(t),this.h()},h(){d(q,"href","https://huggingface.co/microsoft/DialoGPT-large"),d(q,"rel","nofollow")},m(j,T){m(j,n,T),e(n,c),e(c,s),e(n,h),e(n,q),e(q,k),e(n,A)},d(j){j&&t(n)}}}function AM($){let n,c;return n=new N({props:{code:`import json
import requests
headers = {"Authorization": f"Bearer {API_TOKEN}"}
API_URL = "https://api-inference.huggingface.co/models/microsoft/DialoGPT-large"
def query(payload):
    data = json.dumps(payload)
    response = requests.request("POST", API_URL, headers=headers, data=data)
    return json.loads(response.content.decode("utf-8"))
data = query(
    {
        "inputs": {
            "past_user_inputs": ["Which movie is the best ?"],
            "generated_responses": ["It's Die Hard for sure."],
            "text": "Can you explain why ?",
        },
    }
)
# Response
self.assertEqual(
    data,
    {
        "generated_text": "It's the best movie ever.",
        "conversation": {
            "past_user_inputs": [
                "Which movie is the best ?",
                "Can you explain why ?",
            ],
            "generated_responses": [
                "It's Die Hard for sure.",
                "It's the best movie ever.",
            ],
        },
        "warnings": ["Setting \`pad_token_id\` to \`eos_token_id\`:50256 for open-end generation."],
    },
)`,highlighted:`<span class="hljs-keyword">import</span> json
<span class="hljs-keyword">import</span> requests
headers = {<span class="hljs-string">&quot;Authorization&quot;</span>: <span class="hljs-string">f&quot;Bearer <span class="hljs-subst">{API_TOKEN}</span>&quot;</span>}
API_URL = <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/microsoft/DialoGPT-large&quot;</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">payload</span>):
    data = json.dumps(payload)
    response = requests.request(<span class="hljs-string">&quot;POST&quot;</span>, API_URL, headers=headers, data=data)
    <span class="hljs-keyword">return</span> json.loads(response.content.decode(<span class="hljs-string">&quot;utf-8&quot;</span>))
data = query(
    {
        <span class="hljs-string">&quot;inputs&quot;</span>: {
            <span class="hljs-string">&quot;past_user_inputs&quot;</span>: [<span class="hljs-string">&quot;Which movie is the best ?&quot;</span>],
            <span class="hljs-string">&quot;generated_responses&quot;</span>: [<span class="hljs-string">&quot;It&#x27;s Die Hard for sure.&quot;</span>],
            <span class="hljs-string">&quot;text&quot;</span>: <span class="hljs-string">&quot;Can you explain why ?&quot;</span>,
        },
    }
)
<span class="hljs-comment"># Response</span>
self.assertEqual(
    data,
    {
        <span class="hljs-string">&quot;generated_text&quot;</span>: <span class="hljs-string">&quot;It&#x27;s the best movie ever.&quot;</span>,
        <span class="hljs-string">&quot;conversation&quot;</span>: {
            <span class="hljs-string">&quot;past_user_inputs&quot;</span>: [
                <span class="hljs-string">&quot;Which movie is the best ?&quot;</span>,
                <span class="hljs-string">&quot;Can you explain why ?&quot;</span>,
            ],
            <span class="hljs-string">&quot;generated_responses&quot;</span>: [
                <span class="hljs-string">&quot;It&#x27;s Die Hard for sure.&quot;</span>,
                <span class="hljs-string">&quot;It&#x27;s the best movie ever.&quot;</span>,
            ],
        },
        <span class="hljs-string">&quot;warnings&quot;</span>: [<span class="hljs-string">&quot;Setting \`pad_token_id\` to \`eos_token_id\`:50256 for open-end generation.&quot;</span>],
    },
)`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),c=!0},p:P,i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function DM($){let n,c;return n=new R({props:{$$slots:{default:[AM]},$$scope:{ctx:$}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),c=!0},p(s,h){const q={};h&2&&(q.$$scope={dirty:h,ctx:s}),n.$set(q)},i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function OM($){let n,c;return n=new N({props:{code:`import fetch from "node-fetch";
async function query(data) {
    const response = await fetch(
        "https://api-inference.huggingface.co/models/microsoft/DialoGPT-large",
        {
            headers: { Authorization: \`Bearer \${API_TOKEN}\` },
            method: "POST",
            body: JSON.stringify(data),
        }
    );
    const result = await response.json();
    return result;
}
query({inputs: {past_user_inputs: ["Which movie is the best ?"], generated_responses: ["It is Die Hard for sure."], text:"Can you explain why ?"}}).then((response) => {
    console.log(JSON.stringify(response));
});
// {"generated_text":"It's the best movie ever.","conversation":{"past_user_inputs":["Which movie is the best ?","Can you explain why ?"],"generated_responses":["It is Die Hard for sure.","It's the best movie ever."]},"warnings":["Setting \`pad_token_id\` to \`eos_token_id\`:50256 for open-end generation."]}`,highlighted:`<span class="hljs-keyword">import</span> <span class="hljs-keyword">fetch</span> <span class="hljs-keyword">from</span> &quot;node-fetch&quot;;
async <span class="hljs-keyword">function</span> query(data) {
    const response = await <span class="hljs-keyword">fetch</span>(
        &quot;https://api-inference.huggingface.co/models/microsoft/DialoGPT-large&quot;,
        {
            headers: { <span class="hljs-keyword">Authorization</span>: \`Bearer \${API_TOKEN}\` },
            <span class="hljs-keyword">method</span>: &quot;POST&quot;,
            body: <span class="hljs-type">JSON</span>.stringify(data),
        }
    );
    const result = await response.json();
    <span class="hljs-keyword">return</span> result;
}
query({inputs: {past_user_inputs: [&quot;Which movie is the best ?&quot;], generated_responses: [&quot;It is Die Hard for sure.&quot;], <span class="hljs-type">text</span>:&quot;Can you explain why ?&quot;}}).<span class="hljs-keyword">then</span>((response) =&gt; {
    console.log(<span class="hljs-type">JSON</span>.stringify(response));
});
// {&quot;generated_text&quot;:&quot;It&#x27;s the best movie ever.&quot;,&quot;conversation&quot;:{&quot;past_user_inputs&quot;:[&quot;Which movie is the best ?&quot;,&quot;Can you explain why ?&quot;],&quot;generated_responses&quot;:[&quot;It is Die Hard for sure.&quot;,&quot;It&#x27;s the best movie ever.&quot;]},&quot;warnings&quot;:[&quot;Setting \`pad_token_id\` to \`eos_token_id\`:50256 for open-end generation.&quot;]}`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),c=!0},p:P,i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function PM($){let n,c;return n=new R({props:{$$slots:{default:[OM]},$$scope:{ctx:$}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),c=!0},p(s,h){const q={};h&2&&(q.$$scope={dirty:h,ctx:s}),n.$set(q)},i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function RM($){let n,c;return n=new N({props:{code:`curl https://api-inference.huggingface.co/models/microsoft/DialoGPT-large \\
        -X POST \\
        -d '{"inputs": {"past_user_inputs": ["Which movie is the best ?"], "generated_responses": ["It is Die Hard for sure."], "text":"Can you explain why ?"}}' \\
        -H "Authorization: Bearer \${HF_API_TOKEN}"
# {"generated_text":"It's the best movie ever.","conversation":{"past_user_inputs":["Which movie is the best ?","Can you explain why ?"],"generated_responses":["It is Die Hard for sure.","It's the best movie ever."]},"warnings":["Setting \`pad_token_id\` to \`eos_token_id\`:50256 for open-end generation."]}`,highlighted:'curl https://api-inference.huggingface.co/models/microsoft/DialoGPT-large \\\n        -X POST \\\n        -d <span class="hljs-string">&#x27;{&quot;inputs&quot;: {&quot;past_user_inputs&quot;: [&quot;Which movie is the best ?&quot;], &quot;generated_responses&quot;: [&quot;It is Die Hard for sure.&quot;], &quot;text&quot;:&quot;Can you explain why ?&quot;}}&#x27;</span> \\\n        -H <span class="hljs-string">&quot;Authorization: Bearer <span class="hljs-variable">${HF_API_TOKEN}</span>&quot;</span>\n<span class="hljs-comment"># {&quot;generated_text&quot;:&quot;It&#x27;s the best movie ever.&quot;,&quot;conversation&quot;:{&quot;past_user_inputs&quot;:[&quot;Which movie is the best ?&quot;,&quot;Can you explain why ?&quot;],&quot;generated_responses&quot;:[&quot;It is Die Hard for sure.&quot;,&quot;It&#x27;s the best movie ever.&quot;]},&quot;warnings&quot;:[&quot;Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.&quot;]}</span>'}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),c=!0},p:P,i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function NM($){let n,c;return n=new R({props:{$$slots:{default:[RM]},$$scope:{ctx:$}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),c=!0},p(s,h){const q={};h&2&&(q.$$scope={dirty:h,ctx:s}),n.$set(q)},i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function SM($){let n,c,s,h,q,k,A;return{c(){n=r("p"),c=r("strong"),s=i("Recommended model"),h=i(`:
`),q=r("a"),k=i("google/tapas-base-finetuned-wtq"),A=i("."),this.h()},l(j){n=o(j,"P",{});var T=l(n);c=o(T,"STRONG",{});var O=l(c);s=u(O,"Recommended model"),O.forEach(t),h=u(T,`:
`),q=o(T,"A",{href:!0,rel:!0});var D=l(q);k=u(D,"google/tapas-base-finetuned-wtq"),D.forEach(t),A=u(T,"."),T.forEach(t),this.h()},h(){d(q,"href","https://huggingface.co/google/tapas-base-finetuned-wtq"),d(q,"rel","nofollow")},m(j,T){m(j,n,T),e(n,c),e(c,s),e(n,h),e(n,q),e(q,k),e(n,A)},d(j){j&&t(n)}}}function xM($){let n,c;return n=new N({props:{code:`import json
import requests
headers = {"Authorization": f"Bearer {API_TOKEN}"}
API_URL = "https://api-inference.huggingface.co/models/google/tapas-base-finetuned-wtq"
def query(payload):
    data = json.dumps(payload)
    response = requests.request("POST", API_URL, headers=headers, data=data)
    return json.loads(response.content.decode("utf-8"))
data = query(
    {
        "inputs": {
            "query": "How many stars does the transformers repository have?",
            "table": {
                "Repository": ["Transformers", "Datasets", "Tokenizers"],
                "Stars": ["36542", "4512", "3934"],
                "Contributors": ["651", "77", "34"],
                "Programming language": [
                    "Python",
                    "Python",
                    "Rust, Python and NodeJS",
                ],
            },
        }
    }
)`,highlighted:`<span class="hljs-keyword">import</span> json
<span class="hljs-keyword">import</span> requests
headers = {<span class="hljs-string">&quot;Authorization&quot;</span>: <span class="hljs-string">f&quot;Bearer <span class="hljs-subst">{API_TOKEN}</span>&quot;</span>}
API_URL = <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/google/tapas-base-finetuned-wtq&quot;</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">payload</span>):
    data = json.dumps(payload)
    response = requests.request(<span class="hljs-string">&quot;POST&quot;</span>, API_URL, headers=headers, data=data)
    <span class="hljs-keyword">return</span> json.loads(response.content.decode(<span class="hljs-string">&quot;utf-8&quot;</span>))
data = query(
    {
        <span class="hljs-string">&quot;inputs&quot;</span>: {
            <span class="hljs-string">&quot;query&quot;</span>: <span class="hljs-string">&quot;How many stars does the transformers repository have?&quot;</span>,
            <span class="hljs-string">&quot;table&quot;</span>: {
                <span class="hljs-string">&quot;Repository&quot;</span>: [<span class="hljs-string">&quot;Transformers&quot;</span>, <span class="hljs-string">&quot;Datasets&quot;</span>, <span class="hljs-string">&quot;Tokenizers&quot;</span>],
                <span class="hljs-string">&quot;Stars&quot;</span>: [<span class="hljs-string">&quot;36542&quot;</span>, <span class="hljs-string">&quot;4512&quot;</span>, <span class="hljs-string">&quot;3934&quot;</span>],
                <span class="hljs-string">&quot;Contributors&quot;</span>: [<span class="hljs-string">&quot;651&quot;</span>, <span class="hljs-string">&quot;77&quot;</span>, <span class="hljs-string">&quot;34&quot;</span>],
                <span class="hljs-string">&quot;Programming language&quot;</span>: [
                    <span class="hljs-string">&quot;Python&quot;</span>,
                    <span class="hljs-string">&quot;Python&quot;</span>,
                    <span class="hljs-string">&quot;Rust, Python and NodeJS&quot;</span>,
                ],
            },
        }
    }
)`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),c=!0},p:P,i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function IM($){let n,c;return n=new R({props:{$$slots:{default:[xM]},$$scope:{ctx:$}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),c=!0},p(s,h){const q={};h&2&&(q.$$scope={dirty:h,ctx:s}),n.$set(q)},i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function HM($){let n,c;return n=new N({props:{code:`import fetch from "node-fetch";
async function query(data) {
    const response = await fetch(
        "https://api-inference.huggingface.co/models/google/tapas-base-finetuned-wtq",
        {
            headers: { Authorization: \`Bearer \${API_TOKEN}\` },
            method: "POST",
            body: JSON.stringify(data),
        }
    );
    const result = await response.json();
    return result;
}
query({inputs:{query:"How many stars does the transformers repository have?",table:{Repository:["Transformers","Datasets","Tokenizers"],Stars:["36542","4512","3934"],Contributors:["651","77","34"],"Programming language":["Python","Python","Rust, Python and NodeJS"]}}}).then((response) => {
    console.log(JSON.stringify(response));
});
// {"answer":"AVERAGE > 36542","coordinates":[[0,1]],"cells":["36542"],"aggregator":"AVERAGE"}`,highlighted:`<span class="hljs-keyword">import</span> <span class="hljs-keyword">fetch</span> <span class="hljs-keyword">from</span> &quot;node-fetch&quot;;
async <span class="hljs-keyword">function</span> query(data) {
    const response = await <span class="hljs-keyword">fetch</span>(
        &quot;https://api-inference.huggingface.co/models/google/tapas-base-finetuned-wtq&quot;,
        {
            headers: { <span class="hljs-keyword">Authorization</span>: \`Bearer \${API_TOKEN}\` },
            <span class="hljs-keyword">method</span>: &quot;POST&quot;,
            body: <span class="hljs-type">JSON</span>.stringify(data),
        }
    );
    const result = await response.json();
    <span class="hljs-keyword">return</span> result;
}
query({inputs:{query:&quot;How many stars does the transformers repository have?&quot;,<span class="hljs-keyword">table</span>:{Repository:[&quot;Transformers&quot;,&quot;Datasets&quot;,&quot;Tokenizers&quot;],Stars:[&quot;36542&quot;,&quot;4512&quot;,&quot;3934&quot;],Contributors:[&quot;651&quot;,&quot;77&quot;,&quot;34&quot;],&quot;Programming language&quot;:[&quot;Python&quot;,&quot;Python&quot;,&quot;Rust, Python and NodeJS&quot;]}}}).<span class="hljs-keyword">then</span>((response) =&gt; {
    console.log(<span class="hljs-type">JSON</span>.stringify(response));
});
// {&quot;answer&quot;:&quot;AVERAGE &gt; 36542&quot;,&quot;coordinates&quot;:[[<span class="hljs-number">0</span>,<span class="hljs-number">1</span>]],&quot;cells&quot;:[&quot;36542&quot;],&quot;aggregator&quot;:&quot;AVERAGE&quot;}`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),c=!0},p:P,i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function BM($){let n,c;return n=new R({props:{$$slots:{default:[HM]},$$scope:{ctx:$}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),c=!0},p(s,h){const q={};h&2&&(q.$$scope={dirty:h,ctx:s}),n.$set(q)},i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function CM($){let n,c;return n=new N({props:{code:`curl https://api-inference.huggingface.co/models/google/tapas-base-finetuned-wtq \\
        -X POST \\
        -d '{"inputs":{"query":"How many stars does the transformers repository have?","table":{"Repository":["Transformers","Datasets","Tokenizers"],"Stars":["36542","4512","3934"],"Contributors":["651","77","34"],"Programming language":["Python","Python","Rust, Python and NodeJS"]}}}' \\
        -H "Authorization: Bearer \${HF_API_TOKEN}"
# {"answer":"AVERAGE > 36542","coordinates":[[0,1]],"cells":["36542"],"aggregator":"AVERAGE"}`,highlighted:`curl https://api-inference.huggingface.co/models/google/tapas-base-finetuned-wtq \\
        -X POST \\
        -d <span class="hljs-string">&#x27;{&quot;inputs&quot;:{&quot;query&quot;:&quot;How many stars does the transformers repository have?&quot;,&quot;table&quot;:{&quot;Repository&quot;:[&quot;Transformers&quot;,&quot;Datasets&quot;,&quot;Tokenizers&quot;],&quot;Stars&quot;:[&quot;36542&quot;,&quot;4512&quot;,&quot;3934&quot;],&quot;Contributors&quot;:[&quot;651&quot;,&quot;77&quot;,&quot;34&quot;],&quot;Programming language&quot;:[&quot;Python&quot;,&quot;Python&quot;,&quot;Rust, Python and NodeJS&quot;]}}}&#x27;</span> \\
        -H <span class="hljs-string">&quot;Authorization: Bearer <span class="hljs-variable">\${HF_API_TOKEN}</span>&quot;</span>
<span class="hljs-comment"># {&quot;answer&quot;:&quot;AVERAGE &gt; 36542&quot;,&quot;coordinates&quot;:[[0,1]],&quot;cells&quot;:[&quot;36542&quot;],&quot;aggregator&quot;:&quot;AVERAGE&quot;}</span>`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),c=!0},p:P,i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function GM($){let n,c;return n=new R({props:{$$slots:{default:[CM]},$$scope:{ctx:$}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),c=!0},p(s,h){const q={};h&2&&(q.$$scope={dirty:h,ctx:s}),n.$set(q)},i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function LM($){let n,c;return n=new N({props:{code:`self.assertEqual(
    data,
    {
        "answer": "AVERAGE > 36542",
        "coordinates": [[0, 1]],
        "cells": ["36542"],
        "aggregator": "AVERAGE",
    },
)`,highlighted:`self.assertEqual(
    data,
    {
        <span class="hljs-string">&quot;answer&quot;</span>: <span class="hljs-string">&quot;AVERAGE &gt; 36542&quot;</span>,
        <span class="hljs-string">&quot;coordinates&quot;</span>: [[<span class="hljs-number">0</span>, <span class="hljs-number">1</span>]],
        <span class="hljs-string">&quot;cells&quot;</span>: [<span class="hljs-string">&quot;36542&quot;</span>],
        <span class="hljs-string">&quot;aggregator&quot;</span>: <span class="hljs-string">&quot;AVERAGE&quot;</span>,
    },
)`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),c=!0},p:P,i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function UM($){let n,c;return n=new R({props:{$$slots:{default:[LM]},$$scope:{ctx:$}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),c=!0},p(s,h){const q={};h&2&&(q.$$scope={dirty:h,ctx:s}),n.$set(q)},i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function zM($){let n,c,s,h,q,k,A;return{c(){n=r("p"),c=r("strong"),s=i("Recommended model"),h=i(`:
`),q=r("a"),k=i("deepset/roberta-base-squad2"),A=i("."),this.h()},l(j){n=o(j,"P",{});var T=l(n);c=o(T,"STRONG",{});var O=l(c);s=u(O,"Recommended model"),O.forEach(t),h=u(T,`:
`),q=o(T,"A",{href:!0,rel:!0});var D=l(q);k=u(D,"deepset/roberta-base-squad2"),D.forEach(t),A=u(T,"."),T.forEach(t),this.h()},h(){d(q,"href","https://huggingface.co/deepset/roberta-base-squad2"),d(q,"rel","nofollow")},m(j,T){m(j,n,T),e(n,c),e(c,s),e(n,h),e(n,q),e(q,k),e(n,A)},d(j){j&&t(n)}}}function MM($){let n,c;return n=new N({props:{code:`import json
import requests
headers = {"Authorization": f"Bearer {API_TOKEN}"}
API_URL = "https://api-inference.huggingface.co/models/deepset/roberta-base-squad2"
def query(payload):
    data = json.dumps(payload)
    response = requests.request("POST", API_URL, headers=headers, data=data)
    return json.loads(response.content.decode("utf-8"))
data = query(
    {
        "inputs": {
            "question": "What's my name?",
            "context": "My name is Clara and I live in Berkeley.",
        }
    }
)`,highlighted:`<span class="hljs-keyword">import</span> json
<span class="hljs-keyword">import</span> requests
headers = {<span class="hljs-string">&quot;Authorization&quot;</span>: <span class="hljs-string">f&quot;Bearer <span class="hljs-subst">{API_TOKEN}</span>&quot;</span>}
API_URL = <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/deepset/roberta-base-squad2&quot;</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">payload</span>):
    data = json.dumps(payload)
    response = requests.request(<span class="hljs-string">&quot;POST&quot;</span>, API_URL, headers=headers, data=data)
    <span class="hljs-keyword">return</span> json.loads(response.content.decode(<span class="hljs-string">&quot;utf-8&quot;</span>))
data = query(
    {
        <span class="hljs-string">&quot;inputs&quot;</span>: {
            <span class="hljs-string">&quot;question&quot;</span>: <span class="hljs-string">&quot;What&#x27;s my name?&quot;</span>,
            <span class="hljs-string">&quot;context&quot;</span>: <span class="hljs-string">&quot;My name is Clara and I live in Berkeley.&quot;</span>,
        }
    }
)`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),c=!0},p:P,i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function FM($){let n,c;return n=new R({props:{$$slots:{default:[MM]},$$scope:{ctx:$}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),c=!0},p(s,h){const q={};h&2&&(q.$$scope={dirty:h,ctx:s}),n.$set(q)},i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function KM($){let n,c;return n=new N({props:{code:`import fetch from "node-fetch";
async function query(data) {
    const response = await fetch(
        "https://api-inference.huggingface.co/models/deepset/roberta-base-squad2",
        {
            headers: { Authorization: \`Bearer \${API_TOKEN}\` },
            method: "POST",
            body: JSON.stringify(data),
        }
    );
    const result = await response.json();
    return result;
}
query({inputs:{question:"What is my name?",context:"My name is Clara and I live in Berkeley."}}).then((response) => {
    console.log(JSON.stringify(response));
});
// {"score":0.933128833770752,"start":11,"end":16,"answer":"Clara"}`,highlighted:`<span class="hljs-keyword">import</span> fetch <span class="hljs-keyword">from</span> <span class="hljs-string">&quot;node-fetch&quot;</span>;
<span class="hljs-keyword">async</span> <span class="hljs-keyword">function</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">data</span>) {
    <span class="hljs-keyword">const</span> response = <span class="hljs-keyword">await</span> <span class="hljs-title function_">fetch</span>(
        <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/deepset/roberta-base-squad2&quot;</span>,
        {
            <span class="hljs-attr">headers</span>: { <span class="hljs-title class_">Authorization</span>: <span class="hljs-string">\`Bearer <span class="hljs-subst">\${API_TOKEN}</span>\`</span> },
            <span class="hljs-attr">method</span>: <span class="hljs-string">&quot;POST&quot;</span>,
            <span class="hljs-attr">body</span>: <span class="hljs-title class_">JSON</span>.<span class="hljs-title function_">stringify</span>(data),
        }
    );
    <span class="hljs-keyword">const</span> result = <span class="hljs-keyword">await</span> response.<span class="hljs-title function_">json</span>();
    <span class="hljs-keyword">return</span> result;
}
<span class="hljs-title function_">query</span>({<span class="hljs-attr">inputs</span>:{<span class="hljs-attr">question</span>:<span class="hljs-string">&quot;What is my name?&quot;</span>,<span class="hljs-attr">context</span>:<span class="hljs-string">&quot;My name is Clara and I live in Berkeley.&quot;</span>}}).<span class="hljs-title function_">then</span>(<span class="hljs-function">(<span class="hljs-params">response</span>) =&gt;</span> {
    <span class="hljs-variable language_">console</span>.<span class="hljs-title function_">log</span>(<span class="hljs-title class_">JSON</span>.<span class="hljs-title function_">stringify</span>(response));
});
<span class="hljs-comment">// {&quot;score&quot;:0.933128833770752,&quot;start&quot;:11,&quot;end&quot;:16,&quot;answer&quot;:&quot;Clara&quot;}</span>`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),c=!0},p:P,i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function JM($){let n,c;return n=new R({props:{$$slots:{default:[KM]},$$scope:{ctx:$}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),c=!0},p(s,h){const q={};h&2&&(q.$$scope={dirty:h,ctx:s}),n.$set(q)},i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function WM($){let n,c;return n=new N({props:{code:`curl https://api-inference.huggingface.co/models/deepset/roberta-base-squad2 \\
        -X POST \\
        -d '{"inputs":{"question":"What is my name?","context":"My name is Clara and I live in Berkeley."}}' \\
        -H "Authorization: Bearer \${HF_API_TOKEN}"
# {"score":0.933128833770752,"start":11,"end":16,"answer":"Clara"}`,highlighted:`curl https://api-inference.huggingface.co/models/deepset/roberta-base-squad2 \\
        -X POST \\
        -d <span class="hljs-string">&#x27;{&quot;inputs&quot;:{&quot;question&quot;:&quot;What is my name?&quot;,&quot;context&quot;:&quot;My name is Clara and I live in Berkeley.&quot;}}&#x27;</span> \\
        -H <span class="hljs-string">&quot;Authorization: Bearer <span class="hljs-variable">\${HF_API_TOKEN}</span>&quot;</span>
<span class="hljs-comment"># {&quot;score&quot;:0.933128833770752,&quot;start&quot;:11,&quot;end&quot;:16,&quot;answer&quot;:&quot;Clara&quot;}</span>`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),c=!0},p:P,i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function YM($){let n,c;return n=new R({props:{$$slots:{default:[WM]},$$scope:{ctx:$}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),c=!0},p(s,h){const q={};h&2&&(q.$$scope={dirty:h,ctx:s}),n.$set(q)},i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function VM($){let n,c;return n=new N({props:{code:`self.assertEqual(
    deep_round(data),
    {"score": 0.9327, "start": 11, "end": 16, "answer": "Clara"},
)`,highlighted:`self.assertEqual(
    deep_round(data),
    {<span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.9327</span>, <span class="hljs-string">&quot;start&quot;</span>: <span class="hljs-number">11</span>, <span class="hljs-string">&quot;end&quot;</span>: <span class="hljs-number">16</span>, <span class="hljs-string">&quot;answer&quot;</span>: <span class="hljs-string">&quot;Clara&quot;</span>},
)`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),c=!0},p:P,i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function XM($){let n,c;return n=new R({props:{$$slots:{default:[VM]},$$scope:{ctx:$}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),c=!0},p(s,h){const q={};h&2&&(q.$$scope={dirty:h,ctx:s}),n.$set(q)},i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function QM($){let n,c,s,h,q,k;return{c(){n=r("p"),c=r("strong"),s=i("Recommended model"),h=i(`:
`),q=r("a"),k=i("distilbert-base-uncased-finetuned-sst-2-english"),this.h()},l(A){n=o(A,"P",{});var j=l(n);c=o(j,"STRONG",{});var T=l(c);s=u(T,"Recommended model"),T.forEach(t),h=u(j,`:
`),q=o(j,"A",{href:!0,rel:!0});var O=l(q);k=u(O,"distilbert-base-uncased-finetuned-sst-2-english"),O.forEach(t),j.forEach(t),this.h()},h(){d(q,"href","https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english"),d(q,"rel","nofollow")},m(A,j){m(A,n,j),e(n,c),e(c,s),e(n,h),e(n,q),e(q,k)},d(A){A&&t(n)}}}function ZM($){let n,c;return n=new N({props:{code:`import json
import requests
headers = {"Authorization": f"Bearer {API_TOKEN}"}
API_URL = "https://api-inference.huggingface.co/models/distilbert-base-uncased-finetuned-sst-2-english"
def query(payload):
    data = json.dumps(payload)
    response = requests.request("POST", API_URL, headers=headers, data=data)
    return json.loads(response.content.decode("utf-8"))
data = query({"inputs": "I like you. I love you"})`,highlighted:`<span class="hljs-keyword">import</span> json
<span class="hljs-keyword">import</span> requests
headers = {<span class="hljs-string">&quot;Authorization&quot;</span>: <span class="hljs-string">f&quot;Bearer <span class="hljs-subst">{API_TOKEN}</span>&quot;</span>}
API_URL = <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/distilbert-base-uncased-finetuned-sst-2-english&quot;</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">payload</span>):
    data = json.dumps(payload)
    response = requests.request(<span class="hljs-string">&quot;POST&quot;</span>, API_URL, headers=headers, data=data)
    <span class="hljs-keyword">return</span> json.loads(response.content.decode(<span class="hljs-string">&quot;utf-8&quot;</span>))
data = query({<span class="hljs-string">&quot;inputs&quot;</span>: <span class="hljs-string">&quot;I like you. I love you&quot;</span>})`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),c=!0},p:P,i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function eF($){let n,c;return n=new R({props:{$$slots:{default:[ZM]},$$scope:{ctx:$}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),c=!0},p(s,h){const q={};h&2&&(q.$$scope={dirty:h,ctx:s}),n.$set(q)},i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function tF($){let n,c;return n=new N({props:{code:`import fetch from "node-fetch";
async function query(data) {
    const response = await fetch(
        "https://api-inference.huggingface.co/models/distilbert-base-uncased-finetuned-sst-2-english",
        {
            headers: { Authorization: \`Bearer \${API_TOKEN}\` },
            method: "POST",
            body: JSON.stringify(data),
        }
    );
    const result = await response.json();
    return result;
}
query({inputs:"I like you. I love you"}).then((response) => {
    console.log(JSON.stringify(response));
});
// [[{"label":"NEGATIVE","score":0.0001261125144083053},{"label":"POSITIVE","score":0.9998738765716553}]]`,highlighted:`<span class="hljs-keyword">import</span> fetch <span class="hljs-keyword">from</span> <span class="hljs-string">&quot;node-fetch&quot;</span>;
<span class="hljs-keyword">async</span> <span class="hljs-keyword">function</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">data</span>) {
    <span class="hljs-keyword">const</span> response = <span class="hljs-keyword">await</span> <span class="hljs-title function_">fetch</span>(
        <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/distilbert-base-uncased-finetuned-sst-2-english&quot;</span>,
        {
            <span class="hljs-attr">headers</span>: { <span class="hljs-title class_">Authorization</span>: <span class="hljs-string">\`Bearer <span class="hljs-subst">\${API_TOKEN}</span>\`</span> },
            <span class="hljs-attr">method</span>: <span class="hljs-string">&quot;POST&quot;</span>,
            <span class="hljs-attr">body</span>: <span class="hljs-title class_">JSON</span>.<span class="hljs-title function_">stringify</span>(data),
        }
    );
    <span class="hljs-keyword">const</span> result = <span class="hljs-keyword">await</span> response.<span class="hljs-title function_">json</span>();
    <span class="hljs-keyword">return</span> result;
}
<span class="hljs-title function_">query</span>({<span class="hljs-attr">inputs</span>:<span class="hljs-string">&quot;I like you. I love you&quot;</span>}).<span class="hljs-title function_">then</span>(<span class="hljs-function">(<span class="hljs-params">response</span>) =&gt;</span> {
    <span class="hljs-variable language_">console</span>.<span class="hljs-title function_">log</span>(<span class="hljs-title class_">JSON</span>.<span class="hljs-title function_">stringify</span>(response));
});
<span class="hljs-comment">// [[{&quot;label&quot;:&quot;NEGATIVE&quot;,&quot;score&quot;:0.0001261125144083053},{&quot;label&quot;:&quot;POSITIVE&quot;,&quot;score&quot;:0.9998738765716553}]]</span>`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),c=!0},p:P,i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function sF($){let n,c;return n=new R({props:{$$slots:{default:[tF]},$$scope:{ctx:$}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),c=!0},p(s,h){const q={};h&2&&(q.$$scope={dirty:h,ctx:s}),n.$set(q)},i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function aF($){let n,c;return n=new N({props:{code:`curl https://api-inference.huggingface.co/models/distilbert-base-uncased-finetuned-sst-2-english \\
        -X POST \\
        -d '{"inputs":"I like you. I love you"}' \\
        -H "Authorization: Bearer \${HF_API_TOKEN}"
# [[{"label":"NEGATIVE","score":0.0001261125144083053},{"label":"POSITIVE","score":0.9998738765716553}]]`,highlighted:`curl https://api-inference.huggingface.co/models/distilbert-base-uncased-finetuned-sst-2-english \\
        -X POST \\
        -d <span class="hljs-string">&#x27;{&quot;inputs&quot;:&quot;I like you. I love you&quot;}&#x27;</span> \\
        -H <span class="hljs-string">&quot;Authorization: Bearer <span class="hljs-variable">\${HF_API_TOKEN}</span>&quot;</span>
<span class="hljs-comment"># [[{&quot;label&quot;:&quot;NEGATIVE&quot;,&quot;score&quot;:0.0001261125144083053},{&quot;label&quot;:&quot;POSITIVE&quot;,&quot;score&quot;:0.9998738765716553}]]</span>`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),c=!0},p:P,i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function nF($){let n,c;return n=new R({props:{$$slots:{default:[aF]},$$scope:{ctx:$}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),c=!0},p(s,h){const q={};h&2&&(q.$$scope={dirty:h,ctx:s}),n.$set(q)},i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function rF($){let n,c;return n=new N({props:{code:`self.assertEqual(
    deep_round(data),
    [
        [
            {"label": "NEGATIVE", "score": 0.0001},
            {"label": "POSITIVE", "score": 0.9999},
        ]
    ],
)`,highlighted:`self.assertEqual(
    deep_round(data),
    [
        [
            {<span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;NEGATIVE&quot;</span>, <span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.0001</span>},
            {<span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;POSITIVE&quot;</span>, <span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.9999</span>},
        ]
    ],
)`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),c=!0},p:P,i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function oF($){let n,c;return n=new R({props:{$$slots:{default:[rF]},$$scope:{ctx:$}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),c=!0},p(s,h){const q={};h&2&&(q.$$scope={dirty:h,ctx:s}),n.$set(q)},i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function lF($){let n,c,s,h,q,k;return{c(){n=r("p"),c=r("strong"),s=i("Recommended model"),h=i(`:
`),q=r("a"),k=i("dbmdz/bert-large-cased-finetuned-conll03-english"),this.h()},l(A){n=o(A,"P",{});var j=l(n);c=o(j,"STRONG",{});var T=l(c);s=u(T,"Recommended model"),T.forEach(t),h=u(j,`:
`),q=o(j,"A",{href:!0,rel:!0});var O=l(q);k=u(O,"dbmdz/bert-large-cased-finetuned-conll03-english"),O.forEach(t),j.forEach(t),this.h()},h(){d(q,"href","https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english"),d(q,"rel","nofollow")},m(A,j){m(A,n,j),e(n,c),e(c,s),e(n,h),e(n,q),e(q,k)},d(A){A&&t(n)}}}function iF($){let n,c;return n=new N({props:{code:`import json
import requests
headers = {"Authorization": f"Bearer {API_TOKEN}"}
API_URL = "https://api-inference.huggingface.co/models/dbmdz/bert-large-cased-finetuned-conll03-english"
def query(payload):
    data = json.dumps(payload)
    response = requests.request("POST", API_URL, headers=headers, data=data)
    return json.loads(response.content.decode("utf-8"))
data = query({"inputs": "My name is Sarah Jessica Parker but you can call me Jessica"})`,highlighted:`<span class="hljs-keyword">import</span> json
<span class="hljs-keyword">import</span> requests
headers = {<span class="hljs-string">&quot;Authorization&quot;</span>: <span class="hljs-string">f&quot;Bearer <span class="hljs-subst">{API_TOKEN}</span>&quot;</span>}
API_URL = <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/dbmdz/bert-large-cased-finetuned-conll03-english&quot;</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">payload</span>):
    data = json.dumps(payload)
    response = requests.request(<span class="hljs-string">&quot;POST&quot;</span>, API_URL, headers=headers, data=data)
    <span class="hljs-keyword">return</span> json.loads(response.content.decode(<span class="hljs-string">&quot;utf-8&quot;</span>))
data = query({<span class="hljs-string">&quot;inputs&quot;</span>: <span class="hljs-string">&quot;My name is Sarah Jessica Parker but you can call me Jessica&quot;</span>})`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),c=!0},p:P,i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function uF($){let n,c;return n=new R({props:{$$slots:{default:[iF]},$$scope:{ctx:$}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),c=!0},p(s,h){const q={};h&2&&(q.$$scope={dirty:h,ctx:s}),n.$set(q)},i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function cF($){let n,c;return n=new N({props:{code:`import fetch from "node-fetch";
async function query(data) {
    const response = await fetch(
        "https://api-inference.huggingface.co/models/dbmdz/bert-large-cased-finetuned-conll03-english",
        {
            headers: { Authorization: \`Bearer \${API_TOKEN}\` },
            method: "POST",
            body: JSON.stringify(data),
        }
    );
    const result = await response.json();
    return result;
}
query({inputs:"My name is Sarah Jessica Parker but you can call me Jessica"}).then((response) => {
    console.log(JSON.stringify(response));
});
// [{"entity_group":"PER","score":0.9991337060928345,"word":"Sarah Jessica Parker","start":11,"end":31},{"entity_group":"PER","score":0.9979912042617798,"word":"Jessica","start":52,"end":59}]`,highlighted:`<span class="hljs-keyword">import</span> <span class="hljs-keyword">fetch</span> <span class="hljs-keyword">from</span> &quot;node-fetch&quot;;
async <span class="hljs-keyword">function</span> query(data) {
    const response = await <span class="hljs-keyword">fetch</span>(
        &quot;https://api-inference.huggingface.co/models/dbmdz/bert-large-cased-finetuned-conll03-english&quot;,
        {
            headers: { <span class="hljs-keyword">Authorization</span>: \`Bearer \${API_TOKEN}\` },
            <span class="hljs-keyword">method</span>: &quot;POST&quot;,
            body: <span class="hljs-type">JSON</span>.stringify(data),
        }
    );
    const result = await response.json();
    <span class="hljs-keyword">return</span> result;
}
query({inputs:&quot;My name is Sarah Jessica Parker but you can call me Jessica&quot;}).<span class="hljs-keyword">then</span>((response) =&gt; {
    console.log(<span class="hljs-type">JSON</span>.stringify(response));
});
// [{&quot;entity_group&quot;:&quot;PER&quot;,&quot;score&quot;:<span class="hljs-number">0.9991337060928345</span>,&quot;word&quot;:&quot;Sarah Jessica Parker&quot;,&quot;start&quot;:<span class="hljs-number">11</span>,&quot;end&quot;:<span class="hljs-number">31</span>},{&quot;entity_group&quot;:&quot;PER&quot;,&quot;score&quot;:<span class="hljs-number">0.9979912042617798</span>,&quot;word&quot;:&quot;Jessica&quot;,&quot;start&quot;:<span class="hljs-number">52</span>,&quot;end&quot;:<span class="hljs-number">59</span>}]`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),c=!0},p:P,i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function fF($){let n,c;return n=new R({props:{$$slots:{default:[cF]},$$scope:{ctx:$}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),c=!0},p(s,h){const q={};h&2&&(q.$$scope={dirty:h,ctx:s}),n.$set(q)},i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function pF($){let n,c;return n=new N({props:{code:`curl https://api-inference.huggingface.co/models/dbmdz/bert-large-cased-finetuned-conll03-english \\
        -X POST \\
        -d '{"inputs":"My name is Sarah Jessica Parker but you can call me Jessica"}' \\
        -H "Authorization: Bearer \${HF_API_TOKEN}"
# [{"entity_group":"PER","score":0.9991337060928345,"word":"Sarah Jessica Parker","start":11,"end":31},{"entity_group":"PER","score":0.9979912042617798,"word":"Jessica","start":52,"end":59}]`,highlighted:`curl https://api-inference.huggingface.co/models/dbmdz/bert-large-cased-finetuned-conll03-english \\
        -X POST \\
        -d <span class="hljs-string">&#x27;{&quot;inputs&quot;:&quot;My name is Sarah Jessica Parker but you can call me Jessica&quot;}&#x27;</span> \\
        -H <span class="hljs-string">&quot;Authorization: Bearer <span class="hljs-variable">\${HF_API_TOKEN}</span>&quot;</span>
<span class="hljs-comment"># [{&quot;entity_group&quot;:&quot;PER&quot;,&quot;score&quot;:0.9991337060928345,&quot;word&quot;:&quot;Sarah Jessica Parker&quot;,&quot;start&quot;:11,&quot;end&quot;:31},{&quot;entity_group&quot;:&quot;PER&quot;,&quot;score&quot;:0.9979912042617798,&quot;word&quot;:&quot;Jessica&quot;,&quot;start&quot;:52,&quot;end&quot;:59}]</span>`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),c=!0},p:P,i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function dF($){let n,c;return n=new R({props:{$$slots:{default:[pF]},$$scope:{ctx:$}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),c=!0},p(s,h){const q={};h&2&&(q.$$scope={dirty:h,ctx:s}),n.$set(q)},i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function hF($){let n,c;return n=new N({props:{code:`self.assertEqual(
    deep_round(data),
    [
        {
            "entity_group": "PER",
            "score": 0.9991,
            "word": "Sarah Jessica Parker",
            "start": 11,
            "end": 31,
        },
        {
            "entity_group": "PER",
            "score": 0.998,
            "word": "Jessica",
            "start": 52,
            "end": 59,
        },
    ],
)`,highlighted:`self.assertEqual(
    deep_round(data),
    [
        {
            <span class="hljs-string">&quot;entity_group&quot;</span>: <span class="hljs-string">&quot;PER&quot;</span>,
            <span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.9991</span>,
            <span class="hljs-string">&quot;word&quot;</span>: <span class="hljs-string">&quot;Sarah Jessica Parker&quot;</span>,
            <span class="hljs-string">&quot;start&quot;</span>: <span class="hljs-number">11</span>,
            <span class="hljs-string">&quot;end&quot;</span>: <span class="hljs-number">31</span>,
        },
        {
            <span class="hljs-string">&quot;entity_group&quot;</span>: <span class="hljs-string">&quot;PER&quot;</span>,
            <span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.998</span>,
            <span class="hljs-string">&quot;word&quot;</span>: <span class="hljs-string">&quot;Jessica&quot;</span>,
            <span class="hljs-string">&quot;start&quot;</span>: <span class="hljs-number">52</span>,
            <span class="hljs-string">&quot;end&quot;</span>: <span class="hljs-number">59</span>,
        },
    ],
)`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),c=!0},p:P,i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function gF($){let n,c;return n=new R({props:{$$slots:{default:[hF]},$$scope:{ctx:$}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),c=!0},p(s,h){const q={};h&2&&(q.$$scope={dirty:h,ctx:s}),n.$set(q)},i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function mF($){let n,c,s,h,q,k,A;return{c(){n=r("p"),c=r("strong"),s=i("Recommended model"),h=i(": "),q=r("a"),k=i("gpt2"),A=i(" (it\u2019s a simple model, but fun to play with)."),this.h()},l(j){n=o(j,"P",{});var T=l(n);c=o(T,"STRONG",{});var O=l(c);s=u(O,"Recommended model"),O.forEach(t),h=u(T,": "),q=o(T,"A",{href:!0,rel:!0});var D=l(q);k=u(D,"gpt2"),D.forEach(t),A=u(T," (it\u2019s a simple model, but fun to play with)."),T.forEach(t),this.h()},h(){d(q,"href","https://huggingface.co/gpt2"),d(q,"rel","nofollow")},m(j,T){m(j,n,T),e(n,c),e(c,s),e(n,h),e(n,q),e(q,k),e(n,A)},d(j){j&&t(n)}}}function qF($){let n,c;return n=new N({props:{code:`import json
import requests
headers = {"Authorization": f"Bearer {API_TOKEN}"}
API_URL = "https://api-inference.huggingface.co/models/gpt2"
def query(payload):
    data = json.dumps(payload)
    response = requests.request("POST", API_URL, headers=headers, data=data)
    return json.loads(response.content.decode("utf-8"))
data = query({"inputs": "The answer to the universe is"})`,highlighted:`<span class="hljs-keyword">import</span> json
<span class="hljs-keyword">import</span> requests
headers = {<span class="hljs-string">&quot;Authorization&quot;</span>: <span class="hljs-string">f&quot;Bearer <span class="hljs-subst">{API_TOKEN}</span>&quot;</span>}
API_URL = <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/gpt2&quot;</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">payload</span>):
    data = json.dumps(payload)
    response = requests.request(<span class="hljs-string">&quot;POST&quot;</span>, API_URL, headers=headers, data=data)
    <span class="hljs-keyword">return</span> json.loads(response.content.decode(<span class="hljs-string">&quot;utf-8&quot;</span>))
data = query({<span class="hljs-string">&quot;inputs&quot;</span>: <span class="hljs-string">&quot;The answer to the universe is&quot;</span>})`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),c=!0},p:P,i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function $F($){let n,c;return n=new R({props:{$$slots:{default:[qF]},$$scope:{ctx:$}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),c=!0},p(s,h){const q={};h&2&&(q.$$scope={dirty:h,ctx:s}),n.$set(q)},i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function _F($){let n,c;return n=new N({props:{code:`import fetch from "node-fetch";
async function query(data) {
    const response = await fetch(
        "https://api-inference.huggingface.co/models/gpt2",
        {
            headers: { Authorization: \`Bearer \${API_TOKEN}\` },
            method: "POST",
            body: JSON.stringify(data),
        }
    );
    const result = await response.json();
    return result;
}
query({inputs:"The answer to the universe is"}).then((response) => {
    console.log(JSON.stringify(response));
});
// [{"generated_text":"The answer to the universe is in a different shape (or shapeless) than"}]`,highlighted:`<span class="hljs-keyword">import</span> fetch <span class="hljs-keyword">from</span> <span class="hljs-string">&quot;node-fetch&quot;</span>;
<span class="hljs-keyword">async</span> <span class="hljs-keyword">function</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">data</span>) {
    <span class="hljs-keyword">const</span> response = <span class="hljs-keyword">await</span> <span class="hljs-title function_">fetch</span>(
        <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/gpt2&quot;</span>,
        {
            <span class="hljs-attr">headers</span>: { <span class="hljs-title class_">Authorization</span>: <span class="hljs-string">\`Bearer <span class="hljs-subst">\${API_TOKEN}</span>\`</span> },
            <span class="hljs-attr">method</span>: <span class="hljs-string">&quot;POST&quot;</span>,
            <span class="hljs-attr">body</span>: <span class="hljs-title class_">JSON</span>.<span class="hljs-title function_">stringify</span>(data),
        }
    );
    <span class="hljs-keyword">const</span> result = <span class="hljs-keyword">await</span> response.<span class="hljs-title function_">json</span>();
    <span class="hljs-keyword">return</span> result;
}
<span class="hljs-title function_">query</span>({<span class="hljs-attr">inputs</span>:<span class="hljs-string">&quot;The answer to the universe is&quot;</span>}).<span class="hljs-title function_">then</span>(<span class="hljs-function">(<span class="hljs-params">response</span>) =&gt;</span> {
    <span class="hljs-variable language_">console</span>.<span class="hljs-title function_">log</span>(<span class="hljs-title class_">JSON</span>.<span class="hljs-title function_">stringify</span>(response));
});
<span class="hljs-comment">// [{&quot;generated_text&quot;:&quot;The answer to the universe is in a different shape (or shapeless) than&quot;}]</span>`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),c=!0},p:P,i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function vF($){let n,c;return n=new R({props:{$$slots:{default:[_F]},$$scope:{ctx:$}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),c=!0},p(s,h){const q={};h&2&&(q.$$scope={dirty:h,ctx:s}),n.$set(q)},i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function yF($){let n,c;return n=new N({props:{code:`curl https://api-inference.huggingface.co/models/gpt2 \\
        -X POST \\
        -d '{"inputs":"The answer to the universe is"}' \\
        -H "Authorization: Bearer \${HF_API_TOKEN}"
# [{"generated_text":"The answer to the universe is in a different shape (or shapeless) than"}]`,highlighted:`curl https://api-inference.huggingface.co/models/gpt2 \\
        -X POST \\
        -d <span class="hljs-string">&#x27;{&quot;inputs&quot;:&quot;The answer to the universe is&quot;}&#x27;</span> \\
        -H <span class="hljs-string">&quot;Authorization: Bearer <span class="hljs-variable">\${HF_API_TOKEN}</span>&quot;</span>
<span class="hljs-comment"># [{&quot;generated_text&quot;:&quot;The answer to the universe is in a different shape (or shapeless) than&quot;}]</span>`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),c=!0},p:P,i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function EF($){let n,c;return n=new R({props:{$$slots:{default:[yF]},$$scope:{ctx:$}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),c=!0},p(s,h){const q={};h&2&&(q.$$scope={dirty:h,ctx:s}),n.$set(q)},i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function wF($){let n,c;return n=new N({props:{code:`data == [
    {
        "generated_text": 'The answer to the universe is that we are the creation of the entire universe," says Fitch.\\n\\nAs of the 1960s, six times as many Americans still make fewer than six bucks ($17) per year on their way to retirement.'
    }
]`,highlighted:`data == [
    {
        <span class="hljs-string">&quot;generated_text&quot;</span>: <span class="hljs-string">&#x27;The answer to the universe is that we are the creation of the entire universe,&quot; says Fitch.\\n\\nAs of the 1960s, six times as many Americans still make fewer than six bucks ($17) per year on their way to retirement.&#x27;</span>
    }
]`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),c=!0},p:P,i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function bF($){let n,c;return n=new R({props:{$$slots:{default:[wF]},$$scope:{ctx:$}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),c=!0},p(s,h){const q={};h&2&&(q.$$scope={dirty:h,ctx:s}),n.$set(q)},i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function TF($){let n,c,s,h,q,k,A;return{c(){n=r("p"),c=r("strong"),s=i("Recommended model"),h=i(`:
`),q=r("a"),k=i("bert-base-uncased"),A=i(" (it\u2019s a simple model, but fun to play with)."),this.h()},l(j){n=o(j,"P",{});var T=l(n);c=o(T,"STRONG",{});var O=l(c);s=u(O,"Recommended model"),O.forEach(t),h=u(T,`:
`),q=o(T,"A",{href:!0,rel:!0});var D=l(q);k=u(D,"bert-base-uncased"),D.forEach(t),A=u(T," (it\u2019s a simple model, but fun to play with)."),T.forEach(t),this.h()},h(){d(q,"href","https://huggingface.co/bert-base-uncased"),d(q,"rel","nofollow")},m(j,T){m(j,n,T),e(n,c),e(c,s),e(n,h),e(n,q),e(q,k),e(n,A)},d(j){j&&t(n)}}}function jF($){let n,c;return n=new N({props:{code:`import json
import requests
headers = {"Authorization": f"Bearer {API_TOKEN}"}
API_URL = "https://api-inference.huggingface.co/models/bert-base-uncased"
def query(payload):
    data = json.dumps(payload)
    response = requests.request("POST", API_URL, headers=headers, data=data)
    return json.loads(response.content.decode("utf-8"))
data = query({"inputs": "The answer to the universe is [MASK]."})`,highlighted:`<span class="hljs-keyword">import</span> json
<span class="hljs-keyword">import</span> requests
headers = {<span class="hljs-string">&quot;Authorization&quot;</span>: <span class="hljs-string">f&quot;Bearer <span class="hljs-subst">{API_TOKEN}</span>&quot;</span>}
API_URL = <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/bert-base-uncased&quot;</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">payload</span>):
    data = json.dumps(payload)
    response = requests.request(<span class="hljs-string">&quot;POST&quot;</span>, API_URL, headers=headers, data=data)
    <span class="hljs-keyword">return</span> json.loads(response.content.decode(<span class="hljs-string">&quot;utf-8&quot;</span>))
data = query({<span class="hljs-string">&quot;inputs&quot;</span>: <span class="hljs-string">&quot;The answer to the universe is [MASK].&quot;</span>})`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),c=!0},p:P,i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function kF($){let n,c;return n=new R({props:{$$slots:{default:[jF]},$$scope:{ctx:$}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),c=!0},p(s,h){const q={};h&2&&(q.$$scope={dirty:h,ctx:s}),n.$set(q)},i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function AF($){let n,c;return n=new N({props:{code:`import fetch from "node-fetch";
async function query(data) {
    const response = await fetch(
        "https://api-inference.huggingface.co/models/bert-base-uncased",
        {
            headers: { Authorization: \`Bearer \${API_TOKEN}\` },
            method: "POST",
            body: JSON.stringify(data),
        }
    );
    const result = await response.json();
    return result;
}
query({inputs:"The answer to the universe is [MASK]."}).then((response) => {
    console.log(JSON.stringify(response));
});
// [{"sequence":"the answer to the universe is no.","score":0.16963955760002136,"token":2053,"token_str":"no"},{"sequence":"the answer to the universe is nothing.","score":0.07344776391983032,"token":2498,"token_str":"nothing"},{"sequence":"the answer to the universe is yes.","score":0.05803241208195686,"token":2748,"token_str":"yes"},{"sequence":"the answer to the universe is unknown.","score":0.043957844376564026,"token":4242,"token_str":"unknown"},{"sequence":"the answer to the universe is simple.","score":0.04015745222568512,"token":3722,"token_str":"simple"}]`,highlighted:`<span class="hljs-keyword">import</span> <span class="hljs-keyword">fetch</span> <span class="hljs-keyword">from</span> &quot;node-fetch&quot;;
async <span class="hljs-keyword">function</span> query(data) {
    const response = await <span class="hljs-keyword">fetch</span>(
        &quot;https://api-inference.huggingface.co/models/bert-base-uncased&quot;,
        {
            headers: { <span class="hljs-keyword">Authorization</span>: \`Bearer \${API_TOKEN}\` },
            <span class="hljs-keyword">method</span>: &quot;POST&quot;,
            body: <span class="hljs-type">JSON</span>.stringify(data),
        }
    );
    const result = await response.json();
    <span class="hljs-keyword">return</span> result;
}
query({inputs:&quot;The answer to the universe is [MASK].&quot;}).<span class="hljs-keyword">then</span>((response) =&gt; {
    console.log(<span class="hljs-type">JSON</span>.stringify(response));
});
// [{&quot;sequence&quot;:&quot;the answer to the universe is no.&quot;,&quot;score&quot;:<span class="hljs-number">0.16963955760002136</span>,&quot;token&quot;:<span class="hljs-number">2053</span>,&quot;token_str&quot;:&quot;no&quot;},{&quot;sequence&quot;:&quot;the answer to the universe is nothing.&quot;,&quot;score&quot;:<span class="hljs-number">0.07344776391983032</span>,&quot;token&quot;:<span class="hljs-number">2498</span>,&quot;token_str&quot;:&quot;nothing&quot;},{&quot;sequence&quot;:&quot;the answer to the universe is yes.&quot;,&quot;score&quot;:<span class="hljs-number">0.05803241208195686</span>,&quot;token&quot;:<span class="hljs-number">2748</span>,&quot;token_str&quot;:&quot;yes&quot;},{&quot;sequence&quot;:&quot;the answer to the universe is unknown.&quot;,&quot;score&quot;:<span class="hljs-number">0.043957844376564026</span>,&quot;token&quot;:<span class="hljs-number">4242</span>,&quot;token_str&quot;:&quot;unknown&quot;},{&quot;sequence&quot;:&quot;the answer to the universe is simple.&quot;,&quot;score&quot;:<span class="hljs-number">0.04015745222568512</span>,&quot;token&quot;:<span class="hljs-number">3722</span>,&quot;token_str&quot;:&quot;simple&quot;}]`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),c=!0},p:P,i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function DF($){let n,c;return n=new R({props:{$$slots:{default:[AF]},$$scope:{ctx:$}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),c=!0},p(s,h){const q={};h&2&&(q.$$scope={dirty:h,ctx:s}),n.$set(q)},i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function OF($){let n,c;return n=new N({props:{code:`curl https://api-inference.huggingface.co/models/bert-base-uncased \\
        -X POST \\
        -d '{"inputs":"The answer to the universe is [MASK]."}' \\
        -H "Authorization: Bearer \${HF_API_TOKEN}"
# [{"sequence":"the answer to the universe is no.","score":0.16963955760002136,"token":2053,"token_str":"no"},{"sequence":"the answer to the universe is nothing.","score":0.07344776391983032,"token":2498,"token_str":"nothing"},{"sequence":"the answer to the universe is yes.","score":0.05803241208195686,"token":2748,"token_str":"yes"},{"sequence":"the answer to the universe is unknown.","score":0.043957844376564026,"token":4242,"token_str":"unknown"},{"sequence":"the answer to the universe is simple.","score":0.04015745222568512,"token":3722,"token_str":"simple"}]`,highlighted:`curl https://api-inference.huggingface.co/models/bert-base-uncased \\
        -X POST \\
        -d <span class="hljs-string">&#x27;{&quot;inputs&quot;:&quot;The answer to the universe is [MASK].&quot;}&#x27;</span> \\
        -H <span class="hljs-string">&quot;Authorization: Bearer <span class="hljs-variable">\${HF_API_TOKEN}</span>&quot;</span>
<span class="hljs-comment"># [{&quot;sequence&quot;:&quot;the answer to the universe is no.&quot;,&quot;score&quot;:0.16963955760002136,&quot;token&quot;:2053,&quot;token_str&quot;:&quot;no&quot;},{&quot;sequence&quot;:&quot;the answer to the universe is nothing.&quot;,&quot;score&quot;:0.07344776391983032,&quot;token&quot;:2498,&quot;token_str&quot;:&quot;nothing&quot;},{&quot;sequence&quot;:&quot;the answer to the universe is yes.&quot;,&quot;score&quot;:0.05803241208195686,&quot;token&quot;:2748,&quot;token_str&quot;:&quot;yes&quot;},{&quot;sequence&quot;:&quot;the answer to the universe is unknown.&quot;,&quot;score&quot;:0.043957844376564026,&quot;token&quot;:4242,&quot;token_str&quot;:&quot;unknown&quot;},{&quot;sequence&quot;:&quot;the answer to the universe is simple.&quot;,&quot;score&quot;:0.04015745222568512,&quot;token&quot;:3722,&quot;token_str&quot;:&quot;simple&quot;}]</span>`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),c=!0},p:P,i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function PF($){let n,c;return n=new R({props:{$$slots:{default:[OF]},$$scope:{ctx:$}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),c=!0},p(s,h){const q={};h&2&&(q.$$scope={dirty:h,ctx:s}),n.$set(q)},i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function RF($){let n,c;return n=new N({props:{code:`self.assertEqual(
    deep_round(data),
    [
        {
            "sequence": "the answer to the universe is no.",
            "score": 0.1696,
            "token": 2053,
            "token_str": "no",
        },
        {
            "sequence": "the answer to the universe is nothing.",
            "score": 0.0734,
            "token": 2498,
            "token_str": "nothing",
        },
        {
            "sequence": "the answer to the universe is yes.",
            "score": 0.0580,
            "token": 2748,
            "token_str": "yes",
        },
        {
            "sequence": "the answer to the universe is unknown.",
            "score": 0.044,
            "token": 4242,
            "token_str": "unknown",
        },
        {
            "sequence": "the answer to the universe is simple.",
            "score": 0.0402,
            "token": 3722,
            "token_str": "simple",
        },
    ],
)`,highlighted:`self.assertEqual(
    deep_round(data),
    [
        {
            <span class="hljs-string">&quot;sequence&quot;</span>: <span class="hljs-string">&quot;the answer to the universe is no.&quot;</span>,
            <span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.1696</span>,
            <span class="hljs-string">&quot;token&quot;</span>: <span class="hljs-number">2053</span>,
            <span class="hljs-string">&quot;token_str&quot;</span>: <span class="hljs-string">&quot;no&quot;</span>,
        },
        {
            <span class="hljs-string">&quot;sequence&quot;</span>: <span class="hljs-string">&quot;the answer to the universe is nothing.&quot;</span>,
            <span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.0734</span>,
            <span class="hljs-string">&quot;token&quot;</span>: <span class="hljs-number">2498</span>,
            <span class="hljs-string">&quot;token_str&quot;</span>: <span class="hljs-string">&quot;nothing&quot;</span>,
        },
        {
            <span class="hljs-string">&quot;sequence&quot;</span>: <span class="hljs-string">&quot;the answer to the universe is yes.&quot;</span>,
            <span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.0580</span>,
            <span class="hljs-string">&quot;token&quot;</span>: <span class="hljs-number">2748</span>,
            <span class="hljs-string">&quot;token_str&quot;</span>: <span class="hljs-string">&quot;yes&quot;</span>,
        },
        {
            <span class="hljs-string">&quot;sequence&quot;</span>: <span class="hljs-string">&quot;the answer to the universe is unknown.&quot;</span>,
            <span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.044</span>,
            <span class="hljs-string">&quot;token&quot;</span>: <span class="hljs-number">4242</span>,
            <span class="hljs-string">&quot;token_str&quot;</span>: <span class="hljs-string">&quot;unknown&quot;</span>,
        },
        {
            <span class="hljs-string">&quot;sequence&quot;</span>: <span class="hljs-string">&quot;the answer to the universe is simple.&quot;</span>,
            <span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.0402</span>,
            <span class="hljs-string">&quot;token&quot;</span>: <span class="hljs-number">3722</span>,
            <span class="hljs-string">&quot;token_str&quot;</span>: <span class="hljs-string">&quot;simple&quot;</span>,
        },
    ],
)`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),c=!0},p:P,i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function NF($){let n,c;return n=new R({props:{$$slots:{default:[RF]},$$scope:{ctx:$}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),c=!0},p(s,h){const q={};h&2&&(q.$$scope={dirty:h,ctx:s}),n.$set(q)},i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function SF($){let n,c,s,h,q,k,A;return{c(){n=r("p"),c=r("strong"),s=i("Recommended model"),h=i(": "),q=r("a"),k=i(`Check your
langage`),A=i("."),this.h()},l(j){n=o(j,"P",{});var T=l(n);c=o(T,"STRONG",{});var O=l(c);s=u(O,"Recommended model"),O.forEach(t),h=u(T,": "),q=o(T,"A",{href:!0,rel:!0});var D=l(q);k=u(D,`Check your
langage`),D.forEach(t),A=u(T,"."),T.forEach(t),this.h()},h(){d(q,"href","https://huggingface.co/models?pipeline_tag=automatic-speech-recognition"),d(q,"rel","nofollow")},m(j,T){m(j,n,T),e(n,c),e(c,s),e(n,h),e(n,q),e(q,k),e(n,A)},d(j){j&&t(n)}}}function xF($){let n,c,s,h,q,k,A;return{c(){n=r("p"),c=r("strong"),s=i("English"),h=i(`:
`),q=r("a"),k=i("facebook/wav2vec2-large-960h-lv60-self"),A=i("."),this.h()},l(j){n=o(j,"P",{});var T=l(n);c=o(T,"STRONG",{});var O=l(c);s=u(O,"English"),O.forEach(t),h=u(T,`:
`),q=o(T,"A",{href:!0,rel:!0});var D=l(q);k=u(D,"facebook/wav2vec2-large-960h-lv60-self"),D.forEach(t),A=u(T,"."),T.forEach(t),this.h()},h(){d(q,"href","https://huggingface.co/facebook/wav2vec2-large-960h-lv60-self"),d(q,"rel","nofollow")},m(j,T){m(j,n,T),e(n,c),e(c,s),e(n,h),e(n,q),e(q,k),e(n,A)},d(j){j&&t(n)}}}function IF($){let n,c;return n=new N({props:{code:`    import json
    import requests
    headers = {"Authorization": f"Bearer {API_TOKEN}"}
    API_URL = "https://api-inference.huggingface.co/models/facebook/wav2vec2-base-960h"
    def query(filename):
        with open(filename, "rb") as f:
            data = f.read()
        response = requests.request("POST", API_URL, headers=headers, data=data)
        return json.loads(response.content.decode("utf-8"))
    data = query("sample1.flac")`,highlighted:`    <span class="hljs-keyword">import</span> json
    <span class="hljs-keyword">import</span> requests
    headers = {<span class="hljs-string">&quot;Authorization&quot;</span>: <span class="hljs-string">f&quot;Bearer <span class="hljs-subst">{API_TOKEN}</span>&quot;</span>}
    API_URL = <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/facebook/wav2vec2-base-960h&quot;</span>
    <span class="hljs-keyword">def</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">filename</span>):
        <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(filename, <span class="hljs-string">&quot;rb&quot;</span>) <span class="hljs-keyword">as</span> f:
            data = f.read()
        response = requests.request(<span class="hljs-string">&quot;POST&quot;</span>, API_URL, headers=headers, data=data)
        <span class="hljs-keyword">return</span> json.loads(response.content.decode(<span class="hljs-string">&quot;utf-8&quot;</span>))
    data = query(<span class="hljs-string">&quot;sample1.flac&quot;</span>)`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),c=!0},p:P,i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function HF($){let n,c;return n=new R({props:{$$slots:{default:[IF]},$$scope:{ctx:$}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),c=!0},p(s,h){const q={};h&2&&(q.$$scope={dirty:h,ctx:s}),n.$set(q)},i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function BF($){let n,c;return n=new N({props:{code:`    import fetch from "node-fetch";
    import fs from "fs";
    async function query(filename) {
        const data = fs.readFileSync(filename);
        const response = await fetch(
            "https://api-inference.huggingface.co/models/facebook/wav2vec2-base-960h",
            {
                headers: { Authorization: \`Bearer \${API_TOKEN}\` },
                method: "POST",
                body: data,
            }
        );
        const result = await response.json();
        return result;
    }
    query("sample1.flac").then((response) => {
        console.log(JSON.stringify(response));
    });
    // {"text":"GOING ALONG SLUSHY COUNTRY ROADS AND SPEAKING TO DAMP AUDIENCES IN DRAUGHTY SCHOOL ROOMS DAY AFTER DAY FOR A FORTNIGHT HE'LL HAVE TO PUT IN AN APPEARANCE AT SOME PLACE OF WORSHIP ON SUNDAY MORNING AND HE CAN COME TO US IMMEDIATELY AFTERWARDS"}`,highlighted:`    <span class="hljs-keyword">import</span> fetch <span class="hljs-keyword">from</span> <span class="hljs-string">&quot;node-fetch&quot;</span>;
    <span class="hljs-keyword">import</span> fs <span class="hljs-keyword">from</span> <span class="hljs-string">&quot;fs&quot;</span>;
    <span class="hljs-keyword">async</span> <span class="hljs-keyword">function</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">filename</span>) {
        <span class="hljs-keyword">const</span> data = fs.<span class="hljs-title function_">readFileSync</span>(filename);
        <span class="hljs-keyword">const</span> response = <span class="hljs-keyword">await</span> <span class="hljs-title function_">fetch</span>(
            <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/facebook/wav2vec2-base-960h&quot;</span>,
            {
                <span class="hljs-attr">headers</span>: { <span class="hljs-title class_">Authorization</span>: <span class="hljs-string">\`Bearer <span class="hljs-subst">\${API_TOKEN}</span>\`</span> },
                <span class="hljs-attr">method</span>: <span class="hljs-string">&quot;POST&quot;</span>,
                <span class="hljs-attr">body</span>: data,
            }
        );
        <span class="hljs-keyword">const</span> result = <span class="hljs-keyword">await</span> response.<span class="hljs-title function_">json</span>();
        <span class="hljs-keyword">return</span> result;
    }
    <span class="hljs-title function_">query</span>(<span class="hljs-string">&quot;sample1.flac&quot;</span>).<span class="hljs-title function_">then</span>(<span class="hljs-function">(<span class="hljs-params">response</span>) =&gt;</span> {
        <span class="hljs-variable language_">console</span>.<span class="hljs-title function_">log</span>(<span class="hljs-title class_">JSON</span>.<span class="hljs-title function_">stringify</span>(response));
    });
    <span class="hljs-comment">// {&quot;text&quot;:&quot;GOING ALONG SLUSHY COUNTRY ROADS AND SPEAKING TO DAMP AUDIENCES IN DRAUGHTY SCHOOL ROOMS DAY AFTER DAY FOR A FORTNIGHT HE&#x27;LL HAVE TO PUT IN AN APPEARANCE AT SOME PLACE OF WORSHIP ON SUNDAY MORNING AND HE CAN COME TO US IMMEDIATELY AFTERWARDS&quot;}</span>`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),c=!0},p:P,i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function CF($){let n,c;return n=new R({props:{$$slots:{default:[BF]},$$scope:{ctx:$}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),c=!0},p(s,h){const q={};h&2&&(q.$$scope={dirty:h,ctx:s}),n.$set(q)},i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function GF($){let n,c;return n=new N({props:{code:`    curl https://api-inference.huggingface.co/models/facebook/wav2vec2-base-960h \\
            -X POST \\
            --data-binary '@sample1.flac' \\
            -H "Authorization: Bearer \${HF_API_TOKEN}"
    # {"text":"GOING ALONG SLUSHY COUNTRY ROADS AND SPEAKING TO DAMP AUDIENCES IN DRAUGHTY SCHOOL ROOMS DAY AFTER DAY FOR A FORTNIGHT HE'LL HAVE TO PUT IN AN APPEARANCE AT SOME PLACE OF WORSHIP ON SUNDAY MORNING AND HE CAN COME TO US IMMEDIATELY AFTERWARDS"}`,highlighted:`    curl https://api-inference.huggingface.co/models/facebook/wav2vec2-base-960h \\
            -X POST \\
            --data-binary <span class="hljs-string">&#x27;@sample1.flac&#x27;</span> \\
            -H <span class="hljs-string">&quot;Authorization: Bearer <span class="hljs-variable">\${HF_API_TOKEN}</span>&quot;</span>
    <span class="hljs-comment"># {&quot;text&quot;:&quot;GOING ALONG SLUSHY COUNTRY ROADS AND SPEAKING TO DAMP AUDIENCES IN DRAUGHTY SCHOOL ROOMS DAY AFTER DAY FOR A FORTNIGHT HE&#x27;LL HAVE TO PUT IN AN APPEARANCE AT SOME PLACE OF WORSHIP ON SUNDAY MORNING AND HE CAN COME TO US IMMEDIATELY AFTERWARDS&quot;}</span>`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),c=!0},p:P,i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function LF($){let n,c;return n=new R({props:{$$slots:{default:[GF]},$$scope:{ctx:$}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),c=!0},p(s,h){const q={};h&2&&(q.$$scope={dirty:h,ctx:s}),n.$set(q)},i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function UF($){let n,c;return n=new N({props:{code:`    self.assertEqual(
        data,
        {
            "text": "GOING ALONG SLUSHY COUNTRY ROADS AND SPEAKING TO DAMP AUDIENCES IN DRAUGHTY SCHOOL ROOMS DAY AFTER DAY FOR A FORTNIGHT HE'LL HAVE TO PUT IN AN APPEARANCE AT SOME PLACE OF WORSHIP ON SUNDAY MORNING AND HE CAN COME TO US IMMEDIATELY AFTERWARDS"
        },
    )`,highlighted:`    self.assertEqual(
        data,
        {
            <span class="hljs-string">&quot;text&quot;</span>: <span class="hljs-string">&quot;GOING ALONG SLUSHY COUNTRY ROADS AND SPEAKING TO DAMP AUDIENCES IN DRAUGHTY SCHOOL ROOMS DAY AFTER DAY FOR A FORTNIGHT HE&#x27;LL HAVE TO PUT IN AN APPEARANCE AT SOME PLACE OF WORSHIP ON SUNDAY MORNING AND HE CAN COME TO US IMMEDIATELY AFTERWARDS&quot;</span>
        },
    )`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),c=!0},p:P,i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function zF($){let n,c;return n=new R({props:{$$slots:{default:[UF]},$$scope:{ctx:$}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),c=!0},p(s,h){const q={};h&2&&(q.$$scope={dirty:h,ctx:s}),n.$set(q)},i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function MF($){let n,c,s,h,q,k,A;return{c(){n=r("p"),c=r("strong"),s=i("Recommended model"),h=i(`:
`),q=r("a"),k=i("Sentence-transformers"),A=i("."),this.h()},l(j){n=o(j,"P",{});var T=l(n);c=o(T,"STRONG",{});var O=l(c);s=u(O,"Recommended model"),O.forEach(t),h=u(T,`:
`),q=o(T,"A",{href:!0,rel:!0});var D=l(q);k=u(D,"Sentence-transformers"),D.forEach(t),A=u(T,"."),T.forEach(t),this.h()},h(){d(q,"href","https://huggingface.co/sentence-transformers/paraphrase-xlm-r-multilingual-v1"),d(q,"rel","nofollow")},m(j,T){m(j,n,T),e(n,c),e(c,s),e(n,h),e(n,q),e(q,k),e(n,A)},d(j){j&&t(n)}}}function FF($){let n,c,s,h,q,k,A;return{c(){n=r("p"),c=r("strong"),s=i("Recommended model"),h=i(`:
`),q=r("a"),k=i("superb/hubert-large-superb-er"),A=i("."),this.h()},l(j){n=o(j,"P",{});var T=l(n);c=o(T,"STRONG",{});var O=l(c);s=u(O,"Recommended model"),O.forEach(t),h=u(T,`:
`),q=o(T,"A",{href:!0,rel:!0});var D=l(q);k=u(D,"superb/hubert-large-superb-er"),D.forEach(t),A=u(T,"."),T.forEach(t),this.h()},h(){d(q,"href","https://huggingface.co/superb/hubert-large-superb-er"),d(q,"rel","nofollow")},m(j,T){m(j,n,T),e(n,c),e(c,s),e(n,h),e(n,q),e(q,k),e(n,A)},d(j){j&&t(n)}}}function KF($){let n,c;return n=new N({props:{code:`    import json
    import requests
    headers = {"Authorization": f"Bearer {API_TOKEN}"}
    API_URL = "https://api-inference.huggingface.co/models/superb/hubert-large-superb-er"
    def query(filename):
        with open(filename, "rb") as f:
            data = f.read()
        response = requests.request("POST", API_URL, headers=headers, data=data)
        return json.loads(response.content.decode("utf-8"))
    data = query("sample1.flac")`,highlighted:`    <span class="hljs-keyword">import</span> json
    <span class="hljs-keyword">import</span> requests
    headers = {<span class="hljs-string">&quot;Authorization&quot;</span>: <span class="hljs-string">f&quot;Bearer <span class="hljs-subst">{API_TOKEN}</span>&quot;</span>}
    API_URL = <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/superb/hubert-large-superb-er&quot;</span>
    <span class="hljs-keyword">def</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">filename</span>):
        <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(filename, <span class="hljs-string">&quot;rb&quot;</span>) <span class="hljs-keyword">as</span> f:
            data = f.read()
        response = requests.request(<span class="hljs-string">&quot;POST&quot;</span>, API_URL, headers=headers, data=data)
        <span class="hljs-keyword">return</span> json.loads(response.content.decode(<span class="hljs-string">&quot;utf-8&quot;</span>))
    data = query(<span class="hljs-string">&quot;sample1.flac&quot;</span>)`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),c=!0},p:P,i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function JF($){let n,c;return n=new R({props:{$$slots:{default:[KF]},$$scope:{ctx:$}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),c=!0},p(s,h){const q={};h&2&&(q.$$scope={dirty:h,ctx:s}),n.$set(q)},i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function WF($){let n,c;return n=new N({props:{code:`    import fetch from "node-fetch";
    import fs from "fs";
    async function query(filename) {
        const data = fs.readFileSync(filename);
        const response = await fetch(
            "https://api-inference.huggingface.co/models/superb/hubert-large-superb-er",
            {
                headers: { Authorization: \`Bearer \${API_TOKEN}\` },
                method: "POST",
                body: data,
            }
        );
        const result = await response.json();
        return result;
    }
    query("sample1.flac").then((response) => {
        console.log(JSON.stringify(response));
    });
    // [{"score":0.5927661657333374,"label":"neu"},{"score":0.2002529799938202,"label":"hap"},{"score":0.12795612215995789,"label":"ang"},{"score":0.07902472466230392,"label":"sad"}]`,highlighted:`    <span class="hljs-keyword">import</span> <span class="hljs-keyword">fetch</span> <span class="hljs-keyword">from</span> &quot;node-fetch&quot;;
    <span class="hljs-keyword">import</span> fs <span class="hljs-keyword">from</span> &quot;fs&quot;;
    async <span class="hljs-keyword">function</span> query(filename) {
        const data = fs.readFileSync(filename);
        const response = await <span class="hljs-keyword">fetch</span>(
            &quot;https://api-inference.huggingface.co/models/superb/hubert-large-superb-er&quot;,
            {
                headers: { <span class="hljs-keyword">Authorization</span>: \`Bearer \${API_TOKEN}\` },
                <span class="hljs-keyword">method</span>: &quot;POST&quot;,
                body: data,
            }
        );
        const result = await response.json();
        <span class="hljs-keyword">return</span> result;
    }
    query(&quot;sample1.flac&quot;).<span class="hljs-keyword">then</span>((response) =&gt; {
        console.log(<span class="hljs-type">JSON</span>.stringify(response));
    });
    // [{&quot;score&quot;:<span class="hljs-number">0.5927661657333374</span>,&quot;label&quot;:&quot;neu&quot;},{&quot;score&quot;:<span class="hljs-number">0.2002529799938202</span>,&quot;label&quot;:&quot;hap&quot;},{&quot;score&quot;:<span class="hljs-number">0.12795612215995789</span>,&quot;label&quot;:&quot;ang&quot;},{&quot;score&quot;:<span class="hljs-number">0.07902472466230392</span>,&quot;label&quot;:&quot;sad&quot;}]`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),c=!0},p:P,i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function YF($){let n,c;return n=new R({props:{$$slots:{default:[WF]},$$scope:{ctx:$}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),c=!0},p(s,h){const q={};h&2&&(q.$$scope={dirty:h,ctx:s}),n.$set(q)},i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function VF($){let n,c;return n=new N({props:{code:`    curl https://api-inference.huggingface.co/models/superb/hubert-large-superb-er \\
            -X POST \\
            --data-binary '@sample1.flac' \\
            -H "Authorization: Bearer \${HF_API_TOKEN}"
    # [{"score":0.5927661657333374,"label":"neu"},{"score":0.2002529799938202,"label":"hap"},{"score":0.12795612215995789,"label":"ang"},{"score":0.07902472466230392,"label":"sad"}]`,highlighted:`    curl https://api-inference.huggingface.co/models/superb/hubert-large-superb-er \\
            -X POST \\
            --data-binary <span class="hljs-string">&#x27;@sample1.flac&#x27;</span> \\
            -H <span class="hljs-string">&quot;Authorization: Bearer <span class="hljs-variable">\${HF_API_TOKEN}</span>&quot;</span>
    <span class="hljs-comment"># [{&quot;score&quot;:0.5927661657333374,&quot;label&quot;:&quot;neu&quot;},{&quot;score&quot;:0.2002529799938202,&quot;label&quot;:&quot;hap&quot;},{&quot;score&quot;:0.12795612215995789,&quot;label&quot;:&quot;ang&quot;},{&quot;score&quot;:0.07902472466230392,&quot;label&quot;:&quot;sad&quot;}]</span>`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),c=!0},p:P,i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function XF($){let n,c;return n=new R({props:{$$slots:{default:[VF]},$$scope:{ctx:$}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),c=!0},p(s,h){const q={};h&2&&(q.$$scope={dirty:h,ctx:s}),n.$set(q)},i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function QF($){let n,c;return n=new N({props:{code:`    self.assertEqual(
        deep_round(data, 4),
        [
            {"score": 0.5928, "label": "neu"},
            {"score": 0.2003, "label": "hap"},
            {"score": 0.128, "label": "ang"},
            {"score": 0.079, "label": "sad"},
        ],
    )`,highlighted:`    self.assertEqual(
        deep_round(data, <span class="hljs-number">4</span>),
        [
            {<span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.5928</span>, <span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;neu&quot;</span>},
            {<span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.2003</span>, <span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;hap&quot;</span>},
            {<span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.128</span>, <span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;ang&quot;</span>},
            {<span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.079</span>, <span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;sad&quot;</span>},
        ],
    )`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),c=!0},p:P,i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function ZF($){let n,c;return n=new R({props:{$$slots:{default:[QF]},$$scope:{ctx:$}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),c=!0},p(s,h){const q={};h&2&&(q.$$scope={dirty:h,ctx:s}),n.$set(q)},i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function eK($){let n,c,s,h,q,k,A;return{c(){n=r("p"),c=r("strong"),s=i("Recommended model"),h=i(`:
`),q=r("a"),k=i("facebook/detr-resnet-50"),A=i("."),this.h()},l(j){n=o(j,"P",{});var T=l(n);c=o(T,"STRONG",{});var O=l(c);s=u(O,"Recommended model"),O.forEach(t),h=u(T,`:
`),q=o(T,"A",{href:!0,rel:!0});var D=l(q);k=u(D,"facebook/detr-resnet-50"),D.forEach(t),A=u(T,"."),T.forEach(t),this.h()},h(){d(q,"href","https://huggingface.co/facebook/detr-resnet-50"),d(q,"rel","nofollow")},m(j,T){m(j,n,T),e(n,c),e(c,s),e(n,h),e(n,q),e(q,k),e(n,A)},d(j){j&&t(n)}}}function tK($){let n,c;return n=new N({props:{code:`    import json
    import requests
    headers = {"Authorization": f"Bearer {API_TOKEN}"}
    API_URL = "https://api-inference.huggingface.co/models/facebook/detr-resnet-50"
    def query(filename):
        with open(filename, "rb") as f:
            data = f.read()
        response = requests.request("POST", API_URL, headers=headers, data=data)
        return json.loads(response.content.decode("utf-8"))
    data = query("cats.jpg")`,highlighted:`    <span class="hljs-keyword">import</span> json
    <span class="hljs-keyword">import</span> requests
    headers = {<span class="hljs-string">&quot;Authorization&quot;</span>: <span class="hljs-string">f&quot;Bearer <span class="hljs-subst">{API_TOKEN}</span>&quot;</span>}
    API_URL = <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/facebook/detr-resnet-50&quot;</span>
    <span class="hljs-keyword">def</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">filename</span>):
        <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(filename, <span class="hljs-string">&quot;rb&quot;</span>) <span class="hljs-keyword">as</span> f:
            data = f.read()
        response = requests.request(<span class="hljs-string">&quot;POST&quot;</span>, API_URL, headers=headers, data=data)
        <span class="hljs-keyword">return</span> json.loads(response.content.decode(<span class="hljs-string">&quot;utf-8&quot;</span>))
    data = query(<span class="hljs-string">&quot;cats.jpg&quot;</span>)`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),c=!0},p:P,i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function sK($){let n,c;return n=new R({props:{$$slots:{default:[tK]},$$scope:{ctx:$}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),c=!0},p(s,h){const q={};h&2&&(q.$$scope={dirty:h,ctx:s}),n.$set(q)},i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function aK($){let n,c;return n=new N({props:{code:`    import fetch from "node-fetch";
    import fs from "fs";
    async function query(filename) {
        const data = fs.readFileSync(filename);
        const response = await fetch(
            "https://api-inference.huggingface.co/models/facebook/detr-resnet-50",
            {
                headers: { Authorization: \`Bearer \${API_TOKEN}\` },
                method: "POST",
                body: data,
            }
        );
        const result = await response.json();
        return result;
    }
    query("cats.jpg").then((response) => {
        console.log(JSON.stringify(response));
    });
    // [{"score":0.9982201457023621,"label":"remote","box":{"xmin":40,"ymin":70,"xmax":175,"ymax":117}},{"score":0.9960021376609802,"label":"remote","box":{"xmin":333,"ymin":72,"xmax":368,"ymax":187}},{"score":0.9954745173454285,"label":"couch","box":{"xmin":0,"ymin":1,"xmax":639,"ymax":473}},{"score":0.9988006353378296,"label":"cat","box":{"xmin":13,"ymin":52,"xmax":314,"ymax":470}},{"score":0.9986783862113953,"label":"cat","box":{"xmin":345,"ymin":23,"xmax":640,"ymax":368}}]`,highlighted:`    <span class="hljs-keyword">import</span> <span class="hljs-keyword">fetch</span> <span class="hljs-keyword">from</span> &quot;node-fetch&quot;;
    <span class="hljs-keyword">import</span> fs <span class="hljs-keyword">from</span> &quot;fs&quot;;
    async <span class="hljs-keyword">function</span> query(filename) {
        const data = fs.readFileSync(filename);
        const response = await <span class="hljs-keyword">fetch</span>(
            &quot;https://api-inference.huggingface.co/models/facebook/detr-resnet-50&quot;,
            {
                headers: { <span class="hljs-keyword">Authorization</span>: \`Bearer \${API_TOKEN}\` },
                <span class="hljs-keyword">method</span>: &quot;POST&quot;,
                body: data,
            }
        );
        const result = await response.json();
        <span class="hljs-keyword">return</span> result;
    }
    query(&quot;cats.jpg&quot;).<span class="hljs-keyword">then</span>((response) =&gt; {
        console.log(<span class="hljs-type">JSON</span>.stringify(response));
    });
    // [{&quot;score&quot;:<span class="hljs-number">0.9982201457023621</span>,&quot;label&quot;:&quot;remote&quot;,&quot;box&quot;:{&quot;xmin&quot;:<span class="hljs-number">40</span>,&quot;ymin&quot;:<span class="hljs-number">70</span>,&quot;xmax&quot;:<span class="hljs-number">175</span>,&quot;ymax&quot;:<span class="hljs-number">117</span>}},{&quot;score&quot;:<span class="hljs-number">0.9960021376609802</span>,&quot;label&quot;:&quot;remote&quot;,&quot;box&quot;:{&quot;xmin&quot;:<span class="hljs-number">333</span>,&quot;ymin&quot;:<span class="hljs-number">72</span>,&quot;xmax&quot;:<span class="hljs-number">368</span>,&quot;ymax&quot;:<span class="hljs-number">187</span>}},{&quot;score&quot;:<span class="hljs-number">0.9954745173454285</span>,&quot;label&quot;:&quot;couch&quot;,&quot;box&quot;:{&quot;xmin&quot;:<span class="hljs-number">0</span>,&quot;ymin&quot;:<span class="hljs-number">1</span>,&quot;xmax&quot;:<span class="hljs-number">639</span>,&quot;ymax&quot;:<span class="hljs-number">473</span>}},{&quot;score&quot;:<span class="hljs-number">0.9988006353378296</span>,&quot;label&quot;:&quot;cat&quot;,&quot;box&quot;:{&quot;xmin&quot;:<span class="hljs-number">13</span>,&quot;ymin&quot;:<span class="hljs-number">52</span>,&quot;xmax&quot;:<span class="hljs-number">314</span>,&quot;ymax&quot;:<span class="hljs-number">470</span>}},{&quot;score&quot;:<span class="hljs-number">0.9986783862113953</span>,&quot;label&quot;:&quot;cat&quot;,&quot;box&quot;:{&quot;xmin&quot;:<span class="hljs-number">345</span>,&quot;ymin&quot;:<span class="hljs-number">23</span>,&quot;xmax&quot;:<span class="hljs-number">640</span>,&quot;ymax&quot;:<span class="hljs-number">368</span>}}]`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),c=!0},p:P,i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function nK($){let n,c;return n=new R({props:{$$slots:{default:[aK]},$$scope:{ctx:$}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),c=!0},p(s,h){const q={};h&2&&(q.$$scope={dirty:h,ctx:s}),n.$set(q)},i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function rK($){let n,c;return n=new N({props:{code:`    curl https://api-inference.huggingface.co/models/facebook/detr-resnet-50 \\
            -X POST \\
            --data-binary '@cats.jpg' \\
            -H "Authorization: Bearer \${HF_API_TOKEN}"
    # [{"score":0.9982201457023621,"label":"remote","box":{"xmin":40,"ymin":70,"xmax":175,"ymax":117}},{"score":0.9960021376609802,"label":"remote","box":{"xmin":333,"ymin":72,"xmax":368,"ymax":187}},{"score":0.9954745173454285,"label":"couch","box":{"xmin":0,"ymin":1,"xmax":639,"ymax":473}},{"score":0.9988006353378296,"label":"cat","box":{"xmin":13,"ymin":52,"xmax":314,"ymax":470}},{"score":0.9986783862113953,"label":"cat","box":{"xmin":345,"ymin":23,"xmax":640,"ymax":368}}]`,highlighted:`    curl https://api-inference.huggingface.co/models/facebook/detr-resnet-50 \\
            -X POST \\
            --data-binary <span class="hljs-string">&#x27;@cats.jpg&#x27;</span> \\
            -H <span class="hljs-string">&quot;Authorization: Bearer <span class="hljs-variable">\${HF_API_TOKEN}</span>&quot;</span>
    <span class="hljs-comment"># [{&quot;score&quot;:0.9982201457023621,&quot;label&quot;:&quot;remote&quot;,&quot;box&quot;:{&quot;xmin&quot;:40,&quot;ymin&quot;:70,&quot;xmax&quot;:175,&quot;ymax&quot;:117}},{&quot;score&quot;:0.9960021376609802,&quot;label&quot;:&quot;remote&quot;,&quot;box&quot;:{&quot;xmin&quot;:333,&quot;ymin&quot;:72,&quot;xmax&quot;:368,&quot;ymax&quot;:187}},{&quot;score&quot;:0.9954745173454285,&quot;label&quot;:&quot;couch&quot;,&quot;box&quot;:{&quot;xmin&quot;:0,&quot;ymin&quot;:1,&quot;xmax&quot;:639,&quot;ymax&quot;:473}},{&quot;score&quot;:0.9988006353378296,&quot;label&quot;:&quot;cat&quot;,&quot;box&quot;:{&quot;xmin&quot;:13,&quot;ymin&quot;:52,&quot;xmax&quot;:314,&quot;ymax&quot;:470}},{&quot;score&quot;:0.9986783862113953,&quot;label&quot;:&quot;cat&quot;,&quot;box&quot;:{&quot;xmin&quot;:345,&quot;ymin&quot;:23,&quot;xmax&quot;:640,&quot;ymax&quot;:368}}]</span>`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),c=!0},p:P,i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function oK($){let n,c;return n=new R({props:{$$slots:{default:[rK]},$$scope:{ctx:$}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),c=!0},p(s,h){const q={};h&2&&(q.$$scope={dirty:h,ctx:s}),n.$set(q)},i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function lK($){let n,c;return n=new N({props:{code:`    self.assertEqual(
        deep_round(data, 4),
        [
            {
                "score": 0.9982,
                "label": "remote",
                "box": {"xmin": 40, "ymin": 70, "xmax": 175, "ymax": 117},
            },
            {
                "score": 0.9960,
                "label": "remote",
                "box": {"xmin": 333, "ymin": 72, "xmax": 368, "ymax": 187},
            },
            {
                "score": 0.9955,
                "label": "couch",
                "box": {"xmin": 0, "ymin": 1, "xmax": 639, "ymax": 473},
            },
            {
                "score": 0.9988,
                "label": "cat",
                "box": {"xmin": 13, "ymin": 52, "xmax": 314, "ymax": 470},
            },
            {
                "score": 0.9987,
                "label": "cat",
                "box": {"xmin": 345, "ymin": 23, "xmax": 640, "ymax": 368},
            },
        ],
    )`,highlighted:`    self.assertEqual(
        deep_round(data, <span class="hljs-number">4</span>),
        [
            {
                <span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.9982</span>,
                <span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;remote&quot;</span>,
                <span class="hljs-string">&quot;box&quot;</span>: {<span class="hljs-string">&quot;xmin&quot;</span>: <span class="hljs-number">40</span>, <span class="hljs-string">&quot;ymin&quot;</span>: <span class="hljs-number">70</span>, <span class="hljs-string">&quot;xmax&quot;</span>: <span class="hljs-number">175</span>, <span class="hljs-string">&quot;ymax&quot;</span>: <span class="hljs-number">117</span>},
            },
            {
                <span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.9960</span>,
                <span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;remote&quot;</span>,
                <span class="hljs-string">&quot;box&quot;</span>: {<span class="hljs-string">&quot;xmin&quot;</span>: <span class="hljs-number">333</span>, <span class="hljs-string">&quot;ymin&quot;</span>: <span class="hljs-number">72</span>, <span class="hljs-string">&quot;xmax&quot;</span>: <span class="hljs-number">368</span>, <span class="hljs-string">&quot;ymax&quot;</span>: <span class="hljs-number">187</span>},
            },
            {
                <span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.9955</span>,
                <span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;couch&quot;</span>,
                <span class="hljs-string">&quot;box&quot;</span>: {<span class="hljs-string">&quot;xmin&quot;</span>: <span class="hljs-number">0</span>, <span class="hljs-string">&quot;ymin&quot;</span>: <span class="hljs-number">1</span>, <span class="hljs-string">&quot;xmax&quot;</span>: <span class="hljs-number">639</span>, <span class="hljs-string">&quot;ymax&quot;</span>: <span class="hljs-number">473</span>},
            },
            {
                <span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.9988</span>,
                <span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;cat&quot;</span>,
                <span class="hljs-string">&quot;box&quot;</span>: {<span class="hljs-string">&quot;xmin&quot;</span>: <span class="hljs-number">13</span>, <span class="hljs-string">&quot;ymin&quot;</span>: <span class="hljs-number">52</span>, <span class="hljs-string">&quot;xmax&quot;</span>: <span class="hljs-number">314</span>, <span class="hljs-string">&quot;ymax&quot;</span>: <span class="hljs-number">470</span>},
            },
            {
                <span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.9987</span>,
                <span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;cat&quot;</span>,
                <span class="hljs-string">&quot;box&quot;</span>: {<span class="hljs-string">&quot;xmin&quot;</span>: <span class="hljs-number">345</span>, <span class="hljs-string">&quot;ymin&quot;</span>: <span class="hljs-number">23</span>, <span class="hljs-string">&quot;xmax&quot;</span>: <span class="hljs-number">640</span>, <span class="hljs-string">&quot;ymax&quot;</span>: <span class="hljs-number">368</span>},
            },
        ],
    )`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),c=!0},p:P,i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function iK($){let n,c;return n=new R({props:{$$slots:{default:[lK]},$$scope:{ctx:$}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),c=!0},p(s,h){const q={};h&2&&(q.$$scope={dirty:h,ctx:s}),n.$set(q)},i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function uK($){let n,c,s,h,q,k,A;return{c(){n=r("p"),c=r("strong"),s=i("Recommended model"),h=i(`:
`),q=r("a"),k=i("facebook/detr-resnet-50-panoptic"),A=i("."),this.h()},l(j){n=o(j,"P",{});var T=l(n);c=o(T,"STRONG",{});var O=l(c);s=u(O,"Recommended model"),O.forEach(t),h=u(T,`:
`),q=o(T,"A",{href:!0,rel:!0});var D=l(q);k=u(D,"facebook/detr-resnet-50-panoptic"),D.forEach(t),A=u(T,"."),T.forEach(t),this.h()},h(){d(q,"href","https://huggingface.co/facebook/detr-resnet-50-panoptic"),d(q,"rel","nofollow")},m(j,T){m(j,n,T),e(n,c),e(c,s),e(n,h),e(n,q),e(q,k),e(n,A)},d(j){j&&t(n)}}}function cK($){let n,c;return n=new N({props:{code:`    import json
    import requests
    headers = {"Authorization": f"Bearer {API_TOKEN}"}
    API_URL = "https://api-inference.huggingface.co/models/facebook/detr-resnet-50-panoptic"
    def query(filename):
        with open(filename, "rb") as f:
            data = f.read()
        response = requests.request("POST", API_URL, headers=headers, data=data)
        return json.loads(response.content.decode("utf-8"))
    data = query("cats.jpg")`,highlighted:`    <span class="hljs-keyword">import</span> json
    <span class="hljs-keyword">import</span> requests
    headers = {<span class="hljs-string">&quot;Authorization&quot;</span>: <span class="hljs-string">f&quot;Bearer <span class="hljs-subst">{API_TOKEN}</span>&quot;</span>}
    API_URL = <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/facebook/detr-resnet-50-panoptic&quot;</span>
    <span class="hljs-keyword">def</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">filename</span>):
        <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(filename, <span class="hljs-string">&quot;rb&quot;</span>) <span class="hljs-keyword">as</span> f:
            data = f.read()
        response = requests.request(<span class="hljs-string">&quot;POST&quot;</span>, API_URL, headers=headers, data=data)
        <span class="hljs-keyword">return</span> json.loads(response.content.decode(<span class="hljs-string">&quot;utf-8&quot;</span>))
    data = query(<span class="hljs-string">&quot;cats.jpg&quot;</span>)`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),c=!0},p:P,i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function fK($){let n,c;return n=new R({props:{$$slots:{default:[cK]},$$scope:{ctx:$}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),c=!0},p(s,h){const q={};h&2&&(q.$$scope={dirty:h,ctx:s}),n.$set(q)},i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function pK($){let n,c;return n=new N({props:{code:`    import fetch from "node-fetch";
    import fs from "fs";
    async function query(filename) {
        const data = fs.readFileSync(filename);
        const response = await fetch(
            "https://api-inference.huggingface.co/models/facebook/detr-resnet-50-panoptic",
            {
                headers: { Authorization: \`Bearer \${API_TOKEN}\` },
                method: "POST",
                body: data,
            }
        );
        const result = await response.json();
        return result;
    }
    query("cats.jpg").then((response) => {
        console.log(JSON.stringify(response));
    });
    // [{"score": 0.9094282388687134, "label": "blanket", "mask": "ff7cb6da186cb65a657d5bf39964f02e"}, {"score": 0.9940965175628662, "label": "cat", "mask": "a4b62b30c4760fbb82a4c050486774b1"}, {"score": 0.9986692667007446, "label": "remote", "mask": "635b6c45f2dbf09f217ed73e496e04d6"}, {"score": 0.9994757771492004, "label": "remote", "mask": "3a9dd3459a7834fadf02f9150d6da859"}, {"score": 0.9722068309783936, "label": "couch", "mask": "d106ae88eb4069a45ac79d463c2e761f"}, {"score": 0.9994235038757324, "label": "cat", "mask": "c5c2f5fa5168dcb9f4e7713a54f33004"}]`,highlighted:`    <span class="hljs-keyword">import</span> <span class="hljs-keyword">fetch</span> <span class="hljs-keyword">from</span> &quot;node-fetch&quot;;
    <span class="hljs-keyword">import</span> fs <span class="hljs-keyword">from</span> &quot;fs&quot;;
    async <span class="hljs-keyword">function</span> query(filename) {
        const data = fs.readFileSync(filename);
        const response = await <span class="hljs-keyword">fetch</span>(
            &quot;https://api-inference.huggingface.co/models/facebook/detr-resnet-50-panoptic&quot;,
            {
                headers: { <span class="hljs-keyword">Authorization</span>: \`Bearer \${API_TOKEN}\` },
                <span class="hljs-keyword">method</span>: &quot;POST&quot;,
                body: data,
            }
        );
        const result = await response.json();
        <span class="hljs-keyword">return</span> result;
    }
    query(&quot;cats.jpg&quot;).<span class="hljs-keyword">then</span>((response) =&gt; {
        console.log(<span class="hljs-type">JSON</span>.stringify(response));
    });
    // [{&quot;score&quot;: <span class="hljs-number">0.9094282388687134</span>, &quot;label&quot;: &quot;blanket&quot;, &quot;mask&quot;: &quot;ff7cb6da186cb65a657d5bf39964f02e&quot;}, {&quot;score&quot;: <span class="hljs-number">0.9940965175628662</span>, &quot;label&quot;: &quot;cat&quot;, &quot;mask&quot;: &quot;a4b62b30c4760fbb82a4c050486774b1&quot;}, {&quot;score&quot;: <span class="hljs-number">0.9986692667007446</span>, &quot;label&quot;: &quot;remote&quot;, &quot;mask&quot;: &quot;635b6c45f2dbf09f217ed73e496e04d6&quot;}, {&quot;score&quot;: <span class="hljs-number">0.9994757771492004</span>, &quot;label&quot;: &quot;remote&quot;, &quot;mask&quot;: &quot;3a9dd3459a7834fadf02f9150d6da859&quot;}, {&quot;score&quot;: <span class="hljs-number">0.9722068309783936</span>, &quot;label&quot;: &quot;couch&quot;, &quot;mask&quot;: &quot;d106ae88eb4069a45ac79d463c2e761f&quot;}, {&quot;score&quot;: <span class="hljs-number">0.9994235038757324</span>, &quot;label&quot;: &quot;cat&quot;, &quot;mask&quot;: &quot;c5c2f5fa5168dcb9f4e7713a54f33004&quot;}]`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),c=!0},p:P,i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function dK($){let n,c;return n=new R({props:{$$slots:{default:[pK]},$$scope:{ctx:$}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),c=!0},p(s,h){const q={};h&2&&(q.$$scope={dirty:h,ctx:s}),n.$set(q)},i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function hK($){let n,c;return n=new N({props:{code:`    curl https://api-inference.huggingface.co/models/facebook/detr-resnet-50-panoptic \\
            -X POST \\
            --data-binary '@cats.jpg' \\
            -H "Authorization: Bearer \${HF_API_TOKEN}"
    # [{"score": 0.9094282388687134, "label": "blanket", "mask": "ff7cb6da186cb65a657d5bf39964f02e"}, {"score": 0.9940965175628662, "label": "cat", "mask": "a4b62b30c4760fbb82a4c050486774b1"}, {"score": 0.9986692667007446, "label": "remote", "mask": "635b6c45f2dbf09f217ed73e496e04d6"}, {"score": 0.9994757771492004, "label": "remote", "mask": "3a9dd3459a7834fadf02f9150d6da859"}, {"score": 0.9722068309783936, "label": "couch", "mask": "d106ae88eb4069a45ac79d463c2e761f"}, {"score": 0.9994235038757324, "label": "cat", "mask": "c5c2f5fa5168dcb9f4e7713a54f33004"}]`,highlighted:`    curl https://api-inference.huggingface.co/models/facebook/detr-resnet-50-panoptic \\
            -X POST \\
            --data-binary <span class="hljs-string">&#x27;@cats.jpg&#x27;</span> \\
            -H <span class="hljs-string">&quot;Authorization: Bearer <span class="hljs-variable">\${HF_API_TOKEN}</span>&quot;</span>
    <span class="hljs-comment"># [{&quot;score&quot;: 0.9094282388687134, &quot;label&quot;: &quot;blanket&quot;, &quot;mask&quot;: &quot;ff7cb6da186cb65a657d5bf39964f02e&quot;}, {&quot;score&quot;: 0.9940965175628662, &quot;label&quot;: &quot;cat&quot;, &quot;mask&quot;: &quot;a4b62b30c4760fbb82a4c050486774b1&quot;}, {&quot;score&quot;: 0.9986692667007446, &quot;label&quot;: &quot;remote&quot;, &quot;mask&quot;: &quot;635b6c45f2dbf09f217ed73e496e04d6&quot;}, {&quot;score&quot;: 0.9994757771492004, &quot;label&quot;: &quot;remote&quot;, &quot;mask&quot;: &quot;3a9dd3459a7834fadf02f9150d6da859&quot;}, {&quot;score&quot;: 0.9722068309783936, &quot;label&quot;: &quot;couch&quot;, &quot;mask&quot;: &quot;d106ae88eb4069a45ac79d463c2e761f&quot;}, {&quot;score&quot;: 0.9994235038757324, &quot;label&quot;: &quot;cat&quot;, &quot;mask&quot;: &quot;c5c2f5fa5168dcb9f4e7713a54f33004&quot;}]</span>`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),c=!0},p:P,i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function gK($){let n,c;return n=new R({props:{$$slots:{default:[hK]},$$scope:{ctx:$}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),c=!0},p(s,h){const q={};h&2&&(q.$$scope={dirty:h,ctx:s}),n.$set(q)},i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function mK($){let n,c;return n=new N({props:{code:`    self.assertEqual(
        deep_round(data, 4),
        [
            {"score": 0.9094, "label": "blanket", "mask": "ff7cb6da186cb65a657d5bf39964f02e"},
            {"score": 0.9941, "label": "cat", "mask": "a4b62b30c4760fbb82a4c050486774b1"},
            {"score": 0.9987, "label": "remote", "mask": "635b6c45f2dbf09f217ed73e496e04d6"},
            {"score": 0.9995, "label": "remote", "mask": "3a9dd3459a7834fadf02f9150d6da859"},
            {"score": 0.9722, "label": "couch", "mask": "d106ae88eb4069a45ac79d463c2e761f"},
            {"score": 0.9994, "label": "cat", "mask": "c5c2f5fa5168dcb9f4e7713a54f33004"},
        ],
    )`,highlighted:`    self.assertEqual(
        deep_round(data, <span class="hljs-number">4</span>),
        [
            {<span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.9094</span>, <span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;blanket&quot;</span>, <span class="hljs-string">&quot;mask&quot;</span>: <span class="hljs-string">&quot;ff7cb6da186cb65a657d5bf39964f02e&quot;</span>},
            {<span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.9941</span>, <span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;cat&quot;</span>, <span class="hljs-string">&quot;mask&quot;</span>: <span class="hljs-string">&quot;a4b62b30c4760fbb82a4c050486774b1&quot;</span>},
            {<span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.9987</span>, <span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;remote&quot;</span>, <span class="hljs-string">&quot;mask&quot;</span>: <span class="hljs-string">&quot;635b6c45f2dbf09f217ed73e496e04d6&quot;</span>},
            {<span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.9995</span>, <span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;remote&quot;</span>, <span class="hljs-string">&quot;mask&quot;</span>: <span class="hljs-string">&quot;3a9dd3459a7834fadf02f9150d6da859&quot;</span>},
            {<span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.9722</span>, <span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;couch&quot;</span>, <span class="hljs-string">&quot;mask&quot;</span>: <span class="hljs-string">&quot;d106ae88eb4069a45ac79d463c2e761f&quot;</span>},
            {<span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.9994</span>, <span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;cat&quot;</span>, <span class="hljs-string">&quot;mask&quot;</span>: <span class="hljs-string">&quot;c5c2f5fa5168dcb9f4e7713a54f33004&quot;</span>},
        ],
    )`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),c=!0},p:P,i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function qK($){let n,c;return n=new R({props:{$$slots:{default:[mK]},$$scope:{ctx:$}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),c=!0},p(s,h){const q={};h&2&&(q.$$scope={dirty:h,ctx:s}),n.$set(q)},i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function $K($){let n,c,s,h,q,k,A,j,T,O,D,ne,Re,Q,Y,rt,pi,Ua,Ne,Hw,Z$,di,Bw,e_,ot,mS,t_,lt,qS,s_,Se,it,Sd,za,Cw,xd,Gw,a_,hi,Lw,n_,ut,r_,Ma,Uw,Fa,zw,o_,gi,Mw,l_,ct,i_,mi,Fw,u_,ft,Id,Ka,qi,Kw,Jw,Hd,Ww,z,Ja,Wa,Bd,Yw,Vw,Xw,$i,Qw,Zw,Ya,Va,Cd,eb,tb,sb,_i,ab,nb,Xa,vi,rb,ob,de,lb,Gd,ib,ub,Ld,cb,fb,pb,Qa,yi,db,hb,pt,gb,Ud,mb,qb,$b,Za,Ei,zd,_b,vb,wi,yb,Eb,en,bi,wb,bb,dt,Tb,Md,jb,kb,Ab,tn,Ti,Db,Ob,ht,Pb,Fd,Rb,Nb,Sb,sn,ji,xb,Ib,gt,Hb,Kd,Bb,Cb,c_,ki,Gb,f_,Ai,Lb,p_,mt,d_,qt,Jd,an,Di,Ub,zb,Wd,Mb,xe,nn,Oi,Yd,Fb,Kb,Pi,Jb,Wb,rn,Ri,Vd,Yb,Vb,Ni,Xb,Qb,on,Si,Xd,Zb,e3,$t,t3,Qd,s3,a3,h_,Ie,_t,Zd,ln,n3,eh,r3,g_,xi,o3,m_,vt,q_,un,l3,cn,i3,$_,Ii,u3,__,yt,v_,Hi,c3,y_,Et,th,fn,Bi,f3,p3,sh,d3,Z,pn,dn,ah,h3,g3,m3,Ci,q3,$3,hn,Gi,nh,_3,v3,Li,y3,E3,gn,Ui,w3,b3,wt,T3,rh,j3,k3,A3,mn,zi,D3,O3,bt,P3,oh,R3,N3,S3,qn,Mi,x3,I3,Tt,H3,lh,B3,C3,E_,Fi,G3,w_,jt,ih,$n,Ki,L3,U3,uh,z3,ch,_n,Ji,fh,M3,F3,Wi,K3,b_,He,kt,ph,vn,J3,dh,W3,T_,At,Y3,Yi,V3,X3,j_,Dt,k_,yn,Q3,En,Z3,A_,Vi,eT,D_,Ot,O_,Xi,tT,P_,Pt,hh,wn,Qi,sT,aT,gh,nT,G,bn,Tn,mh,rT,oT,lT,Zi,iT,uT,jn,eu,qh,cT,fT,tu,pT,dT,kn,su,hT,gT,he,mT,$h,qT,$T,_h,_T,vT,yT,An,au,ET,wT,ge,bT,vh,TT,jT,yh,kT,AT,DT,Dn,nu,OT,PT,me,RT,Eh,NT,ST,wh,xT,IT,HT,On,ru,BT,CT,re,GT,bh,LT,UT,Th,zT,MT,jh,FT,KT,JT,Pn,ou,WT,YT,qe,VT,kh,XT,QT,Ah,ZT,e5,t5,Rn,lu,s5,a5,Rt,n5,Dh,r5,o5,l5,Nn,iu,i5,u5,Nt,c5,Oh,f5,p5,d5,Sn,uu,Ph,h5,g5,cu,m5,q5,xn,fu,$5,_5,St,v5,Rh,y5,E5,w5,In,pu,b5,T5,xt,j5,Nh,k5,A5,D5,Hn,du,O5,P5,It,R5,Sh,N5,S5,R_,hu,x5,N_,Ht,xh,Bn,gu,I5,H5,Ih,B5,Hh,Cn,mu,Bh,C5,G5,qu,L5,S_,Be,Bt,Ch,Gn,U5,Gh,z5,x_,$u,M5,I_,Ct,H_,Ln,F5,Un,K5,B_,_u,J5,C_,Gt,G_,vu,W5,L_,Lt,Lh,zn,yu,Y5,V5,Uh,X5,S,Mn,Fn,zh,Q5,Z5,e4,Mh,t4,Kn,Eu,s4,a4,wu,n4,r4,Jn,bu,o4,l4,Tu,i4,u4,Wn,ju,c4,f4,Ut,p4,Fh,d4,h4,g4,Yn,ku,Kh,m4,q4,Au,$4,_4,Vn,Du,v4,y4,$e,E4,Jh,w4,b4,Wh,T4,j4,k4,Xn,Ou,A4,D4,_e,O4,Yh,P4,R4,Vh,N4,S4,x4,Qn,Pu,I4,H4,ve,B4,Xh,C4,G4,Qh,L4,U4,z4,Zn,Ru,M4,F4,oe,K4,Zh,J4,W4,eg,Y4,V4,tg,X4,Q4,Z4,er,Nu,ej,tj,ye,sj,sg,aj,nj,ag,rj,oj,lj,tr,Su,ij,uj,zt,cj,ng,fj,pj,dj,sr,xu,hj,gj,Mt,mj,rg,qj,$j,_j,ar,Iu,og,vj,yj,Hu,Ej,wj,nr,Bu,bj,Tj,Ft,jj,lg,kj,Aj,Dj,rr,Cu,Oj,Pj,Kt,Rj,ig,Nj,Sj,xj,or,Gu,Ij,Hj,Jt,Bj,ug,Cj,Gj,U_,Lu,Lj,z_,Wt,cg,lr,Uu,Uj,zj,fg,Mj,ie,ir,zu,pg,Fj,Kj,Mu,Jj,Wj,ur,Fu,dg,Yj,Vj,Ku,Xj,Qj,cr,Ju,Zj,ek,Wu,tk,sk,fr,Yu,ak,nk,Vu,rk,M_,Ce,Yt,hg,pr,ok,gg,lk,F_,Xu,ik,K_,Vt,J_,dr,uk,hr,ck,W_,Qu,fk,Y_,Xt,V_,Zu,pk,X_,Qt,mg,gr,ec,dk,hk,qg,gk,K,mr,qr,$g,mk,qk,$k,_g,_k,$r,tc,vk,yk,sc,Ek,wk,_r,ac,bk,Tk,nc,jk,kk,vr,rc,vg,Ak,Dk,oc,Ok,Pk,yr,lc,Rk,Nk,Zt,Sk,yg,xk,Ik,Hk,Er,ic,Bk,Ck,es,Gk,Eg,Lk,Uk,zk,wr,uc,Mk,Fk,ts,Kk,wg,Jk,Wk,Q_,cc,Yk,Z_,ss,e1,as,bg,br,fc,Vk,Xk,Tg,Qk,ue,Tr,pc,jg,Zk,e6,dc,t6,s6,jr,hc,kg,a6,n6,gc,r6,o6,kr,mc,Ag,l6,i6,qc,u6,c6,Ar,$c,Dg,f6,p6,_c,d6,t1,Ge,ns,Og,Dr,h6,Pg,g6,s1,vc,m6,a1,rs,n1,Le,q6,Or,$6,_6,Pr,v6,r1,yc,y6,o1,os,l1,Ec,E6,i1,wc,w6,u1,ls,c1,is,Rg,Rr,bc,b6,T6,Ng,j6,ce,Nr,Tc,Sg,k6,A6,jc,D6,O6,Sr,kc,xg,P6,R6,Ac,N6,S6,xr,Dc,Ig,x6,I6,us,H6,Hg,B6,C6,G6,Ir,Oc,Bg,L6,U6,cs,z6,Cg,M6,F6,f1,Ue,fs,Gg,Hr,K6,Lg,J6,p1,Pc,W6,d1,ps,h1,Br,Y6,Cr,V6,g1,Rc,X6,m1,ds,q1,Nc,Q6,$1,hs,Ug,Gr,Sc,Z6,e7,zg,t7,ee,Lr,Ur,Mg,s7,a7,n7,xc,r7,o7,zr,Ic,Fg,l7,i7,Hc,u7,c7,Mr,Bc,f7,p7,gs,d7,Kg,h7,g7,m7,Fr,Cc,q7,$7,ms,_7,Jg,v7,y7,E7,Kr,Gc,w7,b7,qs,T7,Wg,j7,k7,_1,Lc,A7,v1,$s,y1,_s,Yg,Jr,Uc,D7,O7,Vg,P7,Wr,Yr,zc,Xg,R7,N7,Mc,S7,x7,Vr,Fc,Qg,I7,H7,Kc,B7,E1,ze,vs,Zg,Xr,C7,em,G7,w1,Qr,L7,Jc,U7,b1,Me,ys,tm,Zr,z7,sm,M7,T1,Wc,F7,j1,Es,k1,Fe,K7,eo,J7,W7,to,Y7,A1,Yc,V7,D1,ws,O1,Vc,X7,P1,bs,am,so,Xc,Q7,Z7,nm,e9,J,ao,no,rm,t9,s9,a9,Qc,n9,r9,ro,Zc,om,o9,l9,ef,i9,u9,oo,tf,c9,f9,x,p9,lm,d9,h9,g9,m9,im,q9,$9,_9,v9,um,y9,E9,w9,b9,cm,T9,j9,fm,k9,A9,D9,O9,pm,P9,R9,dm,N9,S9,x9,I9,hm,H9,B9,gm,C9,G9,L9,lo,sf,mm,U9,z9,af,M9,F9,io,nf,K9,J9,Ts,W9,qm,Y9,V9,X9,uo,rf,Q9,Z9,js,e8,$m,t8,s8,a8,co,of,n8,r8,ks,o8,_m,l8,i8,R1,lf,u8,N1,As,S1,Ds,vm,fo,uf,c8,f8,ym,p8,te,po,cf,Em,d8,h8,ff,g8,m8,ho,pf,wm,q8,$8,df,_8,v8,go,hf,bm,y8,E8,gf,w8,b8,mo,mf,Tm,T8,j8,Os,k8,jm,A8,D8,O8,qo,qf,km,P8,R8,Ps,N8,Am,S8,x8,x1,Ke,Rs,Dm,$o,I8,Om,H8,I1,$f,B8,H1,Ns,B1,_o,C8,vo,G8,C1,_f,L8,G1,Ss,L1,vf,U8,U1,xs,Pm,yo,yf,z8,M8,Rm,F8,I,Eo,wo,Nm,K8,J8,W8,Ef,Y8,V8,bo,wf,Sm,X8,Q8,bf,Z8,eA,To,Tf,tA,sA,Ee,aA,xm,nA,rA,Im,oA,lA,iA,jo,jf,uA,cA,le,fA,Hm,pA,dA,Bm,hA,gA,Cm,mA,qA,$A,ko,kf,_A,vA,we,yA,Gm,EA,wA,Lm,bA,TA,jA,Ao,Af,kA,AA,Is,DA,Um,OA,PA,RA,Do,Df,NA,SA,be,xA,zm,IA,HA,Mm,BA,CA,GA,Oo,Of,LA,UA,Te,zA,Fm,MA,FA,Km,KA,JA,WA,Po,Pf,YA,VA,je,XA,Jm,QA,ZA,Wm,eD,tD,sD,Ro,Rf,aD,nD,Hs,rD,Ym,oD,lD,iD,No,Nf,uD,cD,Bs,fD,Vm,pD,dD,hD,So,Sf,Xm,gD,mD,xf,qD,$D,xo,If,_D,vD,Cs,yD,Qm,ED,wD,bD,Io,Hf,TD,jD,Gs,kD,Zm,AD,DD,OD,Ho,Bf,PD,RD,Ls,ND,eq,SD,xD,z1,Cf,ID,M1,Us,F1,zs,tq,Bo,Gf,HD,BD,sq,CD,aq,Co,Lf,nq,GD,LD,Uf,UD,K1,Je,Ms,rq,Go,zD,oq,MD,J1,Fs,FD,zf,KD,JD,W1,We,Ks,lq,Lo,WD,iq,YD,Y1,Mf,VD,V1,Js,X1,Uo,XD,zo,QD,Q1,Ff,ZD,Z1,Ws,e2,Kf,eO,t2,Ys,uq,Mo,Jf,tO,sO,cq,aO,se,Fo,Ko,fq,nO,rO,oO,Wf,lO,iO,Jo,Yf,pq,uO,cO,Vf,fO,pO,Wo,Xf,dO,hO,Vs,gO,dq,mO,qO,$O,Yo,Qf,_O,vO,Xs,yO,hq,EO,wO,bO,Vo,Zf,TO,jO,Qs,kO,gq,AO,DO,s2,ep,OO,a2,Zs,n2,ea,mq,Xo,tp,PO,RO,qq,NO,fe,Qo,sp,$q,SO,xO,ap,IO,HO,Zo,np,_q,BO,CO,rp,GO,LO,el,op,vq,UO,zO,lp,MO,FO,tl,ip,yq,KO,JO,up,WO,r2,Ye,ta,Eq,sl,YO,wq,VO,o2,cp,XO,l2,sa,i2,aa,u2,pe,QO,al,ZO,eP,nl,tP,sP,rl,aP,c2,fp,nP,f2,na,p2,pp,rP,d2,ra,bq,ol,dp,oP,lP,Tq,iP,jq,ll,il,kq,uP,cP,fP,hp,pP,h2,gp,dP,g2,mp,hP,m2,oa,q2,la,Aq,ul,qp,gP,mP,Dq,qP,Oq,cl,$p,Pq,$P,_P,_p,vP,$2,Ve,ia,Rq,fl,yP,Nq,EP,_2,vp,wP,v2,ua,y2,Xe,bP,pl,TP,jP,dl,kP,E2,yp,AP,w2,ca,Sq,hl,Ep,DP,OP,xq,PP,ae,gl,ml,Iq,RP,NP,SP,wp,xP,IP,ql,bp,Hq,HP,BP,Tp,CP,GP,$l,jp,LP,UP,fa,zP,Bq,MP,FP,KP,_l,kp,JP,WP,pa,YP,Cq,VP,XP,QP,vl,Ap,ZP,eR,da,tR,Gq,sR,aR,b2,Dp,nR,T2,ha,Lq,yl,Op,rR,oR,Uq,lR,zq,El,Pp,Mq,iR,uR,Rp,cR,j2,Np,fR,k2,Qe,ga,Fq,wl,pR,Kq,dR,A2,Sp,hR,D2,ma,O2,Ze,gR,bl,mR,qR,Tl,$R,P2,xp,_R,R2,qa,N2,Ip,vR,S2,$a,Jq,jl,Hp,yR,ER,Wq,wR,Yq,kl,Al,Vq,bR,TR,jR,Bp,kR,x2,Cp,AR,I2,_a,H2,va,Xq,Dl,Gp,DR,OR,Qq,PR,Ol,Pl,Lp,Zq,RR,NR,Up,SR,xR,Rl,zp,e$,IR,HR,Mp,BR,B2,et,ya,t$,Nl,CR,s$,GR,C2,Fp,LR,G2,Ea,L2,Sl,UR,xl,zR,U2,Kp,MR,z2,wa,M2,ba,FR,Il,KR,JR,F2,Ta,a$,Hl,Jp,WR,YR,n$,VR,r$,Bl,Cl,o$,XR,QR,ZR,Wp,eN,K2,Yp,tN,J2,ja,W2,ka,l$,Gl,Vp,sN,aN,i$,nN,tt,Ll,Xp,u$,rN,oN,Qp,lN,iN,Ul,Zp,c$,uN,cN,ed,fN,pN,zl,td,f$,dN,hN,sd,gN,Y2,st,Aa,p$,Ml,mN,d$,qN,V2,ad,$N,X2,Da,Q2,Fl,_N,Kl,vN,Z2,nd,yN,ev,Oa,tv,Pa,EN,Jl,wN,bN,sv,Ra,h$,Wl,rd,TN,jN,g$,kN,m$,Yl,Vl,q$,AN,DN,ON,od,PN,av,ld,RN,nv,Na,rv,Sa,$$,Xl,id,NN,SN,_$,xN,at,Ql,ud,v$,IN,HN,cd,BN,CN,Zl,fd,y$,GN,LN,pd,UN,zN,ei,dd,E$,MN,FN,hd,KN,ov;return k=new F({}),Q=new F({}),za=new F({}),ut=new W({props:{$$slots:{default:[nM]},$$scope:{ctx:$}}}),ct=new U({props:{python:!0,js:!0,curl:!0,$$slots:{curl:[cM],js:[iM],python:[oM]},$$scope:{ctx:$}}}),mt=new U({props:{python:!0,js:!0,curl:!0,$$slots:{python:[pM]},$$scope:{ctx:$}}}),ln=new F({}),vt=new W({props:{$$slots:{default:[dM]},$$scope:{ctx:$}}}),yt=new U({props:{python:!0,js:!0,curl:!0,$$slots:{curl:[_M],js:[qM],python:[gM]},$$scope:{ctx:$}}}),vn=new F({}),Dt=new W({props:{$$slots:{default:[vM]},$$scope:{ctx:$}}}),Ot=new U({props:{python:!0,js:!0,curl:!0,$$slots:{curl:[jM],js:[bM],python:[EM]},$$scope:{ctx:$}}}),Gn=new F({}),Ct=new W({props:{$$slots:{default:[kM]},$$scope:{ctx:$}}}),Gt=new U({props:{python:!0,js:!0,curl:!0,$$slots:{curl:[NM],js:[PM],python:[DM]},$$scope:{ctx:$}}}),pr=new F({}),Vt=new W({props:{$$slots:{default:[SM]},$$scope:{ctx:$}}}),Xt=new U({props:{python:!0,js:!0,curl:!0,$$slots:{curl:[GM],js:[BM],python:[IM]},$$scope:{ctx:$}}}),ss=new U({props:{python:!0,js:!0,curl:!0,$$slots:{python:[UM]},$$scope:{ctx:$}}}),Dr=new F({}),rs=new W({props:{$$slots:{default:[zM]},$$scope:{ctx:$}}}),os=new U({props:{python:!0,js:!0,curl:!0,$$slots:{curl:[YM],js:[JM],python:[FM]},$$scope:{ctx:$}}}),ls=new U({props:{python:!0,js:!0,curl:!0,$$slots:{python:[XM]},$$scope:{ctx:$}}}),Hr=new F({}),ps=new W({props:{$$slots:{default:[QM]},$$scope:{ctx:$}}}),ds=new U({props:{python:!0,js:!0,curl:!0,$$slots:{curl:[nF],js:[sF],python:[eF]},$$scope:{ctx:$}}}),$s=new U({props:{python:!0,js:!0,curl:!0,$$slots:{python:[oF]},$$scope:{ctx:$}}}),Xr=new F({}),Zr=new F({}),Es=new W({props:{$$slots:{default:[lF]},$$scope:{ctx:$}}}),ws=new U({props:{python:!0,js:!0,curl:!0,$$slots:{curl:[dF],js:[fF],python:[uF]},$$scope:{ctx:$}}}),As=new U({props:{python:!0,js:!0,curl:!0,$$slots:{python:[gF]},$$scope:{ctx:$}}}),$o=new F({}),Ns=new W({props:{$$slots:{default:[mF]},$$scope:{ctx:$}}}),Ss=new U({props:{python:!0,js:!0,curl:!0,$$slots:{curl:[EF],js:[vF],python:[$F]},$$scope:{ctx:$}}}),Us=new U({props:{python:!0,js:!0,curl:!0,$$slots:{python:[bF]},$$scope:{ctx:$}}}),Go=new F({}),Lo=new F({}),Js=new W({props:{$$slots:{default:[TF]},$$scope:{ctx:$}}}),Ws=new U({props:{python:!0,js:!0,curl:!0,$$slots:{curl:[PF],js:[DF],python:[kF]},$$scope:{ctx:$}}}),Zs=new U({props:{python:!0,js:!0,curl:!0,$$slots:{python:[NF]},$$scope:{ctx:$}}}),sl=new F({}),sa=new W({props:{$$slots:{default:[SF]},$$scope:{ctx:$}}}),aa=new W({props:{$$slots:{default:[xF]},$$scope:{ctx:$}}}),na=new U({props:{python:!0,js:!0,curl:!0,$$slots:{curl:[LF],js:[CF],python:[HF]},$$scope:{ctx:$}}}),oa=new U({props:{python:!0,js:!0,curl:!0,$$slots:{python:[zF]},$$scope:{ctx:$}}}),fl=new F({}),ua=new W({props:{$$slots:{default:[MF]},$$scope:{ctx:$}}}),wl=new F({}),ma=new W({props:{$$slots:{default:[FF]},$$scope:{ctx:$}}}),qa=new U({props:{python:!0,js:!0,curl:!0,$$slots:{curl:[XF],js:[YF],python:[JF]},$$scope:{ctx:$}}}),_a=new U({props:{python:!0,js:!0,curl:!0,$$slots:{python:[ZF]},$$scope:{ctx:$}}}),Nl=new F({}),Ea=new W({props:{$$slots:{default:[eK]},$$scope:{ctx:$}}}),wa=new U({props:{python:!0,js:!0,curl:!0,$$slots:{curl:[oK],js:[nK],python:[sK]},$$scope:{ctx:$}}}),ja=new U({props:{python:!0,js:!0,curl:!0,$$slots:{python:[iK]},$$scope:{ctx:$}}}),Ml=new F({}),Da=new W({props:{$$slots:{default:[uK]},$$scope:{ctx:$}}}),Oa=new U({props:{python:!0,js:!0,curl:!0,$$slots:{curl:[gK],js:[dK],python:[fK]},$$scope:{ctx:$}}}),Na=new U({props:{python:!0,js:!0,curl:!0,$$slots:{python:[qK]},$$scope:{ctx:$}}}),{c(){n=r("meta"),c=f(),s=r("h1"),h=r("a"),q=r("span"),_(k.$$.fragment),A=f(),j=r("span"),T=i("Detailed parameters"),O=f(),D=r("h2"),ne=r("a"),Re=r("span"),_(Q.$$.fragment),Y=f(),rt=r("span"),pi=i("Which task is used by this model ?"),Ua=f(),Ne=r("p"),Hw=i(`In general the \u{1F917} Hosted API Inference accepts a simple string as an
input. However, more advanced usage depends on the \u201Ctask\u201D that the
model solves.`),Z$=f(),di=r("p"),Bw=i("The \u201Ctask\u201D of a model is defined here on it\u2019s model page:"),e_=f(),ot=r("img"),t_=f(),lt=r("img"),s_=f(),Se=r("h2"),it=r("a"),Sd=r("span"),_(za.$$.fragment),Cw=f(),xd=r("span"),Gw=i("Zero-shot classification task"),a_=f(),hi=r("p"),Lw=i(`This task is super useful to try out classification with zero code,
you simply pass a sentence/paragraph and the possible labels for that
sentence, and you get a result.`),n_=f(),_(ut.$$.fragment),r_=f(),Ma=r("p"),Uw=i("Available with: "),Fa=r("a"),zw=i("\u{1F917} Transformers"),o_=f(),gi=r("p"),Mw=i("Request:"),l_=f(),_(ct.$$.fragment),i_=f(),mi=r("p"),Fw=i(`When sending your request, you should send a JSON encoded payload. Here
are all the options`),u_=f(),ft=r("table"),Id=r("thead"),Ka=r("tr"),qi=r("th"),Kw=i("All parameters"),Jw=f(),Hd=r("th"),Ww=f(),z=r("tbody"),Ja=r("tr"),Wa=r("td"),Bd=r("strong"),Yw=i("inputs"),Vw=i(" (required)"),Xw=f(),$i=r("td"),Qw=i("a string or list of strings"),Zw=f(),Ya=r("tr"),Va=r("td"),Cd=r("strong"),eb=i("parameters"),tb=i(" (required)"),sb=f(),_i=r("td"),ab=i("a dict containing the following keys:"),nb=f(),Xa=r("tr"),vi=r("td"),rb=i("candidate_labels (required)"),ob=f(),de=r("td"),lb=i("a list of strings that are potential classes for "),Gd=r("code"),ib=i("inputs"),ub=i(". (max 10 candidate_labels, for more, simply run multiple requests, results are going to be misleading if using too many candidate_labels anyway. If you want to keep the exact same, you can simply run "),Ld=r("code"),cb=i("multi_label=True"),fb=i(" and do the scaling on your end. )"),pb=f(),Qa=r("tr"),yi=r("td"),db=i("multi_label"),hb=f(),pt=r("td"),gb=i("(Default: "),Ud=r("code"),mb=i("false"),qb=i(") Boolean that is set to True if classes can overlap"),$b=f(),Za=r("tr"),Ei=r("td"),zd=r("strong"),_b=i("options"),vb=f(),wi=r("td"),yb=i("a dict containing the following keys:"),Eb=f(),en=r("tr"),bi=r("td"),wb=i("use_gpu"),bb=f(),dt=r("td"),Tb=i("(Default: "),Md=r("code"),jb=i("false"),kb=i("). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),Ab=f(),tn=r("tr"),Ti=r("td"),Db=i("use_cache"),Ob=f(),ht=r("td"),Pb=i("(Default: "),Fd=r("code"),Rb=i("true"),Nb=i("). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),Sb=f(),sn=r("tr"),ji=r("td"),xb=i("wait_for_model"),Ib=f(),gt=r("td"),Hb=i("(Default: "),Kd=r("code"),Bb=i("false"),Cb=i(") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),c_=f(),ki=r("p"),Gb=i("Return value is either a dict or a list of dicts if you sent a list of inputs"),f_=f(),Ai=r("p"),Lb=i("Response:"),p_=f(),_(mt.$$.fragment),d_=f(),qt=r("table"),Jd=r("thead"),an=r("tr"),Di=r("th"),Ub=i("Returned values"),zb=f(),Wd=r("th"),Mb=f(),xe=r("tbody"),nn=r("tr"),Oi=r("td"),Yd=r("strong"),Fb=i("sequence"),Kb=f(),Pi=r("td"),Jb=i("The string sent as an input"),Wb=f(),rn=r("tr"),Ri=r("td"),Vd=r("strong"),Yb=i("labels"),Vb=f(),Ni=r("td"),Xb=i("The list of strings for labels that you sent (in order)"),Qb=f(),on=r("tr"),Si=r("td"),Xd=r("strong"),Zb=i("scores"),e3=f(),$t=r("td"),t3=i("a list of floats that correspond the the probability of label, in the same order as "),Qd=r("code"),s3=i("labels"),a3=i("."),h_=f(),Ie=r("h2"),_t=r("a"),Zd=r("span"),_(ln.$$.fragment),n3=f(),eh=r("span"),r3=i("Translation task"),g_=f(),xi=r("p"),o3=i("This task is well known to translate text from one language to another"),m_=f(),_(vt.$$.fragment),q_=f(),un=r("p"),l3=i("Available with: "),cn=r("a"),i3=i("\u{1F917} Transformers"),$_=f(),Ii=r("p"),u3=i("Example:"),__=f(),_(yt.$$.fragment),v_=f(),Hi=r("p"),c3=i(`When sending your request, you should send a JSON encoded payload. Here
are all the options`),y_=f(),Et=r("table"),th=r("thead"),fn=r("tr"),Bi=r("th"),f3=i("All parameters"),p3=f(),sh=r("th"),d3=f(),Z=r("tbody"),pn=r("tr"),dn=r("td"),ah=r("strong"),h3=i("inputs"),g3=i(" (required)"),m3=f(),Ci=r("td"),q3=i("a string to be translated in the original languages"),$3=f(),hn=r("tr"),Gi=r("td"),nh=r("strong"),_3=i("options"),v3=f(),Li=r("td"),y3=i("a dict containing the following keys:"),E3=f(),gn=r("tr"),Ui=r("td"),w3=i("use_gpu"),b3=f(),wt=r("td"),T3=i("(Default: "),rh=r("code"),j3=i("false"),k3=i("). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),A3=f(),mn=r("tr"),zi=r("td"),D3=i("use_cache"),O3=f(),bt=r("td"),P3=i("(Default: "),oh=r("code"),R3=i("true"),N3=i("). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),S3=f(),qn=r("tr"),Mi=r("td"),x3=i("wait_for_model"),I3=f(),Tt=r("td"),H3=i("(Default: "),lh=r("code"),B3=i("false"),C3=i(") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),E_=f(),Fi=r("p"),G3=i("Return value is either a dict or a list of dicts if you sent a list of inputs"),w_=f(),jt=r("table"),ih=r("thead"),$n=r("tr"),Ki=r("th"),L3=i("Returned values"),U3=f(),uh=r("th"),z3=f(),ch=r("tbody"),_n=r("tr"),Ji=r("td"),fh=r("strong"),M3=i("translation_text"),F3=f(),Wi=r("td"),K3=i("The string after translation"),b_=f(),He=r("h2"),kt=r("a"),ph=r("span"),_(vn.$$.fragment),J3=f(),dh=r("span"),W3=i("Summarization task"),T_=f(),At=r("p"),Y3=i(`This task is well known to summarize longer text into shorter text.
Be careful, some models have a maximum length of input. That means that
the summary cannot handle full books for instance. Be careful when
choosing your model. If you want to discuss your summarization needs,
please get in touch with us: <`),Yi=r("a"),V3=i("api-enterprise@huggingface.co"),X3=i(">"),j_=f(),_(Dt.$$.fragment),k_=f(),yn=r("p"),Q3=i("Available with: "),En=r("a"),Z3=i("\u{1F917} Transformers"),A_=f(),Vi=r("p"),eT=i("Example:"),D_=f(),_(Ot.$$.fragment),O_=f(),Xi=r("p"),tT=i(`When sending your request, you should send a JSON encoded payload. Here
are all the options`),P_=f(),Pt=r("table"),hh=r("thead"),wn=r("tr"),Qi=r("th"),sT=i("All parameters"),aT=f(),gh=r("th"),nT=f(),G=r("tbody"),bn=r("tr"),Tn=r("td"),mh=r("strong"),rT=i("inputs"),oT=i(" (required)"),lT=f(),Zi=r("td"),iT=i("a string to be summarized"),uT=f(),jn=r("tr"),eu=r("td"),qh=r("strong"),cT=i("parameters"),fT=f(),tu=r("td"),pT=i("a dict containing the following keys:"),dT=f(),kn=r("tr"),su=r("td"),hT=i("min_length"),gT=f(),he=r("td"),mT=i("(Default: "),$h=r("code"),qT=i("None"),$T=i("). Integer to define the minimum length "),_h=r("strong"),_T=i("in tokens"),vT=i(" of the output summary."),yT=f(),An=r("tr"),au=r("td"),ET=i("max_length"),wT=f(),ge=r("td"),bT=i("(Default: "),vh=r("code"),TT=i("None"),jT=i("). Integer to define the maximum length "),yh=r("strong"),kT=i("in tokens"),AT=i(" of the output summary."),DT=f(),Dn=r("tr"),nu=r("td"),OT=i("top_k"),PT=f(),me=r("td"),RT=i("(Default: "),Eh=r("code"),NT=i("None"),ST=i("). Integer to define the top tokens considered within the "),wh=r("code"),xT=i("sample"),IT=i(" operation to create new text."),HT=f(),On=r("tr"),ru=r("td"),BT=i("top_p"),CT=f(),re=r("td"),GT=i("(Default: "),bh=r("code"),LT=i("None"),UT=i("). Float to define the tokens that are within the "),Th=r("code"),zT=i("sample"),MT=i(" operation of text generation. Add tokens in the sample for more probable to least probable until the sum of the probabilities is greater than "),jh=r("code"),FT=i("top_p"),KT=i("."),JT=f(),Pn=r("tr"),ou=r("td"),WT=i("temperature"),YT=f(),qe=r("td"),VT=i("(Default: "),kh=r("code"),XT=i("1.0"),QT=i("). Float (0.0-100.0). The temperature of the sampling operation. 1 means regular sampling, 0 means "),Ah=r("code"),ZT=i("100.0"),e5=i(" is getting closer to uniform probability."),t5=f(),Rn=r("tr"),lu=r("td"),s5=i("repetition_penalty"),a5=f(),Rt=r("td"),n5=i("(Default: "),Dh=r("code"),r5=i("None"),o5=i("). Float (0.0-100.0). The more a token is used within generation the more it is penalized to not be picked in successive generation passes."),l5=f(),Nn=r("tr"),iu=r("td"),i5=i("max_time"),u5=f(),Nt=r("td"),c5=i("(Default: "),Oh=r("code"),f5=i("None"),p5=i("). Float (0-120.0). The amount of time in seconds that the query should take maximum. Network can cause some overhead so it will be a soft limit."),d5=f(),Sn=r("tr"),uu=r("td"),Ph=r("strong"),h5=i("options"),g5=f(),cu=r("td"),m5=i("a dict containing the following keys:"),q5=f(),xn=r("tr"),fu=r("td"),$5=i("use_gpu"),_5=f(),St=r("td"),v5=i("(Default: "),Rh=r("code"),y5=i("false"),E5=i("). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),w5=f(),In=r("tr"),pu=r("td"),b5=i("use_cache"),T5=f(),xt=r("td"),j5=i("(Default: "),Nh=r("code"),k5=i("true"),A5=i("). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),D5=f(),Hn=r("tr"),du=r("td"),O5=i("wait_for_model"),P5=f(),It=r("td"),R5=i("(Default: "),Sh=r("code"),N5=i("false"),S5=i(") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),R_=f(),hu=r("p"),x5=i("Return value is either a dict or a list of dicts if you sent a list of inputs"),N_=f(),Ht=r("table"),xh=r("thead"),Bn=r("tr"),gu=r("th"),I5=i("Returned values"),H5=f(),Ih=r("th"),B5=f(),Hh=r("tbody"),Cn=r("tr"),mu=r("td"),Bh=r("strong"),C5=i("summarization_text"),G5=f(),qu=r("td"),L5=i("The string after translation"),S_=f(),Be=r("h2"),Bt=r("a"),Ch=r("span"),_(Gn.$$.fragment),U5=f(),Gh=r("span"),z5=i("Conversational task"),x_=f(),$u=r("p"),M5=i(`This task corresponds to any chatbot like structure. Models tend to have
shorter max_length, so please check with caution when using a given
model if you need long range dependency or not.`),I_=f(),_(Ct.$$.fragment),H_=f(),Ln=r("p"),F5=i("Available with: "),Un=r("a"),K5=i("\u{1F917} Transformers"),B_=f(),_u=r("p"),J5=i("Example:"),C_=f(),_(Gt.$$.fragment),G_=f(),vu=r("p"),W5=i(`When sending your request, you should send a JSON encoded payload. Here
are all the options`),L_=f(),Lt=r("table"),Lh=r("thead"),zn=r("tr"),yu=r("th"),Y5=i("All parameters"),V5=f(),Uh=r("th"),X5=f(),S=r("tbody"),Mn=r("tr"),Fn=r("td"),zh=r("strong"),Q5=i("inputs"),Z5=i(" (required)"),e4=f(),Mh=r("td"),t4=f(),Kn=r("tr"),Eu=r("td"),s4=i("text (required)"),a4=f(),wu=r("td"),n4=i("The last input from the user in the conversation."),r4=f(),Jn=r("tr"),bu=r("td"),o4=i("generated_responses"),l4=f(),Tu=r("td"),i4=i("A list of strings corresponding to the earlier replies from the model."),u4=f(),Wn=r("tr"),ju=r("td"),c4=i("past_user_inputs"),f4=f(),Ut=r("td"),p4=i("A list of strings corresponding to the earlier replies from the user. Should be of the same length of "),Fh=r("code"),d4=i("generated_responses"),h4=i("."),g4=f(),Yn=r("tr"),ku=r("td"),Kh=r("strong"),m4=i("parameters"),q4=f(),Au=r("td"),$4=i("a dict containing the following keys:"),_4=f(),Vn=r("tr"),Du=r("td"),v4=i("min_length"),y4=f(),$e=r("td"),E4=i("(Default: "),Jh=r("code"),w4=i("None"),b4=i("). Integer to define the minimum length "),Wh=r("strong"),T4=i("in tokens"),j4=i(" of the output summary."),k4=f(),Xn=r("tr"),Ou=r("td"),A4=i("max_length"),D4=f(),_e=r("td"),O4=i("(Default: "),Yh=r("code"),P4=i("None"),R4=i("). Integer to define the maximum length "),Vh=r("strong"),N4=i("in tokens"),S4=i(" of the output summary."),x4=f(),Qn=r("tr"),Pu=r("td"),I4=i("top_k"),H4=f(),ve=r("td"),B4=i("(Default: "),Xh=r("code"),C4=i("None"),G4=i("). Integer to define the top tokens considered within the "),Qh=r("code"),L4=i("sample"),U4=i(" operation to create new text."),z4=f(),Zn=r("tr"),Ru=r("td"),M4=i("top_p"),F4=f(),oe=r("td"),K4=i("(Default: "),Zh=r("code"),J4=i("None"),W4=i("). Float to define the tokens that are within the "),eg=r("code"),Y4=i("sample"),V4=i(" operation of text generation. Add tokens in the sample for more probable to least probable until the sum of the probabilities is greater than "),tg=r("code"),X4=i("top_p"),Q4=i("."),Z4=f(),er=r("tr"),Nu=r("td"),ej=i("temperature"),tj=f(),ye=r("td"),sj=i("(Default: "),sg=r("code"),aj=i("1.0"),nj=i("). Float (0.0-100.0). The temperature of the sampling operation. 1 means regular sampling, 0 means "),ag=r("code"),rj=i("100.0"),oj=i(" is getting closer to uniform probability."),lj=f(),tr=r("tr"),Su=r("td"),ij=i("repetition_penalty"),uj=f(),zt=r("td"),cj=i("(Default: "),ng=r("code"),fj=i("None"),pj=i("). Float (0.0-100.0). The more a token is used within generation the more it is penalized to not be picked in successive generation passes."),dj=f(),sr=r("tr"),xu=r("td"),hj=i("max_time"),gj=f(),Mt=r("td"),mj=i("(Default: "),rg=r("code"),qj=i("None"),$j=i("). Float (0-120.0). The amount of time in seconds that the query should take maximum. Network can cause some overhead so it will be a soft limit."),_j=f(),ar=r("tr"),Iu=r("td"),og=r("strong"),vj=i("options"),yj=f(),Hu=r("td"),Ej=i("a dict containing the following keys:"),wj=f(),nr=r("tr"),Bu=r("td"),bj=i("use_gpu"),Tj=f(),Ft=r("td"),jj=i("(Default: "),lg=r("code"),kj=i("false"),Aj=i("). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),Dj=f(),rr=r("tr"),Cu=r("td"),Oj=i("use_cache"),Pj=f(),Kt=r("td"),Rj=i("(Default: "),ig=r("code"),Nj=i("true"),Sj=i("). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),xj=f(),or=r("tr"),Gu=r("td"),Ij=i("wait_for_model"),Hj=f(),Jt=r("td"),Bj=i("(Default: "),ug=r("code"),Cj=i("false"),Gj=i(") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),U_=f(),Lu=r("p"),Lj=i("Return value is either a dict or a list of dicts if you sent a list of inputs"),z_=f(),Wt=r("table"),cg=r("thead"),lr=r("tr"),Uu=r("th"),Uj=i("Returned values"),zj=f(),fg=r("th"),Mj=f(),ie=r("tbody"),ir=r("tr"),zu=r("td"),pg=r("strong"),Fj=i("generated_text"),Kj=f(),Mu=r("td"),Jj=i("The answer of the bot"),Wj=f(),ur=r("tr"),Fu=r("td"),dg=r("strong"),Yj=i("conversation"),Vj=f(),Ku=r("td"),Xj=i("A facility dictionnary to send back for the next input (with the new user input addition)."),Qj=f(),cr=r("tr"),Ju=r("td"),Zj=i("past_user_inputs"),ek=f(),Wu=r("td"),tk=i("List of strings. The last inputs from the user in the conversation, <em>after the model has run."),sk=f(),fr=r("tr"),Yu=r("td"),ak=i("generated_responses"),nk=f(),Vu=r("td"),rk=i("List of strings. The last outputs from the model in the conversation, <em>after the model has run."),M_=f(),Ce=r("h2"),Yt=r("a"),hg=r("span"),_(pr.$$.fragment),ok=f(),gg=r("span"),lk=i("Table question answering task"),F_=f(),Xu=r("p"),ik=i(`Don\u2019t know SQL? Don\u2019t want to dive into a large spreadsheet? Ask
questions in plain english!`),K_=f(),_(Vt.$$.fragment),J_=f(),dr=r("p"),uk=i("Available with: "),hr=r("a"),ck=i("\u{1F917} Transformers"),W_=f(),Qu=r("p"),fk=i("Example:"),Y_=f(),_(Xt.$$.fragment),V_=f(),Zu=r("p"),pk=i(`When sending your request, you should send a JSON encoded payload. Here
are all the options`),X_=f(),Qt=r("table"),mg=r("thead"),gr=r("tr"),ec=r("th"),dk=i("All parameters"),hk=f(),qg=r("th"),gk=f(),K=r("tbody"),mr=r("tr"),qr=r("td"),$g=r("strong"),mk=i("inputs"),qk=i(" (required)"),$k=f(),_g=r("td"),_k=f(),$r=r("tr"),tc=r("td"),vk=i("query (required)"),yk=f(),sc=r("td"),Ek=i("The query in plain text that you want to ask the table"),wk=f(),_r=r("tr"),ac=r("td"),bk=i("table (required)"),Tk=f(),nc=r("td"),jk=i("A table of data represented as a dict of list where entries are headers and the lists are all the values, all lists must have the same size."),kk=f(),vr=r("tr"),rc=r("td"),vg=r("strong"),Ak=i("options"),Dk=f(),oc=r("td"),Ok=i("a dict containing the following keys:"),Pk=f(),yr=r("tr"),lc=r("td"),Rk=i("use_gpu"),Nk=f(),Zt=r("td"),Sk=i("(Default: "),yg=r("code"),xk=i("false"),Ik=i("). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),Hk=f(),Er=r("tr"),ic=r("td"),Bk=i("use_cache"),Ck=f(),es=r("td"),Gk=i("(Default: "),Eg=r("code"),Lk=i("true"),Uk=i("). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),zk=f(),wr=r("tr"),uc=r("td"),Mk=i("wait_for_model"),Fk=f(),ts=r("td"),Kk=i("(Default: "),wg=r("code"),Jk=i("false"),Wk=i(") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),Q_=f(),cc=r("p"),Yk=i("Return value is either a dict or a list of dicts if you sent a list of inputs"),Z_=f(),_(ss.$$.fragment),e1=f(),as=r("table"),bg=r("thead"),br=r("tr"),fc=r("th"),Vk=i("Returned values"),Xk=f(),Tg=r("th"),Qk=f(),ue=r("tbody"),Tr=r("tr"),pc=r("td"),jg=r("strong"),Zk=i("answer"),e6=f(),dc=r("td"),t6=i("The plaintext answer"),s6=f(),jr=r("tr"),hc=r("td"),kg=r("strong"),a6=i("coordinates"),n6=f(),gc=r("td"),r6=i("a list of coordinates of the cells referenced in the answer"),o6=f(),kr=r("tr"),mc=r("td"),Ag=r("strong"),l6=i("cells"),i6=f(),qc=r("td"),u6=i("a list of coordinates of the cells contents"),c6=f(),Ar=r("tr"),$c=r("td"),Dg=r("strong"),f6=i("aggregator"),p6=f(),_c=r("td"),d6=i("The aggregator used to get the answer"),t1=f(),Ge=r("h2"),ns=r("a"),Og=r("span"),_(Dr.$$.fragment),h6=f(),Pg=r("span"),g6=i("Question answering task"),s1=f(),vc=r("p"),m6=i("Want to have a nice know-it-all bot that can answer any question?"),a1=f(),_(rs.$$.fragment),n1=f(),Le=r("p"),q6=i("Available with: "),Or=r("a"),$6=i("\u{1F917}Transformers"),_6=i(` and
`),Pr=r("a"),v6=i("AllenNLP"),r1=f(),yc=r("p"),y6=i("Example:"),o1=f(),_(os.$$.fragment),l1=f(),Ec=r("p"),E6=i(`When sending your request, you should send a JSON encoded payload. Here
are all the options`),i1=f(),wc=r("p"),w6=i("Return value is either a dict or a list of dicts if you sent a list of inputs"),u1=f(),_(ls.$$.fragment),c1=f(),is=r("table"),Rg=r("thead"),Rr=r("tr"),bc=r("th"),b6=i("Returned values"),T6=f(),Ng=r("th"),j6=f(),ce=r("tbody"),Nr=r("tr"),Tc=r("td"),Sg=r("strong"),k6=i("answer"),A6=f(),jc=r("td"),D6=i("A string that\u2019s the answer within the text."),O6=f(),Sr=r("tr"),kc=r("td"),xg=r("strong"),P6=i("score"),R6=f(),Ac=r("td"),N6=i("A float that represents how likely that the answer is correct"),S6=f(),xr=r("tr"),Dc=r("td"),Ig=r("strong"),x6=i("start"),I6=f(),us=r("td"),H6=i("The index (string wise) of the start of the answer within "),Hg=r("code"),B6=i("context"),C6=i("."),G6=f(),Ir=r("tr"),Oc=r("td"),Bg=r("strong"),L6=i("stop"),U6=f(),cs=r("td"),z6=i("The index (string wise) of the stop of the answer within "),Cg=r("code"),M6=i("context"),F6=i("."),f1=f(),Ue=r("h2"),fs=r("a"),Gg=r("span"),_(Hr.$$.fragment),K6=f(),Lg=r("span"),J6=i("Text-classification task"),p1=f(),Pc=r("p"),W6=i(`Usually used for sentiment-analysis this will output the likelihood of
classes of an input.`),d1=f(),_(ps.$$.fragment),h1=f(),Br=r("p"),Y6=i("Available with: "),Cr=r("a"),V6=i("\u{1F917} Transformers"),g1=f(),Rc=r("p"),X6=i("Example:"),m1=f(),_(ds.$$.fragment),q1=f(),Nc=r("p"),Q6=i(`When sending your request, you should send a JSON encoded payload. Here
are all the options`),$1=f(),hs=r("table"),Ug=r("thead"),Gr=r("tr"),Sc=r("th"),Z6=i("All parameters"),e7=f(),zg=r("th"),t7=f(),ee=r("tbody"),Lr=r("tr"),Ur=r("td"),Mg=r("strong"),s7=i("inputs"),a7=i(" (required)"),n7=f(),xc=r("td"),r7=i("a string to be classified"),o7=f(),zr=r("tr"),Ic=r("td"),Fg=r("strong"),l7=i("options"),i7=f(),Hc=r("td"),u7=i("a dict containing the following keys:"),c7=f(),Mr=r("tr"),Bc=r("td"),f7=i("use_gpu"),p7=f(),gs=r("td"),d7=i("(Default: "),Kg=r("code"),h7=i("false"),g7=i("). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),m7=f(),Fr=r("tr"),Cc=r("td"),q7=i("use_cache"),$7=f(),ms=r("td"),_7=i("(Default: "),Jg=r("code"),v7=i("true"),y7=i("). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),E7=f(),Kr=r("tr"),Gc=r("td"),w7=i("wait_for_model"),b7=f(),qs=r("td"),T7=i("(Default: "),Wg=r("code"),j7=i("false"),k7=i(") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),_1=f(),Lc=r("p"),A7=i("Return value is either a dict or a list of dicts if you sent a list of inputs"),v1=f(),_($s.$$.fragment),y1=f(),_s=r("table"),Yg=r("thead"),Jr=r("tr"),Uc=r("th"),D7=i("Returned values"),O7=f(),Vg=r("th"),P7=f(),Wr=r("tbody"),Yr=r("tr"),zc=r("td"),Xg=r("strong"),R7=i("label"),N7=f(),Mc=r("td"),S7=i("The label for the class (model specific)"),x7=f(),Vr=r("tr"),Fc=r("td"),Qg=r("strong"),I7=i("score"),H7=f(),Kc=r("td"),B7=i("A floats that represents how likely is that the text belongs the this class."),E1=f(),ze=r("h2"),vs=r("a"),Zg=r("span"),_(Xr.$$.fragment),C7=f(),em=r("span"),G7=i("Named Entity Recognition (NER) task"),w1=f(),Qr=r("p"),L7=i("See "),Jc=r("a"),U7=i("Token-classification task"),b1=f(),Me=r("h2"),ys=r("a"),tm=r("span"),_(Zr.$$.fragment),z7=f(),sm=r("span"),M7=i("Token-classification task"),T1=f(),Wc=r("p"),F7=i(`Usually used for sentence parsing, either grammatical, or Named Entity
Recognition (NER) to understand keywords contained within text.`),j1=f(),_(Es.$$.fragment),k1=f(),Fe=r("p"),K7=i("Available with: "),eo=r("a"),J7=i("\u{1F917} Transformers"),W7=i(`,
`),to=r("a"),Y7=i("Flair"),A1=f(),Yc=r("p"),V7=i("Example:"),D1=f(),_(ws.$$.fragment),O1=f(),Vc=r("p"),X7=i(`When sending your request, you should send a JSON encoded payload. Here
are all the options`),P1=f(),bs=r("table"),am=r("thead"),so=r("tr"),Xc=r("th"),Q7=i("All parameters"),Z7=f(),nm=r("th"),e9=f(),J=r("tbody"),ao=r("tr"),no=r("td"),rm=r("strong"),t9=i("inputs"),s9=i(" (required)"),a9=f(),Qc=r("td"),n9=i("a string to be classified"),r9=f(),ro=r("tr"),Zc=r("td"),om=r("strong"),o9=i("parameters"),l9=f(),ef=r("td"),i9=i("a dict containing the following key:"),u9=f(),oo=r("tr"),tf=r("td"),c9=i("aggregation_strategy"),f9=f(),x=r("td"),p9=i("(Default: "),lm=r("code"),d9=i("simple"),h9=i("). There are several aggregation strategies: "),g9=r("br"),m9=f(),im=r("code"),q9=i("none"),$9=i(": Every token gets classified without further aggregation. "),_9=r("br"),v9=f(),um=r("code"),y9=i("simple"),E9=i(": Entities are grouped according to the default schema (B-, I- tags get merged when the tag is similar). "),w9=r("br"),b9=f(),cm=r("code"),T9=i("first"),j9=i(": Same as the "),fm=r("code"),k9=i("simple"),A9=i(" strategy except words cannot end up with different tags. Words will use the tag of the first token when there is ambiguity. "),D9=r("br"),O9=f(),pm=r("code"),P9=i("average"),R9=i(": Same as the "),dm=r("code"),N9=i("simple"),S9=i(" strategy except words cannot end up with different tags. Scores are averaged across tokens and then the maximum label is applied. "),x9=r("br"),I9=f(),hm=r("code"),H9=i("max"),B9=i(": Same as the "),gm=r("code"),C9=i("simple"),G9=i(" strategy except words cannot end up with different tags. Word entity will be the token with the maximum score."),L9=f(),lo=r("tr"),sf=r("td"),mm=r("strong"),U9=i("options"),z9=f(),af=r("td"),M9=i("a dict containing the following keys:"),F9=f(),io=r("tr"),nf=r("td"),K9=i("use_gpu"),J9=f(),Ts=r("td"),W9=i("(Default: "),qm=r("code"),Y9=i("false"),V9=i("). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),X9=f(),uo=r("tr"),rf=r("td"),Q9=i("use_cache"),Z9=f(),js=r("td"),e8=i("(Default: "),$m=r("code"),t8=i("true"),s8=i("). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),a8=f(),co=r("tr"),of=r("td"),n8=i("wait_for_model"),r8=f(),ks=r("td"),o8=i("(Default: "),_m=r("code"),l8=i("false"),i8=i(") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),R1=f(),lf=r("p"),u8=i("Return value is either a dict or a list of dicts if you sent a list of inputs"),N1=f(),_(As.$$.fragment),S1=f(),Ds=r("table"),vm=r("thead"),fo=r("tr"),uf=r("th"),c8=i("Returned values"),f8=f(),ym=r("th"),p8=f(),te=r("tbody"),po=r("tr"),cf=r("td"),Em=r("strong"),d8=i("entity_group"),h8=f(),ff=r("td"),g8=i("The type for the entity being recognized (model specific)."),m8=f(),ho=r("tr"),pf=r("td"),wm=r("strong"),q8=i("score"),$8=f(),df=r("td"),_8=i("How likely the entity was recognized."),v8=f(),go=r("tr"),hf=r("td"),bm=r("strong"),y8=i("word"),E8=f(),gf=r("td"),w8=i("The string that was captured"),b8=f(),mo=r("tr"),mf=r("td"),Tm=r("strong"),T8=i("start"),j8=f(),Os=r("td"),k8=i("The offset stringwise where the answer is located. Useful to disambiguate if "),jm=r("code"),A8=i("word"),D8=i(" occurs multiple times."),O8=f(),qo=r("tr"),qf=r("td"),km=r("strong"),P8=i("end"),R8=f(),Ps=r("td"),N8=i("The offset stringwise where the answer is located. Useful to disambiguate if "),Am=r("code"),S8=i("word"),x8=i(" occurs multiple times."),x1=f(),Ke=r("h2"),Rs=r("a"),Dm=r("span"),_($o.$$.fragment),I8=f(),Om=r("span"),H8=i("Text-generation task"),I1=f(),$f=r("p"),B8=i("Use to continue text from a prompt. This is a very generic task."),H1=f(),_(Ns.$$.fragment),B1=f(),_o=r("p"),C8=i("Available with: "),vo=r("a"),G8=i("\u{1F917} Transformers"),C1=f(),_f=r("p"),L8=i("Example:"),G1=f(),_(Ss.$$.fragment),L1=f(),vf=r("p"),U8=i(`When sending your request, you should send a JSON encoded payload. Here
are all the options`),U1=f(),xs=r("table"),Pm=r("thead"),yo=r("tr"),yf=r("th"),z8=i("All parameters"),M8=f(),Rm=r("th"),F8=f(),I=r("tbody"),Eo=r("tr"),wo=r("td"),Nm=r("strong"),K8=i("inputs"),J8=i(" (required):"),W8=f(),Ef=r("td"),Y8=i("a string to be generated from"),V8=f(),bo=r("tr"),wf=r("td"),Sm=r("strong"),X8=i("parameters"),Q8=f(),bf=r("td"),Z8=i("dict containing the following keys:"),eA=f(),To=r("tr"),Tf=r("td"),tA=i("top_k"),sA=f(),Ee=r("td"),aA=i("(Default: "),xm=r("code"),nA=i("None"),rA=i("). Integer to define the top tokens considered within the "),Im=r("code"),oA=i("sample"),lA=i(" operation to create new text."),iA=f(),jo=r("tr"),jf=r("td"),uA=i("top_p"),cA=f(),le=r("td"),fA=i("(Default: "),Hm=r("code"),pA=i("None"),dA=i("). Float to define the tokens that are within the "),Bm=r("code"),hA=i("sample"),gA=i(" operation of text generation. Add tokens in the sample for more probable to least probable until the sum of the probabilities is greater than "),Cm=r("code"),mA=i("top_p"),qA=i("."),$A=f(),ko=r("tr"),kf=r("td"),_A=i("temperature"),vA=f(),we=r("td"),yA=i("(Default: "),Gm=r("code"),EA=i("1.0"),wA=i("). Float (0.0-100.0). The temperature of the sampling operation. 1 means regular sampling, 0 means "),Lm=r("code"),bA=i("100.0"),TA=i(" is getting closer to uniform probability."),jA=f(),Ao=r("tr"),Af=r("td"),kA=i("repetition_penalty"),AA=f(),Is=r("td"),DA=i("(Default: "),Um=r("code"),OA=i("None"),PA=i("). Float (0.0-100.0). The more a token is used within generation the more it is penalized to not be picked in successive generation passes."),RA=f(),Do=r("tr"),Df=r("td"),NA=i("max_new_tokens"),SA=f(),be=r("td"),xA=i("(Default: "),zm=r("code"),IA=i("None"),HA=i("). Int (0-250). The amount of new tokens to be generated, this does "),Mm=r("strong"),BA=i("not"),CA=i(" include the input length it is a estimate of the size of generated text you want. Each new tokens slows down the request, so look for balance between response times and length of text generated."),GA=f(),Oo=r("tr"),Of=r("td"),LA=i("max_time"),UA=f(),Te=r("td"),zA=i("(Default: "),Fm=r("code"),MA=i("None"),FA=i("). Float (0-120.0). The amount of time in seconds that the query should take maximum. Network can cause some overhead so it will be a soft limit. Use that in combination with "),Km=r("code"),KA=i("max_new_tokens"),JA=i(" for best results."),WA=f(),Po=r("tr"),Pf=r("td"),YA=i("return_full_text"),VA=f(),je=r("td"),XA=i("(Default: "),Jm=r("code"),QA=i("True"),ZA=i("). Bool. If set to False, the return results will "),Wm=r("strong"),eD=i("not"),tD=i(" contain the original query making it easier for prompting."),sD=f(),Ro=r("tr"),Rf=r("td"),aD=i("num_return_sequences"),nD=f(),Hs=r("td"),rD=i("(Default: "),Ym=r("code"),oD=i("1"),lD=i("). Integer. The number of proposition you want to be returned."),iD=f(),No=r("tr"),Nf=r("td"),uD=i("do_sample"),cD=f(),Bs=r("td"),fD=i("(Optional: "),Vm=r("code"),pD=i("True"),dD=i("). Bool. Whether or not to use sampling, use greedy decoding otherwise."),hD=f(),So=r("tr"),Sf=r("td"),Xm=r("strong"),gD=i("options"),mD=f(),xf=r("td"),qD=i("a dict containing the following keys:"),$D=f(),xo=r("tr"),If=r("td"),_D=i("use_gpu"),vD=f(),Cs=r("td"),yD=i("(Default: "),Qm=r("code"),ED=i("false"),wD=i("). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),bD=f(),Io=r("tr"),Hf=r("td"),TD=i("use_cache"),jD=f(),Gs=r("td"),kD=i("(Default: "),Zm=r("code"),AD=i("true"),DD=i("). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),OD=f(),Ho=r("tr"),Bf=r("td"),PD=i("wait_for_model"),RD=f(),Ls=r("td"),ND=i("(Default: "),eq=r("code"),SD=i("false"),xD=i(") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),z1=f(),Cf=r("p"),ID=i("Return value is either a dict or a list of dicts if you sent a list of inputs"),M1=f(),_(Us.$$.fragment),F1=f(),zs=r("table"),tq=r("thead"),Bo=r("tr"),Gf=r("th"),HD=i("Returned values"),BD=f(),sq=r("th"),CD=f(),aq=r("tbody"),Co=r("tr"),Lf=r("td"),nq=r("strong"),GD=i("generated_text"),LD=f(),Uf=r("td"),UD=i("The continuated string"),K1=f(),Je=r("h2"),Ms=r("a"),rq=r("span"),_(Go.$$.fragment),zD=f(),oq=r("span"),MD=i("Text2text-generation task"),J1=f(),Fs=r("p"),FD=i("Essentially "),zf=r("a"),KD=i("Text-generation task"),JD=i(`. But uses
Encoder-Decoder architecture, so might change in the future for more
options.`),W1=f(),We=r("h2"),Ks=r("a"),lq=r("span"),_(Lo.$$.fragment),WD=f(),iq=r("span"),YD=i("Fill mask task"),Y1=f(),Mf=r("p"),VD=i(`Tries to fill in a hole with a missing word (token to be precise).
That\u2019s the base task for BERT models.`),V1=f(),_(Js.$$.fragment),X1=f(),Uo=r("p"),XD=i("Available with: "),zo=r("a"),QD=i("\u{1F917} Transformers"),Q1=f(),Ff=r("p"),ZD=i("Example:"),Z1=f(),_(Ws.$$.fragment),e2=f(),Kf=r("p"),eO=i(`When sending your request, you should send a JSON encoded payload. Here
are all the options`),t2=f(),Ys=r("table"),uq=r("thead"),Mo=r("tr"),Jf=r("th"),tO=i("All parameters"),sO=f(),cq=r("th"),aO=f(),se=r("tbody"),Fo=r("tr"),Ko=r("td"),fq=r("strong"),nO=i("inputs"),rO=i(" (required):"),oO=f(),Wf=r("td"),lO=i("a string to be filled from, must contain the [MASK] token (check model card for exact name of the mask)"),iO=f(),Jo=r("tr"),Yf=r("td"),pq=r("strong"),uO=i("options"),cO=f(),Vf=r("td"),fO=i("a dict containing the following keys:"),pO=f(),Wo=r("tr"),Xf=r("td"),dO=i("use_gpu"),hO=f(),Vs=r("td"),gO=i("(Default: "),dq=r("code"),mO=i("false"),qO=i("). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),$O=f(),Yo=r("tr"),Qf=r("td"),_O=i("use_cache"),vO=f(),Xs=r("td"),yO=i("(Default: "),hq=r("code"),EO=i("true"),wO=i("). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),bO=f(),Vo=r("tr"),Zf=r("td"),TO=i("wait_for_model"),jO=f(),Qs=r("td"),kO=i("(Default: "),gq=r("code"),AO=i("false"),DO=i(") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),s2=f(),ep=r("p"),OO=i("Return value is either a dict or a list of dicts if you sent a list of inputs"),a2=f(),_(Zs.$$.fragment),n2=f(),ea=r("table"),mq=r("thead"),Xo=r("tr"),tp=r("th"),PO=i("Returned values"),RO=f(),qq=r("th"),NO=f(),fe=r("tbody"),Qo=r("tr"),sp=r("td"),$q=r("strong"),SO=i("sequence"),xO=f(),ap=r("td"),IO=i("The actual sequence of tokens that ran against the model (may contain special tokens)"),HO=f(),Zo=r("tr"),np=r("td"),_q=r("strong"),BO=i("score"),CO=f(),rp=r("td"),GO=i("The probability for this token."),LO=f(),el=r("tr"),op=r("td"),vq=r("strong"),UO=i("token"),zO=f(),lp=r("td"),MO=i("The id of the token"),FO=f(),tl=r("tr"),ip=r("td"),yq=r("strong"),KO=i("token_str"),JO=f(),up=r("td"),WO=i("The string representation of the token"),r2=f(),Ye=r("h2"),ta=r("a"),Eq=r("span"),_(sl.$$.fragment),YO=f(),wq=r("span"),VO=i("Automatic speech recognition task"),o2=f(),cp=r("p"),XO=i(`This task reads some audio input and outputs the said words within the
audio files.`),l2=f(),_(sa.$$.fragment),i2=f(),_(aa.$$.fragment),u2=f(),pe=r("p"),QO=i("Available with: "),al=r("a"),ZO=i("\u{1F917} Transformers"),eP=f(),nl=r("a"),tP=i("ESPnet"),sP=i(` and
`),rl=r("a"),aP=i("SpeechBrain"),c2=f(),fp=r("p"),nP=i("Request:"),f2=f(),_(na.$$.fragment),p2=f(),pp=r("p"),rP=i(`When sending your request, you should send a binary payload that simply
contains your audio file. We try to support most formats (Flac, Wav,
Mp3, Ogg etc...). And we automatically rescale the sampling rate to the
appropriate rate for the given model (usually 16KHz).`),d2=f(),ra=r("table"),bq=r("thead"),ol=r("tr"),dp=r("th"),oP=i("All parameters"),lP=f(),Tq=r("th"),iP=f(),jq=r("tbody"),ll=r("tr"),il=r("td"),kq=r("strong"),uP=i("no parameter"),cP=i(" (required)"),fP=f(),hp=r("td"),pP=i("a binary representation of the audio file. No other parameters are currently allowed."),h2=f(),gp=r("p"),dP=i("Return value is either a dict or a list of dicts if you sent a list of inputs"),g2=f(),mp=r("p"),hP=i("Response:"),m2=f(),_(oa.$$.fragment),q2=f(),la=r("table"),Aq=r("thead"),ul=r("tr"),qp=r("th"),gP=i("Returned values"),mP=f(),Dq=r("th"),qP=f(),Oq=r("tbody"),cl=r("tr"),$p=r("td"),Pq=r("strong"),$P=i("text"),_P=f(),_p=r("td"),vP=i("The string that was recognized within the audio file."),$2=f(),Ve=r("h2"),ia=r("a"),Rq=r("span"),_(fl.$$.fragment),yP=f(),Nq=r("span"),EP=i("Feature-extraction task"),_2=f(),vp=r("p"),wP=i(`This task reads some text and outputs raw float values, that are usually
consumed as part of a semantic database/semantic search.`),v2=f(),_(ua.$$.fragment),y2=f(),Xe=r("p"),bP=i("Available with: "),pl=r("a"),TP=i("\u{1F917} Transformers"),jP=f(),dl=r("a"),kP=i("Sentence-transformers"),E2=f(),yp=r("p"),AP=i("Request:"),w2=f(),ca=r("table"),Sq=r("thead"),hl=r("tr"),Ep=r("th"),DP=i("All parameters"),OP=f(),xq=r("th"),PP=f(),ae=r("tbody"),gl=r("tr"),ml=r("td"),Iq=r("strong"),RP=i("inputs"),NP=i(" (required):"),SP=f(),wp=r("td"),xP=i("a string or a list of strings to get the features from."),IP=f(),ql=r("tr"),bp=r("td"),Hq=r("strong"),HP=i("options"),BP=f(),Tp=r("td"),CP=i("a dict containing the following keys:"),GP=f(),$l=r("tr"),jp=r("td"),LP=i("use_gpu"),UP=f(),fa=r("td"),zP=i("(Default: "),Bq=r("code"),MP=i("false"),FP=i("). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),KP=f(),_l=r("tr"),kp=r("td"),JP=i("use_cache"),WP=f(),pa=r("td"),YP=i("(Default: "),Cq=r("code"),VP=i("true"),XP=i("). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),QP=f(),vl=r("tr"),Ap=r("td"),ZP=i("wait_for_model"),eR=f(),da=r("td"),tR=i("(Default: "),Gq=r("code"),sR=i("false"),aR=i(") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),b2=f(),Dp=r("p"),nR=i("Return value is either a dict or a list of dicts if you sent a list of inputs"),T2=f(),ha=r("table"),Lq=r("thead"),yl=r("tr"),Op=r("th"),rR=i("Returned values"),oR=f(),Uq=r("th"),lR=f(),zq=r("tbody"),El=r("tr"),Pp=r("td"),Mq=r("strong"),iR=i("A list of float (or list of list of floats)"),uR=f(),Rp=r("td"),cR=i("The numbers that are the representation features of the input."),j2=f(),Np=r("small"),fR=i(`Returned values are a list of floats, or a list of list of floats
(depending on if you sent a string or a list of string, and if the
automatic reduction, usually mean_pooling for instance was applied for
you or not. This should be explained on the model's README.`),k2=f(),Qe=r("h2"),ga=r("a"),Fq=r("span"),_(wl.$$.fragment),pR=f(),Kq=r("span"),dR=i("Audio-classification task"),A2=f(),Sp=r("p"),hR=i("This task reads some audio input and outputs the likelihood of classes."),D2=f(),_(ma.$$.fragment),O2=f(),Ze=r("p"),gR=i("Available with: "),bl=r("a"),mR=i("\u{1F917} Transformers"),qR=f(),Tl=r("a"),$R=i("SpeechBrain"),P2=f(),xp=r("p"),_R=i("Request:"),R2=f(),_(qa.$$.fragment),N2=f(),Ip=r("p"),vR=i(`When sending your request, you should send a binary payload that simply
contains your audio file. We try to support most formats (Flac, Wav,
Mp3, Ogg etc...). And we automatically rescale the sampling rate to the
appropriate rate for the given model (usually 16KHz).`),S2=f(),$a=r("table"),Jq=r("thead"),jl=r("tr"),Hp=r("th"),yR=i("All parameters"),ER=f(),Wq=r("th"),wR=f(),Yq=r("tbody"),kl=r("tr"),Al=r("td"),Vq=r("strong"),bR=i("no parameter"),TR=i(" (required)"),jR=f(),Bp=r("td"),kR=i("a binary representation of the audio file. No other parameters are currently allowed."),x2=f(),Cp=r("p"),AR=i("Return value is a dict"),I2=f(),_(_a.$$.fragment),H2=f(),va=r("table"),Xq=r("thead"),Dl=r("tr"),Gp=r("th"),DR=i("Returned values"),OR=f(),Qq=r("th"),PR=f(),Ol=r("tbody"),Pl=r("tr"),Lp=r("td"),Zq=r("strong"),RR=i("label"),NR=f(),Up=r("td"),SR=i("The label for the class (model specific)"),xR=f(),Rl=r("tr"),zp=r("td"),e$=r("strong"),IR=i("score"),HR=f(),Mp=r("td"),BR=i("A float that represents how likely it is that the audio file belongs to this class."),B2=f(),et=r("h2"),ya=r("a"),t$=r("span"),_(Nl.$$.fragment),CR=f(),s$=r("span"),GR=i("Object-detection task"),C2=f(),Fp=r("p"),LR=i(`This task reads some image input and outputs the likelihood of classes &
bounding boxes of detected objects.`),G2=f(),_(Ea.$$.fragment),L2=f(),Sl=r("p"),UR=i("Available with: "),xl=r("a"),zR=i("\u{1F917} Transformers"),U2=f(),Kp=r("p"),MR=i("Request:"),z2=f(),_(wa.$$.fragment),M2=f(),ba=r("p"),FR=i(`When sending your request, you should send a binary payload that simply
contains your image file. We support all image formats `),Il=r("a"),KR=i(`Pillow
supports`),JR=i("."),F2=f(),Ta=r("table"),a$=r("thead"),Hl=r("tr"),Jp=r("th"),WR=i("All parameters"),YR=f(),n$=r("th"),VR=f(),r$=r("tbody"),Bl=r("tr"),Cl=r("td"),o$=r("strong"),XR=i("no parameter"),QR=i(" (required)"),ZR=f(),Wp=r("td"),eN=i("a binary representation of the image file. No other parameters are currently allowed."),K2=f(),Yp=r("p"),tN=i("Return value is a dict"),J2=f(),_(ja.$$.fragment),W2=f(),ka=r("table"),l$=r("thead"),Gl=r("tr"),Vp=r("th"),sN=i("Returned values"),aN=f(),i$=r("th"),nN=f(),tt=r("tbody"),Ll=r("tr"),Xp=r("td"),u$=r("strong"),rN=i("label"),oN=f(),Qp=r("td"),lN=i("The label for the class (model specific) of a detected object."),iN=f(),Ul=r("tr"),Zp=r("td"),c$=r("strong"),uN=i("score"),cN=f(),ed=r("td"),fN=i("A float that represents how likely it is that the detected object belongs to the given class."),pN=f(),zl=r("tr"),td=r("td"),f$=r("strong"),dN=i("box"),hN=f(),sd=r("td"),gN=i("A dict (with keys [xmin,ymin,xmax,ymax]) representing the bounding box of a detected object."),Y2=f(),st=r("h2"),Aa=r("a"),p$=r("span"),_(Ml.$$.fragment),mN=f(),d$=r("span"),qN=i("Image Segmentation task"),V2=f(),ad=r("p"),$N=i(`This task reads some image input and outputs the likelihood of classes &
bounding boxes of detected objects.`),X2=f(),_(Da.$$.fragment),Q2=f(),Fl=r("p"),_N=i("Available with: "),Kl=r("a"),vN=i("\u{1F917} Transformers"),Z2=f(),nd=r("p"),yN=i("Request:"),ev=f(),_(Oa.$$.fragment),tv=f(),Pa=r("p"),EN=i(`When sending your request, you should send a binary payload that simply
contains your image file. We support all image formats `),Jl=r("a"),wN=i(`Pillow
supports`),bN=i("."),sv=f(),Ra=r("table"),h$=r("thead"),Wl=r("tr"),rd=r("th"),TN=i("All parameters"),jN=f(),g$=r("th"),kN=f(),m$=r("tbody"),Yl=r("tr"),Vl=r("td"),q$=r("strong"),AN=i("no parameter"),DN=i(" (required)"),ON=f(),od=r("td"),PN=i("a binary representation of the image file. No other parameters are currently allowed."),av=f(),ld=r("p"),RN=i("Return value is a dict"),nv=f(),_(Na.$$.fragment),rv=f(),Sa=r("table"),$$=r("thead"),Xl=r("tr"),id=r("th"),NN=i("Returned values"),SN=f(),_$=r("th"),xN=f(),at=r("tbody"),Ql=r("tr"),ud=r("td"),v$=r("strong"),IN=i("label"),HN=f(),cd=r("td"),BN=i("The label for the class (model specific) of a segment."),CN=f(),Zl=r("tr"),fd=r("td"),y$=r("strong"),GN=i("score"),LN=f(),pd=r("td"),UN=i("A float that represents how likely it is that the segment belongs to the given class."),zN=f(),ei=r("tr"),dd=r("td"),E$=r("strong"),MN=i("mask"),FN=f(),hd=r("td"),KN=i("A str (base64 str of a single channel black-and-white img) representing the mask of a segment."),this.h()},l(a){const g=sM('[data-svelte="svelte-1phssyn"]',document.head);n=o(g,"META",{name:!0,content:!0}),g.forEach(t),c=p(a),s=o(a,"H1",{class:!0});var ti=l(s);h=o(ti,"A",{id:!0,class:!0,href:!0});var w$=l(h);q=o(w$,"SPAN",{});var b$=l(q);v(k.$$.fragment,b$),b$.forEach(t),w$.forEach(t),A=p(ti),j=o(ti,"SPAN",{});var T$=l(j);T=u(T$,"Detailed parameters"),T$.forEach(t),ti.forEach(t),O=p(a),D=o(a,"H2",{class:!0});var si=l(D);ne=o(si,"A",{id:!0,class:!0,href:!0});var j$=l(ne);Re=o(j$,"SPAN",{});var k$=l(Re);v(Q.$$.fragment,k$),k$.forEach(t),j$.forEach(t),Y=p(si),rt=o(si,"SPAN",{});var A$=l(rt);pi=u(A$,"Which task is used by this model ?"),A$.forEach(t),si.forEach(t),Ua=p(a),Ne=o(a,"P",{});var D$=l(Ne);Hw=u(D$,`In general the \u{1F917} Hosted API Inference accepts a simple string as an
input. However, more advanced usage depends on the \u201Ctask\u201D that the
model solves.`),D$.forEach(t),Z$=p(a),di=o(a,"P",{});var O$=l(di);Bw=u(O$,"The \u201Ctask\u201D of a model is defined here on it\u2019s model page:"),O$.forEach(t),e_=p(a),ot=o(a,"IMG",{class:!0,src:!0,width:!0}),t_=p(a),lt=o(a,"IMG",{class:!0,src:!0,width:!0}),s_=p(a),Se=o(a,"H2",{class:!0});var ai=l(Se);it=o(ai,"A",{id:!0,class:!0,href:!0});var P$=l(it);Sd=o(P$,"SPAN",{});var R$=l(Sd);v(za.$$.fragment,R$),R$.forEach(t),P$.forEach(t),Cw=p(ai),xd=o(ai,"SPAN",{});var N$=l(xd);Gw=u(N$,"Zero-shot classification task"),N$.forEach(t),ai.forEach(t),a_=p(a),hi=o(a,"P",{});var S$=l(hi);Lw=u(S$,`This task is super useful to try out classification with zero code,
you simply pass a sentence/paragraph and the possible labels for that
sentence, and you get a result.`),S$.forEach(t),n_=p(a),v(ut.$$.fragment,a),r_=p(a),Ma=o(a,"P",{});var gd=l(Ma);Uw=u(gd,"Available with: "),Fa=o(gd,"A",{href:!0,rel:!0});var x$=l(Fa);zw=u(x$,"\u{1F917} Transformers"),x$.forEach(t),gd.forEach(t),o_=p(a),gi=o(a,"P",{});var I$=l(gi);Mw=u(I$,"Request:"),I$.forEach(t),l_=p(a),v(ct.$$.fragment,a),i_=p(a),mi=o(a,"P",{});var H$=l(mi);Fw=u(H$,`When sending your request, you should send a JSON encoded payload. Here
are all the options`),H$.forEach(t),u_=p(a),ft=o(a,"TABLE",{});var ni=l(ft);Id=o(ni,"THEAD",{});var B$=l(Id);Ka=o(B$,"TR",{});var ri=l(Ka);qi=o(ri,"TH",{align:!0});var C$=l(qi);Kw=u(C$,"All parameters"),C$.forEach(t),Jw=p(ri),Hd=o(ri,"TH",{align:!0}),l(Hd).forEach(t),ri.forEach(t),B$.forEach(t),Ww=p(ni),z=o(ni,"TBODY",{});var M=l(z);Ja=o(M,"TR",{});var oi=l(Ja);Wa=o(oi,"TD",{align:!0});var md=l(Wa);Bd=o(md,"STRONG",{});var G$=l(Bd);Yw=u(G$,"inputs"),G$.forEach(t),Vw=u(md," (required)"),md.forEach(t),Xw=p(oi),$i=o(oi,"TD",{align:!0});var L$=l($i);Qw=u(L$,"a string or list of strings"),L$.forEach(t),oi.forEach(t),Zw=p(M),Ya=o(M,"TR",{});var li=l(Ya);Va=o(li,"TD",{align:!0});var qd=l(Va);Cd=o(qd,"STRONG",{});var U$=l(Cd);eb=u(U$,"parameters"),U$.forEach(t),tb=u(qd," (required)"),qd.forEach(t),sb=p(li),_i=o(li,"TD",{align:!0});var z$=l(_i);ab=u(z$,"a dict containing the following keys:"),z$.forEach(t),li.forEach(t),nb=p(M),Xa=o(M,"TR",{});var ii=l(Xa);vi=o(ii,"TD",{align:!0});var M$=l(vi);rb=u(M$,"candidate_labels (required)"),M$.forEach(t),ob=p(ii),de=o(ii,"TD",{align:!0});var nt=l(de);lb=u(nt,"a list of strings that are potential classes for "),Gd=o(nt,"CODE",{});var F$=l(Gd);ib=u(F$,"inputs"),F$.forEach(t),ub=u(nt,". (max 10 candidate_labels, for more, simply run multiple requests, results are going to be misleading if using too many candidate_labels anyway. If you want to keep the exact same, you can simply run "),Ld=o(nt,"CODE",{});var K$=l(Ld);cb=u(K$,"multi_label=True"),K$.forEach(t),fb=u(nt," and do the scaling on your end. )"),nt.forEach(t),ii.forEach(t),pb=p(M),Qa=o(M,"TR",{});var ui=l(Qa);yi=o(ui,"TD",{align:!0});var J$=l(yi);db=u(J$,"multi_label"),J$.forEach(t),hb=p(ui),pt=o(ui,"TD",{align:!0});var ci=l(pt);gb=u(ci,"(Default: "),Ud=o(ci,"CODE",{});var W$=l(Ud);mb=u(W$,"false"),W$.forEach(t),qb=u(ci,") Boolean that is set to True if classes can overlap"),ci.forEach(t),ui.forEach(t),$b=p(M),Za=o(M,"TR",{});var lv=l(Za);Ei=o(lv,"TD",{align:!0});var $S=l(Ei);zd=o($S,"STRONG",{});var _S=l(zd);_b=u(_S,"options"),_S.forEach(t),$S.forEach(t),vb=p(lv),wi=o(lv,"TD",{align:!0});var vS=l(wi);yb=u(vS,"a dict containing the following keys:"),vS.forEach(t),lv.forEach(t),Eb=p(M),en=o(M,"TR",{});var iv=l(en);bi=o(iv,"TD",{align:!0});var yS=l(bi);wb=u(yS,"use_gpu"),yS.forEach(t),bb=p(iv),dt=o(iv,"TD",{align:!0});var uv=l(dt);Tb=u(uv,"(Default: "),Md=o(uv,"CODE",{});var ES=l(Md);jb=u(ES,"false"),ES.forEach(t),kb=u(uv,"). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),uv.forEach(t),iv.forEach(t),Ab=p(M),tn=o(M,"TR",{});var cv=l(tn);Ti=o(cv,"TD",{align:!0});var wS=l(Ti);Db=u(wS,"use_cache"),wS.forEach(t),Ob=p(cv),ht=o(cv,"TD",{align:!0});var fv=l(ht);Pb=u(fv,"(Default: "),Fd=o(fv,"CODE",{});var bS=l(Fd);Rb=u(bS,"true"),bS.forEach(t),Nb=u(fv,"). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),fv.forEach(t),cv.forEach(t),Sb=p(M),sn=o(M,"TR",{});var pv=l(sn);ji=o(pv,"TD",{align:!0});var TS=l(ji);xb=u(TS,"wait_for_model"),TS.forEach(t),Ib=p(pv),gt=o(pv,"TD",{align:!0});var dv=l(gt);Hb=u(dv,"(Default: "),Kd=o(dv,"CODE",{});var jS=l(Kd);Bb=u(jS,"false"),jS.forEach(t),Cb=u(dv,") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),dv.forEach(t),pv.forEach(t),M.forEach(t),ni.forEach(t),c_=p(a),ki=o(a,"P",{});var kS=l(ki);Gb=u(kS,"Return value is either a dict or a list of dicts if you sent a list of inputs"),kS.forEach(t),f_=p(a),Ai=o(a,"P",{});var AS=l(Ai);Lb=u(AS,"Response:"),AS.forEach(t),p_=p(a),v(mt.$$.fragment,a),d_=p(a),qt=o(a,"TABLE",{});var hv=l(qt);Jd=o(hv,"THEAD",{});var DS=l(Jd);an=o(DS,"TR",{});var gv=l(an);Di=o(gv,"TH",{align:!0});var OS=l(Di);Ub=u(OS,"Returned values"),OS.forEach(t),zb=p(gv),Wd=o(gv,"TH",{align:!0}),l(Wd).forEach(t),gv.forEach(t),DS.forEach(t),Mb=p(hv),xe=o(hv,"TBODY",{});var $d=l(xe);nn=o($d,"TR",{});var mv=l(nn);Oi=o(mv,"TD",{align:!0});var PS=l(Oi);Yd=o(PS,"STRONG",{});var RS=l(Yd);Fb=u(RS,"sequence"),RS.forEach(t),PS.forEach(t),Kb=p(mv),Pi=o(mv,"TD",{align:!0});var NS=l(Pi);Jb=u(NS,"The string sent as an input"),NS.forEach(t),mv.forEach(t),Wb=p($d),rn=o($d,"TR",{});var qv=l(rn);Ri=o(qv,"TD",{align:!0});var SS=l(Ri);Vd=o(SS,"STRONG",{});var xS=l(Vd);Yb=u(xS,"labels"),xS.forEach(t),SS.forEach(t),Vb=p(qv),Ni=o(qv,"TD",{align:!0});var IS=l(Ni);Xb=u(IS,"The list of strings for labels that you sent (in order)"),IS.forEach(t),qv.forEach(t),Qb=p($d),on=o($d,"TR",{});var $v=l(on);Si=o($v,"TD",{align:!0});var HS=l(Si);Xd=o(HS,"STRONG",{});var BS=l(Xd);Zb=u(BS,"scores"),BS.forEach(t),HS.forEach(t),e3=p($v),$t=o($v,"TD",{align:!0});var _v=l($t);t3=u(_v,"a list of floats that correspond the the probability of label, in the same order as "),Qd=o(_v,"CODE",{});var CS=l(Qd);s3=u(CS,"labels"),CS.forEach(t),a3=u(_v,"."),_v.forEach(t),$v.forEach(t),$d.forEach(t),hv.forEach(t),h_=p(a),Ie=o(a,"H2",{class:!0});var vv=l(Ie);_t=o(vv,"A",{id:!0,class:!0,href:!0});var GS=l(_t);Zd=o(GS,"SPAN",{});var LS=l(Zd);v(ln.$$.fragment,LS),LS.forEach(t),GS.forEach(t),n3=p(vv),eh=o(vv,"SPAN",{});var US=l(eh);r3=u(US,"Translation task"),US.forEach(t),vv.forEach(t),g_=p(a),xi=o(a,"P",{});var zS=l(xi);o3=u(zS,"This task is well known to translate text from one language to another"),zS.forEach(t),m_=p(a),v(vt.$$.fragment,a),q_=p(a),un=o(a,"P",{});var JN=l(un);l3=u(JN,"Available with: "),cn=o(JN,"A",{href:!0,rel:!0});var MS=l(cn);i3=u(MS,"\u{1F917} Transformers"),MS.forEach(t),JN.forEach(t),$_=p(a),Ii=o(a,"P",{});var FS=l(Ii);u3=u(FS,"Example:"),FS.forEach(t),__=p(a),v(yt.$$.fragment,a),v_=p(a),Hi=o(a,"P",{});var KS=l(Hi);c3=u(KS,`When sending your request, you should send a JSON encoded payload. Here
are all the options`),KS.forEach(t),y_=p(a),Et=o(a,"TABLE",{});var yv=l(Et);th=o(yv,"THEAD",{});var JS=l(th);fn=o(JS,"TR",{});var Ev=l(fn);Bi=o(Ev,"TH",{align:!0});var WS=l(Bi);f3=u(WS,"All parameters"),WS.forEach(t),p3=p(Ev),sh=o(Ev,"TH",{align:!0}),l(sh).forEach(t),Ev.forEach(t),JS.forEach(t),d3=p(yv),Z=o(yv,"TBODY",{});var ke=l(Z);pn=o(ke,"TR",{});var wv=l(pn);dn=o(wv,"TD",{align:!0});var WN=l(dn);ah=o(WN,"STRONG",{});var YS=l(ah);h3=u(YS,"inputs"),YS.forEach(t),g3=u(WN," (required)"),WN.forEach(t),m3=p(wv),Ci=o(wv,"TD",{align:!0});var VS=l(Ci);q3=u(VS,"a string to be translated in the original languages"),VS.forEach(t),wv.forEach(t),$3=p(ke),hn=o(ke,"TR",{});var bv=l(hn);Gi=o(bv,"TD",{align:!0});var XS=l(Gi);nh=o(XS,"STRONG",{});var QS=l(nh);_3=u(QS,"options"),QS.forEach(t),XS.forEach(t),v3=p(bv),Li=o(bv,"TD",{align:!0});var ZS=l(Li);y3=u(ZS,"a dict containing the following keys:"),ZS.forEach(t),bv.forEach(t),E3=p(ke),gn=o(ke,"TR",{});var Tv=l(gn);Ui=o(Tv,"TD",{align:!0});var ex=l(Ui);w3=u(ex,"use_gpu"),ex.forEach(t),b3=p(Tv),wt=o(Tv,"TD",{align:!0});var jv=l(wt);T3=u(jv,"(Default: "),rh=o(jv,"CODE",{});var tx=l(rh);j3=u(tx,"false"),tx.forEach(t),k3=u(jv,"). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),jv.forEach(t),Tv.forEach(t),A3=p(ke),mn=o(ke,"TR",{});var kv=l(mn);zi=o(kv,"TD",{align:!0});var sx=l(zi);D3=u(sx,"use_cache"),sx.forEach(t),O3=p(kv),bt=o(kv,"TD",{align:!0});var Av=l(bt);P3=u(Av,"(Default: "),oh=o(Av,"CODE",{});var ax=l(oh);R3=u(ax,"true"),ax.forEach(t),N3=u(Av,"). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),Av.forEach(t),kv.forEach(t),S3=p(ke),qn=o(ke,"TR",{});var Dv=l(qn);Mi=o(Dv,"TD",{align:!0});var nx=l(Mi);x3=u(nx,"wait_for_model"),nx.forEach(t),I3=p(Dv),Tt=o(Dv,"TD",{align:!0});var Ov=l(Tt);H3=u(Ov,"(Default: "),lh=o(Ov,"CODE",{});var rx=l(lh);B3=u(rx,"false"),rx.forEach(t),C3=u(Ov,") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),Ov.forEach(t),Dv.forEach(t),ke.forEach(t),yv.forEach(t),E_=p(a),Fi=o(a,"P",{});var ox=l(Fi);G3=u(ox,"Return value is either a dict or a list of dicts if you sent a list of inputs"),ox.forEach(t),w_=p(a),jt=o(a,"TABLE",{});var Pv=l(jt);ih=o(Pv,"THEAD",{});var lx=l(ih);$n=o(lx,"TR",{});var Rv=l($n);Ki=o(Rv,"TH",{align:!0});var ix=l(Ki);L3=u(ix,"Returned values"),ix.forEach(t),U3=p(Rv),uh=o(Rv,"TH",{align:!0}),l(uh).forEach(t),Rv.forEach(t),lx.forEach(t),z3=p(Pv),ch=o(Pv,"TBODY",{});var ux=l(ch);_n=o(ux,"TR",{});var Nv=l(_n);Ji=o(Nv,"TD",{align:!0});var cx=l(Ji);fh=o(cx,"STRONG",{});var fx=l(fh);M3=u(fx,"translation_text"),fx.forEach(t),cx.forEach(t),F3=p(Nv),Wi=o(Nv,"TD",{align:!0});var px=l(Wi);K3=u(px,"The string after translation"),px.forEach(t),Nv.forEach(t),ux.forEach(t),Pv.forEach(t),b_=p(a),He=o(a,"H2",{class:!0});var Sv=l(He);kt=o(Sv,"A",{id:!0,class:!0,href:!0});var dx=l(kt);ph=o(dx,"SPAN",{});var hx=l(ph);v(vn.$$.fragment,hx),hx.forEach(t),dx.forEach(t),J3=p(Sv),dh=o(Sv,"SPAN",{});var gx=l(dh);W3=u(gx,"Summarization task"),gx.forEach(t),Sv.forEach(t),T_=p(a),At=o(a,"P",{});var xv=l(At);Y3=u(xv,`This task is well known to summarize longer text into shorter text.
Be careful, some models have a maximum length of input. That means that
the summary cannot handle full books for instance. Be careful when
choosing your model. If you want to discuss your summarization needs,
please get in touch with us: <`),Yi=o(xv,"A",{href:!0});var mx=l(Yi);V3=u(mx,"api-enterprise@huggingface.co"),mx.forEach(t),X3=u(xv,">"),xv.forEach(t),j_=p(a),v(Dt.$$.fragment,a),k_=p(a),yn=o(a,"P",{});var YN=l(yn);Q3=u(YN,"Available with: "),En=o(YN,"A",{href:!0,rel:!0});var qx=l(En);Z3=u(qx,"\u{1F917} Transformers"),qx.forEach(t),YN.forEach(t),A_=p(a),Vi=o(a,"P",{});var $x=l(Vi);eT=u($x,"Example:"),$x.forEach(t),D_=p(a),v(Ot.$$.fragment,a),O_=p(a),Xi=o(a,"P",{});var _x=l(Xi);tT=u(_x,`When sending your request, you should send a JSON encoded payload. Here
are all the options`),_x.forEach(t),P_=p(a),Pt=o(a,"TABLE",{});var Iv=l(Pt);hh=o(Iv,"THEAD",{});var vx=l(hh);wn=o(vx,"TR",{});var Hv=l(wn);Qi=o(Hv,"TH",{align:!0});var yx=l(Qi);sT=u(yx,"All parameters"),yx.forEach(t),aT=p(Hv),gh=o(Hv,"TH",{align:!0}),l(gh).forEach(t),Hv.forEach(t),vx.forEach(t),nT=p(Iv),G=o(Iv,"TBODY",{});var L=l(G);bn=o(L,"TR",{});var Bv=l(bn);Tn=o(Bv,"TD",{align:!0});var VN=l(Tn);mh=o(VN,"STRONG",{});var Ex=l(mh);rT=u(Ex,"inputs"),Ex.forEach(t),oT=u(VN," (required)"),VN.forEach(t),lT=p(Bv),Zi=o(Bv,"TD",{align:!0});var wx=l(Zi);iT=u(wx,"a string to be summarized"),wx.forEach(t),Bv.forEach(t),uT=p(L),jn=o(L,"TR",{});var Cv=l(jn);eu=o(Cv,"TD",{align:!0});var bx=l(eu);qh=o(bx,"STRONG",{});var Tx=l(qh);cT=u(Tx,"parameters"),Tx.forEach(t),bx.forEach(t),fT=p(Cv),tu=o(Cv,"TD",{align:!0});var jx=l(tu);pT=u(jx,"a dict containing the following keys:"),jx.forEach(t),Cv.forEach(t),dT=p(L),kn=o(L,"TR",{});var Gv=l(kn);su=o(Gv,"TD",{align:!0});var kx=l(su);hT=u(kx,"min_length"),kx.forEach(t),gT=p(Gv),he=o(Gv,"TD",{align:!0});var _d=l(he);mT=u(_d,"(Default: "),$h=o(_d,"CODE",{});var Ax=l($h);qT=u(Ax,"None"),Ax.forEach(t),$T=u(_d,"). Integer to define the minimum length "),_h=o(_d,"STRONG",{});var Dx=l(_h);_T=u(Dx,"in tokens"),Dx.forEach(t),vT=u(_d," of the output summary."),_d.forEach(t),Gv.forEach(t),yT=p(L),An=o(L,"TR",{});var Lv=l(An);au=o(Lv,"TD",{align:!0});var Ox=l(au);ET=u(Ox,"max_length"),Ox.forEach(t),wT=p(Lv),ge=o(Lv,"TD",{align:!0});var vd=l(ge);bT=u(vd,"(Default: "),vh=o(vd,"CODE",{});var Px=l(vh);TT=u(Px,"None"),Px.forEach(t),jT=u(vd,"). Integer to define the maximum length "),yh=o(vd,"STRONG",{});var Rx=l(yh);kT=u(Rx,"in tokens"),Rx.forEach(t),AT=u(vd," of the output summary."),vd.forEach(t),Lv.forEach(t),DT=p(L),Dn=o(L,"TR",{});var Uv=l(Dn);nu=o(Uv,"TD",{align:!0});var Nx=l(nu);OT=u(Nx,"top_k"),Nx.forEach(t),PT=p(Uv),me=o(Uv,"TD",{align:!0});var yd=l(me);RT=u(yd,"(Default: "),Eh=o(yd,"CODE",{});var Sx=l(Eh);NT=u(Sx,"None"),Sx.forEach(t),ST=u(yd,"). Integer to define the top tokens considered within the "),wh=o(yd,"CODE",{});var xx=l(wh);xT=u(xx,"sample"),xx.forEach(t),IT=u(yd," operation to create new text."),yd.forEach(t),Uv.forEach(t),HT=p(L),On=o(L,"TR",{});var zv=l(On);ru=o(zv,"TD",{align:!0});var Ix=l(ru);BT=u(Ix,"top_p"),Ix.forEach(t),CT=p(zv),re=o(zv,"TD",{align:!0});var xa=l(re);GT=u(xa,"(Default: "),bh=o(xa,"CODE",{});var Hx=l(bh);LT=u(Hx,"None"),Hx.forEach(t),UT=u(xa,"). Float to define the tokens that are within the "),Th=o(xa,"CODE",{});var Bx=l(Th);zT=u(Bx,"sample"),Bx.forEach(t),MT=u(xa," operation of text generation. Add tokens in the sample for more probable to least probable until the sum of the probabilities is greater than "),jh=o(xa,"CODE",{});var Cx=l(jh);FT=u(Cx,"top_p"),Cx.forEach(t),KT=u(xa,"."),xa.forEach(t),zv.forEach(t),JT=p(L),Pn=o(L,"TR",{});var Mv=l(Pn);ou=o(Mv,"TD",{align:!0});var Gx=l(ou);WT=u(Gx,"temperature"),Gx.forEach(t),YT=p(Mv),qe=o(Mv,"TD",{align:!0});var Ed=l(qe);VT=u(Ed,"(Default: "),kh=o(Ed,"CODE",{});var Lx=l(kh);XT=u(Lx,"1.0"),Lx.forEach(t),QT=u(Ed,"). Float (0.0-100.0). The temperature of the sampling operation. 1 means regular sampling, 0 means "),Ah=o(Ed,"CODE",{});var Ux=l(Ah);ZT=u(Ux,"100.0"),Ux.forEach(t),e5=u(Ed," is getting closer to uniform probability."),Ed.forEach(t),Mv.forEach(t),t5=p(L),Rn=o(L,"TR",{});var Fv=l(Rn);lu=o(Fv,"TD",{align:!0});var zx=l(lu);s5=u(zx,"repetition_penalty"),zx.forEach(t),a5=p(Fv),Rt=o(Fv,"TD",{align:!0});var Kv=l(Rt);n5=u(Kv,"(Default: "),Dh=o(Kv,"CODE",{});var Mx=l(Dh);r5=u(Mx,"None"),Mx.forEach(t),o5=u(Kv,"). Float (0.0-100.0). The more a token is used within generation the more it is penalized to not be picked in successive generation passes."),Kv.forEach(t),Fv.forEach(t),l5=p(L),Nn=o(L,"TR",{});var Jv=l(Nn);iu=o(Jv,"TD",{align:!0});var Fx=l(iu);i5=u(Fx,"max_time"),Fx.forEach(t),u5=p(Jv),Nt=o(Jv,"TD",{align:!0});var Wv=l(Nt);c5=u(Wv,"(Default: "),Oh=o(Wv,"CODE",{});var Kx=l(Oh);f5=u(Kx,"None"),Kx.forEach(t),p5=u(Wv,"). Float (0-120.0). The amount of time in seconds that the query should take maximum. Network can cause some overhead so it will be a soft limit."),Wv.forEach(t),Jv.forEach(t),d5=p(L),Sn=o(L,"TR",{});var Yv=l(Sn);uu=o(Yv,"TD",{align:!0});var Jx=l(uu);Ph=o(Jx,"STRONG",{});var Wx=l(Ph);h5=u(Wx,"options"),Wx.forEach(t),Jx.forEach(t),g5=p(Yv),cu=o(Yv,"TD",{align:!0});var Yx=l(cu);m5=u(Yx,"a dict containing the following keys:"),Yx.forEach(t),Yv.forEach(t),q5=p(L),xn=o(L,"TR",{});var Vv=l(xn);fu=o(Vv,"TD",{align:!0});var Vx=l(fu);$5=u(Vx,"use_gpu"),Vx.forEach(t),_5=p(Vv),St=o(Vv,"TD",{align:!0});var Xv=l(St);v5=u(Xv,"(Default: "),Rh=o(Xv,"CODE",{});var Xx=l(Rh);y5=u(Xx,"false"),Xx.forEach(t),E5=u(Xv,"). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),Xv.forEach(t),Vv.forEach(t),w5=p(L),In=o(L,"TR",{});var Qv=l(In);pu=o(Qv,"TD",{align:!0});var Qx=l(pu);b5=u(Qx,"use_cache"),Qx.forEach(t),T5=p(Qv),xt=o(Qv,"TD",{align:!0});var Zv=l(xt);j5=u(Zv,"(Default: "),Nh=o(Zv,"CODE",{});var Zx=l(Nh);k5=u(Zx,"true"),Zx.forEach(t),A5=u(Zv,"). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),Zv.forEach(t),Qv.forEach(t),D5=p(L),Hn=o(L,"TR",{});var ey=l(Hn);du=o(ey,"TD",{align:!0});var eI=l(du);O5=u(eI,"wait_for_model"),eI.forEach(t),P5=p(ey),It=o(ey,"TD",{align:!0});var ty=l(It);R5=u(ty,"(Default: "),Sh=o(ty,"CODE",{});var tI=l(Sh);N5=u(tI,"false"),tI.forEach(t),S5=u(ty,") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),ty.forEach(t),ey.forEach(t),L.forEach(t),Iv.forEach(t),R_=p(a),hu=o(a,"P",{});var sI=l(hu);x5=u(sI,"Return value is either a dict or a list of dicts if you sent a list of inputs"),sI.forEach(t),N_=p(a),Ht=o(a,"TABLE",{});var sy=l(Ht);xh=o(sy,"THEAD",{});var aI=l(xh);Bn=o(aI,"TR",{});var ay=l(Bn);gu=o(ay,"TH",{align:!0});var nI=l(gu);I5=u(nI,"Returned values"),nI.forEach(t),H5=p(ay),Ih=o(ay,"TH",{align:!0}),l(Ih).forEach(t),ay.forEach(t),aI.forEach(t),B5=p(sy),Hh=o(sy,"TBODY",{});var rI=l(Hh);Cn=o(rI,"TR",{});var ny=l(Cn);mu=o(ny,"TD",{align:!0});var oI=l(mu);Bh=o(oI,"STRONG",{});var lI=l(Bh);C5=u(lI,"summarization_text"),lI.forEach(t),oI.forEach(t),G5=p(ny),qu=o(ny,"TD",{align:!0});var iI=l(qu);L5=u(iI,"The string after translation"),iI.forEach(t),ny.forEach(t),rI.forEach(t),sy.forEach(t),S_=p(a),Be=o(a,"H2",{class:!0});var ry=l(Be);Bt=o(ry,"A",{id:!0,class:!0,href:!0});var uI=l(Bt);Ch=o(uI,"SPAN",{});var cI=l(Ch);v(Gn.$$.fragment,cI),cI.forEach(t),uI.forEach(t),U5=p(ry),Gh=o(ry,"SPAN",{});var fI=l(Gh);z5=u(fI,"Conversational task"),fI.forEach(t),ry.forEach(t),x_=p(a),$u=o(a,"P",{});var pI=l($u);M5=u(pI,`This task corresponds to any chatbot like structure. Models tend to have
shorter max_length, so please check with caution when using a given
model if you need long range dependency or not.`),pI.forEach(t),I_=p(a),v(Ct.$$.fragment,a),H_=p(a),Ln=o(a,"P",{});var XN=l(Ln);F5=u(XN,"Available with: "),Un=o(XN,"A",{href:!0,rel:!0});var dI=l(Un);K5=u(dI,"\u{1F917} Transformers"),dI.forEach(t),XN.forEach(t),B_=p(a),_u=o(a,"P",{});var hI=l(_u);J5=u(hI,"Example:"),hI.forEach(t),C_=p(a),v(Gt.$$.fragment,a),G_=p(a),vu=o(a,"P",{});var gI=l(vu);W5=u(gI,`When sending your request, you should send a JSON encoded payload. Here
are all the options`),gI.forEach(t),L_=p(a),Lt=o(a,"TABLE",{});var oy=l(Lt);Lh=o(oy,"THEAD",{});var mI=l(Lh);zn=o(mI,"TR",{});var ly=l(zn);yu=o(ly,"TH",{align:!0});var qI=l(yu);Y5=u(qI,"All parameters"),qI.forEach(t),V5=p(ly),Uh=o(ly,"TH",{align:!0}),l(Uh).forEach(t),ly.forEach(t),mI.forEach(t),X5=p(oy),S=o(oy,"TBODY",{});var H=l(S);Mn=o(H,"TR",{});var iy=l(Mn);Fn=o(iy,"TD",{align:!0});var QN=l(Fn);zh=o(QN,"STRONG",{});var $I=l(zh);Q5=u($I,"inputs"),$I.forEach(t),Z5=u(QN," (required)"),QN.forEach(t),e4=p(iy),Mh=o(iy,"TD",{align:!0}),l(Mh).forEach(t),iy.forEach(t),t4=p(H),Kn=o(H,"TR",{});var uy=l(Kn);Eu=o(uy,"TD",{align:!0});var _I=l(Eu);s4=u(_I,"text (required)"),_I.forEach(t),a4=p(uy),wu=o(uy,"TD",{align:!0});var vI=l(wu);n4=u(vI,"The last input from the user in the conversation."),vI.forEach(t),uy.forEach(t),r4=p(H),Jn=o(H,"TR",{});var cy=l(Jn);bu=o(cy,"TD",{align:!0});var yI=l(bu);o4=u(yI,"generated_responses"),yI.forEach(t),l4=p(cy),Tu=o(cy,"TD",{align:!0});var EI=l(Tu);i4=u(EI,"A list of strings corresponding to the earlier replies from the model."),EI.forEach(t),cy.forEach(t),u4=p(H),Wn=o(H,"TR",{});var fy=l(Wn);ju=o(fy,"TD",{align:!0});var wI=l(ju);c4=u(wI,"past_user_inputs"),wI.forEach(t),f4=p(fy),Ut=o(fy,"TD",{align:!0});var py=l(Ut);p4=u(py,"A list of strings corresponding to the earlier replies from the user. Should be of the same length of "),Fh=o(py,"CODE",{});var bI=l(Fh);d4=u(bI,"generated_responses"),bI.forEach(t),h4=u(py,"."),py.forEach(t),fy.forEach(t),g4=p(H),Yn=o(H,"TR",{});var dy=l(Yn);ku=o(dy,"TD",{align:!0});var TI=l(ku);Kh=o(TI,"STRONG",{});var jI=l(Kh);m4=u(jI,"parameters"),jI.forEach(t),TI.forEach(t),q4=p(dy),Au=o(dy,"TD",{align:!0});var kI=l(Au);$4=u(kI,"a dict containing the following keys:"),kI.forEach(t),dy.forEach(t),_4=p(H),Vn=o(H,"TR",{});var hy=l(Vn);Du=o(hy,"TD",{align:!0});var AI=l(Du);v4=u(AI,"min_length"),AI.forEach(t),y4=p(hy),$e=o(hy,"TD",{align:!0});var wd=l($e);E4=u(wd,"(Default: "),Jh=o(wd,"CODE",{});var DI=l(Jh);w4=u(DI,"None"),DI.forEach(t),b4=u(wd,"). Integer to define the minimum length "),Wh=o(wd,"STRONG",{});var OI=l(Wh);T4=u(OI,"in tokens"),OI.forEach(t),j4=u(wd," of the output summary."),wd.forEach(t),hy.forEach(t),k4=p(H),Xn=o(H,"TR",{});var gy=l(Xn);Ou=o(gy,"TD",{align:!0});var PI=l(Ou);A4=u(PI,"max_length"),PI.forEach(t),D4=p(gy),_e=o(gy,"TD",{align:!0});var bd=l(_e);O4=u(bd,"(Default: "),Yh=o(bd,"CODE",{});var RI=l(Yh);P4=u(RI,"None"),RI.forEach(t),R4=u(bd,"). Integer to define the maximum length "),Vh=o(bd,"STRONG",{});var NI=l(Vh);N4=u(NI,"in tokens"),NI.forEach(t),S4=u(bd," of the output summary."),bd.forEach(t),gy.forEach(t),x4=p(H),Qn=o(H,"TR",{});var my=l(Qn);Pu=o(my,"TD",{align:!0});var SI=l(Pu);I4=u(SI,"top_k"),SI.forEach(t),H4=p(my),ve=o(my,"TD",{align:!0});var Td=l(ve);B4=u(Td,"(Default: "),Xh=o(Td,"CODE",{});var xI=l(Xh);C4=u(xI,"None"),xI.forEach(t),G4=u(Td,"). Integer to define the top tokens considered within the "),Qh=o(Td,"CODE",{});var II=l(Qh);L4=u(II,"sample"),II.forEach(t),U4=u(Td," operation to create new text."),Td.forEach(t),my.forEach(t),z4=p(H),Zn=o(H,"TR",{});var qy=l(Zn);Ru=o(qy,"TD",{align:!0});var HI=l(Ru);M4=u(HI,"top_p"),HI.forEach(t),F4=p(qy),oe=o(qy,"TD",{align:!0});var Ia=l(oe);K4=u(Ia,"(Default: "),Zh=o(Ia,"CODE",{});var BI=l(Zh);J4=u(BI,"None"),BI.forEach(t),W4=u(Ia,"). Float to define the tokens that are within the "),eg=o(Ia,"CODE",{});var CI=l(eg);Y4=u(CI,"sample"),CI.forEach(t),V4=u(Ia," operation of text generation. Add tokens in the sample for more probable to least probable until the sum of the probabilities is greater than "),tg=o(Ia,"CODE",{});var GI=l(tg);X4=u(GI,"top_p"),GI.forEach(t),Q4=u(Ia,"."),Ia.forEach(t),qy.forEach(t),Z4=p(H),er=o(H,"TR",{});var $y=l(er);Nu=o($y,"TD",{align:!0});var LI=l(Nu);ej=u(LI,"temperature"),LI.forEach(t),tj=p($y),ye=o($y,"TD",{align:!0});var jd=l(ye);sj=u(jd,"(Default: "),sg=o(jd,"CODE",{});var UI=l(sg);aj=u(UI,"1.0"),UI.forEach(t),nj=u(jd,"). Float (0.0-100.0). The temperature of the sampling operation. 1 means regular sampling, 0 means "),ag=o(jd,"CODE",{});var zI=l(ag);rj=u(zI,"100.0"),zI.forEach(t),oj=u(jd," is getting closer to uniform probability."),jd.forEach(t),$y.forEach(t),lj=p(H),tr=o(H,"TR",{});var _y=l(tr);Su=o(_y,"TD",{align:!0});var MI=l(Su);ij=u(MI,"repetition_penalty"),MI.forEach(t),uj=p(_y),zt=o(_y,"TD",{align:!0});var vy=l(zt);cj=u(vy,"(Default: "),ng=o(vy,"CODE",{});var FI=l(ng);fj=u(FI,"None"),FI.forEach(t),pj=u(vy,"). Float (0.0-100.0). The more a token is used within generation the more it is penalized to not be picked in successive generation passes."),vy.forEach(t),_y.forEach(t),dj=p(H),sr=o(H,"TR",{});var yy=l(sr);xu=o(yy,"TD",{align:!0});var KI=l(xu);hj=u(KI,"max_time"),KI.forEach(t),gj=p(yy),Mt=o(yy,"TD",{align:!0});var Ey=l(Mt);mj=u(Ey,"(Default: "),rg=o(Ey,"CODE",{});var JI=l(rg);qj=u(JI,"None"),JI.forEach(t),$j=u(Ey,"). Float (0-120.0). The amount of time in seconds that the query should take maximum. Network can cause some overhead so it will be a soft limit."),Ey.forEach(t),yy.forEach(t),_j=p(H),ar=o(H,"TR",{});var wy=l(ar);Iu=o(wy,"TD",{align:!0});var WI=l(Iu);og=o(WI,"STRONG",{});var YI=l(og);vj=u(YI,"options"),YI.forEach(t),WI.forEach(t),yj=p(wy),Hu=o(wy,"TD",{align:!0});var VI=l(Hu);Ej=u(VI,"a dict containing the following keys:"),VI.forEach(t),wy.forEach(t),wj=p(H),nr=o(H,"TR",{});var by=l(nr);Bu=o(by,"TD",{align:!0});var XI=l(Bu);bj=u(XI,"use_gpu"),XI.forEach(t),Tj=p(by),Ft=o(by,"TD",{align:!0});var Ty=l(Ft);jj=u(Ty,"(Default: "),lg=o(Ty,"CODE",{});var QI=l(lg);kj=u(QI,"false"),QI.forEach(t),Aj=u(Ty,"). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),Ty.forEach(t),by.forEach(t),Dj=p(H),rr=o(H,"TR",{});var jy=l(rr);Cu=o(jy,"TD",{align:!0});var ZI=l(Cu);Oj=u(ZI,"use_cache"),ZI.forEach(t),Pj=p(jy),Kt=o(jy,"TD",{align:!0});var ky=l(Kt);Rj=u(ky,"(Default: "),ig=o(ky,"CODE",{});var eH=l(ig);Nj=u(eH,"true"),eH.forEach(t),Sj=u(ky,"). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),ky.forEach(t),jy.forEach(t),xj=p(H),or=o(H,"TR",{});var Ay=l(or);Gu=o(Ay,"TD",{align:!0});var tH=l(Gu);Ij=u(tH,"wait_for_model"),tH.forEach(t),Hj=p(Ay),Jt=o(Ay,"TD",{align:!0});var Dy=l(Jt);Bj=u(Dy,"(Default: "),ug=o(Dy,"CODE",{});var sH=l(ug);Cj=u(sH,"false"),sH.forEach(t),Gj=u(Dy,") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),Dy.forEach(t),Ay.forEach(t),H.forEach(t),oy.forEach(t),U_=p(a),Lu=o(a,"P",{});var aH=l(Lu);Lj=u(aH,"Return value is either a dict or a list of dicts if you sent a list of inputs"),aH.forEach(t),z_=p(a),Wt=o(a,"TABLE",{});var Oy=l(Wt);cg=o(Oy,"THEAD",{});var nH=l(cg);lr=o(nH,"TR",{});var Py=l(lr);Uu=o(Py,"TH",{align:!0});var rH=l(Uu);Uj=u(rH,"Returned values"),rH.forEach(t),zj=p(Py),fg=o(Py,"TH",{align:!0}),l(fg).forEach(t),Py.forEach(t),nH.forEach(t),Mj=p(Oy),ie=o(Oy,"TBODY",{});var Ha=l(ie);ir=o(Ha,"TR",{});var Ry=l(ir);zu=o(Ry,"TD",{align:!0});var oH=l(zu);pg=o(oH,"STRONG",{});var lH=l(pg);Fj=u(lH,"generated_text"),lH.forEach(t),oH.forEach(t),Kj=p(Ry),Mu=o(Ry,"TD",{align:!0});var iH=l(Mu);Jj=u(iH,"The answer of the bot"),iH.forEach(t),Ry.forEach(t),Wj=p(Ha),ur=o(Ha,"TR",{});var Ny=l(ur);Fu=o(Ny,"TD",{align:!0});var uH=l(Fu);dg=o(uH,"STRONG",{});var cH=l(dg);Yj=u(cH,"conversation"),cH.forEach(t),uH.forEach(t),Vj=p(Ny),Ku=o(Ny,"TD",{align:!0});var fH=l(Ku);Xj=u(fH,"A facility dictionnary to send back for the next input (with the new user input addition)."),fH.forEach(t),Ny.forEach(t),Qj=p(Ha),cr=o(Ha,"TR",{});var Sy=l(cr);Ju=o(Sy,"TD",{align:!0});var pH=l(Ju);Zj=u(pH,"past_user_inputs"),pH.forEach(t),ek=p(Sy),Wu=o(Sy,"TD",{align:!0});var dH=l(Wu);tk=u(dH,"List of strings. The last inputs from the user in the conversation, <em>after the model has run."),dH.forEach(t),Sy.forEach(t),sk=p(Ha),fr=o(Ha,"TR",{});var xy=l(fr);Yu=o(xy,"TD",{align:!0});var hH=l(Yu);ak=u(hH,"generated_responses"),hH.forEach(t),nk=p(xy),Vu=o(xy,"TD",{align:!0});var gH=l(Vu);rk=u(gH,"List of strings. The last outputs from the model in the conversation, <em>after the model has run."),gH.forEach(t),xy.forEach(t),Ha.forEach(t),Oy.forEach(t),M_=p(a),Ce=o(a,"H2",{class:!0});var Iy=l(Ce);Yt=o(Iy,"A",{id:!0,class:!0,href:!0});var mH=l(Yt);hg=o(mH,"SPAN",{});var qH=l(hg);v(pr.$$.fragment,qH),qH.forEach(t),mH.forEach(t),ok=p(Iy),gg=o(Iy,"SPAN",{});var $H=l(gg);lk=u($H,"Table question answering task"),$H.forEach(t),Iy.forEach(t),F_=p(a),Xu=o(a,"P",{});var _H=l(Xu);ik=u(_H,`Don\u2019t know SQL? Don\u2019t want to dive into a large spreadsheet? Ask
questions in plain english!`),_H.forEach(t),K_=p(a),v(Vt.$$.fragment,a),J_=p(a),dr=o(a,"P",{});var ZN=l(dr);uk=u(ZN,"Available with: "),hr=o(ZN,"A",{href:!0,rel:!0});var vH=l(hr);ck=u(vH,"\u{1F917} Transformers"),vH.forEach(t),ZN.forEach(t),W_=p(a),Qu=o(a,"P",{});var yH=l(Qu);fk=u(yH,"Example:"),yH.forEach(t),Y_=p(a),v(Xt.$$.fragment,a),V_=p(a),Zu=o(a,"P",{});var EH=l(Zu);pk=u(EH,`When sending your request, you should send a JSON encoded payload. Here
are all the options`),EH.forEach(t),X_=p(a),Qt=o(a,"TABLE",{});var Hy=l(Qt);mg=o(Hy,"THEAD",{});var wH=l(mg);gr=o(wH,"TR",{});var By=l(gr);ec=o(By,"TH",{align:!0});var bH=l(ec);dk=u(bH,"All parameters"),bH.forEach(t),hk=p(By),qg=o(By,"TH",{align:!0}),l(qg).forEach(t),By.forEach(t),wH.forEach(t),gk=p(Hy),K=o(Hy,"TBODY",{});var V=l(K);mr=o(V,"TR",{});var Cy=l(mr);qr=o(Cy,"TD",{align:!0});var eS=l(qr);$g=o(eS,"STRONG",{});var TH=l($g);mk=u(TH,"inputs"),TH.forEach(t),qk=u(eS," (required)"),eS.forEach(t),$k=p(Cy),_g=o(Cy,"TD",{align:!0}),l(_g).forEach(t),Cy.forEach(t),_k=p(V),$r=o(V,"TR",{});var Gy=l($r);tc=o(Gy,"TD",{align:!0});var jH=l(tc);vk=u(jH,"query (required)"),jH.forEach(t),yk=p(Gy),sc=o(Gy,"TD",{align:!0});var kH=l(sc);Ek=u(kH,"The query in plain text that you want to ask the table"),kH.forEach(t),Gy.forEach(t),wk=p(V),_r=o(V,"TR",{});var Ly=l(_r);ac=o(Ly,"TD",{align:!0});var AH=l(ac);bk=u(AH,"table (required)"),AH.forEach(t),Tk=p(Ly),nc=o(Ly,"TD",{align:!0});var DH=l(nc);jk=u(DH,"A table of data represented as a dict of list where entries are headers and the lists are all the values, all lists must have the same size."),DH.forEach(t),Ly.forEach(t),kk=p(V),vr=o(V,"TR",{});var Uy=l(vr);rc=o(Uy,"TD",{align:!0});var OH=l(rc);vg=o(OH,"STRONG",{});var PH=l(vg);Ak=u(PH,"options"),PH.forEach(t),OH.forEach(t),Dk=p(Uy),oc=o(Uy,"TD",{align:!0});var RH=l(oc);Ok=u(RH,"a dict containing the following keys:"),RH.forEach(t),Uy.forEach(t),Pk=p(V),yr=o(V,"TR",{});var zy=l(yr);lc=o(zy,"TD",{align:!0});var NH=l(lc);Rk=u(NH,"use_gpu"),NH.forEach(t),Nk=p(zy),Zt=o(zy,"TD",{align:!0});var My=l(Zt);Sk=u(My,"(Default: "),yg=o(My,"CODE",{});var SH=l(yg);xk=u(SH,"false"),SH.forEach(t),Ik=u(My,"). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),My.forEach(t),zy.forEach(t),Hk=p(V),Er=o(V,"TR",{});var Fy=l(Er);ic=o(Fy,"TD",{align:!0});var xH=l(ic);Bk=u(xH,"use_cache"),xH.forEach(t),Ck=p(Fy),es=o(Fy,"TD",{align:!0});var Ky=l(es);Gk=u(Ky,"(Default: "),Eg=o(Ky,"CODE",{});var IH=l(Eg);Lk=u(IH,"true"),IH.forEach(t),Uk=u(Ky,"). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),Ky.forEach(t),Fy.forEach(t),zk=p(V),wr=o(V,"TR",{});var Jy=l(wr);uc=o(Jy,"TD",{align:!0});var HH=l(uc);Mk=u(HH,"wait_for_model"),HH.forEach(t),Fk=p(Jy),ts=o(Jy,"TD",{align:!0});var Wy=l(ts);Kk=u(Wy,"(Default: "),wg=o(Wy,"CODE",{});var BH=l(wg);Jk=u(BH,"false"),BH.forEach(t),Wk=u(Wy,") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),Wy.forEach(t),Jy.forEach(t),V.forEach(t),Hy.forEach(t),Q_=p(a),cc=o(a,"P",{});var CH=l(cc);Yk=u(CH,"Return value is either a dict or a list of dicts if you sent a list of inputs"),CH.forEach(t),Z_=p(a),v(ss.$$.fragment,a),e1=p(a),as=o(a,"TABLE",{});var Yy=l(as);bg=o(Yy,"THEAD",{});var GH=l(bg);br=o(GH,"TR",{});var Vy=l(br);fc=o(Vy,"TH",{align:!0});var LH=l(fc);Vk=u(LH,"Returned values"),LH.forEach(t),Xk=p(Vy),Tg=o(Vy,"TH",{align:!0}),l(Tg).forEach(t),Vy.forEach(t),GH.forEach(t),Qk=p(Yy),ue=o(Yy,"TBODY",{});var Ba=l(ue);Tr=o(Ba,"TR",{});var Xy=l(Tr);pc=o(Xy,"TD",{align:!0});var UH=l(pc);jg=o(UH,"STRONG",{});var zH=l(jg);Zk=u(zH,"answer"),zH.forEach(t),UH.forEach(t),e6=p(Xy),dc=o(Xy,"TD",{align:!0});var MH=l(dc);t6=u(MH,"The plaintext answer"),MH.forEach(t),Xy.forEach(t),s6=p(Ba),jr=o(Ba,"TR",{});var Qy=l(jr);hc=o(Qy,"TD",{align:!0});var FH=l(hc);kg=o(FH,"STRONG",{});var KH=l(kg);a6=u(KH,"coordinates"),KH.forEach(t),FH.forEach(t),n6=p(Qy),gc=o(Qy,"TD",{align:!0});var JH=l(gc);r6=u(JH,"a list of coordinates of the cells referenced in the answer"),JH.forEach(t),Qy.forEach(t),o6=p(Ba),kr=o(Ba,"TR",{});var Zy=l(kr);mc=o(Zy,"TD",{align:!0});var WH=l(mc);Ag=o(WH,"STRONG",{});var YH=l(Ag);l6=u(YH,"cells"),YH.forEach(t),WH.forEach(t),i6=p(Zy),qc=o(Zy,"TD",{align:!0});var VH=l(qc);u6=u(VH,"a list of coordinates of the cells contents"),VH.forEach(t),Zy.forEach(t),c6=p(Ba),Ar=o(Ba,"TR",{});var eE=l(Ar);$c=o(eE,"TD",{align:!0});var XH=l($c);Dg=o(XH,"STRONG",{});var QH=l(Dg);f6=u(QH,"aggregator"),QH.forEach(t),XH.forEach(t),p6=p(eE),_c=o(eE,"TD",{align:!0});var ZH=l(_c);d6=u(ZH,"The aggregator used to get the answer"),ZH.forEach(t),eE.forEach(t),Ba.forEach(t),Yy.forEach(t),t1=p(a),Ge=o(a,"H2",{class:!0});var tE=l(Ge);ns=o(tE,"A",{id:!0,class:!0,href:!0});var eB=l(ns);Og=o(eB,"SPAN",{});var tB=l(Og);v(Dr.$$.fragment,tB),tB.forEach(t),eB.forEach(t),h6=p(tE),Pg=o(tE,"SPAN",{});var sB=l(Pg);g6=u(sB,"Question answering task"),sB.forEach(t),tE.forEach(t),s1=p(a),vc=o(a,"P",{});var aB=l(vc);m6=u(aB,"Want to have a nice know-it-all bot that can answer any question?"),aB.forEach(t),a1=p(a),v(rs.$$.fragment,a),n1=p(a),Le=o(a,"P",{});var Y$=l(Le);q6=u(Y$,"Available with: "),Or=o(Y$,"A",{href:!0,rel:!0});var nB=l(Or);$6=u(nB,"\u{1F917}Transformers"),nB.forEach(t),_6=u(Y$,` and
`),Pr=o(Y$,"A",{href:!0,rel:!0});var rB=l(Pr);v6=u(rB,"AllenNLP"),rB.forEach(t),Y$.forEach(t),r1=p(a),yc=o(a,"P",{});var oB=l(yc);y6=u(oB,"Example:"),oB.forEach(t),o1=p(a),v(os.$$.fragment,a),l1=p(a),Ec=o(a,"P",{});var lB=l(Ec);E6=u(lB,`When sending your request, you should send a JSON encoded payload. Here
are all the options`),lB.forEach(t),i1=p(a),wc=o(a,"P",{});var iB=l(wc);w6=u(iB,"Return value is either a dict or a list of dicts if you sent a list of inputs"),iB.forEach(t),u1=p(a),v(ls.$$.fragment,a),c1=p(a),is=o(a,"TABLE",{});var sE=l(is);Rg=o(sE,"THEAD",{});var uB=l(Rg);Rr=o(uB,"TR",{});var aE=l(Rr);bc=o(aE,"TH",{align:!0});var cB=l(bc);b6=u(cB,"Returned values"),cB.forEach(t),T6=p(aE),Ng=o(aE,"TH",{align:!0}),l(Ng).forEach(t),aE.forEach(t),uB.forEach(t),j6=p(sE),ce=o(sE,"TBODY",{});var Ca=l(ce);Nr=o(Ca,"TR",{});var nE=l(Nr);Tc=o(nE,"TD",{align:!0});var fB=l(Tc);Sg=o(fB,"STRONG",{});var pB=l(Sg);k6=u(pB,"answer"),pB.forEach(t),fB.forEach(t),A6=p(nE),jc=o(nE,"TD",{align:!0});var dB=l(jc);D6=u(dB,"A string that\u2019s the answer within the text."),dB.forEach(t),nE.forEach(t),O6=p(Ca),Sr=o(Ca,"TR",{});var rE=l(Sr);kc=o(rE,"TD",{align:!0});var hB=l(kc);xg=o(hB,"STRONG",{});var gB=l(xg);P6=u(gB,"score"),gB.forEach(t),hB.forEach(t),R6=p(rE),Ac=o(rE,"TD",{align:!0});var mB=l(Ac);N6=u(mB,"A float that represents how likely that the answer is correct"),mB.forEach(t),rE.forEach(t),S6=p(Ca),xr=o(Ca,"TR",{});var oE=l(xr);Dc=o(oE,"TD",{align:!0});var qB=l(Dc);Ig=o(qB,"STRONG",{});var $B=l(Ig);x6=u($B,"start"),$B.forEach(t),qB.forEach(t),I6=p(oE),us=o(oE,"TD",{align:!0});var lE=l(us);H6=u(lE,"The index (string wise) of the start of the answer within "),Hg=o(lE,"CODE",{});var _B=l(Hg);B6=u(_B,"context"),_B.forEach(t),C6=u(lE,"."),lE.forEach(t),oE.forEach(t),G6=p(Ca),Ir=o(Ca,"TR",{});var iE=l(Ir);Oc=o(iE,"TD",{align:!0});var vB=l(Oc);Bg=o(vB,"STRONG",{});var yB=l(Bg);L6=u(yB,"stop"),yB.forEach(t),vB.forEach(t),U6=p(iE),cs=o(iE,"TD",{align:!0});var uE=l(cs);z6=u(uE,"The index (string wise) of the stop of the answer within "),Cg=o(uE,"CODE",{});var EB=l(Cg);M6=u(EB,"context"),EB.forEach(t),F6=u(uE,"."),uE.forEach(t),iE.forEach(t),Ca.forEach(t),sE.forEach(t),f1=p(a),Ue=o(a,"H2",{class:!0});var cE=l(Ue);fs=o(cE,"A",{id:!0,class:!0,href:!0});var wB=l(fs);Gg=o(wB,"SPAN",{});var bB=l(Gg);v(Hr.$$.fragment,bB),bB.forEach(t),wB.forEach(t),K6=p(cE),Lg=o(cE,"SPAN",{});var TB=l(Lg);J6=u(TB,"Text-classification task"),TB.forEach(t),cE.forEach(t),p1=p(a),Pc=o(a,"P",{});var jB=l(Pc);W6=u(jB,`Usually used for sentiment-analysis this will output the likelihood of
classes of an input.`),jB.forEach(t),d1=p(a),v(ps.$$.fragment,a),h1=p(a),Br=o(a,"P",{});var tS=l(Br);Y6=u(tS,"Available with: "),Cr=o(tS,"A",{href:!0,rel:!0});var kB=l(Cr);V6=u(kB,"\u{1F917} Transformers"),kB.forEach(t),tS.forEach(t),g1=p(a),Rc=o(a,"P",{});var AB=l(Rc);X6=u(AB,"Example:"),AB.forEach(t),m1=p(a),v(ds.$$.fragment,a),q1=p(a),Nc=o(a,"P",{});var DB=l(Nc);Q6=u(DB,`When sending your request, you should send a JSON encoded payload. Here
are all the options`),DB.forEach(t),$1=p(a),hs=o(a,"TABLE",{});var fE=l(hs);Ug=o(fE,"THEAD",{});var OB=l(Ug);Gr=o(OB,"TR",{});var pE=l(Gr);Sc=o(pE,"TH",{align:!0});var PB=l(Sc);Z6=u(PB,"All parameters"),PB.forEach(t),e7=p(pE),zg=o(pE,"TH",{align:!0}),l(zg).forEach(t),pE.forEach(t),OB.forEach(t),t7=p(fE),ee=o(fE,"TBODY",{});var Ae=l(ee);Lr=o(Ae,"TR",{});var dE=l(Lr);Ur=o(dE,"TD",{align:!0});var sS=l(Ur);Mg=o(sS,"STRONG",{});var RB=l(Mg);s7=u(RB,"inputs"),RB.forEach(t),a7=u(sS," (required)"),sS.forEach(t),n7=p(dE),xc=o(dE,"TD",{align:!0});var NB=l(xc);r7=u(NB,"a string to be classified"),NB.forEach(t),dE.forEach(t),o7=p(Ae),zr=o(Ae,"TR",{});var hE=l(zr);Ic=o(hE,"TD",{align:!0});var SB=l(Ic);Fg=o(SB,"STRONG",{});var xB=l(Fg);l7=u(xB,"options"),xB.forEach(t),SB.forEach(t),i7=p(hE),Hc=o(hE,"TD",{align:!0});var IB=l(Hc);u7=u(IB,"a dict containing the following keys:"),IB.forEach(t),hE.forEach(t),c7=p(Ae),Mr=o(Ae,"TR",{});var gE=l(Mr);Bc=o(gE,"TD",{align:!0});var HB=l(Bc);f7=u(HB,"use_gpu"),HB.forEach(t),p7=p(gE),gs=o(gE,"TD",{align:!0});var mE=l(gs);d7=u(mE,"(Default: "),Kg=o(mE,"CODE",{});var BB=l(Kg);h7=u(BB,"false"),BB.forEach(t),g7=u(mE,"). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),mE.forEach(t),gE.forEach(t),m7=p(Ae),Fr=o(Ae,"TR",{});var qE=l(Fr);Cc=o(qE,"TD",{align:!0});var CB=l(Cc);q7=u(CB,"use_cache"),CB.forEach(t),$7=p(qE),ms=o(qE,"TD",{align:!0});var $E=l(ms);_7=u($E,"(Default: "),Jg=o($E,"CODE",{});var GB=l(Jg);v7=u(GB,"true"),GB.forEach(t),y7=u($E,"). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),$E.forEach(t),qE.forEach(t),E7=p(Ae),Kr=o(Ae,"TR",{});var _E=l(Kr);Gc=o(_E,"TD",{align:!0});var LB=l(Gc);w7=u(LB,"wait_for_model"),LB.forEach(t),b7=p(_E),qs=o(_E,"TD",{align:!0});var vE=l(qs);T7=u(vE,"(Default: "),Wg=o(vE,"CODE",{});var UB=l(Wg);j7=u(UB,"false"),UB.forEach(t),k7=u(vE,") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),vE.forEach(t),_E.forEach(t),Ae.forEach(t),fE.forEach(t),_1=p(a),Lc=o(a,"P",{});var zB=l(Lc);A7=u(zB,"Return value is either a dict or a list of dicts if you sent a list of inputs"),zB.forEach(t),v1=p(a),v($s.$$.fragment,a),y1=p(a),_s=o(a,"TABLE",{});var yE=l(_s);Yg=o(yE,"THEAD",{});var MB=l(Yg);Jr=o(MB,"TR",{});var EE=l(Jr);Uc=o(EE,"TH",{align:!0});var FB=l(Uc);D7=u(FB,"Returned values"),FB.forEach(t),O7=p(EE),Vg=o(EE,"TH",{align:!0}),l(Vg).forEach(t),EE.forEach(t),MB.forEach(t),P7=p(yE),Wr=o(yE,"TBODY",{});var wE=l(Wr);Yr=o(wE,"TR",{});var bE=l(Yr);zc=o(bE,"TD",{align:!0});var KB=l(zc);Xg=o(KB,"STRONG",{});var JB=l(Xg);R7=u(JB,"label"),JB.forEach(t),KB.forEach(t),N7=p(bE),Mc=o(bE,"TD",{align:!0});var WB=l(Mc);S7=u(WB,"The label for the class (model specific)"),WB.forEach(t),bE.forEach(t),x7=p(wE),Vr=o(wE,"TR",{});var TE=l(Vr);Fc=o(TE,"TD",{align:!0});var YB=l(Fc);Qg=o(YB,"STRONG",{});var VB=l(Qg);I7=u(VB,"score"),VB.forEach(t),YB.forEach(t),H7=p(TE),Kc=o(TE,"TD",{align:!0});var XB=l(Kc);B7=u(XB,"A floats that represents how likely is that the text belongs the this class."),XB.forEach(t),TE.forEach(t),wE.forEach(t),yE.forEach(t),E1=p(a),ze=o(a,"H2",{class:!0});var jE=l(ze);vs=o(jE,"A",{id:!0,class:!0,href:!0});var QB=l(vs);Zg=o(QB,"SPAN",{});var ZB=l(Zg);v(Xr.$$.fragment,ZB),ZB.forEach(t),QB.forEach(t),C7=p(jE),em=o(jE,"SPAN",{});var eC=l(em);G7=u(eC,"Named Entity Recognition (NER) task"),eC.forEach(t),jE.forEach(t),w1=p(a),Qr=o(a,"P",{});var aS=l(Qr);L7=u(aS,"See "),Jc=o(aS,"A",{href:!0});var tC=l(Jc);U7=u(tC,"Token-classification task"),tC.forEach(t),aS.forEach(t),b1=p(a),Me=o(a,"H2",{class:!0});var kE=l(Me);ys=o(kE,"A",{id:!0,class:!0,href:!0});var sC=l(ys);tm=o(sC,"SPAN",{});var aC=l(tm);v(Zr.$$.fragment,aC),aC.forEach(t),sC.forEach(t),z7=p(kE),sm=o(kE,"SPAN",{});var nC=l(sm);M7=u(nC,"Token-classification task"),nC.forEach(t),kE.forEach(t),T1=p(a),Wc=o(a,"P",{});var rC=l(Wc);F7=u(rC,`Usually used for sentence parsing, either grammatical, or Named Entity
Recognition (NER) to understand keywords contained within text.`),rC.forEach(t),j1=p(a),v(Es.$$.fragment,a),k1=p(a),Fe=o(a,"P",{});var V$=l(Fe);K7=u(V$,"Available with: "),eo=o(V$,"A",{href:!0,rel:!0});var oC=l(eo);J7=u(oC,"\u{1F917} Transformers"),oC.forEach(t),W7=u(V$,`,
`),to=o(V$,"A",{href:!0,rel:!0});var lC=l(to);Y7=u(lC,"Flair"),lC.forEach(t),V$.forEach(t),A1=p(a),Yc=o(a,"P",{});var iC=l(Yc);V7=u(iC,"Example:"),iC.forEach(t),D1=p(a),v(ws.$$.fragment,a),O1=p(a),Vc=o(a,"P",{});var uC=l(Vc);X7=u(uC,`When sending your request, you should send a JSON encoded payload. Here
are all the options`),uC.forEach(t),P1=p(a),bs=o(a,"TABLE",{});var AE=l(bs);am=o(AE,"THEAD",{});var cC=l(am);so=o(cC,"TR",{});var DE=l(so);Xc=o(DE,"TH",{align:!0});var fC=l(Xc);Q7=u(fC,"All parameters"),fC.forEach(t),Z7=p(DE),nm=o(DE,"TH",{align:!0}),l(nm).forEach(t),DE.forEach(t),cC.forEach(t),e9=p(AE),J=o(AE,"TBODY",{});var X=l(J);ao=o(X,"TR",{});var OE=l(ao);no=o(OE,"TD",{align:!0});var nS=l(no);rm=o(nS,"STRONG",{});var pC=l(rm);t9=u(pC,"inputs"),pC.forEach(t),s9=u(nS," (required)"),nS.forEach(t),a9=p(OE),Qc=o(OE,"TD",{align:!0});var dC=l(Qc);n9=u(dC,"a string to be classified"),dC.forEach(t),OE.forEach(t),r9=p(X),ro=o(X,"TR",{});var PE=l(ro);Zc=o(PE,"TD",{align:!0});var hC=l(Zc);om=o(hC,"STRONG",{});var gC=l(om);o9=u(gC,"parameters"),gC.forEach(t),hC.forEach(t),l9=p(PE),ef=o(PE,"TD",{align:!0});var mC=l(ef);i9=u(mC,"a dict containing the following key:"),mC.forEach(t),PE.forEach(t),u9=p(X),oo=o(X,"TR",{});var RE=l(oo);tf=o(RE,"TD",{align:!0});var qC=l(tf);c9=u(qC,"aggregation_strategy"),qC.forEach(t),f9=p(RE),x=o(RE,"TD",{align:!0});var B=l(x);p9=u(B,"(Default: "),lm=o(B,"CODE",{});var $C=l(lm);d9=u($C,"simple"),$C.forEach(t),h9=u(B,"). There are several aggregation strategies: "),g9=o(B,"BR",{}),m9=p(B),im=o(B,"CODE",{});var _C=l(im);q9=u(_C,"none"),_C.forEach(t),$9=u(B,": Every token gets classified without further aggregation. "),_9=o(B,"BR",{}),v9=p(B),um=o(B,"CODE",{});var vC=l(um);y9=u(vC,"simple"),vC.forEach(t),E9=u(B,": Entities are grouped according to the default schema (B-, I- tags get merged when the tag is similar). "),w9=o(B,"BR",{}),b9=p(B),cm=o(B,"CODE",{});var yC=l(cm);T9=u(yC,"first"),yC.forEach(t),j9=u(B,": Same as the "),fm=o(B,"CODE",{});var EC=l(fm);k9=u(EC,"simple"),EC.forEach(t),A9=u(B," strategy except words cannot end up with different tags. Words will use the tag of the first token when there is ambiguity. "),D9=o(B,"BR",{}),O9=p(B),pm=o(B,"CODE",{});var wC=l(pm);P9=u(wC,"average"),wC.forEach(t),R9=u(B,": Same as the "),dm=o(B,"CODE",{});var bC=l(dm);N9=u(bC,"simple"),bC.forEach(t),S9=u(B," strategy except words cannot end up with different tags. Scores are averaged across tokens and then the maximum label is applied. "),x9=o(B,"BR",{}),I9=p(B),hm=o(B,"CODE",{});var TC=l(hm);H9=u(TC,"max"),TC.forEach(t),B9=u(B,": Same as the "),gm=o(B,"CODE",{});var jC=l(gm);C9=u(jC,"simple"),jC.forEach(t),G9=u(B," strategy except words cannot end up with different tags. Word entity will be the token with the maximum score."),B.forEach(t),RE.forEach(t),L9=p(X),lo=o(X,"TR",{});var NE=l(lo);sf=o(NE,"TD",{align:!0});var kC=l(sf);mm=o(kC,"STRONG",{});var AC=l(mm);U9=u(AC,"options"),AC.forEach(t),kC.forEach(t),z9=p(NE),af=o(NE,"TD",{align:!0});var DC=l(af);M9=u(DC,"a dict containing the following keys:"),DC.forEach(t),NE.forEach(t),F9=p(X),io=o(X,"TR",{});var SE=l(io);nf=o(SE,"TD",{align:!0});var OC=l(nf);K9=u(OC,"use_gpu"),OC.forEach(t),J9=p(SE),Ts=o(SE,"TD",{align:!0});var xE=l(Ts);W9=u(xE,"(Default: "),qm=o(xE,"CODE",{});var PC=l(qm);Y9=u(PC,"false"),PC.forEach(t),V9=u(xE,"). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),xE.forEach(t),SE.forEach(t),X9=p(X),uo=o(X,"TR",{});var IE=l(uo);rf=o(IE,"TD",{align:!0});var RC=l(rf);Q9=u(RC,"use_cache"),RC.forEach(t),Z9=p(IE),js=o(IE,"TD",{align:!0});var HE=l(js);e8=u(HE,"(Default: "),$m=o(HE,"CODE",{});var NC=l($m);t8=u(NC,"true"),NC.forEach(t),s8=u(HE,"). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),HE.forEach(t),IE.forEach(t),a8=p(X),co=o(X,"TR",{});var BE=l(co);of=o(BE,"TD",{align:!0});var SC=l(of);n8=u(SC,"wait_for_model"),SC.forEach(t),r8=p(BE),ks=o(BE,"TD",{align:!0});var CE=l(ks);o8=u(CE,"(Default: "),_m=o(CE,"CODE",{});var xC=l(_m);l8=u(xC,"false"),xC.forEach(t),i8=u(CE,") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),CE.forEach(t),BE.forEach(t),X.forEach(t),AE.forEach(t),R1=p(a),lf=o(a,"P",{});var IC=l(lf);u8=u(IC,"Return value is either a dict or a list of dicts if you sent a list of inputs"),IC.forEach(t),N1=p(a),v(As.$$.fragment,a),S1=p(a),Ds=o(a,"TABLE",{});var GE=l(Ds);vm=o(GE,"THEAD",{});var HC=l(vm);fo=o(HC,"TR",{});var LE=l(fo);uf=o(LE,"TH",{align:!0});var BC=l(uf);c8=u(BC,"Returned values"),BC.forEach(t),f8=p(LE),ym=o(LE,"TH",{align:!0}),l(ym).forEach(t),LE.forEach(t),HC.forEach(t),p8=p(GE),te=o(GE,"TBODY",{});var De=l(te);po=o(De,"TR",{});var UE=l(po);cf=o(UE,"TD",{align:!0});var CC=l(cf);Em=o(CC,"STRONG",{});var GC=l(Em);d8=u(GC,"entity_group"),GC.forEach(t),CC.forEach(t),h8=p(UE),ff=o(UE,"TD",{align:!0});var LC=l(ff);g8=u(LC,"The type for the entity being recognized (model specific)."),LC.forEach(t),UE.forEach(t),m8=p(De),ho=o(De,"TR",{});var zE=l(ho);pf=o(zE,"TD",{align:!0});var UC=l(pf);wm=o(UC,"STRONG",{});var zC=l(wm);q8=u(zC,"score"),zC.forEach(t),UC.forEach(t),$8=p(zE),df=o(zE,"TD",{align:!0});var MC=l(df);_8=u(MC,"How likely the entity was recognized."),MC.forEach(t),zE.forEach(t),v8=p(De),go=o(De,"TR",{});var ME=l(go);hf=o(ME,"TD",{align:!0});var FC=l(hf);bm=o(FC,"STRONG",{});var KC=l(bm);y8=u(KC,"word"),KC.forEach(t),FC.forEach(t),E8=p(ME),gf=o(ME,"TD",{align:!0});var JC=l(gf);w8=u(JC,"The string that was captured"),JC.forEach(t),ME.forEach(t),b8=p(De),mo=o(De,"TR",{});var FE=l(mo);mf=o(FE,"TD",{align:!0});var WC=l(mf);Tm=o(WC,"STRONG",{});var YC=l(Tm);T8=u(YC,"start"),YC.forEach(t),WC.forEach(t),j8=p(FE),Os=o(FE,"TD",{align:!0});var KE=l(Os);k8=u(KE,"The offset stringwise where the answer is located. Useful to disambiguate if "),jm=o(KE,"CODE",{});var VC=l(jm);A8=u(VC,"word"),VC.forEach(t),D8=u(KE," occurs multiple times."),KE.forEach(t),FE.forEach(t),O8=p(De),qo=o(De,"TR",{});var JE=l(qo);qf=o(JE,"TD",{align:!0});var XC=l(qf);km=o(XC,"STRONG",{});var QC=l(km);P8=u(QC,"end"),QC.forEach(t),XC.forEach(t),R8=p(JE),Ps=o(JE,"TD",{align:!0});var WE=l(Ps);N8=u(WE,"The offset stringwise where the answer is located. Useful to disambiguate if "),Am=o(WE,"CODE",{});var ZC=l(Am);S8=u(ZC,"word"),ZC.forEach(t),x8=u(WE," occurs multiple times."),WE.forEach(t),JE.forEach(t),De.forEach(t),GE.forEach(t),x1=p(a),Ke=o(a,"H2",{class:!0});var YE=l(Ke);Rs=o(YE,"A",{id:!0,class:!0,href:!0});var eG=l(Rs);Dm=o(eG,"SPAN",{});var tG=l(Dm);v($o.$$.fragment,tG),tG.forEach(t),eG.forEach(t),I8=p(YE),Om=o(YE,"SPAN",{});var sG=l(Om);H8=u(sG,"Text-generation task"),sG.forEach(t),YE.forEach(t),I1=p(a),$f=o(a,"P",{});var aG=l($f);B8=u(aG,"Use to continue text from a prompt. This is a very generic task."),aG.forEach(t),H1=p(a),v(Ns.$$.fragment,a),B1=p(a),_o=o(a,"P",{});var rS=l(_o);C8=u(rS,"Available with: "),vo=o(rS,"A",{href:!0,rel:!0});var nG=l(vo);G8=u(nG,"\u{1F917} Transformers"),nG.forEach(t),rS.forEach(t),C1=p(a),_f=o(a,"P",{});var rG=l(_f);L8=u(rG,"Example:"),rG.forEach(t),G1=p(a),v(Ss.$$.fragment,a),L1=p(a),vf=o(a,"P",{});var oG=l(vf);U8=u(oG,`When sending your request, you should send a JSON encoded payload. Here
are all the options`),oG.forEach(t),U1=p(a),xs=o(a,"TABLE",{});var VE=l(xs);Pm=o(VE,"THEAD",{});var lG=l(Pm);yo=o(lG,"TR",{});var XE=l(yo);yf=o(XE,"TH",{align:!0});var iG=l(yf);z8=u(iG,"All parameters"),iG.forEach(t),M8=p(XE),Rm=o(XE,"TH",{align:!0}),l(Rm).forEach(t),XE.forEach(t),lG.forEach(t),F8=p(VE),I=o(VE,"TBODY",{});var C=l(I);Eo=o(C,"TR",{});var QE=l(Eo);wo=o(QE,"TD",{align:!0});var oS=l(wo);Nm=o(oS,"STRONG",{});var uG=l(Nm);K8=u(uG,"inputs"),uG.forEach(t),J8=u(oS," (required):"),oS.forEach(t),W8=p(QE),Ef=o(QE,"TD",{align:!0});var cG=l(Ef);Y8=u(cG,"a string to be generated from"),cG.forEach(t),QE.forEach(t),V8=p(C),bo=o(C,"TR",{});var ZE=l(bo);wf=o(ZE,"TD",{align:!0});var fG=l(wf);Sm=o(fG,"STRONG",{});var pG=l(Sm);X8=u(pG,"parameters"),pG.forEach(t),fG.forEach(t),Q8=p(ZE),bf=o(ZE,"TD",{align:!0});var dG=l(bf);Z8=u(dG,"dict containing the following keys:"),dG.forEach(t),ZE.forEach(t),eA=p(C),To=o(C,"TR",{});var e0=l(To);Tf=o(e0,"TD",{align:!0});var hG=l(Tf);tA=u(hG,"top_k"),hG.forEach(t),sA=p(e0),Ee=o(e0,"TD",{align:!0});var kd=l(Ee);aA=u(kd,"(Default: "),xm=o(kd,"CODE",{});var gG=l(xm);nA=u(gG,"None"),gG.forEach(t),rA=u(kd,"). Integer to define the top tokens considered within the "),Im=o(kd,"CODE",{});var mG=l(Im);oA=u(mG,"sample"),mG.forEach(t),lA=u(kd," operation to create new text."),kd.forEach(t),e0.forEach(t),iA=p(C),jo=o(C,"TR",{});var t0=l(jo);jf=o(t0,"TD",{align:!0});var qG=l(jf);uA=u(qG,"top_p"),qG.forEach(t),cA=p(t0),le=o(t0,"TD",{align:!0});var Ga=l(le);fA=u(Ga,"(Default: "),Hm=o(Ga,"CODE",{});var $G=l(Hm);pA=u($G,"None"),$G.forEach(t),dA=u(Ga,"). Float to define the tokens that are within the "),Bm=o(Ga,"CODE",{});var _G=l(Bm);hA=u(_G,"sample"),_G.forEach(t),gA=u(Ga," operation of text generation. Add tokens in the sample for more probable to least probable until the sum of the probabilities is greater than "),Cm=o(Ga,"CODE",{});var vG=l(Cm);mA=u(vG,"top_p"),vG.forEach(t),qA=u(Ga,"."),Ga.forEach(t),t0.forEach(t),$A=p(C),ko=o(C,"TR",{});var s0=l(ko);kf=o(s0,"TD",{align:!0});var yG=l(kf);_A=u(yG,"temperature"),yG.forEach(t),vA=p(s0),we=o(s0,"TD",{align:!0});var Ad=l(we);yA=u(Ad,"(Default: "),Gm=o(Ad,"CODE",{});var EG=l(Gm);EA=u(EG,"1.0"),EG.forEach(t),wA=u(Ad,"). Float (0.0-100.0). The temperature of the sampling operation. 1 means regular sampling, 0 means "),Lm=o(Ad,"CODE",{});var wG=l(Lm);bA=u(wG,"100.0"),wG.forEach(t),TA=u(Ad," is getting closer to uniform probability."),Ad.forEach(t),s0.forEach(t),jA=p(C),Ao=o(C,"TR",{});var a0=l(Ao);Af=o(a0,"TD",{align:!0});var bG=l(Af);kA=u(bG,"repetition_penalty"),bG.forEach(t),AA=p(a0),Is=o(a0,"TD",{align:!0});var n0=l(Is);DA=u(n0,"(Default: "),Um=o(n0,"CODE",{});var TG=l(Um);OA=u(TG,"None"),TG.forEach(t),PA=u(n0,"). Float (0.0-100.0). The more a token is used within generation the more it is penalized to not be picked in successive generation passes."),n0.forEach(t),a0.forEach(t),RA=p(C),Do=o(C,"TR",{});var r0=l(Do);Df=o(r0,"TD",{align:!0});var jG=l(Df);NA=u(jG,"max_new_tokens"),jG.forEach(t),SA=p(r0),be=o(r0,"TD",{align:!0});var Dd=l(be);xA=u(Dd,"(Default: "),zm=o(Dd,"CODE",{});var kG=l(zm);IA=u(kG,"None"),kG.forEach(t),HA=u(Dd,"). Int (0-250). The amount of new tokens to be generated, this does "),Mm=o(Dd,"STRONG",{});var AG=l(Mm);BA=u(AG,"not"),AG.forEach(t),CA=u(Dd," include the input length it is a estimate of the size of generated text you want. Each new tokens slows down the request, so look for balance between response times and length of text generated."),Dd.forEach(t),r0.forEach(t),GA=p(C),Oo=o(C,"TR",{});var o0=l(Oo);Of=o(o0,"TD",{align:!0});var DG=l(Of);LA=u(DG,"max_time"),DG.forEach(t),UA=p(o0),Te=o(o0,"TD",{align:!0});var Od=l(Te);zA=u(Od,"(Default: "),Fm=o(Od,"CODE",{});var OG=l(Fm);MA=u(OG,"None"),OG.forEach(t),FA=u(Od,"). Float (0-120.0). The amount of time in seconds that the query should take maximum. Network can cause some overhead so it will be a soft limit. Use that in combination with "),Km=o(Od,"CODE",{});var PG=l(Km);KA=u(PG,"max_new_tokens"),PG.forEach(t),JA=u(Od," for best results."),Od.forEach(t),o0.forEach(t),WA=p(C),Po=o(C,"TR",{});var l0=l(Po);Pf=o(l0,"TD",{align:!0});var RG=l(Pf);YA=u(RG,"return_full_text"),RG.forEach(t),VA=p(l0),je=o(l0,"TD",{align:!0});var Pd=l(je);XA=u(Pd,"(Default: "),Jm=o(Pd,"CODE",{});var NG=l(Jm);QA=u(NG,"True"),NG.forEach(t),ZA=u(Pd,"). Bool. If set to False, the return results will "),Wm=o(Pd,"STRONG",{});var SG=l(Wm);eD=u(SG,"not"),SG.forEach(t),tD=u(Pd," contain the original query making it easier for prompting."),Pd.forEach(t),l0.forEach(t),sD=p(C),Ro=o(C,"TR",{});var i0=l(Ro);Rf=o(i0,"TD",{align:!0});var xG=l(Rf);aD=u(xG,"num_return_sequences"),xG.forEach(t),nD=p(i0),Hs=o(i0,"TD",{align:!0});var u0=l(Hs);rD=u(u0,"(Default: "),Ym=o(u0,"CODE",{});var IG=l(Ym);oD=u(IG,"1"),IG.forEach(t),lD=u(u0,"). Integer. The number of proposition you want to be returned."),u0.forEach(t),i0.forEach(t),iD=p(C),No=o(C,"TR",{});var c0=l(No);Nf=o(c0,"TD",{align:!0});var HG=l(Nf);uD=u(HG,"do_sample"),HG.forEach(t),cD=p(c0),Bs=o(c0,"TD",{align:!0});var f0=l(Bs);fD=u(f0,"(Optional: "),Vm=o(f0,"CODE",{});var BG=l(Vm);pD=u(BG,"True"),BG.forEach(t),dD=u(f0,"). Bool. Whether or not to use sampling, use greedy decoding otherwise."),f0.forEach(t),c0.forEach(t),hD=p(C),So=o(C,"TR",{});var p0=l(So);Sf=o(p0,"TD",{align:!0});var CG=l(Sf);Xm=o(CG,"STRONG",{});var GG=l(Xm);gD=u(GG,"options"),GG.forEach(t),CG.forEach(t),mD=p(p0),xf=o(p0,"TD",{align:!0});var LG=l(xf);qD=u(LG,"a dict containing the following keys:"),LG.forEach(t),p0.forEach(t),$D=p(C),xo=o(C,"TR",{});var d0=l(xo);If=o(d0,"TD",{align:!0});var UG=l(If);_D=u(UG,"use_gpu"),UG.forEach(t),vD=p(d0),Cs=o(d0,"TD",{align:!0});var h0=l(Cs);yD=u(h0,"(Default: "),Qm=o(h0,"CODE",{});var zG=l(Qm);ED=u(zG,"false"),zG.forEach(t),wD=u(h0,"). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),h0.forEach(t),d0.forEach(t),bD=p(C),Io=o(C,"TR",{});var g0=l(Io);Hf=o(g0,"TD",{align:!0});var MG=l(Hf);TD=u(MG,"use_cache"),MG.forEach(t),jD=p(g0),Gs=o(g0,"TD",{align:!0});var m0=l(Gs);kD=u(m0,"(Default: "),Zm=o(m0,"CODE",{});var FG=l(Zm);AD=u(FG,"true"),FG.forEach(t),DD=u(m0,"). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),m0.forEach(t),g0.forEach(t),OD=p(C),Ho=o(C,"TR",{});var q0=l(Ho);Bf=o(q0,"TD",{align:!0});var KG=l(Bf);PD=u(KG,"wait_for_model"),KG.forEach(t),RD=p(q0),Ls=o(q0,"TD",{align:!0});var $0=l(Ls);ND=u($0,"(Default: "),eq=o($0,"CODE",{});var JG=l(eq);SD=u(JG,"false"),JG.forEach(t),xD=u($0,") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),$0.forEach(t),q0.forEach(t),C.forEach(t),VE.forEach(t),z1=p(a),Cf=o(a,"P",{});var WG=l(Cf);ID=u(WG,"Return value is either a dict or a list of dicts if you sent a list of inputs"),WG.forEach(t),M1=p(a),v(Us.$$.fragment,a),F1=p(a),zs=o(a,"TABLE",{});var _0=l(zs);tq=o(_0,"THEAD",{});var YG=l(tq);Bo=o(YG,"TR",{});var v0=l(Bo);Gf=o(v0,"TH",{align:!0});var VG=l(Gf);HD=u(VG,"Returned values"),VG.forEach(t),BD=p(v0),sq=o(v0,"TH",{align:!0}),l(sq).forEach(t),v0.forEach(t),YG.forEach(t),CD=p(_0),aq=o(_0,"TBODY",{});var XG=l(aq);Co=o(XG,"TR",{});var y0=l(Co);Lf=o(y0,"TD",{align:!0});var QG=l(Lf);nq=o(QG,"STRONG",{});var ZG=l(nq);GD=u(ZG,"generated_text"),ZG.forEach(t),QG.forEach(t),LD=p(y0),Uf=o(y0,"TD",{align:!0});var eL=l(Uf);UD=u(eL,"The continuated string"),eL.forEach(t),y0.forEach(t),XG.forEach(t),_0.forEach(t),K1=p(a),Je=o(a,"H2",{class:!0});var E0=l(Je);Ms=o(E0,"A",{id:!0,class:!0,href:!0});var tL=l(Ms);rq=o(tL,"SPAN",{});var sL=l(rq);v(Go.$$.fragment,sL),sL.forEach(t),tL.forEach(t),zD=p(E0),oq=o(E0,"SPAN",{});var aL=l(oq);MD=u(aL,"Text2text-generation task"),aL.forEach(t),E0.forEach(t),J1=p(a),Fs=o(a,"P",{});var w0=l(Fs);FD=u(w0,"Essentially "),zf=o(w0,"A",{href:!0});var nL=l(zf);KD=u(nL,"Text-generation task"),nL.forEach(t),JD=u(w0,`. But uses
Encoder-Decoder architecture, so might change in the future for more
options.`),w0.forEach(t),W1=p(a),We=o(a,"H2",{class:!0});var b0=l(We);Ks=o(b0,"A",{id:!0,class:!0,href:!0});var rL=l(Ks);lq=o(rL,"SPAN",{});var oL=l(lq);v(Lo.$$.fragment,oL),oL.forEach(t),rL.forEach(t),WD=p(b0),iq=o(b0,"SPAN",{});var lL=l(iq);YD=u(lL,"Fill mask task"),lL.forEach(t),b0.forEach(t),Y1=p(a),Mf=o(a,"P",{});var iL=l(Mf);VD=u(iL,`Tries to fill in a hole with a missing word (token to be precise).
That\u2019s the base task for BERT models.`),iL.forEach(t),V1=p(a),v(Js.$$.fragment,a),X1=p(a),Uo=o(a,"P",{});var lS=l(Uo);XD=u(lS,"Available with: "),zo=o(lS,"A",{href:!0,rel:!0});var uL=l(zo);QD=u(uL,"\u{1F917} Transformers"),uL.forEach(t),lS.forEach(t),Q1=p(a),Ff=o(a,"P",{});var cL=l(Ff);ZD=u(cL,"Example:"),cL.forEach(t),Z1=p(a),v(Ws.$$.fragment,a),e2=p(a),Kf=o(a,"P",{});var fL=l(Kf);eO=u(fL,`When sending your request, you should send a JSON encoded payload. Here
are all the options`),fL.forEach(t),t2=p(a),Ys=o(a,"TABLE",{});var T0=l(Ys);uq=o(T0,"THEAD",{});var pL=l(uq);Mo=o(pL,"TR",{});var j0=l(Mo);Jf=o(j0,"TH",{align:!0});var dL=l(Jf);tO=u(dL,"All parameters"),dL.forEach(t),sO=p(j0),cq=o(j0,"TH",{align:!0}),l(cq).forEach(t),j0.forEach(t),pL.forEach(t),aO=p(T0),se=o(T0,"TBODY",{});var Oe=l(se);Fo=o(Oe,"TR",{});var k0=l(Fo);Ko=o(k0,"TD",{align:!0});var iS=l(Ko);fq=o(iS,"STRONG",{});var hL=l(fq);nO=u(hL,"inputs"),hL.forEach(t),rO=u(iS," (required):"),iS.forEach(t),oO=p(k0),Wf=o(k0,"TD",{align:!0});var gL=l(Wf);lO=u(gL,"a string to be filled from, must contain the [MASK] token (check model card for exact name of the mask)"),gL.forEach(t),k0.forEach(t),iO=p(Oe),Jo=o(Oe,"TR",{});var A0=l(Jo);Yf=o(A0,"TD",{align:!0});var mL=l(Yf);pq=o(mL,"STRONG",{});var qL=l(pq);uO=u(qL,"options"),qL.forEach(t),mL.forEach(t),cO=p(A0),Vf=o(A0,"TD",{align:!0});var $L=l(Vf);fO=u($L,"a dict containing the following keys:"),$L.forEach(t),A0.forEach(t),pO=p(Oe),Wo=o(Oe,"TR",{});var D0=l(Wo);Xf=o(D0,"TD",{align:!0});var _L=l(Xf);dO=u(_L,"use_gpu"),_L.forEach(t),hO=p(D0),Vs=o(D0,"TD",{align:!0});var O0=l(Vs);gO=u(O0,"(Default: "),dq=o(O0,"CODE",{});var vL=l(dq);mO=u(vL,"false"),vL.forEach(t),qO=u(O0,"). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),O0.forEach(t),D0.forEach(t),$O=p(Oe),Yo=o(Oe,"TR",{});var P0=l(Yo);Qf=o(P0,"TD",{align:!0});var yL=l(Qf);_O=u(yL,"use_cache"),yL.forEach(t),vO=p(P0),Xs=o(P0,"TD",{align:!0});var R0=l(Xs);yO=u(R0,"(Default: "),hq=o(R0,"CODE",{});var EL=l(hq);EO=u(EL,"true"),EL.forEach(t),wO=u(R0,"). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),R0.forEach(t),P0.forEach(t),bO=p(Oe),Vo=o(Oe,"TR",{});var N0=l(Vo);Zf=o(N0,"TD",{align:!0});var wL=l(Zf);TO=u(wL,"wait_for_model"),wL.forEach(t),jO=p(N0),Qs=o(N0,"TD",{align:!0});var S0=l(Qs);kO=u(S0,"(Default: "),gq=o(S0,"CODE",{});var bL=l(gq);AO=u(bL,"false"),bL.forEach(t),DO=u(S0,") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),S0.forEach(t),N0.forEach(t),Oe.forEach(t),T0.forEach(t),s2=p(a),ep=o(a,"P",{});var TL=l(ep);OO=u(TL,"Return value is either a dict or a list of dicts if you sent a list of inputs"),TL.forEach(t),a2=p(a),v(Zs.$$.fragment,a),n2=p(a),ea=o(a,"TABLE",{});var x0=l(ea);mq=o(x0,"THEAD",{});var jL=l(mq);Xo=o(jL,"TR",{});var I0=l(Xo);tp=o(I0,"TH",{align:!0});var kL=l(tp);PO=u(kL,"Returned values"),kL.forEach(t),RO=p(I0),qq=o(I0,"TH",{align:!0}),l(qq).forEach(t),I0.forEach(t),jL.forEach(t),NO=p(x0),fe=o(x0,"TBODY",{});var La=l(fe);Qo=o(La,"TR",{});var H0=l(Qo);sp=o(H0,"TD",{align:!0});var AL=l(sp);$q=o(AL,"STRONG",{});var DL=l($q);SO=u(DL,"sequence"),DL.forEach(t),AL.forEach(t),xO=p(H0),ap=o(H0,"TD",{align:!0});var OL=l(ap);IO=u(OL,"The actual sequence of tokens that ran against the model (may contain special tokens)"),OL.forEach(t),H0.forEach(t),HO=p(La),Zo=o(La,"TR",{});var B0=l(Zo);np=o(B0,"TD",{align:!0});var PL=l(np);_q=o(PL,"STRONG",{});var RL=l(_q);BO=u(RL,"score"),RL.forEach(t),PL.forEach(t),CO=p(B0),rp=o(B0,"TD",{align:!0});var NL=l(rp);GO=u(NL,"The probability for this token."),NL.forEach(t),B0.forEach(t),LO=p(La),el=o(La,"TR",{});var C0=l(el);op=o(C0,"TD",{align:!0});var SL=l(op);vq=o(SL,"STRONG",{});var xL=l(vq);UO=u(xL,"token"),xL.forEach(t),SL.forEach(t),zO=p(C0),lp=o(C0,"TD",{align:!0});var IL=l(lp);MO=u(IL,"The id of the token"),IL.forEach(t),C0.forEach(t),FO=p(La),tl=o(La,"TR",{});var G0=l(tl);ip=o(G0,"TD",{align:!0});var HL=l(ip);yq=o(HL,"STRONG",{});var BL=l(yq);KO=u(BL,"token_str"),BL.forEach(t),HL.forEach(t),JO=p(G0),up=o(G0,"TD",{align:!0});var CL=l(up);WO=u(CL,"The string representation of the token"),CL.forEach(t),G0.forEach(t),La.forEach(t),x0.forEach(t),r2=p(a),Ye=o(a,"H2",{class:!0});var L0=l(Ye);ta=o(L0,"A",{id:!0,class:!0,href:!0});var GL=l(ta);Eq=o(GL,"SPAN",{});var LL=l(Eq);v(sl.$$.fragment,LL),LL.forEach(t),GL.forEach(t),YO=p(L0),wq=o(L0,"SPAN",{});var UL=l(wq);VO=u(UL,"Automatic speech recognition task"),UL.forEach(t),L0.forEach(t),o2=p(a),cp=o(a,"P",{});var zL=l(cp);XO=u(zL,`This task reads some audio input and outputs the said words within the
audio files.`),zL.forEach(t),l2=p(a),v(sa.$$.fragment,a),i2=p(a),v(aa.$$.fragment,a),u2=p(a),pe=o(a,"P",{});var fi=l(pe);QO=u(fi,"Available with: "),al=o(fi,"A",{href:!0,rel:!0});var ML=l(al);ZO=u(ML,"\u{1F917} Transformers"),ML.forEach(t),eP=p(fi),nl=o(fi,"A",{href:!0,rel:!0});var FL=l(nl);tP=u(FL,"ESPnet"),FL.forEach(t),sP=u(fi,` and
`),rl=o(fi,"A",{href:!0,rel:!0});var KL=l(rl);aP=u(KL,"SpeechBrain"),KL.forEach(t),fi.forEach(t),c2=p(a),fp=o(a,"P",{});var JL=l(fp);nP=u(JL,"Request:"),JL.forEach(t),f2=p(a),v(na.$$.fragment,a),p2=p(a),pp=o(a,"P",{});var WL=l(pp);rP=u(WL,`When sending your request, you should send a binary payload that simply
contains your audio file. We try to support most formats (Flac, Wav,
Mp3, Ogg etc...). And we automatically rescale the sampling rate to the
appropriate rate for the given model (usually 16KHz).`),WL.forEach(t),d2=p(a),ra=o(a,"TABLE",{});var U0=l(ra);bq=o(U0,"THEAD",{});var YL=l(bq);ol=o(YL,"TR",{});var z0=l(ol);dp=o(z0,"TH",{align:!0});var VL=l(dp);oP=u(VL,"All parameters"),VL.forEach(t),lP=p(z0),Tq=o(z0,"TH",{align:!0}),l(Tq).forEach(t),z0.forEach(t),YL.forEach(t),iP=p(U0),jq=o(U0,"TBODY",{});var XL=l(jq);ll=o(XL,"TR",{});var M0=l(ll);il=o(M0,"TD",{align:!0});var uS=l(il);kq=o(uS,"STRONG",{});var QL=l(kq);uP=u(QL,"no parameter"),QL.forEach(t),cP=u(uS," (required)"),uS.forEach(t),fP=p(M0),hp=o(M0,"TD",{align:!0});var ZL=l(hp);pP=u(ZL,"a binary representation of the audio file. No other parameters are currently allowed."),ZL.forEach(t),M0.forEach(t),XL.forEach(t),U0.forEach(t),h2=p(a),gp=o(a,"P",{});var eU=l(gp);dP=u(eU,"Return value is either a dict or a list of dicts if you sent a list of inputs"),eU.forEach(t),g2=p(a),mp=o(a,"P",{});var tU=l(mp);hP=u(tU,"Response:"),tU.forEach(t),m2=p(a),v(oa.$$.fragment,a),q2=p(a),la=o(a,"TABLE",{});var F0=l(la);Aq=o(F0,"THEAD",{});var sU=l(Aq);ul=o(sU,"TR",{});var K0=l(ul);qp=o(K0,"TH",{align:!0});var aU=l(qp);gP=u(aU,"Returned values"),aU.forEach(t),mP=p(K0),Dq=o(K0,"TH",{align:!0}),l(Dq).forEach(t),K0.forEach(t),sU.forEach(t),qP=p(F0),Oq=o(F0,"TBODY",{});var nU=l(Oq);cl=o(nU,"TR",{});var J0=l(cl);$p=o(J0,"TD",{align:!0});var rU=l($p);Pq=o(rU,"STRONG",{});var oU=l(Pq);$P=u(oU,"text"),oU.forEach(t),rU.forEach(t),_P=p(J0),_p=o(J0,"TD",{align:!0});var lU=l(_p);vP=u(lU,"The string that was recognized within the audio file."),lU.forEach(t),J0.forEach(t),nU.forEach(t),F0.forEach(t),$2=p(a),Ve=o(a,"H2",{class:!0});var W0=l(Ve);ia=o(W0,"A",{id:!0,class:!0,href:!0});var iU=l(ia);Rq=o(iU,"SPAN",{});var uU=l(Rq);v(fl.$$.fragment,uU),uU.forEach(t),iU.forEach(t),yP=p(W0),Nq=o(W0,"SPAN",{});var cU=l(Nq);EP=u(cU,"Feature-extraction task"),cU.forEach(t),W0.forEach(t),_2=p(a),vp=o(a,"P",{});var fU=l(vp);wP=u(fU,`This task reads some text and outputs raw float values, that are usually
consumed as part of a semantic database/semantic search.`),fU.forEach(t),v2=p(a),v(ua.$$.fragment,a),y2=p(a),Xe=o(a,"P",{});var X$=l(Xe);bP=u(X$,"Available with: "),pl=o(X$,"A",{href:!0,rel:!0});var pU=l(pl);TP=u(pU,"\u{1F917} Transformers"),pU.forEach(t),jP=p(X$),dl=o(X$,"A",{href:!0,rel:!0});var dU=l(dl);kP=u(dU,"Sentence-transformers"),dU.forEach(t),X$.forEach(t),E2=p(a),yp=o(a,"P",{});var hU=l(yp);AP=u(hU,"Request:"),hU.forEach(t),w2=p(a),ca=o(a,"TABLE",{});var Y0=l(ca);Sq=o(Y0,"THEAD",{});var gU=l(Sq);hl=o(gU,"TR",{});var V0=l(hl);Ep=o(V0,"TH",{align:!0});var mU=l(Ep);DP=u(mU,"All parameters"),mU.forEach(t),OP=p(V0),xq=o(V0,"TH",{align:!0}),l(xq).forEach(t),V0.forEach(t),gU.forEach(t),PP=p(Y0),ae=o(Y0,"TBODY",{});var Pe=l(ae);gl=o(Pe,"TR",{});var X0=l(gl);ml=o(X0,"TD",{align:!0});var cS=l(ml);Iq=o(cS,"STRONG",{});var qU=l(Iq);RP=u(qU,"inputs"),qU.forEach(t),NP=u(cS," (required):"),cS.forEach(t),SP=p(X0),wp=o(X0,"TD",{align:!0});var $U=l(wp);xP=u($U,"a string or a list of strings to get the features from."),$U.forEach(t),X0.forEach(t),IP=p(Pe),ql=o(Pe,"TR",{});var Q0=l(ql);bp=o(Q0,"TD",{align:!0});var _U=l(bp);Hq=o(_U,"STRONG",{});var vU=l(Hq);HP=u(vU,"options"),vU.forEach(t),_U.forEach(t),BP=p(Q0),Tp=o(Q0,"TD",{align:!0});var yU=l(Tp);CP=u(yU,"a dict containing the following keys:"),yU.forEach(t),Q0.forEach(t),GP=p(Pe),$l=o(Pe,"TR",{});var Z0=l($l);jp=o(Z0,"TD",{align:!0});var EU=l(jp);LP=u(EU,"use_gpu"),EU.forEach(t),UP=p(Z0),fa=o(Z0,"TD",{align:!0});var ew=l(fa);zP=u(ew,"(Default: "),Bq=o(ew,"CODE",{});var wU=l(Bq);MP=u(wU,"false"),wU.forEach(t),FP=u(ew,"). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),ew.forEach(t),Z0.forEach(t),KP=p(Pe),_l=o(Pe,"TR",{});var tw=l(_l);kp=o(tw,"TD",{align:!0});var bU=l(kp);JP=u(bU,"use_cache"),bU.forEach(t),WP=p(tw),pa=o(tw,"TD",{align:!0});var sw=l(pa);YP=u(sw,"(Default: "),Cq=o(sw,"CODE",{});var TU=l(Cq);VP=u(TU,"true"),TU.forEach(t),XP=u(sw,"). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),sw.forEach(t),tw.forEach(t),QP=p(Pe),vl=o(Pe,"TR",{});var aw=l(vl);Ap=o(aw,"TD",{align:!0});var jU=l(Ap);ZP=u(jU,"wait_for_model"),jU.forEach(t),eR=p(aw),da=o(aw,"TD",{align:!0});var nw=l(da);tR=u(nw,"(Default: "),Gq=o(nw,"CODE",{});var kU=l(Gq);sR=u(kU,"false"),kU.forEach(t),aR=u(nw,") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),nw.forEach(t),aw.forEach(t),Pe.forEach(t),Y0.forEach(t),b2=p(a),Dp=o(a,"P",{});var AU=l(Dp);nR=u(AU,"Return value is either a dict or a list of dicts if you sent a list of inputs"),AU.forEach(t),T2=p(a),ha=o(a,"TABLE",{});var rw=l(ha);Lq=o(rw,"THEAD",{});var DU=l(Lq);yl=o(DU,"TR",{});var ow=l(yl);Op=o(ow,"TH",{align:!0});var OU=l(Op);rR=u(OU,"Returned values"),OU.forEach(t),oR=p(ow),Uq=o(ow,"TH",{align:!0}),l(Uq).forEach(t),ow.forEach(t),DU.forEach(t),lR=p(rw),zq=o(rw,"TBODY",{});var PU=l(zq);El=o(PU,"TR",{});var lw=l(El);Pp=o(lw,"TD",{align:!0});var RU=l(Pp);Mq=o(RU,"STRONG",{});var NU=l(Mq);iR=u(NU,"A list of float (or list of list of floats)"),NU.forEach(t),RU.forEach(t),uR=p(lw),Rp=o(lw,"TD",{align:!0});var SU=l(Rp);cR=u(SU,"The numbers that are the representation features of the input."),SU.forEach(t),lw.forEach(t),PU.forEach(t),rw.forEach(t),j2=p(a),Np=o(a,"SMALL",{});var xU=l(Np);fR=u(xU,`Returned values are a list of floats, or a list of list of floats
(depending on if you sent a string or a list of string, and if the
automatic reduction, usually mean_pooling for instance was applied for
you or not. This should be explained on the model's README.`),xU.forEach(t),k2=p(a),Qe=o(a,"H2",{class:!0});var iw=l(Qe);ga=o(iw,"A",{id:!0,class:!0,href:!0});var IU=l(ga);Fq=o(IU,"SPAN",{});var HU=l(Fq);v(wl.$$.fragment,HU),HU.forEach(t),IU.forEach(t),pR=p(iw),Kq=o(iw,"SPAN",{});var BU=l(Kq);dR=u(BU,"Audio-classification task"),BU.forEach(t),iw.forEach(t),A2=p(a),Sp=o(a,"P",{});var CU=l(Sp);hR=u(CU,"This task reads some audio input and outputs the likelihood of classes."),CU.forEach(t),D2=p(a),v(ma.$$.fragment,a),O2=p(a),Ze=o(a,"P",{});var Q$=l(Ze);gR=u(Q$,"Available with: "),bl=o(Q$,"A",{href:!0,rel:!0});var GU=l(bl);mR=u(GU,"\u{1F917} Transformers"),GU.forEach(t),qR=p(Q$),Tl=o(Q$,"A",{href:!0,rel:!0});var LU=l(Tl);$R=u(LU,"SpeechBrain"),LU.forEach(t),Q$.forEach(t),P2=p(a),xp=o(a,"P",{});var UU=l(xp);_R=u(UU,"Request:"),UU.forEach(t),R2=p(a),v(qa.$$.fragment,a),N2=p(a),Ip=o(a,"P",{});var zU=l(Ip);vR=u(zU,`When sending your request, you should send a binary payload that simply
contains your audio file. We try to support most formats (Flac, Wav,
Mp3, Ogg etc...). And we automatically rescale the sampling rate to the
appropriate rate for the given model (usually 16KHz).`),zU.forEach(t),S2=p(a),$a=o(a,"TABLE",{});var uw=l($a);Jq=o(uw,"THEAD",{});var MU=l(Jq);jl=o(MU,"TR",{});var cw=l(jl);Hp=o(cw,"TH",{align:!0});var FU=l(Hp);yR=u(FU,"All parameters"),FU.forEach(t),ER=p(cw),Wq=o(cw,"TH",{align:!0}),l(Wq).forEach(t),cw.forEach(t),MU.forEach(t),wR=p(uw),Yq=o(uw,"TBODY",{});var KU=l(Yq);kl=o(KU,"TR",{});var fw=l(kl);Al=o(fw,"TD",{align:!0});var fS=l(Al);Vq=o(fS,"STRONG",{});var JU=l(Vq);bR=u(JU,"no parameter"),JU.forEach(t),TR=u(fS," (required)"),fS.forEach(t),jR=p(fw),Bp=o(fw,"TD",{align:!0});var WU=l(Bp);kR=u(WU,"a binary representation of the audio file. No other parameters are currently allowed."),WU.forEach(t),fw.forEach(t),KU.forEach(t),uw.forEach(t),x2=p(a),Cp=o(a,"P",{});var YU=l(Cp);AR=u(YU,"Return value is a dict"),YU.forEach(t),I2=p(a),v(_a.$$.fragment,a),H2=p(a),va=o(a,"TABLE",{});var pw=l(va);Xq=o(pw,"THEAD",{});var VU=l(Xq);Dl=o(VU,"TR",{});var dw=l(Dl);Gp=o(dw,"TH",{align:!0});var XU=l(Gp);DR=u(XU,"Returned values"),XU.forEach(t),OR=p(dw),Qq=o(dw,"TH",{align:!0}),l(Qq).forEach(t),dw.forEach(t),VU.forEach(t),PR=p(pw),Ol=o(pw,"TBODY",{});var hw=l(Ol);Pl=o(hw,"TR",{});var gw=l(Pl);Lp=o(gw,"TD",{align:!0});var QU=l(Lp);Zq=o(QU,"STRONG",{});var ZU=l(Zq);RR=u(ZU,"label"),ZU.forEach(t),QU.forEach(t),NR=p(gw),Up=o(gw,"TD",{align:!0});var ez=l(Up);SR=u(ez,"The label for the class (model specific)"),ez.forEach(t),gw.forEach(t),xR=p(hw),Rl=o(hw,"TR",{});var mw=l(Rl);zp=o(mw,"TD",{align:!0});var tz=l(zp);e$=o(tz,"STRONG",{});var sz=l(e$);IR=u(sz,"score"),sz.forEach(t),tz.forEach(t),HR=p(mw),Mp=o(mw,"TD",{align:!0});var az=l(Mp);BR=u(az,"A float that represents how likely it is that the audio file belongs to this class."),az.forEach(t),mw.forEach(t),hw.forEach(t),pw.forEach(t),B2=p(a),et=o(a,"H2",{class:!0});var qw=l(et);ya=o(qw,"A",{id:!0,class:!0,href:!0});var nz=l(ya);t$=o(nz,"SPAN",{});var rz=l(t$);v(Nl.$$.fragment,rz),rz.forEach(t),nz.forEach(t),CR=p(qw),s$=o(qw,"SPAN",{});var oz=l(s$);GR=u(oz,"Object-detection task"),oz.forEach(t),qw.forEach(t),C2=p(a),Fp=o(a,"P",{});var lz=l(Fp);LR=u(lz,`This task reads some image input and outputs the likelihood of classes &
bounding boxes of detected objects.`),lz.forEach(t),G2=p(a),v(Ea.$$.fragment,a),L2=p(a),Sl=o(a,"P",{});var pS=l(Sl);UR=u(pS,"Available with: "),xl=o(pS,"A",{href:!0,rel:!0});var iz=l(xl);zR=u(iz,"\u{1F917} Transformers"),iz.forEach(t),pS.forEach(t),U2=p(a),Kp=o(a,"P",{});var uz=l(Kp);MR=u(uz,"Request:"),uz.forEach(t),z2=p(a),v(wa.$$.fragment,a),M2=p(a),ba=o(a,"P",{});var $w=l(ba);FR=u($w,`When sending your request, you should send a binary payload that simply
contains your image file. We support all image formats `),Il=o($w,"A",{href:!0,rel:!0});var cz=l(Il);KR=u(cz,`Pillow
supports`),cz.forEach(t),JR=u($w,"."),$w.forEach(t),F2=p(a),Ta=o(a,"TABLE",{});var _w=l(Ta);a$=o(_w,"THEAD",{});var fz=l(a$);Hl=o(fz,"TR",{});var vw=l(Hl);Jp=o(vw,"TH",{align:!0});var pz=l(Jp);WR=u(pz,"All parameters"),pz.forEach(t),YR=p(vw),n$=o(vw,"TH",{align:!0}),l(n$).forEach(t),vw.forEach(t),fz.forEach(t),VR=p(_w),r$=o(_w,"TBODY",{});var dz=l(r$);Bl=o(dz,"TR",{});var yw=l(Bl);Cl=o(yw,"TD",{align:!0});var dS=l(Cl);o$=o(dS,"STRONG",{});var hz=l(o$);XR=u(hz,"no parameter"),hz.forEach(t),QR=u(dS," (required)"),dS.forEach(t),ZR=p(yw),Wp=o(yw,"TD",{align:!0});var gz=l(Wp);eN=u(gz,"a binary representation of the image file. No other parameters are currently allowed."),gz.forEach(t),yw.forEach(t),dz.forEach(t),_w.forEach(t),K2=p(a),Yp=o(a,"P",{});var mz=l(Yp);tN=u(mz,"Return value is a dict"),mz.forEach(t),J2=p(a),v(ja.$$.fragment,a),W2=p(a),ka=o(a,"TABLE",{});var Ew=l(ka);l$=o(Ew,"THEAD",{});var qz=l(l$);Gl=o(qz,"TR",{});var ww=l(Gl);Vp=o(ww,"TH",{align:!0});var $z=l(Vp);sN=u($z,"Returned values"),$z.forEach(t),aN=p(ww),i$=o(ww,"TH",{align:!0}),l(i$).forEach(t),ww.forEach(t),qz.forEach(t),nN=p(Ew),tt=o(Ew,"TBODY",{});var Rd=l(tt);Ll=o(Rd,"TR",{});var bw=l(Ll);Xp=o(bw,"TD",{align:!0});var _z=l(Xp);u$=o(_z,"STRONG",{});var vz=l(u$);rN=u(vz,"label"),vz.forEach(t),_z.forEach(t),oN=p(bw),Qp=o(bw,"TD",{align:!0});var yz=l(Qp);lN=u(yz,"The label for the class (model specific) of a detected object."),yz.forEach(t),bw.forEach(t),iN=p(Rd),Ul=o(Rd,"TR",{});var Tw=l(Ul);Zp=o(Tw,"TD",{align:!0});var Ez=l(Zp);c$=o(Ez,"STRONG",{});var wz=l(c$);uN=u(wz,"score"),wz.forEach(t),Ez.forEach(t),cN=p(Tw),ed=o(Tw,"TD",{align:!0});var bz=l(ed);fN=u(bz,"A float that represents how likely it is that the detected object belongs to the given class."),bz.forEach(t),Tw.forEach(t),pN=p(Rd),zl=o(Rd,"TR",{});var jw=l(zl);td=o(jw,"TD",{align:!0});var Tz=l(td);f$=o(Tz,"STRONG",{});var jz=l(f$);dN=u(jz,"box"),jz.forEach(t),Tz.forEach(t),hN=p(jw),sd=o(jw,"TD",{align:!0});var kz=l(sd);gN=u(kz,"A dict (with keys [xmin,ymin,xmax,ymax]) representing the bounding box of a detected object."),kz.forEach(t),jw.forEach(t),Rd.forEach(t),Ew.forEach(t),Y2=p(a),st=o(a,"H2",{class:!0});var kw=l(st);Aa=o(kw,"A",{id:!0,class:!0,href:!0});var Az=l(Aa);p$=o(Az,"SPAN",{});var Dz=l(p$);v(Ml.$$.fragment,Dz),Dz.forEach(t),Az.forEach(t),mN=p(kw),d$=o(kw,"SPAN",{});var Oz=l(d$);qN=u(Oz,"Image Segmentation task"),Oz.forEach(t),kw.forEach(t),V2=p(a),ad=o(a,"P",{});var Pz=l(ad);$N=u(Pz,`This task reads some image input and outputs the likelihood of classes &
bounding boxes of detected objects.`),Pz.forEach(t),X2=p(a),v(Da.$$.fragment,a),Q2=p(a),Fl=o(a,"P",{});var hS=l(Fl);_N=u(hS,"Available with: "),Kl=o(hS,"A",{href:!0,rel:!0});var Rz=l(Kl);vN=u(Rz,"\u{1F917} Transformers"),Rz.forEach(t),hS.forEach(t),Z2=p(a),nd=o(a,"P",{});var Nz=l(nd);yN=u(Nz,"Request:"),Nz.forEach(t),ev=p(a),v(Oa.$$.fragment,a),tv=p(a),Pa=o(a,"P",{});var Aw=l(Pa);EN=u(Aw,`When sending your request, you should send a binary payload that simply
contains your image file. We support all image formats `),Jl=o(Aw,"A",{href:!0,rel:!0});var Sz=l(Jl);wN=u(Sz,`Pillow
supports`),Sz.forEach(t),bN=u(Aw,"."),Aw.forEach(t),sv=p(a),Ra=o(a,"TABLE",{});var Dw=l(Ra);h$=o(Dw,"THEAD",{});var xz=l(h$);Wl=o(xz,"TR",{});var Ow=l(Wl);rd=o(Ow,"TH",{align:!0});var Iz=l(rd);TN=u(Iz,"All parameters"),Iz.forEach(t),jN=p(Ow),g$=o(Ow,"TH",{align:!0}),l(g$).forEach(t),Ow.forEach(t),xz.forEach(t),kN=p(Dw),m$=o(Dw,"TBODY",{});var Hz=l(m$);Yl=o(Hz,"TR",{});var Pw=l(Yl);Vl=o(Pw,"TD",{align:!0});var gS=l(Vl);q$=o(gS,"STRONG",{});var Bz=l(q$);AN=u(Bz,"no parameter"),Bz.forEach(t),DN=u(gS," (required)"),gS.forEach(t),ON=p(Pw),od=o(Pw,"TD",{align:!0});var Cz=l(od);PN=u(Cz,"a binary representation of the image file. No other parameters are currently allowed."),Cz.forEach(t),Pw.forEach(t),Hz.forEach(t),Dw.forEach(t),av=p(a),ld=o(a,"P",{});var Gz=l(ld);RN=u(Gz,"Return value is a dict"),Gz.forEach(t),nv=p(a),v(Na.$$.fragment,a),rv=p(a),Sa=o(a,"TABLE",{});var Rw=l(Sa);$$=o(Rw,"THEAD",{});var Lz=l($$);Xl=o(Lz,"TR",{});var Nw=l(Xl);id=o(Nw,"TH",{align:!0});var Uz=l(id);NN=u(Uz,"Returned values"),Uz.forEach(t),SN=p(Nw),_$=o(Nw,"TH",{align:!0}),l(_$).forEach(t),Nw.forEach(t),Lz.forEach(t),xN=p(Rw),at=o(Rw,"TBODY",{});var Nd=l(at);Ql=o(Nd,"TR",{});var Sw=l(Ql);ud=o(Sw,"TD",{align:!0});var zz=l(ud);v$=o(zz,"STRONG",{});var Mz=l(v$);IN=u(Mz,"label"),Mz.forEach(t),zz.forEach(t),HN=p(Sw),cd=o(Sw,"TD",{align:!0});var Fz=l(cd);BN=u(Fz,"The label for the class (model specific) of a segment."),Fz.forEach(t),Sw.forEach(t),CN=p(Nd),Zl=o(Nd,"TR",{});var xw=l(Zl);fd=o(xw,"TD",{align:!0});var Kz=l(fd);y$=o(Kz,"STRONG",{});var Jz=l(y$);GN=u(Jz,"score"),Jz.forEach(t),Kz.forEach(t),LN=p(xw),pd=o(xw,"TD",{align:!0});var Wz=l(pd);UN=u(Wz,"A float that represents how likely it is that the segment belongs to the given class."),Wz.forEach(t),xw.forEach(t),zN=p(Nd),ei=o(Nd,"TR",{});var Iw=l(ei);dd=o(Iw,"TD",{align:!0});var Yz=l(dd);E$=o(Yz,"STRONG",{});var Vz=l(E$);MN=u(Vz,"mask"),Vz.forEach(t),Yz.forEach(t),FN=p(Iw),hd=o(Iw,"TD",{align:!0});var Xz=l(hd);KN=u(Xz,"A str (base64 str of a single channel black-and-white img) representing the mask of a segment."),Xz.forEach(t),Iw.forEach(t),Nd.forEach(t),Rw.forEach(t),this.h()},h(){d(n,"name","hf:doc:metadata"),d(n,"content",JSON.stringify(_K)),d(h,"id","detailed-parameters"),d(h,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(h,"href","#detailed-parameters"),d(s,"class","relative group"),d(ne,"id","which-task-is-used-by-this-model"),d(ne,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(ne,"href","#which-task-is-used-by-this-model"),d(D,"class","relative group"),d(ot,"class","block dark:hidden"),Qz(ot.src,mS="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/inference-api/task.png")||d(ot,"src",mS),d(ot,"width","300"),d(lt,"class","hidden dark:block invert"),Qz(lt.src,qS="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/inference-api/task-dark.png")||d(lt,"src",qS),d(lt,"width","300"),d(it,"id","zeroshot-classification-task"),d(it,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(it,"href","#zeroshot-classification-task"),d(Se,"class","relative group"),d(Fa,"href","https://github.com/huggingface/transformers"),d(Fa,"rel","nofollow"),d(qi,"align","left"),d(Hd,"align","left"),d(Wa,"align","left"),d($i,"align","left"),d(Va,"align","left"),d(_i,"align","left"),d(vi,"align","left"),d(de,"align","left"),d(yi,"align","left"),d(pt,"align","left"),d(Ei,"align","left"),d(wi,"align","left"),d(bi,"align","left"),d(dt,"align","left"),d(Ti,"align","left"),d(ht,"align","left"),d(ji,"align","left"),d(gt,"align","left"),d(Di,"align","left"),d(Wd,"align","left"),d(Oi,"align","left"),d(Pi,"align","left"),d(Ri,"align","left"),d(Ni,"align","left"),d(Si,"align","left"),d($t,"align","left"),d(_t,"id","translation-task"),d(_t,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(_t,"href","#translation-task"),d(Ie,"class","relative group"),d(cn,"href","https://github.com/huggingface/transformers"),d(cn,"rel","nofollow"),d(Bi,"align","left"),d(sh,"align","left"),d(dn,"align","left"),d(Ci,"align","left"),d(Gi,"align","left"),d(Li,"align","left"),d(Ui,"align","left"),d(wt,"align","left"),d(zi,"align","left"),d(bt,"align","left"),d(Mi,"align","left"),d(Tt,"align","left"),d(Ki,"align","left"),d(uh,"align","left"),d(Ji,"align","left"),d(Wi,"align","left"),d(kt,"id","summarization-task"),d(kt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(kt,"href","#summarization-task"),d(He,"class","relative group"),d(Yi,"href","mailto:api-enterprise@huggingface.co"),d(En,"href","https://github.com/huggingface/transformers"),d(En,"rel","nofollow"),d(Qi,"align","left"),d(gh,"align","left"),d(Tn,"align","left"),d(Zi,"align","left"),d(eu,"align","left"),d(tu,"align","left"),d(su,"align","left"),d(he,"align","left"),d(au,"align","left"),d(ge,"align","left"),d(nu,"align","left"),d(me,"align","left"),d(ru,"align","left"),d(re,"align","left"),d(ou,"align","left"),d(qe,"align","left"),d(lu,"align","left"),d(Rt,"align","left"),d(iu,"align","left"),d(Nt,"align","left"),d(uu,"align","left"),d(cu,"align","left"),d(fu,"align","left"),d(St,"align","left"),d(pu,"align","left"),d(xt,"align","left"),d(du,"align","left"),d(It,"align","left"),d(gu,"align","left"),d(Ih,"align","left"),d(mu,"align","left"),d(qu,"align","left"),d(Bt,"id","conversational-task"),d(Bt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Bt,"href","#conversational-task"),d(Be,"class","relative group"),d(Un,"href","https://github.com/huggingface/transformers"),d(Un,"rel","nofollow"),d(yu,"align","left"),d(Uh,"align","left"),d(Fn,"align","left"),d(Mh,"align","left"),d(Eu,"align","left"),d(wu,"align","left"),d(bu,"align","left"),d(Tu,"align","left"),d(ju,"align","left"),d(Ut,"align","left"),d(ku,"align","left"),d(Au,"align","left"),d(Du,"align","left"),d($e,"align","left"),d(Ou,"align","left"),d(_e,"align","left"),d(Pu,"align","left"),d(ve,"align","left"),d(Ru,"align","left"),d(oe,"align","left"),d(Nu,"align","left"),d(ye,"align","left"),d(Su,"align","left"),d(zt,"align","left"),d(xu,"align","left"),d(Mt,"align","left"),d(Iu,"align","left"),d(Hu,"align","left"),d(Bu,"align","left"),d(Ft,"align","left"),d(Cu,"align","left"),d(Kt,"align","left"),d(Gu,"align","left"),d(Jt,"align","left"),d(Uu,"align","left"),d(fg,"align","left"),d(zu,"align","left"),d(Mu,"align","left"),d(Fu,"align","left"),d(Ku,"align","left"),d(Ju,"align","left"),d(Wu,"align","left"),d(Yu,"align","left"),d(Vu,"align","left"),d(Yt,"id","table-question-answering-task"),d(Yt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Yt,"href","#table-question-answering-task"),d(Ce,"class","relative group"),d(hr,"href","https://github.com/huggingface/transformers"),d(hr,"rel","nofollow"),d(ec,"align","left"),d(qg,"align","left"),d(qr,"align","left"),d(_g,"align","left"),d(tc,"align","left"),d(sc,"align","left"),d(ac,"align","left"),d(nc,"align","left"),d(rc,"align","left"),d(oc,"align","left"),d(lc,"align","left"),d(Zt,"align","left"),d(ic,"align","left"),d(es,"align","left"),d(uc,"align","left"),d(ts,"align","left"),d(fc,"align","left"),d(Tg,"align","left"),d(pc,"align","left"),d(dc,"align","left"),d(hc,"align","left"),d(gc,"align","left"),d(mc,"align","left"),d(qc,"align","left"),d($c,"align","left"),d(_c,"align","left"),d(ns,"id","question-answering-task"),d(ns,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(ns,"href","#question-answering-task"),d(Ge,"class","relative group"),d(Or,"href","https://github.com/huggingface/transformers"),d(Or,"rel","nofollow"),d(Pr,"href","https://github.com/allenai/allennlp"),d(Pr,"rel","nofollow"),d(bc,"align","left"),d(Ng,"align","left"),d(Tc,"align","left"),d(jc,"align","left"),d(kc,"align","left"),d(Ac,"align","left"),d(Dc,"align","left"),d(us,"align","left"),d(Oc,"align","left"),d(cs,"align","left"),d(fs,"id","textclassification-task"),d(fs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(fs,"href","#textclassification-task"),d(Ue,"class","relative group"),d(Cr,"href","https://github.com/huggingface/transformers"),d(Cr,"rel","nofollow"),d(Sc,"align","left"),d(zg,"align","left"),d(Ur,"align","left"),d(xc,"align","left"),d(Ic,"align","left"),d(Hc,"align","left"),d(Bc,"align","left"),d(gs,"align","left"),d(Cc,"align","left"),d(ms,"align","left"),d(Gc,"align","left"),d(qs,"align","left"),d(Uc,"align","left"),d(Vg,"align","left"),d(zc,"align","left"),d(Mc,"align","left"),d(Fc,"align","left"),d(Kc,"align","left"),d(vs,"id","named-entity-recognition-ner-task"),d(vs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(vs,"href","#named-entity-recognition-ner-task"),d(ze,"class","relative group"),d(Jc,"href","#token-classification-task"),d(ys,"id","tokenclassification-task"),d(ys,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(ys,"href","#tokenclassification-task"),d(Me,"class","relative group"),d(eo,"href","https://github.com/huggingface/transformers"),d(eo,"rel","nofollow"),d(to,"href","https://github.com/flairNLP/flair"),d(to,"rel","nofollow"),d(Xc,"align","left"),d(nm,"align","left"),d(no,"align","left"),d(Qc,"align","left"),d(Zc,"align","left"),d(ef,"align","left"),d(tf,"align","left"),d(x,"align","left"),d(sf,"align","left"),d(af,"align","left"),d(nf,"align","left"),d(Ts,"align","left"),d(rf,"align","left"),d(js,"align","left"),d(of,"align","left"),d(ks,"align","left"),d(uf,"align","left"),d(ym,"align","left"),d(cf,"align","left"),d(ff,"align","left"),d(pf,"align","left"),d(df,"align","left"),d(hf,"align","left"),d(gf,"align","left"),d(mf,"align","left"),d(Os,"align","left"),d(qf,"align","left"),d(Ps,"align","left"),d(Rs,"id","textgeneration-task"),d(Rs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Rs,"href","#textgeneration-task"),d(Ke,"class","relative group"),d(vo,"href","https://github.com/huggingface/transformers"),d(vo,"rel","nofollow"),d(yf,"align","left"),d(Rm,"align","left"),d(wo,"align","left"),d(Ef,"align","left"),d(wf,"align","left"),d(bf,"align","left"),d(Tf,"align","left"),d(Ee,"align","left"),d(jf,"align","left"),d(le,"align","left"),d(kf,"align","left"),d(we,"align","left"),d(Af,"align","left"),d(Is,"align","left"),d(Df,"align","left"),d(be,"align","left"),d(Of,"align","left"),d(Te,"align","left"),d(Pf,"align","left"),d(je,"align","left"),d(Rf,"align","left"),d(Hs,"align","left"),d(Nf,"align","left"),d(Bs,"align","left"),d(Sf,"align","left"),d(xf,"align","left"),d(If,"align","left"),d(Cs,"align","left"),d(Hf,"align","left"),d(Gs,"align","left"),d(Bf,"align","left"),d(Ls,"align","left"),d(Gf,"align","left"),d(sq,"align","left"),d(Lf,"align","left"),d(Uf,"align","left"),d(Ms,"id","text2textgeneration-task"),d(Ms,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Ms,"href","#text2textgeneration-task"),d(Je,"class","relative group"),d(zf,"href","#text-generation-task"),d(Ks,"id","fill-mask-task"),d(Ks,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Ks,"href","#fill-mask-task"),d(We,"class","relative group"),d(zo,"href","https://github.com/huggingface/transformers"),d(zo,"rel","nofollow"),d(Jf,"align","left"),d(cq,"align","left"),d(Ko,"align","left"),d(Wf,"align","left"),d(Yf,"align","left"),d(Vf,"align","left"),d(Xf,"align","left"),d(Vs,"align","left"),d(Qf,"align","left"),d(Xs,"align","left"),d(Zf,"align","left"),d(Qs,"align","left"),d(tp,"align","left"),d(qq,"align","left"),d(sp,"align","left"),d(ap,"align","left"),d(np,"align","left"),d(rp,"align","left"),d(op,"align","left"),d(lp,"align","left"),d(ip,"align","left"),d(up,"align","left"),d(ta,"id","automatic-speech-recognition-task"),d(ta,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(ta,"href","#automatic-speech-recognition-task"),d(Ye,"class","relative group"),d(al,"href","https://github.com/huggingface/transformers"),d(al,"rel","nofollow"),d(nl,"href","https://github.com/espnet/espnet"),d(nl,"rel","nofollow"),d(rl,"href","https://github.com/speechbrain/speechbrain"),d(rl,"rel","nofollow"),d(dp,"align","left"),d(Tq,"align","left"),d(il,"align","left"),d(hp,"align","left"),d(qp,"align","left"),d(Dq,"align","left"),d($p,"align","left"),d(_p,"align","left"),d(ia,"id","featureextraction-task"),d(ia,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(ia,"href","#featureextraction-task"),d(Ve,"class","relative group"),d(pl,"href","https://github.com/huggingface/transformers"),d(pl,"rel","nofollow"),d(dl,"href","https://github.com/UKPLab/sentence-transformers"),d(dl,"rel","nofollow"),d(Ep,"align","left"),d(xq,"align","left"),d(ml,"align","left"),d(wp,"align","left"),d(bp,"align","left"),d(Tp,"align","left"),d(jp,"align","left"),d(fa,"align","left"),d(kp,"align","left"),d(pa,"align","left"),d(Ap,"align","left"),d(da,"align","left"),d(Op,"align","left"),d(Uq,"align","left"),d(Pp,"align","left"),d(Rp,"align","left"),d(ga,"id","audioclassification-task"),d(ga,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(ga,"href","#audioclassification-task"),d(Qe,"class","relative group"),d(bl,"href","https://github.com/huggingface/transformers"),d(bl,"rel","nofollow"),d(Tl,"href","https://github.com/speechbrain/speechbrain"),d(Tl,"rel","nofollow"),d(Hp,"align","left"),d(Wq,"align","left"),d(Al,"align","left"),d(Bp,"align","left"),d(Gp,"align","left"),d(Qq,"align","left"),d(Lp,"align","left"),d(Up,"align","left"),d(zp,"align","left"),d(Mp,"align","left"),d(ya,"id","objectdetection-task"),d(ya,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(ya,"href","#objectdetection-task"),d(et,"class","relative group"),d(xl,"href","https://github.com/huggingface/transformers"),d(xl,"rel","nofollow"),d(Il,"href","https://pillow.readthedocs.io/en/stable/handbook/image-file-formats.html"),d(Il,"rel","nofollow"),d(Jp,"align","left"),d(n$,"align","left"),d(Cl,"align","left"),d(Wp,"align","left"),d(Vp,"align","left"),d(i$,"align","left"),d(Xp,"align","left"),d(Qp,"align","left"),d(Zp,"align","left"),d(ed,"align","left"),d(td,"align","left"),d(sd,"align","left"),d(Aa,"id","image-segmentation-task"),d(Aa,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Aa,"href","#image-segmentation-task"),d(st,"class","relative group"),d(Kl,"href","https://github.com/huggingface/transformers"),d(Kl,"rel","nofollow"),d(Jl,"href","https://pillow.readthedocs.io/en/stable/handbook/image-file-formats.html"),d(Jl,"rel","nofollow"),d(rd,"align","left"),d(g$,"align","left"),d(Vl,"align","left"),d(od,"align","left"),d(id,"align","left"),d(_$,"align","left"),d(ud,"align","left"),d(cd,"align","left"),d(fd,"align","left"),d(pd,"align","left"),d(dd,"align","left"),d(hd,"align","left")},m(a,g){e(document.head,n),m(a,c,g),m(a,s,g),e(s,h),e(h,q),y(k,q,null),e(s,A),e(s,j),e(j,T),m(a,O,g),m(a,D,g),e(D,ne),e(ne,Re),y(Q,Re,null),e(D,Y),e(D,rt),e(rt,pi),m(a,Ua,g),m(a,Ne,g),e(Ne,Hw),m(a,Z$,g),m(a,di,g),e(di,Bw),m(a,e_,g),m(a,ot,g),m(a,t_,g),m(a,lt,g),m(a,s_,g),m(a,Se,g),e(Se,it),e(it,Sd),y(za,Sd,null),e(Se,Cw),e(Se,xd),e(xd,Gw),m(a,a_,g),m(a,hi,g),e(hi,Lw),m(a,n_,g),y(ut,a,g),m(a,r_,g),m(a,Ma,g),e(Ma,Uw),e(Ma,Fa),e(Fa,zw),m(a,o_,g),m(a,gi,g),e(gi,Mw),m(a,l_,g),y(ct,a,g),m(a,i_,g),m(a,mi,g),e(mi,Fw),m(a,u_,g),m(a,ft,g),e(ft,Id),e(Id,Ka),e(Ka,qi),e(qi,Kw),e(Ka,Jw),e(Ka,Hd),e(ft,Ww),e(ft,z),e(z,Ja),e(Ja,Wa),e(Wa,Bd),e(Bd,Yw),e(Wa,Vw),e(Ja,Xw),e(Ja,$i),e($i,Qw),e(z,Zw),e(z,Ya),e(Ya,Va),e(Va,Cd),e(Cd,eb),e(Va,tb),e(Ya,sb),e(Ya,_i),e(_i,ab),e(z,nb),e(z,Xa),e(Xa,vi),e(vi,rb),e(Xa,ob),e(Xa,de),e(de,lb),e(de,Gd),e(Gd,ib),e(de,ub),e(de,Ld),e(Ld,cb),e(de,fb),e(z,pb),e(z,Qa),e(Qa,yi),e(yi,db),e(Qa,hb),e(Qa,pt),e(pt,gb),e(pt,Ud),e(Ud,mb),e(pt,qb),e(z,$b),e(z,Za),e(Za,Ei),e(Ei,zd),e(zd,_b),e(Za,vb),e(Za,wi),e(wi,yb),e(z,Eb),e(z,en),e(en,bi),e(bi,wb),e(en,bb),e(en,dt),e(dt,Tb),e(dt,Md),e(Md,jb),e(dt,kb),e(z,Ab),e(z,tn),e(tn,Ti),e(Ti,Db),e(tn,Ob),e(tn,ht),e(ht,Pb),e(ht,Fd),e(Fd,Rb),e(ht,Nb),e(z,Sb),e(z,sn),e(sn,ji),e(ji,xb),e(sn,Ib),e(sn,gt),e(gt,Hb),e(gt,Kd),e(Kd,Bb),e(gt,Cb),m(a,c_,g),m(a,ki,g),e(ki,Gb),m(a,f_,g),m(a,Ai,g),e(Ai,Lb),m(a,p_,g),y(mt,a,g),m(a,d_,g),m(a,qt,g),e(qt,Jd),e(Jd,an),e(an,Di),e(Di,Ub),e(an,zb),e(an,Wd),e(qt,Mb),e(qt,xe),e(xe,nn),e(nn,Oi),e(Oi,Yd),e(Yd,Fb),e(nn,Kb),e(nn,Pi),e(Pi,Jb),e(xe,Wb),e(xe,rn),e(rn,Ri),e(Ri,Vd),e(Vd,Yb),e(rn,Vb),e(rn,Ni),e(Ni,Xb),e(xe,Qb),e(xe,on),e(on,Si),e(Si,Xd),e(Xd,Zb),e(on,e3),e(on,$t),e($t,t3),e($t,Qd),e(Qd,s3),e($t,a3),m(a,h_,g),m(a,Ie,g),e(Ie,_t),e(_t,Zd),y(ln,Zd,null),e(Ie,n3),e(Ie,eh),e(eh,r3),m(a,g_,g),m(a,xi,g),e(xi,o3),m(a,m_,g),y(vt,a,g),m(a,q_,g),m(a,un,g),e(un,l3),e(un,cn),e(cn,i3),m(a,$_,g),m(a,Ii,g),e(Ii,u3),m(a,__,g),y(yt,a,g),m(a,v_,g),m(a,Hi,g),e(Hi,c3),m(a,y_,g),m(a,Et,g),e(Et,th),e(th,fn),e(fn,Bi),e(Bi,f3),e(fn,p3),e(fn,sh),e(Et,d3),e(Et,Z),e(Z,pn),e(pn,dn),e(dn,ah),e(ah,h3),e(dn,g3),e(pn,m3),e(pn,Ci),e(Ci,q3),e(Z,$3),e(Z,hn),e(hn,Gi),e(Gi,nh),e(nh,_3),e(hn,v3),e(hn,Li),e(Li,y3),e(Z,E3),e(Z,gn),e(gn,Ui),e(Ui,w3),e(gn,b3),e(gn,wt),e(wt,T3),e(wt,rh),e(rh,j3),e(wt,k3),e(Z,A3),e(Z,mn),e(mn,zi),e(zi,D3),e(mn,O3),e(mn,bt),e(bt,P3),e(bt,oh),e(oh,R3),e(bt,N3),e(Z,S3),e(Z,qn),e(qn,Mi),e(Mi,x3),e(qn,I3),e(qn,Tt),e(Tt,H3),e(Tt,lh),e(lh,B3),e(Tt,C3),m(a,E_,g),m(a,Fi,g),e(Fi,G3),m(a,w_,g),m(a,jt,g),e(jt,ih),e(ih,$n),e($n,Ki),e(Ki,L3),e($n,U3),e($n,uh),e(jt,z3),e(jt,ch),e(ch,_n),e(_n,Ji),e(Ji,fh),e(fh,M3),e(_n,F3),e(_n,Wi),e(Wi,K3),m(a,b_,g),m(a,He,g),e(He,kt),e(kt,ph),y(vn,ph,null),e(He,J3),e(He,dh),e(dh,W3),m(a,T_,g),m(a,At,g),e(At,Y3),e(At,Yi),e(Yi,V3),e(At,X3),m(a,j_,g),y(Dt,a,g),m(a,k_,g),m(a,yn,g),e(yn,Q3),e(yn,En),e(En,Z3),m(a,A_,g),m(a,Vi,g),e(Vi,eT),m(a,D_,g),y(Ot,a,g),m(a,O_,g),m(a,Xi,g),e(Xi,tT),m(a,P_,g),m(a,Pt,g),e(Pt,hh),e(hh,wn),e(wn,Qi),e(Qi,sT),e(wn,aT),e(wn,gh),e(Pt,nT),e(Pt,G),e(G,bn),e(bn,Tn),e(Tn,mh),e(mh,rT),e(Tn,oT),e(bn,lT),e(bn,Zi),e(Zi,iT),e(G,uT),e(G,jn),e(jn,eu),e(eu,qh),e(qh,cT),e(jn,fT),e(jn,tu),e(tu,pT),e(G,dT),e(G,kn),e(kn,su),e(su,hT),e(kn,gT),e(kn,he),e(he,mT),e(he,$h),e($h,qT),e(he,$T),e(he,_h),e(_h,_T),e(he,vT),e(G,yT),e(G,An),e(An,au),e(au,ET),e(An,wT),e(An,ge),e(ge,bT),e(ge,vh),e(vh,TT),e(ge,jT),e(ge,yh),e(yh,kT),e(ge,AT),e(G,DT),e(G,Dn),e(Dn,nu),e(nu,OT),e(Dn,PT),e(Dn,me),e(me,RT),e(me,Eh),e(Eh,NT),e(me,ST),e(me,wh),e(wh,xT),e(me,IT),e(G,HT),e(G,On),e(On,ru),e(ru,BT),e(On,CT),e(On,re),e(re,GT),e(re,bh),e(bh,LT),e(re,UT),e(re,Th),e(Th,zT),e(re,MT),e(re,jh),e(jh,FT),e(re,KT),e(G,JT),e(G,Pn),e(Pn,ou),e(ou,WT),e(Pn,YT),e(Pn,qe),e(qe,VT),e(qe,kh),e(kh,XT),e(qe,QT),e(qe,Ah),e(Ah,ZT),e(qe,e5),e(G,t5),e(G,Rn),e(Rn,lu),e(lu,s5),e(Rn,a5),e(Rn,Rt),e(Rt,n5),e(Rt,Dh),e(Dh,r5),e(Rt,o5),e(G,l5),e(G,Nn),e(Nn,iu),e(iu,i5),e(Nn,u5),e(Nn,Nt),e(Nt,c5),e(Nt,Oh),e(Oh,f5),e(Nt,p5),e(G,d5),e(G,Sn),e(Sn,uu),e(uu,Ph),e(Ph,h5),e(Sn,g5),e(Sn,cu),e(cu,m5),e(G,q5),e(G,xn),e(xn,fu),e(fu,$5),e(xn,_5),e(xn,St),e(St,v5),e(St,Rh),e(Rh,y5),e(St,E5),e(G,w5),e(G,In),e(In,pu),e(pu,b5),e(In,T5),e(In,xt),e(xt,j5),e(xt,Nh),e(Nh,k5),e(xt,A5),e(G,D5),e(G,Hn),e(Hn,du),e(du,O5),e(Hn,P5),e(Hn,It),e(It,R5),e(It,Sh),e(Sh,N5),e(It,S5),m(a,R_,g),m(a,hu,g),e(hu,x5),m(a,N_,g),m(a,Ht,g),e(Ht,xh),e(xh,Bn),e(Bn,gu),e(gu,I5),e(Bn,H5),e(Bn,Ih),e(Ht,B5),e(Ht,Hh),e(Hh,Cn),e(Cn,mu),e(mu,Bh),e(Bh,C5),e(Cn,G5),e(Cn,qu),e(qu,L5),m(a,S_,g),m(a,Be,g),e(Be,Bt),e(Bt,Ch),y(Gn,Ch,null),e(Be,U5),e(Be,Gh),e(Gh,z5),m(a,x_,g),m(a,$u,g),e($u,M5),m(a,I_,g),y(Ct,a,g),m(a,H_,g),m(a,Ln,g),e(Ln,F5),e(Ln,Un),e(Un,K5),m(a,B_,g),m(a,_u,g),e(_u,J5),m(a,C_,g),y(Gt,a,g),m(a,G_,g),m(a,vu,g),e(vu,W5),m(a,L_,g),m(a,Lt,g),e(Lt,Lh),e(Lh,zn),e(zn,yu),e(yu,Y5),e(zn,V5),e(zn,Uh),e(Lt,X5),e(Lt,S),e(S,Mn),e(Mn,Fn),e(Fn,zh),e(zh,Q5),e(Fn,Z5),e(Mn,e4),e(Mn,Mh),e(S,t4),e(S,Kn),e(Kn,Eu),e(Eu,s4),e(Kn,a4),e(Kn,wu),e(wu,n4),e(S,r4),e(S,Jn),e(Jn,bu),e(bu,o4),e(Jn,l4),e(Jn,Tu),e(Tu,i4),e(S,u4),e(S,Wn),e(Wn,ju),e(ju,c4),e(Wn,f4),e(Wn,Ut),e(Ut,p4),e(Ut,Fh),e(Fh,d4),e(Ut,h4),e(S,g4),e(S,Yn),e(Yn,ku),e(ku,Kh),e(Kh,m4),e(Yn,q4),e(Yn,Au),e(Au,$4),e(S,_4),e(S,Vn),e(Vn,Du),e(Du,v4),e(Vn,y4),e(Vn,$e),e($e,E4),e($e,Jh),e(Jh,w4),e($e,b4),e($e,Wh),e(Wh,T4),e($e,j4),e(S,k4),e(S,Xn),e(Xn,Ou),e(Ou,A4),e(Xn,D4),e(Xn,_e),e(_e,O4),e(_e,Yh),e(Yh,P4),e(_e,R4),e(_e,Vh),e(Vh,N4),e(_e,S4),e(S,x4),e(S,Qn),e(Qn,Pu),e(Pu,I4),e(Qn,H4),e(Qn,ve),e(ve,B4),e(ve,Xh),e(Xh,C4),e(ve,G4),e(ve,Qh),e(Qh,L4),e(ve,U4),e(S,z4),e(S,Zn),e(Zn,Ru),e(Ru,M4),e(Zn,F4),e(Zn,oe),e(oe,K4),e(oe,Zh),e(Zh,J4),e(oe,W4),e(oe,eg),e(eg,Y4),e(oe,V4),e(oe,tg),e(tg,X4),e(oe,Q4),e(S,Z4),e(S,er),e(er,Nu),e(Nu,ej),e(er,tj),e(er,ye),e(ye,sj),e(ye,sg),e(sg,aj),e(ye,nj),e(ye,ag),e(ag,rj),e(ye,oj),e(S,lj),e(S,tr),e(tr,Su),e(Su,ij),e(tr,uj),e(tr,zt),e(zt,cj),e(zt,ng),e(ng,fj),e(zt,pj),e(S,dj),e(S,sr),e(sr,xu),e(xu,hj),e(sr,gj),e(sr,Mt),e(Mt,mj),e(Mt,rg),e(rg,qj),e(Mt,$j),e(S,_j),e(S,ar),e(ar,Iu),e(Iu,og),e(og,vj),e(ar,yj),e(ar,Hu),e(Hu,Ej),e(S,wj),e(S,nr),e(nr,Bu),e(Bu,bj),e(nr,Tj),e(nr,Ft),e(Ft,jj),e(Ft,lg),e(lg,kj),e(Ft,Aj),e(S,Dj),e(S,rr),e(rr,Cu),e(Cu,Oj),e(rr,Pj),e(rr,Kt),e(Kt,Rj),e(Kt,ig),e(ig,Nj),e(Kt,Sj),e(S,xj),e(S,or),e(or,Gu),e(Gu,Ij),e(or,Hj),e(or,Jt),e(Jt,Bj),e(Jt,ug),e(ug,Cj),e(Jt,Gj),m(a,U_,g),m(a,Lu,g),e(Lu,Lj),m(a,z_,g),m(a,Wt,g),e(Wt,cg),e(cg,lr),e(lr,Uu),e(Uu,Uj),e(lr,zj),e(lr,fg),e(Wt,Mj),e(Wt,ie),e(ie,ir),e(ir,zu),e(zu,pg),e(pg,Fj),e(ir,Kj),e(ir,Mu),e(Mu,Jj),e(ie,Wj),e(ie,ur),e(ur,Fu),e(Fu,dg),e(dg,Yj),e(ur,Vj),e(ur,Ku),e(Ku,Xj),e(ie,Qj),e(ie,cr),e(cr,Ju),e(Ju,Zj),e(cr,ek),e(cr,Wu),e(Wu,tk),e(ie,sk),e(ie,fr),e(fr,Yu),e(Yu,ak),e(fr,nk),e(fr,Vu),e(Vu,rk),m(a,M_,g),m(a,Ce,g),e(Ce,Yt),e(Yt,hg),y(pr,hg,null),e(Ce,ok),e(Ce,gg),e(gg,lk),m(a,F_,g),m(a,Xu,g),e(Xu,ik),m(a,K_,g),y(Vt,a,g),m(a,J_,g),m(a,dr,g),e(dr,uk),e(dr,hr),e(hr,ck),m(a,W_,g),m(a,Qu,g),e(Qu,fk),m(a,Y_,g),y(Xt,a,g),m(a,V_,g),m(a,Zu,g),e(Zu,pk),m(a,X_,g),m(a,Qt,g),e(Qt,mg),e(mg,gr),e(gr,ec),e(ec,dk),e(gr,hk),e(gr,qg),e(Qt,gk),e(Qt,K),e(K,mr),e(mr,qr),e(qr,$g),e($g,mk),e(qr,qk),e(mr,$k),e(mr,_g),e(K,_k),e(K,$r),e($r,tc),e(tc,vk),e($r,yk),e($r,sc),e(sc,Ek),e(K,wk),e(K,_r),e(_r,ac),e(ac,bk),e(_r,Tk),e(_r,nc),e(nc,jk),e(K,kk),e(K,vr),e(vr,rc),e(rc,vg),e(vg,Ak),e(vr,Dk),e(vr,oc),e(oc,Ok),e(K,Pk),e(K,yr),e(yr,lc),e(lc,Rk),e(yr,Nk),e(yr,Zt),e(Zt,Sk),e(Zt,yg),e(yg,xk),e(Zt,Ik),e(K,Hk),e(K,Er),e(Er,ic),e(ic,Bk),e(Er,Ck),e(Er,es),e(es,Gk),e(es,Eg),e(Eg,Lk),e(es,Uk),e(K,zk),e(K,wr),e(wr,uc),e(uc,Mk),e(wr,Fk),e(wr,ts),e(ts,Kk),e(ts,wg),e(wg,Jk),e(ts,Wk),m(a,Q_,g),m(a,cc,g),e(cc,Yk),m(a,Z_,g),y(ss,a,g),m(a,e1,g),m(a,as,g),e(as,bg),e(bg,br),e(br,fc),e(fc,Vk),e(br,Xk),e(br,Tg),e(as,Qk),e(as,ue),e(ue,Tr),e(Tr,pc),e(pc,jg),e(jg,Zk),e(Tr,e6),e(Tr,dc),e(dc,t6),e(ue,s6),e(ue,jr),e(jr,hc),e(hc,kg),e(kg,a6),e(jr,n6),e(jr,gc),e(gc,r6),e(ue,o6),e(ue,kr),e(kr,mc),e(mc,Ag),e(Ag,l6),e(kr,i6),e(kr,qc),e(qc,u6),e(ue,c6),e(ue,Ar),e(Ar,$c),e($c,Dg),e(Dg,f6),e(Ar,p6),e(Ar,_c),e(_c,d6),m(a,t1,g),m(a,Ge,g),e(Ge,ns),e(ns,Og),y(Dr,Og,null),e(Ge,h6),e(Ge,Pg),e(Pg,g6),m(a,s1,g),m(a,vc,g),e(vc,m6),m(a,a1,g),y(rs,a,g),m(a,n1,g),m(a,Le,g),e(Le,q6),e(Le,Or),e(Or,$6),e(Le,_6),e(Le,Pr),e(Pr,v6),m(a,r1,g),m(a,yc,g),e(yc,y6),m(a,o1,g),y(os,a,g),m(a,l1,g),m(a,Ec,g),e(Ec,E6),m(a,i1,g),m(a,wc,g),e(wc,w6),m(a,u1,g),y(ls,a,g),m(a,c1,g),m(a,is,g),e(is,Rg),e(Rg,Rr),e(Rr,bc),e(bc,b6),e(Rr,T6),e(Rr,Ng),e(is,j6),e(is,ce),e(ce,Nr),e(Nr,Tc),e(Tc,Sg),e(Sg,k6),e(Nr,A6),e(Nr,jc),e(jc,D6),e(ce,O6),e(ce,Sr),e(Sr,kc),e(kc,xg),e(xg,P6),e(Sr,R6),e(Sr,Ac),e(Ac,N6),e(ce,S6),e(ce,xr),e(xr,Dc),e(Dc,Ig),e(Ig,x6),e(xr,I6),e(xr,us),e(us,H6),e(us,Hg),e(Hg,B6),e(us,C6),e(ce,G6),e(ce,Ir),e(Ir,Oc),e(Oc,Bg),e(Bg,L6),e(Ir,U6),e(Ir,cs),e(cs,z6),e(cs,Cg),e(Cg,M6),e(cs,F6),m(a,f1,g),m(a,Ue,g),e(Ue,fs),e(fs,Gg),y(Hr,Gg,null),e(Ue,K6),e(Ue,Lg),e(Lg,J6),m(a,p1,g),m(a,Pc,g),e(Pc,W6),m(a,d1,g),y(ps,a,g),m(a,h1,g),m(a,Br,g),e(Br,Y6),e(Br,Cr),e(Cr,V6),m(a,g1,g),m(a,Rc,g),e(Rc,X6),m(a,m1,g),y(ds,a,g),m(a,q1,g),m(a,Nc,g),e(Nc,Q6),m(a,$1,g),m(a,hs,g),e(hs,Ug),e(Ug,Gr),e(Gr,Sc),e(Sc,Z6),e(Gr,e7),e(Gr,zg),e(hs,t7),e(hs,ee),e(ee,Lr),e(Lr,Ur),e(Ur,Mg),e(Mg,s7),e(Ur,a7),e(Lr,n7),e(Lr,xc),e(xc,r7),e(ee,o7),e(ee,zr),e(zr,Ic),e(Ic,Fg),e(Fg,l7),e(zr,i7),e(zr,Hc),e(Hc,u7),e(ee,c7),e(ee,Mr),e(Mr,Bc),e(Bc,f7),e(Mr,p7),e(Mr,gs),e(gs,d7),e(gs,Kg),e(Kg,h7),e(gs,g7),e(ee,m7),e(ee,Fr),e(Fr,Cc),e(Cc,q7),e(Fr,$7),e(Fr,ms),e(ms,_7),e(ms,Jg),e(Jg,v7),e(ms,y7),e(ee,E7),e(ee,Kr),e(Kr,Gc),e(Gc,w7),e(Kr,b7),e(Kr,qs),e(qs,T7),e(qs,Wg),e(Wg,j7),e(qs,k7),m(a,_1,g),m(a,Lc,g),e(Lc,A7),m(a,v1,g),y($s,a,g),m(a,y1,g),m(a,_s,g),e(_s,Yg),e(Yg,Jr),e(Jr,Uc),e(Uc,D7),e(Jr,O7),e(Jr,Vg),e(_s,P7),e(_s,Wr),e(Wr,Yr),e(Yr,zc),e(zc,Xg),e(Xg,R7),e(Yr,N7),e(Yr,Mc),e(Mc,S7),e(Wr,x7),e(Wr,Vr),e(Vr,Fc),e(Fc,Qg),e(Qg,I7),e(Vr,H7),e(Vr,Kc),e(Kc,B7),m(a,E1,g),m(a,ze,g),e(ze,vs),e(vs,Zg),y(Xr,Zg,null),e(ze,C7),e(ze,em),e(em,G7),m(a,w1,g),m(a,Qr,g),e(Qr,L7),e(Qr,Jc),e(Jc,U7),m(a,b1,g),m(a,Me,g),e(Me,ys),e(ys,tm),y(Zr,tm,null),e(Me,z7),e(Me,sm),e(sm,M7),m(a,T1,g),m(a,Wc,g),e(Wc,F7),m(a,j1,g),y(Es,a,g),m(a,k1,g),m(a,Fe,g),e(Fe,K7),e(Fe,eo),e(eo,J7),e(Fe,W7),e(Fe,to),e(to,Y7),m(a,A1,g),m(a,Yc,g),e(Yc,V7),m(a,D1,g),y(ws,a,g),m(a,O1,g),m(a,Vc,g),e(Vc,X7),m(a,P1,g),m(a,bs,g),e(bs,am),e(am,so),e(so,Xc),e(Xc,Q7),e(so,Z7),e(so,nm),e(bs,e9),e(bs,J),e(J,ao),e(ao,no),e(no,rm),e(rm,t9),e(no,s9),e(ao,a9),e(ao,Qc),e(Qc,n9),e(J,r9),e(J,ro),e(ro,Zc),e(Zc,om),e(om,o9),e(ro,l9),e(ro,ef),e(ef,i9),e(J,u9),e(J,oo),e(oo,tf),e(tf,c9),e(oo,f9),e(oo,x),e(x,p9),e(x,lm),e(lm,d9),e(x,h9),e(x,g9),e(x,m9),e(x,im),e(im,q9),e(x,$9),e(x,_9),e(x,v9),e(x,um),e(um,y9),e(x,E9),e(x,w9),e(x,b9),e(x,cm),e(cm,T9),e(x,j9),e(x,fm),e(fm,k9),e(x,A9),e(x,D9),e(x,O9),e(x,pm),e(pm,P9),e(x,R9),e(x,dm),e(dm,N9),e(x,S9),e(x,x9),e(x,I9),e(x,hm),e(hm,H9),e(x,B9),e(x,gm),e(gm,C9),e(x,G9),e(J,L9),e(J,lo),e(lo,sf),e(sf,mm),e(mm,U9),e(lo,z9),e(lo,af),e(af,M9),e(J,F9),e(J,io),e(io,nf),e(nf,K9),e(io,J9),e(io,Ts),e(Ts,W9),e(Ts,qm),e(qm,Y9),e(Ts,V9),e(J,X9),e(J,uo),e(uo,rf),e(rf,Q9),e(uo,Z9),e(uo,js),e(js,e8),e(js,$m),e($m,t8),e(js,s8),e(J,a8),e(J,co),e(co,of),e(of,n8),e(co,r8),e(co,ks),e(ks,o8),e(ks,_m),e(_m,l8),e(ks,i8),m(a,R1,g),m(a,lf,g),e(lf,u8),m(a,N1,g),y(As,a,g),m(a,S1,g),m(a,Ds,g),e(Ds,vm),e(vm,fo),e(fo,uf),e(uf,c8),e(fo,f8),e(fo,ym),e(Ds,p8),e(Ds,te),e(te,po),e(po,cf),e(cf,Em),e(Em,d8),e(po,h8),e(po,ff),e(ff,g8),e(te,m8),e(te,ho),e(ho,pf),e(pf,wm),e(wm,q8),e(ho,$8),e(ho,df),e(df,_8),e(te,v8),e(te,go),e(go,hf),e(hf,bm),e(bm,y8),e(go,E8),e(go,gf),e(gf,w8),e(te,b8),e(te,mo),e(mo,mf),e(mf,Tm),e(Tm,T8),e(mo,j8),e(mo,Os),e(Os,k8),e(Os,jm),e(jm,A8),e(Os,D8),e(te,O8),e(te,qo),e(qo,qf),e(qf,km),e(km,P8),e(qo,R8),e(qo,Ps),e(Ps,N8),e(Ps,Am),e(Am,S8),e(Ps,x8),m(a,x1,g),m(a,Ke,g),e(Ke,Rs),e(Rs,Dm),y($o,Dm,null),e(Ke,I8),e(Ke,Om),e(Om,H8),m(a,I1,g),m(a,$f,g),e($f,B8),m(a,H1,g),y(Ns,a,g),m(a,B1,g),m(a,_o,g),e(_o,C8),e(_o,vo),e(vo,G8),m(a,C1,g),m(a,_f,g),e(_f,L8),m(a,G1,g),y(Ss,a,g),m(a,L1,g),m(a,vf,g),e(vf,U8),m(a,U1,g),m(a,xs,g),e(xs,Pm),e(Pm,yo),e(yo,yf),e(yf,z8),e(yo,M8),e(yo,Rm),e(xs,F8),e(xs,I),e(I,Eo),e(Eo,wo),e(wo,Nm),e(Nm,K8),e(wo,J8),e(Eo,W8),e(Eo,Ef),e(Ef,Y8),e(I,V8),e(I,bo),e(bo,wf),e(wf,Sm),e(Sm,X8),e(bo,Q8),e(bo,bf),e(bf,Z8),e(I,eA),e(I,To),e(To,Tf),e(Tf,tA),e(To,sA),e(To,Ee),e(Ee,aA),e(Ee,xm),e(xm,nA),e(Ee,rA),e(Ee,Im),e(Im,oA),e(Ee,lA),e(I,iA),e(I,jo),e(jo,jf),e(jf,uA),e(jo,cA),e(jo,le),e(le,fA),e(le,Hm),e(Hm,pA),e(le,dA),e(le,Bm),e(Bm,hA),e(le,gA),e(le,Cm),e(Cm,mA),e(le,qA),e(I,$A),e(I,ko),e(ko,kf),e(kf,_A),e(ko,vA),e(ko,we),e(we,yA),e(we,Gm),e(Gm,EA),e(we,wA),e(we,Lm),e(Lm,bA),e(we,TA),e(I,jA),e(I,Ao),e(Ao,Af),e(Af,kA),e(Ao,AA),e(Ao,Is),e(Is,DA),e(Is,Um),e(Um,OA),e(Is,PA),e(I,RA),e(I,Do),e(Do,Df),e(Df,NA),e(Do,SA),e(Do,be),e(be,xA),e(be,zm),e(zm,IA),e(be,HA),e(be,Mm),e(Mm,BA),e(be,CA),e(I,GA),e(I,Oo),e(Oo,Of),e(Of,LA),e(Oo,UA),e(Oo,Te),e(Te,zA),e(Te,Fm),e(Fm,MA),e(Te,FA),e(Te,Km),e(Km,KA),e(Te,JA),e(I,WA),e(I,Po),e(Po,Pf),e(Pf,YA),e(Po,VA),e(Po,je),e(je,XA),e(je,Jm),e(Jm,QA),e(je,ZA),e(je,Wm),e(Wm,eD),e(je,tD),e(I,sD),e(I,Ro),e(Ro,Rf),e(Rf,aD),e(Ro,nD),e(Ro,Hs),e(Hs,rD),e(Hs,Ym),e(Ym,oD),e(Hs,lD),e(I,iD),e(I,No),e(No,Nf),e(Nf,uD),e(No,cD),e(No,Bs),e(Bs,fD),e(Bs,Vm),e(Vm,pD),e(Bs,dD),e(I,hD),e(I,So),e(So,Sf),e(Sf,Xm),e(Xm,gD),e(So,mD),e(So,xf),e(xf,qD),e(I,$D),e(I,xo),e(xo,If),e(If,_D),e(xo,vD),e(xo,Cs),e(Cs,yD),e(Cs,Qm),e(Qm,ED),e(Cs,wD),e(I,bD),e(I,Io),e(Io,Hf),e(Hf,TD),e(Io,jD),e(Io,Gs),e(Gs,kD),e(Gs,Zm),e(Zm,AD),e(Gs,DD),e(I,OD),e(I,Ho),e(Ho,Bf),e(Bf,PD),e(Ho,RD),e(Ho,Ls),e(Ls,ND),e(Ls,eq),e(eq,SD),e(Ls,xD),m(a,z1,g),m(a,Cf,g),e(Cf,ID),m(a,M1,g),y(Us,a,g),m(a,F1,g),m(a,zs,g),e(zs,tq),e(tq,Bo),e(Bo,Gf),e(Gf,HD),e(Bo,BD),e(Bo,sq),e(zs,CD),e(zs,aq),e(aq,Co),e(Co,Lf),e(Lf,nq),e(nq,GD),e(Co,LD),e(Co,Uf),e(Uf,UD),m(a,K1,g),m(a,Je,g),e(Je,Ms),e(Ms,rq),y(Go,rq,null),e(Je,zD),e(Je,oq),e(oq,MD),m(a,J1,g),m(a,Fs,g),e(Fs,FD),e(Fs,zf),e(zf,KD),e(Fs,JD),m(a,W1,g),m(a,We,g),e(We,Ks),e(Ks,lq),y(Lo,lq,null),e(We,WD),e(We,iq),e(iq,YD),m(a,Y1,g),m(a,Mf,g),e(Mf,VD),m(a,V1,g),y(Js,a,g),m(a,X1,g),m(a,Uo,g),e(Uo,XD),e(Uo,zo),e(zo,QD),m(a,Q1,g),m(a,Ff,g),e(Ff,ZD),m(a,Z1,g),y(Ws,a,g),m(a,e2,g),m(a,Kf,g),e(Kf,eO),m(a,t2,g),m(a,Ys,g),e(Ys,uq),e(uq,Mo),e(Mo,Jf),e(Jf,tO),e(Mo,sO),e(Mo,cq),e(Ys,aO),e(Ys,se),e(se,Fo),e(Fo,Ko),e(Ko,fq),e(fq,nO),e(Ko,rO),e(Fo,oO),e(Fo,Wf),e(Wf,lO),e(se,iO),e(se,Jo),e(Jo,Yf),e(Yf,pq),e(pq,uO),e(Jo,cO),e(Jo,Vf),e(Vf,fO),e(se,pO),e(se,Wo),e(Wo,Xf),e(Xf,dO),e(Wo,hO),e(Wo,Vs),e(Vs,gO),e(Vs,dq),e(dq,mO),e(Vs,qO),e(se,$O),e(se,Yo),e(Yo,Qf),e(Qf,_O),e(Yo,vO),e(Yo,Xs),e(Xs,yO),e(Xs,hq),e(hq,EO),e(Xs,wO),e(se,bO),e(se,Vo),e(Vo,Zf),e(Zf,TO),e(Vo,jO),e(Vo,Qs),e(Qs,kO),e(Qs,gq),e(gq,AO),e(Qs,DO),m(a,s2,g),m(a,ep,g),e(ep,OO),m(a,a2,g),y(Zs,a,g),m(a,n2,g),m(a,ea,g),e(ea,mq),e(mq,Xo),e(Xo,tp),e(tp,PO),e(Xo,RO),e(Xo,qq),e(ea,NO),e(ea,fe),e(fe,Qo),e(Qo,sp),e(sp,$q),e($q,SO),e(Qo,xO),e(Qo,ap),e(ap,IO),e(fe,HO),e(fe,Zo),e(Zo,np),e(np,_q),e(_q,BO),e(Zo,CO),e(Zo,rp),e(rp,GO),e(fe,LO),e(fe,el),e(el,op),e(op,vq),e(vq,UO),e(el,zO),e(el,lp),e(lp,MO),e(fe,FO),e(fe,tl),e(tl,ip),e(ip,yq),e(yq,KO),e(tl,JO),e(tl,up),e(up,WO),m(a,r2,g),m(a,Ye,g),e(Ye,ta),e(ta,Eq),y(sl,Eq,null),e(Ye,YO),e(Ye,wq),e(wq,VO),m(a,o2,g),m(a,cp,g),e(cp,XO),m(a,l2,g),y(sa,a,g),m(a,i2,g),y(aa,a,g),m(a,u2,g),m(a,pe,g),e(pe,QO),e(pe,al),e(al,ZO),e(pe,eP),e(pe,nl),e(nl,tP),e(pe,sP),e(pe,rl),e(rl,aP),m(a,c2,g),m(a,fp,g),e(fp,nP),m(a,f2,g),y(na,a,g),m(a,p2,g),m(a,pp,g),e(pp,rP),m(a,d2,g),m(a,ra,g),e(ra,bq),e(bq,ol),e(ol,dp),e(dp,oP),e(ol,lP),e(ol,Tq),e(ra,iP),e(ra,jq),e(jq,ll),e(ll,il),e(il,kq),e(kq,uP),e(il,cP),e(ll,fP),e(ll,hp),e(hp,pP),m(a,h2,g),m(a,gp,g),e(gp,dP),m(a,g2,g),m(a,mp,g),e(mp,hP),m(a,m2,g),y(oa,a,g),m(a,q2,g),m(a,la,g),e(la,Aq),e(Aq,ul),e(ul,qp),e(qp,gP),e(ul,mP),e(ul,Dq),e(la,qP),e(la,Oq),e(Oq,cl),e(cl,$p),e($p,Pq),e(Pq,$P),e(cl,_P),e(cl,_p),e(_p,vP),m(a,$2,g),m(a,Ve,g),e(Ve,ia),e(ia,Rq),y(fl,Rq,null),e(Ve,yP),e(Ve,Nq),e(Nq,EP),m(a,_2,g),m(a,vp,g),e(vp,wP),m(a,v2,g),y(ua,a,g),m(a,y2,g),m(a,Xe,g),e(Xe,bP),e(Xe,pl),e(pl,TP),e(Xe,jP),e(Xe,dl),e(dl,kP),m(a,E2,g),m(a,yp,g),e(yp,AP),m(a,w2,g),m(a,ca,g),e(ca,Sq),e(Sq,hl),e(hl,Ep),e(Ep,DP),e(hl,OP),e(hl,xq),e(ca,PP),e(ca,ae),e(ae,gl),e(gl,ml),e(ml,Iq),e(Iq,RP),e(ml,NP),e(gl,SP),e(gl,wp),e(wp,xP),e(ae,IP),e(ae,ql),e(ql,bp),e(bp,Hq),e(Hq,HP),e(ql,BP),e(ql,Tp),e(Tp,CP),e(ae,GP),e(ae,$l),e($l,jp),e(jp,LP),e($l,UP),e($l,fa),e(fa,zP),e(fa,Bq),e(Bq,MP),e(fa,FP),e(ae,KP),e(ae,_l),e(_l,kp),e(kp,JP),e(_l,WP),e(_l,pa),e(pa,YP),e(pa,Cq),e(Cq,VP),e(pa,XP),e(ae,QP),e(ae,vl),e(vl,Ap),e(Ap,ZP),e(vl,eR),e(vl,da),e(da,tR),e(da,Gq),e(Gq,sR),e(da,aR),m(a,b2,g),m(a,Dp,g),e(Dp,nR),m(a,T2,g),m(a,ha,g),e(ha,Lq),e(Lq,yl),e(yl,Op),e(Op,rR),e(yl,oR),e(yl,Uq),e(ha,lR),e(ha,zq),e(zq,El),e(El,Pp),e(Pp,Mq),e(Mq,iR),e(El,uR),e(El,Rp),e(Rp,cR),m(a,j2,g),m(a,Np,g),e(Np,fR),m(a,k2,g),m(a,Qe,g),e(Qe,ga),e(ga,Fq),y(wl,Fq,null),e(Qe,pR),e(Qe,Kq),e(Kq,dR),m(a,A2,g),m(a,Sp,g),e(Sp,hR),m(a,D2,g),y(ma,a,g),m(a,O2,g),m(a,Ze,g),e(Ze,gR),e(Ze,bl),e(bl,mR),e(Ze,qR),e(Ze,Tl),e(Tl,$R),m(a,P2,g),m(a,xp,g),e(xp,_R),m(a,R2,g),y(qa,a,g),m(a,N2,g),m(a,Ip,g),e(Ip,vR),m(a,S2,g),m(a,$a,g),e($a,Jq),e(Jq,jl),e(jl,Hp),e(Hp,yR),e(jl,ER),e(jl,Wq),e($a,wR),e($a,Yq),e(Yq,kl),e(kl,Al),e(Al,Vq),e(Vq,bR),e(Al,TR),e(kl,jR),e(kl,Bp),e(Bp,kR),m(a,x2,g),m(a,Cp,g),e(Cp,AR),m(a,I2,g),y(_a,a,g),m(a,H2,g),m(a,va,g),e(va,Xq),e(Xq,Dl),e(Dl,Gp),e(Gp,DR),e(Dl,OR),e(Dl,Qq),e(va,PR),e(va,Ol),e(Ol,Pl),e(Pl,Lp),e(Lp,Zq),e(Zq,RR),e(Pl,NR),e(Pl,Up),e(Up,SR),e(Ol,xR),e(Ol,Rl),e(Rl,zp),e(zp,e$),e(e$,IR),e(Rl,HR),e(Rl,Mp),e(Mp,BR),m(a,B2,g),m(a,et,g),e(et,ya),e(ya,t$),y(Nl,t$,null),e(et,CR),e(et,s$),e(s$,GR),m(a,C2,g),m(a,Fp,g),e(Fp,LR),m(a,G2,g),y(Ea,a,g),m(a,L2,g),m(a,Sl,g),e(Sl,UR),e(Sl,xl),e(xl,zR),m(a,U2,g),m(a,Kp,g),e(Kp,MR),m(a,z2,g),y(wa,a,g),m(a,M2,g),m(a,ba,g),e(ba,FR),e(ba,Il),e(Il,KR),e(ba,JR),m(a,F2,g),m(a,Ta,g),e(Ta,a$),e(a$,Hl),e(Hl,Jp),e(Jp,WR),e(Hl,YR),e(Hl,n$),e(Ta,VR),e(Ta,r$),e(r$,Bl),e(Bl,Cl),e(Cl,o$),e(o$,XR),e(Cl,QR),e(Bl,ZR),e(Bl,Wp),e(Wp,eN),m(a,K2,g),m(a,Yp,g),e(Yp,tN),m(a,J2,g),y(ja,a,g),m(a,W2,g),m(a,ka,g),e(ka,l$),e(l$,Gl),e(Gl,Vp),e(Vp,sN),e(Gl,aN),e(Gl,i$),e(ka,nN),e(ka,tt),e(tt,Ll),e(Ll,Xp),e(Xp,u$),e(u$,rN),e(Ll,oN),e(Ll,Qp),e(Qp,lN),e(tt,iN),e(tt,Ul),e(Ul,Zp),e(Zp,c$),e(c$,uN),e(Ul,cN),e(Ul,ed),e(ed,fN),e(tt,pN),e(tt,zl),e(zl,td),e(td,f$),e(f$,dN),e(zl,hN),e(zl,sd),e(sd,gN),m(a,Y2,g),m(a,st,g),e(st,Aa),e(Aa,p$),y(Ml,p$,null),e(st,mN),e(st,d$),e(d$,qN),m(a,V2,g),m(a,ad,g),e(ad,$N),m(a,X2,g),y(Da,a,g),m(a,Q2,g),m(a,Fl,g),e(Fl,_N),e(Fl,Kl),e(Kl,vN),m(a,Z2,g),m(a,nd,g),e(nd,yN),m(a,ev,g),y(Oa,a,g),m(a,tv,g),m(a,Pa,g),e(Pa,EN),e(Pa,Jl),e(Jl,wN),e(Pa,bN),m(a,sv,g),m(a,Ra,g),e(Ra,h$),e(h$,Wl),e(Wl,rd),e(rd,TN),e(Wl,jN),e(Wl,g$),e(Ra,kN),e(Ra,m$),e(m$,Yl),e(Yl,Vl),e(Vl,q$),e(q$,AN),e(Vl,DN),e(Yl,ON),e(Yl,od),e(od,PN),m(a,av,g),m(a,ld,g),e(ld,RN),m(a,nv,g),y(Na,a,g),m(a,rv,g),m(a,Sa,g),e(Sa,$$),e($$,Xl),e(Xl,id),e(id,NN),e(Xl,SN),e(Xl,_$),e(Sa,xN),e(Sa,at),e(at,Ql),e(Ql,ud),e(ud,v$),e(v$,IN),e(Ql,HN),e(Ql,cd),e(cd,BN),e(at,CN),e(at,Zl),e(Zl,fd),e(fd,y$),e(y$,GN),e(Zl,LN),e(Zl,pd),e(pd,UN),e(at,zN),e(at,ei),e(ei,dd),e(dd,E$),e(E$,MN),e(ei,FN),e(ei,hd),e(hd,KN),ov=!0},p(a,[g]){const ti={};g&2&&(ti.$$scope={dirty:g,ctx:a}),ut.$set(ti);const w$={};g&2&&(w$.$$scope={dirty:g,ctx:a}),ct.$set(w$);const b$={};g&2&&(b$.$$scope={dirty:g,ctx:a}),mt.$set(b$);const T$={};g&2&&(T$.$$scope={dirty:g,ctx:a}),vt.$set(T$);const si={};g&2&&(si.$$scope={dirty:g,ctx:a}),yt.$set(si);const j$={};g&2&&(j$.$$scope={dirty:g,ctx:a}),Dt.$set(j$);const k$={};g&2&&(k$.$$scope={dirty:g,ctx:a}),Ot.$set(k$);const A$={};g&2&&(A$.$$scope={dirty:g,ctx:a}),Ct.$set(A$);const D$={};g&2&&(D$.$$scope={dirty:g,ctx:a}),Gt.$set(D$);const O$={};g&2&&(O$.$$scope={dirty:g,ctx:a}),Vt.$set(O$);const ai={};g&2&&(ai.$$scope={dirty:g,ctx:a}),Xt.$set(ai);const P$={};g&2&&(P$.$$scope={dirty:g,ctx:a}),ss.$set(P$);const R$={};g&2&&(R$.$$scope={dirty:g,ctx:a}),rs.$set(R$);const N$={};g&2&&(N$.$$scope={dirty:g,ctx:a}),os.$set(N$);const S$={};g&2&&(S$.$$scope={dirty:g,ctx:a}),ls.$set(S$);const gd={};g&2&&(gd.$$scope={dirty:g,ctx:a}),ps.$set(gd);const x$={};g&2&&(x$.$$scope={dirty:g,ctx:a}),ds.$set(x$);const I$={};g&2&&(I$.$$scope={dirty:g,ctx:a}),$s.$set(I$);const H$={};g&2&&(H$.$$scope={dirty:g,ctx:a}),Es.$set(H$);const ni={};g&2&&(ni.$$scope={dirty:g,ctx:a}),ws.$set(ni);const B$={};g&2&&(B$.$$scope={dirty:g,ctx:a}),As.$set(B$);const ri={};g&2&&(ri.$$scope={dirty:g,ctx:a}),Ns.$set(ri);const C$={};g&2&&(C$.$$scope={dirty:g,ctx:a}),Ss.$set(C$);const M={};g&2&&(M.$$scope={dirty:g,ctx:a}),Us.$set(M);const oi={};g&2&&(oi.$$scope={dirty:g,ctx:a}),Js.$set(oi);const md={};g&2&&(md.$$scope={dirty:g,ctx:a}),Ws.$set(md);const G$={};g&2&&(G$.$$scope={dirty:g,ctx:a}),Zs.$set(G$);const L$={};g&2&&(L$.$$scope={dirty:g,ctx:a}),sa.$set(L$);const li={};g&2&&(li.$$scope={dirty:g,ctx:a}),aa.$set(li);const qd={};g&2&&(qd.$$scope={dirty:g,ctx:a}),na.$set(qd);const U$={};g&2&&(U$.$$scope={dirty:g,ctx:a}),oa.$set(U$);const z$={};g&2&&(z$.$$scope={dirty:g,ctx:a}),ua.$set(z$);const ii={};g&2&&(ii.$$scope={dirty:g,ctx:a}),ma.$set(ii);const M$={};g&2&&(M$.$$scope={dirty:g,ctx:a}),qa.$set(M$);const nt={};g&2&&(nt.$$scope={dirty:g,ctx:a}),_a.$set(nt);const F$={};g&2&&(F$.$$scope={dirty:g,ctx:a}),Ea.$set(F$);const K$={};g&2&&(K$.$$scope={dirty:g,ctx:a}),wa.$set(K$);const ui={};g&2&&(ui.$$scope={dirty:g,ctx:a}),ja.$set(ui);const J$={};g&2&&(J$.$$scope={dirty:g,ctx:a}),Da.$set(J$);const ci={};g&2&&(ci.$$scope={dirty:g,ctx:a}),Oa.$set(ci);const W$={};g&2&&(W$.$$scope={dirty:g,ctx:a}),Na.$set(W$)},i(a){ov||(E(k.$$.fragment,a),E(Q.$$.fragment,a),E(za.$$.fragment,a),E(ut.$$.fragment,a),E(ct.$$.fragment,a),E(mt.$$.fragment,a),E(ln.$$.fragment,a),E(vt.$$.fragment,a),E(yt.$$.fragment,a),E(vn.$$.fragment,a),E(Dt.$$.fragment,a),E(Ot.$$.fragment,a),E(Gn.$$.fragment,a),E(Ct.$$.fragment,a),E(Gt.$$.fragment,a),E(pr.$$.fragment,a),E(Vt.$$.fragment,a),E(Xt.$$.fragment,a),E(ss.$$.fragment,a),E(Dr.$$.fragment,a),E(rs.$$.fragment,a),E(os.$$.fragment,a),E(ls.$$.fragment,a),E(Hr.$$.fragment,a),E(ps.$$.fragment,a),E(ds.$$.fragment,a),E($s.$$.fragment,a),E(Xr.$$.fragment,a),E(Zr.$$.fragment,a),E(Es.$$.fragment,a),E(ws.$$.fragment,a),E(As.$$.fragment,a),E($o.$$.fragment,a),E(Ns.$$.fragment,a),E(Ss.$$.fragment,a),E(Us.$$.fragment,a),E(Go.$$.fragment,a),E(Lo.$$.fragment,a),E(Js.$$.fragment,a),E(Ws.$$.fragment,a),E(Zs.$$.fragment,a),E(sl.$$.fragment,a),E(sa.$$.fragment,a),E(aa.$$.fragment,a),E(na.$$.fragment,a),E(oa.$$.fragment,a),E(fl.$$.fragment,a),E(ua.$$.fragment,a),E(wl.$$.fragment,a),E(ma.$$.fragment,a),E(qa.$$.fragment,a),E(_a.$$.fragment,a),E(Nl.$$.fragment,a),E(Ea.$$.fragment,a),E(wa.$$.fragment,a),E(ja.$$.fragment,a),E(Ml.$$.fragment,a),E(Da.$$.fragment,a),E(Oa.$$.fragment,a),E(Na.$$.fragment,a),ov=!0)},o(a){w(k.$$.fragment,a),w(Q.$$.fragment,a),w(za.$$.fragment,a),w(ut.$$.fragment,a),w(ct.$$.fragment,a),w(mt.$$.fragment,a),w(ln.$$.fragment,a),w(vt.$$.fragment,a),w(yt.$$.fragment,a),w(vn.$$.fragment,a),w(Dt.$$.fragment,a),w(Ot.$$.fragment,a),w(Gn.$$.fragment,a),w(Ct.$$.fragment,a),w(Gt.$$.fragment,a),w(pr.$$.fragment,a),w(Vt.$$.fragment,a),w(Xt.$$.fragment,a),w(ss.$$.fragment,a),w(Dr.$$.fragment,a),w(rs.$$.fragment,a),w(os.$$.fragment,a),w(ls.$$.fragment,a),w(Hr.$$.fragment,a),w(ps.$$.fragment,a),w(ds.$$.fragment,a),w($s.$$.fragment,a),w(Xr.$$.fragment,a),w(Zr.$$.fragment,a),w(Es.$$.fragment,a),w(ws.$$.fragment,a),w(As.$$.fragment,a),w($o.$$.fragment,a),w(Ns.$$.fragment,a),w(Ss.$$.fragment,a),w(Us.$$.fragment,a),w(Go.$$.fragment,a),w(Lo.$$.fragment,a),w(Js.$$.fragment,a),w(Ws.$$.fragment,a),w(Zs.$$.fragment,a),w(sl.$$.fragment,a),w(sa.$$.fragment,a),w(aa.$$.fragment,a),w(na.$$.fragment,a),w(oa.$$.fragment,a),w(fl.$$.fragment,a),w(ua.$$.fragment,a),w(wl.$$.fragment,a),w(ma.$$.fragment,a),w(qa.$$.fragment,a),w(_a.$$.fragment,a),w(Nl.$$.fragment,a),w(Ea.$$.fragment,a),w(wa.$$.fragment,a),w(ja.$$.fragment,a),w(Ml.$$.fragment,a),w(Da.$$.fragment,a),w(Oa.$$.fragment,a),w(Na.$$.fragment,a),ov=!1},d(a){t(n),a&&t(c),a&&t(s),b(k),a&&t(O),a&&t(D),b(Q),a&&t(Ua),a&&t(Ne),a&&t(Z$),a&&t(di),a&&t(e_),a&&t(ot),a&&t(t_),a&&t(lt),a&&t(s_),a&&t(Se),b(za),a&&t(a_),a&&t(hi),a&&t(n_),b(ut,a),a&&t(r_),a&&t(Ma),a&&t(o_),a&&t(gi),a&&t(l_),b(ct,a),a&&t(i_),a&&t(mi),a&&t(u_),a&&t(ft),a&&t(c_),a&&t(ki),a&&t(f_),a&&t(Ai),a&&t(p_),b(mt,a),a&&t(d_),a&&t(qt),a&&t(h_),a&&t(Ie),b(ln),a&&t(g_),a&&t(xi),a&&t(m_),b(vt,a),a&&t(q_),a&&t(un),a&&t($_),a&&t(Ii),a&&t(__),b(yt,a),a&&t(v_),a&&t(Hi),a&&t(y_),a&&t(Et),a&&t(E_),a&&t(Fi),a&&t(w_),a&&t(jt),a&&t(b_),a&&t(He),b(vn),a&&t(T_),a&&t(At),a&&t(j_),b(Dt,a),a&&t(k_),a&&t(yn),a&&t(A_),a&&t(Vi),a&&t(D_),b(Ot,a),a&&t(O_),a&&t(Xi),a&&t(P_),a&&t(Pt),a&&t(R_),a&&t(hu),a&&t(N_),a&&t(Ht),a&&t(S_),a&&t(Be),b(Gn),a&&t(x_),a&&t($u),a&&t(I_),b(Ct,a),a&&t(H_),a&&t(Ln),a&&t(B_),a&&t(_u),a&&t(C_),b(Gt,a),a&&t(G_),a&&t(vu),a&&t(L_),a&&t(Lt),a&&t(U_),a&&t(Lu),a&&t(z_),a&&t(Wt),a&&t(M_),a&&t(Ce),b(pr),a&&t(F_),a&&t(Xu),a&&t(K_),b(Vt,a),a&&t(J_),a&&t(dr),a&&t(W_),a&&t(Qu),a&&t(Y_),b(Xt,a),a&&t(V_),a&&t(Zu),a&&t(X_),a&&t(Qt),a&&t(Q_),a&&t(cc),a&&t(Z_),b(ss,a),a&&t(e1),a&&t(as),a&&t(t1),a&&t(Ge),b(Dr),a&&t(s1),a&&t(vc),a&&t(a1),b(rs,a),a&&t(n1),a&&t(Le),a&&t(r1),a&&t(yc),a&&t(o1),b(os,a),a&&t(l1),a&&t(Ec),a&&t(i1),a&&t(wc),a&&t(u1),b(ls,a),a&&t(c1),a&&t(is),a&&t(f1),a&&t(Ue),b(Hr),a&&t(p1),a&&t(Pc),a&&t(d1),b(ps,a),a&&t(h1),a&&t(Br),a&&t(g1),a&&t(Rc),a&&t(m1),b(ds,a),a&&t(q1),a&&t(Nc),a&&t($1),a&&t(hs),a&&t(_1),a&&t(Lc),a&&t(v1),b($s,a),a&&t(y1),a&&t(_s),a&&t(E1),a&&t(ze),b(Xr),a&&t(w1),a&&t(Qr),a&&t(b1),a&&t(Me),b(Zr),a&&t(T1),a&&t(Wc),a&&t(j1),b(Es,a),a&&t(k1),a&&t(Fe),a&&t(A1),a&&t(Yc),a&&t(D1),b(ws,a),a&&t(O1),a&&t(Vc),a&&t(P1),a&&t(bs),a&&t(R1),a&&t(lf),a&&t(N1),b(As,a),a&&t(S1),a&&t(Ds),a&&t(x1),a&&t(Ke),b($o),a&&t(I1),a&&t($f),a&&t(H1),b(Ns,a),a&&t(B1),a&&t(_o),a&&t(C1),a&&t(_f),a&&t(G1),b(Ss,a),a&&t(L1),a&&t(vf),a&&t(U1),a&&t(xs),a&&t(z1),a&&t(Cf),a&&t(M1),b(Us,a),a&&t(F1),a&&t(zs),a&&t(K1),a&&t(Je),b(Go),a&&t(J1),a&&t(Fs),a&&t(W1),a&&t(We),b(Lo),a&&t(Y1),a&&t(Mf),a&&t(V1),b(Js,a),a&&t(X1),a&&t(Uo),a&&t(Q1),a&&t(Ff),a&&t(Z1),b(Ws,a),a&&t(e2),a&&t(Kf),a&&t(t2),a&&t(Ys),a&&t(s2),a&&t(ep),a&&t(a2),b(Zs,a),a&&t(n2),a&&t(ea),a&&t(r2),a&&t(Ye),b(sl),a&&t(o2),a&&t(cp),a&&t(l2),b(sa,a),a&&t(i2),b(aa,a),a&&t(u2),a&&t(pe),a&&t(c2),a&&t(fp),a&&t(f2),b(na,a),a&&t(p2),a&&t(pp),a&&t(d2),a&&t(ra),a&&t(h2),a&&t(gp),a&&t(g2),a&&t(mp),a&&t(m2),b(oa,a),a&&t(q2),a&&t(la),a&&t($2),a&&t(Ve),b(fl),a&&t(_2),a&&t(vp),a&&t(v2),b(ua,a),a&&t(y2),a&&t(Xe),a&&t(E2),a&&t(yp),a&&t(w2),a&&t(ca),a&&t(b2),a&&t(Dp),a&&t(T2),a&&t(ha),a&&t(j2),a&&t(Np),a&&t(k2),a&&t(Qe),b(wl),a&&t(A2),a&&t(Sp),a&&t(D2),b(ma,a),a&&t(O2),a&&t(Ze),a&&t(P2),a&&t(xp),a&&t(R2),b(qa,a),a&&t(N2),a&&t(Ip),a&&t(S2),a&&t($a),a&&t(x2),a&&t(Cp),a&&t(I2),b(_a,a),a&&t(H2),a&&t(va),a&&t(B2),a&&t(et),b(Nl),a&&t(C2),a&&t(Fp),a&&t(G2),b(Ea,a),a&&t(L2),a&&t(Sl),a&&t(U2),a&&t(Kp),a&&t(z2),b(wa,a),a&&t(M2),a&&t(ba),a&&t(F2),a&&t(Ta),a&&t(K2),a&&t(Yp),a&&t(J2),b(ja,a),a&&t(W2),a&&t(ka),a&&t(Y2),a&&t(st),b(Ml),a&&t(V2),a&&t(ad),a&&t(X2),b(Da,a),a&&t(Q2),a&&t(Fl),a&&t(Z2),a&&t(nd),a&&t(ev),b(Oa,a),a&&t(tv),a&&t(Pa),a&&t(sv),a&&t(Ra),a&&t(av),a&&t(ld),a&&t(nv),b(Na,a),a&&t(rv),a&&t(Sa)}}}const _K={local:"detailed-parameters",sections:[{local:"which-task-is-used-by-this-model",title:"Which task is used by this model ?"},{local:"zeroshot-classification-task",title:"Zero-shot classification task"},{local:"translation-task",title:"Translation task"},{local:"summarization-task",title:"Summarization task"},{local:"conversational-task",title:"Conversational task"},{local:"table-question-answering-task",title:"Table question answering task"},{local:"question-answering-task",title:"Question answering task"},{local:"textclassification-task",title:"Text-classification task"},{local:"named-entity-recognition-ner-task",title:"Named Entity Recognition (NER) task"},{local:"tokenclassification-task",title:"Token-classification task"},{local:"textgeneration-task",title:"Text-generation task"},{local:"text2textgeneration-task",title:"Text2text-generation task"},{local:"fill-mask-task",title:"Fill mask task"},{local:"automatic-speech-recognition-task",title:"Automatic speech recognition task"},{local:"featureextraction-task",title:"Feature-extraction task"},{local:"audioclassification-task",title:"Audio-classification task"},{local:"objectdetection-task",title:"Object-detection task"},{local:"image-segmentation-task",title:"Image Segmentation task"}],title:"Detailed parameters"};function vK($){return aM(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class TK extends Zz{constructor(n){super();eM(this,n,vK,$K,tM,{})}}export{TK as default,_K as metadata};
