import{S as tK,i as sK,s as aK,e as r,k as f,w as _,t as i,M as nK,c as o,d as s,m as p,a as l,x as v,h as u,b as h,N as eK,F as e,g as m,y,q as b,o as E,B as w,v as rK,L as O}from"../chunks/vendor-7c454903.js";import{T as W}from"../chunks/Tip-735285fc.js";import{I as z}from"../chunks/IconCopyLink-5457534b.js";import{I as L,M as P,C as R}from"../chunks/InferenceApi-041dc1b2.js";function oK($){let n,c,t,d,q,k,A;return{c(){n=r("p"),c=r("strong"),t=i("Recommended model"),d=i(`:
`),q=r("a"),k=i("facebook/bart-large-mnli"),A=i("."),this.h()},l(j){n=o(j,"P",{});var T=l(n);c=o(T,"STRONG",{});var N=l(c);t=u(N,"Recommended model"),N.forEach(s),d=u(T,`:
`),q=o(T,"A",{href:!0,rel:!0});var D=l(q);k=u(D,"facebook/bart-large-mnli"),D.forEach(s),A=u(T,"."),T.forEach(s),this.h()},h(){h(q,"href","https://huggingface.co/facebook/bart-large-mnli"),h(q,"rel","nofollow")},m(j,T){m(j,n,T),e(n,c),e(c,t),e(n,d),e(n,q),e(q,k),e(n,A)},d(j){j&&s(n)}}}function lK($){let n,c;return n=new R({props:{code:`import json
import requests
headers = {"Authorization": f"Bearer {API_TOKEN}"}
API_URL = "https://api-inference.huggingface.co/models/facebook/bart-large-mnli"
def query(payload):
    data = json.dumps(payload)
    response = requests.request("POST", API_URL, headers=headers, data=data)
    return json.loads(response.content.decode("utf-8"))
data = query(
    {
        "inputs": "Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!",
        "parameters": {"candidate_labels": ["refund", "legal", "faq"]},
    }
)`,highlighted:`<span class="hljs-keyword">import</span> json
<span class="hljs-keyword">import</span> requests
headers = {<span class="hljs-string">&quot;Authorization&quot;</span>: <span class="hljs-string">f&quot;Bearer <span class="hljs-subst">{API_TOKEN}</span>&quot;</span>}
API_URL = <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/facebook/bart-large-mnli&quot;</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">payload</span>):
    data = json.dumps(payload)
    response = requests.request(<span class="hljs-string">&quot;POST&quot;</span>, API_URL, headers=headers, data=data)
    <span class="hljs-keyword">return</span> json.loads(response.content.decode(<span class="hljs-string">&quot;utf-8&quot;</span>))
data = query(
    {
        <span class="hljs-string">&quot;inputs&quot;</span>: <span class="hljs-string">&quot;Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!&quot;</span>,
        <span class="hljs-string">&quot;parameters&quot;</span>: {<span class="hljs-string">&quot;candidate_labels&quot;</span>: [<span class="hljs-string">&quot;refund&quot;</span>, <span class="hljs-string">&quot;legal&quot;</span>, <span class="hljs-string">&quot;faq&quot;</span>]},
    }
)`}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),c=!0},p:O,i(t){c||(b(n.$$.fragment,t),c=!0)},o(t){E(n.$$.fragment,t),c=!1},d(t){w(n,t)}}}function iK($){let n,c;return n=new P({props:{$$slots:{default:[lK]},$$scope:{ctx:$}}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),c=!0},p(t,d){const q={};d&2&&(q.$$scope={dirty:d,ctx:t}),n.$set(q)},i(t){c||(b(n.$$.fragment,t),c=!0)},o(t){E(n.$$.fragment,t),c=!1},d(t){w(n,t)}}}function uK($){let n,c;return n=new R({props:{code:`import fetch from "node-fetch";
async function query(data) {
    const response = await fetch(
        "https://api-inference.huggingface.co/models/facebook/bart-large-mnli",
        {
            headers: { Authorization: \`Bearer \${API_TOKEN}\` },
            method: "POST",
            body: JSON.stringify(data),
        }
    );
    const result = await response.json();
    return result;
}
query({inputs: "Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!", parameters: {candidate_labels: ["refund", "legal", "faq"]}}).then((response) => {
    console.log(JSON.stringify(response));
});
// {"sequence":"Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!","labels":["refund","faq","legal"],"scores":[0.8778, 0.1052, 0.017]}`,highlighted:`<span class="hljs-keyword">import</span> fetch <span class="hljs-keyword">from</span> <span class="hljs-string">&quot;node-fetch&quot;</span>;
<span class="hljs-keyword">async</span> <span class="hljs-keyword">function</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">data</span>) {
    <span class="hljs-keyword">const</span> response = <span class="hljs-keyword">await</span> <span class="hljs-title function_">fetch</span>(
        <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/facebook/bart-large-mnli&quot;</span>,
        {
            <span class="hljs-attr">headers</span>: { <span class="hljs-title class_">Authorization</span>: <span class="hljs-string">\`Bearer <span class="hljs-subst">\${API_TOKEN}</span>\`</span> },
            <span class="hljs-attr">method</span>: <span class="hljs-string">&quot;POST&quot;</span>,
            <span class="hljs-attr">body</span>: <span class="hljs-title class_">JSON</span>.<span class="hljs-title function_">stringify</span>(data),
        }
    );
    <span class="hljs-keyword">const</span> result = <span class="hljs-keyword">await</span> response.<span class="hljs-title function_">json</span>();
    <span class="hljs-keyword">return</span> result;
}
<span class="hljs-title function_">query</span>({<span class="hljs-attr">inputs</span>: <span class="hljs-string">&quot;Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!&quot;</span>, <span class="hljs-attr">parameters</span>: {<span class="hljs-attr">candidate_labels</span>: [<span class="hljs-string">&quot;refund&quot;</span>, <span class="hljs-string">&quot;legal&quot;</span>, <span class="hljs-string">&quot;faq&quot;</span>]}}).<span class="hljs-title function_">then</span>(<span class="hljs-function">(<span class="hljs-params">response</span>) =&gt;</span> {
    <span class="hljs-variable language_">console</span>.<span class="hljs-title function_">log</span>(<span class="hljs-title class_">JSON</span>.<span class="hljs-title function_">stringify</span>(response));
});
<span class="hljs-comment">// {&quot;sequence&quot;:&quot;Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!&quot;,&quot;labels&quot;:[&quot;refund&quot;,&quot;faq&quot;,&quot;legal&quot;],&quot;scores&quot;:[0.8778, 0.1052, 0.017]}</span>`}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),c=!0},p:O,i(t){c||(b(n.$$.fragment,t),c=!0)},o(t){E(n.$$.fragment,t),c=!1},d(t){w(n,t)}}}function cK($){let n,c;return n=new P({props:{$$slots:{default:[uK]},$$scope:{ctx:$}}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),c=!0},p(t,d){const q={};d&2&&(q.$$scope={dirty:d,ctx:t}),n.$set(q)},i(t){c||(b(n.$$.fragment,t),c=!0)},o(t){E(n.$$.fragment,t),c=!1},d(t){w(n,t)}}}function fK($){let n,c;return n=new R({props:{code:`curl https://api-inference.huggingface.co/models/facebook/bart-large-mnli \\
        -X POST \\
        -d '{"inputs": "Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!", "parameters": {"candidate_labels": ["refund", "legal", "faq"]}}' \\
        -H "Authorization: Bearer \${HF_API_TOKEN}"
# {"sequence":"Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!","labels":["refund","faq","legal"],"scores":[0.8778, 0.1052, 0.017]}`,highlighted:`curl https://api-inference.huggingface.co/models/facebook/bart-large-mnli \\
        -X POST \\
        -d <span class="hljs-string">&#x27;{&quot;inputs&quot;: &quot;Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!&quot;, &quot;parameters&quot;: {&quot;candidate_labels&quot;: [&quot;refund&quot;, &quot;legal&quot;, &quot;faq&quot;]}}&#x27;</span> \\
        -H <span class="hljs-string">&quot;Authorization: Bearer <span class="hljs-variable">\${HF_API_TOKEN}</span>&quot;</span>
<span class="hljs-comment"># {&quot;sequence&quot;:&quot;Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!&quot;,&quot;labels&quot;:[&quot;refund&quot;,&quot;faq&quot;,&quot;legal&quot;],&quot;scores&quot;:[0.8778, 0.1052, 0.017]}</span>`}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),c=!0},p:O,i(t){c||(b(n.$$.fragment,t),c=!0)},o(t){E(n.$$.fragment,t),c=!1},d(t){w(n,t)}}}function pK($){let n,c;return n=new P({props:{$$slots:{default:[fK]},$$scope:{ctx:$}}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),c=!0},p(t,d){const q={};d&2&&(q.$$scope={dirty:d,ctx:t}),n.$set(q)},i(t){c||(b(n.$$.fragment,t),c=!0)},o(t){E(n.$$.fragment,t),c=!1},d(t){w(n,t)}}}function hK($){let n,c;return n=new R({props:{code:`self.assertEqual(
    deep_round(data),
    {
        "sequence": "Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!",
        "labels": ["refund", "faq", "legal"],
        "scores": [
            # 88% refund
            0.8778,
            0.1052,
            0.017,
        ],
    },
)`,highlighted:`self.assertEqual(
    deep_round(data),
    {
        <span class="hljs-string">&quot;sequence&quot;</span>: <span class="hljs-string">&quot;Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!&quot;</span>,
        <span class="hljs-string">&quot;labels&quot;</span>: [<span class="hljs-string">&quot;refund&quot;</span>, <span class="hljs-string">&quot;faq&quot;</span>, <span class="hljs-string">&quot;legal&quot;</span>],
        <span class="hljs-string">&quot;scores&quot;</span>: [
            <span class="hljs-comment"># 88% refund</span>
            <span class="hljs-number">0.8778</span>,
            <span class="hljs-number">0.1052</span>,
            <span class="hljs-number">0.017</span>,
        ],
    },
)`}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),c=!0},p:O,i(t){c||(b(n.$$.fragment,t),c=!0)},o(t){E(n.$$.fragment,t),c=!1},d(t){w(n,t)}}}function dK($){let n,c;return n=new P({props:{$$slots:{default:[hK]},$$scope:{ctx:$}}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),c=!0},p(t,d){const q={};d&2&&(q.$$scope={dirty:d,ctx:t}),n.$set(q)},i(t){c||(b(n.$$.fragment,t),c=!0)},o(t){E(n.$$.fragment,t),c=!1},d(t){w(n,t)}}}function gK($){let n,c,t,d,q,k,A,j,T,N,D,ne,Re;return{c(){n=r("p"),c=r("strong"),t=i("Recommended model"),d=i(`:
`),q=r("a"),k=i("Helsinki-NLP/opus-mt-ru-en"),A=i(`.
Helsinki-NLP uploaded many models with many language pairs.
`),j=r("strong"),T=i("Recommended model"),N=i(": "),D=r("a"),ne=i("t5-base"),Re=i("."),this.h()},l(Q){n=o(Q,"P",{});var Y=l(n);c=o(Y,"STRONG",{});var ot=l(c);t=u(ot,"Recommended model"),ot.forEach(s),d=u(Y,`:
`),q=o(Y,"A",{href:!0,rel:!0});var Ri=l(q);k=u(Ri,"Helsinki-NLP/opus-mt-ru-en"),Ri.forEach(s),A=u(Y,`.
Helsinki-NLP uploaded many models with many language pairs.
`),j=o(Y,"STRONG",{});var Va=l(j);T=u(Va,"Recommended model"),Va.forEach(s),N=u(Y,": "),D=o(Y,"A",{href:!0,rel:!0});var Ne=l(D);ne=u(Ne,"t5-base"),Ne.forEach(s),Re=u(Y,"."),Y.forEach(s),this.h()},h(){h(q,"href","https://huggingface.co/Helsinki-NLP/opus-mt-ru-en"),h(q,"rel","nofollow"),h(D,"href","https://huggingface.co/t5-base"),h(D,"rel","nofollow")},m(Q,Y){m(Q,n,Y),e(n,c),e(c,t),e(n,d),e(n,q),e(q,k),e(n,A),e(n,j),e(j,T),e(n,N),e(n,D),e(D,ne),e(n,Re)},d(Q){Q&&s(n)}}}function mK($){let n,c;return n=new R({props:{code:`import json
import requests
headers = {"Authorization": f"Bearer {API_TOKEN}"}
API_URL = "https://api-inference.huggingface.co/models/Helsinki-NLP/opus-mt-ru-en"
def query(payload):
    data = json.dumps(payload)
    response = requests.request("POST", API_URL, headers=headers, data=data)
    return json.loads(response.content.decode("utf-8"))
data = query(
    {
        "inputs": "\u041C\u0435\u043D\u044F \u0437\u043E\u0432\u0443\u0442 \u0412\u043E\u043B\u044C\u0444\u0433\u0430\u043D\u0433 \u0438 \u044F \u0436\u0438\u0432\u0443 \u0432 \u0411\u0435\u0440\u043B\u0438\u043D\u0435",
    }
)
# Response
self.assertEqual(
    data,
    [
        {
            "translation_text": "My name is Wolfgang and I live in Berlin.",
        },
    ],
)`,highlighted:`<span class="hljs-keyword">import</span> json
<span class="hljs-keyword">import</span> requests
headers = {<span class="hljs-string">&quot;Authorization&quot;</span>: <span class="hljs-string">f&quot;Bearer <span class="hljs-subst">{API_TOKEN}</span>&quot;</span>}
API_URL = <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/Helsinki-NLP/opus-mt-ru-en&quot;</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">payload</span>):
    data = json.dumps(payload)
    response = requests.request(<span class="hljs-string">&quot;POST&quot;</span>, API_URL, headers=headers, data=data)
    <span class="hljs-keyword">return</span> json.loads(response.content.decode(<span class="hljs-string">&quot;utf-8&quot;</span>))
data = query(
    {
        <span class="hljs-string">&quot;inputs&quot;</span>: <span class="hljs-string">&quot;\u041C\u0435\u043D\u044F \u0437\u043E\u0432\u0443\u0442 \u0412\u043E\u043B\u044C\u0444\u0433\u0430\u043D\u0433 \u0438 \u044F \u0436\u0438\u0432\u0443 \u0432 \u0411\u0435\u0440\u043B\u0438\u043D\u0435&quot;</span>,
    }
)
<span class="hljs-comment"># Response</span>
self.assertEqual(
    data,
    [
        {
            <span class="hljs-string">&quot;translation_text&quot;</span>: <span class="hljs-string">&quot;My name is Wolfgang and I live in Berlin.&quot;</span>,
        },
    ],
)`}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),c=!0},p:O,i(t){c||(b(n.$$.fragment,t),c=!0)},o(t){E(n.$$.fragment,t),c=!1},d(t){w(n,t)}}}function qK($){let n,c;return n=new P({props:{$$slots:{default:[mK]},$$scope:{ctx:$}}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),c=!0},p(t,d){const q={};d&2&&(q.$$scope={dirty:d,ctx:t}),n.$set(q)},i(t){c||(b(n.$$.fragment,t),c=!0)},o(t){E(n.$$.fragment,t),c=!1},d(t){w(n,t)}}}function $K($){let n,c;return n=new R({props:{code:`import fetch from "node-fetch";
async function query(data) {
    const response = await fetch(
        "https://api-inference.huggingface.co/models/Helsinki-NLP/opus-mt-ru-en",
        {
            headers: { Authorization: \`Bearer \${API_TOKEN}\` },
            method: "POST",
            body: JSON.stringify(data),
        }
    );
    const result = await response.json();
    return result;
}
query({inputs: "\u041C\u0435\u043D\u044F \u0437\u043E\u0432\u0443\u0442 \u0412\u043E\u043B\u044C\u0444\u0433\u0430\u043D\u0433 \u0438 \u044F \u0436\u0438\u0432\u0443 \u0432 \u0411\u0435\u0440\u043B\u0438\u043D\u0435"}).then((response) => {
    console.log(JSON.stringify(response));
});
// [{"translation_text":"My name is Wolfgang and I live in Berlin."}]`,highlighted:`<span class="hljs-keyword">import</span> fetch <span class="hljs-keyword">from</span> <span class="hljs-string">&quot;node-fetch&quot;</span>;
<span class="hljs-keyword">async</span> <span class="hljs-keyword">function</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">data</span>) {
    <span class="hljs-keyword">const</span> response = <span class="hljs-keyword">await</span> <span class="hljs-title function_">fetch</span>(
        <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/Helsinki-NLP/opus-mt-ru-en&quot;</span>,
        {
            <span class="hljs-attr">headers</span>: { <span class="hljs-title class_">Authorization</span>: <span class="hljs-string">\`Bearer <span class="hljs-subst">\${API_TOKEN}</span>\`</span> },
            <span class="hljs-attr">method</span>: <span class="hljs-string">&quot;POST&quot;</span>,
            <span class="hljs-attr">body</span>: <span class="hljs-title class_">JSON</span>.<span class="hljs-title function_">stringify</span>(data),
        }
    );
    <span class="hljs-keyword">const</span> result = <span class="hljs-keyword">await</span> response.<span class="hljs-title function_">json</span>();
    <span class="hljs-keyword">return</span> result;
}
<span class="hljs-title function_">query</span>({<span class="hljs-attr">inputs</span>: <span class="hljs-string">&quot;\u041C\u0435\u043D\u044F \u0437\u043E\u0432\u0443\u0442 \u0412\u043E\u043B\u044C\u0444\u0433\u0430\u043D\u0433 \u0438 \u044F \u0436\u0438\u0432\u0443 \u0432 \u0411\u0435\u0440\u043B\u0438\u043D\u0435&quot;</span>}).<span class="hljs-title function_">then</span>(<span class="hljs-function">(<span class="hljs-params">response</span>) =&gt;</span> {
    <span class="hljs-variable language_">console</span>.<span class="hljs-title function_">log</span>(<span class="hljs-title class_">JSON</span>.<span class="hljs-title function_">stringify</span>(response));
});
<span class="hljs-comment">// [{&quot;translation_text&quot;:&quot;My name is Wolfgang and I live in Berlin.&quot;}]</span>`}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),c=!0},p:O,i(t){c||(b(n.$$.fragment,t),c=!0)},o(t){E(n.$$.fragment,t),c=!1},d(t){w(n,t)}}}function _K($){let n,c;return n=new P({props:{$$slots:{default:[$K]},$$scope:{ctx:$}}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),c=!0},p(t,d){const q={};d&2&&(q.$$scope={dirty:d,ctx:t}),n.$set(q)},i(t){c||(b(n.$$.fragment,t),c=!0)},o(t){E(n.$$.fragment,t),c=!1},d(t){w(n,t)}}}function vK($){let n,c;return n=new R({props:{code:`curl https://api-inference.huggingface.co/models/Helsinki-NLP/opus-mt-ru-en \\
        -X POST \\
        -d '{"inputs": "\u041C\u0435\u043D\u044F \u0437\u043E\u0432\u0443\u0442 \u0412\u043E\u043B\u044C\u0444\u0433\u0430\u043D\u0433 \u0438 \u044F \u0436\u0438\u0432\u0443 \u0432 \u0411\u0435\u0440\u043B\u0438\u043D\u0435"}' \\
        -H "Authorization: Bearer \${HF_API_TOKEN}"
# [{"translation_text":"My name is Wolfgang and I live in Berlin."}]`,highlighted:`curl https://api-inference.huggingface.co/models/Helsinki-NLP/opus-mt-ru-en \\
        -X POST \\
        -d <span class="hljs-string">&#x27;{&quot;inputs&quot;: &quot;\u041C\u0435\u043D\u044F \u0437\u043E\u0432\u0443\u0442 \u0412\u043E\u043B\u044C\u0444\u0433\u0430\u043D\u0433 \u0438 \u044F \u0436\u0438\u0432\u0443 \u0432 \u0411\u0435\u0440\u043B\u0438\u043D\u0435&quot;}&#x27;</span> \\
        -H <span class="hljs-string">&quot;Authorization: Bearer <span class="hljs-variable">\${HF_API_TOKEN}</span>&quot;</span>
<span class="hljs-comment"># [{&quot;translation_text&quot;:&quot;My name is Wolfgang and I live in Berlin.&quot;}]</span>`}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),c=!0},p:O,i(t){c||(b(n.$$.fragment,t),c=!0)},o(t){E(n.$$.fragment,t),c=!1},d(t){w(n,t)}}}function yK($){let n,c;return n=new P({props:{$$slots:{default:[vK]},$$scope:{ctx:$}}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),c=!0},p(t,d){const q={};d&2&&(q.$$scope={dirty:d,ctx:t}),n.$set(q)},i(t){c||(b(n.$$.fragment,t),c=!0)},o(t){E(n.$$.fragment,t),c=!1},d(t){w(n,t)}}}function bK($){let n,c,t,d,q,k,A;return{c(){n=r("p"),c=r("strong"),t=i("Recommended model"),d=i(`:
`),q=r("a"),k=i("facebook/bart-large-cnn"),A=i("."),this.h()},l(j){n=o(j,"P",{});var T=l(n);c=o(T,"STRONG",{});var N=l(c);t=u(N,"Recommended model"),N.forEach(s),d=u(T,`:
`),q=o(T,"A",{href:!0,rel:!0});var D=l(q);k=u(D,"facebook/bart-large-cnn"),D.forEach(s),A=u(T,"."),T.forEach(s),this.h()},h(){h(q,"href","https://huggingface.co/facebook/bart-large-cnn"),h(q,"rel","nofollow")},m(j,T){m(j,n,T),e(n,c),e(c,t),e(n,d),e(n,q),e(q,k),e(n,A)},d(j){j&&s(n)}}}function EK($){let n,c;return n=new R({props:{code:`import json
import requests
headers = {"Authorization": f"Bearer {API_TOKEN}"}
API_URL = "https://api-inference.huggingface.co/models/facebook/bart-large-cnn"
def query(payload):
    data = json.dumps(payload)
    response = requests.request("POST", API_URL, headers=headers, data=data)
    return json.loads(response.content.decode("utf-8"))
data = query(
    {
        "inputs": "The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world, a title it held for 41 years until the Chrysler Building in New York City was finished in 1930. It was the first structure to reach a height of 300 metres. Due to the addition of a broadcasting aerial at the top of the tower in 1957, it is now taller than the Chrysler Building by 5.2 metres (17 ft). Excluding transmitters, the Eiffel Tower is the second tallest free-standing structure in France after the Millau Viaduct.",
        "parameters": {"do_sample": False},
    }
)
# Response
self.assertEqual(
    data,
    [
        {
            "summary_text": "The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world.",
        },
    ],
)`,highlighted:`<span class="hljs-keyword">import</span> json
<span class="hljs-keyword">import</span> requests
headers = {<span class="hljs-string">&quot;Authorization&quot;</span>: <span class="hljs-string">f&quot;Bearer <span class="hljs-subst">{API_TOKEN}</span>&quot;</span>}
API_URL = <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/facebook/bart-large-cnn&quot;</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">payload</span>):
    data = json.dumps(payload)
    response = requests.request(<span class="hljs-string">&quot;POST&quot;</span>, API_URL, headers=headers, data=data)
    <span class="hljs-keyword">return</span> json.loads(response.content.decode(<span class="hljs-string">&quot;utf-8&quot;</span>))
data = query(
    {
        <span class="hljs-string">&quot;inputs&quot;</span>: <span class="hljs-string">&quot;The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world, a title it held for 41 years until the Chrysler Building in New York City was finished in 1930. It was the first structure to reach a height of 300 metres. Due to the addition of a broadcasting aerial at the top of the tower in 1957, it is now taller than the Chrysler Building by 5.2 metres (17 ft). Excluding transmitters, the Eiffel Tower is the second tallest free-standing structure in France after the Millau Viaduct.&quot;</span>,
        <span class="hljs-string">&quot;parameters&quot;</span>: {<span class="hljs-string">&quot;do_sample&quot;</span>: <span class="hljs-literal">False</span>},
    }
)
<span class="hljs-comment"># Response</span>
self.assertEqual(
    data,
    [
        {
            <span class="hljs-string">&quot;summary_text&quot;</span>: <span class="hljs-string">&quot;The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world.&quot;</span>,
        },
    ],
)`}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),c=!0},p:O,i(t){c||(b(n.$$.fragment,t),c=!0)},o(t){E(n.$$.fragment,t),c=!1},d(t){w(n,t)}}}function wK($){let n,c;return n=new P({props:{$$slots:{default:[EK]},$$scope:{ctx:$}}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),c=!0},p(t,d){const q={};d&2&&(q.$$scope={dirty:d,ctx:t}),n.$set(q)},i(t){c||(b(n.$$.fragment,t),c=!0)},o(t){E(n.$$.fragment,t),c=!1},d(t){w(n,t)}}}function TK($){let n,c;return n=new R({props:{code:`import fetch from "node-fetch";
async function query(data) {
    const response = await fetch(
        "https://api-inference.huggingface.co/models/facebook/bart-large-cnn",
        {
            headers: { Authorization: \`Bearer \${API_TOKEN}\` },
            method: "POST",
            body: JSON.stringify(data),
        }
    );
    const result = await response.json();
    return result;
}
query({inputs: "The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world, a title it held for 41 years until the Chrysler Building in New York City was finished in 1930. It was the first structure to reach a height of 300 metres. Due to the addition of a broadcasting aerial at the top of the tower in 1957, it is now taller than the Chrysler Building by 5.2 metres (17 ft). Excluding transmitters, the Eiffel Tower is the second tallest free-standing structure in France after the Millau Viaduct."}).then((response) => {
    console.log(JSON.stringify(response));
});
// [{"summary_text":"The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side. It was the first structure to reach a height of 300 metres."}]`,highlighted:`import fetch from <span class="hljs-comment">&quot;node-fetch&quot;</span>;
async function query(data) {
    const response = await fetch(
        <span class="hljs-comment">&quot;https://api-inference.huggingface.co/models/facebook/bart-large-cnn&quot;</span>,
        {
            headers: { <span class="hljs-type">Authorization</span>: \`<span class="hljs-type">Bearer</span> <span class="hljs-string">\${</span><span class="hljs-type">API_TOKEN</span>}\` },
            method: <span class="hljs-comment">&quot;POST&quot;</span>,
            body: <span class="hljs-type">JSON</span>.stringify(data),
        }
    );
    const result = await response.json();
    return result;
}
query({inputs: <span class="hljs-comment">&quot;The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world, a title it held for 41 years until the Chrysler Building in New York City was finished in 1930. It was the first structure to reach a height of 300 metres. Due to the addition of a broadcasting aerial at the top of the tower in 1957, it is now taller than the Chrysler Building by 5.2 metres (17 ft). Excluding transmitters, the Eiffel Tower is the second tallest free-standing structure in France after the Millau Viaduct.&quot;</span>}).then((response) =&gt; {
    console.log(<span class="hljs-type">JSON</span>.stringify(response));
});
// [{<span class="hljs-comment">&quot;summary_text&quot;</span>:<span class="hljs-comment">&quot;The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side. It was the first structure to reach a height of 300 metres.&quot;</span>}]`}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),c=!0},p:O,i(t){c||(b(n.$$.fragment,t),c=!0)},o(t){E(n.$$.fragment,t),c=!1},d(t){w(n,t)}}}function jK($){let n,c;return n=new P({props:{$$slots:{default:[TK]},$$scope:{ctx:$}}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),c=!0},p(t,d){const q={};d&2&&(q.$$scope={dirty:d,ctx:t}),n.$set(q)},i(t){c||(b(n.$$.fragment,t),c=!0)},o(t){E(n.$$.fragment,t),c=!1},d(t){w(n,t)}}}function kK($){let n,c;return n=new R({props:{code:`curl https://api-inference.huggingface.co/models/facebook/bart-large-cnn \\
        -X POST \\
        -d '{"inputs": "The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world, a title it held for 41 years until the Chrysler Building in New York City was finished in 1930. It was the first structure to reach a height of 300 metres. Due to the addition of a broadcasting aerial at the top of the tower in 1957, it is now taller than the Chrysler Building by 5.2 metres (17 ft). Excluding transmitters, the Eiffel Tower is the second tallest free-standing structure in France after the Millau Viaduct.", "parameters": {"do_sample": false}}' \\
        -H "Authorization: Bearer \${HF_API_TOKEN}"
# [{"summary_text":"The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world."}]`,highlighted:`curl https://api-inference.huggingface.co/models/facebook/bart-large-cnn \\
        -X POST \\
        -d <span class="hljs-string">&#x27;{&quot;inputs&quot;: &quot;The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world, a title it held for 41 years until the Chrysler Building in New York City was finished in 1930. It was the first structure to reach a height of 300 metres. Due to the addition of a broadcasting aerial at the top of the tower in 1957, it is now taller than the Chrysler Building by 5.2 metres (17 ft). Excluding transmitters, the Eiffel Tower is the second tallest free-standing structure in France after the Millau Viaduct.&quot;, &quot;parameters&quot;: {&quot;do_sample&quot;: false}}&#x27;</span> \\
        -H <span class="hljs-string">&quot;Authorization: Bearer <span class="hljs-variable">\${HF_API_TOKEN}</span>&quot;</span>
<span class="hljs-comment"># [{&quot;summary_text&quot;:&quot;The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world.&quot;}]</span>`}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),c=!0},p:O,i(t){c||(b(n.$$.fragment,t),c=!0)},o(t){E(n.$$.fragment,t),c=!1},d(t){w(n,t)}}}function AK($){let n,c;return n=new P({props:{$$slots:{default:[kK]},$$scope:{ctx:$}}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),c=!0},p(t,d){const q={};d&2&&(q.$$scope={dirty:d,ctx:t}),n.$set(q)},i(t){c||(b(n.$$.fragment,t),c=!0)},o(t){E(n.$$.fragment,t),c=!1},d(t){w(n,t)}}}function DK($){let n,c,t,d,q,k,A;return{c(){n=r("p"),c=r("strong"),t=i("Recommended model"),d=i(`:
`),q=r("a"),k=i("microsoft/DialoGPT-large"),A=i("."),this.h()},l(j){n=o(j,"P",{});var T=l(n);c=o(T,"STRONG",{});var N=l(c);t=u(N,"Recommended model"),N.forEach(s),d=u(T,`:
`),q=o(T,"A",{href:!0,rel:!0});var D=l(q);k=u(D,"microsoft/DialoGPT-large"),D.forEach(s),A=u(T,"."),T.forEach(s),this.h()},h(){h(q,"href","https://huggingface.co/microsoft/DialoGPT-large"),h(q,"rel","nofollow")},m(j,T){m(j,n,T),e(n,c),e(c,t),e(n,d),e(n,q),e(q,k),e(n,A)},d(j){j&&s(n)}}}function OK($){let n,c;return n=new R({props:{code:`import json
import requests
headers = {"Authorization": f"Bearer {API_TOKEN}"}
API_URL = "https://api-inference.huggingface.co/models/microsoft/DialoGPT-large"
def query(payload):
    data = json.dumps(payload)
    response = requests.request("POST", API_URL, headers=headers, data=data)
    return json.loads(response.content.decode("utf-8"))
data = query(
    {
        "inputs": {
            "past_user_inputs": ["Which movie is the best ?"],
            "generated_responses": ["It's Die Hard for sure."],
            "text": "Can you explain why ?",
        },
    }
)
# Response
self.assertEqual(
    data,
    {
        "generated_text": "It's the best movie ever.",
        "conversation": {
            "past_user_inputs": [
                "Which movie is the best ?",
                "Can you explain why ?",
            ],
            "generated_responses": [
                "It's Die Hard for sure.",
                "It's the best movie ever.",
            ],
        },
        "warnings": ["Setting \`pad_token_id\` to \`eos_token_id\`:50256 for open-end generation."],
    },
)`,highlighted:`<span class="hljs-keyword">import</span> json
<span class="hljs-keyword">import</span> requests
headers = {<span class="hljs-string">&quot;Authorization&quot;</span>: <span class="hljs-string">f&quot;Bearer <span class="hljs-subst">{API_TOKEN}</span>&quot;</span>}
API_URL = <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/microsoft/DialoGPT-large&quot;</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">payload</span>):
    data = json.dumps(payload)
    response = requests.request(<span class="hljs-string">&quot;POST&quot;</span>, API_URL, headers=headers, data=data)
    <span class="hljs-keyword">return</span> json.loads(response.content.decode(<span class="hljs-string">&quot;utf-8&quot;</span>))
data = query(
    {
        <span class="hljs-string">&quot;inputs&quot;</span>: {
            <span class="hljs-string">&quot;past_user_inputs&quot;</span>: [<span class="hljs-string">&quot;Which movie is the best ?&quot;</span>],
            <span class="hljs-string">&quot;generated_responses&quot;</span>: [<span class="hljs-string">&quot;It&#x27;s Die Hard for sure.&quot;</span>],
            <span class="hljs-string">&quot;text&quot;</span>: <span class="hljs-string">&quot;Can you explain why ?&quot;</span>,
        },
    }
)
<span class="hljs-comment"># Response</span>
self.assertEqual(
    data,
    {
        <span class="hljs-string">&quot;generated_text&quot;</span>: <span class="hljs-string">&quot;It&#x27;s the best movie ever.&quot;</span>,
        <span class="hljs-string">&quot;conversation&quot;</span>: {
            <span class="hljs-string">&quot;past_user_inputs&quot;</span>: [
                <span class="hljs-string">&quot;Which movie is the best ?&quot;</span>,
                <span class="hljs-string">&quot;Can you explain why ?&quot;</span>,
            ],
            <span class="hljs-string">&quot;generated_responses&quot;</span>: [
                <span class="hljs-string">&quot;It&#x27;s Die Hard for sure.&quot;</span>,
                <span class="hljs-string">&quot;It&#x27;s the best movie ever.&quot;</span>,
            ],
        },
        <span class="hljs-string">&quot;warnings&quot;</span>: [<span class="hljs-string">&quot;Setting \`pad_token_id\` to \`eos_token_id\`:50256 for open-end generation.&quot;</span>],
    },
)`}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),c=!0},p:O,i(t){c||(b(n.$$.fragment,t),c=!0)},o(t){E(n.$$.fragment,t),c=!1},d(t){w(n,t)}}}function PK($){let n,c;return n=new P({props:{$$slots:{default:[OK]},$$scope:{ctx:$}}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),c=!0},p(t,d){const q={};d&2&&(q.$$scope={dirty:d,ctx:t}),n.$set(q)},i(t){c||(b(n.$$.fragment,t),c=!0)},o(t){E(n.$$.fragment,t),c=!1},d(t){w(n,t)}}}function RK($){let n,c;return n=new R({props:{code:`import fetch from "node-fetch";
async function query(data) {
    const response = await fetch(
        "https://api-inference.huggingface.co/models/microsoft/DialoGPT-large",
        {
            headers: { Authorization: \`Bearer \${API_TOKEN}\` },
            method: "POST",
            body: JSON.stringify(data),
        }
    );
    const result = await response.json();
    return result;
}
query({inputs: {past_user_inputs: ["Which movie is the best ?"], generated_responses: ["It is Die Hard for sure."], text:"Can you explain why ?"}}).then((response) => {
    console.log(JSON.stringify(response));
});
// {"generated_text":"It's the best movie ever.","conversation":{"past_user_inputs":["Which movie is the best ?","Can you explain why ?"],"generated_responses":["It is Die Hard for sure.","It's the best movie ever."]},"warnings":["Setting \`pad_token_id\` to \`eos_token_id\`:50256 for open-end generation."]}`,highlighted:`<span class="hljs-keyword">import</span> <span class="hljs-keyword">fetch</span> <span class="hljs-keyword">from</span> &quot;node-fetch&quot;;
async <span class="hljs-keyword">function</span> query(data) {
    const response = await <span class="hljs-keyword">fetch</span>(
        &quot;https://api-inference.huggingface.co/models/microsoft/DialoGPT-large&quot;,
        {
            headers: { <span class="hljs-keyword">Authorization</span>: \`Bearer \${API_TOKEN}\` },
            <span class="hljs-keyword">method</span>: &quot;POST&quot;,
            body: <span class="hljs-type">JSON</span>.stringify(data),
        }
    );
    const result = await response.json();
    <span class="hljs-keyword">return</span> result;
}
query({inputs: {past_user_inputs: [&quot;Which movie is the best ?&quot;], generated_responses: [&quot;It is Die Hard for sure.&quot;], <span class="hljs-type">text</span>:&quot;Can you explain why ?&quot;}}).<span class="hljs-keyword">then</span>((response) =&gt; {
    console.log(<span class="hljs-type">JSON</span>.stringify(response));
});
// {&quot;generated_text&quot;:&quot;It&#x27;s the best movie ever.&quot;,&quot;conversation&quot;:{&quot;past_user_inputs&quot;:[&quot;Which movie is the best ?&quot;,&quot;Can you explain why ?&quot;],&quot;generated_responses&quot;:[&quot;It is Die Hard for sure.&quot;,&quot;It&#x27;s the best movie ever.&quot;]},&quot;warnings&quot;:[&quot;Setting \`pad_token_id\` to \`eos_token_id\`:50256 for open-end generation.&quot;]}`}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),c=!0},p:O,i(t){c||(b(n.$$.fragment,t),c=!0)},o(t){E(n.$$.fragment,t),c=!1},d(t){w(n,t)}}}function NK($){let n,c;return n=new P({props:{$$slots:{default:[RK]},$$scope:{ctx:$}}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),c=!0},p(t,d){const q={};d&2&&(q.$$scope={dirty:d,ctx:t}),n.$set(q)},i(t){c||(b(n.$$.fragment,t),c=!0)},o(t){E(n.$$.fragment,t),c=!1},d(t){w(n,t)}}}function SK($){let n,c;return n=new R({props:{code:`curl https://api-inference.huggingface.co/models/microsoft/DialoGPT-large \\
        -X POST \\
        -d '{"inputs": {"past_user_inputs": ["Which movie is the best ?"], "generated_responses": ["It is Die Hard for sure."], "text":"Can you explain why ?"}}' \\
        -H "Authorization: Bearer \${HF_API_TOKEN}"
# {"generated_text":"It's the best movie ever.","conversation":{"past_user_inputs":["Which movie is the best ?","Can you explain why ?"],"generated_responses":["It is Die Hard for sure.","It's the best movie ever."]},"warnings":["Setting \`pad_token_id\` to \`eos_token_id\`:50256 for open-end generation."]}`,highlighted:'curl https://api-inference.huggingface.co/models/microsoft/DialoGPT-large \\\n        -X POST \\\n        -d <span class="hljs-string">&#x27;{&quot;inputs&quot;: {&quot;past_user_inputs&quot;: [&quot;Which movie is the best ?&quot;], &quot;generated_responses&quot;: [&quot;It is Die Hard for sure.&quot;], &quot;text&quot;:&quot;Can you explain why ?&quot;}}&#x27;</span> \\\n        -H <span class="hljs-string">&quot;Authorization: Bearer <span class="hljs-variable">${HF_API_TOKEN}</span>&quot;</span>\n<span class="hljs-comment"># {&quot;generated_text&quot;:&quot;It&#x27;s the best movie ever.&quot;,&quot;conversation&quot;:{&quot;past_user_inputs&quot;:[&quot;Which movie is the best ?&quot;,&quot;Can you explain why ?&quot;],&quot;generated_responses&quot;:[&quot;It is Die Hard for sure.&quot;,&quot;It&#x27;s the best movie ever.&quot;]},&quot;warnings&quot;:[&quot;Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.&quot;]}</span>'}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),c=!0},p:O,i(t){c||(b(n.$$.fragment,t),c=!0)},o(t){E(n.$$.fragment,t),c=!1},d(t){w(n,t)}}}function xK($){let n,c;return n=new P({props:{$$slots:{default:[SK]},$$scope:{ctx:$}}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),c=!0},p(t,d){const q={};d&2&&(q.$$scope={dirty:d,ctx:t}),n.$set(q)},i(t){c||(b(n.$$.fragment,t),c=!0)},o(t){E(n.$$.fragment,t),c=!1},d(t){w(n,t)}}}function IK($){let n,c,t,d,q,k,A;return{c(){n=r("p"),c=r("strong"),t=i("Recommended model"),d=i(`:
`),q=r("a"),k=i("google/tapas-base-finetuned-wtq"),A=i("."),this.h()},l(j){n=o(j,"P",{});var T=l(n);c=o(T,"STRONG",{});var N=l(c);t=u(N,"Recommended model"),N.forEach(s),d=u(T,`:
`),q=o(T,"A",{href:!0,rel:!0});var D=l(q);k=u(D,"google/tapas-base-finetuned-wtq"),D.forEach(s),A=u(T,"."),T.forEach(s),this.h()},h(){h(q,"href","https://huggingface.co/google/tapas-base-finetuned-wtq"),h(q,"rel","nofollow")},m(j,T){m(j,n,T),e(n,c),e(c,t),e(n,d),e(n,q),e(q,k),e(n,A)},d(j){j&&s(n)}}}function HK($){let n,c;return n=new R({props:{code:`import json
import requests
headers = {"Authorization": f"Bearer {API_TOKEN}"}
API_URL = "https://api-inference.huggingface.co/models/google/tapas-base-finetuned-wtq"
def query(payload):
    data = json.dumps(payload)
    response = requests.request("POST", API_URL, headers=headers, data=data)
    return json.loads(response.content.decode("utf-8"))
data = query(
    {
        "inputs": {
            "query": "How many stars does the transformers repository have?",
            "table": {
                "Repository": ["Transformers", "Datasets", "Tokenizers"],
                "Stars": ["36542", "4512", "3934"],
                "Contributors": ["651", "77", "34"],
                "Programming language": [
                    "Python",
                    "Python",
                    "Rust, Python and NodeJS",
                ],
            },
        }
    }
)`,highlighted:`<span class="hljs-keyword">import</span> json
<span class="hljs-keyword">import</span> requests
headers = {<span class="hljs-string">&quot;Authorization&quot;</span>: <span class="hljs-string">f&quot;Bearer <span class="hljs-subst">{API_TOKEN}</span>&quot;</span>}
API_URL = <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/google/tapas-base-finetuned-wtq&quot;</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">payload</span>):
    data = json.dumps(payload)
    response = requests.request(<span class="hljs-string">&quot;POST&quot;</span>, API_URL, headers=headers, data=data)
    <span class="hljs-keyword">return</span> json.loads(response.content.decode(<span class="hljs-string">&quot;utf-8&quot;</span>))
data = query(
    {
        <span class="hljs-string">&quot;inputs&quot;</span>: {
            <span class="hljs-string">&quot;query&quot;</span>: <span class="hljs-string">&quot;How many stars does the transformers repository have?&quot;</span>,
            <span class="hljs-string">&quot;table&quot;</span>: {
                <span class="hljs-string">&quot;Repository&quot;</span>: [<span class="hljs-string">&quot;Transformers&quot;</span>, <span class="hljs-string">&quot;Datasets&quot;</span>, <span class="hljs-string">&quot;Tokenizers&quot;</span>],
                <span class="hljs-string">&quot;Stars&quot;</span>: [<span class="hljs-string">&quot;36542&quot;</span>, <span class="hljs-string">&quot;4512&quot;</span>, <span class="hljs-string">&quot;3934&quot;</span>],
                <span class="hljs-string">&quot;Contributors&quot;</span>: [<span class="hljs-string">&quot;651&quot;</span>, <span class="hljs-string">&quot;77&quot;</span>, <span class="hljs-string">&quot;34&quot;</span>],
                <span class="hljs-string">&quot;Programming language&quot;</span>: [
                    <span class="hljs-string">&quot;Python&quot;</span>,
                    <span class="hljs-string">&quot;Python&quot;</span>,
                    <span class="hljs-string">&quot;Rust, Python and NodeJS&quot;</span>,
                ],
            },
        }
    }
)`}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),c=!0},p:O,i(t){c||(b(n.$$.fragment,t),c=!0)},o(t){E(n.$$.fragment,t),c=!1},d(t){w(n,t)}}}function BK($){let n,c;return n=new P({props:{$$slots:{default:[HK]},$$scope:{ctx:$}}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),c=!0},p(t,d){const q={};d&2&&(q.$$scope={dirty:d,ctx:t}),n.$set(q)},i(t){c||(b(n.$$.fragment,t),c=!0)},o(t){E(n.$$.fragment,t),c=!1},d(t){w(n,t)}}}function CK($){let n,c;return n=new R({props:{code:`import fetch from "node-fetch";
async function query(data) {
    const response = await fetch(
        "https://api-inference.huggingface.co/models/google/tapas-base-finetuned-wtq",
        {
            headers: { Authorization: \`Bearer \${API_TOKEN}\` },
            method: "POST",
            body: JSON.stringify(data),
        }
    );
    const result = await response.json();
    return result;
}
query({inputs:{query:"How many stars does the transformers repository have?",table:{Repository:["Transformers","Datasets","Tokenizers"],Stars:["36542","4512","3934"],Contributors:["651","77","34"],"Programming language":["Python","Python","Rust, Python and NodeJS"]}}}).then((response) => {
    console.log(JSON.stringify(response));
});
// {"answer":"AVERAGE > 36542","coordinates":[[0,1]],"cells":["36542"],"aggregator":"AVERAGE"}`,highlighted:`<span class="hljs-keyword">import</span> <span class="hljs-keyword">fetch</span> <span class="hljs-keyword">from</span> &quot;node-fetch&quot;;
async <span class="hljs-keyword">function</span> query(data) {
    const response = await <span class="hljs-keyword">fetch</span>(
        &quot;https://api-inference.huggingface.co/models/google/tapas-base-finetuned-wtq&quot;,
        {
            headers: { <span class="hljs-keyword">Authorization</span>: \`Bearer \${API_TOKEN}\` },
            <span class="hljs-keyword">method</span>: &quot;POST&quot;,
            body: <span class="hljs-type">JSON</span>.stringify(data),
        }
    );
    const result = await response.json();
    <span class="hljs-keyword">return</span> result;
}
query({inputs:{query:&quot;How many stars does the transformers repository have?&quot;,<span class="hljs-keyword">table</span>:{Repository:[&quot;Transformers&quot;,&quot;Datasets&quot;,&quot;Tokenizers&quot;],Stars:[&quot;36542&quot;,&quot;4512&quot;,&quot;3934&quot;],Contributors:[&quot;651&quot;,&quot;77&quot;,&quot;34&quot;],&quot;Programming language&quot;:[&quot;Python&quot;,&quot;Python&quot;,&quot;Rust, Python and NodeJS&quot;]}}}).<span class="hljs-keyword">then</span>((response) =&gt; {
    console.log(<span class="hljs-type">JSON</span>.stringify(response));
});
// {&quot;answer&quot;:&quot;AVERAGE &gt; 36542&quot;,&quot;coordinates&quot;:[[<span class="hljs-number">0</span>,<span class="hljs-number">1</span>]],&quot;cells&quot;:[&quot;36542&quot;],&quot;aggregator&quot;:&quot;AVERAGE&quot;}`}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),c=!0},p:O,i(t){c||(b(n.$$.fragment,t),c=!0)},o(t){E(n.$$.fragment,t),c=!1},d(t){w(n,t)}}}function GK($){let n,c;return n=new P({props:{$$slots:{default:[CK]},$$scope:{ctx:$}}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),c=!0},p(t,d){const q={};d&2&&(q.$$scope={dirty:d,ctx:t}),n.$set(q)},i(t){c||(b(n.$$.fragment,t),c=!0)},o(t){E(n.$$.fragment,t),c=!1},d(t){w(n,t)}}}function LK($){let n,c;return n=new R({props:{code:`curl https://api-inference.huggingface.co/models/google/tapas-base-finetuned-wtq \\
        -X POST \\
        -d '{"inputs":{"query":"How many stars does the transformers repository have?","table":{"Repository":["Transformers","Datasets","Tokenizers"],"Stars":["36542","4512","3934"],"Contributors":["651","77","34"],"Programming language":["Python","Python","Rust, Python and NodeJS"]}}}' \\
        -H "Authorization: Bearer \${HF_API_TOKEN}"
# {"answer":"AVERAGE > 36542","coordinates":[[0,1]],"cells":["36542"],"aggregator":"AVERAGE"}`,highlighted:`curl https://api-inference.huggingface.co/models/google/tapas-base-finetuned-wtq \\
        -X POST \\
        -d <span class="hljs-string">&#x27;{&quot;inputs&quot;:{&quot;query&quot;:&quot;How many stars does the transformers repository have?&quot;,&quot;table&quot;:{&quot;Repository&quot;:[&quot;Transformers&quot;,&quot;Datasets&quot;,&quot;Tokenizers&quot;],&quot;Stars&quot;:[&quot;36542&quot;,&quot;4512&quot;,&quot;3934&quot;],&quot;Contributors&quot;:[&quot;651&quot;,&quot;77&quot;,&quot;34&quot;],&quot;Programming language&quot;:[&quot;Python&quot;,&quot;Python&quot;,&quot;Rust, Python and NodeJS&quot;]}}}&#x27;</span> \\
        -H <span class="hljs-string">&quot;Authorization: Bearer <span class="hljs-variable">\${HF_API_TOKEN}</span>&quot;</span>
<span class="hljs-comment"># {&quot;answer&quot;:&quot;AVERAGE &gt; 36542&quot;,&quot;coordinates&quot;:[[0,1]],&quot;cells&quot;:[&quot;36542&quot;],&quot;aggregator&quot;:&quot;AVERAGE&quot;}</span>`}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),c=!0},p:O,i(t){c||(b(n.$$.fragment,t),c=!0)},o(t){E(n.$$.fragment,t),c=!1},d(t){w(n,t)}}}function UK($){let n,c;return n=new P({props:{$$slots:{default:[LK]},$$scope:{ctx:$}}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),c=!0},p(t,d){const q={};d&2&&(q.$$scope={dirty:d,ctx:t}),n.$set(q)},i(t){c||(b(n.$$.fragment,t),c=!0)},o(t){E(n.$$.fragment,t),c=!1},d(t){w(n,t)}}}function zK($){let n,c;return n=new R({props:{code:`self.assertEqual(
    data,
    {
        "answer": "AVERAGE > 36542",
        "coordinates": [[0, 1]],
        "cells": ["36542"],
        "aggregator": "AVERAGE",
    },
)`,highlighted:`self.assertEqual(
    data,
    {
        <span class="hljs-string">&quot;answer&quot;</span>: <span class="hljs-string">&quot;AVERAGE &gt; 36542&quot;</span>,
        <span class="hljs-string">&quot;coordinates&quot;</span>: [[<span class="hljs-number">0</span>, <span class="hljs-number">1</span>]],
        <span class="hljs-string">&quot;cells&quot;</span>: [<span class="hljs-string">&quot;36542&quot;</span>],
        <span class="hljs-string">&quot;aggregator&quot;</span>: <span class="hljs-string">&quot;AVERAGE&quot;</span>,
    },
)`}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),c=!0},p:O,i(t){c||(b(n.$$.fragment,t),c=!0)},o(t){E(n.$$.fragment,t),c=!1},d(t){w(n,t)}}}function MK($){let n,c;return n=new P({props:{$$slots:{default:[zK]},$$scope:{ctx:$}}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),c=!0},p(t,d){const q={};d&2&&(q.$$scope={dirty:d,ctx:t}),n.$set(q)},i(t){c||(b(n.$$.fragment,t),c=!0)},o(t){E(n.$$.fragment,t),c=!1},d(t){w(n,t)}}}function FK($){let n,c,t,d,q,k,A;return{c(){n=r("p"),c=r("strong"),t=i("Recommended model"),d=i(`:
`),q=r("a"),k=i("deepset/roberta-base-squad2"),A=i("."),this.h()},l(j){n=o(j,"P",{});var T=l(n);c=o(T,"STRONG",{});var N=l(c);t=u(N,"Recommended model"),N.forEach(s),d=u(T,`:
`),q=o(T,"A",{href:!0,rel:!0});var D=l(q);k=u(D,"deepset/roberta-base-squad2"),D.forEach(s),A=u(T,"."),T.forEach(s),this.h()},h(){h(q,"href","https://huggingface.co/deepset/roberta-base-squad2"),h(q,"rel","nofollow")},m(j,T){m(j,n,T),e(n,c),e(c,t),e(n,d),e(n,q),e(q,k),e(n,A)},d(j){j&&s(n)}}}function KK($){let n,c;return n=new R({props:{code:`import json
import requests
headers = {"Authorization": f"Bearer {API_TOKEN}"}
API_URL = "https://api-inference.huggingface.co/models/deepset/roberta-base-squad2"
def query(payload):
    data = json.dumps(payload)
    response = requests.request("POST", API_URL, headers=headers, data=data)
    return json.loads(response.content.decode("utf-8"))
data = query(
    {
        "inputs": {
            "question": "What's my name?",
            "context": "My name is Clara and I live in Berkeley.",
        }
    }
)`,highlighted:`<span class="hljs-keyword">import</span> json
<span class="hljs-keyword">import</span> requests
headers = {<span class="hljs-string">&quot;Authorization&quot;</span>: <span class="hljs-string">f&quot;Bearer <span class="hljs-subst">{API_TOKEN}</span>&quot;</span>}
API_URL = <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/deepset/roberta-base-squad2&quot;</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">payload</span>):
    data = json.dumps(payload)
    response = requests.request(<span class="hljs-string">&quot;POST&quot;</span>, API_URL, headers=headers, data=data)
    <span class="hljs-keyword">return</span> json.loads(response.content.decode(<span class="hljs-string">&quot;utf-8&quot;</span>))
data = query(
    {
        <span class="hljs-string">&quot;inputs&quot;</span>: {
            <span class="hljs-string">&quot;question&quot;</span>: <span class="hljs-string">&quot;What&#x27;s my name?&quot;</span>,
            <span class="hljs-string">&quot;context&quot;</span>: <span class="hljs-string">&quot;My name is Clara and I live in Berkeley.&quot;</span>,
        }
    }
)`}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),c=!0},p:O,i(t){c||(b(n.$$.fragment,t),c=!0)},o(t){E(n.$$.fragment,t),c=!1},d(t){w(n,t)}}}function JK($){let n,c;return n=new P({props:{$$slots:{default:[KK]},$$scope:{ctx:$}}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),c=!0},p(t,d){const q={};d&2&&(q.$$scope={dirty:d,ctx:t}),n.$set(q)},i(t){c||(b(n.$$.fragment,t),c=!0)},o(t){E(n.$$.fragment,t),c=!1},d(t){w(n,t)}}}function WK($){let n,c;return n=new R({props:{code:`import fetch from "node-fetch";
async function query(data) {
    const response = await fetch(
        "https://api-inference.huggingface.co/models/deepset/roberta-base-squad2",
        {
            headers: { Authorization: \`Bearer \${API_TOKEN}\` },
            method: "POST",
            body: JSON.stringify(data),
        }
    );
    const result = await response.json();
    return result;
}
query({inputs:{question:"What is my name?",context:"My name is Clara and I live in Berkeley."}}).then((response) => {
    console.log(JSON.stringify(response));
});
// {"score":0.933128833770752,"start":11,"end":16,"answer":"Clara"}`,highlighted:`<span class="hljs-keyword">import</span> fetch <span class="hljs-keyword">from</span> <span class="hljs-string">&quot;node-fetch&quot;</span>;
<span class="hljs-keyword">async</span> <span class="hljs-keyword">function</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">data</span>) {
    <span class="hljs-keyword">const</span> response = <span class="hljs-keyword">await</span> <span class="hljs-title function_">fetch</span>(
        <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/deepset/roberta-base-squad2&quot;</span>,
        {
            <span class="hljs-attr">headers</span>: { <span class="hljs-title class_">Authorization</span>: <span class="hljs-string">\`Bearer <span class="hljs-subst">\${API_TOKEN}</span>\`</span> },
            <span class="hljs-attr">method</span>: <span class="hljs-string">&quot;POST&quot;</span>,
            <span class="hljs-attr">body</span>: <span class="hljs-title class_">JSON</span>.<span class="hljs-title function_">stringify</span>(data),
        }
    );
    <span class="hljs-keyword">const</span> result = <span class="hljs-keyword">await</span> response.<span class="hljs-title function_">json</span>();
    <span class="hljs-keyword">return</span> result;
}
<span class="hljs-title function_">query</span>({<span class="hljs-attr">inputs</span>:{<span class="hljs-attr">question</span>:<span class="hljs-string">&quot;What is my name?&quot;</span>,<span class="hljs-attr">context</span>:<span class="hljs-string">&quot;My name is Clara and I live in Berkeley.&quot;</span>}}).<span class="hljs-title function_">then</span>(<span class="hljs-function">(<span class="hljs-params">response</span>) =&gt;</span> {
    <span class="hljs-variable language_">console</span>.<span class="hljs-title function_">log</span>(<span class="hljs-title class_">JSON</span>.<span class="hljs-title function_">stringify</span>(response));
});
<span class="hljs-comment">// {&quot;score&quot;:0.933128833770752,&quot;start&quot;:11,&quot;end&quot;:16,&quot;answer&quot;:&quot;Clara&quot;}</span>`}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),c=!0},p:O,i(t){c||(b(n.$$.fragment,t),c=!0)},o(t){E(n.$$.fragment,t),c=!1},d(t){w(n,t)}}}function YK($){let n,c;return n=new P({props:{$$slots:{default:[WK]},$$scope:{ctx:$}}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),c=!0},p(t,d){const q={};d&2&&(q.$$scope={dirty:d,ctx:t}),n.$set(q)},i(t){c||(b(n.$$.fragment,t),c=!0)},o(t){E(n.$$.fragment,t),c=!1},d(t){w(n,t)}}}function VK($){let n,c;return n=new R({props:{code:`curl https://api-inference.huggingface.co/models/deepset/roberta-base-squad2 \\
        -X POST \\
        -d '{"inputs":{"question":"What is my name?","context":"My name is Clara and I live in Berkeley."}}' \\
        -H "Authorization: Bearer \${HF_API_TOKEN}"
# {"score":0.933128833770752,"start":11,"end":16,"answer":"Clara"}`,highlighted:`curl https://api-inference.huggingface.co/models/deepset/roberta-base-squad2 \\
        -X POST \\
        -d <span class="hljs-string">&#x27;{&quot;inputs&quot;:{&quot;question&quot;:&quot;What is my name?&quot;,&quot;context&quot;:&quot;My name is Clara and I live in Berkeley.&quot;}}&#x27;</span> \\
        -H <span class="hljs-string">&quot;Authorization: Bearer <span class="hljs-variable">\${HF_API_TOKEN}</span>&quot;</span>
<span class="hljs-comment"># {&quot;score&quot;:0.933128833770752,&quot;start&quot;:11,&quot;end&quot;:16,&quot;answer&quot;:&quot;Clara&quot;}</span>`}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),c=!0},p:O,i(t){c||(b(n.$$.fragment,t),c=!0)},o(t){E(n.$$.fragment,t),c=!1},d(t){w(n,t)}}}function XK($){let n,c;return n=new P({props:{$$slots:{default:[VK]},$$scope:{ctx:$}}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),c=!0},p(t,d){const q={};d&2&&(q.$$scope={dirty:d,ctx:t}),n.$set(q)},i(t){c||(b(n.$$.fragment,t),c=!0)},o(t){E(n.$$.fragment,t),c=!1},d(t){w(n,t)}}}function QK($){let n,c;return n=new R({props:{code:`self.assertEqual(
    deep_round(data),
    {"score": 0.9327, "start": 11, "end": 16, "answer": "Clara"},
)`,highlighted:`self.assertEqual(
    deep_round(data),
    {<span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.9327</span>, <span class="hljs-string">&quot;start&quot;</span>: <span class="hljs-number">11</span>, <span class="hljs-string">&quot;end&quot;</span>: <span class="hljs-number">16</span>, <span class="hljs-string">&quot;answer&quot;</span>: <span class="hljs-string">&quot;Clara&quot;</span>},
)`}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),c=!0},p:O,i(t){c||(b(n.$$.fragment,t),c=!0)},o(t){E(n.$$.fragment,t),c=!1},d(t){w(n,t)}}}function ZK($){let n,c;return n=new P({props:{$$slots:{default:[QK]},$$scope:{ctx:$}}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),c=!0},p(t,d){const q={};d&2&&(q.$$scope={dirty:d,ctx:t}),n.$set(q)},i(t){c||(b(n.$$.fragment,t),c=!0)},o(t){E(n.$$.fragment,t),c=!1},d(t){w(n,t)}}}function eJ($){let n,c,t,d,q,k;return{c(){n=r("p"),c=r("strong"),t=i("Recommended model"),d=i(`:
`),q=r("a"),k=i("distilbert-base-uncased-finetuned-sst-2-english"),this.h()},l(A){n=o(A,"P",{});var j=l(n);c=o(j,"STRONG",{});var T=l(c);t=u(T,"Recommended model"),T.forEach(s),d=u(j,`:
`),q=o(j,"A",{href:!0,rel:!0});var N=l(q);k=u(N,"distilbert-base-uncased-finetuned-sst-2-english"),N.forEach(s),j.forEach(s),this.h()},h(){h(q,"href","https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english"),h(q,"rel","nofollow")},m(A,j){m(A,n,j),e(n,c),e(c,t),e(n,d),e(n,q),e(q,k)},d(A){A&&s(n)}}}function tJ($){let n,c;return n=new R({props:{code:`import json
import requests
headers = {"Authorization": f"Bearer {API_TOKEN}"}
API_URL = "https://api-inference.huggingface.co/models/distilbert-base-uncased-finetuned-sst-2-english"
def query(payload):
    data = json.dumps(payload)
    response = requests.request("POST", API_URL, headers=headers, data=data)
    return json.loads(response.content.decode("utf-8"))
data = query({"inputs": "I like you. I love you"})`,highlighted:`<span class="hljs-keyword">import</span> json
<span class="hljs-keyword">import</span> requests
headers = {<span class="hljs-string">&quot;Authorization&quot;</span>: <span class="hljs-string">f&quot;Bearer <span class="hljs-subst">{API_TOKEN}</span>&quot;</span>}
API_URL = <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/distilbert-base-uncased-finetuned-sst-2-english&quot;</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">payload</span>):
    data = json.dumps(payload)
    response = requests.request(<span class="hljs-string">&quot;POST&quot;</span>, API_URL, headers=headers, data=data)
    <span class="hljs-keyword">return</span> json.loads(response.content.decode(<span class="hljs-string">&quot;utf-8&quot;</span>))
data = query({<span class="hljs-string">&quot;inputs&quot;</span>: <span class="hljs-string">&quot;I like you. I love you&quot;</span>})`}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),c=!0},p:O,i(t){c||(b(n.$$.fragment,t),c=!0)},o(t){E(n.$$.fragment,t),c=!1},d(t){w(n,t)}}}function sJ($){let n,c;return n=new P({props:{$$slots:{default:[tJ]},$$scope:{ctx:$}}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),c=!0},p(t,d){const q={};d&2&&(q.$$scope={dirty:d,ctx:t}),n.$set(q)},i(t){c||(b(n.$$.fragment,t),c=!0)},o(t){E(n.$$.fragment,t),c=!1},d(t){w(n,t)}}}function aJ($){let n,c;return n=new R({props:{code:`import fetch from "node-fetch";
async function query(data) {
    const response = await fetch(
        "https://api-inference.huggingface.co/models/distilbert-base-uncased-finetuned-sst-2-english",
        {
            headers: { Authorization: \`Bearer \${API_TOKEN}\` },
            method: "POST",
            body: JSON.stringify(data),
        }
    );
    const result = await response.json();
    return result;
}
query({inputs:"I like you. I love you"}).then((response) => {
    console.log(JSON.stringify(response));
});
// [[{"label":"NEGATIVE","score":0.0001261125144083053},{"label":"POSITIVE","score":0.9998738765716553}]]`,highlighted:`<span class="hljs-keyword">import</span> fetch <span class="hljs-keyword">from</span> <span class="hljs-string">&quot;node-fetch&quot;</span>;
<span class="hljs-keyword">async</span> <span class="hljs-keyword">function</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">data</span>) {
    <span class="hljs-keyword">const</span> response = <span class="hljs-keyword">await</span> <span class="hljs-title function_">fetch</span>(
        <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/distilbert-base-uncased-finetuned-sst-2-english&quot;</span>,
        {
            <span class="hljs-attr">headers</span>: { <span class="hljs-title class_">Authorization</span>: <span class="hljs-string">\`Bearer <span class="hljs-subst">\${API_TOKEN}</span>\`</span> },
            <span class="hljs-attr">method</span>: <span class="hljs-string">&quot;POST&quot;</span>,
            <span class="hljs-attr">body</span>: <span class="hljs-title class_">JSON</span>.<span class="hljs-title function_">stringify</span>(data),
        }
    );
    <span class="hljs-keyword">const</span> result = <span class="hljs-keyword">await</span> response.<span class="hljs-title function_">json</span>();
    <span class="hljs-keyword">return</span> result;
}
<span class="hljs-title function_">query</span>({<span class="hljs-attr">inputs</span>:<span class="hljs-string">&quot;I like you. I love you&quot;</span>}).<span class="hljs-title function_">then</span>(<span class="hljs-function">(<span class="hljs-params">response</span>) =&gt;</span> {
    <span class="hljs-variable language_">console</span>.<span class="hljs-title function_">log</span>(<span class="hljs-title class_">JSON</span>.<span class="hljs-title function_">stringify</span>(response));
});
<span class="hljs-comment">// [[{&quot;label&quot;:&quot;NEGATIVE&quot;,&quot;score&quot;:0.0001261125144083053},{&quot;label&quot;:&quot;POSITIVE&quot;,&quot;score&quot;:0.9998738765716553}]]</span>`}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),c=!0},p:O,i(t){c||(b(n.$$.fragment,t),c=!0)},o(t){E(n.$$.fragment,t),c=!1},d(t){w(n,t)}}}function nJ($){let n,c;return n=new P({props:{$$slots:{default:[aJ]},$$scope:{ctx:$}}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),c=!0},p(t,d){const q={};d&2&&(q.$$scope={dirty:d,ctx:t}),n.$set(q)},i(t){c||(b(n.$$.fragment,t),c=!0)},o(t){E(n.$$.fragment,t),c=!1},d(t){w(n,t)}}}function rJ($){let n,c;return n=new R({props:{code:`curl https://api-inference.huggingface.co/models/distilbert-base-uncased-finetuned-sst-2-english \\
        -X POST \\
        -d '{"inputs":"I like you. I love you"}' \\
        -H "Authorization: Bearer \${HF_API_TOKEN}"
# [[{"label":"NEGATIVE","score":0.0001261125144083053},{"label":"POSITIVE","score":0.9998738765716553}]]`,highlighted:`curl https://api-inference.huggingface.co/models/distilbert-base-uncased-finetuned-sst-2-english \\
        -X POST \\
        -d <span class="hljs-string">&#x27;{&quot;inputs&quot;:&quot;I like you. I love you&quot;}&#x27;</span> \\
        -H <span class="hljs-string">&quot;Authorization: Bearer <span class="hljs-variable">\${HF_API_TOKEN}</span>&quot;</span>
<span class="hljs-comment"># [[{&quot;label&quot;:&quot;NEGATIVE&quot;,&quot;score&quot;:0.0001261125144083053},{&quot;label&quot;:&quot;POSITIVE&quot;,&quot;score&quot;:0.9998738765716553}]]</span>`}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),c=!0},p:O,i(t){c||(b(n.$$.fragment,t),c=!0)},o(t){E(n.$$.fragment,t),c=!1},d(t){w(n,t)}}}function oJ($){let n,c;return n=new P({props:{$$slots:{default:[rJ]},$$scope:{ctx:$}}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),c=!0},p(t,d){const q={};d&2&&(q.$$scope={dirty:d,ctx:t}),n.$set(q)},i(t){c||(b(n.$$.fragment,t),c=!0)},o(t){E(n.$$.fragment,t),c=!1},d(t){w(n,t)}}}function lJ($){let n,c;return n=new R({props:{code:`self.assertEqual(
    deep_round(data),
    [
        [
            {"label": "NEGATIVE", "score": 0.0001},
            {"label": "POSITIVE", "score": 0.9999},
        ]
    ],
)`,highlighted:`self.assertEqual(
    deep_round(data),
    [
        [
            {<span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;NEGATIVE&quot;</span>, <span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.0001</span>},
            {<span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;POSITIVE&quot;</span>, <span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.9999</span>},
        ]
    ],
)`}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),c=!0},p:O,i(t){c||(b(n.$$.fragment,t),c=!0)},o(t){E(n.$$.fragment,t),c=!1},d(t){w(n,t)}}}function iJ($){let n,c;return n=new P({props:{$$slots:{default:[lJ]},$$scope:{ctx:$}}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),c=!0},p(t,d){const q={};d&2&&(q.$$scope={dirty:d,ctx:t}),n.$set(q)},i(t){c||(b(n.$$.fragment,t),c=!0)},o(t){E(n.$$.fragment,t),c=!1},d(t){w(n,t)}}}function uJ($){let n,c,t,d,q,k;return{c(){n=r("p"),c=r("strong"),t=i("Recommended model"),d=i(`:
`),q=r("a"),k=i("dbmdz/bert-large-cased-finetuned-conll03-english"),this.h()},l(A){n=o(A,"P",{});var j=l(n);c=o(j,"STRONG",{});var T=l(c);t=u(T,"Recommended model"),T.forEach(s),d=u(j,`:
`),q=o(j,"A",{href:!0,rel:!0});var N=l(q);k=u(N,"dbmdz/bert-large-cased-finetuned-conll03-english"),N.forEach(s),j.forEach(s),this.h()},h(){h(q,"href","https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english"),h(q,"rel","nofollow")},m(A,j){m(A,n,j),e(n,c),e(c,t),e(n,d),e(n,q),e(q,k)},d(A){A&&s(n)}}}function cJ($){let n,c;return n=new R({props:{code:`import json
import requests
headers = {"Authorization": f"Bearer {API_TOKEN}"}
API_URL = "https://api-inference.huggingface.co/models/dbmdz/bert-large-cased-finetuned-conll03-english"
def query(payload):
    data = json.dumps(payload)
    response = requests.request("POST", API_URL, headers=headers, data=data)
    return json.loads(response.content.decode("utf-8"))
data = query({"inputs": "My name is Sarah Jessica Parker but you can call me Jessica"})`,highlighted:`<span class="hljs-keyword">import</span> json
<span class="hljs-keyword">import</span> requests
headers = {<span class="hljs-string">&quot;Authorization&quot;</span>: <span class="hljs-string">f&quot;Bearer <span class="hljs-subst">{API_TOKEN}</span>&quot;</span>}
API_URL = <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/dbmdz/bert-large-cased-finetuned-conll03-english&quot;</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">payload</span>):
    data = json.dumps(payload)
    response = requests.request(<span class="hljs-string">&quot;POST&quot;</span>, API_URL, headers=headers, data=data)
    <span class="hljs-keyword">return</span> json.loads(response.content.decode(<span class="hljs-string">&quot;utf-8&quot;</span>))
data = query({<span class="hljs-string">&quot;inputs&quot;</span>: <span class="hljs-string">&quot;My name is Sarah Jessica Parker but you can call me Jessica&quot;</span>})`}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),c=!0},p:O,i(t){c||(b(n.$$.fragment,t),c=!0)},o(t){E(n.$$.fragment,t),c=!1},d(t){w(n,t)}}}function fJ($){let n,c;return n=new P({props:{$$slots:{default:[cJ]},$$scope:{ctx:$}}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),c=!0},p(t,d){const q={};d&2&&(q.$$scope={dirty:d,ctx:t}),n.$set(q)},i(t){c||(b(n.$$.fragment,t),c=!0)},o(t){E(n.$$.fragment,t),c=!1},d(t){w(n,t)}}}function pJ($){let n,c;return n=new R({props:{code:`import fetch from "node-fetch";
async function query(data) {
    const response = await fetch(
        "https://api-inference.huggingface.co/models/dbmdz/bert-large-cased-finetuned-conll03-english",
        {
            headers: { Authorization: \`Bearer \${API_TOKEN}\` },
            method: "POST",
            body: JSON.stringify(data),
        }
    );
    const result = await response.json();
    return result;
}
query({inputs:"My name is Sarah Jessica Parker but you can call me Jessica"}).then((response) => {
    console.log(JSON.stringify(response));
});
// [{"entity_group":"PER","score":0.9991337060928345,"word":"Sarah Jessica Parker","start":11,"end":31},{"entity_group":"PER","score":0.9979912042617798,"word":"Jessica","start":52,"end":59}]`,highlighted:`<span class="hljs-keyword">import</span> <span class="hljs-keyword">fetch</span> <span class="hljs-keyword">from</span> &quot;node-fetch&quot;;
async <span class="hljs-keyword">function</span> query(data) {
    const response = await <span class="hljs-keyword">fetch</span>(
        &quot;https://api-inference.huggingface.co/models/dbmdz/bert-large-cased-finetuned-conll03-english&quot;,
        {
            headers: { <span class="hljs-keyword">Authorization</span>: \`Bearer \${API_TOKEN}\` },
            <span class="hljs-keyword">method</span>: &quot;POST&quot;,
            body: <span class="hljs-type">JSON</span>.stringify(data),
        }
    );
    const result = await response.json();
    <span class="hljs-keyword">return</span> result;
}
query({inputs:&quot;My name is Sarah Jessica Parker but you can call me Jessica&quot;}).<span class="hljs-keyword">then</span>((response) =&gt; {
    console.log(<span class="hljs-type">JSON</span>.stringify(response));
});
// [{&quot;entity_group&quot;:&quot;PER&quot;,&quot;score&quot;:<span class="hljs-number">0.9991337060928345</span>,&quot;word&quot;:&quot;Sarah Jessica Parker&quot;,&quot;start&quot;:<span class="hljs-number">11</span>,&quot;end&quot;:<span class="hljs-number">31</span>},{&quot;entity_group&quot;:&quot;PER&quot;,&quot;score&quot;:<span class="hljs-number">0.9979912042617798</span>,&quot;word&quot;:&quot;Jessica&quot;,&quot;start&quot;:<span class="hljs-number">52</span>,&quot;end&quot;:<span class="hljs-number">59</span>}]`}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),c=!0},p:O,i(t){c||(b(n.$$.fragment,t),c=!0)},o(t){E(n.$$.fragment,t),c=!1},d(t){w(n,t)}}}function hJ($){let n,c;return n=new P({props:{$$slots:{default:[pJ]},$$scope:{ctx:$}}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),c=!0},p(t,d){const q={};d&2&&(q.$$scope={dirty:d,ctx:t}),n.$set(q)},i(t){c||(b(n.$$.fragment,t),c=!0)},o(t){E(n.$$.fragment,t),c=!1},d(t){w(n,t)}}}function dJ($){let n,c;return n=new R({props:{code:`curl https://api-inference.huggingface.co/models/dbmdz/bert-large-cased-finetuned-conll03-english \\
        -X POST \\
        -d '{"inputs":"My name is Sarah Jessica Parker but you can call me Jessica"}' \\
        -H "Authorization: Bearer \${HF_API_TOKEN}"
# [{"entity_group":"PER","score":0.9991337060928345,"word":"Sarah Jessica Parker","start":11,"end":31},{"entity_group":"PER","score":0.9979912042617798,"word":"Jessica","start":52,"end":59}]`,highlighted:`curl https://api-inference.huggingface.co/models/dbmdz/bert-large-cased-finetuned-conll03-english \\
        -X POST \\
        -d <span class="hljs-string">&#x27;{&quot;inputs&quot;:&quot;My name is Sarah Jessica Parker but you can call me Jessica&quot;}&#x27;</span> \\
        -H <span class="hljs-string">&quot;Authorization: Bearer <span class="hljs-variable">\${HF_API_TOKEN}</span>&quot;</span>
<span class="hljs-comment"># [{&quot;entity_group&quot;:&quot;PER&quot;,&quot;score&quot;:0.9991337060928345,&quot;word&quot;:&quot;Sarah Jessica Parker&quot;,&quot;start&quot;:11,&quot;end&quot;:31},{&quot;entity_group&quot;:&quot;PER&quot;,&quot;score&quot;:0.9979912042617798,&quot;word&quot;:&quot;Jessica&quot;,&quot;start&quot;:52,&quot;end&quot;:59}]</span>`}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),c=!0},p:O,i(t){c||(b(n.$$.fragment,t),c=!0)},o(t){E(n.$$.fragment,t),c=!1},d(t){w(n,t)}}}function gJ($){let n,c;return n=new P({props:{$$slots:{default:[dJ]},$$scope:{ctx:$}}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),c=!0},p(t,d){const q={};d&2&&(q.$$scope={dirty:d,ctx:t}),n.$set(q)},i(t){c||(b(n.$$.fragment,t),c=!0)},o(t){E(n.$$.fragment,t),c=!1},d(t){w(n,t)}}}function mJ($){let n,c;return n=new R({props:{code:`self.assertEqual(
    deep_round(data),
    [
        {
            "entity_group": "PER",
            "score": 0.9991,
            "word": "Sarah Jessica Parker",
            "start": 11,
            "end": 31,
        },
        {
            "entity_group": "PER",
            "score": 0.998,
            "word": "Jessica",
            "start": 52,
            "end": 59,
        },
    ],
)`,highlighted:`self.assertEqual(
    deep_round(data),
    [
        {
            <span class="hljs-string">&quot;entity_group&quot;</span>: <span class="hljs-string">&quot;PER&quot;</span>,
            <span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.9991</span>,
            <span class="hljs-string">&quot;word&quot;</span>: <span class="hljs-string">&quot;Sarah Jessica Parker&quot;</span>,
            <span class="hljs-string">&quot;start&quot;</span>: <span class="hljs-number">11</span>,
            <span class="hljs-string">&quot;end&quot;</span>: <span class="hljs-number">31</span>,
        },
        {
            <span class="hljs-string">&quot;entity_group&quot;</span>: <span class="hljs-string">&quot;PER&quot;</span>,
            <span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.998</span>,
            <span class="hljs-string">&quot;word&quot;</span>: <span class="hljs-string">&quot;Jessica&quot;</span>,
            <span class="hljs-string">&quot;start&quot;</span>: <span class="hljs-number">52</span>,
            <span class="hljs-string">&quot;end&quot;</span>: <span class="hljs-number">59</span>,
        },
    ],
)`}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),c=!0},p:O,i(t){c||(b(n.$$.fragment,t),c=!0)},o(t){E(n.$$.fragment,t),c=!1},d(t){w(n,t)}}}function qJ($){let n,c;return n=new P({props:{$$slots:{default:[mJ]},$$scope:{ctx:$}}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),c=!0},p(t,d){const q={};d&2&&(q.$$scope={dirty:d,ctx:t}),n.$set(q)},i(t){c||(b(n.$$.fragment,t),c=!0)},o(t){E(n.$$.fragment,t),c=!1},d(t){w(n,t)}}}function $J($){let n,c,t,d,q,k,A;return{c(){n=r("p"),c=r("strong"),t=i("Recommended model"),d=i(": "),q=r("a"),k=i("gpt2"),A=i(" (it\u2019s a simple model, but fun to play with)."),this.h()},l(j){n=o(j,"P",{});var T=l(n);c=o(T,"STRONG",{});var N=l(c);t=u(N,"Recommended model"),N.forEach(s),d=u(T,": "),q=o(T,"A",{href:!0,rel:!0});var D=l(q);k=u(D,"gpt2"),D.forEach(s),A=u(T," (it\u2019s a simple model, but fun to play with)."),T.forEach(s),this.h()},h(){h(q,"href","https://huggingface.co/gpt2"),h(q,"rel","nofollow")},m(j,T){m(j,n,T),e(n,c),e(c,t),e(n,d),e(n,q),e(q,k),e(n,A)},d(j){j&&s(n)}}}function _J($){let n,c;return n=new R({props:{code:`import json
import requests
headers = {"Authorization": f"Bearer {API_TOKEN}"}
API_URL = "https://api-inference.huggingface.co/models/gpt2"
def query(payload):
    data = json.dumps(payload)
    response = requests.request("POST", API_URL, headers=headers, data=data)
    return json.loads(response.content.decode("utf-8"))
data = query({"inputs": "The answer to the universe is"})`,highlighted:`<span class="hljs-keyword">import</span> json
<span class="hljs-keyword">import</span> requests
headers = {<span class="hljs-string">&quot;Authorization&quot;</span>: <span class="hljs-string">f&quot;Bearer <span class="hljs-subst">{API_TOKEN}</span>&quot;</span>}
API_URL = <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/gpt2&quot;</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">payload</span>):
    data = json.dumps(payload)
    response = requests.request(<span class="hljs-string">&quot;POST&quot;</span>, API_URL, headers=headers, data=data)
    <span class="hljs-keyword">return</span> json.loads(response.content.decode(<span class="hljs-string">&quot;utf-8&quot;</span>))
data = query({<span class="hljs-string">&quot;inputs&quot;</span>: <span class="hljs-string">&quot;The answer to the universe is&quot;</span>})`}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),c=!0},p:O,i(t){c||(b(n.$$.fragment,t),c=!0)},o(t){E(n.$$.fragment,t),c=!1},d(t){w(n,t)}}}function vJ($){let n,c;return n=new P({props:{$$slots:{default:[_J]},$$scope:{ctx:$}}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),c=!0},p(t,d){const q={};d&2&&(q.$$scope={dirty:d,ctx:t}),n.$set(q)},i(t){c||(b(n.$$.fragment,t),c=!0)},o(t){E(n.$$.fragment,t),c=!1},d(t){w(n,t)}}}function yJ($){let n,c;return n=new R({props:{code:`import fetch from "node-fetch";
async function query(data) {
    const response = await fetch(
        "https://api-inference.huggingface.co/models/gpt2",
        {
            headers: { Authorization: \`Bearer \${API_TOKEN}\` },
            method: "POST",
            body: JSON.stringify(data),
        }
    );
    const result = await response.json();
    return result;
}
query({inputs:"The answer to the universe is"}).then((response) => {
    console.log(JSON.stringify(response));
});
// [{"generated_text":"The answer to the universe is in a different shape (or shapeless) than"}]`,highlighted:`<span class="hljs-keyword">import</span> fetch <span class="hljs-keyword">from</span> <span class="hljs-string">&quot;node-fetch&quot;</span>;
<span class="hljs-keyword">async</span> <span class="hljs-keyword">function</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">data</span>) {
    <span class="hljs-keyword">const</span> response = <span class="hljs-keyword">await</span> <span class="hljs-title function_">fetch</span>(
        <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/gpt2&quot;</span>,
        {
            <span class="hljs-attr">headers</span>: { <span class="hljs-title class_">Authorization</span>: <span class="hljs-string">\`Bearer <span class="hljs-subst">\${API_TOKEN}</span>\`</span> },
            <span class="hljs-attr">method</span>: <span class="hljs-string">&quot;POST&quot;</span>,
            <span class="hljs-attr">body</span>: <span class="hljs-title class_">JSON</span>.<span class="hljs-title function_">stringify</span>(data),
        }
    );
    <span class="hljs-keyword">const</span> result = <span class="hljs-keyword">await</span> response.<span class="hljs-title function_">json</span>();
    <span class="hljs-keyword">return</span> result;
}
<span class="hljs-title function_">query</span>({<span class="hljs-attr">inputs</span>:<span class="hljs-string">&quot;The answer to the universe is&quot;</span>}).<span class="hljs-title function_">then</span>(<span class="hljs-function">(<span class="hljs-params">response</span>) =&gt;</span> {
    <span class="hljs-variable language_">console</span>.<span class="hljs-title function_">log</span>(<span class="hljs-title class_">JSON</span>.<span class="hljs-title function_">stringify</span>(response));
});
<span class="hljs-comment">// [{&quot;generated_text&quot;:&quot;The answer to the universe is in a different shape (or shapeless) than&quot;}]</span>`}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),c=!0},p:O,i(t){c||(b(n.$$.fragment,t),c=!0)},o(t){E(n.$$.fragment,t),c=!1},d(t){w(n,t)}}}function bJ($){let n,c;return n=new P({props:{$$slots:{default:[yJ]},$$scope:{ctx:$}}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),c=!0},p(t,d){const q={};d&2&&(q.$$scope={dirty:d,ctx:t}),n.$set(q)},i(t){c||(b(n.$$.fragment,t),c=!0)},o(t){E(n.$$.fragment,t),c=!1},d(t){w(n,t)}}}function EJ($){let n,c;return n=new R({props:{code:`curl https://api-inference.huggingface.co/models/gpt2 \\
        -X POST \\
        -d '{"inputs":"The answer to the universe is"}' \\
        -H "Authorization: Bearer \${HF_API_TOKEN}"
# [{"generated_text":"The answer to the universe is in a different shape (or shapeless) than"}]`,highlighted:`curl https://api-inference.huggingface.co/models/gpt2 \\
        -X POST \\
        -d <span class="hljs-string">&#x27;{&quot;inputs&quot;:&quot;The answer to the universe is&quot;}&#x27;</span> \\
        -H <span class="hljs-string">&quot;Authorization: Bearer <span class="hljs-variable">\${HF_API_TOKEN}</span>&quot;</span>
<span class="hljs-comment"># [{&quot;generated_text&quot;:&quot;The answer to the universe is in a different shape (or shapeless) than&quot;}]</span>`}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),c=!0},p:O,i(t){c||(b(n.$$.fragment,t),c=!0)},o(t){E(n.$$.fragment,t),c=!1},d(t){w(n,t)}}}function wJ($){let n,c;return n=new P({props:{$$slots:{default:[EJ]},$$scope:{ctx:$}}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),c=!0},p(t,d){const q={};d&2&&(q.$$scope={dirty:d,ctx:t}),n.$set(q)},i(t){c||(b(n.$$.fragment,t),c=!0)},o(t){E(n.$$.fragment,t),c=!1},d(t){w(n,t)}}}function TJ($){let n,c;return n=new R({props:{code:`data == [
    {
        "generated_text": 'The answer to the universe is that we are the creation of the entire universe," says Fitch.\\n\\nAs of the 1960s, six times as many Americans still make fewer than six bucks ($17) per year on their way to retirement.'
    }
]`,highlighted:`data == [
    {
        <span class="hljs-string">&quot;generated_text&quot;</span>: <span class="hljs-string">&#x27;The answer to the universe is that we are the creation of the entire universe,&quot; says Fitch.\\n\\nAs of the 1960s, six times as many Americans still make fewer than six bucks ($17) per year on their way to retirement.&#x27;</span>
    }
]`}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),c=!0},p:O,i(t){c||(b(n.$$.fragment,t),c=!0)},o(t){E(n.$$.fragment,t),c=!1},d(t){w(n,t)}}}function jJ($){let n,c;return n=new P({props:{$$slots:{default:[TJ]},$$scope:{ctx:$}}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),c=!0},p(t,d){const q={};d&2&&(q.$$scope={dirty:d,ctx:t}),n.$set(q)},i(t){c||(b(n.$$.fragment,t),c=!0)},o(t){E(n.$$.fragment,t),c=!1},d(t){w(n,t)}}}function kJ($){let n,c,t,d,q,k,A;return{c(){n=r("p"),c=r("strong"),t=i("Recommended model"),d=i(`:
`),q=r("a"),k=i("bert-base-uncased"),A=i(" (it\u2019s a simple model, but fun to play with)."),this.h()},l(j){n=o(j,"P",{});var T=l(n);c=o(T,"STRONG",{});var N=l(c);t=u(N,"Recommended model"),N.forEach(s),d=u(T,`:
`),q=o(T,"A",{href:!0,rel:!0});var D=l(q);k=u(D,"bert-base-uncased"),D.forEach(s),A=u(T," (it\u2019s a simple model, but fun to play with)."),T.forEach(s),this.h()},h(){h(q,"href","https://huggingface.co/bert-base-uncased"),h(q,"rel","nofollow")},m(j,T){m(j,n,T),e(n,c),e(c,t),e(n,d),e(n,q),e(q,k),e(n,A)},d(j){j&&s(n)}}}function AJ($){let n,c;return n=new R({props:{code:`import json
import requests
headers = {"Authorization": f"Bearer {API_TOKEN}"}
API_URL = "https://api-inference.huggingface.co/models/bert-base-uncased"
def query(payload):
    data = json.dumps(payload)
    response = requests.request("POST", API_URL, headers=headers, data=data)
    return json.loads(response.content.decode("utf-8"))
data = query({"inputs": "The answer to the universe is [MASK]."})`,highlighted:`<span class="hljs-keyword">import</span> json
<span class="hljs-keyword">import</span> requests
headers = {<span class="hljs-string">&quot;Authorization&quot;</span>: <span class="hljs-string">f&quot;Bearer <span class="hljs-subst">{API_TOKEN}</span>&quot;</span>}
API_URL = <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/bert-base-uncased&quot;</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">payload</span>):
    data = json.dumps(payload)
    response = requests.request(<span class="hljs-string">&quot;POST&quot;</span>, API_URL, headers=headers, data=data)
    <span class="hljs-keyword">return</span> json.loads(response.content.decode(<span class="hljs-string">&quot;utf-8&quot;</span>))
data = query({<span class="hljs-string">&quot;inputs&quot;</span>: <span class="hljs-string">&quot;The answer to the universe is [MASK].&quot;</span>})`}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),c=!0},p:O,i(t){c||(b(n.$$.fragment,t),c=!0)},o(t){E(n.$$.fragment,t),c=!1},d(t){w(n,t)}}}function DJ($){let n,c;return n=new P({props:{$$slots:{default:[AJ]},$$scope:{ctx:$}}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),c=!0},p(t,d){const q={};d&2&&(q.$$scope={dirty:d,ctx:t}),n.$set(q)},i(t){c||(b(n.$$.fragment,t),c=!0)},o(t){E(n.$$.fragment,t),c=!1},d(t){w(n,t)}}}function OJ($){let n,c;return n=new R({props:{code:`import fetch from "node-fetch";
async function query(data) {
    const response = await fetch(
        "https://api-inference.huggingface.co/models/bert-base-uncased",
        {
            headers: { Authorization: \`Bearer \${API_TOKEN}\` },
            method: "POST",
            body: JSON.stringify(data),
        }
    );
    const result = await response.json();
    return result;
}
query({inputs:"The answer to the universe is [MASK]."}).then((response) => {
    console.log(JSON.stringify(response));
});
// [{"sequence":"the answer to the universe is no.","score":0.16963955760002136,"token":2053,"token_str":"no"},{"sequence":"the answer to the universe is nothing.","score":0.07344776391983032,"token":2498,"token_str":"nothing"},{"sequence":"the answer to the universe is yes.","score":0.05803241208195686,"token":2748,"token_str":"yes"},{"sequence":"the answer to the universe is unknown.","score":0.043957844376564026,"token":4242,"token_str":"unknown"},{"sequence":"the answer to the universe is simple.","score":0.04015745222568512,"token":3722,"token_str":"simple"}]`,highlighted:`<span class="hljs-keyword">import</span> <span class="hljs-keyword">fetch</span> <span class="hljs-keyword">from</span> &quot;node-fetch&quot;;
async <span class="hljs-keyword">function</span> query(data) {
    const response = await <span class="hljs-keyword">fetch</span>(
        &quot;https://api-inference.huggingface.co/models/bert-base-uncased&quot;,
        {
            headers: { <span class="hljs-keyword">Authorization</span>: \`Bearer \${API_TOKEN}\` },
            <span class="hljs-keyword">method</span>: &quot;POST&quot;,
            body: <span class="hljs-type">JSON</span>.stringify(data),
        }
    );
    const result = await response.json();
    <span class="hljs-keyword">return</span> result;
}
query({inputs:&quot;The answer to the universe is [MASK].&quot;}).<span class="hljs-keyword">then</span>((response) =&gt; {
    console.log(<span class="hljs-type">JSON</span>.stringify(response));
});
// [{&quot;sequence&quot;:&quot;the answer to the universe is no.&quot;,&quot;score&quot;:<span class="hljs-number">0.16963955760002136</span>,&quot;token&quot;:<span class="hljs-number">2053</span>,&quot;token_str&quot;:&quot;no&quot;},{&quot;sequence&quot;:&quot;the answer to the universe is nothing.&quot;,&quot;score&quot;:<span class="hljs-number">0.07344776391983032</span>,&quot;token&quot;:<span class="hljs-number">2498</span>,&quot;token_str&quot;:&quot;nothing&quot;},{&quot;sequence&quot;:&quot;the answer to the universe is yes.&quot;,&quot;score&quot;:<span class="hljs-number">0.05803241208195686</span>,&quot;token&quot;:<span class="hljs-number">2748</span>,&quot;token_str&quot;:&quot;yes&quot;},{&quot;sequence&quot;:&quot;the answer to the universe is unknown.&quot;,&quot;score&quot;:<span class="hljs-number">0.043957844376564026</span>,&quot;token&quot;:<span class="hljs-number">4242</span>,&quot;token_str&quot;:&quot;unknown&quot;},{&quot;sequence&quot;:&quot;the answer to the universe is simple.&quot;,&quot;score&quot;:<span class="hljs-number">0.04015745222568512</span>,&quot;token&quot;:<span class="hljs-number">3722</span>,&quot;token_str&quot;:&quot;simple&quot;}]`}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),c=!0},p:O,i(t){c||(b(n.$$.fragment,t),c=!0)},o(t){E(n.$$.fragment,t),c=!1},d(t){w(n,t)}}}function PJ($){let n,c;return n=new P({props:{$$slots:{default:[OJ]},$$scope:{ctx:$}}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),c=!0},p(t,d){const q={};d&2&&(q.$$scope={dirty:d,ctx:t}),n.$set(q)},i(t){c||(b(n.$$.fragment,t),c=!0)},o(t){E(n.$$.fragment,t),c=!1},d(t){w(n,t)}}}function RJ($){let n,c;return n=new R({props:{code:`curl https://api-inference.huggingface.co/models/bert-base-uncased \\
        -X POST \\
        -d '{"inputs":"The answer to the universe is [MASK]."}' \\
        -H "Authorization: Bearer \${HF_API_TOKEN}"
# [{"sequence":"the answer to the universe is no.","score":0.16963955760002136,"token":2053,"token_str":"no"},{"sequence":"the answer to the universe is nothing.","score":0.07344776391983032,"token":2498,"token_str":"nothing"},{"sequence":"the answer to the universe is yes.","score":0.05803241208195686,"token":2748,"token_str":"yes"},{"sequence":"the answer to the universe is unknown.","score":0.043957844376564026,"token":4242,"token_str":"unknown"},{"sequence":"the answer to the universe is simple.","score":0.04015745222568512,"token":3722,"token_str":"simple"}]`,highlighted:`curl https://api-inference.huggingface.co/models/bert-base-uncased \\
        -X POST \\
        -d <span class="hljs-string">&#x27;{&quot;inputs&quot;:&quot;The answer to the universe is [MASK].&quot;}&#x27;</span> \\
        -H <span class="hljs-string">&quot;Authorization: Bearer <span class="hljs-variable">\${HF_API_TOKEN}</span>&quot;</span>
<span class="hljs-comment"># [{&quot;sequence&quot;:&quot;the answer to the universe is no.&quot;,&quot;score&quot;:0.16963955760002136,&quot;token&quot;:2053,&quot;token_str&quot;:&quot;no&quot;},{&quot;sequence&quot;:&quot;the answer to the universe is nothing.&quot;,&quot;score&quot;:0.07344776391983032,&quot;token&quot;:2498,&quot;token_str&quot;:&quot;nothing&quot;},{&quot;sequence&quot;:&quot;the answer to the universe is yes.&quot;,&quot;score&quot;:0.05803241208195686,&quot;token&quot;:2748,&quot;token_str&quot;:&quot;yes&quot;},{&quot;sequence&quot;:&quot;the answer to the universe is unknown.&quot;,&quot;score&quot;:0.043957844376564026,&quot;token&quot;:4242,&quot;token_str&quot;:&quot;unknown&quot;},{&quot;sequence&quot;:&quot;the answer to the universe is simple.&quot;,&quot;score&quot;:0.04015745222568512,&quot;token&quot;:3722,&quot;token_str&quot;:&quot;simple&quot;}]</span>`}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),c=!0},p:O,i(t){c||(b(n.$$.fragment,t),c=!0)},o(t){E(n.$$.fragment,t),c=!1},d(t){w(n,t)}}}function NJ($){let n,c;return n=new P({props:{$$slots:{default:[RJ]},$$scope:{ctx:$}}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),c=!0},p(t,d){const q={};d&2&&(q.$$scope={dirty:d,ctx:t}),n.$set(q)},i(t){c||(b(n.$$.fragment,t),c=!0)},o(t){E(n.$$.fragment,t),c=!1},d(t){w(n,t)}}}function SJ($){let n,c;return n=new R({props:{code:`self.assertEqual(
    deep_round(data),
    [
        {
            "sequence": "the answer to the universe is no.",
            "score": 0.1696,
            "token": 2053,
            "token_str": "no",
        },
        {
            "sequence": "the answer to the universe is nothing.",
            "score": 0.0734,
            "token": 2498,
            "token_str": "nothing",
        },
        {
            "sequence": "the answer to the universe is yes.",
            "score": 0.0580,
            "token": 2748,
            "token_str": "yes",
        },
        {
            "sequence": "the answer to the universe is unknown.",
            "score": 0.044,
            "token": 4242,
            "token_str": "unknown",
        },
        {
            "sequence": "the answer to the universe is simple.",
            "score": 0.0402,
            "token": 3722,
            "token_str": "simple",
        },
    ],
)`,highlighted:`self.assertEqual(
    deep_round(data),
    [
        {
            <span class="hljs-string">&quot;sequence&quot;</span>: <span class="hljs-string">&quot;the answer to the universe is no.&quot;</span>,
            <span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.1696</span>,
            <span class="hljs-string">&quot;token&quot;</span>: <span class="hljs-number">2053</span>,
            <span class="hljs-string">&quot;token_str&quot;</span>: <span class="hljs-string">&quot;no&quot;</span>,
        },
        {
            <span class="hljs-string">&quot;sequence&quot;</span>: <span class="hljs-string">&quot;the answer to the universe is nothing.&quot;</span>,
            <span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.0734</span>,
            <span class="hljs-string">&quot;token&quot;</span>: <span class="hljs-number">2498</span>,
            <span class="hljs-string">&quot;token_str&quot;</span>: <span class="hljs-string">&quot;nothing&quot;</span>,
        },
        {
            <span class="hljs-string">&quot;sequence&quot;</span>: <span class="hljs-string">&quot;the answer to the universe is yes.&quot;</span>,
            <span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.0580</span>,
            <span class="hljs-string">&quot;token&quot;</span>: <span class="hljs-number">2748</span>,
            <span class="hljs-string">&quot;token_str&quot;</span>: <span class="hljs-string">&quot;yes&quot;</span>,
        },
        {
            <span class="hljs-string">&quot;sequence&quot;</span>: <span class="hljs-string">&quot;the answer to the universe is unknown.&quot;</span>,
            <span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.044</span>,
            <span class="hljs-string">&quot;token&quot;</span>: <span class="hljs-number">4242</span>,
            <span class="hljs-string">&quot;token_str&quot;</span>: <span class="hljs-string">&quot;unknown&quot;</span>,
        },
        {
            <span class="hljs-string">&quot;sequence&quot;</span>: <span class="hljs-string">&quot;the answer to the universe is simple.&quot;</span>,
            <span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.0402</span>,
            <span class="hljs-string">&quot;token&quot;</span>: <span class="hljs-number">3722</span>,
            <span class="hljs-string">&quot;token_str&quot;</span>: <span class="hljs-string">&quot;simple&quot;</span>,
        },
    ],
)`}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),c=!0},p:O,i(t){c||(b(n.$$.fragment,t),c=!0)},o(t){E(n.$$.fragment,t),c=!1},d(t){w(n,t)}}}function xJ($){let n,c;return n=new P({props:{$$slots:{default:[SJ]},$$scope:{ctx:$}}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),c=!0},p(t,d){const q={};d&2&&(q.$$scope={dirty:d,ctx:t}),n.$set(q)},i(t){c||(b(n.$$.fragment,t),c=!0)},o(t){E(n.$$.fragment,t),c=!1},d(t){w(n,t)}}}function IJ($){let n,c,t,d,q,k,A;return{c(){n=r("p"),c=r("strong"),t=i("Recommended model"),d=i(": "),q=r("a"),k=i(`Check your
langage`),A=i("."),this.h()},l(j){n=o(j,"P",{});var T=l(n);c=o(T,"STRONG",{});var N=l(c);t=u(N,"Recommended model"),N.forEach(s),d=u(T,": "),q=o(T,"A",{href:!0,rel:!0});var D=l(q);k=u(D,`Check your
langage`),D.forEach(s),A=u(T,"."),T.forEach(s),this.h()},h(){h(q,"href","https://huggingface.co/models?pipeline_tag=automatic-speech-recognition"),h(q,"rel","nofollow")},m(j,T){m(j,n,T),e(n,c),e(c,t),e(n,d),e(n,q),e(q,k),e(n,A)},d(j){j&&s(n)}}}function HJ($){let n,c,t,d,q,k,A;return{c(){n=r("p"),c=r("strong"),t=i("English"),d=i(`:
`),q=r("a"),k=i("facebook/wav2vec2-large-960h-lv60-self"),A=i("."),this.h()},l(j){n=o(j,"P",{});var T=l(n);c=o(T,"STRONG",{});var N=l(c);t=u(N,"English"),N.forEach(s),d=u(T,`:
`),q=o(T,"A",{href:!0,rel:!0});var D=l(q);k=u(D,"facebook/wav2vec2-large-960h-lv60-self"),D.forEach(s),A=u(T,"."),T.forEach(s),this.h()},h(){h(q,"href","https://huggingface.co/facebook/wav2vec2-large-960h-lv60-self"),h(q,"rel","nofollow")},m(j,T){m(j,n,T),e(n,c),e(c,t),e(n,d),e(n,q),e(q,k),e(n,A)},d(j){j&&s(n)}}}function BJ($){let n,c;return n=new R({props:{code:`import json
import requests
headers = {"Authorization": f"Bearer {API_TOKEN}"}
API_URL = "https://api-inference.huggingface.co/models/facebook/wav2vec2-base-960h"
def query(filename):
    with open(filename, "rb") as f:
        data = f.read()
    response = requests.request("POST", API_URL, headers=headers, data=data)
    return json.loads(response.content.decode("utf-8"))
data = query("sample1.flac")`,highlighted:`<span class="hljs-keyword">import</span> json
<span class="hljs-keyword">import</span> requests
headers = {<span class="hljs-string">&quot;Authorization&quot;</span>: <span class="hljs-string">f&quot;Bearer <span class="hljs-subst">{API_TOKEN}</span>&quot;</span>}
API_URL = <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/facebook/wav2vec2-base-960h&quot;</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">filename</span>):
    <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(filename, <span class="hljs-string">&quot;rb&quot;</span>) <span class="hljs-keyword">as</span> f:
        data = f.read()
    response = requests.request(<span class="hljs-string">&quot;POST&quot;</span>, API_URL, headers=headers, data=data)
    <span class="hljs-keyword">return</span> json.loads(response.content.decode(<span class="hljs-string">&quot;utf-8&quot;</span>))
data = query(<span class="hljs-string">&quot;sample1.flac&quot;</span>)`}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),c=!0},p:O,i(t){c||(b(n.$$.fragment,t),c=!0)},o(t){E(n.$$.fragment,t),c=!1},d(t){w(n,t)}}}function CJ($){let n,c;return n=new P({props:{$$slots:{default:[BJ]},$$scope:{ctx:$}}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),c=!0},p(t,d){const q={};d&2&&(q.$$scope={dirty:d,ctx:t}),n.$set(q)},i(t){c||(b(n.$$.fragment,t),c=!0)},o(t){E(n.$$.fragment,t),c=!1},d(t){w(n,t)}}}function GJ($){let n,c;return n=new R({props:{code:`import fetch from "node-fetch";
import fs from "fs";
async function query(filename) {
    const data = fs.readFileSync(filename);
    const response = await fetch(
        "https://api-inference.huggingface.co/models/facebook/wav2vec2-base-960h",
        {
            headers: { Authorization: \`Bearer \${API_TOKEN}\` },
            method: "POST",
            body: data,
        }
    );
    const result = await response.json();
    return result;
}
query("sample1.flac").then((response) => {
    console.log(JSON.stringify(response));
});
// {"text":"GOING ALONG SLUSHY COUNTRY ROADS AND SPEAKING TO DAMP AUDIENCES IN DRAUGHTY SCHOOL ROOMS DAY AFTER DAY FOR A FORTNIGHT HE'LL HAVE TO PUT IN AN APPEARANCE AT SOME PLACE OF WORSHIP ON SUNDAY MORNING AND HE CAN COME TO US IMMEDIATELY AFTERWARDS"}`,highlighted:`<span class="hljs-keyword">import</span> fetch <span class="hljs-keyword">from</span> <span class="hljs-string">&quot;node-fetch&quot;</span>;
<span class="hljs-keyword">import</span> fs <span class="hljs-keyword">from</span> <span class="hljs-string">&quot;fs&quot;</span>;
<span class="hljs-keyword">async</span> <span class="hljs-keyword">function</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">filename</span>) {
    <span class="hljs-keyword">const</span> data = fs.<span class="hljs-title function_">readFileSync</span>(filename);
    <span class="hljs-keyword">const</span> response = <span class="hljs-keyword">await</span> <span class="hljs-title function_">fetch</span>(
        <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/facebook/wav2vec2-base-960h&quot;</span>,
        {
            <span class="hljs-attr">headers</span>: { <span class="hljs-title class_">Authorization</span>: <span class="hljs-string">\`Bearer <span class="hljs-subst">\${API_TOKEN}</span>\`</span> },
            <span class="hljs-attr">method</span>: <span class="hljs-string">&quot;POST&quot;</span>,
            <span class="hljs-attr">body</span>: data,
        }
    );
    <span class="hljs-keyword">const</span> result = <span class="hljs-keyword">await</span> response.<span class="hljs-title function_">json</span>();
    <span class="hljs-keyword">return</span> result;
}
<span class="hljs-title function_">query</span>(<span class="hljs-string">&quot;sample1.flac&quot;</span>).<span class="hljs-title function_">then</span>(<span class="hljs-function">(<span class="hljs-params">response</span>) =&gt;</span> {
    <span class="hljs-variable language_">console</span>.<span class="hljs-title function_">log</span>(<span class="hljs-title class_">JSON</span>.<span class="hljs-title function_">stringify</span>(response));
});
<span class="hljs-comment">// {&quot;text&quot;:&quot;GOING ALONG SLUSHY COUNTRY ROADS AND SPEAKING TO DAMP AUDIENCES IN DRAUGHTY SCHOOL ROOMS DAY AFTER DAY FOR A FORTNIGHT HE&#x27;LL HAVE TO PUT IN AN APPEARANCE AT SOME PLACE OF WORSHIP ON SUNDAY MORNING AND HE CAN COME TO US IMMEDIATELY AFTERWARDS&quot;}</span>`}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),c=!0},p:O,i(t){c||(b(n.$$.fragment,t),c=!0)},o(t){E(n.$$.fragment,t),c=!1},d(t){w(n,t)}}}function LJ($){let n,c;return n=new P({props:{$$slots:{default:[GJ]},$$scope:{ctx:$}}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),c=!0},p(t,d){const q={};d&2&&(q.$$scope={dirty:d,ctx:t}),n.$set(q)},i(t){c||(b(n.$$.fragment,t),c=!0)},o(t){E(n.$$.fragment,t),c=!1},d(t){w(n,t)}}}function UJ($){let n,c;return n=new R({props:{code:`curl https://api-inference.huggingface.co/models/facebook/wav2vec2-base-960h \\
        -X POST \\
        --data-binary '@sample1.flac' \\
        -H "Authorization: Bearer \${HF_API_TOKEN}"
# {"text":"GOING ALONG SLUSHY COUNTRY ROADS AND SPEAKING TO DAMP AUDIENCES IN DRAUGHTY SCHOOL ROOMS DAY AFTER DAY FOR A FORTNIGHT HE'LL HAVE TO PUT IN AN APPEARANCE AT SOME PLACE OF WORSHIP ON SUNDAY MORNING AND HE CAN COME TO US IMMEDIATELY AFTERWARDS"}`,highlighted:`curl https://api-inference.huggingface.co/models/facebook/wav2vec2-base-960h \\
        -X POST \\
        --data-binary <span class="hljs-string">&#x27;@sample1.flac&#x27;</span> \\
        -H <span class="hljs-string">&quot;Authorization: Bearer <span class="hljs-variable">\${HF_API_TOKEN}</span>&quot;</span>
<span class="hljs-comment"># {&quot;text&quot;:&quot;GOING ALONG SLUSHY COUNTRY ROADS AND SPEAKING TO DAMP AUDIENCES IN DRAUGHTY SCHOOL ROOMS DAY AFTER DAY FOR A FORTNIGHT HE&#x27;LL HAVE TO PUT IN AN APPEARANCE AT SOME PLACE OF WORSHIP ON SUNDAY MORNING AND HE CAN COME TO US IMMEDIATELY AFTERWARDS&quot;}</span>`}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),c=!0},p:O,i(t){c||(b(n.$$.fragment,t),c=!0)},o(t){E(n.$$.fragment,t),c=!1},d(t){w(n,t)}}}function zJ($){let n,c;return n=new P({props:{$$slots:{default:[UJ]},$$scope:{ctx:$}}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),c=!0},p(t,d){const q={};d&2&&(q.$$scope={dirty:d,ctx:t}),n.$set(q)},i(t){c||(b(n.$$.fragment,t),c=!0)},o(t){E(n.$$.fragment,t),c=!1},d(t){w(n,t)}}}function MJ($){let n,c;return n=new R({props:{code:`self.assertEqual(
    data,
    {
        "text": "GOING ALONG SLUSHY COUNTRY ROADS AND SPEAKING TO DAMP AUDIENCES IN DRAUGHTY SCHOOL ROOMS DAY AFTER DAY FOR A FORTNIGHT HE'LL HAVE TO PUT IN AN APPEARANCE AT SOME PLACE OF WORSHIP ON SUNDAY MORNING AND HE CAN COME TO US IMMEDIATELY AFTERWARDS"
    },
)`,highlighted:`self.assertEqual(
    data,
    {
        <span class="hljs-string">&quot;text&quot;</span>: <span class="hljs-string">&quot;GOING ALONG SLUSHY COUNTRY ROADS AND SPEAKING TO DAMP AUDIENCES IN DRAUGHTY SCHOOL ROOMS DAY AFTER DAY FOR A FORTNIGHT HE&#x27;LL HAVE TO PUT IN AN APPEARANCE AT SOME PLACE OF WORSHIP ON SUNDAY MORNING AND HE CAN COME TO US IMMEDIATELY AFTERWARDS&quot;</span>
    },
)`}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),c=!0},p:O,i(t){c||(b(n.$$.fragment,t),c=!0)},o(t){E(n.$$.fragment,t),c=!1},d(t){w(n,t)}}}function FJ($){let n,c;return n=new P({props:{$$slots:{default:[MJ]},$$scope:{ctx:$}}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),c=!0},p(t,d){const q={};d&2&&(q.$$scope={dirty:d,ctx:t}),n.$set(q)},i(t){c||(b(n.$$.fragment,t),c=!0)},o(t){E(n.$$.fragment,t),c=!1},d(t){w(n,t)}}}function KJ($){let n,c,t,d,q,k,A;return{c(){n=r("p"),c=r("strong"),t=i("Recommended model"),d=i(`:
`),q=r("a"),k=i("Sentence-transformers"),A=i("."),this.h()},l(j){n=o(j,"P",{});var T=l(n);c=o(T,"STRONG",{});var N=l(c);t=u(N,"Recommended model"),N.forEach(s),d=u(T,`:
`),q=o(T,"A",{href:!0,rel:!0});var D=l(q);k=u(D,"Sentence-transformers"),D.forEach(s),A=u(T,"."),T.forEach(s),this.h()},h(){h(q,"href","https://huggingface.co/sentence-transformers/paraphrase-xlm-r-multilingual-v1"),h(q,"rel","nofollow")},m(j,T){m(j,n,T),e(n,c),e(c,t),e(n,d),e(n,q),e(q,k),e(n,A)},d(j){j&&s(n)}}}function JJ($){let n,c,t,d,q,k,A;return{c(){n=r("p"),c=r("strong"),t=i("Recommended model"),d=i(`:
`),q=r("a"),k=i("superb/hubert-large-superb-er"),A=i("."),this.h()},l(j){n=o(j,"P",{});var T=l(n);c=o(T,"STRONG",{});var N=l(c);t=u(N,"Recommended model"),N.forEach(s),d=u(T,`:
`),q=o(T,"A",{href:!0,rel:!0});var D=l(q);k=u(D,"superb/hubert-large-superb-er"),D.forEach(s),A=u(T,"."),T.forEach(s),this.h()},h(){h(q,"href","https://huggingface.co/superb/hubert-large-superb-er"),h(q,"rel","nofollow")},m(j,T){m(j,n,T),e(n,c),e(c,t),e(n,d),e(n,q),e(q,k),e(n,A)},d(j){j&&s(n)}}}function WJ($){let n,c;return n=new R({props:{code:`import json
import requests
headers = {"Authorization": f"Bearer {API_TOKEN}"}
API_URL = "https://api-inference.huggingface.co/models/superb/hubert-large-superb-er"
def query(filename):
    with open(filename, "rb") as f:
        data = f.read()
    response = requests.request("POST", API_URL, headers=headers, data=data)
    return json.loads(response.content.decode("utf-8"))
data = query("sample1.flac")`,highlighted:`<span class="hljs-keyword">import</span> json
<span class="hljs-keyword">import</span> requests
headers = {<span class="hljs-string">&quot;Authorization&quot;</span>: <span class="hljs-string">f&quot;Bearer <span class="hljs-subst">{API_TOKEN}</span>&quot;</span>}
API_URL = <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/superb/hubert-large-superb-er&quot;</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">filename</span>):
    <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(filename, <span class="hljs-string">&quot;rb&quot;</span>) <span class="hljs-keyword">as</span> f:
        data = f.read()
    response = requests.request(<span class="hljs-string">&quot;POST&quot;</span>, API_URL, headers=headers, data=data)
    <span class="hljs-keyword">return</span> json.loads(response.content.decode(<span class="hljs-string">&quot;utf-8&quot;</span>))
data = query(<span class="hljs-string">&quot;sample1.flac&quot;</span>)`}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),c=!0},p:O,i(t){c||(b(n.$$.fragment,t),c=!0)},o(t){E(n.$$.fragment,t),c=!1},d(t){w(n,t)}}}function YJ($){let n,c;return n=new P({props:{$$slots:{default:[WJ]},$$scope:{ctx:$}}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),c=!0},p(t,d){const q={};d&2&&(q.$$scope={dirty:d,ctx:t}),n.$set(q)},i(t){c||(b(n.$$.fragment,t),c=!0)},o(t){E(n.$$.fragment,t),c=!1},d(t){w(n,t)}}}function VJ($){let n,c;return n=new R({props:{code:`import fetch from "node-fetch";
import fs from "fs";
async function query(filename) {
    const data = fs.readFileSync(filename);
    const response = await fetch(
        "https://api-inference.huggingface.co/models/superb/hubert-large-superb-er",
        {
            headers: { Authorization: \`Bearer \${API_TOKEN}\` },
            method: "POST",
            body: data,
        }
    );
    const result = await response.json();
    return result;
}
query("sample1.flac").then((response) => {
    console.log(JSON.stringify(response));
});
// [{"score":0.5927661657333374,"label":"neu"},{"score":0.2002529799938202,"label":"hap"},{"score":0.12795612215995789,"label":"ang"},{"score":0.07902472466230392,"label":"sad"}]`,highlighted:`<span class="hljs-keyword">import</span> <span class="hljs-keyword">fetch</span> <span class="hljs-keyword">from</span> &quot;node-fetch&quot;;
<span class="hljs-keyword">import</span> fs <span class="hljs-keyword">from</span> &quot;fs&quot;;
async <span class="hljs-keyword">function</span> query(filename) {
    const data = fs.readFileSync(filename);
    const response = await <span class="hljs-keyword">fetch</span>(
        &quot;https://api-inference.huggingface.co/models/superb/hubert-large-superb-er&quot;,
        {
            headers: { <span class="hljs-keyword">Authorization</span>: \`Bearer \${API_TOKEN}\` },
            <span class="hljs-keyword">method</span>: &quot;POST&quot;,
            body: data,
        }
    );
    const result = await response.json();
    <span class="hljs-keyword">return</span> result;
}
query(&quot;sample1.flac&quot;).<span class="hljs-keyword">then</span>((response) =&gt; {
    console.log(<span class="hljs-type">JSON</span>.stringify(response));
});
// [{&quot;score&quot;:<span class="hljs-number">0.5927661657333374</span>,&quot;label&quot;:&quot;neu&quot;},{&quot;score&quot;:<span class="hljs-number">0.2002529799938202</span>,&quot;label&quot;:&quot;hap&quot;},{&quot;score&quot;:<span class="hljs-number">0.12795612215995789</span>,&quot;label&quot;:&quot;ang&quot;},{&quot;score&quot;:<span class="hljs-number">0.07902472466230392</span>,&quot;label&quot;:&quot;sad&quot;}]`}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),c=!0},p:O,i(t){c||(b(n.$$.fragment,t),c=!0)},o(t){E(n.$$.fragment,t),c=!1},d(t){w(n,t)}}}function XJ($){let n,c;return n=new P({props:{$$slots:{default:[VJ]},$$scope:{ctx:$}}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),c=!0},p(t,d){const q={};d&2&&(q.$$scope={dirty:d,ctx:t}),n.$set(q)},i(t){c||(b(n.$$.fragment,t),c=!0)},o(t){E(n.$$.fragment,t),c=!1},d(t){w(n,t)}}}function QJ($){let n,c;return n=new R({props:{code:`curl https://api-inference.huggingface.co/models/superb/hubert-large-superb-er \\
        -X POST \\
        --data-binary '@sample1.flac' \\
        -H "Authorization: Bearer \${HF_API_TOKEN}"
# [{"score":0.5927661657333374,"label":"neu"},{"score":0.2002529799938202,"label":"hap"},{"score":0.12795612215995789,"label":"ang"},{"score":0.07902472466230392,"label":"sad"}]`,highlighted:`curl https://api-inference.huggingface.co/models/superb/hubert-large-superb-er \\
        -X POST \\
        --data-binary <span class="hljs-string">&#x27;@sample1.flac&#x27;</span> \\
        -H <span class="hljs-string">&quot;Authorization: Bearer <span class="hljs-variable">\${HF_API_TOKEN}</span>&quot;</span>
<span class="hljs-comment"># [{&quot;score&quot;:0.5927661657333374,&quot;label&quot;:&quot;neu&quot;},{&quot;score&quot;:0.2002529799938202,&quot;label&quot;:&quot;hap&quot;},{&quot;score&quot;:0.12795612215995789,&quot;label&quot;:&quot;ang&quot;},{&quot;score&quot;:0.07902472466230392,&quot;label&quot;:&quot;sad&quot;}]</span>`}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),c=!0},p:O,i(t){c||(b(n.$$.fragment,t),c=!0)},o(t){E(n.$$.fragment,t),c=!1},d(t){w(n,t)}}}function ZJ($){let n,c;return n=new P({props:{$$slots:{default:[QJ]},$$scope:{ctx:$}}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),c=!0},p(t,d){const q={};d&2&&(q.$$scope={dirty:d,ctx:t}),n.$set(q)},i(t){c||(b(n.$$.fragment,t),c=!0)},o(t){E(n.$$.fragment,t),c=!1},d(t){w(n,t)}}}function eW($){let n,c;return n=new R({props:{code:`self.assertEqual(
    deep_round(data, 4),
    [
        {"score": 0.5928, "label": "neu"},
        {"score": 0.2003, "label": "hap"},
        {"score": 0.128, "label": "ang"},
        {"score": 0.079, "label": "sad"},
    ],
)`,highlighted:`self.assertEqual(
    deep_round(data, <span class="hljs-number">4</span>),
    [
        {<span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.5928</span>, <span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;neu&quot;</span>},
        {<span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.2003</span>, <span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;hap&quot;</span>},
        {<span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.128</span>, <span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;ang&quot;</span>},
        {<span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.079</span>, <span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;sad&quot;</span>},
    ],
)`}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),c=!0},p:O,i(t){c||(b(n.$$.fragment,t),c=!0)},o(t){E(n.$$.fragment,t),c=!1},d(t){w(n,t)}}}function tW($){let n,c;return n=new P({props:{$$slots:{default:[eW]},$$scope:{ctx:$}}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),c=!0},p(t,d){const q={};d&2&&(q.$$scope={dirty:d,ctx:t}),n.$set(q)},i(t){c||(b(n.$$.fragment,t),c=!0)},o(t){E(n.$$.fragment,t),c=!1},d(t){w(n,t)}}}function sW($){let n,c,t,d,q,k,A;return{c(){n=r("p"),c=r("strong"),t=i("Recommended model"),d=i(`:
`),q=r("a"),k=i("facebook/detr-resnet-50"),A=i("."),this.h()},l(j){n=o(j,"P",{});var T=l(n);c=o(T,"STRONG",{});var N=l(c);t=u(N,"Recommended model"),N.forEach(s),d=u(T,`:
`),q=o(T,"A",{href:!0,rel:!0});var D=l(q);k=u(D,"facebook/detr-resnet-50"),D.forEach(s),A=u(T,"."),T.forEach(s),this.h()},h(){h(q,"href","https://huggingface.co/facebook/detr-resnet-50"),h(q,"rel","nofollow")},m(j,T){m(j,n,T),e(n,c),e(c,t),e(n,d),e(n,q),e(q,k),e(n,A)},d(j){j&&s(n)}}}function aW($){let n,c;return n=new R({props:{code:`import json
import requests
headers = {"Authorization": f"Bearer {API_TOKEN}"}
API_URL = "https://api-inference.huggingface.co/models/facebook/detr-resnet-50"
def query(filename):
    with open(filename, "rb") as f:
        data = f.read()
    response = requests.request("POST", API_URL, headers=headers, data=data)
    return json.loads(response.content.decode("utf-8"))
data = query("cats.jpg")`,highlighted:`<span class="hljs-keyword">import</span> json
<span class="hljs-keyword">import</span> requests
headers = {<span class="hljs-string">&quot;Authorization&quot;</span>: <span class="hljs-string">f&quot;Bearer <span class="hljs-subst">{API_TOKEN}</span>&quot;</span>}
API_URL = <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/facebook/detr-resnet-50&quot;</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">filename</span>):
    <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(filename, <span class="hljs-string">&quot;rb&quot;</span>) <span class="hljs-keyword">as</span> f:
        data = f.read()
    response = requests.request(<span class="hljs-string">&quot;POST&quot;</span>, API_URL, headers=headers, data=data)
    <span class="hljs-keyword">return</span> json.loads(response.content.decode(<span class="hljs-string">&quot;utf-8&quot;</span>))
data = query(<span class="hljs-string">&quot;cats.jpg&quot;</span>)`}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),c=!0},p:O,i(t){c||(b(n.$$.fragment,t),c=!0)},o(t){E(n.$$.fragment,t),c=!1},d(t){w(n,t)}}}function nW($){let n,c;return n=new P({props:{$$slots:{default:[aW]},$$scope:{ctx:$}}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),c=!0},p(t,d){const q={};d&2&&(q.$$scope={dirty:d,ctx:t}),n.$set(q)},i(t){c||(b(n.$$.fragment,t),c=!0)},o(t){E(n.$$.fragment,t),c=!1},d(t){w(n,t)}}}function rW($){let n,c;return n=new R({props:{code:`import fetch from "node-fetch";
import fs from "fs";
async function query(filename) {
    const data = fs.readFileSync(filename);
    const response = await fetch(
        "https://api-inference.huggingface.co/models/facebook/detr-resnet-50",
        {
            headers: { Authorization: \`Bearer \${API_TOKEN}\` },
            method: "POST",
            body: data,
        }
    );
    const result = await response.json();
    return result;
}
query("cats.jpg").then((response) => {
    console.log(JSON.stringify(response));
});
// [{"score":0.9982201457023621,"label":"remote","box":{"xmin":40,"ymin":70,"xmax":175,"ymax":117}},{"score":0.9960021376609802,"label":"remote","box":{"xmin":333,"ymin":72,"xmax":368,"ymax":187}},{"score":0.9954745173454285,"label":"couch","box":{"xmin":0,"ymin":1,"xmax":639,"ymax":473}},{"score":0.9988006353378296,"label":"cat","box":{"xmin":13,"ymin":52,"xmax":314,"ymax":470}},{"score":0.9986783862113953,"label":"cat","box":{"xmin":345,"ymin":23,"xmax":640,"ymax":368}}]`,highlighted:`<span class="hljs-keyword">import</span> <span class="hljs-keyword">fetch</span> <span class="hljs-keyword">from</span> &quot;node-fetch&quot;;
<span class="hljs-keyword">import</span> fs <span class="hljs-keyword">from</span> &quot;fs&quot;;
async <span class="hljs-keyword">function</span> query(filename) {
    const data = fs.readFileSync(filename);
    const response = await <span class="hljs-keyword">fetch</span>(
        &quot;https://api-inference.huggingface.co/models/facebook/detr-resnet-50&quot;,
        {
            headers: { <span class="hljs-keyword">Authorization</span>: \`Bearer \${API_TOKEN}\` },
            <span class="hljs-keyword">method</span>: &quot;POST&quot;,
            body: data,
        }
    );
    const result = await response.json();
    <span class="hljs-keyword">return</span> result;
}
query(&quot;cats.jpg&quot;).<span class="hljs-keyword">then</span>((response) =&gt; {
    console.log(<span class="hljs-type">JSON</span>.stringify(response));
});
// [{&quot;score&quot;:<span class="hljs-number">0.9982201457023621</span>,&quot;label&quot;:&quot;remote&quot;,&quot;box&quot;:{&quot;xmin&quot;:<span class="hljs-number">40</span>,&quot;ymin&quot;:<span class="hljs-number">70</span>,&quot;xmax&quot;:<span class="hljs-number">175</span>,&quot;ymax&quot;:<span class="hljs-number">117</span>}},{&quot;score&quot;:<span class="hljs-number">0.9960021376609802</span>,&quot;label&quot;:&quot;remote&quot;,&quot;box&quot;:{&quot;xmin&quot;:<span class="hljs-number">333</span>,&quot;ymin&quot;:<span class="hljs-number">72</span>,&quot;xmax&quot;:<span class="hljs-number">368</span>,&quot;ymax&quot;:<span class="hljs-number">187</span>}},{&quot;score&quot;:<span class="hljs-number">0.9954745173454285</span>,&quot;label&quot;:&quot;couch&quot;,&quot;box&quot;:{&quot;xmin&quot;:<span class="hljs-number">0</span>,&quot;ymin&quot;:<span class="hljs-number">1</span>,&quot;xmax&quot;:<span class="hljs-number">639</span>,&quot;ymax&quot;:<span class="hljs-number">473</span>}},{&quot;score&quot;:<span class="hljs-number">0.9988006353378296</span>,&quot;label&quot;:&quot;cat&quot;,&quot;box&quot;:{&quot;xmin&quot;:<span class="hljs-number">13</span>,&quot;ymin&quot;:<span class="hljs-number">52</span>,&quot;xmax&quot;:<span class="hljs-number">314</span>,&quot;ymax&quot;:<span class="hljs-number">470</span>}},{&quot;score&quot;:<span class="hljs-number">0.9986783862113953</span>,&quot;label&quot;:&quot;cat&quot;,&quot;box&quot;:{&quot;xmin&quot;:<span class="hljs-number">345</span>,&quot;ymin&quot;:<span class="hljs-number">23</span>,&quot;xmax&quot;:<span class="hljs-number">640</span>,&quot;ymax&quot;:<span class="hljs-number">368</span>}}]`}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),c=!0},p:O,i(t){c||(b(n.$$.fragment,t),c=!0)},o(t){E(n.$$.fragment,t),c=!1},d(t){w(n,t)}}}function oW($){let n,c;return n=new P({props:{$$slots:{default:[rW]},$$scope:{ctx:$}}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),c=!0},p(t,d){const q={};d&2&&(q.$$scope={dirty:d,ctx:t}),n.$set(q)},i(t){c||(b(n.$$.fragment,t),c=!0)},o(t){E(n.$$.fragment,t),c=!1},d(t){w(n,t)}}}function lW($){let n,c;return n=new R({props:{code:`curl https://api-inference.huggingface.co/models/facebook/detr-resnet-50 \\
        -X POST \\
        --data-binary '@cats.jpg' \\
        -H "Authorization: Bearer \${HF_API_TOKEN}"
# [{"score":0.9982201457023621,"label":"remote","box":{"xmin":40,"ymin":70,"xmax":175,"ymax":117}},{"score":0.9960021376609802,"label":"remote","box":{"xmin":333,"ymin":72,"xmax":368,"ymax":187}},{"score":0.9954745173454285,"label":"couch","box":{"xmin":0,"ymin":1,"xmax":639,"ymax":473}},{"score":0.9988006353378296,"label":"cat","box":{"xmin":13,"ymin":52,"xmax":314,"ymax":470}},{"score":0.9986783862113953,"label":"cat","box":{"xmin":345,"ymin":23,"xmax":640,"ymax":368}}]`,highlighted:`curl https://api-inference.huggingface.co/models/facebook/detr-resnet-50 \\
        -X POST \\
        --data-binary <span class="hljs-string">&#x27;@cats.jpg&#x27;</span> \\
        -H <span class="hljs-string">&quot;Authorization: Bearer <span class="hljs-variable">\${HF_API_TOKEN}</span>&quot;</span>
<span class="hljs-comment"># [{&quot;score&quot;:0.9982201457023621,&quot;label&quot;:&quot;remote&quot;,&quot;box&quot;:{&quot;xmin&quot;:40,&quot;ymin&quot;:70,&quot;xmax&quot;:175,&quot;ymax&quot;:117}},{&quot;score&quot;:0.9960021376609802,&quot;label&quot;:&quot;remote&quot;,&quot;box&quot;:{&quot;xmin&quot;:333,&quot;ymin&quot;:72,&quot;xmax&quot;:368,&quot;ymax&quot;:187}},{&quot;score&quot;:0.9954745173454285,&quot;label&quot;:&quot;couch&quot;,&quot;box&quot;:{&quot;xmin&quot;:0,&quot;ymin&quot;:1,&quot;xmax&quot;:639,&quot;ymax&quot;:473}},{&quot;score&quot;:0.9988006353378296,&quot;label&quot;:&quot;cat&quot;,&quot;box&quot;:{&quot;xmin&quot;:13,&quot;ymin&quot;:52,&quot;xmax&quot;:314,&quot;ymax&quot;:470}},{&quot;score&quot;:0.9986783862113953,&quot;label&quot;:&quot;cat&quot;,&quot;box&quot;:{&quot;xmin&quot;:345,&quot;ymin&quot;:23,&quot;xmax&quot;:640,&quot;ymax&quot;:368}}]</span>`}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),c=!0},p:O,i(t){c||(b(n.$$.fragment,t),c=!0)},o(t){E(n.$$.fragment,t),c=!1},d(t){w(n,t)}}}function iW($){let n,c;return n=new P({props:{$$slots:{default:[lW]},$$scope:{ctx:$}}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),c=!0},p(t,d){const q={};d&2&&(q.$$scope={dirty:d,ctx:t}),n.$set(q)},i(t){c||(b(n.$$.fragment,t),c=!0)},o(t){E(n.$$.fragment,t),c=!1},d(t){w(n,t)}}}function uW($){let n,c;return n=new R({props:{code:`self.assertEqual(
    deep_round(data, 4),
    [
        {
            "score": 0.9982,
            "label": "remote",
            "box": {"xmin": 40, "ymin": 70, "xmax": 175, "ymax": 117},
        },
        {
            "score": 0.9960,
            "label": "remote",
            "box": {"xmin": 333, "ymin": 72, "xmax": 368, "ymax": 187},
        },
        {
            "score": 0.9955,
            "label": "couch",
            "box": {"xmin": 0, "ymin": 1, "xmax": 639, "ymax": 473},
        },
        {
            "score": 0.9988,
            "label": "cat",
            "box": {"xmin": 13, "ymin": 52, "xmax": 314, "ymax": 470},
        },
        {
            "score": 0.9987,
            "label": "cat",
            "box": {"xmin": 345, "ymin": 23, "xmax": 640, "ymax": 368},
        },
    ],
)`,highlighted:`self.assertEqual(
    deep_round(data, <span class="hljs-number">4</span>),
    [
        {
            <span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.9982</span>,
            <span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;remote&quot;</span>,
            <span class="hljs-string">&quot;box&quot;</span>: {<span class="hljs-string">&quot;xmin&quot;</span>: <span class="hljs-number">40</span>, <span class="hljs-string">&quot;ymin&quot;</span>: <span class="hljs-number">70</span>, <span class="hljs-string">&quot;xmax&quot;</span>: <span class="hljs-number">175</span>, <span class="hljs-string">&quot;ymax&quot;</span>: <span class="hljs-number">117</span>},
        },
        {
            <span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.9960</span>,
            <span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;remote&quot;</span>,
            <span class="hljs-string">&quot;box&quot;</span>: {<span class="hljs-string">&quot;xmin&quot;</span>: <span class="hljs-number">333</span>, <span class="hljs-string">&quot;ymin&quot;</span>: <span class="hljs-number">72</span>, <span class="hljs-string">&quot;xmax&quot;</span>: <span class="hljs-number">368</span>, <span class="hljs-string">&quot;ymax&quot;</span>: <span class="hljs-number">187</span>},
        },
        {
            <span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.9955</span>,
            <span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;couch&quot;</span>,
            <span class="hljs-string">&quot;box&quot;</span>: {<span class="hljs-string">&quot;xmin&quot;</span>: <span class="hljs-number">0</span>, <span class="hljs-string">&quot;ymin&quot;</span>: <span class="hljs-number">1</span>, <span class="hljs-string">&quot;xmax&quot;</span>: <span class="hljs-number">639</span>, <span class="hljs-string">&quot;ymax&quot;</span>: <span class="hljs-number">473</span>},
        },
        {
            <span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.9988</span>,
            <span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;cat&quot;</span>,
            <span class="hljs-string">&quot;box&quot;</span>: {<span class="hljs-string">&quot;xmin&quot;</span>: <span class="hljs-number">13</span>, <span class="hljs-string">&quot;ymin&quot;</span>: <span class="hljs-number">52</span>, <span class="hljs-string">&quot;xmax&quot;</span>: <span class="hljs-number">314</span>, <span class="hljs-string">&quot;ymax&quot;</span>: <span class="hljs-number">470</span>},
        },
        {
            <span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.9987</span>,
            <span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;cat&quot;</span>,
            <span class="hljs-string">&quot;box&quot;</span>: {<span class="hljs-string">&quot;xmin&quot;</span>: <span class="hljs-number">345</span>, <span class="hljs-string">&quot;ymin&quot;</span>: <span class="hljs-number">23</span>, <span class="hljs-string">&quot;xmax&quot;</span>: <span class="hljs-number">640</span>, <span class="hljs-string">&quot;ymax&quot;</span>: <span class="hljs-number">368</span>},
        },
    ],
)`}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),c=!0},p:O,i(t){c||(b(n.$$.fragment,t),c=!0)},o(t){E(n.$$.fragment,t),c=!1},d(t){w(n,t)}}}function cW($){let n,c;return n=new P({props:{$$slots:{default:[uW]},$$scope:{ctx:$}}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),c=!0},p(t,d){const q={};d&2&&(q.$$scope={dirty:d,ctx:t}),n.$set(q)},i(t){c||(b(n.$$.fragment,t),c=!0)},o(t){E(n.$$.fragment,t),c=!1},d(t){w(n,t)}}}function fW($){let n,c,t,d,q,k,A;return{c(){n=r("p"),c=r("strong"),t=i("Recommended model"),d=i(`:
`),q=r("a"),k=i("facebook/detr-resnet-50-panoptic"),A=i("."),this.h()},l(j){n=o(j,"P",{});var T=l(n);c=o(T,"STRONG",{});var N=l(c);t=u(N,"Recommended model"),N.forEach(s),d=u(T,`:
`),q=o(T,"A",{href:!0,rel:!0});var D=l(q);k=u(D,"facebook/detr-resnet-50-panoptic"),D.forEach(s),A=u(T,"."),T.forEach(s),this.h()},h(){h(q,"href","https://huggingface.co/facebook/detr-resnet-50-panoptic"),h(q,"rel","nofollow")},m(j,T){m(j,n,T),e(n,c),e(c,t),e(n,d),e(n,q),e(q,k),e(n,A)},d(j){j&&s(n)}}}function pW($){let n,c;return n=new R({props:{code:`import json
import requests
headers = {"Authorization": f"Bearer {API_TOKEN}"}
API_URL = "https://api-inference.huggingface.co/models/facebook/detr-resnet-50-panoptic"
def query(filename):
    with open(filename, "rb") as f:
        data = f.read()
    response = requests.request("POST", API_URL, headers=headers, data=data)
    return json.loads(response.content.decode("utf-8"))
data = query("cats.jpg")`,highlighted:`<span class="hljs-keyword">import</span> json
<span class="hljs-keyword">import</span> requests
headers = {<span class="hljs-string">&quot;Authorization&quot;</span>: <span class="hljs-string">f&quot;Bearer <span class="hljs-subst">{API_TOKEN}</span>&quot;</span>}
API_URL = <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/facebook/detr-resnet-50-panoptic&quot;</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">filename</span>):
    <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(filename, <span class="hljs-string">&quot;rb&quot;</span>) <span class="hljs-keyword">as</span> f:
        data = f.read()
    response = requests.request(<span class="hljs-string">&quot;POST&quot;</span>, API_URL, headers=headers, data=data)
    <span class="hljs-keyword">return</span> json.loads(response.content.decode(<span class="hljs-string">&quot;utf-8&quot;</span>))
data = query(<span class="hljs-string">&quot;cats.jpg&quot;</span>)`}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),c=!0},p:O,i(t){c||(b(n.$$.fragment,t),c=!0)},o(t){E(n.$$.fragment,t),c=!1},d(t){w(n,t)}}}function hW($){let n,c;return n=new P({props:{$$slots:{default:[pW]},$$scope:{ctx:$}}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),c=!0},p(t,d){const q={};d&2&&(q.$$scope={dirty:d,ctx:t}),n.$set(q)},i(t){c||(b(n.$$.fragment,t),c=!0)},o(t){E(n.$$.fragment,t),c=!1},d(t){w(n,t)}}}function dW($){let n,c;return n=new R({props:{code:`import fetch from "node-fetch";
import fs from "fs";
async function query(filename) {
    const data = fs.readFileSync(filename);
    const response = await fetch(
        "https://api-inference.huggingface.co/models/facebook/detr-resnet-50-panoptic",
        {
            headers: { Authorization: \`Bearer \${API_TOKEN}\` },
            method: "POST",
            body: data,
        }
    );
    const result = await response.json();
    return result;
}
query("cats.jpg").then((response) => {
    console.log(JSON.stringify(response));
});
// [{"score": 0.9094282388687134, "label": "blanket", "mask": "ff7cb6da186cb65a657d5bf39964f02e"}, {"score": 0.9940965175628662, "label": "cat", "mask": "a4b62b30c4760fbb82a4c050486774b1"}, {"score": 0.9986692667007446, "label": "remote", "mask": "635b6c45f2dbf09f217ed73e496e04d6"}, {"score": 0.9994757771492004, "label": "remote", "mask": "3a9dd3459a7834fadf02f9150d6da859"}, {"score": 0.9722068309783936, "label": "couch", "mask": "d106ae88eb4069a45ac79d463c2e761f"}, {"score": 0.9994235038757324, "label": "cat", "mask": "c5c2f5fa5168dcb9f4e7713a54f33004"}]`,highlighted:`<span class="hljs-keyword">import</span> <span class="hljs-keyword">fetch</span> <span class="hljs-keyword">from</span> &quot;node-fetch&quot;;
<span class="hljs-keyword">import</span> fs <span class="hljs-keyword">from</span> &quot;fs&quot;;
async <span class="hljs-keyword">function</span> query(filename) {
    const data = fs.readFileSync(filename);
    const response = await <span class="hljs-keyword">fetch</span>(
        &quot;https://api-inference.huggingface.co/models/facebook/detr-resnet-50-panoptic&quot;,
        {
            headers: { <span class="hljs-keyword">Authorization</span>: \`Bearer \${API_TOKEN}\` },
            <span class="hljs-keyword">method</span>: &quot;POST&quot;,
            body: data,
        }
    );
    const result = await response.json();
    <span class="hljs-keyword">return</span> result;
}
query(&quot;cats.jpg&quot;).<span class="hljs-keyword">then</span>((response) =&gt; {
    console.log(<span class="hljs-type">JSON</span>.stringify(response));
});
// [{&quot;score&quot;: <span class="hljs-number">0.9094282388687134</span>, &quot;label&quot;: &quot;blanket&quot;, &quot;mask&quot;: &quot;ff7cb6da186cb65a657d5bf39964f02e&quot;}, {&quot;score&quot;: <span class="hljs-number">0.9940965175628662</span>, &quot;label&quot;: &quot;cat&quot;, &quot;mask&quot;: &quot;a4b62b30c4760fbb82a4c050486774b1&quot;}, {&quot;score&quot;: <span class="hljs-number">0.9986692667007446</span>, &quot;label&quot;: &quot;remote&quot;, &quot;mask&quot;: &quot;635b6c45f2dbf09f217ed73e496e04d6&quot;}, {&quot;score&quot;: <span class="hljs-number">0.9994757771492004</span>, &quot;label&quot;: &quot;remote&quot;, &quot;mask&quot;: &quot;3a9dd3459a7834fadf02f9150d6da859&quot;}, {&quot;score&quot;: <span class="hljs-number">0.9722068309783936</span>, &quot;label&quot;: &quot;couch&quot;, &quot;mask&quot;: &quot;d106ae88eb4069a45ac79d463c2e761f&quot;}, {&quot;score&quot;: <span class="hljs-number">0.9994235038757324</span>, &quot;label&quot;: &quot;cat&quot;, &quot;mask&quot;: &quot;c5c2f5fa5168dcb9f4e7713a54f33004&quot;}]`}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),c=!0},p:O,i(t){c||(b(n.$$.fragment,t),c=!0)},o(t){E(n.$$.fragment,t),c=!1},d(t){w(n,t)}}}function gW($){let n,c;return n=new P({props:{$$slots:{default:[dW]},$$scope:{ctx:$}}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),c=!0},p(t,d){const q={};d&2&&(q.$$scope={dirty:d,ctx:t}),n.$set(q)},i(t){c||(b(n.$$.fragment,t),c=!0)},o(t){E(n.$$.fragment,t),c=!1},d(t){w(n,t)}}}function mW($){let n,c;return n=new R({props:{code:`curl https://api-inference.huggingface.co/models/facebook/detr-resnet-50-panoptic \\
        -X POST \\
        --data-binary '@cats.jpg' \\
        -H "Authorization: Bearer \${HF_API_TOKEN}"
# [{"score": 0.9094282388687134, "label": "blanket", "mask": "ff7cb6da186cb65a657d5bf39964f02e"}, {"score": 0.9940965175628662, "label": "cat", "mask": "a4b62b30c4760fbb82a4c050486774b1"}, {"score": 0.9986692667007446, "label": "remote", "mask": "635b6c45f2dbf09f217ed73e496e04d6"}, {"score": 0.9994757771492004, "label": "remote", "mask": "3a9dd3459a7834fadf02f9150d6da859"}, {"score": 0.9722068309783936, "label": "couch", "mask": "d106ae88eb4069a45ac79d463c2e761f"}, {"score": 0.9994235038757324, "label": "cat", "mask": "c5c2f5fa5168dcb9f4e7713a54f33004"}]`,highlighted:`curl https://api-inference.huggingface.co/models/facebook/detr-resnet-50-panoptic \\
        -X POST \\
        --data-binary <span class="hljs-string">&#x27;@cats.jpg&#x27;</span> \\
        -H <span class="hljs-string">&quot;Authorization: Bearer <span class="hljs-variable">\${HF_API_TOKEN}</span>&quot;</span>
<span class="hljs-comment"># [{&quot;score&quot;: 0.9094282388687134, &quot;label&quot;: &quot;blanket&quot;, &quot;mask&quot;: &quot;ff7cb6da186cb65a657d5bf39964f02e&quot;}, {&quot;score&quot;: 0.9940965175628662, &quot;label&quot;: &quot;cat&quot;, &quot;mask&quot;: &quot;a4b62b30c4760fbb82a4c050486774b1&quot;}, {&quot;score&quot;: 0.9986692667007446, &quot;label&quot;: &quot;remote&quot;, &quot;mask&quot;: &quot;635b6c45f2dbf09f217ed73e496e04d6&quot;}, {&quot;score&quot;: 0.9994757771492004, &quot;label&quot;: &quot;remote&quot;, &quot;mask&quot;: &quot;3a9dd3459a7834fadf02f9150d6da859&quot;}, {&quot;score&quot;: 0.9722068309783936, &quot;label&quot;: &quot;couch&quot;, &quot;mask&quot;: &quot;d106ae88eb4069a45ac79d463c2e761f&quot;}, {&quot;score&quot;: 0.9994235038757324, &quot;label&quot;: &quot;cat&quot;, &quot;mask&quot;: &quot;c5c2f5fa5168dcb9f4e7713a54f33004&quot;}]</span>`}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),c=!0},p:O,i(t){c||(b(n.$$.fragment,t),c=!0)},o(t){E(n.$$.fragment,t),c=!1},d(t){w(n,t)}}}function qW($){let n,c;return n=new P({props:{$$slots:{default:[mW]},$$scope:{ctx:$}}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),c=!0},p(t,d){const q={};d&2&&(q.$$scope={dirty:d,ctx:t}),n.$set(q)},i(t){c||(b(n.$$.fragment,t),c=!0)},o(t){E(n.$$.fragment,t),c=!1},d(t){w(n,t)}}}function $W($){let n,c;return n=new R({props:{code:`self.assertEqual(
    deep_round(data, 4),
    [
        {"score": 0.9094, "label": "blanket", "mask": "ff7cb6da186cb65a657d5bf39964f02e"},
        {"score": 0.9941, "label": "cat", "mask": "a4b62b30c4760fbb82a4c050486774b1"},
        {"score": 0.9987, "label": "remote", "mask": "635b6c45f2dbf09f217ed73e496e04d6"},
        {"score": 0.9995, "label": "remote", "mask": "3a9dd3459a7834fadf02f9150d6da859"},
        {"score": 0.9722, "label": "couch", "mask": "d106ae88eb4069a45ac79d463c2e761f"},
        {"score": 0.9994, "label": "cat", "mask": "c5c2f5fa5168dcb9f4e7713a54f33004"},
    ],
)`,highlighted:`self.assertEqual(
    deep_round(data, <span class="hljs-number">4</span>),
    [
        {<span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.9094</span>, <span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;blanket&quot;</span>, <span class="hljs-string">&quot;mask&quot;</span>: <span class="hljs-string">&quot;ff7cb6da186cb65a657d5bf39964f02e&quot;</span>},
        {<span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.9941</span>, <span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;cat&quot;</span>, <span class="hljs-string">&quot;mask&quot;</span>: <span class="hljs-string">&quot;a4b62b30c4760fbb82a4c050486774b1&quot;</span>},
        {<span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.9987</span>, <span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;remote&quot;</span>, <span class="hljs-string">&quot;mask&quot;</span>: <span class="hljs-string">&quot;635b6c45f2dbf09f217ed73e496e04d6&quot;</span>},
        {<span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.9995</span>, <span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;remote&quot;</span>, <span class="hljs-string">&quot;mask&quot;</span>: <span class="hljs-string">&quot;3a9dd3459a7834fadf02f9150d6da859&quot;</span>},
        {<span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.9722</span>, <span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;couch&quot;</span>, <span class="hljs-string">&quot;mask&quot;</span>: <span class="hljs-string">&quot;d106ae88eb4069a45ac79d463c2e761f&quot;</span>},
        {<span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.9994</span>, <span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;cat&quot;</span>, <span class="hljs-string">&quot;mask&quot;</span>: <span class="hljs-string">&quot;c5c2f5fa5168dcb9f4e7713a54f33004&quot;</span>},
    ],
)`}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),c=!0},p:O,i(t){c||(b(n.$$.fragment,t),c=!0)},o(t){E(n.$$.fragment,t),c=!1},d(t){w(n,t)}}}function _W($){let n,c;return n=new P({props:{$$slots:{default:[$W]},$$scope:{ctx:$}}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),c=!0},p(t,d){const q={};d&2&&(q.$$scope={dirty:d,ctx:t}),n.$set(q)},i(t){c||(b(n.$$.fragment,t),c=!0)},o(t){E(n.$$.fragment,t),c=!1},d(t){w(n,t)}}}function vW($){let n,c,t,d,q,k,A;return{c(){n=r("p"),c=r("strong"),t=i("Recommended model"),d=i(`:
`),q=r("a"),k=i("google/vit-base-patch16-224"),A=i("."),this.h()},l(j){n=o(j,"P",{});var T=l(n);c=o(T,"STRONG",{});var N=l(c);t=u(N,"Recommended model"),N.forEach(s),d=u(T,`:
`),q=o(T,"A",{href:!0,rel:!0});var D=l(q);k=u(D,"google/vit-base-patch16-224"),D.forEach(s),A=u(T,"."),T.forEach(s),this.h()},h(){h(q,"href","https://huggingface.co/google/vit-base-patch16-224"),h(q,"rel","nofollow")},m(j,T){m(j,n,T),e(n,c),e(c,t),e(n,d),e(n,q),e(q,k),e(n,A)},d(j){j&&s(n)}}}function yW($){let n,c;return n=new R({props:{code:`import json
import requests
headers = {"Authorization": f"Bearer {API_TOKEN}"}
API_URL = "https://api-inference.huggingface.co/models/google/vit-base-patch16-224"
def query(filename):
    with open(filename, "rb") as f:
        data = f.read()
    response = requests.request("POST", API_URL, headers=headers, data=data)
    return json.loads(response.content.decode("utf-8"))
data = query("cats.jpg")`,highlighted:`<span class="hljs-keyword">import</span> json
<span class="hljs-keyword">import</span> requests
headers = {<span class="hljs-string">&quot;Authorization&quot;</span>: <span class="hljs-string">f&quot;Bearer <span class="hljs-subst">{API_TOKEN}</span>&quot;</span>}
API_URL = <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/google/vit-base-patch16-224&quot;</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">filename</span>):
    <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(filename, <span class="hljs-string">&quot;rb&quot;</span>) <span class="hljs-keyword">as</span> f:
        data = f.read()
    response = requests.request(<span class="hljs-string">&quot;POST&quot;</span>, API_URL, headers=headers, data=data)
    <span class="hljs-keyword">return</span> json.loads(response.content.decode(<span class="hljs-string">&quot;utf-8&quot;</span>))
data = query(<span class="hljs-string">&quot;cats.jpg&quot;</span>)`}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),c=!0},p:O,i(t){c||(b(n.$$.fragment,t),c=!0)},o(t){E(n.$$.fragment,t),c=!1},d(t){w(n,t)}}}function bW($){let n,c;return n=new P({props:{$$slots:{default:[yW]},$$scope:{ctx:$}}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),c=!0},p(t,d){const q={};d&2&&(q.$$scope={dirty:d,ctx:t}),n.$set(q)},i(t){c||(b(n.$$.fragment,t),c=!0)},o(t){E(n.$$.fragment,t),c=!1},d(t){w(n,t)}}}function EW($){let n,c;return n=new R({props:{code:`import fetch from "node-fetch";
import fs from "fs";
async function query(filename) {
    const data = fs.readFileSync(filename);
    const response = await fetch(
        "https://api-inference.huggingface.co/models/google/vit-base-patch16-224",
        {
            headers: { Authorization: \`Bearer \${API_TOKEN}\` },
            method: "POST",
            body: data,
        }
    );
    const result = await response.json();
    return result;
}
query("cats.jpg").then((response) => {
    console.log(JSON.stringify(response));
});
// [{"score":0.9374412894248962,"label":"Egyptian cat"},{"score":0.03844260051846504,"label":"tabby, tabby cat"},{"score":0.014411412179470062,"label":"tiger cat"},{"score":0.003274323185905814,"label":"lynx, catamount"},{"score":0.0006795919616706669,"label":"Siamese cat, Siamese"}]`,highlighted:`<span class="hljs-keyword">import</span> <span class="hljs-keyword">fetch</span> <span class="hljs-keyword">from</span> &quot;node-fetch&quot;;
<span class="hljs-keyword">import</span> fs <span class="hljs-keyword">from</span> &quot;fs&quot;;
async <span class="hljs-keyword">function</span> query(filename) {
    const data = fs.readFileSync(filename);
    const response = await <span class="hljs-keyword">fetch</span>(
        &quot;https://api-inference.huggingface.co/models/google/vit-base-patch16-224&quot;,
        {
            headers: { <span class="hljs-keyword">Authorization</span>: \`Bearer \${API_TOKEN}\` },
            <span class="hljs-keyword">method</span>: &quot;POST&quot;,
            body: data,
        }
    );
    const result = await response.json();
    <span class="hljs-keyword">return</span> result;
}
query(&quot;cats.jpg&quot;).<span class="hljs-keyword">then</span>((response) =&gt; {
    console.log(<span class="hljs-type">JSON</span>.stringify(response));
});
// [{&quot;score&quot;:<span class="hljs-number">0.9374412894248962</span>,&quot;label&quot;:&quot;Egyptian cat&quot;},{&quot;score&quot;:<span class="hljs-number">0.03844260051846504</span>,&quot;label&quot;:&quot;tabby, tabby cat&quot;},{&quot;score&quot;:<span class="hljs-number">0.014411412179470062</span>,&quot;label&quot;:&quot;tiger cat&quot;},{&quot;score&quot;:<span class="hljs-number">0.003274323185905814</span>,&quot;label&quot;:&quot;lynx, catamount&quot;},{&quot;score&quot;:<span class="hljs-number">0.0006795919616706669</span>,&quot;label&quot;:&quot;Siamese cat, Siamese&quot;}]`}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),c=!0},p:O,i(t){c||(b(n.$$.fragment,t),c=!0)},o(t){E(n.$$.fragment,t),c=!1},d(t){w(n,t)}}}function wW($){let n,c;return n=new P({props:{$$slots:{default:[EW]},$$scope:{ctx:$}}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),c=!0},p(t,d){const q={};d&2&&(q.$$scope={dirty:d,ctx:t}),n.$set(q)},i(t){c||(b(n.$$.fragment,t),c=!0)},o(t){E(n.$$.fragment,t),c=!1},d(t){w(n,t)}}}function TW($){let n,c;return n=new R({props:{code:`curl https://api-inference.huggingface.co/models/google/vit-base-patch16-224 \\
        -X POST \\
        --data-binary '@cats.jpg' \\
        -H "Authorization: Bearer \${HF_API_TOKEN}"
# [{"score":0.9374412894248962,"label":"Egyptian cat"},{"score":0.03844260051846504,"label":"tabby, tabby cat"},{"score":0.014411412179470062,"label":"tiger cat"},{"score":0.003274323185905814,"label":"lynx, catamount"},{"score":0.0006795919616706669,"label":"Siamese cat, Siamese"}]`,highlighted:`curl https://api-inference.huggingface.co/models/google/vit-base-patch16-224 \\
        -X POST \\
        --data-binary <span class="hljs-string">&#x27;@cats.jpg&#x27;</span> \\
        -H <span class="hljs-string">&quot;Authorization: Bearer <span class="hljs-variable">\${HF_API_TOKEN}</span>&quot;</span>
<span class="hljs-comment"># [{&quot;score&quot;:0.9374412894248962,&quot;label&quot;:&quot;Egyptian cat&quot;},{&quot;score&quot;:0.03844260051846504,&quot;label&quot;:&quot;tabby, tabby cat&quot;},{&quot;score&quot;:0.014411412179470062,&quot;label&quot;:&quot;tiger cat&quot;},{&quot;score&quot;:0.003274323185905814,&quot;label&quot;:&quot;lynx, catamount&quot;},{&quot;score&quot;:0.0006795919616706669,&quot;label&quot;:&quot;Siamese cat, Siamese&quot;}]</span>`}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),c=!0},p:O,i(t){c||(b(n.$$.fragment,t),c=!0)},o(t){E(n.$$.fragment,t),c=!1},d(t){w(n,t)}}}function jW($){let n,c;return n=new P({props:{$$slots:{default:[TW]},$$scope:{ctx:$}}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),c=!0},p(t,d){const q={};d&2&&(q.$$scope={dirty:d,ctx:t}),n.$set(q)},i(t){c||(b(n.$$.fragment,t),c=!0)},o(t){E(n.$$.fragment,t),c=!1},d(t){w(n,t)}}}function kW($){let n,c;return n=new R({props:{code:`self.assertEqual(
    deep_round(data, 4),
    [
        {"score": 0.9374, "label": "Egyptian cat"},
        {"score": 0.0384, "label": "tabby, tabby cat"},
        {"score": 0.0144, "label": "tiger cat"},
        {"score": 0.0033, "label": "lynx, catamount"},
        {"score": 0.0007, "label": "Siamese cat, Siamese"},
    ],
)`,highlighted:`self.assertEqual(
    deep_round(data, <span class="hljs-number">4</span>),
    [
        {<span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.9374</span>, <span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;Egyptian cat&quot;</span>},
        {<span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.0384</span>, <span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;tabby, tabby cat&quot;</span>},
        {<span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.0144</span>, <span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;tiger cat&quot;</span>},
        {<span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.0033</span>, <span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;lynx, catamount&quot;</span>},
        {<span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.0007</span>, <span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;Siamese cat, Siamese&quot;</span>},
    ],
)`}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),c=!0},p:O,i(t){c||(b(n.$$.fragment,t),c=!0)},o(t){E(n.$$.fragment,t),c=!1},d(t){w(n,t)}}}function AW($){let n,c;return n=new P({props:{$$slots:{default:[kW]},$$scope:{ctx:$}}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),c=!0},p(t,d){const q={};d&2&&(q.$$scope={dirty:d,ctx:t}),n.$set(q)},i(t){c||(b(n.$$.fragment,t),c=!0)},o(t){E(n.$$.fragment,t),c=!1},d(t){w(n,t)}}}function DW($){let n,c,t,d,q,k,A,j,T,N,D,ne,Re,Q,Y,ot,Ri,Va,Ne,Fw,L_,Ni,Kw,U_,lt,Zx,z_,it,eI,M_,Se,ut,cd,Xa,Jw,fd,Ww,F_,Si,Yw,K_,ct,J_,Qa,Vw,Za,Xw,W_,xi,Qw,Y_,ft,V_,Ii,Zw,X_,pt,pd,en,Hi,e3,t3,hd,s3,M,tn,sn,dd,a3,n3,r3,Bi,o3,l3,an,nn,gd,i3,u3,c3,Ci,f3,p3,rn,Gi,h3,d3,he,g3,md,m3,q3,qd,$3,_3,v3,on,Li,y3,b3,ht,E3,$d,w3,T3,j3,ln,Ui,_d,k3,A3,zi,D3,O3,un,Mi,P3,R3,dt,N3,vd,S3,x3,I3,cn,Fi,H3,B3,gt,C3,yd,G3,L3,U3,fn,Ki,z3,M3,mt,F3,bd,K3,J3,Q_,Ji,W3,Z_,Wi,Y3,e1,qt,t1,$t,Ed,pn,Yi,V3,X3,wd,Q3,xe,hn,Vi,Td,Z3,eT,Xi,tT,sT,dn,Qi,jd,aT,nT,Zi,rT,oT,gn,eu,kd,lT,iT,_t,uT,Ad,cT,fT,s1,Ie,vt,Dd,mn,pT,Od,hT,a1,tu,dT,n1,yt,r1,qn,gT,$n,mT,o1,su,qT,l1,bt,i1,au,$T,u1,Et,Pd,_n,nu,_T,vT,Rd,yT,Z,vn,yn,Nd,bT,ET,wT,ru,TT,jT,bn,ou,Sd,kT,AT,lu,DT,OT,En,iu,PT,RT,wt,NT,xd,ST,xT,IT,wn,uu,HT,BT,Tt,CT,Id,GT,LT,UT,Tn,cu,zT,MT,jt,FT,Hd,KT,JT,c1,fu,WT,f1,kt,Bd,jn,pu,YT,VT,Cd,XT,Gd,kn,hu,Ld,QT,ZT,du,e4,p1,He,At,Ud,An,t4,zd,s4,h1,Dt,a4,gu,n4,r4,d1,Ot,g1,Dn,o4,On,l4,m1,mu,i4,q1,Pt,$1,qu,u4,_1,Rt,Md,Pn,$u,c4,f4,Fd,p4,G,Rn,Nn,Kd,h4,d4,g4,_u,m4,q4,Sn,vu,Jd,$4,_4,yu,v4,y4,xn,bu,b4,E4,de,w4,Wd,T4,j4,Yd,k4,A4,D4,In,Eu,O4,P4,ge,R4,Vd,N4,S4,Xd,x4,I4,H4,Hn,wu,B4,C4,me,G4,Qd,L4,U4,Zd,z4,M4,F4,Bn,Tu,K4,J4,re,W4,eg,Y4,V4,tg,X4,Q4,sg,Z4,e5,t5,Cn,ju,s5,a5,qe,n5,ag,r5,o5,ng,l5,i5,u5,Gn,ku,c5,f5,Nt,p5,rg,h5,d5,g5,Ln,Au,m5,q5,St,$5,og,_5,v5,y5,Un,Du,lg,b5,E5,Ou,w5,T5,zn,Pu,j5,k5,xt,A5,ig,D5,O5,P5,Mn,Ru,R5,N5,It,S5,ug,x5,I5,H5,Fn,Nu,B5,C5,Ht,G5,cg,L5,U5,v1,Su,z5,y1,Bt,fg,Kn,xu,M5,F5,pg,K5,hg,Jn,Iu,dg,J5,W5,Hu,Y5,b1,Be,Ct,gg,Wn,V5,mg,X5,E1,Bu,Q5,w1,Gt,T1,Yn,Z5,Vn,ej,j1,Cu,tj,k1,Lt,A1,Gu,sj,D1,Ut,qg,Xn,Lu,aj,nj,$g,rj,S,Qn,Zn,_g,oj,lj,ij,vg,uj,er,Uu,cj,fj,zu,pj,hj,tr,Mu,dj,gj,Fu,mj,qj,sr,Ku,$j,_j,zt,vj,yg,yj,bj,Ej,ar,Ju,bg,wj,Tj,Wu,jj,kj,nr,Yu,Aj,Dj,$e,Oj,Eg,Pj,Rj,wg,Nj,Sj,xj,rr,Vu,Ij,Hj,_e,Bj,Tg,Cj,Gj,jg,Lj,Uj,zj,or,Xu,Mj,Fj,ve,Kj,kg,Jj,Wj,Ag,Yj,Vj,Xj,lr,Qu,Qj,Zj,oe,e6,Dg,t6,s6,Og,a6,n6,Pg,r6,o6,l6,ir,Zu,i6,u6,ye,c6,Rg,f6,p6,Ng,h6,d6,g6,ur,ec,m6,q6,Mt,$6,Sg,_6,v6,y6,cr,tc,b6,E6,Ft,w6,xg,T6,j6,k6,fr,sc,Ig,A6,D6,ac,O6,P6,pr,nc,R6,N6,Kt,S6,Hg,x6,I6,H6,hr,rc,B6,C6,Jt,G6,Bg,L6,U6,z6,dr,oc,M6,F6,Wt,K6,Cg,J6,W6,O1,lc,Y6,P1,Yt,Gg,gr,ic,V6,X6,Lg,Q6,ie,mr,uc,Ug,Z6,ek,cc,tk,sk,qr,fc,zg,ak,nk,pc,rk,ok,$r,hc,lk,ik,dc,uk,ck,_r,gc,fk,pk,mc,hk,R1,Ce,Vt,Mg,vr,dk,Fg,gk,N1,qc,mk,S1,Xt,x1,yr,qk,br,$k,I1,$c,_k,H1,Qt,B1,_c,vk,C1,Zt,Kg,Er,vc,yk,bk,Jg,Ek,K,wr,Tr,Wg,wk,Tk,jk,Yg,kk,jr,yc,Ak,Dk,bc,Ok,Pk,kr,Ec,Rk,Nk,wc,Sk,xk,Ar,Tc,Vg,Ik,Hk,jc,Bk,Ck,Dr,kc,Gk,Lk,es,Uk,Xg,zk,Mk,Fk,Or,Ac,Kk,Jk,ts,Wk,Qg,Yk,Vk,Xk,Pr,Dc,Qk,Zk,ss,e7,Zg,t7,s7,G1,Oc,a7,L1,as,U1,ns,em,Rr,Pc,n7,r7,tm,o7,ue,Nr,Rc,sm,l7,i7,Nc,u7,c7,Sr,Sc,am,f7,p7,xc,h7,d7,xr,Ic,nm,g7,m7,Hc,q7,$7,Ir,Bc,rm,_7,v7,Cc,y7,z1,Ge,rs,om,Hr,b7,lm,E7,M1,Gc,w7,F1,os,K1,Le,T7,Br,j7,k7,Cr,A7,J1,Lc,D7,W1,ls,Y1,Uc,O7,V1,zc,P7,X1,is,Q1,us,im,Gr,Mc,R7,N7,um,S7,ce,Lr,Fc,cm,x7,I7,Kc,H7,B7,Ur,Jc,fm,C7,G7,Wc,L7,U7,zr,Yc,pm,z7,M7,cs,F7,hm,K7,J7,W7,Mr,Vc,dm,Y7,V7,fs,X7,gm,Q7,Z7,Z1,Ue,ps,mm,Fr,e9,qm,t9,e2,Xc,s9,t2,hs,s2,Kr,a9,Jr,n9,a2,Qc,r9,n2,ds,r2,Zc,o9,o2,gs,$m,Wr,ef,l9,i9,_m,u9,ee,Yr,Vr,vm,c9,f9,p9,tf,h9,d9,Xr,sf,ym,g9,m9,af,q9,$9,Qr,nf,_9,v9,ms,y9,bm,b9,E9,w9,Zr,rf,T9,j9,qs,k9,Em,A9,D9,O9,eo,of,P9,R9,$s,N9,wm,S9,x9,l2,lf,I9,i2,_s,u2,vs,Tm,to,uf,H9,B9,jm,C9,so,ao,cf,km,G9,L9,ff,U9,z9,no,pf,Am,M9,F9,hf,K9,c2,ze,ys,Dm,ro,J9,Om,W9,f2,oo,Y9,df,V9,p2,Me,bs,Pm,lo,X9,Rm,Q9,h2,gf,Z9,d2,Es,g2,Fe,e8,io,t8,s8,uo,a8,m2,mf,n8,q2,ws,$2,qf,r8,_2,Ts,Nm,co,$f,o8,l8,Sm,i8,J,fo,po,xm,u8,c8,f8,_f,p8,h8,ho,vf,Im,d8,g8,yf,m8,q8,go,bf,$8,_8,x,v8,Hm,y8,b8,E8,w8,Bm,T8,j8,k8,A8,Cm,D8,O8,P8,R8,Gm,N8,S8,Lm,x8,I8,H8,B8,Um,C8,G8,zm,L8,U8,z8,M8,Mm,F8,K8,Fm,J8,W8,Y8,mo,Ef,Km,V8,X8,wf,Q8,Z8,qo,Tf,eA,tA,js,sA,Jm,aA,nA,rA,$o,jf,oA,lA,ks,iA,Wm,uA,cA,fA,_o,kf,pA,hA,As,dA,Ym,gA,mA,v2,Af,qA,y2,Ds,b2,Os,Vm,vo,Df,$A,_A,Xm,vA,te,yo,Of,Qm,yA,bA,Pf,EA,wA,bo,Rf,Zm,TA,jA,Nf,kA,AA,Eo,Sf,eq,DA,OA,xf,PA,RA,wo,If,tq,NA,SA,Ps,xA,sq,IA,HA,BA,To,Hf,aq,CA,GA,Rs,LA,nq,UA,zA,E2,Ke,Ns,rq,jo,MA,oq,FA,w2,Bf,KA,T2,Ss,j2,ko,JA,Ao,WA,k2,Cf,YA,A2,xs,D2,Gf,VA,O2,Is,lq,Do,Lf,XA,QA,iq,ZA,I,Oo,Po,uq,eD,tD,sD,Uf,aD,nD,Ro,zf,cq,rD,oD,Mf,lD,iD,No,Ff,uD,cD,be,fD,fq,pD,hD,pq,dD,gD,mD,So,Kf,qD,$D,le,_D,hq,vD,yD,dq,bD,ED,gq,wD,TD,jD,xo,Jf,kD,AD,Ee,DD,mq,OD,PD,qq,RD,ND,SD,Io,Wf,xD,ID,Hs,HD,$q,BD,CD,GD,Ho,Yf,LD,UD,we,zD,_q,MD,FD,vq,KD,JD,WD,Bo,Vf,YD,VD,Te,XD,yq,QD,ZD,bq,eO,tO,sO,Co,Xf,aO,nO,je,rO,Eq,oO,lO,wq,iO,uO,cO,Go,Qf,fO,pO,Bs,hO,Tq,dO,gO,mO,Lo,Zf,qO,$O,Cs,_O,jq,vO,yO,bO,Uo,ep,kq,EO,wO,tp,TO,jO,zo,sp,kO,AO,Gs,DO,Aq,OO,PO,RO,Mo,ap,NO,SO,Ls,xO,Dq,IO,HO,BO,Fo,np,CO,GO,Us,LO,Oq,UO,zO,P2,rp,MO,R2,zs,N2,Ms,Pq,Ko,op,FO,KO,Rq,JO,Nq,Jo,lp,Sq,WO,YO,ip,VO,S2,Je,Fs,xq,Wo,XO,Iq,QO,x2,Ks,ZO,up,eP,tP,I2,We,Js,Hq,Yo,sP,Bq,aP,H2,cp,nP,B2,Ws,C2,Vo,rP,Xo,oP,G2,fp,lP,L2,Ys,U2,pp,iP,z2,Vs,Cq,Qo,hp,uP,cP,Gq,fP,se,Zo,el,Lq,pP,hP,dP,dp,gP,mP,tl,gp,Uq,qP,$P,mp,_P,vP,sl,qp,yP,bP,Xs,EP,zq,wP,TP,jP,al,$p,kP,AP,Qs,DP,Mq,OP,PP,RP,nl,_p,NP,SP,Zs,xP,Fq,IP,HP,M2,vp,BP,F2,ea,K2,ta,Kq,rl,yp,CP,GP,Jq,LP,fe,ol,bp,Wq,UP,zP,Ep,MP,FP,ll,wp,Yq,KP,JP,Tp,WP,YP,il,jp,Vq,VP,XP,kp,QP,ZP,ul,Ap,Xq,eR,tR,Dp,sR,J2,Ye,sa,Qq,cl,aR,Zq,nR,W2,Op,rR,Y2,aa,V2,na,X2,pe,oR,fl,lR,iR,pl,uR,cR,hl,fR,Q2,Pp,pR,Z2,ra,ev,Rp,hR,tv,oa,e$,dl,Np,dR,gR,t$,mR,s$,gl,ml,a$,qR,$R,_R,Sp,vR,sv,xp,yR,av,Ip,bR,nv,la,rv,ia,n$,ql,Hp,ER,wR,r$,TR,o$,$l,Bp,l$,jR,kR,Cp,AR,ov,Ve,ua,i$,_l,DR,u$,OR,lv,Gp,PR,iv,ca,uv,Xe,RR,vl,NR,SR,yl,xR,cv,Lp,IR,fv,fa,c$,bl,Up,HR,BR,f$,CR,ae,El,wl,p$,GR,LR,UR,zp,zR,MR,Tl,Mp,h$,FR,KR,Fp,JR,WR,jl,Kp,YR,VR,pa,XR,d$,QR,ZR,eN,kl,Jp,tN,sN,ha,aN,g$,nN,rN,oN,Al,Wp,lN,iN,da,uN,m$,cN,fN,pv,Yp,pN,hv,ga,q$,Dl,Vp,hN,dN,$$,gN,_$,Ol,Xp,v$,mN,qN,Qp,$N,dv,Zp,_N,gv,Qe,ma,y$,Pl,vN,b$,yN,mv,eh,bN,qv,qa,$v,Ze,EN,Rl,wN,TN,Nl,jN,_v,th,kN,vv,$a,yv,sh,AN,bv,_a,E$,Sl,ah,DN,ON,w$,PN,T$,xl,Il,j$,RN,NN,SN,nh,xN,Ev,rh,IN,wv,va,Tv,ya,k$,Hl,oh,HN,BN,A$,CN,Bl,Cl,lh,D$,GN,LN,ih,UN,zN,Gl,uh,O$,MN,FN,ch,KN,jv,et,ba,P$,Ll,JN,R$,WN,kv,fh,YN,Av,Ea,Dv,Ul,VN,zl,XN,Ov,ph,QN,Pv,wa,Rv,Ta,ZN,Ml,eS,tS,Nv,ja,N$,Fl,hh,sS,aS,S$,nS,x$,Kl,Jl,I$,rS,oS,lS,dh,iS,Sv,gh,uS,xv,ka,Iv,Aa,H$,Wl,mh,cS,fS,B$,pS,tt,Yl,qh,C$,hS,dS,$h,gS,mS,Vl,_h,G$,qS,$S,vh,_S,vS,Xl,yh,L$,yS,bS,bh,ES,Hv,st,Da,U$,Ql,wS,z$,TS,Bv,Eh,jS,Cv,Oa,Gv,Zl,kS,ei,AS,Lv,wh,DS,Uv,Pa,zv,Ra,OS,ti,PS,RS,Mv,Na,M$,si,Th,NS,SS,F$,xS,K$,ai,ni,J$,IS,HS,BS,jh,CS,Fv,kh,GS,Kv,Sa,Jv,xa,W$,ri,Ah,LS,US,Y$,zS,at,oi,Dh,V$,MS,FS,Oh,KS,JS,li,Ph,X$,WS,YS,Rh,VS,XS,ii,Nh,Q$,QS,ZS,Sh,ex,Wv,nt,Ia,Z$,ui,tx,e_,sx,Yv,xh,ax,Vv,Ha,Xv,ci,nx,fi,rx,Qv,Ih,ox,Zv,Ba,e0,Ca,lx,pi,ix,ux,t0,Ga,t_,hi,Hh,cx,fx,s_,px,a_,di,gi,n_,hx,dx,gx,Bh,mx,s0,Ch,qx,a0,La,n0,Ua,r_,mi,Gh,$x,_x,o_,vx,qi,$i,Lh,l_,yx,bx,Uh,Ex,wx,_i,zh,i_,Tx,jx,Mh,kx,r0;return k=new z({}),Q=new z({}),Xa=new z({}),ct=new W({props:{$$slots:{default:[oK]},$$scope:{ctx:$}}}),ft=new L({props:{python:!0,js:!0,curl:!0,$$slots:{curl:[pK],js:[cK],python:[iK]},$$scope:{ctx:$}}}),qt=new L({props:{python:!0,js:!0,curl:!0,$$slots:{python:[dK]},$$scope:{ctx:$}}}),mn=new z({}),yt=new W({props:{$$slots:{default:[gK]},$$scope:{ctx:$}}}),bt=new L({props:{python:!0,js:!0,curl:!0,$$slots:{curl:[yK],js:[_K],python:[qK]},$$scope:{ctx:$}}}),An=new z({}),Ot=new W({props:{$$slots:{default:[bK]},$$scope:{ctx:$}}}),Pt=new L({props:{python:!0,js:!0,curl:!0,$$slots:{curl:[AK],js:[jK],python:[wK]},$$scope:{ctx:$}}}),Wn=new z({}),Gt=new W({props:{$$slots:{default:[DK]},$$scope:{ctx:$}}}),Lt=new L({props:{python:!0,js:!0,curl:!0,$$slots:{curl:[xK],js:[NK],python:[PK]},$$scope:{ctx:$}}}),vr=new z({}),Xt=new W({props:{$$slots:{default:[IK]},$$scope:{ctx:$}}}),Qt=new L({props:{python:!0,js:!0,curl:!0,$$slots:{curl:[UK],js:[GK],python:[BK]},$$scope:{ctx:$}}}),as=new L({props:{python:!0,js:!0,curl:!0,$$slots:{python:[MK]},$$scope:{ctx:$}}}),Hr=new z({}),os=new W({props:{$$slots:{default:[FK]},$$scope:{ctx:$}}}),ls=new L({props:{python:!0,js:!0,curl:!0,$$slots:{curl:[XK],js:[YK],python:[JK]},$$scope:{ctx:$}}}),is=new L({props:{python:!0,js:!0,curl:!0,$$slots:{python:[ZK]},$$scope:{ctx:$}}}),Fr=new z({}),hs=new W({props:{$$slots:{default:[eJ]},$$scope:{ctx:$}}}),ds=new L({props:{python:!0,js:!0,curl:!0,$$slots:{curl:[oJ],js:[nJ],python:[sJ]},$$scope:{ctx:$}}}),_s=new L({props:{python:!0,js:!0,curl:!0,$$slots:{python:[iJ]},$$scope:{ctx:$}}}),ro=new z({}),lo=new z({}),Es=new W({props:{$$slots:{default:[uJ]},$$scope:{ctx:$}}}),ws=new L({props:{python:!0,js:!0,curl:!0,$$slots:{curl:[gJ],js:[hJ],python:[fJ]},$$scope:{ctx:$}}}),Ds=new L({props:{python:!0,js:!0,curl:!0,$$slots:{python:[qJ]},$$scope:{ctx:$}}}),jo=new z({}),Ss=new W({props:{$$slots:{default:[$J]},$$scope:{ctx:$}}}),xs=new L({props:{python:!0,js:!0,curl:!0,$$slots:{curl:[wJ],js:[bJ],python:[vJ]},$$scope:{ctx:$}}}),zs=new L({props:{python:!0,js:!0,curl:!0,$$slots:{python:[jJ]},$$scope:{ctx:$}}}),Wo=new z({}),Yo=new z({}),Ws=new W({props:{$$slots:{default:[kJ]},$$scope:{ctx:$}}}),Ys=new L({props:{python:!0,js:!0,curl:!0,$$slots:{curl:[NJ],js:[PJ],python:[DJ]},$$scope:{ctx:$}}}),ea=new L({props:{python:!0,js:!0,curl:!0,$$slots:{python:[xJ]},$$scope:{ctx:$}}}),cl=new z({}),aa=new W({props:{$$slots:{default:[IJ]},$$scope:{ctx:$}}}),na=new W({props:{$$slots:{default:[HJ]},$$scope:{ctx:$}}}),ra=new L({props:{python:!0,js:!0,curl:!0,$$slots:{curl:[zJ],js:[LJ],python:[CJ]},$$scope:{ctx:$}}}),la=new L({props:{python:!0,js:!0,curl:!0,$$slots:{python:[FJ]},$$scope:{ctx:$}}}),_l=new z({}),ca=new W({props:{$$slots:{default:[KJ]},$$scope:{ctx:$}}}),Pl=new z({}),qa=new W({props:{$$slots:{default:[JJ]},$$scope:{ctx:$}}}),$a=new L({props:{python:!0,js:!0,curl:!0,$$slots:{curl:[ZJ],js:[XJ],python:[YJ]},$$scope:{ctx:$}}}),va=new L({props:{python:!0,js:!0,curl:!0,$$slots:{python:[tW]},$$scope:{ctx:$}}}),Ll=new z({}),Ea=new W({props:{$$slots:{default:[sW]},$$scope:{ctx:$}}}),wa=new L({props:{python:!0,js:!0,curl:!0,$$slots:{curl:[iW],js:[oW],python:[nW]},$$scope:{ctx:$}}}),ka=new L({props:{python:!0,js:!0,curl:!0,$$slots:{python:[cW]},$$scope:{ctx:$}}}),Ql=new z({}),Oa=new W({props:{$$slots:{default:[fW]},$$scope:{ctx:$}}}),Pa=new L({props:{python:!0,js:!0,curl:!0,$$slots:{curl:[qW],js:[gW],python:[hW]},$$scope:{ctx:$}}}),Sa=new L({props:{python:!0,js:!0,curl:!0,$$slots:{python:[_W]},$$scope:{ctx:$}}}),ui=new z({}),Ha=new W({props:{$$slots:{default:[vW]},$$scope:{ctx:$}}}),Ba=new L({props:{python:!0,js:!0,curl:!0,$$slots:{curl:[jW],js:[wW],python:[bW]},$$scope:{ctx:$}}}),La=new L({props:{python:!0,js:!0,curl:!0,$$slots:{python:[AW]},$$scope:{ctx:$}}}),{c(){n=r("meta"),c=f(),t=r("h1"),d=r("a"),q=r("span"),_(k.$$.fragment),A=f(),j=r("span"),T=i("Detailed parameters"),N=f(),D=r("h2"),ne=r("a"),Re=r("span"),_(Q.$$.fragment),Y=f(),ot=r("span"),Ri=i("Which task is used by this model ?"),Va=f(),Ne=r("p"),Fw=i(`In general the \u{1F917} Hosted API Inference accepts a simple string as an
input. However, more advanced usage depends on the \u201Ctask\u201D that the
model solves.`),L_=f(),Ni=r("p"),Kw=i("The \u201Ctask\u201D of a model is defined here on it\u2019s model page:"),U_=f(),lt=r("img"),z_=f(),it=r("img"),M_=f(),Se=r("h2"),ut=r("a"),cd=r("span"),_(Xa.$$.fragment),Jw=f(),fd=r("span"),Ww=i("Zero-shot classification task"),F_=f(),Si=r("p"),Yw=i(`This task is super useful to try out classification with zero code,
you simply pass a sentence/paragraph and the possible labels for that
sentence, and you get a result.`),K_=f(),_(ct.$$.fragment),J_=f(),Qa=r("p"),Vw=i("Available with: "),Za=r("a"),Xw=i("\u{1F917} Transformers"),W_=f(),xi=r("p"),Qw=i("Request:"),Y_=f(),_(ft.$$.fragment),V_=f(),Ii=r("p"),Zw=i(`When sending your request, you should send a JSON encoded payload. Here
are all the options`),X_=f(),pt=r("table"),pd=r("thead"),en=r("tr"),Hi=r("th"),e3=i("All parameters"),t3=f(),hd=r("th"),s3=f(),M=r("tbody"),tn=r("tr"),sn=r("td"),dd=r("strong"),a3=i("inputs"),n3=i(" (required)"),r3=f(),Bi=r("td"),o3=i("a string or list of strings"),l3=f(),an=r("tr"),nn=r("td"),gd=r("strong"),i3=i("parameters"),u3=i(" (required)"),c3=f(),Ci=r("td"),f3=i("a dict containing the following keys:"),p3=f(),rn=r("tr"),Gi=r("td"),h3=i("candidate_labels (required)"),d3=f(),he=r("td"),g3=i("a list of strings that are potential classes for "),md=r("code"),m3=i("inputs"),q3=i(". (max 10 candidate_labels, for more, simply run multiple requests, results are going to be misleading if using too many candidate_labels anyway. If you want to keep the exact same, you can simply run "),qd=r("code"),$3=i("multi_label=True"),_3=i(" and do the scaling on your end. )"),v3=f(),on=r("tr"),Li=r("td"),y3=i("multi_label"),b3=f(),ht=r("td"),E3=i("(Default: "),$d=r("code"),w3=i("false"),T3=i(") Boolean that is set to True if classes can overlap"),j3=f(),ln=r("tr"),Ui=r("td"),_d=r("strong"),k3=i("options"),A3=f(),zi=r("td"),D3=i("a dict containing the following keys:"),O3=f(),un=r("tr"),Mi=r("td"),P3=i("use_gpu"),R3=f(),dt=r("td"),N3=i("(Default: "),vd=r("code"),S3=i("false"),x3=i("). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),I3=f(),cn=r("tr"),Fi=r("td"),H3=i("use_cache"),B3=f(),gt=r("td"),C3=i("(Default: "),yd=r("code"),G3=i("true"),L3=i("). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),U3=f(),fn=r("tr"),Ki=r("td"),z3=i("wait_for_model"),M3=f(),mt=r("td"),F3=i("(Default: "),bd=r("code"),K3=i("false"),J3=i(") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),Q_=f(),Ji=r("p"),W3=i("Return value is either a dict or a list of dicts if you sent a list of inputs"),Z_=f(),Wi=r("p"),Y3=i("Response:"),e1=f(),_(qt.$$.fragment),t1=f(),$t=r("table"),Ed=r("thead"),pn=r("tr"),Yi=r("th"),V3=i("Returned values"),X3=f(),wd=r("th"),Q3=f(),xe=r("tbody"),hn=r("tr"),Vi=r("td"),Td=r("strong"),Z3=i("sequence"),eT=f(),Xi=r("td"),tT=i("The string sent as an input"),sT=f(),dn=r("tr"),Qi=r("td"),jd=r("strong"),aT=i("labels"),nT=f(),Zi=r("td"),rT=i("The list of strings for labels that you sent (in order)"),oT=f(),gn=r("tr"),eu=r("td"),kd=r("strong"),lT=i("scores"),iT=f(),_t=r("td"),uT=i("a list of floats that correspond the the probability of label, in the same order as "),Ad=r("code"),cT=i("labels"),fT=i("."),s1=f(),Ie=r("h2"),vt=r("a"),Dd=r("span"),_(mn.$$.fragment),pT=f(),Od=r("span"),hT=i("Translation task"),a1=f(),tu=r("p"),dT=i("This task is well known to translate text from one language to another"),n1=f(),_(yt.$$.fragment),r1=f(),qn=r("p"),gT=i("Available with: "),$n=r("a"),mT=i("\u{1F917} Transformers"),o1=f(),su=r("p"),qT=i("Example:"),l1=f(),_(bt.$$.fragment),i1=f(),au=r("p"),$T=i(`When sending your request, you should send a JSON encoded payload. Here
are all the options`),u1=f(),Et=r("table"),Pd=r("thead"),_n=r("tr"),nu=r("th"),_T=i("All parameters"),vT=f(),Rd=r("th"),yT=f(),Z=r("tbody"),vn=r("tr"),yn=r("td"),Nd=r("strong"),bT=i("inputs"),ET=i(" (required)"),wT=f(),ru=r("td"),TT=i("a string to be translated in the original languages"),jT=f(),bn=r("tr"),ou=r("td"),Sd=r("strong"),kT=i("options"),AT=f(),lu=r("td"),DT=i("a dict containing the following keys:"),OT=f(),En=r("tr"),iu=r("td"),PT=i("use_gpu"),RT=f(),wt=r("td"),NT=i("(Default: "),xd=r("code"),ST=i("false"),xT=i("). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),IT=f(),wn=r("tr"),uu=r("td"),HT=i("use_cache"),BT=f(),Tt=r("td"),CT=i("(Default: "),Id=r("code"),GT=i("true"),LT=i("). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),UT=f(),Tn=r("tr"),cu=r("td"),zT=i("wait_for_model"),MT=f(),jt=r("td"),FT=i("(Default: "),Hd=r("code"),KT=i("false"),JT=i(") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),c1=f(),fu=r("p"),WT=i("Return value is either a dict or a list of dicts if you sent a list of inputs"),f1=f(),kt=r("table"),Bd=r("thead"),jn=r("tr"),pu=r("th"),YT=i("Returned values"),VT=f(),Cd=r("th"),XT=f(),Gd=r("tbody"),kn=r("tr"),hu=r("td"),Ld=r("strong"),QT=i("translation_text"),ZT=f(),du=r("td"),e4=i("The string after translation"),p1=f(),He=r("h2"),At=r("a"),Ud=r("span"),_(An.$$.fragment),t4=f(),zd=r("span"),s4=i("Summarization task"),h1=f(),Dt=r("p"),a4=i(`This task is well known to summarize longer text into shorter text.
Be careful, some models have a maximum length of input. That means that
the summary cannot handle full books for instance. Be careful when
choosing your model. If you want to discuss your summarization needs,
please get in touch with us: <`),gu=r("a"),n4=i("api-enterprise@huggingface.co"),r4=i(">"),d1=f(),_(Ot.$$.fragment),g1=f(),Dn=r("p"),o4=i("Available with: "),On=r("a"),l4=i("\u{1F917} Transformers"),m1=f(),mu=r("p"),i4=i("Example:"),q1=f(),_(Pt.$$.fragment),$1=f(),qu=r("p"),u4=i(`When sending your request, you should send a JSON encoded payload. Here
are all the options`),_1=f(),Rt=r("table"),Md=r("thead"),Pn=r("tr"),$u=r("th"),c4=i("All parameters"),f4=f(),Fd=r("th"),p4=f(),G=r("tbody"),Rn=r("tr"),Nn=r("td"),Kd=r("strong"),h4=i("inputs"),d4=i(" (required)"),g4=f(),_u=r("td"),m4=i("a string to be summarized"),q4=f(),Sn=r("tr"),vu=r("td"),Jd=r("strong"),$4=i("parameters"),_4=f(),yu=r("td"),v4=i("a dict containing the following keys:"),y4=f(),xn=r("tr"),bu=r("td"),b4=i("min_length"),E4=f(),de=r("td"),w4=i("(Default: "),Wd=r("code"),T4=i("None"),j4=i("). Integer to define the minimum length "),Yd=r("strong"),k4=i("in tokens"),A4=i(" of the output summary."),D4=f(),In=r("tr"),Eu=r("td"),O4=i("max_length"),P4=f(),ge=r("td"),R4=i("(Default: "),Vd=r("code"),N4=i("None"),S4=i("). Integer to define the maximum length "),Xd=r("strong"),x4=i("in tokens"),I4=i(" of the output summary."),H4=f(),Hn=r("tr"),wu=r("td"),B4=i("top_k"),C4=f(),me=r("td"),G4=i("(Default: "),Qd=r("code"),L4=i("None"),U4=i("). Integer to define the top tokens considered within the "),Zd=r("code"),z4=i("sample"),M4=i(" operation to create new text."),F4=f(),Bn=r("tr"),Tu=r("td"),K4=i("top_p"),J4=f(),re=r("td"),W4=i("(Default: "),eg=r("code"),Y4=i("None"),V4=i("). Float to define the tokens that are within the "),tg=r("code"),X4=i("sample"),Q4=i(" operation of text generation. Add tokens in the sample for more probable to least probable until the sum of the probabilities is greater than "),sg=r("code"),Z4=i("top_p"),e5=i("."),t5=f(),Cn=r("tr"),ju=r("td"),s5=i("temperature"),a5=f(),qe=r("td"),n5=i("(Default: "),ag=r("code"),r5=i("1.0"),o5=i("). Float (0.0-100.0). The temperature of the sampling operation. 1 means regular sampling, 0 means "),ng=r("code"),l5=i("100.0"),i5=i(" is getting closer to uniform probability."),u5=f(),Gn=r("tr"),ku=r("td"),c5=i("repetition_penalty"),f5=f(),Nt=r("td"),p5=i("(Default: "),rg=r("code"),h5=i("None"),d5=i("). Float (0.0-100.0). The more a token is used within generation the more it is penalized to not be picked in successive generation passes."),g5=f(),Ln=r("tr"),Au=r("td"),m5=i("max_time"),q5=f(),St=r("td"),$5=i("(Default: "),og=r("code"),_5=i("None"),v5=i("). Float (0-120.0). The amount of time in seconds that the query should take maximum. Network can cause some overhead so it will be a soft limit."),y5=f(),Un=r("tr"),Du=r("td"),lg=r("strong"),b5=i("options"),E5=f(),Ou=r("td"),w5=i("a dict containing the following keys:"),T5=f(),zn=r("tr"),Pu=r("td"),j5=i("use_gpu"),k5=f(),xt=r("td"),A5=i("(Default: "),ig=r("code"),D5=i("false"),O5=i("). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),P5=f(),Mn=r("tr"),Ru=r("td"),R5=i("use_cache"),N5=f(),It=r("td"),S5=i("(Default: "),ug=r("code"),x5=i("true"),I5=i("). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),H5=f(),Fn=r("tr"),Nu=r("td"),B5=i("wait_for_model"),C5=f(),Ht=r("td"),G5=i("(Default: "),cg=r("code"),L5=i("false"),U5=i(") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),v1=f(),Su=r("p"),z5=i("Return value is either a dict or a list of dicts if you sent a list of inputs"),y1=f(),Bt=r("table"),fg=r("thead"),Kn=r("tr"),xu=r("th"),M5=i("Returned values"),F5=f(),pg=r("th"),K5=f(),hg=r("tbody"),Jn=r("tr"),Iu=r("td"),dg=r("strong"),J5=i("summarization_text"),W5=f(),Hu=r("td"),Y5=i("The string after translation"),b1=f(),Be=r("h2"),Ct=r("a"),gg=r("span"),_(Wn.$$.fragment),V5=f(),mg=r("span"),X5=i("Conversational task"),E1=f(),Bu=r("p"),Q5=i(`This task corresponds to any chatbot like structure. Models tend to have
shorter max_length, so please check with caution when using a given
model if you need long range dependency or not.`),w1=f(),_(Gt.$$.fragment),T1=f(),Yn=r("p"),Z5=i("Available with: "),Vn=r("a"),ej=i("\u{1F917} Transformers"),j1=f(),Cu=r("p"),tj=i("Example:"),k1=f(),_(Lt.$$.fragment),A1=f(),Gu=r("p"),sj=i(`When sending your request, you should send a JSON encoded payload. Here
are all the options`),D1=f(),Ut=r("table"),qg=r("thead"),Xn=r("tr"),Lu=r("th"),aj=i("All parameters"),nj=f(),$g=r("th"),rj=f(),S=r("tbody"),Qn=r("tr"),Zn=r("td"),_g=r("strong"),oj=i("inputs"),lj=i(" (required)"),ij=f(),vg=r("td"),uj=f(),er=r("tr"),Uu=r("td"),cj=i("text (required)"),fj=f(),zu=r("td"),pj=i("The last input from the user in the conversation."),hj=f(),tr=r("tr"),Mu=r("td"),dj=i("generated_responses"),gj=f(),Fu=r("td"),mj=i("A list of strings corresponding to the earlier replies from the model."),qj=f(),sr=r("tr"),Ku=r("td"),$j=i("past_user_inputs"),_j=f(),zt=r("td"),vj=i("A list of strings corresponding to the earlier replies from the user. Should be of the same length of "),yg=r("code"),yj=i("generated_responses"),bj=i("."),Ej=f(),ar=r("tr"),Ju=r("td"),bg=r("strong"),wj=i("parameters"),Tj=f(),Wu=r("td"),jj=i("a dict containing the following keys:"),kj=f(),nr=r("tr"),Yu=r("td"),Aj=i("min_length"),Dj=f(),$e=r("td"),Oj=i("(Default: "),Eg=r("code"),Pj=i("None"),Rj=i("). Integer to define the minimum length "),wg=r("strong"),Nj=i("in tokens"),Sj=i(" of the output summary."),xj=f(),rr=r("tr"),Vu=r("td"),Ij=i("max_length"),Hj=f(),_e=r("td"),Bj=i("(Default: "),Tg=r("code"),Cj=i("None"),Gj=i("). Integer to define the maximum length "),jg=r("strong"),Lj=i("in tokens"),Uj=i(" of the output summary."),zj=f(),or=r("tr"),Xu=r("td"),Mj=i("top_k"),Fj=f(),ve=r("td"),Kj=i("(Default: "),kg=r("code"),Jj=i("None"),Wj=i("). Integer to define the top tokens considered within the "),Ag=r("code"),Yj=i("sample"),Vj=i(" operation to create new text."),Xj=f(),lr=r("tr"),Qu=r("td"),Qj=i("top_p"),Zj=f(),oe=r("td"),e6=i("(Default: "),Dg=r("code"),t6=i("None"),s6=i("). Float to define the tokens that are within the "),Og=r("code"),a6=i("sample"),n6=i(" operation of text generation. Add tokens in the sample for more probable to least probable until the sum of the probabilities is greater than "),Pg=r("code"),r6=i("top_p"),o6=i("."),l6=f(),ir=r("tr"),Zu=r("td"),i6=i("temperature"),u6=f(),ye=r("td"),c6=i("(Default: "),Rg=r("code"),f6=i("1.0"),p6=i("). Float (0.0-100.0). The temperature of the sampling operation. 1 means regular sampling, 0 means "),Ng=r("code"),h6=i("100.0"),d6=i(" is getting closer to uniform probability."),g6=f(),ur=r("tr"),ec=r("td"),m6=i("repetition_penalty"),q6=f(),Mt=r("td"),$6=i("(Default: "),Sg=r("code"),_6=i("None"),v6=i("). Float (0.0-100.0). The more a token is used within generation the more it is penalized to not be picked in successive generation passes."),y6=f(),cr=r("tr"),tc=r("td"),b6=i("max_time"),E6=f(),Ft=r("td"),w6=i("(Default: "),xg=r("code"),T6=i("None"),j6=i("). Float (0-120.0). The amount of time in seconds that the query should take maximum. Network can cause some overhead so it will be a soft limit."),k6=f(),fr=r("tr"),sc=r("td"),Ig=r("strong"),A6=i("options"),D6=f(),ac=r("td"),O6=i("a dict containing the following keys:"),P6=f(),pr=r("tr"),nc=r("td"),R6=i("use_gpu"),N6=f(),Kt=r("td"),S6=i("(Default: "),Hg=r("code"),x6=i("false"),I6=i("). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),H6=f(),hr=r("tr"),rc=r("td"),B6=i("use_cache"),C6=f(),Jt=r("td"),G6=i("(Default: "),Bg=r("code"),L6=i("true"),U6=i("). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),z6=f(),dr=r("tr"),oc=r("td"),M6=i("wait_for_model"),F6=f(),Wt=r("td"),K6=i("(Default: "),Cg=r("code"),J6=i("false"),W6=i(") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),O1=f(),lc=r("p"),Y6=i("Return value is either a dict or a list of dicts if you sent a list of inputs"),P1=f(),Yt=r("table"),Gg=r("thead"),gr=r("tr"),ic=r("th"),V6=i("Returned values"),X6=f(),Lg=r("th"),Q6=f(),ie=r("tbody"),mr=r("tr"),uc=r("td"),Ug=r("strong"),Z6=i("generated_text"),ek=f(),cc=r("td"),tk=i("The answer of the bot"),sk=f(),qr=r("tr"),fc=r("td"),zg=r("strong"),ak=i("conversation"),nk=f(),pc=r("td"),rk=i("A facility dictionnary to send back for the next input (with the new user input addition)."),ok=f(),$r=r("tr"),hc=r("td"),lk=i("past_user_inputs"),ik=f(),dc=r("td"),uk=i("List of strings. The last inputs from the user in the conversation, <em>after the model has run."),ck=f(),_r=r("tr"),gc=r("td"),fk=i("generated_responses"),pk=f(),mc=r("td"),hk=i("List of strings. The last outputs from the model in the conversation, <em>after the model has run."),R1=f(),Ce=r("h2"),Vt=r("a"),Mg=r("span"),_(vr.$$.fragment),dk=f(),Fg=r("span"),gk=i("Table question answering task"),N1=f(),qc=r("p"),mk=i(`Don\u2019t know SQL? Don\u2019t want to dive into a large spreadsheet? Ask
questions in plain english!`),S1=f(),_(Xt.$$.fragment),x1=f(),yr=r("p"),qk=i("Available with: "),br=r("a"),$k=i("\u{1F917} Transformers"),I1=f(),$c=r("p"),_k=i("Example:"),H1=f(),_(Qt.$$.fragment),B1=f(),_c=r("p"),vk=i(`When sending your request, you should send a JSON encoded payload. Here
are all the options`),C1=f(),Zt=r("table"),Kg=r("thead"),Er=r("tr"),vc=r("th"),yk=i("All parameters"),bk=f(),Jg=r("th"),Ek=f(),K=r("tbody"),wr=r("tr"),Tr=r("td"),Wg=r("strong"),wk=i("inputs"),Tk=i(" (required)"),jk=f(),Yg=r("td"),kk=f(),jr=r("tr"),yc=r("td"),Ak=i("query (required)"),Dk=f(),bc=r("td"),Ok=i("The query in plain text that you want to ask the table"),Pk=f(),kr=r("tr"),Ec=r("td"),Rk=i("table (required)"),Nk=f(),wc=r("td"),Sk=i("A table of data represented as a dict of list where entries are headers and the lists are all the values, all lists must have the same size."),xk=f(),Ar=r("tr"),Tc=r("td"),Vg=r("strong"),Ik=i("options"),Hk=f(),jc=r("td"),Bk=i("a dict containing the following keys:"),Ck=f(),Dr=r("tr"),kc=r("td"),Gk=i("use_gpu"),Lk=f(),es=r("td"),Uk=i("(Default: "),Xg=r("code"),zk=i("false"),Mk=i("). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),Fk=f(),Or=r("tr"),Ac=r("td"),Kk=i("use_cache"),Jk=f(),ts=r("td"),Wk=i("(Default: "),Qg=r("code"),Yk=i("true"),Vk=i("). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),Xk=f(),Pr=r("tr"),Dc=r("td"),Qk=i("wait_for_model"),Zk=f(),ss=r("td"),e7=i("(Default: "),Zg=r("code"),t7=i("false"),s7=i(") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),G1=f(),Oc=r("p"),a7=i("Return value is either a dict or a list of dicts if you sent a list of inputs"),L1=f(),_(as.$$.fragment),U1=f(),ns=r("table"),em=r("thead"),Rr=r("tr"),Pc=r("th"),n7=i("Returned values"),r7=f(),tm=r("th"),o7=f(),ue=r("tbody"),Nr=r("tr"),Rc=r("td"),sm=r("strong"),l7=i("answer"),i7=f(),Nc=r("td"),u7=i("The plaintext answer"),c7=f(),Sr=r("tr"),Sc=r("td"),am=r("strong"),f7=i("coordinates"),p7=f(),xc=r("td"),h7=i("a list of coordinates of the cells referenced in the answer"),d7=f(),xr=r("tr"),Ic=r("td"),nm=r("strong"),g7=i("cells"),m7=f(),Hc=r("td"),q7=i("a list of coordinates of the cells contents"),$7=f(),Ir=r("tr"),Bc=r("td"),rm=r("strong"),_7=i("aggregator"),v7=f(),Cc=r("td"),y7=i("The aggregator used to get the answer"),z1=f(),Ge=r("h2"),rs=r("a"),om=r("span"),_(Hr.$$.fragment),b7=f(),lm=r("span"),E7=i("Question answering task"),M1=f(),Gc=r("p"),w7=i("Want to have a nice know-it-all bot that can answer any question?"),F1=f(),_(os.$$.fragment),K1=f(),Le=r("p"),T7=i("Available with: "),Br=r("a"),j7=i("\u{1F917}Transformers"),k7=i(` and
`),Cr=r("a"),A7=i("AllenNLP"),J1=f(),Lc=r("p"),D7=i("Example:"),W1=f(),_(ls.$$.fragment),Y1=f(),Uc=r("p"),O7=i(`When sending your request, you should send a JSON encoded payload. Here
are all the options`),V1=f(),zc=r("p"),P7=i("Return value is either a dict or a list of dicts if you sent a list of inputs"),X1=f(),_(is.$$.fragment),Q1=f(),us=r("table"),im=r("thead"),Gr=r("tr"),Mc=r("th"),R7=i("Returned values"),N7=f(),um=r("th"),S7=f(),ce=r("tbody"),Lr=r("tr"),Fc=r("td"),cm=r("strong"),x7=i("answer"),I7=f(),Kc=r("td"),H7=i("A string that\u2019s the answer within the text."),B7=f(),Ur=r("tr"),Jc=r("td"),fm=r("strong"),C7=i("score"),G7=f(),Wc=r("td"),L7=i("A float that represents how likely that the answer is correct"),U7=f(),zr=r("tr"),Yc=r("td"),pm=r("strong"),z7=i("start"),M7=f(),cs=r("td"),F7=i("The index (string wise) of the start of the answer within "),hm=r("code"),K7=i("context"),J7=i("."),W7=f(),Mr=r("tr"),Vc=r("td"),dm=r("strong"),Y7=i("stop"),V7=f(),fs=r("td"),X7=i("The index (string wise) of the stop of the answer within "),gm=r("code"),Q7=i("context"),Z7=i("."),Z1=f(),Ue=r("h2"),ps=r("a"),mm=r("span"),_(Fr.$$.fragment),e9=f(),qm=r("span"),t9=i("Text-classification task"),e2=f(),Xc=r("p"),s9=i(`Usually used for sentiment-analysis this will output the likelihood of
classes of an input.`),t2=f(),_(hs.$$.fragment),s2=f(),Kr=r("p"),a9=i("Available with: "),Jr=r("a"),n9=i("\u{1F917} Transformers"),a2=f(),Qc=r("p"),r9=i("Example:"),n2=f(),_(ds.$$.fragment),r2=f(),Zc=r("p"),o9=i(`When sending your request, you should send a JSON encoded payload. Here
are all the options`),o2=f(),gs=r("table"),$m=r("thead"),Wr=r("tr"),ef=r("th"),l9=i("All parameters"),i9=f(),_m=r("th"),u9=f(),ee=r("tbody"),Yr=r("tr"),Vr=r("td"),vm=r("strong"),c9=i("inputs"),f9=i(" (required)"),p9=f(),tf=r("td"),h9=i("a string to be classified"),d9=f(),Xr=r("tr"),sf=r("td"),ym=r("strong"),g9=i("options"),m9=f(),af=r("td"),q9=i("a dict containing the following keys:"),$9=f(),Qr=r("tr"),nf=r("td"),_9=i("use_gpu"),v9=f(),ms=r("td"),y9=i("(Default: "),bm=r("code"),b9=i("false"),E9=i("). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),w9=f(),Zr=r("tr"),rf=r("td"),T9=i("use_cache"),j9=f(),qs=r("td"),k9=i("(Default: "),Em=r("code"),A9=i("true"),D9=i("). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),O9=f(),eo=r("tr"),of=r("td"),P9=i("wait_for_model"),R9=f(),$s=r("td"),N9=i("(Default: "),wm=r("code"),S9=i("false"),x9=i(") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),l2=f(),lf=r("p"),I9=i("Return value is either a dict or a list of dicts if you sent a list of inputs"),i2=f(),_(_s.$$.fragment),u2=f(),vs=r("table"),Tm=r("thead"),to=r("tr"),uf=r("th"),H9=i("Returned values"),B9=f(),jm=r("th"),C9=f(),so=r("tbody"),ao=r("tr"),cf=r("td"),km=r("strong"),G9=i("label"),L9=f(),ff=r("td"),U9=i("The label for the class (model specific)"),z9=f(),no=r("tr"),pf=r("td"),Am=r("strong"),M9=i("score"),F9=f(),hf=r("td"),K9=i("A floats that represents how likely is that the text belongs the this class."),c2=f(),ze=r("h2"),ys=r("a"),Dm=r("span"),_(ro.$$.fragment),J9=f(),Om=r("span"),W9=i("Named Entity Recognition (NER) task"),f2=f(),oo=r("p"),Y9=i("See "),df=r("a"),V9=i("Token-classification task"),p2=f(),Me=r("h2"),bs=r("a"),Pm=r("span"),_(lo.$$.fragment),X9=f(),Rm=r("span"),Q9=i("Token-classification task"),h2=f(),gf=r("p"),Z9=i(`Usually used for sentence parsing, either grammatical, or Named Entity
Recognition (NER) to understand keywords contained within text.`),d2=f(),_(Es.$$.fragment),g2=f(),Fe=r("p"),e8=i("Available with: "),io=r("a"),t8=i("\u{1F917} Transformers"),s8=i(`,
`),uo=r("a"),a8=i("Flair"),m2=f(),mf=r("p"),n8=i("Example:"),q2=f(),_(ws.$$.fragment),$2=f(),qf=r("p"),r8=i(`When sending your request, you should send a JSON encoded payload. Here
are all the options`),_2=f(),Ts=r("table"),Nm=r("thead"),co=r("tr"),$f=r("th"),o8=i("All parameters"),l8=f(),Sm=r("th"),i8=f(),J=r("tbody"),fo=r("tr"),po=r("td"),xm=r("strong"),u8=i("inputs"),c8=i(" (required)"),f8=f(),_f=r("td"),p8=i("a string to be classified"),h8=f(),ho=r("tr"),vf=r("td"),Im=r("strong"),d8=i("parameters"),g8=f(),yf=r("td"),m8=i("a dict containing the following key:"),q8=f(),go=r("tr"),bf=r("td"),$8=i("aggregation_strategy"),_8=f(),x=r("td"),v8=i("(Default: "),Hm=r("code"),y8=i("simple"),b8=i("). There are several aggregation strategies: "),E8=r("br"),w8=f(),Bm=r("code"),T8=i("none"),j8=i(": Every token gets classified without further aggregation. "),k8=r("br"),A8=f(),Cm=r("code"),D8=i("simple"),O8=i(": Entities are grouped according to the default schema (B-, I- tags get merged when the tag is similar). "),P8=r("br"),R8=f(),Gm=r("code"),N8=i("first"),S8=i(": Same as the "),Lm=r("code"),x8=i("simple"),I8=i(" strategy except words cannot end up with different tags. Words will use the tag of the first token when there is ambiguity. "),H8=r("br"),B8=f(),Um=r("code"),C8=i("average"),G8=i(": Same as the "),zm=r("code"),L8=i("simple"),U8=i(" strategy except words cannot end up with different tags. Scores are averaged across tokens and then the maximum label is applied. "),z8=r("br"),M8=f(),Mm=r("code"),F8=i("max"),K8=i(": Same as the "),Fm=r("code"),J8=i("simple"),W8=i(" strategy except words cannot end up with different tags. Word entity will be the token with the maximum score."),Y8=f(),mo=r("tr"),Ef=r("td"),Km=r("strong"),V8=i("options"),X8=f(),wf=r("td"),Q8=i("a dict containing the following keys:"),Z8=f(),qo=r("tr"),Tf=r("td"),eA=i("use_gpu"),tA=f(),js=r("td"),sA=i("(Default: "),Jm=r("code"),aA=i("false"),nA=i("). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),rA=f(),$o=r("tr"),jf=r("td"),oA=i("use_cache"),lA=f(),ks=r("td"),iA=i("(Default: "),Wm=r("code"),uA=i("true"),cA=i("). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),fA=f(),_o=r("tr"),kf=r("td"),pA=i("wait_for_model"),hA=f(),As=r("td"),dA=i("(Default: "),Ym=r("code"),gA=i("false"),mA=i(") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),v2=f(),Af=r("p"),qA=i("Return value is either a dict or a list of dicts if you sent a list of inputs"),y2=f(),_(Ds.$$.fragment),b2=f(),Os=r("table"),Vm=r("thead"),vo=r("tr"),Df=r("th"),$A=i("Returned values"),_A=f(),Xm=r("th"),vA=f(),te=r("tbody"),yo=r("tr"),Of=r("td"),Qm=r("strong"),yA=i("entity_group"),bA=f(),Pf=r("td"),EA=i("The type for the entity being recognized (model specific)."),wA=f(),bo=r("tr"),Rf=r("td"),Zm=r("strong"),TA=i("score"),jA=f(),Nf=r("td"),kA=i("How likely the entity was recognized."),AA=f(),Eo=r("tr"),Sf=r("td"),eq=r("strong"),DA=i("word"),OA=f(),xf=r("td"),PA=i("The string that was captured"),RA=f(),wo=r("tr"),If=r("td"),tq=r("strong"),NA=i("start"),SA=f(),Ps=r("td"),xA=i("The offset stringwise where the answer is located. Useful to disambiguate if "),sq=r("code"),IA=i("word"),HA=i(" occurs multiple times."),BA=f(),To=r("tr"),Hf=r("td"),aq=r("strong"),CA=i("end"),GA=f(),Rs=r("td"),LA=i("The offset stringwise where the answer is located. Useful to disambiguate if "),nq=r("code"),UA=i("word"),zA=i(" occurs multiple times."),E2=f(),Ke=r("h2"),Ns=r("a"),rq=r("span"),_(jo.$$.fragment),MA=f(),oq=r("span"),FA=i("Text-generation task"),w2=f(),Bf=r("p"),KA=i("Use to continue text from a prompt. This is a very generic task."),T2=f(),_(Ss.$$.fragment),j2=f(),ko=r("p"),JA=i("Available with: "),Ao=r("a"),WA=i("\u{1F917} Transformers"),k2=f(),Cf=r("p"),YA=i("Example:"),A2=f(),_(xs.$$.fragment),D2=f(),Gf=r("p"),VA=i(`When sending your request, you should send a JSON encoded payload. Here
are all the options`),O2=f(),Is=r("table"),lq=r("thead"),Do=r("tr"),Lf=r("th"),XA=i("All parameters"),QA=f(),iq=r("th"),ZA=f(),I=r("tbody"),Oo=r("tr"),Po=r("td"),uq=r("strong"),eD=i("inputs"),tD=i(" (required):"),sD=f(),Uf=r("td"),aD=i("a string to be generated from"),nD=f(),Ro=r("tr"),zf=r("td"),cq=r("strong"),rD=i("parameters"),oD=f(),Mf=r("td"),lD=i("dict containing the following keys:"),iD=f(),No=r("tr"),Ff=r("td"),uD=i("top_k"),cD=f(),be=r("td"),fD=i("(Default: "),fq=r("code"),pD=i("None"),hD=i("). Integer to define the top tokens considered within the "),pq=r("code"),dD=i("sample"),gD=i(" operation to create new text."),mD=f(),So=r("tr"),Kf=r("td"),qD=i("top_p"),$D=f(),le=r("td"),_D=i("(Default: "),hq=r("code"),vD=i("None"),yD=i("). Float to define the tokens that are within the "),dq=r("code"),bD=i("sample"),ED=i(" operation of text generation. Add tokens in the sample for more probable to least probable until the sum of the probabilities is greater than "),gq=r("code"),wD=i("top_p"),TD=i("."),jD=f(),xo=r("tr"),Jf=r("td"),kD=i("temperature"),AD=f(),Ee=r("td"),DD=i("(Default: "),mq=r("code"),OD=i("1.0"),PD=i("). Float (0.0-100.0). The temperature of the sampling operation. 1 means regular sampling, 0 means "),qq=r("code"),RD=i("100.0"),ND=i(" is getting closer to uniform probability."),SD=f(),Io=r("tr"),Wf=r("td"),xD=i("repetition_penalty"),ID=f(),Hs=r("td"),HD=i("(Default: "),$q=r("code"),BD=i("None"),CD=i("). Float (0.0-100.0). The more a token is used within generation the more it is penalized to not be picked in successive generation passes."),GD=f(),Ho=r("tr"),Yf=r("td"),LD=i("max_new_tokens"),UD=f(),we=r("td"),zD=i("(Default: "),_q=r("code"),MD=i("None"),FD=i("). Int (0-250). The amount of new tokens to be generated, this does "),vq=r("strong"),KD=i("not"),JD=i(" include the input length it is a estimate of the size of generated text you want. Each new tokens slows down the request, so look for balance between response times and length of text generated."),WD=f(),Bo=r("tr"),Vf=r("td"),YD=i("max_time"),VD=f(),Te=r("td"),XD=i("(Default: "),yq=r("code"),QD=i("None"),ZD=i("). Float (0-120.0). The amount of time in seconds that the query should take maximum. Network can cause some overhead so it will be a soft limit. Use that in combination with "),bq=r("code"),eO=i("max_new_tokens"),tO=i(" for best results."),sO=f(),Co=r("tr"),Xf=r("td"),aO=i("return_full_text"),nO=f(),je=r("td"),rO=i("(Default: "),Eq=r("code"),oO=i("True"),lO=i("). Bool. If set to False, the return results will "),wq=r("strong"),iO=i("not"),uO=i(" contain the original query making it easier for prompting."),cO=f(),Go=r("tr"),Qf=r("td"),fO=i("num_return_sequences"),pO=f(),Bs=r("td"),hO=i("(Default: "),Tq=r("code"),dO=i("1"),gO=i("). Integer. The number of proposition you want to be returned."),mO=f(),Lo=r("tr"),Zf=r("td"),qO=i("do_sample"),$O=f(),Cs=r("td"),_O=i("(Optional: "),jq=r("code"),vO=i("True"),yO=i("). Bool. Whether or not to use sampling, use greedy decoding otherwise."),bO=f(),Uo=r("tr"),ep=r("td"),kq=r("strong"),EO=i("options"),wO=f(),tp=r("td"),TO=i("a dict containing the following keys:"),jO=f(),zo=r("tr"),sp=r("td"),kO=i("use_gpu"),AO=f(),Gs=r("td"),DO=i("(Default: "),Aq=r("code"),OO=i("false"),PO=i("). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),RO=f(),Mo=r("tr"),ap=r("td"),NO=i("use_cache"),SO=f(),Ls=r("td"),xO=i("(Default: "),Dq=r("code"),IO=i("true"),HO=i("). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),BO=f(),Fo=r("tr"),np=r("td"),CO=i("wait_for_model"),GO=f(),Us=r("td"),LO=i("(Default: "),Oq=r("code"),UO=i("false"),zO=i(") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),P2=f(),rp=r("p"),MO=i("Return value is either a dict or a list of dicts if you sent a list of inputs"),R2=f(),_(zs.$$.fragment),N2=f(),Ms=r("table"),Pq=r("thead"),Ko=r("tr"),op=r("th"),FO=i("Returned values"),KO=f(),Rq=r("th"),JO=f(),Nq=r("tbody"),Jo=r("tr"),lp=r("td"),Sq=r("strong"),WO=i("generated_text"),YO=f(),ip=r("td"),VO=i("The continuated string"),S2=f(),Je=r("h2"),Fs=r("a"),xq=r("span"),_(Wo.$$.fragment),XO=f(),Iq=r("span"),QO=i("Text2text-generation task"),x2=f(),Ks=r("p"),ZO=i("Essentially "),up=r("a"),eP=i("Text-generation task"),tP=i(`. But uses
Encoder-Decoder architecture, so might change in the future for more
options.`),I2=f(),We=r("h2"),Js=r("a"),Hq=r("span"),_(Yo.$$.fragment),sP=f(),Bq=r("span"),aP=i("Fill mask task"),H2=f(),cp=r("p"),nP=i(`Tries to fill in a hole with a missing word (token to be precise).
That\u2019s the base task for BERT models.`),B2=f(),_(Ws.$$.fragment),C2=f(),Vo=r("p"),rP=i("Available with: "),Xo=r("a"),oP=i("\u{1F917} Transformers"),G2=f(),fp=r("p"),lP=i("Example:"),L2=f(),_(Ys.$$.fragment),U2=f(),pp=r("p"),iP=i(`When sending your request, you should send a JSON encoded payload. Here
are all the options`),z2=f(),Vs=r("table"),Cq=r("thead"),Qo=r("tr"),hp=r("th"),uP=i("All parameters"),cP=f(),Gq=r("th"),fP=f(),se=r("tbody"),Zo=r("tr"),el=r("td"),Lq=r("strong"),pP=i("inputs"),hP=i(" (required):"),dP=f(),dp=r("td"),gP=i("a string to be filled from, must contain the [MASK] token (check model card for exact name of the mask)"),mP=f(),tl=r("tr"),gp=r("td"),Uq=r("strong"),qP=i("options"),$P=f(),mp=r("td"),_P=i("a dict containing the following keys:"),vP=f(),sl=r("tr"),qp=r("td"),yP=i("use_gpu"),bP=f(),Xs=r("td"),EP=i("(Default: "),zq=r("code"),wP=i("false"),TP=i("). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),jP=f(),al=r("tr"),$p=r("td"),kP=i("use_cache"),AP=f(),Qs=r("td"),DP=i("(Default: "),Mq=r("code"),OP=i("true"),PP=i("). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),RP=f(),nl=r("tr"),_p=r("td"),NP=i("wait_for_model"),SP=f(),Zs=r("td"),xP=i("(Default: "),Fq=r("code"),IP=i("false"),HP=i(") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),M2=f(),vp=r("p"),BP=i("Return value is either a dict or a list of dicts if you sent a list of inputs"),F2=f(),_(ea.$$.fragment),K2=f(),ta=r("table"),Kq=r("thead"),rl=r("tr"),yp=r("th"),CP=i("Returned values"),GP=f(),Jq=r("th"),LP=f(),fe=r("tbody"),ol=r("tr"),bp=r("td"),Wq=r("strong"),UP=i("sequence"),zP=f(),Ep=r("td"),MP=i("The actual sequence of tokens that ran against the model (may contain special tokens)"),FP=f(),ll=r("tr"),wp=r("td"),Yq=r("strong"),KP=i("score"),JP=f(),Tp=r("td"),WP=i("The probability for this token."),YP=f(),il=r("tr"),jp=r("td"),Vq=r("strong"),VP=i("token"),XP=f(),kp=r("td"),QP=i("The id of the token"),ZP=f(),ul=r("tr"),Ap=r("td"),Xq=r("strong"),eR=i("token_str"),tR=f(),Dp=r("td"),sR=i("The string representation of the token"),J2=f(),Ye=r("h2"),sa=r("a"),Qq=r("span"),_(cl.$$.fragment),aR=f(),Zq=r("span"),nR=i("Automatic speech recognition task"),W2=f(),Op=r("p"),rR=i(`This task reads some audio input and outputs the said words within the
audio files.`),Y2=f(),_(aa.$$.fragment),V2=f(),_(na.$$.fragment),X2=f(),pe=r("p"),oR=i("Available with: "),fl=r("a"),lR=i("\u{1F917} Transformers"),iR=f(),pl=r("a"),uR=i("ESPnet"),cR=i(` and
`),hl=r("a"),fR=i("SpeechBrain"),Q2=f(),Pp=r("p"),pR=i("Request:"),Z2=f(),_(ra.$$.fragment),ev=f(),Rp=r("p"),hR=i(`When sending your request, you should send a binary payload that simply
contains your audio file. We try to support most formats (Flac, Wav,
Mp3, Ogg etc...). And we automatically rescale the sampling rate to the
appropriate rate for the given model (usually 16KHz).`),tv=f(),oa=r("table"),e$=r("thead"),dl=r("tr"),Np=r("th"),dR=i("All parameters"),gR=f(),t$=r("th"),mR=f(),s$=r("tbody"),gl=r("tr"),ml=r("td"),a$=r("strong"),qR=i("no parameter"),$R=i(" (required)"),_R=f(),Sp=r("td"),vR=i("a binary representation of the audio file. No other parameters are currently allowed."),sv=f(),xp=r("p"),yR=i("Return value is either a dict or a list of dicts if you sent a list of inputs"),av=f(),Ip=r("p"),bR=i("Response:"),nv=f(),_(la.$$.fragment),rv=f(),ia=r("table"),n$=r("thead"),ql=r("tr"),Hp=r("th"),ER=i("Returned values"),wR=f(),r$=r("th"),TR=f(),o$=r("tbody"),$l=r("tr"),Bp=r("td"),l$=r("strong"),jR=i("text"),kR=f(),Cp=r("td"),AR=i("The string that was recognized within the audio file."),ov=f(),Ve=r("h2"),ua=r("a"),i$=r("span"),_(_l.$$.fragment),DR=f(),u$=r("span"),OR=i("Feature-extraction task"),lv=f(),Gp=r("p"),PR=i(`This task reads some text and outputs raw float values, that are usually
consumed as part of a semantic database/semantic search.`),iv=f(),_(ca.$$.fragment),uv=f(),Xe=r("p"),RR=i("Available with: "),vl=r("a"),NR=i("\u{1F917} Transformers"),SR=f(),yl=r("a"),xR=i("Sentence-transformers"),cv=f(),Lp=r("p"),IR=i("Request:"),fv=f(),fa=r("table"),c$=r("thead"),bl=r("tr"),Up=r("th"),HR=i("All parameters"),BR=f(),f$=r("th"),CR=f(),ae=r("tbody"),El=r("tr"),wl=r("td"),p$=r("strong"),GR=i("inputs"),LR=i(" (required):"),UR=f(),zp=r("td"),zR=i("a string or a list of strings to get the features from."),MR=f(),Tl=r("tr"),Mp=r("td"),h$=r("strong"),FR=i("options"),KR=f(),Fp=r("td"),JR=i("a dict containing the following keys:"),WR=f(),jl=r("tr"),Kp=r("td"),YR=i("use_gpu"),VR=f(),pa=r("td"),XR=i("(Default: "),d$=r("code"),QR=i("false"),ZR=i("). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),eN=f(),kl=r("tr"),Jp=r("td"),tN=i("use_cache"),sN=f(),ha=r("td"),aN=i("(Default: "),g$=r("code"),nN=i("true"),rN=i("). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),oN=f(),Al=r("tr"),Wp=r("td"),lN=i("wait_for_model"),iN=f(),da=r("td"),uN=i("(Default: "),m$=r("code"),cN=i("false"),fN=i(") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),pv=f(),Yp=r("p"),pN=i("Return value is either a dict or a list of dicts if you sent a list of inputs"),hv=f(),ga=r("table"),q$=r("thead"),Dl=r("tr"),Vp=r("th"),hN=i("Returned values"),dN=f(),$$=r("th"),gN=f(),_$=r("tbody"),Ol=r("tr"),Xp=r("td"),v$=r("strong"),mN=i("A list of float (or list of list of floats)"),qN=f(),Qp=r("td"),$N=i("The numbers that are the representation features of the input."),dv=f(),Zp=r("small"),_N=i(`Returned values are a list of floats, or a list of list of floats
(depending on if you sent a string or a list of string, and if the
automatic reduction, usually mean_pooling for instance was applied for
you or not. This should be explained on the model's README.`),gv=f(),Qe=r("h2"),ma=r("a"),y$=r("span"),_(Pl.$$.fragment),vN=f(),b$=r("span"),yN=i("Audio-classification task"),mv=f(),eh=r("p"),bN=i("This task reads some audio input and outputs the likelihood of classes."),qv=f(),_(qa.$$.fragment),$v=f(),Ze=r("p"),EN=i("Available with: "),Rl=r("a"),wN=i("\u{1F917} Transformers"),TN=f(),Nl=r("a"),jN=i("SpeechBrain"),_v=f(),th=r("p"),kN=i("Request:"),vv=f(),_($a.$$.fragment),yv=f(),sh=r("p"),AN=i(`When sending your request, you should send a binary payload that simply
contains your audio file. We try to support most formats (Flac, Wav,
Mp3, Ogg etc...). And we automatically rescale the sampling rate to the
appropriate rate for the given model (usually 16KHz).`),bv=f(),_a=r("table"),E$=r("thead"),Sl=r("tr"),ah=r("th"),DN=i("All parameters"),ON=f(),w$=r("th"),PN=f(),T$=r("tbody"),xl=r("tr"),Il=r("td"),j$=r("strong"),RN=i("no parameter"),NN=i(" (required)"),SN=f(),nh=r("td"),xN=i("a binary representation of the audio file. No other parameters are currently allowed."),Ev=f(),rh=r("p"),IN=i("Return value is a dict"),wv=f(),_(va.$$.fragment),Tv=f(),ya=r("table"),k$=r("thead"),Hl=r("tr"),oh=r("th"),HN=i("Returned values"),BN=f(),A$=r("th"),CN=f(),Bl=r("tbody"),Cl=r("tr"),lh=r("td"),D$=r("strong"),GN=i("label"),LN=f(),ih=r("td"),UN=i("The label for the class (model specific)"),zN=f(),Gl=r("tr"),uh=r("td"),O$=r("strong"),MN=i("score"),FN=f(),ch=r("td"),KN=i("A float that represents how likely it is that the audio file belongs to this class."),jv=f(),et=r("h2"),ba=r("a"),P$=r("span"),_(Ll.$$.fragment),JN=f(),R$=r("span"),WN=i("Object-detection task"),kv=f(),fh=r("p"),YN=i(`This task reads some image input and outputs the likelihood of classes &
bounding boxes of detected objects.`),Av=f(),_(Ea.$$.fragment),Dv=f(),Ul=r("p"),VN=i("Available with: "),zl=r("a"),XN=i("\u{1F917} Transformers"),Ov=f(),ph=r("p"),QN=i("Request:"),Pv=f(),_(wa.$$.fragment),Rv=f(),Ta=r("p"),ZN=i(`When sending your request, you should send a binary payload that simply
contains your image file. We support all image formats `),Ml=r("a"),eS=i(`Pillow
supports`),tS=i("."),Nv=f(),ja=r("table"),N$=r("thead"),Fl=r("tr"),hh=r("th"),sS=i("All parameters"),aS=f(),S$=r("th"),nS=f(),x$=r("tbody"),Kl=r("tr"),Jl=r("td"),I$=r("strong"),rS=i("no parameter"),oS=i(" (required)"),lS=f(),dh=r("td"),iS=i("a binary representation of the image file. No other parameters are currently allowed."),Sv=f(),gh=r("p"),uS=i("Return value is a dict"),xv=f(),_(ka.$$.fragment),Iv=f(),Aa=r("table"),H$=r("thead"),Wl=r("tr"),mh=r("th"),cS=i("Returned values"),fS=f(),B$=r("th"),pS=f(),tt=r("tbody"),Yl=r("tr"),qh=r("td"),C$=r("strong"),hS=i("label"),dS=f(),$h=r("td"),gS=i("The label for the class (model specific) of a detected object."),mS=f(),Vl=r("tr"),_h=r("td"),G$=r("strong"),qS=i("score"),$S=f(),vh=r("td"),_S=i("A float that represents how likely it is that the detected object belongs to the given class."),vS=f(),Xl=r("tr"),yh=r("td"),L$=r("strong"),yS=i("box"),bS=f(),bh=r("td"),ES=i("A dict (with keys [xmin,ymin,xmax,ymax]) representing the bounding box of a detected object."),Hv=f(),st=r("h2"),Da=r("a"),U$=r("span"),_(Ql.$$.fragment),wS=f(),z$=r("span"),TS=i("Image Segmentation task"),Bv=f(),Eh=r("p"),jS=i(`This task reads some image input and outputs the likelihood of classes &
bounding boxes of detected objects.`),Cv=f(),_(Oa.$$.fragment),Gv=f(),Zl=r("p"),kS=i("Available with: "),ei=r("a"),AS=i("\u{1F917} Transformers"),Lv=f(),wh=r("p"),DS=i("Request:"),Uv=f(),_(Pa.$$.fragment),zv=f(),Ra=r("p"),OS=i(`When sending your request, you should send a binary payload that simply
contains your image file. We support all image formats `),ti=r("a"),PS=i(`Pillow
supports`),RS=i("."),Mv=f(),Na=r("table"),M$=r("thead"),si=r("tr"),Th=r("th"),NS=i("All parameters"),SS=f(),F$=r("th"),xS=f(),K$=r("tbody"),ai=r("tr"),ni=r("td"),J$=r("strong"),IS=i("no parameter"),HS=i(" (required)"),BS=f(),jh=r("td"),CS=i("a binary representation of the image file. No other parameters are currently allowed."),Fv=f(),kh=r("p"),GS=i("Return value is a dict"),Kv=f(),_(Sa.$$.fragment),Jv=f(),xa=r("table"),W$=r("thead"),ri=r("tr"),Ah=r("th"),LS=i("Returned values"),US=f(),Y$=r("th"),zS=f(),at=r("tbody"),oi=r("tr"),Dh=r("td"),V$=r("strong"),MS=i("label"),FS=f(),Oh=r("td"),KS=i("The label for the class (model specific) of a segment."),JS=f(),li=r("tr"),Ph=r("td"),X$=r("strong"),WS=i("score"),YS=f(),Rh=r("td"),VS=i("A float that represents how likely it is that the segment belongs to the given class."),XS=f(),ii=r("tr"),Nh=r("td"),Q$=r("strong"),QS=i("mask"),ZS=f(),Sh=r("td"),ex=i("A str (base64 str of a single channel black-and-white img) representing the mask of a segment."),Wv=f(),nt=r("h2"),Ia=r("a"),Z$=r("span"),_(ui.$$.fragment),tx=f(),e_=r("span"),sx=i("Image Classification task"),Yv=f(),xh=r("p"),ax=i("This task reads some image input and outputs the likelihood of classes."),Vv=f(),_(Ha.$$.fragment),Xv=f(),ci=r("p"),nx=i("Available with: "),fi=r("a"),rx=i("\u{1F917} Transformers"),Qv=f(),Ih=r("p"),ox=i("Request:"),Zv=f(),_(Ba.$$.fragment),e0=f(),Ca=r("p"),lx=i(`When sending your request, you should send a binary payload that simply
contains your image file. We support all image formats `),pi=r("a"),ix=i(`Pillow
supports`),ux=i("."),t0=f(),Ga=r("table"),t_=r("thead"),hi=r("tr"),Hh=r("th"),cx=i("All parameters"),fx=f(),s_=r("th"),px=f(),a_=r("tbody"),di=r("tr"),gi=r("td"),n_=r("strong"),hx=i("no parameter"),dx=i(" (required)"),gx=f(),Bh=r("td"),mx=i("a binary representation of the image file. No other parameters are currently allowed."),s0=f(),Ch=r("p"),qx=i("Return value is a dict"),a0=f(),_(La.$$.fragment),n0=f(),Ua=r("table"),r_=r("thead"),mi=r("tr"),Gh=r("th"),$x=i("Returned values"),_x=f(),o_=r("th"),vx=f(),qi=r("tbody"),$i=r("tr"),Lh=r("td"),l_=r("strong"),yx=i("label"),bx=f(),Uh=r("td"),Ex=i("The label for the class (model specific)"),wx=f(),_i=r("tr"),zh=r("td"),i_=r("strong"),Tx=i("score"),jx=f(),Mh=r("td"),kx=i("A float that represents how likely it is that the image file belongs to this class."),this.h()},l(a){const g=nK('[data-svelte="svelte-1phssyn"]',document.head);n=o(g,"META",{name:!0,content:!0}),g.forEach(s),c=p(a),t=o(a,"H1",{class:!0});var vi=l(t);d=o(vi,"A",{id:!0,class:!0,href:!0});var u_=l(d);q=o(u_,"SPAN",{});var c_=l(q);v(k.$$.fragment,c_),c_.forEach(s),u_.forEach(s),A=p(vi),j=o(vi,"SPAN",{});var f_=l(j);T=u(f_,"Detailed parameters"),f_.forEach(s),vi.forEach(s),N=p(a),D=o(a,"H2",{class:!0});var yi=l(D);ne=o(yi,"A",{id:!0,class:!0,href:!0});var p_=l(ne);Re=o(p_,"SPAN",{});var h_=l(Re);v(Q.$$.fragment,h_),h_.forEach(s),p_.forEach(s),Y=p(yi),ot=o(yi,"SPAN",{});var d_=l(ot);Ri=u(d_,"Which task is used by this model ?"),d_.forEach(s),yi.forEach(s),Va=p(a),Ne=o(a,"P",{});var g_=l(Ne);Fw=u(g_,`In general the \u{1F917} Hosted API Inference accepts a simple string as an
input. However, more advanced usage depends on the \u201Ctask\u201D that the
model solves.`),g_.forEach(s),L_=p(a),Ni=o(a,"P",{});var m_=l(Ni);Kw=u(m_,"The \u201Ctask\u201D of a model is defined here on it\u2019s model page:"),m_.forEach(s),U_=p(a),lt=o(a,"IMG",{class:!0,src:!0,width:!0}),z_=p(a),it=o(a,"IMG",{class:!0,src:!0,width:!0}),M_=p(a),Se=o(a,"H2",{class:!0});var bi=l(Se);ut=o(bi,"A",{id:!0,class:!0,href:!0});var q_=l(ut);cd=o(q_,"SPAN",{});var $_=l(cd);v(Xa.$$.fragment,$_),$_.forEach(s),q_.forEach(s),Jw=p(bi),fd=o(bi,"SPAN",{});var __=l(fd);Ww=u(__,"Zero-shot classification task"),__.forEach(s),bi.forEach(s),F_=p(a),Si=o(a,"P",{});var v_=l(Si);Yw=u(v_,`This task is super useful to try out classification with zero code,
you simply pass a sentence/paragraph and the possible labels for that
sentence, and you get a result.`),v_.forEach(s),K_=p(a),v(ct.$$.fragment,a),J_=p(a),Qa=o(a,"P",{});var Fh=l(Qa);Vw=u(Fh,"Available with: "),Za=o(Fh,"A",{href:!0,rel:!0});var y_=l(Za);Xw=u(y_,"\u{1F917} Transformers"),y_.forEach(s),Fh.forEach(s),W_=p(a),xi=o(a,"P",{});var b_=l(xi);Qw=u(b_,"Request:"),b_.forEach(s),Y_=p(a),v(ft.$$.fragment,a),V_=p(a),Ii=o(a,"P",{});var E_=l(Ii);Zw=u(E_,`When sending your request, you should send a JSON encoded payload. Here
are all the options`),E_.forEach(s),X_=p(a),pt=o(a,"TABLE",{});var Ei=l(pt);pd=o(Ei,"THEAD",{});var w_=l(pd);en=o(w_,"TR",{});var wi=l(en);Hi=o(wi,"TH",{align:!0});var T_=l(Hi);e3=u(T_,"All parameters"),T_.forEach(s),t3=p(wi),hd=o(wi,"TH",{align:!0}),l(hd).forEach(s),wi.forEach(s),w_.forEach(s),s3=p(Ei),M=o(Ei,"TBODY",{});var F=l(M);tn=o(F,"TR",{});var Ti=l(tn);sn=o(Ti,"TD",{align:!0});var Kh=l(sn);dd=o(Kh,"STRONG",{});var j_=l(dd);a3=u(j_,"inputs"),j_.forEach(s),n3=u(Kh," (required)"),Kh.forEach(s),r3=p(Ti),Bi=o(Ti,"TD",{align:!0});var k_=l(Bi);o3=u(k_,"a string or list of strings"),k_.forEach(s),Ti.forEach(s),l3=p(F),an=o(F,"TR",{});var ji=l(an);nn=o(ji,"TD",{align:!0});var Jh=l(nn);gd=o(Jh,"STRONG",{});var A_=l(gd);i3=u(A_,"parameters"),A_.forEach(s),u3=u(Jh," (required)"),Jh.forEach(s),c3=p(ji),Ci=o(ji,"TD",{align:!0});var D_=l(Ci);f3=u(D_,"a dict containing the following keys:"),D_.forEach(s),ji.forEach(s),p3=p(F),rn=o(F,"TR",{});var ki=l(rn);Gi=o(ki,"TD",{align:!0});var O_=l(Gi);h3=u(O_,"candidate_labels (required)"),O_.forEach(s),d3=p(ki),he=o(ki,"TD",{align:!0});var rt=l(he);g3=u(rt,"a list of strings that are potential classes for "),md=o(rt,"CODE",{});var P_=l(md);m3=u(P_,"inputs"),P_.forEach(s),q3=u(rt,". (max 10 candidate_labels, for more, simply run multiple requests, results are going to be misleading if using too many candidate_labels anyway. If you want to keep the exact same, you can simply run "),qd=o(rt,"CODE",{});var R_=l(qd);$3=u(R_,"multi_label=True"),R_.forEach(s),_3=u(rt," and do the scaling on your end. )"),rt.forEach(s),ki.forEach(s),v3=p(F),on=o(F,"TR",{});var Ai=l(on);Li=o(Ai,"TD",{align:!0});var N_=l(Li);y3=u(N_,"multi_label"),N_.forEach(s),b3=p(Ai),ht=o(Ai,"TD",{align:!0});var Di=l(ht);E3=u(Di,"(Default: "),$d=o(Di,"CODE",{});var S_=l($d);w3=u(S_,"false"),S_.forEach(s),T3=u(Di,") Boolean that is set to True if classes can overlap"),Di.forEach(s),Ai.forEach(s),j3=p(F),ln=o(F,"TR",{});var Oi=l(ln);Ui=o(Oi,"TD",{align:!0});var x_=l(Ui);_d=o(x_,"STRONG",{});var I_=l(_d);k3=u(I_,"options"),I_.forEach(s),x_.forEach(s),A3=p(Oi),zi=o(Oi,"TD",{align:!0});var tI=l(zi);D3=u(tI,"a dict containing the following keys:"),tI.forEach(s),Oi.forEach(s),O3=p(F),un=o(F,"TR",{});var o0=l(un);Mi=o(o0,"TD",{align:!0});var sI=l(Mi);P3=u(sI,"use_gpu"),sI.forEach(s),R3=p(o0),dt=o(o0,"TD",{align:!0});var l0=l(dt);N3=u(l0,"(Default: "),vd=o(l0,"CODE",{});var aI=l(vd);S3=u(aI,"false"),aI.forEach(s),x3=u(l0,"). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),l0.forEach(s),o0.forEach(s),I3=p(F),cn=o(F,"TR",{});var i0=l(cn);Fi=o(i0,"TD",{align:!0});var nI=l(Fi);H3=u(nI,"use_cache"),nI.forEach(s),B3=p(i0),gt=o(i0,"TD",{align:!0});var u0=l(gt);C3=u(u0,"(Default: "),yd=o(u0,"CODE",{});var rI=l(yd);G3=u(rI,"true"),rI.forEach(s),L3=u(u0,"). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),u0.forEach(s),i0.forEach(s),U3=p(F),fn=o(F,"TR",{});var c0=l(fn);Ki=o(c0,"TD",{align:!0});var oI=l(Ki);z3=u(oI,"wait_for_model"),oI.forEach(s),M3=p(c0),mt=o(c0,"TD",{align:!0});var f0=l(mt);F3=u(f0,"(Default: "),bd=o(f0,"CODE",{});var lI=l(bd);K3=u(lI,"false"),lI.forEach(s),J3=u(f0,") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),f0.forEach(s),c0.forEach(s),F.forEach(s),Ei.forEach(s),Q_=p(a),Ji=o(a,"P",{});var iI=l(Ji);W3=u(iI,"Return value is either a dict or a list of dicts if you sent a list of inputs"),iI.forEach(s),Z_=p(a),Wi=o(a,"P",{});var uI=l(Wi);Y3=u(uI,"Response:"),uI.forEach(s),e1=p(a),v(qt.$$.fragment,a),t1=p(a),$t=o(a,"TABLE",{});var p0=l($t);Ed=o(p0,"THEAD",{});var cI=l(Ed);pn=o(cI,"TR",{});var h0=l(pn);Yi=o(h0,"TH",{align:!0});var fI=l(Yi);V3=u(fI,"Returned values"),fI.forEach(s),X3=p(h0),wd=o(h0,"TH",{align:!0}),l(wd).forEach(s),h0.forEach(s),cI.forEach(s),Q3=p(p0),xe=o(p0,"TBODY",{});var Wh=l(xe);hn=o(Wh,"TR",{});var d0=l(hn);Vi=o(d0,"TD",{align:!0});var pI=l(Vi);Td=o(pI,"STRONG",{});var hI=l(Td);Z3=u(hI,"sequence"),hI.forEach(s),pI.forEach(s),eT=p(d0),Xi=o(d0,"TD",{align:!0});var dI=l(Xi);tT=u(dI,"The string sent as an input"),dI.forEach(s),d0.forEach(s),sT=p(Wh),dn=o(Wh,"TR",{});var g0=l(dn);Qi=o(g0,"TD",{align:!0});var gI=l(Qi);jd=o(gI,"STRONG",{});var mI=l(jd);aT=u(mI,"labels"),mI.forEach(s),gI.forEach(s),nT=p(g0),Zi=o(g0,"TD",{align:!0});var qI=l(Zi);rT=u(qI,"The list of strings for labels that you sent (in order)"),qI.forEach(s),g0.forEach(s),oT=p(Wh),gn=o(Wh,"TR",{});var m0=l(gn);eu=o(m0,"TD",{align:!0});var $I=l(eu);kd=o($I,"STRONG",{});var _I=l(kd);lT=u(_I,"scores"),_I.forEach(s),$I.forEach(s),iT=p(m0),_t=o(m0,"TD",{align:!0});var q0=l(_t);uT=u(q0,"a list of floats that correspond the the probability of label, in the same order as "),Ad=o(q0,"CODE",{});var vI=l(Ad);cT=u(vI,"labels"),vI.forEach(s),fT=u(q0,"."),q0.forEach(s),m0.forEach(s),Wh.forEach(s),p0.forEach(s),s1=p(a),Ie=o(a,"H2",{class:!0});var $0=l(Ie);vt=o($0,"A",{id:!0,class:!0,href:!0});var yI=l(vt);Dd=o(yI,"SPAN",{});var bI=l(Dd);v(mn.$$.fragment,bI),bI.forEach(s),yI.forEach(s),pT=p($0),Od=o($0,"SPAN",{});var EI=l(Od);hT=u(EI,"Translation task"),EI.forEach(s),$0.forEach(s),a1=p(a),tu=o(a,"P",{});var wI=l(tu);dT=u(wI,"This task is well known to translate text from one language to another"),wI.forEach(s),n1=p(a),v(yt.$$.fragment,a),r1=p(a),qn=o(a,"P",{});var Ax=l(qn);gT=u(Ax,"Available with: "),$n=o(Ax,"A",{href:!0,rel:!0});var TI=l($n);mT=u(TI,"\u{1F917} Transformers"),TI.forEach(s),Ax.forEach(s),o1=p(a),su=o(a,"P",{});var jI=l(su);qT=u(jI,"Example:"),jI.forEach(s),l1=p(a),v(bt.$$.fragment,a),i1=p(a),au=o(a,"P",{});var kI=l(au);$T=u(kI,`When sending your request, you should send a JSON encoded payload. Here
are all the options`),kI.forEach(s),u1=p(a),Et=o(a,"TABLE",{});var _0=l(Et);Pd=o(_0,"THEAD",{});var AI=l(Pd);_n=o(AI,"TR",{});var v0=l(_n);nu=o(v0,"TH",{align:!0});var DI=l(nu);_T=u(DI,"All parameters"),DI.forEach(s),vT=p(v0),Rd=o(v0,"TH",{align:!0}),l(Rd).forEach(s),v0.forEach(s),AI.forEach(s),yT=p(_0),Z=o(_0,"TBODY",{});var ke=l(Z);vn=o(ke,"TR",{});var y0=l(vn);yn=o(y0,"TD",{align:!0});var Dx=l(yn);Nd=o(Dx,"STRONG",{});var OI=l(Nd);bT=u(OI,"inputs"),OI.forEach(s),ET=u(Dx," (required)"),Dx.forEach(s),wT=p(y0),ru=o(y0,"TD",{align:!0});var PI=l(ru);TT=u(PI,"a string to be translated in the original languages"),PI.forEach(s),y0.forEach(s),jT=p(ke),bn=o(ke,"TR",{});var b0=l(bn);ou=o(b0,"TD",{align:!0});var RI=l(ou);Sd=o(RI,"STRONG",{});var NI=l(Sd);kT=u(NI,"options"),NI.forEach(s),RI.forEach(s),AT=p(b0),lu=o(b0,"TD",{align:!0});var SI=l(lu);DT=u(SI,"a dict containing the following keys:"),SI.forEach(s),b0.forEach(s),OT=p(ke),En=o(ke,"TR",{});var E0=l(En);iu=o(E0,"TD",{align:!0});var xI=l(iu);PT=u(xI,"use_gpu"),xI.forEach(s),RT=p(E0),wt=o(E0,"TD",{align:!0});var w0=l(wt);NT=u(w0,"(Default: "),xd=o(w0,"CODE",{});var II=l(xd);ST=u(II,"false"),II.forEach(s),xT=u(w0,"). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),w0.forEach(s),E0.forEach(s),IT=p(ke),wn=o(ke,"TR",{});var T0=l(wn);uu=o(T0,"TD",{align:!0});var HI=l(uu);HT=u(HI,"use_cache"),HI.forEach(s),BT=p(T0),Tt=o(T0,"TD",{align:!0});var j0=l(Tt);CT=u(j0,"(Default: "),Id=o(j0,"CODE",{});var BI=l(Id);GT=u(BI,"true"),BI.forEach(s),LT=u(j0,"). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),j0.forEach(s),T0.forEach(s),UT=p(ke),Tn=o(ke,"TR",{});var k0=l(Tn);cu=o(k0,"TD",{align:!0});var CI=l(cu);zT=u(CI,"wait_for_model"),CI.forEach(s),MT=p(k0),jt=o(k0,"TD",{align:!0});var A0=l(jt);FT=u(A0,"(Default: "),Hd=o(A0,"CODE",{});var GI=l(Hd);KT=u(GI,"false"),GI.forEach(s),JT=u(A0,") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),A0.forEach(s),k0.forEach(s),ke.forEach(s),_0.forEach(s),c1=p(a),fu=o(a,"P",{});var LI=l(fu);WT=u(LI,"Return value is either a dict or a list of dicts if you sent a list of inputs"),LI.forEach(s),f1=p(a),kt=o(a,"TABLE",{});var D0=l(kt);Bd=o(D0,"THEAD",{});var UI=l(Bd);jn=o(UI,"TR",{});var O0=l(jn);pu=o(O0,"TH",{align:!0});var zI=l(pu);YT=u(zI,"Returned values"),zI.forEach(s),VT=p(O0),Cd=o(O0,"TH",{align:!0}),l(Cd).forEach(s),O0.forEach(s),UI.forEach(s),XT=p(D0),Gd=o(D0,"TBODY",{});var MI=l(Gd);kn=o(MI,"TR",{});var P0=l(kn);hu=o(P0,"TD",{align:!0});var FI=l(hu);Ld=o(FI,"STRONG",{});var KI=l(Ld);QT=u(KI,"translation_text"),KI.forEach(s),FI.forEach(s),ZT=p(P0),du=o(P0,"TD",{align:!0});var JI=l(du);e4=u(JI,"The string after translation"),JI.forEach(s),P0.forEach(s),MI.forEach(s),D0.forEach(s),p1=p(a),He=o(a,"H2",{class:!0});var R0=l(He);At=o(R0,"A",{id:!0,class:!0,href:!0});var WI=l(At);Ud=o(WI,"SPAN",{});var YI=l(Ud);v(An.$$.fragment,YI),YI.forEach(s),WI.forEach(s),t4=p(R0),zd=o(R0,"SPAN",{});var VI=l(zd);s4=u(VI,"Summarization task"),VI.forEach(s),R0.forEach(s),h1=p(a),Dt=o(a,"P",{});var N0=l(Dt);a4=u(N0,`This task is well known to summarize longer text into shorter text.
Be careful, some models have a maximum length of input. That means that
the summary cannot handle full books for instance. Be careful when
choosing your model. If you want to discuss your summarization needs,
please get in touch with us: <`),gu=o(N0,"A",{href:!0});var XI=l(gu);n4=u(XI,"api-enterprise@huggingface.co"),XI.forEach(s),r4=u(N0,">"),N0.forEach(s),d1=p(a),v(Ot.$$.fragment,a),g1=p(a),Dn=o(a,"P",{});var Ox=l(Dn);o4=u(Ox,"Available with: "),On=o(Ox,"A",{href:!0,rel:!0});var QI=l(On);l4=u(QI,"\u{1F917} Transformers"),QI.forEach(s),Ox.forEach(s),m1=p(a),mu=o(a,"P",{});var ZI=l(mu);i4=u(ZI,"Example:"),ZI.forEach(s),q1=p(a),v(Pt.$$.fragment,a),$1=p(a),qu=o(a,"P",{});var eH=l(qu);u4=u(eH,`When sending your request, you should send a JSON encoded payload. Here
are all the options`),eH.forEach(s),_1=p(a),Rt=o(a,"TABLE",{});var S0=l(Rt);Md=o(S0,"THEAD",{});var tH=l(Md);Pn=o(tH,"TR",{});var x0=l(Pn);$u=o(x0,"TH",{align:!0});var sH=l($u);c4=u(sH,"All parameters"),sH.forEach(s),f4=p(x0),Fd=o(x0,"TH",{align:!0}),l(Fd).forEach(s),x0.forEach(s),tH.forEach(s),p4=p(S0),G=o(S0,"TBODY",{});var U=l(G);Rn=o(U,"TR",{});var I0=l(Rn);Nn=o(I0,"TD",{align:!0});var Px=l(Nn);Kd=o(Px,"STRONG",{});var aH=l(Kd);h4=u(aH,"inputs"),aH.forEach(s),d4=u(Px," (required)"),Px.forEach(s),g4=p(I0),_u=o(I0,"TD",{align:!0});var nH=l(_u);m4=u(nH,"a string to be summarized"),nH.forEach(s),I0.forEach(s),q4=p(U),Sn=o(U,"TR",{});var H0=l(Sn);vu=o(H0,"TD",{align:!0});var rH=l(vu);Jd=o(rH,"STRONG",{});var oH=l(Jd);$4=u(oH,"parameters"),oH.forEach(s),rH.forEach(s),_4=p(H0),yu=o(H0,"TD",{align:!0});var lH=l(yu);v4=u(lH,"a dict containing the following keys:"),lH.forEach(s),H0.forEach(s),y4=p(U),xn=o(U,"TR",{});var B0=l(xn);bu=o(B0,"TD",{align:!0});var iH=l(bu);b4=u(iH,"min_length"),iH.forEach(s),E4=p(B0),de=o(B0,"TD",{align:!0});var Yh=l(de);w4=u(Yh,"(Default: "),Wd=o(Yh,"CODE",{});var uH=l(Wd);T4=u(uH,"None"),uH.forEach(s),j4=u(Yh,"). Integer to define the minimum length "),Yd=o(Yh,"STRONG",{});var cH=l(Yd);k4=u(cH,"in tokens"),cH.forEach(s),A4=u(Yh," of the output summary."),Yh.forEach(s),B0.forEach(s),D4=p(U),In=o(U,"TR",{});var C0=l(In);Eu=o(C0,"TD",{align:!0});var fH=l(Eu);O4=u(fH,"max_length"),fH.forEach(s),P4=p(C0),ge=o(C0,"TD",{align:!0});var Vh=l(ge);R4=u(Vh,"(Default: "),Vd=o(Vh,"CODE",{});var pH=l(Vd);N4=u(pH,"None"),pH.forEach(s),S4=u(Vh,"). Integer to define the maximum length "),Xd=o(Vh,"STRONG",{});var hH=l(Xd);x4=u(hH,"in tokens"),hH.forEach(s),I4=u(Vh," of the output summary."),Vh.forEach(s),C0.forEach(s),H4=p(U),Hn=o(U,"TR",{});var G0=l(Hn);wu=o(G0,"TD",{align:!0});var dH=l(wu);B4=u(dH,"top_k"),dH.forEach(s),C4=p(G0),me=o(G0,"TD",{align:!0});var Xh=l(me);G4=u(Xh,"(Default: "),Qd=o(Xh,"CODE",{});var gH=l(Qd);L4=u(gH,"None"),gH.forEach(s),U4=u(Xh,"). Integer to define the top tokens considered within the "),Zd=o(Xh,"CODE",{});var mH=l(Zd);z4=u(mH,"sample"),mH.forEach(s),M4=u(Xh," operation to create new text."),Xh.forEach(s),G0.forEach(s),F4=p(U),Bn=o(U,"TR",{});var L0=l(Bn);Tu=o(L0,"TD",{align:!0});var qH=l(Tu);K4=u(qH,"top_p"),qH.forEach(s),J4=p(L0),re=o(L0,"TD",{align:!0});var za=l(re);W4=u(za,"(Default: "),eg=o(za,"CODE",{});var $H=l(eg);Y4=u($H,"None"),$H.forEach(s),V4=u(za,"). Float to define the tokens that are within the "),tg=o(za,"CODE",{});var _H=l(tg);X4=u(_H,"sample"),_H.forEach(s),Q4=u(za," operation of text generation. Add tokens in the sample for more probable to least probable until the sum of the probabilities is greater than "),sg=o(za,"CODE",{});var vH=l(sg);Z4=u(vH,"top_p"),vH.forEach(s),e5=u(za,"."),za.forEach(s),L0.forEach(s),t5=p(U),Cn=o(U,"TR",{});var U0=l(Cn);ju=o(U0,"TD",{align:!0});var yH=l(ju);s5=u(yH,"temperature"),yH.forEach(s),a5=p(U0),qe=o(U0,"TD",{align:!0});var Qh=l(qe);n5=u(Qh,"(Default: "),ag=o(Qh,"CODE",{});var bH=l(ag);r5=u(bH,"1.0"),bH.forEach(s),o5=u(Qh,"). Float (0.0-100.0). The temperature of the sampling operation. 1 means regular sampling, 0 means "),ng=o(Qh,"CODE",{});var EH=l(ng);l5=u(EH,"100.0"),EH.forEach(s),i5=u(Qh," is getting closer to uniform probability."),Qh.forEach(s),U0.forEach(s),u5=p(U),Gn=o(U,"TR",{});var z0=l(Gn);ku=o(z0,"TD",{align:!0});var wH=l(ku);c5=u(wH,"repetition_penalty"),wH.forEach(s),f5=p(z0),Nt=o(z0,"TD",{align:!0});var M0=l(Nt);p5=u(M0,"(Default: "),rg=o(M0,"CODE",{});var TH=l(rg);h5=u(TH,"None"),TH.forEach(s),d5=u(M0,"). Float (0.0-100.0). The more a token is used within generation the more it is penalized to not be picked in successive generation passes."),M0.forEach(s),z0.forEach(s),g5=p(U),Ln=o(U,"TR",{});var F0=l(Ln);Au=o(F0,"TD",{align:!0});var jH=l(Au);m5=u(jH,"max_time"),jH.forEach(s),q5=p(F0),St=o(F0,"TD",{align:!0});var K0=l(St);$5=u(K0,"(Default: "),og=o(K0,"CODE",{});var kH=l(og);_5=u(kH,"None"),kH.forEach(s),v5=u(K0,"). Float (0-120.0). The amount of time in seconds that the query should take maximum. Network can cause some overhead so it will be a soft limit."),K0.forEach(s),F0.forEach(s),y5=p(U),Un=o(U,"TR",{});var J0=l(Un);Du=o(J0,"TD",{align:!0});var AH=l(Du);lg=o(AH,"STRONG",{});var DH=l(lg);b5=u(DH,"options"),DH.forEach(s),AH.forEach(s),E5=p(J0),Ou=o(J0,"TD",{align:!0});var OH=l(Ou);w5=u(OH,"a dict containing the following keys:"),OH.forEach(s),J0.forEach(s),T5=p(U),zn=o(U,"TR",{});var W0=l(zn);Pu=o(W0,"TD",{align:!0});var PH=l(Pu);j5=u(PH,"use_gpu"),PH.forEach(s),k5=p(W0),xt=o(W0,"TD",{align:!0});var Y0=l(xt);A5=u(Y0,"(Default: "),ig=o(Y0,"CODE",{});var RH=l(ig);D5=u(RH,"false"),RH.forEach(s),O5=u(Y0,"). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),Y0.forEach(s),W0.forEach(s),P5=p(U),Mn=o(U,"TR",{});var V0=l(Mn);Ru=o(V0,"TD",{align:!0});var NH=l(Ru);R5=u(NH,"use_cache"),NH.forEach(s),N5=p(V0),It=o(V0,"TD",{align:!0});var X0=l(It);S5=u(X0,"(Default: "),ug=o(X0,"CODE",{});var SH=l(ug);x5=u(SH,"true"),SH.forEach(s),I5=u(X0,"). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),X0.forEach(s),V0.forEach(s),H5=p(U),Fn=o(U,"TR",{});var Q0=l(Fn);Nu=o(Q0,"TD",{align:!0});var xH=l(Nu);B5=u(xH,"wait_for_model"),xH.forEach(s),C5=p(Q0),Ht=o(Q0,"TD",{align:!0});var Z0=l(Ht);G5=u(Z0,"(Default: "),cg=o(Z0,"CODE",{});var IH=l(cg);L5=u(IH,"false"),IH.forEach(s),U5=u(Z0,") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),Z0.forEach(s),Q0.forEach(s),U.forEach(s),S0.forEach(s),v1=p(a),Su=o(a,"P",{});var HH=l(Su);z5=u(HH,"Return value is either a dict or a list of dicts if you sent a list of inputs"),HH.forEach(s),y1=p(a),Bt=o(a,"TABLE",{});var ey=l(Bt);fg=o(ey,"THEAD",{});var BH=l(fg);Kn=o(BH,"TR",{});var ty=l(Kn);xu=o(ty,"TH",{align:!0});var CH=l(xu);M5=u(CH,"Returned values"),CH.forEach(s),F5=p(ty),pg=o(ty,"TH",{align:!0}),l(pg).forEach(s),ty.forEach(s),BH.forEach(s),K5=p(ey),hg=o(ey,"TBODY",{});var GH=l(hg);Jn=o(GH,"TR",{});var sy=l(Jn);Iu=o(sy,"TD",{align:!0});var LH=l(Iu);dg=o(LH,"STRONG",{});var UH=l(dg);J5=u(UH,"summarization_text"),UH.forEach(s),LH.forEach(s),W5=p(sy),Hu=o(sy,"TD",{align:!0});var zH=l(Hu);Y5=u(zH,"The string after translation"),zH.forEach(s),sy.forEach(s),GH.forEach(s),ey.forEach(s),b1=p(a),Be=o(a,"H2",{class:!0});var ay=l(Be);Ct=o(ay,"A",{id:!0,class:!0,href:!0});var MH=l(Ct);gg=o(MH,"SPAN",{});var FH=l(gg);v(Wn.$$.fragment,FH),FH.forEach(s),MH.forEach(s),V5=p(ay),mg=o(ay,"SPAN",{});var KH=l(mg);X5=u(KH,"Conversational task"),KH.forEach(s),ay.forEach(s),E1=p(a),Bu=o(a,"P",{});var JH=l(Bu);Q5=u(JH,`This task corresponds to any chatbot like structure. Models tend to have
shorter max_length, so please check with caution when using a given
model if you need long range dependency or not.`),JH.forEach(s),w1=p(a),v(Gt.$$.fragment,a),T1=p(a),Yn=o(a,"P",{});var Rx=l(Yn);Z5=u(Rx,"Available with: "),Vn=o(Rx,"A",{href:!0,rel:!0});var WH=l(Vn);ej=u(WH,"\u{1F917} Transformers"),WH.forEach(s),Rx.forEach(s),j1=p(a),Cu=o(a,"P",{});var YH=l(Cu);tj=u(YH,"Example:"),YH.forEach(s),k1=p(a),v(Lt.$$.fragment,a),A1=p(a),Gu=o(a,"P",{});var VH=l(Gu);sj=u(VH,`When sending your request, you should send a JSON encoded payload. Here
are all the options`),VH.forEach(s),D1=p(a),Ut=o(a,"TABLE",{});var ny=l(Ut);qg=o(ny,"THEAD",{});var XH=l(qg);Xn=o(XH,"TR",{});var ry=l(Xn);Lu=o(ry,"TH",{align:!0});var QH=l(Lu);aj=u(QH,"All parameters"),QH.forEach(s),nj=p(ry),$g=o(ry,"TH",{align:!0}),l($g).forEach(s),ry.forEach(s),XH.forEach(s),rj=p(ny),S=o(ny,"TBODY",{});var H=l(S);Qn=o(H,"TR",{});var oy=l(Qn);Zn=o(oy,"TD",{align:!0});var Nx=l(Zn);_g=o(Nx,"STRONG",{});var ZH=l(_g);oj=u(ZH,"inputs"),ZH.forEach(s),lj=u(Nx," (required)"),Nx.forEach(s),ij=p(oy),vg=o(oy,"TD",{align:!0}),l(vg).forEach(s),oy.forEach(s),uj=p(H),er=o(H,"TR",{});var ly=l(er);Uu=o(ly,"TD",{align:!0});var eB=l(Uu);cj=u(eB,"text (required)"),eB.forEach(s),fj=p(ly),zu=o(ly,"TD",{align:!0});var tB=l(zu);pj=u(tB,"The last input from the user in the conversation."),tB.forEach(s),ly.forEach(s),hj=p(H),tr=o(H,"TR",{});var iy=l(tr);Mu=o(iy,"TD",{align:!0});var sB=l(Mu);dj=u(sB,"generated_responses"),sB.forEach(s),gj=p(iy),Fu=o(iy,"TD",{align:!0});var aB=l(Fu);mj=u(aB,"A list of strings corresponding to the earlier replies from the model."),aB.forEach(s),iy.forEach(s),qj=p(H),sr=o(H,"TR",{});var uy=l(sr);Ku=o(uy,"TD",{align:!0});var nB=l(Ku);$j=u(nB,"past_user_inputs"),nB.forEach(s),_j=p(uy),zt=o(uy,"TD",{align:!0});var cy=l(zt);vj=u(cy,"A list of strings corresponding to the earlier replies from the user. Should be of the same length of "),yg=o(cy,"CODE",{});var rB=l(yg);yj=u(rB,"generated_responses"),rB.forEach(s),bj=u(cy,"."),cy.forEach(s),uy.forEach(s),Ej=p(H),ar=o(H,"TR",{});var fy=l(ar);Ju=o(fy,"TD",{align:!0});var oB=l(Ju);bg=o(oB,"STRONG",{});var lB=l(bg);wj=u(lB,"parameters"),lB.forEach(s),oB.forEach(s),Tj=p(fy),Wu=o(fy,"TD",{align:!0});var iB=l(Wu);jj=u(iB,"a dict containing the following keys:"),iB.forEach(s),fy.forEach(s),kj=p(H),nr=o(H,"TR",{});var py=l(nr);Yu=o(py,"TD",{align:!0});var uB=l(Yu);Aj=u(uB,"min_length"),uB.forEach(s),Dj=p(py),$e=o(py,"TD",{align:!0});var Zh=l($e);Oj=u(Zh,"(Default: "),Eg=o(Zh,"CODE",{});var cB=l(Eg);Pj=u(cB,"None"),cB.forEach(s),Rj=u(Zh,"). Integer to define the minimum length "),wg=o(Zh,"STRONG",{});var fB=l(wg);Nj=u(fB,"in tokens"),fB.forEach(s),Sj=u(Zh," of the output summary."),Zh.forEach(s),py.forEach(s),xj=p(H),rr=o(H,"TR",{});var hy=l(rr);Vu=o(hy,"TD",{align:!0});var pB=l(Vu);Ij=u(pB,"max_length"),pB.forEach(s),Hj=p(hy),_e=o(hy,"TD",{align:!0});var ed=l(_e);Bj=u(ed,"(Default: "),Tg=o(ed,"CODE",{});var hB=l(Tg);Cj=u(hB,"None"),hB.forEach(s),Gj=u(ed,"). Integer to define the maximum length "),jg=o(ed,"STRONG",{});var dB=l(jg);Lj=u(dB,"in tokens"),dB.forEach(s),Uj=u(ed," of the output summary."),ed.forEach(s),hy.forEach(s),zj=p(H),or=o(H,"TR",{});var dy=l(or);Xu=o(dy,"TD",{align:!0});var gB=l(Xu);Mj=u(gB,"top_k"),gB.forEach(s),Fj=p(dy),ve=o(dy,"TD",{align:!0});var td=l(ve);Kj=u(td,"(Default: "),kg=o(td,"CODE",{});var mB=l(kg);Jj=u(mB,"None"),mB.forEach(s),Wj=u(td,"). Integer to define the top tokens considered within the "),Ag=o(td,"CODE",{});var qB=l(Ag);Yj=u(qB,"sample"),qB.forEach(s),Vj=u(td," operation to create new text."),td.forEach(s),dy.forEach(s),Xj=p(H),lr=o(H,"TR",{});var gy=l(lr);Qu=o(gy,"TD",{align:!0});var $B=l(Qu);Qj=u($B,"top_p"),$B.forEach(s),Zj=p(gy),oe=o(gy,"TD",{align:!0});var Ma=l(oe);e6=u(Ma,"(Default: "),Dg=o(Ma,"CODE",{});var _B=l(Dg);t6=u(_B,"None"),_B.forEach(s),s6=u(Ma,"). Float to define the tokens that are within the "),Og=o(Ma,"CODE",{});var vB=l(Og);a6=u(vB,"sample"),vB.forEach(s),n6=u(Ma," operation of text generation. Add tokens in the sample for more probable to least probable until the sum of the probabilities is greater than "),Pg=o(Ma,"CODE",{});var yB=l(Pg);r6=u(yB,"top_p"),yB.forEach(s),o6=u(Ma,"."),Ma.forEach(s),gy.forEach(s),l6=p(H),ir=o(H,"TR",{});var my=l(ir);Zu=o(my,"TD",{align:!0});var bB=l(Zu);i6=u(bB,"temperature"),bB.forEach(s),u6=p(my),ye=o(my,"TD",{align:!0});var sd=l(ye);c6=u(sd,"(Default: "),Rg=o(sd,"CODE",{});var EB=l(Rg);f6=u(EB,"1.0"),EB.forEach(s),p6=u(sd,"). Float (0.0-100.0). The temperature of the sampling operation. 1 means regular sampling, 0 means "),Ng=o(sd,"CODE",{});var wB=l(Ng);h6=u(wB,"100.0"),wB.forEach(s),d6=u(sd," is getting closer to uniform probability."),sd.forEach(s),my.forEach(s),g6=p(H),ur=o(H,"TR",{});var qy=l(ur);ec=o(qy,"TD",{align:!0});var TB=l(ec);m6=u(TB,"repetition_penalty"),TB.forEach(s),q6=p(qy),Mt=o(qy,"TD",{align:!0});var $y=l(Mt);$6=u($y,"(Default: "),Sg=o($y,"CODE",{});var jB=l(Sg);_6=u(jB,"None"),jB.forEach(s),v6=u($y,"). Float (0.0-100.0). The more a token is used within generation the more it is penalized to not be picked in successive generation passes."),$y.forEach(s),qy.forEach(s),y6=p(H),cr=o(H,"TR",{});var _y=l(cr);tc=o(_y,"TD",{align:!0});var kB=l(tc);b6=u(kB,"max_time"),kB.forEach(s),E6=p(_y),Ft=o(_y,"TD",{align:!0});var vy=l(Ft);w6=u(vy,"(Default: "),xg=o(vy,"CODE",{});var AB=l(xg);T6=u(AB,"None"),AB.forEach(s),j6=u(vy,"). Float (0-120.0). The amount of time in seconds that the query should take maximum. Network can cause some overhead so it will be a soft limit."),vy.forEach(s),_y.forEach(s),k6=p(H),fr=o(H,"TR",{});var yy=l(fr);sc=o(yy,"TD",{align:!0});var DB=l(sc);Ig=o(DB,"STRONG",{});var OB=l(Ig);A6=u(OB,"options"),OB.forEach(s),DB.forEach(s),D6=p(yy),ac=o(yy,"TD",{align:!0});var PB=l(ac);O6=u(PB,"a dict containing the following keys:"),PB.forEach(s),yy.forEach(s),P6=p(H),pr=o(H,"TR",{});var by=l(pr);nc=o(by,"TD",{align:!0});var RB=l(nc);R6=u(RB,"use_gpu"),RB.forEach(s),N6=p(by),Kt=o(by,"TD",{align:!0});var Ey=l(Kt);S6=u(Ey,"(Default: "),Hg=o(Ey,"CODE",{});var NB=l(Hg);x6=u(NB,"false"),NB.forEach(s),I6=u(Ey,"). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),Ey.forEach(s),by.forEach(s),H6=p(H),hr=o(H,"TR",{});var wy=l(hr);rc=o(wy,"TD",{align:!0});var SB=l(rc);B6=u(SB,"use_cache"),SB.forEach(s),C6=p(wy),Jt=o(wy,"TD",{align:!0});var Ty=l(Jt);G6=u(Ty,"(Default: "),Bg=o(Ty,"CODE",{});var xB=l(Bg);L6=u(xB,"true"),xB.forEach(s),U6=u(Ty,"). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),Ty.forEach(s),wy.forEach(s),z6=p(H),dr=o(H,"TR",{});var jy=l(dr);oc=o(jy,"TD",{align:!0});var IB=l(oc);M6=u(IB,"wait_for_model"),IB.forEach(s),F6=p(jy),Wt=o(jy,"TD",{align:!0});var ky=l(Wt);K6=u(ky,"(Default: "),Cg=o(ky,"CODE",{});var HB=l(Cg);J6=u(HB,"false"),HB.forEach(s),W6=u(ky,") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),ky.forEach(s),jy.forEach(s),H.forEach(s),ny.forEach(s),O1=p(a),lc=o(a,"P",{});var BB=l(lc);Y6=u(BB,"Return value is either a dict or a list of dicts if you sent a list of inputs"),BB.forEach(s),P1=p(a),Yt=o(a,"TABLE",{});var Ay=l(Yt);Gg=o(Ay,"THEAD",{});var CB=l(Gg);gr=o(CB,"TR",{});var Dy=l(gr);ic=o(Dy,"TH",{align:!0});var GB=l(ic);V6=u(GB,"Returned values"),GB.forEach(s),X6=p(Dy),Lg=o(Dy,"TH",{align:!0}),l(Lg).forEach(s),Dy.forEach(s),CB.forEach(s),Q6=p(Ay),ie=o(Ay,"TBODY",{});var Fa=l(ie);mr=o(Fa,"TR",{});var Oy=l(mr);uc=o(Oy,"TD",{align:!0});var LB=l(uc);Ug=o(LB,"STRONG",{});var UB=l(Ug);Z6=u(UB,"generated_text"),UB.forEach(s),LB.forEach(s),ek=p(Oy),cc=o(Oy,"TD",{align:!0});var zB=l(cc);tk=u(zB,"The answer of the bot"),zB.forEach(s),Oy.forEach(s),sk=p(Fa),qr=o(Fa,"TR",{});var Py=l(qr);fc=o(Py,"TD",{align:!0});var MB=l(fc);zg=o(MB,"STRONG",{});var FB=l(zg);ak=u(FB,"conversation"),FB.forEach(s),MB.forEach(s),nk=p(Py),pc=o(Py,"TD",{align:!0});var KB=l(pc);rk=u(KB,"A facility dictionnary to send back for the next input (with the new user input addition)."),KB.forEach(s),Py.forEach(s),ok=p(Fa),$r=o(Fa,"TR",{});var Ry=l($r);hc=o(Ry,"TD",{align:!0});var JB=l(hc);lk=u(JB,"past_user_inputs"),JB.forEach(s),ik=p(Ry),dc=o(Ry,"TD",{align:!0});var WB=l(dc);uk=u(WB,"List of strings. The last inputs from the user in the conversation, <em>after the model has run."),WB.forEach(s),Ry.forEach(s),ck=p(Fa),_r=o(Fa,"TR",{});var Ny=l(_r);gc=o(Ny,"TD",{align:!0});var YB=l(gc);fk=u(YB,"generated_responses"),YB.forEach(s),pk=p(Ny),mc=o(Ny,"TD",{align:!0});var VB=l(mc);hk=u(VB,"List of strings. The last outputs from the model in the conversation, <em>after the model has run."),VB.forEach(s),Ny.forEach(s),Fa.forEach(s),Ay.forEach(s),R1=p(a),Ce=o(a,"H2",{class:!0});var Sy=l(Ce);Vt=o(Sy,"A",{id:!0,class:!0,href:!0});var XB=l(Vt);Mg=o(XB,"SPAN",{});var QB=l(Mg);v(vr.$$.fragment,QB),QB.forEach(s),XB.forEach(s),dk=p(Sy),Fg=o(Sy,"SPAN",{});var ZB=l(Fg);gk=u(ZB,"Table question answering task"),ZB.forEach(s),Sy.forEach(s),N1=p(a),qc=o(a,"P",{});var eC=l(qc);mk=u(eC,`Don\u2019t know SQL? Don\u2019t want to dive into a large spreadsheet? Ask
questions in plain english!`),eC.forEach(s),S1=p(a),v(Xt.$$.fragment,a),x1=p(a),yr=o(a,"P",{});var Sx=l(yr);qk=u(Sx,"Available with: "),br=o(Sx,"A",{href:!0,rel:!0});var tC=l(br);$k=u(tC,"\u{1F917} Transformers"),tC.forEach(s),Sx.forEach(s),I1=p(a),$c=o(a,"P",{});var sC=l($c);_k=u(sC,"Example:"),sC.forEach(s),H1=p(a),v(Qt.$$.fragment,a),B1=p(a),_c=o(a,"P",{});var aC=l(_c);vk=u(aC,`When sending your request, you should send a JSON encoded payload. Here
are all the options`),aC.forEach(s),C1=p(a),Zt=o(a,"TABLE",{});var xy=l(Zt);Kg=o(xy,"THEAD",{});var nC=l(Kg);Er=o(nC,"TR",{});var Iy=l(Er);vc=o(Iy,"TH",{align:!0});var rC=l(vc);yk=u(rC,"All parameters"),rC.forEach(s),bk=p(Iy),Jg=o(Iy,"TH",{align:!0}),l(Jg).forEach(s),Iy.forEach(s),nC.forEach(s),Ek=p(xy),K=o(xy,"TBODY",{});var V=l(K);wr=o(V,"TR",{});var Hy=l(wr);Tr=o(Hy,"TD",{align:!0});var xx=l(Tr);Wg=o(xx,"STRONG",{});var oC=l(Wg);wk=u(oC,"inputs"),oC.forEach(s),Tk=u(xx," (required)"),xx.forEach(s),jk=p(Hy),Yg=o(Hy,"TD",{align:!0}),l(Yg).forEach(s),Hy.forEach(s),kk=p(V),jr=o(V,"TR",{});var By=l(jr);yc=o(By,"TD",{align:!0});var lC=l(yc);Ak=u(lC,"query (required)"),lC.forEach(s),Dk=p(By),bc=o(By,"TD",{align:!0});var iC=l(bc);Ok=u(iC,"The query in plain text that you want to ask the table"),iC.forEach(s),By.forEach(s),Pk=p(V),kr=o(V,"TR",{});var Cy=l(kr);Ec=o(Cy,"TD",{align:!0});var uC=l(Ec);Rk=u(uC,"table (required)"),uC.forEach(s),Nk=p(Cy),wc=o(Cy,"TD",{align:!0});var cC=l(wc);Sk=u(cC,"A table of data represented as a dict of list where entries are headers and the lists are all the values, all lists must have the same size."),cC.forEach(s),Cy.forEach(s),xk=p(V),Ar=o(V,"TR",{});var Gy=l(Ar);Tc=o(Gy,"TD",{align:!0});var fC=l(Tc);Vg=o(fC,"STRONG",{});var pC=l(Vg);Ik=u(pC,"options"),pC.forEach(s),fC.forEach(s),Hk=p(Gy),jc=o(Gy,"TD",{align:!0});var hC=l(jc);Bk=u(hC,"a dict containing the following keys:"),hC.forEach(s),Gy.forEach(s),Ck=p(V),Dr=o(V,"TR",{});var Ly=l(Dr);kc=o(Ly,"TD",{align:!0});var dC=l(kc);Gk=u(dC,"use_gpu"),dC.forEach(s),Lk=p(Ly),es=o(Ly,"TD",{align:!0});var Uy=l(es);Uk=u(Uy,"(Default: "),Xg=o(Uy,"CODE",{});var gC=l(Xg);zk=u(gC,"false"),gC.forEach(s),Mk=u(Uy,"). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),Uy.forEach(s),Ly.forEach(s),Fk=p(V),Or=o(V,"TR",{});var zy=l(Or);Ac=o(zy,"TD",{align:!0});var mC=l(Ac);Kk=u(mC,"use_cache"),mC.forEach(s),Jk=p(zy),ts=o(zy,"TD",{align:!0});var My=l(ts);Wk=u(My,"(Default: "),Qg=o(My,"CODE",{});var qC=l(Qg);Yk=u(qC,"true"),qC.forEach(s),Vk=u(My,"). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),My.forEach(s),zy.forEach(s),Xk=p(V),Pr=o(V,"TR",{});var Fy=l(Pr);Dc=o(Fy,"TD",{align:!0});var $C=l(Dc);Qk=u($C,"wait_for_model"),$C.forEach(s),Zk=p(Fy),ss=o(Fy,"TD",{align:!0});var Ky=l(ss);e7=u(Ky,"(Default: "),Zg=o(Ky,"CODE",{});var _C=l(Zg);t7=u(_C,"false"),_C.forEach(s),s7=u(Ky,") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),Ky.forEach(s),Fy.forEach(s),V.forEach(s),xy.forEach(s),G1=p(a),Oc=o(a,"P",{});var vC=l(Oc);a7=u(vC,"Return value is either a dict or a list of dicts if you sent a list of inputs"),vC.forEach(s),L1=p(a),v(as.$$.fragment,a),U1=p(a),ns=o(a,"TABLE",{});var Jy=l(ns);em=o(Jy,"THEAD",{});var yC=l(em);Rr=o(yC,"TR",{});var Wy=l(Rr);Pc=o(Wy,"TH",{align:!0});var bC=l(Pc);n7=u(bC,"Returned values"),bC.forEach(s),r7=p(Wy),tm=o(Wy,"TH",{align:!0}),l(tm).forEach(s),Wy.forEach(s),yC.forEach(s),o7=p(Jy),ue=o(Jy,"TBODY",{});var Ka=l(ue);Nr=o(Ka,"TR",{});var Yy=l(Nr);Rc=o(Yy,"TD",{align:!0});var EC=l(Rc);sm=o(EC,"STRONG",{});var wC=l(sm);l7=u(wC,"answer"),wC.forEach(s),EC.forEach(s),i7=p(Yy),Nc=o(Yy,"TD",{align:!0});var TC=l(Nc);u7=u(TC,"The plaintext answer"),TC.forEach(s),Yy.forEach(s),c7=p(Ka),Sr=o(Ka,"TR",{});var Vy=l(Sr);Sc=o(Vy,"TD",{align:!0});var jC=l(Sc);am=o(jC,"STRONG",{});var kC=l(am);f7=u(kC,"coordinates"),kC.forEach(s),jC.forEach(s),p7=p(Vy),xc=o(Vy,"TD",{align:!0});var AC=l(xc);h7=u(AC,"a list of coordinates of the cells referenced in the answer"),AC.forEach(s),Vy.forEach(s),d7=p(Ka),xr=o(Ka,"TR",{});var Xy=l(xr);Ic=o(Xy,"TD",{align:!0});var DC=l(Ic);nm=o(DC,"STRONG",{});var OC=l(nm);g7=u(OC,"cells"),OC.forEach(s),DC.forEach(s),m7=p(Xy),Hc=o(Xy,"TD",{align:!0});var PC=l(Hc);q7=u(PC,"a list of coordinates of the cells contents"),PC.forEach(s),Xy.forEach(s),$7=p(Ka),Ir=o(Ka,"TR",{});var Qy=l(Ir);Bc=o(Qy,"TD",{align:!0});var RC=l(Bc);rm=o(RC,"STRONG",{});var NC=l(rm);_7=u(NC,"aggregator"),NC.forEach(s),RC.forEach(s),v7=p(Qy),Cc=o(Qy,"TD",{align:!0});var SC=l(Cc);y7=u(SC,"The aggregator used to get the answer"),SC.forEach(s),Qy.forEach(s),Ka.forEach(s),Jy.forEach(s),z1=p(a),Ge=o(a,"H2",{class:!0});var Zy=l(Ge);rs=o(Zy,"A",{id:!0,class:!0,href:!0});var xC=l(rs);om=o(xC,"SPAN",{});var IC=l(om);v(Hr.$$.fragment,IC),IC.forEach(s),xC.forEach(s),b7=p(Zy),lm=o(Zy,"SPAN",{});var HC=l(lm);E7=u(HC,"Question answering task"),HC.forEach(s),Zy.forEach(s),M1=p(a),Gc=o(a,"P",{});var BC=l(Gc);w7=u(BC,"Want to have a nice know-it-all bot that can answer any question?"),BC.forEach(s),F1=p(a),v(os.$$.fragment,a),K1=p(a),Le=o(a,"P",{});var H_=l(Le);T7=u(H_,"Available with: "),Br=o(H_,"A",{href:!0,rel:!0});var CC=l(Br);j7=u(CC,"\u{1F917}Transformers"),CC.forEach(s),k7=u(H_,` and
`),Cr=o(H_,"A",{href:!0,rel:!0});var GC=l(Cr);A7=u(GC,"AllenNLP"),GC.forEach(s),H_.forEach(s),J1=p(a),Lc=o(a,"P",{});var LC=l(Lc);D7=u(LC,"Example:"),LC.forEach(s),W1=p(a),v(ls.$$.fragment,a),Y1=p(a),Uc=o(a,"P",{});var UC=l(Uc);O7=u(UC,`When sending your request, you should send a JSON encoded payload. Here
are all the options`),UC.forEach(s),V1=p(a),zc=o(a,"P",{});var zC=l(zc);P7=u(zC,"Return value is either a dict or a list of dicts if you sent a list of inputs"),zC.forEach(s),X1=p(a),v(is.$$.fragment,a),Q1=p(a),us=o(a,"TABLE",{});var eb=l(us);im=o(eb,"THEAD",{});var MC=l(im);Gr=o(MC,"TR",{});var tb=l(Gr);Mc=o(tb,"TH",{align:!0});var FC=l(Mc);R7=u(FC,"Returned values"),FC.forEach(s),N7=p(tb),um=o(tb,"TH",{align:!0}),l(um).forEach(s),tb.forEach(s),MC.forEach(s),S7=p(eb),ce=o(eb,"TBODY",{});var Ja=l(ce);Lr=o(Ja,"TR",{});var sb=l(Lr);Fc=o(sb,"TD",{align:!0});var KC=l(Fc);cm=o(KC,"STRONG",{});var JC=l(cm);x7=u(JC,"answer"),JC.forEach(s),KC.forEach(s),I7=p(sb),Kc=o(sb,"TD",{align:!0});var WC=l(Kc);H7=u(WC,"A string that\u2019s the answer within the text."),WC.forEach(s),sb.forEach(s),B7=p(Ja),Ur=o(Ja,"TR",{});var ab=l(Ur);Jc=o(ab,"TD",{align:!0});var YC=l(Jc);fm=o(YC,"STRONG",{});var VC=l(fm);C7=u(VC,"score"),VC.forEach(s),YC.forEach(s),G7=p(ab),Wc=o(ab,"TD",{align:!0});var XC=l(Wc);L7=u(XC,"A float that represents how likely that the answer is correct"),XC.forEach(s),ab.forEach(s),U7=p(Ja),zr=o(Ja,"TR",{});var nb=l(zr);Yc=o(nb,"TD",{align:!0});var QC=l(Yc);pm=o(QC,"STRONG",{});var ZC=l(pm);z7=u(ZC,"start"),ZC.forEach(s),QC.forEach(s),M7=p(nb),cs=o(nb,"TD",{align:!0});var rb=l(cs);F7=u(rb,"The index (string wise) of the start of the answer within "),hm=o(rb,"CODE",{});var eG=l(hm);K7=u(eG,"context"),eG.forEach(s),J7=u(rb,"."),rb.forEach(s),nb.forEach(s),W7=p(Ja),Mr=o(Ja,"TR",{});var ob=l(Mr);Vc=o(ob,"TD",{align:!0});var tG=l(Vc);dm=o(tG,"STRONG",{});var sG=l(dm);Y7=u(sG,"stop"),sG.forEach(s),tG.forEach(s),V7=p(ob),fs=o(ob,"TD",{align:!0});var lb=l(fs);X7=u(lb,"The index (string wise) of the stop of the answer within "),gm=o(lb,"CODE",{});var aG=l(gm);Q7=u(aG,"context"),aG.forEach(s),Z7=u(lb,"."),lb.forEach(s),ob.forEach(s),Ja.forEach(s),eb.forEach(s),Z1=p(a),Ue=o(a,"H2",{class:!0});var ib=l(Ue);ps=o(ib,"A",{id:!0,class:!0,href:!0});var nG=l(ps);mm=o(nG,"SPAN",{});var rG=l(mm);v(Fr.$$.fragment,rG),rG.forEach(s),nG.forEach(s),e9=p(ib),qm=o(ib,"SPAN",{});var oG=l(qm);t9=u(oG,"Text-classification task"),oG.forEach(s),ib.forEach(s),e2=p(a),Xc=o(a,"P",{});var lG=l(Xc);s9=u(lG,`Usually used for sentiment-analysis this will output the likelihood of
classes of an input.`),lG.forEach(s),t2=p(a),v(hs.$$.fragment,a),s2=p(a),Kr=o(a,"P",{});var Ix=l(Kr);a9=u(Ix,"Available with: "),Jr=o(Ix,"A",{href:!0,rel:!0});var iG=l(Jr);n9=u(iG,"\u{1F917} Transformers"),iG.forEach(s),Ix.forEach(s),a2=p(a),Qc=o(a,"P",{});var uG=l(Qc);r9=u(uG,"Example:"),uG.forEach(s),n2=p(a),v(ds.$$.fragment,a),r2=p(a),Zc=o(a,"P",{});var cG=l(Zc);o9=u(cG,`When sending your request, you should send a JSON encoded payload. Here
are all the options`),cG.forEach(s),o2=p(a),gs=o(a,"TABLE",{});var ub=l(gs);$m=o(ub,"THEAD",{});var fG=l($m);Wr=o(fG,"TR",{});var cb=l(Wr);ef=o(cb,"TH",{align:!0});var pG=l(ef);l9=u(pG,"All parameters"),pG.forEach(s),i9=p(cb),_m=o(cb,"TH",{align:!0}),l(_m).forEach(s),cb.forEach(s),fG.forEach(s),u9=p(ub),ee=o(ub,"TBODY",{});var Ae=l(ee);Yr=o(Ae,"TR",{});var fb=l(Yr);Vr=o(fb,"TD",{align:!0});var Hx=l(Vr);vm=o(Hx,"STRONG",{});var hG=l(vm);c9=u(hG,"inputs"),hG.forEach(s),f9=u(Hx," (required)"),Hx.forEach(s),p9=p(fb),tf=o(fb,"TD",{align:!0});var dG=l(tf);h9=u(dG,"a string to be classified"),dG.forEach(s),fb.forEach(s),d9=p(Ae),Xr=o(Ae,"TR",{});var pb=l(Xr);sf=o(pb,"TD",{align:!0});var gG=l(sf);ym=o(gG,"STRONG",{});var mG=l(ym);g9=u(mG,"options"),mG.forEach(s),gG.forEach(s),m9=p(pb),af=o(pb,"TD",{align:!0});var qG=l(af);q9=u(qG,"a dict containing the following keys:"),qG.forEach(s),pb.forEach(s),$9=p(Ae),Qr=o(Ae,"TR",{});var hb=l(Qr);nf=o(hb,"TD",{align:!0});var $G=l(nf);_9=u($G,"use_gpu"),$G.forEach(s),v9=p(hb),ms=o(hb,"TD",{align:!0});var db=l(ms);y9=u(db,"(Default: "),bm=o(db,"CODE",{});var _G=l(bm);b9=u(_G,"false"),_G.forEach(s),E9=u(db,"). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),db.forEach(s),hb.forEach(s),w9=p(Ae),Zr=o(Ae,"TR",{});var gb=l(Zr);rf=o(gb,"TD",{align:!0});var vG=l(rf);T9=u(vG,"use_cache"),vG.forEach(s),j9=p(gb),qs=o(gb,"TD",{align:!0});var mb=l(qs);k9=u(mb,"(Default: "),Em=o(mb,"CODE",{});var yG=l(Em);A9=u(yG,"true"),yG.forEach(s),D9=u(mb,"). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),mb.forEach(s),gb.forEach(s),O9=p(Ae),eo=o(Ae,"TR",{});var qb=l(eo);of=o(qb,"TD",{align:!0});var bG=l(of);P9=u(bG,"wait_for_model"),bG.forEach(s),R9=p(qb),$s=o(qb,"TD",{align:!0});var $b=l($s);N9=u($b,"(Default: "),wm=o($b,"CODE",{});var EG=l(wm);S9=u(EG,"false"),EG.forEach(s),x9=u($b,") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),$b.forEach(s),qb.forEach(s),Ae.forEach(s),ub.forEach(s),l2=p(a),lf=o(a,"P",{});var wG=l(lf);I9=u(wG,"Return value is either a dict or a list of dicts if you sent a list of inputs"),wG.forEach(s),i2=p(a),v(_s.$$.fragment,a),u2=p(a),vs=o(a,"TABLE",{});var _b=l(vs);Tm=o(_b,"THEAD",{});var TG=l(Tm);to=o(TG,"TR",{});var vb=l(to);uf=o(vb,"TH",{align:!0});var jG=l(uf);H9=u(jG,"Returned values"),jG.forEach(s),B9=p(vb),jm=o(vb,"TH",{align:!0}),l(jm).forEach(s),vb.forEach(s),TG.forEach(s),C9=p(_b),so=o(_b,"TBODY",{});var yb=l(so);ao=o(yb,"TR",{});var bb=l(ao);cf=o(bb,"TD",{align:!0});var kG=l(cf);km=o(kG,"STRONG",{});var AG=l(km);G9=u(AG,"label"),AG.forEach(s),kG.forEach(s),L9=p(bb),ff=o(bb,"TD",{align:!0});var DG=l(ff);U9=u(DG,"The label for the class (model specific)"),DG.forEach(s),bb.forEach(s),z9=p(yb),no=o(yb,"TR",{});var Eb=l(no);pf=o(Eb,"TD",{align:!0});var OG=l(pf);Am=o(OG,"STRONG",{});var PG=l(Am);M9=u(PG,"score"),PG.forEach(s),OG.forEach(s),F9=p(Eb),hf=o(Eb,"TD",{align:!0});var RG=l(hf);K9=u(RG,"A floats that represents how likely is that the text belongs the this class."),RG.forEach(s),Eb.forEach(s),yb.forEach(s),_b.forEach(s),c2=p(a),ze=o(a,"H2",{class:!0});var wb=l(ze);ys=o(wb,"A",{id:!0,class:!0,href:!0});var NG=l(ys);Dm=o(NG,"SPAN",{});var SG=l(Dm);v(ro.$$.fragment,SG),SG.forEach(s),NG.forEach(s),J9=p(wb),Om=o(wb,"SPAN",{});var xG=l(Om);W9=u(xG,"Named Entity Recognition (NER) task"),xG.forEach(s),wb.forEach(s),f2=p(a),oo=o(a,"P",{});var Bx=l(oo);Y9=u(Bx,"See "),df=o(Bx,"A",{href:!0});var IG=l(df);V9=u(IG,"Token-classification task"),IG.forEach(s),Bx.forEach(s),p2=p(a),Me=o(a,"H2",{class:!0});var Tb=l(Me);bs=o(Tb,"A",{id:!0,class:!0,href:!0});var HG=l(bs);Pm=o(HG,"SPAN",{});var BG=l(Pm);v(lo.$$.fragment,BG),BG.forEach(s),HG.forEach(s),X9=p(Tb),Rm=o(Tb,"SPAN",{});var CG=l(Rm);Q9=u(CG,"Token-classification task"),CG.forEach(s),Tb.forEach(s),h2=p(a),gf=o(a,"P",{});var GG=l(gf);Z9=u(GG,`Usually used for sentence parsing, either grammatical, or Named Entity
Recognition (NER) to understand keywords contained within text.`),GG.forEach(s),d2=p(a),v(Es.$$.fragment,a),g2=p(a),Fe=o(a,"P",{});var B_=l(Fe);e8=u(B_,"Available with: "),io=o(B_,"A",{href:!0,rel:!0});var LG=l(io);t8=u(LG,"\u{1F917} Transformers"),LG.forEach(s),s8=u(B_,`,
`),uo=o(B_,"A",{href:!0,rel:!0});var UG=l(uo);a8=u(UG,"Flair"),UG.forEach(s),B_.forEach(s),m2=p(a),mf=o(a,"P",{});var zG=l(mf);n8=u(zG,"Example:"),zG.forEach(s),q2=p(a),v(ws.$$.fragment,a),$2=p(a),qf=o(a,"P",{});var MG=l(qf);r8=u(MG,`When sending your request, you should send a JSON encoded payload. Here
are all the options`),MG.forEach(s),_2=p(a),Ts=o(a,"TABLE",{});var jb=l(Ts);Nm=o(jb,"THEAD",{});var FG=l(Nm);co=o(FG,"TR",{});var kb=l(co);$f=o(kb,"TH",{align:!0});var KG=l($f);o8=u(KG,"All parameters"),KG.forEach(s),l8=p(kb),Sm=o(kb,"TH",{align:!0}),l(Sm).forEach(s),kb.forEach(s),FG.forEach(s),i8=p(jb),J=o(jb,"TBODY",{});var X=l(J);fo=o(X,"TR",{});var Ab=l(fo);po=o(Ab,"TD",{align:!0});var Cx=l(po);xm=o(Cx,"STRONG",{});var JG=l(xm);u8=u(JG,"inputs"),JG.forEach(s),c8=u(Cx," (required)"),Cx.forEach(s),f8=p(Ab),_f=o(Ab,"TD",{align:!0});var WG=l(_f);p8=u(WG,"a string to be classified"),WG.forEach(s),Ab.forEach(s),h8=p(X),ho=o(X,"TR",{});var Db=l(ho);vf=o(Db,"TD",{align:!0});var YG=l(vf);Im=o(YG,"STRONG",{});var VG=l(Im);d8=u(VG,"parameters"),VG.forEach(s),YG.forEach(s),g8=p(Db),yf=o(Db,"TD",{align:!0});var XG=l(yf);m8=u(XG,"a dict containing the following key:"),XG.forEach(s),Db.forEach(s),q8=p(X),go=o(X,"TR",{});var Ob=l(go);bf=o(Ob,"TD",{align:!0});var QG=l(bf);$8=u(QG,"aggregation_strategy"),QG.forEach(s),_8=p(Ob),x=o(Ob,"TD",{align:!0});var B=l(x);v8=u(B,"(Default: "),Hm=o(B,"CODE",{});var ZG=l(Hm);y8=u(ZG,"simple"),ZG.forEach(s),b8=u(B,"). There are several aggregation strategies: "),E8=o(B,"BR",{}),w8=p(B),Bm=o(B,"CODE",{});var eL=l(Bm);T8=u(eL,"none"),eL.forEach(s),j8=u(B,": Every token gets classified without further aggregation. "),k8=o(B,"BR",{}),A8=p(B),Cm=o(B,"CODE",{});var tL=l(Cm);D8=u(tL,"simple"),tL.forEach(s),O8=u(B,": Entities are grouped according to the default schema (B-, I- tags get merged when the tag is similar). "),P8=o(B,"BR",{}),R8=p(B),Gm=o(B,"CODE",{});var sL=l(Gm);N8=u(sL,"first"),sL.forEach(s),S8=u(B,": Same as the "),Lm=o(B,"CODE",{});var aL=l(Lm);x8=u(aL,"simple"),aL.forEach(s),I8=u(B," strategy except words cannot end up with different tags. Words will use the tag of the first token when there is ambiguity. "),H8=o(B,"BR",{}),B8=p(B),Um=o(B,"CODE",{});var nL=l(Um);C8=u(nL,"average"),nL.forEach(s),G8=u(B,": Same as the "),zm=o(B,"CODE",{});var rL=l(zm);L8=u(rL,"simple"),rL.forEach(s),U8=u(B," strategy except words cannot end up with different tags. Scores are averaged across tokens and then the maximum label is applied. "),z8=o(B,"BR",{}),M8=p(B),Mm=o(B,"CODE",{});var oL=l(Mm);F8=u(oL,"max"),oL.forEach(s),K8=u(B,": Same as the "),Fm=o(B,"CODE",{});var lL=l(Fm);J8=u(lL,"simple"),lL.forEach(s),W8=u(B," strategy except words cannot end up with different tags. Word entity will be the token with the maximum score."),B.forEach(s),Ob.forEach(s),Y8=p(X),mo=o(X,"TR",{});var Pb=l(mo);Ef=o(Pb,"TD",{align:!0});var iL=l(Ef);Km=o(iL,"STRONG",{});var uL=l(Km);V8=u(uL,"options"),uL.forEach(s),iL.forEach(s),X8=p(Pb),wf=o(Pb,"TD",{align:!0});var cL=l(wf);Q8=u(cL,"a dict containing the following keys:"),cL.forEach(s),Pb.forEach(s),Z8=p(X),qo=o(X,"TR",{});var Rb=l(qo);Tf=o(Rb,"TD",{align:!0});var fL=l(Tf);eA=u(fL,"use_gpu"),fL.forEach(s),tA=p(Rb),js=o(Rb,"TD",{align:!0});var Nb=l(js);sA=u(Nb,"(Default: "),Jm=o(Nb,"CODE",{});var pL=l(Jm);aA=u(pL,"false"),pL.forEach(s),nA=u(Nb,"). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),Nb.forEach(s),Rb.forEach(s),rA=p(X),$o=o(X,"TR",{});var Sb=l($o);jf=o(Sb,"TD",{align:!0});var hL=l(jf);oA=u(hL,"use_cache"),hL.forEach(s),lA=p(Sb),ks=o(Sb,"TD",{align:!0});var xb=l(ks);iA=u(xb,"(Default: "),Wm=o(xb,"CODE",{});var dL=l(Wm);uA=u(dL,"true"),dL.forEach(s),cA=u(xb,"). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),xb.forEach(s),Sb.forEach(s),fA=p(X),_o=o(X,"TR",{});var Ib=l(_o);kf=o(Ib,"TD",{align:!0});var gL=l(kf);pA=u(gL,"wait_for_model"),gL.forEach(s),hA=p(Ib),As=o(Ib,"TD",{align:!0});var Hb=l(As);dA=u(Hb,"(Default: "),Ym=o(Hb,"CODE",{});var mL=l(Ym);gA=u(mL,"false"),mL.forEach(s),mA=u(Hb,") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),Hb.forEach(s),Ib.forEach(s),X.forEach(s),jb.forEach(s),v2=p(a),Af=o(a,"P",{});var qL=l(Af);qA=u(qL,"Return value is either a dict or a list of dicts if you sent a list of inputs"),qL.forEach(s),y2=p(a),v(Ds.$$.fragment,a),b2=p(a),Os=o(a,"TABLE",{});var Bb=l(Os);Vm=o(Bb,"THEAD",{});var $L=l(Vm);vo=o($L,"TR",{});var Cb=l(vo);Df=o(Cb,"TH",{align:!0});var _L=l(Df);$A=u(_L,"Returned values"),_L.forEach(s),_A=p(Cb),Xm=o(Cb,"TH",{align:!0}),l(Xm).forEach(s),Cb.forEach(s),$L.forEach(s),vA=p(Bb),te=o(Bb,"TBODY",{});var De=l(te);yo=o(De,"TR",{});var Gb=l(yo);Of=o(Gb,"TD",{align:!0});var vL=l(Of);Qm=o(vL,"STRONG",{});var yL=l(Qm);yA=u(yL,"entity_group"),yL.forEach(s),vL.forEach(s),bA=p(Gb),Pf=o(Gb,"TD",{align:!0});var bL=l(Pf);EA=u(bL,"The type for the entity being recognized (model specific)."),bL.forEach(s),Gb.forEach(s),wA=p(De),bo=o(De,"TR",{});var Lb=l(bo);Rf=o(Lb,"TD",{align:!0});var EL=l(Rf);Zm=o(EL,"STRONG",{});var wL=l(Zm);TA=u(wL,"score"),wL.forEach(s),EL.forEach(s),jA=p(Lb),Nf=o(Lb,"TD",{align:!0});var TL=l(Nf);kA=u(TL,"How likely the entity was recognized."),TL.forEach(s),Lb.forEach(s),AA=p(De),Eo=o(De,"TR",{});var Ub=l(Eo);Sf=o(Ub,"TD",{align:!0});var jL=l(Sf);eq=o(jL,"STRONG",{});var kL=l(eq);DA=u(kL,"word"),kL.forEach(s),jL.forEach(s),OA=p(Ub),xf=o(Ub,"TD",{align:!0});var AL=l(xf);PA=u(AL,"The string that was captured"),AL.forEach(s),Ub.forEach(s),RA=p(De),wo=o(De,"TR",{});var zb=l(wo);If=o(zb,"TD",{align:!0});var DL=l(If);tq=o(DL,"STRONG",{});var OL=l(tq);NA=u(OL,"start"),OL.forEach(s),DL.forEach(s),SA=p(zb),Ps=o(zb,"TD",{align:!0});var Mb=l(Ps);xA=u(Mb,"The offset stringwise where the answer is located. Useful to disambiguate if "),sq=o(Mb,"CODE",{});var PL=l(sq);IA=u(PL,"word"),PL.forEach(s),HA=u(Mb," occurs multiple times."),Mb.forEach(s),zb.forEach(s),BA=p(De),To=o(De,"TR",{});var Fb=l(To);Hf=o(Fb,"TD",{align:!0});var RL=l(Hf);aq=o(RL,"STRONG",{});var NL=l(aq);CA=u(NL,"end"),NL.forEach(s),RL.forEach(s),GA=p(Fb),Rs=o(Fb,"TD",{align:!0});var Kb=l(Rs);LA=u(Kb,"The offset stringwise where the answer is located. Useful to disambiguate if "),nq=o(Kb,"CODE",{});var SL=l(nq);UA=u(SL,"word"),SL.forEach(s),zA=u(Kb," occurs multiple times."),Kb.forEach(s),Fb.forEach(s),De.forEach(s),Bb.forEach(s),E2=p(a),Ke=o(a,"H2",{class:!0});var Jb=l(Ke);Ns=o(Jb,"A",{id:!0,class:!0,href:!0});var xL=l(Ns);rq=o(xL,"SPAN",{});var IL=l(rq);v(jo.$$.fragment,IL),IL.forEach(s),xL.forEach(s),MA=p(Jb),oq=o(Jb,"SPAN",{});var HL=l(oq);FA=u(HL,"Text-generation task"),HL.forEach(s),Jb.forEach(s),w2=p(a),Bf=o(a,"P",{});var BL=l(Bf);KA=u(BL,"Use to continue text from a prompt. This is a very generic task."),BL.forEach(s),T2=p(a),v(Ss.$$.fragment,a),j2=p(a),ko=o(a,"P",{});var Gx=l(ko);JA=u(Gx,"Available with: "),Ao=o(Gx,"A",{href:!0,rel:!0});var CL=l(Ao);WA=u(CL,"\u{1F917} Transformers"),CL.forEach(s),Gx.forEach(s),k2=p(a),Cf=o(a,"P",{});var GL=l(Cf);YA=u(GL,"Example:"),GL.forEach(s),A2=p(a),v(xs.$$.fragment,a),D2=p(a),Gf=o(a,"P",{});var LL=l(Gf);VA=u(LL,`When sending your request, you should send a JSON encoded payload. Here
are all the options`),LL.forEach(s),O2=p(a),Is=o(a,"TABLE",{});var Wb=l(Is);lq=o(Wb,"THEAD",{});var UL=l(lq);Do=o(UL,"TR",{});var Yb=l(Do);Lf=o(Yb,"TH",{align:!0});var zL=l(Lf);XA=u(zL,"All parameters"),zL.forEach(s),QA=p(Yb),iq=o(Yb,"TH",{align:!0}),l(iq).forEach(s),Yb.forEach(s),UL.forEach(s),ZA=p(Wb),I=o(Wb,"TBODY",{});var C=l(I);Oo=o(C,"TR",{});var Vb=l(Oo);Po=o(Vb,"TD",{align:!0});var Lx=l(Po);uq=o(Lx,"STRONG",{});var ML=l(uq);eD=u(ML,"inputs"),ML.forEach(s),tD=u(Lx," (required):"),Lx.forEach(s),sD=p(Vb),Uf=o(Vb,"TD",{align:!0});var FL=l(Uf);aD=u(FL,"a string to be generated from"),FL.forEach(s),Vb.forEach(s),nD=p(C),Ro=o(C,"TR",{});var Xb=l(Ro);zf=o(Xb,"TD",{align:!0});var KL=l(zf);cq=o(KL,"STRONG",{});var JL=l(cq);rD=u(JL,"parameters"),JL.forEach(s),KL.forEach(s),oD=p(Xb),Mf=o(Xb,"TD",{align:!0});var WL=l(Mf);lD=u(WL,"dict containing the following keys:"),WL.forEach(s),Xb.forEach(s),iD=p(C),No=o(C,"TR",{});var Qb=l(No);Ff=o(Qb,"TD",{align:!0});var YL=l(Ff);uD=u(YL,"top_k"),YL.forEach(s),cD=p(Qb),be=o(Qb,"TD",{align:!0});var ad=l(be);fD=u(ad,"(Default: "),fq=o(ad,"CODE",{});var VL=l(fq);pD=u(VL,"None"),VL.forEach(s),hD=u(ad,"). Integer to define the top tokens considered within the "),pq=o(ad,"CODE",{});var XL=l(pq);dD=u(XL,"sample"),XL.forEach(s),gD=u(ad," operation to create new text."),ad.forEach(s),Qb.forEach(s),mD=p(C),So=o(C,"TR",{});var Zb=l(So);Kf=o(Zb,"TD",{align:!0});var QL=l(Kf);qD=u(QL,"top_p"),QL.forEach(s),$D=p(Zb),le=o(Zb,"TD",{align:!0});var Wa=l(le);_D=u(Wa,"(Default: "),hq=o(Wa,"CODE",{});var ZL=l(hq);vD=u(ZL,"None"),ZL.forEach(s),yD=u(Wa,"). Float to define the tokens that are within the "),dq=o(Wa,"CODE",{});var eU=l(dq);bD=u(eU,"sample"),eU.forEach(s),ED=u(Wa," operation of text generation. Add tokens in the sample for more probable to least probable until the sum of the probabilities is greater than "),gq=o(Wa,"CODE",{});var tU=l(gq);wD=u(tU,"top_p"),tU.forEach(s),TD=u(Wa,"."),Wa.forEach(s),Zb.forEach(s),jD=p(C),xo=o(C,"TR",{});var eE=l(xo);Jf=o(eE,"TD",{align:!0});var sU=l(Jf);kD=u(sU,"temperature"),sU.forEach(s),AD=p(eE),Ee=o(eE,"TD",{align:!0});var nd=l(Ee);DD=u(nd,"(Default: "),mq=o(nd,"CODE",{});var aU=l(mq);OD=u(aU,"1.0"),aU.forEach(s),PD=u(nd,"). Float (0.0-100.0). The temperature of the sampling operation. 1 means regular sampling, 0 means "),qq=o(nd,"CODE",{});var nU=l(qq);RD=u(nU,"100.0"),nU.forEach(s),ND=u(nd," is getting closer to uniform probability."),nd.forEach(s),eE.forEach(s),SD=p(C),Io=o(C,"TR",{});var tE=l(Io);Wf=o(tE,"TD",{align:!0});var rU=l(Wf);xD=u(rU,"repetition_penalty"),rU.forEach(s),ID=p(tE),Hs=o(tE,"TD",{align:!0});var sE=l(Hs);HD=u(sE,"(Default: "),$q=o(sE,"CODE",{});var oU=l($q);BD=u(oU,"None"),oU.forEach(s),CD=u(sE,"). Float (0.0-100.0). The more a token is used within generation the more it is penalized to not be picked in successive generation passes."),sE.forEach(s),tE.forEach(s),GD=p(C),Ho=o(C,"TR",{});var aE=l(Ho);Yf=o(aE,"TD",{align:!0});var lU=l(Yf);LD=u(lU,"max_new_tokens"),lU.forEach(s),UD=p(aE),we=o(aE,"TD",{align:!0});var rd=l(we);zD=u(rd,"(Default: "),_q=o(rd,"CODE",{});var iU=l(_q);MD=u(iU,"None"),iU.forEach(s),FD=u(rd,"). Int (0-250). The amount of new tokens to be generated, this does "),vq=o(rd,"STRONG",{});var uU=l(vq);KD=u(uU,"not"),uU.forEach(s),JD=u(rd," include the input length it is a estimate of the size of generated text you want. Each new tokens slows down the request, so look for balance between response times and length of text generated."),rd.forEach(s),aE.forEach(s),WD=p(C),Bo=o(C,"TR",{});var nE=l(Bo);Vf=o(nE,"TD",{align:!0});var cU=l(Vf);YD=u(cU,"max_time"),cU.forEach(s),VD=p(nE),Te=o(nE,"TD",{align:!0});var od=l(Te);XD=u(od,"(Default: "),yq=o(od,"CODE",{});var fU=l(yq);QD=u(fU,"None"),fU.forEach(s),ZD=u(od,"). Float (0-120.0). The amount of time in seconds that the query should take maximum. Network can cause some overhead so it will be a soft limit. Use that in combination with "),bq=o(od,"CODE",{});var pU=l(bq);eO=u(pU,"max_new_tokens"),pU.forEach(s),tO=u(od," for best results."),od.forEach(s),nE.forEach(s),sO=p(C),Co=o(C,"TR",{});var rE=l(Co);Xf=o(rE,"TD",{align:!0});var hU=l(Xf);aO=u(hU,"return_full_text"),hU.forEach(s),nO=p(rE),je=o(rE,"TD",{align:!0});var ld=l(je);rO=u(ld,"(Default: "),Eq=o(ld,"CODE",{});var dU=l(Eq);oO=u(dU,"True"),dU.forEach(s),lO=u(ld,"). Bool. If set to False, the return results will "),wq=o(ld,"STRONG",{});var gU=l(wq);iO=u(gU,"not"),gU.forEach(s),uO=u(ld," contain the original query making it easier for prompting."),ld.forEach(s),rE.forEach(s),cO=p(C),Go=o(C,"TR",{});var oE=l(Go);Qf=o(oE,"TD",{align:!0});var mU=l(Qf);fO=u(mU,"num_return_sequences"),mU.forEach(s),pO=p(oE),Bs=o(oE,"TD",{align:!0});var lE=l(Bs);hO=u(lE,"(Default: "),Tq=o(lE,"CODE",{});var qU=l(Tq);dO=u(qU,"1"),qU.forEach(s),gO=u(lE,"). Integer. The number of proposition you want to be returned."),lE.forEach(s),oE.forEach(s),mO=p(C),Lo=o(C,"TR",{});var iE=l(Lo);Zf=o(iE,"TD",{align:!0});var $U=l(Zf);qO=u($U,"do_sample"),$U.forEach(s),$O=p(iE),Cs=o(iE,"TD",{align:!0});var uE=l(Cs);_O=u(uE,"(Optional: "),jq=o(uE,"CODE",{});var _U=l(jq);vO=u(_U,"True"),_U.forEach(s),yO=u(uE,"). Bool. Whether or not to use sampling, use greedy decoding otherwise."),uE.forEach(s),iE.forEach(s),bO=p(C),Uo=o(C,"TR",{});var cE=l(Uo);ep=o(cE,"TD",{align:!0});var vU=l(ep);kq=o(vU,"STRONG",{});var yU=l(kq);EO=u(yU,"options"),yU.forEach(s),vU.forEach(s),wO=p(cE),tp=o(cE,"TD",{align:!0});var bU=l(tp);TO=u(bU,"a dict containing the following keys:"),bU.forEach(s),cE.forEach(s),jO=p(C),zo=o(C,"TR",{});var fE=l(zo);sp=o(fE,"TD",{align:!0});var EU=l(sp);kO=u(EU,"use_gpu"),EU.forEach(s),AO=p(fE),Gs=o(fE,"TD",{align:!0});var pE=l(Gs);DO=u(pE,"(Default: "),Aq=o(pE,"CODE",{});var wU=l(Aq);OO=u(wU,"false"),wU.forEach(s),PO=u(pE,"). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),pE.forEach(s),fE.forEach(s),RO=p(C),Mo=o(C,"TR",{});var hE=l(Mo);ap=o(hE,"TD",{align:!0});var TU=l(ap);NO=u(TU,"use_cache"),TU.forEach(s),SO=p(hE),Ls=o(hE,"TD",{align:!0});var dE=l(Ls);xO=u(dE,"(Default: "),Dq=o(dE,"CODE",{});var jU=l(Dq);IO=u(jU,"true"),jU.forEach(s),HO=u(dE,"). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),dE.forEach(s),hE.forEach(s),BO=p(C),Fo=o(C,"TR",{});var gE=l(Fo);np=o(gE,"TD",{align:!0});var kU=l(np);CO=u(kU,"wait_for_model"),kU.forEach(s),GO=p(gE),Us=o(gE,"TD",{align:!0});var mE=l(Us);LO=u(mE,"(Default: "),Oq=o(mE,"CODE",{});var AU=l(Oq);UO=u(AU,"false"),AU.forEach(s),zO=u(mE,") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),mE.forEach(s),gE.forEach(s),C.forEach(s),Wb.forEach(s),P2=p(a),rp=o(a,"P",{});var DU=l(rp);MO=u(DU,"Return value is either a dict or a list of dicts if you sent a list of inputs"),DU.forEach(s),R2=p(a),v(zs.$$.fragment,a),N2=p(a),Ms=o(a,"TABLE",{});var qE=l(Ms);Pq=o(qE,"THEAD",{});var OU=l(Pq);Ko=o(OU,"TR",{});var $E=l(Ko);op=o($E,"TH",{align:!0});var PU=l(op);FO=u(PU,"Returned values"),PU.forEach(s),KO=p($E),Rq=o($E,"TH",{align:!0}),l(Rq).forEach(s),$E.forEach(s),OU.forEach(s),JO=p(qE),Nq=o(qE,"TBODY",{});var RU=l(Nq);Jo=o(RU,"TR",{});var _E=l(Jo);lp=o(_E,"TD",{align:!0});var NU=l(lp);Sq=o(NU,"STRONG",{});var SU=l(Sq);WO=u(SU,"generated_text"),SU.forEach(s),NU.forEach(s),YO=p(_E),ip=o(_E,"TD",{align:!0});var xU=l(ip);VO=u(xU,"The continuated string"),xU.forEach(s),_E.forEach(s),RU.forEach(s),qE.forEach(s),S2=p(a),Je=o(a,"H2",{class:!0});var vE=l(Je);Fs=o(vE,"A",{id:!0,class:!0,href:!0});var IU=l(Fs);xq=o(IU,"SPAN",{});var HU=l(xq);v(Wo.$$.fragment,HU),HU.forEach(s),IU.forEach(s),XO=p(vE),Iq=o(vE,"SPAN",{});var BU=l(Iq);QO=u(BU,"Text2text-generation task"),BU.forEach(s),vE.forEach(s),x2=p(a),Ks=o(a,"P",{});var yE=l(Ks);ZO=u(yE,"Essentially "),up=o(yE,"A",{href:!0});var CU=l(up);eP=u(CU,"Text-generation task"),CU.forEach(s),tP=u(yE,`. But uses
Encoder-Decoder architecture, so might change in the future for more
options.`),yE.forEach(s),I2=p(a),We=o(a,"H2",{class:!0});var bE=l(We);Js=o(bE,"A",{id:!0,class:!0,href:!0});var GU=l(Js);Hq=o(GU,"SPAN",{});var LU=l(Hq);v(Yo.$$.fragment,LU),LU.forEach(s),GU.forEach(s),sP=p(bE),Bq=o(bE,"SPAN",{});var UU=l(Bq);aP=u(UU,"Fill mask task"),UU.forEach(s),bE.forEach(s),H2=p(a),cp=o(a,"P",{});var zU=l(cp);nP=u(zU,`Tries to fill in a hole with a missing word (token to be precise).
That\u2019s the base task for BERT models.`),zU.forEach(s),B2=p(a),v(Ws.$$.fragment,a),C2=p(a),Vo=o(a,"P",{});var Ux=l(Vo);rP=u(Ux,"Available with: "),Xo=o(Ux,"A",{href:!0,rel:!0});var MU=l(Xo);oP=u(MU,"\u{1F917} Transformers"),MU.forEach(s),Ux.forEach(s),G2=p(a),fp=o(a,"P",{});var FU=l(fp);lP=u(FU,"Example:"),FU.forEach(s),L2=p(a),v(Ys.$$.fragment,a),U2=p(a),pp=o(a,"P",{});var KU=l(pp);iP=u(KU,`When sending your request, you should send a JSON encoded payload. Here
are all the options`),KU.forEach(s),z2=p(a),Vs=o(a,"TABLE",{});var EE=l(Vs);Cq=o(EE,"THEAD",{});var JU=l(Cq);Qo=o(JU,"TR",{});var wE=l(Qo);hp=o(wE,"TH",{align:!0});var WU=l(hp);uP=u(WU,"All parameters"),WU.forEach(s),cP=p(wE),Gq=o(wE,"TH",{align:!0}),l(Gq).forEach(s),wE.forEach(s),JU.forEach(s),fP=p(EE),se=o(EE,"TBODY",{});var Oe=l(se);Zo=o(Oe,"TR",{});var TE=l(Zo);el=o(TE,"TD",{align:!0});var zx=l(el);Lq=o(zx,"STRONG",{});var YU=l(Lq);pP=u(YU,"inputs"),YU.forEach(s),hP=u(zx," (required):"),zx.forEach(s),dP=p(TE),dp=o(TE,"TD",{align:!0});var VU=l(dp);gP=u(VU,"a string to be filled from, must contain the [MASK] token (check model card for exact name of the mask)"),VU.forEach(s),TE.forEach(s),mP=p(Oe),tl=o(Oe,"TR",{});var jE=l(tl);gp=o(jE,"TD",{align:!0});var XU=l(gp);Uq=o(XU,"STRONG",{});var QU=l(Uq);qP=u(QU,"options"),QU.forEach(s),XU.forEach(s),$P=p(jE),mp=o(jE,"TD",{align:!0});var ZU=l(mp);_P=u(ZU,"a dict containing the following keys:"),ZU.forEach(s),jE.forEach(s),vP=p(Oe),sl=o(Oe,"TR",{});var kE=l(sl);qp=o(kE,"TD",{align:!0});var ez=l(qp);yP=u(ez,"use_gpu"),ez.forEach(s),bP=p(kE),Xs=o(kE,"TD",{align:!0});var AE=l(Xs);EP=u(AE,"(Default: "),zq=o(AE,"CODE",{});var tz=l(zq);wP=u(tz,"false"),tz.forEach(s),TP=u(AE,"). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),AE.forEach(s),kE.forEach(s),jP=p(Oe),al=o(Oe,"TR",{});var DE=l(al);$p=o(DE,"TD",{align:!0});var sz=l($p);kP=u(sz,"use_cache"),sz.forEach(s),AP=p(DE),Qs=o(DE,"TD",{align:!0});var OE=l(Qs);DP=u(OE,"(Default: "),Mq=o(OE,"CODE",{});var az=l(Mq);OP=u(az,"true"),az.forEach(s),PP=u(OE,"). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),OE.forEach(s),DE.forEach(s),RP=p(Oe),nl=o(Oe,"TR",{});var PE=l(nl);_p=o(PE,"TD",{align:!0});var nz=l(_p);NP=u(nz,"wait_for_model"),nz.forEach(s),SP=p(PE),Zs=o(PE,"TD",{align:!0});var RE=l(Zs);xP=u(RE,"(Default: "),Fq=o(RE,"CODE",{});var rz=l(Fq);IP=u(rz,"false"),rz.forEach(s),HP=u(RE,") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),RE.forEach(s),PE.forEach(s),Oe.forEach(s),EE.forEach(s),M2=p(a),vp=o(a,"P",{});var oz=l(vp);BP=u(oz,"Return value is either a dict or a list of dicts if you sent a list of inputs"),oz.forEach(s),F2=p(a),v(ea.$$.fragment,a),K2=p(a),ta=o(a,"TABLE",{});var NE=l(ta);Kq=o(NE,"THEAD",{});var lz=l(Kq);rl=o(lz,"TR",{});var SE=l(rl);yp=o(SE,"TH",{align:!0});var iz=l(yp);CP=u(iz,"Returned values"),iz.forEach(s),GP=p(SE),Jq=o(SE,"TH",{align:!0}),l(Jq).forEach(s),SE.forEach(s),lz.forEach(s),LP=p(NE),fe=o(NE,"TBODY",{});var Ya=l(fe);ol=o(Ya,"TR",{});var xE=l(ol);bp=o(xE,"TD",{align:!0});var uz=l(bp);Wq=o(uz,"STRONG",{});var cz=l(Wq);UP=u(cz,"sequence"),cz.forEach(s),uz.forEach(s),zP=p(xE),Ep=o(xE,"TD",{align:!0});var fz=l(Ep);MP=u(fz,"The actual sequence of tokens that ran against the model (may contain special tokens)"),fz.forEach(s),xE.forEach(s),FP=p(Ya),ll=o(Ya,"TR",{});var IE=l(ll);wp=o(IE,"TD",{align:!0});var pz=l(wp);Yq=o(pz,"STRONG",{});var hz=l(Yq);KP=u(hz,"score"),hz.forEach(s),pz.forEach(s),JP=p(IE),Tp=o(IE,"TD",{align:!0});var dz=l(Tp);WP=u(dz,"The probability for this token."),dz.forEach(s),IE.forEach(s),YP=p(Ya),il=o(Ya,"TR",{});var HE=l(il);jp=o(HE,"TD",{align:!0});var gz=l(jp);Vq=o(gz,"STRONG",{});var mz=l(Vq);VP=u(mz,"token"),mz.forEach(s),gz.forEach(s),XP=p(HE),kp=o(HE,"TD",{align:!0});var qz=l(kp);QP=u(qz,"The id of the token"),qz.forEach(s),HE.forEach(s),ZP=p(Ya),ul=o(Ya,"TR",{});var BE=l(ul);Ap=o(BE,"TD",{align:!0});var $z=l(Ap);Xq=o($z,"STRONG",{});var _z=l(Xq);eR=u(_z,"token_str"),_z.forEach(s),$z.forEach(s),tR=p(BE),Dp=o(BE,"TD",{align:!0});var vz=l(Dp);sR=u(vz,"The string representation of the token"),vz.forEach(s),BE.forEach(s),Ya.forEach(s),NE.forEach(s),J2=p(a),Ye=o(a,"H2",{class:!0});var CE=l(Ye);sa=o(CE,"A",{id:!0,class:!0,href:!0});var yz=l(sa);Qq=o(yz,"SPAN",{});var bz=l(Qq);v(cl.$$.fragment,bz),bz.forEach(s),yz.forEach(s),aR=p(CE),Zq=o(CE,"SPAN",{});var Ez=l(Zq);nR=u(Ez,"Automatic speech recognition task"),Ez.forEach(s),CE.forEach(s),W2=p(a),Op=o(a,"P",{});var wz=l(Op);rR=u(wz,`This task reads some audio input and outputs the said words within the
audio files.`),wz.forEach(s),Y2=p(a),v(aa.$$.fragment,a),V2=p(a),v(na.$$.fragment,a),X2=p(a),pe=o(a,"P",{});var Pi=l(pe);oR=u(Pi,"Available with: "),fl=o(Pi,"A",{href:!0,rel:!0});var Tz=l(fl);lR=u(Tz,"\u{1F917} Transformers"),Tz.forEach(s),iR=p(Pi),pl=o(Pi,"A",{href:!0,rel:!0});var jz=l(pl);uR=u(jz,"ESPnet"),jz.forEach(s),cR=u(Pi,` and
`),hl=o(Pi,"A",{href:!0,rel:!0});var kz=l(hl);fR=u(kz,"SpeechBrain"),kz.forEach(s),Pi.forEach(s),Q2=p(a),Pp=o(a,"P",{});var Az=l(Pp);pR=u(Az,"Request:"),Az.forEach(s),Z2=p(a),v(ra.$$.fragment,a),ev=p(a),Rp=o(a,"P",{});var Dz=l(Rp);hR=u(Dz,`When sending your request, you should send a binary payload that simply
contains your audio file. We try to support most formats (Flac, Wav,
Mp3, Ogg etc...). And we automatically rescale the sampling rate to the
appropriate rate for the given model (usually 16KHz).`),Dz.forEach(s),tv=p(a),oa=o(a,"TABLE",{});var GE=l(oa);e$=o(GE,"THEAD",{});var Oz=l(e$);dl=o(Oz,"TR",{});var LE=l(dl);Np=o(LE,"TH",{align:!0});var Pz=l(Np);dR=u(Pz,"All parameters"),Pz.forEach(s),gR=p(LE),t$=o(LE,"TH",{align:!0}),l(t$).forEach(s),LE.forEach(s),Oz.forEach(s),mR=p(GE),s$=o(GE,"TBODY",{});var Rz=l(s$);gl=o(Rz,"TR",{});var UE=l(gl);ml=o(UE,"TD",{align:!0});var Mx=l(ml);a$=o(Mx,"STRONG",{});var Nz=l(a$);qR=u(Nz,"no parameter"),Nz.forEach(s),$R=u(Mx," (required)"),Mx.forEach(s),_R=p(UE),Sp=o(UE,"TD",{align:!0});var Sz=l(Sp);vR=u(Sz,"a binary representation of the audio file. No other parameters are currently allowed."),Sz.forEach(s),UE.forEach(s),Rz.forEach(s),GE.forEach(s),sv=p(a),xp=o(a,"P",{});var xz=l(xp);yR=u(xz,"Return value is either a dict or a list of dicts if you sent a list of inputs"),xz.forEach(s),av=p(a),Ip=o(a,"P",{});var Iz=l(Ip);bR=u(Iz,"Response:"),Iz.forEach(s),nv=p(a),v(la.$$.fragment,a),rv=p(a),ia=o(a,"TABLE",{});var zE=l(ia);n$=o(zE,"THEAD",{});var Hz=l(n$);ql=o(Hz,"TR",{});var ME=l(ql);Hp=o(ME,"TH",{align:!0});var Bz=l(Hp);ER=u(Bz,"Returned values"),Bz.forEach(s),wR=p(ME),r$=o(ME,"TH",{align:!0}),l(r$).forEach(s),ME.forEach(s),Hz.forEach(s),TR=p(zE),o$=o(zE,"TBODY",{});var Cz=l(o$);$l=o(Cz,"TR",{});var FE=l($l);Bp=o(FE,"TD",{align:!0});var Gz=l(Bp);l$=o(Gz,"STRONG",{});var Lz=l(l$);jR=u(Lz,"text"),Lz.forEach(s),Gz.forEach(s),kR=p(FE),Cp=o(FE,"TD",{align:!0});var Uz=l(Cp);AR=u(Uz,"The string that was recognized within the audio file."),Uz.forEach(s),FE.forEach(s),Cz.forEach(s),zE.forEach(s),ov=p(a),Ve=o(a,"H2",{class:!0});var KE=l(Ve);ua=o(KE,"A",{id:!0,class:!0,href:!0});var zz=l(ua);i$=o(zz,"SPAN",{});var Mz=l(i$);v(_l.$$.fragment,Mz),Mz.forEach(s),zz.forEach(s),DR=p(KE),u$=o(KE,"SPAN",{});var Fz=l(u$);OR=u(Fz,"Feature-extraction task"),Fz.forEach(s),KE.forEach(s),lv=p(a),Gp=o(a,"P",{});var Kz=l(Gp);PR=u(Kz,`This task reads some text and outputs raw float values, that are usually
consumed as part of a semantic database/semantic search.`),Kz.forEach(s),iv=p(a),v(ca.$$.fragment,a),uv=p(a),Xe=o(a,"P",{});var C_=l(Xe);RR=u(C_,"Available with: "),vl=o(C_,"A",{href:!0,rel:!0});var Jz=l(vl);NR=u(Jz,"\u{1F917} Transformers"),Jz.forEach(s),SR=p(C_),yl=o(C_,"A",{href:!0,rel:!0});var Wz=l(yl);xR=u(Wz,"Sentence-transformers"),Wz.forEach(s),C_.forEach(s),cv=p(a),Lp=o(a,"P",{});var Yz=l(Lp);IR=u(Yz,"Request:"),Yz.forEach(s),fv=p(a),fa=o(a,"TABLE",{});var JE=l(fa);c$=o(JE,"THEAD",{});var Vz=l(c$);bl=o(Vz,"TR",{});var WE=l(bl);Up=o(WE,"TH",{align:!0});var Xz=l(Up);HR=u(Xz,"All parameters"),Xz.forEach(s),BR=p(WE),f$=o(WE,"TH",{align:!0}),l(f$).forEach(s),WE.forEach(s),Vz.forEach(s),CR=p(JE),ae=o(JE,"TBODY",{});var Pe=l(ae);El=o(Pe,"TR",{});var YE=l(El);wl=o(YE,"TD",{align:!0});var Fx=l(wl);p$=o(Fx,"STRONG",{});var Qz=l(p$);GR=u(Qz,"inputs"),Qz.forEach(s),LR=u(Fx," (required):"),Fx.forEach(s),UR=p(YE),zp=o(YE,"TD",{align:!0});var Zz=l(zp);zR=u(Zz,"a string or a list of strings to get the features from."),Zz.forEach(s),YE.forEach(s),MR=p(Pe),Tl=o(Pe,"TR",{});var VE=l(Tl);Mp=o(VE,"TD",{align:!0});var eM=l(Mp);h$=o(eM,"STRONG",{});var tM=l(h$);FR=u(tM,"options"),tM.forEach(s),eM.forEach(s),KR=p(VE),Fp=o(VE,"TD",{align:!0});var sM=l(Fp);JR=u(sM,"a dict containing the following keys:"),sM.forEach(s),VE.forEach(s),WR=p(Pe),jl=o(Pe,"TR",{});var XE=l(jl);Kp=o(XE,"TD",{align:!0});var aM=l(Kp);YR=u(aM,"use_gpu"),aM.forEach(s),VR=p(XE),pa=o(XE,"TD",{align:!0});var QE=l(pa);XR=u(QE,"(Default: "),d$=o(QE,"CODE",{});var nM=l(d$);QR=u(nM,"false"),nM.forEach(s),ZR=u(QE,"). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),QE.forEach(s),XE.forEach(s),eN=p(Pe),kl=o(Pe,"TR",{});var ZE=l(kl);Jp=o(ZE,"TD",{align:!0});var rM=l(Jp);tN=u(rM,"use_cache"),rM.forEach(s),sN=p(ZE),ha=o(ZE,"TD",{align:!0});var ew=l(ha);aN=u(ew,"(Default: "),g$=o(ew,"CODE",{});var oM=l(g$);nN=u(oM,"true"),oM.forEach(s),rN=u(ew,"). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),ew.forEach(s),ZE.forEach(s),oN=p(Pe),Al=o(Pe,"TR",{});var tw=l(Al);Wp=o(tw,"TD",{align:!0});var lM=l(Wp);lN=u(lM,"wait_for_model"),lM.forEach(s),iN=p(tw),da=o(tw,"TD",{align:!0});var sw=l(da);uN=u(sw,"(Default: "),m$=o(sw,"CODE",{});var iM=l(m$);cN=u(iM,"false"),iM.forEach(s),fN=u(sw,") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),sw.forEach(s),tw.forEach(s),Pe.forEach(s),JE.forEach(s),pv=p(a),Yp=o(a,"P",{});var uM=l(Yp);pN=u(uM,"Return value is either a dict or a list of dicts if you sent a list of inputs"),uM.forEach(s),hv=p(a),ga=o(a,"TABLE",{});var aw=l(ga);q$=o(aw,"THEAD",{});var cM=l(q$);Dl=o(cM,"TR",{});var nw=l(Dl);Vp=o(nw,"TH",{align:!0});var fM=l(Vp);hN=u(fM,"Returned values"),fM.forEach(s),dN=p(nw),$$=o(nw,"TH",{align:!0}),l($$).forEach(s),nw.forEach(s),cM.forEach(s),gN=p(aw),_$=o(aw,"TBODY",{});var pM=l(_$);Ol=o(pM,"TR",{});var rw=l(Ol);Xp=o(rw,"TD",{align:!0});var hM=l(Xp);v$=o(hM,"STRONG",{});var dM=l(v$);mN=u(dM,"A list of float (or list of list of floats)"),dM.forEach(s),hM.forEach(s),qN=p(rw),Qp=o(rw,"TD",{align:!0});var gM=l(Qp);$N=u(gM,"The numbers that are the representation features of the input."),gM.forEach(s),rw.forEach(s),pM.forEach(s),aw.forEach(s),dv=p(a),Zp=o(a,"SMALL",{});var mM=l(Zp);_N=u(mM,`Returned values are a list of floats, or a list of list of floats
(depending on if you sent a string or a list of string, and if the
automatic reduction, usually mean_pooling for instance was applied for
you or not. This should be explained on the model's README.`),mM.forEach(s),gv=p(a),Qe=o(a,"H2",{class:!0});var ow=l(Qe);ma=o(ow,"A",{id:!0,class:!0,href:!0});var qM=l(ma);y$=o(qM,"SPAN",{});var $M=l(y$);v(Pl.$$.fragment,$M),$M.forEach(s),qM.forEach(s),vN=p(ow),b$=o(ow,"SPAN",{});var _M=l(b$);yN=u(_M,"Audio-classification task"),_M.forEach(s),ow.forEach(s),mv=p(a),eh=o(a,"P",{});var vM=l(eh);bN=u(vM,"This task reads some audio input and outputs the likelihood of classes."),vM.forEach(s),qv=p(a),v(qa.$$.fragment,a),$v=p(a),Ze=o(a,"P",{});var G_=l(Ze);EN=u(G_,"Available with: "),Rl=o(G_,"A",{href:!0,rel:!0});var yM=l(Rl);wN=u(yM,"\u{1F917} Transformers"),yM.forEach(s),TN=p(G_),Nl=o(G_,"A",{href:!0,rel:!0});var bM=l(Nl);jN=u(bM,"SpeechBrain"),bM.forEach(s),G_.forEach(s),_v=p(a),th=o(a,"P",{});var EM=l(th);kN=u(EM,"Request:"),EM.forEach(s),vv=p(a),v($a.$$.fragment,a),yv=p(a),sh=o(a,"P",{});var wM=l(sh);AN=u(wM,`When sending your request, you should send a binary payload that simply
contains your audio file. We try to support most formats (Flac, Wav,
Mp3, Ogg etc...). And we automatically rescale the sampling rate to the
appropriate rate for the given model (usually 16KHz).`),wM.forEach(s),bv=p(a),_a=o(a,"TABLE",{});var lw=l(_a);E$=o(lw,"THEAD",{});var TM=l(E$);Sl=o(TM,"TR",{});var iw=l(Sl);ah=o(iw,"TH",{align:!0});var jM=l(ah);DN=u(jM,"All parameters"),jM.forEach(s),ON=p(iw),w$=o(iw,"TH",{align:!0}),l(w$).forEach(s),iw.forEach(s),TM.forEach(s),PN=p(lw),T$=o(lw,"TBODY",{});var kM=l(T$);xl=o(kM,"TR",{});var uw=l(xl);Il=o(uw,"TD",{align:!0});var Kx=l(Il);j$=o(Kx,"STRONG",{});var AM=l(j$);RN=u(AM,"no parameter"),AM.forEach(s),NN=u(Kx," (required)"),Kx.forEach(s),SN=p(uw),nh=o(uw,"TD",{align:!0});var DM=l(nh);xN=u(DM,"a binary representation of the audio file. No other parameters are currently allowed."),DM.forEach(s),uw.forEach(s),kM.forEach(s),lw.forEach(s),Ev=p(a),rh=o(a,"P",{});var OM=l(rh);IN=u(OM,"Return value is a dict"),OM.forEach(s),wv=p(a),v(va.$$.fragment,a),Tv=p(a),ya=o(a,"TABLE",{});var cw=l(ya);k$=o(cw,"THEAD",{});var PM=l(k$);Hl=o(PM,"TR",{});var fw=l(Hl);oh=o(fw,"TH",{align:!0});var RM=l(oh);HN=u(RM,"Returned values"),RM.forEach(s),BN=p(fw),A$=o(fw,"TH",{align:!0}),l(A$).forEach(s),fw.forEach(s),PM.forEach(s),CN=p(cw),Bl=o(cw,"TBODY",{});var pw=l(Bl);Cl=o(pw,"TR",{});var hw=l(Cl);lh=o(hw,"TD",{align:!0});var NM=l(lh);D$=o(NM,"STRONG",{});var SM=l(D$);GN=u(SM,"label"),SM.forEach(s),NM.forEach(s),LN=p(hw),ih=o(hw,"TD",{align:!0});var xM=l(ih);UN=u(xM,"The label for the class (model specific)"),xM.forEach(s),hw.forEach(s),zN=p(pw),Gl=o(pw,"TR",{});var dw=l(Gl);uh=o(dw,"TD",{align:!0});var IM=l(uh);O$=o(IM,"STRONG",{});var HM=l(O$);MN=u(HM,"score"),HM.forEach(s),IM.forEach(s),FN=p(dw),ch=o(dw,"TD",{align:!0});var BM=l(ch);KN=u(BM,"A float that represents how likely it is that the audio file belongs to this class."),BM.forEach(s),dw.forEach(s),pw.forEach(s),cw.forEach(s),jv=p(a),et=o(a,"H2",{class:!0});var gw=l(et);ba=o(gw,"A",{id:!0,class:!0,href:!0});var CM=l(ba);P$=o(CM,"SPAN",{});var GM=l(P$);v(Ll.$$.fragment,GM),GM.forEach(s),CM.forEach(s),JN=p(gw),R$=o(gw,"SPAN",{});var LM=l(R$);WN=u(LM,"Object-detection task"),LM.forEach(s),gw.forEach(s),kv=p(a),fh=o(a,"P",{});var UM=l(fh);YN=u(UM,`This task reads some image input and outputs the likelihood of classes &
bounding boxes of detected objects.`),UM.forEach(s),Av=p(a),v(Ea.$$.fragment,a),Dv=p(a),Ul=o(a,"P",{});var Jx=l(Ul);VN=u(Jx,"Available with: "),zl=o(Jx,"A",{href:!0,rel:!0});var zM=l(zl);XN=u(zM,"\u{1F917} Transformers"),zM.forEach(s),Jx.forEach(s),Ov=p(a),ph=o(a,"P",{});var MM=l(ph);QN=u(MM,"Request:"),MM.forEach(s),Pv=p(a),v(wa.$$.fragment,a),Rv=p(a),Ta=o(a,"P",{});var mw=l(Ta);ZN=u(mw,`When sending your request, you should send a binary payload that simply
contains your image file. We support all image formats `),Ml=o(mw,"A",{href:!0,rel:!0});var FM=l(Ml);eS=u(FM,`Pillow
supports`),FM.forEach(s),tS=u(mw,"."),mw.forEach(s),Nv=p(a),ja=o(a,"TABLE",{});var qw=l(ja);N$=o(qw,"THEAD",{});var KM=l(N$);Fl=o(KM,"TR",{});var $w=l(Fl);hh=o($w,"TH",{align:!0});var JM=l(hh);sS=u(JM,"All parameters"),JM.forEach(s),aS=p($w),S$=o($w,"TH",{align:!0}),l(S$).forEach(s),$w.forEach(s),KM.forEach(s),nS=p(qw),x$=o(qw,"TBODY",{});var WM=l(x$);Kl=o(WM,"TR",{});var _w=l(Kl);Jl=o(_w,"TD",{align:!0});var Wx=l(Jl);I$=o(Wx,"STRONG",{});var YM=l(I$);rS=u(YM,"no parameter"),YM.forEach(s),oS=u(Wx," (required)"),Wx.forEach(s),lS=p(_w),dh=o(_w,"TD",{align:!0});var VM=l(dh);iS=u(VM,"a binary representation of the image file. No other parameters are currently allowed."),VM.forEach(s),_w.forEach(s),WM.forEach(s),qw.forEach(s),Sv=p(a),gh=o(a,"P",{});var XM=l(gh);uS=u(XM,"Return value is a dict"),XM.forEach(s),xv=p(a),v(ka.$$.fragment,a),Iv=p(a),Aa=o(a,"TABLE",{});var vw=l(Aa);H$=o(vw,"THEAD",{});var QM=l(H$);Wl=o(QM,"TR",{});var yw=l(Wl);mh=o(yw,"TH",{align:!0});var ZM=l(mh);cS=u(ZM,"Returned values"),ZM.forEach(s),fS=p(yw),B$=o(yw,"TH",{align:!0}),l(B$).forEach(s),yw.forEach(s),QM.forEach(s),pS=p(vw),tt=o(vw,"TBODY",{});var id=l(tt);Yl=o(id,"TR",{});var bw=l(Yl);qh=o(bw,"TD",{align:!0});var eF=l(qh);C$=o(eF,"STRONG",{});var tF=l(C$);hS=u(tF,"label"),tF.forEach(s),eF.forEach(s),dS=p(bw),$h=o(bw,"TD",{align:!0});var sF=l($h);gS=u(sF,"The label for the class (model specific) of a detected object."),sF.forEach(s),bw.forEach(s),mS=p(id),Vl=o(id,"TR",{});var Ew=l(Vl);_h=o(Ew,"TD",{align:!0});var aF=l(_h);G$=o(aF,"STRONG",{});var nF=l(G$);qS=u(nF,"score"),nF.forEach(s),aF.forEach(s),$S=p(Ew),vh=o(Ew,"TD",{align:!0});var rF=l(vh);_S=u(rF,"A float that represents how likely it is that the detected object belongs to the given class."),rF.forEach(s),Ew.forEach(s),vS=p(id),Xl=o(id,"TR",{});var ww=l(Xl);yh=o(ww,"TD",{align:!0});var oF=l(yh);L$=o(oF,"STRONG",{});var lF=l(L$);yS=u(lF,"box"),lF.forEach(s),oF.forEach(s),bS=p(ww),bh=o(ww,"TD",{align:!0});var iF=l(bh);ES=u(iF,"A dict (with keys [xmin,ymin,xmax,ymax]) representing the bounding box of a detected object."),iF.forEach(s),ww.forEach(s),id.forEach(s),vw.forEach(s),Hv=p(a),st=o(a,"H2",{class:!0});var Tw=l(st);Da=o(Tw,"A",{id:!0,class:!0,href:!0});var uF=l(Da);U$=o(uF,"SPAN",{});var cF=l(U$);v(Ql.$$.fragment,cF),cF.forEach(s),uF.forEach(s),wS=p(Tw),z$=o(Tw,"SPAN",{});var fF=l(z$);TS=u(fF,"Image Segmentation task"),fF.forEach(s),Tw.forEach(s),Bv=p(a),Eh=o(a,"P",{});var pF=l(Eh);jS=u(pF,`This task reads some image input and outputs the likelihood of classes &
bounding boxes of detected objects.`),pF.forEach(s),Cv=p(a),v(Oa.$$.fragment,a),Gv=p(a),Zl=o(a,"P",{});var Yx=l(Zl);kS=u(Yx,"Available with: "),ei=o(Yx,"A",{href:!0,rel:!0});var hF=l(ei);AS=u(hF,"\u{1F917} Transformers"),hF.forEach(s),Yx.forEach(s),Lv=p(a),wh=o(a,"P",{});var dF=l(wh);DS=u(dF,"Request:"),dF.forEach(s),Uv=p(a),v(Pa.$$.fragment,a),zv=p(a),Ra=o(a,"P",{});var jw=l(Ra);OS=u(jw,`When sending your request, you should send a binary payload that simply
contains your image file. We support all image formats `),ti=o(jw,"A",{href:!0,rel:!0});var gF=l(ti);PS=u(gF,`Pillow
supports`),gF.forEach(s),RS=u(jw,"."),jw.forEach(s),Mv=p(a),Na=o(a,"TABLE",{});var kw=l(Na);M$=o(kw,"THEAD",{});var mF=l(M$);si=o(mF,"TR",{});var Aw=l(si);Th=o(Aw,"TH",{align:!0});var qF=l(Th);NS=u(qF,"All parameters"),qF.forEach(s),SS=p(Aw),F$=o(Aw,"TH",{align:!0}),l(F$).forEach(s),Aw.forEach(s),mF.forEach(s),xS=p(kw),K$=o(kw,"TBODY",{});var $F=l(K$);ai=o($F,"TR",{});var Dw=l(ai);ni=o(Dw,"TD",{align:!0});var Vx=l(ni);J$=o(Vx,"STRONG",{});var _F=l(J$);IS=u(_F,"no parameter"),_F.forEach(s),HS=u(Vx," (required)"),Vx.forEach(s),BS=p(Dw),jh=o(Dw,"TD",{align:!0});var vF=l(jh);CS=u(vF,"a binary representation of the image file. No other parameters are currently allowed."),vF.forEach(s),Dw.forEach(s),$F.forEach(s),kw.forEach(s),Fv=p(a),kh=o(a,"P",{});var yF=l(kh);GS=u(yF,"Return value is a dict"),yF.forEach(s),Kv=p(a),v(Sa.$$.fragment,a),Jv=p(a),xa=o(a,"TABLE",{});var Ow=l(xa);W$=o(Ow,"THEAD",{});var bF=l(W$);ri=o(bF,"TR",{});var Pw=l(ri);Ah=o(Pw,"TH",{align:!0});var EF=l(Ah);LS=u(EF,"Returned values"),EF.forEach(s),US=p(Pw),Y$=o(Pw,"TH",{align:!0}),l(Y$).forEach(s),Pw.forEach(s),bF.forEach(s),zS=p(Ow),at=o(Ow,"TBODY",{});var ud=l(at);oi=o(ud,"TR",{});var Rw=l(oi);Dh=o(Rw,"TD",{align:!0});var wF=l(Dh);V$=o(wF,"STRONG",{});var TF=l(V$);MS=u(TF,"label"),TF.forEach(s),wF.forEach(s),FS=p(Rw),Oh=o(Rw,"TD",{align:!0});var jF=l(Oh);KS=u(jF,"The label for the class (model specific) of a segment."),jF.forEach(s),Rw.forEach(s),JS=p(ud),li=o(ud,"TR",{});var Nw=l(li);Ph=o(Nw,"TD",{align:!0});var kF=l(Ph);X$=o(kF,"STRONG",{});var AF=l(X$);WS=u(AF,"score"),AF.forEach(s),kF.forEach(s),YS=p(Nw),Rh=o(Nw,"TD",{align:!0});var DF=l(Rh);VS=u(DF,"A float that represents how likely it is that the segment belongs to the given class."),DF.forEach(s),Nw.forEach(s),XS=p(ud),ii=o(ud,"TR",{});var Sw=l(ii);Nh=o(Sw,"TD",{align:!0});var OF=l(Nh);Q$=o(OF,"STRONG",{});var PF=l(Q$);QS=u(PF,"mask"),PF.forEach(s),OF.forEach(s),ZS=p(Sw),Sh=o(Sw,"TD",{align:!0});var RF=l(Sh);ex=u(RF,"A str (base64 str of a single channel black-and-white img) representing the mask of a segment."),RF.forEach(s),Sw.forEach(s),ud.forEach(s),Ow.forEach(s),Wv=p(a),nt=o(a,"H2",{class:!0});var xw=l(nt);Ia=o(xw,"A",{id:!0,class:!0,href:!0});var NF=l(Ia);Z$=o(NF,"SPAN",{});var SF=l(Z$);v(ui.$$.fragment,SF),SF.forEach(s),NF.forEach(s),tx=p(xw),e_=o(xw,"SPAN",{});var xF=l(e_);sx=u(xF,"Image Classification task"),xF.forEach(s),xw.forEach(s),Yv=p(a),xh=o(a,"P",{});var IF=l(xh);ax=u(IF,"This task reads some image input and outputs the likelihood of classes."),IF.forEach(s),Vv=p(a),v(Ha.$$.fragment,a),Xv=p(a),ci=o(a,"P",{});var Xx=l(ci);nx=u(Xx,"Available with: "),fi=o(Xx,"A",{href:!0,rel:!0});var HF=l(fi);rx=u(HF,"\u{1F917} Transformers"),HF.forEach(s),Xx.forEach(s),Qv=p(a),Ih=o(a,"P",{});var BF=l(Ih);ox=u(BF,"Request:"),BF.forEach(s),Zv=p(a),v(Ba.$$.fragment,a),e0=p(a),Ca=o(a,"P",{});var Iw=l(Ca);lx=u(Iw,`When sending your request, you should send a binary payload that simply
contains your image file. We support all image formats `),pi=o(Iw,"A",{href:!0,rel:!0});var CF=l(pi);ix=u(CF,`Pillow
supports`),CF.forEach(s),ux=u(Iw,"."),Iw.forEach(s),t0=p(a),Ga=o(a,"TABLE",{});var Hw=l(Ga);t_=o(Hw,"THEAD",{});var GF=l(t_);hi=o(GF,"TR",{});var Bw=l(hi);Hh=o(Bw,"TH",{align:!0});var LF=l(Hh);cx=u(LF,"All parameters"),LF.forEach(s),fx=p(Bw),s_=o(Bw,"TH",{align:!0}),l(s_).forEach(s),Bw.forEach(s),GF.forEach(s),px=p(Hw),a_=o(Hw,"TBODY",{});var UF=l(a_);di=o(UF,"TR",{});var Cw=l(di);gi=o(Cw,"TD",{align:!0});var Qx=l(gi);n_=o(Qx,"STRONG",{});var zF=l(n_);hx=u(zF,"no parameter"),zF.forEach(s),dx=u(Qx," (required)"),Qx.forEach(s),gx=p(Cw),Bh=o(Cw,"TD",{align:!0});var MF=l(Bh);mx=u(MF,"a binary representation of the image file. No other parameters are currently allowed."),MF.forEach(s),Cw.forEach(s),UF.forEach(s),Hw.forEach(s),s0=p(a),Ch=o(a,"P",{});var FF=l(Ch);qx=u(FF,"Return value is a dict"),FF.forEach(s),a0=p(a),v(La.$$.fragment,a),n0=p(a),Ua=o(a,"TABLE",{});var Gw=l(Ua);r_=o(Gw,"THEAD",{});var KF=l(r_);mi=o(KF,"TR",{});var Lw=l(mi);Gh=o(Lw,"TH",{align:!0});var JF=l(Gh);$x=u(JF,"Returned values"),JF.forEach(s),_x=p(Lw),o_=o(Lw,"TH",{align:!0}),l(o_).forEach(s),Lw.forEach(s),KF.forEach(s),vx=p(Gw),qi=o(Gw,"TBODY",{});var Uw=l(qi);$i=o(Uw,"TR",{});var zw=l($i);Lh=o(zw,"TD",{align:!0});var WF=l(Lh);l_=o(WF,"STRONG",{});var YF=l(l_);yx=u(YF,"label"),YF.forEach(s),WF.forEach(s),bx=p(zw),Uh=o(zw,"TD",{align:!0});var VF=l(Uh);Ex=u(VF,"The label for the class (model specific)"),VF.forEach(s),zw.forEach(s),wx=p(Uw),_i=o(Uw,"TR",{});var Mw=l(_i);zh=o(Mw,"TD",{align:!0});var XF=l(zh);i_=o(XF,"STRONG",{});var QF=l(i_);Tx=u(QF,"score"),QF.forEach(s),XF.forEach(s),jx=p(Mw),Mh=o(Mw,"TD",{align:!0});var ZF=l(Mh);kx=u(ZF,"A float that represents how likely it is that the image file belongs to this class."),ZF.forEach(s),Mw.forEach(s),Uw.forEach(s),Gw.forEach(s),this.h()},h(){h(n,"name","hf:doc:metadata"),h(n,"content",JSON.stringify(OW)),h(d,"id","detailed-parameters"),h(d,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(d,"href","#detailed-parameters"),h(t,"class","relative group"),h(ne,"id","which-task-is-used-by-this-model"),h(ne,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(ne,"href","#which-task-is-used-by-this-model"),h(D,"class","relative group"),h(lt,"class","block dark:hidden"),eK(lt.src,Zx="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/inference-api/task.png")||h(lt,"src",Zx),h(lt,"width","300"),h(it,"class","hidden dark:block invert"),eK(it.src,eI="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/inference-api/task-dark.png")||h(it,"src",eI),h(it,"width","300"),h(ut,"id","zeroshot-classification-task"),h(ut,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(ut,"href","#zeroshot-classification-task"),h(Se,"class","relative group"),h(Za,"href","https://github.com/huggingface/transformers"),h(Za,"rel","nofollow"),h(Hi,"align","left"),h(hd,"align","left"),h(sn,"align","left"),h(Bi,"align","left"),h(nn,"align","left"),h(Ci,"align","left"),h(Gi,"align","left"),h(he,"align","left"),h(Li,"align","left"),h(ht,"align","left"),h(Ui,"align","left"),h(zi,"align","left"),h(Mi,"align","left"),h(dt,"align","left"),h(Fi,"align","left"),h(gt,"align","left"),h(Ki,"align","left"),h(mt,"align","left"),h(Yi,"align","left"),h(wd,"align","left"),h(Vi,"align","left"),h(Xi,"align","left"),h(Qi,"align","left"),h(Zi,"align","left"),h(eu,"align","left"),h(_t,"align","left"),h(vt,"id","translation-task"),h(vt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(vt,"href","#translation-task"),h(Ie,"class","relative group"),h($n,"href","https://github.com/huggingface/transformers"),h($n,"rel","nofollow"),h(nu,"align","left"),h(Rd,"align","left"),h(yn,"align","left"),h(ru,"align","left"),h(ou,"align","left"),h(lu,"align","left"),h(iu,"align","left"),h(wt,"align","left"),h(uu,"align","left"),h(Tt,"align","left"),h(cu,"align","left"),h(jt,"align","left"),h(pu,"align","left"),h(Cd,"align","left"),h(hu,"align","left"),h(du,"align","left"),h(At,"id","summarization-task"),h(At,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(At,"href","#summarization-task"),h(He,"class","relative group"),h(gu,"href","mailto:api-enterprise@huggingface.co"),h(On,"href","https://github.com/huggingface/transformers"),h(On,"rel","nofollow"),h($u,"align","left"),h(Fd,"align","left"),h(Nn,"align","left"),h(_u,"align","left"),h(vu,"align","left"),h(yu,"align","left"),h(bu,"align","left"),h(de,"align","left"),h(Eu,"align","left"),h(ge,"align","left"),h(wu,"align","left"),h(me,"align","left"),h(Tu,"align","left"),h(re,"align","left"),h(ju,"align","left"),h(qe,"align","left"),h(ku,"align","left"),h(Nt,"align","left"),h(Au,"align","left"),h(St,"align","left"),h(Du,"align","left"),h(Ou,"align","left"),h(Pu,"align","left"),h(xt,"align","left"),h(Ru,"align","left"),h(It,"align","left"),h(Nu,"align","left"),h(Ht,"align","left"),h(xu,"align","left"),h(pg,"align","left"),h(Iu,"align","left"),h(Hu,"align","left"),h(Ct,"id","conversational-task"),h(Ct,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(Ct,"href","#conversational-task"),h(Be,"class","relative group"),h(Vn,"href","https://github.com/huggingface/transformers"),h(Vn,"rel","nofollow"),h(Lu,"align","left"),h($g,"align","left"),h(Zn,"align","left"),h(vg,"align","left"),h(Uu,"align","left"),h(zu,"align","left"),h(Mu,"align","left"),h(Fu,"align","left"),h(Ku,"align","left"),h(zt,"align","left"),h(Ju,"align","left"),h(Wu,"align","left"),h(Yu,"align","left"),h($e,"align","left"),h(Vu,"align","left"),h(_e,"align","left"),h(Xu,"align","left"),h(ve,"align","left"),h(Qu,"align","left"),h(oe,"align","left"),h(Zu,"align","left"),h(ye,"align","left"),h(ec,"align","left"),h(Mt,"align","left"),h(tc,"align","left"),h(Ft,"align","left"),h(sc,"align","left"),h(ac,"align","left"),h(nc,"align","left"),h(Kt,"align","left"),h(rc,"align","left"),h(Jt,"align","left"),h(oc,"align","left"),h(Wt,"align","left"),h(ic,"align","left"),h(Lg,"align","left"),h(uc,"align","left"),h(cc,"align","left"),h(fc,"align","left"),h(pc,"align","left"),h(hc,"align","left"),h(dc,"align","left"),h(gc,"align","left"),h(mc,"align","left"),h(Vt,"id","table-question-answering-task"),h(Vt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(Vt,"href","#table-question-answering-task"),h(Ce,"class","relative group"),h(br,"href","https://github.com/huggingface/transformers"),h(br,"rel","nofollow"),h(vc,"align","left"),h(Jg,"align","left"),h(Tr,"align","left"),h(Yg,"align","left"),h(yc,"align","left"),h(bc,"align","left"),h(Ec,"align","left"),h(wc,"align","left"),h(Tc,"align","left"),h(jc,"align","left"),h(kc,"align","left"),h(es,"align","left"),h(Ac,"align","left"),h(ts,"align","left"),h(Dc,"align","left"),h(ss,"align","left"),h(Pc,"align","left"),h(tm,"align","left"),h(Rc,"align","left"),h(Nc,"align","left"),h(Sc,"align","left"),h(xc,"align","left"),h(Ic,"align","left"),h(Hc,"align","left"),h(Bc,"align","left"),h(Cc,"align","left"),h(rs,"id","question-answering-task"),h(rs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(rs,"href","#question-answering-task"),h(Ge,"class","relative group"),h(Br,"href","https://github.com/huggingface/transformers"),h(Br,"rel","nofollow"),h(Cr,"href","https://github.com/allenai/allennlp"),h(Cr,"rel","nofollow"),h(Mc,"align","left"),h(um,"align","left"),h(Fc,"align","left"),h(Kc,"align","left"),h(Jc,"align","left"),h(Wc,"align","left"),h(Yc,"align","left"),h(cs,"align","left"),h(Vc,"align","left"),h(fs,"align","left"),h(ps,"id","textclassification-task"),h(ps,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(ps,"href","#textclassification-task"),h(Ue,"class","relative group"),h(Jr,"href","https://github.com/huggingface/transformers"),h(Jr,"rel","nofollow"),h(ef,"align","left"),h(_m,"align","left"),h(Vr,"align","left"),h(tf,"align","left"),h(sf,"align","left"),h(af,"align","left"),h(nf,"align","left"),h(ms,"align","left"),h(rf,"align","left"),h(qs,"align","left"),h(of,"align","left"),h($s,"align","left"),h(uf,"align","left"),h(jm,"align","left"),h(cf,"align","left"),h(ff,"align","left"),h(pf,"align","left"),h(hf,"align","left"),h(ys,"id","named-entity-recognition-ner-task"),h(ys,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(ys,"href","#named-entity-recognition-ner-task"),h(ze,"class","relative group"),h(df,"href","#token-classification-task"),h(bs,"id","tokenclassification-task"),h(bs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(bs,"href","#tokenclassification-task"),h(Me,"class","relative group"),h(io,"href","https://github.com/huggingface/transformers"),h(io,"rel","nofollow"),h(uo,"href","https://github.com/flairNLP/flair"),h(uo,"rel","nofollow"),h($f,"align","left"),h(Sm,"align","left"),h(po,"align","left"),h(_f,"align","left"),h(vf,"align","left"),h(yf,"align","left"),h(bf,"align","left"),h(x,"align","left"),h(Ef,"align","left"),h(wf,"align","left"),h(Tf,"align","left"),h(js,"align","left"),h(jf,"align","left"),h(ks,"align","left"),h(kf,"align","left"),h(As,"align","left"),h(Df,"align","left"),h(Xm,"align","left"),h(Of,"align","left"),h(Pf,"align","left"),h(Rf,"align","left"),h(Nf,"align","left"),h(Sf,"align","left"),h(xf,"align","left"),h(If,"align","left"),h(Ps,"align","left"),h(Hf,"align","left"),h(Rs,"align","left"),h(Ns,"id","textgeneration-task"),h(Ns,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(Ns,"href","#textgeneration-task"),h(Ke,"class","relative group"),h(Ao,"href","https://github.com/huggingface/transformers"),h(Ao,"rel","nofollow"),h(Lf,"align","left"),h(iq,"align","left"),h(Po,"align","left"),h(Uf,"align","left"),h(zf,"align","left"),h(Mf,"align","left"),h(Ff,"align","left"),h(be,"align","left"),h(Kf,"align","left"),h(le,"align","left"),h(Jf,"align","left"),h(Ee,"align","left"),h(Wf,"align","left"),h(Hs,"align","left"),h(Yf,"align","left"),h(we,"align","left"),h(Vf,"align","left"),h(Te,"align","left"),h(Xf,"align","left"),h(je,"align","left"),h(Qf,"align","left"),h(Bs,"align","left"),h(Zf,"align","left"),h(Cs,"align","left"),h(ep,"align","left"),h(tp,"align","left"),h(sp,"align","left"),h(Gs,"align","left"),h(ap,"align","left"),h(Ls,"align","left"),h(np,"align","left"),h(Us,"align","left"),h(op,"align","left"),h(Rq,"align","left"),h(lp,"align","left"),h(ip,"align","left"),h(Fs,"id","text2textgeneration-task"),h(Fs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(Fs,"href","#text2textgeneration-task"),h(Je,"class","relative group"),h(up,"href","#text-generation-task"),h(Js,"id","fill-mask-task"),h(Js,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(Js,"href","#fill-mask-task"),h(We,"class","relative group"),h(Xo,"href","https://github.com/huggingface/transformers"),h(Xo,"rel","nofollow"),h(hp,"align","left"),h(Gq,"align","left"),h(el,"align","left"),h(dp,"align","left"),h(gp,"align","left"),h(mp,"align","left"),h(qp,"align","left"),h(Xs,"align","left"),h($p,"align","left"),h(Qs,"align","left"),h(_p,"align","left"),h(Zs,"align","left"),h(yp,"align","left"),h(Jq,"align","left"),h(bp,"align","left"),h(Ep,"align","left"),h(wp,"align","left"),h(Tp,"align","left"),h(jp,"align","left"),h(kp,"align","left"),h(Ap,"align","left"),h(Dp,"align","left"),h(sa,"id","automatic-speech-recognition-task"),h(sa,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(sa,"href","#automatic-speech-recognition-task"),h(Ye,"class","relative group"),h(fl,"href","https://github.com/huggingface/transformers"),h(fl,"rel","nofollow"),h(pl,"href","https://github.com/espnet/espnet"),h(pl,"rel","nofollow"),h(hl,"href","https://github.com/speechbrain/speechbrain"),h(hl,"rel","nofollow"),h(Np,"align","left"),h(t$,"align","left"),h(ml,"align","left"),h(Sp,"align","left"),h(Hp,"align","left"),h(r$,"align","left"),h(Bp,"align","left"),h(Cp,"align","left"),h(ua,"id","featureextraction-task"),h(ua,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(ua,"href","#featureextraction-task"),h(Ve,"class","relative group"),h(vl,"href","https://github.com/huggingface/transformers"),h(vl,"rel","nofollow"),h(yl,"href","https://github.com/UKPLab/sentence-transformers"),h(yl,"rel","nofollow"),h(Up,"align","left"),h(f$,"align","left"),h(wl,"align","left"),h(zp,"align","left"),h(Mp,"align","left"),h(Fp,"align","left"),h(Kp,"align","left"),h(pa,"align","left"),h(Jp,"align","left"),h(ha,"align","left"),h(Wp,"align","left"),h(da,"align","left"),h(Vp,"align","left"),h($$,"align","left"),h(Xp,"align","left"),h(Qp,"align","left"),h(ma,"id","audioclassification-task"),h(ma,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(ma,"href","#audioclassification-task"),h(Qe,"class","relative group"),h(Rl,"href","https://github.com/huggingface/transformers"),h(Rl,"rel","nofollow"),h(Nl,"href","https://github.com/speechbrain/speechbrain"),h(Nl,"rel","nofollow"),h(ah,"align","left"),h(w$,"align","left"),h(Il,"align","left"),h(nh,"align","left"),h(oh,"align","left"),h(A$,"align","left"),h(lh,"align","left"),h(ih,"align","left"),h(uh,"align","left"),h(ch,"align","left"),h(ba,"id","objectdetection-task"),h(ba,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(ba,"href","#objectdetection-task"),h(et,"class","relative group"),h(zl,"href","https://github.com/huggingface/transformers"),h(zl,"rel","nofollow"),h(Ml,"href","https://pillow.readthedocs.io/en/stable/handbook/image-file-formats.html"),h(Ml,"rel","nofollow"),h(hh,"align","left"),h(S$,"align","left"),h(Jl,"align","left"),h(dh,"align","left"),h(mh,"align","left"),h(B$,"align","left"),h(qh,"align","left"),h($h,"align","left"),h(_h,"align","left"),h(vh,"align","left"),h(yh,"align","left"),h(bh,"align","left"),h(Da,"id","image-segmentation-task"),h(Da,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(Da,"href","#image-segmentation-task"),h(st,"class","relative group"),h(ei,"href","https://github.com/huggingface/transformers"),h(ei,"rel","nofollow"),h(ti,"href","https://pillow.readthedocs.io/en/stable/handbook/image-file-formats.html"),h(ti,"rel","nofollow"),h(Th,"align","left"),h(F$,"align","left"),h(ni,"align","left"),h(jh,"align","left"),h(Ah,"align","left"),h(Y$,"align","left"),h(Dh,"align","left"),h(Oh,"align","left"),h(Ph,"align","left"),h(Rh,"align","left"),h(Nh,"align","left"),h(Sh,"align","left"),h(Ia,"id","image-classification-task"),h(Ia,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(Ia,"href","#image-classification-task"),h(nt,"class","relative group"),h(fi,"href","https://github.com/huggingface/transformers"),h(fi,"rel","nofollow"),h(pi,"href","https://pillow.readthedocs.io/en/stable/handbook/image-file-formats.html"),h(pi,"rel","nofollow"),h(Hh,"align","left"),h(s_,"align","left"),h(gi,"align","left"),h(Bh,"align","left"),h(Gh,"align","left"),h(o_,"align","left"),h(Lh,"align","left"),h(Uh,"align","left"),h(zh,"align","left"),h(Mh,"align","left")},m(a,g){e(document.head,n),m(a,c,g),m(a,t,g),e(t,d),e(d,q),y(k,q,null),e(t,A),e(t,j),e(j,T),m(a,N,g),m(a,D,g),e(D,ne),e(ne,Re),y(Q,Re,null),e(D,Y),e(D,ot),e(ot,Ri),m(a,Va,g),m(a,Ne,g),e(Ne,Fw),m(a,L_,g),m(a,Ni,g),e(Ni,Kw),m(a,U_,g),m(a,lt,g),m(a,z_,g),m(a,it,g),m(a,M_,g),m(a,Se,g),e(Se,ut),e(ut,cd),y(Xa,cd,null),e(Se,Jw),e(Se,fd),e(fd,Ww),m(a,F_,g),m(a,Si,g),e(Si,Yw),m(a,K_,g),y(ct,a,g),m(a,J_,g),m(a,Qa,g),e(Qa,Vw),e(Qa,Za),e(Za,Xw),m(a,W_,g),m(a,xi,g),e(xi,Qw),m(a,Y_,g),y(ft,a,g),m(a,V_,g),m(a,Ii,g),e(Ii,Zw),m(a,X_,g),m(a,pt,g),e(pt,pd),e(pd,en),e(en,Hi),e(Hi,e3),e(en,t3),e(en,hd),e(pt,s3),e(pt,M),e(M,tn),e(tn,sn),e(sn,dd),e(dd,a3),e(sn,n3),e(tn,r3),e(tn,Bi),e(Bi,o3),e(M,l3),e(M,an),e(an,nn),e(nn,gd),e(gd,i3),e(nn,u3),e(an,c3),e(an,Ci),e(Ci,f3),e(M,p3),e(M,rn),e(rn,Gi),e(Gi,h3),e(rn,d3),e(rn,he),e(he,g3),e(he,md),e(md,m3),e(he,q3),e(he,qd),e(qd,$3),e(he,_3),e(M,v3),e(M,on),e(on,Li),e(Li,y3),e(on,b3),e(on,ht),e(ht,E3),e(ht,$d),e($d,w3),e(ht,T3),e(M,j3),e(M,ln),e(ln,Ui),e(Ui,_d),e(_d,k3),e(ln,A3),e(ln,zi),e(zi,D3),e(M,O3),e(M,un),e(un,Mi),e(Mi,P3),e(un,R3),e(un,dt),e(dt,N3),e(dt,vd),e(vd,S3),e(dt,x3),e(M,I3),e(M,cn),e(cn,Fi),e(Fi,H3),e(cn,B3),e(cn,gt),e(gt,C3),e(gt,yd),e(yd,G3),e(gt,L3),e(M,U3),e(M,fn),e(fn,Ki),e(Ki,z3),e(fn,M3),e(fn,mt),e(mt,F3),e(mt,bd),e(bd,K3),e(mt,J3),m(a,Q_,g),m(a,Ji,g),e(Ji,W3),m(a,Z_,g),m(a,Wi,g),e(Wi,Y3),m(a,e1,g),y(qt,a,g),m(a,t1,g),m(a,$t,g),e($t,Ed),e(Ed,pn),e(pn,Yi),e(Yi,V3),e(pn,X3),e(pn,wd),e($t,Q3),e($t,xe),e(xe,hn),e(hn,Vi),e(Vi,Td),e(Td,Z3),e(hn,eT),e(hn,Xi),e(Xi,tT),e(xe,sT),e(xe,dn),e(dn,Qi),e(Qi,jd),e(jd,aT),e(dn,nT),e(dn,Zi),e(Zi,rT),e(xe,oT),e(xe,gn),e(gn,eu),e(eu,kd),e(kd,lT),e(gn,iT),e(gn,_t),e(_t,uT),e(_t,Ad),e(Ad,cT),e(_t,fT),m(a,s1,g),m(a,Ie,g),e(Ie,vt),e(vt,Dd),y(mn,Dd,null),e(Ie,pT),e(Ie,Od),e(Od,hT),m(a,a1,g),m(a,tu,g),e(tu,dT),m(a,n1,g),y(yt,a,g),m(a,r1,g),m(a,qn,g),e(qn,gT),e(qn,$n),e($n,mT),m(a,o1,g),m(a,su,g),e(su,qT),m(a,l1,g),y(bt,a,g),m(a,i1,g),m(a,au,g),e(au,$T),m(a,u1,g),m(a,Et,g),e(Et,Pd),e(Pd,_n),e(_n,nu),e(nu,_T),e(_n,vT),e(_n,Rd),e(Et,yT),e(Et,Z),e(Z,vn),e(vn,yn),e(yn,Nd),e(Nd,bT),e(yn,ET),e(vn,wT),e(vn,ru),e(ru,TT),e(Z,jT),e(Z,bn),e(bn,ou),e(ou,Sd),e(Sd,kT),e(bn,AT),e(bn,lu),e(lu,DT),e(Z,OT),e(Z,En),e(En,iu),e(iu,PT),e(En,RT),e(En,wt),e(wt,NT),e(wt,xd),e(xd,ST),e(wt,xT),e(Z,IT),e(Z,wn),e(wn,uu),e(uu,HT),e(wn,BT),e(wn,Tt),e(Tt,CT),e(Tt,Id),e(Id,GT),e(Tt,LT),e(Z,UT),e(Z,Tn),e(Tn,cu),e(cu,zT),e(Tn,MT),e(Tn,jt),e(jt,FT),e(jt,Hd),e(Hd,KT),e(jt,JT),m(a,c1,g),m(a,fu,g),e(fu,WT),m(a,f1,g),m(a,kt,g),e(kt,Bd),e(Bd,jn),e(jn,pu),e(pu,YT),e(jn,VT),e(jn,Cd),e(kt,XT),e(kt,Gd),e(Gd,kn),e(kn,hu),e(hu,Ld),e(Ld,QT),e(kn,ZT),e(kn,du),e(du,e4),m(a,p1,g),m(a,He,g),e(He,At),e(At,Ud),y(An,Ud,null),e(He,t4),e(He,zd),e(zd,s4),m(a,h1,g),m(a,Dt,g),e(Dt,a4),e(Dt,gu),e(gu,n4),e(Dt,r4),m(a,d1,g),y(Ot,a,g),m(a,g1,g),m(a,Dn,g),e(Dn,o4),e(Dn,On),e(On,l4),m(a,m1,g),m(a,mu,g),e(mu,i4),m(a,q1,g),y(Pt,a,g),m(a,$1,g),m(a,qu,g),e(qu,u4),m(a,_1,g),m(a,Rt,g),e(Rt,Md),e(Md,Pn),e(Pn,$u),e($u,c4),e(Pn,f4),e(Pn,Fd),e(Rt,p4),e(Rt,G),e(G,Rn),e(Rn,Nn),e(Nn,Kd),e(Kd,h4),e(Nn,d4),e(Rn,g4),e(Rn,_u),e(_u,m4),e(G,q4),e(G,Sn),e(Sn,vu),e(vu,Jd),e(Jd,$4),e(Sn,_4),e(Sn,yu),e(yu,v4),e(G,y4),e(G,xn),e(xn,bu),e(bu,b4),e(xn,E4),e(xn,de),e(de,w4),e(de,Wd),e(Wd,T4),e(de,j4),e(de,Yd),e(Yd,k4),e(de,A4),e(G,D4),e(G,In),e(In,Eu),e(Eu,O4),e(In,P4),e(In,ge),e(ge,R4),e(ge,Vd),e(Vd,N4),e(ge,S4),e(ge,Xd),e(Xd,x4),e(ge,I4),e(G,H4),e(G,Hn),e(Hn,wu),e(wu,B4),e(Hn,C4),e(Hn,me),e(me,G4),e(me,Qd),e(Qd,L4),e(me,U4),e(me,Zd),e(Zd,z4),e(me,M4),e(G,F4),e(G,Bn),e(Bn,Tu),e(Tu,K4),e(Bn,J4),e(Bn,re),e(re,W4),e(re,eg),e(eg,Y4),e(re,V4),e(re,tg),e(tg,X4),e(re,Q4),e(re,sg),e(sg,Z4),e(re,e5),e(G,t5),e(G,Cn),e(Cn,ju),e(ju,s5),e(Cn,a5),e(Cn,qe),e(qe,n5),e(qe,ag),e(ag,r5),e(qe,o5),e(qe,ng),e(ng,l5),e(qe,i5),e(G,u5),e(G,Gn),e(Gn,ku),e(ku,c5),e(Gn,f5),e(Gn,Nt),e(Nt,p5),e(Nt,rg),e(rg,h5),e(Nt,d5),e(G,g5),e(G,Ln),e(Ln,Au),e(Au,m5),e(Ln,q5),e(Ln,St),e(St,$5),e(St,og),e(og,_5),e(St,v5),e(G,y5),e(G,Un),e(Un,Du),e(Du,lg),e(lg,b5),e(Un,E5),e(Un,Ou),e(Ou,w5),e(G,T5),e(G,zn),e(zn,Pu),e(Pu,j5),e(zn,k5),e(zn,xt),e(xt,A5),e(xt,ig),e(ig,D5),e(xt,O5),e(G,P5),e(G,Mn),e(Mn,Ru),e(Ru,R5),e(Mn,N5),e(Mn,It),e(It,S5),e(It,ug),e(ug,x5),e(It,I5),e(G,H5),e(G,Fn),e(Fn,Nu),e(Nu,B5),e(Fn,C5),e(Fn,Ht),e(Ht,G5),e(Ht,cg),e(cg,L5),e(Ht,U5),m(a,v1,g),m(a,Su,g),e(Su,z5),m(a,y1,g),m(a,Bt,g),e(Bt,fg),e(fg,Kn),e(Kn,xu),e(xu,M5),e(Kn,F5),e(Kn,pg),e(Bt,K5),e(Bt,hg),e(hg,Jn),e(Jn,Iu),e(Iu,dg),e(dg,J5),e(Jn,W5),e(Jn,Hu),e(Hu,Y5),m(a,b1,g),m(a,Be,g),e(Be,Ct),e(Ct,gg),y(Wn,gg,null),e(Be,V5),e(Be,mg),e(mg,X5),m(a,E1,g),m(a,Bu,g),e(Bu,Q5),m(a,w1,g),y(Gt,a,g),m(a,T1,g),m(a,Yn,g),e(Yn,Z5),e(Yn,Vn),e(Vn,ej),m(a,j1,g),m(a,Cu,g),e(Cu,tj),m(a,k1,g),y(Lt,a,g),m(a,A1,g),m(a,Gu,g),e(Gu,sj),m(a,D1,g),m(a,Ut,g),e(Ut,qg),e(qg,Xn),e(Xn,Lu),e(Lu,aj),e(Xn,nj),e(Xn,$g),e(Ut,rj),e(Ut,S),e(S,Qn),e(Qn,Zn),e(Zn,_g),e(_g,oj),e(Zn,lj),e(Qn,ij),e(Qn,vg),e(S,uj),e(S,er),e(er,Uu),e(Uu,cj),e(er,fj),e(er,zu),e(zu,pj),e(S,hj),e(S,tr),e(tr,Mu),e(Mu,dj),e(tr,gj),e(tr,Fu),e(Fu,mj),e(S,qj),e(S,sr),e(sr,Ku),e(Ku,$j),e(sr,_j),e(sr,zt),e(zt,vj),e(zt,yg),e(yg,yj),e(zt,bj),e(S,Ej),e(S,ar),e(ar,Ju),e(Ju,bg),e(bg,wj),e(ar,Tj),e(ar,Wu),e(Wu,jj),e(S,kj),e(S,nr),e(nr,Yu),e(Yu,Aj),e(nr,Dj),e(nr,$e),e($e,Oj),e($e,Eg),e(Eg,Pj),e($e,Rj),e($e,wg),e(wg,Nj),e($e,Sj),e(S,xj),e(S,rr),e(rr,Vu),e(Vu,Ij),e(rr,Hj),e(rr,_e),e(_e,Bj),e(_e,Tg),e(Tg,Cj),e(_e,Gj),e(_e,jg),e(jg,Lj),e(_e,Uj),e(S,zj),e(S,or),e(or,Xu),e(Xu,Mj),e(or,Fj),e(or,ve),e(ve,Kj),e(ve,kg),e(kg,Jj),e(ve,Wj),e(ve,Ag),e(Ag,Yj),e(ve,Vj),e(S,Xj),e(S,lr),e(lr,Qu),e(Qu,Qj),e(lr,Zj),e(lr,oe),e(oe,e6),e(oe,Dg),e(Dg,t6),e(oe,s6),e(oe,Og),e(Og,a6),e(oe,n6),e(oe,Pg),e(Pg,r6),e(oe,o6),e(S,l6),e(S,ir),e(ir,Zu),e(Zu,i6),e(ir,u6),e(ir,ye),e(ye,c6),e(ye,Rg),e(Rg,f6),e(ye,p6),e(ye,Ng),e(Ng,h6),e(ye,d6),e(S,g6),e(S,ur),e(ur,ec),e(ec,m6),e(ur,q6),e(ur,Mt),e(Mt,$6),e(Mt,Sg),e(Sg,_6),e(Mt,v6),e(S,y6),e(S,cr),e(cr,tc),e(tc,b6),e(cr,E6),e(cr,Ft),e(Ft,w6),e(Ft,xg),e(xg,T6),e(Ft,j6),e(S,k6),e(S,fr),e(fr,sc),e(sc,Ig),e(Ig,A6),e(fr,D6),e(fr,ac),e(ac,O6),e(S,P6),e(S,pr),e(pr,nc),e(nc,R6),e(pr,N6),e(pr,Kt),e(Kt,S6),e(Kt,Hg),e(Hg,x6),e(Kt,I6),e(S,H6),e(S,hr),e(hr,rc),e(rc,B6),e(hr,C6),e(hr,Jt),e(Jt,G6),e(Jt,Bg),e(Bg,L6),e(Jt,U6),e(S,z6),e(S,dr),e(dr,oc),e(oc,M6),e(dr,F6),e(dr,Wt),e(Wt,K6),e(Wt,Cg),e(Cg,J6),e(Wt,W6),m(a,O1,g),m(a,lc,g),e(lc,Y6),m(a,P1,g),m(a,Yt,g),e(Yt,Gg),e(Gg,gr),e(gr,ic),e(ic,V6),e(gr,X6),e(gr,Lg),e(Yt,Q6),e(Yt,ie),e(ie,mr),e(mr,uc),e(uc,Ug),e(Ug,Z6),e(mr,ek),e(mr,cc),e(cc,tk),e(ie,sk),e(ie,qr),e(qr,fc),e(fc,zg),e(zg,ak),e(qr,nk),e(qr,pc),e(pc,rk),e(ie,ok),e(ie,$r),e($r,hc),e(hc,lk),e($r,ik),e($r,dc),e(dc,uk),e(ie,ck),e(ie,_r),e(_r,gc),e(gc,fk),e(_r,pk),e(_r,mc),e(mc,hk),m(a,R1,g),m(a,Ce,g),e(Ce,Vt),e(Vt,Mg),y(vr,Mg,null),e(Ce,dk),e(Ce,Fg),e(Fg,gk),m(a,N1,g),m(a,qc,g),e(qc,mk),m(a,S1,g),y(Xt,a,g),m(a,x1,g),m(a,yr,g),e(yr,qk),e(yr,br),e(br,$k),m(a,I1,g),m(a,$c,g),e($c,_k),m(a,H1,g),y(Qt,a,g),m(a,B1,g),m(a,_c,g),e(_c,vk),m(a,C1,g),m(a,Zt,g),e(Zt,Kg),e(Kg,Er),e(Er,vc),e(vc,yk),e(Er,bk),e(Er,Jg),e(Zt,Ek),e(Zt,K),e(K,wr),e(wr,Tr),e(Tr,Wg),e(Wg,wk),e(Tr,Tk),e(wr,jk),e(wr,Yg),e(K,kk),e(K,jr),e(jr,yc),e(yc,Ak),e(jr,Dk),e(jr,bc),e(bc,Ok),e(K,Pk),e(K,kr),e(kr,Ec),e(Ec,Rk),e(kr,Nk),e(kr,wc),e(wc,Sk),e(K,xk),e(K,Ar),e(Ar,Tc),e(Tc,Vg),e(Vg,Ik),e(Ar,Hk),e(Ar,jc),e(jc,Bk),e(K,Ck),e(K,Dr),e(Dr,kc),e(kc,Gk),e(Dr,Lk),e(Dr,es),e(es,Uk),e(es,Xg),e(Xg,zk),e(es,Mk),e(K,Fk),e(K,Or),e(Or,Ac),e(Ac,Kk),e(Or,Jk),e(Or,ts),e(ts,Wk),e(ts,Qg),e(Qg,Yk),e(ts,Vk),e(K,Xk),e(K,Pr),e(Pr,Dc),e(Dc,Qk),e(Pr,Zk),e(Pr,ss),e(ss,e7),e(ss,Zg),e(Zg,t7),e(ss,s7),m(a,G1,g),m(a,Oc,g),e(Oc,a7),m(a,L1,g),y(as,a,g),m(a,U1,g),m(a,ns,g),e(ns,em),e(em,Rr),e(Rr,Pc),e(Pc,n7),e(Rr,r7),e(Rr,tm),e(ns,o7),e(ns,ue),e(ue,Nr),e(Nr,Rc),e(Rc,sm),e(sm,l7),e(Nr,i7),e(Nr,Nc),e(Nc,u7),e(ue,c7),e(ue,Sr),e(Sr,Sc),e(Sc,am),e(am,f7),e(Sr,p7),e(Sr,xc),e(xc,h7),e(ue,d7),e(ue,xr),e(xr,Ic),e(Ic,nm),e(nm,g7),e(xr,m7),e(xr,Hc),e(Hc,q7),e(ue,$7),e(ue,Ir),e(Ir,Bc),e(Bc,rm),e(rm,_7),e(Ir,v7),e(Ir,Cc),e(Cc,y7),m(a,z1,g),m(a,Ge,g),e(Ge,rs),e(rs,om),y(Hr,om,null),e(Ge,b7),e(Ge,lm),e(lm,E7),m(a,M1,g),m(a,Gc,g),e(Gc,w7),m(a,F1,g),y(os,a,g),m(a,K1,g),m(a,Le,g),e(Le,T7),e(Le,Br),e(Br,j7),e(Le,k7),e(Le,Cr),e(Cr,A7),m(a,J1,g),m(a,Lc,g),e(Lc,D7),m(a,W1,g),y(ls,a,g),m(a,Y1,g),m(a,Uc,g),e(Uc,O7),m(a,V1,g),m(a,zc,g),e(zc,P7),m(a,X1,g),y(is,a,g),m(a,Q1,g),m(a,us,g),e(us,im),e(im,Gr),e(Gr,Mc),e(Mc,R7),e(Gr,N7),e(Gr,um),e(us,S7),e(us,ce),e(ce,Lr),e(Lr,Fc),e(Fc,cm),e(cm,x7),e(Lr,I7),e(Lr,Kc),e(Kc,H7),e(ce,B7),e(ce,Ur),e(Ur,Jc),e(Jc,fm),e(fm,C7),e(Ur,G7),e(Ur,Wc),e(Wc,L7),e(ce,U7),e(ce,zr),e(zr,Yc),e(Yc,pm),e(pm,z7),e(zr,M7),e(zr,cs),e(cs,F7),e(cs,hm),e(hm,K7),e(cs,J7),e(ce,W7),e(ce,Mr),e(Mr,Vc),e(Vc,dm),e(dm,Y7),e(Mr,V7),e(Mr,fs),e(fs,X7),e(fs,gm),e(gm,Q7),e(fs,Z7),m(a,Z1,g),m(a,Ue,g),e(Ue,ps),e(ps,mm),y(Fr,mm,null),e(Ue,e9),e(Ue,qm),e(qm,t9),m(a,e2,g),m(a,Xc,g),e(Xc,s9),m(a,t2,g),y(hs,a,g),m(a,s2,g),m(a,Kr,g),e(Kr,a9),e(Kr,Jr),e(Jr,n9),m(a,a2,g),m(a,Qc,g),e(Qc,r9),m(a,n2,g),y(ds,a,g),m(a,r2,g),m(a,Zc,g),e(Zc,o9),m(a,o2,g),m(a,gs,g),e(gs,$m),e($m,Wr),e(Wr,ef),e(ef,l9),e(Wr,i9),e(Wr,_m),e(gs,u9),e(gs,ee),e(ee,Yr),e(Yr,Vr),e(Vr,vm),e(vm,c9),e(Vr,f9),e(Yr,p9),e(Yr,tf),e(tf,h9),e(ee,d9),e(ee,Xr),e(Xr,sf),e(sf,ym),e(ym,g9),e(Xr,m9),e(Xr,af),e(af,q9),e(ee,$9),e(ee,Qr),e(Qr,nf),e(nf,_9),e(Qr,v9),e(Qr,ms),e(ms,y9),e(ms,bm),e(bm,b9),e(ms,E9),e(ee,w9),e(ee,Zr),e(Zr,rf),e(rf,T9),e(Zr,j9),e(Zr,qs),e(qs,k9),e(qs,Em),e(Em,A9),e(qs,D9),e(ee,O9),e(ee,eo),e(eo,of),e(of,P9),e(eo,R9),e(eo,$s),e($s,N9),e($s,wm),e(wm,S9),e($s,x9),m(a,l2,g),m(a,lf,g),e(lf,I9),m(a,i2,g),y(_s,a,g),m(a,u2,g),m(a,vs,g),e(vs,Tm),e(Tm,to),e(to,uf),e(uf,H9),e(to,B9),e(to,jm),e(vs,C9),e(vs,so),e(so,ao),e(ao,cf),e(cf,km),e(km,G9),e(ao,L9),e(ao,ff),e(ff,U9),e(so,z9),e(so,no),e(no,pf),e(pf,Am),e(Am,M9),e(no,F9),e(no,hf),e(hf,K9),m(a,c2,g),m(a,ze,g),e(ze,ys),e(ys,Dm),y(ro,Dm,null),e(ze,J9),e(ze,Om),e(Om,W9),m(a,f2,g),m(a,oo,g),e(oo,Y9),e(oo,df),e(df,V9),m(a,p2,g),m(a,Me,g),e(Me,bs),e(bs,Pm),y(lo,Pm,null),e(Me,X9),e(Me,Rm),e(Rm,Q9),m(a,h2,g),m(a,gf,g),e(gf,Z9),m(a,d2,g),y(Es,a,g),m(a,g2,g),m(a,Fe,g),e(Fe,e8),e(Fe,io),e(io,t8),e(Fe,s8),e(Fe,uo),e(uo,a8),m(a,m2,g),m(a,mf,g),e(mf,n8),m(a,q2,g),y(ws,a,g),m(a,$2,g),m(a,qf,g),e(qf,r8),m(a,_2,g),m(a,Ts,g),e(Ts,Nm),e(Nm,co),e(co,$f),e($f,o8),e(co,l8),e(co,Sm),e(Ts,i8),e(Ts,J),e(J,fo),e(fo,po),e(po,xm),e(xm,u8),e(po,c8),e(fo,f8),e(fo,_f),e(_f,p8),e(J,h8),e(J,ho),e(ho,vf),e(vf,Im),e(Im,d8),e(ho,g8),e(ho,yf),e(yf,m8),e(J,q8),e(J,go),e(go,bf),e(bf,$8),e(go,_8),e(go,x),e(x,v8),e(x,Hm),e(Hm,y8),e(x,b8),e(x,E8),e(x,w8),e(x,Bm),e(Bm,T8),e(x,j8),e(x,k8),e(x,A8),e(x,Cm),e(Cm,D8),e(x,O8),e(x,P8),e(x,R8),e(x,Gm),e(Gm,N8),e(x,S8),e(x,Lm),e(Lm,x8),e(x,I8),e(x,H8),e(x,B8),e(x,Um),e(Um,C8),e(x,G8),e(x,zm),e(zm,L8),e(x,U8),e(x,z8),e(x,M8),e(x,Mm),e(Mm,F8),e(x,K8),e(x,Fm),e(Fm,J8),e(x,W8),e(J,Y8),e(J,mo),e(mo,Ef),e(Ef,Km),e(Km,V8),e(mo,X8),e(mo,wf),e(wf,Q8),e(J,Z8),e(J,qo),e(qo,Tf),e(Tf,eA),e(qo,tA),e(qo,js),e(js,sA),e(js,Jm),e(Jm,aA),e(js,nA),e(J,rA),e(J,$o),e($o,jf),e(jf,oA),e($o,lA),e($o,ks),e(ks,iA),e(ks,Wm),e(Wm,uA),e(ks,cA),e(J,fA),e(J,_o),e(_o,kf),e(kf,pA),e(_o,hA),e(_o,As),e(As,dA),e(As,Ym),e(Ym,gA),e(As,mA),m(a,v2,g),m(a,Af,g),e(Af,qA),m(a,y2,g),y(Ds,a,g),m(a,b2,g),m(a,Os,g),e(Os,Vm),e(Vm,vo),e(vo,Df),e(Df,$A),e(vo,_A),e(vo,Xm),e(Os,vA),e(Os,te),e(te,yo),e(yo,Of),e(Of,Qm),e(Qm,yA),e(yo,bA),e(yo,Pf),e(Pf,EA),e(te,wA),e(te,bo),e(bo,Rf),e(Rf,Zm),e(Zm,TA),e(bo,jA),e(bo,Nf),e(Nf,kA),e(te,AA),e(te,Eo),e(Eo,Sf),e(Sf,eq),e(eq,DA),e(Eo,OA),e(Eo,xf),e(xf,PA),e(te,RA),e(te,wo),e(wo,If),e(If,tq),e(tq,NA),e(wo,SA),e(wo,Ps),e(Ps,xA),e(Ps,sq),e(sq,IA),e(Ps,HA),e(te,BA),e(te,To),e(To,Hf),e(Hf,aq),e(aq,CA),e(To,GA),e(To,Rs),e(Rs,LA),e(Rs,nq),e(nq,UA),e(Rs,zA),m(a,E2,g),m(a,Ke,g),e(Ke,Ns),e(Ns,rq),y(jo,rq,null),e(Ke,MA),e(Ke,oq),e(oq,FA),m(a,w2,g),m(a,Bf,g),e(Bf,KA),m(a,T2,g),y(Ss,a,g),m(a,j2,g),m(a,ko,g),e(ko,JA),e(ko,Ao),e(Ao,WA),m(a,k2,g),m(a,Cf,g),e(Cf,YA),m(a,A2,g),y(xs,a,g),m(a,D2,g),m(a,Gf,g),e(Gf,VA),m(a,O2,g),m(a,Is,g),e(Is,lq),e(lq,Do),e(Do,Lf),e(Lf,XA),e(Do,QA),e(Do,iq),e(Is,ZA),e(Is,I),e(I,Oo),e(Oo,Po),e(Po,uq),e(uq,eD),e(Po,tD),e(Oo,sD),e(Oo,Uf),e(Uf,aD),e(I,nD),e(I,Ro),e(Ro,zf),e(zf,cq),e(cq,rD),e(Ro,oD),e(Ro,Mf),e(Mf,lD),e(I,iD),e(I,No),e(No,Ff),e(Ff,uD),e(No,cD),e(No,be),e(be,fD),e(be,fq),e(fq,pD),e(be,hD),e(be,pq),e(pq,dD),e(be,gD),e(I,mD),e(I,So),e(So,Kf),e(Kf,qD),e(So,$D),e(So,le),e(le,_D),e(le,hq),e(hq,vD),e(le,yD),e(le,dq),e(dq,bD),e(le,ED),e(le,gq),e(gq,wD),e(le,TD),e(I,jD),e(I,xo),e(xo,Jf),e(Jf,kD),e(xo,AD),e(xo,Ee),e(Ee,DD),e(Ee,mq),e(mq,OD),e(Ee,PD),e(Ee,qq),e(qq,RD),e(Ee,ND),e(I,SD),e(I,Io),e(Io,Wf),e(Wf,xD),e(Io,ID),e(Io,Hs),e(Hs,HD),e(Hs,$q),e($q,BD),e(Hs,CD),e(I,GD),e(I,Ho),e(Ho,Yf),e(Yf,LD),e(Ho,UD),e(Ho,we),e(we,zD),e(we,_q),e(_q,MD),e(we,FD),e(we,vq),e(vq,KD),e(we,JD),e(I,WD),e(I,Bo),e(Bo,Vf),e(Vf,YD),e(Bo,VD),e(Bo,Te),e(Te,XD),e(Te,yq),e(yq,QD),e(Te,ZD),e(Te,bq),e(bq,eO),e(Te,tO),e(I,sO),e(I,Co),e(Co,Xf),e(Xf,aO),e(Co,nO),e(Co,je),e(je,rO),e(je,Eq),e(Eq,oO),e(je,lO),e(je,wq),e(wq,iO),e(je,uO),e(I,cO),e(I,Go),e(Go,Qf),e(Qf,fO),e(Go,pO),e(Go,Bs),e(Bs,hO),e(Bs,Tq),e(Tq,dO),e(Bs,gO),e(I,mO),e(I,Lo),e(Lo,Zf),e(Zf,qO),e(Lo,$O),e(Lo,Cs),e(Cs,_O),e(Cs,jq),e(jq,vO),e(Cs,yO),e(I,bO),e(I,Uo),e(Uo,ep),e(ep,kq),e(kq,EO),e(Uo,wO),e(Uo,tp),e(tp,TO),e(I,jO),e(I,zo),e(zo,sp),e(sp,kO),e(zo,AO),e(zo,Gs),e(Gs,DO),e(Gs,Aq),e(Aq,OO),e(Gs,PO),e(I,RO),e(I,Mo),e(Mo,ap),e(ap,NO),e(Mo,SO),e(Mo,Ls),e(Ls,xO),e(Ls,Dq),e(Dq,IO),e(Ls,HO),e(I,BO),e(I,Fo),e(Fo,np),e(np,CO),e(Fo,GO),e(Fo,Us),e(Us,LO),e(Us,Oq),e(Oq,UO),e(Us,zO),m(a,P2,g),m(a,rp,g),e(rp,MO),m(a,R2,g),y(zs,a,g),m(a,N2,g),m(a,Ms,g),e(Ms,Pq),e(Pq,Ko),e(Ko,op),e(op,FO),e(Ko,KO),e(Ko,Rq),e(Ms,JO),e(Ms,Nq),e(Nq,Jo),e(Jo,lp),e(lp,Sq),e(Sq,WO),e(Jo,YO),e(Jo,ip),e(ip,VO),m(a,S2,g),m(a,Je,g),e(Je,Fs),e(Fs,xq),y(Wo,xq,null),e(Je,XO),e(Je,Iq),e(Iq,QO),m(a,x2,g),m(a,Ks,g),e(Ks,ZO),e(Ks,up),e(up,eP),e(Ks,tP),m(a,I2,g),m(a,We,g),e(We,Js),e(Js,Hq),y(Yo,Hq,null),e(We,sP),e(We,Bq),e(Bq,aP),m(a,H2,g),m(a,cp,g),e(cp,nP),m(a,B2,g),y(Ws,a,g),m(a,C2,g),m(a,Vo,g),e(Vo,rP),e(Vo,Xo),e(Xo,oP),m(a,G2,g),m(a,fp,g),e(fp,lP),m(a,L2,g),y(Ys,a,g),m(a,U2,g),m(a,pp,g),e(pp,iP),m(a,z2,g),m(a,Vs,g),e(Vs,Cq),e(Cq,Qo),e(Qo,hp),e(hp,uP),e(Qo,cP),e(Qo,Gq),e(Vs,fP),e(Vs,se),e(se,Zo),e(Zo,el),e(el,Lq),e(Lq,pP),e(el,hP),e(Zo,dP),e(Zo,dp),e(dp,gP),e(se,mP),e(se,tl),e(tl,gp),e(gp,Uq),e(Uq,qP),e(tl,$P),e(tl,mp),e(mp,_P),e(se,vP),e(se,sl),e(sl,qp),e(qp,yP),e(sl,bP),e(sl,Xs),e(Xs,EP),e(Xs,zq),e(zq,wP),e(Xs,TP),e(se,jP),e(se,al),e(al,$p),e($p,kP),e(al,AP),e(al,Qs),e(Qs,DP),e(Qs,Mq),e(Mq,OP),e(Qs,PP),e(se,RP),e(se,nl),e(nl,_p),e(_p,NP),e(nl,SP),e(nl,Zs),e(Zs,xP),e(Zs,Fq),e(Fq,IP),e(Zs,HP),m(a,M2,g),m(a,vp,g),e(vp,BP),m(a,F2,g),y(ea,a,g),m(a,K2,g),m(a,ta,g),e(ta,Kq),e(Kq,rl),e(rl,yp),e(yp,CP),e(rl,GP),e(rl,Jq),e(ta,LP),e(ta,fe),e(fe,ol),e(ol,bp),e(bp,Wq),e(Wq,UP),e(ol,zP),e(ol,Ep),e(Ep,MP),e(fe,FP),e(fe,ll),e(ll,wp),e(wp,Yq),e(Yq,KP),e(ll,JP),e(ll,Tp),e(Tp,WP),e(fe,YP),e(fe,il),e(il,jp),e(jp,Vq),e(Vq,VP),e(il,XP),e(il,kp),e(kp,QP),e(fe,ZP),e(fe,ul),e(ul,Ap),e(Ap,Xq),e(Xq,eR),e(ul,tR),e(ul,Dp),e(Dp,sR),m(a,J2,g),m(a,Ye,g),e(Ye,sa),e(sa,Qq),y(cl,Qq,null),e(Ye,aR),e(Ye,Zq),e(Zq,nR),m(a,W2,g),m(a,Op,g),e(Op,rR),m(a,Y2,g),y(aa,a,g),m(a,V2,g),y(na,a,g),m(a,X2,g),m(a,pe,g),e(pe,oR),e(pe,fl),e(fl,lR),e(pe,iR),e(pe,pl),e(pl,uR),e(pe,cR),e(pe,hl),e(hl,fR),m(a,Q2,g),m(a,Pp,g),e(Pp,pR),m(a,Z2,g),y(ra,a,g),m(a,ev,g),m(a,Rp,g),e(Rp,hR),m(a,tv,g),m(a,oa,g),e(oa,e$),e(e$,dl),e(dl,Np),e(Np,dR),e(dl,gR),e(dl,t$),e(oa,mR),e(oa,s$),e(s$,gl),e(gl,ml),e(ml,a$),e(a$,qR),e(ml,$R),e(gl,_R),e(gl,Sp),e(Sp,vR),m(a,sv,g),m(a,xp,g),e(xp,yR),m(a,av,g),m(a,Ip,g),e(Ip,bR),m(a,nv,g),y(la,a,g),m(a,rv,g),m(a,ia,g),e(ia,n$),e(n$,ql),e(ql,Hp),e(Hp,ER),e(ql,wR),e(ql,r$),e(ia,TR),e(ia,o$),e(o$,$l),e($l,Bp),e(Bp,l$),e(l$,jR),e($l,kR),e($l,Cp),e(Cp,AR),m(a,ov,g),m(a,Ve,g),e(Ve,ua),e(ua,i$),y(_l,i$,null),e(Ve,DR),e(Ve,u$),e(u$,OR),m(a,lv,g),m(a,Gp,g),e(Gp,PR),m(a,iv,g),y(ca,a,g),m(a,uv,g),m(a,Xe,g),e(Xe,RR),e(Xe,vl),e(vl,NR),e(Xe,SR),e(Xe,yl),e(yl,xR),m(a,cv,g),m(a,Lp,g),e(Lp,IR),m(a,fv,g),m(a,fa,g),e(fa,c$),e(c$,bl),e(bl,Up),e(Up,HR),e(bl,BR),e(bl,f$),e(fa,CR),e(fa,ae),e(ae,El),e(El,wl),e(wl,p$),e(p$,GR),e(wl,LR),e(El,UR),e(El,zp),e(zp,zR),e(ae,MR),e(ae,Tl),e(Tl,Mp),e(Mp,h$),e(h$,FR),e(Tl,KR),e(Tl,Fp),e(Fp,JR),e(ae,WR),e(ae,jl),e(jl,Kp),e(Kp,YR),e(jl,VR),e(jl,pa),e(pa,XR),e(pa,d$),e(d$,QR),e(pa,ZR),e(ae,eN),e(ae,kl),e(kl,Jp),e(Jp,tN),e(kl,sN),e(kl,ha),e(ha,aN),e(ha,g$),e(g$,nN),e(ha,rN),e(ae,oN),e(ae,Al),e(Al,Wp),e(Wp,lN),e(Al,iN),e(Al,da),e(da,uN),e(da,m$),e(m$,cN),e(da,fN),m(a,pv,g),m(a,Yp,g),e(Yp,pN),m(a,hv,g),m(a,ga,g),e(ga,q$),e(q$,Dl),e(Dl,Vp),e(Vp,hN),e(Dl,dN),e(Dl,$$),e(ga,gN),e(ga,_$),e(_$,Ol),e(Ol,Xp),e(Xp,v$),e(v$,mN),e(Ol,qN),e(Ol,Qp),e(Qp,$N),m(a,dv,g),m(a,Zp,g),e(Zp,_N),m(a,gv,g),m(a,Qe,g),e(Qe,ma),e(ma,y$),y(Pl,y$,null),e(Qe,vN),e(Qe,b$),e(b$,yN),m(a,mv,g),m(a,eh,g),e(eh,bN),m(a,qv,g),y(qa,a,g),m(a,$v,g),m(a,Ze,g),e(Ze,EN),e(Ze,Rl),e(Rl,wN),e(Ze,TN),e(Ze,Nl),e(Nl,jN),m(a,_v,g),m(a,th,g),e(th,kN),m(a,vv,g),y($a,a,g),m(a,yv,g),m(a,sh,g),e(sh,AN),m(a,bv,g),m(a,_a,g),e(_a,E$),e(E$,Sl),e(Sl,ah),e(ah,DN),e(Sl,ON),e(Sl,w$),e(_a,PN),e(_a,T$),e(T$,xl),e(xl,Il),e(Il,j$),e(j$,RN),e(Il,NN),e(xl,SN),e(xl,nh),e(nh,xN),m(a,Ev,g),m(a,rh,g),e(rh,IN),m(a,wv,g),y(va,a,g),m(a,Tv,g),m(a,ya,g),e(ya,k$),e(k$,Hl),e(Hl,oh),e(oh,HN),e(Hl,BN),e(Hl,A$),e(ya,CN),e(ya,Bl),e(Bl,Cl),e(Cl,lh),e(lh,D$),e(D$,GN),e(Cl,LN),e(Cl,ih),e(ih,UN),e(Bl,zN),e(Bl,Gl),e(Gl,uh),e(uh,O$),e(O$,MN),e(Gl,FN),e(Gl,ch),e(ch,KN),m(a,jv,g),m(a,et,g),e(et,ba),e(ba,P$),y(Ll,P$,null),e(et,JN),e(et,R$),e(R$,WN),m(a,kv,g),m(a,fh,g),e(fh,YN),m(a,Av,g),y(Ea,a,g),m(a,Dv,g),m(a,Ul,g),e(Ul,VN),e(Ul,zl),e(zl,XN),m(a,Ov,g),m(a,ph,g),e(ph,QN),m(a,Pv,g),y(wa,a,g),m(a,Rv,g),m(a,Ta,g),e(Ta,ZN),e(Ta,Ml),e(Ml,eS),e(Ta,tS),m(a,Nv,g),m(a,ja,g),e(ja,N$),e(N$,Fl),e(Fl,hh),e(hh,sS),e(Fl,aS),e(Fl,S$),e(ja,nS),e(ja,x$),e(x$,Kl),e(Kl,Jl),e(Jl,I$),e(I$,rS),e(Jl,oS),e(Kl,lS),e(Kl,dh),e(dh,iS),m(a,Sv,g),m(a,gh,g),e(gh,uS),m(a,xv,g),y(ka,a,g),m(a,Iv,g),m(a,Aa,g),e(Aa,H$),e(H$,Wl),e(Wl,mh),e(mh,cS),e(Wl,fS),e(Wl,B$),e(Aa,pS),e(Aa,tt),e(tt,Yl),e(Yl,qh),e(qh,C$),e(C$,hS),e(Yl,dS),e(Yl,$h),e($h,gS),e(tt,mS),e(tt,Vl),e(Vl,_h),e(_h,G$),e(G$,qS),e(Vl,$S),e(Vl,vh),e(vh,_S),e(tt,vS),e(tt,Xl),e(Xl,yh),e(yh,L$),e(L$,yS),e(Xl,bS),e(Xl,bh),e(bh,ES),m(a,Hv,g),m(a,st,g),e(st,Da),e(Da,U$),y(Ql,U$,null),e(st,wS),e(st,z$),e(z$,TS),m(a,Bv,g),m(a,Eh,g),e(Eh,jS),m(a,Cv,g),y(Oa,a,g),m(a,Gv,g),m(a,Zl,g),e(Zl,kS),e(Zl,ei),e(ei,AS),m(a,Lv,g),m(a,wh,g),e(wh,DS),m(a,Uv,g),y(Pa,a,g),m(a,zv,g),m(a,Ra,g),e(Ra,OS),e(Ra,ti),e(ti,PS),e(Ra,RS),m(a,Mv,g),m(a,Na,g),e(Na,M$),e(M$,si),e(si,Th),e(Th,NS),e(si,SS),e(si,F$),e(Na,xS),e(Na,K$),e(K$,ai),e(ai,ni),e(ni,J$),e(J$,IS),e(ni,HS),e(ai,BS),e(ai,jh),e(jh,CS),m(a,Fv,g),m(a,kh,g),e(kh,GS),m(a,Kv,g),y(Sa,a,g),m(a,Jv,g),m(a,xa,g),e(xa,W$),e(W$,ri),e(ri,Ah),e(Ah,LS),e(ri,US),e(ri,Y$),e(xa,zS),e(xa,at),e(at,oi),e(oi,Dh),e(Dh,V$),e(V$,MS),e(oi,FS),e(oi,Oh),e(Oh,KS),e(at,JS),e(at,li),e(li,Ph),e(Ph,X$),e(X$,WS),e(li,YS),e(li,Rh),e(Rh,VS),e(at,XS),e(at,ii),e(ii,Nh),e(Nh,Q$),e(Q$,QS),e(ii,ZS),e(ii,Sh),e(Sh,ex),m(a,Wv,g),m(a,nt,g),e(nt,Ia),e(Ia,Z$),y(ui,Z$,null),e(nt,tx),e(nt,e_),e(e_,sx),m(a,Yv,g),m(a,xh,g),e(xh,ax),m(a,Vv,g),y(Ha,a,g),m(a,Xv,g),m(a,ci,g),e(ci,nx),e(ci,fi),e(fi,rx),m(a,Qv,g),m(a,Ih,g),e(Ih,ox),m(a,Zv,g),y(Ba,a,g),m(a,e0,g),m(a,Ca,g),e(Ca,lx),e(Ca,pi),e(pi,ix),e(Ca,ux),m(a,t0,g),m(a,Ga,g),e(Ga,t_),e(t_,hi),e(hi,Hh),e(Hh,cx),e(hi,fx),e(hi,s_),e(Ga,px),e(Ga,a_),e(a_,di),e(di,gi),e(gi,n_),e(n_,hx),e(gi,dx),e(di,gx),e(di,Bh),e(Bh,mx),m(a,s0,g),m(a,Ch,g),e(Ch,qx),m(a,a0,g),y(La,a,g),m(a,n0,g),m(a,Ua,g),e(Ua,r_),e(r_,mi),e(mi,Gh),e(Gh,$x),e(mi,_x),e(mi,o_),e(Ua,vx),e(Ua,qi),e(qi,$i),e($i,Lh),e(Lh,l_),e(l_,yx),e($i,bx),e($i,Uh),e(Uh,Ex),e(qi,wx),e(qi,_i),e(_i,zh),e(zh,i_),e(i_,Tx),e(_i,jx),e(_i,Mh),e(Mh,kx),r0=!0},p(a,[g]){const vi={};g&2&&(vi.$$scope={dirty:g,ctx:a}),ct.$set(vi);const u_={};g&2&&(u_.$$scope={dirty:g,ctx:a}),ft.$set(u_);const c_={};g&2&&(c_.$$scope={dirty:g,ctx:a}),qt.$set(c_);const f_={};g&2&&(f_.$$scope={dirty:g,ctx:a}),yt.$set(f_);const yi={};g&2&&(yi.$$scope={dirty:g,ctx:a}),bt.$set(yi);const p_={};g&2&&(p_.$$scope={dirty:g,ctx:a}),Ot.$set(p_);const h_={};g&2&&(h_.$$scope={dirty:g,ctx:a}),Pt.$set(h_);const d_={};g&2&&(d_.$$scope={dirty:g,ctx:a}),Gt.$set(d_);const g_={};g&2&&(g_.$$scope={dirty:g,ctx:a}),Lt.$set(g_);const m_={};g&2&&(m_.$$scope={dirty:g,ctx:a}),Xt.$set(m_);const bi={};g&2&&(bi.$$scope={dirty:g,ctx:a}),Qt.$set(bi);const q_={};g&2&&(q_.$$scope={dirty:g,ctx:a}),as.$set(q_);const $_={};g&2&&($_.$$scope={dirty:g,ctx:a}),os.$set($_);const __={};g&2&&(__.$$scope={dirty:g,ctx:a}),ls.$set(__);const v_={};g&2&&(v_.$$scope={dirty:g,ctx:a}),is.$set(v_);const Fh={};g&2&&(Fh.$$scope={dirty:g,ctx:a}),hs.$set(Fh);const y_={};g&2&&(y_.$$scope={dirty:g,ctx:a}),ds.$set(y_);const b_={};g&2&&(b_.$$scope={dirty:g,ctx:a}),_s.$set(b_);const E_={};g&2&&(E_.$$scope={dirty:g,ctx:a}),Es.$set(E_);const Ei={};g&2&&(Ei.$$scope={dirty:g,ctx:a}),ws.$set(Ei);const w_={};g&2&&(w_.$$scope={dirty:g,ctx:a}),Ds.$set(w_);const wi={};g&2&&(wi.$$scope={dirty:g,ctx:a}),Ss.$set(wi);const T_={};g&2&&(T_.$$scope={dirty:g,ctx:a}),xs.$set(T_);const F={};g&2&&(F.$$scope={dirty:g,ctx:a}),zs.$set(F);const Ti={};g&2&&(Ti.$$scope={dirty:g,ctx:a}),Ws.$set(Ti);const Kh={};g&2&&(Kh.$$scope={dirty:g,ctx:a}),Ys.$set(Kh);const j_={};g&2&&(j_.$$scope={dirty:g,ctx:a}),ea.$set(j_);const k_={};g&2&&(k_.$$scope={dirty:g,ctx:a}),aa.$set(k_);const ji={};g&2&&(ji.$$scope={dirty:g,ctx:a}),na.$set(ji);const Jh={};g&2&&(Jh.$$scope={dirty:g,ctx:a}),ra.$set(Jh);const A_={};g&2&&(A_.$$scope={dirty:g,ctx:a}),la.$set(A_);const D_={};g&2&&(D_.$$scope={dirty:g,ctx:a}),ca.$set(D_);const ki={};g&2&&(ki.$$scope={dirty:g,ctx:a}),qa.$set(ki);const O_={};g&2&&(O_.$$scope={dirty:g,ctx:a}),$a.$set(O_);const rt={};g&2&&(rt.$$scope={dirty:g,ctx:a}),va.$set(rt);const P_={};g&2&&(P_.$$scope={dirty:g,ctx:a}),Ea.$set(P_);const R_={};g&2&&(R_.$$scope={dirty:g,ctx:a}),wa.$set(R_);const Ai={};g&2&&(Ai.$$scope={dirty:g,ctx:a}),ka.$set(Ai);const N_={};g&2&&(N_.$$scope={dirty:g,ctx:a}),Oa.$set(N_);const Di={};g&2&&(Di.$$scope={dirty:g,ctx:a}),Pa.$set(Di);const S_={};g&2&&(S_.$$scope={dirty:g,ctx:a}),Sa.$set(S_);const Oi={};g&2&&(Oi.$$scope={dirty:g,ctx:a}),Ha.$set(Oi);const x_={};g&2&&(x_.$$scope={dirty:g,ctx:a}),Ba.$set(x_);const I_={};g&2&&(I_.$$scope={dirty:g,ctx:a}),La.$set(I_)},i(a){r0||(b(k.$$.fragment,a),b(Q.$$.fragment,a),b(Xa.$$.fragment,a),b(ct.$$.fragment,a),b(ft.$$.fragment,a),b(qt.$$.fragment,a),b(mn.$$.fragment,a),b(yt.$$.fragment,a),b(bt.$$.fragment,a),b(An.$$.fragment,a),b(Ot.$$.fragment,a),b(Pt.$$.fragment,a),b(Wn.$$.fragment,a),b(Gt.$$.fragment,a),b(Lt.$$.fragment,a),b(vr.$$.fragment,a),b(Xt.$$.fragment,a),b(Qt.$$.fragment,a),b(as.$$.fragment,a),b(Hr.$$.fragment,a),b(os.$$.fragment,a),b(ls.$$.fragment,a),b(is.$$.fragment,a),b(Fr.$$.fragment,a),b(hs.$$.fragment,a),b(ds.$$.fragment,a),b(_s.$$.fragment,a),b(ro.$$.fragment,a),b(lo.$$.fragment,a),b(Es.$$.fragment,a),b(ws.$$.fragment,a),b(Ds.$$.fragment,a),b(jo.$$.fragment,a),b(Ss.$$.fragment,a),b(xs.$$.fragment,a),b(zs.$$.fragment,a),b(Wo.$$.fragment,a),b(Yo.$$.fragment,a),b(Ws.$$.fragment,a),b(Ys.$$.fragment,a),b(ea.$$.fragment,a),b(cl.$$.fragment,a),b(aa.$$.fragment,a),b(na.$$.fragment,a),b(ra.$$.fragment,a),b(la.$$.fragment,a),b(_l.$$.fragment,a),b(ca.$$.fragment,a),b(Pl.$$.fragment,a),b(qa.$$.fragment,a),b($a.$$.fragment,a),b(va.$$.fragment,a),b(Ll.$$.fragment,a),b(Ea.$$.fragment,a),b(wa.$$.fragment,a),b(ka.$$.fragment,a),b(Ql.$$.fragment,a),b(Oa.$$.fragment,a),b(Pa.$$.fragment,a),b(Sa.$$.fragment,a),b(ui.$$.fragment,a),b(Ha.$$.fragment,a),b(Ba.$$.fragment,a),b(La.$$.fragment,a),r0=!0)},o(a){E(k.$$.fragment,a),E(Q.$$.fragment,a),E(Xa.$$.fragment,a),E(ct.$$.fragment,a),E(ft.$$.fragment,a),E(qt.$$.fragment,a),E(mn.$$.fragment,a),E(yt.$$.fragment,a),E(bt.$$.fragment,a),E(An.$$.fragment,a),E(Ot.$$.fragment,a),E(Pt.$$.fragment,a),E(Wn.$$.fragment,a),E(Gt.$$.fragment,a),E(Lt.$$.fragment,a),E(vr.$$.fragment,a),E(Xt.$$.fragment,a),E(Qt.$$.fragment,a),E(as.$$.fragment,a),E(Hr.$$.fragment,a),E(os.$$.fragment,a),E(ls.$$.fragment,a),E(is.$$.fragment,a),E(Fr.$$.fragment,a),E(hs.$$.fragment,a),E(ds.$$.fragment,a),E(_s.$$.fragment,a),E(ro.$$.fragment,a),E(lo.$$.fragment,a),E(Es.$$.fragment,a),E(ws.$$.fragment,a),E(Ds.$$.fragment,a),E(jo.$$.fragment,a),E(Ss.$$.fragment,a),E(xs.$$.fragment,a),E(zs.$$.fragment,a),E(Wo.$$.fragment,a),E(Yo.$$.fragment,a),E(Ws.$$.fragment,a),E(Ys.$$.fragment,a),E(ea.$$.fragment,a),E(cl.$$.fragment,a),E(aa.$$.fragment,a),E(na.$$.fragment,a),E(ra.$$.fragment,a),E(la.$$.fragment,a),E(_l.$$.fragment,a),E(ca.$$.fragment,a),E(Pl.$$.fragment,a),E(qa.$$.fragment,a),E($a.$$.fragment,a),E(va.$$.fragment,a),E(Ll.$$.fragment,a),E(Ea.$$.fragment,a),E(wa.$$.fragment,a),E(ka.$$.fragment,a),E(Ql.$$.fragment,a),E(Oa.$$.fragment,a),E(Pa.$$.fragment,a),E(Sa.$$.fragment,a),E(ui.$$.fragment,a),E(Ha.$$.fragment,a),E(Ba.$$.fragment,a),E(La.$$.fragment,a),r0=!1},d(a){s(n),a&&s(c),a&&s(t),w(k),a&&s(N),a&&s(D),w(Q),a&&s(Va),a&&s(Ne),a&&s(L_),a&&s(Ni),a&&s(U_),a&&s(lt),a&&s(z_),a&&s(it),a&&s(M_),a&&s(Se),w(Xa),a&&s(F_),a&&s(Si),a&&s(K_),w(ct,a),a&&s(J_),a&&s(Qa),a&&s(W_),a&&s(xi),a&&s(Y_),w(ft,a),a&&s(V_),a&&s(Ii),a&&s(X_),a&&s(pt),a&&s(Q_),a&&s(Ji),a&&s(Z_),a&&s(Wi),a&&s(e1),w(qt,a),a&&s(t1),a&&s($t),a&&s(s1),a&&s(Ie),w(mn),a&&s(a1),a&&s(tu),a&&s(n1),w(yt,a),a&&s(r1),a&&s(qn),a&&s(o1),a&&s(su),a&&s(l1),w(bt,a),a&&s(i1),a&&s(au),a&&s(u1),a&&s(Et),a&&s(c1),a&&s(fu),a&&s(f1),a&&s(kt),a&&s(p1),a&&s(He),w(An),a&&s(h1),a&&s(Dt),a&&s(d1),w(Ot,a),a&&s(g1),a&&s(Dn),a&&s(m1),a&&s(mu),a&&s(q1),w(Pt,a),a&&s($1),a&&s(qu),a&&s(_1),a&&s(Rt),a&&s(v1),a&&s(Su),a&&s(y1),a&&s(Bt),a&&s(b1),a&&s(Be),w(Wn),a&&s(E1),a&&s(Bu),a&&s(w1),w(Gt,a),a&&s(T1),a&&s(Yn),a&&s(j1),a&&s(Cu),a&&s(k1),w(Lt,a),a&&s(A1),a&&s(Gu),a&&s(D1),a&&s(Ut),a&&s(O1),a&&s(lc),a&&s(P1),a&&s(Yt),a&&s(R1),a&&s(Ce),w(vr),a&&s(N1),a&&s(qc),a&&s(S1),w(Xt,a),a&&s(x1),a&&s(yr),a&&s(I1),a&&s($c),a&&s(H1),w(Qt,a),a&&s(B1),a&&s(_c),a&&s(C1),a&&s(Zt),a&&s(G1),a&&s(Oc),a&&s(L1),w(as,a),a&&s(U1),a&&s(ns),a&&s(z1),a&&s(Ge),w(Hr),a&&s(M1),a&&s(Gc),a&&s(F1),w(os,a),a&&s(K1),a&&s(Le),a&&s(J1),a&&s(Lc),a&&s(W1),w(ls,a),a&&s(Y1),a&&s(Uc),a&&s(V1),a&&s(zc),a&&s(X1),w(is,a),a&&s(Q1),a&&s(us),a&&s(Z1),a&&s(Ue),w(Fr),a&&s(e2),a&&s(Xc),a&&s(t2),w(hs,a),a&&s(s2),a&&s(Kr),a&&s(a2),a&&s(Qc),a&&s(n2),w(ds,a),a&&s(r2),a&&s(Zc),a&&s(o2),a&&s(gs),a&&s(l2),a&&s(lf),a&&s(i2),w(_s,a),a&&s(u2),a&&s(vs),a&&s(c2),a&&s(ze),w(ro),a&&s(f2),a&&s(oo),a&&s(p2),a&&s(Me),w(lo),a&&s(h2),a&&s(gf),a&&s(d2),w(Es,a),a&&s(g2),a&&s(Fe),a&&s(m2),a&&s(mf),a&&s(q2),w(ws,a),a&&s($2),a&&s(qf),a&&s(_2),a&&s(Ts),a&&s(v2),a&&s(Af),a&&s(y2),w(Ds,a),a&&s(b2),a&&s(Os),a&&s(E2),a&&s(Ke),w(jo),a&&s(w2),a&&s(Bf),a&&s(T2),w(Ss,a),a&&s(j2),a&&s(ko),a&&s(k2),a&&s(Cf),a&&s(A2),w(xs,a),a&&s(D2),a&&s(Gf),a&&s(O2),a&&s(Is),a&&s(P2),a&&s(rp),a&&s(R2),w(zs,a),a&&s(N2),a&&s(Ms),a&&s(S2),a&&s(Je),w(Wo),a&&s(x2),a&&s(Ks),a&&s(I2),a&&s(We),w(Yo),a&&s(H2),a&&s(cp),a&&s(B2),w(Ws,a),a&&s(C2),a&&s(Vo),a&&s(G2),a&&s(fp),a&&s(L2),w(Ys,a),a&&s(U2),a&&s(pp),a&&s(z2),a&&s(Vs),a&&s(M2),a&&s(vp),a&&s(F2),w(ea,a),a&&s(K2),a&&s(ta),a&&s(J2),a&&s(Ye),w(cl),a&&s(W2),a&&s(Op),a&&s(Y2),w(aa,a),a&&s(V2),w(na,a),a&&s(X2),a&&s(pe),a&&s(Q2),a&&s(Pp),a&&s(Z2),w(ra,a),a&&s(ev),a&&s(Rp),a&&s(tv),a&&s(oa),a&&s(sv),a&&s(xp),a&&s(av),a&&s(Ip),a&&s(nv),w(la,a),a&&s(rv),a&&s(ia),a&&s(ov),a&&s(Ve),w(_l),a&&s(lv),a&&s(Gp),a&&s(iv),w(ca,a),a&&s(uv),a&&s(Xe),a&&s(cv),a&&s(Lp),a&&s(fv),a&&s(fa),a&&s(pv),a&&s(Yp),a&&s(hv),a&&s(ga),a&&s(dv),a&&s(Zp),a&&s(gv),a&&s(Qe),w(Pl),a&&s(mv),a&&s(eh),a&&s(qv),w(qa,a),a&&s($v),a&&s(Ze),a&&s(_v),a&&s(th),a&&s(vv),w($a,a),a&&s(yv),a&&s(sh),a&&s(bv),a&&s(_a),a&&s(Ev),a&&s(rh),a&&s(wv),w(va,a),a&&s(Tv),a&&s(ya),a&&s(jv),a&&s(et),w(Ll),a&&s(kv),a&&s(fh),a&&s(Av),w(Ea,a),a&&s(Dv),a&&s(Ul),a&&s(Ov),a&&s(ph),a&&s(Pv),w(wa,a),a&&s(Rv),a&&s(Ta),a&&s(Nv),a&&s(ja),a&&s(Sv),a&&s(gh),a&&s(xv),w(ka,a),a&&s(Iv),a&&s(Aa),a&&s(Hv),a&&s(st),w(Ql),a&&s(Bv),a&&s(Eh),a&&s(Cv),w(Oa,a),a&&s(Gv),a&&s(Zl),a&&s(Lv),a&&s(wh),a&&s(Uv),w(Pa,a),a&&s(zv),a&&s(Ra),a&&s(Mv),a&&s(Na),a&&s(Fv),a&&s(kh),a&&s(Kv),w(Sa,a),a&&s(Jv),a&&s(xa),a&&s(Wv),a&&s(nt),w(ui),a&&s(Yv),a&&s(xh),a&&s(Vv),w(Ha,a),a&&s(Xv),a&&s(ci),a&&s(Qv),a&&s(Ih),a&&s(Zv),w(Ba,a),a&&s(e0),a&&s(Ca),a&&s(t0),a&&s(Ga),a&&s(s0),a&&s(Ch),a&&s(a0),w(La,a),a&&s(n0),a&&s(Ua)}}}const OW={local:"detailed-parameters",sections:[{local:"which-task-is-used-by-this-model",title:"Which task is used by this model ?"},{local:"zeroshot-classification-task",title:"Zero-shot classification task"},{local:"translation-task",title:"Translation task"},{local:"summarization-task",title:"Summarization task"},{local:"conversational-task",title:"Conversational task"},{local:"table-question-answering-task",title:"Table question answering task"},{local:"question-answering-task",title:"Question answering task"},{local:"textclassification-task",title:"Text-classification task"},{local:"named-entity-recognition-ner-task",title:"Named Entity Recognition (NER) task"},{local:"tokenclassification-task",title:"Token-classification task"},{local:"textgeneration-task",title:"Text-generation task"},{local:"text2textgeneration-task",title:"Text2text-generation task"},{local:"fill-mask-task",title:"Fill mask task"},{local:"automatic-speech-recognition-task",title:"Automatic speech recognition task"},{local:"featureextraction-task",title:"Feature-extraction task"},{local:"audioclassification-task",title:"Audio-classification task"},{local:"objectdetection-task",title:"Object-detection task"},{local:"image-segmentation-task",title:"Image Segmentation task"},{local:"image-classification-task",title:"Image Classification task"}],title:"Detailed parameters"};function PW($){return rK(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class IW extends tK{constructor(n){super();sK(this,n,PW,DW,aK,{})}}export{IW as default,OW as metadata};
