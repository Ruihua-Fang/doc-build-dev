import{S as HF,i as BF,s as CF,e as n,k as p,w as _,t as i,M as GF,c as o,d as t,m as h,a as l,x as v,h as u,b as f,N as IF,F as e,g as m,y as E,q as y,o as w,B as b,v as LF,L as O}from"../chunks/vendor-7c454903.js";import{T as J}from"../chunks/Tip-735285fc.js";import{I as z}from"../chunks/IconCopyLink-5457534b.js";import{I as L,M as P,C as R}from"../chunks/InferenceApi-041dc1b2.js";function UF(q){let r,c,s,d,$,k,A;return{c(){r=n("p"),c=n("strong"),s=i("Recommended model"),d=i(`:
`),$=n("a"),k=i("bert-base-uncased"),A=i(" (it\u2019s a simple model, but fun to play with)."),this.h()},l(j){r=o(j,"P",{});var T=l(r);c=o(T,"STRONG",{});var S=l(c);s=u(S,"Recommended model"),S.forEach(t),d=u(T,`:
`),$=o(T,"A",{href:!0,rel:!0});var D=l($);k=u(D,"bert-base-uncased"),D.forEach(t),A=u(T," (it\u2019s a simple model, but fun to play with)."),T.forEach(t),this.h()},h(){f($,"href","https://huggingface.co/bert-base-uncased"),f($,"rel","nofollow")},m(j,T){m(j,r,T),e(r,c),e(c,s),e(r,d),e(r,$),e($,k),e(r,A)},d(j){j&&t(r)}}}function zF(q){let r,c;return r=new R({props:{code:`import json
import requests
headers = {"Authorization": f"Bearer {API_TOKEN}"}
API_URL = "https://api-inference.huggingface.co/models/bert-base-uncased"
def query(payload):
    data = json.dumps(payload)
    response = requests.request("POST", API_URL, headers=headers, data=data)
    return json.loads(response.content.decode("utf-8"))
data = query({"inputs": "The answer to the universe is [MASK]."})`,highlighted:`<span class="hljs-keyword">import</span> json
<span class="hljs-keyword">import</span> requests
headers = {<span class="hljs-string">&quot;Authorization&quot;</span>: <span class="hljs-string">f&quot;Bearer <span class="hljs-subst">{API_TOKEN}</span>&quot;</span>}
API_URL = <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/bert-base-uncased&quot;</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">payload</span>):
    data = json.dumps(payload)
    response = requests.request(<span class="hljs-string">&quot;POST&quot;</span>, API_URL, headers=headers, data=data)
    <span class="hljs-keyword">return</span> json.loads(response.content.decode(<span class="hljs-string">&quot;utf-8&quot;</span>))
data = query({<span class="hljs-string">&quot;inputs&quot;</span>: <span class="hljs-string">&quot;The answer to the universe is [MASK].&quot;</span>})`}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p:O,i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function MF(q){let r,c;return r=new P({props:{$$slots:{default:[zF]},$$scope:{ctx:q}}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),r.$set($)},i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function KF(q){let r,c;return r=new R({props:{code:`import fetch from "node-fetch";
async function query(data) {
    const response = await fetch(
        "https://api-inference.huggingface.co/models/bert-base-uncased",
        {
            headers: { Authorization: \`Bearer \${API_TOKEN}\` },
            method: "POST",
            body: JSON.stringify(data),
        }
    );
    const result = await response.json();
    return result;
}
query({inputs:"The answer to the universe is [MASK]."}).then((response) => {
    console.log(JSON.stringify(response));
});
// [{"sequence":"the answer to the universe is no.","score":0.16963955760002136,"token":2053,"token_str":"no"},{"sequence":"the answer to the universe is nothing.","score":0.07344776391983032,"token":2498,"token_str":"nothing"},{"sequence":"the answer to the universe is yes.","score":0.05803241208195686,"token":2748,"token_str":"yes"},{"sequence":"the answer to the universe is unknown.","score":0.043957844376564026,"token":4242,"token_str":"unknown"},{"sequence":"the answer to the universe is simple.","score":0.04015745222568512,"token":3722,"token_str":"simple"}]`,highlighted:`<span class="hljs-keyword">import</span> <span class="hljs-keyword">fetch</span> <span class="hljs-keyword">from</span> &quot;node-fetch&quot;;
async <span class="hljs-keyword">function</span> query(data) {
    const response = await <span class="hljs-keyword">fetch</span>(
        &quot;https://api-inference.huggingface.co/models/bert-base-uncased&quot;,
        {
            headers: { <span class="hljs-keyword">Authorization</span>: \`Bearer \${API_TOKEN}\` },
            <span class="hljs-keyword">method</span>: &quot;POST&quot;,
            body: <span class="hljs-type">JSON</span>.stringify(data),
        }
    );
    const result = await response.json();
    <span class="hljs-keyword">return</span> result;
}
query({inputs:&quot;The answer to the universe is [MASK].&quot;}).<span class="hljs-keyword">then</span>((response) =&gt; {
    console.log(<span class="hljs-type">JSON</span>.stringify(response));
});
// [{&quot;sequence&quot;:&quot;the answer to the universe is no.&quot;,&quot;score&quot;:<span class="hljs-number">0.16963955760002136</span>,&quot;token&quot;:<span class="hljs-number">2053</span>,&quot;token_str&quot;:&quot;no&quot;},{&quot;sequence&quot;:&quot;the answer to the universe is nothing.&quot;,&quot;score&quot;:<span class="hljs-number">0.07344776391983032</span>,&quot;token&quot;:<span class="hljs-number">2498</span>,&quot;token_str&quot;:&quot;nothing&quot;},{&quot;sequence&quot;:&quot;the answer to the universe is yes.&quot;,&quot;score&quot;:<span class="hljs-number">0.05803241208195686</span>,&quot;token&quot;:<span class="hljs-number">2748</span>,&quot;token_str&quot;:&quot;yes&quot;},{&quot;sequence&quot;:&quot;the answer to the universe is unknown.&quot;,&quot;score&quot;:<span class="hljs-number">0.043957844376564026</span>,&quot;token&quot;:<span class="hljs-number">4242</span>,&quot;token_str&quot;:&quot;unknown&quot;},{&quot;sequence&quot;:&quot;the answer to the universe is simple.&quot;,&quot;score&quot;:<span class="hljs-number">0.04015745222568512</span>,&quot;token&quot;:<span class="hljs-number">3722</span>,&quot;token_str&quot;:&quot;simple&quot;}]`}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p:O,i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function FF(q){let r,c;return r=new P({props:{$$slots:{default:[KF]},$$scope:{ctx:q}}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),r.$set($)},i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function JF(q){let r,c;return r=new R({props:{code:`curl https://api-inference.huggingface.co/models/bert-base-uncased \\
        -X POST \\
        -d '{"inputs":"The answer to the universe is [MASK]."}' \\
        -H "Authorization: Bearer \${HF_API_TOKEN}"
# [{"sequence":"the answer to the universe is no.","score":0.16963955760002136,"token":2053,"token_str":"no"},{"sequence":"the answer to the universe is nothing.","score":0.07344776391983032,"token":2498,"token_str":"nothing"},{"sequence":"the answer to the universe is yes.","score":0.05803241208195686,"token":2748,"token_str":"yes"},{"sequence":"the answer to the universe is unknown.","score":0.043957844376564026,"token":4242,"token_str":"unknown"},{"sequence":"the answer to the universe is simple.","score":0.04015745222568512,"token":3722,"token_str":"simple"}]`,highlighted:`curl https://api-inference.huggingface.co/models/bert-base-uncased \\
        -X POST \\
        -d <span class="hljs-string">&#x27;{&quot;inputs&quot;:&quot;The answer to the universe is [MASK].&quot;}&#x27;</span> \\
        -H <span class="hljs-string">&quot;Authorization: Bearer <span class="hljs-variable">\${HF_API_TOKEN}</span>&quot;</span>
<span class="hljs-comment"># [{&quot;sequence&quot;:&quot;the answer to the universe is no.&quot;,&quot;score&quot;:0.16963955760002136,&quot;token&quot;:2053,&quot;token_str&quot;:&quot;no&quot;},{&quot;sequence&quot;:&quot;the answer to the universe is nothing.&quot;,&quot;score&quot;:0.07344776391983032,&quot;token&quot;:2498,&quot;token_str&quot;:&quot;nothing&quot;},{&quot;sequence&quot;:&quot;the answer to the universe is yes.&quot;,&quot;score&quot;:0.05803241208195686,&quot;token&quot;:2748,&quot;token_str&quot;:&quot;yes&quot;},{&quot;sequence&quot;:&quot;the answer to the universe is unknown.&quot;,&quot;score&quot;:0.043957844376564026,&quot;token&quot;:4242,&quot;token_str&quot;:&quot;unknown&quot;},{&quot;sequence&quot;:&quot;the answer to the universe is simple.&quot;,&quot;score&quot;:0.04015745222568512,&quot;token&quot;:3722,&quot;token_str&quot;:&quot;simple&quot;}]</span>`}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p:O,i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function WF(q){let r,c;return r=new P({props:{$$slots:{default:[JF]},$$scope:{ctx:q}}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),r.$set($)},i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function YF(q){let r,c;return r=new R({props:{code:`self.assertEqual(
    deep_round(data),
    [
        {
            "sequence": "the answer to the universe is no.",
            "score": 0.1696,
            "token": 2053,
            "token_str": "no",
        },
        {
            "sequence": "the answer to the universe is nothing.",
            "score": 0.0734,
            "token": 2498,
            "token_str": "nothing",
        },
        {
            "sequence": "the answer to the universe is yes.",
            "score": 0.0580,
            "token": 2748,
            "token_str": "yes",
        },
        {
            "sequence": "the answer to the universe is unknown.",
            "score": 0.044,
            "token": 4242,
            "token_str": "unknown",
        },
        {
            "sequence": "the answer to the universe is simple.",
            "score": 0.0402,
            "token": 3722,
            "token_str": "simple",
        },
    ],
)`,highlighted:`self.assertEqual(
    deep_round(data),
    [
        {
            <span class="hljs-string">&quot;sequence&quot;</span>: <span class="hljs-string">&quot;the answer to the universe is no.&quot;</span>,
            <span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.1696</span>,
            <span class="hljs-string">&quot;token&quot;</span>: <span class="hljs-number">2053</span>,
            <span class="hljs-string">&quot;token_str&quot;</span>: <span class="hljs-string">&quot;no&quot;</span>,
        },
        {
            <span class="hljs-string">&quot;sequence&quot;</span>: <span class="hljs-string">&quot;the answer to the universe is nothing.&quot;</span>,
            <span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.0734</span>,
            <span class="hljs-string">&quot;token&quot;</span>: <span class="hljs-number">2498</span>,
            <span class="hljs-string">&quot;token_str&quot;</span>: <span class="hljs-string">&quot;nothing&quot;</span>,
        },
        {
            <span class="hljs-string">&quot;sequence&quot;</span>: <span class="hljs-string">&quot;the answer to the universe is yes.&quot;</span>,
            <span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.0580</span>,
            <span class="hljs-string">&quot;token&quot;</span>: <span class="hljs-number">2748</span>,
            <span class="hljs-string">&quot;token_str&quot;</span>: <span class="hljs-string">&quot;yes&quot;</span>,
        },
        {
            <span class="hljs-string">&quot;sequence&quot;</span>: <span class="hljs-string">&quot;the answer to the universe is unknown.&quot;</span>,
            <span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.044</span>,
            <span class="hljs-string">&quot;token&quot;</span>: <span class="hljs-number">4242</span>,
            <span class="hljs-string">&quot;token_str&quot;</span>: <span class="hljs-string">&quot;unknown&quot;</span>,
        },
        {
            <span class="hljs-string">&quot;sequence&quot;</span>: <span class="hljs-string">&quot;the answer to the universe is simple.&quot;</span>,
            <span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.0402</span>,
            <span class="hljs-string">&quot;token&quot;</span>: <span class="hljs-number">3722</span>,
            <span class="hljs-string">&quot;token_str&quot;</span>: <span class="hljs-string">&quot;simple&quot;</span>,
        },
    ],
)`}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p:O,i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function VF(q){let r,c;return r=new P({props:{$$slots:{default:[YF]},$$scope:{ctx:q}}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),r.$set($)},i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function XF(q){let r,c,s,d,$,k,A;return{c(){r=n("p"),c=n("strong"),s=i("Recommended model"),d=i(`:
`),$=n("a"),k=i("facebook/bart-large-cnn"),A=i("."),this.h()},l(j){r=o(j,"P",{});var T=l(r);c=o(T,"STRONG",{});var S=l(c);s=u(S,"Recommended model"),S.forEach(t),d=u(T,`:
`),$=o(T,"A",{href:!0,rel:!0});var D=l($);k=u(D,"facebook/bart-large-cnn"),D.forEach(t),A=u(T,"."),T.forEach(t),this.h()},h(){f($,"href","https://huggingface.co/facebook/bart-large-cnn"),f($,"rel","nofollow")},m(j,T){m(j,r,T),e(r,c),e(c,s),e(r,d),e(r,$),e($,k),e(r,A)},d(j){j&&t(r)}}}function QF(q){let r,c;return r=new R({props:{code:`import json
import requests
headers = {"Authorization": f"Bearer {API_TOKEN}"}
API_URL = "https://api-inference.huggingface.co/models/facebook/bart-large-cnn"
def query(payload):
    data = json.dumps(payload)
    response = requests.request("POST", API_URL, headers=headers, data=data)
    return json.loads(response.content.decode("utf-8"))
data = query(
    {
        "inputs": "The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world, a title it held for 41 years until the Chrysler Building in New York City was finished in 1930. It was the first structure to reach a height of 300 metres. Due to the addition of a broadcasting aerial at the top of the tower in 1957, it is now taller than the Chrysler Building by 5.2 metres (17 ft). Excluding transmitters, the Eiffel Tower is the second tallest free-standing structure in France after the Millau Viaduct.",
        "parameters": {"do_sample": False},
    }
)
# Response
self.assertEqual(
    data,
    [
        {
            "summary_text": "The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world.",
        },
    ],
)`,highlighted:`<span class="hljs-keyword">import</span> json
<span class="hljs-keyword">import</span> requests
headers = {<span class="hljs-string">&quot;Authorization&quot;</span>: <span class="hljs-string">f&quot;Bearer <span class="hljs-subst">{API_TOKEN}</span>&quot;</span>}
API_URL = <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/facebook/bart-large-cnn&quot;</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">payload</span>):
    data = json.dumps(payload)
    response = requests.request(<span class="hljs-string">&quot;POST&quot;</span>, API_URL, headers=headers, data=data)
    <span class="hljs-keyword">return</span> json.loads(response.content.decode(<span class="hljs-string">&quot;utf-8&quot;</span>))
data = query(
    {
        <span class="hljs-string">&quot;inputs&quot;</span>: <span class="hljs-string">&quot;The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world, a title it held for 41 years until the Chrysler Building in New York City was finished in 1930. It was the first structure to reach a height of 300 metres. Due to the addition of a broadcasting aerial at the top of the tower in 1957, it is now taller than the Chrysler Building by 5.2 metres (17 ft). Excluding transmitters, the Eiffel Tower is the second tallest free-standing structure in France after the Millau Viaduct.&quot;</span>,
        <span class="hljs-string">&quot;parameters&quot;</span>: {<span class="hljs-string">&quot;do_sample&quot;</span>: <span class="hljs-literal">False</span>},
    }
)
<span class="hljs-comment"># Response</span>
self.assertEqual(
    data,
    [
        {
            <span class="hljs-string">&quot;summary_text&quot;</span>: <span class="hljs-string">&quot;The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world.&quot;</span>,
        },
    ],
)`}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p:O,i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function ZF(q){let r,c;return r=new P({props:{$$slots:{default:[QF]},$$scope:{ctx:q}}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),r.$set($)},i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function eJ(q){let r,c;return r=new R({props:{code:`import fetch from "node-fetch";
async function query(data) {
    const response = await fetch(
        "https://api-inference.huggingface.co/models/facebook/bart-large-cnn",
        {
            headers: { Authorization: \`Bearer \${API_TOKEN}\` },
            method: "POST",
            body: JSON.stringify(data),
        }
    );
    const result = await response.json();
    return result;
}
query({inputs: "The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world, a title it held for 41 years until the Chrysler Building in New York City was finished in 1930. It was the first structure to reach a height of 300 metres. Due to the addition of a broadcasting aerial at the top of the tower in 1957, it is now taller than the Chrysler Building by 5.2 metres (17 ft). Excluding transmitters, the Eiffel Tower is the second tallest free-standing structure in France after the Millau Viaduct."}).then((response) => {
    console.log(JSON.stringify(response));
});
// [{"summary_text":"The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side. It was the first structure to reach a height of 300 metres."}]`,highlighted:`import fetch from <span class="hljs-comment">&quot;node-fetch&quot;</span>;
async function query(data) {
    const response = await fetch(
        <span class="hljs-comment">&quot;https://api-inference.huggingface.co/models/facebook/bart-large-cnn&quot;</span>,
        {
            headers: { <span class="hljs-type">Authorization</span>: \`<span class="hljs-type">Bearer</span> <span class="hljs-string">\${</span><span class="hljs-type">API_TOKEN</span>}\` },
            method: <span class="hljs-comment">&quot;POST&quot;</span>,
            body: <span class="hljs-type">JSON</span>.stringify(data),
        }
    );
    const result = await response.json();
    return result;
}
query({inputs: <span class="hljs-comment">&quot;The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world, a title it held for 41 years until the Chrysler Building in New York City was finished in 1930. It was the first structure to reach a height of 300 metres. Due to the addition of a broadcasting aerial at the top of the tower in 1957, it is now taller than the Chrysler Building by 5.2 metres (17 ft). Excluding transmitters, the Eiffel Tower is the second tallest free-standing structure in France after the Millau Viaduct.&quot;</span>}).then((response) =&gt; {
    console.log(<span class="hljs-type">JSON</span>.stringify(response));
});
// [{<span class="hljs-comment">&quot;summary_text&quot;</span>:<span class="hljs-comment">&quot;The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side. It was the first structure to reach a height of 300 metres.&quot;</span>}]`}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p:O,i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function tJ(q){let r,c;return r=new P({props:{$$slots:{default:[eJ]},$$scope:{ctx:q}}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),r.$set($)},i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function sJ(q){let r,c;return r=new R({props:{code:`curl https://api-inference.huggingface.co/models/facebook/bart-large-cnn \\
        -X POST \\
        -d '{"inputs": "The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world, a title it held for 41 years until the Chrysler Building in New York City was finished in 1930. It was the first structure to reach a height of 300 metres. Due to the addition of a broadcasting aerial at the top of the tower in 1957, it is now taller than the Chrysler Building by 5.2 metres (17 ft). Excluding transmitters, the Eiffel Tower is the second tallest free-standing structure in France after the Millau Viaduct.", "parameters": {"do_sample": false}}' \\
        -H "Authorization: Bearer \${HF_API_TOKEN}"
# [{"summary_text":"The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world."}]`,highlighted:`curl https://api-inference.huggingface.co/models/facebook/bart-large-cnn \\
        -X POST \\
        -d <span class="hljs-string">&#x27;{&quot;inputs&quot;: &quot;The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world, a title it held for 41 years until the Chrysler Building in New York City was finished in 1930. It was the first structure to reach a height of 300 metres. Due to the addition of a broadcasting aerial at the top of the tower in 1957, it is now taller than the Chrysler Building by 5.2 metres (17 ft). Excluding transmitters, the Eiffel Tower is the second tallest free-standing structure in France after the Millau Viaduct.&quot;, &quot;parameters&quot;: {&quot;do_sample&quot;: false}}&#x27;</span> \\
        -H <span class="hljs-string">&quot;Authorization: Bearer <span class="hljs-variable">\${HF_API_TOKEN}</span>&quot;</span>
<span class="hljs-comment"># [{&quot;summary_text&quot;:&quot;The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world.&quot;}]</span>`}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p:O,i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function aJ(q){let r,c;return r=new P({props:{$$slots:{default:[sJ]},$$scope:{ctx:q}}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),r.$set($)},i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function rJ(q){let r,c,s,d,$,k,A;return{c(){r=n("p"),c=n("strong"),s=i("Recommended model"),d=i(`:
`),$=n("a"),k=i("deepset/roberta-base-squad2"),A=i("."),this.h()},l(j){r=o(j,"P",{});var T=l(r);c=o(T,"STRONG",{});var S=l(c);s=u(S,"Recommended model"),S.forEach(t),d=u(T,`:
`),$=o(T,"A",{href:!0,rel:!0});var D=l($);k=u(D,"deepset/roberta-base-squad2"),D.forEach(t),A=u(T,"."),T.forEach(t),this.h()},h(){f($,"href","https://huggingface.co/deepset/roberta-base-squad2"),f($,"rel","nofollow")},m(j,T){m(j,r,T),e(r,c),e(c,s),e(r,d),e(r,$),e($,k),e(r,A)},d(j){j&&t(r)}}}function nJ(q){let r,c;return r=new R({props:{code:`import json
import requests
headers = {"Authorization": f"Bearer {API_TOKEN}"}
API_URL = "https://api-inference.huggingface.co/models/deepset/roberta-base-squad2"
def query(payload):
    data = json.dumps(payload)
    response = requests.request("POST", API_URL, headers=headers, data=data)
    return json.loads(response.content.decode("utf-8"))
data = query(
    {
        "inputs": {
            "question": "What's my name?",
            "context": "My name is Clara and I live in Berkeley.",
        }
    }
)`,highlighted:`<span class="hljs-keyword">import</span> json
<span class="hljs-keyword">import</span> requests
headers = {<span class="hljs-string">&quot;Authorization&quot;</span>: <span class="hljs-string">f&quot;Bearer <span class="hljs-subst">{API_TOKEN}</span>&quot;</span>}
API_URL = <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/deepset/roberta-base-squad2&quot;</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">payload</span>):
    data = json.dumps(payload)
    response = requests.request(<span class="hljs-string">&quot;POST&quot;</span>, API_URL, headers=headers, data=data)
    <span class="hljs-keyword">return</span> json.loads(response.content.decode(<span class="hljs-string">&quot;utf-8&quot;</span>))
data = query(
    {
        <span class="hljs-string">&quot;inputs&quot;</span>: {
            <span class="hljs-string">&quot;question&quot;</span>: <span class="hljs-string">&quot;What&#x27;s my name?&quot;</span>,
            <span class="hljs-string">&quot;context&quot;</span>: <span class="hljs-string">&quot;My name is Clara and I live in Berkeley.&quot;</span>,
        }
    }
)`}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p:O,i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function oJ(q){let r,c;return r=new P({props:{$$slots:{default:[nJ]},$$scope:{ctx:q}}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),r.$set($)},i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function lJ(q){let r,c;return r=new R({props:{code:`import fetch from "node-fetch";
async function query(data) {
    const response = await fetch(
        "https://api-inference.huggingface.co/models/deepset/roberta-base-squad2",
        {
            headers: { Authorization: \`Bearer \${API_TOKEN}\` },
            method: "POST",
            body: JSON.stringify(data),
        }
    );
    const result = await response.json();
    return result;
}
query({inputs:{question:"What is my name?",context:"My name is Clara and I live in Berkeley."}}).then((response) => {
    console.log(JSON.stringify(response));
});
// {"score":0.933128833770752,"start":11,"end":16,"answer":"Clara"}`,highlighted:`<span class="hljs-keyword">import</span> fetch <span class="hljs-keyword">from</span> <span class="hljs-string">&quot;node-fetch&quot;</span>;
<span class="hljs-keyword">async</span> <span class="hljs-keyword">function</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">data</span>) {
    <span class="hljs-keyword">const</span> response = <span class="hljs-keyword">await</span> <span class="hljs-title function_">fetch</span>(
        <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/deepset/roberta-base-squad2&quot;</span>,
        {
            <span class="hljs-attr">headers</span>: { <span class="hljs-title class_">Authorization</span>: <span class="hljs-string">\`Bearer <span class="hljs-subst">\${API_TOKEN}</span>\`</span> },
            <span class="hljs-attr">method</span>: <span class="hljs-string">&quot;POST&quot;</span>,
            <span class="hljs-attr">body</span>: <span class="hljs-title class_">JSON</span>.<span class="hljs-title function_">stringify</span>(data),
        }
    );
    <span class="hljs-keyword">const</span> result = <span class="hljs-keyword">await</span> response.<span class="hljs-title function_">json</span>();
    <span class="hljs-keyword">return</span> result;
}
<span class="hljs-title function_">query</span>({<span class="hljs-attr">inputs</span>:{<span class="hljs-attr">question</span>:<span class="hljs-string">&quot;What is my name?&quot;</span>,<span class="hljs-attr">context</span>:<span class="hljs-string">&quot;My name is Clara and I live in Berkeley.&quot;</span>}}).<span class="hljs-title function_">then</span>(<span class="hljs-function">(<span class="hljs-params">response</span>) =&gt;</span> {
    <span class="hljs-variable language_">console</span>.<span class="hljs-title function_">log</span>(<span class="hljs-title class_">JSON</span>.<span class="hljs-title function_">stringify</span>(response));
});
<span class="hljs-comment">// {&quot;score&quot;:0.933128833770752,&quot;start&quot;:11,&quot;end&quot;:16,&quot;answer&quot;:&quot;Clara&quot;}</span>`}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p:O,i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function iJ(q){let r,c;return r=new P({props:{$$slots:{default:[lJ]},$$scope:{ctx:q}}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),r.$set($)},i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function uJ(q){let r,c;return r=new R({props:{code:`curl https://api-inference.huggingface.co/models/deepset/roberta-base-squad2 \\
        -X POST \\
        -d '{"inputs":{"question":"What is my name?","context":"My name is Clara and I live in Berkeley."}}' \\
        -H "Authorization: Bearer \${HF_API_TOKEN}"
# {"score":0.933128833770752,"start":11,"end":16,"answer":"Clara"}`,highlighted:`curl https://api-inference.huggingface.co/models/deepset/roberta-base-squad2 \\
        -X POST \\
        -d <span class="hljs-string">&#x27;{&quot;inputs&quot;:{&quot;question&quot;:&quot;What is my name?&quot;,&quot;context&quot;:&quot;My name is Clara and I live in Berkeley.&quot;}}&#x27;</span> \\
        -H <span class="hljs-string">&quot;Authorization: Bearer <span class="hljs-variable">\${HF_API_TOKEN}</span>&quot;</span>
<span class="hljs-comment"># {&quot;score&quot;:0.933128833770752,&quot;start&quot;:11,&quot;end&quot;:16,&quot;answer&quot;:&quot;Clara&quot;}</span>`}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p:O,i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function cJ(q){let r,c;return r=new P({props:{$$slots:{default:[uJ]},$$scope:{ctx:q}}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),r.$set($)},i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function fJ(q){let r,c;return r=new R({props:{code:`self.assertEqual(
    deep_round(data),
    {"score": 0.9327, "start": 11, "end": 16, "answer": "Clara"},
)`,highlighted:`self.assertEqual(
    deep_round(data),
    {<span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.9327</span>, <span class="hljs-string">&quot;start&quot;</span>: <span class="hljs-number">11</span>, <span class="hljs-string">&quot;end&quot;</span>: <span class="hljs-number">16</span>, <span class="hljs-string">&quot;answer&quot;</span>: <span class="hljs-string">&quot;Clara&quot;</span>},
)`}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p:O,i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function pJ(q){let r,c;return r=new P({props:{$$slots:{default:[fJ]},$$scope:{ctx:q}}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),r.$set($)},i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function hJ(q){let r,c,s,d,$,k,A;return{c(){r=n("p"),c=n("strong"),s=i("Recommended model"),d=i(`:
`),$=n("a"),k=i("google/tapas-base-finetuned-wtq"),A=i("."),this.h()},l(j){r=o(j,"P",{});var T=l(r);c=o(T,"STRONG",{});var S=l(c);s=u(S,"Recommended model"),S.forEach(t),d=u(T,`:
`),$=o(T,"A",{href:!0,rel:!0});var D=l($);k=u(D,"google/tapas-base-finetuned-wtq"),D.forEach(t),A=u(T,"."),T.forEach(t),this.h()},h(){f($,"href","https://huggingface.co/google/tapas-base-finetuned-wtq"),f($,"rel","nofollow")},m(j,T){m(j,r,T),e(r,c),e(c,s),e(r,d),e(r,$),e($,k),e(r,A)},d(j){j&&t(r)}}}function dJ(q){let r,c;return r=new R({props:{code:`import json
import requests
headers = {"Authorization": f"Bearer {API_TOKEN}"}
API_URL = "https://api-inference.huggingface.co/models/google/tapas-base-finetuned-wtq"
def query(payload):
    data = json.dumps(payload)
    response = requests.request("POST", API_URL, headers=headers, data=data)
    return json.loads(response.content.decode("utf-8"))
data = query(
    {
        "inputs": {
            "query": "How many stars does the transformers repository have?",
            "table": {
                "Repository": ["Transformers", "Datasets", "Tokenizers"],
                "Stars": ["36542", "4512", "3934"],
                "Contributors": ["651", "77", "34"],
                "Programming language": [
                    "Python",
                    "Python",
                    "Rust, Python and NodeJS",
                ],
            },
        }
    }
)`,highlighted:`<span class="hljs-keyword">import</span> json
<span class="hljs-keyword">import</span> requests
headers = {<span class="hljs-string">&quot;Authorization&quot;</span>: <span class="hljs-string">f&quot;Bearer <span class="hljs-subst">{API_TOKEN}</span>&quot;</span>}
API_URL = <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/google/tapas-base-finetuned-wtq&quot;</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">payload</span>):
    data = json.dumps(payload)
    response = requests.request(<span class="hljs-string">&quot;POST&quot;</span>, API_URL, headers=headers, data=data)
    <span class="hljs-keyword">return</span> json.loads(response.content.decode(<span class="hljs-string">&quot;utf-8&quot;</span>))
data = query(
    {
        <span class="hljs-string">&quot;inputs&quot;</span>: {
            <span class="hljs-string">&quot;query&quot;</span>: <span class="hljs-string">&quot;How many stars does the transformers repository have?&quot;</span>,
            <span class="hljs-string">&quot;table&quot;</span>: {
                <span class="hljs-string">&quot;Repository&quot;</span>: [<span class="hljs-string">&quot;Transformers&quot;</span>, <span class="hljs-string">&quot;Datasets&quot;</span>, <span class="hljs-string">&quot;Tokenizers&quot;</span>],
                <span class="hljs-string">&quot;Stars&quot;</span>: [<span class="hljs-string">&quot;36542&quot;</span>, <span class="hljs-string">&quot;4512&quot;</span>, <span class="hljs-string">&quot;3934&quot;</span>],
                <span class="hljs-string">&quot;Contributors&quot;</span>: [<span class="hljs-string">&quot;651&quot;</span>, <span class="hljs-string">&quot;77&quot;</span>, <span class="hljs-string">&quot;34&quot;</span>],
                <span class="hljs-string">&quot;Programming language&quot;</span>: [
                    <span class="hljs-string">&quot;Python&quot;</span>,
                    <span class="hljs-string">&quot;Python&quot;</span>,
                    <span class="hljs-string">&quot;Rust, Python and NodeJS&quot;</span>,
                ],
            },
        }
    }
)`}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p:O,i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function gJ(q){let r,c;return r=new P({props:{$$slots:{default:[dJ]},$$scope:{ctx:q}}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),r.$set($)},i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function mJ(q){let r,c;return r=new R({props:{code:`import fetch from "node-fetch";
async function query(data) {
    const response = await fetch(
        "https://api-inference.huggingface.co/models/google/tapas-base-finetuned-wtq",
        {
            headers: { Authorization: \`Bearer \${API_TOKEN}\` },
            method: "POST",
            body: JSON.stringify(data),
        }
    );
    const result = await response.json();
    return result;
}
query({inputs:{query:"How many stars does the transformers repository have?",table:{Repository:["Transformers","Datasets","Tokenizers"],Stars:["36542","4512","3934"],Contributors:["651","77","34"],"Programming language":["Python","Python","Rust, Python and NodeJS"]}}}).then((response) => {
    console.log(JSON.stringify(response));
});
// {"answer":"AVERAGE > 36542","coordinates":[[0,1]],"cells":["36542"],"aggregator":"AVERAGE"}`,highlighted:`<span class="hljs-keyword">import</span> <span class="hljs-keyword">fetch</span> <span class="hljs-keyword">from</span> &quot;node-fetch&quot;;
async <span class="hljs-keyword">function</span> query(data) {
    const response = await <span class="hljs-keyword">fetch</span>(
        &quot;https://api-inference.huggingface.co/models/google/tapas-base-finetuned-wtq&quot;,
        {
            headers: { <span class="hljs-keyword">Authorization</span>: \`Bearer \${API_TOKEN}\` },
            <span class="hljs-keyword">method</span>: &quot;POST&quot;,
            body: <span class="hljs-type">JSON</span>.stringify(data),
        }
    );
    const result = await response.json();
    <span class="hljs-keyword">return</span> result;
}
query({inputs:{query:&quot;How many stars does the transformers repository have?&quot;,<span class="hljs-keyword">table</span>:{Repository:[&quot;Transformers&quot;,&quot;Datasets&quot;,&quot;Tokenizers&quot;],Stars:[&quot;36542&quot;,&quot;4512&quot;,&quot;3934&quot;],Contributors:[&quot;651&quot;,&quot;77&quot;,&quot;34&quot;],&quot;Programming language&quot;:[&quot;Python&quot;,&quot;Python&quot;,&quot;Rust, Python and NodeJS&quot;]}}}).<span class="hljs-keyword">then</span>((response) =&gt; {
    console.log(<span class="hljs-type">JSON</span>.stringify(response));
});
// {&quot;answer&quot;:&quot;AVERAGE &gt; 36542&quot;,&quot;coordinates&quot;:[[<span class="hljs-number">0</span>,<span class="hljs-number">1</span>]],&quot;cells&quot;:[&quot;36542&quot;],&quot;aggregator&quot;:&quot;AVERAGE&quot;}`}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p:O,i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function $J(q){let r,c;return r=new P({props:{$$slots:{default:[mJ]},$$scope:{ctx:q}}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),r.$set($)},i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function qJ(q){let r,c;return r=new R({props:{code:`curl https://api-inference.huggingface.co/models/google/tapas-base-finetuned-wtq \\
        -X POST \\
        -d '{"inputs":{"query":"How many stars does the transformers repository have?","table":{"Repository":["Transformers","Datasets","Tokenizers"],"Stars":["36542","4512","3934"],"Contributors":["651","77","34"],"Programming language":["Python","Python","Rust, Python and NodeJS"]}}}' \\
        -H "Authorization: Bearer \${HF_API_TOKEN}"
# {"answer":"AVERAGE > 36542","coordinates":[[0,1]],"cells":["36542"],"aggregator":"AVERAGE"}`,highlighted:`curl https://api-inference.huggingface.co/models/google/tapas-base-finetuned-wtq \\
        -X POST \\
        -d <span class="hljs-string">&#x27;{&quot;inputs&quot;:{&quot;query&quot;:&quot;How many stars does the transformers repository have?&quot;,&quot;table&quot;:{&quot;Repository&quot;:[&quot;Transformers&quot;,&quot;Datasets&quot;,&quot;Tokenizers&quot;],&quot;Stars&quot;:[&quot;36542&quot;,&quot;4512&quot;,&quot;3934&quot;],&quot;Contributors&quot;:[&quot;651&quot;,&quot;77&quot;,&quot;34&quot;],&quot;Programming language&quot;:[&quot;Python&quot;,&quot;Python&quot;,&quot;Rust, Python and NodeJS&quot;]}}}&#x27;</span> \\
        -H <span class="hljs-string">&quot;Authorization: Bearer <span class="hljs-variable">\${HF_API_TOKEN}</span>&quot;</span>
<span class="hljs-comment"># {&quot;answer&quot;:&quot;AVERAGE &gt; 36542&quot;,&quot;coordinates&quot;:[[0,1]],&quot;cells&quot;:[&quot;36542&quot;],&quot;aggregator&quot;:&quot;AVERAGE&quot;}</span>`}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p:O,i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function _J(q){let r,c;return r=new P({props:{$$slots:{default:[qJ]},$$scope:{ctx:q}}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),r.$set($)},i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function vJ(q){let r,c;return r=new R({props:{code:`self.assertEqual(
    data,
    {
        "answer": "AVERAGE > 36542",
        "coordinates": [[0, 1]],
        "cells": ["36542"],
        "aggregator": "AVERAGE",
    },
)`,highlighted:`self.assertEqual(
    data,
    {
        <span class="hljs-string">&quot;answer&quot;</span>: <span class="hljs-string">&quot;AVERAGE &gt; 36542&quot;</span>,
        <span class="hljs-string">&quot;coordinates&quot;</span>: [[<span class="hljs-number">0</span>, <span class="hljs-number">1</span>]],
        <span class="hljs-string">&quot;cells&quot;</span>: [<span class="hljs-string">&quot;36542&quot;</span>],
        <span class="hljs-string">&quot;aggregator&quot;</span>: <span class="hljs-string">&quot;AVERAGE&quot;</span>,
    },
)`}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p:O,i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function EJ(q){let r,c;return r=new P({props:{$$slots:{default:[vJ]},$$scope:{ctx:q}}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),r.$set($)},i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function yJ(q){let r,c,s,d,$,k;return{c(){r=n("p"),c=n("strong"),s=i("Recommended model"),d=i(`:
`),$=n("a"),k=i("distilbert-base-uncased-finetuned-sst-2-english"),this.h()},l(A){r=o(A,"P",{});var j=l(r);c=o(j,"STRONG",{});var T=l(c);s=u(T,"Recommended model"),T.forEach(t),d=u(j,`:
`),$=o(j,"A",{href:!0,rel:!0});var S=l($);k=u(S,"distilbert-base-uncased-finetuned-sst-2-english"),S.forEach(t),j.forEach(t),this.h()},h(){f($,"href","https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english"),f($,"rel","nofollow")},m(A,j){m(A,r,j),e(r,c),e(c,s),e(r,d),e(r,$),e($,k)},d(A){A&&t(r)}}}function wJ(q){let r,c;return r=new R({props:{code:`import json
import requests
headers = {"Authorization": f"Bearer {API_TOKEN}"}
API_URL = "https://api-inference.huggingface.co/models/distilbert-base-uncased-finetuned-sst-2-english"
def query(payload):
    data = json.dumps(payload)
    response = requests.request("POST", API_URL, headers=headers, data=data)
    return json.loads(response.content.decode("utf-8"))
data = query({"inputs": "I like you. I love you"})`,highlighted:`<span class="hljs-keyword">import</span> json
<span class="hljs-keyword">import</span> requests
headers = {<span class="hljs-string">&quot;Authorization&quot;</span>: <span class="hljs-string">f&quot;Bearer <span class="hljs-subst">{API_TOKEN}</span>&quot;</span>}
API_URL = <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/distilbert-base-uncased-finetuned-sst-2-english&quot;</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">payload</span>):
    data = json.dumps(payload)
    response = requests.request(<span class="hljs-string">&quot;POST&quot;</span>, API_URL, headers=headers, data=data)
    <span class="hljs-keyword">return</span> json.loads(response.content.decode(<span class="hljs-string">&quot;utf-8&quot;</span>))
data = query({<span class="hljs-string">&quot;inputs&quot;</span>: <span class="hljs-string">&quot;I like you. I love you&quot;</span>})`}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p:O,i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function bJ(q){let r,c;return r=new P({props:{$$slots:{default:[wJ]},$$scope:{ctx:q}}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),r.$set($)},i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function TJ(q){let r,c;return r=new R({props:{code:`import fetch from "node-fetch";
async function query(data) {
    const response = await fetch(
        "https://api-inference.huggingface.co/models/distilbert-base-uncased-finetuned-sst-2-english",
        {
            headers: { Authorization: \`Bearer \${API_TOKEN}\` },
            method: "POST",
            body: JSON.stringify(data),
        }
    );
    const result = await response.json();
    return result;
}
query({inputs:"I like you. I love you"}).then((response) => {
    console.log(JSON.stringify(response));
});
// [[{"label":"NEGATIVE","score":0.0001261125144083053},{"label":"POSITIVE","score":0.9998738765716553}]]`,highlighted:`<span class="hljs-keyword">import</span> fetch <span class="hljs-keyword">from</span> <span class="hljs-string">&quot;node-fetch&quot;</span>;
<span class="hljs-keyword">async</span> <span class="hljs-keyword">function</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">data</span>) {
    <span class="hljs-keyword">const</span> response = <span class="hljs-keyword">await</span> <span class="hljs-title function_">fetch</span>(
        <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/distilbert-base-uncased-finetuned-sst-2-english&quot;</span>,
        {
            <span class="hljs-attr">headers</span>: { <span class="hljs-title class_">Authorization</span>: <span class="hljs-string">\`Bearer <span class="hljs-subst">\${API_TOKEN}</span>\`</span> },
            <span class="hljs-attr">method</span>: <span class="hljs-string">&quot;POST&quot;</span>,
            <span class="hljs-attr">body</span>: <span class="hljs-title class_">JSON</span>.<span class="hljs-title function_">stringify</span>(data),
        }
    );
    <span class="hljs-keyword">const</span> result = <span class="hljs-keyword">await</span> response.<span class="hljs-title function_">json</span>();
    <span class="hljs-keyword">return</span> result;
}
<span class="hljs-title function_">query</span>({<span class="hljs-attr">inputs</span>:<span class="hljs-string">&quot;I like you. I love you&quot;</span>}).<span class="hljs-title function_">then</span>(<span class="hljs-function">(<span class="hljs-params">response</span>) =&gt;</span> {
    <span class="hljs-variable language_">console</span>.<span class="hljs-title function_">log</span>(<span class="hljs-title class_">JSON</span>.<span class="hljs-title function_">stringify</span>(response));
});
<span class="hljs-comment">// [[{&quot;label&quot;:&quot;NEGATIVE&quot;,&quot;score&quot;:0.0001261125144083053},{&quot;label&quot;:&quot;POSITIVE&quot;,&quot;score&quot;:0.9998738765716553}]]</span>`}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p:O,i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function jJ(q){let r,c;return r=new P({props:{$$slots:{default:[TJ]},$$scope:{ctx:q}}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),r.$set($)},i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function kJ(q){let r,c;return r=new R({props:{code:`curl https://api-inference.huggingface.co/models/distilbert-base-uncased-finetuned-sst-2-english \\
        -X POST \\
        -d '{"inputs":"I like you. I love you"}' \\
        -H "Authorization: Bearer \${HF_API_TOKEN}"
# [[{"label":"NEGATIVE","score":0.0001261125144083053},{"label":"POSITIVE","score":0.9998738765716553}]]`,highlighted:`curl https://api-inference.huggingface.co/models/distilbert-base-uncased-finetuned-sst-2-english \\
        -X POST \\
        -d <span class="hljs-string">&#x27;{&quot;inputs&quot;:&quot;I like you. I love you&quot;}&#x27;</span> \\
        -H <span class="hljs-string">&quot;Authorization: Bearer <span class="hljs-variable">\${HF_API_TOKEN}</span>&quot;</span>
<span class="hljs-comment"># [[{&quot;label&quot;:&quot;NEGATIVE&quot;,&quot;score&quot;:0.0001261125144083053},{&quot;label&quot;:&quot;POSITIVE&quot;,&quot;score&quot;:0.9998738765716553}]]</span>`}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p:O,i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function AJ(q){let r,c;return r=new P({props:{$$slots:{default:[kJ]},$$scope:{ctx:q}}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),r.$set($)},i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function DJ(q){let r,c;return r=new R({props:{code:`self.assertEqual(
    deep_round(data),
    [
        [
            {"label": "NEGATIVE", "score": 0.0001},
            {"label": "POSITIVE", "score": 0.9999},
        ]
    ],
)`,highlighted:`self.assertEqual(
    deep_round(data),
    [
        [
            {<span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;NEGATIVE&quot;</span>, <span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.0001</span>},
            {<span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;POSITIVE&quot;</span>, <span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.9999</span>},
        ]
    ],
)`}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p:O,i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function OJ(q){let r,c;return r=new P({props:{$$slots:{default:[DJ]},$$scope:{ctx:q}}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),r.$set($)},i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function PJ(q){let r,c,s,d,$,k,A;return{c(){r=n("p"),c=n("strong"),s=i("Recommended model"),d=i(": "),$=n("a"),k=i("gpt2"),A=i(" (it\u2019s a simple model, but fun to play with)."),this.h()},l(j){r=o(j,"P",{});var T=l(r);c=o(T,"STRONG",{});var S=l(c);s=u(S,"Recommended model"),S.forEach(t),d=u(T,": "),$=o(T,"A",{href:!0,rel:!0});var D=l($);k=u(D,"gpt2"),D.forEach(t),A=u(T," (it\u2019s a simple model, but fun to play with)."),T.forEach(t),this.h()},h(){f($,"href","https://huggingface.co/gpt2"),f($,"rel","nofollow")},m(j,T){m(j,r,T),e(r,c),e(c,s),e(r,d),e(r,$),e($,k),e(r,A)},d(j){j&&t(r)}}}function RJ(q){let r,c;return r=new R({props:{code:`import json
import requests
headers = {"Authorization": f"Bearer {API_TOKEN}"}
API_URL = "https://api-inference.huggingface.co/models/gpt2"
def query(payload):
    data = json.dumps(payload)
    response = requests.request("POST", API_URL, headers=headers, data=data)
    return json.loads(response.content.decode("utf-8"))
data = query({"inputs": "The answer to the universe is"})`,highlighted:`<span class="hljs-keyword">import</span> json
<span class="hljs-keyword">import</span> requests
headers = {<span class="hljs-string">&quot;Authorization&quot;</span>: <span class="hljs-string">f&quot;Bearer <span class="hljs-subst">{API_TOKEN}</span>&quot;</span>}
API_URL = <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/gpt2&quot;</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">payload</span>):
    data = json.dumps(payload)
    response = requests.request(<span class="hljs-string">&quot;POST&quot;</span>, API_URL, headers=headers, data=data)
    <span class="hljs-keyword">return</span> json.loads(response.content.decode(<span class="hljs-string">&quot;utf-8&quot;</span>))
data = query({<span class="hljs-string">&quot;inputs&quot;</span>: <span class="hljs-string">&quot;The answer to the universe is&quot;</span>})`}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p:O,i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function SJ(q){let r,c;return r=new P({props:{$$slots:{default:[RJ]},$$scope:{ctx:q}}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),r.$set($)},i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function NJ(q){let r,c;return r=new R({props:{code:`import fetch from "node-fetch";
async function query(data) {
    const response = await fetch(
        "https://api-inference.huggingface.co/models/gpt2",
        {
            headers: { Authorization: \`Bearer \${API_TOKEN}\` },
            method: "POST",
            body: JSON.stringify(data),
        }
    );
    const result = await response.json();
    return result;
}
query({inputs:"The answer to the universe is"}).then((response) => {
    console.log(JSON.stringify(response));
});
// [{"generated_text":"The answer to the universe is in a different shape (or shapeless) than"}]`,highlighted:`<span class="hljs-keyword">import</span> fetch <span class="hljs-keyword">from</span> <span class="hljs-string">&quot;node-fetch&quot;</span>;
<span class="hljs-keyword">async</span> <span class="hljs-keyword">function</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">data</span>) {
    <span class="hljs-keyword">const</span> response = <span class="hljs-keyword">await</span> <span class="hljs-title function_">fetch</span>(
        <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/gpt2&quot;</span>,
        {
            <span class="hljs-attr">headers</span>: { <span class="hljs-title class_">Authorization</span>: <span class="hljs-string">\`Bearer <span class="hljs-subst">\${API_TOKEN}</span>\`</span> },
            <span class="hljs-attr">method</span>: <span class="hljs-string">&quot;POST&quot;</span>,
            <span class="hljs-attr">body</span>: <span class="hljs-title class_">JSON</span>.<span class="hljs-title function_">stringify</span>(data),
        }
    );
    <span class="hljs-keyword">const</span> result = <span class="hljs-keyword">await</span> response.<span class="hljs-title function_">json</span>();
    <span class="hljs-keyword">return</span> result;
}
<span class="hljs-title function_">query</span>({<span class="hljs-attr">inputs</span>:<span class="hljs-string">&quot;The answer to the universe is&quot;</span>}).<span class="hljs-title function_">then</span>(<span class="hljs-function">(<span class="hljs-params">response</span>) =&gt;</span> {
    <span class="hljs-variable language_">console</span>.<span class="hljs-title function_">log</span>(<span class="hljs-title class_">JSON</span>.<span class="hljs-title function_">stringify</span>(response));
});
<span class="hljs-comment">// [{&quot;generated_text&quot;:&quot;The answer to the universe is in a different shape (or shapeless) than&quot;}]</span>`}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p:O,i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function xJ(q){let r,c;return r=new P({props:{$$slots:{default:[NJ]},$$scope:{ctx:q}}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),r.$set($)},i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function IJ(q){let r,c;return r=new R({props:{code:`curl https://api-inference.huggingface.co/models/gpt2 \\
        -X POST \\
        -d '{"inputs":"The answer to the universe is"}' \\
        -H "Authorization: Bearer \${HF_API_TOKEN}"
# [{"generated_text":"The answer to the universe is in a different shape (or shapeless) than"}]`,highlighted:`curl https://api-inference.huggingface.co/models/gpt2 \\
        -X POST \\
        -d <span class="hljs-string">&#x27;{&quot;inputs&quot;:&quot;The answer to the universe is&quot;}&#x27;</span> \\
        -H <span class="hljs-string">&quot;Authorization: Bearer <span class="hljs-variable">\${HF_API_TOKEN}</span>&quot;</span>
<span class="hljs-comment"># [{&quot;generated_text&quot;:&quot;The answer to the universe is in a different shape (or shapeless) than&quot;}]</span>`}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p:O,i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function HJ(q){let r,c;return r=new P({props:{$$slots:{default:[IJ]},$$scope:{ctx:q}}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),r.$set($)},i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function BJ(q){let r,c;return r=new R({props:{code:`data == [
    {
        "generated_text": 'The answer to the universe is that we are the creation of the entire universe," says Fitch.\\n\\nAs of the 1960s, six times as many Americans still make fewer than six bucks ($17) per year on their way to retirement.'
    }
]`,highlighted:`data == [
    {
        <span class="hljs-string">&quot;generated_text&quot;</span>: <span class="hljs-string">&#x27;The answer to the universe is that we are the creation of the entire universe,&quot; says Fitch.\\n\\nAs of the 1960s, six times as many Americans still make fewer than six bucks ($17) per year on their way to retirement.&#x27;</span>
    }
]`}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p:O,i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function CJ(q){let r,c;return r=new P({props:{$$slots:{default:[BJ]},$$scope:{ctx:q}}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),r.$set($)},i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function GJ(q){let r,c,s,d,$,k;return{c(){r=n("p"),c=n("strong"),s=i("Recommended model"),d=i(`:
`),$=n("a"),k=i("dbmdz/bert-large-cased-finetuned-conll03-english"),this.h()},l(A){r=o(A,"P",{});var j=l(r);c=o(j,"STRONG",{});var T=l(c);s=u(T,"Recommended model"),T.forEach(t),d=u(j,`:
`),$=o(j,"A",{href:!0,rel:!0});var S=l($);k=u(S,"dbmdz/bert-large-cased-finetuned-conll03-english"),S.forEach(t),j.forEach(t),this.h()},h(){f($,"href","https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english"),f($,"rel","nofollow")},m(A,j){m(A,r,j),e(r,c),e(c,s),e(r,d),e(r,$),e($,k)},d(A){A&&t(r)}}}function LJ(q){let r,c;return r=new R({props:{code:`import json
import requests
headers = {"Authorization": f"Bearer {API_TOKEN}"}
API_URL = "https://api-inference.huggingface.co/models/dbmdz/bert-large-cased-finetuned-conll03-english"
def query(payload):
    data = json.dumps(payload)
    response = requests.request("POST", API_URL, headers=headers, data=data)
    return json.loads(response.content.decode("utf-8"))
data = query({"inputs": "My name is Sarah Jessica Parker but you can call me Jessica"})`,highlighted:`<span class="hljs-keyword">import</span> json
<span class="hljs-keyword">import</span> requests
headers = {<span class="hljs-string">&quot;Authorization&quot;</span>: <span class="hljs-string">f&quot;Bearer <span class="hljs-subst">{API_TOKEN}</span>&quot;</span>}
API_URL = <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/dbmdz/bert-large-cased-finetuned-conll03-english&quot;</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">payload</span>):
    data = json.dumps(payload)
    response = requests.request(<span class="hljs-string">&quot;POST&quot;</span>, API_URL, headers=headers, data=data)
    <span class="hljs-keyword">return</span> json.loads(response.content.decode(<span class="hljs-string">&quot;utf-8&quot;</span>))
data = query({<span class="hljs-string">&quot;inputs&quot;</span>: <span class="hljs-string">&quot;My name is Sarah Jessica Parker but you can call me Jessica&quot;</span>})`}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p:O,i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function UJ(q){let r,c;return r=new P({props:{$$slots:{default:[LJ]},$$scope:{ctx:q}}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),r.$set($)},i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function zJ(q){let r,c;return r=new R({props:{code:`import fetch from "node-fetch";
async function query(data) {
    const response = await fetch(
        "https://api-inference.huggingface.co/models/dbmdz/bert-large-cased-finetuned-conll03-english",
        {
            headers: { Authorization: \`Bearer \${API_TOKEN}\` },
            method: "POST",
            body: JSON.stringify(data),
        }
    );
    const result = await response.json();
    return result;
}
query({inputs:"My name is Sarah Jessica Parker but you can call me Jessica"}).then((response) => {
    console.log(JSON.stringify(response));
});
// [{"entity_group":"PER","score":0.9991337060928345,"word":"Sarah Jessica Parker","start":11,"end":31},{"entity_group":"PER","score":0.9979912042617798,"word":"Jessica","start":52,"end":59}]`,highlighted:`<span class="hljs-keyword">import</span> <span class="hljs-keyword">fetch</span> <span class="hljs-keyword">from</span> &quot;node-fetch&quot;;
async <span class="hljs-keyword">function</span> query(data) {
    const response = await <span class="hljs-keyword">fetch</span>(
        &quot;https://api-inference.huggingface.co/models/dbmdz/bert-large-cased-finetuned-conll03-english&quot;,
        {
            headers: { <span class="hljs-keyword">Authorization</span>: \`Bearer \${API_TOKEN}\` },
            <span class="hljs-keyword">method</span>: &quot;POST&quot;,
            body: <span class="hljs-type">JSON</span>.stringify(data),
        }
    );
    const result = await response.json();
    <span class="hljs-keyword">return</span> result;
}
query({inputs:&quot;My name is Sarah Jessica Parker but you can call me Jessica&quot;}).<span class="hljs-keyword">then</span>((response) =&gt; {
    console.log(<span class="hljs-type">JSON</span>.stringify(response));
});
// [{&quot;entity_group&quot;:&quot;PER&quot;,&quot;score&quot;:<span class="hljs-number">0.9991337060928345</span>,&quot;word&quot;:&quot;Sarah Jessica Parker&quot;,&quot;start&quot;:<span class="hljs-number">11</span>,&quot;end&quot;:<span class="hljs-number">31</span>},{&quot;entity_group&quot;:&quot;PER&quot;,&quot;score&quot;:<span class="hljs-number">0.9979912042617798</span>,&quot;word&quot;:&quot;Jessica&quot;,&quot;start&quot;:<span class="hljs-number">52</span>,&quot;end&quot;:<span class="hljs-number">59</span>}]`}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p:O,i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function MJ(q){let r,c;return r=new P({props:{$$slots:{default:[zJ]},$$scope:{ctx:q}}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),r.$set($)},i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function KJ(q){let r,c;return r=new R({props:{code:`curl https://api-inference.huggingface.co/models/dbmdz/bert-large-cased-finetuned-conll03-english \\
        -X POST \\
        -d '{"inputs":"My name is Sarah Jessica Parker but you can call me Jessica"}' \\
        -H "Authorization: Bearer \${HF_API_TOKEN}"
# [{"entity_group":"PER","score":0.9991337060928345,"word":"Sarah Jessica Parker","start":11,"end":31},{"entity_group":"PER","score":0.9979912042617798,"word":"Jessica","start":52,"end":59}]`,highlighted:`curl https://api-inference.huggingface.co/models/dbmdz/bert-large-cased-finetuned-conll03-english \\
        -X POST \\
        -d <span class="hljs-string">&#x27;{&quot;inputs&quot;:&quot;My name is Sarah Jessica Parker but you can call me Jessica&quot;}&#x27;</span> \\
        -H <span class="hljs-string">&quot;Authorization: Bearer <span class="hljs-variable">\${HF_API_TOKEN}</span>&quot;</span>
<span class="hljs-comment"># [{&quot;entity_group&quot;:&quot;PER&quot;,&quot;score&quot;:0.9991337060928345,&quot;word&quot;:&quot;Sarah Jessica Parker&quot;,&quot;start&quot;:11,&quot;end&quot;:31},{&quot;entity_group&quot;:&quot;PER&quot;,&quot;score&quot;:0.9979912042617798,&quot;word&quot;:&quot;Jessica&quot;,&quot;start&quot;:52,&quot;end&quot;:59}]</span>`}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p:O,i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function FJ(q){let r,c;return r=new P({props:{$$slots:{default:[KJ]},$$scope:{ctx:q}}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),r.$set($)},i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function JJ(q){let r,c;return r=new R({props:{code:`self.assertEqual(
    deep_round(data),
    [
        {
            "entity_group": "PER",
            "score": 0.9991,
            "word": "Sarah Jessica Parker",
            "start": 11,
            "end": 31,
        },
        {
            "entity_group": "PER",
            "score": 0.998,
            "word": "Jessica",
            "start": 52,
            "end": 59,
        },
    ],
)`,highlighted:`self.assertEqual(
    deep_round(data),
    [
        {
            <span class="hljs-string">&quot;entity_group&quot;</span>: <span class="hljs-string">&quot;PER&quot;</span>,
            <span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.9991</span>,
            <span class="hljs-string">&quot;word&quot;</span>: <span class="hljs-string">&quot;Sarah Jessica Parker&quot;</span>,
            <span class="hljs-string">&quot;start&quot;</span>: <span class="hljs-number">11</span>,
            <span class="hljs-string">&quot;end&quot;</span>: <span class="hljs-number">31</span>,
        },
        {
            <span class="hljs-string">&quot;entity_group&quot;</span>: <span class="hljs-string">&quot;PER&quot;</span>,
            <span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.998</span>,
            <span class="hljs-string">&quot;word&quot;</span>: <span class="hljs-string">&quot;Jessica&quot;</span>,
            <span class="hljs-string">&quot;start&quot;</span>: <span class="hljs-number">52</span>,
            <span class="hljs-string">&quot;end&quot;</span>: <span class="hljs-number">59</span>,
        },
    ],
)`}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p:O,i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function WJ(q){let r,c;return r=new P({props:{$$slots:{default:[JJ]},$$scope:{ctx:q}}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),r.$set($)},i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function YJ(q){let r,c,s,d,$,k,A,j,T,S,D,ne,Re;return{c(){r=n("p"),c=n("strong"),s=i("Recommended model"),d=i(`:
`),$=n("a"),k=i("Helsinki-NLP/opus-mt-ru-en"),A=i(`.
Helsinki-NLP uploaded many models with many language pairs.
`),j=n("strong"),T=i("Recommended model"),S=i(": "),D=n("a"),ne=i("t5-base"),Re=i("."),this.h()},l(Q){r=o(Q,"P",{});var Y=l(r);c=o(Y,"STRONG",{});var it=l(c);s=u(it,"Recommended model"),it.forEach(t),d=u(Y,`:
`),$=o(Y,"A",{href:!0,rel:!0});var Li=l($);k=u(Li,"Helsinki-NLP/opus-mt-ru-en"),Li.forEach(t),A=u(Y,`.
Helsinki-NLP uploaded many models with many language pairs.
`),j=o(Y,"STRONG",{});var tr=l(j);T=u(tr,"Recommended model"),tr.forEach(t),S=u(Y,": "),D=o(Y,"A",{href:!0,rel:!0});var Se=l(D);ne=u(Se,"t5-base"),Se.forEach(t),Re=u(Y,"."),Y.forEach(t),this.h()},h(){f($,"href","https://huggingface.co/Helsinki-NLP/opus-mt-ru-en"),f($,"rel","nofollow"),f(D,"href","https://huggingface.co/t5-base"),f(D,"rel","nofollow")},m(Q,Y){m(Q,r,Y),e(r,c),e(c,s),e(r,d),e(r,$),e($,k),e(r,A),e(r,j),e(j,T),e(r,S),e(r,D),e(D,ne),e(r,Re)},d(Q){Q&&t(r)}}}function VJ(q){let r,c;return r=new R({props:{code:`import json
import requests
headers = {"Authorization": f"Bearer {API_TOKEN}"}
API_URL = "https://api-inference.huggingface.co/models/Helsinki-NLP/opus-mt-ru-en"
def query(payload):
    data = json.dumps(payload)
    response = requests.request("POST", API_URL, headers=headers, data=data)
    return json.loads(response.content.decode("utf-8"))
data = query(
    {
        "inputs": "\u041C\u0435\u043D\u044F \u0437\u043E\u0432\u0443\u0442 \u0412\u043E\u043B\u044C\u0444\u0433\u0430\u043D\u0433 \u0438 \u044F \u0436\u0438\u0432\u0443 \u0432 \u0411\u0435\u0440\u043B\u0438\u043D\u0435",
    }
)
# Response
self.assertEqual(
    data,
    [
        {
            "translation_text": "My name is Wolfgang and I live in Berlin.",
        },
    ],
)`,highlighted:`<span class="hljs-keyword">import</span> json
<span class="hljs-keyword">import</span> requests
headers = {<span class="hljs-string">&quot;Authorization&quot;</span>: <span class="hljs-string">f&quot;Bearer <span class="hljs-subst">{API_TOKEN}</span>&quot;</span>}
API_URL = <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/Helsinki-NLP/opus-mt-ru-en&quot;</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">payload</span>):
    data = json.dumps(payload)
    response = requests.request(<span class="hljs-string">&quot;POST&quot;</span>, API_URL, headers=headers, data=data)
    <span class="hljs-keyword">return</span> json.loads(response.content.decode(<span class="hljs-string">&quot;utf-8&quot;</span>))
data = query(
    {
        <span class="hljs-string">&quot;inputs&quot;</span>: <span class="hljs-string">&quot;\u041C\u0435\u043D\u044F \u0437\u043E\u0432\u0443\u0442 \u0412\u043E\u043B\u044C\u0444\u0433\u0430\u043D\u0433 \u0438 \u044F \u0436\u0438\u0432\u0443 \u0432 \u0411\u0435\u0440\u043B\u0438\u043D\u0435&quot;</span>,
    }
)
<span class="hljs-comment"># Response</span>
self.assertEqual(
    data,
    [
        {
            <span class="hljs-string">&quot;translation_text&quot;</span>: <span class="hljs-string">&quot;My name is Wolfgang and I live in Berlin.&quot;</span>,
        },
    ],
)`}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p:O,i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function XJ(q){let r,c;return r=new P({props:{$$slots:{default:[VJ]},$$scope:{ctx:q}}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),r.$set($)},i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function QJ(q){let r,c;return r=new R({props:{code:`import fetch from "node-fetch";
async function query(data) {
    const response = await fetch(
        "https://api-inference.huggingface.co/models/Helsinki-NLP/opus-mt-ru-en",
        {
            headers: { Authorization: \`Bearer \${API_TOKEN}\` },
            method: "POST",
            body: JSON.stringify(data),
        }
    );
    const result = await response.json();
    return result;
}
query({inputs: "\u041C\u0435\u043D\u044F \u0437\u043E\u0432\u0443\u0442 \u0412\u043E\u043B\u044C\u0444\u0433\u0430\u043D\u0433 \u0438 \u044F \u0436\u0438\u0432\u0443 \u0432 \u0411\u0435\u0440\u043B\u0438\u043D\u0435"}).then((response) => {
    console.log(JSON.stringify(response));
});
// [{"translation_text":"My name is Wolfgang and I live in Berlin."}]`,highlighted:`<span class="hljs-keyword">import</span> fetch <span class="hljs-keyword">from</span> <span class="hljs-string">&quot;node-fetch&quot;</span>;
<span class="hljs-keyword">async</span> <span class="hljs-keyword">function</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">data</span>) {
    <span class="hljs-keyword">const</span> response = <span class="hljs-keyword">await</span> <span class="hljs-title function_">fetch</span>(
        <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/Helsinki-NLP/opus-mt-ru-en&quot;</span>,
        {
            <span class="hljs-attr">headers</span>: { <span class="hljs-title class_">Authorization</span>: <span class="hljs-string">\`Bearer <span class="hljs-subst">\${API_TOKEN}</span>\`</span> },
            <span class="hljs-attr">method</span>: <span class="hljs-string">&quot;POST&quot;</span>,
            <span class="hljs-attr">body</span>: <span class="hljs-title class_">JSON</span>.<span class="hljs-title function_">stringify</span>(data),
        }
    );
    <span class="hljs-keyword">const</span> result = <span class="hljs-keyword">await</span> response.<span class="hljs-title function_">json</span>();
    <span class="hljs-keyword">return</span> result;
}
<span class="hljs-title function_">query</span>({<span class="hljs-attr">inputs</span>: <span class="hljs-string">&quot;\u041C\u0435\u043D\u044F \u0437\u043E\u0432\u0443\u0442 \u0412\u043E\u043B\u044C\u0444\u0433\u0430\u043D\u0433 \u0438 \u044F \u0436\u0438\u0432\u0443 \u0432 \u0411\u0435\u0440\u043B\u0438\u043D\u0435&quot;</span>}).<span class="hljs-title function_">then</span>(<span class="hljs-function">(<span class="hljs-params">response</span>) =&gt;</span> {
    <span class="hljs-variable language_">console</span>.<span class="hljs-title function_">log</span>(<span class="hljs-title class_">JSON</span>.<span class="hljs-title function_">stringify</span>(response));
});
<span class="hljs-comment">// [{&quot;translation_text&quot;:&quot;My name is Wolfgang and I live in Berlin.&quot;}]</span>`}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p:O,i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function ZJ(q){let r,c;return r=new P({props:{$$slots:{default:[QJ]},$$scope:{ctx:q}}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),r.$set($)},i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function eW(q){let r,c;return r=new R({props:{code:`curl https://api-inference.huggingface.co/models/Helsinki-NLP/opus-mt-ru-en \\
        -X POST \\
        -d '{"inputs": "\u041C\u0435\u043D\u044F \u0437\u043E\u0432\u0443\u0442 \u0412\u043E\u043B\u044C\u0444\u0433\u0430\u043D\u0433 \u0438 \u044F \u0436\u0438\u0432\u0443 \u0432 \u0411\u0435\u0440\u043B\u0438\u043D\u0435"}' \\
        -H "Authorization: Bearer \${HF_API_TOKEN}"
# [{"translation_text":"My name is Wolfgang and I live in Berlin."}]`,highlighted:`curl https://api-inference.huggingface.co/models/Helsinki-NLP/opus-mt-ru-en \\
        -X POST \\
        -d <span class="hljs-string">&#x27;{&quot;inputs&quot;: &quot;\u041C\u0435\u043D\u044F \u0437\u043E\u0432\u0443\u0442 \u0412\u043E\u043B\u044C\u0444\u0433\u0430\u043D\u0433 \u0438 \u044F \u0436\u0438\u0432\u0443 \u0432 \u0411\u0435\u0440\u043B\u0438\u043D\u0435&quot;}&#x27;</span> \\
        -H <span class="hljs-string">&quot;Authorization: Bearer <span class="hljs-variable">\${HF_API_TOKEN}</span>&quot;</span>
<span class="hljs-comment"># [{&quot;translation_text&quot;:&quot;My name is Wolfgang and I live in Berlin.&quot;}]</span>`}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p:O,i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function tW(q){let r,c;return r=new P({props:{$$slots:{default:[eW]},$$scope:{ctx:q}}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),r.$set($)},i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function sW(q){let r,c,s,d,$,k,A;return{c(){r=n("p"),c=n("strong"),s=i("Recommended model"),d=i(`:
`),$=n("a"),k=i("facebook/bart-large-mnli"),A=i("."),this.h()},l(j){r=o(j,"P",{});var T=l(r);c=o(T,"STRONG",{});var S=l(c);s=u(S,"Recommended model"),S.forEach(t),d=u(T,`:
`),$=o(T,"A",{href:!0,rel:!0});var D=l($);k=u(D,"facebook/bart-large-mnli"),D.forEach(t),A=u(T,"."),T.forEach(t),this.h()},h(){f($,"href","https://huggingface.co/facebook/bart-large-mnli"),f($,"rel","nofollow")},m(j,T){m(j,r,T),e(r,c),e(c,s),e(r,d),e(r,$),e($,k),e(r,A)},d(j){j&&t(r)}}}function aW(q){let r,c;return r=new R({props:{code:`import json
import requests
headers = {"Authorization": f"Bearer {API_TOKEN}"}
API_URL = "https://api-inference.huggingface.co/models/facebook/bart-large-mnli"
def query(payload):
    data = json.dumps(payload)
    response = requests.request("POST", API_URL, headers=headers, data=data)
    return json.loads(response.content.decode("utf-8"))
data = query(
    {
        "inputs": "Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!",
        "parameters": {"candidate_labels": ["refund", "legal", "faq"]},
    }
)`,highlighted:`<span class="hljs-keyword">import</span> json
<span class="hljs-keyword">import</span> requests
headers = {<span class="hljs-string">&quot;Authorization&quot;</span>: <span class="hljs-string">f&quot;Bearer <span class="hljs-subst">{API_TOKEN}</span>&quot;</span>}
API_URL = <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/facebook/bart-large-mnli&quot;</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">payload</span>):
    data = json.dumps(payload)
    response = requests.request(<span class="hljs-string">&quot;POST&quot;</span>, API_URL, headers=headers, data=data)
    <span class="hljs-keyword">return</span> json.loads(response.content.decode(<span class="hljs-string">&quot;utf-8&quot;</span>))
data = query(
    {
        <span class="hljs-string">&quot;inputs&quot;</span>: <span class="hljs-string">&quot;Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!&quot;</span>,
        <span class="hljs-string">&quot;parameters&quot;</span>: {<span class="hljs-string">&quot;candidate_labels&quot;</span>: [<span class="hljs-string">&quot;refund&quot;</span>, <span class="hljs-string">&quot;legal&quot;</span>, <span class="hljs-string">&quot;faq&quot;</span>]},
    }
)`}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p:O,i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function rW(q){let r,c;return r=new P({props:{$$slots:{default:[aW]},$$scope:{ctx:q}}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),r.$set($)},i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function nW(q){let r,c;return r=new R({props:{code:`import fetch from "node-fetch";
async function query(data) {
    const response = await fetch(
        "https://api-inference.huggingface.co/models/facebook/bart-large-mnli",
        {
            headers: { Authorization: \`Bearer \${API_TOKEN}\` },
            method: "POST",
            body: JSON.stringify(data),
        }
    );
    const result = await response.json();
    return result;
}
query({inputs: "Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!", parameters: {candidate_labels: ["refund", "legal", "faq"]}}).then((response) => {
    console.log(JSON.stringify(response));
});
// {"sequence":"Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!","labels":["refund","faq","legal"],"scores":[0.8778, 0.1052, 0.017]}`,highlighted:`<span class="hljs-keyword">import</span> fetch <span class="hljs-keyword">from</span> <span class="hljs-string">&quot;node-fetch&quot;</span>;
<span class="hljs-keyword">async</span> <span class="hljs-keyword">function</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">data</span>) {
    <span class="hljs-keyword">const</span> response = <span class="hljs-keyword">await</span> <span class="hljs-title function_">fetch</span>(
        <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/facebook/bart-large-mnli&quot;</span>,
        {
            <span class="hljs-attr">headers</span>: { <span class="hljs-title class_">Authorization</span>: <span class="hljs-string">\`Bearer <span class="hljs-subst">\${API_TOKEN}</span>\`</span> },
            <span class="hljs-attr">method</span>: <span class="hljs-string">&quot;POST&quot;</span>,
            <span class="hljs-attr">body</span>: <span class="hljs-title class_">JSON</span>.<span class="hljs-title function_">stringify</span>(data),
        }
    );
    <span class="hljs-keyword">const</span> result = <span class="hljs-keyword">await</span> response.<span class="hljs-title function_">json</span>();
    <span class="hljs-keyword">return</span> result;
}
<span class="hljs-title function_">query</span>({<span class="hljs-attr">inputs</span>: <span class="hljs-string">&quot;Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!&quot;</span>, <span class="hljs-attr">parameters</span>: {<span class="hljs-attr">candidate_labels</span>: [<span class="hljs-string">&quot;refund&quot;</span>, <span class="hljs-string">&quot;legal&quot;</span>, <span class="hljs-string">&quot;faq&quot;</span>]}}).<span class="hljs-title function_">then</span>(<span class="hljs-function">(<span class="hljs-params">response</span>) =&gt;</span> {
    <span class="hljs-variable language_">console</span>.<span class="hljs-title function_">log</span>(<span class="hljs-title class_">JSON</span>.<span class="hljs-title function_">stringify</span>(response));
});
<span class="hljs-comment">// {&quot;sequence&quot;:&quot;Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!&quot;,&quot;labels&quot;:[&quot;refund&quot;,&quot;faq&quot;,&quot;legal&quot;],&quot;scores&quot;:[0.8778, 0.1052, 0.017]}</span>`}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p:O,i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function oW(q){let r,c;return r=new P({props:{$$slots:{default:[nW]},$$scope:{ctx:q}}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),r.$set($)},i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function lW(q){let r,c;return r=new R({props:{code:`curl https://api-inference.huggingface.co/models/facebook/bart-large-mnli \\
        -X POST \\
        -d '{"inputs": "Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!", "parameters": {"candidate_labels": ["refund", "legal", "faq"]}}' \\
        -H "Authorization: Bearer \${HF_API_TOKEN}"
# {"sequence":"Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!","labels":["refund","faq","legal"],"scores":[0.8778, 0.1052, 0.017]}`,highlighted:`curl https://api-inference.huggingface.co/models/facebook/bart-large-mnli \\
        -X POST \\
        -d <span class="hljs-string">&#x27;{&quot;inputs&quot;: &quot;Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!&quot;, &quot;parameters&quot;: {&quot;candidate_labels&quot;: [&quot;refund&quot;, &quot;legal&quot;, &quot;faq&quot;]}}&#x27;</span> \\
        -H <span class="hljs-string">&quot;Authorization: Bearer <span class="hljs-variable">\${HF_API_TOKEN}</span>&quot;</span>
<span class="hljs-comment"># {&quot;sequence&quot;:&quot;Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!&quot;,&quot;labels&quot;:[&quot;refund&quot;,&quot;faq&quot;,&quot;legal&quot;],&quot;scores&quot;:[0.8778, 0.1052, 0.017]}</span>`}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p:O,i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function iW(q){let r,c;return r=new P({props:{$$slots:{default:[lW]},$$scope:{ctx:q}}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),r.$set($)},i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function uW(q){let r,c;return r=new R({props:{code:`self.assertEqual(
    deep_round(data),
    {
        "sequence": "Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!",
        "labels": ["refund", "faq", "legal"],
        "scores": [
            # 88% refund
            0.8778,
            0.1052,
            0.017,
        ],
    },
)`,highlighted:`self.assertEqual(
    deep_round(data),
    {
        <span class="hljs-string">&quot;sequence&quot;</span>: <span class="hljs-string">&quot;Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!&quot;</span>,
        <span class="hljs-string">&quot;labels&quot;</span>: [<span class="hljs-string">&quot;refund&quot;</span>, <span class="hljs-string">&quot;faq&quot;</span>, <span class="hljs-string">&quot;legal&quot;</span>],
        <span class="hljs-string">&quot;scores&quot;</span>: [
            <span class="hljs-comment"># 88% refund</span>
            <span class="hljs-number">0.8778</span>,
            <span class="hljs-number">0.1052</span>,
            <span class="hljs-number">0.017</span>,
        ],
    },
)`}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p:O,i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function cW(q){let r,c;return r=new P({props:{$$slots:{default:[uW]},$$scope:{ctx:q}}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),r.$set($)},i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function fW(q){let r,c,s,d,$,k,A;return{c(){r=n("p"),c=n("strong"),s=i("Recommended model"),d=i(`:
`),$=n("a"),k=i("microsoft/DialoGPT-large"),A=i("."),this.h()},l(j){r=o(j,"P",{});var T=l(r);c=o(T,"STRONG",{});var S=l(c);s=u(S,"Recommended model"),S.forEach(t),d=u(T,`:
`),$=o(T,"A",{href:!0,rel:!0});var D=l($);k=u(D,"microsoft/DialoGPT-large"),D.forEach(t),A=u(T,"."),T.forEach(t),this.h()},h(){f($,"href","https://huggingface.co/microsoft/DialoGPT-large"),f($,"rel","nofollow")},m(j,T){m(j,r,T),e(r,c),e(c,s),e(r,d),e(r,$),e($,k),e(r,A)},d(j){j&&t(r)}}}function pW(q){let r,c;return r=new R({props:{code:`import json
import requests
headers = {"Authorization": f"Bearer {API_TOKEN}"}
API_URL = "https://api-inference.huggingface.co/models/microsoft/DialoGPT-large"
def query(payload):
    data = json.dumps(payload)
    response = requests.request("POST", API_URL, headers=headers, data=data)
    return json.loads(response.content.decode("utf-8"))
data = query(
    {
        "inputs": {
            "past_user_inputs": ["Which movie is the best ?"],
            "generated_responses": ["It's Die Hard for sure."],
            "text": "Can you explain why ?",
        },
    }
)
# Response
self.assertEqual(
    data,
    {
        "generated_text": "It's the best movie ever.",
        "conversation": {
            "past_user_inputs": [
                "Which movie is the best ?",
                "Can you explain why ?",
            ],
            "generated_responses": [
                "It's Die Hard for sure.",
                "It's the best movie ever.",
            ],
        },
        "warnings": ["Setting \`pad_token_id\` to \`eos_token_id\`:50256 for open-end generation."],
    },
)`,highlighted:`<span class="hljs-keyword">import</span> json
<span class="hljs-keyword">import</span> requests
headers = {<span class="hljs-string">&quot;Authorization&quot;</span>: <span class="hljs-string">f&quot;Bearer <span class="hljs-subst">{API_TOKEN}</span>&quot;</span>}
API_URL = <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/microsoft/DialoGPT-large&quot;</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">payload</span>):
    data = json.dumps(payload)
    response = requests.request(<span class="hljs-string">&quot;POST&quot;</span>, API_URL, headers=headers, data=data)
    <span class="hljs-keyword">return</span> json.loads(response.content.decode(<span class="hljs-string">&quot;utf-8&quot;</span>))
data = query(
    {
        <span class="hljs-string">&quot;inputs&quot;</span>: {
            <span class="hljs-string">&quot;past_user_inputs&quot;</span>: [<span class="hljs-string">&quot;Which movie is the best ?&quot;</span>],
            <span class="hljs-string">&quot;generated_responses&quot;</span>: [<span class="hljs-string">&quot;It&#x27;s Die Hard for sure.&quot;</span>],
            <span class="hljs-string">&quot;text&quot;</span>: <span class="hljs-string">&quot;Can you explain why ?&quot;</span>,
        },
    }
)
<span class="hljs-comment"># Response</span>
self.assertEqual(
    data,
    {
        <span class="hljs-string">&quot;generated_text&quot;</span>: <span class="hljs-string">&quot;It&#x27;s the best movie ever.&quot;</span>,
        <span class="hljs-string">&quot;conversation&quot;</span>: {
            <span class="hljs-string">&quot;past_user_inputs&quot;</span>: [
                <span class="hljs-string">&quot;Which movie is the best ?&quot;</span>,
                <span class="hljs-string">&quot;Can you explain why ?&quot;</span>,
            ],
            <span class="hljs-string">&quot;generated_responses&quot;</span>: [
                <span class="hljs-string">&quot;It&#x27;s Die Hard for sure.&quot;</span>,
                <span class="hljs-string">&quot;It&#x27;s the best movie ever.&quot;</span>,
            ],
        },
        <span class="hljs-string">&quot;warnings&quot;</span>: [<span class="hljs-string">&quot;Setting \`pad_token_id\` to \`eos_token_id\`:50256 for open-end generation.&quot;</span>],
    },
)`}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p:O,i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function hW(q){let r,c;return r=new P({props:{$$slots:{default:[pW]},$$scope:{ctx:q}}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),r.$set($)},i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function dW(q){let r,c;return r=new R({props:{code:`import fetch from "node-fetch";
async function query(data) {
    const response = await fetch(
        "https://api-inference.huggingface.co/models/microsoft/DialoGPT-large",
        {
            headers: { Authorization: \`Bearer \${API_TOKEN}\` },
            method: "POST",
            body: JSON.stringify(data),
        }
    );
    const result = await response.json();
    return result;
}
query({inputs: {past_user_inputs: ["Which movie is the best ?"], generated_responses: ["It is Die Hard for sure."], text:"Can you explain why ?"}}).then((response) => {
    console.log(JSON.stringify(response));
});
// {"generated_text":"It's the best movie ever.","conversation":{"past_user_inputs":["Which movie is the best ?","Can you explain why ?"],"generated_responses":["It is Die Hard for sure.","It's the best movie ever."]},"warnings":["Setting \`pad_token_id\` to \`eos_token_id\`:50256 for open-end generation."]}`,highlighted:`<span class="hljs-keyword">import</span> <span class="hljs-keyword">fetch</span> <span class="hljs-keyword">from</span> &quot;node-fetch&quot;;
async <span class="hljs-keyword">function</span> query(data) {
    const response = await <span class="hljs-keyword">fetch</span>(
        &quot;https://api-inference.huggingface.co/models/microsoft/DialoGPT-large&quot;,
        {
            headers: { <span class="hljs-keyword">Authorization</span>: \`Bearer \${API_TOKEN}\` },
            <span class="hljs-keyword">method</span>: &quot;POST&quot;,
            body: <span class="hljs-type">JSON</span>.stringify(data),
        }
    );
    const result = await response.json();
    <span class="hljs-keyword">return</span> result;
}
query({inputs: {past_user_inputs: [&quot;Which movie is the best ?&quot;], generated_responses: [&quot;It is Die Hard for sure.&quot;], <span class="hljs-type">text</span>:&quot;Can you explain why ?&quot;}}).<span class="hljs-keyword">then</span>((response) =&gt; {
    console.log(<span class="hljs-type">JSON</span>.stringify(response));
});
// {&quot;generated_text&quot;:&quot;It&#x27;s the best movie ever.&quot;,&quot;conversation&quot;:{&quot;past_user_inputs&quot;:[&quot;Which movie is the best ?&quot;,&quot;Can you explain why ?&quot;],&quot;generated_responses&quot;:[&quot;It is Die Hard for sure.&quot;,&quot;It&#x27;s the best movie ever.&quot;]},&quot;warnings&quot;:[&quot;Setting \`pad_token_id\` to \`eos_token_id\`:50256 for open-end generation.&quot;]}`}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p:O,i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function gW(q){let r,c;return r=new P({props:{$$slots:{default:[dW]},$$scope:{ctx:q}}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),r.$set($)},i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function mW(q){let r,c;return r=new R({props:{code:`curl https://api-inference.huggingface.co/models/microsoft/DialoGPT-large \\
        -X POST \\
        -d '{"inputs": {"past_user_inputs": ["Which movie is the best ?"], "generated_responses": ["It is Die Hard for sure."], "text":"Can you explain why ?"}}' \\
        -H "Authorization: Bearer \${HF_API_TOKEN}"
# {"generated_text":"It's the best movie ever.","conversation":{"past_user_inputs":["Which movie is the best ?","Can you explain why ?"],"generated_responses":["It is Die Hard for sure.","It's the best movie ever."]},"warnings":["Setting \`pad_token_id\` to \`eos_token_id\`:50256 for open-end generation."]}`,highlighted:'curl https://api-inference.huggingface.co/models/microsoft/DialoGPT-large \\\n        -X POST \\\n        -d <span class="hljs-string">&#x27;{&quot;inputs&quot;: {&quot;past_user_inputs&quot;: [&quot;Which movie is the best ?&quot;], &quot;generated_responses&quot;: [&quot;It is Die Hard for sure.&quot;], &quot;text&quot;:&quot;Can you explain why ?&quot;}}&#x27;</span> \\\n        -H <span class="hljs-string">&quot;Authorization: Bearer <span class="hljs-variable">${HF_API_TOKEN}</span>&quot;</span>\n<span class="hljs-comment"># {&quot;generated_text&quot;:&quot;It&#x27;s the best movie ever.&quot;,&quot;conversation&quot;:{&quot;past_user_inputs&quot;:[&quot;Which movie is the best ?&quot;,&quot;Can you explain why ?&quot;],&quot;generated_responses&quot;:[&quot;It is Die Hard for sure.&quot;,&quot;It&#x27;s the best movie ever.&quot;]},&quot;warnings&quot;:[&quot;Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.&quot;]}</span>'}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p:O,i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function $W(q){let r,c;return r=new P({props:{$$slots:{default:[mW]},$$scope:{ctx:q}}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),r.$set($)},i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function qW(q){let r,c,s,d,$,k,A;return{c(){r=n("p"),c=n("strong"),s=i("Recommended model"),d=i(`:
`),$=n("a"),k=i("Sentence-transformers"),A=i("."),this.h()},l(j){r=o(j,"P",{});var T=l(r);c=o(T,"STRONG",{});var S=l(c);s=u(S,"Recommended model"),S.forEach(t),d=u(T,`:
`),$=o(T,"A",{href:!0,rel:!0});var D=l($);k=u(D,"Sentence-transformers"),D.forEach(t),A=u(T,"."),T.forEach(t),this.h()},h(){f($,"href","https://huggingface.co/sentence-transformers/paraphrase-xlm-r-multilingual-v1"),f($,"rel","nofollow")},m(j,T){m(j,r,T),e(r,c),e(c,s),e(r,d),e(r,$),e($,k),e(r,A)},d(j){j&&t(r)}}}function _W(q){let r,c,s,d,$,k,A;return{c(){r=n("p"),c=n("strong"),s=i("Recommended model"),d=i(": "),$=n("a"),k=i(`Check your
langage`),A=i("."),this.h()},l(j){r=o(j,"P",{});var T=l(r);c=o(T,"STRONG",{});var S=l(c);s=u(S,"Recommended model"),S.forEach(t),d=u(T,": "),$=o(T,"A",{href:!0,rel:!0});var D=l($);k=u(D,`Check your
langage`),D.forEach(t),A=u(T,"."),T.forEach(t),this.h()},h(){f($,"href","https://huggingface.co/models?pipeline_tag=automatic-speech-recognition"),f($,"rel","nofollow")},m(j,T){m(j,r,T),e(r,c),e(c,s),e(r,d),e(r,$),e($,k),e(r,A)},d(j){j&&t(r)}}}function vW(q){let r,c,s,d,$,k,A;return{c(){r=n("p"),c=n("strong"),s=i("English"),d=i(`:
`),$=n("a"),k=i("facebook/wav2vec2-large-960h-lv60-self"),A=i("."),this.h()},l(j){r=o(j,"P",{});var T=l(r);c=o(T,"STRONG",{});var S=l(c);s=u(S,"English"),S.forEach(t),d=u(T,`:
`),$=o(T,"A",{href:!0,rel:!0});var D=l($);k=u(D,"facebook/wav2vec2-large-960h-lv60-self"),D.forEach(t),A=u(T,"."),T.forEach(t),this.h()},h(){f($,"href","https://huggingface.co/facebook/wav2vec2-large-960h-lv60-self"),f($,"rel","nofollow")},m(j,T){m(j,r,T),e(r,c),e(c,s),e(r,d),e(r,$),e($,k),e(r,A)},d(j){j&&t(r)}}}function EW(q){let r,c;return r=new R({props:{code:`import json
import requests
headers = {"Authorization": f"Bearer {API_TOKEN}"}
API_URL = "https://api-inference.huggingface.co/models/facebook/wav2vec2-base-960h"
def query(filename):
    with open(filename, "rb") as f:
        data = f.read()
    response = requests.request("POST", API_URL, headers=headers, data=data)
    return json.loads(response.content.decode("utf-8"))
data = query("sample1.flac")`,highlighted:`<span class="hljs-keyword">import</span> json
<span class="hljs-keyword">import</span> requests
headers = {<span class="hljs-string">&quot;Authorization&quot;</span>: <span class="hljs-string">f&quot;Bearer <span class="hljs-subst">{API_TOKEN}</span>&quot;</span>}
API_URL = <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/facebook/wav2vec2-base-960h&quot;</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">filename</span>):
    <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(filename, <span class="hljs-string">&quot;rb&quot;</span>) <span class="hljs-keyword">as</span> f:
        data = f.read()
    response = requests.request(<span class="hljs-string">&quot;POST&quot;</span>, API_URL, headers=headers, data=data)
    <span class="hljs-keyword">return</span> json.loads(response.content.decode(<span class="hljs-string">&quot;utf-8&quot;</span>))
data = query(<span class="hljs-string">&quot;sample1.flac&quot;</span>)`}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p:O,i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function yW(q){let r,c;return r=new P({props:{$$slots:{default:[EW]},$$scope:{ctx:q}}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),r.$set($)},i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function wW(q){let r,c;return r=new R({props:{code:`import fetch from "node-fetch";
import fs from "fs";
async function query(filename) {
    const data = fs.readFileSync(filename);
    const response = await fetch(
        "https://api-inference.huggingface.co/models/facebook/wav2vec2-base-960h",
        {
            headers: { Authorization: \`Bearer \${API_TOKEN}\` },
            method: "POST",
            body: data,
        }
    );
    const result = await response.json();
    return result;
}
query("sample1.flac").then((response) => {
    console.log(JSON.stringify(response));
});
// {"text":"GOING ALONG SLUSHY COUNTRY ROADS AND SPEAKING TO DAMP AUDIENCES IN DRAUGHTY SCHOOL ROOMS DAY AFTER DAY FOR A FORTNIGHT HE'LL HAVE TO PUT IN AN APPEARANCE AT SOME PLACE OF WORSHIP ON SUNDAY MORNING AND HE CAN COME TO US IMMEDIATELY AFTERWARDS"}`,highlighted:`<span class="hljs-keyword">import</span> fetch <span class="hljs-keyword">from</span> <span class="hljs-string">&quot;node-fetch&quot;</span>;
<span class="hljs-keyword">import</span> fs <span class="hljs-keyword">from</span> <span class="hljs-string">&quot;fs&quot;</span>;
<span class="hljs-keyword">async</span> <span class="hljs-keyword">function</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">filename</span>) {
    <span class="hljs-keyword">const</span> data = fs.<span class="hljs-title function_">readFileSync</span>(filename);
    <span class="hljs-keyword">const</span> response = <span class="hljs-keyword">await</span> <span class="hljs-title function_">fetch</span>(
        <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/facebook/wav2vec2-base-960h&quot;</span>,
        {
            <span class="hljs-attr">headers</span>: { <span class="hljs-title class_">Authorization</span>: <span class="hljs-string">\`Bearer <span class="hljs-subst">\${API_TOKEN}</span>\`</span> },
            <span class="hljs-attr">method</span>: <span class="hljs-string">&quot;POST&quot;</span>,
            <span class="hljs-attr">body</span>: data,
        }
    );
    <span class="hljs-keyword">const</span> result = <span class="hljs-keyword">await</span> response.<span class="hljs-title function_">json</span>();
    <span class="hljs-keyword">return</span> result;
}
<span class="hljs-title function_">query</span>(<span class="hljs-string">&quot;sample1.flac&quot;</span>).<span class="hljs-title function_">then</span>(<span class="hljs-function">(<span class="hljs-params">response</span>) =&gt;</span> {
    <span class="hljs-variable language_">console</span>.<span class="hljs-title function_">log</span>(<span class="hljs-title class_">JSON</span>.<span class="hljs-title function_">stringify</span>(response));
});
<span class="hljs-comment">// {&quot;text&quot;:&quot;GOING ALONG SLUSHY COUNTRY ROADS AND SPEAKING TO DAMP AUDIENCES IN DRAUGHTY SCHOOL ROOMS DAY AFTER DAY FOR A FORTNIGHT HE&#x27;LL HAVE TO PUT IN AN APPEARANCE AT SOME PLACE OF WORSHIP ON SUNDAY MORNING AND HE CAN COME TO US IMMEDIATELY AFTERWARDS&quot;}</span>`}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p:O,i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function bW(q){let r,c;return r=new P({props:{$$slots:{default:[wW]},$$scope:{ctx:q}}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),r.$set($)},i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function TW(q){let r,c;return r=new R({props:{code:`curl https://api-inference.huggingface.co/models/facebook/wav2vec2-base-960h \\
        -X POST \\
        --data-binary '@sample1.flac' \\
        -H "Authorization: Bearer \${HF_API_TOKEN}"
# {"text":"GOING ALONG SLUSHY COUNTRY ROADS AND SPEAKING TO DAMP AUDIENCES IN DRAUGHTY SCHOOL ROOMS DAY AFTER DAY FOR A FORTNIGHT HE'LL HAVE TO PUT IN AN APPEARANCE AT SOME PLACE OF WORSHIP ON SUNDAY MORNING AND HE CAN COME TO US IMMEDIATELY AFTERWARDS"}`,highlighted:`curl https://api-inference.huggingface.co/models/facebook/wav2vec2-base-960h \\
        -X POST \\
        --data-binary <span class="hljs-string">&#x27;@sample1.flac&#x27;</span> \\
        -H <span class="hljs-string">&quot;Authorization: Bearer <span class="hljs-variable">\${HF_API_TOKEN}</span>&quot;</span>
<span class="hljs-comment"># {&quot;text&quot;:&quot;GOING ALONG SLUSHY COUNTRY ROADS AND SPEAKING TO DAMP AUDIENCES IN DRAUGHTY SCHOOL ROOMS DAY AFTER DAY FOR A FORTNIGHT HE&#x27;LL HAVE TO PUT IN AN APPEARANCE AT SOME PLACE OF WORSHIP ON SUNDAY MORNING AND HE CAN COME TO US IMMEDIATELY AFTERWARDS&quot;}</span>`}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p:O,i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function jW(q){let r,c;return r=new P({props:{$$slots:{default:[TW]},$$scope:{ctx:q}}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),r.$set($)},i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function kW(q){let r,c;return r=new R({props:{code:`self.assertEqual(
    data,
    {
        "text": "GOING ALONG SLUSHY COUNTRY ROADS AND SPEAKING TO DAMP AUDIENCES IN DRAUGHTY SCHOOL ROOMS DAY AFTER DAY FOR A FORTNIGHT HE'LL HAVE TO PUT IN AN APPEARANCE AT SOME PLACE OF WORSHIP ON SUNDAY MORNING AND HE CAN COME TO US IMMEDIATELY AFTERWARDS"
    },
)`,highlighted:`self.assertEqual(
    data,
    {
        <span class="hljs-string">&quot;text&quot;</span>: <span class="hljs-string">&quot;GOING ALONG SLUSHY COUNTRY ROADS AND SPEAKING TO DAMP AUDIENCES IN DRAUGHTY SCHOOL ROOMS DAY AFTER DAY FOR A FORTNIGHT HE&#x27;LL HAVE TO PUT IN AN APPEARANCE AT SOME PLACE OF WORSHIP ON SUNDAY MORNING AND HE CAN COME TO US IMMEDIATELY AFTERWARDS&quot;</span>
    },
)`}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p:O,i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function AW(q){let r,c;return r=new P({props:{$$slots:{default:[kW]},$$scope:{ctx:q}}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),r.$set($)},i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function DW(q){let r,c,s,d,$,k,A;return{c(){r=n("p"),c=n("strong"),s=i("Recommended model"),d=i(`:
`),$=n("a"),k=i("superb/hubert-large-superb-er"),A=i("."),this.h()},l(j){r=o(j,"P",{});var T=l(r);c=o(T,"STRONG",{});var S=l(c);s=u(S,"Recommended model"),S.forEach(t),d=u(T,`:
`),$=o(T,"A",{href:!0,rel:!0});var D=l($);k=u(D,"superb/hubert-large-superb-er"),D.forEach(t),A=u(T,"."),T.forEach(t),this.h()},h(){f($,"href","https://huggingface.co/superb/hubert-large-superb-er"),f($,"rel","nofollow")},m(j,T){m(j,r,T),e(r,c),e(c,s),e(r,d),e(r,$),e($,k),e(r,A)},d(j){j&&t(r)}}}function OW(q){let r,c;return r=new R({props:{code:`import json
import requests
headers = {"Authorization": f"Bearer {API_TOKEN}"}
API_URL = "https://api-inference.huggingface.co/models/superb/hubert-large-superb-er"
def query(filename):
    with open(filename, "rb") as f:
        data = f.read()
    response = requests.request("POST", API_URL, headers=headers, data=data)
    return json.loads(response.content.decode("utf-8"))
data = query("sample1.flac")`,highlighted:`<span class="hljs-keyword">import</span> json
<span class="hljs-keyword">import</span> requests
headers = {<span class="hljs-string">&quot;Authorization&quot;</span>: <span class="hljs-string">f&quot;Bearer <span class="hljs-subst">{API_TOKEN}</span>&quot;</span>}
API_URL = <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/superb/hubert-large-superb-er&quot;</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">filename</span>):
    <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(filename, <span class="hljs-string">&quot;rb&quot;</span>) <span class="hljs-keyword">as</span> f:
        data = f.read()
    response = requests.request(<span class="hljs-string">&quot;POST&quot;</span>, API_URL, headers=headers, data=data)
    <span class="hljs-keyword">return</span> json.loads(response.content.decode(<span class="hljs-string">&quot;utf-8&quot;</span>))
data = query(<span class="hljs-string">&quot;sample1.flac&quot;</span>)`}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p:O,i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function PW(q){let r,c;return r=new P({props:{$$slots:{default:[OW]},$$scope:{ctx:q}}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),r.$set($)},i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function RW(q){let r,c;return r=new R({props:{code:`import fetch from "node-fetch";
import fs from "fs";
async function query(filename) {
    const data = fs.readFileSync(filename);
    const response = await fetch(
        "https://api-inference.huggingface.co/models/superb/hubert-large-superb-er",
        {
            headers: { Authorization: \`Bearer \${API_TOKEN}\` },
            method: "POST",
            body: data,
        }
    );
    const result = await response.json();
    return result;
}
query("sample1.flac").then((response) => {
    console.log(JSON.stringify(response));
});
// [{"score":0.5927661657333374,"label":"neu"},{"score":0.2002529799938202,"label":"hap"},{"score":0.12795612215995789,"label":"ang"},{"score":0.07902472466230392,"label":"sad"}]`,highlighted:`<span class="hljs-keyword">import</span> <span class="hljs-keyword">fetch</span> <span class="hljs-keyword">from</span> &quot;node-fetch&quot;;
<span class="hljs-keyword">import</span> fs <span class="hljs-keyword">from</span> &quot;fs&quot;;
async <span class="hljs-keyword">function</span> query(filename) {
    const data = fs.readFileSync(filename);
    const response = await <span class="hljs-keyword">fetch</span>(
        &quot;https://api-inference.huggingface.co/models/superb/hubert-large-superb-er&quot;,
        {
            headers: { <span class="hljs-keyword">Authorization</span>: \`Bearer \${API_TOKEN}\` },
            <span class="hljs-keyword">method</span>: &quot;POST&quot;,
            body: data,
        }
    );
    const result = await response.json();
    <span class="hljs-keyword">return</span> result;
}
query(&quot;sample1.flac&quot;).<span class="hljs-keyword">then</span>((response) =&gt; {
    console.log(<span class="hljs-type">JSON</span>.stringify(response));
});
// [{&quot;score&quot;:<span class="hljs-number">0.5927661657333374</span>,&quot;label&quot;:&quot;neu&quot;},{&quot;score&quot;:<span class="hljs-number">0.2002529799938202</span>,&quot;label&quot;:&quot;hap&quot;},{&quot;score&quot;:<span class="hljs-number">0.12795612215995789</span>,&quot;label&quot;:&quot;ang&quot;},{&quot;score&quot;:<span class="hljs-number">0.07902472466230392</span>,&quot;label&quot;:&quot;sad&quot;}]`}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p:O,i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function SW(q){let r,c;return r=new P({props:{$$slots:{default:[RW]},$$scope:{ctx:q}}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),r.$set($)},i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function NW(q){let r,c;return r=new R({props:{code:`curl https://api-inference.huggingface.co/models/superb/hubert-large-superb-er \\
        -X POST \\
        --data-binary '@sample1.flac' \\
        -H "Authorization: Bearer \${HF_API_TOKEN}"
# [{"score":0.5927661657333374,"label":"neu"},{"score":0.2002529799938202,"label":"hap"},{"score":0.12795612215995789,"label":"ang"},{"score":0.07902472466230392,"label":"sad"}]`,highlighted:`curl https://api-inference.huggingface.co/models/superb/hubert-large-superb-er \\
        -X POST \\
        --data-binary <span class="hljs-string">&#x27;@sample1.flac&#x27;</span> \\
        -H <span class="hljs-string">&quot;Authorization: Bearer <span class="hljs-variable">\${HF_API_TOKEN}</span>&quot;</span>
<span class="hljs-comment"># [{&quot;score&quot;:0.5927661657333374,&quot;label&quot;:&quot;neu&quot;},{&quot;score&quot;:0.2002529799938202,&quot;label&quot;:&quot;hap&quot;},{&quot;score&quot;:0.12795612215995789,&quot;label&quot;:&quot;ang&quot;},{&quot;score&quot;:0.07902472466230392,&quot;label&quot;:&quot;sad&quot;}]</span>`}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p:O,i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function xW(q){let r,c;return r=new P({props:{$$slots:{default:[NW]},$$scope:{ctx:q}}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),r.$set($)},i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function IW(q){let r,c;return r=new R({props:{code:`self.assertEqual(
    deep_round(data, 4),
    [
        {"score": 0.5928, "label": "neu"},
        {"score": 0.2003, "label": "hap"},
        {"score": 0.128, "label": "ang"},
        {"score": 0.079, "label": "sad"},
    ],
)`,highlighted:`self.assertEqual(
    deep_round(data, <span class="hljs-number">4</span>),
    [
        {<span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.5928</span>, <span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;neu&quot;</span>},
        {<span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.2003</span>, <span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;hap&quot;</span>},
        {<span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.128</span>, <span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;ang&quot;</span>},
        {<span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.079</span>, <span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;sad&quot;</span>},
    ],
)`}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p:O,i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function HW(q){let r,c;return r=new P({props:{$$slots:{default:[IW]},$$scope:{ctx:q}}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),r.$set($)},i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function BW(q){let r,c,s,d,$,k,A;return{c(){r=n("p"),c=n("strong"),s=i("Recommended model"),d=i(`:
`),$=n("a"),k=i("google/vit-base-patch16-224"),A=i("."),this.h()},l(j){r=o(j,"P",{});var T=l(r);c=o(T,"STRONG",{});var S=l(c);s=u(S,"Recommended model"),S.forEach(t),d=u(T,`:
`),$=o(T,"A",{href:!0,rel:!0});var D=l($);k=u(D,"google/vit-base-patch16-224"),D.forEach(t),A=u(T,"."),T.forEach(t),this.h()},h(){f($,"href","https://huggingface.co/google/vit-base-patch16-224"),f($,"rel","nofollow")},m(j,T){m(j,r,T),e(r,c),e(c,s),e(r,d),e(r,$),e($,k),e(r,A)},d(j){j&&t(r)}}}function CW(q){let r,c;return r=new R({props:{code:`import json
import requests
headers = {"Authorization": f"Bearer {API_TOKEN}"}
API_URL = "https://api-inference.huggingface.co/models/google/vit-base-patch16-224"
def query(filename):
    with open(filename, "rb") as f:
        data = f.read()
    response = requests.request("POST", API_URL, headers=headers, data=data)
    return json.loads(response.content.decode("utf-8"))
data = query("cats.jpg")`,highlighted:`<span class="hljs-keyword">import</span> json
<span class="hljs-keyword">import</span> requests
headers = {<span class="hljs-string">&quot;Authorization&quot;</span>: <span class="hljs-string">f&quot;Bearer <span class="hljs-subst">{API_TOKEN}</span>&quot;</span>}
API_URL = <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/google/vit-base-patch16-224&quot;</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">filename</span>):
    <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(filename, <span class="hljs-string">&quot;rb&quot;</span>) <span class="hljs-keyword">as</span> f:
        data = f.read()
    response = requests.request(<span class="hljs-string">&quot;POST&quot;</span>, API_URL, headers=headers, data=data)
    <span class="hljs-keyword">return</span> json.loads(response.content.decode(<span class="hljs-string">&quot;utf-8&quot;</span>))
data = query(<span class="hljs-string">&quot;cats.jpg&quot;</span>)`}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p:O,i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function GW(q){let r,c;return r=new P({props:{$$slots:{default:[CW]},$$scope:{ctx:q}}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),r.$set($)},i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function LW(q){let r,c;return r=new R({props:{code:`import fetch from "node-fetch";
import fs from "fs";
async function query(filename) {
    const data = fs.readFileSync(filename);
    const response = await fetch(
        "https://api-inference.huggingface.co/models/google/vit-base-patch16-224",
        {
            headers: { Authorization: \`Bearer \${API_TOKEN}\` },
            method: "POST",
            body: data,
        }
    );
    const result = await response.json();
    return result;
}
query("cats.jpg").then((response) => {
    console.log(JSON.stringify(response));
});
// [{"score":0.9374412894248962,"label":"Egyptian cat"},{"score":0.03844260051846504,"label":"tabby, tabby cat"},{"score":0.014411412179470062,"label":"tiger cat"},{"score":0.003274323185905814,"label":"lynx, catamount"},{"score":0.0006795919616706669,"label":"Siamese cat, Siamese"}]`,highlighted:`<span class="hljs-keyword">import</span> <span class="hljs-keyword">fetch</span> <span class="hljs-keyword">from</span> &quot;node-fetch&quot;;
<span class="hljs-keyword">import</span> fs <span class="hljs-keyword">from</span> &quot;fs&quot;;
async <span class="hljs-keyword">function</span> query(filename) {
    const data = fs.readFileSync(filename);
    const response = await <span class="hljs-keyword">fetch</span>(
        &quot;https://api-inference.huggingface.co/models/google/vit-base-patch16-224&quot;,
        {
            headers: { <span class="hljs-keyword">Authorization</span>: \`Bearer \${API_TOKEN}\` },
            <span class="hljs-keyword">method</span>: &quot;POST&quot;,
            body: data,
        }
    );
    const result = await response.json();
    <span class="hljs-keyword">return</span> result;
}
query(&quot;cats.jpg&quot;).<span class="hljs-keyword">then</span>((response) =&gt; {
    console.log(<span class="hljs-type">JSON</span>.stringify(response));
});
// [{&quot;score&quot;:<span class="hljs-number">0.9374412894248962</span>,&quot;label&quot;:&quot;Egyptian cat&quot;},{&quot;score&quot;:<span class="hljs-number">0.03844260051846504</span>,&quot;label&quot;:&quot;tabby, tabby cat&quot;},{&quot;score&quot;:<span class="hljs-number">0.014411412179470062</span>,&quot;label&quot;:&quot;tiger cat&quot;},{&quot;score&quot;:<span class="hljs-number">0.003274323185905814</span>,&quot;label&quot;:&quot;lynx, catamount&quot;},{&quot;score&quot;:<span class="hljs-number">0.0006795919616706669</span>,&quot;label&quot;:&quot;Siamese cat, Siamese&quot;}]`}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p:O,i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function UW(q){let r,c;return r=new P({props:{$$slots:{default:[LW]},$$scope:{ctx:q}}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),r.$set($)},i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function zW(q){let r,c;return r=new R({props:{code:`curl https://api-inference.huggingface.co/models/google/vit-base-patch16-224 \\
        -X POST \\
        --data-binary '@cats.jpg' \\
        -H "Authorization: Bearer \${HF_API_TOKEN}"
# [{"score":0.9374412894248962,"label":"Egyptian cat"},{"score":0.03844260051846504,"label":"tabby, tabby cat"},{"score":0.014411412179470062,"label":"tiger cat"},{"score":0.003274323185905814,"label":"lynx, catamount"},{"score":0.0006795919616706669,"label":"Siamese cat, Siamese"}]`,highlighted:`curl https://api-inference.huggingface.co/models/google/vit-base-patch16-224 \\
        -X POST \\
        --data-binary <span class="hljs-string">&#x27;@cats.jpg&#x27;</span> \\
        -H <span class="hljs-string">&quot;Authorization: Bearer <span class="hljs-variable">\${HF_API_TOKEN}</span>&quot;</span>
<span class="hljs-comment"># [{&quot;score&quot;:0.9374412894248962,&quot;label&quot;:&quot;Egyptian cat&quot;},{&quot;score&quot;:0.03844260051846504,&quot;label&quot;:&quot;tabby, tabby cat&quot;},{&quot;score&quot;:0.014411412179470062,&quot;label&quot;:&quot;tiger cat&quot;},{&quot;score&quot;:0.003274323185905814,&quot;label&quot;:&quot;lynx, catamount&quot;},{&quot;score&quot;:0.0006795919616706669,&quot;label&quot;:&quot;Siamese cat, Siamese&quot;}]</span>`}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p:O,i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function MW(q){let r,c;return r=new P({props:{$$slots:{default:[zW]},$$scope:{ctx:q}}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),r.$set($)},i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function KW(q){let r,c;return r=new R({props:{code:`self.assertEqual(
    deep_round(data, 4),
    [
        {"score": 0.9374, "label": "Egyptian cat"},
        {"score": 0.0384, "label": "tabby, tabby cat"},
        {"score": 0.0144, "label": "tiger cat"},
        {"score": 0.0033, "label": "lynx, catamount"},
        {"score": 0.0007, "label": "Siamese cat, Siamese"},
    ],
)`,highlighted:`self.assertEqual(
    deep_round(data, <span class="hljs-number">4</span>),
    [
        {<span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.9374</span>, <span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;Egyptian cat&quot;</span>},
        {<span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.0384</span>, <span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;tabby, tabby cat&quot;</span>},
        {<span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.0144</span>, <span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;tiger cat&quot;</span>},
        {<span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.0033</span>, <span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;lynx, catamount&quot;</span>},
        {<span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.0007</span>, <span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;Siamese cat, Siamese&quot;</span>},
    ],
)`}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p:O,i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function FW(q){let r,c;return r=new P({props:{$$slots:{default:[KW]},$$scope:{ctx:q}}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),r.$set($)},i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function JW(q){let r,c,s,d,$,k,A;return{c(){r=n("p"),c=n("strong"),s=i("Recommended model"),d=i(`:
`),$=n("a"),k=i("facebook/detr-resnet-50"),A=i("."),this.h()},l(j){r=o(j,"P",{});var T=l(r);c=o(T,"STRONG",{});var S=l(c);s=u(S,"Recommended model"),S.forEach(t),d=u(T,`:
`),$=o(T,"A",{href:!0,rel:!0});var D=l($);k=u(D,"facebook/detr-resnet-50"),D.forEach(t),A=u(T,"."),T.forEach(t),this.h()},h(){f($,"href","https://huggingface.co/facebook/detr-resnet-50"),f($,"rel","nofollow")},m(j,T){m(j,r,T),e(r,c),e(c,s),e(r,d),e(r,$),e($,k),e(r,A)},d(j){j&&t(r)}}}function WW(q){let r,c;return r=new R({props:{code:`import json
import requests
headers = {"Authorization": f"Bearer {API_TOKEN}"}
API_URL = "https://api-inference.huggingface.co/models/facebook/detr-resnet-50"
def query(filename):
    with open(filename, "rb") as f:
        data = f.read()
    response = requests.request("POST", API_URL, headers=headers, data=data)
    return json.loads(response.content.decode("utf-8"))
data = query("cats.jpg")`,highlighted:`<span class="hljs-keyword">import</span> json
<span class="hljs-keyword">import</span> requests
headers = {<span class="hljs-string">&quot;Authorization&quot;</span>: <span class="hljs-string">f&quot;Bearer <span class="hljs-subst">{API_TOKEN}</span>&quot;</span>}
API_URL = <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/facebook/detr-resnet-50&quot;</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">filename</span>):
    <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(filename, <span class="hljs-string">&quot;rb&quot;</span>) <span class="hljs-keyword">as</span> f:
        data = f.read()
    response = requests.request(<span class="hljs-string">&quot;POST&quot;</span>, API_URL, headers=headers, data=data)
    <span class="hljs-keyword">return</span> json.loads(response.content.decode(<span class="hljs-string">&quot;utf-8&quot;</span>))
data = query(<span class="hljs-string">&quot;cats.jpg&quot;</span>)`}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p:O,i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function YW(q){let r,c;return r=new P({props:{$$slots:{default:[WW]},$$scope:{ctx:q}}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),r.$set($)},i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function VW(q){let r,c;return r=new R({props:{code:`import fetch from "node-fetch";
import fs from "fs";
async function query(filename) {
    const data = fs.readFileSync(filename);
    const response = await fetch(
        "https://api-inference.huggingface.co/models/facebook/detr-resnet-50",
        {
            headers: { Authorization: \`Bearer \${API_TOKEN}\` },
            method: "POST",
            body: data,
        }
    );
    const result = await response.json();
    return result;
}
query("cats.jpg").then((response) => {
    console.log(JSON.stringify(response));
});
// [{"score":0.9982201457023621,"label":"remote","box":{"xmin":40,"ymin":70,"xmax":175,"ymax":117}},{"score":0.9960021376609802,"label":"remote","box":{"xmin":333,"ymin":72,"xmax":368,"ymax":187}},{"score":0.9954745173454285,"label":"couch","box":{"xmin":0,"ymin":1,"xmax":639,"ymax":473}},{"score":0.9988006353378296,"label":"cat","box":{"xmin":13,"ymin":52,"xmax":314,"ymax":470}},{"score":0.9986783862113953,"label":"cat","box":{"xmin":345,"ymin":23,"xmax":640,"ymax":368}}]`,highlighted:`<span class="hljs-keyword">import</span> <span class="hljs-keyword">fetch</span> <span class="hljs-keyword">from</span> &quot;node-fetch&quot;;
<span class="hljs-keyword">import</span> fs <span class="hljs-keyword">from</span> &quot;fs&quot;;
async <span class="hljs-keyword">function</span> query(filename) {
    const data = fs.readFileSync(filename);
    const response = await <span class="hljs-keyword">fetch</span>(
        &quot;https://api-inference.huggingface.co/models/facebook/detr-resnet-50&quot;,
        {
            headers: { <span class="hljs-keyword">Authorization</span>: \`Bearer \${API_TOKEN}\` },
            <span class="hljs-keyword">method</span>: &quot;POST&quot;,
            body: data,
        }
    );
    const result = await response.json();
    <span class="hljs-keyword">return</span> result;
}
query(&quot;cats.jpg&quot;).<span class="hljs-keyword">then</span>((response) =&gt; {
    console.log(<span class="hljs-type">JSON</span>.stringify(response));
});
// [{&quot;score&quot;:<span class="hljs-number">0.9982201457023621</span>,&quot;label&quot;:&quot;remote&quot;,&quot;box&quot;:{&quot;xmin&quot;:<span class="hljs-number">40</span>,&quot;ymin&quot;:<span class="hljs-number">70</span>,&quot;xmax&quot;:<span class="hljs-number">175</span>,&quot;ymax&quot;:<span class="hljs-number">117</span>}},{&quot;score&quot;:<span class="hljs-number">0.9960021376609802</span>,&quot;label&quot;:&quot;remote&quot;,&quot;box&quot;:{&quot;xmin&quot;:<span class="hljs-number">333</span>,&quot;ymin&quot;:<span class="hljs-number">72</span>,&quot;xmax&quot;:<span class="hljs-number">368</span>,&quot;ymax&quot;:<span class="hljs-number">187</span>}},{&quot;score&quot;:<span class="hljs-number">0.9954745173454285</span>,&quot;label&quot;:&quot;couch&quot;,&quot;box&quot;:{&quot;xmin&quot;:<span class="hljs-number">0</span>,&quot;ymin&quot;:<span class="hljs-number">1</span>,&quot;xmax&quot;:<span class="hljs-number">639</span>,&quot;ymax&quot;:<span class="hljs-number">473</span>}},{&quot;score&quot;:<span class="hljs-number">0.9988006353378296</span>,&quot;label&quot;:&quot;cat&quot;,&quot;box&quot;:{&quot;xmin&quot;:<span class="hljs-number">13</span>,&quot;ymin&quot;:<span class="hljs-number">52</span>,&quot;xmax&quot;:<span class="hljs-number">314</span>,&quot;ymax&quot;:<span class="hljs-number">470</span>}},{&quot;score&quot;:<span class="hljs-number">0.9986783862113953</span>,&quot;label&quot;:&quot;cat&quot;,&quot;box&quot;:{&quot;xmin&quot;:<span class="hljs-number">345</span>,&quot;ymin&quot;:<span class="hljs-number">23</span>,&quot;xmax&quot;:<span class="hljs-number">640</span>,&quot;ymax&quot;:<span class="hljs-number">368</span>}}]`}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p:O,i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function XW(q){let r,c;return r=new P({props:{$$slots:{default:[VW]},$$scope:{ctx:q}}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),r.$set($)},i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function QW(q){let r,c;return r=new R({props:{code:`curl https://api-inference.huggingface.co/models/facebook/detr-resnet-50 \\
        -X POST \\
        --data-binary '@cats.jpg' \\
        -H "Authorization: Bearer \${HF_API_TOKEN}"
# [{"score":0.9982201457023621,"label":"remote","box":{"xmin":40,"ymin":70,"xmax":175,"ymax":117}},{"score":0.9960021376609802,"label":"remote","box":{"xmin":333,"ymin":72,"xmax":368,"ymax":187}},{"score":0.9954745173454285,"label":"couch","box":{"xmin":0,"ymin":1,"xmax":639,"ymax":473}},{"score":0.9988006353378296,"label":"cat","box":{"xmin":13,"ymin":52,"xmax":314,"ymax":470}},{"score":0.9986783862113953,"label":"cat","box":{"xmin":345,"ymin":23,"xmax":640,"ymax":368}}]`,highlighted:`curl https://api-inference.huggingface.co/models/facebook/detr-resnet-50 \\
        -X POST \\
        --data-binary <span class="hljs-string">&#x27;@cats.jpg&#x27;</span> \\
        -H <span class="hljs-string">&quot;Authorization: Bearer <span class="hljs-variable">\${HF_API_TOKEN}</span>&quot;</span>
<span class="hljs-comment"># [{&quot;score&quot;:0.9982201457023621,&quot;label&quot;:&quot;remote&quot;,&quot;box&quot;:{&quot;xmin&quot;:40,&quot;ymin&quot;:70,&quot;xmax&quot;:175,&quot;ymax&quot;:117}},{&quot;score&quot;:0.9960021376609802,&quot;label&quot;:&quot;remote&quot;,&quot;box&quot;:{&quot;xmin&quot;:333,&quot;ymin&quot;:72,&quot;xmax&quot;:368,&quot;ymax&quot;:187}},{&quot;score&quot;:0.9954745173454285,&quot;label&quot;:&quot;couch&quot;,&quot;box&quot;:{&quot;xmin&quot;:0,&quot;ymin&quot;:1,&quot;xmax&quot;:639,&quot;ymax&quot;:473}},{&quot;score&quot;:0.9988006353378296,&quot;label&quot;:&quot;cat&quot;,&quot;box&quot;:{&quot;xmin&quot;:13,&quot;ymin&quot;:52,&quot;xmax&quot;:314,&quot;ymax&quot;:470}},{&quot;score&quot;:0.9986783862113953,&quot;label&quot;:&quot;cat&quot;,&quot;box&quot;:{&quot;xmin&quot;:345,&quot;ymin&quot;:23,&quot;xmax&quot;:640,&quot;ymax&quot;:368}}]</span>`}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p:O,i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function ZW(q){let r,c;return r=new P({props:{$$slots:{default:[QW]},$$scope:{ctx:q}}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),r.$set($)},i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function eY(q){let r,c;return r=new R({props:{code:`self.assertEqual(
    deep_round(data, 4),
    [
        {
            "score": 0.9982,
            "label": "remote",
            "box": {"xmin": 40, "ymin": 70, "xmax": 175, "ymax": 117},
        },
        {
            "score": 0.9960,
            "label": "remote",
            "box": {"xmin": 333, "ymin": 72, "xmax": 368, "ymax": 187},
        },
        {
            "score": 0.9955,
            "label": "couch",
            "box": {"xmin": 0, "ymin": 1, "xmax": 639, "ymax": 473},
        },
        {
            "score": 0.9988,
            "label": "cat",
            "box": {"xmin": 13, "ymin": 52, "xmax": 314, "ymax": 470},
        },
        {
            "score": 0.9987,
            "label": "cat",
            "box": {"xmin": 345, "ymin": 23, "xmax": 640, "ymax": 368},
        },
    ],
)`,highlighted:`self.assertEqual(
    deep_round(data, <span class="hljs-number">4</span>),
    [
        {
            <span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.9982</span>,
            <span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;remote&quot;</span>,
            <span class="hljs-string">&quot;box&quot;</span>: {<span class="hljs-string">&quot;xmin&quot;</span>: <span class="hljs-number">40</span>, <span class="hljs-string">&quot;ymin&quot;</span>: <span class="hljs-number">70</span>, <span class="hljs-string">&quot;xmax&quot;</span>: <span class="hljs-number">175</span>, <span class="hljs-string">&quot;ymax&quot;</span>: <span class="hljs-number">117</span>},
        },
        {
            <span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.9960</span>,
            <span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;remote&quot;</span>,
            <span class="hljs-string">&quot;box&quot;</span>: {<span class="hljs-string">&quot;xmin&quot;</span>: <span class="hljs-number">333</span>, <span class="hljs-string">&quot;ymin&quot;</span>: <span class="hljs-number">72</span>, <span class="hljs-string">&quot;xmax&quot;</span>: <span class="hljs-number">368</span>, <span class="hljs-string">&quot;ymax&quot;</span>: <span class="hljs-number">187</span>},
        },
        {
            <span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.9955</span>,
            <span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;couch&quot;</span>,
            <span class="hljs-string">&quot;box&quot;</span>: {<span class="hljs-string">&quot;xmin&quot;</span>: <span class="hljs-number">0</span>, <span class="hljs-string">&quot;ymin&quot;</span>: <span class="hljs-number">1</span>, <span class="hljs-string">&quot;xmax&quot;</span>: <span class="hljs-number">639</span>, <span class="hljs-string">&quot;ymax&quot;</span>: <span class="hljs-number">473</span>},
        },
        {
            <span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.9988</span>,
            <span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;cat&quot;</span>,
            <span class="hljs-string">&quot;box&quot;</span>: {<span class="hljs-string">&quot;xmin&quot;</span>: <span class="hljs-number">13</span>, <span class="hljs-string">&quot;ymin&quot;</span>: <span class="hljs-number">52</span>, <span class="hljs-string">&quot;xmax&quot;</span>: <span class="hljs-number">314</span>, <span class="hljs-string">&quot;ymax&quot;</span>: <span class="hljs-number">470</span>},
        },
        {
            <span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.9987</span>,
            <span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;cat&quot;</span>,
            <span class="hljs-string">&quot;box&quot;</span>: {<span class="hljs-string">&quot;xmin&quot;</span>: <span class="hljs-number">345</span>, <span class="hljs-string">&quot;ymin&quot;</span>: <span class="hljs-number">23</span>, <span class="hljs-string">&quot;xmax&quot;</span>: <span class="hljs-number">640</span>, <span class="hljs-string">&quot;ymax&quot;</span>: <span class="hljs-number">368</span>},
        },
    ],
)`}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p:O,i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function tY(q){let r,c;return r=new P({props:{$$slots:{default:[eY]},$$scope:{ctx:q}}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),r.$set($)},i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function sY(q){let r,c,s,d,$,k,A;return{c(){r=n("p"),c=n("strong"),s=i("Recommended model"),d=i(`:
`),$=n("a"),k=i("facebook/detr-resnet-50-panoptic"),A=i("."),this.h()},l(j){r=o(j,"P",{});var T=l(r);c=o(T,"STRONG",{});var S=l(c);s=u(S,"Recommended model"),S.forEach(t),d=u(T,`:
`),$=o(T,"A",{href:!0,rel:!0});var D=l($);k=u(D,"facebook/detr-resnet-50-panoptic"),D.forEach(t),A=u(T,"."),T.forEach(t),this.h()},h(){f($,"href","https://huggingface.co/facebook/detr-resnet-50-panoptic"),f($,"rel","nofollow")},m(j,T){m(j,r,T),e(r,c),e(c,s),e(r,d),e(r,$),e($,k),e(r,A)},d(j){j&&t(r)}}}function aY(q){let r,c;return r=new R({props:{code:`import json
import requests
headers = {"Authorization": f"Bearer {API_TOKEN}"}
API_URL = "https://api-inference.huggingface.co/models/facebook/detr-resnet-50-panoptic"
def query(filename):
    with open(filename, "rb") as f:
        data = f.read()
    response = requests.request("POST", API_URL, headers=headers, data=data)
    return json.loads(response.content.decode("utf-8"))
data = query("cats.jpg")`,highlighted:`<span class="hljs-keyword">import</span> json
<span class="hljs-keyword">import</span> requests
headers = {<span class="hljs-string">&quot;Authorization&quot;</span>: <span class="hljs-string">f&quot;Bearer <span class="hljs-subst">{API_TOKEN}</span>&quot;</span>}
API_URL = <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/facebook/detr-resnet-50-panoptic&quot;</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">filename</span>):
    <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(filename, <span class="hljs-string">&quot;rb&quot;</span>) <span class="hljs-keyword">as</span> f:
        data = f.read()
    response = requests.request(<span class="hljs-string">&quot;POST&quot;</span>, API_URL, headers=headers, data=data)
    <span class="hljs-keyword">return</span> json.loads(response.content.decode(<span class="hljs-string">&quot;utf-8&quot;</span>))
data = query(<span class="hljs-string">&quot;cats.jpg&quot;</span>)`}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p:O,i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function rY(q){let r,c;return r=new P({props:{$$slots:{default:[aY]},$$scope:{ctx:q}}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),r.$set($)},i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function nY(q){let r,c;return r=new R({props:{code:`import fetch from "node-fetch";
import fs from "fs";
async function query(filename) {
    const data = fs.readFileSync(filename);
    const response = await fetch(
        "https://api-inference.huggingface.co/models/facebook/detr-resnet-50-panoptic",
        {
            headers: { Authorization: \`Bearer \${API_TOKEN}\` },
            method: "POST",
            body: data,
        }
    );
    const result = await response.json();
    return result;
}
query("cats.jpg").then((response) => {
    console.log(JSON.stringify(response));
});
// [{"score": 0.9094282388687134, "label": "blanket", "mask": "BASE64ENCODED_MASK"}, {"score": 0.9940965175628662, "label": "cat", "mask": "BASE64ENCODED_MASK"}, {"score": 0.9986692667007446, "label": "remote", "mask": "BASE64ENCODED_MASK"}, {"score": 0.9994757771492004, "label": "remote", "mask": "BASE64ENCODED_MASK"}, {"score": 0.9722068309783936, "label": "couch", "mask": "BASE64ENCODED_MASK"}, {"score": 0.9994235038757324, "label": "cat", "mask": "BASE64ENCODED_MASK"}]`,highlighted:`<span class="hljs-keyword">import</span> <span class="hljs-keyword">fetch</span> <span class="hljs-keyword">from</span> &quot;node-fetch&quot;;
<span class="hljs-keyword">import</span> fs <span class="hljs-keyword">from</span> &quot;fs&quot;;
async <span class="hljs-keyword">function</span> query(filename) {
    const data = fs.readFileSync(filename);
    const response = await <span class="hljs-keyword">fetch</span>(
        &quot;https://api-inference.huggingface.co/models/facebook/detr-resnet-50-panoptic&quot;,
        {
            headers: { <span class="hljs-keyword">Authorization</span>: \`Bearer \${API_TOKEN}\` },
            <span class="hljs-keyword">method</span>: &quot;POST&quot;,
            body: data,
        }
    );
    const result = await response.json();
    <span class="hljs-keyword">return</span> result;
}
query(&quot;cats.jpg&quot;).<span class="hljs-keyword">then</span>((response) =&gt; {
    console.log(<span class="hljs-type">JSON</span>.stringify(response));
});
// [{&quot;score&quot;: <span class="hljs-number">0.9094282388687134</span>, &quot;label&quot;: &quot;blanket&quot;, &quot;mask&quot;: &quot;BASE64ENCODED_MASK&quot;}, {&quot;score&quot;: <span class="hljs-number">0.9940965175628662</span>, &quot;label&quot;: &quot;cat&quot;, &quot;mask&quot;: &quot;BASE64ENCODED_MASK&quot;}, {&quot;score&quot;: <span class="hljs-number">0.9986692667007446</span>, &quot;label&quot;: &quot;remote&quot;, &quot;mask&quot;: &quot;BASE64ENCODED_MASK&quot;}, {&quot;score&quot;: <span class="hljs-number">0.9994757771492004</span>, &quot;label&quot;: &quot;remote&quot;, &quot;mask&quot;: &quot;BASE64ENCODED_MASK&quot;}, {&quot;score&quot;: <span class="hljs-number">0.9722068309783936</span>, &quot;label&quot;: &quot;couch&quot;, &quot;mask&quot;: &quot;BASE64ENCODED_MASK&quot;}, {&quot;score&quot;: <span class="hljs-number">0.9994235038757324</span>, &quot;label&quot;: &quot;cat&quot;, &quot;mask&quot;: &quot;BASE64ENCODED_MASK&quot;}]`}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p:O,i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function oY(q){let r,c;return r=new P({props:{$$slots:{default:[nY]},$$scope:{ctx:q}}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),r.$set($)},i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function lY(q){let r,c;return r=new R({props:{code:`curl https://api-inference.huggingface.co/models/facebook/detr-resnet-50-panoptic \\
        -X POST \\
        --data-binary '@cats.jpg' \\
        -H "Authorization: Bearer \${HF_API_TOKEN}"
# [{"score": 0.9094282388687134, "label": "blanket", "mask": "BASE64ENCODED_MASK"}, {"score": 0.9940965175628662, "label": "cat", "mask": "BASE64ENCODED_MASK"}, {"score": 0.9986692667007446, "label": "remote", "mask": "BASE64ENCODED_MASK"}, {"score": 0.9994757771492004, "label": "remote", "mask": "BASE64ENCODED_MASK"}, {"score": 0.9722068309783936, "label": "couch", "mask": "BASE64ENCODED_MASK"}, {"score": 0.9994235038757324, "label": "cat", "mask": "BASE64ENCODED_MASK"}]`,highlighted:`curl https://api-inference.huggingface.co/models/facebook/detr-resnet-50-panoptic \\
        -X POST \\
        --data-binary <span class="hljs-string">&#x27;@cats.jpg&#x27;</span> \\
        -H <span class="hljs-string">&quot;Authorization: Bearer <span class="hljs-variable">\${HF_API_TOKEN}</span>&quot;</span>
<span class="hljs-comment"># [{&quot;score&quot;: 0.9094282388687134, &quot;label&quot;: &quot;blanket&quot;, &quot;mask&quot;: &quot;BASE64ENCODED_MASK&quot;}, {&quot;score&quot;: 0.9940965175628662, &quot;label&quot;: &quot;cat&quot;, &quot;mask&quot;: &quot;BASE64ENCODED_MASK&quot;}, {&quot;score&quot;: 0.9986692667007446, &quot;label&quot;: &quot;remote&quot;, &quot;mask&quot;: &quot;BASE64ENCODED_MASK&quot;}, {&quot;score&quot;: 0.9994757771492004, &quot;label&quot;: &quot;remote&quot;, &quot;mask&quot;: &quot;BASE64ENCODED_MASK&quot;}, {&quot;score&quot;: 0.9722068309783936, &quot;label&quot;: &quot;couch&quot;, &quot;mask&quot;: &quot;BASE64ENCODED_MASK&quot;}, {&quot;score&quot;: 0.9994235038757324, &quot;label&quot;: &quot;cat&quot;, &quot;mask&quot;: &quot;BASE64ENCODED_MASK&quot;}]</span>`}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p:O,i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function iY(q){let r,c;return r=new P({props:{$$slots:{default:[lY]},$$scope:{ctx:q}}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),r.$set($)},i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function uY(q){let r,c;return r=new R({props:{code:`import base64
from io import BytesIO
from PIL import Image
with Image.open("cats.jpg") as img:
    masks = [d["mask"] for d in data]
    self.assertEqual(img.size, (640, 480))
    mask_imgs = [Image.open(BytesIO(base64.b64decode(mask))) for mask in masks]
    for mask_img in mask_imgs:
        self.assertEqual(mask_img.size, img.size)
        self.assertEqual(mask_img.mode, "L")  # L (8-bit pixels, black and white)
    first_mask_img = mask_imgs[0]
    min_pxl_val, max_pxl_val = first_mask_img.getextrema()
    self.assertGreaterEqual(min_pxl_val, 0)
    self.assertLessEqual(max_pxl_val, 255)`,highlighted:`<span class="hljs-keyword">import</span> base64
<span class="hljs-keyword">from</span> io <span class="hljs-keyword">import</span> BytesIO
<span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-keyword">with</span> Image.<span class="hljs-built_in">open</span>(<span class="hljs-string">&quot;cats.jpg&quot;</span>) <span class="hljs-keyword">as</span> img:
    masks = [d[<span class="hljs-string">&quot;mask&quot;</span>] <span class="hljs-keyword">for</span> d <span class="hljs-keyword">in</span> data]
    self.assertEqual(img.size, (<span class="hljs-number">640</span>, <span class="hljs-number">480</span>))
    mask_imgs = [Image.<span class="hljs-built_in">open</span>(BytesIO(base64.b64decode(mask))) <span class="hljs-keyword">for</span> mask <span class="hljs-keyword">in</span> masks]
    <span class="hljs-keyword">for</span> mask_img <span class="hljs-keyword">in</span> mask_imgs:
        self.assertEqual(mask_img.size, img.size)
        self.assertEqual(mask_img.mode, <span class="hljs-string">&quot;L&quot;</span>)  <span class="hljs-comment"># L (8-bit pixels, black and white)</span>
    first_mask_img = mask_imgs[<span class="hljs-number">0</span>]
    min_pxl_val, max_pxl_val = first_mask_img.getextrema()
    self.assertGreaterEqual(min_pxl_val, <span class="hljs-number">0</span>)
    self.assertLessEqual(max_pxl_val, <span class="hljs-number">255</span>)`}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p:O,i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function cY(q){let r,c;return r=new P({props:{$$slots:{default:[uY]},$$scope:{ctx:q}}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),r.$set($)},i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function fY(q){let r,c,s,d,$,k,A,j,T,S,D,ne,Re,Q,Y,it,Li,tr,Se,p3,a1,Ui,h3,r1,ut,kI,n1,ct,AI,o1,Ne,ft,_d,sr,d3,vd,g3,l1,xe,pt,Ed,ar,m3,yd,$3,i1,zi,q3,u1,ht,c1,rr,_3,nr,v3,f1,Mi,E3,p1,dt,h1,Ki,y3,d1,gt,wd,or,Fi,w3,b3,bd,T3,Z,lr,ir,Td,j3,k3,A3,Ji,D3,O3,ur,Wi,jd,P3,R3,Yi,S3,N3,cr,Vi,x3,I3,mt,H3,kd,B3,C3,G3,fr,Xi,L3,U3,$t,z3,Ad,M3,K3,F3,pr,Qi,J3,W3,qt,Y3,Dd,V3,X3,g1,Zi,Q3,m1,_t,$1,vt,Od,hr,eu,Z3,eT,Pd,tT,ue,dr,tu,Rd,sT,aT,su,rT,nT,gr,au,Sd,oT,lT,ru,iT,uT,mr,nu,Nd,cT,fT,ou,pT,hT,$r,lu,xd,dT,gT,iu,mT,q1,Ie,Et,Id,qr,$T,Hd,qT,_1,yt,_T,uu,vT,ET,v1,wt,E1,_r,yT,vr,wT,y1,cu,bT,w1,bt,b1,fu,TT,T1,Tt,Bd,Er,pu,jT,kT,Cd,AT,G,yr,wr,Gd,DT,OT,PT,hu,RT,ST,br,du,Ld,NT,xT,gu,IT,HT,Tr,mu,BT,CT,de,GT,Ud,LT,UT,zd,zT,MT,KT,jr,$u,FT,JT,ge,WT,Md,YT,VT,Kd,XT,QT,ZT,kr,qu,ej,tj,me,sj,Fd,aj,rj,Jd,nj,oj,lj,Ar,_u,ij,uj,oe,cj,Wd,fj,pj,Yd,hj,dj,Vd,gj,mj,$j,Dr,vu,qj,_j,$e,vj,Xd,Ej,yj,Qd,wj,bj,Tj,Or,Eu,jj,kj,jt,Aj,Zd,Dj,Oj,Pj,Pr,yu,Rj,Sj,kt,Nj,eg,xj,Ij,Hj,Rr,wu,tg,Bj,Cj,bu,Gj,Lj,Sr,Tu,Uj,zj,At,Mj,sg,Kj,Fj,Jj,Nr,ju,Wj,Yj,Dt,Vj,ag,Xj,Qj,Zj,xr,ku,e4,t4,Ot,s4,rg,a4,r4,j1,Au,n4,k1,Pt,ng,Ir,Du,o4,l4,og,i4,lg,Hr,Ou,ig,u4,c4,Pu,f4,A1,He,Rt,ug,Br,p4,cg,h4,D1,Ru,d4,O1,St,P1,Be,g4,Cr,m4,$4,Gr,q4,R1,Su,_4,S1,Nt,N1,Nu,v4,x1,xu,E4,I1,xt,H1,It,fg,Lr,Iu,y4,w4,pg,b4,ce,Ur,Hu,hg,T4,j4,Bu,k4,A4,zr,Cu,dg,D4,O4,Gu,P4,R4,Mr,Lu,gg,S4,N4,Ht,x4,mg,I4,H4,B4,Kr,Uu,$g,C4,G4,Bt,L4,qg,U4,z4,B1,Ce,Ct,_g,Fr,M4,vg,K4,C1,zu,F4,G1,Gt,L1,Jr,J4,Wr,W4,U1,Mu,Y4,z1,Lt,M1,Ku,V4,K1,Ut,Eg,Yr,Fu,X4,Q4,yg,Z4,K,Vr,Xr,wg,ek,tk,sk,bg,ak,Qr,Ju,rk,nk,Wu,ok,lk,Zr,Yu,ik,uk,Vu,ck,fk,en,Xu,Tg,pk,hk,Qu,dk,gk,tn,Zu,mk,$k,zt,qk,jg,_k,vk,Ek,sn,ec,yk,wk,Mt,bk,kg,Tk,jk,kk,an,tc,Ak,Dk,Kt,Ok,Ag,Pk,Rk,F1,sc,Sk,J1,Ft,W1,Jt,Dg,rn,ac,Nk,xk,Og,Ik,fe,nn,rc,Pg,Hk,Bk,nc,Ck,Gk,on,oc,Rg,Lk,Uk,lc,zk,Mk,ln,ic,Sg,Kk,Fk,uc,Jk,Wk,un,cc,Ng,Yk,Vk,fc,Xk,Y1,Ge,Wt,xg,cn,Qk,Ig,Zk,V1,pc,e5,X1,Yt,Q1,fn,t5,pn,s5,Z1,hc,a5,ev,Vt,tv,dc,r5,sv,Xt,Hg,hn,gc,n5,o5,Bg,l5,ee,dn,gn,Cg,i5,u5,c5,mc,f5,p5,mn,$c,Gg,h5,d5,qc,g5,m5,$n,_c,$5,q5,Qt,_5,Lg,v5,E5,y5,qn,vc,w5,b5,Zt,T5,Ug,j5,k5,A5,_n,Ec,D5,O5,es,P5,zg,R5,S5,av,yc,N5,rv,ts,nv,ss,Mg,vn,wc,x5,I5,Kg,H5,En,yn,bc,Fg,B5,C5,Tc,G5,L5,wn,jc,Jg,U5,z5,kc,M5,ov,Le,as,Wg,bn,K5,Yg,F5,lv,Ac,J5,iv,rs,uv,Tn,W5,jn,Y5,cv,Dc,V5,fv,ns,pv,Oc,X5,hv,os,Vg,kn,Pc,Q5,Z5,Xg,e6,I,An,Dn,Qg,t6,s6,a6,Rc,r6,n6,On,Sc,Zg,o6,l6,Nc,i6,u6,Pn,xc,c6,f6,qe,p6,em,h6,d6,tm,g6,m6,$6,Rn,Ic,q6,_6,le,v6,sm,E6,y6,am,w6,b6,rm,T6,j6,k6,Sn,Hc,A6,D6,_e,O6,nm,P6,R6,om,S6,N6,x6,Nn,Bc,I6,H6,ls,B6,lm,C6,G6,L6,xn,Cc,U6,z6,ve,M6,im,K6,F6,um,J6,W6,Y6,In,Gc,V6,X6,Ee,Q6,cm,Z6,e7,fm,t7,s7,a7,Hn,Lc,r7,n7,ye,o7,pm,l7,i7,hm,u7,c7,f7,Bn,Uc,p7,h7,is,d7,dm,g7,m7,$7,Cn,zc,q7,_7,us,v7,gm,E7,y7,w7,Gn,Mc,mm,b7,T7,Kc,j7,k7,Ln,Fc,A7,D7,cs,O7,$m,P7,R7,S7,Un,Jc,N7,x7,fs,I7,qm,H7,B7,C7,zn,Wc,G7,L7,ps,U7,_m,z7,M7,dv,Yc,K7,gv,hs,mv,ds,vm,Mn,Vc,F7,J7,Em,W7,ym,Kn,Xc,wm,Y7,V7,Qc,X7,$v,Ue,gs,bm,Fn,Q7,Tm,Z7,qv,ms,e9,Zc,t9,s9,_v,ze,$s,jm,Jn,a9,km,r9,vv,ef,n9,Ev,qs,yv,Me,o9,Wn,l9,i9,Yn,u9,wv,tf,c9,bv,_s,Tv,sf,f9,jv,vs,Am,Vn,af,p9,h9,Dm,d9,F,Xn,Qn,Om,g9,m9,$9,rf,q9,_9,Zn,nf,Pm,v9,E9,of,y9,w9,eo,lf,b9,T9,x,j9,Rm,k9,A9,D9,O9,Sm,P9,R9,S9,N9,Nm,x9,I9,H9,B9,xm,C9,G9,Im,L9,U9,z9,M9,Hm,K9,F9,Bm,J9,W9,Y9,V9,Cm,X9,Q9,Gm,Z9,e8,t8,to,uf,Lm,s8,a8,cf,r8,n8,so,ff,o8,l8,Es,i8,Um,u8,c8,f8,ao,pf,p8,h8,ys,d8,zm,g8,m8,$8,ro,hf,q8,_8,ws,v8,Mm,E8,y8,kv,df,w8,Av,bs,Dv,Ts,Km,no,gf,b8,T8,Fm,j8,te,oo,mf,Jm,k8,A8,$f,D8,O8,lo,qf,Wm,P8,R8,_f,S8,N8,io,vf,Ym,x8,I8,Ef,H8,B8,uo,yf,Vm,C8,G8,js,L8,Xm,U8,z8,M8,co,wf,Qm,K8,F8,ks,J8,Zm,W8,Y8,Ov,Ke,As,e$,fo,V8,t$,X8,Pv,po,Q8,bf,Z8,Rv,Fe,Ds,s$,ho,eA,a$,tA,Sv,Tf,sA,Nv,Os,xv,go,aA,mo,rA,Iv,jf,nA,Hv,Ps,Bv,kf,oA,Cv,Rs,r$,$o,Af,lA,iA,n$,uA,se,qo,_o,o$,cA,fA,pA,Df,hA,dA,vo,Of,l$,gA,mA,Pf,$A,qA,Eo,Rf,_A,vA,Ss,EA,i$,yA,wA,bA,yo,Sf,TA,jA,Ns,kA,u$,AA,DA,OA,wo,Nf,PA,RA,xs,SA,c$,NA,xA,Gv,xf,IA,Lv,Is,f$,bo,If,HA,BA,p$,CA,h$,To,Hf,d$,GA,LA,Bf,UA,Uv,Je,Hs,g$,jo,zA,m$,MA,zv,Cf,KA,Mv,Bs,Kv,ko,FA,Ao,JA,Fv,Gf,WA,Jv,Cs,Wv,Lf,YA,Yv,Gs,$$,Do,Uf,VA,XA,q$,QA,M,Oo,Po,_$,ZA,eD,tD,zf,sD,aD,Ro,So,v$,rD,nD,oD,Mf,lD,iD,No,Kf,uD,cD,we,fD,E$,pD,hD,y$,dD,gD,mD,xo,Ff,$D,qD,Ls,_D,w$,vD,ED,yD,Io,Jf,b$,wD,bD,Wf,TD,jD,Ho,Yf,kD,AD,Us,DD,T$,OD,PD,RD,Bo,Vf,SD,ND,zs,xD,j$,ID,HD,BD,Co,Xf,CD,GD,Ms,LD,k$,UD,zD,Vv,Qf,MD,Xv,Zf,KD,Qv,Ks,Zv,Fs,A$,Go,ep,FD,JD,D$,WD,We,Lo,tp,O$,YD,VD,sp,XD,QD,Uo,ap,P$,ZD,eO,rp,tO,sO,zo,np,R$,aO,rO,Js,nO,S$,oO,lO,e2,Ye,Ws,N$,Mo,iO,x$,uO,t2,op,cO,s2,Ys,a2,Ko,fO,Fo,pO,r2,lp,hO,n2,Vs,o2,ip,dO,l2,Xs,I$,Jo,up,gO,mO,H$,$O,N,Wo,Yo,B$,qO,_O,vO,C$,EO,Vo,cp,yO,wO,fp,bO,TO,Xo,pp,jO,kO,hp,AO,DO,Qo,dp,OO,PO,Qs,RO,G$,SO,NO,xO,Zo,gp,L$,IO,HO,mp,BO,CO,el,$p,GO,LO,be,UO,U$,zO,MO,z$,KO,FO,JO,tl,qp,WO,YO,Te,VO,M$,XO,QO,K$,ZO,eP,tP,sl,_p,sP,aP,je,rP,F$,nP,oP,J$,lP,iP,uP,al,vp,cP,fP,ie,pP,W$,hP,dP,Y$,gP,mP,V$,$P,qP,_P,rl,Ep,vP,EP,ke,yP,X$,wP,bP,Q$,TP,jP,kP,nl,yp,AP,DP,Zs,OP,Z$,PP,RP,SP,ol,wp,NP,xP,ea,IP,eq,HP,BP,CP,ll,bp,tq,GP,LP,Tp,UP,zP,il,jp,MP,KP,ta,FP,sq,JP,WP,YP,ul,kp,VP,XP,sa,QP,aq,ZP,eR,tR,cl,Ap,sR,aR,aa,rR,rq,nR,oR,i2,Dp,lR,u2,ra,nq,fl,Op,iR,uR,oq,cR,pe,pl,Pp,lq,fR,pR,Rp,hR,dR,hl,Sp,iq,gR,mR,Np,$R,qR,dl,xp,_R,vR,Ip,ER,yR,gl,Hp,wR,bR,Bp,TR,c2,Ve,na,uq,ml,jR,cq,kR,f2,Cp,AR,p2,oa,h2,Xe,DR,$l,OR,PR,ql,RR,d2,Gp,SR,g2,la,fq,_l,Lp,NR,xR,pq,IR,ae,vl,El,hq,HR,BR,CR,Up,GR,LR,yl,zp,dq,UR,zR,Mp,MR,KR,wl,Kp,FR,JR,ia,WR,gq,YR,VR,XR,bl,Fp,QR,ZR,ua,eS,mq,tS,sS,aS,Tl,Jp,rS,nS,ca,oS,$q,lS,iS,m2,Wp,uS,$2,fa,qq,jl,Yp,cS,fS,_q,pS,vq,kl,Vp,Eq,hS,dS,Xp,gS,q2,Qp,mS,_2,Qe,pa,yq,Al,$S,wq,qS,v2,Ze,ha,bq,Dl,_S,Tq,vS,E2,Zp,ES,y2,da,w2,ga,b2,he,yS,Ol,wS,bS,Pl,TS,jS,Rl,kS,T2,eh,AS,j2,ma,k2,th,DS,A2,$a,jq,Sl,sh,OS,PS,kq,RS,Aq,Nl,xl,Dq,SS,NS,xS,ah,IS,D2,rh,HS,O2,nh,BS,P2,qa,R2,_a,Oq,Il,oh,CS,GS,Pq,LS,Rq,Hl,lh,Sq,US,zS,ih,MS,S2,et,va,Nq,Bl,KS,xq,FS,N2,uh,JS,x2,Ea,I2,tt,WS,Cl,YS,VS,Gl,XS,H2,ch,QS,B2,ya,C2,fh,ZS,G2,wa,Iq,Ll,ph,eN,tN,Hq,sN,Bq,Ul,zl,Cq,aN,rN,nN,hh,oN,L2,dh,lN,U2,ba,z2,Ta,Gq,Ml,gh,iN,uN,Lq,cN,Kl,Fl,mh,Uq,fN,pN,$h,hN,dN,Jl,qh,zq,gN,mN,_h,$N,M2,st,ja,Mq,Wl,qN,Kq,_N,K2,at,ka,Fq,Yl,vN,Jq,EN,F2,vh,yN,J2,Aa,W2,Vl,wN,Xl,bN,Y2,Eh,TN,V2,Da,X2,Oa,jN,Ql,kN,AN,Q2,Pa,Wq,Zl,yh,DN,ON,Yq,PN,Vq,ei,ti,Xq,RN,SN,NN,wh,xN,Z2,bh,IN,eE,Ra,tE,Sa,Qq,si,Th,HN,BN,Zq,CN,ai,ri,jh,e_,GN,LN,kh,UN,zN,ni,Ah,t_,MN,KN,Dh,FN,sE,rt,Na,s_,oi,JN,a_,WN,aE,Oh,YN,rE,xa,nE,li,VN,ii,XN,oE,Ph,QN,lE,Ia,iE,Ha,ZN,ui,ex,tx,uE,Ba,r_,ci,Rh,sx,ax,n_,rx,o_,fi,pi,l_,nx,ox,lx,Sh,ix,cE,Nh,ux,fE,Ca,pE,Ga,i_,hi,xh,cx,fx,u_,px,nt,di,Ih,c_,hx,dx,Hh,gx,mx,gi,Bh,f_,$x,qx,Ch,_x,vx,mi,Gh,p_,Ex,yx,Lh,wx,hE,ot,La,h_,$i,bx,d_,Tx,dE,Uh,jx,gE,Ua,mE,qi,kx,_i,Ax,$E,zh,Dx,qE,za,_E,Ma,Ox,vi,Px,Rx,vE,Ka,g_,Ei,Mh,Sx,Nx,m_,xx,$_,yi,wi,q_,Ix,Hx,Bx,Kh,Cx,EE,Fh,Gx,yE,Fa,wE,Ja,__,bi,Jh,Lx,Ux,v_,zx,lt,Ti,Wh,E_,Mx,Kx,Yh,Fx,Jx,ji,Vh,y_,Wx,Yx,Xh,Vx,Xx,ki,Qh,w_,Qx,Zx,Zh,eI,bE;return k=new z({}),Q=new z({}),sr=new z({}),ar=new z({}),ht=new J({props:{$$slots:{default:[UF]},$$scope:{ctx:q}}}),dt=new L({props:{python:!0,js:!0,curl:!0,$$slots:{curl:[WF],js:[FF],python:[MF]},$$scope:{ctx:q}}}),_t=new L({props:{python:!0,js:!0,curl:!0,$$slots:{python:[VF]},$$scope:{ctx:q}}}),qr=new z({}),wt=new J({props:{$$slots:{default:[XF]},$$scope:{ctx:q}}}),bt=new L({props:{python:!0,js:!0,curl:!0,$$slots:{curl:[aJ],js:[tJ],python:[ZF]},$$scope:{ctx:q}}}),Br=new z({}),St=new J({props:{$$slots:{default:[rJ]},$$scope:{ctx:q}}}),Nt=new L({props:{python:!0,js:!0,curl:!0,$$slots:{curl:[cJ],js:[iJ],python:[oJ]},$$scope:{ctx:q}}}),xt=new L({props:{python:!0,js:!0,curl:!0,$$slots:{python:[pJ]},$$scope:{ctx:q}}}),Fr=new z({}),Gt=new J({props:{$$slots:{default:[hJ]},$$scope:{ctx:q}}}),Lt=new L({props:{python:!0,js:!0,curl:!0,$$slots:{curl:[_J],js:[$J],python:[gJ]},$$scope:{ctx:q}}}),Ft=new L({props:{python:!0,js:!0,curl:!0,$$slots:{python:[EJ]},$$scope:{ctx:q}}}),cn=new z({}),Yt=new J({props:{$$slots:{default:[yJ]},$$scope:{ctx:q}}}),Vt=new L({props:{python:!0,js:!0,curl:!0,$$slots:{curl:[AJ],js:[jJ],python:[bJ]},$$scope:{ctx:q}}}),ts=new L({props:{python:!0,js:!0,curl:!0,$$slots:{python:[OJ]},$$scope:{ctx:q}}}),bn=new z({}),rs=new J({props:{$$slots:{default:[PJ]},$$scope:{ctx:q}}}),ns=new L({props:{python:!0,js:!0,curl:!0,$$slots:{curl:[HJ],js:[xJ],python:[SJ]},$$scope:{ctx:q}}}),hs=new L({props:{python:!0,js:!0,curl:!0,$$slots:{python:[CJ]},$$scope:{ctx:q}}}),Fn=new z({}),Jn=new z({}),qs=new J({props:{$$slots:{default:[GJ]},$$scope:{ctx:q}}}),_s=new L({props:{python:!0,js:!0,curl:!0,$$slots:{curl:[FJ],js:[MJ],python:[UJ]},$$scope:{ctx:q}}}),bs=new L({props:{python:!0,js:!0,curl:!0,$$slots:{python:[WJ]},$$scope:{ctx:q}}}),fo=new z({}),ho=new z({}),Os=new J({props:{$$slots:{default:[YJ]},$$scope:{ctx:q}}}),Ps=new L({props:{python:!0,js:!0,curl:!0,$$slots:{curl:[tW],js:[ZJ],python:[XJ]},$$scope:{ctx:q}}}),jo=new z({}),Bs=new J({props:{$$slots:{default:[sW]},$$scope:{ctx:q}}}),Cs=new L({props:{python:!0,js:!0,curl:!0,$$slots:{curl:[iW],js:[oW],python:[rW]},$$scope:{ctx:q}}}),Ks=new L({props:{python:!0,js:!0,curl:!0,$$slots:{python:[cW]},$$scope:{ctx:q}}}),Mo=new z({}),Ys=new J({props:{$$slots:{default:[fW]},$$scope:{ctx:q}}}),Vs=new L({props:{python:!0,js:!0,curl:!0,$$slots:{curl:[$W],js:[gW],python:[hW]},$$scope:{ctx:q}}}),ml=new z({}),oa=new J({props:{$$slots:{default:[qW]},$$scope:{ctx:q}}}),Al=new z({}),Dl=new z({}),da=new J({props:{$$slots:{default:[_W]},$$scope:{ctx:q}}}),ga=new J({props:{$$slots:{default:[vW]},$$scope:{ctx:q}}}),ma=new L({props:{python:!0,js:!0,curl:!0,$$slots:{curl:[jW],js:[bW],python:[yW]},$$scope:{ctx:q}}}),qa=new L({props:{python:!0,js:!0,curl:!0,$$slots:{python:[AW]},$$scope:{ctx:q}}}),Bl=new z({}),Ea=new J({props:{$$slots:{default:[DW]},$$scope:{ctx:q}}}),ya=new L({props:{python:!0,js:!0,curl:!0,$$slots:{curl:[xW],js:[SW],python:[PW]},$$scope:{ctx:q}}}),ba=new L({props:{python:!0,js:!0,curl:!0,$$slots:{python:[HW]},$$scope:{ctx:q}}}),Wl=new z({}),Yl=new z({}),Aa=new J({props:{$$slots:{default:[BW]},$$scope:{ctx:q}}}),Da=new L({props:{python:!0,js:!0,curl:!0,$$slots:{curl:[MW],js:[UW],python:[GW]},$$scope:{ctx:q}}}),Ra=new L({props:{python:!0,js:!0,curl:!0,$$slots:{python:[FW]},$$scope:{ctx:q}}}),oi=new z({}),xa=new J({props:{$$slots:{default:[JW]},$$scope:{ctx:q}}}),Ia=new L({props:{python:!0,js:!0,curl:!0,$$slots:{curl:[ZW],js:[XW],python:[YW]},$$scope:{ctx:q}}}),Ca=new L({props:{python:!0,js:!0,curl:!0,$$slots:{python:[tY]},$$scope:{ctx:q}}}),$i=new z({}),Ua=new J({props:{$$slots:{default:[sY]},$$scope:{ctx:q}}}),za=new L({props:{python:!0,js:!0,curl:!0,$$slots:{curl:[iY],js:[oY],python:[rY]},$$scope:{ctx:q}}}),Fa=new L({props:{python:!0,js:!0,curl:!0,$$slots:{python:[cY]},$$scope:{ctx:q}}}),{c(){r=n("meta"),c=p(),s=n("h1"),d=n("a"),$=n("span"),_(k.$$.fragment),A=p(),j=n("span"),T=i("Detailed parameters"),S=p(),D=n("h2"),ne=n("a"),Re=n("span"),_(Q.$$.fragment),Y=p(),it=n("span"),Li=i("Which task is used by this model ?"),tr=p(),Se=n("p"),p3=i(`In general the \u{1F917} Hosted API Inference accepts a simple string as an
input. However, more advanced usage depends on the \u201Ctask\u201D that the
model solves.`),a1=p(),Ui=n("p"),h3=i("The \u201Ctask\u201D of a model is defined here on it\u2019s model page:"),r1=p(),ut=n("img"),n1=p(),ct=n("img"),o1=p(),Ne=n("h2"),ft=n("a"),_d=n("span"),_(sr.$$.fragment),d3=p(),vd=n("span"),g3=i("Natural Language Processing"),l1=p(),xe=n("h3"),pt=n("a"),Ed=n("span"),_(ar.$$.fragment),m3=p(),yd=n("span"),$3=i("Fill Mask task"),i1=p(),zi=n("p"),q3=i(`Tries to fill in a hole with a missing word (token to be precise).
That\u2019s the base task for BERT models.`),u1=p(),_(ht.$$.fragment),c1=p(),rr=n("p"),_3=i("Available with: "),nr=n("a"),v3=i("\u{1F917} Transformers"),f1=p(),Mi=n("p"),E3=i("Example:"),p1=p(),_(dt.$$.fragment),h1=p(),Ki=n("p"),y3=i(`When sending your request, you should send a JSON encoded payload. Here
are all the options`),d1=p(),gt=n("table"),wd=n("thead"),or=n("tr"),Fi=n("th"),w3=i("All parameters"),b3=p(),bd=n("th"),T3=p(),Z=n("tbody"),lr=n("tr"),ir=n("td"),Td=n("strong"),j3=i("inputs"),k3=i(" (required):"),A3=p(),Ji=n("td"),D3=i("a string to be filled from, must contain the [MASK] token (check model card for exact name of the mask)"),O3=p(),ur=n("tr"),Wi=n("td"),jd=n("strong"),P3=i("options"),R3=p(),Yi=n("td"),S3=i("a dict containing the following keys:"),N3=p(),cr=n("tr"),Vi=n("td"),x3=i("use_gpu"),I3=p(),mt=n("td"),H3=i("(Default: "),kd=n("code"),B3=i("false"),C3=i("). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),G3=p(),fr=n("tr"),Xi=n("td"),L3=i("use_cache"),U3=p(),$t=n("td"),z3=i("(Default: "),Ad=n("code"),M3=i("true"),K3=i("). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),F3=p(),pr=n("tr"),Qi=n("td"),J3=i("wait_for_model"),W3=p(),qt=n("td"),Y3=i("(Default: "),Dd=n("code"),V3=i("false"),X3=i(") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),g1=p(),Zi=n("p"),Q3=i("Return value is either a dict or a list of dicts if you sent a list of inputs"),m1=p(),_(_t.$$.fragment),$1=p(),vt=n("table"),Od=n("thead"),hr=n("tr"),eu=n("th"),Z3=i("Returned values"),eT=p(),Pd=n("th"),tT=p(),ue=n("tbody"),dr=n("tr"),tu=n("td"),Rd=n("strong"),sT=i("sequence"),aT=p(),su=n("td"),rT=i("The actual sequence of tokens that ran against the model (may contain special tokens)"),nT=p(),gr=n("tr"),au=n("td"),Sd=n("strong"),oT=i("score"),lT=p(),ru=n("td"),iT=i("The probability for this token."),uT=p(),mr=n("tr"),nu=n("td"),Nd=n("strong"),cT=i("token"),fT=p(),ou=n("td"),pT=i("The id of the token"),hT=p(),$r=n("tr"),lu=n("td"),xd=n("strong"),dT=i("token_str"),gT=p(),iu=n("td"),mT=i("The string representation of the token"),q1=p(),Ie=n("h3"),Et=n("a"),Id=n("span"),_(qr.$$.fragment),$T=p(),Hd=n("span"),qT=i("Summarization task"),_1=p(),yt=n("p"),_T=i(`This task is well known to summarize longer text into shorter text.
Be careful, some models have a maximum length of input. That means that
the summary cannot handle full books for instance. Be careful when
choosing your model. If you want to discuss your summarization needs,
please get in touch with us: <`),uu=n("a"),vT=i("api-enterprise@huggingface.co"),ET=i(">"),v1=p(),_(wt.$$.fragment),E1=p(),_r=n("p"),yT=i("Available with: "),vr=n("a"),wT=i("\u{1F917} Transformers"),y1=p(),cu=n("p"),bT=i("Example:"),w1=p(),_(bt.$$.fragment),b1=p(),fu=n("p"),TT=i(`When sending your request, you should send a JSON encoded payload. Here
are all the options`),T1=p(),Tt=n("table"),Bd=n("thead"),Er=n("tr"),pu=n("th"),jT=i("All parameters"),kT=p(),Cd=n("th"),AT=p(),G=n("tbody"),yr=n("tr"),wr=n("td"),Gd=n("strong"),DT=i("inputs"),OT=i(" (required)"),PT=p(),hu=n("td"),RT=i("a string to be summarized"),ST=p(),br=n("tr"),du=n("td"),Ld=n("strong"),NT=i("parameters"),xT=p(),gu=n("td"),IT=i("a dict containing the following keys:"),HT=p(),Tr=n("tr"),mu=n("td"),BT=i("min_length"),CT=p(),de=n("td"),GT=i("(Default: "),Ud=n("code"),LT=i("None"),UT=i("). Integer to define the minimum length "),zd=n("strong"),zT=i("in tokens"),MT=i(" of the output summary."),KT=p(),jr=n("tr"),$u=n("td"),FT=i("max_length"),JT=p(),ge=n("td"),WT=i("(Default: "),Md=n("code"),YT=i("None"),VT=i("). Integer to define the maximum length "),Kd=n("strong"),XT=i("in tokens"),QT=i(" of the output summary."),ZT=p(),kr=n("tr"),qu=n("td"),ej=i("top_k"),tj=p(),me=n("td"),sj=i("(Default: "),Fd=n("code"),aj=i("None"),rj=i("). Integer to define the top tokens considered within the "),Jd=n("code"),nj=i("sample"),oj=i(" operation to create new text."),lj=p(),Ar=n("tr"),_u=n("td"),ij=i("top_p"),uj=p(),oe=n("td"),cj=i("(Default: "),Wd=n("code"),fj=i("None"),pj=i("). Float to define the tokens that are within the "),Yd=n("code"),hj=i("sample"),dj=i(" operation of text generation. Add tokens in the sample for more probable to least probable until the sum of the probabilities is greater than "),Vd=n("code"),gj=i("top_p"),mj=i("."),$j=p(),Dr=n("tr"),vu=n("td"),qj=i("temperature"),_j=p(),$e=n("td"),vj=i("(Default: "),Xd=n("code"),Ej=i("1.0"),yj=i("). Float (0.0-100.0). The temperature of the sampling operation. 1 means regular sampling, 0 means "),Qd=n("code"),wj=i("100.0"),bj=i(" is getting closer to uniform probability."),Tj=p(),Or=n("tr"),Eu=n("td"),jj=i("repetition_penalty"),kj=p(),jt=n("td"),Aj=i("(Default: "),Zd=n("code"),Dj=i("None"),Oj=i("). Float (0.0-100.0). The more a token is used within generation the more it is penalized to not be picked in successive generation passes."),Pj=p(),Pr=n("tr"),yu=n("td"),Rj=i("max_time"),Sj=p(),kt=n("td"),Nj=i("(Default: "),eg=n("code"),xj=i("None"),Ij=i("). Float (0-120.0). The amount of time in seconds that the query should take maximum. Network can cause some overhead so it will be a soft limit."),Hj=p(),Rr=n("tr"),wu=n("td"),tg=n("strong"),Bj=i("options"),Cj=p(),bu=n("td"),Gj=i("a dict containing the following keys:"),Lj=p(),Sr=n("tr"),Tu=n("td"),Uj=i("use_gpu"),zj=p(),At=n("td"),Mj=i("(Default: "),sg=n("code"),Kj=i("false"),Fj=i("). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),Jj=p(),Nr=n("tr"),ju=n("td"),Wj=i("use_cache"),Yj=p(),Dt=n("td"),Vj=i("(Default: "),ag=n("code"),Xj=i("true"),Qj=i("). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),Zj=p(),xr=n("tr"),ku=n("td"),e4=i("wait_for_model"),t4=p(),Ot=n("td"),s4=i("(Default: "),rg=n("code"),a4=i("false"),r4=i(") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),j1=p(),Au=n("p"),n4=i("Return value is either a dict or a list of dicts if you sent a list of inputs"),k1=p(),Pt=n("table"),ng=n("thead"),Ir=n("tr"),Du=n("th"),o4=i("Returned values"),l4=p(),og=n("th"),i4=p(),lg=n("tbody"),Hr=n("tr"),Ou=n("td"),ig=n("strong"),u4=i("summarization_text"),c4=p(),Pu=n("td"),f4=i("The string after translation"),A1=p(),He=n("h3"),Rt=n("a"),ug=n("span"),_(Br.$$.fragment),p4=p(),cg=n("span"),h4=i("Question Answering task"),D1=p(),Ru=n("p"),d4=i("Want to have a nice know-it-all bot that can answer any question?"),O1=p(),_(St.$$.fragment),P1=p(),Be=n("p"),g4=i("Available with: "),Cr=n("a"),m4=i("\u{1F917}Transformers"),$4=i(` and
`),Gr=n("a"),q4=i("AllenNLP"),R1=p(),Su=n("p"),_4=i("Example:"),S1=p(),_(Nt.$$.fragment),N1=p(),Nu=n("p"),v4=i(`When sending your request, you should send a JSON encoded payload. Here
are all the options`),x1=p(),xu=n("p"),E4=i("Return value is either a dict or a list of dicts if you sent a list of inputs"),I1=p(),_(xt.$$.fragment),H1=p(),It=n("table"),fg=n("thead"),Lr=n("tr"),Iu=n("th"),y4=i("Returned values"),w4=p(),pg=n("th"),b4=p(),ce=n("tbody"),Ur=n("tr"),Hu=n("td"),hg=n("strong"),T4=i("answer"),j4=p(),Bu=n("td"),k4=i("A string that\u2019s the answer within the text."),A4=p(),zr=n("tr"),Cu=n("td"),dg=n("strong"),D4=i("score"),O4=p(),Gu=n("td"),P4=i("A float that represents how likely that the answer is correct"),R4=p(),Mr=n("tr"),Lu=n("td"),gg=n("strong"),S4=i("start"),N4=p(),Ht=n("td"),x4=i("The index (string wise) of the start of the answer within "),mg=n("code"),I4=i("context"),H4=i("."),B4=p(),Kr=n("tr"),Uu=n("td"),$g=n("strong"),C4=i("stop"),G4=p(),Bt=n("td"),L4=i("The index (string wise) of the stop of the answer within "),qg=n("code"),U4=i("context"),z4=i("."),B1=p(),Ce=n("h3"),Ct=n("a"),_g=n("span"),_(Fr.$$.fragment),M4=p(),vg=n("span"),K4=i("Table Question Answering task"),C1=p(),zu=n("p"),F4=i(`Don\u2019t know SQL? Don\u2019t want to dive into a large spreadsheet? Ask
questions in plain english!`),G1=p(),_(Gt.$$.fragment),L1=p(),Jr=n("p"),J4=i("Available with: "),Wr=n("a"),W4=i("\u{1F917} Transformers"),U1=p(),Mu=n("p"),Y4=i("Example:"),z1=p(),_(Lt.$$.fragment),M1=p(),Ku=n("p"),V4=i(`When sending your request, you should send a JSON encoded payload. Here
are all the options`),K1=p(),Ut=n("table"),Eg=n("thead"),Yr=n("tr"),Fu=n("th"),X4=i("All parameters"),Q4=p(),yg=n("th"),Z4=p(),K=n("tbody"),Vr=n("tr"),Xr=n("td"),wg=n("strong"),ek=i("inputs"),tk=i(" (required)"),sk=p(),bg=n("td"),ak=p(),Qr=n("tr"),Ju=n("td"),rk=i("query (required)"),nk=p(),Wu=n("td"),ok=i("The query in plain text that you want to ask the table"),lk=p(),Zr=n("tr"),Yu=n("td"),ik=i("table (required)"),uk=p(),Vu=n("td"),ck=i("A table of data represented as a dict of list where entries are headers and the lists are all the values, all lists must have the same size."),fk=p(),en=n("tr"),Xu=n("td"),Tg=n("strong"),pk=i("options"),hk=p(),Qu=n("td"),dk=i("a dict containing the following keys:"),gk=p(),tn=n("tr"),Zu=n("td"),mk=i("use_gpu"),$k=p(),zt=n("td"),qk=i("(Default: "),jg=n("code"),_k=i("false"),vk=i("). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),Ek=p(),sn=n("tr"),ec=n("td"),yk=i("use_cache"),wk=p(),Mt=n("td"),bk=i("(Default: "),kg=n("code"),Tk=i("true"),jk=i("). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),kk=p(),an=n("tr"),tc=n("td"),Ak=i("wait_for_model"),Dk=p(),Kt=n("td"),Ok=i("(Default: "),Ag=n("code"),Pk=i("false"),Rk=i(") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),F1=p(),sc=n("p"),Sk=i("Return value is either a dict or a list of dicts if you sent a list of inputs"),J1=p(),_(Ft.$$.fragment),W1=p(),Jt=n("table"),Dg=n("thead"),rn=n("tr"),ac=n("th"),Nk=i("Returned values"),xk=p(),Og=n("th"),Ik=p(),fe=n("tbody"),nn=n("tr"),rc=n("td"),Pg=n("strong"),Hk=i("answer"),Bk=p(),nc=n("td"),Ck=i("The plaintext answer"),Gk=p(),on=n("tr"),oc=n("td"),Rg=n("strong"),Lk=i("coordinates"),Uk=p(),lc=n("td"),zk=i("a list of coordinates of the cells referenced in the answer"),Mk=p(),ln=n("tr"),ic=n("td"),Sg=n("strong"),Kk=i("cells"),Fk=p(),uc=n("td"),Jk=i("a list of coordinates of the cells contents"),Wk=p(),un=n("tr"),cc=n("td"),Ng=n("strong"),Yk=i("aggregator"),Vk=p(),fc=n("td"),Xk=i("The aggregator used to get the answer"),Y1=p(),Ge=n("h3"),Wt=n("a"),xg=n("span"),_(cn.$$.fragment),Qk=p(),Ig=n("span"),Zk=i("Text Classification task"),V1=p(),pc=n("p"),e5=i(`Usually used for sentiment-analysis this will output the likelihood of
classes of an input.`),X1=p(),_(Yt.$$.fragment),Q1=p(),fn=n("p"),t5=i("Available with: "),pn=n("a"),s5=i("\u{1F917} Transformers"),Z1=p(),hc=n("p"),a5=i("Example:"),ev=p(),_(Vt.$$.fragment),tv=p(),dc=n("p"),r5=i(`When sending your request, you should send a JSON encoded payload. Here
are all the options`),sv=p(),Xt=n("table"),Hg=n("thead"),hn=n("tr"),gc=n("th"),n5=i("All parameters"),o5=p(),Bg=n("th"),l5=p(),ee=n("tbody"),dn=n("tr"),gn=n("td"),Cg=n("strong"),i5=i("inputs"),u5=i(" (required)"),c5=p(),mc=n("td"),f5=i("a string to be classified"),p5=p(),mn=n("tr"),$c=n("td"),Gg=n("strong"),h5=i("options"),d5=p(),qc=n("td"),g5=i("a dict containing the following keys:"),m5=p(),$n=n("tr"),_c=n("td"),$5=i("use_gpu"),q5=p(),Qt=n("td"),_5=i("(Default: "),Lg=n("code"),v5=i("false"),E5=i("). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),y5=p(),qn=n("tr"),vc=n("td"),w5=i("use_cache"),b5=p(),Zt=n("td"),T5=i("(Default: "),Ug=n("code"),j5=i("true"),k5=i("). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),A5=p(),_n=n("tr"),Ec=n("td"),D5=i("wait_for_model"),O5=p(),es=n("td"),P5=i("(Default: "),zg=n("code"),R5=i("false"),S5=i(") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),av=p(),yc=n("p"),N5=i("Return value is either a dict or a list of dicts if you sent a list of inputs"),rv=p(),_(ts.$$.fragment),nv=p(),ss=n("table"),Mg=n("thead"),vn=n("tr"),wc=n("th"),x5=i("Returned values"),I5=p(),Kg=n("th"),H5=p(),En=n("tbody"),yn=n("tr"),bc=n("td"),Fg=n("strong"),B5=i("label"),C5=p(),Tc=n("td"),G5=i("The label for the class (model specific)"),L5=p(),wn=n("tr"),jc=n("td"),Jg=n("strong"),U5=i("score"),z5=p(),kc=n("td"),M5=i("A floats that represents how likely is that the text belongs the this class."),ov=p(),Le=n("h3"),as=n("a"),Wg=n("span"),_(bn.$$.fragment),K5=p(),Yg=n("span"),F5=i("Text Generation task"),lv=p(),Ac=n("p"),J5=i("Use to continue text from a prompt. This is a very generic task."),iv=p(),_(rs.$$.fragment),uv=p(),Tn=n("p"),W5=i("Available with: "),jn=n("a"),Y5=i("\u{1F917} Transformers"),cv=p(),Dc=n("p"),V5=i("Example:"),fv=p(),_(ns.$$.fragment),pv=p(),Oc=n("p"),X5=i(`When sending your request, you should send a JSON encoded payload. Here
are all the options`),hv=p(),os=n("table"),Vg=n("thead"),kn=n("tr"),Pc=n("th"),Q5=i("All parameters"),Z5=p(),Xg=n("th"),e6=p(),I=n("tbody"),An=n("tr"),Dn=n("td"),Qg=n("strong"),t6=i("inputs"),s6=i(" (required):"),a6=p(),Rc=n("td"),r6=i("a string to be generated from"),n6=p(),On=n("tr"),Sc=n("td"),Zg=n("strong"),o6=i("parameters"),l6=p(),Nc=n("td"),i6=i("dict containing the following keys:"),u6=p(),Pn=n("tr"),xc=n("td"),c6=i("top_k"),f6=p(),qe=n("td"),p6=i("(Default: "),em=n("code"),h6=i("None"),d6=i("). Integer to define the top tokens considered within the "),tm=n("code"),g6=i("sample"),m6=i(" operation to create new text."),$6=p(),Rn=n("tr"),Ic=n("td"),q6=i("top_p"),_6=p(),le=n("td"),v6=i("(Default: "),sm=n("code"),E6=i("None"),y6=i("). Float to define the tokens that are within the "),am=n("code"),w6=i("sample"),b6=i(" operation of text generation. Add tokens in the sample for more probable to least probable until the sum of the probabilities is greater than "),rm=n("code"),T6=i("top_p"),j6=i("."),k6=p(),Sn=n("tr"),Hc=n("td"),A6=i("temperature"),D6=p(),_e=n("td"),O6=i("(Default: "),nm=n("code"),P6=i("1.0"),R6=i("). Float (0.0-100.0). The temperature of the sampling operation. 1 means regular sampling, 0 means "),om=n("code"),S6=i("100.0"),N6=i(" is getting closer to uniform probability."),x6=p(),Nn=n("tr"),Bc=n("td"),I6=i("repetition_penalty"),H6=p(),ls=n("td"),B6=i("(Default: "),lm=n("code"),C6=i("None"),G6=i("). Float (0.0-100.0). The more a token is used within generation the more it is penalized to not be picked in successive generation passes."),L6=p(),xn=n("tr"),Cc=n("td"),U6=i("max_new_tokens"),z6=p(),ve=n("td"),M6=i("(Default: "),im=n("code"),K6=i("None"),F6=i("). Int (0-250). The amount of new tokens to be generated, this does "),um=n("strong"),J6=i("not"),W6=i(" include the input length it is a estimate of the size of generated text you want. Each new tokens slows down the request, so look for balance between response times and length of text generated."),Y6=p(),In=n("tr"),Gc=n("td"),V6=i("max_time"),X6=p(),Ee=n("td"),Q6=i("(Default: "),cm=n("code"),Z6=i("None"),e7=i("). Float (0-120.0). The amount of time in seconds that the query should take maximum. Network can cause some overhead so it will be a soft limit. Use that in combination with "),fm=n("code"),t7=i("max_new_tokens"),s7=i(" for best results."),a7=p(),Hn=n("tr"),Lc=n("td"),r7=i("return_full_text"),n7=p(),ye=n("td"),o7=i("(Default: "),pm=n("code"),l7=i("True"),i7=i("). Bool. If set to False, the return results will "),hm=n("strong"),u7=i("not"),c7=i(" contain the original query making it easier for prompting."),f7=p(),Bn=n("tr"),Uc=n("td"),p7=i("num_return_sequences"),h7=p(),is=n("td"),d7=i("(Default: "),dm=n("code"),g7=i("1"),m7=i("). Integer. The number of proposition you want to be returned."),$7=p(),Cn=n("tr"),zc=n("td"),q7=i("do_sample"),_7=p(),us=n("td"),v7=i("(Optional: "),gm=n("code"),E7=i("True"),y7=i("). Bool. Whether or not to use sampling, use greedy decoding otherwise."),w7=p(),Gn=n("tr"),Mc=n("td"),mm=n("strong"),b7=i("options"),T7=p(),Kc=n("td"),j7=i("a dict containing the following keys:"),k7=p(),Ln=n("tr"),Fc=n("td"),A7=i("use_gpu"),D7=p(),cs=n("td"),O7=i("(Default: "),$m=n("code"),P7=i("false"),R7=i("). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),S7=p(),Un=n("tr"),Jc=n("td"),N7=i("use_cache"),x7=p(),fs=n("td"),I7=i("(Default: "),qm=n("code"),H7=i("true"),B7=i("). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),C7=p(),zn=n("tr"),Wc=n("td"),G7=i("wait_for_model"),L7=p(),ps=n("td"),U7=i("(Default: "),_m=n("code"),z7=i("false"),M7=i(") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),dv=p(),Yc=n("p"),K7=i("Return value is either a dict or a list of dicts if you sent a list of inputs"),gv=p(),_(hs.$$.fragment),mv=p(),ds=n("table"),vm=n("thead"),Mn=n("tr"),Vc=n("th"),F7=i("Returned values"),J7=p(),Em=n("th"),W7=p(),ym=n("tbody"),Kn=n("tr"),Xc=n("td"),wm=n("strong"),Y7=i("generated_text"),V7=p(),Qc=n("td"),X7=i("The continuated string"),$v=p(),Ue=n("h3"),gs=n("a"),bm=n("span"),_(Fn.$$.fragment),Q7=p(),Tm=n("span"),Z7=i("Text2Text Generation task"),qv=p(),ms=n("p"),e9=i("Essentially "),Zc=n("a"),t9=i("Text-generation task"),s9=i(`. But uses
Encoder-Decoder architecture, so might change in the future for more
options.`),_v=p(),ze=n("h3"),$s=n("a"),jm=n("span"),_(Jn.$$.fragment),a9=p(),km=n("span"),r9=i("Token Classification task"),vv=p(),ef=n("p"),n9=i(`Usually used for sentence parsing, either grammatical, or Named Entity
Recognition (NER) to understand keywords contained within text.`),Ev=p(),_(qs.$$.fragment),yv=p(),Me=n("p"),o9=i("Available with: "),Wn=n("a"),l9=i("\u{1F917} Transformers"),i9=i(`,
`),Yn=n("a"),u9=i("Flair"),wv=p(),tf=n("p"),c9=i("Example:"),bv=p(),_(_s.$$.fragment),Tv=p(),sf=n("p"),f9=i(`When sending your request, you should send a JSON encoded payload. Here
are all the options`),jv=p(),vs=n("table"),Am=n("thead"),Vn=n("tr"),af=n("th"),p9=i("All parameters"),h9=p(),Dm=n("th"),d9=p(),F=n("tbody"),Xn=n("tr"),Qn=n("td"),Om=n("strong"),g9=i("inputs"),m9=i(" (required)"),$9=p(),rf=n("td"),q9=i("a string to be classified"),_9=p(),Zn=n("tr"),nf=n("td"),Pm=n("strong"),v9=i("parameters"),E9=p(),of=n("td"),y9=i("a dict containing the following key:"),w9=p(),eo=n("tr"),lf=n("td"),b9=i("aggregation_strategy"),T9=p(),x=n("td"),j9=i("(Default: "),Rm=n("code"),k9=i("simple"),A9=i("). There are several aggregation strategies: "),D9=n("br"),O9=p(),Sm=n("code"),P9=i("none"),R9=i(": Every token gets classified without further aggregation. "),S9=n("br"),N9=p(),Nm=n("code"),x9=i("simple"),I9=i(": Entities are grouped according to the default schema (B-, I- tags get merged when the tag is similar). "),H9=n("br"),B9=p(),xm=n("code"),C9=i("first"),G9=i(": Same as the "),Im=n("code"),L9=i("simple"),U9=i(" strategy except words cannot end up with different tags. Words will use the tag of the first token when there is ambiguity. "),z9=n("br"),M9=p(),Hm=n("code"),K9=i("average"),F9=i(": Same as the "),Bm=n("code"),J9=i("simple"),W9=i(" strategy except words cannot end up with different tags. Scores are averaged across tokens and then the maximum label is applied. "),Y9=n("br"),V9=p(),Cm=n("code"),X9=i("max"),Q9=i(": Same as the "),Gm=n("code"),Z9=i("simple"),e8=i(" strategy except words cannot end up with different tags. Word entity will be the token with the maximum score."),t8=p(),to=n("tr"),uf=n("td"),Lm=n("strong"),s8=i("options"),a8=p(),cf=n("td"),r8=i("a dict containing the following keys:"),n8=p(),so=n("tr"),ff=n("td"),o8=i("use_gpu"),l8=p(),Es=n("td"),i8=i("(Default: "),Um=n("code"),u8=i("false"),c8=i("). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),f8=p(),ao=n("tr"),pf=n("td"),p8=i("use_cache"),h8=p(),ys=n("td"),d8=i("(Default: "),zm=n("code"),g8=i("true"),m8=i("). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),$8=p(),ro=n("tr"),hf=n("td"),q8=i("wait_for_model"),_8=p(),ws=n("td"),v8=i("(Default: "),Mm=n("code"),E8=i("false"),y8=i(") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),kv=p(),df=n("p"),w8=i("Return value is either a dict or a list of dicts if you sent a list of inputs"),Av=p(),_(bs.$$.fragment),Dv=p(),Ts=n("table"),Km=n("thead"),no=n("tr"),gf=n("th"),b8=i("Returned values"),T8=p(),Fm=n("th"),j8=p(),te=n("tbody"),oo=n("tr"),mf=n("td"),Jm=n("strong"),k8=i("entity_group"),A8=p(),$f=n("td"),D8=i("The type for the entity being recognized (model specific)."),O8=p(),lo=n("tr"),qf=n("td"),Wm=n("strong"),P8=i("score"),R8=p(),_f=n("td"),S8=i("How likely the entity was recognized."),N8=p(),io=n("tr"),vf=n("td"),Ym=n("strong"),x8=i("word"),I8=p(),Ef=n("td"),H8=i("The string that was captured"),B8=p(),uo=n("tr"),yf=n("td"),Vm=n("strong"),C8=i("start"),G8=p(),js=n("td"),L8=i("The offset stringwise where the answer is located. Useful to disambiguate if "),Xm=n("code"),U8=i("word"),z8=i(" occurs multiple times."),M8=p(),co=n("tr"),wf=n("td"),Qm=n("strong"),K8=i("end"),F8=p(),ks=n("td"),J8=i("The offset stringwise where the answer is located. Useful to disambiguate if "),Zm=n("code"),W8=i("word"),Y8=i(" occurs multiple times."),Ov=p(),Ke=n("h3"),As=n("a"),e$=n("span"),_(fo.$$.fragment),V8=p(),t$=n("span"),X8=i("Named Entity Recognition (NER) task"),Pv=p(),po=n("p"),Q8=i("See "),bf=n("a"),Z8=i("Token-classification task"),Rv=p(),Fe=n("h3"),Ds=n("a"),s$=n("span"),_(ho.$$.fragment),eA=p(),a$=n("span"),tA=i("Translation task"),Sv=p(),Tf=n("p"),sA=i("This task is well known to translate text from one language to another"),Nv=p(),_(Os.$$.fragment),xv=p(),go=n("p"),aA=i("Available with: "),mo=n("a"),rA=i("\u{1F917} Transformers"),Iv=p(),jf=n("p"),nA=i("Example:"),Hv=p(),_(Ps.$$.fragment),Bv=p(),kf=n("p"),oA=i(`When sending your request, you should send a JSON encoded payload. Here
are all the options`),Cv=p(),Rs=n("table"),r$=n("thead"),$o=n("tr"),Af=n("th"),lA=i("All parameters"),iA=p(),n$=n("th"),uA=p(),se=n("tbody"),qo=n("tr"),_o=n("td"),o$=n("strong"),cA=i("inputs"),fA=i(" (required)"),pA=p(),Df=n("td"),hA=i("a string to be translated in the original languages"),dA=p(),vo=n("tr"),Of=n("td"),l$=n("strong"),gA=i("options"),mA=p(),Pf=n("td"),$A=i("a dict containing the following keys:"),qA=p(),Eo=n("tr"),Rf=n("td"),_A=i("use_gpu"),vA=p(),Ss=n("td"),EA=i("(Default: "),i$=n("code"),yA=i("false"),wA=i("). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),bA=p(),yo=n("tr"),Sf=n("td"),TA=i("use_cache"),jA=p(),Ns=n("td"),kA=i("(Default: "),u$=n("code"),AA=i("true"),DA=i("). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),OA=p(),wo=n("tr"),Nf=n("td"),PA=i("wait_for_model"),RA=p(),xs=n("td"),SA=i("(Default: "),c$=n("code"),NA=i("false"),xA=i(") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),Gv=p(),xf=n("p"),IA=i("Return value is either a dict or a list of dicts if you sent a list of inputs"),Lv=p(),Is=n("table"),f$=n("thead"),bo=n("tr"),If=n("th"),HA=i("Returned values"),BA=p(),p$=n("th"),CA=p(),h$=n("tbody"),To=n("tr"),Hf=n("td"),d$=n("strong"),GA=i("translation_text"),LA=p(),Bf=n("td"),UA=i("The string after translation"),Uv=p(),Je=n("h3"),Hs=n("a"),g$=n("span"),_(jo.$$.fragment),zA=p(),m$=n("span"),MA=i("Zero-Shot Classification task"),zv=p(),Cf=n("p"),KA=i(`This task is super useful to try out classification with zero code,
you simply pass a sentence/paragraph and the possible labels for that
sentence, and you get a result.`),Mv=p(),_(Bs.$$.fragment),Kv=p(),ko=n("p"),FA=i("Available with: "),Ao=n("a"),JA=i("\u{1F917} Transformers"),Fv=p(),Gf=n("p"),WA=i("Request:"),Jv=p(),_(Cs.$$.fragment),Wv=p(),Lf=n("p"),YA=i(`When sending your request, you should send a JSON encoded payload. Here
are all the options`),Yv=p(),Gs=n("table"),$$=n("thead"),Do=n("tr"),Uf=n("th"),VA=i("All parameters"),XA=p(),q$=n("th"),QA=p(),M=n("tbody"),Oo=n("tr"),Po=n("td"),_$=n("strong"),ZA=i("inputs"),eD=i(" (required)"),tD=p(),zf=n("td"),sD=i("a string or list of strings"),aD=p(),Ro=n("tr"),So=n("td"),v$=n("strong"),rD=i("parameters"),nD=i(" (required)"),oD=p(),Mf=n("td"),lD=i("a dict containing the following keys:"),iD=p(),No=n("tr"),Kf=n("td"),uD=i("candidate_labels (required)"),cD=p(),we=n("td"),fD=i("a list of strings that are potential classes for "),E$=n("code"),pD=i("inputs"),hD=i(". (max 10 candidate_labels, for more, simply run multiple requests, results are going to be misleading if using too many candidate_labels anyway. If you want to keep the exact same, you can simply run "),y$=n("code"),dD=i("multi_label=True"),gD=i(" and do the scaling on your end. )"),mD=p(),xo=n("tr"),Ff=n("td"),$D=i("multi_label"),qD=p(),Ls=n("td"),_D=i("(Default: "),w$=n("code"),vD=i("false"),ED=i(") Boolean that is set to True if classes can overlap"),yD=p(),Io=n("tr"),Jf=n("td"),b$=n("strong"),wD=i("options"),bD=p(),Wf=n("td"),TD=i("a dict containing the following keys:"),jD=p(),Ho=n("tr"),Yf=n("td"),kD=i("use_gpu"),AD=p(),Us=n("td"),DD=i("(Default: "),T$=n("code"),OD=i("false"),PD=i("). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),RD=p(),Bo=n("tr"),Vf=n("td"),SD=i("use_cache"),ND=p(),zs=n("td"),xD=i("(Default: "),j$=n("code"),ID=i("true"),HD=i("). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),BD=p(),Co=n("tr"),Xf=n("td"),CD=i("wait_for_model"),GD=p(),Ms=n("td"),LD=i("(Default: "),k$=n("code"),UD=i("false"),zD=i(") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),Vv=p(),Qf=n("p"),MD=i("Return value is either a dict or a list of dicts if you sent a list of inputs"),Xv=p(),Zf=n("p"),KD=i("Response:"),Qv=p(),_(Ks.$$.fragment),Zv=p(),Fs=n("table"),A$=n("thead"),Go=n("tr"),ep=n("th"),FD=i("Returned values"),JD=p(),D$=n("th"),WD=p(),We=n("tbody"),Lo=n("tr"),tp=n("td"),O$=n("strong"),YD=i("sequence"),VD=p(),sp=n("td"),XD=i("The string sent as an input"),QD=p(),Uo=n("tr"),ap=n("td"),P$=n("strong"),ZD=i("labels"),eO=p(),rp=n("td"),tO=i("The list of strings for labels that you sent (in order)"),sO=p(),zo=n("tr"),np=n("td"),R$=n("strong"),aO=i("scores"),rO=p(),Js=n("td"),nO=i("a list of floats that correspond the the probability of label, in the same order as "),S$=n("code"),oO=i("labels"),lO=i("."),e2=p(),Ye=n("h3"),Ws=n("a"),N$=n("span"),_(Mo.$$.fragment),iO=p(),x$=n("span"),uO=i("Conversational task"),t2=p(),op=n("p"),cO=i(`This task corresponds to any chatbot like structure. Models tend to have
shorter max_length, so please check with caution when using a given
model if you need long range dependency or not.`),s2=p(),_(Ys.$$.fragment),a2=p(),Ko=n("p"),fO=i("Available with: "),Fo=n("a"),pO=i("\u{1F917} Transformers"),r2=p(),lp=n("p"),hO=i("Example:"),n2=p(),_(Vs.$$.fragment),o2=p(),ip=n("p"),dO=i(`When sending your request, you should send a JSON encoded payload. Here
are all the options`),l2=p(),Xs=n("table"),I$=n("thead"),Jo=n("tr"),up=n("th"),gO=i("All parameters"),mO=p(),H$=n("th"),$O=p(),N=n("tbody"),Wo=n("tr"),Yo=n("td"),B$=n("strong"),qO=i("inputs"),_O=i(" (required)"),vO=p(),C$=n("td"),EO=p(),Vo=n("tr"),cp=n("td"),yO=i("text (required)"),wO=p(),fp=n("td"),bO=i("The last input from the user in the conversation."),TO=p(),Xo=n("tr"),pp=n("td"),jO=i("generated_responses"),kO=p(),hp=n("td"),AO=i("A list of strings corresponding to the earlier replies from the model."),DO=p(),Qo=n("tr"),dp=n("td"),OO=i("past_user_inputs"),PO=p(),Qs=n("td"),RO=i("A list of strings corresponding to the earlier replies from the user. Should be of the same length of "),G$=n("code"),SO=i("generated_responses"),NO=i("."),xO=p(),Zo=n("tr"),gp=n("td"),L$=n("strong"),IO=i("parameters"),HO=p(),mp=n("td"),BO=i("a dict containing the following keys:"),CO=p(),el=n("tr"),$p=n("td"),GO=i("min_length"),LO=p(),be=n("td"),UO=i("(Default: "),U$=n("code"),zO=i("None"),MO=i("). Integer to define the minimum length "),z$=n("strong"),KO=i("in tokens"),FO=i(" of the output summary."),JO=p(),tl=n("tr"),qp=n("td"),WO=i("max_length"),YO=p(),Te=n("td"),VO=i("(Default: "),M$=n("code"),XO=i("None"),QO=i("). Integer to define the maximum length "),K$=n("strong"),ZO=i("in tokens"),eP=i(" of the output summary."),tP=p(),sl=n("tr"),_p=n("td"),sP=i("top_k"),aP=p(),je=n("td"),rP=i("(Default: "),F$=n("code"),nP=i("None"),oP=i("). Integer to define the top tokens considered within the "),J$=n("code"),lP=i("sample"),iP=i(" operation to create new text."),uP=p(),al=n("tr"),vp=n("td"),cP=i("top_p"),fP=p(),ie=n("td"),pP=i("(Default: "),W$=n("code"),hP=i("None"),dP=i("). Float to define the tokens that are within the "),Y$=n("code"),gP=i("sample"),mP=i(" operation of text generation. Add tokens in the sample for more probable to least probable until the sum of the probabilities is greater than "),V$=n("code"),$P=i("top_p"),qP=i("."),_P=p(),rl=n("tr"),Ep=n("td"),vP=i("temperature"),EP=p(),ke=n("td"),yP=i("(Default: "),X$=n("code"),wP=i("1.0"),bP=i("). Float (0.0-100.0). The temperature of the sampling operation. 1 means regular sampling, 0 means "),Q$=n("code"),TP=i("100.0"),jP=i(" is getting closer to uniform probability."),kP=p(),nl=n("tr"),yp=n("td"),AP=i("repetition_penalty"),DP=p(),Zs=n("td"),OP=i("(Default: "),Z$=n("code"),PP=i("None"),RP=i("). Float (0.0-100.0). The more a token is used within generation the more it is penalized to not be picked in successive generation passes."),SP=p(),ol=n("tr"),wp=n("td"),NP=i("max_time"),xP=p(),ea=n("td"),IP=i("(Default: "),eq=n("code"),HP=i("None"),BP=i("). Float (0-120.0). The amount of time in seconds that the query should take maximum. Network can cause some overhead so it will be a soft limit."),CP=p(),ll=n("tr"),bp=n("td"),tq=n("strong"),GP=i("options"),LP=p(),Tp=n("td"),UP=i("a dict containing the following keys:"),zP=p(),il=n("tr"),jp=n("td"),MP=i("use_gpu"),KP=p(),ta=n("td"),FP=i("(Default: "),sq=n("code"),JP=i("false"),WP=i("). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),YP=p(),ul=n("tr"),kp=n("td"),VP=i("use_cache"),XP=p(),sa=n("td"),QP=i("(Default: "),aq=n("code"),ZP=i("true"),eR=i("). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),tR=p(),cl=n("tr"),Ap=n("td"),sR=i("wait_for_model"),aR=p(),aa=n("td"),rR=i("(Default: "),rq=n("code"),nR=i("false"),oR=i(") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),i2=p(),Dp=n("p"),lR=i("Return value is either a dict or a list of dicts if you sent a list of inputs"),u2=p(),ra=n("table"),nq=n("thead"),fl=n("tr"),Op=n("th"),iR=i("Returned values"),uR=p(),oq=n("th"),cR=p(),pe=n("tbody"),pl=n("tr"),Pp=n("td"),lq=n("strong"),fR=i("generated_text"),pR=p(),Rp=n("td"),hR=i("The answer of the bot"),dR=p(),hl=n("tr"),Sp=n("td"),iq=n("strong"),gR=i("conversation"),mR=p(),Np=n("td"),$R=i("A facility dictionnary to send back for the next input (with the new user input addition)."),qR=p(),dl=n("tr"),xp=n("td"),_R=i("past_user_inputs"),vR=p(),Ip=n("td"),ER=i("List of strings. The last inputs from the user in the conversation, <em>after the model has run."),yR=p(),gl=n("tr"),Hp=n("td"),wR=i("generated_responses"),bR=p(),Bp=n("td"),TR=i("List of strings. The last outputs from the model in the conversation, <em>after the model has run."),c2=p(),Ve=n("h3"),na=n("a"),uq=n("span"),_(ml.$$.fragment),jR=p(),cq=n("span"),kR=i("Feature Extraction task"),f2=p(),Cp=n("p"),AR=i(`This task reads some text and outputs raw float values, that are usually
consumed as part of a semantic database/semantic search.`),p2=p(),_(oa.$$.fragment),h2=p(),Xe=n("p"),DR=i("Available with: "),$l=n("a"),OR=i("\u{1F917} Transformers"),PR=p(),ql=n("a"),RR=i("Sentence-transformers"),d2=p(),Gp=n("p"),SR=i("Request:"),g2=p(),la=n("table"),fq=n("thead"),_l=n("tr"),Lp=n("th"),NR=i("All parameters"),xR=p(),pq=n("th"),IR=p(),ae=n("tbody"),vl=n("tr"),El=n("td"),hq=n("strong"),HR=i("inputs"),BR=i(" (required):"),CR=p(),Up=n("td"),GR=i("a string or a list of strings to get the features from."),LR=p(),yl=n("tr"),zp=n("td"),dq=n("strong"),UR=i("options"),zR=p(),Mp=n("td"),MR=i("a dict containing the following keys:"),KR=p(),wl=n("tr"),Kp=n("td"),FR=i("use_gpu"),JR=p(),ia=n("td"),WR=i("(Default: "),gq=n("code"),YR=i("false"),VR=i("). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),XR=p(),bl=n("tr"),Fp=n("td"),QR=i("use_cache"),ZR=p(),ua=n("td"),eS=i("(Default: "),mq=n("code"),tS=i("true"),sS=i("). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),aS=p(),Tl=n("tr"),Jp=n("td"),rS=i("wait_for_model"),nS=p(),ca=n("td"),oS=i("(Default: "),$q=n("code"),lS=i("false"),iS=i(") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),m2=p(),Wp=n("p"),uS=i("Return value is either a dict or a list of dicts if you sent a list of inputs"),$2=p(),fa=n("table"),qq=n("thead"),jl=n("tr"),Yp=n("th"),cS=i("Returned values"),fS=p(),_q=n("th"),pS=p(),vq=n("tbody"),kl=n("tr"),Vp=n("td"),Eq=n("strong"),hS=i("A list of float (or list of list of floats)"),dS=p(),Xp=n("td"),gS=i("The numbers that are the representation features of the input."),q2=p(),Qp=n("small"),mS=i(`Returned values are a list of floats, or a list of list of floats
(depending on if you sent a string or a list of string, and if the
automatic reduction, usually mean_pooling for instance was applied for
you or not. This should be explained on the model's README.`),_2=p(),Qe=n("h2"),pa=n("a"),yq=n("span"),_(Al.$$.fragment),$S=p(),wq=n("span"),qS=i("Audio"),v2=p(),Ze=n("h3"),ha=n("a"),bq=n("span"),_(Dl.$$.fragment),_S=p(),Tq=n("span"),vS=i("Automatic Speech Recognition task"),E2=p(),Zp=n("p"),ES=i(`This task reads some audio input and outputs the said words within the
audio files.`),y2=p(),_(da.$$.fragment),w2=p(),_(ga.$$.fragment),b2=p(),he=n("p"),yS=i("Available with: "),Ol=n("a"),wS=i("\u{1F917} Transformers"),bS=p(),Pl=n("a"),TS=i("ESPnet"),jS=i(` and
`),Rl=n("a"),kS=i("SpeechBrain"),T2=p(),eh=n("p"),AS=i("Request:"),j2=p(),_(ma.$$.fragment),k2=p(),th=n("p"),DS=i(`When sending your request, you should send a binary payload that simply
contains your audio file. We try to support most formats (Flac, Wav,
Mp3, Ogg etc...). And we automatically rescale the sampling rate to the
appropriate rate for the given model (usually 16KHz).`),A2=p(),$a=n("table"),jq=n("thead"),Sl=n("tr"),sh=n("th"),OS=i("All parameters"),PS=p(),kq=n("th"),RS=p(),Aq=n("tbody"),Nl=n("tr"),xl=n("td"),Dq=n("strong"),SS=i("no parameter"),NS=i(" (required)"),xS=p(),ah=n("td"),IS=i("a binary representation of the audio file. No other parameters are currently allowed."),D2=p(),rh=n("p"),HS=i("Return value is either a dict or a list of dicts if you sent a list of inputs"),O2=p(),nh=n("p"),BS=i("Response:"),P2=p(),_(qa.$$.fragment),R2=p(),_a=n("table"),Oq=n("thead"),Il=n("tr"),oh=n("th"),CS=i("Returned values"),GS=p(),Pq=n("th"),LS=p(),Rq=n("tbody"),Hl=n("tr"),lh=n("td"),Sq=n("strong"),US=i("text"),zS=p(),ih=n("td"),MS=i("The string that was recognized within the audio file."),S2=p(),et=n("h3"),va=n("a"),Nq=n("span"),_(Bl.$$.fragment),KS=p(),xq=n("span"),FS=i("Audio Classification task"),N2=p(),uh=n("p"),JS=i("This task reads some audio input and outputs the likelihood of classes."),x2=p(),_(Ea.$$.fragment),I2=p(),tt=n("p"),WS=i("Available with: "),Cl=n("a"),YS=i("\u{1F917} Transformers"),VS=p(),Gl=n("a"),XS=i("SpeechBrain"),H2=p(),ch=n("p"),QS=i("Request:"),B2=p(),_(ya.$$.fragment),C2=p(),fh=n("p"),ZS=i(`When sending your request, you should send a binary payload that simply
contains your audio file. We try to support most formats (Flac, Wav,
Mp3, Ogg etc...). And we automatically rescale the sampling rate to the
appropriate rate for the given model (usually 16KHz).`),G2=p(),wa=n("table"),Iq=n("thead"),Ll=n("tr"),ph=n("th"),eN=i("All parameters"),tN=p(),Hq=n("th"),sN=p(),Bq=n("tbody"),Ul=n("tr"),zl=n("td"),Cq=n("strong"),aN=i("no parameter"),rN=i(" (required)"),nN=p(),hh=n("td"),oN=i("a binary representation of the audio file. No other parameters are currently allowed."),L2=p(),dh=n("p"),lN=i("Return value is a dict"),U2=p(),_(ba.$$.fragment),z2=p(),Ta=n("table"),Gq=n("thead"),Ml=n("tr"),gh=n("th"),iN=i("Returned values"),uN=p(),Lq=n("th"),cN=p(),Kl=n("tbody"),Fl=n("tr"),mh=n("td"),Uq=n("strong"),fN=i("label"),pN=p(),$h=n("td"),hN=i("The label for the class (model specific)"),dN=p(),Jl=n("tr"),qh=n("td"),zq=n("strong"),gN=i("score"),mN=p(),_h=n("td"),$N=i("A float that represents how likely it is that the audio file belongs to this class."),M2=p(),st=n("h2"),ja=n("a"),Mq=n("span"),_(Wl.$$.fragment),qN=p(),Kq=n("span"),_N=i("Computer Vision"),K2=p(),at=n("h3"),ka=n("a"),Fq=n("span"),_(Yl.$$.fragment),vN=p(),Jq=n("span"),EN=i("Image Classification task"),F2=p(),vh=n("p"),yN=i("This task reads some image input and outputs the likelihood of classes."),J2=p(),_(Aa.$$.fragment),W2=p(),Vl=n("p"),wN=i("Available with: "),Xl=n("a"),bN=i("\u{1F917} Transformers"),Y2=p(),Eh=n("p"),TN=i("Request:"),V2=p(),_(Da.$$.fragment),X2=p(),Oa=n("p"),jN=i(`When sending your request, you should send a binary payload that simply
contains your image file. We support all image formats `),Ql=n("a"),kN=i(`Pillow
supports`),AN=i("."),Q2=p(),Pa=n("table"),Wq=n("thead"),Zl=n("tr"),yh=n("th"),DN=i("All parameters"),ON=p(),Yq=n("th"),PN=p(),Vq=n("tbody"),ei=n("tr"),ti=n("td"),Xq=n("strong"),RN=i("no parameter"),SN=i(" (required)"),NN=p(),wh=n("td"),xN=i("a binary representation of the image file. No other parameters are currently allowed."),Z2=p(),bh=n("p"),IN=i("Return value is a dict"),eE=p(),_(Ra.$$.fragment),tE=p(),Sa=n("table"),Qq=n("thead"),si=n("tr"),Th=n("th"),HN=i("Returned values"),BN=p(),Zq=n("th"),CN=p(),ai=n("tbody"),ri=n("tr"),jh=n("td"),e_=n("strong"),GN=i("label"),LN=p(),kh=n("td"),UN=i("The label for the class (model specific)"),zN=p(),ni=n("tr"),Ah=n("td"),t_=n("strong"),MN=i("score"),KN=p(),Dh=n("td"),FN=i("A float that represents how likely it is that the image file belongs to this class."),sE=p(),rt=n("h3"),Na=n("a"),s_=n("span"),_(oi.$$.fragment),JN=p(),a_=n("span"),WN=i("Object Detection task"),aE=p(),Oh=n("p"),YN=i(`This task reads some image input and outputs the likelihood of classes &
bounding boxes of detected objects.`),rE=p(),_(xa.$$.fragment),nE=p(),li=n("p"),VN=i("Available with: "),ii=n("a"),XN=i("\u{1F917} Transformers"),oE=p(),Ph=n("p"),QN=i("Request:"),lE=p(),_(Ia.$$.fragment),iE=p(),Ha=n("p"),ZN=i(`When sending your request, you should send a binary payload that simply
contains your image file. We support all image formats `),ui=n("a"),ex=i(`Pillow
supports`),tx=i("."),uE=p(),Ba=n("table"),r_=n("thead"),ci=n("tr"),Rh=n("th"),sx=i("All parameters"),ax=p(),n_=n("th"),rx=p(),o_=n("tbody"),fi=n("tr"),pi=n("td"),l_=n("strong"),nx=i("no parameter"),ox=i(" (required)"),lx=p(),Sh=n("td"),ix=i("a binary representation of the image file. No other parameters are currently allowed."),cE=p(),Nh=n("p"),ux=i("Return value is a dict"),fE=p(),_(Ca.$$.fragment),pE=p(),Ga=n("table"),i_=n("thead"),hi=n("tr"),xh=n("th"),cx=i("Returned values"),fx=p(),u_=n("th"),px=p(),nt=n("tbody"),di=n("tr"),Ih=n("td"),c_=n("strong"),hx=i("label"),dx=p(),Hh=n("td"),gx=i("The label for the class (model specific) of a detected object."),mx=p(),gi=n("tr"),Bh=n("td"),f_=n("strong"),$x=i("score"),qx=p(),Ch=n("td"),_x=i("A float that represents how likely it is that the detected object belongs to the given class."),vx=p(),mi=n("tr"),Gh=n("td"),p_=n("strong"),Ex=i("box"),yx=p(),Lh=n("td"),wx=i("A dict (with keys [xmin,ymin,xmax,ymax]) representing the bounding box of a detected object."),hE=p(),ot=n("h3"),La=n("a"),h_=n("span"),_($i.$$.fragment),bx=p(),d_=n("span"),Tx=i("Image Segmentation task"),dE=p(),Uh=n("p"),jx=i(`This task reads some image input and outputs the likelihood of classes &
bounding boxes of detected objects.`),gE=p(),_(Ua.$$.fragment),mE=p(),qi=n("p"),kx=i("Available with: "),_i=n("a"),Ax=i("\u{1F917} Transformers"),$E=p(),zh=n("p"),Dx=i("Request:"),qE=p(),_(za.$$.fragment),_E=p(),Ma=n("p"),Ox=i(`When sending your request, you should send a binary payload that simply
contains your image file. We support all image formats `),vi=n("a"),Px=i(`Pillow
supports`),Rx=i("."),vE=p(),Ka=n("table"),g_=n("thead"),Ei=n("tr"),Mh=n("th"),Sx=i("All parameters"),Nx=p(),m_=n("th"),xx=p(),$_=n("tbody"),yi=n("tr"),wi=n("td"),q_=n("strong"),Ix=i("no parameter"),Hx=i(" (required)"),Bx=p(),Kh=n("td"),Cx=i("a binary representation of the image file. No other parameters are currently allowed."),EE=p(),Fh=n("p"),Gx=i("Return value is a dict"),yE=p(),_(Fa.$$.fragment),wE=p(),Ja=n("table"),__=n("thead"),bi=n("tr"),Jh=n("th"),Lx=i("Returned values"),Ux=p(),v_=n("th"),zx=p(),lt=n("tbody"),Ti=n("tr"),Wh=n("td"),E_=n("strong"),Mx=i("label"),Kx=p(),Yh=n("td"),Fx=i("The label for the class (model specific) of a segment."),Jx=p(),ji=n("tr"),Vh=n("td"),y_=n("strong"),Wx=i("score"),Yx=p(),Xh=n("td"),Vx=i("A float that represents how likely it is that the segment belongs to the given class."),Xx=p(),ki=n("tr"),Qh=n("td"),w_=n("strong"),Qx=i("mask"),Zx=p(),Zh=n("td"),eI=i("A str (base64 str of a single channel black-and-white img) representing the mask of a segment."),this.h()},l(a){const g=GF('[data-svelte="svelte-1phssyn"]',document.head);r=o(g,"META",{name:!0,content:!0}),g.forEach(t),c=h(a),s=o(a,"H1",{class:!0});var Ai=l(s);d=o(Ai,"A",{id:!0,class:!0,href:!0});var b_=l(d);$=o(b_,"SPAN",{});var T_=l($);v(k.$$.fragment,T_),T_.forEach(t),b_.forEach(t),A=h(Ai),j=o(Ai,"SPAN",{});var j_=l(j);T=u(j_,"Detailed parameters"),j_.forEach(t),Ai.forEach(t),S=h(a),D=o(a,"H2",{class:!0});var Di=l(D);ne=o(Di,"A",{id:!0,class:!0,href:!0});var k_=l(ne);Re=o(k_,"SPAN",{});var A_=l(Re);v(Q.$$.fragment,A_),A_.forEach(t),k_.forEach(t),Y=h(Di),it=o(Di,"SPAN",{});var D_=l(it);Li=u(D_,"Which task is used by this model ?"),D_.forEach(t),Di.forEach(t),tr=h(a),Se=o(a,"P",{});var O_=l(Se);p3=u(O_,`In general the \u{1F917} Hosted API Inference accepts a simple string as an
input. However, more advanced usage depends on the \u201Ctask\u201D that the
model solves.`),O_.forEach(t),a1=h(a),Ui=o(a,"P",{});var P_=l(Ui);h3=u(P_,"The \u201Ctask\u201D of a model is defined here on it\u2019s model page:"),P_.forEach(t),r1=h(a),ut=o(a,"IMG",{class:!0,src:!0,width:!0}),n1=h(a),ct=o(a,"IMG",{class:!0,src:!0,width:!0}),o1=h(a),Ne=o(a,"H2",{class:!0});var Oi=l(Ne);ft=o(Oi,"A",{id:!0,class:!0,href:!0});var R_=l(ft);_d=o(R_,"SPAN",{});var S_=l(_d);v(sr.$$.fragment,S_),S_.forEach(t),R_.forEach(t),d3=h(Oi),vd=o(Oi,"SPAN",{});var N_=l(vd);g3=u(N_,"Natural Language Processing"),N_.forEach(t),Oi.forEach(t),l1=h(a),xe=o(a,"H3",{class:!0});var Pi=l(xe);pt=o(Pi,"A",{id:!0,class:!0,href:!0});var x_=l(pt);Ed=o(x_,"SPAN",{});var I_=l(Ed);v(ar.$$.fragment,I_),I_.forEach(t),x_.forEach(t),m3=h(Pi),yd=o(Pi,"SPAN",{});var H_=l(yd);$3=u(H_,"Fill Mask task"),H_.forEach(t),Pi.forEach(t),i1=h(a),zi=o(a,"P",{});var B_=l(zi);q3=u(B_,`Tries to fill in a hole with a missing word (token to be precise).
That\u2019s the base task for BERT models.`),B_.forEach(t),u1=h(a),v(ht.$$.fragment,a),c1=h(a),rr=o(a,"P",{});var ed=l(rr);_3=u(ed,"Available with: "),nr=o(ed,"A",{href:!0,rel:!0});var C_=l(nr);v3=u(C_,"\u{1F917} Transformers"),C_.forEach(t),ed.forEach(t),f1=h(a),Mi=o(a,"P",{});var G_=l(Mi);E3=u(G_,"Example:"),G_.forEach(t),p1=h(a),v(dt.$$.fragment,a),h1=h(a),Ki=o(a,"P",{});var L_=l(Ki);y3=u(L_,`When sending your request, you should send a JSON encoded payload. Here
are all the options`),L_.forEach(t),d1=h(a),gt=o(a,"TABLE",{});var Ri=l(gt);wd=o(Ri,"THEAD",{});var U_=l(wd);or=o(U_,"TR",{});var Si=l(or);Fi=o(Si,"TH",{align:!0});var z_=l(Fi);w3=u(z_,"All parameters"),z_.forEach(t),b3=h(Si),bd=o(Si,"TH",{align:!0}),l(bd).forEach(t),Si.forEach(t),U_.forEach(t),T3=h(Ri),Z=o(Ri,"TBODY",{});var re=l(Z);lr=o(re,"TR",{});var Ni=l(lr);ir=o(Ni,"TD",{align:!0});var td=l(ir);Td=o(td,"STRONG",{});var M_=l(Td);j3=u(M_,"inputs"),M_.forEach(t),k3=u(td," (required):"),td.forEach(t),A3=h(Ni),Ji=o(Ni,"TD",{align:!0});var K_=l(Ji);D3=u(K_,"a string to be filled from, must contain the [MASK] token (check model card for exact name of the mask)"),K_.forEach(t),Ni.forEach(t),O3=h(re),ur=o(re,"TR",{});var xi=l(ur);Wi=o(xi,"TD",{align:!0});var F_=l(Wi);jd=o(F_,"STRONG",{});var J_=l(jd);P3=u(J_,"options"),J_.forEach(t),F_.forEach(t),R3=h(xi),Yi=o(xi,"TD",{align:!0});var W_=l(Yi);S3=u(W_,"a dict containing the following keys:"),W_.forEach(t),xi.forEach(t),N3=h(re),cr=o(re,"TR",{});var Ii=l(cr);Vi=o(Ii,"TD",{align:!0});var Y_=l(Vi);x3=u(Y_,"use_gpu"),Y_.forEach(t),I3=h(Ii),mt=o(Ii,"TD",{align:!0});var Hi=l(mt);H3=u(Hi,"(Default: "),kd=o(Hi,"CODE",{});var V_=l(kd);B3=u(V_,"false"),V_.forEach(t),C3=u(Hi,"). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),Hi.forEach(t),Ii.forEach(t),G3=h(re),fr=o(re,"TR",{});var Bi=l(fr);Xi=o(Bi,"TD",{align:!0});var X_=l(Xi);L3=u(X_,"use_cache"),X_.forEach(t),U3=h(Bi),$t=o(Bi,"TD",{align:!0});var Ci=l($t);z3=u(Ci,"(Default: "),Ad=o(Ci,"CODE",{});var Q_=l(Ad);M3=u(Q_,"true"),Q_.forEach(t),K3=u(Ci,"). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),Ci.forEach(t),Bi.forEach(t),F3=h(re),pr=o(re,"TR",{});var TE=l(pr);Qi=o(TE,"TD",{align:!0});var DI=l(Qi);J3=u(DI,"wait_for_model"),DI.forEach(t),W3=h(TE),qt=o(TE,"TD",{align:!0});var jE=l(qt);Y3=u(jE,"(Default: "),Dd=o(jE,"CODE",{});var OI=l(Dd);V3=u(OI,"false"),OI.forEach(t),X3=u(jE,") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),jE.forEach(t),TE.forEach(t),re.forEach(t),Ri.forEach(t),g1=h(a),Zi=o(a,"P",{});var PI=l(Zi);Q3=u(PI,"Return value is either a dict or a list of dicts if you sent a list of inputs"),PI.forEach(t),m1=h(a),v(_t.$$.fragment,a),$1=h(a),vt=o(a,"TABLE",{});var kE=l(vt);Od=o(kE,"THEAD",{});var RI=l(Od);hr=o(RI,"TR",{});var AE=l(hr);eu=o(AE,"TH",{align:!0});var SI=l(eu);Z3=u(SI,"Returned values"),SI.forEach(t),eT=h(AE),Pd=o(AE,"TH",{align:!0}),l(Pd).forEach(t),AE.forEach(t),RI.forEach(t),tT=h(kE),ue=o(kE,"TBODY",{});var Wa=l(ue);dr=o(Wa,"TR",{});var DE=l(dr);tu=o(DE,"TD",{align:!0});var NI=l(tu);Rd=o(NI,"STRONG",{});var xI=l(Rd);sT=u(xI,"sequence"),xI.forEach(t),NI.forEach(t),aT=h(DE),su=o(DE,"TD",{align:!0});var II=l(su);rT=u(II,"The actual sequence of tokens that ran against the model (may contain special tokens)"),II.forEach(t),DE.forEach(t),nT=h(Wa),gr=o(Wa,"TR",{});var OE=l(gr);au=o(OE,"TD",{align:!0});var HI=l(au);Sd=o(HI,"STRONG",{});var BI=l(Sd);oT=u(BI,"score"),BI.forEach(t),HI.forEach(t),lT=h(OE),ru=o(OE,"TD",{align:!0});var CI=l(ru);iT=u(CI,"The probability for this token."),CI.forEach(t),OE.forEach(t),uT=h(Wa),mr=o(Wa,"TR",{});var PE=l(mr);nu=o(PE,"TD",{align:!0});var GI=l(nu);Nd=o(GI,"STRONG",{});var LI=l(Nd);cT=u(LI,"token"),LI.forEach(t),GI.forEach(t),fT=h(PE),ou=o(PE,"TD",{align:!0});var UI=l(ou);pT=u(UI,"The id of the token"),UI.forEach(t),PE.forEach(t),hT=h(Wa),$r=o(Wa,"TR",{});var RE=l($r);lu=o(RE,"TD",{align:!0});var zI=l(lu);xd=o(zI,"STRONG",{});var MI=l(xd);dT=u(MI,"token_str"),MI.forEach(t),zI.forEach(t),gT=h(RE),iu=o(RE,"TD",{align:!0});var KI=l(iu);mT=u(KI,"The string representation of the token"),KI.forEach(t),RE.forEach(t),Wa.forEach(t),kE.forEach(t),q1=h(a),Ie=o(a,"H3",{class:!0});var SE=l(Ie);Et=o(SE,"A",{id:!0,class:!0,href:!0});var FI=l(Et);Id=o(FI,"SPAN",{});var JI=l(Id);v(qr.$$.fragment,JI),JI.forEach(t),FI.forEach(t),$T=h(SE),Hd=o(SE,"SPAN",{});var WI=l(Hd);qT=u(WI,"Summarization task"),WI.forEach(t),SE.forEach(t),_1=h(a),yt=o(a,"P",{});var NE=l(yt);_T=u(NE,`This task is well known to summarize longer text into shorter text.
Be careful, some models have a maximum length of input. That means that
the summary cannot handle full books for instance. Be careful when
choosing your model. If you want to discuss your summarization needs,
please get in touch with us: <`),uu=o(NE,"A",{href:!0});var YI=l(uu);vT=u(YI,"api-enterprise@huggingface.co"),YI.forEach(t),ET=u(NE,">"),NE.forEach(t),v1=h(a),v(wt.$$.fragment,a),E1=h(a),_r=o(a,"P",{});var tI=l(_r);yT=u(tI,"Available with: "),vr=o(tI,"A",{href:!0,rel:!0});var VI=l(vr);wT=u(VI,"\u{1F917} Transformers"),VI.forEach(t),tI.forEach(t),y1=h(a),cu=o(a,"P",{});var XI=l(cu);bT=u(XI,"Example:"),XI.forEach(t),w1=h(a),v(bt.$$.fragment,a),b1=h(a),fu=o(a,"P",{});var QI=l(fu);TT=u(QI,`When sending your request, you should send a JSON encoded payload. Here
are all the options`),QI.forEach(t),T1=h(a),Tt=o(a,"TABLE",{});var xE=l(Tt);Bd=o(xE,"THEAD",{});var ZI=l(Bd);Er=o(ZI,"TR",{});var IE=l(Er);pu=o(IE,"TH",{align:!0});var eH=l(pu);jT=u(eH,"All parameters"),eH.forEach(t),kT=h(IE),Cd=o(IE,"TH",{align:!0}),l(Cd).forEach(t),IE.forEach(t),ZI.forEach(t),AT=h(xE),G=o(xE,"TBODY",{});var U=l(G);yr=o(U,"TR",{});var HE=l(yr);wr=o(HE,"TD",{align:!0});var sI=l(wr);Gd=o(sI,"STRONG",{});var tH=l(Gd);DT=u(tH,"inputs"),tH.forEach(t),OT=u(sI," (required)"),sI.forEach(t),PT=h(HE),hu=o(HE,"TD",{align:!0});var sH=l(hu);RT=u(sH,"a string to be summarized"),sH.forEach(t),HE.forEach(t),ST=h(U),br=o(U,"TR",{});var BE=l(br);du=o(BE,"TD",{align:!0});var aH=l(du);Ld=o(aH,"STRONG",{});var rH=l(Ld);NT=u(rH,"parameters"),rH.forEach(t),aH.forEach(t),xT=h(BE),gu=o(BE,"TD",{align:!0});var nH=l(gu);IT=u(nH,"a dict containing the following keys:"),nH.forEach(t),BE.forEach(t),HT=h(U),Tr=o(U,"TR",{});var CE=l(Tr);mu=o(CE,"TD",{align:!0});var oH=l(mu);BT=u(oH,"min_length"),oH.forEach(t),CT=h(CE),de=o(CE,"TD",{align:!0});var sd=l(de);GT=u(sd,"(Default: "),Ud=o(sd,"CODE",{});var lH=l(Ud);LT=u(lH,"None"),lH.forEach(t),UT=u(sd,"). Integer to define the minimum length "),zd=o(sd,"STRONG",{});var iH=l(zd);zT=u(iH,"in tokens"),iH.forEach(t),MT=u(sd," of the output summary."),sd.forEach(t),CE.forEach(t),KT=h(U),jr=o(U,"TR",{});var GE=l(jr);$u=o(GE,"TD",{align:!0});var uH=l($u);FT=u(uH,"max_length"),uH.forEach(t),JT=h(GE),ge=o(GE,"TD",{align:!0});var ad=l(ge);WT=u(ad,"(Default: "),Md=o(ad,"CODE",{});var cH=l(Md);YT=u(cH,"None"),cH.forEach(t),VT=u(ad,"). Integer to define the maximum length "),Kd=o(ad,"STRONG",{});var fH=l(Kd);XT=u(fH,"in tokens"),fH.forEach(t),QT=u(ad," of the output summary."),ad.forEach(t),GE.forEach(t),ZT=h(U),kr=o(U,"TR",{});var LE=l(kr);qu=o(LE,"TD",{align:!0});var pH=l(qu);ej=u(pH,"top_k"),pH.forEach(t),tj=h(LE),me=o(LE,"TD",{align:!0});var rd=l(me);sj=u(rd,"(Default: "),Fd=o(rd,"CODE",{});var hH=l(Fd);aj=u(hH,"None"),hH.forEach(t),rj=u(rd,"). Integer to define the top tokens considered within the "),Jd=o(rd,"CODE",{});var dH=l(Jd);nj=u(dH,"sample"),dH.forEach(t),oj=u(rd," operation to create new text."),rd.forEach(t),LE.forEach(t),lj=h(U),Ar=o(U,"TR",{});var UE=l(Ar);_u=o(UE,"TD",{align:!0});var gH=l(_u);ij=u(gH,"top_p"),gH.forEach(t),uj=h(UE),oe=o(UE,"TD",{align:!0});var Ya=l(oe);cj=u(Ya,"(Default: "),Wd=o(Ya,"CODE",{});var mH=l(Wd);fj=u(mH,"None"),mH.forEach(t),pj=u(Ya,"). Float to define the tokens that are within the "),Yd=o(Ya,"CODE",{});var $H=l(Yd);hj=u($H,"sample"),$H.forEach(t),dj=u(Ya," operation of text generation. Add tokens in the sample for more probable to least probable until the sum of the probabilities is greater than "),Vd=o(Ya,"CODE",{});var qH=l(Vd);gj=u(qH,"top_p"),qH.forEach(t),mj=u(Ya,"."),Ya.forEach(t),UE.forEach(t),$j=h(U),Dr=o(U,"TR",{});var zE=l(Dr);vu=o(zE,"TD",{align:!0});var _H=l(vu);qj=u(_H,"temperature"),_H.forEach(t),_j=h(zE),$e=o(zE,"TD",{align:!0});var nd=l($e);vj=u(nd,"(Default: "),Xd=o(nd,"CODE",{});var vH=l(Xd);Ej=u(vH,"1.0"),vH.forEach(t),yj=u(nd,"). Float (0.0-100.0). The temperature of the sampling operation. 1 means regular sampling, 0 means "),Qd=o(nd,"CODE",{});var EH=l(Qd);wj=u(EH,"100.0"),EH.forEach(t),bj=u(nd," is getting closer to uniform probability."),nd.forEach(t),zE.forEach(t),Tj=h(U),Or=o(U,"TR",{});var ME=l(Or);Eu=o(ME,"TD",{align:!0});var yH=l(Eu);jj=u(yH,"repetition_penalty"),yH.forEach(t),kj=h(ME),jt=o(ME,"TD",{align:!0});var KE=l(jt);Aj=u(KE,"(Default: "),Zd=o(KE,"CODE",{});var wH=l(Zd);Dj=u(wH,"None"),wH.forEach(t),Oj=u(KE,"). Float (0.0-100.0). The more a token is used within generation the more it is penalized to not be picked in successive generation passes."),KE.forEach(t),ME.forEach(t),Pj=h(U),Pr=o(U,"TR",{});var FE=l(Pr);yu=o(FE,"TD",{align:!0});var bH=l(yu);Rj=u(bH,"max_time"),bH.forEach(t),Sj=h(FE),kt=o(FE,"TD",{align:!0});var JE=l(kt);Nj=u(JE,"(Default: "),eg=o(JE,"CODE",{});var TH=l(eg);xj=u(TH,"None"),TH.forEach(t),Ij=u(JE,"). Float (0-120.0). The amount of time in seconds that the query should take maximum. Network can cause some overhead so it will be a soft limit."),JE.forEach(t),FE.forEach(t),Hj=h(U),Rr=o(U,"TR",{});var WE=l(Rr);wu=o(WE,"TD",{align:!0});var jH=l(wu);tg=o(jH,"STRONG",{});var kH=l(tg);Bj=u(kH,"options"),kH.forEach(t),jH.forEach(t),Cj=h(WE),bu=o(WE,"TD",{align:!0});var AH=l(bu);Gj=u(AH,"a dict containing the following keys:"),AH.forEach(t),WE.forEach(t),Lj=h(U),Sr=o(U,"TR",{});var YE=l(Sr);Tu=o(YE,"TD",{align:!0});var DH=l(Tu);Uj=u(DH,"use_gpu"),DH.forEach(t),zj=h(YE),At=o(YE,"TD",{align:!0});var VE=l(At);Mj=u(VE,"(Default: "),sg=o(VE,"CODE",{});var OH=l(sg);Kj=u(OH,"false"),OH.forEach(t),Fj=u(VE,"). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),VE.forEach(t),YE.forEach(t),Jj=h(U),Nr=o(U,"TR",{});var XE=l(Nr);ju=o(XE,"TD",{align:!0});var PH=l(ju);Wj=u(PH,"use_cache"),PH.forEach(t),Yj=h(XE),Dt=o(XE,"TD",{align:!0});var QE=l(Dt);Vj=u(QE,"(Default: "),ag=o(QE,"CODE",{});var RH=l(ag);Xj=u(RH,"true"),RH.forEach(t),Qj=u(QE,"). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),QE.forEach(t),XE.forEach(t),Zj=h(U),xr=o(U,"TR",{});var ZE=l(xr);ku=o(ZE,"TD",{align:!0});var SH=l(ku);e4=u(SH,"wait_for_model"),SH.forEach(t),t4=h(ZE),Ot=o(ZE,"TD",{align:!0});var ey=l(Ot);s4=u(ey,"(Default: "),rg=o(ey,"CODE",{});var NH=l(rg);a4=u(NH,"false"),NH.forEach(t),r4=u(ey,") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),ey.forEach(t),ZE.forEach(t),U.forEach(t),xE.forEach(t),j1=h(a),Au=o(a,"P",{});var xH=l(Au);n4=u(xH,"Return value is either a dict or a list of dicts if you sent a list of inputs"),xH.forEach(t),k1=h(a),Pt=o(a,"TABLE",{});var ty=l(Pt);ng=o(ty,"THEAD",{});var IH=l(ng);Ir=o(IH,"TR",{});var sy=l(Ir);Du=o(sy,"TH",{align:!0});var HH=l(Du);o4=u(HH,"Returned values"),HH.forEach(t),l4=h(sy),og=o(sy,"TH",{align:!0}),l(og).forEach(t),sy.forEach(t),IH.forEach(t),i4=h(ty),lg=o(ty,"TBODY",{});var BH=l(lg);Hr=o(BH,"TR",{});var ay=l(Hr);Ou=o(ay,"TD",{align:!0});var CH=l(Ou);ig=o(CH,"STRONG",{});var GH=l(ig);u4=u(GH,"summarization_text"),GH.forEach(t),CH.forEach(t),c4=h(ay),Pu=o(ay,"TD",{align:!0});var LH=l(Pu);f4=u(LH,"The string after translation"),LH.forEach(t),ay.forEach(t),BH.forEach(t),ty.forEach(t),A1=h(a),He=o(a,"H3",{class:!0});var ry=l(He);Rt=o(ry,"A",{id:!0,class:!0,href:!0});var UH=l(Rt);ug=o(UH,"SPAN",{});var zH=l(ug);v(Br.$$.fragment,zH),zH.forEach(t),UH.forEach(t),p4=h(ry),cg=o(ry,"SPAN",{});var MH=l(cg);h4=u(MH,"Question Answering task"),MH.forEach(t),ry.forEach(t),D1=h(a),Ru=o(a,"P",{});var KH=l(Ru);d4=u(KH,"Want to have a nice know-it-all bot that can answer any question?"),KH.forEach(t),O1=h(a),v(St.$$.fragment,a),P1=h(a),Be=o(a,"P",{});var Z_=l(Be);g4=u(Z_,"Available with: "),Cr=o(Z_,"A",{href:!0,rel:!0});var FH=l(Cr);m4=u(FH,"\u{1F917}Transformers"),FH.forEach(t),$4=u(Z_,` and
`),Gr=o(Z_,"A",{href:!0,rel:!0});var JH=l(Gr);q4=u(JH,"AllenNLP"),JH.forEach(t),Z_.forEach(t),R1=h(a),Su=o(a,"P",{});var WH=l(Su);_4=u(WH,"Example:"),WH.forEach(t),S1=h(a),v(Nt.$$.fragment,a),N1=h(a),Nu=o(a,"P",{});var YH=l(Nu);v4=u(YH,`When sending your request, you should send a JSON encoded payload. Here
are all the options`),YH.forEach(t),x1=h(a),xu=o(a,"P",{});var VH=l(xu);E4=u(VH,"Return value is either a dict or a list of dicts if you sent a list of inputs"),VH.forEach(t),I1=h(a),v(xt.$$.fragment,a),H1=h(a),It=o(a,"TABLE",{});var ny=l(It);fg=o(ny,"THEAD",{});var XH=l(fg);Lr=o(XH,"TR",{});var oy=l(Lr);Iu=o(oy,"TH",{align:!0});var QH=l(Iu);y4=u(QH,"Returned values"),QH.forEach(t),w4=h(oy),pg=o(oy,"TH",{align:!0}),l(pg).forEach(t),oy.forEach(t),XH.forEach(t),b4=h(ny),ce=o(ny,"TBODY",{});var Va=l(ce);Ur=o(Va,"TR",{});var ly=l(Ur);Hu=o(ly,"TD",{align:!0});var ZH=l(Hu);hg=o(ZH,"STRONG",{});var eB=l(hg);T4=u(eB,"answer"),eB.forEach(t),ZH.forEach(t),j4=h(ly),Bu=o(ly,"TD",{align:!0});var tB=l(Bu);k4=u(tB,"A string that\u2019s the answer within the text."),tB.forEach(t),ly.forEach(t),A4=h(Va),zr=o(Va,"TR",{});var iy=l(zr);Cu=o(iy,"TD",{align:!0});var sB=l(Cu);dg=o(sB,"STRONG",{});var aB=l(dg);D4=u(aB,"score"),aB.forEach(t),sB.forEach(t),O4=h(iy),Gu=o(iy,"TD",{align:!0});var rB=l(Gu);P4=u(rB,"A float that represents how likely that the answer is correct"),rB.forEach(t),iy.forEach(t),R4=h(Va),Mr=o(Va,"TR",{});var uy=l(Mr);Lu=o(uy,"TD",{align:!0});var nB=l(Lu);gg=o(nB,"STRONG",{});var oB=l(gg);S4=u(oB,"start"),oB.forEach(t),nB.forEach(t),N4=h(uy),Ht=o(uy,"TD",{align:!0});var cy=l(Ht);x4=u(cy,"The index (string wise) of the start of the answer within "),mg=o(cy,"CODE",{});var lB=l(mg);I4=u(lB,"context"),lB.forEach(t),H4=u(cy,"."),cy.forEach(t),uy.forEach(t),B4=h(Va),Kr=o(Va,"TR",{});var fy=l(Kr);Uu=o(fy,"TD",{align:!0});var iB=l(Uu);$g=o(iB,"STRONG",{});var uB=l($g);C4=u(uB,"stop"),uB.forEach(t),iB.forEach(t),G4=h(fy),Bt=o(fy,"TD",{align:!0});var py=l(Bt);L4=u(py,"The index (string wise) of the stop of the answer within "),qg=o(py,"CODE",{});var cB=l(qg);U4=u(cB,"context"),cB.forEach(t),z4=u(py,"."),py.forEach(t),fy.forEach(t),Va.forEach(t),ny.forEach(t),B1=h(a),Ce=o(a,"H3",{class:!0});var hy=l(Ce);Ct=o(hy,"A",{id:!0,class:!0,href:!0});var fB=l(Ct);_g=o(fB,"SPAN",{});var pB=l(_g);v(Fr.$$.fragment,pB),pB.forEach(t),fB.forEach(t),M4=h(hy),vg=o(hy,"SPAN",{});var hB=l(vg);K4=u(hB,"Table Question Answering task"),hB.forEach(t),hy.forEach(t),C1=h(a),zu=o(a,"P",{});var dB=l(zu);F4=u(dB,`Don\u2019t know SQL? Don\u2019t want to dive into a large spreadsheet? Ask
questions in plain english!`),dB.forEach(t),G1=h(a),v(Gt.$$.fragment,a),L1=h(a),Jr=o(a,"P",{});var aI=l(Jr);J4=u(aI,"Available with: "),Wr=o(aI,"A",{href:!0,rel:!0});var gB=l(Wr);W4=u(gB,"\u{1F917} Transformers"),gB.forEach(t),aI.forEach(t),U1=h(a),Mu=o(a,"P",{});var mB=l(Mu);Y4=u(mB,"Example:"),mB.forEach(t),z1=h(a),v(Lt.$$.fragment,a),M1=h(a),Ku=o(a,"P",{});var $B=l(Ku);V4=u($B,`When sending your request, you should send a JSON encoded payload. Here
are all the options`),$B.forEach(t),K1=h(a),Ut=o(a,"TABLE",{});var dy=l(Ut);Eg=o(dy,"THEAD",{});var qB=l(Eg);Yr=o(qB,"TR",{});var gy=l(Yr);Fu=o(gy,"TH",{align:!0});var _B=l(Fu);X4=u(_B,"All parameters"),_B.forEach(t),Q4=h(gy),yg=o(gy,"TH",{align:!0}),l(yg).forEach(t),gy.forEach(t),qB.forEach(t),Z4=h(dy),K=o(dy,"TBODY",{});var V=l(K);Vr=o(V,"TR",{});var my=l(Vr);Xr=o(my,"TD",{align:!0});var rI=l(Xr);wg=o(rI,"STRONG",{});var vB=l(wg);ek=u(vB,"inputs"),vB.forEach(t),tk=u(rI," (required)"),rI.forEach(t),sk=h(my),bg=o(my,"TD",{align:!0}),l(bg).forEach(t),my.forEach(t),ak=h(V),Qr=o(V,"TR",{});var $y=l(Qr);Ju=o($y,"TD",{align:!0});var EB=l(Ju);rk=u(EB,"query (required)"),EB.forEach(t),nk=h($y),Wu=o($y,"TD",{align:!0});var yB=l(Wu);ok=u(yB,"The query in plain text that you want to ask the table"),yB.forEach(t),$y.forEach(t),lk=h(V),Zr=o(V,"TR",{});var qy=l(Zr);Yu=o(qy,"TD",{align:!0});var wB=l(Yu);ik=u(wB,"table (required)"),wB.forEach(t),uk=h(qy),Vu=o(qy,"TD",{align:!0});var bB=l(Vu);ck=u(bB,"A table of data represented as a dict of list where entries are headers and the lists are all the values, all lists must have the same size."),bB.forEach(t),qy.forEach(t),fk=h(V),en=o(V,"TR",{});var _y=l(en);Xu=o(_y,"TD",{align:!0});var TB=l(Xu);Tg=o(TB,"STRONG",{});var jB=l(Tg);pk=u(jB,"options"),jB.forEach(t),TB.forEach(t),hk=h(_y),Qu=o(_y,"TD",{align:!0});var kB=l(Qu);dk=u(kB,"a dict containing the following keys:"),kB.forEach(t),_y.forEach(t),gk=h(V),tn=o(V,"TR",{});var vy=l(tn);Zu=o(vy,"TD",{align:!0});var AB=l(Zu);mk=u(AB,"use_gpu"),AB.forEach(t),$k=h(vy),zt=o(vy,"TD",{align:!0});var Ey=l(zt);qk=u(Ey,"(Default: "),jg=o(Ey,"CODE",{});var DB=l(jg);_k=u(DB,"false"),DB.forEach(t),vk=u(Ey,"). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),Ey.forEach(t),vy.forEach(t),Ek=h(V),sn=o(V,"TR",{});var yy=l(sn);ec=o(yy,"TD",{align:!0});var OB=l(ec);yk=u(OB,"use_cache"),OB.forEach(t),wk=h(yy),Mt=o(yy,"TD",{align:!0});var wy=l(Mt);bk=u(wy,"(Default: "),kg=o(wy,"CODE",{});var PB=l(kg);Tk=u(PB,"true"),PB.forEach(t),jk=u(wy,"). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),wy.forEach(t),yy.forEach(t),kk=h(V),an=o(V,"TR",{});var by=l(an);tc=o(by,"TD",{align:!0});var RB=l(tc);Ak=u(RB,"wait_for_model"),RB.forEach(t),Dk=h(by),Kt=o(by,"TD",{align:!0});var Ty=l(Kt);Ok=u(Ty,"(Default: "),Ag=o(Ty,"CODE",{});var SB=l(Ag);Pk=u(SB,"false"),SB.forEach(t),Rk=u(Ty,") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),Ty.forEach(t),by.forEach(t),V.forEach(t),dy.forEach(t),F1=h(a),sc=o(a,"P",{});var NB=l(sc);Sk=u(NB,"Return value is either a dict or a list of dicts if you sent a list of inputs"),NB.forEach(t),J1=h(a),v(Ft.$$.fragment,a),W1=h(a),Jt=o(a,"TABLE",{});var jy=l(Jt);Dg=o(jy,"THEAD",{});var xB=l(Dg);rn=o(xB,"TR",{});var ky=l(rn);ac=o(ky,"TH",{align:!0});var IB=l(ac);Nk=u(IB,"Returned values"),IB.forEach(t),xk=h(ky),Og=o(ky,"TH",{align:!0}),l(Og).forEach(t),ky.forEach(t),xB.forEach(t),Ik=h(jy),fe=o(jy,"TBODY",{});var Xa=l(fe);nn=o(Xa,"TR",{});var Ay=l(nn);rc=o(Ay,"TD",{align:!0});var HB=l(rc);Pg=o(HB,"STRONG",{});var BB=l(Pg);Hk=u(BB,"answer"),BB.forEach(t),HB.forEach(t),Bk=h(Ay),nc=o(Ay,"TD",{align:!0});var CB=l(nc);Ck=u(CB,"The plaintext answer"),CB.forEach(t),Ay.forEach(t),Gk=h(Xa),on=o(Xa,"TR",{});var Dy=l(on);oc=o(Dy,"TD",{align:!0});var GB=l(oc);Rg=o(GB,"STRONG",{});var LB=l(Rg);Lk=u(LB,"coordinates"),LB.forEach(t),GB.forEach(t),Uk=h(Dy),lc=o(Dy,"TD",{align:!0});var UB=l(lc);zk=u(UB,"a list of coordinates of the cells referenced in the answer"),UB.forEach(t),Dy.forEach(t),Mk=h(Xa),ln=o(Xa,"TR",{});var Oy=l(ln);ic=o(Oy,"TD",{align:!0});var zB=l(ic);Sg=o(zB,"STRONG",{});var MB=l(Sg);Kk=u(MB,"cells"),MB.forEach(t),zB.forEach(t),Fk=h(Oy),uc=o(Oy,"TD",{align:!0});var KB=l(uc);Jk=u(KB,"a list of coordinates of the cells contents"),KB.forEach(t),Oy.forEach(t),Wk=h(Xa),un=o(Xa,"TR",{});var Py=l(un);cc=o(Py,"TD",{align:!0});var FB=l(cc);Ng=o(FB,"STRONG",{});var JB=l(Ng);Yk=u(JB,"aggregator"),JB.forEach(t),FB.forEach(t),Vk=h(Py),fc=o(Py,"TD",{align:!0});var WB=l(fc);Xk=u(WB,"The aggregator used to get the answer"),WB.forEach(t),Py.forEach(t),Xa.forEach(t),jy.forEach(t),Y1=h(a),Ge=o(a,"H3",{class:!0});var Ry=l(Ge);Wt=o(Ry,"A",{id:!0,class:!0,href:!0});var YB=l(Wt);xg=o(YB,"SPAN",{});var VB=l(xg);v(cn.$$.fragment,VB),VB.forEach(t),YB.forEach(t),Qk=h(Ry),Ig=o(Ry,"SPAN",{});var XB=l(Ig);Zk=u(XB,"Text Classification task"),XB.forEach(t),Ry.forEach(t),V1=h(a),pc=o(a,"P",{});var QB=l(pc);e5=u(QB,`Usually used for sentiment-analysis this will output the likelihood of
classes of an input.`),QB.forEach(t),X1=h(a),v(Yt.$$.fragment,a),Q1=h(a),fn=o(a,"P",{});var nI=l(fn);t5=u(nI,"Available with: "),pn=o(nI,"A",{href:!0,rel:!0});var ZB=l(pn);s5=u(ZB,"\u{1F917} Transformers"),ZB.forEach(t),nI.forEach(t),Z1=h(a),hc=o(a,"P",{});var eC=l(hc);a5=u(eC,"Example:"),eC.forEach(t),ev=h(a),v(Vt.$$.fragment,a),tv=h(a),dc=o(a,"P",{});var tC=l(dc);r5=u(tC,`When sending your request, you should send a JSON encoded payload. Here
are all the options`),tC.forEach(t),sv=h(a),Xt=o(a,"TABLE",{});var Sy=l(Xt);Hg=o(Sy,"THEAD",{});var sC=l(Hg);hn=o(sC,"TR",{});var Ny=l(hn);gc=o(Ny,"TH",{align:!0});var aC=l(gc);n5=u(aC,"All parameters"),aC.forEach(t),o5=h(Ny),Bg=o(Ny,"TH",{align:!0}),l(Bg).forEach(t),Ny.forEach(t),sC.forEach(t),l5=h(Sy),ee=o(Sy,"TBODY",{});var Ae=l(ee);dn=o(Ae,"TR",{});var xy=l(dn);gn=o(xy,"TD",{align:!0});var oI=l(gn);Cg=o(oI,"STRONG",{});var rC=l(Cg);i5=u(rC,"inputs"),rC.forEach(t),u5=u(oI," (required)"),oI.forEach(t),c5=h(xy),mc=o(xy,"TD",{align:!0});var nC=l(mc);f5=u(nC,"a string to be classified"),nC.forEach(t),xy.forEach(t),p5=h(Ae),mn=o(Ae,"TR",{});var Iy=l(mn);$c=o(Iy,"TD",{align:!0});var oC=l($c);Gg=o(oC,"STRONG",{});var lC=l(Gg);h5=u(lC,"options"),lC.forEach(t),oC.forEach(t),d5=h(Iy),qc=o(Iy,"TD",{align:!0});var iC=l(qc);g5=u(iC,"a dict containing the following keys:"),iC.forEach(t),Iy.forEach(t),m5=h(Ae),$n=o(Ae,"TR",{});var Hy=l($n);_c=o(Hy,"TD",{align:!0});var uC=l(_c);$5=u(uC,"use_gpu"),uC.forEach(t),q5=h(Hy),Qt=o(Hy,"TD",{align:!0});var By=l(Qt);_5=u(By,"(Default: "),Lg=o(By,"CODE",{});var cC=l(Lg);v5=u(cC,"false"),cC.forEach(t),E5=u(By,"). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),By.forEach(t),Hy.forEach(t),y5=h(Ae),qn=o(Ae,"TR",{});var Cy=l(qn);vc=o(Cy,"TD",{align:!0});var fC=l(vc);w5=u(fC,"use_cache"),fC.forEach(t),b5=h(Cy),Zt=o(Cy,"TD",{align:!0});var Gy=l(Zt);T5=u(Gy,"(Default: "),Ug=o(Gy,"CODE",{});var pC=l(Ug);j5=u(pC,"true"),pC.forEach(t),k5=u(Gy,"). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),Gy.forEach(t),Cy.forEach(t),A5=h(Ae),_n=o(Ae,"TR",{});var Ly=l(_n);Ec=o(Ly,"TD",{align:!0});var hC=l(Ec);D5=u(hC,"wait_for_model"),hC.forEach(t),O5=h(Ly),es=o(Ly,"TD",{align:!0});var Uy=l(es);P5=u(Uy,"(Default: "),zg=o(Uy,"CODE",{});var dC=l(zg);R5=u(dC,"false"),dC.forEach(t),S5=u(Uy,") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),Uy.forEach(t),Ly.forEach(t),Ae.forEach(t),Sy.forEach(t),av=h(a),yc=o(a,"P",{});var gC=l(yc);N5=u(gC,"Return value is either a dict or a list of dicts if you sent a list of inputs"),gC.forEach(t),rv=h(a),v(ts.$$.fragment,a),nv=h(a),ss=o(a,"TABLE",{});var zy=l(ss);Mg=o(zy,"THEAD",{});var mC=l(Mg);vn=o(mC,"TR",{});var My=l(vn);wc=o(My,"TH",{align:!0});var $C=l(wc);x5=u($C,"Returned values"),$C.forEach(t),I5=h(My),Kg=o(My,"TH",{align:!0}),l(Kg).forEach(t),My.forEach(t),mC.forEach(t),H5=h(zy),En=o(zy,"TBODY",{});var Ky=l(En);yn=o(Ky,"TR",{});var Fy=l(yn);bc=o(Fy,"TD",{align:!0});var qC=l(bc);Fg=o(qC,"STRONG",{});var _C=l(Fg);B5=u(_C,"label"),_C.forEach(t),qC.forEach(t),C5=h(Fy),Tc=o(Fy,"TD",{align:!0});var vC=l(Tc);G5=u(vC,"The label for the class (model specific)"),vC.forEach(t),Fy.forEach(t),L5=h(Ky),wn=o(Ky,"TR",{});var Jy=l(wn);jc=o(Jy,"TD",{align:!0});var EC=l(jc);Jg=o(EC,"STRONG",{});var yC=l(Jg);U5=u(yC,"score"),yC.forEach(t),EC.forEach(t),z5=h(Jy),kc=o(Jy,"TD",{align:!0});var wC=l(kc);M5=u(wC,"A floats that represents how likely is that the text belongs the this class."),wC.forEach(t),Jy.forEach(t),Ky.forEach(t),zy.forEach(t),ov=h(a),Le=o(a,"H3",{class:!0});var Wy=l(Le);as=o(Wy,"A",{id:!0,class:!0,href:!0});var bC=l(as);Wg=o(bC,"SPAN",{});var TC=l(Wg);v(bn.$$.fragment,TC),TC.forEach(t),bC.forEach(t),K5=h(Wy),Yg=o(Wy,"SPAN",{});var jC=l(Yg);F5=u(jC,"Text Generation task"),jC.forEach(t),Wy.forEach(t),lv=h(a),Ac=o(a,"P",{});var kC=l(Ac);J5=u(kC,"Use to continue text from a prompt. This is a very generic task."),kC.forEach(t),iv=h(a),v(rs.$$.fragment,a),uv=h(a),Tn=o(a,"P",{});var lI=l(Tn);W5=u(lI,"Available with: "),jn=o(lI,"A",{href:!0,rel:!0});var AC=l(jn);Y5=u(AC,"\u{1F917} Transformers"),AC.forEach(t),lI.forEach(t),cv=h(a),Dc=o(a,"P",{});var DC=l(Dc);V5=u(DC,"Example:"),DC.forEach(t),fv=h(a),v(ns.$$.fragment,a),pv=h(a),Oc=o(a,"P",{});var OC=l(Oc);X5=u(OC,`When sending your request, you should send a JSON encoded payload. Here
are all the options`),OC.forEach(t),hv=h(a),os=o(a,"TABLE",{});var Yy=l(os);Vg=o(Yy,"THEAD",{});var PC=l(Vg);kn=o(PC,"TR",{});var Vy=l(kn);Pc=o(Vy,"TH",{align:!0});var RC=l(Pc);Q5=u(RC,"All parameters"),RC.forEach(t),Z5=h(Vy),Xg=o(Vy,"TH",{align:!0}),l(Xg).forEach(t),Vy.forEach(t),PC.forEach(t),e6=h(Yy),I=o(Yy,"TBODY",{});var B=l(I);An=o(B,"TR",{});var Xy=l(An);Dn=o(Xy,"TD",{align:!0});var iI=l(Dn);Qg=o(iI,"STRONG",{});var SC=l(Qg);t6=u(SC,"inputs"),SC.forEach(t),s6=u(iI," (required):"),iI.forEach(t),a6=h(Xy),Rc=o(Xy,"TD",{align:!0});var NC=l(Rc);r6=u(NC,"a string to be generated from"),NC.forEach(t),Xy.forEach(t),n6=h(B),On=o(B,"TR",{});var Qy=l(On);Sc=o(Qy,"TD",{align:!0});var xC=l(Sc);Zg=o(xC,"STRONG",{});var IC=l(Zg);o6=u(IC,"parameters"),IC.forEach(t),xC.forEach(t),l6=h(Qy),Nc=o(Qy,"TD",{align:!0});var HC=l(Nc);i6=u(HC,"dict containing the following keys:"),HC.forEach(t),Qy.forEach(t),u6=h(B),Pn=o(B,"TR",{});var Zy=l(Pn);xc=o(Zy,"TD",{align:!0});var BC=l(xc);c6=u(BC,"top_k"),BC.forEach(t),f6=h(Zy),qe=o(Zy,"TD",{align:!0});var od=l(qe);p6=u(od,"(Default: "),em=o(od,"CODE",{});var CC=l(em);h6=u(CC,"None"),CC.forEach(t),d6=u(od,"). Integer to define the top tokens considered within the "),tm=o(od,"CODE",{});var GC=l(tm);g6=u(GC,"sample"),GC.forEach(t),m6=u(od," operation to create new text."),od.forEach(t),Zy.forEach(t),$6=h(B),Rn=o(B,"TR",{});var ew=l(Rn);Ic=o(ew,"TD",{align:!0});var LC=l(Ic);q6=u(LC,"top_p"),LC.forEach(t),_6=h(ew),le=o(ew,"TD",{align:!0});var Qa=l(le);v6=u(Qa,"(Default: "),sm=o(Qa,"CODE",{});var UC=l(sm);E6=u(UC,"None"),UC.forEach(t),y6=u(Qa,"). Float to define the tokens that are within the "),am=o(Qa,"CODE",{});var zC=l(am);w6=u(zC,"sample"),zC.forEach(t),b6=u(Qa," operation of text generation. Add tokens in the sample for more probable to least probable until the sum of the probabilities is greater than "),rm=o(Qa,"CODE",{});var MC=l(rm);T6=u(MC,"top_p"),MC.forEach(t),j6=u(Qa,"."),Qa.forEach(t),ew.forEach(t),k6=h(B),Sn=o(B,"TR",{});var tw=l(Sn);Hc=o(tw,"TD",{align:!0});var KC=l(Hc);A6=u(KC,"temperature"),KC.forEach(t),D6=h(tw),_e=o(tw,"TD",{align:!0});var ld=l(_e);O6=u(ld,"(Default: "),nm=o(ld,"CODE",{});var FC=l(nm);P6=u(FC,"1.0"),FC.forEach(t),R6=u(ld,"). Float (0.0-100.0). The temperature of the sampling operation. 1 means regular sampling, 0 means "),om=o(ld,"CODE",{});var JC=l(om);S6=u(JC,"100.0"),JC.forEach(t),N6=u(ld," is getting closer to uniform probability."),ld.forEach(t),tw.forEach(t),x6=h(B),Nn=o(B,"TR",{});var sw=l(Nn);Bc=o(sw,"TD",{align:!0});var WC=l(Bc);I6=u(WC,"repetition_penalty"),WC.forEach(t),H6=h(sw),ls=o(sw,"TD",{align:!0});var aw=l(ls);B6=u(aw,"(Default: "),lm=o(aw,"CODE",{});var YC=l(lm);C6=u(YC,"None"),YC.forEach(t),G6=u(aw,"). Float (0.0-100.0). The more a token is used within generation the more it is penalized to not be picked in successive generation passes."),aw.forEach(t),sw.forEach(t),L6=h(B),xn=o(B,"TR",{});var rw=l(xn);Cc=o(rw,"TD",{align:!0});var VC=l(Cc);U6=u(VC,"max_new_tokens"),VC.forEach(t),z6=h(rw),ve=o(rw,"TD",{align:!0});var id=l(ve);M6=u(id,"(Default: "),im=o(id,"CODE",{});var XC=l(im);K6=u(XC,"None"),XC.forEach(t),F6=u(id,"). Int (0-250). The amount of new tokens to be generated, this does "),um=o(id,"STRONG",{});var QC=l(um);J6=u(QC,"not"),QC.forEach(t),W6=u(id," include the input length it is a estimate of the size of generated text you want. Each new tokens slows down the request, so look for balance between response times and length of text generated."),id.forEach(t),rw.forEach(t),Y6=h(B),In=o(B,"TR",{});var nw=l(In);Gc=o(nw,"TD",{align:!0});var ZC=l(Gc);V6=u(ZC,"max_time"),ZC.forEach(t),X6=h(nw),Ee=o(nw,"TD",{align:!0});var ud=l(Ee);Q6=u(ud,"(Default: "),cm=o(ud,"CODE",{});var eG=l(cm);Z6=u(eG,"None"),eG.forEach(t),e7=u(ud,"). Float (0-120.0). The amount of time in seconds that the query should take maximum. Network can cause some overhead so it will be a soft limit. Use that in combination with "),fm=o(ud,"CODE",{});var tG=l(fm);t7=u(tG,"max_new_tokens"),tG.forEach(t),s7=u(ud," for best results."),ud.forEach(t),nw.forEach(t),a7=h(B),Hn=o(B,"TR",{});var ow=l(Hn);Lc=o(ow,"TD",{align:!0});var sG=l(Lc);r7=u(sG,"return_full_text"),sG.forEach(t),n7=h(ow),ye=o(ow,"TD",{align:!0});var cd=l(ye);o7=u(cd,"(Default: "),pm=o(cd,"CODE",{});var aG=l(pm);l7=u(aG,"True"),aG.forEach(t),i7=u(cd,"). Bool. If set to False, the return results will "),hm=o(cd,"STRONG",{});var rG=l(hm);u7=u(rG,"not"),rG.forEach(t),c7=u(cd," contain the original query making it easier for prompting."),cd.forEach(t),ow.forEach(t),f7=h(B),Bn=o(B,"TR",{});var lw=l(Bn);Uc=o(lw,"TD",{align:!0});var nG=l(Uc);p7=u(nG,"num_return_sequences"),nG.forEach(t),h7=h(lw),is=o(lw,"TD",{align:!0});var iw=l(is);d7=u(iw,"(Default: "),dm=o(iw,"CODE",{});var oG=l(dm);g7=u(oG,"1"),oG.forEach(t),m7=u(iw,"). Integer. The number of proposition you want to be returned."),iw.forEach(t),lw.forEach(t),$7=h(B),Cn=o(B,"TR",{});var uw=l(Cn);zc=o(uw,"TD",{align:!0});var lG=l(zc);q7=u(lG,"do_sample"),lG.forEach(t),_7=h(uw),us=o(uw,"TD",{align:!0});var cw=l(us);v7=u(cw,"(Optional: "),gm=o(cw,"CODE",{});var iG=l(gm);E7=u(iG,"True"),iG.forEach(t),y7=u(cw,"). Bool. Whether or not to use sampling, use greedy decoding otherwise."),cw.forEach(t),uw.forEach(t),w7=h(B),Gn=o(B,"TR",{});var fw=l(Gn);Mc=o(fw,"TD",{align:!0});var uG=l(Mc);mm=o(uG,"STRONG",{});var cG=l(mm);b7=u(cG,"options"),cG.forEach(t),uG.forEach(t),T7=h(fw),Kc=o(fw,"TD",{align:!0});var fG=l(Kc);j7=u(fG,"a dict containing the following keys:"),fG.forEach(t),fw.forEach(t),k7=h(B),Ln=o(B,"TR",{});var pw=l(Ln);Fc=o(pw,"TD",{align:!0});var pG=l(Fc);A7=u(pG,"use_gpu"),pG.forEach(t),D7=h(pw),cs=o(pw,"TD",{align:!0});var hw=l(cs);O7=u(hw,"(Default: "),$m=o(hw,"CODE",{});var hG=l($m);P7=u(hG,"false"),hG.forEach(t),R7=u(hw,"). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),hw.forEach(t),pw.forEach(t),S7=h(B),Un=o(B,"TR",{});var dw=l(Un);Jc=o(dw,"TD",{align:!0});var dG=l(Jc);N7=u(dG,"use_cache"),dG.forEach(t),x7=h(dw),fs=o(dw,"TD",{align:!0});var gw=l(fs);I7=u(gw,"(Default: "),qm=o(gw,"CODE",{});var gG=l(qm);H7=u(gG,"true"),gG.forEach(t),B7=u(gw,"). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),gw.forEach(t),dw.forEach(t),C7=h(B),zn=o(B,"TR",{});var mw=l(zn);Wc=o(mw,"TD",{align:!0});var mG=l(Wc);G7=u(mG,"wait_for_model"),mG.forEach(t),L7=h(mw),ps=o(mw,"TD",{align:!0});var $w=l(ps);U7=u($w,"(Default: "),_m=o($w,"CODE",{});var $G=l(_m);z7=u($G,"false"),$G.forEach(t),M7=u($w,") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),$w.forEach(t),mw.forEach(t),B.forEach(t),Yy.forEach(t),dv=h(a),Yc=o(a,"P",{});var qG=l(Yc);K7=u(qG,"Return value is either a dict or a list of dicts if you sent a list of inputs"),qG.forEach(t),gv=h(a),v(hs.$$.fragment,a),mv=h(a),ds=o(a,"TABLE",{});var qw=l(ds);vm=o(qw,"THEAD",{});var _G=l(vm);Mn=o(_G,"TR",{});var _w=l(Mn);Vc=o(_w,"TH",{align:!0});var vG=l(Vc);F7=u(vG,"Returned values"),vG.forEach(t),J7=h(_w),Em=o(_w,"TH",{align:!0}),l(Em).forEach(t),_w.forEach(t),_G.forEach(t),W7=h(qw),ym=o(qw,"TBODY",{});var EG=l(ym);Kn=o(EG,"TR",{});var vw=l(Kn);Xc=o(vw,"TD",{align:!0});var yG=l(Xc);wm=o(yG,"STRONG",{});var wG=l(wm);Y7=u(wG,"generated_text"),wG.forEach(t),yG.forEach(t),V7=h(vw),Qc=o(vw,"TD",{align:!0});var bG=l(Qc);X7=u(bG,"The continuated string"),bG.forEach(t),vw.forEach(t),EG.forEach(t),qw.forEach(t),$v=h(a),Ue=o(a,"H3",{class:!0});var Ew=l(Ue);gs=o(Ew,"A",{id:!0,class:!0,href:!0});var TG=l(gs);bm=o(TG,"SPAN",{});var jG=l(bm);v(Fn.$$.fragment,jG),jG.forEach(t),TG.forEach(t),Q7=h(Ew),Tm=o(Ew,"SPAN",{});var kG=l(Tm);Z7=u(kG,"Text2Text Generation task"),kG.forEach(t),Ew.forEach(t),qv=h(a),ms=o(a,"P",{});var yw=l(ms);e9=u(yw,"Essentially "),Zc=o(yw,"A",{href:!0});var AG=l(Zc);t9=u(AG,"Text-generation task"),AG.forEach(t),s9=u(yw,`. But uses
Encoder-Decoder architecture, so might change in the future for more
options.`),yw.forEach(t),_v=h(a),ze=o(a,"H3",{class:!0});var ww=l(ze);$s=o(ww,"A",{id:!0,class:!0,href:!0});var DG=l($s);jm=o(DG,"SPAN",{});var OG=l(jm);v(Jn.$$.fragment,OG),OG.forEach(t),DG.forEach(t),a9=h(ww),km=o(ww,"SPAN",{});var PG=l(km);r9=u(PG,"Token Classification task"),PG.forEach(t),ww.forEach(t),vv=h(a),ef=o(a,"P",{});var RG=l(ef);n9=u(RG,`Usually used for sentence parsing, either grammatical, or Named Entity
Recognition (NER) to understand keywords contained within text.`),RG.forEach(t),Ev=h(a),v(qs.$$.fragment,a),yv=h(a),Me=o(a,"P",{});var e1=l(Me);o9=u(e1,"Available with: "),Wn=o(e1,"A",{href:!0,rel:!0});var SG=l(Wn);l9=u(SG,"\u{1F917} Transformers"),SG.forEach(t),i9=u(e1,`,
`),Yn=o(e1,"A",{href:!0,rel:!0});var NG=l(Yn);u9=u(NG,"Flair"),NG.forEach(t),e1.forEach(t),wv=h(a),tf=o(a,"P",{});var xG=l(tf);c9=u(xG,"Example:"),xG.forEach(t),bv=h(a),v(_s.$$.fragment,a),Tv=h(a),sf=o(a,"P",{});var IG=l(sf);f9=u(IG,`When sending your request, you should send a JSON encoded payload. Here
are all the options`),IG.forEach(t),jv=h(a),vs=o(a,"TABLE",{});var bw=l(vs);Am=o(bw,"THEAD",{});var HG=l(Am);Vn=o(HG,"TR",{});var Tw=l(Vn);af=o(Tw,"TH",{align:!0});var BG=l(af);p9=u(BG,"All parameters"),BG.forEach(t),h9=h(Tw),Dm=o(Tw,"TH",{align:!0}),l(Dm).forEach(t),Tw.forEach(t),HG.forEach(t),d9=h(bw),F=o(bw,"TBODY",{});var X=l(F);Xn=o(X,"TR",{});var jw=l(Xn);Qn=o(jw,"TD",{align:!0});var uI=l(Qn);Om=o(uI,"STRONG",{});var CG=l(Om);g9=u(CG,"inputs"),CG.forEach(t),m9=u(uI," (required)"),uI.forEach(t),$9=h(jw),rf=o(jw,"TD",{align:!0});var GG=l(rf);q9=u(GG,"a string to be classified"),GG.forEach(t),jw.forEach(t),_9=h(X),Zn=o(X,"TR",{});var kw=l(Zn);nf=o(kw,"TD",{align:!0});var LG=l(nf);Pm=o(LG,"STRONG",{});var UG=l(Pm);v9=u(UG,"parameters"),UG.forEach(t),LG.forEach(t),E9=h(kw),of=o(kw,"TD",{align:!0});var zG=l(of);y9=u(zG,"a dict containing the following key:"),zG.forEach(t),kw.forEach(t),w9=h(X),eo=o(X,"TR",{});var Aw=l(eo);lf=o(Aw,"TD",{align:!0});var MG=l(lf);b9=u(MG,"aggregation_strategy"),MG.forEach(t),T9=h(Aw),x=o(Aw,"TD",{align:!0});var C=l(x);j9=u(C,"(Default: "),Rm=o(C,"CODE",{});var KG=l(Rm);k9=u(KG,"simple"),KG.forEach(t),A9=u(C,"). There are several aggregation strategies: "),D9=o(C,"BR",{}),O9=h(C),Sm=o(C,"CODE",{});var FG=l(Sm);P9=u(FG,"none"),FG.forEach(t),R9=u(C,": Every token gets classified without further aggregation. "),S9=o(C,"BR",{}),N9=h(C),Nm=o(C,"CODE",{});var JG=l(Nm);x9=u(JG,"simple"),JG.forEach(t),I9=u(C,": Entities are grouped according to the default schema (B-, I- tags get merged when the tag is similar). "),H9=o(C,"BR",{}),B9=h(C),xm=o(C,"CODE",{});var WG=l(xm);C9=u(WG,"first"),WG.forEach(t),G9=u(C,": Same as the "),Im=o(C,"CODE",{});var YG=l(Im);L9=u(YG,"simple"),YG.forEach(t),U9=u(C," strategy except words cannot end up with different tags. Words will use the tag of the first token when there is ambiguity. "),z9=o(C,"BR",{}),M9=h(C),Hm=o(C,"CODE",{});var VG=l(Hm);K9=u(VG,"average"),VG.forEach(t),F9=u(C,": Same as the "),Bm=o(C,"CODE",{});var XG=l(Bm);J9=u(XG,"simple"),XG.forEach(t),W9=u(C," strategy except words cannot end up with different tags. Scores are averaged across tokens and then the maximum label is applied. "),Y9=o(C,"BR",{}),V9=h(C),Cm=o(C,"CODE",{});var QG=l(Cm);X9=u(QG,"max"),QG.forEach(t),Q9=u(C,": Same as the "),Gm=o(C,"CODE",{});var ZG=l(Gm);Z9=u(ZG,"simple"),ZG.forEach(t),e8=u(C," strategy except words cannot end up with different tags. Word entity will be the token with the maximum score."),C.forEach(t),Aw.forEach(t),t8=h(X),to=o(X,"TR",{});var Dw=l(to);uf=o(Dw,"TD",{align:!0});var eL=l(uf);Lm=o(eL,"STRONG",{});var tL=l(Lm);s8=u(tL,"options"),tL.forEach(t),eL.forEach(t),a8=h(Dw),cf=o(Dw,"TD",{align:!0});var sL=l(cf);r8=u(sL,"a dict containing the following keys:"),sL.forEach(t),Dw.forEach(t),n8=h(X),so=o(X,"TR",{});var Ow=l(so);ff=o(Ow,"TD",{align:!0});var aL=l(ff);o8=u(aL,"use_gpu"),aL.forEach(t),l8=h(Ow),Es=o(Ow,"TD",{align:!0});var Pw=l(Es);i8=u(Pw,"(Default: "),Um=o(Pw,"CODE",{});var rL=l(Um);u8=u(rL,"false"),rL.forEach(t),c8=u(Pw,"). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),Pw.forEach(t),Ow.forEach(t),f8=h(X),ao=o(X,"TR",{});var Rw=l(ao);pf=o(Rw,"TD",{align:!0});var nL=l(pf);p8=u(nL,"use_cache"),nL.forEach(t),h8=h(Rw),ys=o(Rw,"TD",{align:!0});var Sw=l(ys);d8=u(Sw,"(Default: "),zm=o(Sw,"CODE",{});var oL=l(zm);g8=u(oL,"true"),oL.forEach(t),m8=u(Sw,"). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),Sw.forEach(t),Rw.forEach(t),$8=h(X),ro=o(X,"TR",{});var Nw=l(ro);hf=o(Nw,"TD",{align:!0});var lL=l(hf);q8=u(lL,"wait_for_model"),lL.forEach(t),_8=h(Nw),ws=o(Nw,"TD",{align:!0});var xw=l(ws);v8=u(xw,"(Default: "),Mm=o(xw,"CODE",{});var iL=l(Mm);E8=u(iL,"false"),iL.forEach(t),y8=u(xw,") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),xw.forEach(t),Nw.forEach(t),X.forEach(t),bw.forEach(t),kv=h(a),df=o(a,"P",{});var uL=l(df);w8=u(uL,"Return value is either a dict or a list of dicts if you sent a list of inputs"),uL.forEach(t),Av=h(a),v(bs.$$.fragment,a),Dv=h(a),Ts=o(a,"TABLE",{});var Iw=l(Ts);Km=o(Iw,"THEAD",{});var cL=l(Km);no=o(cL,"TR",{});var Hw=l(no);gf=o(Hw,"TH",{align:!0});var fL=l(gf);b8=u(fL,"Returned values"),fL.forEach(t),T8=h(Hw),Fm=o(Hw,"TH",{align:!0}),l(Fm).forEach(t),Hw.forEach(t),cL.forEach(t),j8=h(Iw),te=o(Iw,"TBODY",{});var De=l(te);oo=o(De,"TR",{});var Bw=l(oo);mf=o(Bw,"TD",{align:!0});var pL=l(mf);Jm=o(pL,"STRONG",{});var hL=l(Jm);k8=u(hL,"entity_group"),hL.forEach(t),pL.forEach(t),A8=h(Bw),$f=o(Bw,"TD",{align:!0});var dL=l($f);D8=u(dL,"The type for the entity being recognized (model specific)."),dL.forEach(t),Bw.forEach(t),O8=h(De),lo=o(De,"TR",{});var Cw=l(lo);qf=o(Cw,"TD",{align:!0});var gL=l(qf);Wm=o(gL,"STRONG",{});var mL=l(Wm);P8=u(mL,"score"),mL.forEach(t),gL.forEach(t),R8=h(Cw),_f=o(Cw,"TD",{align:!0});var $L=l(_f);S8=u($L,"How likely the entity was recognized."),$L.forEach(t),Cw.forEach(t),N8=h(De),io=o(De,"TR",{});var Gw=l(io);vf=o(Gw,"TD",{align:!0});var qL=l(vf);Ym=o(qL,"STRONG",{});var _L=l(Ym);x8=u(_L,"word"),_L.forEach(t),qL.forEach(t),I8=h(Gw),Ef=o(Gw,"TD",{align:!0});var vL=l(Ef);H8=u(vL,"The string that was captured"),vL.forEach(t),Gw.forEach(t),B8=h(De),uo=o(De,"TR",{});var Lw=l(uo);yf=o(Lw,"TD",{align:!0});var EL=l(yf);Vm=o(EL,"STRONG",{});var yL=l(Vm);C8=u(yL,"start"),yL.forEach(t),EL.forEach(t),G8=h(Lw),js=o(Lw,"TD",{align:!0});var Uw=l(js);L8=u(Uw,"The offset stringwise where the answer is located. Useful to disambiguate if "),Xm=o(Uw,"CODE",{});var wL=l(Xm);U8=u(wL,"word"),wL.forEach(t),z8=u(Uw," occurs multiple times."),Uw.forEach(t),Lw.forEach(t),M8=h(De),co=o(De,"TR",{});var zw=l(co);wf=o(zw,"TD",{align:!0});var bL=l(wf);Qm=o(bL,"STRONG",{});var TL=l(Qm);K8=u(TL,"end"),TL.forEach(t),bL.forEach(t),F8=h(zw),ks=o(zw,"TD",{align:!0});var Mw=l(ks);J8=u(Mw,"The offset stringwise where the answer is located. Useful to disambiguate if "),Zm=o(Mw,"CODE",{});var jL=l(Zm);W8=u(jL,"word"),jL.forEach(t),Y8=u(Mw," occurs multiple times."),Mw.forEach(t),zw.forEach(t),De.forEach(t),Iw.forEach(t),Ov=h(a),Ke=o(a,"H3",{class:!0});var Kw=l(Ke);As=o(Kw,"A",{id:!0,class:!0,href:!0});var kL=l(As);e$=o(kL,"SPAN",{});var AL=l(e$);v(fo.$$.fragment,AL),AL.forEach(t),kL.forEach(t),V8=h(Kw),t$=o(Kw,"SPAN",{});var DL=l(t$);X8=u(DL,"Named Entity Recognition (NER) task"),DL.forEach(t),Kw.forEach(t),Pv=h(a),po=o(a,"P",{});var cI=l(po);Q8=u(cI,"See "),bf=o(cI,"A",{href:!0});var OL=l(bf);Z8=u(OL,"Token-classification task"),OL.forEach(t),cI.forEach(t),Rv=h(a),Fe=o(a,"H3",{class:!0});var Fw=l(Fe);Ds=o(Fw,"A",{id:!0,class:!0,href:!0});var PL=l(Ds);s$=o(PL,"SPAN",{});var RL=l(s$);v(ho.$$.fragment,RL),RL.forEach(t),PL.forEach(t),eA=h(Fw),a$=o(Fw,"SPAN",{});var SL=l(a$);tA=u(SL,"Translation task"),SL.forEach(t),Fw.forEach(t),Sv=h(a),Tf=o(a,"P",{});var NL=l(Tf);sA=u(NL,"This task is well known to translate text from one language to another"),NL.forEach(t),Nv=h(a),v(Os.$$.fragment,a),xv=h(a),go=o(a,"P",{});var fI=l(go);aA=u(fI,"Available with: "),mo=o(fI,"A",{href:!0,rel:!0});var xL=l(mo);rA=u(xL,"\u{1F917} Transformers"),xL.forEach(t),fI.forEach(t),Iv=h(a),jf=o(a,"P",{});var IL=l(jf);nA=u(IL,"Example:"),IL.forEach(t),Hv=h(a),v(Ps.$$.fragment,a),Bv=h(a),kf=o(a,"P",{});var HL=l(kf);oA=u(HL,`When sending your request, you should send a JSON encoded payload. Here
are all the options`),HL.forEach(t),Cv=h(a),Rs=o(a,"TABLE",{});var Jw=l(Rs);r$=o(Jw,"THEAD",{});var BL=l(r$);$o=o(BL,"TR",{});var Ww=l($o);Af=o(Ww,"TH",{align:!0});var CL=l(Af);lA=u(CL,"All parameters"),CL.forEach(t),iA=h(Ww),n$=o(Ww,"TH",{align:!0}),l(n$).forEach(t),Ww.forEach(t),BL.forEach(t),uA=h(Jw),se=o(Jw,"TBODY",{});var Oe=l(se);qo=o(Oe,"TR",{});var Yw=l(qo);_o=o(Yw,"TD",{align:!0});var pI=l(_o);o$=o(pI,"STRONG",{});var GL=l(o$);cA=u(GL,"inputs"),GL.forEach(t),fA=u(pI," (required)"),pI.forEach(t),pA=h(Yw),Df=o(Yw,"TD",{align:!0});var LL=l(Df);hA=u(LL,"a string to be translated in the original languages"),LL.forEach(t),Yw.forEach(t),dA=h(Oe),vo=o(Oe,"TR",{});var Vw=l(vo);Of=o(Vw,"TD",{align:!0});var UL=l(Of);l$=o(UL,"STRONG",{});var zL=l(l$);gA=u(zL,"options"),zL.forEach(t),UL.forEach(t),mA=h(Vw),Pf=o(Vw,"TD",{align:!0});var ML=l(Pf);$A=u(ML,"a dict containing the following keys:"),ML.forEach(t),Vw.forEach(t),qA=h(Oe),Eo=o(Oe,"TR",{});var Xw=l(Eo);Rf=o(Xw,"TD",{align:!0});var KL=l(Rf);_A=u(KL,"use_gpu"),KL.forEach(t),vA=h(Xw),Ss=o(Xw,"TD",{align:!0});var Qw=l(Ss);EA=u(Qw,"(Default: "),i$=o(Qw,"CODE",{});var FL=l(i$);yA=u(FL,"false"),FL.forEach(t),wA=u(Qw,"). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),Qw.forEach(t),Xw.forEach(t),bA=h(Oe),yo=o(Oe,"TR",{});var Zw=l(yo);Sf=o(Zw,"TD",{align:!0});var JL=l(Sf);TA=u(JL,"use_cache"),JL.forEach(t),jA=h(Zw),Ns=o(Zw,"TD",{align:!0});var e0=l(Ns);kA=u(e0,"(Default: "),u$=o(e0,"CODE",{});var WL=l(u$);AA=u(WL,"true"),WL.forEach(t),DA=u(e0,"). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),e0.forEach(t),Zw.forEach(t),OA=h(Oe),wo=o(Oe,"TR",{});var t0=l(wo);Nf=o(t0,"TD",{align:!0});var YL=l(Nf);PA=u(YL,"wait_for_model"),YL.forEach(t),RA=h(t0),xs=o(t0,"TD",{align:!0});var s0=l(xs);SA=u(s0,"(Default: "),c$=o(s0,"CODE",{});var VL=l(c$);NA=u(VL,"false"),VL.forEach(t),xA=u(s0,") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),s0.forEach(t),t0.forEach(t),Oe.forEach(t),Jw.forEach(t),Gv=h(a),xf=o(a,"P",{});var XL=l(xf);IA=u(XL,"Return value is either a dict or a list of dicts if you sent a list of inputs"),XL.forEach(t),Lv=h(a),Is=o(a,"TABLE",{});var a0=l(Is);f$=o(a0,"THEAD",{});var QL=l(f$);bo=o(QL,"TR",{});var r0=l(bo);If=o(r0,"TH",{align:!0});var ZL=l(If);HA=u(ZL,"Returned values"),ZL.forEach(t),BA=h(r0),p$=o(r0,"TH",{align:!0}),l(p$).forEach(t),r0.forEach(t),QL.forEach(t),CA=h(a0),h$=o(a0,"TBODY",{});var eU=l(h$);To=o(eU,"TR",{});var n0=l(To);Hf=o(n0,"TD",{align:!0});var tU=l(Hf);d$=o(tU,"STRONG",{});var sU=l(d$);GA=u(sU,"translation_text"),sU.forEach(t),tU.forEach(t),LA=h(n0),Bf=o(n0,"TD",{align:!0});var aU=l(Bf);UA=u(aU,"The string after translation"),aU.forEach(t),n0.forEach(t),eU.forEach(t),a0.forEach(t),Uv=h(a),Je=o(a,"H3",{class:!0});var o0=l(Je);Hs=o(o0,"A",{id:!0,class:!0,href:!0});var rU=l(Hs);g$=o(rU,"SPAN",{});var nU=l(g$);v(jo.$$.fragment,nU),nU.forEach(t),rU.forEach(t),zA=h(o0),m$=o(o0,"SPAN",{});var oU=l(m$);MA=u(oU,"Zero-Shot Classification task"),oU.forEach(t),o0.forEach(t),zv=h(a),Cf=o(a,"P",{});var lU=l(Cf);KA=u(lU,`This task is super useful to try out classification with zero code,
you simply pass a sentence/paragraph and the possible labels for that
sentence, and you get a result.`),lU.forEach(t),Mv=h(a),v(Bs.$$.fragment,a),Kv=h(a),ko=o(a,"P",{});var hI=l(ko);FA=u(hI,"Available with: "),Ao=o(hI,"A",{href:!0,rel:!0});var iU=l(Ao);JA=u(iU,"\u{1F917} Transformers"),iU.forEach(t),hI.forEach(t),Fv=h(a),Gf=o(a,"P",{});var uU=l(Gf);WA=u(uU,"Request:"),uU.forEach(t),Jv=h(a),v(Cs.$$.fragment,a),Wv=h(a),Lf=o(a,"P",{});var cU=l(Lf);YA=u(cU,`When sending your request, you should send a JSON encoded payload. Here
are all the options`),cU.forEach(t),Yv=h(a),Gs=o(a,"TABLE",{});var l0=l(Gs);$$=o(l0,"THEAD",{});var fU=l($$);Do=o(fU,"TR",{});var i0=l(Do);Uf=o(i0,"TH",{align:!0});var pU=l(Uf);VA=u(pU,"All parameters"),pU.forEach(t),XA=h(i0),q$=o(i0,"TH",{align:!0}),l(q$).forEach(t),i0.forEach(t),fU.forEach(t),QA=h(l0),M=o(l0,"TBODY",{});var W=l(M);Oo=o(W,"TR",{});var u0=l(Oo);Po=o(u0,"TD",{align:!0});var dI=l(Po);_$=o(dI,"STRONG",{});var hU=l(_$);ZA=u(hU,"inputs"),hU.forEach(t),eD=u(dI," (required)"),dI.forEach(t),tD=h(u0),zf=o(u0,"TD",{align:!0});var dU=l(zf);sD=u(dU,"a string or list of strings"),dU.forEach(t),u0.forEach(t),aD=h(W),Ro=o(W,"TR",{});var c0=l(Ro);So=o(c0,"TD",{align:!0});var gI=l(So);v$=o(gI,"STRONG",{});var gU=l(v$);rD=u(gU,"parameters"),gU.forEach(t),nD=u(gI," (required)"),gI.forEach(t),oD=h(c0),Mf=o(c0,"TD",{align:!0});var mU=l(Mf);lD=u(mU,"a dict containing the following keys:"),mU.forEach(t),c0.forEach(t),iD=h(W),No=o(W,"TR",{});var f0=l(No);Kf=o(f0,"TD",{align:!0});var $U=l(Kf);uD=u($U,"candidate_labels (required)"),$U.forEach(t),cD=h(f0),we=o(f0,"TD",{align:!0});var fd=l(we);fD=u(fd,"a list of strings that are potential classes for "),E$=o(fd,"CODE",{});var qU=l(E$);pD=u(qU,"inputs"),qU.forEach(t),hD=u(fd,". (max 10 candidate_labels, for more, simply run multiple requests, results are going to be misleading if using too many candidate_labels anyway. If you want to keep the exact same, you can simply run "),y$=o(fd,"CODE",{});var _U=l(y$);dD=u(_U,"multi_label=True"),_U.forEach(t),gD=u(fd," and do the scaling on your end. )"),fd.forEach(t),f0.forEach(t),mD=h(W),xo=o(W,"TR",{});var p0=l(xo);Ff=o(p0,"TD",{align:!0});var vU=l(Ff);$D=u(vU,"multi_label"),vU.forEach(t),qD=h(p0),Ls=o(p0,"TD",{align:!0});var h0=l(Ls);_D=u(h0,"(Default: "),w$=o(h0,"CODE",{});var EU=l(w$);vD=u(EU,"false"),EU.forEach(t),ED=u(h0,") Boolean that is set to True if classes can overlap"),h0.forEach(t),p0.forEach(t),yD=h(W),Io=o(W,"TR",{});var d0=l(Io);Jf=o(d0,"TD",{align:!0});var yU=l(Jf);b$=o(yU,"STRONG",{});var wU=l(b$);wD=u(wU,"options"),wU.forEach(t),yU.forEach(t),bD=h(d0),Wf=o(d0,"TD",{align:!0});var bU=l(Wf);TD=u(bU,"a dict containing the following keys:"),bU.forEach(t),d0.forEach(t),jD=h(W),Ho=o(W,"TR",{});var g0=l(Ho);Yf=o(g0,"TD",{align:!0});var TU=l(Yf);kD=u(TU,"use_gpu"),TU.forEach(t),AD=h(g0),Us=o(g0,"TD",{align:!0});var m0=l(Us);DD=u(m0,"(Default: "),T$=o(m0,"CODE",{});var jU=l(T$);OD=u(jU,"false"),jU.forEach(t),PD=u(m0,"). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),m0.forEach(t),g0.forEach(t),RD=h(W),Bo=o(W,"TR",{});var $0=l(Bo);Vf=o($0,"TD",{align:!0});var kU=l(Vf);SD=u(kU,"use_cache"),kU.forEach(t),ND=h($0),zs=o($0,"TD",{align:!0});var q0=l(zs);xD=u(q0,"(Default: "),j$=o(q0,"CODE",{});var AU=l(j$);ID=u(AU,"true"),AU.forEach(t),HD=u(q0,"). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),q0.forEach(t),$0.forEach(t),BD=h(W),Co=o(W,"TR",{});var _0=l(Co);Xf=o(_0,"TD",{align:!0});var DU=l(Xf);CD=u(DU,"wait_for_model"),DU.forEach(t),GD=h(_0),Ms=o(_0,"TD",{align:!0});var v0=l(Ms);LD=u(v0,"(Default: "),k$=o(v0,"CODE",{});var OU=l(k$);UD=u(OU,"false"),OU.forEach(t),zD=u(v0,") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),v0.forEach(t),_0.forEach(t),W.forEach(t),l0.forEach(t),Vv=h(a),Qf=o(a,"P",{});var PU=l(Qf);MD=u(PU,"Return value is either a dict or a list of dicts if you sent a list of inputs"),PU.forEach(t),Xv=h(a),Zf=o(a,"P",{});var RU=l(Zf);KD=u(RU,"Response:"),RU.forEach(t),Qv=h(a),v(Ks.$$.fragment,a),Zv=h(a),Fs=o(a,"TABLE",{});var E0=l(Fs);A$=o(E0,"THEAD",{});var SU=l(A$);Go=o(SU,"TR",{});var y0=l(Go);ep=o(y0,"TH",{align:!0});var NU=l(ep);FD=u(NU,"Returned values"),NU.forEach(t),JD=h(y0),D$=o(y0,"TH",{align:!0}),l(D$).forEach(t),y0.forEach(t),SU.forEach(t),WD=h(E0),We=o(E0,"TBODY",{});var pd=l(We);Lo=o(pd,"TR",{});var w0=l(Lo);tp=o(w0,"TD",{align:!0});var xU=l(tp);O$=o(xU,"STRONG",{});var IU=l(O$);YD=u(IU,"sequence"),IU.forEach(t),xU.forEach(t),VD=h(w0),sp=o(w0,"TD",{align:!0});var HU=l(sp);XD=u(HU,"The string sent as an input"),HU.forEach(t),w0.forEach(t),QD=h(pd),Uo=o(pd,"TR",{});var b0=l(Uo);ap=o(b0,"TD",{align:!0});var BU=l(ap);P$=o(BU,"STRONG",{});var CU=l(P$);ZD=u(CU,"labels"),CU.forEach(t),BU.forEach(t),eO=h(b0),rp=o(b0,"TD",{align:!0});var GU=l(rp);tO=u(GU,"The list of strings for labels that you sent (in order)"),GU.forEach(t),b0.forEach(t),sO=h(pd),zo=o(pd,"TR",{});var T0=l(zo);np=o(T0,"TD",{align:!0});var LU=l(np);R$=o(LU,"STRONG",{});var UU=l(R$);aO=u(UU,"scores"),UU.forEach(t),LU.forEach(t),rO=h(T0),Js=o(T0,"TD",{align:!0});var j0=l(Js);nO=u(j0,"a list of floats that correspond the the probability of label, in the same order as "),S$=o(j0,"CODE",{});var zU=l(S$);oO=u(zU,"labels"),zU.forEach(t),lO=u(j0,"."),j0.forEach(t),T0.forEach(t),pd.forEach(t),E0.forEach(t),e2=h(a),Ye=o(a,"H3",{class:!0});var k0=l(Ye);Ws=o(k0,"A",{id:!0,class:!0,href:!0});var MU=l(Ws);N$=o(MU,"SPAN",{});var KU=l(N$);v(Mo.$$.fragment,KU),KU.forEach(t),MU.forEach(t),iO=h(k0),x$=o(k0,"SPAN",{});var FU=l(x$);uO=u(FU,"Conversational task"),FU.forEach(t),k0.forEach(t),t2=h(a),op=o(a,"P",{});var JU=l(op);cO=u(JU,`This task corresponds to any chatbot like structure. Models tend to have
shorter max_length, so please check with caution when using a given
model if you need long range dependency or not.`),JU.forEach(t),s2=h(a),v(Ys.$$.fragment,a),a2=h(a),Ko=o(a,"P",{});var mI=l(Ko);fO=u(mI,"Available with: "),Fo=o(mI,"A",{href:!0,rel:!0});var WU=l(Fo);pO=u(WU,"\u{1F917} Transformers"),WU.forEach(t),mI.forEach(t),r2=h(a),lp=o(a,"P",{});var YU=l(lp);hO=u(YU,"Example:"),YU.forEach(t),n2=h(a),v(Vs.$$.fragment,a),o2=h(a),ip=o(a,"P",{});var VU=l(ip);dO=u(VU,`When sending your request, you should send a JSON encoded payload. Here
are all the options`),VU.forEach(t),l2=h(a),Xs=o(a,"TABLE",{});var A0=l(Xs);I$=o(A0,"THEAD",{});var XU=l(I$);Jo=o(XU,"TR",{});var D0=l(Jo);up=o(D0,"TH",{align:!0});var QU=l(up);gO=u(QU,"All parameters"),QU.forEach(t),mO=h(D0),H$=o(D0,"TH",{align:!0}),l(H$).forEach(t),D0.forEach(t),XU.forEach(t),$O=h(A0),N=o(A0,"TBODY",{});var H=l(N);Wo=o(H,"TR",{});var O0=l(Wo);Yo=o(O0,"TD",{align:!0});var $I=l(Yo);B$=o($I,"STRONG",{});var ZU=l(B$);qO=u(ZU,"inputs"),ZU.forEach(t),_O=u($I," (required)"),$I.forEach(t),vO=h(O0),C$=o(O0,"TD",{align:!0}),l(C$).forEach(t),O0.forEach(t),EO=h(H),Vo=o(H,"TR",{});var P0=l(Vo);cp=o(P0,"TD",{align:!0});var ez=l(cp);yO=u(ez,"text (required)"),ez.forEach(t),wO=h(P0),fp=o(P0,"TD",{align:!0});var tz=l(fp);bO=u(tz,"The last input from the user in the conversation."),tz.forEach(t),P0.forEach(t),TO=h(H),Xo=o(H,"TR",{});var R0=l(Xo);pp=o(R0,"TD",{align:!0});var sz=l(pp);jO=u(sz,"generated_responses"),sz.forEach(t),kO=h(R0),hp=o(R0,"TD",{align:!0});var az=l(hp);AO=u(az,"A list of strings corresponding to the earlier replies from the model."),az.forEach(t),R0.forEach(t),DO=h(H),Qo=o(H,"TR",{});var S0=l(Qo);dp=o(S0,"TD",{align:!0});var rz=l(dp);OO=u(rz,"past_user_inputs"),rz.forEach(t),PO=h(S0),Qs=o(S0,"TD",{align:!0});var N0=l(Qs);RO=u(N0,"A list of strings corresponding to the earlier replies from the user. Should be of the same length of "),G$=o(N0,"CODE",{});var nz=l(G$);SO=u(nz,"generated_responses"),nz.forEach(t),NO=u(N0,"."),N0.forEach(t),S0.forEach(t),xO=h(H),Zo=o(H,"TR",{});var x0=l(Zo);gp=o(x0,"TD",{align:!0});var oz=l(gp);L$=o(oz,"STRONG",{});var lz=l(L$);IO=u(lz,"parameters"),lz.forEach(t),oz.forEach(t),HO=h(x0),mp=o(x0,"TD",{align:!0});var iz=l(mp);BO=u(iz,"a dict containing the following keys:"),iz.forEach(t),x0.forEach(t),CO=h(H),el=o(H,"TR",{});var I0=l(el);$p=o(I0,"TD",{align:!0});var uz=l($p);GO=u(uz,"min_length"),uz.forEach(t),LO=h(I0),be=o(I0,"TD",{align:!0});var hd=l(be);UO=u(hd,"(Default: "),U$=o(hd,"CODE",{});var cz=l(U$);zO=u(cz,"None"),cz.forEach(t),MO=u(hd,"). Integer to define the minimum length "),z$=o(hd,"STRONG",{});var fz=l(z$);KO=u(fz,"in tokens"),fz.forEach(t),FO=u(hd," of the output summary."),hd.forEach(t),I0.forEach(t),JO=h(H),tl=o(H,"TR",{});var H0=l(tl);qp=o(H0,"TD",{align:!0});var pz=l(qp);WO=u(pz,"max_length"),pz.forEach(t),YO=h(H0),Te=o(H0,"TD",{align:!0});var dd=l(Te);VO=u(dd,"(Default: "),M$=o(dd,"CODE",{});var hz=l(M$);XO=u(hz,"None"),hz.forEach(t),QO=u(dd,"). Integer to define the maximum length "),K$=o(dd,"STRONG",{});var dz=l(K$);ZO=u(dz,"in tokens"),dz.forEach(t),eP=u(dd," of the output summary."),dd.forEach(t),H0.forEach(t),tP=h(H),sl=o(H,"TR",{});var B0=l(sl);_p=o(B0,"TD",{align:!0});var gz=l(_p);sP=u(gz,"top_k"),gz.forEach(t),aP=h(B0),je=o(B0,"TD",{align:!0});var gd=l(je);rP=u(gd,"(Default: "),F$=o(gd,"CODE",{});var mz=l(F$);nP=u(mz,"None"),mz.forEach(t),oP=u(gd,"). Integer to define the top tokens considered within the "),J$=o(gd,"CODE",{});var $z=l(J$);lP=u($z,"sample"),$z.forEach(t),iP=u(gd," operation to create new text."),gd.forEach(t),B0.forEach(t),uP=h(H),al=o(H,"TR",{});var C0=l(al);vp=o(C0,"TD",{align:!0});var qz=l(vp);cP=u(qz,"top_p"),qz.forEach(t),fP=h(C0),ie=o(C0,"TD",{align:!0});var Za=l(ie);pP=u(Za,"(Default: "),W$=o(Za,"CODE",{});var _z=l(W$);hP=u(_z,"None"),_z.forEach(t),dP=u(Za,"). Float to define the tokens that are within the "),Y$=o(Za,"CODE",{});var vz=l(Y$);gP=u(vz,"sample"),vz.forEach(t),mP=u(Za," operation of text generation. Add tokens in the sample for more probable to least probable until the sum of the probabilities is greater than "),V$=o(Za,"CODE",{});var Ez=l(V$);$P=u(Ez,"top_p"),Ez.forEach(t),qP=u(Za,"."),Za.forEach(t),C0.forEach(t),_P=h(H),rl=o(H,"TR",{});var G0=l(rl);Ep=o(G0,"TD",{align:!0});var yz=l(Ep);vP=u(yz,"temperature"),yz.forEach(t),EP=h(G0),ke=o(G0,"TD",{align:!0});var md=l(ke);yP=u(md,"(Default: "),X$=o(md,"CODE",{});var wz=l(X$);wP=u(wz,"1.0"),wz.forEach(t),bP=u(md,"). Float (0.0-100.0). The temperature of the sampling operation. 1 means regular sampling, 0 means "),Q$=o(md,"CODE",{});var bz=l(Q$);TP=u(bz,"100.0"),bz.forEach(t),jP=u(md," is getting closer to uniform probability."),md.forEach(t),G0.forEach(t),kP=h(H),nl=o(H,"TR",{});var L0=l(nl);yp=o(L0,"TD",{align:!0});var Tz=l(yp);AP=u(Tz,"repetition_penalty"),Tz.forEach(t),DP=h(L0),Zs=o(L0,"TD",{align:!0});var U0=l(Zs);OP=u(U0,"(Default: "),Z$=o(U0,"CODE",{});var jz=l(Z$);PP=u(jz,"None"),jz.forEach(t),RP=u(U0,"). Float (0.0-100.0). The more a token is used within generation the more it is penalized to not be picked in successive generation passes."),U0.forEach(t),L0.forEach(t),SP=h(H),ol=o(H,"TR",{});var z0=l(ol);wp=o(z0,"TD",{align:!0});var kz=l(wp);NP=u(kz,"max_time"),kz.forEach(t),xP=h(z0),ea=o(z0,"TD",{align:!0});var M0=l(ea);IP=u(M0,"(Default: "),eq=o(M0,"CODE",{});var Az=l(eq);HP=u(Az,"None"),Az.forEach(t),BP=u(M0,"). Float (0-120.0). The amount of time in seconds that the query should take maximum. Network can cause some overhead so it will be a soft limit."),M0.forEach(t),z0.forEach(t),CP=h(H),ll=o(H,"TR",{});var K0=l(ll);bp=o(K0,"TD",{align:!0});var Dz=l(bp);tq=o(Dz,"STRONG",{});var Oz=l(tq);GP=u(Oz,"options"),Oz.forEach(t),Dz.forEach(t),LP=h(K0),Tp=o(K0,"TD",{align:!0});var Pz=l(Tp);UP=u(Pz,"a dict containing the following keys:"),Pz.forEach(t),K0.forEach(t),zP=h(H),il=o(H,"TR",{});var F0=l(il);jp=o(F0,"TD",{align:!0});var Rz=l(jp);MP=u(Rz,"use_gpu"),Rz.forEach(t),KP=h(F0),ta=o(F0,"TD",{align:!0});var J0=l(ta);FP=u(J0,"(Default: "),sq=o(J0,"CODE",{});var Sz=l(sq);JP=u(Sz,"false"),Sz.forEach(t),WP=u(J0,"). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),J0.forEach(t),F0.forEach(t),YP=h(H),ul=o(H,"TR",{});var W0=l(ul);kp=o(W0,"TD",{align:!0});var Nz=l(kp);VP=u(Nz,"use_cache"),Nz.forEach(t),XP=h(W0),sa=o(W0,"TD",{align:!0});var Y0=l(sa);QP=u(Y0,"(Default: "),aq=o(Y0,"CODE",{});var xz=l(aq);ZP=u(xz,"true"),xz.forEach(t),eR=u(Y0,"). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),Y0.forEach(t),W0.forEach(t),tR=h(H),cl=o(H,"TR",{});var V0=l(cl);Ap=o(V0,"TD",{align:!0});var Iz=l(Ap);sR=u(Iz,"wait_for_model"),Iz.forEach(t),aR=h(V0),aa=o(V0,"TD",{align:!0});var X0=l(aa);rR=u(X0,"(Default: "),rq=o(X0,"CODE",{});var Hz=l(rq);nR=u(Hz,"false"),Hz.forEach(t),oR=u(X0,") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),X0.forEach(t),V0.forEach(t),H.forEach(t),A0.forEach(t),i2=h(a),Dp=o(a,"P",{});var Bz=l(Dp);lR=u(Bz,"Return value is either a dict or a list of dicts if you sent a list of inputs"),Bz.forEach(t),u2=h(a),ra=o(a,"TABLE",{});var Q0=l(ra);nq=o(Q0,"THEAD",{});var Cz=l(nq);fl=o(Cz,"TR",{});var Z0=l(fl);Op=o(Z0,"TH",{align:!0});var Gz=l(Op);iR=u(Gz,"Returned values"),Gz.forEach(t),uR=h(Z0),oq=o(Z0,"TH",{align:!0}),l(oq).forEach(t),Z0.forEach(t),Cz.forEach(t),cR=h(Q0),pe=o(Q0,"TBODY",{});var er=l(pe);pl=o(er,"TR",{});var eb=l(pl);Pp=o(eb,"TD",{align:!0});var Lz=l(Pp);lq=o(Lz,"STRONG",{});var Uz=l(lq);fR=u(Uz,"generated_text"),Uz.forEach(t),Lz.forEach(t),pR=h(eb),Rp=o(eb,"TD",{align:!0});var zz=l(Rp);hR=u(zz,"The answer of the bot"),zz.forEach(t),eb.forEach(t),dR=h(er),hl=o(er,"TR",{});var tb=l(hl);Sp=o(tb,"TD",{align:!0});var Mz=l(Sp);iq=o(Mz,"STRONG",{});var Kz=l(iq);gR=u(Kz,"conversation"),Kz.forEach(t),Mz.forEach(t),mR=h(tb),Np=o(tb,"TD",{align:!0});var Fz=l(Np);$R=u(Fz,"A facility dictionnary to send back for the next input (with the new user input addition)."),Fz.forEach(t),tb.forEach(t),qR=h(er),dl=o(er,"TR",{});var sb=l(dl);xp=o(sb,"TD",{align:!0});var Jz=l(xp);_R=u(Jz,"past_user_inputs"),Jz.forEach(t),vR=h(sb),Ip=o(sb,"TD",{align:!0});var Wz=l(Ip);ER=u(Wz,"List of strings. The last inputs from the user in the conversation, <em>after the model has run."),Wz.forEach(t),sb.forEach(t),yR=h(er),gl=o(er,"TR",{});var ab=l(gl);Hp=o(ab,"TD",{align:!0});var Yz=l(Hp);wR=u(Yz,"generated_responses"),Yz.forEach(t),bR=h(ab),Bp=o(ab,"TD",{align:!0});var Vz=l(Bp);TR=u(Vz,"List of strings. The last outputs from the model in the conversation, <em>after the model has run."),Vz.forEach(t),ab.forEach(t),er.forEach(t),Q0.forEach(t),c2=h(a),Ve=o(a,"H3",{class:!0});var rb=l(Ve);na=o(rb,"A",{id:!0,class:!0,href:!0});var Xz=l(na);uq=o(Xz,"SPAN",{});var Qz=l(uq);v(ml.$$.fragment,Qz),Qz.forEach(t),Xz.forEach(t),jR=h(rb),cq=o(rb,"SPAN",{});var Zz=l(cq);kR=u(Zz,"Feature Extraction task"),Zz.forEach(t),rb.forEach(t),f2=h(a),Cp=o(a,"P",{});var eM=l(Cp);AR=u(eM,`This task reads some text and outputs raw float values, that are usually
consumed as part of a semantic database/semantic search.`),eM.forEach(t),p2=h(a),v(oa.$$.fragment,a),h2=h(a),Xe=o(a,"P",{});var t1=l(Xe);DR=u(t1,"Available with: "),$l=o(t1,"A",{href:!0,rel:!0});var tM=l($l);OR=u(tM,"\u{1F917} Transformers"),tM.forEach(t),PR=h(t1),ql=o(t1,"A",{href:!0,rel:!0});var sM=l(ql);RR=u(sM,"Sentence-transformers"),sM.forEach(t),t1.forEach(t),d2=h(a),Gp=o(a,"P",{});var aM=l(Gp);SR=u(aM,"Request:"),aM.forEach(t),g2=h(a),la=o(a,"TABLE",{});var nb=l(la);fq=o(nb,"THEAD",{});var rM=l(fq);_l=o(rM,"TR",{});var ob=l(_l);Lp=o(ob,"TH",{align:!0});var nM=l(Lp);NR=u(nM,"All parameters"),nM.forEach(t),xR=h(ob),pq=o(ob,"TH",{align:!0}),l(pq).forEach(t),ob.forEach(t),rM.forEach(t),IR=h(nb),ae=o(nb,"TBODY",{});var Pe=l(ae);vl=o(Pe,"TR",{});var lb=l(vl);El=o(lb,"TD",{align:!0});var qI=l(El);hq=o(qI,"STRONG",{});var oM=l(hq);HR=u(oM,"inputs"),oM.forEach(t),BR=u(qI," (required):"),qI.forEach(t),CR=h(lb),Up=o(lb,"TD",{align:!0});var lM=l(Up);GR=u(lM,"a string or a list of strings to get the features from."),lM.forEach(t),lb.forEach(t),LR=h(Pe),yl=o(Pe,"TR",{});var ib=l(yl);zp=o(ib,"TD",{align:!0});var iM=l(zp);dq=o(iM,"STRONG",{});var uM=l(dq);UR=u(uM,"options"),uM.forEach(t),iM.forEach(t),zR=h(ib),Mp=o(ib,"TD",{align:!0});var cM=l(Mp);MR=u(cM,"a dict containing the following keys:"),cM.forEach(t),ib.forEach(t),KR=h(Pe),wl=o(Pe,"TR",{});var ub=l(wl);Kp=o(ub,"TD",{align:!0});var fM=l(Kp);FR=u(fM,"use_gpu"),fM.forEach(t),JR=h(ub),ia=o(ub,"TD",{align:!0});var cb=l(ia);WR=u(cb,"(Default: "),gq=o(cb,"CODE",{});var pM=l(gq);YR=u(pM,"false"),pM.forEach(t),VR=u(cb,"). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),cb.forEach(t),ub.forEach(t),XR=h(Pe),bl=o(Pe,"TR",{});var fb=l(bl);Fp=o(fb,"TD",{align:!0});var hM=l(Fp);QR=u(hM,"use_cache"),hM.forEach(t),ZR=h(fb),ua=o(fb,"TD",{align:!0});var pb=l(ua);eS=u(pb,"(Default: "),mq=o(pb,"CODE",{});var dM=l(mq);tS=u(dM,"true"),dM.forEach(t),sS=u(pb,"). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),pb.forEach(t),fb.forEach(t),aS=h(Pe),Tl=o(Pe,"TR",{});var hb=l(Tl);Jp=o(hb,"TD",{align:!0});var gM=l(Jp);rS=u(gM,"wait_for_model"),gM.forEach(t),nS=h(hb),ca=o(hb,"TD",{align:!0});var db=l(ca);oS=u(db,"(Default: "),$q=o(db,"CODE",{});var mM=l($q);lS=u(mM,"false"),mM.forEach(t),iS=u(db,") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),db.forEach(t),hb.forEach(t),Pe.forEach(t),nb.forEach(t),m2=h(a),Wp=o(a,"P",{});var $M=l(Wp);uS=u($M,"Return value is either a dict or a list of dicts if you sent a list of inputs"),$M.forEach(t),$2=h(a),fa=o(a,"TABLE",{});var gb=l(fa);qq=o(gb,"THEAD",{});var qM=l(qq);jl=o(qM,"TR",{});var mb=l(jl);Yp=o(mb,"TH",{align:!0});var _M=l(Yp);cS=u(_M,"Returned values"),_M.forEach(t),fS=h(mb),_q=o(mb,"TH",{align:!0}),l(_q).forEach(t),mb.forEach(t),qM.forEach(t),pS=h(gb),vq=o(gb,"TBODY",{});var vM=l(vq);kl=o(vM,"TR",{});var $b=l(kl);Vp=o($b,"TD",{align:!0});var EM=l(Vp);Eq=o(EM,"STRONG",{});var yM=l(Eq);hS=u(yM,"A list of float (or list of list of floats)"),yM.forEach(t),EM.forEach(t),dS=h($b),Xp=o($b,"TD",{align:!0});var wM=l(Xp);gS=u(wM,"The numbers that are the representation features of the input."),wM.forEach(t),$b.forEach(t),vM.forEach(t),gb.forEach(t),q2=h(a),Qp=o(a,"SMALL",{});var bM=l(Qp);mS=u(bM,`Returned values are a list of floats, or a list of list of floats
(depending on if you sent a string or a list of string, and if the
automatic reduction, usually mean_pooling for instance was applied for
you or not. This should be explained on the model's README.`),bM.forEach(t),_2=h(a),Qe=o(a,"H2",{class:!0});var qb=l(Qe);pa=o(qb,"A",{id:!0,class:!0,href:!0});var TM=l(pa);yq=o(TM,"SPAN",{});var jM=l(yq);v(Al.$$.fragment,jM),jM.forEach(t),TM.forEach(t),$S=h(qb),wq=o(qb,"SPAN",{});var kM=l(wq);qS=u(kM,"Audio"),kM.forEach(t),qb.forEach(t),v2=h(a),Ze=o(a,"H3",{class:!0});var _b=l(Ze);ha=o(_b,"A",{id:!0,class:!0,href:!0});var AM=l(ha);bq=o(AM,"SPAN",{});var DM=l(bq);v(Dl.$$.fragment,DM),DM.forEach(t),AM.forEach(t),_S=h(_b),Tq=o(_b,"SPAN",{});var OM=l(Tq);vS=u(OM,"Automatic Speech Recognition task"),OM.forEach(t),_b.forEach(t),E2=h(a),Zp=o(a,"P",{});var PM=l(Zp);ES=u(PM,`This task reads some audio input and outputs the said words within the
audio files.`),PM.forEach(t),y2=h(a),v(da.$$.fragment,a),w2=h(a),v(ga.$$.fragment,a),b2=h(a),he=o(a,"P",{});var Gi=l(he);yS=u(Gi,"Available with: "),Ol=o(Gi,"A",{href:!0,rel:!0});var RM=l(Ol);wS=u(RM,"\u{1F917} Transformers"),RM.forEach(t),bS=h(Gi),Pl=o(Gi,"A",{href:!0,rel:!0});var SM=l(Pl);TS=u(SM,"ESPnet"),SM.forEach(t),jS=u(Gi,` and
`),Rl=o(Gi,"A",{href:!0,rel:!0});var NM=l(Rl);kS=u(NM,"SpeechBrain"),NM.forEach(t),Gi.forEach(t),T2=h(a),eh=o(a,"P",{});var xM=l(eh);AS=u(xM,"Request:"),xM.forEach(t),j2=h(a),v(ma.$$.fragment,a),k2=h(a),th=o(a,"P",{});var IM=l(th);DS=u(IM,`When sending your request, you should send a binary payload that simply
contains your audio file. We try to support most formats (Flac, Wav,
Mp3, Ogg etc...). And we automatically rescale the sampling rate to the
appropriate rate for the given model (usually 16KHz).`),IM.forEach(t),A2=h(a),$a=o(a,"TABLE",{});var vb=l($a);jq=o(vb,"THEAD",{});var HM=l(jq);Sl=o(HM,"TR",{});var Eb=l(Sl);sh=o(Eb,"TH",{align:!0});var BM=l(sh);OS=u(BM,"All parameters"),BM.forEach(t),PS=h(Eb),kq=o(Eb,"TH",{align:!0}),l(kq).forEach(t),Eb.forEach(t),HM.forEach(t),RS=h(vb),Aq=o(vb,"TBODY",{});var CM=l(Aq);Nl=o(CM,"TR",{});var yb=l(Nl);xl=o(yb,"TD",{align:!0});var _I=l(xl);Dq=o(_I,"STRONG",{});var GM=l(Dq);SS=u(GM,"no parameter"),GM.forEach(t),NS=u(_I," (required)"),_I.forEach(t),xS=h(yb),ah=o(yb,"TD",{align:!0});var LM=l(ah);IS=u(LM,"a binary representation of the audio file. No other parameters are currently allowed."),LM.forEach(t),yb.forEach(t),CM.forEach(t),vb.forEach(t),D2=h(a),rh=o(a,"P",{});var UM=l(rh);HS=u(UM,"Return value is either a dict or a list of dicts if you sent a list of inputs"),UM.forEach(t),O2=h(a),nh=o(a,"P",{});var zM=l(nh);BS=u(zM,"Response:"),zM.forEach(t),P2=h(a),v(qa.$$.fragment,a),R2=h(a),_a=o(a,"TABLE",{});var wb=l(_a);Oq=o(wb,"THEAD",{});var MM=l(Oq);Il=o(MM,"TR",{});var bb=l(Il);oh=o(bb,"TH",{align:!0});var KM=l(oh);CS=u(KM,"Returned values"),KM.forEach(t),GS=h(bb),Pq=o(bb,"TH",{align:!0}),l(Pq).forEach(t),bb.forEach(t),MM.forEach(t),LS=h(wb),Rq=o(wb,"TBODY",{});var FM=l(Rq);Hl=o(FM,"TR",{});var Tb=l(Hl);lh=o(Tb,"TD",{align:!0});var JM=l(lh);Sq=o(JM,"STRONG",{});var WM=l(Sq);US=u(WM,"text"),WM.forEach(t),JM.forEach(t),zS=h(Tb),ih=o(Tb,"TD",{align:!0});var YM=l(ih);MS=u(YM,"The string that was recognized within the audio file."),YM.forEach(t),Tb.forEach(t),FM.forEach(t),wb.forEach(t),S2=h(a),et=o(a,"H3",{class:!0});var jb=l(et);va=o(jb,"A",{id:!0,class:!0,href:!0});var VM=l(va);Nq=o(VM,"SPAN",{});var XM=l(Nq);v(Bl.$$.fragment,XM),XM.forEach(t),VM.forEach(t),KS=h(jb),xq=o(jb,"SPAN",{});var QM=l(xq);FS=u(QM,"Audio Classification task"),QM.forEach(t),jb.forEach(t),N2=h(a),uh=o(a,"P",{});var ZM=l(uh);JS=u(ZM,"This task reads some audio input and outputs the likelihood of classes."),ZM.forEach(t),x2=h(a),v(Ea.$$.fragment,a),I2=h(a),tt=o(a,"P",{});var s1=l(tt);WS=u(s1,"Available with: "),Cl=o(s1,"A",{href:!0,rel:!0});var eK=l(Cl);YS=u(eK,"\u{1F917} Transformers"),eK.forEach(t),VS=h(s1),Gl=o(s1,"A",{href:!0,rel:!0});var tK=l(Gl);XS=u(tK,"SpeechBrain"),tK.forEach(t),s1.forEach(t),H2=h(a),ch=o(a,"P",{});var sK=l(ch);QS=u(sK,"Request:"),sK.forEach(t),B2=h(a),v(ya.$$.fragment,a),C2=h(a),fh=o(a,"P",{});var aK=l(fh);ZS=u(aK,`When sending your request, you should send a binary payload that simply
contains your audio file. We try to support most formats (Flac, Wav,
Mp3, Ogg etc...). And we automatically rescale the sampling rate to the
appropriate rate for the given model (usually 16KHz).`),aK.forEach(t),G2=h(a),wa=o(a,"TABLE",{});var kb=l(wa);Iq=o(kb,"THEAD",{});var rK=l(Iq);Ll=o(rK,"TR",{});var Ab=l(Ll);ph=o(Ab,"TH",{align:!0});var nK=l(ph);eN=u(nK,"All parameters"),nK.forEach(t),tN=h(Ab),Hq=o(Ab,"TH",{align:!0}),l(Hq).forEach(t),Ab.forEach(t),rK.forEach(t),sN=h(kb),Bq=o(kb,"TBODY",{});var oK=l(Bq);Ul=o(oK,"TR",{});var Db=l(Ul);zl=o(Db,"TD",{align:!0});var vI=l(zl);Cq=o(vI,"STRONG",{});var lK=l(Cq);aN=u(lK,"no parameter"),lK.forEach(t),rN=u(vI," (required)"),vI.forEach(t),nN=h(Db),hh=o(Db,"TD",{align:!0});var iK=l(hh);oN=u(iK,"a binary representation of the audio file. No other parameters are currently allowed."),iK.forEach(t),Db.forEach(t),oK.forEach(t),kb.forEach(t),L2=h(a),dh=o(a,"P",{});var uK=l(dh);lN=u(uK,"Return value is a dict"),uK.forEach(t),U2=h(a),v(ba.$$.fragment,a),z2=h(a),Ta=o(a,"TABLE",{});var Ob=l(Ta);Gq=o(Ob,"THEAD",{});var cK=l(Gq);Ml=o(cK,"TR",{});var Pb=l(Ml);gh=o(Pb,"TH",{align:!0});var fK=l(gh);iN=u(fK,"Returned values"),fK.forEach(t),uN=h(Pb),Lq=o(Pb,"TH",{align:!0}),l(Lq).forEach(t),Pb.forEach(t),cK.forEach(t),cN=h(Ob),Kl=o(Ob,"TBODY",{});var Rb=l(Kl);Fl=o(Rb,"TR",{});var Sb=l(Fl);mh=o(Sb,"TD",{align:!0});var pK=l(mh);Uq=o(pK,"STRONG",{});var hK=l(Uq);fN=u(hK,"label"),hK.forEach(t),pK.forEach(t),pN=h(Sb),$h=o(Sb,"TD",{align:!0});var dK=l($h);hN=u(dK,"The label for the class (model specific)"),dK.forEach(t),Sb.forEach(t),dN=h(Rb),Jl=o(Rb,"TR",{});var Nb=l(Jl);qh=o(Nb,"TD",{align:!0});var gK=l(qh);zq=o(gK,"STRONG",{});var mK=l(zq);gN=u(mK,"score"),mK.forEach(t),gK.forEach(t),mN=h(Nb),_h=o(Nb,"TD",{align:!0});var $K=l(_h);$N=u($K,"A float that represents how likely it is that the audio file belongs to this class."),$K.forEach(t),Nb.forEach(t),Rb.forEach(t),Ob.forEach(t),M2=h(a),st=o(a,"H2",{class:!0});var xb=l(st);ja=o(xb,"A",{id:!0,class:!0,href:!0});var qK=l(ja);Mq=o(qK,"SPAN",{});var _K=l(Mq);v(Wl.$$.fragment,_K),_K.forEach(t),qK.forEach(t),qN=h(xb),Kq=o(xb,"SPAN",{});var vK=l(Kq);_N=u(vK,"Computer Vision"),vK.forEach(t),xb.forEach(t),K2=h(a),at=o(a,"H3",{class:!0});var Ib=l(at);ka=o(Ib,"A",{id:!0,class:!0,href:!0});var EK=l(ka);Fq=o(EK,"SPAN",{});var yK=l(Fq);v(Yl.$$.fragment,yK),yK.forEach(t),EK.forEach(t),vN=h(Ib),Jq=o(Ib,"SPAN",{});var wK=l(Jq);EN=u(wK,"Image Classification task"),wK.forEach(t),Ib.forEach(t),F2=h(a),vh=o(a,"P",{});var bK=l(vh);yN=u(bK,"This task reads some image input and outputs the likelihood of classes."),bK.forEach(t),J2=h(a),v(Aa.$$.fragment,a),W2=h(a),Vl=o(a,"P",{});var EI=l(Vl);wN=u(EI,"Available with: "),Xl=o(EI,"A",{href:!0,rel:!0});var TK=l(Xl);bN=u(TK,"\u{1F917} Transformers"),TK.forEach(t),EI.forEach(t),Y2=h(a),Eh=o(a,"P",{});var jK=l(Eh);TN=u(jK,"Request:"),jK.forEach(t),V2=h(a),v(Da.$$.fragment,a),X2=h(a),Oa=o(a,"P",{});var Hb=l(Oa);jN=u(Hb,`When sending your request, you should send a binary payload that simply
contains your image file. We support all image formats `),Ql=o(Hb,"A",{href:!0,rel:!0});var kK=l(Ql);kN=u(kK,`Pillow
supports`),kK.forEach(t),AN=u(Hb,"."),Hb.forEach(t),Q2=h(a),Pa=o(a,"TABLE",{});var Bb=l(Pa);Wq=o(Bb,"THEAD",{});var AK=l(Wq);Zl=o(AK,"TR",{});var Cb=l(Zl);yh=o(Cb,"TH",{align:!0});var DK=l(yh);DN=u(DK,"All parameters"),DK.forEach(t),ON=h(Cb),Yq=o(Cb,"TH",{align:!0}),l(Yq).forEach(t),Cb.forEach(t),AK.forEach(t),PN=h(Bb),Vq=o(Bb,"TBODY",{});var OK=l(Vq);ei=o(OK,"TR",{});var Gb=l(ei);ti=o(Gb,"TD",{align:!0});var yI=l(ti);Xq=o(yI,"STRONG",{});var PK=l(Xq);RN=u(PK,"no parameter"),PK.forEach(t),SN=u(yI," (required)"),yI.forEach(t),NN=h(Gb),wh=o(Gb,"TD",{align:!0});var RK=l(wh);xN=u(RK,"a binary representation of the image file. No other parameters are currently allowed."),RK.forEach(t),Gb.forEach(t),OK.forEach(t),Bb.forEach(t),Z2=h(a),bh=o(a,"P",{});var SK=l(bh);IN=u(SK,"Return value is a dict"),SK.forEach(t),eE=h(a),v(Ra.$$.fragment,a),tE=h(a),Sa=o(a,"TABLE",{});var Lb=l(Sa);Qq=o(Lb,"THEAD",{});var NK=l(Qq);si=o(NK,"TR",{});var Ub=l(si);Th=o(Ub,"TH",{align:!0});var xK=l(Th);HN=u(xK,"Returned values"),xK.forEach(t),BN=h(Ub),Zq=o(Ub,"TH",{align:!0}),l(Zq).forEach(t),Ub.forEach(t),NK.forEach(t),CN=h(Lb),ai=o(Lb,"TBODY",{});var zb=l(ai);ri=o(zb,"TR",{});var Mb=l(ri);jh=o(Mb,"TD",{align:!0});var IK=l(jh);e_=o(IK,"STRONG",{});var HK=l(e_);GN=u(HK,"label"),HK.forEach(t),IK.forEach(t),LN=h(Mb),kh=o(Mb,"TD",{align:!0});var BK=l(kh);UN=u(BK,"The label for the class (model specific)"),BK.forEach(t),Mb.forEach(t),zN=h(zb),ni=o(zb,"TR",{});var Kb=l(ni);Ah=o(Kb,"TD",{align:!0});var CK=l(Ah);t_=o(CK,"STRONG",{});var GK=l(t_);MN=u(GK,"score"),GK.forEach(t),CK.forEach(t),KN=h(Kb),Dh=o(Kb,"TD",{align:!0});var LK=l(Dh);FN=u(LK,"A float that represents how likely it is that the image file belongs to this class."),LK.forEach(t),Kb.forEach(t),zb.forEach(t),Lb.forEach(t),sE=h(a),rt=o(a,"H3",{class:!0});var Fb=l(rt);Na=o(Fb,"A",{id:!0,class:!0,href:!0});var UK=l(Na);s_=o(UK,"SPAN",{});var zK=l(s_);v(oi.$$.fragment,zK),zK.forEach(t),UK.forEach(t),JN=h(Fb),a_=o(Fb,"SPAN",{});var MK=l(a_);WN=u(MK,"Object Detection task"),MK.forEach(t),Fb.forEach(t),aE=h(a),Oh=o(a,"P",{});var KK=l(Oh);YN=u(KK,`This task reads some image input and outputs the likelihood of classes &
bounding boxes of detected objects.`),KK.forEach(t),rE=h(a),v(xa.$$.fragment,a),nE=h(a),li=o(a,"P",{});var wI=l(li);VN=u(wI,"Available with: "),ii=o(wI,"A",{href:!0,rel:!0});var FK=l(ii);XN=u(FK,"\u{1F917} Transformers"),FK.forEach(t),wI.forEach(t),oE=h(a),Ph=o(a,"P",{});var JK=l(Ph);QN=u(JK,"Request:"),JK.forEach(t),lE=h(a),v(Ia.$$.fragment,a),iE=h(a),Ha=o(a,"P",{});var Jb=l(Ha);ZN=u(Jb,`When sending your request, you should send a binary payload that simply
contains your image file. We support all image formats `),ui=o(Jb,"A",{href:!0,rel:!0});var WK=l(ui);ex=u(WK,`Pillow
supports`),WK.forEach(t),tx=u(Jb,"."),Jb.forEach(t),uE=h(a),Ba=o(a,"TABLE",{});var Wb=l(Ba);r_=o(Wb,"THEAD",{});var YK=l(r_);ci=o(YK,"TR",{});var Yb=l(ci);Rh=o(Yb,"TH",{align:!0});var VK=l(Rh);sx=u(VK,"All parameters"),VK.forEach(t),ax=h(Yb),n_=o(Yb,"TH",{align:!0}),l(n_).forEach(t),Yb.forEach(t),YK.forEach(t),rx=h(Wb),o_=o(Wb,"TBODY",{});var XK=l(o_);fi=o(XK,"TR",{});var Vb=l(fi);pi=o(Vb,"TD",{align:!0});var bI=l(pi);l_=o(bI,"STRONG",{});var QK=l(l_);nx=u(QK,"no parameter"),QK.forEach(t),ox=u(bI," (required)"),bI.forEach(t),lx=h(Vb),Sh=o(Vb,"TD",{align:!0});var ZK=l(Sh);ix=u(ZK,"a binary representation of the image file. No other parameters are currently allowed."),ZK.forEach(t),Vb.forEach(t),XK.forEach(t),Wb.forEach(t),cE=h(a),Nh=o(a,"P",{});var eF=l(Nh);ux=u(eF,"Return value is a dict"),eF.forEach(t),fE=h(a),v(Ca.$$.fragment,a),pE=h(a),Ga=o(a,"TABLE",{});var Xb=l(Ga);i_=o(Xb,"THEAD",{});var tF=l(i_);hi=o(tF,"TR",{});var Qb=l(hi);xh=o(Qb,"TH",{align:!0});var sF=l(xh);cx=u(sF,"Returned values"),sF.forEach(t),fx=h(Qb),u_=o(Qb,"TH",{align:!0}),l(u_).forEach(t),Qb.forEach(t),tF.forEach(t),px=h(Xb),nt=o(Xb,"TBODY",{});var $d=l(nt);di=o($d,"TR",{});var Zb=l(di);Ih=o(Zb,"TD",{align:!0});var aF=l(Ih);c_=o(aF,"STRONG",{});var rF=l(c_);hx=u(rF,"label"),rF.forEach(t),aF.forEach(t),dx=h(Zb),Hh=o(Zb,"TD",{align:!0});var nF=l(Hh);gx=u(nF,"The label for the class (model specific) of a detected object."),nF.forEach(t),Zb.forEach(t),mx=h($d),gi=o($d,"TR",{});var e3=l(gi);Bh=o(e3,"TD",{align:!0});var oF=l(Bh);f_=o(oF,"STRONG",{});var lF=l(f_);$x=u(lF,"score"),lF.forEach(t),oF.forEach(t),qx=h(e3),Ch=o(e3,"TD",{align:!0});var iF=l(Ch);_x=u(iF,"A float that represents how likely it is that the detected object belongs to the given class."),iF.forEach(t),e3.forEach(t),vx=h($d),mi=o($d,"TR",{});var t3=l(mi);Gh=o(t3,"TD",{align:!0});var uF=l(Gh);p_=o(uF,"STRONG",{});var cF=l(p_);Ex=u(cF,"box"),cF.forEach(t),uF.forEach(t),yx=h(t3),Lh=o(t3,"TD",{align:!0});var fF=l(Lh);wx=u(fF,"A dict (with keys [xmin,ymin,xmax,ymax]) representing the bounding box of a detected object."),fF.forEach(t),t3.forEach(t),$d.forEach(t),Xb.forEach(t),hE=h(a),ot=o(a,"H3",{class:!0});var s3=l(ot);La=o(s3,"A",{id:!0,class:!0,href:!0});var pF=l(La);h_=o(pF,"SPAN",{});var hF=l(h_);v($i.$$.fragment,hF),hF.forEach(t),pF.forEach(t),bx=h(s3),d_=o(s3,"SPAN",{});var dF=l(d_);Tx=u(dF,"Image Segmentation task"),dF.forEach(t),s3.forEach(t),dE=h(a),Uh=o(a,"P",{});var gF=l(Uh);jx=u(gF,`This task reads some image input and outputs the likelihood of classes &
bounding boxes of detected objects.`),gF.forEach(t),gE=h(a),v(Ua.$$.fragment,a),mE=h(a),qi=o(a,"P",{});var TI=l(qi);kx=u(TI,"Available with: "),_i=o(TI,"A",{href:!0,rel:!0});var mF=l(_i);Ax=u(mF,"\u{1F917} Transformers"),mF.forEach(t),TI.forEach(t),$E=h(a),zh=o(a,"P",{});var $F=l(zh);Dx=u($F,"Request:"),$F.forEach(t),qE=h(a),v(za.$$.fragment,a),_E=h(a),Ma=o(a,"P",{});var a3=l(Ma);Ox=u(a3,`When sending your request, you should send a binary payload that simply
contains your image file. We support all image formats `),vi=o(a3,"A",{href:!0,rel:!0});var qF=l(vi);Px=u(qF,`Pillow
supports`),qF.forEach(t),Rx=u(a3,"."),a3.forEach(t),vE=h(a),Ka=o(a,"TABLE",{});var r3=l(Ka);g_=o(r3,"THEAD",{});var _F=l(g_);Ei=o(_F,"TR",{});var n3=l(Ei);Mh=o(n3,"TH",{align:!0});var vF=l(Mh);Sx=u(vF,"All parameters"),vF.forEach(t),Nx=h(n3),m_=o(n3,"TH",{align:!0}),l(m_).forEach(t),n3.forEach(t),_F.forEach(t),xx=h(r3),$_=o(r3,"TBODY",{});var EF=l($_);yi=o(EF,"TR",{});var o3=l(yi);wi=o(o3,"TD",{align:!0});var jI=l(wi);q_=o(jI,"STRONG",{});var yF=l(q_);Ix=u(yF,"no parameter"),yF.forEach(t),Hx=u(jI," (required)"),jI.forEach(t),Bx=h(o3),Kh=o(o3,"TD",{align:!0});var wF=l(Kh);Cx=u(wF,"a binary representation of the image file. No other parameters are currently allowed."),wF.forEach(t),o3.forEach(t),EF.forEach(t),r3.forEach(t),EE=h(a),Fh=o(a,"P",{});var bF=l(Fh);Gx=u(bF,"Return value is a dict"),bF.forEach(t),yE=h(a),v(Fa.$$.fragment,a),wE=h(a),Ja=o(a,"TABLE",{});var l3=l(Ja);__=o(l3,"THEAD",{});var TF=l(__);bi=o(TF,"TR",{});var i3=l(bi);Jh=o(i3,"TH",{align:!0});var jF=l(Jh);Lx=u(jF,"Returned values"),jF.forEach(t),Ux=h(i3),v_=o(i3,"TH",{align:!0}),l(v_).forEach(t),i3.forEach(t),TF.forEach(t),zx=h(l3),lt=o(l3,"TBODY",{});var qd=l(lt);Ti=o(qd,"TR",{});var u3=l(Ti);Wh=o(u3,"TD",{align:!0});var kF=l(Wh);E_=o(kF,"STRONG",{});var AF=l(E_);Mx=u(AF,"label"),AF.forEach(t),kF.forEach(t),Kx=h(u3),Yh=o(u3,"TD",{align:!0});var DF=l(Yh);Fx=u(DF,"The label for the class (model specific) of a segment."),DF.forEach(t),u3.forEach(t),Jx=h(qd),ji=o(qd,"TR",{});var c3=l(ji);Vh=o(c3,"TD",{align:!0});var OF=l(Vh);y_=o(OF,"STRONG",{});var PF=l(y_);Wx=u(PF,"score"),PF.forEach(t),OF.forEach(t),Yx=h(c3),Xh=o(c3,"TD",{align:!0});var RF=l(Xh);Vx=u(RF,"A float that represents how likely it is that the segment belongs to the given class."),RF.forEach(t),c3.forEach(t),Xx=h(qd),ki=o(qd,"TR",{});var f3=l(ki);Qh=o(f3,"TD",{align:!0});var SF=l(Qh);w_=o(SF,"STRONG",{});var NF=l(w_);Qx=u(NF,"mask"),NF.forEach(t),SF.forEach(t),Zx=h(f3),Zh=o(f3,"TD",{align:!0});var xF=l(Zh);eI=u(xF,"A str (base64 str of a single channel black-and-white img) representing the mask of a segment."),xF.forEach(t),f3.forEach(t),qd.forEach(t),l3.forEach(t),this.h()},h(){f(r,"name","hf:doc:metadata"),f(r,"content",JSON.stringify(pY)),f(d,"id","detailed-parameters"),f(d,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(d,"href","#detailed-parameters"),f(s,"class","relative group"),f(ne,"id","which-task-is-used-by-this-model"),f(ne,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(ne,"href","#which-task-is-used-by-this-model"),f(D,"class","relative group"),f(ut,"class","block dark:hidden"),IF(ut.src,kI="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/inference-api/task.png")||f(ut,"src",kI),f(ut,"width","300"),f(ct,"class","hidden dark:block invert"),IF(ct.src,AI="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/inference-api/task-dark.png")||f(ct,"src",AI),f(ct,"width","300"),f(ft,"id","natural-language-processing"),f(ft,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(ft,"href","#natural-language-processing"),f(Ne,"class","relative group"),f(pt,"id","fill-mask-task"),f(pt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(pt,"href","#fill-mask-task"),f(xe,"class","relative group"),f(nr,"href","https://github.com/huggingface/transformers"),f(nr,"rel","nofollow"),f(Fi,"align","left"),f(bd,"align","left"),f(ir,"align","left"),f(Ji,"align","left"),f(Wi,"align","left"),f(Yi,"align","left"),f(Vi,"align","left"),f(mt,"align","left"),f(Xi,"align","left"),f($t,"align","left"),f(Qi,"align","left"),f(qt,"align","left"),f(eu,"align","left"),f(Pd,"align","left"),f(tu,"align","left"),f(su,"align","left"),f(au,"align","left"),f(ru,"align","left"),f(nu,"align","left"),f(ou,"align","left"),f(lu,"align","left"),f(iu,"align","left"),f(Et,"id","summarization-task"),f(Et,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(Et,"href","#summarization-task"),f(Ie,"class","relative group"),f(uu,"href","mailto:api-enterprise@huggingface.co"),f(vr,"href","https://github.com/huggingface/transformers"),f(vr,"rel","nofollow"),f(pu,"align","left"),f(Cd,"align","left"),f(wr,"align","left"),f(hu,"align","left"),f(du,"align","left"),f(gu,"align","left"),f(mu,"align","left"),f(de,"align","left"),f($u,"align","left"),f(ge,"align","left"),f(qu,"align","left"),f(me,"align","left"),f(_u,"align","left"),f(oe,"align","left"),f(vu,"align","left"),f($e,"align","left"),f(Eu,"align","left"),f(jt,"align","left"),f(yu,"align","left"),f(kt,"align","left"),f(wu,"align","left"),f(bu,"align","left"),f(Tu,"align","left"),f(At,"align","left"),f(ju,"align","left"),f(Dt,"align","left"),f(ku,"align","left"),f(Ot,"align","left"),f(Du,"align","left"),f(og,"align","left"),f(Ou,"align","left"),f(Pu,"align","left"),f(Rt,"id","question-answering-task"),f(Rt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(Rt,"href","#question-answering-task"),f(He,"class","relative group"),f(Cr,"href","https://github.com/huggingface/transformers"),f(Cr,"rel","nofollow"),f(Gr,"href","https://github.com/allenai/allennlp"),f(Gr,"rel","nofollow"),f(Iu,"align","left"),f(pg,"align","left"),f(Hu,"align","left"),f(Bu,"align","left"),f(Cu,"align","left"),f(Gu,"align","left"),f(Lu,"align","left"),f(Ht,"align","left"),f(Uu,"align","left"),f(Bt,"align","left"),f(Ct,"id","table-question-answering-task"),f(Ct,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(Ct,"href","#table-question-answering-task"),f(Ce,"class","relative group"),f(Wr,"href","https://github.com/huggingface/transformers"),f(Wr,"rel","nofollow"),f(Fu,"align","left"),f(yg,"align","left"),f(Xr,"align","left"),f(bg,"align","left"),f(Ju,"align","left"),f(Wu,"align","left"),f(Yu,"align","left"),f(Vu,"align","left"),f(Xu,"align","left"),f(Qu,"align","left"),f(Zu,"align","left"),f(zt,"align","left"),f(ec,"align","left"),f(Mt,"align","left"),f(tc,"align","left"),f(Kt,"align","left"),f(ac,"align","left"),f(Og,"align","left"),f(rc,"align","left"),f(nc,"align","left"),f(oc,"align","left"),f(lc,"align","left"),f(ic,"align","left"),f(uc,"align","left"),f(cc,"align","left"),f(fc,"align","left"),f(Wt,"id","text-classification-task"),f(Wt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(Wt,"href","#text-classification-task"),f(Ge,"class","relative group"),f(pn,"href","https://github.com/huggingface/transformers"),f(pn,"rel","nofollow"),f(gc,"align","left"),f(Bg,"align","left"),f(gn,"align","left"),f(mc,"align","left"),f($c,"align","left"),f(qc,"align","left"),f(_c,"align","left"),f(Qt,"align","left"),f(vc,"align","left"),f(Zt,"align","left"),f(Ec,"align","left"),f(es,"align","left"),f(wc,"align","left"),f(Kg,"align","left"),f(bc,"align","left"),f(Tc,"align","left"),f(jc,"align","left"),f(kc,"align","left"),f(as,"id","text-generation-task"),f(as,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(as,"href","#text-generation-task"),f(Le,"class","relative group"),f(jn,"href","https://github.com/huggingface/transformers"),f(jn,"rel","nofollow"),f(Pc,"align","left"),f(Xg,"align","left"),f(Dn,"align","left"),f(Rc,"align","left"),f(Sc,"align","left"),f(Nc,"align","left"),f(xc,"align","left"),f(qe,"align","left"),f(Ic,"align","left"),f(le,"align","left"),f(Hc,"align","left"),f(_e,"align","left"),f(Bc,"align","left"),f(ls,"align","left"),f(Cc,"align","left"),f(ve,"align","left"),f(Gc,"align","left"),f(Ee,"align","left"),f(Lc,"align","left"),f(ye,"align","left"),f(Uc,"align","left"),f(is,"align","left"),f(zc,"align","left"),f(us,"align","left"),f(Mc,"align","left"),f(Kc,"align","left"),f(Fc,"align","left"),f(cs,"align","left"),f(Jc,"align","left"),f(fs,"align","left"),f(Wc,"align","left"),f(ps,"align","left"),f(Vc,"align","left"),f(Em,"align","left"),f(Xc,"align","left"),f(Qc,"align","left"),f(gs,"id","text2text-generation-task"),f(gs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(gs,"href","#text2text-generation-task"),f(Ue,"class","relative group"),f(Zc,"href","#text-generation-task"),f($s,"id","token-classification-task"),f($s,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f($s,"href","#token-classification-task"),f(ze,"class","relative group"),f(Wn,"href","https://github.com/huggingface/transformers"),f(Wn,"rel","nofollow"),f(Yn,"href","https://github.com/flairNLP/flair"),f(Yn,"rel","nofollow"),f(af,"align","left"),f(Dm,"align","left"),f(Qn,"align","left"),f(rf,"align","left"),f(nf,"align","left"),f(of,"align","left"),f(lf,"align","left"),f(x,"align","left"),f(uf,"align","left"),f(cf,"align","left"),f(ff,"align","left"),f(Es,"align","left"),f(pf,"align","left"),f(ys,"align","left"),f(hf,"align","left"),f(ws,"align","left"),f(gf,"align","left"),f(Fm,"align","left"),f(mf,"align","left"),f($f,"align","left"),f(qf,"align","left"),f(_f,"align","left"),f(vf,"align","left"),f(Ef,"align","left"),f(yf,"align","left"),f(js,"align","left"),f(wf,"align","left"),f(ks,"align","left"),f(As,"id","named-entity-recognition-ner-task"),f(As,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(As,"href","#named-entity-recognition-ner-task"),f(Ke,"class","relative group"),f(bf,"href","#token-classification-task"),f(Ds,"id","translation-task"),f(Ds,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(Ds,"href","#translation-task"),f(Fe,"class","relative group"),f(mo,"href","https://github.com/huggingface/transformers"),f(mo,"rel","nofollow"),f(Af,"align","left"),f(n$,"align","left"),f(_o,"align","left"),f(Df,"align","left"),f(Of,"align","left"),f(Pf,"align","left"),f(Rf,"align","left"),f(Ss,"align","left"),f(Sf,"align","left"),f(Ns,"align","left"),f(Nf,"align","left"),f(xs,"align","left"),f(If,"align","left"),f(p$,"align","left"),f(Hf,"align","left"),f(Bf,"align","left"),f(Hs,"id","zeroshot-classification-task"),f(Hs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(Hs,"href","#zeroshot-classification-task"),f(Je,"class","relative group"),f(Ao,"href","https://github.com/huggingface/transformers"),f(Ao,"rel","nofollow"),f(Uf,"align","left"),f(q$,"align","left"),f(Po,"align","left"),f(zf,"align","left"),f(So,"align","left"),f(Mf,"align","left"),f(Kf,"align","left"),f(we,"align","left"),f(Ff,"align","left"),f(Ls,"align","left"),f(Jf,"align","left"),f(Wf,"align","left"),f(Yf,"align","left"),f(Us,"align","left"),f(Vf,"align","left"),f(zs,"align","left"),f(Xf,"align","left"),f(Ms,"align","left"),f(ep,"align","left"),f(D$,"align","left"),f(tp,"align","left"),f(sp,"align","left"),f(ap,"align","left"),f(rp,"align","left"),f(np,"align","left"),f(Js,"align","left"),f(Ws,"id","conversational-task"),f(Ws,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(Ws,"href","#conversational-task"),f(Ye,"class","relative group"),f(Fo,"href","https://github.com/huggingface/transformers"),f(Fo,"rel","nofollow"),f(up,"align","left"),f(H$,"align","left"),f(Yo,"align","left"),f(C$,"align","left"),f(cp,"align","left"),f(fp,"align","left"),f(pp,"align","left"),f(hp,"align","left"),f(dp,"align","left"),f(Qs,"align","left"),f(gp,"align","left"),f(mp,"align","left"),f($p,"align","left"),f(be,"align","left"),f(qp,"align","left"),f(Te,"align","left"),f(_p,"align","left"),f(je,"align","left"),f(vp,"align","left"),f(ie,"align","left"),f(Ep,"align","left"),f(ke,"align","left"),f(yp,"align","left"),f(Zs,"align","left"),f(wp,"align","left"),f(ea,"align","left"),f(bp,"align","left"),f(Tp,"align","left"),f(jp,"align","left"),f(ta,"align","left"),f(kp,"align","left"),f(sa,"align","left"),f(Ap,"align","left"),f(aa,"align","left"),f(Op,"align","left"),f(oq,"align","left"),f(Pp,"align","left"),f(Rp,"align","left"),f(Sp,"align","left"),f(Np,"align","left"),f(xp,"align","left"),f(Ip,"align","left"),f(Hp,"align","left"),f(Bp,"align","left"),f(na,"id","feature-extraction-task"),f(na,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(na,"href","#feature-extraction-task"),f(Ve,"class","relative group"),f($l,"href","https://github.com/huggingface/transformers"),f($l,"rel","nofollow"),f(ql,"href","https://github.com/UKPLab/sentence-transformers"),f(ql,"rel","nofollow"),f(Lp,"align","left"),f(pq,"align","left"),f(El,"align","left"),f(Up,"align","left"),f(zp,"align","left"),f(Mp,"align","left"),f(Kp,"align","left"),f(ia,"align","left"),f(Fp,"align","left"),f(ua,"align","left"),f(Jp,"align","left"),f(ca,"align","left"),f(Yp,"align","left"),f(_q,"align","left"),f(Vp,"align","left"),f(Xp,"align","left"),f(pa,"id","audio"),f(pa,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(pa,"href","#audio"),f(Qe,"class","relative group"),f(ha,"id","automatic-speech-recognition-task"),f(ha,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(ha,"href","#automatic-speech-recognition-task"),f(Ze,"class","relative group"),f(Ol,"href","https://github.com/huggingface/transformers"),f(Ol,"rel","nofollow"),f(Pl,"href","https://github.com/espnet/espnet"),f(Pl,"rel","nofollow"),f(Rl,"href","https://github.com/speechbrain/speechbrain"),f(Rl,"rel","nofollow"),f(sh,"align","left"),f(kq,"align","left"),f(xl,"align","left"),f(ah,"align","left"),f(oh,"align","left"),f(Pq,"align","left"),f(lh,"align","left"),f(ih,"align","left"),f(va,"id","audio-classification-task"),f(va,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(va,"href","#audio-classification-task"),f(et,"class","relative group"),f(Cl,"href","https://github.com/huggingface/transformers"),f(Cl,"rel","nofollow"),f(Gl,"href","https://github.com/speechbrain/speechbrain"),f(Gl,"rel","nofollow"),f(ph,"align","left"),f(Hq,"align","left"),f(zl,"align","left"),f(hh,"align","left"),f(gh,"align","left"),f(Lq,"align","left"),f(mh,"align","left"),f($h,"align","left"),f(qh,"align","left"),f(_h,"align","left"),f(ja,"id","computer-vision"),f(ja,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(ja,"href","#computer-vision"),f(st,"class","relative group"),f(ka,"id","image-classification-task"),f(ka,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(ka,"href","#image-classification-task"),f(at,"class","relative group"),f(Xl,"href","https://github.com/huggingface/transformers"),f(Xl,"rel","nofollow"),f(Ql,"href","https://pillow.readthedocs.io/en/stable/handbook/image-file-formats.html"),f(Ql,"rel","nofollow"),f(yh,"align","left"),f(Yq,"align","left"),f(ti,"align","left"),f(wh,"align","left"),f(Th,"align","left"),f(Zq,"align","left"),f(jh,"align","left"),f(kh,"align","left"),f(Ah,"align","left"),f(Dh,"align","left"),f(Na,"id","object-detection-task"),f(Na,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(Na,"href","#object-detection-task"),f(rt,"class","relative group"),f(ii,"href","https://github.com/huggingface/transformers"),f(ii,"rel","nofollow"),f(ui,"href","https://pillow.readthedocs.io/en/stable/handbook/image-file-formats.html"),f(ui,"rel","nofollow"),f(Rh,"align","left"),f(n_,"align","left"),f(pi,"align","left"),f(Sh,"align","left"),f(xh,"align","left"),f(u_,"align","left"),f(Ih,"align","left"),f(Hh,"align","left"),f(Bh,"align","left"),f(Ch,"align","left"),f(Gh,"align","left"),f(Lh,"align","left"),f(La,"id","image-segmentation-task"),f(La,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(La,"href","#image-segmentation-task"),f(ot,"class","relative group"),f(_i,"href","https://github.com/huggingface/transformers"),f(_i,"rel","nofollow"),f(vi,"href","https://pillow.readthedocs.io/en/stable/handbook/image-file-formats.html"),f(vi,"rel","nofollow"),f(Mh,"align","left"),f(m_,"align","left"),f(wi,"align","left"),f(Kh,"align","left"),f(Jh,"align","left"),f(v_,"align","left"),f(Wh,"align","left"),f(Yh,"align","left"),f(Vh,"align","left"),f(Xh,"align","left"),f(Qh,"align","left"),f(Zh,"align","left")},m(a,g){e(document.head,r),m(a,c,g),m(a,s,g),e(s,d),e(d,$),E(k,$,null),e(s,A),e(s,j),e(j,T),m(a,S,g),m(a,D,g),e(D,ne),e(ne,Re),E(Q,Re,null),e(D,Y),e(D,it),e(it,Li),m(a,tr,g),m(a,Se,g),e(Se,p3),m(a,a1,g),m(a,Ui,g),e(Ui,h3),m(a,r1,g),m(a,ut,g),m(a,n1,g),m(a,ct,g),m(a,o1,g),m(a,Ne,g),e(Ne,ft),e(ft,_d),E(sr,_d,null),e(Ne,d3),e(Ne,vd),e(vd,g3),m(a,l1,g),m(a,xe,g),e(xe,pt),e(pt,Ed),E(ar,Ed,null),e(xe,m3),e(xe,yd),e(yd,$3),m(a,i1,g),m(a,zi,g),e(zi,q3),m(a,u1,g),E(ht,a,g),m(a,c1,g),m(a,rr,g),e(rr,_3),e(rr,nr),e(nr,v3),m(a,f1,g),m(a,Mi,g),e(Mi,E3),m(a,p1,g),E(dt,a,g),m(a,h1,g),m(a,Ki,g),e(Ki,y3),m(a,d1,g),m(a,gt,g),e(gt,wd),e(wd,or),e(or,Fi),e(Fi,w3),e(or,b3),e(or,bd),e(gt,T3),e(gt,Z),e(Z,lr),e(lr,ir),e(ir,Td),e(Td,j3),e(ir,k3),e(lr,A3),e(lr,Ji),e(Ji,D3),e(Z,O3),e(Z,ur),e(ur,Wi),e(Wi,jd),e(jd,P3),e(ur,R3),e(ur,Yi),e(Yi,S3),e(Z,N3),e(Z,cr),e(cr,Vi),e(Vi,x3),e(cr,I3),e(cr,mt),e(mt,H3),e(mt,kd),e(kd,B3),e(mt,C3),e(Z,G3),e(Z,fr),e(fr,Xi),e(Xi,L3),e(fr,U3),e(fr,$t),e($t,z3),e($t,Ad),e(Ad,M3),e($t,K3),e(Z,F3),e(Z,pr),e(pr,Qi),e(Qi,J3),e(pr,W3),e(pr,qt),e(qt,Y3),e(qt,Dd),e(Dd,V3),e(qt,X3),m(a,g1,g),m(a,Zi,g),e(Zi,Q3),m(a,m1,g),E(_t,a,g),m(a,$1,g),m(a,vt,g),e(vt,Od),e(Od,hr),e(hr,eu),e(eu,Z3),e(hr,eT),e(hr,Pd),e(vt,tT),e(vt,ue),e(ue,dr),e(dr,tu),e(tu,Rd),e(Rd,sT),e(dr,aT),e(dr,su),e(su,rT),e(ue,nT),e(ue,gr),e(gr,au),e(au,Sd),e(Sd,oT),e(gr,lT),e(gr,ru),e(ru,iT),e(ue,uT),e(ue,mr),e(mr,nu),e(nu,Nd),e(Nd,cT),e(mr,fT),e(mr,ou),e(ou,pT),e(ue,hT),e(ue,$r),e($r,lu),e(lu,xd),e(xd,dT),e($r,gT),e($r,iu),e(iu,mT),m(a,q1,g),m(a,Ie,g),e(Ie,Et),e(Et,Id),E(qr,Id,null),e(Ie,$T),e(Ie,Hd),e(Hd,qT),m(a,_1,g),m(a,yt,g),e(yt,_T),e(yt,uu),e(uu,vT),e(yt,ET),m(a,v1,g),E(wt,a,g),m(a,E1,g),m(a,_r,g),e(_r,yT),e(_r,vr),e(vr,wT),m(a,y1,g),m(a,cu,g),e(cu,bT),m(a,w1,g),E(bt,a,g),m(a,b1,g),m(a,fu,g),e(fu,TT),m(a,T1,g),m(a,Tt,g),e(Tt,Bd),e(Bd,Er),e(Er,pu),e(pu,jT),e(Er,kT),e(Er,Cd),e(Tt,AT),e(Tt,G),e(G,yr),e(yr,wr),e(wr,Gd),e(Gd,DT),e(wr,OT),e(yr,PT),e(yr,hu),e(hu,RT),e(G,ST),e(G,br),e(br,du),e(du,Ld),e(Ld,NT),e(br,xT),e(br,gu),e(gu,IT),e(G,HT),e(G,Tr),e(Tr,mu),e(mu,BT),e(Tr,CT),e(Tr,de),e(de,GT),e(de,Ud),e(Ud,LT),e(de,UT),e(de,zd),e(zd,zT),e(de,MT),e(G,KT),e(G,jr),e(jr,$u),e($u,FT),e(jr,JT),e(jr,ge),e(ge,WT),e(ge,Md),e(Md,YT),e(ge,VT),e(ge,Kd),e(Kd,XT),e(ge,QT),e(G,ZT),e(G,kr),e(kr,qu),e(qu,ej),e(kr,tj),e(kr,me),e(me,sj),e(me,Fd),e(Fd,aj),e(me,rj),e(me,Jd),e(Jd,nj),e(me,oj),e(G,lj),e(G,Ar),e(Ar,_u),e(_u,ij),e(Ar,uj),e(Ar,oe),e(oe,cj),e(oe,Wd),e(Wd,fj),e(oe,pj),e(oe,Yd),e(Yd,hj),e(oe,dj),e(oe,Vd),e(Vd,gj),e(oe,mj),e(G,$j),e(G,Dr),e(Dr,vu),e(vu,qj),e(Dr,_j),e(Dr,$e),e($e,vj),e($e,Xd),e(Xd,Ej),e($e,yj),e($e,Qd),e(Qd,wj),e($e,bj),e(G,Tj),e(G,Or),e(Or,Eu),e(Eu,jj),e(Or,kj),e(Or,jt),e(jt,Aj),e(jt,Zd),e(Zd,Dj),e(jt,Oj),e(G,Pj),e(G,Pr),e(Pr,yu),e(yu,Rj),e(Pr,Sj),e(Pr,kt),e(kt,Nj),e(kt,eg),e(eg,xj),e(kt,Ij),e(G,Hj),e(G,Rr),e(Rr,wu),e(wu,tg),e(tg,Bj),e(Rr,Cj),e(Rr,bu),e(bu,Gj),e(G,Lj),e(G,Sr),e(Sr,Tu),e(Tu,Uj),e(Sr,zj),e(Sr,At),e(At,Mj),e(At,sg),e(sg,Kj),e(At,Fj),e(G,Jj),e(G,Nr),e(Nr,ju),e(ju,Wj),e(Nr,Yj),e(Nr,Dt),e(Dt,Vj),e(Dt,ag),e(ag,Xj),e(Dt,Qj),e(G,Zj),e(G,xr),e(xr,ku),e(ku,e4),e(xr,t4),e(xr,Ot),e(Ot,s4),e(Ot,rg),e(rg,a4),e(Ot,r4),m(a,j1,g),m(a,Au,g),e(Au,n4),m(a,k1,g),m(a,Pt,g),e(Pt,ng),e(ng,Ir),e(Ir,Du),e(Du,o4),e(Ir,l4),e(Ir,og),e(Pt,i4),e(Pt,lg),e(lg,Hr),e(Hr,Ou),e(Ou,ig),e(ig,u4),e(Hr,c4),e(Hr,Pu),e(Pu,f4),m(a,A1,g),m(a,He,g),e(He,Rt),e(Rt,ug),E(Br,ug,null),e(He,p4),e(He,cg),e(cg,h4),m(a,D1,g),m(a,Ru,g),e(Ru,d4),m(a,O1,g),E(St,a,g),m(a,P1,g),m(a,Be,g),e(Be,g4),e(Be,Cr),e(Cr,m4),e(Be,$4),e(Be,Gr),e(Gr,q4),m(a,R1,g),m(a,Su,g),e(Su,_4),m(a,S1,g),E(Nt,a,g),m(a,N1,g),m(a,Nu,g),e(Nu,v4),m(a,x1,g),m(a,xu,g),e(xu,E4),m(a,I1,g),E(xt,a,g),m(a,H1,g),m(a,It,g),e(It,fg),e(fg,Lr),e(Lr,Iu),e(Iu,y4),e(Lr,w4),e(Lr,pg),e(It,b4),e(It,ce),e(ce,Ur),e(Ur,Hu),e(Hu,hg),e(hg,T4),e(Ur,j4),e(Ur,Bu),e(Bu,k4),e(ce,A4),e(ce,zr),e(zr,Cu),e(Cu,dg),e(dg,D4),e(zr,O4),e(zr,Gu),e(Gu,P4),e(ce,R4),e(ce,Mr),e(Mr,Lu),e(Lu,gg),e(gg,S4),e(Mr,N4),e(Mr,Ht),e(Ht,x4),e(Ht,mg),e(mg,I4),e(Ht,H4),e(ce,B4),e(ce,Kr),e(Kr,Uu),e(Uu,$g),e($g,C4),e(Kr,G4),e(Kr,Bt),e(Bt,L4),e(Bt,qg),e(qg,U4),e(Bt,z4),m(a,B1,g),m(a,Ce,g),e(Ce,Ct),e(Ct,_g),E(Fr,_g,null),e(Ce,M4),e(Ce,vg),e(vg,K4),m(a,C1,g),m(a,zu,g),e(zu,F4),m(a,G1,g),E(Gt,a,g),m(a,L1,g),m(a,Jr,g),e(Jr,J4),e(Jr,Wr),e(Wr,W4),m(a,U1,g),m(a,Mu,g),e(Mu,Y4),m(a,z1,g),E(Lt,a,g),m(a,M1,g),m(a,Ku,g),e(Ku,V4),m(a,K1,g),m(a,Ut,g),e(Ut,Eg),e(Eg,Yr),e(Yr,Fu),e(Fu,X4),e(Yr,Q4),e(Yr,yg),e(Ut,Z4),e(Ut,K),e(K,Vr),e(Vr,Xr),e(Xr,wg),e(wg,ek),e(Xr,tk),e(Vr,sk),e(Vr,bg),e(K,ak),e(K,Qr),e(Qr,Ju),e(Ju,rk),e(Qr,nk),e(Qr,Wu),e(Wu,ok),e(K,lk),e(K,Zr),e(Zr,Yu),e(Yu,ik),e(Zr,uk),e(Zr,Vu),e(Vu,ck),e(K,fk),e(K,en),e(en,Xu),e(Xu,Tg),e(Tg,pk),e(en,hk),e(en,Qu),e(Qu,dk),e(K,gk),e(K,tn),e(tn,Zu),e(Zu,mk),e(tn,$k),e(tn,zt),e(zt,qk),e(zt,jg),e(jg,_k),e(zt,vk),e(K,Ek),e(K,sn),e(sn,ec),e(ec,yk),e(sn,wk),e(sn,Mt),e(Mt,bk),e(Mt,kg),e(kg,Tk),e(Mt,jk),e(K,kk),e(K,an),e(an,tc),e(tc,Ak),e(an,Dk),e(an,Kt),e(Kt,Ok),e(Kt,Ag),e(Ag,Pk),e(Kt,Rk),m(a,F1,g),m(a,sc,g),e(sc,Sk),m(a,J1,g),E(Ft,a,g),m(a,W1,g),m(a,Jt,g),e(Jt,Dg),e(Dg,rn),e(rn,ac),e(ac,Nk),e(rn,xk),e(rn,Og),e(Jt,Ik),e(Jt,fe),e(fe,nn),e(nn,rc),e(rc,Pg),e(Pg,Hk),e(nn,Bk),e(nn,nc),e(nc,Ck),e(fe,Gk),e(fe,on),e(on,oc),e(oc,Rg),e(Rg,Lk),e(on,Uk),e(on,lc),e(lc,zk),e(fe,Mk),e(fe,ln),e(ln,ic),e(ic,Sg),e(Sg,Kk),e(ln,Fk),e(ln,uc),e(uc,Jk),e(fe,Wk),e(fe,un),e(un,cc),e(cc,Ng),e(Ng,Yk),e(un,Vk),e(un,fc),e(fc,Xk),m(a,Y1,g),m(a,Ge,g),e(Ge,Wt),e(Wt,xg),E(cn,xg,null),e(Ge,Qk),e(Ge,Ig),e(Ig,Zk),m(a,V1,g),m(a,pc,g),e(pc,e5),m(a,X1,g),E(Yt,a,g),m(a,Q1,g),m(a,fn,g),e(fn,t5),e(fn,pn),e(pn,s5),m(a,Z1,g),m(a,hc,g),e(hc,a5),m(a,ev,g),E(Vt,a,g),m(a,tv,g),m(a,dc,g),e(dc,r5),m(a,sv,g),m(a,Xt,g),e(Xt,Hg),e(Hg,hn),e(hn,gc),e(gc,n5),e(hn,o5),e(hn,Bg),e(Xt,l5),e(Xt,ee),e(ee,dn),e(dn,gn),e(gn,Cg),e(Cg,i5),e(gn,u5),e(dn,c5),e(dn,mc),e(mc,f5),e(ee,p5),e(ee,mn),e(mn,$c),e($c,Gg),e(Gg,h5),e(mn,d5),e(mn,qc),e(qc,g5),e(ee,m5),e(ee,$n),e($n,_c),e(_c,$5),e($n,q5),e($n,Qt),e(Qt,_5),e(Qt,Lg),e(Lg,v5),e(Qt,E5),e(ee,y5),e(ee,qn),e(qn,vc),e(vc,w5),e(qn,b5),e(qn,Zt),e(Zt,T5),e(Zt,Ug),e(Ug,j5),e(Zt,k5),e(ee,A5),e(ee,_n),e(_n,Ec),e(Ec,D5),e(_n,O5),e(_n,es),e(es,P5),e(es,zg),e(zg,R5),e(es,S5),m(a,av,g),m(a,yc,g),e(yc,N5),m(a,rv,g),E(ts,a,g),m(a,nv,g),m(a,ss,g),e(ss,Mg),e(Mg,vn),e(vn,wc),e(wc,x5),e(vn,I5),e(vn,Kg),e(ss,H5),e(ss,En),e(En,yn),e(yn,bc),e(bc,Fg),e(Fg,B5),e(yn,C5),e(yn,Tc),e(Tc,G5),e(En,L5),e(En,wn),e(wn,jc),e(jc,Jg),e(Jg,U5),e(wn,z5),e(wn,kc),e(kc,M5),m(a,ov,g),m(a,Le,g),e(Le,as),e(as,Wg),E(bn,Wg,null),e(Le,K5),e(Le,Yg),e(Yg,F5),m(a,lv,g),m(a,Ac,g),e(Ac,J5),m(a,iv,g),E(rs,a,g),m(a,uv,g),m(a,Tn,g),e(Tn,W5),e(Tn,jn),e(jn,Y5),m(a,cv,g),m(a,Dc,g),e(Dc,V5),m(a,fv,g),E(ns,a,g),m(a,pv,g),m(a,Oc,g),e(Oc,X5),m(a,hv,g),m(a,os,g),e(os,Vg),e(Vg,kn),e(kn,Pc),e(Pc,Q5),e(kn,Z5),e(kn,Xg),e(os,e6),e(os,I),e(I,An),e(An,Dn),e(Dn,Qg),e(Qg,t6),e(Dn,s6),e(An,a6),e(An,Rc),e(Rc,r6),e(I,n6),e(I,On),e(On,Sc),e(Sc,Zg),e(Zg,o6),e(On,l6),e(On,Nc),e(Nc,i6),e(I,u6),e(I,Pn),e(Pn,xc),e(xc,c6),e(Pn,f6),e(Pn,qe),e(qe,p6),e(qe,em),e(em,h6),e(qe,d6),e(qe,tm),e(tm,g6),e(qe,m6),e(I,$6),e(I,Rn),e(Rn,Ic),e(Ic,q6),e(Rn,_6),e(Rn,le),e(le,v6),e(le,sm),e(sm,E6),e(le,y6),e(le,am),e(am,w6),e(le,b6),e(le,rm),e(rm,T6),e(le,j6),e(I,k6),e(I,Sn),e(Sn,Hc),e(Hc,A6),e(Sn,D6),e(Sn,_e),e(_e,O6),e(_e,nm),e(nm,P6),e(_e,R6),e(_e,om),e(om,S6),e(_e,N6),e(I,x6),e(I,Nn),e(Nn,Bc),e(Bc,I6),e(Nn,H6),e(Nn,ls),e(ls,B6),e(ls,lm),e(lm,C6),e(ls,G6),e(I,L6),e(I,xn),e(xn,Cc),e(Cc,U6),e(xn,z6),e(xn,ve),e(ve,M6),e(ve,im),e(im,K6),e(ve,F6),e(ve,um),e(um,J6),e(ve,W6),e(I,Y6),e(I,In),e(In,Gc),e(Gc,V6),e(In,X6),e(In,Ee),e(Ee,Q6),e(Ee,cm),e(cm,Z6),e(Ee,e7),e(Ee,fm),e(fm,t7),e(Ee,s7),e(I,a7),e(I,Hn),e(Hn,Lc),e(Lc,r7),e(Hn,n7),e(Hn,ye),e(ye,o7),e(ye,pm),e(pm,l7),e(ye,i7),e(ye,hm),e(hm,u7),e(ye,c7),e(I,f7),e(I,Bn),e(Bn,Uc),e(Uc,p7),e(Bn,h7),e(Bn,is),e(is,d7),e(is,dm),e(dm,g7),e(is,m7),e(I,$7),e(I,Cn),e(Cn,zc),e(zc,q7),e(Cn,_7),e(Cn,us),e(us,v7),e(us,gm),e(gm,E7),e(us,y7),e(I,w7),e(I,Gn),e(Gn,Mc),e(Mc,mm),e(mm,b7),e(Gn,T7),e(Gn,Kc),e(Kc,j7),e(I,k7),e(I,Ln),e(Ln,Fc),e(Fc,A7),e(Ln,D7),e(Ln,cs),e(cs,O7),e(cs,$m),e($m,P7),e(cs,R7),e(I,S7),e(I,Un),e(Un,Jc),e(Jc,N7),e(Un,x7),e(Un,fs),e(fs,I7),e(fs,qm),e(qm,H7),e(fs,B7),e(I,C7),e(I,zn),e(zn,Wc),e(Wc,G7),e(zn,L7),e(zn,ps),e(ps,U7),e(ps,_m),e(_m,z7),e(ps,M7),m(a,dv,g),m(a,Yc,g),e(Yc,K7),m(a,gv,g),E(hs,a,g),m(a,mv,g),m(a,ds,g),e(ds,vm),e(vm,Mn),e(Mn,Vc),e(Vc,F7),e(Mn,J7),e(Mn,Em),e(ds,W7),e(ds,ym),e(ym,Kn),e(Kn,Xc),e(Xc,wm),e(wm,Y7),e(Kn,V7),e(Kn,Qc),e(Qc,X7),m(a,$v,g),m(a,Ue,g),e(Ue,gs),e(gs,bm),E(Fn,bm,null),e(Ue,Q7),e(Ue,Tm),e(Tm,Z7),m(a,qv,g),m(a,ms,g),e(ms,e9),e(ms,Zc),e(Zc,t9),e(ms,s9),m(a,_v,g),m(a,ze,g),e(ze,$s),e($s,jm),E(Jn,jm,null),e(ze,a9),e(ze,km),e(km,r9),m(a,vv,g),m(a,ef,g),e(ef,n9),m(a,Ev,g),E(qs,a,g),m(a,yv,g),m(a,Me,g),e(Me,o9),e(Me,Wn),e(Wn,l9),e(Me,i9),e(Me,Yn),e(Yn,u9),m(a,wv,g),m(a,tf,g),e(tf,c9),m(a,bv,g),E(_s,a,g),m(a,Tv,g),m(a,sf,g),e(sf,f9),m(a,jv,g),m(a,vs,g),e(vs,Am),e(Am,Vn),e(Vn,af),e(af,p9),e(Vn,h9),e(Vn,Dm),e(vs,d9),e(vs,F),e(F,Xn),e(Xn,Qn),e(Qn,Om),e(Om,g9),e(Qn,m9),e(Xn,$9),e(Xn,rf),e(rf,q9),e(F,_9),e(F,Zn),e(Zn,nf),e(nf,Pm),e(Pm,v9),e(Zn,E9),e(Zn,of),e(of,y9),e(F,w9),e(F,eo),e(eo,lf),e(lf,b9),e(eo,T9),e(eo,x),e(x,j9),e(x,Rm),e(Rm,k9),e(x,A9),e(x,D9),e(x,O9),e(x,Sm),e(Sm,P9),e(x,R9),e(x,S9),e(x,N9),e(x,Nm),e(Nm,x9),e(x,I9),e(x,H9),e(x,B9),e(x,xm),e(xm,C9),e(x,G9),e(x,Im),e(Im,L9),e(x,U9),e(x,z9),e(x,M9),e(x,Hm),e(Hm,K9),e(x,F9),e(x,Bm),e(Bm,J9),e(x,W9),e(x,Y9),e(x,V9),e(x,Cm),e(Cm,X9),e(x,Q9),e(x,Gm),e(Gm,Z9),e(x,e8),e(F,t8),e(F,to),e(to,uf),e(uf,Lm),e(Lm,s8),e(to,a8),e(to,cf),e(cf,r8),e(F,n8),e(F,so),e(so,ff),e(ff,o8),e(so,l8),e(so,Es),e(Es,i8),e(Es,Um),e(Um,u8),e(Es,c8),e(F,f8),e(F,ao),e(ao,pf),e(pf,p8),e(ao,h8),e(ao,ys),e(ys,d8),e(ys,zm),e(zm,g8),e(ys,m8),e(F,$8),e(F,ro),e(ro,hf),e(hf,q8),e(ro,_8),e(ro,ws),e(ws,v8),e(ws,Mm),e(Mm,E8),e(ws,y8),m(a,kv,g),m(a,df,g),e(df,w8),m(a,Av,g),E(bs,a,g),m(a,Dv,g),m(a,Ts,g),e(Ts,Km),e(Km,no),e(no,gf),e(gf,b8),e(no,T8),e(no,Fm),e(Ts,j8),e(Ts,te),e(te,oo),e(oo,mf),e(mf,Jm),e(Jm,k8),e(oo,A8),e(oo,$f),e($f,D8),e(te,O8),e(te,lo),e(lo,qf),e(qf,Wm),e(Wm,P8),e(lo,R8),e(lo,_f),e(_f,S8),e(te,N8),e(te,io),e(io,vf),e(vf,Ym),e(Ym,x8),e(io,I8),e(io,Ef),e(Ef,H8),e(te,B8),e(te,uo),e(uo,yf),e(yf,Vm),e(Vm,C8),e(uo,G8),e(uo,js),e(js,L8),e(js,Xm),e(Xm,U8),e(js,z8),e(te,M8),e(te,co),e(co,wf),e(wf,Qm),e(Qm,K8),e(co,F8),e(co,ks),e(ks,J8),e(ks,Zm),e(Zm,W8),e(ks,Y8),m(a,Ov,g),m(a,Ke,g),e(Ke,As),e(As,e$),E(fo,e$,null),e(Ke,V8),e(Ke,t$),e(t$,X8),m(a,Pv,g),m(a,po,g),e(po,Q8),e(po,bf),e(bf,Z8),m(a,Rv,g),m(a,Fe,g),e(Fe,Ds),e(Ds,s$),E(ho,s$,null),e(Fe,eA),e(Fe,a$),e(a$,tA),m(a,Sv,g),m(a,Tf,g),e(Tf,sA),m(a,Nv,g),E(Os,a,g),m(a,xv,g),m(a,go,g),e(go,aA),e(go,mo),e(mo,rA),m(a,Iv,g),m(a,jf,g),e(jf,nA),m(a,Hv,g),E(Ps,a,g),m(a,Bv,g),m(a,kf,g),e(kf,oA),m(a,Cv,g),m(a,Rs,g),e(Rs,r$),e(r$,$o),e($o,Af),e(Af,lA),e($o,iA),e($o,n$),e(Rs,uA),e(Rs,se),e(se,qo),e(qo,_o),e(_o,o$),e(o$,cA),e(_o,fA),e(qo,pA),e(qo,Df),e(Df,hA),e(se,dA),e(se,vo),e(vo,Of),e(Of,l$),e(l$,gA),e(vo,mA),e(vo,Pf),e(Pf,$A),e(se,qA),e(se,Eo),e(Eo,Rf),e(Rf,_A),e(Eo,vA),e(Eo,Ss),e(Ss,EA),e(Ss,i$),e(i$,yA),e(Ss,wA),e(se,bA),e(se,yo),e(yo,Sf),e(Sf,TA),e(yo,jA),e(yo,Ns),e(Ns,kA),e(Ns,u$),e(u$,AA),e(Ns,DA),e(se,OA),e(se,wo),e(wo,Nf),e(Nf,PA),e(wo,RA),e(wo,xs),e(xs,SA),e(xs,c$),e(c$,NA),e(xs,xA),m(a,Gv,g),m(a,xf,g),e(xf,IA),m(a,Lv,g),m(a,Is,g),e(Is,f$),e(f$,bo),e(bo,If),e(If,HA),e(bo,BA),e(bo,p$),e(Is,CA),e(Is,h$),e(h$,To),e(To,Hf),e(Hf,d$),e(d$,GA),e(To,LA),e(To,Bf),e(Bf,UA),m(a,Uv,g),m(a,Je,g),e(Je,Hs),e(Hs,g$),E(jo,g$,null),e(Je,zA),e(Je,m$),e(m$,MA),m(a,zv,g),m(a,Cf,g),e(Cf,KA),m(a,Mv,g),E(Bs,a,g),m(a,Kv,g),m(a,ko,g),e(ko,FA),e(ko,Ao),e(Ao,JA),m(a,Fv,g),m(a,Gf,g),e(Gf,WA),m(a,Jv,g),E(Cs,a,g),m(a,Wv,g),m(a,Lf,g),e(Lf,YA),m(a,Yv,g),m(a,Gs,g),e(Gs,$$),e($$,Do),e(Do,Uf),e(Uf,VA),e(Do,XA),e(Do,q$),e(Gs,QA),e(Gs,M),e(M,Oo),e(Oo,Po),e(Po,_$),e(_$,ZA),e(Po,eD),e(Oo,tD),e(Oo,zf),e(zf,sD),e(M,aD),e(M,Ro),e(Ro,So),e(So,v$),e(v$,rD),e(So,nD),e(Ro,oD),e(Ro,Mf),e(Mf,lD),e(M,iD),e(M,No),e(No,Kf),e(Kf,uD),e(No,cD),e(No,we),e(we,fD),e(we,E$),e(E$,pD),e(we,hD),e(we,y$),e(y$,dD),e(we,gD),e(M,mD),e(M,xo),e(xo,Ff),e(Ff,$D),e(xo,qD),e(xo,Ls),e(Ls,_D),e(Ls,w$),e(w$,vD),e(Ls,ED),e(M,yD),e(M,Io),e(Io,Jf),e(Jf,b$),e(b$,wD),e(Io,bD),e(Io,Wf),e(Wf,TD),e(M,jD),e(M,Ho),e(Ho,Yf),e(Yf,kD),e(Ho,AD),e(Ho,Us),e(Us,DD),e(Us,T$),e(T$,OD),e(Us,PD),e(M,RD),e(M,Bo),e(Bo,Vf),e(Vf,SD),e(Bo,ND),e(Bo,zs),e(zs,xD),e(zs,j$),e(j$,ID),e(zs,HD),e(M,BD),e(M,Co),e(Co,Xf),e(Xf,CD),e(Co,GD),e(Co,Ms),e(Ms,LD),e(Ms,k$),e(k$,UD),e(Ms,zD),m(a,Vv,g),m(a,Qf,g),e(Qf,MD),m(a,Xv,g),m(a,Zf,g),e(Zf,KD),m(a,Qv,g),E(Ks,a,g),m(a,Zv,g),m(a,Fs,g),e(Fs,A$),e(A$,Go),e(Go,ep),e(ep,FD),e(Go,JD),e(Go,D$),e(Fs,WD),e(Fs,We),e(We,Lo),e(Lo,tp),e(tp,O$),e(O$,YD),e(Lo,VD),e(Lo,sp),e(sp,XD),e(We,QD),e(We,Uo),e(Uo,ap),e(ap,P$),e(P$,ZD),e(Uo,eO),e(Uo,rp),e(rp,tO),e(We,sO),e(We,zo),e(zo,np),e(np,R$),e(R$,aO),e(zo,rO),e(zo,Js),e(Js,nO),e(Js,S$),e(S$,oO),e(Js,lO),m(a,e2,g),m(a,Ye,g),e(Ye,Ws),e(Ws,N$),E(Mo,N$,null),e(Ye,iO),e(Ye,x$),e(x$,uO),m(a,t2,g),m(a,op,g),e(op,cO),m(a,s2,g),E(Ys,a,g),m(a,a2,g),m(a,Ko,g),e(Ko,fO),e(Ko,Fo),e(Fo,pO),m(a,r2,g),m(a,lp,g),e(lp,hO),m(a,n2,g),E(Vs,a,g),m(a,o2,g),m(a,ip,g),e(ip,dO),m(a,l2,g),m(a,Xs,g),e(Xs,I$),e(I$,Jo),e(Jo,up),e(up,gO),e(Jo,mO),e(Jo,H$),e(Xs,$O),e(Xs,N),e(N,Wo),e(Wo,Yo),e(Yo,B$),e(B$,qO),e(Yo,_O),e(Wo,vO),e(Wo,C$),e(N,EO),e(N,Vo),e(Vo,cp),e(cp,yO),e(Vo,wO),e(Vo,fp),e(fp,bO),e(N,TO),e(N,Xo),e(Xo,pp),e(pp,jO),e(Xo,kO),e(Xo,hp),e(hp,AO),e(N,DO),e(N,Qo),e(Qo,dp),e(dp,OO),e(Qo,PO),e(Qo,Qs),e(Qs,RO),e(Qs,G$),e(G$,SO),e(Qs,NO),e(N,xO),e(N,Zo),e(Zo,gp),e(gp,L$),e(L$,IO),e(Zo,HO),e(Zo,mp),e(mp,BO),e(N,CO),e(N,el),e(el,$p),e($p,GO),e(el,LO),e(el,be),e(be,UO),e(be,U$),e(U$,zO),e(be,MO),e(be,z$),e(z$,KO),e(be,FO),e(N,JO),e(N,tl),e(tl,qp),e(qp,WO),e(tl,YO),e(tl,Te),e(Te,VO),e(Te,M$),e(M$,XO),e(Te,QO),e(Te,K$),e(K$,ZO),e(Te,eP),e(N,tP),e(N,sl),e(sl,_p),e(_p,sP),e(sl,aP),e(sl,je),e(je,rP),e(je,F$),e(F$,nP),e(je,oP),e(je,J$),e(J$,lP),e(je,iP),e(N,uP),e(N,al),e(al,vp),e(vp,cP),e(al,fP),e(al,ie),e(ie,pP),e(ie,W$),e(W$,hP),e(ie,dP),e(ie,Y$),e(Y$,gP),e(ie,mP),e(ie,V$),e(V$,$P),e(ie,qP),e(N,_P),e(N,rl),e(rl,Ep),e(Ep,vP),e(rl,EP),e(rl,ke),e(ke,yP),e(ke,X$),e(X$,wP),e(ke,bP),e(ke,Q$),e(Q$,TP),e(ke,jP),e(N,kP),e(N,nl),e(nl,yp),e(yp,AP),e(nl,DP),e(nl,Zs),e(Zs,OP),e(Zs,Z$),e(Z$,PP),e(Zs,RP),e(N,SP),e(N,ol),e(ol,wp),e(wp,NP),e(ol,xP),e(ol,ea),e(ea,IP),e(ea,eq),e(eq,HP),e(ea,BP),e(N,CP),e(N,ll),e(ll,bp),e(bp,tq),e(tq,GP),e(ll,LP),e(ll,Tp),e(Tp,UP),e(N,zP),e(N,il),e(il,jp),e(jp,MP),e(il,KP),e(il,ta),e(ta,FP),e(ta,sq),e(sq,JP),e(ta,WP),e(N,YP),e(N,ul),e(ul,kp),e(kp,VP),e(ul,XP),e(ul,sa),e(sa,QP),e(sa,aq),e(aq,ZP),e(sa,eR),e(N,tR),e(N,cl),e(cl,Ap),e(Ap,sR),e(cl,aR),e(cl,aa),e(aa,rR),e(aa,rq),e(rq,nR),e(aa,oR),m(a,i2,g),m(a,Dp,g),e(Dp,lR),m(a,u2,g),m(a,ra,g),e(ra,nq),e(nq,fl),e(fl,Op),e(Op,iR),e(fl,uR),e(fl,oq),e(ra,cR),e(ra,pe),e(pe,pl),e(pl,Pp),e(Pp,lq),e(lq,fR),e(pl,pR),e(pl,Rp),e(Rp,hR),e(pe,dR),e(pe,hl),e(hl,Sp),e(Sp,iq),e(iq,gR),e(hl,mR),e(hl,Np),e(Np,$R),e(pe,qR),e(pe,dl),e(dl,xp),e(xp,_R),e(dl,vR),e(dl,Ip),e(Ip,ER),e(pe,yR),e(pe,gl),e(gl,Hp),e(Hp,wR),e(gl,bR),e(gl,Bp),e(Bp,TR),m(a,c2,g),m(a,Ve,g),e(Ve,na),e(na,uq),E(ml,uq,null),e(Ve,jR),e(Ve,cq),e(cq,kR),m(a,f2,g),m(a,Cp,g),e(Cp,AR),m(a,p2,g),E(oa,a,g),m(a,h2,g),m(a,Xe,g),e(Xe,DR),e(Xe,$l),e($l,OR),e(Xe,PR),e(Xe,ql),e(ql,RR),m(a,d2,g),m(a,Gp,g),e(Gp,SR),m(a,g2,g),m(a,la,g),e(la,fq),e(fq,_l),e(_l,Lp),e(Lp,NR),e(_l,xR),e(_l,pq),e(la,IR),e(la,ae),e(ae,vl),e(vl,El),e(El,hq),e(hq,HR),e(El,BR),e(vl,CR),e(vl,Up),e(Up,GR),e(ae,LR),e(ae,yl),e(yl,zp),e(zp,dq),e(dq,UR),e(yl,zR),e(yl,Mp),e(Mp,MR),e(ae,KR),e(ae,wl),e(wl,Kp),e(Kp,FR),e(wl,JR),e(wl,ia),e(ia,WR),e(ia,gq),e(gq,YR),e(ia,VR),e(ae,XR),e(ae,bl),e(bl,Fp),e(Fp,QR),e(bl,ZR),e(bl,ua),e(ua,eS),e(ua,mq),e(mq,tS),e(ua,sS),e(ae,aS),e(ae,Tl),e(Tl,Jp),e(Jp,rS),e(Tl,nS),e(Tl,ca),e(ca,oS),e(ca,$q),e($q,lS),e(ca,iS),m(a,m2,g),m(a,Wp,g),e(Wp,uS),m(a,$2,g),m(a,fa,g),e(fa,qq),e(qq,jl),e(jl,Yp),e(Yp,cS),e(jl,fS),e(jl,_q),e(fa,pS),e(fa,vq),e(vq,kl),e(kl,Vp),e(Vp,Eq),e(Eq,hS),e(kl,dS),e(kl,Xp),e(Xp,gS),m(a,q2,g),m(a,Qp,g),e(Qp,mS),m(a,_2,g),m(a,Qe,g),e(Qe,pa),e(pa,yq),E(Al,yq,null),e(Qe,$S),e(Qe,wq),e(wq,qS),m(a,v2,g),m(a,Ze,g),e(Ze,ha),e(ha,bq),E(Dl,bq,null),e(Ze,_S),e(Ze,Tq),e(Tq,vS),m(a,E2,g),m(a,Zp,g),e(Zp,ES),m(a,y2,g),E(da,a,g),m(a,w2,g),E(ga,a,g),m(a,b2,g),m(a,he,g),e(he,yS),e(he,Ol),e(Ol,wS),e(he,bS),e(he,Pl),e(Pl,TS),e(he,jS),e(he,Rl),e(Rl,kS),m(a,T2,g),m(a,eh,g),e(eh,AS),m(a,j2,g),E(ma,a,g),m(a,k2,g),m(a,th,g),e(th,DS),m(a,A2,g),m(a,$a,g),e($a,jq),e(jq,Sl),e(Sl,sh),e(sh,OS),e(Sl,PS),e(Sl,kq),e($a,RS),e($a,Aq),e(Aq,Nl),e(Nl,xl),e(xl,Dq),e(Dq,SS),e(xl,NS),e(Nl,xS),e(Nl,ah),e(ah,IS),m(a,D2,g),m(a,rh,g),e(rh,HS),m(a,O2,g),m(a,nh,g),e(nh,BS),m(a,P2,g),E(qa,a,g),m(a,R2,g),m(a,_a,g),e(_a,Oq),e(Oq,Il),e(Il,oh),e(oh,CS),e(Il,GS),e(Il,Pq),e(_a,LS),e(_a,Rq),e(Rq,Hl),e(Hl,lh),e(lh,Sq),e(Sq,US),e(Hl,zS),e(Hl,ih),e(ih,MS),m(a,S2,g),m(a,et,g),e(et,va),e(va,Nq),E(Bl,Nq,null),e(et,KS),e(et,xq),e(xq,FS),m(a,N2,g),m(a,uh,g),e(uh,JS),m(a,x2,g),E(Ea,a,g),m(a,I2,g),m(a,tt,g),e(tt,WS),e(tt,Cl),e(Cl,YS),e(tt,VS),e(tt,Gl),e(Gl,XS),m(a,H2,g),m(a,ch,g),e(ch,QS),m(a,B2,g),E(ya,a,g),m(a,C2,g),m(a,fh,g),e(fh,ZS),m(a,G2,g),m(a,wa,g),e(wa,Iq),e(Iq,Ll),e(Ll,ph),e(ph,eN),e(Ll,tN),e(Ll,Hq),e(wa,sN),e(wa,Bq),e(Bq,Ul),e(Ul,zl),e(zl,Cq),e(Cq,aN),e(zl,rN),e(Ul,nN),e(Ul,hh),e(hh,oN),m(a,L2,g),m(a,dh,g),e(dh,lN),m(a,U2,g),E(ba,a,g),m(a,z2,g),m(a,Ta,g),e(Ta,Gq),e(Gq,Ml),e(Ml,gh),e(gh,iN),e(Ml,uN),e(Ml,Lq),e(Ta,cN),e(Ta,Kl),e(Kl,Fl),e(Fl,mh),e(mh,Uq),e(Uq,fN),e(Fl,pN),e(Fl,$h),e($h,hN),e(Kl,dN),e(Kl,Jl),e(Jl,qh),e(qh,zq),e(zq,gN),e(Jl,mN),e(Jl,_h),e(_h,$N),m(a,M2,g),m(a,st,g),e(st,ja),e(ja,Mq),E(Wl,Mq,null),e(st,qN),e(st,Kq),e(Kq,_N),m(a,K2,g),m(a,at,g),e(at,ka),e(ka,Fq),E(Yl,Fq,null),e(at,vN),e(at,Jq),e(Jq,EN),m(a,F2,g),m(a,vh,g),e(vh,yN),m(a,J2,g),E(Aa,a,g),m(a,W2,g),m(a,Vl,g),e(Vl,wN),e(Vl,Xl),e(Xl,bN),m(a,Y2,g),m(a,Eh,g),e(Eh,TN),m(a,V2,g),E(Da,a,g),m(a,X2,g),m(a,Oa,g),e(Oa,jN),e(Oa,Ql),e(Ql,kN),e(Oa,AN),m(a,Q2,g),m(a,Pa,g),e(Pa,Wq),e(Wq,Zl),e(Zl,yh),e(yh,DN),e(Zl,ON),e(Zl,Yq),e(Pa,PN),e(Pa,Vq),e(Vq,ei),e(ei,ti),e(ti,Xq),e(Xq,RN),e(ti,SN),e(ei,NN),e(ei,wh),e(wh,xN),m(a,Z2,g),m(a,bh,g),e(bh,IN),m(a,eE,g),E(Ra,a,g),m(a,tE,g),m(a,Sa,g),e(Sa,Qq),e(Qq,si),e(si,Th),e(Th,HN),e(si,BN),e(si,Zq),e(Sa,CN),e(Sa,ai),e(ai,ri),e(ri,jh),e(jh,e_),e(e_,GN),e(ri,LN),e(ri,kh),e(kh,UN),e(ai,zN),e(ai,ni),e(ni,Ah),e(Ah,t_),e(t_,MN),e(ni,KN),e(ni,Dh),e(Dh,FN),m(a,sE,g),m(a,rt,g),e(rt,Na),e(Na,s_),E(oi,s_,null),e(rt,JN),e(rt,a_),e(a_,WN),m(a,aE,g),m(a,Oh,g),e(Oh,YN),m(a,rE,g),E(xa,a,g),m(a,nE,g),m(a,li,g),e(li,VN),e(li,ii),e(ii,XN),m(a,oE,g),m(a,Ph,g),e(Ph,QN),m(a,lE,g),E(Ia,a,g),m(a,iE,g),m(a,Ha,g),e(Ha,ZN),e(Ha,ui),e(ui,ex),e(Ha,tx),m(a,uE,g),m(a,Ba,g),e(Ba,r_),e(r_,ci),e(ci,Rh),e(Rh,sx),e(ci,ax),e(ci,n_),e(Ba,rx),e(Ba,o_),e(o_,fi),e(fi,pi),e(pi,l_),e(l_,nx),e(pi,ox),e(fi,lx),e(fi,Sh),e(Sh,ix),m(a,cE,g),m(a,Nh,g),e(Nh,ux),m(a,fE,g),E(Ca,a,g),m(a,pE,g),m(a,Ga,g),e(Ga,i_),e(i_,hi),e(hi,xh),e(xh,cx),e(hi,fx),e(hi,u_),e(Ga,px),e(Ga,nt),e(nt,di),e(di,Ih),e(Ih,c_),e(c_,hx),e(di,dx),e(di,Hh),e(Hh,gx),e(nt,mx),e(nt,gi),e(gi,Bh),e(Bh,f_),e(f_,$x),e(gi,qx),e(gi,Ch),e(Ch,_x),e(nt,vx),e(nt,mi),e(mi,Gh),e(Gh,p_),e(p_,Ex),e(mi,yx),e(mi,Lh),e(Lh,wx),m(a,hE,g),m(a,ot,g),e(ot,La),e(La,h_),E($i,h_,null),e(ot,bx),e(ot,d_),e(d_,Tx),m(a,dE,g),m(a,Uh,g),e(Uh,jx),m(a,gE,g),E(Ua,a,g),m(a,mE,g),m(a,qi,g),e(qi,kx),e(qi,_i),e(_i,Ax),m(a,$E,g),m(a,zh,g),e(zh,Dx),m(a,qE,g),E(za,a,g),m(a,_E,g),m(a,Ma,g),e(Ma,Ox),e(Ma,vi),e(vi,Px),e(Ma,Rx),m(a,vE,g),m(a,Ka,g),e(Ka,g_),e(g_,Ei),e(Ei,Mh),e(Mh,Sx),e(Ei,Nx),e(Ei,m_),e(Ka,xx),e(Ka,$_),e($_,yi),e(yi,wi),e(wi,q_),e(q_,Ix),e(wi,Hx),e(yi,Bx),e(yi,Kh),e(Kh,Cx),m(a,EE,g),m(a,Fh,g),e(Fh,Gx),m(a,yE,g),E(Fa,a,g),m(a,wE,g),m(a,Ja,g),e(Ja,__),e(__,bi),e(bi,Jh),e(Jh,Lx),e(bi,Ux),e(bi,v_),e(Ja,zx),e(Ja,lt),e(lt,Ti),e(Ti,Wh),e(Wh,E_),e(E_,Mx),e(Ti,Kx),e(Ti,Yh),e(Yh,Fx),e(lt,Jx),e(lt,ji),e(ji,Vh),e(Vh,y_),e(y_,Wx),e(ji,Yx),e(ji,Xh),e(Xh,Vx),e(lt,Xx),e(lt,ki),e(ki,Qh),e(Qh,w_),e(w_,Qx),e(ki,Zx),e(ki,Zh),e(Zh,eI),bE=!0},p(a,[g]){const Ai={};g&2&&(Ai.$$scope={dirty:g,ctx:a}),ht.$set(Ai);const b_={};g&2&&(b_.$$scope={dirty:g,ctx:a}),dt.$set(b_);const T_={};g&2&&(T_.$$scope={dirty:g,ctx:a}),_t.$set(T_);const j_={};g&2&&(j_.$$scope={dirty:g,ctx:a}),wt.$set(j_);const Di={};g&2&&(Di.$$scope={dirty:g,ctx:a}),bt.$set(Di);const k_={};g&2&&(k_.$$scope={dirty:g,ctx:a}),St.$set(k_);const A_={};g&2&&(A_.$$scope={dirty:g,ctx:a}),Nt.$set(A_);const D_={};g&2&&(D_.$$scope={dirty:g,ctx:a}),xt.$set(D_);const O_={};g&2&&(O_.$$scope={dirty:g,ctx:a}),Gt.$set(O_);const P_={};g&2&&(P_.$$scope={dirty:g,ctx:a}),Lt.$set(P_);const Oi={};g&2&&(Oi.$$scope={dirty:g,ctx:a}),Ft.$set(Oi);const R_={};g&2&&(R_.$$scope={dirty:g,ctx:a}),Yt.$set(R_);const S_={};g&2&&(S_.$$scope={dirty:g,ctx:a}),Vt.$set(S_);const N_={};g&2&&(N_.$$scope={dirty:g,ctx:a}),ts.$set(N_);const Pi={};g&2&&(Pi.$$scope={dirty:g,ctx:a}),rs.$set(Pi);const x_={};g&2&&(x_.$$scope={dirty:g,ctx:a}),ns.$set(x_);const I_={};g&2&&(I_.$$scope={dirty:g,ctx:a}),hs.$set(I_);const H_={};g&2&&(H_.$$scope={dirty:g,ctx:a}),qs.$set(H_);const B_={};g&2&&(B_.$$scope={dirty:g,ctx:a}),_s.$set(B_);const ed={};g&2&&(ed.$$scope={dirty:g,ctx:a}),bs.$set(ed);const C_={};g&2&&(C_.$$scope={dirty:g,ctx:a}),Os.$set(C_);const G_={};g&2&&(G_.$$scope={dirty:g,ctx:a}),Ps.$set(G_);const L_={};g&2&&(L_.$$scope={dirty:g,ctx:a}),Bs.$set(L_);const Ri={};g&2&&(Ri.$$scope={dirty:g,ctx:a}),Cs.$set(Ri);const U_={};g&2&&(U_.$$scope={dirty:g,ctx:a}),Ks.$set(U_);const Si={};g&2&&(Si.$$scope={dirty:g,ctx:a}),Ys.$set(Si);const z_={};g&2&&(z_.$$scope={dirty:g,ctx:a}),Vs.$set(z_);const re={};g&2&&(re.$$scope={dirty:g,ctx:a}),oa.$set(re);const Ni={};g&2&&(Ni.$$scope={dirty:g,ctx:a}),da.$set(Ni);const td={};g&2&&(td.$$scope={dirty:g,ctx:a}),ga.$set(td);const M_={};g&2&&(M_.$$scope={dirty:g,ctx:a}),ma.$set(M_);const K_={};g&2&&(K_.$$scope={dirty:g,ctx:a}),qa.$set(K_);const xi={};g&2&&(xi.$$scope={dirty:g,ctx:a}),Ea.$set(xi);const F_={};g&2&&(F_.$$scope={dirty:g,ctx:a}),ya.$set(F_);const J_={};g&2&&(J_.$$scope={dirty:g,ctx:a}),ba.$set(J_);const W_={};g&2&&(W_.$$scope={dirty:g,ctx:a}),Aa.$set(W_);const Ii={};g&2&&(Ii.$$scope={dirty:g,ctx:a}),Da.$set(Ii);const Y_={};g&2&&(Y_.$$scope={dirty:g,ctx:a}),Ra.$set(Y_);const Hi={};g&2&&(Hi.$$scope={dirty:g,ctx:a}),xa.$set(Hi);const V_={};g&2&&(V_.$$scope={dirty:g,ctx:a}),Ia.$set(V_);const Bi={};g&2&&(Bi.$$scope={dirty:g,ctx:a}),Ca.$set(Bi);const X_={};g&2&&(X_.$$scope={dirty:g,ctx:a}),Ua.$set(X_);const Ci={};g&2&&(Ci.$$scope={dirty:g,ctx:a}),za.$set(Ci);const Q_={};g&2&&(Q_.$$scope={dirty:g,ctx:a}),Fa.$set(Q_)},i(a){bE||(y(k.$$.fragment,a),y(Q.$$.fragment,a),y(sr.$$.fragment,a),y(ar.$$.fragment,a),y(ht.$$.fragment,a),y(dt.$$.fragment,a),y(_t.$$.fragment,a),y(qr.$$.fragment,a),y(wt.$$.fragment,a),y(bt.$$.fragment,a),y(Br.$$.fragment,a),y(St.$$.fragment,a),y(Nt.$$.fragment,a),y(xt.$$.fragment,a),y(Fr.$$.fragment,a),y(Gt.$$.fragment,a),y(Lt.$$.fragment,a),y(Ft.$$.fragment,a),y(cn.$$.fragment,a),y(Yt.$$.fragment,a),y(Vt.$$.fragment,a),y(ts.$$.fragment,a),y(bn.$$.fragment,a),y(rs.$$.fragment,a),y(ns.$$.fragment,a),y(hs.$$.fragment,a),y(Fn.$$.fragment,a),y(Jn.$$.fragment,a),y(qs.$$.fragment,a),y(_s.$$.fragment,a),y(bs.$$.fragment,a),y(fo.$$.fragment,a),y(ho.$$.fragment,a),y(Os.$$.fragment,a),y(Ps.$$.fragment,a),y(jo.$$.fragment,a),y(Bs.$$.fragment,a),y(Cs.$$.fragment,a),y(Ks.$$.fragment,a),y(Mo.$$.fragment,a),y(Ys.$$.fragment,a),y(Vs.$$.fragment,a),y(ml.$$.fragment,a),y(oa.$$.fragment,a),y(Al.$$.fragment,a),y(Dl.$$.fragment,a),y(da.$$.fragment,a),y(ga.$$.fragment,a),y(ma.$$.fragment,a),y(qa.$$.fragment,a),y(Bl.$$.fragment,a),y(Ea.$$.fragment,a),y(ya.$$.fragment,a),y(ba.$$.fragment,a),y(Wl.$$.fragment,a),y(Yl.$$.fragment,a),y(Aa.$$.fragment,a),y(Da.$$.fragment,a),y(Ra.$$.fragment,a),y(oi.$$.fragment,a),y(xa.$$.fragment,a),y(Ia.$$.fragment,a),y(Ca.$$.fragment,a),y($i.$$.fragment,a),y(Ua.$$.fragment,a),y(za.$$.fragment,a),y(Fa.$$.fragment,a),bE=!0)},o(a){w(k.$$.fragment,a),w(Q.$$.fragment,a),w(sr.$$.fragment,a),w(ar.$$.fragment,a),w(ht.$$.fragment,a),w(dt.$$.fragment,a),w(_t.$$.fragment,a),w(qr.$$.fragment,a),w(wt.$$.fragment,a),w(bt.$$.fragment,a),w(Br.$$.fragment,a),w(St.$$.fragment,a),w(Nt.$$.fragment,a),w(xt.$$.fragment,a),w(Fr.$$.fragment,a),w(Gt.$$.fragment,a),w(Lt.$$.fragment,a),w(Ft.$$.fragment,a),w(cn.$$.fragment,a),w(Yt.$$.fragment,a),w(Vt.$$.fragment,a),w(ts.$$.fragment,a),w(bn.$$.fragment,a),w(rs.$$.fragment,a),w(ns.$$.fragment,a),w(hs.$$.fragment,a),w(Fn.$$.fragment,a),w(Jn.$$.fragment,a),w(qs.$$.fragment,a),w(_s.$$.fragment,a),w(bs.$$.fragment,a),w(fo.$$.fragment,a),w(ho.$$.fragment,a),w(Os.$$.fragment,a),w(Ps.$$.fragment,a),w(jo.$$.fragment,a),w(Bs.$$.fragment,a),w(Cs.$$.fragment,a),w(Ks.$$.fragment,a),w(Mo.$$.fragment,a),w(Ys.$$.fragment,a),w(Vs.$$.fragment,a),w(ml.$$.fragment,a),w(oa.$$.fragment,a),w(Al.$$.fragment,a),w(Dl.$$.fragment,a),w(da.$$.fragment,a),w(ga.$$.fragment,a),w(ma.$$.fragment,a),w(qa.$$.fragment,a),w(Bl.$$.fragment,a),w(Ea.$$.fragment,a),w(ya.$$.fragment,a),w(ba.$$.fragment,a),w(Wl.$$.fragment,a),w(Yl.$$.fragment,a),w(Aa.$$.fragment,a),w(Da.$$.fragment,a),w(Ra.$$.fragment,a),w(oi.$$.fragment,a),w(xa.$$.fragment,a),w(Ia.$$.fragment,a),w(Ca.$$.fragment,a),w($i.$$.fragment,a),w(Ua.$$.fragment,a),w(za.$$.fragment,a),w(Fa.$$.fragment,a),bE=!1},d(a){t(r),a&&t(c),a&&t(s),b(k),a&&t(S),a&&t(D),b(Q),a&&t(tr),a&&t(Se),a&&t(a1),a&&t(Ui),a&&t(r1),a&&t(ut),a&&t(n1),a&&t(ct),a&&t(o1),a&&t(Ne),b(sr),a&&t(l1),a&&t(xe),b(ar),a&&t(i1),a&&t(zi),a&&t(u1),b(ht,a),a&&t(c1),a&&t(rr),a&&t(f1),a&&t(Mi),a&&t(p1),b(dt,a),a&&t(h1),a&&t(Ki),a&&t(d1),a&&t(gt),a&&t(g1),a&&t(Zi),a&&t(m1),b(_t,a),a&&t($1),a&&t(vt),a&&t(q1),a&&t(Ie),b(qr),a&&t(_1),a&&t(yt),a&&t(v1),b(wt,a),a&&t(E1),a&&t(_r),a&&t(y1),a&&t(cu),a&&t(w1),b(bt,a),a&&t(b1),a&&t(fu),a&&t(T1),a&&t(Tt),a&&t(j1),a&&t(Au),a&&t(k1),a&&t(Pt),a&&t(A1),a&&t(He),b(Br),a&&t(D1),a&&t(Ru),a&&t(O1),b(St,a),a&&t(P1),a&&t(Be),a&&t(R1),a&&t(Su),a&&t(S1),b(Nt,a),a&&t(N1),a&&t(Nu),a&&t(x1),a&&t(xu),a&&t(I1),b(xt,a),a&&t(H1),a&&t(It),a&&t(B1),a&&t(Ce),b(Fr),a&&t(C1),a&&t(zu),a&&t(G1),b(Gt,a),a&&t(L1),a&&t(Jr),a&&t(U1),a&&t(Mu),a&&t(z1),b(Lt,a),a&&t(M1),a&&t(Ku),a&&t(K1),a&&t(Ut),a&&t(F1),a&&t(sc),a&&t(J1),b(Ft,a),a&&t(W1),a&&t(Jt),a&&t(Y1),a&&t(Ge),b(cn),a&&t(V1),a&&t(pc),a&&t(X1),b(Yt,a),a&&t(Q1),a&&t(fn),a&&t(Z1),a&&t(hc),a&&t(ev),b(Vt,a),a&&t(tv),a&&t(dc),a&&t(sv),a&&t(Xt),a&&t(av),a&&t(yc),a&&t(rv),b(ts,a),a&&t(nv),a&&t(ss),a&&t(ov),a&&t(Le),b(bn),a&&t(lv),a&&t(Ac),a&&t(iv),b(rs,a),a&&t(uv),a&&t(Tn),a&&t(cv),a&&t(Dc),a&&t(fv),b(ns,a),a&&t(pv),a&&t(Oc),a&&t(hv),a&&t(os),a&&t(dv),a&&t(Yc),a&&t(gv),b(hs,a),a&&t(mv),a&&t(ds),a&&t($v),a&&t(Ue),b(Fn),a&&t(qv),a&&t(ms),a&&t(_v),a&&t(ze),b(Jn),a&&t(vv),a&&t(ef),a&&t(Ev),b(qs,a),a&&t(yv),a&&t(Me),a&&t(wv),a&&t(tf),a&&t(bv),b(_s,a),a&&t(Tv),a&&t(sf),a&&t(jv),a&&t(vs),a&&t(kv),a&&t(df),a&&t(Av),b(bs,a),a&&t(Dv),a&&t(Ts),a&&t(Ov),a&&t(Ke),b(fo),a&&t(Pv),a&&t(po),a&&t(Rv),a&&t(Fe),b(ho),a&&t(Sv),a&&t(Tf),a&&t(Nv),b(Os,a),a&&t(xv),a&&t(go),a&&t(Iv),a&&t(jf),a&&t(Hv),b(Ps,a),a&&t(Bv),a&&t(kf),a&&t(Cv),a&&t(Rs),a&&t(Gv),a&&t(xf),a&&t(Lv),a&&t(Is),a&&t(Uv),a&&t(Je),b(jo),a&&t(zv),a&&t(Cf),a&&t(Mv),b(Bs,a),a&&t(Kv),a&&t(ko),a&&t(Fv),a&&t(Gf),a&&t(Jv),b(Cs,a),a&&t(Wv),a&&t(Lf),a&&t(Yv),a&&t(Gs),a&&t(Vv),a&&t(Qf),a&&t(Xv),a&&t(Zf),a&&t(Qv),b(Ks,a),a&&t(Zv),a&&t(Fs),a&&t(e2),a&&t(Ye),b(Mo),a&&t(t2),a&&t(op),a&&t(s2),b(Ys,a),a&&t(a2),a&&t(Ko),a&&t(r2),a&&t(lp),a&&t(n2),b(Vs,a),a&&t(o2),a&&t(ip),a&&t(l2),a&&t(Xs),a&&t(i2),a&&t(Dp),a&&t(u2),a&&t(ra),a&&t(c2),a&&t(Ve),b(ml),a&&t(f2),a&&t(Cp),a&&t(p2),b(oa,a),a&&t(h2),a&&t(Xe),a&&t(d2),a&&t(Gp),a&&t(g2),a&&t(la),a&&t(m2),a&&t(Wp),a&&t($2),a&&t(fa),a&&t(q2),a&&t(Qp),a&&t(_2),a&&t(Qe),b(Al),a&&t(v2),a&&t(Ze),b(Dl),a&&t(E2),a&&t(Zp),a&&t(y2),b(da,a),a&&t(w2),b(ga,a),a&&t(b2),a&&t(he),a&&t(T2),a&&t(eh),a&&t(j2),b(ma,a),a&&t(k2),a&&t(th),a&&t(A2),a&&t($a),a&&t(D2),a&&t(rh),a&&t(O2),a&&t(nh),a&&t(P2),b(qa,a),a&&t(R2),a&&t(_a),a&&t(S2),a&&t(et),b(Bl),a&&t(N2),a&&t(uh),a&&t(x2),b(Ea,a),a&&t(I2),a&&t(tt),a&&t(H2),a&&t(ch),a&&t(B2),b(ya,a),a&&t(C2),a&&t(fh),a&&t(G2),a&&t(wa),a&&t(L2),a&&t(dh),a&&t(U2),b(ba,a),a&&t(z2),a&&t(Ta),a&&t(M2),a&&t(st),b(Wl),a&&t(K2),a&&t(at),b(Yl),a&&t(F2),a&&t(vh),a&&t(J2),b(Aa,a),a&&t(W2),a&&t(Vl),a&&t(Y2),a&&t(Eh),a&&t(V2),b(Da,a),a&&t(X2),a&&t(Oa),a&&t(Q2),a&&t(Pa),a&&t(Z2),a&&t(bh),a&&t(eE),b(Ra,a),a&&t(tE),a&&t(Sa),a&&t(sE),a&&t(rt),b(oi),a&&t(aE),a&&t(Oh),a&&t(rE),b(xa,a),a&&t(nE),a&&t(li),a&&t(oE),a&&t(Ph),a&&t(lE),b(Ia,a),a&&t(iE),a&&t(Ha),a&&t(uE),a&&t(Ba),a&&t(cE),a&&t(Nh),a&&t(fE),b(Ca,a),a&&t(pE),a&&t(Ga),a&&t(hE),a&&t(ot),b($i),a&&t(dE),a&&t(Uh),a&&t(gE),b(Ua,a),a&&t(mE),a&&t(qi),a&&t($E),a&&t(zh),a&&t(qE),b(za,a),a&&t(_E),a&&t(Ma),a&&t(vE),a&&t(Ka),a&&t(EE),a&&t(Fh),a&&t(yE),b(Fa,a),a&&t(wE),a&&t(Ja)}}}const pY={local:"detailed-parameters",sections:[{local:"which-task-is-used-by-this-model",title:"Which task is used by this model ?"},{local:"natural-language-processing",sections:[{local:"fill-mask-task",title:"Fill Mask task"},{local:"summarization-task",title:"Summarization task"},{local:"question-answering-task",title:"Question Answering task"},{local:"table-question-answering-task",title:"Table Question Answering task"},{local:"text-classification-task",title:"Text Classification task"},{local:"text-generation-task",title:"Text Generation task"},{local:"text2text-generation-task",title:"Text2Text Generation task"},{local:"token-classification-task",title:"Token Classification task"},{local:"named-entity-recognition-ner-task",title:"Named Entity Recognition (NER) task"},{local:"translation-task",title:"Translation task"},{local:"zeroshot-classification-task",title:"Zero-Shot Classification task"},{local:"conversational-task",title:"Conversational task"},{local:"feature-extraction-task",title:"Feature Extraction task"}],title:"Natural Language Processing"},{local:"audio",sections:[{local:"automatic-speech-recognition-task",title:"Automatic Speech Recognition task"},{local:"audio-classification-task",title:"Audio Classification task"}],title:"Audio"},{local:"computer-vision",sections:[{local:"image-classification-task",title:"Image Classification task"},{local:"object-detection-task",title:"Object Detection task"},{local:"image-segmentation-task",title:"Image Segmentation task"}],title:"Computer Vision"}],title:"Detailed parameters"};function hY(q){return LF(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class qY extends HF{constructor(r){super();BF(this,r,hY,fY,CF,{})}}export{qY as default,pY as metadata};
