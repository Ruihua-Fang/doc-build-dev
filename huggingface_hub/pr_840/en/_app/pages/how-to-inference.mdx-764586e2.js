import{S as Js,i as Us,s as Qs,e as r,k as f,w as L,t as a,M as Vs,c as o,d as t,m as h,a as l,x as z,h as n,b as p,N as Xs,F as s,g as c,y as G,L as Zs,q as R,o as W,B as J,v as et}from"../chunks/vendor-d3924577.js";import{I as st}from"../chunks/IconCopyLink-f94c3d80.js";import{C as fe}from"../chunks/CodeBlock-ff545b14.js";function tt(ks){let I,he,w,k,U,j,Pe,Q,$e,ue,b,Te,V,Oe,Ne,X,Ce,De,de,K,q,He,P,Se,Ke,me,F,B,As,ge,d,Fe,M,Be,Me,Z,Ye,Le,ee,ze,Ge,_e,$,be,m,Re,T,We,Je,O,Ue,Qe,se,Ve,Xe,ye,N,Ie,g,Ze,te,es,ss,ae,ts,as,ne,ns,rs,we,C,ke,_,os,D,ls,is,re,cs,ps,oe,fs,hs,Ae,H,Ee,u,us,le,ds,ms,ie,gs,_s,ce,bs,ys,pe,Is,ws,ve,S,xe;return j=new st({}),$=new fe({props:{code:`from huggingface_hub.inference_api import InferenceApi
inference = InferenceApi(repo_id="bert-base-uncased", token=API_TOKEN)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> huggingface_hub.inference_api <span class="hljs-keyword">import</span> InferenceApi
<span class="hljs-meta">&gt;&gt;&gt; </span>inference = InferenceApi(repo_id=<span class="hljs-string">&quot;bert-base-uncased&quot;</span>, token=API_TOKEN)`}}),N=new fe({props:{code:`from huggingface_hub.inference_api import InferenceApi
inference = InferenceApi(repo_id="bert-base-uncased", token=API_TOKEN)
inference(inputs="The goal of life is [MASK].")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> huggingface_hub.inference_api <span class="hljs-keyword">import</span> InferenceApi
<span class="hljs-meta">&gt;&gt;&gt; </span>inference = InferenceApi(repo_id=<span class="hljs-string">&quot;bert-base-uncased&quot;</span>, token=API_TOKEN)
<span class="hljs-meta">&gt;&gt;&gt; </span>inference(inputs=<span class="hljs-string">&quot;The goal of life is [MASK].&quot;</span>)
[{<span class="hljs-string">&#x27;sequence&#x27;</span>: <span class="hljs-string">&#x27;the goal of life is life.&#x27;</span>, <span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.10933292657136917</span>, <span class="hljs-string">&#x27;token&#x27;</span>: <span class="hljs-number">2166</span>, <span class="hljs-string">&#x27;token_str&#x27;</span>: <span class="hljs-string">&#x27;life&#x27;</span>}]`}}),C=new fe({props:{code:`inference = InferenceApi(repo_id="deepset/roberta-base-squad2", token=API_TOKEN)
inputs = {"question":"Where is Hugging Face headquarters?", "context":"Hugging Face is based in Brooklyn, New York. There is also an office in Paris, France."}
inference(inputs)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>inference = InferenceApi(repo_id=<span class="hljs-string">&quot;deepset/roberta-base-squad2&quot;</span>, token=API_TOKEN)
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = {<span class="hljs-string">&quot;question&quot;</span>:<span class="hljs-string">&quot;Where is Hugging Face headquarters?&quot;</span>, <span class="hljs-string">&quot;context&quot;</span>:<span class="hljs-string">&quot;Hugging Face is based in Brooklyn, New York. There is also an office in Paris, France.&quot;</span>}
<span class="hljs-meta">&gt;&gt;&gt; </span>inference(inputs)
{<span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.94622403383255</span>, <span class="hljs-string">&#x27;start&#x27;</span>: <span class="hljs-number">25</span>, <span class="hljs-string">&#x27;end&#x27;</span>: <span class="hljs-number">43</span>, <span class="hljs-string">&#x27;answer&#x27;</span>: <span class="hljs-string">&#x27;Brooklyn, New York&#x27;</span>}`}}),H=new fe({props:{code:`inference = InferenceApi(repo_id="typeform/distilbert-base-uncased-mnli", token=API_TOKEN)
inputs = "Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!"
params = {"candidate_labels":["refund", "legal", "faq"]}
inference(inputs, params)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>inference = InferenceApi(repo_id=<span class="hljs-string">&quot;typeform/distilbert-base-uncased-mnli&quot;</span>, token=API_TOKEN)
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = <span class="hljs-string">&quot;Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>params = {<span class="hljs-string">&quot;candidate_labels&quot;</span>:[<span class="hljs-string">&quot;refund&quot;</span>, <span class="hljs-string">&quot;legal&quot;</span>, <span class="hljs-string">&quot;faq&quot;</span>]}
<span class="hljs-meta">&gt;&gt;&gt; </span>inference(inputs, params)
{<span class="hljs-string">&#x27;sequence&#x27;</span>: <span class="hljs-string">&#x27;Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!&#x27;</span>, <span class="hljs-string">&#x27;labels&#x27;</span>: [<span class="hljs-string">&#x27;refund&#x27;</span>, <span class="hljs-string">&#x27;faq&#x27;</span>, <span class="hljs-string">&#x27;legal&#x27;</span>], <span class="hljs-string">&#x27;scores&#x27;</span>: [<span class="hljs-number">0.9378499388694763</span>, <span class="hljs-number">0.04914155602455139</span>, <span class="hljs-number">0.013008488342165947</span>]}`}}),S=new fe({props:{code:'inference = InferenceApi(repo_id="paraphrase-xlm-r-multilingual-v1", task="feature-extraction", token=API_TOKEN)',highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>inference = InferenceApi(repo_id=<span class="hljs-string">&quot;paraphrase-xlm-r-multilingual-v1&quot;</span>, task=<span class="hljs-string">&quot;feature-extraction&quot;</span>, token=API_TOKEN)'}}),{c(){I=r("meta"),he=f(),w=r("h1"),k=r("a"),U=r("span"),L(j.$$.fragment),Pe=f(),Q=r("span"),$e=a("How to access the Inference API"),ue=f(),b=r("p"),Te=a("The Inference API provides fast inference for your hosted models. The Inference API can be accessed via usual HTTP requests with your favorite programming languages, but the "),V=r("code"),Oe=a("huggingface_hub"),Ne=a(" library has a client wrapper to access the Inference API programmatically. This guide will show you how to make calls to the Inference API with the "),X=r("code"),Ce=a("huggingface_hub"),De=a(" library."),de=f(),K=r("p"),q=r("strong"),He=a("If you want to make the HTTP calls directly, please refer to "),P=r("a"),Se=a("Accelerated Inference API Documentation"),Ke=a(" or to the sample snippets visible on every supported model page."),me=f(),F=r("p"),B=r("img"),ge=f(),d=r("p"),Fe=a("Begin by creating an instance of the "),M=r("a"),Be=a("InferenceApi"),Me=a(" with the model repository ID of the model you want to use. You can find your "),Z=r("code"),Ye=a("API_TOKEN"),Le=a(" under Settings from your Hugging Face account. The "),ee=r("code"),ze=a("API_TOKEN"),Ge=a(" will allow you to send requests to the Inference API."),_e=f(),L($.$$.fragment),be=f(),m=r("p"),Re=a("The pipeline is determined from the metadata in the model card and configuration files (see "),T=r("a"),We=a("here"),Je=a(" for more details). For example, when using the "),O=r("a"),Ue=a("bert-base-uncased"),Qe=a(" model, the Inference API can automatically infer that this model should be used for a "),se=r("code"),Ve=a("fill-mask"),Xe=a(" task."),ye=f(),L(N.$$.fragment),Ie=f(),g=r("p"),Ze=a("Each task requires a different type of input. A "),te=r("code"),es=a("question-answering"),ss=a(" task expects a dictionary with the "),ae=r("code"),ts=a("question"),as=a(" and "),ne=r("code"),ns=a("context"),rs=a(" keys as the input:"),we=f(),L(C.$$.fragment),ke=f(),_=r("p"),os=a("Some tasks may require additional parameters (see "),D=r("a"),ls=a("here"),is=a(" for a detailed list of all parameters for each task). As an example, for "),re=r("code"),cs=a("zero-shot-classification"),ps=a(" tasks, the model needs candidate labels that can be supplied to "),oe=r("code"),fs=a("params"),hs=a(":"),Ae=f(),L(H.$$.fragment),Ee=f(),u=r("p"),us=a("Some models may support multiple tasks. The "),le=r("code"),ds=a("sentence-transformers"),ms=a(" models can complete both "),ie=r("code"),gs=a("sentence-similarity"),_s=a(" and "),ce=r("code"),bs=a("feature-extraction"),ys=a(" tasks. Specify which task you want to perform with the "),pe=r("code"),Is=a("task"),ws=a(" parameter:"),ve=f(),L(S.$$.fragment),this.h()},l(e){const i=Vs('[data-svelte="svelte-1phssyn"]',document.head);I=o(i,"META",{name:!0,content:!0}),i.forEach(t),he=h(e),w=o(e,"H1",{class:!0});var je=l(w);k=o(je,"A",{id:!0,class:!0,href:!0});var Es=l(k);U=o(Es,"SPAN",{});var vs=l(U);z(j.$$.fragment,vs),vs.forEach(t),Es.forEach(t),Pe=h(je),Q=o(je,"SPAN",{});var xs=l(Q);$e=n(xs,"How to access the Inference API"),xs.forEach(t),je.forEach(t),ue=h(e),b=o(e,"P",{});var Y=l(b);Te=n(Y,"The Inference API provides fast inference for your hosted models. The Inference API can be accessed via usual HTTP requests with your favorite programming languages, but the "),V=o(Y,"CODE",{});var js=l(V);Oe=n(js,"huggingface_hub"),js.forEach(t),Ne=n(Y," library has a client wrapper to access the Inference API programmatically. This guide will show you how to make calls to the Inference API with the "),X=o(Y,"CODE",{});var qs=l(X);Ce=n(qs,"huggingface_hub"),qs.forEach(t),De=n(Y," library."),Y.forEach(t),de=h(e),K=o(e,"P",{});var Ps=l(K);q=o(Ps,"STRONG",{});var qe=l(q);He=n(qe,"If you want to make the HTTP calls directly, please refer to "),P=o(qe,"A",{href:!0,rel:!0});var $s=l(P);Se=n($s,"Accelerated Inference API Documentation"),$s.forEach(t),Ke=n(qe," or to the sample snippets visible on every supported model page."),qe.forEach(t),Ps.forEach(t),me=h(e),F=o(e,"P",{});var Ts=l(F);B=o(Ts,"IMG",{src:!0,alt:!0}),Ts.forEach(t),ge=h(e),d=o(e,"P",{});var A=l(d);Fe=n(A,"Begin by creating an instance of the "),M=o(A,"A",{href:!0});var Os=l(M);Be=n(Os,"InferenceApi"),Os.forEach(t),Me=n(A," with the model repository ID of the model you want to use. You can find your "),Z=o(A,"CODE",{});var Ns=l(Z);Ye=n(Ns,"API_TOKEN"),Ns.forEach(t),Le=n(A," under Settings from your Hugging Face account. The "),ee=o(A,"CODE",{});var Cs=l(ee);ze=n(Cs,"API_TOKEN"),Cs.forEach(t),Ge=n(A," will allow you to send requests to the Inference API."),A.forEach(t),_e=h(e),z($.$$.fragment,e),be=h(e),m=o(e,"P",{});var E=l(m);Re=n(E,"The pipeline is determined from the metadata in the model card and configuration files (see "),T=o(E,"A",{href:!0,rel:!0});var Ds=l(T);We=n(Ds,"here"),Ds.forEach(t),Je=n(E," for more details). For example, when using the "),O=o(E,"A",{href:!0,rel:!0});var Hs=l(O);Ue=n(Hs,"bert-base-uncased"),Hs.forEach(t),Qe=n(E," model, the Inference API can automatically infer that this model should be used for a "),se=o(E,"CODE",{});var Ss=l(se);Ve=n(Ss,"fill-mask"),Ss.forEach(t),Xe=n(E," task."),E.forEach(t),ye=h(e),z(N.$$.fragment,e),Ie=h(e),g=o(e,"P",{});var v=l(g);Ze=n(v,"Each task requires a different type of input. A "),te=o(v,"CODE",{});var Ks=l(te);es=n(Ks,"question-answering"),Ks.forEach(t),ss=n(v," task expects a dictionary with the "),ae=o(v,"CODE",{});var Fs=l(ae);ts=n(Fs,"question"),Fs.forEach(t),as=n(v," and "),ne=o(v,"CODE",{});var Bs=l(ne);ns=n(Bs,"context"),Bs.forEach(t),rs=n(v," keys as the input:"),v.forEach(t),we=h(e),z(C.$$.fragment,e),ke=h(e),_=o(e,"P",{});var x=l(_);os=n(x,"Some tasks may require additional parameters (see "),D=o(x,"A",{href:!0,rel:!0});var Ms=l(D);ls=n(Ms,"here"),Ms.forEach(t),is=n(x," for a detailed list of all parameters for each task). As an example, for "),re=o(x,"CODE",{});var Ys=l(re);cs=n(Ys,"zero-shot-classification"),Ys.forEach(t),ps=n(x," tasks, the model needs candidate labels that can be supplied to "),oe=o(x,"CODE",{});var Ls=l(oe);fs=n(Ls,"params"),Ls.forEach(t),hs=n(x,":"),x.forEach(t),Ae=h(e),z(H.$$.fragment,e),Ee=h(e),u=o(e,"P",{});var y=l(u);us=n(y,"Some models may support multiple tasks. The "),le=o(y,"CODE",{});var zs=l(le);ds=n(zs,"sentence-transformers"),zs.forEach(t),ms=n(y," models can complete both "),ie=o(y,"CODE",{});var Gs=l(ie);gs=n(Gs,"sentence-similarity"),Gs.forEach(t),_s=n(y," and "),ce=o(y,"CODE",{});var Rs=l(ce);bs=n(Rs,"feature-extraction"),Rs.forEach(t),ys=n(y," tasks. Specify which task you want to perform with the "),pe=o(y,"CODE",{});var Ws=l(pe);Is=n(Ws,"task"),Ws.forEach(t),ws=n(y," parameter:"),y.forEach(t),ve=h(e),z(S.$$.fragment,e),this.h()},h(){p(I,"name","hf:doc:metadata"),p(I,"content",JSON.stringify(at)),p(k,"id","how-to-access-the-inference-api"),p(k,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(k,"href","#how-to-access-the-inference-api"),p(w,"class","relative group"),p(P,"href","https://api-inference.huggingface.co/docs/python/html/index.html"),p(P,"rel","nofollow"),Xs(B.src,As="/docs/assets/hub/inference_api_snippet.png")||p(B,"src",As),p(B,"alt","Snippet of code to make calls to the Inference API"),p(M,"href","/docs/huggingface_hub/pr_840/en/package_reference/inference_api#huggingface_hub.InferenceApi"),p(T,"href","https://huggingface.co/docs/hub/main#how-is-a-models-type-of-inference-api-and-widget-determined"),p(T,"rel","nofollow"),p(O,"href","https://huggingface.co/bert-base-uncased"),p(O,"rel","nofollow"),p(D,"href","https://api-inference.huggingface.co/docs/python/html/detailed_parameters.html"),p(D,"rel","nofollow")},m(e,i){s(document.head,I),c(e,he,i),c(e,w,i),s(w,k),s(k,U),G(j,U,null),s(w,Pe),s(w,Q),s(Q,$e),c(e,ue,i),c(e,b,i),s(b,Te),s(b,V),s(V,Oe),s(b,Ne),s(b,X),s(X,Ce),s(b,De),c(e,de,i),c(e,K,i),s(K,q),s(q,He),s(q,P),s(P,Se),s(q,Ke),c(e,me,i),c(e,F,i),s(F,B),c(e,ge,i),c(e,d,i),s(d,Fe),s(d,M),s(M,Be),s(d,Me),s(d,Z),s(Z,Ye),s(d,Le),s(d,ee),s(ee,ze),s(d,Ge),c(e,_e,i),G($,e,i),c(e,be,i),c(e,m,i),s(m,Re),s(m,T),s(T,We),s(m,Je),s(m,O),s(O,Ue),s(m,Qe),s(m,se),s(se,Ve),s(m,Xe),c(e,ye,i),G(N,e,i),c(e,Ie,i),c(e,g,i),s(g,Ze),s(g,te),s(te,es),s(g,ss),s(g,ae),s(ae,ts),s(g,as),s(g,ne),s(ne,ns),s(g,rs),c(e,we,i),G(C,e,i),c(e,ke,i),c(e,_,i),s(_,os),s(_,D),s(D,ls),s(_,is),s(_,re),s(re,cs),s(_,ps),s(_,oe),s(oe,fs),s(_,hs),c(e,Ae,i),G(H,e,i),c(e,Ee,i),c(e,u,i),s(u,us),s(u,le),s(le,ds),s(u,ms),s(u,ie),s(ie,gs),s(u,_s),s(u,ce),s(ce,bs),s(u,ys),s(u,pe),s(pe,Is),s(u,ws),c(e,ve,i),G(S,e,i),xe=!0},p:Zs,i(e){xe||(R(j.$$.fragment,e),R($.$$.fragment,e),R(N.$$.fragment,e),R(C.$$.fragment,e),R(H.$$.fragment,e),R(S.$$.fragment,e),xe=!0)},o(e){W(j.$$.fragment,e),W($.$$.fragment,e),W(N.$$.fragment,e),W(C.$$.fragment,e),W(H.$$.fragment,e),W(S.$$.fragment,e),xe=!1},d(e){t(I),e&&t(he),e&&t(w),J(j),e&&t(ue),e&&t(b),e&&t(de),e&&t(K),e&&t(me),e&&t(F),e&&t(ge),e&&t(d),e&&t(_e),J($,e),e&&t(be),e&&t(m),e&&t(ye),J(N,e),e&&t(Ie),e&&t(g),e&&t(we),J(C,e),e&&t(ke),e&&t(_),e&&t(Ae),J(H,e),e&&t(Ee),e&&t(u),e&&t(ve),J(S,e)}}}const at={local:"how-to-access-the-inference-api",title:"How to access the Inference API"};function nt(ks){return et(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class it extends Js{constructor(I){super();Us(this,I,nt,tt,Qs,{})}}export{it as default,at as metadata};
