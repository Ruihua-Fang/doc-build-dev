import{S as cs,i as ps,s as fs,e as r,k as f,w as z,t as a,M as hs,c as o,d as s,m as h,a as l,x as G,h as n,b as p,N as us,F as t,g as c,y as L,L as ds,q as R,o as W,B as J}from"../chunks/vendor-7b1da053.js";import{I as is}from"../chunks/IconCopyLink-d24e9a6f.js";import{C as we}from"../chunks/CodeBlock-5f39b59e.js";function ms(Ke){let y,Q,$,P,I,v,ee,T,Fe,te,Be,be,k,A,se,O,Ye,ae,Me,Ie,w,ze,ne,Ge,Le,re,Re,We,ke,U,N,Je,H,Qe,Ue,ve,V,X,Ht,Ae,d,Ve,oe,Xe,Ze,le,et,tt,ie,st,at,Ee,C,je,m,nt,D,rt,ot,S,lt,it,ce,ct,pt,qe,K,xe,g,ft,pe,ht,ut,fe,dt,mt,he,gt,_t,$e,F,Pe,_,yt,B,wt,bt,ue,It,kt,de,vt,At,Te,Y,Oe,u,Et,me,jt,qt,ge,xt,$t,_e,Pt,Tt,ye,Ot,Nt,Ne,M,He;return T=new is({}),O=new is({}),C=new we({props:{code:`from huggingface_hub.inference_api import InferenceApi
inference = InferenceApi(repo_id="bert-base-uncased", token=API_TOKEN)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> huggingface_hub.inference_api <span class="hljs-keyword">import</span> InferenceApi
<span class="hljs-meta">&gt;&gt;&gt; </span>inference = InferenceApi(repo_id=<span class="hljs-string">&quot;bert-base-uncased&quot;</span>, token=API_TOKEN)`}}),K=new we({props:{code:`from huggingface_hub.inference_api import InferenceApi
inference = InferenceApi(repo_id="bert-base-uncased", token=API_TOKEN)
inference(inputs="The goal of life is [MASK].")
[{'sequence': 'the goal of life is life.', 'score': 0.10933292657136917, 'token': 2166, 'token_str': 'life'}]`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> huggingface_hub.inference_api <span class="hljs-keyword">import</span> InferenceApi
<span class="hljs-meta">&gt;&gt;&gt; </span>inference = InferenceApi(repo_id=<span class="hljs-string">&quot;bert-base-uncased&quot;</span>, token=API_TOKEN)
<span class="hljs-meta">&gt;&gt;&gt; </span>inference(inputs=<span class="hljs-string">&quot;The goal of life is [MASK].&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>[{<span class="hljs-string">&#x27;sequence&#x27;</span>: <span class="hljs-string">&#x27;the goal of life is life.&#x27;</span>, <span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.10933292657136917</span>, <span class="hljs-string">&#x27;token&#x27;</span>: <span class="hljs-number">2166</span>, <span class="hljs-string">&#x27;token_str&#x27;</span>: <span class="hljs-string">&#x27;life&#x27;</span>}]`}}),F=new we({props:{code:`inference = InferenceApi(repo_id="deepset/roberta-base-squad2", token=API_TOKEN)
inputs = {"question":"Where is Hugging Face headquarters?", "context":"Hugging Face is based in Brooklyn, New York. There is also an office in Paris, France."}
inference(inputs)
{'score': 0.94622403383255, 'start': 25, 'end': 43, 'answer': 'Brooklyn, New York'}`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>inference = InferenceApi(repo_id=<span class="hljs-string">&quot;deepset/roberta-base-squad2&quot;</span>, token=API_TOKEN)
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = {<span class="hljs-string">&quot;question&quot;</span>:<span class="hljs-string">&quot;Where is Hugging Face headquarters?&quot;</span>, <span class="hljs-string">&quot;context&quot;</span>:<span class="hljs-string">&quot;Hugging Face is based in Brooklyn, New York. There is also an office in Paris, France.&quot;</span>}
<span class="hljs-meta">&gt;&gt;&gt; </span>inference(inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>{<span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.94622403383255</span>, <span class="hljs-string">&#x27;start&#x27;</span>: <span class="hljs-number">25</span>, <span class="hljs-string">&#x27;end&#x27;</span>: <span class="hljs-number">43</span>, <span class="hljs-string">&#x27;answer&#x27;</span>: <span class="hljs-string">&#x27;Brooklyn, New York&#x27;</span>}`}}),Y=new we({props:{code:`inference = InferenceApi(repo_id="typeform/distilbert-base-uncased-mnli", token=API_TOKEN)
inputs = "Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!"
params = {"candidate_labels":["refund", "legal", "faq"]}
inference(inputs, params)
{'sequence': 'Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!', 'labels': ['refund', 'faq', 'legal'], 'scores': [0.9378499388694763, 0.04914155602455139, 0.013008488342165947]}`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>inference = InferenceApi(repo_id=<span class="hljs-string">&quot;typeform/distilbert-base-uncased-mnli&quot;</span>, token=API_TOKEN)
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = <span class="hljs-string">&quot;Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>params = {<span class="hljs-string">&quot;candidate_labels&quot;</span>:[<span class="hljs-string">&quot;refund&quot;</span>, <span class="hljs-string">&quot;legal&quot;</span>, <span class="hljs-string">&quot;faq&quot;</span>]}
<span class="hljs-meta">&gt;&gt;&gt; </span>inference(inputs, params)
<span class="hljs-meta">&gt;&gt;&gt; </span>{<span class="hljs-string">&#x27;sequence&#x27;</span>: <span class="hljs-string">&#x27;Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!&#x27;</span>, <span class="hljs-string">&#x27;labels&#x27;</span>: [<span class="hljs-string">&#x27;refund&#x27;</span>, <span class="hljs-string">&#x27;faq&#x27;</span>, <span class="hljs-string">&#x27;legal&#x27;</span>], <span class="hljs-string">&#x27;scores&#x27;</span>: [<span class="hljs-number">0.9378499388694763</span>, <span class="hljs-number">0.04914155602455139</span>, <span class="hljs-number">0.013008488342165947</span>]}`}}),M=new we({props:{code:'inference = InferenceApi(repo_id="paraphrase-xlm-r-multilingual-v1", task="feature-extraction", token=API_TOKEN)',highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>inference = InferenceApi(repo_id=<span class="hljs-string">&quot;paraphrase-xlm-r-multilingual-v1&quot;</span>, task=<span class="hljs-string">&quot;feature-extraction&quot;</span>, token=API_TOKEN)'}}),{c(){y=r("meta"),Q=f(),$=r("hr"),P=f(),I=r("h2"),v=r("a"),ee=r("span"),z(T.$$.fragment),Fe=f(),te=r("span"),Be=a("title: How to programmatically access the Inference API"),be=f(),k=r("h1"),A=r("a"),se=r("span"),z(O.$$.fragment),Ye=f(),ae=r("span"),Me=a("How to programmatically access the Inference API"),Ie=f(),w=r("p"),ze=a("The Inference API provides fast inference for your hosted models. The Inference API can be accessed via usual HTTP requests with your favorite programming languages, but the "),ne=r("code"),Ge=a("huggingface_hub"),Le=a(" library has a client wrapper to access the Inference API programmatically. This guide will show you how to make calls to the Inference API with the "),re=r("code"),Re=a("huggingface_hub"),We=a(" library."),ke=f(),U=r("p"),N=r("strong"),Je=a("If you want to make the HTTP calls directly, please refer to "),H=r("a"),Qe=a("Accelerated Inference API Documentation"),Ue=a(" or to the sample snippets visible on every supported model page."),ve=f(),V=r("p"),X=r("img"),Ae=f(),d=r("p"),Ve=a("Begin by creating an instance of the "),oe=r("code"),Xe=a("InferenceApi"),Ze=a(" with a specific model repository ID. You can find your "),le=r("code"),et=a("API_TOKEN"),tt=a(" under Settings from your Hugging Face account. The "),ie=r("code"),st=a("API_TOKEN"),at=a(" will allow you to send requests to the Inference API."),Ee=f(),z(C.$$.fragment),je=f(),m=r("p"),nt=a("The pipeline is determined from the metadata in the model card and configuration files (see "),D=r("a"),rt=a("here"),ot=a(" for more details). For example, when using the "),S=r("a"),lt=a("bert-base-uncased"),it=a(" model, the Inference API can automatically infer that this model should be used for a "),ce=r("code"),ct=a("fill-mask"),pt=a(" task."),qe=f(),z(K.$$.fragment),xe=f(),g=r("p"),ft=a("Each task requires a different type of input. A "),pe=r("code"),ht=a("question-answering"),ut=a(" task expects a dictionary with the "),fe=r("code"),dt=a("question"),mt=a(" and "),he=r("code"),gt=a("context"),_t=a(" keys as the input:"),$e=f(),z(F.$$.fragment),Pe=f(),_=r("p"),yt=a("Some tasks may require additional parameters (see "),B=r("a"),wt=a("here"),bt=a(" for a detailed list of all parameters for each task). As an example, for "),ue=r("code"),It=a("zero-shot-classification"),kt=a(" tasks, the model needs candidate labels that can be supplied to "),de=r("code"),vt=a("params"),At=a(":"),Te=f(),z(Y.$$.fragment),Oe=f(),u=r("p"),Et=a("Some models may support multiple tasks. The "),me=r("code"),jt=a("sentence-transformers"),qt=a(" models can complete both "),ge=r("code"),xt=a("sentence-similarity"),$t=a(" and "),_e=r("code"),Pt=a("feature-extraction"),Tt=a(" tasks. Specify which task you want to perform with the "),ye=r("code"),Ot=a("task"),Nt=a(" parameter:"),Ne=f(),z(M.$$.fragment),this.h()},l(e){const i=hs('[data-svelte="svelte-1phssyn"]',document.head);y=o(i,"META",{name:!0,content:!0}),i.forEach(s),Q=h(e),$=o(e,"HR",{}),P=h(e),I=o(e,"H2",{class:!0});var Ce=l(I);v=o(Ce,"A",{id:!0,class:!0,href:!0});var Ct=l(v);ee=o(Ct,"SPAN",{});var Dt=l(ee);G(T.$$.fragment,Dt),Dt.forEach(s),Ct.forEach(s),Fe=h(Ce),te=o(Ce,"SPAN",{});var St=l(te);Be=n(St,"title: How to programmatically access the Inference API"),St.forEach(s),Ce.forEach(s),be=h(e),k=o(e,"H1",{class:!0});var De=l(k);A=o(De,"A",{id:!0,class:!0,href:!0});var Kt=l(A);se=o(Kt,"SPAN",{});var Ft=l(se);G(O.$$.fragment,Ft),Ft.forEach(s),Kt.forEach(s),Ye=h(De),ae=o(De,"SPAN",{});var Bt=l(ae);Me=n(Bt,"How to programmatically access the Inference API"),Bt.forEach(s),De.forEach(s),Ie=h(e),w=o(e,"P",{});var Z=l(w);ze=n(Z,"The Inference API provides fast inference for your hosted models. The Inference API can be accessed via usual HTTP requests with your favorite programming languages, but the "),ne=o(Z,"CODE",{});var Yt=l(ne);Ge=n(Yt,"huggingface_hub"),Yt.forEach(s),Le=n(Z," library has a client wrapper to access the Inference API programmatically. This guide will show you how to make calls to the Inference API with the "),re=o(Z,"CODE",{});var Mt=l(re);Re=n(Mt,"huggingface_hub"),Mt.forEach(s),We=n(Z," library."),Z.forEach(s),ke=h(e),U=o(e,"P",{});var zt=l(U);N=o(zt,"STRONG",{});var Se=l(N);Je=n(Se,"If you want to make the HTTP calls directly, please refer to "),H=o(Se,"A",{href:!0,rel:!0});var Gt=l(H);Qe=n(Gt,"Accelerated Inference API Documentation"),Gt.forEach(s),Ue=n(Se," or to the sample snippets visible on every supported model page."),Se.forEach(s),zt.forEach(s),ve=h(e),V=o(e,"P",{});var Lt=l(V);X=o(Lt,"IMG",{src:!0,alt:!0}),Lt.forEach(s),Ae=h(e),d=o(e,"P",{});var E=l(d);Ve=n(E,"Begin by creating an instance of the "),oe=o(E,"CODE",{});var Rt=l(oe);Xe=n(Rt,"InferenceApi"),Rt.forEach(s),Ze=n(E," with a specific model repository ID. You can find your "),le=o(E,"CODE",{});var Wt=l(le);et=n(Wt,"API_TOKEN"),Wt.forEach(s),tt=n(E," under Settings from your Hugging Face account. The "),ie=o(E,"CODE",{});var Jt=l(ie);st=n(Jt,"API_TOKEN"),Jt.forEach(s),at=n(E," will allow you to send requests to the Inference API."),E.forEach(s),Ee=h(e),G(C.$$.fragment,e),je=h(e),m=o(e,"P",{});var j=l(m);nt=n(j,"The pipeline is determined from the metadata in the model card and configuration files (see "),D=o(j,"A",{href:!0,rel:!0});var Qt=l(D);rt=n(Qt,"here"),Qt.forEach(s),ot=n(j," for more details). For example, when using the "),S=o(j,"A",{href:!0,rel:!0});var Ut=l(S);lt=n(Ut,"bert-base-uncased"),Ut.forEach(s),it=n(j," model, the Inference API can automatically infer that this model should be used for a "),ce=o(j,"CODE",{});var Vt=l(ce);ct=n(Vt,"fill-mask"),Vt.forEach(s),pt=n(j," task."),j.forEach(s),qe=h(e),G(K.$$.fragment,e),xe=h(e),g=o(e,"P",{});var q=l(g);ft=n(q,"Each task requires a different type of input. A "),pe=o(q,"CODE",{});var Xt=l(pe);ht=n(Xt,"question-answering"),Xt.forEach(s),ut=n(q," task expects a dictionary with the "),fe=o(q,"CODE",{});var Zt=l(fe);dt=n(Zt,"question"),Zt.forEach(s),mt=n(q," and "),he=o(q,"CODE",{});var es=l(he);gt=n(es,"context"),es.forEach(s),_t=n(q," keys as the input:"),q.forEach(s),$e=h(e),G(F.$$.fragment,e),Pe=h(e),_=o(e,"P",{});var x=l(_);yt=n(x,"Some tasks may require additional parameters (see "),B=o(x,"A",{href:!0,rel:!0});var ts=l(B);wt=n(ts,"here"),ts.forEach(s),bt=n(x," for a detailed list of all parameters for each task). As an example, for "),ue=o(x,"CODE",{});var ss=l(ue);It=n(ss,"zero-shot-classification"),ss.forEach(s),kt=n(x," tasks, the model needs candidate labels that can be supplied to "),de=o(x,"CODE",{});var as=l(de);vt=n(as,"params"),as.forEach(s),At=n(x,":"),x.forEach(s),Te=h(e),G(Y.$$.fragment,e),Oe=h(e),u=o(e,"P",{});var b=l(u);Et=n(b,"Some models may support multiple tasks. The "),me=o(b,"CODE",{});var ns=l(me);jt=n(ns,"sentence-transformers"),ns.forEach(s),qt=n(b," models can complete both "),ge=o(b,"CODE",{});var rs=l(ge);xt=n(rs,"sentence-similarity"),rs.forEach(s),$t=n(b," and "),_e=o(b,"CODE",{});var os=l(_e);Pt=n(os,"feature-extraction"),os.forEach(s),Tt=n(b," tasks. Specify which task you want to perform with the "),ye=o(b,"CODE",{});var ls=l(ye);Ot=n(ls,"task"),ls.forEach(s),Nt=n(b," parameter:"),b.forEach(s),Ne=h(e),G(M.$$.fragment,e),this.h()},h(){p(y,"name","hf:doc:metadata"),p(y,"content",JSON.stringify(gs)),p(v,"id","title:-how-to-programmatically-access-the-inference-api"),p(v,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(v,"href","#title:-how-to-programmatically-access-the-inference-api"),p(I,"class","relative group"),p(A,"id","how-to-programmatically-access-the-inference-api"),p(A,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(A,"href","#how-to-programmatically-access-the-inference-api"),p(k,"class","relative group"),p(H,"href","https://api-inference.huggingface.co/docs/python/html/index.html"),p(H,"rel","nofollow"),us(X.src,Ht="/docs/assets/hub/inference_api_snippet.png")||p(X,"src",Ht),p(X,"alt","Snippet of code to make calls to the Inference API"),p(D,"href","https://huggingface.co/docs/hub/main#how-is-a-models-type-of-inference-api-and-widget-determined"),p(D,"rel","nofollow"),p(S,"href","https://huggingface.co/bert-base-uncased"),p(S,"rel","nofollow"),p(B,"href","https://api-inference.huggingface.co/docs/python/html/detailed_parameters.html"),p(B,"rel","nofollow")},m(e,i){t(document.head,y),c(e,Q,i),c(e,$,i),c(e,P,i),c(e,I,i),t(I,v),t(v,ee),L(T,ee,null),t(I,Fe),t(I,te),t(te,Be),c(e,be,i),c(e,k,i),t(k,A),t(A,se),L(O,se,null),t(k,Ye),t(k,ae),t(ae,Me),c(e,Ie,i),c(e,w,i),t(w,ze),t(w,ne),t(ne,Ge),t(w,Le),t(w,re),t(re,Re),t(w,We),c(e,ke,i),c(e,U,i),t(U,N),t(N,Je),t(N,H),t(H,Qe),t(N,Ue),c(e,ve,i),c(e,V,i),t(V,X),c(e,Ae,i),c(e,d,i),t(d,Ve),t(d,oe),t(oe,Xe),t(d,Ze),t(d,le),t(le,et),t(d,tt),t(d,ie),t(ie,st),t(d,at),c(e,Ee,i),L(C,e,i),c(e,je,i),c(e,m,i),t(m,nt),t(m,D),t(D,rt),t(m,ot),t(m,S),t(S,lt),t(m,it),t(m,ce),t(ce,ct),t(m,pt),c(e,qe,i),L(K,e,i),c(e,xe,i),c(e,g,i),t(g,ft),t(g,pe),t(pe,ht),t(g,ut),t(g,fe),t(fe,dt),t(g,mt),t(g,he),t(he,gt),t(g,_t),c(e,$e,i),L(F,e,i),c(e,Pe,i),c(e,_,i),t(_,yt),t(_,B),t(B,wt),t(_,bt),t(_,ue),t(ue,It),t(_,kt),t(_,de),t(de,vt),t(_,At),c(e,Te,i),L(Y,e,i),c(e,Oe,i),c(e,u,i),t(u,Et),t(u,me),t(me,jt),t(u,qt),t(u,ge),t(ge,xt),t(u,$t),t(u,_e),t(_e,Pt),t(u,Tt),t(u,ye),t(ye,Ot),t(u,Nt),c(e,Ne,i),L(M,e,i),He=!0},p:ds,i(e){He||(R(T.$$.fragment,e),R(O.$$.fragment,e),R(C.$$.fragment,e),R(K.$$.fragment,e),R(F.$$.fragment,e),R(Y.$$.fragment,e),R(M.$$.fragment,e),He=!0)},o(e){W(T.$$.fragment,e),W(O.$$.fragment,e),W(C.$$.fragment,e),W(K.$$.fragment,e),W(F.$$.fragment,e),W(Y.$$.fragment,e),W(M.$$.fragment,e),He=!1},d(e){s(y),e&&s(Q),e&&s($),e&&s(P),e&&s(I),J(T),e&&s(be),e&&s(k),J(O),e&&s(Ie),e&&s(w),e&&s(ke),e&&s(U),e&&s(ve),e&&s(V),e&&s(Ae),e&&s(d),e&&s(Ee),J(C,e),e&&s(je),e&&s(m),e&&s(qe),J(K,e),e&&s(xe),e&&s(g),e&&s($e),J(F,e),e&&s(Pe),e&&s(_),e&&s(Te),J(Y,e),e&&s(Oe),e&&s(u),e&&s(Ne),J(M,e)}}}const gs={local:"how-to-programmatically-access-the-inference-api",title:"How to programmatically access the Inference API"};function _s(Ke,y,Q){let{fw:$}=y;return Ke.$$set=P=>{"fw"in P&&Q(0,$=P.fw)},[$]}class Is extends cs{constructor(y){super();ps(this,y,_s,ms,fs,{fw:0})}}export{Is as default,gs as metadata};
